N-gram-based Machine Translation
Jose? B. Marin?o?
Rafael E. Banchs?
Josep M. Crego?
Adria` de Gispert?
Patrik Lambert?
Jose? A. R. Fonollosa?
Marta R. Costa-jussa`?
Universitat Polite`cnica de Catalunya
This article describes in detail an n-gram approach to statistical machine translation. This ap-
proach consists of a log-linear combination of a translation model based on n-grams of bilingual
units, which are referred to as tuples, along with four specific feature functions. Translation
performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English
and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS).
1. Introduction
The beginnings of statistical machine translation (SMT) can be traced back to the early
fifties, closely related to the ideas from which information theory arose (Shannon and
Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during
World War II. According to this view, machine translation was conceived as the problem
of finding a sentence by decoding a given ?encrypted? version of it (Weaver 1955).
Although the idea seemed very feasible, enthusiasm faded shortly afterward because of
the computational limitations of the time (Hutchins 1986). Finally, during the nineties,
two factors made it possible for SMT to become an actual and practical technology:
first, significant increment in both the computational power and storage capacity of
computers, and second, the availability of large volumes of bilingual data.
The first SMT systems were developed in the early nineties (Brown et al 1990, 1993).
These systems were based on the so-called noisy channel approach, which models the
probability of a target language sentence T given a source language sentence S as the
product of a translation-model probability p(S|T), which accounts for adequacy of trans-
lation contents, times a target language probability p(T), which accounts for fluency
of target constructions. For these first SMT systems, translation-model probabilities at
the sentence level were approximated from word-based translation models that were
trained by using bilingual corpora (Brown et al 1993). In the case of target language
probabilities, these were generally trained from monolingual data by using n-grams.
Present SMT systems have evolved from the original ones in such a way that
mainly differ from them in two respects: first, word-based translation models have been
? Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain.
Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for
publication: 5 July 2006
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 4
replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och,
and Marcu 2003) which are directly estimated from aligned bilingual corpora by consid-
ering relative frequencies, and second, the noisy channel approach has been expanded
to a more general maximum entropy approach in which a log-linear combination of
multiple feature functions is implemented (Och and Ney 2002).
As an extension of the machine translation problem, technological advances in the
fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it
possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron,
and Norvig 1992). According to this, SMT has also been approached from a finite-state
point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini,
and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi
2000). In this SMT approach, translation models are implemented by means of finite-
state transducers for which transition probabilities are learned from bilingual data.
As opposed to phrase-based translation models, which consider probabilities between
target and source units referred to as phrases, finite-state translation models rely on
probabilities among sequences of bilingual units, which are defined by the transitions
of the transducer.
The translation system described in this article implements a translation model that
has been derived from the finite-state perspective?more specifically, from the work of
Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier
work the translation model is implemented by using a finite-state transducer, in the sys-
tem presented here the translation model is implemented by using n-grams. In this way,
the proposed translation system can take full advantage of the smoothing and consist-
ency provided by standard back-off n-gram models. The translation model presented
here actually constitutes a language model of a sort of ?bilanguage? composed of bilin-
gual units, which will be referred to as tuples (de Gispert and Marin?o 2002). An alterna-
tive approach, which relies on bilingual-unit unigram probabilities, was developed by
Tillmann and Xia (2003); in contrast, the approach presented here considers bilingual-
unit n-gram probabilities. In addition to the tuple n-gram translation model, the
translation system presented here implements four specific feature functions that are
log-linearly combined along with the translation model for performing the decoding
(Marin?o et al 2005).
This article is intended to provide a detailed description of the n-gram-based
translation system, as well as to demonstrate the system performance in a wide-
domain, large-vocabulary translation task. The article is structured as follows. First,
Section 2 presents a complete description of the n-gram-based translation model. Then,
Section 3 describes in detail the additional feature functions that, along with the trans-
lation model, compose the n-gram-based SMT system implemented. Section 4 describes
the European Parliament Plenary Session (EPPS) data, as well as the most relevant
details about the translation tasks considered. Section 5 presents and discusses the
translation experiments and their results. Finally, Section 6 presents some conclusions
and intended further work.
2. The Tuple N-gram Model
This section describes in detail the tuple n-gram translation model, which constitutes
the core model implemented by the n-gram-based SMT system. First, the bilingual unit
definition and model computation are presented in Section 2.1. Then, some important
refinements to the basic translation model are provided and discussed in Section 2.2.
Finally, Section 2.3 discusses issues related to n-gram-based decoding.
528
Marin?o et al N-gram-based Machine Translation
2.1 Tuple Extraction and Model Computation
As already mentioned, the translation model implemented by the described SMT sys-
tem is based on bilingual n-grams. This model actually constitutes a language model of
a particular bilanguage composed of bilingual units that are referred to as tuples. In this
way, the translation model probabilities at the sentence level are approximated by using
n-grams of tuples, such as described by the following equation:
p(T, S) ?
K
?
k=1
p((t, s)k|(t, s)k?1, (t, s)k?2, . . . , (t, s)k?n+1) (1)
where t refers to target, s to source, and (t, s)k to the kth tuple of a given bilingual
sentence pair. It is important to note that since both languages are linked up in tuples,
the context information provided by this translation model is bilingual.
Tuples are extracted from a word-to-word aligned corpus in such a way that a
unique segmentation of the bilingual corpus is achieved. Although in principle any
Viterbi alignment should allow for tuple extraction, the resulting tuple vocabulary
depends highly on the particular alignment set considered, and this impacts the trans-
lation results. According to our experience, the best performance is achieved when
the union of the source-to-target and target-to-source alignment sets (IBM models;
Brown et al [1993]) is used for tuple extraction (some experimental results regarding
this issue are presented in Section 4.2.2). Additionally, the use of the union can also
be justified from a theoretical point of view by considering that the union set typically
exhibits higher recall values than do other alignment sets such as the intersection and
source-to-target.
In this way, as opposed to other implementations, where one-to-one (Bangalore
and Riccardi 2000) or one-to-many (Casacuberta and Vidal 2004) alignments are used,
tuples are extracted from many-to-many alignments. This implementation produces
a monotonic segmentation of bilingual sentence pairs, which allows for simulta-
neously capturing contextual and reordering information into the bilingual translation
unit structures. This segmentation also allows for estimating the n-gram probabil-
ities appearing in (1). In order to guarantee a unique segmentation of the corpus,
tuple extraction is performed according to the following constraints (Crego, Marin?o,
and de Gispert 2004):
 a monotonic segmentation of each bilingual sentence pair is produced,
 no word inside the tuple is aligned to words outside the tuple, and
 no smaller tuples can be extracted without violating the previous
constraints.
Notice that, according to this, tuples can be formally defined as the set of shortest
phrases that provides a monotonic segmentation of the bilingual corpus. Figure 1
presents a simple example illustrating the unique tuple segmentation for a given pair of
sentences, as well as the complete phrase set.
The first important observation from Figure 1 is related to the possible occurrence
of tuples containing unaligned elements on the target side. This is the case for tuple 1.
Tuples of this kind should be handled in an alternative way for the system to be able
to provide appropriate translations for such unaligned elements. The problem of how
529
Computational Linguistics Volume 32, Number 4
Figure 1
Example of tuple extraction. Tuples are extracted from Viterbi alignments in such a way that the
set of shortest bilingual units that provide a monotonous segmentation of the bilingual sentence
pair is achieved.
to handle this kind of situation, which we refer to as involving source-nulled tuples, is
discussed in detail in Section 2.2.2.
Also, as observed from Figure 1, the total number of tuples is significantly lower
than the total number of phrases, and, in most of the cases, longer phrases can be
constructed by considering tuple n-grams, which is the case for phrases 2, 6, 7, 9, 10,
and 11. However, phrases 4 and 5 cannot be generated from tuples. In general, the tuple
representation is not able to provide translations for individual words that appear tied
to other words unless they occur alone in some other tuple. This problem, which we
refer to as embedded words, is discussed in detail in Section 2.2.1.
Another important observation from Figure 1 is that each tuple length is implicitly
defined by the word links in the alignment. As opposed to phrase-extraction proce-
dures, for which a maximum phrase length should be defined to avoid a vocabulary
explosion, tuple extraction procedures do not have any control over tuple lengths.
According to this, the tuple approach will strongly benefit from the structural similarity
between the languages under consideration. Then, for close language pairs, tuples are
expected to successfully handle those short reordering patterns that are included in
the tuple structure, as in the case of ?traducciones perfectas : perfect translations?
presented in Figure 1. On the other hand, in the case of distant pairs of languages, for
which a large number of long tuples are expected to occur, the approach will more easily
fail to provide a good translation model due to tuple sparseness.
2.2 Translation Model Refinements
The basic n-gram translation model, as defined in the previous section, exhibits some
important limitations that can be easily overcome by incorporating specific changes in
530
Marin?o et al N-gram-based Machine Translation
either the tuple vocabulary or the n-gram model. This section describes such limitations
and provides a detailed description of the implemented refinements.
2.2.1 Embedded Words. The first issue regarding the n-gram translation model is related
to the already mentioned problem of embedded words, which refers to the fact that
the tuple representation is not able to provide translations for individual words all the
time. Embedded words can become a serious drawback when they occur in relatively
significant numbers in the tuple vocabulary.
Consider for example the word translations in Figure 1. As seen from the figure, this
word appears embedded into tuple ?traducciones perfectas : perfect translations.? If a
similar situation is encountered for all other occurrences of that word in the training
corpus, then no translation probability for an independent occurrence of that word
will exist. A more relevant example would be the case of the embedded word perfect
since this adjective always moves relative to the noun it is modifying. In this case,
providing the translation system with a word-to-word translation probability for ?per-
fectas : perfect? only guarantees that the decoder will have a translation option for an
isolated occurrence of such words but does not guarantee anything about word order.
So, certainly, any adjective?noun combination including the word perfect, which has not
been seen during the training stage, will be translated in the wrong order. Accordingly,
the problem resulting from embedded words can be partially solved by incorporating a
bilingual dictionary able to provide word-to-word translation when required by the
translation system. A more complete treatment for this problem must consider the
implementation of a word-reordering strategy for the proposed SMT approach (as will
be discussed in Section 6, this constitutes one of the main concerns for our further
research).
In our n-gram-based SMT implementation, the following strategy for handling em-
bedded words is considered. First, one-word tuples for each detected embedded word
are extracted from the training data and their corresponding word-to-word translation
probabilities are computed by using relative frequencies. Then, the tuple n-gram model
is enhanced by including all embedded-word tuples as unigrams into the model. Since
a high-precision alignment set is desirable for extracting such one-word tuples and
estimating their probabilities, the intersection of both alignments, source to target and
target-to-source, is used instead of the union.
In the particular case of the EPPS tasks considered in this work, embedded words
do not constitute a real problem because of the great amount of training material and
the reduced size of the test data set (see Section 4.1 for a detailed description of the
EPPS data set). On the contrary, in other translation tasks with less available training
material, the embedded-word handling strategy described above has been very useful
(de Gispert, Marin?o, and Crego 2004).
2.2.2 Tuples with Empty Source Sides. The second important issue regarding the
n-gram translation model is related to tuples with empty source sides, hereinafter
referred to as source-nulled tuples. In the tuple n-gram model implementation, it fre-
quently happens that some target words linked to NULL end up producing tuples with
NULL source sides. Consider, for example, the first tuple of the example presented in
Figure 1. In this example, ?NULL : we? is a source-nulled tuple if Spanish is considered
to be the source language. Notice that tuples of this kind cannot be allowed since no
NULL is expected to occur in a translation input.
The classical solution to this problem in the finite-state transducer framework is
the inclusion of epsilon arcs (Knight and Al-Onaizan 1998; Bangalore and Riccardi
531
Computational Linguistics Volume 32, Number 4
2000). However, epsilon arcs significantly increase decoding complexity. In our n-gram
system implementation, this problem is easily solved by preprocessing the union set of
alignments before extracting tuples, in such a way that any target word that is linked
to NULL is attached to either its preceding word or its following word. In this way, no
target word remains linked to NULL, and source-nulled tuples will not occur during
tuple extraction.
Some different strategies for handling target words aligned to NULL have been
considered. In the simplest strategy, which will be referred to as the attach-to-right strat-
egy, target words aligned to NULL are always attached to their following word. This
simple strategy happens to provide better results, for English-to-Spanish and Spanish-
to-English translations, than the opposite one (attachment to the previous word), and
also better than a more sophisticated strategy that considers bigram probabilities for
deciding whether a given word should be attached to the following or to the pre-
vious one.
Notice that in the particular cases of Spanish and English, the attach-to-right strat-
egy can be justified heuristically. Indeed, when translating from Spanish to English,
most of the source-nulled tuples result from omitted verbal subjects, which is a very
common situation in Spanish. This is the case for the first tuple in Figure 1. Suppose,
for instance, that the attach-to-right strategy is used in Figure 1; in such a case, the
tuple ?quisie?ramos : would like? will be replaced by the new tuple ?quisie?ramos : we
would like,? which actually makes a better translation unit, at least from a grammatical
point of view. Similarly, some common situations can be identified for translations in
the English-to-Spanish direction, such as omitted determiners (e.g., ?I want information
about European countries : quiero informacio?n sobre los pa??ses Europeos?). Again,
the attach-to-right strategy for the unaligned Spanish determiner los seems to be the
best one.
Experimental results comparing the attach-to-right strategy to an additional strat-
egy based on a statistical translation lexicon are provided in Section 5.1.3.
2.2.3 Tuple Vocabulary Pruning. The third and last issue regarding the n-gram transla-
tion model is related to the computational costs resulting from the tuple vocabulary size
during decoding. The idea behind this refinement is to reduce both computation time
and storage requirements without degrading translation performance. In our n-gram-
based SMT system implementation, the tuple vocabulary is pruned by using histogram
counts. This pruning is performed by keeping the N most frequent tuples with common
source sides.
Notice that such a pruning, because it is performed before computing tuple n-gram
probabilities, has a direct impact on the translation model probabilities and then on
the overall system performance. For this reason, the pruning parameter N is critical
for efficient usage of the translation system. While a low value of N will significantly
decrease translation quality, on the other hand, a large value of N will provide the
same translation quality than a more adequate N, but with a significant increment in
computational costs. The optimal value for this parameter depends on data and should
be adjusted empirically for each considered translation task.
2.3 N-gram-based Decoding
Decoding for the n-gram-based translation model is slightly different from phrase-
based decoding. For this reason, a specific decoding tool had to be implemented. This
532
Marin?o et al N-gram-based Machine Translation
section briefly describes MARIE, the n-gram based search engine developed for our
SMT system (Crego, Marin?o, and de Gispert 2005a).
MARIE implements a beam-search strategy based on dynamic programming. The
decoding is performed monotonically and is guided by the source. During decoding,
partial-translation hypotheses are arranged into different stacks according to the total
number of source words they cover. In this way, a given hypothesis only competes with
those hypotheses that provide the same source-word coverage. At every translation
step, stacks are pruned to keep decoding tractable. MARIE allows for two different
pruning methods:
 Threshold pruning: for which all partial-translation hypotheses scoring
below a predetermined threshold value are eliminated.
 Histogram pruning: for which the maximum number of partial-translation
hypotheses to be considered is limited to the K-best ranked ones.
Additionally, MARIE allows for hypothesis recombination, which provides a more
efficient search. In the implemented algorithm, partial-translation hypotheses are re-
combined if they coincide exactly in both the present tuple and the tuple trigram history.
MARIE also allows for considering additional feature functions during decoding.
All these models are taken into account simultaneously, along with the n-gram trans-
lation model. In our SMT system implementation, four additional feature functions are
considered. These functions are described in detail in Section 3.2.
3. Feature Functions for the N-gram-based SMT System
This section describes in detail some feature functions that are implemented along with
the n-gram translation model for the complete translation system. First, in subsection
3.1, the log-linear combination framework and the implemented optimization proce-
dure are discussed. Then, four specific feature functions that constitute our SMT system
are detailed in Section 3.2.
3.1 Log-linear Combination Framework
As mentioned in the Introduction, in recent translation systems the noisy channel ap-
proach has been replaced by a more general approach, which is founded on the princi-
ples of maximum entropy (Berger, Della Pietra, and Della Pietra 1996). In this approach,
the corresponding translation for a given source language sentence S is defined by the
target language sentence that maximizes a log-linear combination of multiple feature
functions hi(S, T) (Och and Ney 2002), such as described by the following equation:
argmax
T
?
m
?mhm(S, T) (2)
where ?m represents the coefficient of the mth feature function hm(S, T), which ac-
tually corresponds to a log-scaled version of the mth-model probabilities. Optimal
values for the ?m coefficients are estimated via an optimization procedure by using a
development data set.
533
Computational Linguistics Volume 32, Number 4
3.2 Translation System Features
In addition to the tuple n-gram translation model, our n-gram-based SMT system
implements four feature functions: a target-language model, a word-bonus model, and
two lexicon models. These system features are described next.
3.2.1 Target-language Model. This feature provides information about the target lan-
guage structure and fluency. It favors those partial-translation hypotheses that are more
likely to constitute correctly structured target sentences over those that are not. The
model is implemented by using a word n-gram model of the target language, which is
computed according to the following expression:
hTL(T, S) = hTL(T) = log
K
?
k=1
p(wk|wk?1, wk?2, . . . , wk?n+1) (3)
where wk refers to the kth word in the considered partial-translation hypothesis. Notice
that this model only depends on the target side of the data, and can in fact be trained by
including additional information from other available monolingual corpora.
3.2.2 Word-bonus Model. This feature introduces a bonus that depends on the partial-
translation hypothesis length. This is done to compensate for the system preference for
short translations over large ones. The model is implemented through a bonus factor
that directly depends on the total number of words contained in the partial-translation
hypothesis, and it is computed as follows:
hWP(T, S) = hWP(T) = M (4)
where M is the number of words contained in the partial-translation hypothesis.
3.2.3 Source-to-Target Lexicon Model. This feature actually constitutes a complemen-
tary translation model. This model provides, for a given tuple, a translation probability
estimate between its source and target sides. This feature is implemented by using the
IBM-1 lexical parameters (Brown et al 1993; Och et al 2004). Accordingly, the source-
to-target lexicon probability is computed for each tuple according to the following
equation:
hLF(T, S) = log 1(I + 1)J
J
?
j=1
I
?
i=0
q(tnj |sni ) (5)
where sni and t
n
j are the ith and jth words in the source and target sides of tuple (t, s)n,
with I and J the corresponding total number of words in each side. In the equation,
q(.) refers to IBM-1 lexical parameters, which are estimated from alignments computed
in the source-to-target direction.
3.2.4 Target-to-Source Lexicon Model. Similar to the previous feature, this feature
function constitutes a complementary translation model too. It is computed in ex-
534
Marin?o et al N-gram-based Machine Translation
actly the same way the previous model is, with the only difference that IBM-1 lexical
parameters are estimated from alignments computed in the target-to-source direction
instead.
4. EPPS Translation Task
This section describes in detail the most relevant issues about the translation tasks con-
sidered. Section 4.1 describes the EPPS data set that is used, and Section 4.2 presents the
overall implementation details in regard to preprocessing, training, and optimization.
4.1 Corpus Description
The EPPS data set is composed of the official plenary session transcriptions of the Eu-
ropean Parliament, which are currently available in eleven different languages (Koehn
2002). However, in the case of the results presented here, we have used the Spanish and
English versions of the EPPS data that have been prepared by RWTH Aachen University
in the context of the European Project TC-STAR. The training, development, and test
data used include session transcriptions from April 1996 until September 2004, from
October 21 until October 28, 2004, and from November 15 until November 18, 2004,
respectively.
Table 1 presents the basic statistics for the training, development, and test data sets
for each considered language. More specifically, the statistics shown in Table 1 are the
number of sentences, the number of words, the vocabulary size (or number of distinct
words), the average sentence length in number of words, and the number of available
translation references.
As seen from Table 1, although the total number of words in the training set is
very similar for both languages, vocabulary sizes are substantially different. Indeed,
the Spanish vocabulary is approximately 60% larger than the English vocabulary. This
can be explained by the more inflected nature of Spanish, which is particularly evident
in the case of nouns, adjectives, and verbs, which may have many different forms de-
pending on gender, number, tense, and mode. As will be seen from results presented in
Section 5, this difference in vocabulary size has important consequences in translation
quality for the English-to-Spanish direction.
Regarding the development data set, only 1, 008 sentences were considered. Notice
from Table 1 that in this case, the Spanish vocabulary is 20% larger than the English
Table 1
Basic statistics for the training, development, and test data sets (M and k stand for millions and
thousands, respectively; Lmean refers to the average sentence length in number of words, and
Ref. to the number of available translation references).
Set Language Sentences Words Vocabulary Lmean Ref.
Train English 1.22 M 33.4 M 105 k 23.7 1
Spanish 1.22 M 34.8 M 169 k 28.4 1
Dev. English 1008 26.0 k 3.2 k 25.8 3
Spanish 1008 25.7 k 3.9 k 25.5 3
Test English 1094 26.8 k 3.9 k 24.5 2
Spanish 840 22.7 k 4.0 k 27.0 2
535
Computational Linguistics Volume 32, Number 4
vocabulary. Another important issue regarding the development data set is the number
of unseen words, that is, those words present in the development data that are not
present in the training data. In this case, 35 words (0.13%) out of the total number of
words in the English development set did not occur in the training data. From these 35
words, only 30 corresponded to different words. Similarly, 61 words (0.24%) out of the
total number of words in the Spanish development set were not in the training data. In
this case, 57 different words occurred.
Notice also in Table 1 that a different test set was used for each translation direction,
and although a different number of sentences is considered in each case, vocabulary
sizes are almost equivalent. Regarding unseen words, in this case, 112 words (0.42%) out
of the total number of words in the English test set did not occur in the training data.
From these 112 words, only 81 corresponded to different words. Similarly, 46 words
(0.20%) out of the total number of words in the Spanish test were not in the training
data. In this case, 40 different words occurred.
4.2 Preprocessing, Training, and System Optimization
This section presents the overall implementation details in regard to preprocessing,
training, and optimization of the translation system. Two languages, English and Span-
ish, and both translation directions between them are considered for several different
system configurations.
4.2.1 Preprocessing and Alignment. The training data are preprocessed by using stan-
dard tools for tokenizing and filtering. In the filtering stage, some sentence pairs are
removed from the training data to allow for a better performance of the alignment tool.
Sentence pairs are removed according to the following two criteria:
 Fertility filtering: removes sentence pairs with a word ratio larger than a
predefined threshold value.
 Length filtering: removes sentence pairs with at least one sentence of more
than 100 words in length. This helps to maintain bounded alignment
computational times.
After preprocessing, word-to-word alignments are performed in both directions,
source-to-target and target-to-source. In our system implementation, GIZA++ (Och and
Ney 2000) is used for computing the alignments. A total of five iterations for models
IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed.
Then, the obtained alignment sets are used for computing the intersection and the
union of alignments from which tuples and embedded-word tuples are extracted,
respectively.
4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is ex-
tracted from the union set of alignments while avoiding source-nulled tuples by using
the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are
pruned according to the procedure described in Section 2.2.3. In the case of the EPPS
data under consideration, pruning parameter values of N = 20 and N = 30 are used for
Spanish-to-English and English-to-Spanish, respectively.
In order to better justify such alignment set and pruning parameter selections,
Tables 2 and 3 present model sizes and translation accuracies for the tuple n-gram model
536
Marin?o et al N-gram-based Machine Translation
Table 2
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy when tuples are extracted from different alignment sets. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model alone.
Direction Alignment set Tuple voc. Bigrams Trigrams BLEU
ES ? EN Source-to-target 1.920 6.426 2.353 0.4424
union 2.040 6.009 1.798 0.4745
refined 2.111 6.851 2.398 0.4594
EN ? ES Source-to-target 1.813 6.263 2.268 0.4152
union 2.023 6.092 1.747 0.4276
refined 2.081 6.920 2.323 0.4193
when tuples are extracted from different alignment sets and when different pruning
parameters are used, respectively. Translation accuracy is measured in terms of the
BLEU score (Papineni et al 2002), which is computed here for translations generated
by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2,
in the case of Table 3. Both translation directions, Spanish to English (ES ? EN) and
English to Spanish (EN ? ES), are considered in each table.
In the case of Table 2, model size and translation accuracy are evaluated against
the type of alignment set used for extracting tuples. Three different alignment sets are
considered: source-to-target, the union of source-to-target and target-to-source, and the
?refined? alignment method described by Och and Ney (2003). For the results presented
in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English
direction, while a value of N = 30 was used for the English-to-Spanish direction.
As can be clearly seen in Table 2, the union alignment set happens to be the most
favorable one for extracting tuples in both translation directions since it provides a
significantly better translation accuracy, in terms of BLEU score, than the other two
alignment sets considered. Notice also in Table 2 that the union set is the one providing
the smallest model sizes according to the number of bigrams and trigrams. This might
explain the improvement observed in translation accuracy, with respect to the other two
cases, in terms of model sparseness.
Table 3
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy for different pruning values and both translation directions. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2.
Direction Pruning Tuple voc. Bigrams Trigrams BLEU
ES ? EN N = 30 2.109 6.233 1.805 0.5440
N = 20 2.040 6.009 1.798 0.5434
N = 10 1.921 5.567 1.759 0.5399
EN ? ES N = 30 2.023 6.092 1.747 0.4688
N = 20 1.956 5.840 1.733 0.4671
N = 10 1.843 5.342 1.677 0.4595
537
Computational Linguistics Volume 32, Number 4
In the case of Table 3, model size and translation accuracy are compared for three
different pruning conditions: N = 30, N = 20, and N = 10. For all the cases presented in
the table, tuples were extracted from the union set of alignments.
Notice in Table 3 how translation accuracy is clearly affected by pruning. In the
case of Spanish to English, values of N = 20 and N = 10, while providing tuple vo-
cabulary reductions of 3.27% and 8.91% with respect to N = 30, respectively, produce
a translation BLEU score reductions of 0.11% and 0.75%. On the other hand, in the
case of English to Spanish, values of N = 20 and N = 10 provide tuple vocabulary
reductions of 3.31% and 8.89% and a translation BLEU score reductions of 0.36% and
1.98% with respect to N = 30, respectively. According to these results, a similar tuple
vocabulary reduction seems to affect English-to-Spanish translations more than it af-
fects Spanish-to-English translations. For this reason, we finally adopted N = 20 and
N = 30 as the pruning parameter values for Spanish to English and English to Spanish,
respectively.
Another important observation derived from Table 3 is the higher BLEU score
values with respect to the ones presented in Table 2. This is because, as mentioned
above, the results presented in Table 3 were obtained by considering a full translation
system that implements the tuple n-gram model along with the additional four feature
functions described in Section 3.2. The relative impact of the described feature functions
on translation accuracy is studied in detail in Section 5.1.1.
4.2.3 Translation Model and Feature Function Training. After pruning, a tuple n-gram
model is trained for each translation direction by using the SRI Language Modeling
toolkit (Stolcke 2002). The options for Kneser?Ney smoothing (Kneser and Ney 1995)
and interpolation of higher and lower n-grams are used in these trainings. Then, each
tuple n-gram translation model is finally enhanced by including the unigram probabil-
ities for the embedded-word tuples such as described in Section 2.2.2.
Similarly, a word n-gram target language model is trained for each translation
direction by using the SRI Language Modeling toolkit. Again, as in the case of the
tuple n-gram model, Kneser?Ney smoothing and interpolation of higher and lower
n-grams are used. Extended target language models might also be obtained by adding
additional information from other available monolingual corpora. However, in the
translation tasks described here, target language models are estimated by using only
the information contained in the target side of the training data set.
In our SMT system implementation, trigram models are considered for both the
tuple translation model and the target language model. This selection is based on
perplexity measurements (over the development data set) obtained for n-gram models
computed from the EPPS training data by using different n-gram sizes. Table 4 presents
Table 4
Perplexity measurements for translation and target language models of different n-gram sizes.
Type of model Language Bigram Trigram 4-gram 5-gram
Translation ES ? EN 201.75 161.26 156.88 157.24
Translation EN ? ES 223.94 179.12 174.10 174.49
Language Spanish 81.98 52.49 48.03 47.54
Language English 78.91 50.59 46.22 45.59
538
Marin?o et al N-gram-based Machine Translation
perplexity values obtained for translation and target language models with different
n-gram sizes.
Although our system implements trigram models, the performance of translation
systems using different n-gram sized models is also evaluated. These results are pre-
sented and discussed in Section 5.1.2.
Finally, the source-to-target and target-to-source lexicon models are computed for
each translation direction according to the procedure described in Section 3.2.3. For each
considered lexicon model, either the alignment set in the source-to-target direction or
the alignment set in the target-to-source direction is used, accordingly.
4.2.4 System Optimization. Once the models are computed, a set of optimal log-linear
coefficients is estimated for each translation direction and system configuration via
an optimization procedure, which is described as follows. First, a development data
set that does not overlap either the training set or the test set is required. Then, trans-
lation quality over the development set is maximized by iteratively varying the set of
coefficients. In our SMT system implementation, this optimization procedure is per-
formed by using a tool developed in-house, which is based on a simplex method (Press
et al 2002), and the BLEU score (Papineni et al 2002) is used as a translation quality
measurement.
As will be described in the next section, several different system configurations
are considered in the experiments. For all these optimizations, the development data
described in Table 1 are used. As presented in the table, the development data included
three translation references for both English and Spanish, which are used to compute
the BLEU score at each iteration of the optimization procedures.
The same decoder settings are used for all system optimizations. These settings are
the following:
 decoding is performed monotonically, that is, no reordering capabilities
are used,
 decoding is guided by the source sentence to be translated,
 although available in the decoder, threshold pruning is not used, and
 a value of K = 50 for during-decoding histogram pruning is used.
5. Translation Experiments and Error Analysis
This section presents all translation experiments performed and a brief error analysis
of the obtained results. In order to evaluate the relative contributions of different
system elements to the overall performance of the n-gram-based translation system,
three different experimental settings are considered. The experiments and their re-
sults are described in Section 5.1, and a brief error analysis of results is presented in
Section 5.2. Finally, a comparison between n-gram-based SMT and state-of-the-art
phrase-based translation systems is presented in Section 5.3.
5.1 Translation Experiments and Results
As already mentioned, three experimental settings are considered. For each setting,
the impact on translation quality of a different system parameter is evaluated, namely,
539
Computational Linguistics Volume 32, Number 4
feature function, n-gram size, and the source-nulled tuple strategy. Evaluations in all
three experimental settings are performed with respect to the same standard system
configuration, which is defined in terms of the following parameters:
 Alignment set used for tuple extraction: UNION
 Tuple vocabulary pruning parameter: N = 20 for Spanish to English, and
N = 30 for English to Spanish
 N-gram size used in translation model: 3
 N-gram size used in target language model: 3
 Expanded translation model with embedded-word tuples: YES
 Source-nulled tuple handling strategy: attach-to-right
 Feature functions considered: target language, word-bonus,
source-to-target lexicon, and target-to-source lexicon
In the three experimental settings considered, which are presented in the following
subsections, a total of seven different system configurations are evaluated in both
translation directions, English to Spanish and Spanish to English. Thus, a total of 14
different translation experiments are performed. For each of these cases, the corre-
sponding test set is translated by using the corresponding estimated models and set
of optimal coefficients. The same decoder settings (which were previously described in
Section 4.2.4) that were used during the optimizations are used for all translation
experiments. Translation results are evaluated in terms of mWER and BLEU by using
the two references available for each language test set.
5.1.1 Feature Function Contributions. This experiment is designed to evaluate the
relative contribution of feature functions to the overall system performance. In this
section, four different systems are evaluated. These systems are:
 System A. This constitutes the basic n-gram translation system, which
implements the tuple trigram translation model alone, that is, no
additional feature function is used.
 System B. This is a target-reinforced system. In this system, the translation
model is used along with the target-language and word-bonus models.
 System C. This is a lexicon-reinforced system. In this system, the
translation model is used along with the source-to-target and
target-to-source lexicon models.
 System D. This constitutes the full system, that is, the translation model is
used along with all four additional feature functions. This system
corresponds to the standard system configuration that was defined at the
beginning of Section 5.1.
Table 5 summarizes the results of this evaluation, in terms of BLEU and mWER, for
the four systems considered. As can be seen from the table, both translation directions,
540
Marin?o et al N-gram-based Machine Translation
Table 5
Evaluation results for experiments on feature function contribution.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN A ? ? ? ? 39.71 0.4745
B 0.29 0.31 ? ? 39.51 0.4856
C ? ? 0.77 0.08 35.77 0.5356
D 0.49 0.30 0.94 0.25 34.94 0.5434
EN ? ES A ? ? ? ? 44.46 0.4276
B 0.33 0.27 ? ? 44.67 0.4367
C ? ? 0.29 0.15 41.69 0.4482
D 0.66 0.73 0.32 0.47 40.34 0.4688
Spanish to English and English to Spanish, are considered. Table 5 also presents the
optimized log-linear coefficients associated with the features considered in each system
configuration (the log-linear weight of the translation model has been omitted from the
table because its value is fixed to 1 in all cases).
As can be observed in Table 5, the inclusion of the four feature functions into
the translation system definitively produces a significant improvement in translation
quality in both translation directions. In particular, it becomes evident that the features
with the most impact on translation quality are the lexicon models. The target language
model and the word bonus also contribute to improving translation quality, but to a
lesser degree.
Also, although it is more evident in the English-to-Spanish direction than in the
opposite one, it can be noticed from the presented results that the contribution of
target-language and word-bonus models is more relevant when the lexicon mod-
els are used (full system). In fact, as seen from the ?lm values in Table 5, when
the lexicon models are not included, the target-language model contribution to the
overall translation system becomes much less significant. A comparative analysis of
the resulting translations suggests that including the lexicon models tends to favor
short tuples over long ones, so the target-language model becomes more important
for providing target context information when the lexicon models are used. How-
ever, more experimentation and research are required for fully understanding this
interesting result.
Another important observation, which follows from comparing results between
both translation directions, is that in all cases the Spanish-to-English translations are
consistently and significantly better than the English-to-Spanish translations. This is
clearly due to the more inflected nature of Spanish vocabulary. For example, the single
English word the can generate any of the four Spanish words el, la, los, and las. Similar
situations occur with nouns, adjectives, and verbs that may have many different forms
in Spanish. This would suggest that the English-to-Spanish translation task is more
difficult than the Spanish-to-English task.
5.1.2 Translation and Language N-gram Size. This experiment is designed to evaluate
the impact of translation- and language-model n-gram sizes on overall system perform-
ance. In this section, the full system (System D in the previous experiment) is com-
pared with two similar systems for which 4-grams are used for training the translation
541
Computational Linguistics Volume 32, Number 4
model and/or the target language model. More specifically, the three systems compared
in this experiment are:
 System D, which implements a tuple trigram translation model and a word
trigram target language model. This system corresponds to the standard
system configuration that was defined at the beginning of Section 5.1.
 System E, which implements a tuple trigram translation model and a word
4-gram target language model.
 System F, which implements a tuple 4-gram translation model and a word
4-gram target language model.
Table 6 summarizes the results of this evaluation for Systems E, F, and D. Again, both
translation directions are considered and the optimized coefficients associated with the
four feature functions are also presented for each system configuration.
As can be seen in Table 6, the use of 4-grams for model computation does not
provide a clear improvement in translation quality. This is more evident in the English-
to-Spanish direction for which System F happens to be the worst ranked one, while
System D is the one obtaining the best mWER score and system E is the one obtaining
the best BLEU score. On the other hand, in the Spanish-to-English direction, it seems
that a little improvement with respect to System D is achieved by using 4-grams.
However, it is not clear which system performs the best since System E obtains the
best BLEU score while System F obtains the best mWER score.
According to these results, more experimentation and research are required to fully
understand the interaction between the n-gram sizes of translation and target language
models. Notice that in the particular case of the n-gram SMT system described here,
such an interaction is not evident at all since the n-gram-based translation model itself
contains some of the target language model information.
5.1.3 Source-nulled Tuple Strategy Comparison. This experiment is designed to eval-
uate a different strategy for handling source-nulled tuples. In this section, the standard
system configuration (System D) presented at the beginning of Section 5.1, which imple-
ments the attach-to-right strategy described in Section 2.2.2, is compared with a similar
system (referred to as System G) implementing a more complex strategy for handling
those tuples with NULL source sides. More specifically, the latter system uses the
IBM-1 lexical parameters (Brown et al 1993) for computing the translation probabilities
of two possible new tuples: the one resulting when the null-aligned-word is attached to
Table 6
Evaluation results for experiments on n-gram size incidence.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
E 0.50 0.54 0.66 0.45 34.66 0.5483
F 0.66 0.50 1.01 0.57 34.59 0.5464
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
E 0.57 0.45 0.51 0.26 40.55 0.4714
F 1.24 1.07 0.99 0.57 40.91 0.4688
542
Marin?o et al N-gram-based Machine Translation
the previous word and the one resulting when it is attached to the following one. Then,
the attachment direction is selected according to the tuple with the highest translation
probability.
Table 7 summarizes the results of evaluation Systems D and G. Again, both trans-
lation directions are considered and the optimized coefficients associated with the four
feature functions are also presented for each system configuration.
As can be seen in Table 7, consistently better results are obtained in both translation
tasks when using IBM-1 lexicon probabilities to handle tuples with a NULL source
side. Even though slight improvements are achieved in both cases, especially with
the English-to-Spanish translation task, the results show how the initial attach-to-right
strategy is easily improved by making use of some bilingual knowledge.
5.2 Error Analysis
In this last section, we present a brief description of an error analysis performed
on some of the outputs provided by the standard system configuration that was de-
scribed in Section 5.1 (system D). More specifically, a detailed review of 100 trans-
lated sentences and their corresponding source sentences, in each direction, was
conducted. This analysis was very useful since it allowed us to identify the most com-
mon errors and problems related to our n-gram based SMT system in each translation
direction.
A detailed analysis of all the reviewed translations reveals that most translation
problems encountered are typically related to four basic different types of errors:
 Verbal forms: A significant number of wrong verbal tenses and auxiliary
forms were detected. This problem turned out to be the most common
one, reflecting the difficulty of the current statistical approach to capture
the linguistic phenomena that shape head verbs, auxiliary verbs, and
pronouns into full verbal forms in each language, especially given the
inflected nature of the Spanish language.
 Omitted translations: A large number of translations involving tuples with
NULL target sides were detected. Although in some cases these situations
corresponded to correct translations, most of the time they resulted in
omitted-word errors.
 Reordering problems: The two specific situations that most commonly
occurred were problems related to adjective?noun and subject?verb
structures.
Table 7
Evaluation results for experiments on strategies for handling source-nulled tuples.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
G 0.49 0.45 0.78 0.39 34.15 0.5451
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
G 0.96 0.93 0.53 0.44 40.12 0.4694
543
Computational Linguistics Volume 32, Number 4
 Concordance problems: Inconsistencies related to gender and number
were the most commonly found.
Table 8 presents the relative number of occurrences for each of the four types of errors
identified in both translation directions.
Notice in Table 8 that the most common errors in both translation directions are
those related to verbal forms. However, it is important to mention that 29.5% of verbal-
form errors in the English-to-Spanish direction actually correspond to verbal omissions.
Similarly, 12.8% of verbal-form errors in the Spanish-to-English direction are verbal
omissions. According to this, if errors due to omitted translations and to omitted verbal
forms are considered together, it is evident that errors involving omissions constitute
the most important group, especially in the case of English-to-Spanish translations. It
is also interesting to note that the Spanish-to-English direction exhibits more omitted-
translation errors that are not related to verbal forms than the English-to-Spanish
direction.
Also in Table 8, it can be seen that concordance errors affect more than twice as many
English-to-Spanish translations as Spanish-to-English ones. This result can be explained
by the more inflected nature of Spanish.
Finally, as an illustrative example, three Spanish-to-English translation outputs are
presented below. For each presented example, errors have been boldfaced and correct
translations are provided in brackets:
Example 1
The policy of the European Union on Cuba NULL must [must not] change.
Example 2
To achieve these purposes, it is necessary NULL for the governments to be allocated
[to allocate], at least, 60,000 million NULL dollars a year . . .
Example 3
In the UK we have NULL [already] laws enough [enough laws], but we want to encourage
NULL other States . . .
5.3 N-gram-based SMT Compared with Phrase-Based SMT
The n-gram-based translation system here described has been also evaluated and com-
pared to other phrase-based translation systems in the context of the European Project
Table 8
Percentage of occurrence for each type of error in English-to-Spanish and Spanish-to-English
translations that were studied.
Type of error English-to-Spanish Spanish-to-English
Verbal forms 31.3% 29.9%
Omitted translations 22.0% 26.1%
Reordering problems 15.9% 19.7%
Concordance problems 10.8% 4.6%
Other errors 20.0% 19.7%
544
Marin?o et al N-gram-based Machine Translation
TC-STAR. A detailed description of the first evaluation campaign (including the main
characteristics of every system) is available through the consortium?s Web site as a
progress report (Ney et al 2005).
Table 9 presents the four best BLEU results for the EPPS translation task in the
first TC-STAR?s evaluation campaign, where the results corresponding to our n-gram-
based translation system are provided in brackets. A total of six systems were evaluated
in this evaluation campaign. The task consisted of two translation directions: English
to Spanish and Spanish to English, and three different evaluation conditions: final
text edition, verbatim, and ASR output. The final text edition condition corresponds
to the official transcripts of the EPPS, so it is actually a written-language translation
condition. On the other hand, the other two conditions are spoken-language transla-
tion conditions. More specifically, the verbatim condition corresponds to literal tran-
scriptions of parliamentary speeches, which include hesitations, repeated words, and
other spontaneous speech effects; and the ASR output condition corresponds to the
output of an automatic speech recognition system, so it additionally includes speech-
recognition errors.
As can be seen in Table 9, performance of the n-gram-based translation system is
among the three best systems for the translation directions and conditions considered
in the first TC-STAR evaluation campaign.
Another independent comparison of the translation system proposed here with
other phrase-based translation systems is available through the results of the second
shared task of the ACL 2005 workshop on ?Building and using parallel texts: Data-
driven machine translation and beyond.? In this shared task, which was entitled ?Ex-
ploiting Parallel Texts for Statistical Machine Translation,? our n-gram-based translation
system was evaluated in four different translation directions: Spanish to English, French
to English, German to English, and Finish to English (Banchs et al 2005). The domain
of this task was also the European Parliament; however, the data set considered in this
evaluation was different from the one used in TC-STAR?s evaluation campaign. The
final text edition condition (official transcripts) was the only one considered here. A total
of twelve different systems participated in this shared task. Table 10 presents the four
best BLEU results for each of the four translation directions considered in the shared
task. Again, results corresponding to our n-gram-based translation system are provided
in brackets.
As can be seen in Table 10, the performance of the n-gram-based translation system
is among the three best systems for the four translation directions considered in the
ACL 2005 workshop shared task. The third system in Table 10 for ES to EN translation
Table 9
The four best BLEU results for the EPPS translation task in TC-STAR?s first evaluation campaign.
N-gram based system results are provided in brackets. All BLEU values presented here have
been taken from TC-STAR?s SLT Progress Report, available at: http://www.tc-star.org/.
Direction Condition First Second Third Fourth
ES ? EN Final text edition [53.3] 53.1 47.5 46.1
Verbatim 45.9 44.1 [42.1] 38.1
ASR output 41.5 39.7 [37.7] 34.7
EN ? ES Final text edition [46.2] 45.2 38.9 37.6
Verbatim 42.5 [38.1] 36.8 33.4
ASR output 38.7 34.3 [33.8] 33.0
545
Computational Linguistics Volume 32, Number 4
Table 10
The four best BLEU results for the four translation directions considered in the shared task
?Exploiting Parallel Texts for Statistical Machine Translation? (ACL 2005 workshop on
?Building and using parallel texts: Data-driven machine translation and beyond?). N-gram-
based system results are provided in brackets. All BLEU values presented here have been
taken from the shared task?s Web site: http://www.statmt.org/wpt05/mt-shared-task/.
Direction Condition First Second Third Fourth
FR ? EN Final text edition 30.27 [30.20] 29.53 28.89
ES ? EN Final text edition 30.95 [30.07] 29.84 29.08
DE ? EN Final text edition 24.77 [24.26] 23.21 22.91
FI ? EN Final text edition 22.01 20.95 [20.31] 18.87
deserves some comment. This system is a conventional phrase-based system sharing
the same decoder MARIE, IBM features, word bonus, and target-language model as the
n-gram-based system. The specific characteristics of the phrase-based system are direct
and inverse phrase conditional probabilities and phrase penalty. Additional compar-
isons between an n-gram system and a phrase-based system sharing a common decoder
and training and test framework can be found in Crego et al (2005c).
6. Conclusions and Further Work
As can be concluded from the results presented, the tuple n-gram translation model,
when used along with additional feature functions, provides state-of-the-art transla-
tions for the considered translation directions.
Another important result is that the quality of Spanish-to-English translations is
significantly and consistently better than those obtained in English-to-Spanish transla-
tions. Consequently, significant efforts should be dedicated towards properly exploiting
morphological analysis and synthesis methods for improving English-to-Spanish trans-
lation quality.
Additionally, four commonly occurring types of translation errors were identified
by reviewing a significant number of translated sentence pairs. This analysis has pro-
vided us with useful hints for future research and improvement of our SMT system.
However, more evaluation and discussion are required in this area in order to fully
understand these common translation failures and then implementing appropriate
solutions.
All the experiments presented in this work were performed using monotone de-
coding, and no reordering strategies were implemented. Although this system con-
figuration proved to provide state-of-the-art translations for the tasks presented, this
may not hold for tasks involving more distant language pairs for which reordering
capabilities must be implemented. Accordingly, along with other results obtained in
the present work, we consider that further research on n-gram SMT should focus on the
following issues:
 Reordering strategies, as well as non-monotonous decoding schemes, for
the proposed SMT system must be developed and tested. As mentioned
before, reordering problems specifically related to adjective?noun and
subject?verb structures occur very often in Spanish-to-English and
546
Marin?o et al N-gram-based Machine Translation
English-to-Spanish translations. Preliminary results concerning the use of
word class deterministic reordering and POS-tag-based reordering
patterns can be found in Costa-jussa`, Fonollosa, and Monte (2006) and
Crego and Marin?o (2006), respectively.
 An effective long-tuple unfolding strategy must be developed to avoid
the occurrence of long tuples resulting from long alignment links, which
happens to be a common situation when dealing with translations
between distant pairs of languages. This problem is closely related to
reordering, and some preliminary results have been presented by Crego,
Marin?o, and de Gispert (2005b).
 The definition of the tuple as a bilingual pair will be revised in order to
better handle unaligned words in both the source and the target sides. As
mentioned above, a better strategy for dealing with target words aligned
to NULL is required. Similarly, a better handling of NULLs in the target
side will result in fewer omitted-translation errors.
 The extension of the embedded-word concept to the more general idea of
embedded n-grams should be evaluated and implemented. Accordingly, a
translation probability should be estimated for those groups of words
that always occur embedded in tuples. This would guarantee that the
decoder will always have a translation option for any given word or word
combination previously seen in the training data. Further work is required
to determine the relative impact of these embedded n-grams on the
translation model, and the most appropriate strategy for handling them.
 Linguistic information must be used to cope with the observed
morphological problems in the English-to-Spanish translation direction,
as well as the more general problem of incorrect verbal form translations.
In this regard, ongoing research on linguistic tuples classification is
being done in order to improve translation results. Preliminary results
on detecting and classifying verb forms have been presented by
de Gispert (2005).
 A more detailed error analysis than the one presented in Section 5.2 is
required to fully understand the n-gram SMT system behavior and the
specific causes of each resulting type of error. It would be very useful for
improving our translation system performance to clearly identify whether
these errors are due to unseen information while training, to modeling
problems, or to decoding errors.
Acknowledgments
This work has been partly funded by the
European Union under the integrated project
TC-STAR (Technology and Corpora for
Speech to Speech Translation) (IST-2002-
FP6-506738, http://www.tc-star.org), the
Spanish Department of Education and
Science (MEC), the Department of
Universities, Research and Information
Society (Generalitat de Catalunya), and
the Universitat Polite`cnica de Catalunya.
References
Banchs, Rachel E., Josep Maria Crego,
Adria` de Gispert, Patrik Lambert, and
Jose? Bernardo Marin?o. 2005. Statistical
machine translation of Euparl data by
using bilingual n-grams. In ACL Workshop
on Data-Driven Machine Translation and
Beyond, pages 133?136, Ann Arbor, MI.
Bangalore, Srinivas and Giuseppe Riccardi.
2000. Stochastic finite-state models for
spoken language machine translation.
547
Computational Linguistics Volume 32, Number 4
In Proceedings of the Workshop on Embedded
Machine Translation Systems, pages 52?59,
Seattle, WA.
Berger, Adam, Stephen Della Pietra, and
Vincent Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Brown, Peter, John Cocke, Stephen Della
Pietra, Vincent Della Pietra, Frederick
Jelinek, John Lafferty, Robert Mercer, and
Paul S. Roossin. 1990. A statistical
approach to machine translation.
Computational Linguistics, 16(2):79?85.
Brown, Peter, Stephen Della Pietra, Vincent
Della Pietra, and Robert Mercer. 1993.
The mathematics of statistical machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Casacuberta, Francisco. 2001. Finite-state
transducers for speech input translation. In
Proceedings IEEE ASRU, pages 375?380,
Madonna di Campiglio, Italy.
Casacuberta, Francisco and Enrique Vidal.
2004. Machine translation with inferred
stochastic finite-state transducers.
Computational Linguistics, 30(2):205?225.
Costa-jussa`, Marta Ruiz, Jose? Adria?n
Rodriguez Fonollosa, and Enric Monte.
2006. Using reordering in statistical
machine translation based on alignment
block classification. Internal Report.
http://gps-tsc.upc.es/veu/personal/
mruiz/docs/br06.pdf.
Crego, Josep Maria, Jose? Bernardo
Marin?o, and Adria` de Gispert. 2004.
Finite-state-based and phrase-based
statistical machine translation. In
Proceedings of the 8th International
Conference on Spoken Language
Processing, pages 37?40, Jeju, Korea.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005a. An
Ngram-based statistical machine
translation decoder. In INTERSPEECH
2005, pages 3185?3188, Lisbon, Portugal.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005b. Reordered
search and tuple unfolding for Ngram-
based SMT. Proceedings of the Tenth
Machine Translation Summit, pages 283?289,
Phuket, Thailand.
Crego, Josep Maria, Marta Ruiz Costa-jussa`,
Jose? Bernardo Marin?o, and Jose? Adria?n
Rodriguez Fonollosa. 2005c. Ngram-
based versus phrase-based statistical
machine translation. In Proceedings of the
International Workshop on Spoken Language
Translation, pages 177?184, Pittsburgh, PA.
Crego, Josep Maria and Jose? Bernardo
Marin?o. 2006. Integration of POStag-based
source reordering into SMT decoding by
an extended search graph. In Proceedings of
the 7th Biennial Conference of the Association
for Machine Translation in the Americas,
Boston, MA.
de Gispert, Adria` and Jose? Bernardo Marin?o.
2002. Using X-grams for speech-to-
speech translation. In Proceedings of the
7th International Conference on Spoken
Language Processing, pages 1885?1888,
Denver, CO.
de Gispert, Adria`, Jose? Bernardo Marin?o, and
Josep Maria Crego. 2004. TALP:
Xgram-based spoken language translation
system. In Proceedings of the International
Workshop on Spoken Language Translation,
pages 85?90, Kyoto, Japan.
de Gispert, Adria`. 2005. Phrase linguistic
classification and generalization for
improving statistical machine translation.
In ACL?05 Student Workshop, pages 67?72,
Ann Arbor, MI.
Hutchins, John. 1986. Machine Translation:
Past, Present and Future. Ellis Horwood,
Chichester, England.
Kay, Martin, Jean Mark Gawron, and Peter
Norvig. 1992. Verbmobil: A Translation
System for Face-to-Face Dialog. CSLI.
Kneser, Reinhard and Hermann Ney. 1995.
Improved backing-off for m-gram
language modeling. In IEEE International
Conference on Acoustics, Speech and Signal
Processing, pages 49?52, Detroit, MI.
Knight, Kevin and Yaser Al-Onaizan.
1998. Translation with finite-state
devices. In AI Lecture Notes in Artificial
Intelligence, volume 1529, Springer-Verlag,
pages 421?437.
Koehn, Philippe, Franz Joseph Och,
and Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings
of the 2003 Meeting of the North American
chapter of the ACL, pages 48?54, Edmonton,
Alberta, Canada.
Koehn, Philippe. 2002. Europarl: A
multilingual corpus for evaluation
of machine translation. Available
online at: http://people.csail.mit.edu/
people/koehn/publications/europarl/.
Marin?o, Jose? Bernardo, Rafael E. Banchs,
Josep Maria Crego, Adria` de Gispert,
Patrik Lambert, Jose? Adria?n Rodriguez
Fonollosa, and Marta Ruiz. 2005. Bilingual
N-gram statistical machine translation.
In Proceedings of the Tenth Machine
Translation Summit, pages 275?282,
Phuket, Thailand.
548
Marin?o et al N-gram-based Machine Translation
Ney, Hermann, Volker Steinbiss, Richard
Zens, Evgeny Matusov, Jorge Gonza?lez,
Young-suk Lee, Salim Roukos, Marcello
Federico, Muntsin Kolss, and Rafael
Banchs. 2005. SLT progress report.
TC-STAR Deliverable D5, European
Community project no. FP6-506738.
Available online at: http://www.
tc-star.org/pages/f documents.htm.
Och, Franz Joseph and Hermann Ney.
2000. Improved statistical alignment
models. In Proceedings of the 38th Annual
Meeting of the ACL, pages 440?447,
Hong Kong, China.
Och, Franz Joseph and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 295?302,
Philadelphia, PA.
Och, Franz Joseph and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Och, Franz Joseph, Daniel Gildea, Sanjeev
Khudanpur, Anoop Sarkar, Kenji Yamada,
Alexander Fraser, Shankar Kumar, Libin
Shen, David Smith, Katharine Eng, Viren
Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical
machine translation. In Proceedings of the
Human Language Technology Conference
NAACL, pages 161?168, Boston, MA, May.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Conference of the ACL,
pages 311?318, Philadelphia, PA.
Press, William H., Saul Teukolsky, William
Vetterling, and Brian P. Flannery.
2002. Numerical Recipes in C++: The
Art of Scientific Computing, Cambridge
University Press.
Riccardi, Giuseppe, Roberto Pieraccini, and
Enrico Bocchieri. 1996. Stochastic automata
for language modeling. Computer Speech
and Language, 10(4):265?293.
Shannon, Claude E. 1949. Communication
theory of secrecy systems. Bell System
Technical Journal, 28:656?715.
Shannon, Claude E. 1951. Prediction and
entropy of printed English. Bell System
Technical Journal, 30:50?64.
Shannon, Claude E. and Warren Weaver.
1949. The Mathematical Theory of
Communication, University of Illinois
Press, Urbana, IL.
Stolcke, Andreas 2002. SRLIM: An extensible
language modeling toolkit. In Proceedings
of the International Conference on Spoken
Language Processing, pages 901?904,
Denver, CO.
Tillmann, Christoph and Fei Xia. 2003. A
phrase-based unigram model for statistical
machine translation. In Proceedings of
HLT-NAACL - Short Papers, pages 106?108,
Edmonton, Alberta, Canada.
Vidal, Enrique. 1997. Finite-state speech-to-
speech translation. In Proceedings of 1997
IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 111?114,
Munich, Germany.
Weaver, Warren. 1955. Translation. In
William Locke and A. Donald Booth,
editors, Machine Translation of Languages:
Fourteen Essays. John Wiley & Sons, New
York, pages 15?23.
Zens, Richard, Franz Joseph Och, and
Hermann Ney. 2002. Phrase-based
statistical machine translation. In
25th German Conference on Artificial
Intelligence, pages 18?32, September.
Aachen, Springer Verlag.
549

Proceedings of NAACL HLT 2007, Companion Volume, pages 85?88,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Discriminative Alignment Training without Annotated Data
for Machine Translation
Patrik Lambert, Rafael E. Banchs and Josep M. Crego
TALP Research Center
Jordi Girona Salgado 1?3
08034 Barcelona, Spain
{lambert, rbanchs, jmcrego}@gps.tsc.upc.edu
Abstract
In present Statistical Machine Translation
(SMT) systems, alignment is trained in a
previous stage as the translation model.
Consequently, alignment model parame-
ters are not tuned in function of the trans-
lation task, but only indirectly. In this
paper, we propose a novel framework for
discriminative training of alignment mod-
els with automated translation metrics as
maximization criterion. In this approach,
alignments are optimized for the transla-
tion task. In addition, no link labels at the
word level are needed. This framework
is evaluated in terms of automatic trans-
lation evaluation metrics, and an improve-
ment of translation quality is observed.
1 Introduction
In the first SMT systems (Brown et al, 1993), word
alignment was introduced as a hidden variable of
the translation model. When word-based translation
models have been replaced by phrase-based mod-
els (Zens et al, 2002), alignment1 and translation
model training have become two separated tasks.
The system of Brown et al was based on the
noisy channel approach. Present SMT systems use a
more general maximum entropy approach in which a
log-linear combination of multiple feature functions
is implemented (Och and Ney, 2002). Within this
1Hereinafter, alignment will refer to word alignment, unless
otherwise stated.
new framework translation quality can be tuned by
adjusting the weight of each feature function in the
log-linear combination. In order to improve transla-
tion quality, this tuning can be effectively performed
by minimizing translation error over a development
corpus for which manually translated references are
available (Och, 2003). As a separate first stage of the
process, alignment is not in practice directly tuned in
function of the machine translation task.
Tuning alignment for an MT system is subject to
practical difficulties. Unsupervised systems (Och
and Ney, 2003; Liang et al, 2006) are based on gen-
erative models trained with the EM algorithm. They
require large computational resources, and incorpo-
rating new features is difficult. In contrast, adding
new features to some supervised systems (Liu et al,
2005; Moore, 2005; Ittycheriah and Roukos, 2005)
is easy, but the need of annotated data is a problem.
A more general difficulty, however, is that of find-
ing an alignment evaluation metric favoring align-
ments which benefit Machine Translation. The fact
that the required alignment characteristics depend
on each particular system makes it even more dif-
ficult. It seems that high precision alignments are
better for phrase-based SMT (Chen and Federico,
2006; Ayan and Dorr, 2006), whereas high recall
alignments are more suited to N-gram SMT (Marin?o
et al, 2006). In this context, alignment quality im-
provements does not necessarily imply translation
quality improvements. This is in agreement with
the observation of a poor correlation between word
alignment error rate (AER (Och and Ney, 2000)) and
automatic translation evaluation metrics (Ittycheriah
and Roukos, 2005; Vilar et al, 2006).
85
Recently some alignment evaluation metrics have
been proposed which are more informative when
the alignments are used to extract translation
units (Fraser and Marcu, 2006; Ayan and Dorr,
2006). However, these metrics assess translation
quality very indirectly.
In this paper, we propose a novel framework for
discriminative training of alignment models with au-
tomated translation metrics as maximization crite-
rion. Thus we just need a reference aligned at the
sentence level instead of link labels at the word level.
The paper is structured as follows. Section 2 ex-
plains the models used in our word aligner, focusing
on the features designed to account for the specifici-
ties of the SMT system. In section 3, our minimum
error training procedure is described and experimen-
tal results are shown. Finally, some concluding re-
marks and lines of further research are given.
2 Bilingual Word Aligner
For versatility and efficiency requirements, we im-
plemented BIA, a BIlingual word Aligner similar
to that of Moore (2005). BIA consists in a beam-
search decoder searching, for each sentence pair, the
alignment which minimizes the cost of a linear com-
bination of various models. The differences with
the system of Moore lie in the features, which we
specially designed to suit our translation system (N-
gram SMT (Marin?o et al, 2006)). Its particularity
is the translation model, which is based on a 4-gram
language model of bilingual units referred to as tu-
ples. Two issues regarding this translation model can
be dealt with at the alignment stage.
Firstly, in order to estimate the bilingual n-gram
model, only one monotonic segmentation of each
sentence pair is performed. Thus long reorderings
cause long and sparse tuples to be extracted. For ex-
ample, if the first source word is linked to the last
target word, only one tuple can be extracted, which
contains the whole sentence pair. This kind of tuple
is not reusable, and the data between its two extreme
words are lost.
Secondly, it occurs very often that unlinked words
(i.e. linked to NULL) end up producing tuples with
NULL source sides. This cannot be allowed since
no NULL is expected to occur in a translation input.
This problem is solved by preprocessing alignments
before tuple extraction such that any unlinked target
word is attached to either its precedent or its follow-
ing word.
Taking theses issues into account, we imple-
mented the following features:
? distinct source and target unlinked word penal-
ties: since unlinked words have a different im-
pact whether they appear in the source or target
language, we introduced an unlinked word fea-
ture for each side of the sentence pair.
? link bonus: in order to accommodate the N-
gram model preference for higher recall align-
ment, we introduced a feature which adds a
bonus for each link in the alignment.
? embedded word position penalty: this feature
penalizes situations like the one depicted in fig-
ure 1. In this example, the bilingual units s2-t2
and s3-t3 cannot be extracted because word po-
sitions s2 and s3 are embedded between links
s1-t1 and s4-t1. Thus the link s4-t1 may intro-
duces data sparseness in the translation model,
although it may be a correct link. So we want
to have a feature which counts the number of
embedded word positions in an alignment.
Figure 1: Word positions embedded in a tuple.
In addition to the embedded word position feature,
we used the same two distortion features as Moore
to penalize reorderings in the alignment (one sums
the number of crossing links, and the other one sums
the amplitude of crossing links). We also used the ?2
score (Gale and Church, 1991) as a word association
model, and as a POS-tags association model.
3 Experimental Work
For these experiments we used the Chinese-
English data provided for IWSLT?06 evaluation
campaign (Paul, 2006). The training set contains
46000 sentences (of 6.7 and 7.0 average length). Pa-
rameters were tuned over the development set (dev4)
provided, consisting of 489 sentences of 11.2 words
in average, with 7 references. Our test set was a se-
lection of 500 sentences (of 6 words in average, with
16 references) among dev1, dev2 and dev3 sets.
86
3.1 Optimization Procedure
Once the alignment models were computed, a set of
optimal log-linear coefficients was estimated via the
optimization procedure depicted in Figure 2.
Figure 2: Optimization loop.
The training corpus was aligned with a set of ini-
tial parameters ?1, . . . , ?7. This alignment was used
to extract tuples and build a bilingual N-gram trans-
lation model (TM). A baseline SMT system, consist-
ing of MARIE decoder and this translation model as
unique feature2, was used to produce a translation
(OUT) of the development source set. Then, trans-
lation quality over the development set is maximized
by iteratively varying the set of coefficients.
The optimization procedure was performed by us-
ing the SPSA algorithm (Spall, 1992). SPSA is a
stochastic implementation of the conjugate gradient
method which requires only two evaluations of the
objective function. It was observed to be more ro-
bust than the Downhill Simplex method when tuning
SMT coefficients (Lambert and Banchs, 2006).
Each function evaluation required to align the
training corpus and build a new translation model.
The algorithm converged after about 80 evaluations,
lasting each 17 minutes with a 3 GHz processor.
Alignment decoding was performed with a beam of
10 (it took 50 seconds and required 8 MB memory).
Finally, the corpus was aligned with the opti-
mum set of coefficients, and a full SMT system was
build, with a target language model (trained on the
provided training data), a word bonus model and
two lexical models. SMT models weights were op-
timized with a standard Minimum Error Training
(MET) strategy3 and the test corpus was translated
2An N-gram SMT system can produce good translations
without additional target language model since the target lan-
guage is modeled inside the bilingual N-gram model.
3SMT parameters are not optimized together with alignment
with the full system. To contrast the results, full
translation systems were also build extracting tuples
from various combinations of GIZA++ alignments
(trained with 50 classes and respectively 4,5 and 4
iterations of models 1,HMM and 4). In order to limit
the error introduced by MET, we translated the test
corpus with three sets of SMT model weights, and
took the average and standard deviation.
3.2 Results
Table 1 shows results obtained with the full SMT
system on the test corpus, with GIZA++ alignments,
and BIA alignments optimized in function of three
metrics: BLEU, NIST, and BLEU+4*NIST. The
standard deviation is indicated in parentheses. Al-
though results for systems trained with different BIA
alignments present more variability than systems
trained with GIZA++ alignments, they achieve bet-
ter average scores, and one of them obtains much
higher scores. Unexpectedly, BIA alignments tuned
with NIST yield the system with worse NIST score.
4 Conclusions and further work
We proposed a novel framework for discriminative
training of alignment models with automated trans-
lation metrics as maximization criterion. Accord-
ing to this type of metrics, the translation systems
trained from the optimized alignments clearly per-
formed better than the ones trained from Giza++
alignment combinations.
In addition, this first version of the alignment
system has very basic models and could be im-
proved. We could certainly improve the association
score model, for example adding discount factors or
adding more association score types, or dictionaries.
During the alignment coefficient optimization de-
picted in Figure 2, only the baseline SMT system
is used. In future work, we could consider using
various SMT features (as would be required for a
phrase-based SMT system).
Our approach, as it is, cannot be applied to a large
corpus, since it requires to align the whole training
corpus at each iteration. Thus an interesting further
research would consist in determining whether the
parameters for two main reasons. Firstly, translation is more
sensitive to variations of SMT parameters. Secondly, alignment
is optimized over the full training set, whereas SMT is tuned
over the development set.
87
System BLEU NIST PER WER
GIZA++ union 42.7 (1.1) 8.82 (0.07) 34.7 (0.2) 43.7 (0.4)
GIZA++ intersection 42.4 (0.9) 8.53 (0.07) 37.0 (0.9) 45.0 (1.3)
GIZA++ Zh?En 43.7 (0.9) 8.90 (0.2) 37.2 (1.4) 45.5 (2.0)
BIA (BLEU) 44.8 (0.4) 9.00 (0.04) 35.7 (0.07) 43.8 (0.09)
BIA (BLEU+4*NIST) 47.0 (1.5) 8.83 (0.4) 32.9 (0.8) 40.9 (0.5)
BIA (NIST) 44.8 (0.1) 8.55 (0.14) 33.0 (0.2) 41.4 (0.5)
Table 1: Automatic translation evaluation results.
alignment parameters trained on a part of the corpus
are valid for the whole corpus.
Finally, some Giza++ parameters may also be
tuned, in the same way as for BIA parameters.
5 Acknowledgments
This work has been partially funded by the Euro-
pean Union under the integrated project TC-STAR
- Technology and Corpora for Speech to Speech
Translation -(IST-2002-FP6-506738, http://www.tc-
star.org) and by the Spanish Government under grant
TEC2006-13964-C03 (AVIVAVOZ project).
References
Necip F. Ayan and Bonnie J. Dorr. 2006. Going Beyond
AER: An Extensive Analysis of Word Alignments and
Their Impact on MT. In Proc. COLING-ACL, pages
9?16, Sydney, Australia.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
Boxing Chen and Marcello Federico. 2006. Improving
phrase-based statistical translation through combina-
tion of word alignment. In Proc. FinTAL, Turku, Fin-
land.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proc. COLING-ACL, pages 769?776, Sydney, Aus-
tralia.
W. Gale and K. W. Church. 1991. Identifying word cor-
respondences in parallel texts. In DARPA Speech and
Natural Language Workshop, Asilomar, CA.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In Proc. HLT-EMNLP, pages 89?96, Van-
couver, Canada.
Patrik Lambert and Rafael E. Banchs. 2006. Tuning
Machine Translation Parameters with SPSA. In Proc.
IWSLT, pages 190?196, Kyoto, Japan.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proc. the HLT-NAACL, pages
104?111, New York City, USA.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In Proc. ACL, pages 459?
466, Ann Arbor, Michigan.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrik Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-jussa`. 2006. N-gram based machine
translation. Computational Linguistics, 32(4):527?
549.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proc. HLT-EMNLP,
pages 81?88, Vancouver, Canada.
Franz Josef Och and Hermann Ney. 2000. A compari-
son of alignment models for statistical machine trans-
lation. In Proc. COLING, pages 1086?1090, Saar-
brucken,Germany.
F.J. Och and H. Ney. 2002. Dicriminative training
and maximum entropy models for statistical machine
translation. In Proc. ACL, pages 295?302, Philadel-
phia, PA.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51, March.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, pages 160?167.
Michael Paul. 2006. Overview of the IWSLT 2006 Eval-
uation Campaign. In Proc. IWSLT, pages 1?15, Kyoto,
Japan.
James C. Spall. 1992. Multivariate stochastic approxi-
mation using a simultaneous perturbation gradient ap-
proximation. IEEE Trans. Automat. Control, 37:332?
341.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
AER: Do we need to ?improve? our alignments? In
Proc. IWSLT, pages 205?212, Kyoto, Japan.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In Springer Verlag, editor,
Proc. German Conf. on Artificial Intelligence (KI).
88
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 133?136,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Statistical Machine Translation of Euparl Data by using Bilingual N-grams
Rafael E. Banchs Josep M. Crego Adria` de Gispert
Department of Signal Theory and Communications
Universitat Polite`cnica de Catalunya, Barcelona 08034, Spain
{rbanchs,jmcrego,agispert,lambert,canton}@gps.tsc.upc.edu
Patrik Lambert Jose? B. Marin?o
Abstract
This work discusses translation results for
the four Euparl data sets which were made
available for the shared task ?Exploit-
ing Parallel Texts for Statistical Machine
Translation?. All results presented were
generated by using a statistical machine
translation system which implements a
log-linear combination of feature func-
tions along with a bilingual n-gram trans-
lation model.
1 Introduction
During the last decade, statistical machine transla-
tion (SMT) systems have evolved from the orig-
inal word-based approach (Brown et al, 1993)
into phrase-based translation systems (Koehn et al,
2003). Similarly, the noisy channel approach has
been expanded to a more general maximum entropy
approach in which a log-linear combination of mul-
tiple models is implemented (Och and Ney, 2002).
The SMT approach used in this work implements
a log-linear combination of feature functions along
with a translation model which is based on bilingual
n-grams. This translation model was developed by
de Gispert and Marin?o (2002), and it differs from the
well known phrase-based translation model in two
basic issues: first, training data is monotonously seg-
mented into bilingual units; and second, the model
considers n-gram probabilities instead of relative
frequencies. This model is described in section 2.
Translation results from the four source languages
made available for the shared task (es: Spanish, fr:
French, de: German, and fi: Finnish) into English
(en) are presented and discussed.
The paper is structured as follows. Section 2 de-
scribes the bilingual n-gram translation model. Sec-
tion 3 presents a brief overview of the whole SMT
procedure. Section 4 presents and discusses the
shared task results and other interesting experimen-
tation. Finally, section 5 presents some conclusions
and further work.
2 Bilingual N-gram Translation Model
As already mentioned, the translation model used
here is based on bilingual n-grams. It actually con-
stitutes a language model of bilingual units which
are referred to as tuples (de Gispert and Marin?o,
2002). This model approximates the joint probabil-
ity between source and target languages by using 3-
grams as it is described in the following equation:
p(T, S) ?
N
?
n=1
p((t, s)n|(t, s)n?2, (t, s)n?1) (1)
where t refers to target, s to source and (t, s)n to the
nth tuple of a given bilingual sentence pair.
Tuples are extracted from a word-to-word aligned
corpus according to the following two constraints:
first, tuple extraction should produce a monotonic
segmentation of bilingual sentence pairs; and sec-
ond, the produced segmentation is maximal in the
sense that no smaller tuples can be extracted with-
out violating the previous constraint (Crego et al,
2004). According to this, tuple extraction provides a
unique segmentation for a given bilingual sentence
pair alignment. Figure 1 illustrates this idea with a
simple example.
133
We would like to achieve perfect translations
NULL quisieramos lograr traducciones perfectas
t1 t2 t3 t4
Figure 1: Example of tuple extraction from an
aligned sentence pair.
Two important issues regarding this translation
model must be mentioned. First, when extracting
tuples, some words always appear embedded into tu-
ples containing two or more words, so no translation
probability for an independent occurrence of such
words exists. To overcome this problem, the tuple
3-gram model is enhanced by incorporating 1-gram
translation probabilities for all the embedded words
(de Gispert et al, 2004).
Second, some words linked to NULL end up pro-
ducing tuples with NULL source sides. This cannot
be allowed since no NULL is expected to occur in a
translation input. This problem is solved by prepro-
cessing alignments before tuple extraction such that
any target word that is linked to NULL is attached
to either its precedent or its following word.
3 SMT Procedure Description
This section describes the procedure followed for
preprocessing the data, training the models and op-
timizing the translation system parameters.
3.1 Preprocessing and Alignment
The Euparl data provided for this shared task (Eu-
parl, 2003) was preprocessed for eliminating all sen-
tence pairs with a word ratio larger than 2.4. As a
result of this preprocessing, the number of sentences
in each training set was slightly reduced. However,
no significant reduction was produced.
In the case of French, a re-tokenizing procedure
was performed in which all apostrophes appearing
alone were attached to their corresponding words.
For example, pairs of tokens such as l ? and qu ?
were reduced to single tokens such as l? and qu?.
Once the training data was preprocessed, a word-
to-word alignment was performed in both direc-
tions, source-to-target and target-to-source, by us-
ing GIZA++ (Och and Ney, 2000). As an approxi-
mation to the most probable alignment, the Viterbi
alignment was considered. Then, the intersection
and union of alignment sets in both directions were
computed for each training set.
3.2 Feature Function Computation
The considered translation system implements a to-
tal of five feature functions. The first of these mod-
els is the tuple 3-gram model, which was already de-
scribed in section 2. Tuples for the translation model
were extracted from the union set of alignments as
shown in Figure 1. Once tuples had been extracted,
the tuple vocabulary was pruned by using histogram
pruning. The same pruning parameter, which was
actually estimated for Spanish-English, was used for
the other three language pairs. After pruning, the
tuple 3-gram model was trained by using the SRI
Language Modeling toolkit (Stolcke, 2002). Finally,
the obtained model was enhanced by incorporating
1-gram probabilities for the embedded word tuples,
which were extracted from the intersection set of
alignments.
Table 1 presents the total number of running
words, distinct tokens and tuples, for each of the four
training data sets.
Table 1: Total number of running words, distinct to-
kens and tuples in training.
source running distinct tuple
language words tokens vocabulary
Spanish 15670801 113570 1288770
French 14844465 78408 1173424
German 15207550 204949 1391425
Finnish 11228947 389223 1496417
The second feature function considered was a tar-
get language model. This feature actually consisted
of a word 3-gram model, which was trained from the
target side of the bilingual corpus by using the SRI
Language Modeling toolkit.
The third feature function was given by a word
penalty model. This function introduces a sentence
length penalization in order to compensate the sys-
134
tem preference for short output sentences. More
specifically, the penalization factor was given by the
total number of words contained in the translation
hypothesis.
Finally, the fourth and fifth feature functions cor-
responded to two lexicon models based on IBM
Model 1 lexical parameters p(t|s) (Brown et al,
1993). These lexicon models were calculated for
each tuple according to the following equation:
plexicon((t, s)n) =
1
(I + 1)J
J
?
j=1
I
?
i=0
p(tin|sjn) (2)
where sjn and tin are the jth and ith words in the
source and target sides of tuple (t, s)n, being J and
I the corresponding total number words in each side
of it.
The forward lexicon model uses IBM Model 1 pa-
rameters obtained from source-to-target algnments,
while the backward lexicon model uses parameters
obtained from target-to-source alignments.
3.3 Decoding and Optimization
The search engine for this translation system was
developed by Crego et al (2005). It implements
a beam-search strategy based on dynamic program-
ming and takes into account all the five feature func-
tions described above simultaneously. It also allows
for three different pruning methods: threshold prun-
ing, histogram pruning, and hypothesis recombina-
tion. For all the results presented in this work the
decoder?s monotonic search modality was used.
An optimization tool, which is based on a simplex
method (Press et al, 2002), was developed and used
for computing log-linear weights for each of the fea-
ture functions described above. This algorithm ad-
justs the log-linear weights so that BLEU (Papineni
et al, 2002) is maximized over a given development
set. One optimization for each language pair was
performed by using the 2000-sentence development
sets made available for the shared task.
4 Shared Task Results
Table 2 presents the BLEU scores obtained for the
shared task test data. Each test set consisted of 2000
sentences. The computed BLEU scores were case
insensitive and used one translation reference.
Table 2: BLEU scores (shared task test sets).
es - en fr - en de - en fi - en
0.3007 0.3020 0.2426 0.2031
As can be seen from Table 2 the best ranked trans-
lations were those obtained for French, followed by
Spanish, German and Finnish. A big difference is
observed between the best and the worst results.
Differences can be observed from translation out-
puts too. Consider, for example, the following seg-
ments taken from one of the test sentences:
es-en: We know very well that the present Treaties are not
enough and that , in the future , it will be necessary to develop
a structure better and different for the European Union...
fr-en: We know very well that the Treaties in their current
are not enough and that it will be necessary for the future to
develop a structure more effective and different for the Union...
de-en: We very much aware that the relevant treaties are
inadequate and , in future to another , more efficient structure
for the European Union that must be developed...
fi-en: We know full well that the current Treaties are not
sufficient and that , in the future , it is necessary to develop the
Union better and a different structure...
It is evident from these translation outputs that
translation quality decreases when moving from
Spanish and French to German and Finnish. A
detailed observation of translation outputs reveals
that there are basically two problems related to this
degradation in quality. The first has to do with re-
ordering, which seems to be affecting Finnish and,
specially, German translations.
The second problem has to do with vocabulary. It
is well known that large vocabularies produce data
sparseness problems (Koehn, 2002). As can be con-
firmed from Tables 1 and 2, translation quality de-
creases as vocabulary size increases. However, it is
not clear yet, in which degree such degradation is
due to monotonic decoding and/or vocabulary size.
Finally, we also evaluated how much the full fea-
ture function system differs from the baseline tu-
ple 3-gram model alone. In this way, BLEU scores
were computed for translation outputs obtained for
the baseline system and the full system. Since the
English reference for the test set was not available,
we computed translations and BLEU scores over de-
135
velopment sets. Table 3 presents the results for both
the full system and the baseline.1
Table 3: Baseline- and full-system BLEU scores
(computed over development sets).
language pair baseline full
es - en 0.2588 0.3004
fr - en 0.2547 0.2938
de - en 0.1844 0.2350
fi - en 0.1526 0.1989
From Table 3, it is evident that the four additional
feature functions produce important improvements
in translation quality.
5 Conclusions and Further Work
As can be concluded from the presented results, per-
formance of the translation system used is much bet-
ter for French and Spanish than for German and
Finnish. As some results suggest, reordering and
vocabulary size are the most important problems re-
lated to the low translation quality achieved for Ger-
man and Finnish.
It is also evident that the bilingual n-gram model
used requires the additional feature functions to pro-
duce better translations. However, more experimen-
tation is required in order to fully understand each
individual feature?s influence on the overall log-
linear model performance.
6 Acknowledgments
This work has been funded by the European Union
under the integrated project TC-STAR - Technology
and Corpora for Speech to Speech Translation -(IST-
2002-FP6-506738, http://www.tc-star.org).
The authors also want to thank Jose? A. R. Fonol-
losa and Marta Ruiz Costa-jussa` for their participa-
tion in discussions related to this work.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. ?The mathemat-
1Differently from BLEU scores presented in Table 2, which
are case insensitive, BLEU scores presented in Table 3 are case
sensitive.
ics of statistical machine translation: parameter esti-
mation?. Computational Linguistics, 19(2):263?311.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2004. ?Finite-state-based and phrase-based statistical
machine translation?. Proc. of the 8th Int. Conf. on
Spoken Language Processing, :37?40, October.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2005. ?A Ngram-based Statistical Machine Transla-
tion Decoder?. Submitted to INTERSPEECH 2005.
Adria` de Gispert, and Jose? B. Marin?o. 2002. ?Using X-
grams for speech-to-speech translation?. Proc. of the
7th Int. Conf. on Spoken Language Processing.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2004. ?TALP: Xgram-based spoken language transla-
tion system?. Proc. of the Int. Workshop on Spoken
Language Translation, :85?90. Kyoto, Japan, October.
EUPARL: European Parliament Proceedings Parallel
Corpus 1996-2003. Available on-line at: http://
people.csail.mit.edu/people/koehn/public
ations/europarl/
Philipp Koehn. 2002. ?Europarl: A Multilingual Cor-
pus for Evaluation of Machine Translation?. Avail-
able on-line at: http://people.csail.mit.edu/
people/koehn/publications/europarl/
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
?Statistical phrase-based translation?. Proc. of the
2003 Meeting of the North American chapter of the
ACL, Edmonton, Alberta.
Franz J. Och and Hermann Ney. 2000. ?Improved statis-
tical alignment models?. Proc. of the 38th Ann. Meet-
ing of the ACL, Hong Kong, China, October.
Franz J. Och and Hermann Ney. 2002. ?Discriminative
training and maximum entropy models for statistical
machine translation?. Proc. of the 40th Ann. Meeting
of the ACL, :295?302, Philadelphia, PA, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. ?Bleu: a method for automatic eval-
uation of machine translation?. Proc. of the 40th Ann.
Conf. of the ACL, Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art of Scientific Computing, Cambridge
University Press.
Andreas Stolcke. 2002. ?SRLIM: an extensible language
modeling toolkit?. Proc. of the Int. Conf. on Spoken
Language Processing :901?904, Denver, CO, Septem-
ber. Available on line at: http://www.speech.sr
i.com/projects/srilm/
136
Grouping Multi-word Expressions According to Part-Of-Speech in
Statistical Machine Translation
Patrik Lambert
TALP Research Center
Jordi Girona Salgado, 1-3
08034 Barcelona, Spain
lambert@gps.tsc.upc.edu
Rafael Banchs
TALP Research Center
Jordi Girona Salgado, 1-3
08034 Barcelona, Spain
rbanchs@gps.tsc.upc.edu
Abstract
This paper studies a strategy for identify-
ing and using multi-word expressions in
Statistical Machine Translation. The per-
formance of the proposed strategy for var-
ious types of multi-word expressions (like
nouns or verbs) is evaluated in terms of
alignment quality as well as translation ac-
curacy. Evaluations are performed by us-
ing real-life data, namely the European
Parliament corpus. Results from trans-
lation tasks from English-to-Spanish and
from Spanish-to-English are presented and
discussed.
1 Introduction
Statistical machine translation (SMT) was origi-
nally focused on word to word translation and was
based on the noisy channel approach (Brown et
al., 1993). Present SMT systems have evolved
from the original ones in such a way that mainly
differ from them in two issues: first, word-based
translation models have been replaced by phrase-
based translation models (Zens et al, 2002) and
(Koehn et al, 2003); and second, the noisy chan-
nel approach has been expanded to a more general
maximum entropy approach in which a log-linear
combination of multiple feature functions is im-
plemented (Och and Ney, 2002).
Nevertheless, it is interesting to call the atten-
tion about one important fact. Despite the change
from a word-based to a phrase-based translation
approach, word to word approaches for inferring
alignment models from bilingual data (Vogel et al,
1996; Och and Ney, 2003) continue to be widely
used.
On the other hand, from observing bilingual
data sets, it becomes evident that in some cases it
is just impossible to perform a word to word align-
ment between two phrases that are translations of
each other. For example, certain combination of
words might convey a meaning which is somehow
independent from the words it contains. This is
the case of bilingual pairs such as ?fire engine?
and ?camio?n de bomberos?.
Notice that a word-to-word alignment strategy
would most probably1 provide the follow-
ing Viterbi alignments for words contained
in the previous example: ?camio?n:truck?,
?bomberos:firefighters?, ?fuego:fire?, and
?ma?quina:engine?.
Of course, it cannot be concluded from these
examples that a SMT system which uses a word
to word alignment strategy will not be able to han-
dle properly the kind of word expression described
above. This is because there are other models and
feature functions involved which can actually help
the SMT system to get the right translation.
However these ideas motivate for exploring
alternatives for using multi-word expression in-
formation in order to improve alignment quality
and consequently translation accuracy. In this
sense, our idea of a multi-word expression (here-
after MWE) refers in principle to word sequences
which cannot be translated literally word-to-word.
However, the automatic technique studied in this
work for extracting and identifying MWEs does
not necessarily follow this definition rigorously.
In a preliminary study (Lambert and Banchs,
2005), we presented a technique for extracting
bilingual multi-word expressions (BMWE) from
parallel corpora. In that study, BMWEs identified
in a small corpus2 were grouped as a unique to-
1Of course, alignment results strongly depends on corpus
statistics.
2VERBMOBIL (Arranz et al, 2003)
9
ken before training alignment models. As a re-
sult, both alignment quality and translation accu-
racy were slightly improved.
In this paper we applied the same BMWE ex-
traction technique, with various improvements, to
a large corpus (EPPS, described in section 4.1).
Since this is a statistical technique, and frequen-
cies of multi-word expressions are low (Baldwin
and Villavicencio, 2002), the size of the corpus is
an important factor. A few very basic rules based
on part-of-speech have also been added to filter out
noisy entries in the dictionary. Finally, BMWEs
have been classified into three categories (nouns,
verbs and others). In addition to the impact of the
whole set, the impact of each category has been
evaluated separately.
The technique will be explained in section 3, af-
ter presenting the baseline translation system used
(section 2). Experimental results are presented in
section 4. Finally some conclusions are presented
and further work in this area is depicted.
2 Baseline Translation System
This section describes the SMT approach that was
used in this work. A more detailed description
of the presented translation system is available in
Marin?o et al (2005). This approach implements
a translation model which is based on bilingual
n-grams, and was developed by de Gispert and
Marin?o (2002).
The bilingual n-gram translation model actu-
ally constitutes a language model of bilingual units
which are referred to as tuples. This model ap-
proximates the joint probability between source
and target languages by using 3-grams as it is de-
scribed in the following equation:
p(T, S) ?
N?
n=1
p((t, s)n|(t, s)n?2, (t, s)n?1) (1)
where t refers to target, s to source and (t, s)n to
the nth tuple of a given bilingual sentence pair.
Tuples are extracted from a word-to-word
aligned corpus. More specifically, word-to-
word alignments are performed in both direc-
tions, source-to-target and target-to-source, by us-
ing GIZA++ (Och and Ney, 2003), and tuples are
extracted from the union set of alignments accord-
ing to the following constraints (de Gispert and
Marin?o, 2004):
? a monotonous segmentation of each bilingual
sentence pairs is produced,
? no word inside the tuple is aligned to words
outside the tuple, and
? no smaller tuples can be extracted without vi-
olating the previous constraints.
As a consequence of these constraints, only one
segmentation is possible for a given sentence pair.
Figure 1 presents a simple example illustrating the
tuple extraction process.
Figure 1: Example of tuple extraction from an
aligned bilingual sentence pair.
A tuple set is extracted for each transla-
tion direction, Spanish-to-English and English-to-
Spanish. Then the tuple 3-gram models are trained
by using the SRI Language Modelling toolkit
(Stolcke, 2002).
The search engine for this translation system
was developed by Crego et al (2005). It imple-
ments a beam-search strategy based on dynamic
programming. The decoder?s monotonic search
modality was used.
This decoder was designed to take into account
various different models simultaneously, so trans-
lation hypotheses are evaluated by considering a
log-linear combination of feature functions. These
feature functions are the translation model, a tar-
get language model, a word bonus model, a lexical
model and an inverse lexical model.
3 Experimental Procedure
In this section we describe the technique used to
see the effect of multi-words information on the
translation model described in section 2.
3.1 Bilingual Multi-words Extraction
First, BMWEs were automatically extracted from
the parallel training corpus and the most relevant
ones were stored in a dictionary.
3.1.1 Asymmetry Based Extraction
For BMWE extraction, the method proposed
by Lambert and Castell (2004) was used. This
10
verdad . . . . . . +
es . . . . . + .
esto . . . . + . .
; . . . + . . .
siento . ?+ . . .
lo ? ? . . . .
I ?m so
rr
y
, th
is
is tru
e
Figure 2: There is an asymmetry in the word-to-
word alignments of the idiomatic expression ?lo
siento ? I ?m sorry?. Source-target and target-
source links are represented respectively by hor-
izontal and vertical dashes.
method is based on word-to-word alignments
which are different in the source-target and target-
source directions, such as the alignments trained
to extract tuples (section 2). Multi-words like id-
iomatic expressions or collocations can typically
not be aligned word-to-word, and cause a (source-
target and target-source) asymmetry in the align-
ment matrix. An asymmetry in the alignment ma-
trix is a sub-matrix where source-target and target-
source links are different. If a word is part of an
asymmetry, all words linked to it are also part of
this asymmetry. An example is depicted in figure
2.
In this method, asymmetries in the training cor-
pus are detected and stored as possible BMWEs.
Accurate statistics are needed to score each
BMWE entry. In the identification phase (sec-
tion 3.3), these scores permit to prioritise for
the selection of some entries with respect to oth-
ers. Previous experiments (Lambert and Banchs,
2005) have shown than the large set of bilingual
phrases described in the following section pro-
vides better statistics than the set of asymmetry-
based BMWEs.
3.1.2 Scoring Based on Bilingual Phrases
Here we refer to Bilingual Phrase (BP) as the
bilingual phrases used by Och and Ney (2004).
The BP are pairs of word groups which are sup-
posed to be the translation of each other. The set
of BP is consistent with the alignment and con-
sists of all phrase pairs in which all words within
the target language are only aligned to the words
of the source language and vice versa. At least
one word of the target language phrase has to be
aligned with at least one word of the source lan-
guage phrase. Finally, the algorithm takes into ac-
count possibly unaligned words at the boundaries
of the target or source language phrases.
We extracted all BP of length up to four words,
with the algorithm described by Och and Ney.
Then we estimated the phrase translation proba-
bility distribution by relative frequency:
p(t|s) =
N(t, s)
N(s)
(2)
In equation 2, s and t stand for the source and
target side of the BP, respectively. N(t, s) is the
number of times the phrase s is translated by t,
and N(s) is the number of times s occurs in the
corpus. We took the minimum of both direct and
inverse relative frequencies as probability of a BP.
If this minimum was below some threshold, the
BP was pruned. Otherwise, this probability was
multiplied by the number of occurrences N(t, s)
of this phrase pair in the whole corpus. A weight
? was introduced to balance the respective impor-
tance of relative frequency and number of occur-
rences, as shown in equation 3:
score = min(p(t|s), p(s|t)) N(t, s)?
= min(
N(t, s)1+?
N(s)
,
N(t, s)1+?
N(t)
)
(3)
We performed the intersection between the en-
tire BP set and the entire asymmetry based multi-
words set, keeping BP scores. Notice that the en-
tire set of BP is not adequate for our dictionary be-
cause BP are extracted from all parts of the align-
ment (and not in asymmetries only), so most BP
are not BMWEs but word sequences that can be
decomposed and translated word to word.
3.2 Lexical and Morpho-syntactic Filters
In English and Spanish, a list of stop words3
(respectively 19 and 26) was established. The
BMWE dictionary was also processed by a Part-
Of-Speech (POS) tagger and eight rules were writ-
ten to filter out noisy entries. These rules depend
on the tag set used. Examples of criteria to reject
a BMWE include:
? Its source or target side only contains stop
words
? Its source or target side ends with a coordina-
tion conjunction
3frequently occurring, semantically insignificant words
like ?in?, ?of?, ?on?.
11
? Its source or target side begins with a coordi-
nation conjunction (except ?nor?, in English)
? Its source or target side ends with an indefi-
nite determiner
English data have been POS-tagged using the TnT
tagger (Brants, 2000), after the lemmas have been
extracted with wnmorph, included in the Wordnet
package (Miller et al, 1991). POS-tagging for
Spanish has been performed using the FreeLing
analysis tool (Carreras et al, 2004).
Finally, the BMWE set has been divided in three
subsets, according to the following criteria, ap-
plied in this order:
? If source AND target sides of a BMWE con-
tain at least a verb, it is assigned to the ?verb?
class.
? If source AND target sides of a BMWE con-
tain at least a noun, it is assigned to the
?noun? class.
? Otherwise, it is assigned to the ?misc? class
(miscellaneous). Note that this class is
mainly composed of adverbial phrases.
3.3 Multi-Words Identification
Identification consists, first, of the detection of all
possible BMWE(s) in the corpus, and second, of
the selection of the relevant candidates.
The detection part simply means matching the
entries of the dictionaries described in the previ-
ous subsections. In the example of figure 2, the
following BMWEs would have been detected (the
number on the right is the score):
i am sorry ||| lo siento ||| 1566
am sorry ||| siento ||| 890
it is ||| es ||| 1004407
it is ||| esto es ||| 269
true ||| es verdad ||| 63
Then, selection in a sentence pair runs as fol-
lows. First, the BMWE with highest score among
the possible candidates is considered and its cor-
responding positions are set as covered. If this
BMWE satisfies the selection criterion, the corre-
sponding words in the source and target sentences
are grouped as a unique token. This process is re-
peated until all word positions are covered in the
sentence pair, or until no BMWE matches the po-
sitions remaining to cover.
The selection criterion rejects candidates whose
words are linked to exactly one word. Thus in the
example, ?esto ? this is? would not be selected.
This is correct, because the subject ?esto? (this)
of the verb ?es? (is) in Spanish is not omitted, so
that ?this is ? es? does not act as BMWE (?esto?
should be translated to ?this? and ?is? to ?es?).
At the end of the identification process the sen-
tence pair of figure 2 would be the following:
?lo siento ; esto es verdad ? I ?m sorry , this is
true?.
In order to increase the recall, BMWE detec-
tion was insensitive to the case of the first letter
of each multi-word. The detection engine also al-
lows a search based on lemmas. Two strategies
are possible. In the first one, search is first carried
out with full forms, so that lemmas are resorted to
only if no match is found with full forms. In the
second strategy, only lemmas are considered.
3.4 Re-alignment
The modified training corpus, with identified
BMWEs grouped in a unique ?super-token? was
aligned again in the same way as explained in sec-
tion 2. By grouping multi-words, we increased the
size of the vocabulary and thus the sparseness of
data. However, we expect that if the meaning of
the multi-words expressions we grouped is effec-
tively different from the meaning of the words they
contain, the individual word probabilities should
be improved.
After re-aligning, we unjoined the super-tokens
that had been grouped in the previous stage, cor-
recting the alignment set accordingly. More pre-
cisely, if two super-tokens A and B were linked
together, after ungrouping them into various to-
kens, every word of A was linked to every word
of B. Translation units were extracted from this
corrected alignment, with the unjoined sentence
pairs (i.e.the same as in the baseline). So the only
difference with respect to the baseline lied in the
alignment, and thus in the distribution of transla-
tion units and in lexical model probabilities.
4 Experimental Results
4.1 Training and Test Data
Our task was word alignment and translation of
parliamentary session transcriptions of the Eu-
ropean Parliament (EPPS). These data are cur-
rently available at the Parliament?s website.4 They
were distributed through the TC-STAR consor-
tium.5 The training and translation test data used
4http://www.euro parl.eu.int/
5http: //www.tc-star.org/
12
included session transcriptions from April 1996
until September 2004, and from November 15th
until November 18th, 2004, respectively. Transla-
tion test data include two reference sets. Align-
ment test data was a subset of the training data
(Lambert et al, 2006).
Table 1 presents some statistics of the various
data sets for each considered language: English
(eng) and Spanish (spa). More specifically, the
statistics presented in Table 1 are, the total num-
ber of sentences, the total number of words, the
vocabulary size (or total number of distinct words)
and the average number of words per sentence.
1.a.- Training data set
Lang. Sentences Words Vocab. Aver.
Eng 1.22 M 33.4 M 105 k 27.3
Spa 1.22 M 35.0 M 151 k 28.6
1.b.- Test data set for translation
Lang. Sentences Words Vocab. Aver.
Eng 1094 26.8 k 3.9 k 24.5
Spa 840 22.7 k 4.0 k 27.0
1.c.- Word alignment reference
Lang. Sentences Words Vocab. Aver.
Eng 400 11.7 k 2.7 k 29.1
Spa 400 12.3 k 3.1 k 30.4
Table 1: Basic statistics for the considered training
(a) translation test (b) and alignment test (c) data
sets (M and k stands for millions and thousands,
respectively).
4.2 Evaluation measures
Details about alignment evaluation can be found
in Lambert et al (2006). The alignment test data
contain unambiguous links (called S or Sure) and
ambiguous links (called P or Possible). If there
is a P link between two words in the reference, a
computed link (i.e. to be evaluated) between these
words is acceptable, but not compulsory. On the
contrary, if there would be an S link between these
words in the reference, a computed link would
be compulsory. In this paper, precision refers to
the proportion of computed links that are present
in the reference. Recall refers to the proportion
of reference Sure links that were computed. The
alignment error rate (AER) is given by the follow-
ing formula:
AER = 1?
|A ? GS |+ |A ? G|
|A|+ |GS |
(4)
where A is the set of computed links, GS is the set
of Sure reference links and G is the entire set of
reference links.
As for translation evaluation, we used the fol-
lowing measures:
WER (word error rate) or mWER (multi-
reference word error rate) The WER is the
minimum number of substitution, insertion
and deletion operations that must be per-
formed to convert the generated sentence into
the reference target sentence. For the mWER,
a whole set of reference translations is used.
In this case, for each translation hypothesis,
the edit distance to the most similar sentence
is calculated.
BLEU score This score measures the precision of
unigrams, bigrams, trigrams, and fourgrams
with respect to a whole set of reference trans-
lations, and with a penalty for too short sen-
tences (Papineni et al, 2001). BLEU mea-
sures accuracy, thus larger scores are better.
4.3 Multi-words in Training Data
In this section we describe the results of the
BMWE extraction and detection techniques ap-
plied to the training data.
4.3.1 Description of the BMWE dictionaries
Parameters of the extraction process have been
optimised with the alignment development corpus
available with the alignment test corpus. With
these parameters, a dictionary of 60k entries was
extracted. After applying the lexical and morpho-
syntactic filters, 45k entries were left. The best
30k entries (hereinafter referred to as all) have
been selected for the experiments and divided in
the three groups mentioned in section 3.2. verb,
noun and misc (miscellaneous) dictionaries con-
tained respectively 11797, 9709 and 8494 entries.
Table 2 shows recall and precision for the
BMWEs identified with each dictionary. The
first line is the evaluation of the MWEs obtained
with the best 30k entries of the dictionary before
filtering. Alignments evaluated in table 2 con-
tained only links corresponding to the identified
BMWEs. For an identified BMWE, a link was in-
troduced between each word of the source side and
each word of the target side. Nevertheless, the test
data contained the whole set of links.
From table 2 we see the dramatic effect of the
filters. The precision for nouns is lower than for
13
Recall Precision
Best 30k (no filters) 13.6 53.6
Best 30k (filters) 11.4 79.3
VERB (filters) 3.7 81.8
NOUN (filters) 4.0 72.8
MISC (filters) 4.1 80.8
Table 2: Quality of the BMWEs identified from
the various dictionaries.
the other categories because many word groups
which were identified, like ?European Parliament -
Parlamento europeo?, are not aligned as a group in
the alignment reference. Notice also that the data
in table 2 reflects the precision of bilingual MWE,
which is a lower bound of the precision of ?super-
tokens? formed in each sentence, the quantity that
matters in our experiment.
Identification of BMWE based on lemmas has
also been experimented. However, with lemmas,
the selection phase is more delicate. With our ba-
sic selection criterion (see section 3.3), the quality
of MWEs identified was worse so we based iden-
tification on full forms.
Figure 3 shows the first 10 entries in the misc
dictionary, along with their renormalised score.
Notice that ?the EU - la UE?, ?young people -
jo?venes? and ?the WTO - la OMC? have been in-
correctly classified due to POS-tagging errors.
the EU ||| la UE ||| 770731
secondly ||| en segundo lugar ||| 610599
however ||| sin embargo ||| 443042
finally ||| por u?ltimo ||| 421879
firstly ||| en primer lugar ||| 324396
thirdly ||| en tercer lugar ||| 286924
young people ||| jo?venes ||| 178571
the WTO ||| la OMC ||| 174496
once again ||| una vez ma?s ||| 169317
once ||| una vez ||| 150139
Figure 3: Examples of BMWEs of the misc cate-
gory.
4.3.2 BMWE Identification Statistics
Table 3 shows, for each language, the MWE vo-
cabulary size after the identification process, and
how many times a MWE has been grouped as a
unique token (instances). The different number
of instances between Spanish and English corre-
spond to one-to-many BMWEs. In general more
MWEs are grouped in the Spanish side, because
English is a denser language. However, the omis-
sion of the subject in Spanish causes the inverse
situation for verbs.
Vocabulary Instances
ENG SPA ENG SPA
ALL 12.2k 12.6k 1.28M 1.56M
VERB 6.0k 3.3k 738k 237k
NOUN 3.9k 5.9k 288k 827k
MISC 3.1k 4.3k 336k 557k
Table 3: Statistics for the BMWEs identified from
the various dictionaries. ALL refers to the 30k best
entries with filters.
4.4 Alignment and Translation Results
Tables 4 and 5 show the effect of aligning the cor-
pus when the various categories of multi-words
have been previously grouped.
IBM1 lexical probabilities baseline All
p(in other words|es decir) - 0.94
p(words|decir) 0.23 0.0013
p(other|decir) 0.026 6 10?5
p(say|decir) 0.45 0.49
Table 4: Single word lexical probabilities of the
alignment model in the baseline and after group-
ing MWE with all dictionary entries. The multi-
word tokens ?in other words? and ?es decir? do
not exist in the baseline.
In table 4 we see how word-to-word lexi-
cal probabilities of the alignment model can be
favourably modified. In the baseline, due to pres-
ence of the fixed expression ?in other words -
es decir?, the probability of ?words? given ?de-
cir? (?say? in English) is high. With this ex-
pression grouped, probabilities p(words|decir) and
p(other|decir) vanish, while p(say|decir) is rein-
forced. These observations allowed to expect that
with many individual probabilities improved, a
global improvement of the alignment would occur.
However, table 5 shows that alignment is not
better when trained with BMWEs grouped as a
unique token.
A closer insight into alignments confirms that
they have not been improved globally. Changes
with respect to the baseline are very localised and
correspond directly to the grouping of the BMWEs
present in each sentence pair.
Table 6 presents the automatic translation eval-
14
Recall Precision AER
Baseline 76.3 85.0 19.4
All 78.0 82.0 19.9
Verb 77.0 84.5 19.3
Noun 76.8 83.0 20.0
Misc 77.0 84.1 19.4
Table 5: Alignment results
uation results. In the Spanish to English direc-
tion, BMWEs seem to have a negative influence.
In the English to Spanish direction, no significant
improvement or worsening is observed.
S?E E?S
mWER BLEU mWER BLEU
Baseline 34.4 0.547 40.2 0.472
All 36.4 0.517 40.7 0.470
Verb 35.1 0.537 40.2 0.472
Noun 35.1 0.537 40.7 0.469
Misc 35.8 0.527 41.1 0.466
Table 6: Translation results in Spanish-to-English
(S?E) and English-to-Spanish (E?S) directions.
In order to understand these results better, we
performed a manual error analysis for the first 50
sentences of the test corpus. We analysed, for the
experiment with all dictionary entries (?All? line
of table 6), the changes in translation with respect
to the baseline. We counted how many changes
had a neutral, positive or negative effect on trans-
lation quality. Results are shown in table 7. Notice
that approximatively half of these changes were
directly related to the presence some BMWE.
This study permitted to see interesting qualita-
tive features. First, BMWEs have a clear influence
on translation, sometimes positive and sometimes
negative, with a balance which appears to be null
in this experiment. In many examples BMWEs
allowed a group translation instead of an incor-
rect word to word literal translation. For instance,
?Red Crescent? was translated by ?Media Luna
Roja? instead of ?Cruz Luna? (cross moon).
Two main types of error were observed. The
first ones are related to the quality of BMWEs. De-
terminers, or particles like ?of?, which are present
in BMWEs are mistakenly inserted in the trans-
lations. Some errors are caused by inadequate
BMWEs. For example ?looking at ? si anal-
izamos? (?if we analyse?) cannot be used in the
sense of looking with the eyes. The second type of
error is related to the rigidity and data sparseness
introduced in the bilingual n-gram model. For ex-
ample, when inflected forms are encapsulated in
a BMWE, the model looses flexibility to trans-
late the correct inflection. Another typical error
is caused by the use of back-off (n-1)-grams in
the bilingual language model, when the n-gram is
not any more available because of increased data
sparseness.
The error analysis did not give explanation for
why the effect of BMWEs is so different for dif-
ferent translation directions. A possible hypothe-
sis would be that BMWEs help in translating from
a denser language. However, in this case, verbs
would be expected to help relatively more in the
Spanish to English direction, since there are more
verb group instances in the English side.
Neutral Positive Negative
S?E 43 20 22
E?S 49 19 17
Table 7: Effect on quality of differences in the
translations between the baseline and the BMWE
experiment with ?ALL? dictionary. S and E stand
for Spanish and English, respectively.
5 Conclusions and Further work
We applied a technique for extracting and using
BMWEs in Statistical Machine Translation. This
technique is based on grouping BMWEs before
performing statistical alignment. On a large cor-
pus with real-life data, this technique failed to
clearly improve alignment quality or translation
accuracy.
After performing a detailed error analysis, we
believe that when the considered MWEs are fixed
expressions, grouping them before training helps
for their correct translation in test. However,
grouping MWEs which could in fact be translated
word to word, doesn?t help and introduces unnec-
essary rigidity and data sparseness in the models.
The main strength of the n-gram translation
model (its history capability) is reduced when tu-
ples become longer. So we plan to run this experi-
ment with a phrase-based translation model. Since
these models use unigrams, they are more flexible
and less sensitive to data sparseness.
Some errors were also caused by noise in the
automatic generation of BMWEs. Thus filter-
15
ing techniques should be improved, and differ-
ent methods for extracting and identifying MWEs
must be developed and evaluated. Resources build
manually, like Wordnet multi-word expressions,
should also be considered.
The proposed method considers the bilingual
multi-words as units ; the use of each side of the
BMWEs as independent monolingual multi-words
must be considered and evaluated.
Acknowledgements
This work has been partially funded by the Eu-
ropean Union under the integrated project TC-
STAR - Technology and Corpora for Speech
to Speech Translation -(IST-2002-FP6-506738,
http://www.tc-star.org).
References
V. Arranz, N. Castell, and J. Gime?nez. 2003. Devel-
opment of language resources for speech-to-speech
translation. In Proc. of the International Conference
on Recent Advances in Natural Language Process-
ing (RANLP), Borovets, Bulgary, September, 10-12.
T. Baldwin and A. Villavicencio. 2002. Extracting the
unextractable: A case study on verb-particles. In
Computational Natural Language Learning Work-
shop (CoNLL).
T. Brants. 2000. Tnt ? a statistical part-of-speech
tagger. In Proc. of Applied Natural Language Pro-
cessing (ANLP), Seattle, WA.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
Xavier Carreras, I. Chao, L. Padro?, and M. Padro?.
2004. Freeling: An open-source suite of lan-
guage analyzers. In Proc. of the 4th International
Conference on Linguistic Resources and Evaluation
(LREC), Lisbon, Portugal, May.
J. M. Crego, J. Marin?o, and A. de Gispert. 2005. A
ngram-based statistical machine translation decoder.
In Proc. of the 9th European Conf. on Speech Com-
munication and Technology (Interspeech), pages
3185?88, Lisbon, Portugal.
A. de Gispert and J. Marin?o. 2002. Using X-grams for
speech-to-speech translation. Proc. of the 7th Int.
Conf. on Spoken Language Processing, ICSLP?02,
September.
A. de Gispert and J. Marin?o. 2004. Talp: Xgram-
based spoken language translation system. Proc. of
the Int. Workshop on Spoken Language Translation,
IWSLT?04, pages 85?90, October.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of the 41th An-
nual Meeting of the Association for Computational
Linguistics.
P. Lambert and R. Banchs. 2005. Data inferred multi-
word expressions for statistical machine translation.
In Proc. of Machine Translation Summit X, pages
396?403, Phuket, Tailandia.
P. Lambert and N. Castell. 2004. Alignment of parallel
corpora exploiting asymmetrically aligned phrases.
In Proc. of the LREC 2004 Workshop on the Amaz-
ing Utility of Parallel and Comparable Corpora,
Lisbon, Portugal, May 25.
P. Lambert, A. de Gispert, R. Banchs, and J. Marin?o.
2006. Guidelines for word alignment and manual
alignment. Accepted for publication in Language
Resources and Evaluation.
J. Marin?o, R. Banchs, J. M. Crego, A. de Gispert,
P. Lambert, J.A. Fonollosa, and M. Ruiz. 2005.
Bilingual n-gram statistical machine translation. In
Proc. of Machine Translation Summit X, pages 275?
82, Phuket, Thailand.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross,
K. Miller, and R. Tengi. 1991. Five papers on word-
net. Special Issue of International Journal of Lexi-
cography, 3(4):235?312.
F.J. Och and H. Ney. 2002. Dicriminative training
and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 295?302, Philadelphia, PA, July.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
F.J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449, December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Re-
port, RC22176, September.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proc. of the Int. Conf. on Spo-
ken Language Processing, pages 901?904, Denver,
CO.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING?96: The 16thInt. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Springer Verlag,
editor, Proc. German Conference on Artificial Intel-
ligence (KI), september.
16
Proceedings of the Workshop on Statistical Machine Translation, pages 1?6,
New York City, June 2006. c?2006 Association for Computational Linguistics
Morpho-syntactic Information for Automatic Error Analysis of Statistical
Machine Translation Output
Maja Popovic??
Hermann Ney?
Adria` de Gispert?
Jose? B. Marin?o?
Deepa Gupta?
Marcello Federico?
Patrik Lambert?
Rafael Banchs?
? Lehrstuhl fu?r Informatik VI - Computer Science Department, RWTH Aachen University, Aachen, Germany
? TALP Research Center, Universitat Polite`cnica de Catalunya (UPC), Barcelona, Spain
? ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy
{popovic,ney}@informatik.rwth-aachen.de {agispert,canton}@gps.tsc.upc.es
{gupta,federico}@itc.it {lambert,banchs}@gps.tsc.upc.es
Abstract
Evaluation of machine translation output
is an important but difficult task. Over the
last years, a variety of automatic evalua-
tion measures have been studied, some of
them like Word Error Rate (WER), Posi-
tion Independent Word Error Rate (PER)
and BLEU and NIST scores have become
widely used tools for comparing different
systems as well as for evaluating improve-
ments within one system. However, these
measures do not give any details about
the nature of translation errors. Therefore
some analysis of the generated output is
needed in order to identify the main prob-
lems and to focus the research efforts. On
the other hand, human evaluation is a time
consuming and expensive task. In this
paper, we investigate methods for using
of morpho-syntactic information for auto-
matic evaluation: standard error measures
WER and PER are calculated on distinct
word classes and forms in order to get a
better idea about the nature of translation
errors and possibilities for improvements.
1 Introduction
The evaluation of the generated output is an impor-
tant issue for all natural language processing (NLP)
tasks, especially for machine translation (MT). Au-
tomatic evaluation is preferred because human eval-
uation is a time consuming and expensive task.
A variety of automatic evaluation measures have
been proposed and studied over the last years, some
of them are shown to be a very useful tool for com-
paring different systems as well as for evaluating
improvements within one system. The most widely
used are Word Error Rate (WER), Position Indepen-
dent Word Error Rate (PER), the BLEU score (Pap-
ineni et al, 2002) and the NIST score (Doddington,
2002). However, none of these measures give any
details about the nature of translation errors. A rela-
tionship between these error measures and the actual
errors in the translation outputs is not easy to find.
Therefore some analysis of the translation errors is
necessary in order to define the main problems and
to focus the research efforts. A framework for hu-
man error analysis and error classification has been
proposed in (Vilar et al, 2006), but like human eval-
uation, this is also a time consuming task.
The goal of this work is to present a framework
for automatic error analysis of machine translation
output based on morpho-syntactic information.
2 Related Work
There is a number of publications dealing with
various automatic evaluation measures for machine
translation output, some of them proposing new
measures, some proposing improvements and exten-
sions of the existing ones (Doddington, 2002; Pap-
ineni et al, 2002; Babych and Hartley, 2004; Ma-
tusov et al, 2005). Semi-automatic evaluation mea-
sures have been also investigated, for example in
(Nie?en et al, 2000). An automatic metric which
uses base forms and synonyms of the words in or-
der to correlate better to human judgements has been
1
proposed in (Banerjee and Lavie, 2005). However,
error analysis is still a rather unexplored area. A
framework for human error analysis and error clas-
sification has been proposed in (Vilar et al, 2006)
and a detailed analysis of the obtained results has
been carried out. Automatic methods for error anal-
ysis to our knowledge have not been studied yet.
Many publications propose the use of morpho-
syntactic information for improving the perfor-
mance of a statistical machine translation system.
Various methods for treating morphological and
syntactical differences between German and English
are investigated in (Nie?en and Ney, 2000; Nie?en
and Ney, 2001a; Nie?en and Ney, 2001b). Mor-
phological analysis has been used for improving
Arabic-English translation (Lee, 2004), for Serbian-
English translation (Popovic? et al, 2005) as well as
for Czech-English translation (Goldwater and Mc-
Closky, 2005). Inflectional morphology of Spanish
verbs is dealt with in (Popovic? and Ney, 2004; de
Gispert et al, 2005). To the best of our knowledge,
the use of morpho-syntactic information for error
analysis of translation output has not been investi-
gated so far.
3 Morpho-syntactic Information and
Automatic Evaluation
We propose the use of morpho-syntactic informa-
tion in combination with the automatic evaluation
measures WER and PER in order to get more details
about the translation errors.
We investigate two types of potential problems for
the translation with the Spanish-English language
pair:
? syntactic differences between the two lan-
guages considering nouns and adjectives
? inflections in the Spanish language considering
mainly verbs, adjectives and nouns
As any other automatic evaluation measures,
these novel measures will be far from perfect. Pos-
sible POS-tagging errors may introduce additional
noise. However, we expect this noise to be suffi-
ciently small and the new measures to be able to give
sufficiently clear ideas about particular errors.
3.1 Syntactic differences
Adjectives in the Spanish language are usually
placed after the corresponding noun, whereas in En-
glish is the other way round. Although in most cases
the phrase based translation system is able to han-
dle these local permutations correctly, some errors
are still present, especially for unseen or rarely seen
noun-adjective groups. In order to investigate this
type of errors, we extract the nouns and adjectives
from both the reference translations and the sys-
tem output and then calculate WER and PER. If the
difference between the obtained WER and PER is
large, this indicates reordering errors: a number of
nouns and adjectives is translated correctly but in the
wrong order.
3.2 Spanish inflections
Spanish has a rich inflectional morphology, espe-
cially for verbs. Person and tense are expressed
by the suffix so that many different full forms of
one verb exist. Spanish adjectives, in contrast to
English, have four possible inflectional forms de-
pending on gender and number. Therefore the er-
ror rates for those word classes are expected to be
higher for Spanish than for English. Also, the er-
ror rates for the Spanish base forms are expected to
be lower than for the full forms. In order to investi-
gate potential inflection errors, we compare the PER
for verbs, adjectives and nouns for both languages.
For the Spanish language, we also investigate differ-
ences between full form PER and base form PER:
the larger these differences, more inflection errors
are present.
4 Experimental Settings
4.1 Task and Corpus
The corpus analysed in this work is built in the
framework of the TC-Star project. It contains more
than one million sentences and about 35 million run-
ning words of the Spanish and English European
Parliament Plenary Sessions (EPPS). A description
of the EPPS data can be found in (Vilar et al, 2005).
In order to analyse effects of data sparseness, we
have randomly extracted a small subset referred to
as 13k containing about thirteen thousand sentences
and 370k running words (about 1% of the original
2
Training corpus: Spanish English
full Sentences 1281427
Running Words 36578514 34918192
Vocabulary 153124 106496
Singletons [%] 35.2 36.2
13k Sentences 13360
Running Words 385198 366055
Vocabulary 22425 16326
Singletons [%] 47.6 43.7
Dev: Sentences 1008
Running Words 25778 26070
Distinct Words 3895 3173
OOVs (full) [%] 0.15 0.09
OOVs (13k) [%] 2.7 1.7
Test: Sentences 840 1094
Running Words 22774 26917
Distinct Words 4081 3958
OOVs (full) [%] 0.14 0.25
OOVs (13k) [%] 2.8 2.6
Table 1: Corpus statistics for the Spanish-English
EPPS task (running words include punctuation
marks)
corpus). The statistics of the corpora can be seen in
Table 1.
4.2 Translation System
The statistical machine translation system used in
this work is based on a log-linear combination of
seven different models. The most important ones are
phrase based models in both directions, additionally
IBM1 models at the phrase level in both directions
as well as phrase and length penalty are used. A
more detailed description of the system can be found
in (Vilar et al, 2005; Zens et al, 2005).
4.3 Experiments
The translation experiments have been done in both
translation directions on both sizes of the corpus. In
order to examine improvements of the baseline sys-
tem, a new system with POS-based word reorderings
of nouns and adjectives as proposed in (Popovic? and
Ney, 2006) is also analysed. Adjectives in the Span-
ish language are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, local reorderings of nouns and ad-
Spanish?English WER PER BLEU
full baseline 34.5 25.5 54.7
reorder 33.5 25.2 56.4
13k baseline 41.8 30.7 43.2
reorder 38.9 29.5 48.5
English?Spanish WER PER BLEU
full baseline 39.7 30.6 47.8
reorder 39.6 30.5 48.3
13k baseline 49.6 37.4 36.2
reorder 48.1 36.5 37.7
Table 2: Translation Results [%]
jective groups in the source language have been ap-
plied. If the source language is Spanish, each noun is
moved behind the corresponding adjective group. If
the source language is English, each adjective group
is moved behind the corresponding noun. An adverb
followed by an adjective (e.g. ?more important?) or
two adjectives with a coordinate conjunction in be-
tween (e.g. ?economic and political?) are treated as
an adjective group. Standard translation results are
presented in Table 2.
5 Error Analysis
5.1 Syntactic errors
As explained in Section 3.1, reordering errors due
to syntactic differences between two languages have
been measured by the relative difference between
WER and PER calculated on nouns and adjectives.
Corresponding relative differences are calculated
also for verbs as well as adjectives and nouns sep-
arately.
Table 3 presents the relative differences for the
English and Spanish output. It can be seen that
the PER/WER difference for nouns and adjectives
is relatively high for both language pairs (more than
20%), and for the English output is higher than for
the Spanish one. This corresponds to the fact that
the Spanish language has a rather free word order:
although the adjective usually is placed behind the
noun, this is not always the case. On the other hand,
adjectives in English are always placed before the
corresponding noun. It can also be seen that the
difference is higher for the reduced corpus for both
outputs indicating that the local reordering problem
3
English output 1? PERWER
full nouns+adjectives 24.7
+reordering 20.8
verbs 4.1
adjectives 10.2
nouns 20.1
13k nouns+adjectives 25.7
+reordering 20.1
verbs 4.6
adjectives 8.4
nouns 19.1
Spanish output 1? PERWER
full nouns+adjectives 21.5
+reordering 20.3
verbs 3.3
adjectives 5.6
nouns 16.9
13k nouns+adjectives 22.9
+reordering 19.8
verbs 3.9
adjectives 5.4
nouns 19.3
Table 3: Relative difference between PER and
WER [%] for different word classes
is more important when only small amount of train-
ing data is available. As mentioned in Section 3.1,
the phrase based translation system is able to gen-
erate frequent noun-adjective groups in the correct
word order, but unseen or rarely seen groups intro-
duce difficulties.
Furthermore, the results show that the POS-based
reordering of adjectives and nouns leads to a de-
crease of the PER/WER difference for both out-
puts and for both corpora. Relative decrease of the
PER/WER difference is larger for the small corpus
than for the full corpus. It can also be noted that the
relative decrease for both corpora is larger for the
English output than for the Spanish one due to free
word order - since the Spanish adjective group is not
always placed behind the noun, some reorderings in
English are not really needed.
For the verbs, PER/WER difference is less than
5% for both outputs and both training corpora, in-
dicating that the word order of verbs is not an im-
English output PER
full verbs 44.8
adjectives 27.3
nouns 23.0
13k verbs 56.1
adjectives 38.1
nouns 31.7
Spanish output PER
full verbs 61.4
adjectives 41.8
nouns 28.5
13k verbs 73.0
adjectives 50.9
nouns 37.0
Table 4: PER [%] for different word classes
portant issue for the Spanish-English language pair.
PER/WER difference for adjectives and nouns is
higher than for verbs, for the nouns being signifi-
cantly higher than for adjectives. The reason for this
is probably the fact that word order differences in-
volving only the nouns are also present, for example
?export control = control de exportacio?n?.
5.2 Inflectional errors
Table 4 presents the PER for different word classes
for the English and Spanish output respectively. It
can be seen that all PERs are higher for the Spanish
output than for the English one due to the rich in-
flectional morphology of the Spanish language. It
can be also seen that the Spanish verbs are espe-
cially problematic (as stated in (Vilar et al, 2006))
reaching 60% of PER for the full corpus and more
than 70% for the reduced corpus. Spanish adjectives
also have a significantly higher PER than the English
ones, whereas for the nouns this difference is not so
high.
Results of the further analysis of inflectional er-
rors are presented in Table 5. Relative difference
between full form PER and base form PER is sig-
nificantly lower for adjectives and nouns than for
verbs, thus showing that the verb inflections are the
main source of translation errors into the Spanish
language.
Furthermore, it can be seen that for the small cor-
4
Spanish output 1? PERbPERf
full verbs 26.9
adjectives 9.3
nouns 8.4
13k verbs 23.7
adjectives 15.1
nouns 6.5
Table 5: Relative difference between PER of base
forms and PER of full forms [%] for the Spanish
output
pus base/full PER difference for verbs and nouns is
basically the same as for the full corpus. Since nouns
in Spanish only have singular and plural form as in
English, the number of unseen forms is not partic-
ularly enlarged by the reduction of the training cor-
pus. On the other hand, base/full PER difference of
adjectives is significantly higher for the small corpus
due to an increased number of unseen adjective full
forms.
As for verbs, intuitively it might be expected that
the number of inflectional errors for this word class
also increases by reducing the training corpus, even
more than for adjectives. However, the base/full
PER difference is not larger for the small corpus,
but even smaller. This is indicating that the problem
of choosing the right inflection of a Spanish verb ap-
parently is not related to the number of unseen full
forms since the number of inflectional errors is very
high even when the translation system is trained on
a very large corpus.
6 Conclusion
In this work, we presented a framework for auto-
matic analysis of translation errors based on the use
of morpho-syntactic information. We carried out a
detailed analysis which has shown that the results
obtained by our method correspond to those ob-
tained by human error analysis in (Vilar et al, 2006).
Additionally, it has been shown that the improve-
ments of the baseline system can be adequately mea-
sured as well.
This work is just a first step towards the devel-
opment of linguistically-informed evaluation mea-
sures which provide partial and more specific infor-
mation of certain translation problems. Such mea-
sures are very important to understand what are the
weaknesses of a statistical machine translation sys-
tem, and what are the best ways and methods for
improvements.
For our future work, we plan to extend the pro-
posed measures in order to carry out a more de-
tailed error analysis, for example examinating dif-
ferent types of inflection errors for Spanish verbs.
We also plan to investigate other types of translation
errors and other language pairs.
Acknowledgements
This work was partly supported by the TC-STAR
project by the European Community (FP6-506738)
and partly by the Generalitat de Catalunya and the
European Social Fund.
References
Bogdan Babych and Anthony Hartley. 2004. Extending
bleu mt evaluation method with frequency weighting.
In Proc. of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain, July.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for mt evaluation with improved
correlation with human judgements. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, pages 65?72,
Ann Arbor, MI, June.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2005. Improving statistical machine translation by
classifying and generalizing inflected verb forms. In
Proc. of the 9th European Conf. on Speech Commu-
nication and Technology (Interspeech), pages 3185?
3188, Lisbon, Portugal, September.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology, pages 128?132, San Diego.
Sharon Goldwater and David McClosky. 2005. Improv-
ing stastistical machine translation through morpho-
logical analysis. In Proc. of the Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
Vancouver, Canada, October.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In Proc. 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), Boston, MA, May.
5
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Hermann Ney. 2005. Evaluating machine transla-
tion output with automatic sentence segmentation. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 148?154, Pitts-
burgh, PA, October.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In COLING
?00: The 18th Int. Conf. on Computational Linguistics,
pages 1081?1085, Saarbru?cken, Germany, July.
Sonja Nie?en and Hermann Ney. 2001a. Morpho-
syntactic analysis for reordering in statistical machine
translation. In Proc. MT Summit VIII, pages 247?252,
Santiago de Compostela, Galicia, Spain, September.
Sonja Nie?en and Hermann Ney. 2001b. Toward hier-
archical models for statistical machine translation of
inflected languages. In Data-Driven Machine Trans-
lation Workshop, pages 47?54, Toulouse, France, July.
Sonja Nie?en, Franz J. Och, Gregor Leusch, and Her-
mann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research. In
Proc. Second Int. Conf. on Language Resources and
Evaluation (LREC), pages 39?45, Athens, Greece,
May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Maja Popovic? and Hermann Ney. 2004. Towards the use
of word stems & suffixes for statistical machine trans-
lation. In Proc. 4th Int. Conf. on Language Resources
and Evaluation (LREC), pages 1585?1588, Lissabon,
Portugal, May.
Maja Popovic? and Hermann Ney. 2006. POS-based
word reorderings for statistical machine translation. In
Proc. of the Fifth Int. Conf. on Language Resources
and Evaluation (LREC), Genova, Italy, May.
Maja Popovic?, David Vilar, Hermann Ney, Slobodan
Jovic?ic?, and Zoran ?Saric?. 2005. Augmenting a small
parallel text with morpho-syntactic language resources
for Serbian?English statistical machine translation. In
43rd Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Building and Using
Parallel Texts: Data-Driven Machine Translation and
Beyond, pages 41?48, Ann Arbor, MI, June.
David Vilar, Evgeny Matusov, Sas?a Hasan, Richard Zens,
and Hermann Ney. 2005. Statistical machine transla-
tion of european parliamentary speeches. In Proc. MT
Summit X, pages 259?266, Phuket, Thailand, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of statistical machine
translation output. In Proc. of the Fifth Int. Conf. on
Language Resources and Evaluation (LREC), page to
appear, Genova, Italy, May.
Richard Zens, Oliver Bender, Sas?a Hasan, Shahram
Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and
Hermann Ney. 2005. The RWTH phrase-based statis-
tical machine translation system. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 155?162, Pittsburgh, PA, October.
6
Proceedings of the Workshop on Statistical Machine Translation, pages 142?145,
New York City, June 2006. c?2006 Association for Computational Linguistics
TALP Phrase-based statistical translation system for European language
pairs
Marta R. Costa-jussa`
Patrik Lambert
Jose? B. Marin?o
Josep M. Crego
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,jmcrego,agispert,lambert,khalilov,canton,adrian, rbanchs)@gps.tsc.upc.edu
Adria` de Gispert
Rafael E. Banchs
Abstract
This paper reports translation results for
the ?Exploiting Parallel Texts for Statis-
tical Machine Translation? (HLT-NAACL
Workshop on Parallel Texts 2006). We
have studied different techniques to im-
prove the standard Phrase-Based transla-
tion system. Mainly we introduce two re-
ordering approaches and add morphologi-
cal information.
1 Introduction
Nowadays most Statistical Machine Translation
(SMT) systems use phrases as translation units. In
addition, the decision rule is commonly modelled
through a log-linear maximum entropy framework
which is based on several feature functions (in-
cluding the translation model), hm. Each feature
function models the probability that a sentence e in
the target language is a translation of a given sen-
tence f in the source language. The weights, ?i,
of each feature function are typically optimized to
maximize a scoring function. It has the advantage
that additional features functions can be easily in-
tegrated in the overall system.
This paper describes a Phrase-Based system
whose baseline is similar to the system in Costa-
jussa` and Fonollosa (2005). Here we introduce
two reordering approaches and add morphological
information. Translation results for all six trans-
lation directions proposed in the shared task are
presented and discussed. More specifically, four
different languages are considered: English (en),
Spanish (es), French (fr) and German (de); and
both translation directions are considered for the
pairs: EnEs, EnFr, and EnDe. The paper is orga-
nized as follows: Section 2 describes the system;
0This work has been supported by the European Union
under grant FP6-506738 (TC-STAR project) and the TALP
Research Center (under a TALP-UPC-Recerca grant).
Section 3 presents the shared task results; and, fi-
nally, in Section 4, we conclude.
2 System Description
This section describes the system procedure fol-
lowed for the data provided.
2.1 Alignment
Given a bilingual corpus, we use GIZA++ (Och,
2003) as word alignment core algorithm. During
word alignment, we use 50 classes per language
estimated by ?mkcls?, a freely-available tool along
with GIZA++. Before aligning we work with low-
ercase text (which leads to an Alignment Error
Rate reduction) and we recover truecase after the
alignment is done.
In addition, the alignment (in specific pairs of
languages) was improved using two strategies:
Full verb forms The morphology of the verbs
usually differs in each language. Therefore, it is
interesting to classify the verbs in order to address
the rich variety of verbal forms. Each verb is re-
duced into its base form and reduced POS tag as
explained in (de Gispert, 2005). This transforma-
tion is only done for the alignment, and its goal
is to simplify the work of the word alignment im-
proving its quality.
Block reordering (br) The difference in word
order between two languages is one of the most
significant sources of error in SMT. Related works
either deal with reordering in general as (Kanthak
et al, 2005) or deal with local reordering as (Till-
mann and Ney, 2003). We report a local reorder-
ing technique, which is implemented as a pre-
processing stage, with two applications: (1) to im-
prove only alignment quality, and (2) to improve
alignment quality and to infer reordering in trans-
lation. Here, we present a short explanation of the
algorithm, for further details see Costa-jussa` and
Fonollosa (2006).
142
Figure 1: Example of an Alignment Block, i.e. a
pair of consecutive blocks whose target translation
is swapped
This reordering strategy is intended to infer the
most probable reordering for sequences of words,
which are referred to as blocks, in order to mono-
tonize current data alignments and generalize re-
ordering for unseen pairs of blocks.
Given a word alignment, we identify those pairs
of consecutive source blocks whose translation is
swapped, i.e. those blocks which, if swapped,
generate a correct monotone translation. Figure 1
shows an example of these pairs (hereinafter called
Alignment Blocks).
Then, the list of Alignment Blocks (LAB) is
processed in order to decide whether two consec-
utive blocks have to be reordered or not. By using
the classification algorithm, see the Appendix, we
divide the LAB in groups (Gn, n = 1 . . . N ). In-
side the same group, we allow new internal com-
bination in order to generalize the reordering to
unseen pairs of blocks (i.e. new Alignment Blocks
are created). Based on this information, the source
side of the bilingual corpora are reordered.
In case of applying the reordering technique for
purpose (1), we modify only the source training
corpora to realign and then we recover the origi-
nal order of the training corpora. In case of using
Block Reordering for purpose (2), we modify all
the source corpora (both training and test), and we
use the new training corpora to realign and build
the final translation system.
2.2 Phrase Extraction
Given a sentence pair and a corresponding word
alignment, phrases are extracted following the cri-
terion in Och and Ney (2004). A phrase (or
bilingual phrase) is any pair of m source words
and n target words that satisfies two basic con-
straints: words are consecutive along both sides
of the bilingual phrase, and no word on either side
of the phrase is aligned to a word out of the phrase.
We limit the maximum size of any given phrase to
7. The huge increase in computational and storage
cost of including longer phrases does not provide
a significant improvement in quality (Koehn et al,
2003) as the probability of reappearance of larger
phrases decreases.
2.3 Feature functions
Conditional and posterior probability (cp, pp)
Given the collected phrase pairs, we estimate the
phrase translation probability distribution by rela-
tive frequency in both directions.
The target language model (lm) consists of an
n-gram model, in which the probability of a trans-
lation hypothesis is approximated by the product
of word n-gram probabilities. As default language
model feature, we use a standard word-based 5-
gram language model generated with Kneser-Ney
smoothing and interpolation of higher and lower
order n-grams (Stolcke, 2002).
The POS target language model (tpos) con-
sists of an N-gram language model estimated over
the same target-side of the training corpus but us-
ing POS tags instead of raw words.
The forward and backwards lexicon mod-
els (ibm1, ibm1?1) provide lexicon translation
probabilities for each phrase based on the word
IBM model 1 probabilities. For computing the
forward lexicon model, IBM model 1 probabili-
ties from GIZA++ source-to-target algnments are
used. In the case of the backwards lexicon model,
target-to-source alignments are used instead.
The word bonus model (wb) introduces a sen-
tence length bonus in order to compensate the sys-
tem preference for short output sentences.
The phrase bonus model (pb) introduces a con-
stant bonus per produced phrase.
2.4 Decoding
The search engine for this translation system is de-
scribed in Crego et al (2005) which takes into ac-
count the features described above.
Using reordering in the decoder (rgraph) A
highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
143
extend the monotone search graph with additional
arcs. See the details in Crego et al (2006).
2.5 Optimization
It is based on a simplex method (Nelder and
Mead, 1965). This algorithm adjusts the log-
linear weights in order to maximize a non-linear
combination of translation BLEU and NIST: 10 ?
log10((BLEU ? 100) + 1) + NIST. The max-
imization is done over the provided development
set for each of the six translation directions under
consideration. We have experimented an improve-
ment in the coherence between all the automatic
figures by integrating two of these figures in the
optimization function.
3 Shared Task Results
3.1 Data
The data provided for this shared task corresponds
to a subset of the official transcriptions of the
European Parliament Plenary Sessions, and it
is available through the shared task website at:
http://www.statmt.org/wmt06/shared-task/.
The development set used to tune the system
consists of a subset (500 first sentences) of the
official development set made available for the
Shared Task.
We carried out a morphological analysis of the
data. The English POS-tagging has been carried
out using freely available TNT tagger (Brants,
2000). In the Spanish case, we have used the
Freeling (Carreras et al, 2004) analysis tool
which generates the POS-tagging for each input
word.
3.2 Systems configurations
The baseline system is the same for all tasks and
includes the following features functions: cp, pp,
lm, ibm1, ibm1?1, wb, pb. The POStag target
language model has been used in those tasks for
which the tagger was available. Table 1 shows the
reordering configuration used for each task.
The Block Reordering (application 2) has been
used when the source language belongs to the Ro-
manic family. The length of the block is lim-
ited to 1 (i.e. it allows the swapping of single
words). The main reason is that specific errors are
solved in the tasks from a Romanic language to
a Germanic language (as the common reorder of
Noun + Adjective that turns into Adjective +
Noun). Although the Block Reordering approach
Task Reordering Configuration
Es2En br2
En2Es br1 + rgraph
Fr2En br2
En2Fr br1 + rgraph
De2En -
En2De -
Table 1: Additional reordering models for each
task: br1 (br2) stands for Block Reordering ap-
plication 1 (application 2); and rgraph refers to
the reordering integrated in the decoder
does not depend on the task, we have not done
the corresponding experiments to observe its ef-
ficiency in all the pairs used in this evaluation.
The rgraph has been applied in those cases
where: we do not use br2 (there is no sense in
applying them simultaneously); and we have the
tagger for the source language model available.
In the case of the pair GeEn, we have not exper-
imented any reordering, we left the application of
both reordering approaches as future work.
3.3 Discussion
Table 2 presents the BLEU scores evaluated on the
test set (using TRUECASE) for each configuration.
The official results were slightly better because a
lowercase evaluation was used, see (Koehn and
Monz, 2006).
For both, Es2En and Fr2En tasks, br helps
slightly. The improvement of the approach de-
pends on the quality of the alignment. The better
alignments allow to extract higher quality Align-
ment Blocks (Costa-jussa` and Fonollosa, 2006).
The En2Es task is improved when adding both
br1 and rgraph. Similarly, the En2Fr task seems to
perform fairly well when using the rgraph. In this
case, the improvement of the approach depends on
the quality of the alignment patterns (Crego et al,
2006). However, it has the advantage of delay-
ing the final decision of reordering to the overall
search, where all models are used to take a fully
informed decision.
Finally, the tpos does not help much when trans-
lating to English. It is not surprising because it was
used in order to improve the gender and number
agreement, and in English there is no need. How-
ever, in the direction to Spanish, the tpos added
to the corresponding reordering helps more as the
Spanish language has gender and number agree-
ment.
144
Task Baseline +tpos +rc +tpos+rc
Es2En 29.08 29.08 29.89 29.98
En2Es 27.73 27.66 28.79 28.99
Fr2En 27.05 27.06 27.43 27.23
En2Fr 26.16 - 27.80 -
De2En 21.59 21.33 - -
En2De 15.20 - - -
Table 2: Results evaluated using TRUECASE on
the test set for each conguration: rc stands for
Reordering Conguration and refers to Table 1.
The bold results were the congurations submit-
ted.
4 Conclusions
Reordering is important when using a Phrase-
Based system. Although local reordering is sup-
posed to be included in the phrase structure, per-
forming local reordering improves the translation
quality. In fact, local reordering, provided by the
reordering approaches, allows for those general-
izations which phrases could not achieve. Re-
ordering in the DeEn task is left as further work.
References
T. Brants. 2000. Tnt - a statistical part-of-speech tag-
ger. Proceedings of the Sixth Applied Natural Lan-
guage Processing.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyz-
ers. 4th Int. Conf. on Language Resources and Eval-
uation, LREC?04.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2005. Im-
proving the phrase-based statistical translation by
modifying phrase extraction and including new fea-
tures. Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts: Data-Driven Machine
Translation and Beyond.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2006. Using
reordering in statistical machine translation based on
alignment block classification. Internal Report.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005.
An Ngram-based statistical machine translation de-
coder. Proc. of the 9th Int. Conf. on Spoken Lan-
guage Processing, ICSLP?05.
J. M. Crego, A. de Gispert, P. Lambert, M. R.
Costa-jussa`, M. Khalilov, J. Marin?o, J. A. Fonol-
losa, and R. Banchs. 2006. Ngram-based smt
system enhanced with reordering patterns. HLT-
NAACL06 Workshop on Building and Using Paral-
lel Texts: Data-Driven Machine Translation and Be-
yond, June.
A. de Gispert. 2005. Phrase linguistic classification for
improving statistical machine translation. ACL 2005
Students Workshop, June.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H.
Ney. 2005. Novel reordering approaches in phrase-
based statistical machine translation. Proceedings
of the ACL Workshop on Building and Using Par-
allel Texts: Data-Driven Machine Translation and
Beyond, pages 167?174, June.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. Proc. of the Human Lan-
guage Technology Conference, HLT-NAACL?2003,
May.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449, December.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language mod-
eling toolkit. Proc. of the 7th Int. Conf. on Spoken
Language Processing, ICSLP?02, September.
C. Tillmann and H. Ney. 2003. Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):97?133, March.
A Appendix
Here we describe the classification algorithm used
in Section 1.
1. Initialization: set n? 1 and LAB ? ? LAB.
2. Main part: while LAB ? is not empty do
? Gn = {(?k, ?k)} where (?k, ?k) is any
element of LAB ?, i.e. ?k is the first
block and ?k is the second block of the
Alignment Block k of the LAB ?.
? Recursively, move elements (?i, ?i)
from LAB? to Gn if there is an element
(?j , ?j) ? Gn such that ?i = ?j or
?i = ?j
? Increase n (i.e. n? n + 1)
3. Ending: For each Gn, construct the two sets
An and Bn which consists on the first and
second element of the pairs in Gn, respec-
tively.
145
Proceedings of the Workshop on Statistical Machine Translation, pages 162?165,
New York City, June 2006. c?2006 Association for Computational Linguistics
N-gram-based SMT System Enhanced with Reordering Patterns
Josep M. Crego
Marta R. Costa-jussa`
Jose? B. Marin?o
Adria` de Gispert
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
{jmcrego,agispert,lambert,mruiz,khalilov,rbanchs,canton,adrian}@gps.tsc.upc.edu
Patrik Lambert
Rafael E. Banchs
Abstract
This work presents translation results for
the three data sets made available in the
shared task ?Exploiting Parallel Texts for
Statistical Machine Translation? of the
HLT-NAACL 2006 Workshop on Statisti-
cal Machine Translation. All results pre-
sented were generated by using the N-
gram-based statistical machine translation
system which has been enhanced from the
last year?s evaluation with a tagged target
language model (using Part-Of-Speech
tags). For both Spanish-English transla-
tion directions and the English-to-French
translation task, the baseline system al-
lows for linguistically motivated source-
side reorderings.
1 Introduction
The statistical machine translation approach used
in this work implements a log-linear combination
of feature functions along with a translation model
which is based on bilingual n-grams (de Gispert and
Marin?o, 2002).
This translation model differs from the well
known phrase-based translation approach (Koehn
et al, 2003) in two basic issues: first, training data
is monotonously segmented into bilingual units; and
second, the model considers n-gram probabilities in-
stead of relative frequencies. This translation ap-
proach is described in detail in (Marin?o et al, 2005).
For those translation tasks with Spanish or En-
glish as target language, an additional tagged (us-
ing POS information) target language model is used.
Additionally a reordering strategy that includes POS
information is described and evaluated.
Translation results for all six translation directions
proposed in the shared task are presented and dis-
cussed. Both translation directions are considered
for the pairs: English-Spanish, English-French,
and English-German.
The paper is structured as follows: Section 2
briefly outlines the baseline system. Section 3 de-
scribes in detail the implemented POS-based re-
ordering strategy. Section 4 presents and discusses
the shared task results and, finally, section 5 presents
some conclusions and further work.
2 Baseline N-gram-based SMT System
As already mentioned, the translation model used
here is based on bilingual n-grams. It actually con-
stitutes a language model of bilingual units, referred
to as tuples, which approximates the joint probabil-
ity between source and target languages by using
bilingual n-grams (de Gispert and Marin?o, 2002).
Tuples are extracted from a word-to-word aligned
corpus according to the following two constraints:
first, tuple extraction should produce a monotonic
segmentation of bilingual sentence pairs; and sec-
ond, no smaller tuples can be extracted without vi-
olating the previous constraint. See (Crego et al,
2004) for further details.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tu-
ples. In addition to this bilingual n-gram translation
model, the baseline system implements a log linear
combination of five feature functions.
162
These five additional models are:
? A target language model. 5-gram of the target
side of the bilingual corpus.
? A word bonus. Based on the number of tar-
get words in the partial-translation hypothesis,
to compensate the LM preference for short sen-
tences.
? A Source-to-target lexicon model. Based on
IBM Model 1 lexical parameters(Brown et al,
1993), providing a complementary probability
for each tuple in the translation table. These
parameters are obtained from source-to-target
alignments.
? A Target-to-source lexicon model. Analo-
gous to the previous feature, but obtained from
target-to-source alignments.
? A Tagged (POS) target language model. This
feature implements a 5-gram language model
of target POS-tags. In this case, each trans-
lation unit carried the information of its target
side POS-tags, though this is not used for trans-
lation model estimation (only in order to eval-
uate the target POS language model at decod-
ing time). Due to the non-availability of POS-
taggers for French and German, it was not pos-
sible to incorporate this feature in all transla-
tion tasks considered, being only used for those
translation tasks with Spanish and English as
target languages.
The search engine for this translation system is
described in (Crego et al, 2005) and implements
a beam-search strategy based on dynamic program-
ming, taking into account all feature functions de-
scribed above, along with the bilingual n-gram trans-
lation model. Monotone search is performed, in-
cluding histogram and threshold pruning and hy-
pothesis recombination.
An optimization tool, which is based on a down-
hill simplex method was developed and used for
computing log-linear weights for each of the feature
functions. This algorithm adjusts the weights so that
a non-linear combination of BLEU and NIST scores
is maximized over the development set for each of
the six translation directions considered.
This baseline system is actually very similar to
the system used for last year?s shared task ?Exploit-
ing Parallel Texts for Statistical Machine Transla-
tion? of ACL?05 Workshop on Building and Us-
ing Parallel Texts: Data-Driven Machine Translation
and Beyond (Banchs et al, 2005), whose results
are available at: http://www.statmt.org/wpt05/
mt-shared-task/. A more detailed description of
the system can be found in (2005).
The tools used for POS-tagging were Freel-
ing (Carreras et al, 2004) for Spanish and
TnT (Brants, 2000) for English. All language mod-
els were estimated using the SRI language mod-
eling toolkit. Word-to-word alignments were ex-
tracted with GIZA++. Improvements in word-to-
word alignments were achieved through verb group
classification as described in (de Gispert, 2005).
3 Reordering Framework
In this section we outline the reordering framework
used for the experiments (Crego and Marin?o, 2006).
A highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
extend the monotone search graph with additional
arcs.
To extract patterns, we use the word-to-word
alignments (the union of both alignment directions)
and source-side POS tags. The main procedure con-
sists of identifying all crossings produced in the
Figure 1: Reordering patterns are extracted using
word-to-word alignments. The generalization power
is achieved through the POS tags. Three instances of
different patterns are extracted using the sentences
in the example.
163
word-to-word alignments. Once a crossing has been
detected, its source POS tags and alignments are
used to account for a new instance of pattern. The
target side of a pattern (source-side positions after
reordering), is computed using the original order
of the target words to which the source words are
aligned. See figure 1 for a clarifying example of
pattern extraction.
The monotone search graph is extended with re-
orderings following the patterns found in training.
The procedure identifies first the sequences of words
in the input sentence that match any available pat-
tern. Then, each of the matchings implies the ad-
dition of an arc into the search graph (encoding the
reordering learnt in the pattern). However, this ad-
dition of a new arc is not performed if a translation
unit with the same source-side words already exists
in the training. Figure 2 shows an example of the
procedure.
Figure 2: Three additional arcs have been added
to the original monotone graph (bold arcs) given
the reordering patterns found matching any of the
source POS tags sequence.
Once the search graph is built, the decoder tra-
verses the graph looking for the best translation.
Hence, the winner hypothesis is computed using
all the available information (the whole SMT mod-
els). The reordering strategy is additionally sup-
ported by a 5-gram language model of reordered
source POS-tags. In training, POS-tags are re-
ordered according with the extracted reordering pat-
terns and word-to-word links. The resulting se-
quence of source POS-tags are used to train the n-
gram LM.
Notice that this reordering framework has only
been used for some translation tasks (Spanish-
to-English, English-to-Spanish and English-to-
French). The reason is double: first, because we
did not have available a French POS-tagger. Second,
because the technique used to learn reorderings (de-
tailed below) does not seem to apply for language
pairs like German-English, because the agglutina-
tive characteristic of German (words are formed by
joining morphemes together).
Table 1: BLEU, NIST and mWER scores (com-
puted using two reference translations) obtained for
both translation directions (Spanish-to-English and
English-to-Spanish).
Conf BLEU NIST mWER
Spanish-to-English
base 55.23 10.69 34.40
+rgraph 55.59 10.70 34.23
+pos 56.39 10.75 33.75
English-to-Spanish
base 48.03 9.84 41.18
+rgraph 48.53 9.81 41.15
+pos 48.91 9.91 40.29
Table 1 shows the improvement of the original
baseline system described in section 2 (base), en-
hanced using reordering graphs (+rgraph) and pro-
vided the tagged-source language model (+pos).
The experiments in table 1 were not carried out over
the official corpus of this shared task. The Spanish-
English corpus of the TC-Star 2005 Evaluation was
used. Due to the high similarities between both cor-
pus (this shared task corpus consists of a subset of
the whole corpus used in the TC-Star 2005 Evalua-
tion), it makes sense to think that comparable results
would be obtained.
It is worth mentioning that the official corpus of
the shared task (HLT-NAACL 2006) was used when
building and tuning the present shared task system.
4 Shared Task Results
The data provided for this shared task corresponds
to a subset of the official transcriptions of the Euro-
pean Parliament Plenary Sessions. The development
set used to tune the system consists of a subset (500
first sentences) of the official development set made
available for the Shared Task.
164
Table 2 presents the BLEU, NIST and mWER
scores obtained for the development-test data set.
The last column shows whether the target POS lan-
guage model feature was used or not. Computed
scores are case sensitive and compare to one refer-
ence translation. Tasks in bold were conducted al-
lowing for the reordering framework. For French-
to-English task, block reordering strategy was used,
which is described in (Costa-jussa` et al, 2006). As it
can be seen, for the English-to-German task we did
not use any of the previous enhancements.
Table 2: Translation results
Task BLEU NIST mWER tPOS
en ? es 29.50 7.32 58.95 yes
es ? en 30.29 7.51 57.72 yes
en ? fr 30.23 7.40 59.76 no
fr ? en 30.21 7.61 56.97 yes
en ? de 17.40 5.61 71.18 no
de ? en 23.78 6.70 65.83 yes
Important differences can be observed between
the German-English and the rest of translation tasks.
They result from the greater differences in word
order present in this language pair (the German-
English results are obtained under monotone decod-
ing conditions). Also because the greater vocabulary
of words of German, which increases sparseness in
any task where German is envolved. As expected,
differences in translation accuracy between Spanish-
English and French-English are smaller.
5 Conclusions and Further Work
As it can be concluded from the presented results,
although in principle some language pairs (Spanish-
English-French) seem to have very little need for re-
orderings (due to their similar word order), the use
of linguistically-based reorderings proves to be use-
ful to improve translation accuracy.
Additional work is to be conducted to allow for
reorderings when translating from/to German.
6 Acknowledgments
This work was partly funded by the European Union
under the integrated project TC-STAR1: Technology
and Corpora for Speech to Speech Translation (IST-
2002-FP6-506738) and the European Social Fund.
1http://www.tc-star.org
References
R. E. Banchs, J. M. Crego, A. de Gispert, P. Lambert, and
J. B. Marin?o. 2005. Statistical machine translation of
euparl data by using bilingual n-grams. Proc. of the
ACL Workshop on Building and Using Parallel Texts
(ACL?05/Wkshp), pages 67?72, June.
T. Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proc. of the Sixth Applied Natural Language
Processing (ANLP-2000), Seattle, WA.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263?311.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyzers.
4th Int. Conf. on Language Resources and Evaluation,
LREC?04, May.
M.R. Costa-jussa`, J.M. Crego, A. de Gispert, P. Lam-
bert, M. Khalilov, R. Banchs, J.B. Marin?o, and J.A.R.
Fonollosa. 2006. Talp phrase-based statistical transla-
tion system for european language pairs. Proc. of the
HLT/NAACL Workshop on Statistical Machine Trans-
lation, June.
J. M. Crego and J. Marin?o. 2006. A reordering frame-
work for statistical machine translation. Internal Re-
port.
J. M. Crego, J. Marin?o, and A. de Gispert. 2004. Finite-
state-based and phrase-based statistical machine trans-
lation. Proc. of the 8th Int. Conf. on Spoken Language
Processing, ICSLP?04, pages 37?40, October.
J. M. Crego, J. Marin?o, and A. Gispert. 2005. An ngram-
based statistical machine translation decoder. Proc. of
the 9th European Conference on Speech Communica-
tion and Technology, Interspeech?05, September.
A. de Gispert and J. Marin?o. 2002. Using X-grams
for speech-to-speech translation. Proc. of the 7th
Int. Conf. on Spoken Language Processing, ICSLP?02,
September.
A. de Gispert. 2005. Phrase linguistic classification and
generalization for improving statistical machine trans-
lation. Proc. of the ACL Student Research Workshop
(ACL?05/SRW), June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. Proc. of the Human
Language Technology Conference, HLT-NAACL?2003,
May.
J.B. Marin?o, R Banchs, J.M. Crego, A. de Gispert,
P. Lambert, M. R. Costa-jussa`, and J.A.R. Fonollosa.
2005. Bilingual n?gram statistical machine transla-
tion. Proc. of the MT Summit X, September.
165
Proceedings of the Second Workshop on Statistical Machine Translation, pages 167?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Ngram-based statistical machine translation enhanced with multiple
weighted reordering hypotheses
Marta R. Costa-jussa`, Josep M. Crego, Patrik Lambert, Maxim Khalilov
Jose? A. R. Fonollosa, Jose? B. Marin?o and Rafael E. Banchs
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,jmcrego,lambert,khalilov,adrian,canton,rbanchs)@gps.tsc.upc.edu
Abstract
This paper describes the 2007 Ngram-based sta-
tistical machine translation system developed at
the TALP Research Center of the UPC (Uni-
versitat Polite`cnica de Catalunya) in Barcelona.
Emphasis is put on improvements and extensions
of the previous years system, being highlighted
and empirically compared. Mainly, these include
a novel word ordering strategy based on: (1) sta-
tistically monotonizing the training source cor-
pus and (2) a novel reordering approach based
on weighted reordering graphs. In addition, this
system introduces a target language model based
on statistical classes, a feature for out-of-domain
units and an improved optimization procedure.
The paper provides details of this system par-
ticipation in the ACL 2007 SECOND WORK-
SHOP ON STATISTICAL MACHINE TRANSLA-
TION. Results on three pairs of languages are
reported, namely from Spanish, French and Ger-
man into English (and the other way round) for
both the in-domain and out-of-domain tasks.
1 Introduction
Based on estimating a joint-probability model between
the source and the target languages, Ngram-based SMT
has proved to be a very competitive alternatively to
phrase-based and other state-of-the-art systems in previ-
ous evaluation campaigns, as shown in (Koehn and Monz,
2005; Koehn and Monz, 2006).
Given the challenge of domain adaptation, efforts have
been focused on improving strategies for Ngram-based
SMT which could generalize better. Specifically, a novel
reordering strategy is explored. It is based on extending
the search by using precomputed statistical information.
Results are promising while keeping computational ex-
penses at a similar level as monotonic search. Addition-
ally, a bonus for tuples from the out-of-domain corpus is
introduced, as well as a target language model based on
statistical classes. One of the advantages of working with
statistical classes is that they can easily be used for any
pair of languages.
This paper is organized as follows. Section 2 briefly
reviews last year?s system, including tuple definition and
extraction, translation model and feature functions, de-
coding tool and optimization criterion. Section 3 delves
into the word ordering problem, by contrasting last year
strategy with the novel weighted reordering input graph.
Section 4 focuses on new features: both tuple-domain
bonus and target language model based on classes. Later
on, Section 5 reports on all experiments carried out for
WMT 2007. Finally, Section 6 sums up the main conclu-
sions from the paper and discusses future research lines.
2 Baseline N-gram-based SMT System
The translation model is based on bilingual n-grams. It
actually constitutes a language model of bilingual units,
referred to as tuples, which approximates the joint proba-
bility between source and target languages by using bilin-
gual n-grams.
Tuples are extracted from a word-to-word aligned cor-
pus according to the following two constraints: first, tu-
ple extraction should produce a monotonic segmentation
of bilingual sentence pairs; and second, no smaller tuples
can be extracted without violating the previous constraint.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tuples.
In addition to this bilingual n-gram translation model, the
baseline system implements a log linear combination of
four feature functions. These four additional models are:
a target language model (a 5-gram model of words);
a word bonus; a source-to-target lexicon model and a
target-to-source lexicon model, both features provide a
complementary probability for each tuple in the transla-
tion table.
The decoder (called MARIE) for this translation sys-
167
tem is based on a beam search 1.
This baseline system is actually the same system used
for the first shared task ?Exploiting Parallel Texts for Sta-
tistical Machine Translation? of the ACL 2005 Work-
shop on Building and Using Parallel Texts: Data-Driven
Machine Translation and Beyond. A more detailed de-
scription of the system can be found in (Marin?o et al,
2006).
3 Baseline System Enhanced with a
Weighted Reordering Input Graph
This section briefly describes the statistical machine re-
ordering (SMR) technique. Further details on the archi-
tecture of SMR system can be found on (Costa-jussa` and
Fonollosa, 2006).
3.1 Concept
The SMR system can be seen as a SMT system which
translates from an original source language (S) to a re-
ordered source language (S?), given a target language
(T). The SMR technique works with statistical word
classes (Och, 1999) instead of words themselves (partic-
ularly, we have used 200 classes in all experiments).
Figure 1: SMR approach in the (A) training step (B) in
the test step (the weight of each arch is in brackets).
3.2 Using SMR technique to improve SMT training
The original source corpus S is translated into the re-
ordered source corpus S? with the SMR system. Fig-
ure 1 (A) shows the corresponding block diagram. The
reordered training source corpus and the original training
target corpus are used to build the SMT system.
The main difference here is that the training is com-
puted with the S?2T task instead of the S2T original task.
Figure 2 (A) shows an example of the alignment com-
puted on the original training corpus. Figure 2 (B) shows
the same links but with the source training corpus in a
different order (this training corpus comes from the SMR
output). Although, the quality in alignment is the same,
the tuples that can be extracted change (notice that the
tuple extraction is monotonic). We are able to extract
1http://gps-tsc.upc.es/veu/soft/soft/marie/
smaller tuples which reduces the translation vocabulary
sparseness. These new tuples are used to build the SMT
system.
Figure 2: Alignment and tuple extraction (A) original
training source corpus (B) reordered training source cor-
pus.
3.3 Using SMR technique to generate multiple
weighted reordering hypotheses
The SMR system, having its own search, can generate ei-
ther an output 1-best or an output graph. In decoding, the
SMR technique generates an output graph which is used
as an input graph by the SMT system. Figure 1 (B) shows
the corresponding block diagram in decoding: the SMR
output graph is given as an input graph to the SMT sys-
tem. Hereinafter, this either SMR output graph or SMT
input graph will be referred to as (weighted) reordering
graph. The monotonic search in the SMT system is ex-
tended with reorderings following this reordering graph.
This reordering graph has multiple paths and each path
has its own weight. This weight is added as a feature
function in the log-linear framework. Figure 3 shows the
weighted reordering graph.
The main difference with the reordering technique for
WMT06 (Crego et al, 2006) lies in (1) the tuples are ex-
tracted from the word alignment between the reordered
source training corpus and the given target training cor-
pus and (2) the graph structure: the SMR graph provides
weights for each reordering path.
4 Other features and functionalities
In addition to the novel reordering strategy, we consider
two new features functions.
4.1 Target Language Model based on Statistical
Classes
This feature implements a 5-gram language model of tar-
get statistical classes (Och, 1999). This model is trained
by considering statistical classes, instead of words, for
168
Figure 3: Weighted reordering input graph for SMT sys-
tem.
the target side of the training corpus. Accordingly, the tu-
ple translation unit is redefined in terms of a triplet which
includes: a source string containing the source side of
the tuple, a target string containing the target side of the
tuple, and a class string containing the statistical classes
corresponding to the words in the target strings.
4.2 Bonus for out-of-domain tuples
This feature adds a bonus to those tuples which comes
from the training of the out-of-domain task. This feature
is added when optimizing with the development of the
out-of-domain task.
4.3 Optimization
Finally, a n-best re-ranking strategy is implemented
which is used for optimization purposes just as pro-
posed in http://www.statmt.org/jhuws/. This procedure
allows for a faster and more efficient adjustment of model
weights by means of a double-loop optimization, which
provides significant reduction of the number of transla-
tions that should be carried out. The current optimization
procedure uses the Simplex algorithm.
5 Shared Task Framework
5.1 Data
The data provided for this shared task corresponds to a
subset of the official transcriptions of the European Par-
liament Plenary Sessions 2. Additionally, there was avail-
able a smaller corpus called News-Commentary. For all
tasks and domains, our training corpus was the catenation
of both.
2http://www.statmt.org/wmt07/shared-task/
5.2 Processing details
Word Alignment. The word alignment is automati-
cally computed by using GIZA++ 3 in both directions,
which are symmetrized by using the union operation. In-
stead of aligning words themselves, stems are used for
aligning. Afterwards case sensitive words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al are splited into de el or a
el. As a post-processing, in the En2Es direction we used
a POS target language model as a feature (instead of the
target language model based on classes) that allowed to
recover the segmentations (de Gispert, 2006).
Language Model Interpolation. In other to better
adapt the system to the out-of-domain condition, the
target language model feature was built by combining
two 5-gram target language models (using SRILM 4).
One was trained from the EuroParl training data set, and
the other from the available, but much smaller, news-
commentary data set. The combination weights for the
EuroParl and news-commentary language models were
empirically adjusted by following a minimum perplexity
criterion. A relative perplexity reduction around 10-15%
respect to original EuroParl language model was achieved
in all the tasks.
5.3 Experiments and Results
The main difference between this year?s and last year?s
systems are: the amount of data provided; the word align-
ment; the Spanish morphology reduction; the reordering
technique; the extra target language model based on sta-
tistical classes (except for the En2Es); and the bonus for
the out-of-domain task (only for the En2Es task).
Among them, the most important is the reordering
technique. That is why we provide a fair comparison be-
tween the reordering patterns (Crego and Marin?o, 2006)
technique and the SMR reordering technique. Table 1
shows the system described above using either reorder-
ing patterns or the SMR technique. The BLEU calcula-
tion was case insensitive and sensitive to tokenization.
Table 2 presents the BLEU score obtained for the 2006
test data set comparing last year?s and this year?s systems.
The computed BLEU scores are case insensitive, sensi-
tive to tokenization and uses one translation reference.
The improvement in BLEU results shown from UPC-jm
3http://www.fjoch.com/GIZA++.html
4http://www.speech.sri.com/projects/srilm/
169
Task Reordering patterns SMR technique
es2en 31.21 33.34
en2es 31.67 32.33
Table 1: BLEU comparison: reordering patterns vs. SMR
technique.
Task UPC-jm 2006 UPC 2007
in-d out-d in-d out-d
es2en 31.01 27.92 33.34 32.85
en2es 30.44 25.59 32.33 33.07
fr2en 30.42 21.79 32.44 26.93
en2fr 31.75 23.30 32.30 27.03
de2en 24.43 17.57 26.54 21.63
en2de 17.73 10.96 19.74 15.06
Table 2: BLEU scores for each of the six translation di-
rections considered (computed over 2006 test set) com-
paring last year?s and this year?s system results (in-
domain and out-domain).
2006 Table 2 and reordering patterns Table 1 in the En-
glish/Spanish in-domain task comes from the combina-
tion of: the additional corpora, the word alignment, the
Spanish morphology reduction and the extra target lan-
guage model based on classes (only in the Es2En direc-
tion).
6 Conclusions and Further Work
This paper describes the UPC system for the WMT07
Evaluation. In the framework of Ngram-based system, a
novel reordering strategy which can be used for any pair
of languages has been presented and it has been showed
to significantly improve translation performance. Ad-
ditionally two features has been added to the log-lineal
scheme: the target language model based on classes and
the bonus for out-of-domain translation units.
7 Acknowledgments
This work has been funded by the European Union un-
der the TC-STAR project (IST-2002-FP6-506738) and
the Spanish Government under grant TEC2006-13964-
C03 (AVIVAVOZ project).
References
M.R. Costa-jussa` and J.A.R. Fonollosa. 2006. Statistical
machine reordering. In EMNLP, pages 71?77, Sydney,
July. ACL.
J.M. Crego and J.B. Marin?o. 2006. Reordering experi-
ments for n-gram-based smt. In SLT, pages 242?245,
Aruba.
Josep M. Crego, Adria` de Gispert, Patrik Lambert,
Marta R. Costa-jussa`, Maxim Khalilov, Rafael Banchs,
Jose? B. Marin?o, and Jose? A. R. Fonollosa. 2006. N-
gram-based smt system enhanced with reordering pat-
terns. In WMT, pages 162?165, New York City, June.
ACL.
Adria` de Gispert. 2006. Introducing Linguistic Knowl-
edge in Statistical Machine Translation. Ph.D. thesis,
Universitat Polite`cnica de Catalunya, December.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between european lan-
guages. In WMT, pages 119?124, Michigan, June.
ACL.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In WMT, pages 102?121, New
York City, June. ACL.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram based machine translation. Computa-
tional Linguistics, 32(4):527?549, December.
F.J. Och. 1999. An efficient method for determin-
ing bilingual word classes. In EACL, pages 71?76,
Bergen, Norway, June.
170
Proceedings of the Third Workshop on Statistical Machine Translation, pages 127?130,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The TALP-UPC Ngram-based statistical machine translation system for
ACL-WMT 2008
Maxim Khalilov, Adolfo Hern?ndez H., Marta R. Costa-juss?,
Josep M. Crego, Carlos A. Henr?quez Q., Patrik Lambert,
Jos? A. R. Fonollosa, Jos? B. Mari?o and Rafael E. Banchs
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(khalilov, adolfohh, mruiz, jmcrego, carloshq, lambert, adrian, canton, rbanchs)@gps.tsc.upc.edu
Abstract
This paper reports on the participation of the TALP
Research Center of the UPC (Universitat Polit?cnica
de Catalunya) to the ACL WMT 2008 evaluation
campaign.
This year?s system is the evolution of the one we em-
ployed for the 2007 campaign. Main updates and
extensions involve linguistically motivated word re-
ordering based on the reordering patterns technique.
In addition, this system introduces a target language
model, based on linguistic classes (Part-of-Speech),
morphology reduction for an inflectional language
(Spanish) and an improved optimization procedure.
Results obtained over the development and test sets
on Spanish to English (and the other way round)
translations for both the traditional Europarl and
a challenging News stories tasks are analyzed and
commented.
1 Introduction
Over the past few years, the Statistical Machine Transla-
tion (SMT) group of the TALP-UPC has been develop-
ing the Ngram-based SMT system (Mari?o et al, 2006).
In previous evaluation campaigns the Ngram-based ap-
proach has proved to be comparable with the state-of-
the-art phrase-based systems, as shown in Koehn and
Monz(2006), Callison-Burch et al (2007).
We present a summary of the TALP-UPC Ngram-
based SMT system used for this shared task. We dis-
cuss the system configuration and novel features, namely
linguistically motivated reordering technique, which is
applied on the decoding step. Additionally, the reorder-
ing procedure is supported by an Ngram language model
(LM) of reordered source Part-of-Speech tags (POS).
In this year?s evaluation we submitted systems for
Spanish-English and English-Spanish language pairs for
the traditional (Europarl) and challenging (News) tasks.
In each case, we used only the supplied data for each lan-
guage pair for models training and optimization.
This paper is organized as follows. Section 2 briefly
outlines the 2008 system, including tuple definition and
extraction, translation model and additional feature mod-
els, decoding tool and optimization procedure. Section 3
describes the word reordering problem and presents the
proposed technique of reordering patterns learning and
application. Later on, Section 4 reports on the experi-
mental setups of the WMT 2008 evaluation campaign. In
Section 5 we sum up the main conclusions from the pa-
per.
2 Ngram-based SMT System
Our translation system implements a log-linear model in
which a foreign language sentence fJ1 = f1, f2, ..., fJ
is translated into another language eI1 = f1, f2, ..., eI by
searching for the translation hypothesis e?I1 maximizing a
log-linear combination of several feature models (Brown
et al, 1990):
e?I1 = argmax
eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
where the feature functions hm refer to the system models
and the set of ?m refers to the weights corresponding to
these models.
The core part of the system constructed in that way
is a translation model, which is based on bilingual n-
grams. It actually constitutes an Ngram-based LM of
bilingual units (called tuples), which approximates the
joint probability between the languages under consider-
ation. The procedure of tuples extraction from a word-
to-word alignment according to certain constraints is ex-
plained in detail in Mari?o et al (2006).
The Ngram-based approach differs from the phrase-
based SMT mainly by distinct representating of the bilin-
gual units defined by word alignment and using a higher
127
order HMM of the translation process. While regular
phrase-based SMT considers context only for phrase re-
ordering but not for translation, the N-gram based ap-
proach conditions translation decisions on previous trans-
lation decisions.
The TALP-UPC 2008 translation system, besides the
bilingual translation model, which consists of a 4-gram
LM of tuples with Kneser-Ney discounting (estimated
with SRI Language Modeling Toolkit1), implements a
log-linear combination of five additional feature models:
? a target language model (a 4-gram model of words,
estimated with Kneser-Ney smoothing);
? a POS target language model (a 4-gram model of
tags with Good-Turing discounting (TPOS));
? a word bonus model, which is used to compensate
the system?s preference for short output sentences;
? a source-to-target lexicon model and a target-to-
source lexicon model, these models use word-to-
word IBM Model 1 probabilities (Och and Ney,
2004) to estimate the lexical weights for each tuple
in the translation table.
Decisions on the particular LM configuration and
smoothing technique were taken on the minimal-
perplexity and maximal-BLEU bases.
The decoder (called MARIE), an open source tool2,
implementing a beam search strategy with distortion ca-
pabilities was used in the translation system.
Given the development set and references, the log-
linear combination of weights was adjusted using a sim-
plex optimization method (with the optimization criteria
of the highest BLEU score ) and an n-best re-ranking
just as described in http://www.statmt.org/jhuws/. This
strategy allows for a faster and more efficient adjustment
of model weights by means of a double-loop optimiza-
tion, which provides significant reduction of the number
of translations that should be carried out.
3 Reordering framework
For a great number of translation tasks a certain reorder-
ing strategy is required. This is especially important
when the translation is performed between pairs of lan-
guages with non-monotonic word order. There are var-
ious types of distortion models, simplifying bilingual
translation. In our system we use an extended monotone
reordering model based on automatically learned reorder-
ing rules. A detailed description can be found in Crego
and Mari?o (2006).
1http://www.speech.sri.com/projects/srilm/
2http://gps-tsc.upc.es/veu/soft/soft/marie/
Apart from that, tuples were extracted by an unfold-
ing technique: this means that the tuples are broken into
smaller tuples, and these are sequenced in the order of the
target words.
3.1 Reordering patterns
Word movements are realized according to the reordering
rewrite rules, which have the form of:
t1, ..., tn 7? i1, ..., in
where t1, ..., tn is a sequence of POS tags (relating a
sequence of source words), and i1, ..., in indicates which
order of the source words generate monotonically the tar-
get words.
Patterns are extracted in training from the crossed links
found in the word alignment, in other words, found in
translation tuples (as no word within a tuple can be linked
to a word out of it (Crego and Mari?o, 2006)).
Having all the instances of rewrite patterns, a score for
each pattern on the basis of relative frequency is calcu-
lated as shown below:
p(t1, ..., tn 7? i1, ..., in) =
N(t1, ..., tn 7? i1, ..., in)
NN(t1, ..., tn)
3.2 Search graph extension and source POS model
The monotone search graph is extended with reorderings
following the patterns found in training. Once the search
graph is built, the decoder traverses the graph looking for
the best translation. Hence, the winning hypothesis is
computed using all the available information (the whole
SMT models).
Figure 1: Search graph extension. NC, CC and AQ stand re-
spectively for name, conjunction and adjective.
The procedure identifies first the sequences of words
in the input sentence that match any available pattern.
Then, each of the matchings implies the addition of an arc
into the search graph (encoding the reordering learned in
the pattern). However, this addition of a new arc is not
128
Task BL BL+SPOS
Europarl News Europarl News
es2en 32.79 36.09 32.88 36.36
en2es 32.05 33.91 32.10 33.63
Table 1: BLEU comparison demonstrating the impact of the
source-side POS tags model.
performed if a translation unit with the same source-side
words already exists in the training. Figure 1 shows how
two rewrite rules applied over an input sentence extend
the search graph given the reordering patterns that match
the source POS tag sequence.
The reordering strategy is additionally supported by
a 4-gram language model (estimated with Good-Turing
smoothing) of reordered source POS tags (SPOS). In
training, POS tags are reordered according with the ex-
tracted reordering patterns and word-to-word links. The
resulting sequence of source POS tags is used to train the
Ngram LM.
Table 1 presents the effect of the source POS LM in-
troduction to the reordering module of the Ngram-based
SMT. As it can be seen, the impactya le h of the source-
side POS LM is minimal, however we decided to consider
the model aiming at improving it in future. The reported
results are related to the Europarl and News Commen-
tary (News) development sets. BLEU calculation is case
insensitive and insensitive to tokenization. BL (baseline)
refers to the presented Ngram-based system considering
all the features, apart from the target and source POS
models.
4 WMT 2008 Evaluation Framework
4.1 Corpus
An extraction of the official transcriptions of the 3rd re-
lease of the European Parliament Plenary Sessions3 was
provided for the ACL WMT 2008 shared translation task.
About 40 times smaller corpus from news domain (called
News Commentary) was also available. For both tasks,
our training corpus was the catenation of the Europarl and
News Commentary corpora.
TALP UPC participated in the constraint to the
provided training data track for Spanish-English and
English-Spanish translation tasks. We used the same
training material for the traditional and challenging tasks,
while the development sets used to tune the system were
distinct (2000 sentences for Europarl task and 1057
for News Commentary, one reference translation for
each of them). A brief training and development corpora
statistics is presented in Table 2.
3http://www.statmt.org/wmt08/shared-task.html
Spanish English
Train
Sentences 1.3 M 1.3 M
Words 38.2 M 35.8 K
Vocabulary 156 K 120 K
Development Europarl
Sentences 2000 2000
Words 61.8 K 58.7 K
Vocabulary 8 K 6.5 K
Development News Commentary
Sentences 1057 1057
Words 29.8 K 25.8 K
Vocabulary 5.4 K 4.9 K
Table 2: Basic statistics of ACL WMT 2008 corpus.
4.2 Processing details
The training data was preprocessed by using provided
tools for tokenizing and filtering.
POS tagging. POS information for the source and the
target languages was considered for both translation tasks
that we have participated. The software tools available
for performing POS-tagging were Freeling (Carreras et
al., 2004) for Spanish and TnT (Brants, 2000) for En-
glish. The number of classes for English is 44, while
Spanish is considered as a more inflectional language,
and the tag set contains 376 different tags.
Word Alignment. The word alignment is automati-
cally computed by using GIZA++4(Och and Ney, 2000)
in both directions, which are symmetrized by using the
union operation. Instead of aligning words themselves,
stems are used for aligning. Afterwards case sensitive
words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al were splitted into de el or
a el. As a post-processing, in the En2Es direction we
used a POS target LM as a feature (instead of the target
language model based on classes) that allowed to recover
the segmentations (de Gispert, 2006).
4.3 Experiments and Results
In contrast to the last year?s system where statistical
classes were used to train the target-side tags LM, this
year we used linguistically motivated word classes
4http://code.google.com/p/giza-pp/
129
Task BL+SPOS BL+SPOS+TPOS
(UPC 2008)
Europarl News Europarl News
es2en 32.88 36.36 32.89 36.31
en2es 31.52 34.13 30.72 32.72
en2es "clean"5 32.10 33.63 32.09 35.04
Table 3: BLEU scores for Spanish-English and English-Spanish
2008 development corpora (Europarl and News Commentary).
Task UPC 2008
Europarl News
es2en 32.80 19.61
en2es 31.31 19.28
en2es "clean"5 32.34 20.05
Table 4: BLEU scores for official tests 2008.
(POS) which were considered to train the POS target LM
and extract the reordering patterns. Other characteristics
of this year?s system are:
? reordering patterns technique;
? source POS model, supporting word reordering;
? no LM interpolation. For this year?s evaluation, we
trained two separate LMs for each domain-specific
corpus (i.e., Europarl and News Commentary tasks).
It is important to mention that 2008 training material is
identical to the one provided for the 2007 shared transla-
tion task.
Table 3 presents the BLEU score obtained for the 2008
development data sets and shows the impact of the target-
side POS LM introduction, which can be characterized as
highly corpus- and language-dependent feature. BL refers
to the same system configuration as described in subsec-
tion 3.2. The computed BLEU scores are case insensitive,
insensitive to tokenization and use one translation refer-
ence.
After submitting the systems we discovered a bug re-
lated to incorrect implementation of the target LMs of
words and tags for Spanish, it caused serious reduction
of translation quality (1.4 BLEU points for development
set in case of English-to-Spanish Europarl task and 2.3
points in case of the corresponding News Commentary
task). The last raw of table 3 (en2es "clean") repre-
sents the results corresponding to the UPC 2008 post-
evaluation system, while the previous one (en2es) refers
to the "bugged" system submitted to the evaluation.
The experiments presented in Table 4 correspond to the
2008 test evaluation sets.
5Corrected post-evaluation results (see subsection 4.3.)
5 Conclusions
In this paper we introduced the TALP UPC Ngram-based
SMT system participating in the WMT08 evaluation.
Apart from briefly summarizing the decoding and opti-
mization processes, we have presented the feature mod-
els that were taken into account, along with the bilingual
Ngram translation model. A reordering strategy based on
linguistically-motivated reordering patterns to harmonize
the source and target word order has been presented in
the framework of the Ngram-based system.
6 Acknowledgments
This work has been funded by the Spanish Government
under grant TEC2006-13964-C03 (AVIVAVOZ project).
The authors want to thank Adri? de Gispert (Cambridge
University) for his contribution to this work.
References
T. Brants. 2000. TnT ? a statistical part-of-speech tagger. In
Proceedings of the 6th Applied Natural Language Processing
(ANLP-2000).
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek,
J. D. Lafferty, R. Mercer, and P. S. Roossin. 1990. A sta-
tistical approach to machine translation. Computational Lin-
guistics, 16(2):79?85.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine trans-
lation. In Proceedings of the ACL 2007 Workshop on Statis-
tical and Hybrid methods for Machine Translation (WMT),
pages 136?158.
X. Carreras, I. Chao, L. Padr?, and M. Padr?. 2004. Freeling:
An open-source suite of language analyzers. In Proceedings
of the 4th Int. Conf. on Language Resources and Evaluation
(LREC?04).
J. M. Crego and J. B. Mari?o. 2006. Improving statistical MT
by coupling reordering and decoding. Machine Translation,
20(3):199?215.
A. de Gispert. 2006. Introducing linguistic knowledge into
statistical machine translation. Ph.D. thesis, Universitat
Polit?cnica de Catalunya, December.
P. Koehn and C. Monz. 2006. Manual and automatic eval-
uation of machine translation between european languages.
In Proceedings of the ACL 2006 Workshop on Statistical and
Hybrid methods for Machine Translation (WMT), pages 102?
121.
J. B. Mari?o, R. E. Banchs, J. M. Crego, A. de Gispert, P. Lam-
bert, J. A. R. Fonollosa, and M. R. Costa-juss?. 2006. N-
gram based machine translation. Computational Linguistics,
32(4):527?549, December.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of the the 38th Annual Meeting
on Association for Computational Linguistics (ACL), pages
440?447.
F. Och and H. Ney. 2004. The alignment template approach to
statistical machine translation. 30(4):417 ? 449, December.
130
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 11?15,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Collaborative Machine Translation Service for Scientific texts
Patrik Lambert
University of Le Mans
patrik.lambert@lium.univ-lemans.fr
Jean Senellart
Systran SA
senellart@systran.fr
Laurent Romary
Humboldt Universita?t Berlin /
INRIA Saclay - Ile de France
laurent.romary@inria.fr
Holger Schwenk
University of Le Mans
holger.schwenk@lium.univ-lemans.fr
Florian Zipser
Humboldt Universita?t Berlin
f.zipser@gmx.de
Patrice Lopez
Humboldt Universita?t Berlin /
INRIA Saclay - Ile de France
patrice.lopez@inria.fr
Fre?de?ric Blain
Systran SA /
University of Le Mans
frederic.blain@
lium.univ-lemans.fr
Abstract
French researchers are required to fre-
quently translate into French the descrip-
tion of their work published in English. At
the same time, the need for French people
to access articles in English, or to interna-
tional researchers to access theses or pa-
pers in French, is incorrectly resolved via
the use of generic translation tools. We
propose the demonstration of an end-to-end
tool integrated in the HAL open archive for
enabling efficient translation for scientific
texts. This tool can give translation sugges-
tions adapted to the scientific domain, im-
proving by more than 10 points the BLEU
score of a generic system. It also provides
a post-edition service which captures user
post-editing data that can be used to incre-
mentally improve the translations engines.
Thus it is helpful for users which need to
translate or to access scientific texts.
1 Introduction
Due to the globalisation of research, the English
language is today the universal language of sci-
entific communication. In France, regulations re-
quire the use of the French language in progress
reports, academic dissertations, manuscripts, and
French is the official educational language of the
country. This situation forces researchers to fre-
quently translate their own articles, lectures, pre-
sentations, reports, and abstracts between English
and French. In addition, students and the general
public are also challenged by language, when it
comes to find published articles in English or to
understand these articles. Finally, international
scientists not even consider to look for French
publications (for instance PhD theses) because
they are not available in their native languages.
This problem, incorrectly resolved through the
use of generic translation tools, actually reveals
an interesting generic problem where a commu-
nity of specialists are regularly performing trans-
lations tasks on a very limited domain. At the
same time, other communities of users seek trans-
lations for the same type of documents. Without
appropriate tools, the expertise and time spent for
translation activity by the first community is lost
and do not benefit to translation requests of the
other communities.
We propose the demonstration of an end-to-end
tool for enabling efficient translation for scientific
texts. This system, developed for the COSMAT
ANR project,1 is closely integrated into the HAL
open archive,2 a multidisciplinary open-access
archive which was created in 2006 to archive pub-
lications from all the French scientific commu-
nity. The tool deals with handling of source doc-
ument format, generally a pdf file, specialised
translation of the content, and user-friendly user-
interface allowing to post-edit the output. Behind
1http://www.cosmat.fr/
2http://hal.archives-ouvertes.fr/?langue=en
11
the scene, the post-editing tool captures user post-
editing data which are used to incrementally im-
prove the translations engines. The only equip-
ment required by this demonstration is a computer
with an Internet browser installed and an Internet
connection.
In this paper, we first describe the complete
work-flow from data acquisition to final post-
editing. Then we focus on the text extraction pro-
cedure. In Section 4, we give details about the
translation system. Then in section 5, we present
the translation and post-editing interface. We fi-
nally give some concluding remarks.
The system will be demonstrated at EACL in
his tight integration with the HAL paper deposit
system. If the organizers agree, we would like to
offer the use of our system during the EACL con-
ference. It would automatically translate all the
abstracts of the accepted papers and also offers
the possibility to correct the outputs. This result-
ing data would be made freely available.
2 Complete Processing Work-flow
The entry point for the system are ?ready to pub-
lish? scientific papers. The goal of our system
was to extract content keeping as many meta-
information as possible from the document, to
translate the content, to allow the user to perform
post-editing, and to render the result in a format as
close as possible to the source format. To train our
system, we collected from the HAL archive more
than 40 000 documents in physics and computer
science, including articles, PhD theses or research
reports (see Section 4). This material was used to
train the translation engines and to extract domain
bilingual terminology.
The user scenario is the following:
? A user uploads an article in PDF format3 on
the system.
? The document is processed by the open-
source Grobid tool (see section 3) to extract
3The commonly used publishing format is PDF files
while authoring format is principally a mix of Microsoft
Word file and LaTeX documents using a variety of styles.
The originality of our approach is to work on the PDF file
and not on these source formats. The rationale being that 1/
the source format is almost never available, 2/ even if we had
access to the source format, we would need to implement a
filter specific to each individual template required by such or
such conference for a good quality content extraction
the content. The extracted paper is structured
in the TEI format where title, authors, refer-
ences, footnotes, figure captions are identi-
fied with a very high accuracy.
? An entity recognition process is performed
for markup of domain entities such as:
chemical compounds for chemical papers,
mathematical formulas, pseudo-code and ob-
ject references in computer science papers,
but also miscellaneous acronyms commonly
used in scientific communication.
? Specialised terminology is then recognised
using the Termsciences4 reference termi-
nology database, completed with terminol-
ogy automatically extracted from the train-
ing corpus. The actual translation of the pa-
per is performed using adapted translation as
described in Section 4.
? The translation process generates a bilingual
TEI format preserving the source structure
and integrating the entity annotation, multi-
ple terminology choices when available, and
the token alignment between source and tar-
get sentences.
? The translation is proposed to the user for
post-editing through a rich interactive inter-
face described in Section 5.
? The final version of the document is then
archived in TEI format and available for dis-
play in HTML using dedicated XSLT style
sheets.
3 The Grobid System
Based on state-of-the-art machine learning tech-
niques, Grobid (Lopez, 2009) performs reliable
bibliographic data extraction from scholar articles
combined with multi-level term extraction. These
two types of extraction present synergies and cor-
respond to complementary descriptions of an arti-
cle.
This tool parses and converts scientific arti-
cles in PDF format into a structured TEI docu-
ment5 compliant with the good practices devel-
oped within the European PEER project (Bretel et
al., 2010). Grobid is trained on a set of annotated
4http://www.termsciences.fr
5http://www.tei-c.org
12
scientific article and can be re-trained to fit tem-
plates used for a specific conference or to extract
additional fields.
4 Translation of Scientific Texts
The translation system used is a Hybrid Machine
Translation (HMT) system from French to En-
glish and from English to French, adapted to
translate scientific texts in several domains (so
far physics and computer science). This sys-
tem is composed of a statistical engine, cou-
pled with rule-based modules to translate spe-
cial parts of the text such as mathematical for-
mulas, chemical compounds, pseudo-code, and
enriched with domain bilingual terminology (see
Section 2). Large amounts of monolingual and
parallel data are available to train a SMT system
between French and English, but not in the scien-
tific domain. In order to improve the performance
of our translation system in this task, we extracted
in-domain monolingual and parallel data from the
HAL archive. All the PDF files deposited in HAL
in computer science and physics were made avail-
able to us. These files were then converted to
plain text using the Grobid tool, as described in
the previous section. We extracted text from all
the documents from HAL that were made avail-
able to us to train our language model. We built
a small parallel corpus from the abstracts of the
PhD theses from French universities, which must
include both an abstract in French and in English.
Table 1 presents statistics of these in-domain data.
The data extracted from HAL were used to
adapt a generic system to the scientific litera-
ture domain. The generic system was mostly
trained on data provided for the shared task of
Sixth Workshop on Statistical Machine Transla-
tion6 (WMT 2011), described in Table 2.
Table 3 presents results showing, in the
English?French direction, the impact on the sta-
tistical engine of introducing the resources ex-
tracted from HAL, as well as the impact of do-
main adaptation techniques. The baseline statis-
tical engine is a standard PBSMT system based
on Moses (Koehn et al 2007) and the SRILM
tookit (Stolcke, 2002). Is was trained and tuned
only on WMT11 data (out-of-domain). Incorpo-
rating the HAL data into the language model and
tuning the system on the HAL development set,
6http://www.statmt.org/wmt11/translation-task.html
Set Domain Lg Sent. Words Vocab.
Parallel data
Train cs+phys En 55.9 k 1.41 M 43.3 k
Fr 55.9 k 1.63 M 47.9 k
Dev cs En 1100 25.8 k 4.6 k
Fr 1100 28.7 k 5.1 k
phys En 1000 26.1 k 5.1 k
Fr 1000 29.1 k 5.6 k
Test cs En 1100 26.1 k 4.6 k
Fr 1100 29.2 k 5.2 k
phys En 1000 25.9 k 5.1 k
Fr 1000 28.8 k 5.5 k
Monolingual data
Train cs En 2.5 M 54 M 457 k
Fr 761 k 19 M 274 k
phys En 2.1 M 50 M 646 k
Fr 662 k 17 M 292 k
Table 1: Statistics for the parallel training, develop-
ment, and test data sets extracted from thesis abstracts
contained in HAL, as well as monolingual data ex-
tracted from all documents in HAL, in computer sci-
ence (cs) and physics (phys). The following statistics
are given for the English (En) and French (Fr) sides
(Lg) of the corpus: the number of sentences, the num-
ber of running words (after tokenisation) and the num-
ber of words in the vocabulary (M and k stand for mil-
lions and thousands, respectively).
yielded a gain of more than 7 BLEU points, in
both domains (computer science and physics). In-
cluding the theses abstracts in the parallel training
corpus, a further gain of 2.3 BLEU points is ob-
served for computer science, and 3.1 points for
physics. The last experiment performed aims at
increasing the amount of in-domain parallel texts
by translating automatically in-domain monolin-
gual data, as suggested by Schwenk (2008). The
synthesised bitext does not bring new words into
the system, but increases the probability of in-
domain bilingual phrases. By adding a synthetic
bitext of 12 million words to the parallel training
data, we observed a gain of 0.5 BLEU point for
computer science, and 0.7 points for physics.
Although not shown here, similar results were
obtained in the French?English direction. The
French?English system is actually slightly bet-
ter than the English?French one as it is an easier
translation direction.
13
Translation Model Language Model Tuning Domain CS PHYS
words (M) Bleu words (M) Bleu
wmt11 wmt11 wmt11 371 27.3 371 27.1
wmt11 wmt11+hal hal 371 36.0 371 36.2
wmt11+hal wmt11+hal hal 287 38.3 287 39.3
wmt11+hal+adapted wmt11+hal hal 299 38.8 307 40.0
Table 3: Results (BLEU score) for the English?French systems. The type of parallel data used to train the
translation model or language model are indicated, as well as the set (in-domain or out-of-domain) used to tune
the models. Finally, the number of words in the parallel corpus and the BLEU score on the in-domain test set are
indicated for each domain: computer science and physics.
Figure 1: Translation and post-editing interface.
Corpus English French
Bitexts:
Europarl 50.5M 54.4M
News Commentary 2.9M 3.3M
Crawled (109 bitexts) 667M 794M
Development data:
newstest2009 65k 73k
newstest2010 62k 71k
Monolingual data:
LDC Gigaword 4.1G 920M
Crawled news 2.6G 612M
Table 2: Out-of-domain development and training data
used (number of words after tokenisation).
5 Post-editing Interface
The collaborative aspect of the demonstrated ma-
chine translation service is based on a post-editing
tool, whose interface is shown in Figure 1. This
tool provides the following features:
? WYSIWYG display of the source and target
texts (Zones 1+2)
? Alignment at the sentence level (Zone 3)
? Zone to review the translation with align-
ment of source and target terms (Zone 4) and
terminology reference (Zone 5)
? Alternative translations (Zone 6)
The tool allows the user to perform sentence
level post-editing and records details of post-
editing activity, such as keystrokes, terminology
selection, actual edits and time log for the com-
plete action.
6 Conclusions and Perspectives
We proposed the demonstration of an end-to-end
tool integrated into the HAL archive and enabling
14
efficient translation for scientific texts. This tool
consists of a high-accuracy PDF extractor, a hy-
brid machine translation engine adapted to the sci-
entific domain and a post-edition tool. Thanks to
in-domain data collected from HAL, the statisti-
cal engine was improved by more than 10 BLEU
points with respect to a generic system trained on
WMT11 data.
Our system was deployed for a physic confer-
ence organised in Paris in Sept 2011. All accepted
abstracts were translated into author?s native lan-
guages (around 70% of them) and proposed for
post-editing. The experience was promoted by
the organisation committee and 50 scientists vol-
unteered (34 finally performed their post-editing).
The same experience will be proposed for authors
of the LREC conference. We would like to offer
a complete demonstration of the system at EACL.
The goal of these experiences is to collect and dis-
tribute detailed ?post-editing? data for enabling
research on this activity.
Acknowledgements
This work has been partially funded by the French
Government under the project COSMAT (ANR
ANR-09-CORD-004).
References
Foudil Bretel, Patrice Lopez, Maud Medves, Alain
Monteil, and Laurent Romary. 2010. Back to
meaning ? information structuring in the PEER
project. In TEI Conference, Zadar, Croatie.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of the 45th Annual
Meeting of the Association for Computational Lin-
guistics (Demo and Poster Sessions), pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Patrice Lopez. 2009. GROBID: Combining auto-
matic bibliographic data recognition and term ex-
traction for scholarship publications. In Proceed-
ings of ECDL 2009, 13th European Conference on
Digital Library, Corfu, Greece.
Holger Schwenk. 2008. Investigations on large-scale
lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proc. of the Int. Conf. on Spo-
ken Language Processing, pages 901?904, Denver,
CO.
15
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 483?489, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FBM: Combining lexicon-based ML and heuristics
for Social Media Polarities
Carlos Rodr??guez-Penagos, Jordi Atserias, Joan Codina-Filba`,
David Garc??a-Narbona, Jens Grivolla, Patrik Lambert, Roser Saur??
Barcelona Media
Av. Diagonal 177, Barcelona 08018
Corresponding author: carlos.rodriguez@barcelonamedia.org
Abstract
This paper describes the system implemented
by Fundacio? Barcelona Media (FBM) for clas-
sifying the polarity of opinion expressions in
tweets and SMSs, and which is supported by
a UIMA pipeline for rich linguistic and sen-
timent annotations. FBM participated in the
SEMEVAL 2013 Task 2 on polarity classifi-
cation. It ranked 5th in Task A (constrained
track) using an ensemble system combining
ML algorithms with dictionary-based heuris-
tics, and 7th (Task B, constrained) using an
SVM classifier with features derived from the
linguistic annotations and some heuristics.
1 Introduction
We introduce the FBM system for classifying the
polarity of short user-generated text (tweets and
SMSs), which participated in the two subtasks of
SEMEVAL 2013 Task 2 on Sentiment Analysis in
Twitter. These are: Task A. Contextual Polarity Dis-
ambiguation, and Task B. Message Polarity Classifi-
cation. The former aimed at classifying the polarity
of already identified opinion expressions (or cues),
whereas the latter consisted in classifying the polar-
ity of the whole text (Wilson et al, 2013).
The literature agrees on two main approaches for
classifying opinion expressions: using supervised
learning methods and applying dictionary/rule-
based knowledge (see (Liu, 2012) for an overview).
Each of them on its own has been used in work-
able systems, and a principled combination of both
of them can yield good results on noisy data, since
generally one (dictionaries/rules) offers good preci-
sion while the other (ML) is able to discover unseen
examples and thus enhances recall.
FBM combined both approaches in order to bene-
fit from their respective strengths and compensating
as much as possible their weaknesses. For Task A
we used linguistic (lexical and syntactic) annotations
to implement both types of approaches. On the one
hand, we built machine learning classifiers based on
Support Vector Machines (SVMs) and Conditional
Random Fields (CRFs). On the other, we imple-
mented a basic classification system mainly based
on polarity dictionaries and negation information, as
well as simple decision tree-like heuristics extracted
from the training data. For task B we trained an
SVM classifier using some of the annotations from
Task A.
The paper first presents the process of data com-
pilation and preprocessing (section 2), and then de-
scribes the systems for Tasks A (section 3) and B
(section 4). Results and conclusions are discussed
in the last section.
2 Data Compilation and Processing
2.1 Making data available
The corpus of SMSs was provided to the partici-
pants by the organizers of the task. As for the corpus
of tweets, legal restrictions on twitter data distribu-
tion required the participants to download the tex-
tual contents of the corpus from a list of tweet ids.
We retrieved the tweet text using the official twit-
ter API instead of script provided by the organizers,
but not all the tweets were available for download
483
due to restrictions of different types (e.g. geograph-
ical), or because the twitter account was temporarily
suspended. In total, we managed to retrieve 10,764
tweets out of 11,777 ids provided by the organizers
(91.4%). It is worth pointing out that the restric-
tions on tweets distribution can become an issue for
future users of the dataset, as the amount of avail-
able tweets will diminish over time. By contrast, the
twitter test corpus was distributed with the full text
to avoid those problems.
2.2 Leveraging the data with rich linguistic
information
We applied the same linguistic processing to both
corpora (SMSs and tweets), even though the SMS
test data presents very different characteristics from
the twitter data, not only because of what can be ap-
preciated as genre differences, but also due to the
fact that is apparently written in Singaporean En-
glish, which differs significantly from American or
British English. No efforts were made to adapt
our linguistic processing modules and dictionaries
to this data.
Tweets and SMSs were processed with a UIMA1-
based pipeline consisting of a set of linguistic and
opinion-oriented modules, which includes:
Basic linguistic processing: Sentence segmen-
tation, tokenization, POS-tagging, lemmatiza-
tion.
Syntax: Dependency parsing.
Lexicon-based annotations:
? Basic polarity, distinguishing among: positive,
negative, and neutral, as encoded in Wilson et
al. (2010).
? Polarity strength, using the score for pos-
itive and negative polarity in SentiWordnet
3.0 (Baccianella et al, 2010). Each Sen-
tiWordNet synset has an associated triplet of
numerical scores (positive, negative,
and objective) expressing the intensity of
positive, negative and objective polarity of the
terms it contains. They range from 0.0 to 1.0,
and their sum is 1.0 for each synset (Esuli and
Sebastiani, 2007). We selected only the synset
1http://uima.apache.org/uima-specification.html
with positive or negative scores higher than 0.5,
containing a total of 16,791 words.
? Subjectiviy clues, from Wilson et al (2010),
which are classified as weak or strong depend-
ing on their degree of subjectivity.
? Sentiment expressions, from the Linguistic In-
quiry and Word Count (LIWC) 2001 Dictio-
nary (Pennebaker et al, 2001).
? In-house compiled lexicons of negation mark-
ers (such as ?no?, ?never?, ?none?) and quanti-
fiers (?all?, ?many?, etc.), the latter further clas-
sified into low, medium and high according to
their quantification degree.
The different classifiers employed by FBM con-
structed their vectors from this output to learn global
and contextual polarities.
3 Task A: Ensemble System
Our system combined Machine Learning and rule-
based approaches. The aim was to combine the
strengths of each individual component while avoid-
ing as much as possible their weaknesses. In what
follows we describe each system component as well
as the way the ensemble system worked out the col-
lective decisions.
3.1 Conditional Random Fields
One of the classifiers uses the Conditional Random
Fields implementation of a biomedical Named En-
tity Recognition system (JNET from JulieLab) 2, ex-
ploiting the classification capabilities of the system
(rather than its span detection) by strongly associat-
ing already defined ?marked instances? with a polar-
ity, and exploring a 5-word window. It uses depen-
dency labels, POS tags, polar words, sentiwordnet
and LWIC sentiment annotations, as well as indica-
tions for quantifiers and negation markers.
3.2 Support Vector Machines
This classifier was implemented using an SVM algo-
rithm with a linear kernel and the C parameter set to
0.2 (determined using a 5 fold cross-validation). The
features set includes those that we used in RepLab
2http://www.julielab.de
484
2012 (Chenlo et al, 2012) (including number of:
characters, words, links, hashtags, positive and neg-
ative emoticons, question-exclamation marks, ad-
jectives, nouns, verbs, adverbs, uppercased words,
words with duplicated vowels), plus a set of new
features at tweet level obtained from the linguistic
annotations: number of high/medium/low polarity
quantifiers, number of positive and negative polar
words, sentiwordnet applied to both the cue and the
whole tweet.
Moreover, the RepLab polarity calculation based
on different dictionaries was modified to take into
account negation (in a 3-word window) potentially
inverting the polarity (negPol). This polarity mea-
sure was applied to the cue and to the whole tweet,
thus generating two additional features.
3.3 Heuristic Approach
In task A, in parallel to the supervised learning sys-
tem, we developed a method (named Heur) based
on polarity dictionary lookup and simple heuristics
(see Figure 1) taking into account opinion words
as well as negation markers and quantifiers. These
heuristics were implemented so as to maximize the
number of correct positive and negative labels in the
training data. To this end, we calculated the aggre-
gate polarity of a cue segment as the sum of word
polarities found in the polarity lexicon. The aggre-
gate values in the training set ranged from -3 to +3,
taking respectively 1, 0 and -1 as the polarity of pos-
itive, neutral and negative words. The label distri-
bution of cue segments with an aggregate polarity
value of -1 is shown in Table 1.
Aggregate polarity -1
Negation no yes
negative 1,032 30
neutral 37 4
positive 178 71
Table 1: Cue segment polarity statistics in training data
for an aggregate polarity value of -1.
In this case, if no negation is present in the cue
segment, a majority (1,032) of examples had the
negative label. In case there was at least a negation, a
majority (71) of examples had a positive label. This
behaviour was observed with all negative aggregate
1: if has polar word(CUE) then
2: polarity= lex(P)-0.5*lex(QP)
3: -lex(N)+0.5*lex(QN)
4: if polarity>0 then
5: if has negation(CUE) then negative
6: else positive
7: end if
8: else if polarity<0 then
9: if has negation(CUE) then positive
10: else negative
11: end if
12: else
13: if has negation(CUE) then positive
14: else negative
15: end if
16: end if
17: else if has negation(CUE) then negative
18: else
19: polarity= tlex(P)-0.5*tlex(QP)
20: -tlex(N)+0.5*tlex(QN)
21: if polarity<0 then negative
22: else if tlex(NEU)>0 then neutral
23: else if polarity>0 then positive
24: else if has negemo(CUE) then negative
25: else if has posemo(CUE) then positive
26: else unknwn
27: end if
28: end if
Figure 1: Heuristics used by the lexicon-based system to
classify the polarity of a segment marked up as opinion
cue (Task A).
polarity values in training data, yielding the rule in
lines 8 to 11 of Figure 1. Similar rules were ex-
tracted for the other aggregate polarity values (lines
4 to 16 of Figure 1).
Figure 1 details the complete classification algo-
rithm. Note (lines 1 to 17) that we first rely on the
basic polarity lexicon annotations (described in sec-
tion 2). The final aggregate polarity formula (lines
2-3) was refined to distinguish sentiment words
which act as quantifiers, such as pretty in pretty mad.
The word pretty is both a positive polar word and a
quantifier. We want its polarity to be positive in case
it occurs in isolation, but less than one so that the
sum with a following negative polar word (such as
mad) be negative. We thus give this kind of words
a polarity of 0.5 by substracting 0.5 for each polar
word which is also a quantifier. In the polarity for-
mula of lines 2-3, lex(X) refers to the number of
words annotated as X, P and N refer respectively
to positive and negative polar words, and QP and
485
QN refer to positive and negative polar words which
are also quantifiers. Quantifiers which are not polar
words are not taken into account because they are
not likely to change the opinion polarity.
In case that no annotations from the basic polar-
ity, quantifiers, and negative markers lexicons are
found (lines 18 to 28), we look up in dictionaries
built from the training data (tlex in lines 19-20).
To build these dictionaries, we counted how many
times each word was labeled positive, negative and
neutral. We considered that a word has a given po-
larity if the number of times it was assigned to this
class is greater than the number of times it was as-
signed to any other class by a given threshold. We
calculated the polarity in the same way as before,
but now with the counts from the lexicon automati-
cally compiled from the training data. To improve
the recall of the dictionary lookup, we performed
some text normalization: lowercasing, deletion of
repeated characters (such as gooood) and deletion of
the hashtag ?#? character. Finally, if no polar word
is found in the automatically compiled lexicon, we
look at the sentiment annotations (extracted from the
LIWC dictionary).
3.4 Ensemble Voting Algorithm
As already mentioned, we combined the results from
the described polarity methods to build a collective
decision. Table 2 shows the performance (in terms
of F1 measure) of the different single methods over
the tweet test data.
SVM Heur Heur+ CRF
Test 80.74 83.47 84.62 62.85
Table 2: Twitter Task A results for different methods
Although the heuristic method outperforms the
ML methods, they are not only different in nature
(ML vs. heuristic) but also use different information
(see Table 5). This suggests that the ensemble solu-
tion will be complementary and capable of obtaining
better results than any of the individual methods by
itself.
The development set was used to calculate the en-
semble response given the individual votes of the
different systems in a way similar to the behavior
knowledge space method (Huang and Suen, 1993).
Table 3 shows an example of how the assemble
voting is built. For each method vote combina-
tion (SVM-Heuristics-CRF) the number of positives
/ negatives / neutral is calculated in the development
data. The ensemble (EV) selects the vote that max-
imizes the number of correct votes in the develop-
ment data (in bold).
SVM Heur CRF EV
# Instances
pos neg neu
? + ? ? 0 6 0
? ? + ? 1 23 2
? ? ? ? 3 125 2
? u + + 1 0 0
+ u n ? 0 1 0
+ ? + + 17 13 2
+ + + + 314 18 17
+ ? n + 3 1 0
Table 3: Oracle building example (EV: Ensemble Vote,
+:positive, ?:negative, n:neutral, u:unknown)
The test data contains some combination of votes
that were not seen in the development data. Thus,
in order to deal with these unseen combinations of
votes in the test set we use the following backup
heuristics based on the preformance figures of the
individual methods: Use the vote of the heuristic
method. If this method does not vote (u), then se-
lect the SVM vote.
Table 4 shows the results of the proposed ensem-
ble method, the well-known majority voting and the
upper bound of this ensemble method (calculated
with the same strategy over the test data), over the
development and test tweet data
Ensemble Majority Upper
Voting Voting Bound
Dev 85.48 81.31 85.48
Test 85.50 82.70 89.37
Table 4: Results for different ensemble strategies
In the development corpus, the upper bound and
ensemble results are the same, given that they ap-
ply the same knowledge. The difference is in the
test dataset, where the ensemble voting is calculated
based on the knowledge obtained from the develop-
ment corpus, while the upper bound uses the knowl-
edge that can be derived from the test corpus.
486
Table 5 illustrates the features used by each com-
ponent.
SVM SVM CRF Heur
(task A) (task B)
word ? ? ?
lemma
pos ? ?
deps ?
pol ? ? ? ?
polW ?
sent ? ? ?
sentiwn ? ? ?
quant ? ? ? ?
neg ? ? ? ?
links ?
hashTags ?
Table 5: Information used (pos: part-of-speech; deps: de-
pendencies; pol: basic polarity classification; polW: basic
polarity word; sent: LIWC sentiments; sentwn: Senti-
Wordnet; quant/neg: quantifiers and negation markers.)
4 Task B: A Support Vector
Machine-based System
The system presented for task B is based on ML us-
ing a SVM model. The feature vector used as input
for the SVM component is composed of the annota-
tions provided by the linguistic annotation pipeline,
extended with a feature obtained by applying nega-
tion to the next polar words (window of size 3).
The features used do not include the words (or
their lemmas) because the number of tweets avail-
able for training is small (104) compared to the num-
ber of different words (4 ? 104). A model based on
bag-of-words would suffer from overfitting and thus
be very domain and time-dependent. If the train and
test sets were randomly selected from a bigger set,
the use of words could increase the model?s accu-
racy, but the model would also be too narrowly ap-
plied to this specific dataset.
From the annotation pipeline we extracted as fea-
tures: the polar words (PolW) and their basic po-
larity (Pol); the sentiment annotations from LIWC
(Sent); the negation markers (Neg) and quantifiers
(Quant). The model was trained using Weka (Hall
et al, 2009).
The model used is SVM with the C parameter set
to 1.0 and applying a 10 fold cross-validation. The
option of doing first a model to discriminate polar
and neutral tweets was discarded because Weka al-
ready does that when training classifiers for more
than two training classes, and the combination of the
two classifiers (a first one between polar and opin-
ionated and a second one between positive and neg-
ative) would produce the same results.
5 Results and Discussion
The results of our system in each subcorpus and task
are presented in Table 5 (average of the F1-measure
over the classes positive and negative, constrained
track), with the ranking achieved in the competition
in parentheses.
Tweet Corpus SMS Corpus
Task A 0.86 (5th) 0.73 (11th)
Task B 0.61 (7th) 0.47 (28th)
Table 6: FBM system performance (F1 average over pos-
itive and negative classes, constrained track) and rankings
Given the differences in style and vocabularies be-
tween the SMS and tweet corpora, and the fact that
we made not effort whatsoever to adapt our system
or models to them, the drop in performance from
one to the other is considerable, but to be expected
since domain customization is an important aspect
of opinion mining.
Task A: The confusion matrix in Table 7 shows
an acceptable performance for the most frequent
classes in the corpus (with an error of 7.75% and
19.5% for postive and negative cues, respectively)
and a very poor job for neutral cues (98.1% of er-
ror), clearly a minority class in the training corpus
(5% of the data).
GOLD: Pos Neg Neu
SYSTEM: Pos 2,522 296 126
Neg 206 1,240 31
Neu 6 5 3
Table 7: Task A confusion matrix
Given the skewed distribution of polarity cate-
gories in the test corpus, however, neutral mistakes
amount to only 23% of our system error, and so we
487
focus our analysis on the problems in positive and
negative cues, respectively amounting to 31.7% and
44.8% of the total error. There are 2 main sources of
error:
? Limitations of the dictionaries employed,
which were short in covering somewhat fre-
quent slang words (e.g., wacky, baddest, shit-
loads), expressions (e.g., ouch, yukk, C?MON),
or phrases (e.g., over the top), some of which
express a particular polarity but contain a word
expressing just the opposite (have a blast, to
want something bad/ly).
? Problems in UGC processing, mainly related to
normalization (e.g., fooooool) and tokenization
(Perfect...not sure), which put at risk the cor-
rect identification of lexical elements that are
crucial for polarity classification.
Task B: The average F-score of positive and neg-
ative classes was 0.62 in the development set (that
was included in the training set) and the averaged F-
score for the test set was 0.61 (so they are very simi-
lar). If focusing on precision and recall, the positive
and negative classes have higher precision but lower
recall in the test set. We think that this low degrada-
tion of perfomance indicates the model?s potential
for generalization.
6 Conclusions
From our results, we can conclude that the use of
ensemble combination of orthogonal methods pro-
vides good performance for Task A. Similar results
could be expected for Task B (judging from mix-
ing dictionaries and ML in similar tasks at RepLab
2012 (Chenlo et al, 2012)). The ML methods that
we applied for Task B are essentially additive, and
hence have difficulties in applying features such as
polarity shifters. To overcome this, one of the fea-
tures includes negation of polar words when a polar-
ity shifter is near.
Overall, the SemEval Tasks have make evident the
usual challenges when mining opinions from Social
Media channels: noisy text, irregular grammar and
orthography, highly specific lingo, etc. Moreover,
temporal dependencies can affect the performance if
the training and test data have been gathered at dif-
ferent times, as is the case with text of such a volatile
nature as tweets and SMSs.
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
30.00%
35.00%
40.00%
45.00%
50.00%
train
dev
test
Figure 2: Distribution of tweets over time
The histogram in Figure 2 shows that this also ap-
plies to the Semeval tweets dataset. It illustrates the
distribution of tweets over time (extrapolated from
the sequential ids) in the 3 subcorpora (train, devel-
opment and test), showing some divergence between
the test corpus on the one hand, and the develop-
ment and training corpora on the other. Neverthe-
less, our system shows little performance degrada-
tion between development and testing results, as at-
tested in Table 4 (ensemble voting column).
Our work here and at other competitions already
cited validate a system that combines stochastic and
symbolic methodologies in a principled, data-driven
approach. Time and domain dependencies of Social
Media data make system and model generalization
highly desirable, and our system hybrid nature also
contribute to this objective.
Acknowledgments
This work has been partially funded by the Spanish
Government project Holopedia, TIN2010-21128-
C02-02, the CENIT program project Social Media,
CEN-20101037, and the Marie Curie Reintegration
Grant PIRG04-GA-2008-239414.
488
References
Baccianella, Stefano, Andrea Esuli and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th conference on International
Language Resources and Evaluation, Valletta, Malta.
Chenlo, Jose M., Jordi Atserias, Carlos Rodr??guez-
Penagos and Roi Blanco. 2012. FBM-Yahoo!
at RepLab 2012. In: P. Forner, J. Karlgren,
C. Womser-Hacker (eds.) CLEF 2012 Evalua-
tion Labs and Workshop, Online Working Notes.
http://clef2012.org/index.php?page=Pages/procee-
dings.php.
Esuli, Andrea and Fabrizio Sebastiani. 2007. SEN-
TIWORDNET: a high-coverage lexical resource for
opinion mining. Technical Report ISTI-PP-002/2007,
Institute of Information Science and Technologies
(ISTI) of the Italian National Research Council
(CNR).
Hall, Mark, Frank Eibe, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann and Ian H. Witten. 2009.
The WEKA data mining software: an update. In:
ACM SIGKDD Explorations Newsletter, 1: 10?18.
Huang, Y. S. and C. Y. Suen. 1993. Behavior-knowledge
space method for combination of multiple classifiers.
In Proceedings of IEEE Computer Vision and Pattern
Recognition, 347?352.
Liu, Bing. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
(5-1), 1?167.
Pennebaker, James W., Martha E. Francis and Roger
J. Booth. 2001. Linguistic inquiry and word count:
LIWC 2001. Mahway: Lawrence Erlbaum Asso-
ciates.
Wilson, Theresa, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov and Alan. Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13.
Wilson, Theresa, Janyce Wiebe and Paul Hoffmann.
2010. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35(3), 399?433.
489
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 121?126,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
LIUM SMT Machine Translation System for WMT 2010
Patrik Lambert, Sadaf Abdul-Rauf and Holger Schwenk
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French ma-
chine translation systems for the 2010
WMT shared task evaluation. These sys-
tems were standard phrase-based statisti-
cal systems based on the Moses decoder,
trained on the provided data only. Most
of our efforts were devoted to the choice
and extraction of bilingual data used for
training. We filtered out some bilingual
corpora and pruned the phrase table. We
also investigated the impact of adding two
types of additional bilingual texts, ex-
tracted automatically from the available
monolingual data. We first collected bilin-
gual data by performing automatic trans-
lations of monolingual texts. The second
type of bilingual text was harvested from
comparable corpora with Information Re-
trieval techniques.
1 Introduction
This paper describes the machine translation sys-
tems developed by the Computer Science labora-
tory at the University of Le Mans (LIUM) for the
2010 WMT shared task evaluation. We only con-
sidered the translation between French and En-
glish (in both directions). The main differences
with respect to previous year?s system (Schwenk
et al, 2009) are as follows: restriction to the data
recommended for the workshop, usage of the (fil-
tered) French?English gigaword bitext, pruning of
the phrase table, and usage of automatic trans-
lations of the monolingual news corpus to im-
prove the translation model. We also used a larger
amount of bilingual data extracted from compara-
ble corpora than was done in 2009. These different
points are described in the rest of the paper, to-
gether with a summary of the experimental results
showing the impact of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used
to train the translation and language models of the
system.
2.1 Bilingual data
Our system was developed in two stages. First,
a baseline system was built to generate automatic
translations of some of the monolingual data avail-
able. These automatic translations may be used
directly with the source texts to build additional
bitexts, or as queries of an Information Retrieval
(IR) system to extract new bitexts from compara-
ble corpora. In a second stage, these additional
bilingual data were incorporated to the system (see
Section 4 and Tables 1 and 2).
The latest version of the News-Commentary
(NC) corpus, of the Europarl (Eparl) corpus (ver-
sion 5), and of the United Nations (UN) corpus
were used. We also took as training data a sub-
set of the French?English Gigaword (109) cor-
pus. Since a significant part of the data was
crawled from the web, we thought that many sen-
tence pairs may be only approximate translations
of each other. We applied a lexical filter to dis-
card them. Furthermore, some sentences of this
corpus were extracted from web page menus and
are not grammatical. Although we could have
used a part of the menu items as a dictionary, for
simplicity we applied an n-gram language model
(LM) filter to remove all non-grammatical sen-
tences. Thanks to this filter, sentences out of the
language model domain (in this case, mainly the
news domain), may also have been discarded be-
cause they contain many unknown or unfrequent
n-grams. The lexical filter was based on the IBM
model 1 cost (Brown et al, 1993) of each side of
a sentence pair given the other side, normalised
with respect to both sentence lengths. This filter
121
was trained on a corpus composed of Eparl, NC,
and UN data. The language model filter was an
n-gram LM cost of the target sentence (see Sec-
tion 3), normalised with respect to its length. This
filter was trained with all monolingual resources
available except the 109 data. We generated a first
subset, 1091, selecting sentence pairs with a lexi-
cal cost inferior to 4 and an LM cost inferior to
2.3. The corpus selected in this way contains 115
million words in the English side (out of 580 mil-
lion in the original corpus). Close to the evaluation
deadline we decided to generate a second corpus
(1092) by raising the LM cost threshold to 2.6. The
1092 corpus contains 232 million words on the En-
glish side (twice as much as in the 1091 corpus).
In the French side of the bilingual corpora, for
the French?English direction only, the contrac-
tions ?du? (?of the?), ?au? and ?aux? (?to the? singu-
lar and plural) were substituted by their expanded
forms (?de le?, ?a` le? and ?a` les?).
2.2 Use of Automatic Translations and
Comparable corpora
Available human translated bitexts such as the UN
corpus seem to be out-of domain for this task.
We used two types of automatically extracted re-
sources to adapt our system to the task domain.
First, we generated automatic translations of the
French News corpus provided (231M words), and
selected the sentences with a normalised transla-
tion cost (returned by the decoder) inferior to a
threshold. The resulting bitext has no new words
in the English side, since all words of the transla-
tion output come from the translation model, but
it contains new combinations (phrases) of known
words, and reinforces the probability of some
phrase pairs (Schwenk, 2008).
Second, as in last year?s evaluation, we auto-
matically extracted and aligned parallel sentences
from comparable in-domain corpora. This year
we used the AFP and APW news texts since there
are available in the French and English LDC Gi-
gaword corpora. The general architecture of our
parallel sentence extraction system is described in
detail by Abdul-Rauf and Schwenk (2009). We
first translated 91M words from French into En-
glish using our first stage SMT system. These En-
glish sentences were then used to search for trans-
lations in the English AFP and APW texts of the
Gigaword corpus using information retrieval tech-
niques. The Lemur toolkit (Ogilvie and Callan,
2001) was used for this purpose. Search was lim-
ited to a window of ?5 days of the date of the
French news text. The retrieved candidate sen-
tences were then filtered using the Translation Er-
ror Rate (TER) with respect to the automatic trans-
lations. In this study, sentences with a TER be-
low 65% for the French?English system and 75%
for the English?French system were kept. Sen-
tences with a large length difference (French ver-
sus English) or containing a large fraction of num-
bers were also discarded. By these means, about
15M words of additional bitexts were obtained to
include in the French?English system, and 21M
words to include in the English?French system.
Note that these additional bitexts do not depend
on the translation direction. The most suitable
amount of additional data was just different in
the French?English and English?French transla-
tion directions.
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. In
addition, LDC?s Gigaword collection was used for
both languages. Data corresponding to the devel-
opment and test periods were removed from the
Gigaword collections.
2.4 Development data
All development was done on news-test2008, and
newstest2009 was used as internal test set. For all
corpora except the French side of the bitexts used
to train the French?English system (see above),
the default Moses tokenization was used. How-
ever, we added abbreviations for the French tok-
enizer. All our models are case sensitive and in-
clude punctuation. The BLEU scores reported in
this paper were calculated with the multi-bleu.perl
tool and are case sensitive. The BLEU score
was one of metrics with the best correlation with
human ratings in last year evaluation (Callison-
Burch et al, 2009) for the French?English and
English?French directions.
3 Architecture of the SMT system
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source
sentence f . It is today common practice to use
phrases as translation units (Koehn et al, 2003;
Och and Ney, 2003) and a log linear framework in
order to introduce several models explaining the
122
translation process:
e? = argmax
e
p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system mod-
els and the ?i weights are typically optimized to
maximize a scoring function on a development
set (Och and Ney, 2002). In our system fourteen
features functions were used, namely phrase and
lexical translation probabilities in both directions,
seven features for the lexicalized distortion model,
a word and a phrase penalty and a target language
model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++
that can appear with rare words.
Phrases and lexical reorderings are extracted
using the default settings of the Moses toolkit.
The parameters of Moses were tuned on news-
test2008, using the ?new? MERT tool. We repeated
the training process three times, each with a differ-
ent seed value for the optimisation algorithm. In
this way we have an rough idea of the error intro-
duced by the tuning process.
4-gram back-off LMs were used. The word
list contains all the words of the bitext used to
train the translation model and all words that ap-
pear at least ten times in the monolingual corpora.
Words of the monolingual corpora containing spe-
cial characters or sequences of uppercase charac-
ters were not included in the word list. Separate
LMs were build on each data source with the SRI
LM toolkit (Stolcke, 2002) and then linearly in-
terpolated, optimizing the coefficients with an EM
procedure. The perplexities of these LMs were
103.4 for French and 149.2 for English.
4 Results and Discussion
The results of our SMT system for the French?
English and English?French tasks are summarized
in Tables 1 and 2, respectively. The MT metric
scores are the average of three optimisations per-
formed with different seeds (see Section 3). The
1The source is available at http://www.cs.cmu.
edu/?qing/
numbers in parentheses are the standard deviation
of these three values. The standard deviation gives
a lower bound of the significance of the difference
between two systems. If the difference between
two average scores is less than the sum of the stan-
dard deviations, we can say that this difference is
not significant. The reverse is not true. Note that
most of the improvements shown in the tables are
small and not significant. However many of the
gains are cumulative and the sum of several small
gains makes a significant difference.
Phrase-table Pruning
We tried to prune the phrase-table as proposed by
Johnson et. al. (2007), and available in moses
(?sigtest-filter?). We used the ? ?  filter2. As
lines 3 and 4 of Table 1, and lines 3 and 4 of Ta-
ble 2 reveal, in addition to the reduction 43% of
the phrase-table, a small gain in BLEU score (0.15
and 0.11 respectively) was obtained with the prun-
ing.
Baseline French?English System
The first section of Table 1 (lines 1 to 5) shows re-
sults of the development of the baseline SMT sys-
tem, used to generate automatic translations. Al-
though being out-of-domain data, the introduction
of the UN corpus yields an improvement of one
BLEU point with respect to Eparl+NC. Adding the
1091 corpus, we gain 0.7 BLEU point more. Ac-
tually, we obtained the same score with the 1091
added directly to Eparl+NC (line 5). However, we
choose to include the UN corpus to generate trans-
lations to have a larger vocabulary. The system
highlighted in bold (line 4) is the one we choose
to generate our English translations.
Although no French translations were gener-
ated, we did similar experiments in the English?
French direction (lines 1 to 4 of Table 2). In this
direction, the 1091 corpus is still more valuable than
the UN corpus when added to Eparl+NC, but with
less difference in terms of BLEU score. In this di-
2The p-value of two-by-two contingency tables (describ-
ing the degree of association between a source and a target
phrase) is calculated with Fisher exact test. This probability
is interpreted as the probability of observing by chance an as-
sociation that is at least as strong as the given one, and hence
as its significance. An important special case of a table oc-
curs when a phrase pair occurs exactly once in the corpus,
and each of the component phrases occurs exactly once in its
side of the parallel corpus (1-1-1 phrase pairs). In this case
the negative log of the p-value is ? = logN (N is number of
sentence pairs in the corpus). ? ?  is the largest threshold
that results in all of the 1-1-1 phrase pairs being included.
123
rection, we obtain a gain by adding the UN corpus
to Eparl+NC+1091.
Filtering the 109 Corpus
Lines 5 to 7 of Table 1 show the impact of filtering
the 109 corpus. The system trained on the full 109
corpus added to Eparl+NC achieves a BLEU score
of 26.83. Substituting the full 109 corpus by 1091 (5
times smaller), i.e. using the first filtering settings,
we gain 0.13 BLEU point. Using 1092 instead of
1091, we gain another 0.16 BLEU point, that is 0.3
in total. With respect to not using the 109 data at
all (as we did last year), we gain 0.8 BLEU point.
Impact of the Additional Bitexts
With the baseline French?English SMT system
(see above), we translated the French News cor-
pus to generated an additional bitext (News). We
also translated some parts of the French LDC Gi-
gaword corpus, to serve as queries to our IR sys-
tem (see section 2.2). The resulting additional bi-
text is referred to as IR. Lines 8 to 13 of Table 1
and lines 6 to 12 of Table 2 summarize the system
development including the additional bitexts.
With the News additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the baseline system used to generate
the automatic translations, but with less than 30%
of the data. This holds in both translation direc-
tions. Adding the News corpus to a larger corpus,
such as Eparl+NC+1091, has less impact but still
yields some improvement: 0.15 BLEU point in
French?English and 0.3 in English?French. Thus,
the News bitext translated from French to English
may have more impact when translating from En-
glish to French than in the opposite direction. Note
that the number of additional phrase-table entries
per additional running word is twice as high for
the News bitext than for the other corpora. For
example, with respect to Eparl+NC+UN+1091 (Ta-
ble 2), Eparl+NC+UN+1091+News has 56M more
words and 116M more entries in the phrase-table,
thus the ratio is more than 2. For all other cor-
pora, the ratio is equal to 1 or less. This is un-
expected, particularly in this case where the News
bitext has no new English vocabulary with respect
to the Eparl+NC+UN+1091 corpus, from which its
English side was generated.
With the IR additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the system trained on Eparl+NC+UN,
while the IR bitext is 10 times smaller than the
UN corpus. Added to Eparl+NC+1091+News, the
IR bitext allows gains of 0.13 and 0.2 BLEU point
respectively in the French?English and English?
French directions.
Comparing the systems trained on
Eparl+NC+1091 or Eparl+NC+10
9
2 to the sys-
tems trained on the same corpora plus News+IR,
we can estimate the cumulative impact of the
additional bitexts. The gain is around 0.3 BLEU
point for French?English and around 0.5 BLEU
point for English?French.
Final System
In both translation directions our best system was
the one trained on Eparl+NC+1092+News+IR. We
further achieved small improvements (0.3 BLEU
point) by pruning the phrase-table (as above) and
by using a language model with no cut-off together
with increasing the beam size and/or the maxi-
mum number of translation table entries per input
phrase. Note that the English LM with cut-off had
a size of 6G, and the one with no cut-off had a
size of 29G. It was too much to fit in our 72G
machines so we pruned it with the SRILM prun-
ing tool down to a size of 19G. The French LM
with cut-off had a size of 2G and the one with
no cut-off had a size of 9G. These sizes corre-
spond to the binary format. Taking as example the
French?English direction, the running time went
from 8600 seconds for the system of line 14 (with
a threshold pruning coefficient of 0.4 and a LM
with cut-off) to 28200 seconds for the system sub-
mitted (with the LM without cut-off pruned by the
SRILM tool and a threshold pruning coefficient of
0.00001).
5 Conclusions and Further Work
We presented the development of our machine
translation system for the French?English and
English?French 2010 WMT shared task. Our sys-
tem was actually a standard phrase-based SMT
system based on the Moses decoder. Its original-
ity mostly lied in the choice and extraction of the
training data used.
We decided to use a part of the 109 French?
English corpus. We found this resource useful,
even without filtering. We nevertheless gained 0.3
BLEU point by selecting sentences based on an
IBM Model 1 filter and a language model filter.
We pruned the phrase table with the ?sigtest-
filter? distributed in Moses, yielding improve-
124
Bitext #Fr Words P-table Mem news-test2008 newstest2009
(M) size (M) (G) BLEU BLEU
1 Eparl+NC 52 66 19.3 22.80 (0.03) 25.31 (0.2)
2 Eparl+NC+UN 275 250 22.8 23.38 (0.1) 26.30 (0.2)
3 Eparl+NC+UN+1091 406 376 25.1 23.81 (0.05) 27.0 (0.2)
4 Eparl+NC+UN+1091 pruned 406 215 21.4 23.96 (0.1) 27.15 (0.18)
5 Eparl+NC+1091 183 198 22.1 23.83 (0.07) 26.96 (0.04)
6 Eparl+NC+1092 320 319 24.1 23.95 (0.03) 27.12 (0.1)
7 Eparl+NC+109 733 580 29.5 23.65 (0.09) 26.83 (0.2)
8 Eparl+NC+News 111 188 19.5 23.46 (0.1) 26.95 (0.2)
9 Eparl+NC+1091+News 242 317 22.5 23.77 (0.04) 27.11 (0.04)
10 Eparl+NC+IR 68 78 19.5 22.97 (0.03) 26.20 (0.1)
11 Eparl+NC+News+IR 127 198 20.1 23.62 (0.01) 27.04 (0.06)
12 Eparl+NC+1091+News+IR 258 327 22.8 23.75 (0.05) 27.24 (0.05)
13 Eparl+NC+1092+News+IR 395 441 24.4 23.87 (0.03) 27.43 (0.08)
14 Eparl+NC+1092+News+IR pruned 395 285 62.5 24.04 27.72
(+larger beam, +no-cutoff LM)
Table 1: French?English results: number of French words (in million), number of entries in the phrase-
table (in million), memory needed during decoding (in gigabytes) and BLEU scores in the development
(news-test2008) and internal test (newstest2009) sets for the different systems developped. The BLEU
scores and the number in parentheses are the average and standard deviation over 3 values (see Section 3.)
ments of 0.1 to 0.2 BLEU point for a 43% reduc-
tion of the phrase-table size.
We used additional bitexts extracted automati-
cally from the available monolingual corpora. The
first type of additional bitext is generated with au-
tomatic translations of the monolingual data with
a baseline SMT system. The second one is ex-
tracted from comparable corpora, with Informa-
tion Retrieval techniques. With the additional bi-
texts we gained 0.3 and 0.5 BLEU point for the
French?English and English?French systems, re-
spectively.
Next year we want to perform an improved se-
lection of parallel training data with re-sampling
techniques. We also want to use a continuous
space language model (Schwenk, 2007) in an n-
best list rescoring step after decoding. Finally, we
plan to train different types of systems (such as
a hierarchical SMT system and a Statistical Post-
Editing system) and combine their outputs with
the MANY open source system combination soft-
ware (Barrault, 2010).
Acknowledgments
This work has been partially funded by
the European Union under the EuroMatrix
Plus project ? Bringing Machine Transla-
tion for European Languages to the User ?
(http://www.euromatrixplus.net, IST-2007.2.2-
FP7-231720).
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages
16?23, Athens, Greece.
Lo??c Barrault. 2010. MANY : Open source machine
translation system combination. Prague Bulletin
of Mathematical Linguistics, Special Issue on Open
Source Tools for Machine Translation, 93:147?155.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the ACL Fourth Workshop on Sta-
tistical Machine Translation, pages 1?28, Athens,
Greece.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
125
Bitext #En Words Phrase-table news-test2008 newstest2009
(M) size (M) BLEU BLEU
1 Eparl+NC+UN 242 258 24.21 (0.01) 25.29 (0.12)
2 Eparl+NC+1091 163 203 24.24 (0.06) 25.51 (0.13)
3 Eparl+NC+UN+1091 357 385 24.46 (0.08) 25.73 (0.20)
4 Eparl+NC+UN+1091 pruned 357 221 24.42 (0.1) 25.84 (0.05)
5 Eparl+NC+1092 280 330 24.43 (0.04) 25.68 (0.12)
6 Eparl+NC+News 103 188 24.27 (0.2) 25.70 (0.15)
7 Eparl+NC+1091+News 218 321 24.51 (0.05) 25.83 (0.05)
8 Eparl+NC+UN+1091+News 413 501 24.70 (0.1) 25.86 (0.14)
9 Eparl+NC+IR 69 81 24.14 (0.05) 25.17 (0.2)
10 Eparl+NC+News+IR 124 201 24.32 (0.12) 25.84 (0.17)
11 Eparl+NC+1091+News+IR 239 333 24.54 (0.1) 26.03 (0.15)
12 Eparl+NC+1092+News+IR 356 453 24.68 (0.04) 26.19 (0.05)
13 Eparl+NC+1092+News+IR pruned 356 293 25.06 26.53
(+larger beam, +no-cutoff LM)
Table 2: English?French results: number of English words (in million), number of entries in the phrase-
table (in million) and BLEU scores in the development (news-test2008) and internal test (newstest2009)
sets for the different systems developped. The BLEU scores and the number in parentheses are the
average and standard deviation over 3 values (see Section 3.)
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignement
models. Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments
using the Lemur toolkit. In In Proceedings of the
Tenth Text Retrieval Conference (TREC-10), pages
103?108.
Holger Schwenk, Sadaf Abdul Rauf, Lo??c Barrault,
and Jean Senellart. 2009. SMT and SPE machine
translation systems for WMT?09. In Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 130?134, Athens, Greece. Association
for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proc. of the Int. Conf. on Spo-
ken Language Processing, pages 901?904, Denver,
CO.
126
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 284?293,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Investigations on Translation Model Adaptation Using Monolingual Data
Patrik Lambert, Holger Schwenk, Christophe Servan and Sadaf Abdul-Rauf
LIUM, University of Le Mans
72085 Le Mans, France
FirstName.LastName@lium.univ-lemans.fr
Abstract
Most of the freely available parallel data to
train the translation model of a statistical ma-
chine translation system comes from very spe-
cific sources (European parliament, United
Nations, etc). Therefore, there is increasing
interest in methods to perform an adaptation
of the translation model. A popular approach
is based on unsupervised training, also called
self-enhancing. Both only use monolingual
data to adapt the translation model. In this pa-
per we extend the previous work and provide
new insight in the existing methods. We report
results on the translation between French and
English. Improvements of up to 0.5 BLEU
were observed with respect to a very com-
petitive baseline trained on more than 280M
words of human translated parallel data.
1 Introduction
Adaptation of a statistical machine translation sys-
tem (SMT) is a topic of increasing interest during
the last years. Statistical (n-gram) language models
are used in many domains and several approaches to
adapt such models were proposed in the literature,
for instance in the framework of automatic speech
recognition. Many of these approaches were suc-
cessfully used to adapt the language model of an
SMT system. On the other hand, it seems more chal-
lenging to adapt the other components of an SMT
system, namely the translation and reordering mod-
els. In this work we consider the adaptation of the
translation model of a phrase-based SMT system.
While rule-based machine translation rely on
rules and linguistic resources built for that purpose,
SMT systems can be developed without the need of
any language-specific expertise and are only based
on bilingual sentence-aligned data (?bitexts?) and
large monolingual texts. However, while monolin-
gual data are usually available in large amounts and
for a variety of tasks, bilingual texts are a sparse re-
source for most language pairs.
Current parallel corpora mostly come from one
domain (proceedings of the Canadian or European
Parliament, or of the United Nations). This is prob-
lematic when SMT systems trained on such corpora
are used for general translations, as the language jar-
gon heavily used in these corpora is not appropriate
for everyday life translations or translations in some
other domain. This problem could be attacked by ei-
ther searching for more in-domain training data, e.g.
by exploring comparable corpora or the WEB, or by
adapting the translation model to the task. In this
work we consider translation model adaptation with-
out using additional bilingual data. One can dis-
tinguish two types of translation model adaptation:
first, adding new source words or/and new transla-
tions to the model; and second, modifying the prob-
abilities of the existing model to better fit the topic
of the task. These two directions are complementary
and could be simultaneously applied. In this work
we focus on the second type of adaptation.
In this work, we focus on statistical phrase-
based machine translations systems (PBSMT), but
the methods could be also applied to hierarchical
systems. In PBSMT, the translation model is rep-
resented by a large list of all known source phrases
and their translations. Each entry is weighted us-
ing several probabilities, e.g. the popular Moses
284
system uses phrase translation probabilities in the
forward and backward direction, as well as lexical
probabilities in both directions. The entries of the
phrase-table are automatically extracted from sen-
tence aligned parallel data and they are usually quite
noisy. It is not uncommon to encounter several hun-
dreds, or even thousands of possible translations of
frequent source phrases. Many of these automati-
cally extracted translations are probably wrong and
are never used since their probabilities are (fortu-
nately) small in comparison to better translations.
Therefore, several approaches were proposed to fil-
ter these phrase-tables, reducing considerably their
size without any loss of the quality, or even achiev-
ing improved performance (Johnson et al, 2007).
Given these observations, adaptation of the trans-
lation model of PBSMT systems could be performed
by modifying the probability distribution of the ex-
isting phrases without necessarily modifying the en-
tries. The idea is of course to increase the prob-
abilities of translations that are appropriate to the
task and to decrease the probabilities of the other
ones. Ideally, we should also add new translations or
source phrase, but this seems to be more challenging
without any additional parallel data.
A common way to modify a statistical model is to
use a mixture model and to optimize the coefficients
to the adaptation domain. This was investigated in
the framework of SMT by several authors, for in-
stance for word alignment (Civera and Juan, 2007),
for language modeling (Zhao et al, 2004; Koehn
and Schroeder, 2007) and to a lesser extent for the
translation model (Foster and Kuhn, 2007; Chen et
al., 2008). This mixture approach has the advan-
tage that only few parameters need to be modified,
the mixture coefficients. On the other hand, many
translation probabilities are modified at once and it
is not possible to selectively modify the probabilities
of particular phrases.
Another direction of research is self-enhancing of
the translation model. This was first proposed by
Ueffing (2006). The idea is to translate the test data,
to filter the translations with help of a confidence
score and to use the most reliable ones to train an
additional small phrase table that is jointly used with
the generic phrase table. This could be also seen as a
mixture model with the in-domain component being
build on-the-fly for each test set. In practice, such
an approach is probably only feasible when large
amounts of test data are collected and processed at
once, e.g. a typical evaluation set up with a test set of
about 50k words. This method of self-enhancing the
translation model seems to be more difficult to apply
for on-line SMT, e.g. a WEB service, since often the
translation of some sentences only is requested. In
follow up work, this approach was refined (Ueffing
et al, 2007). Domain adaptation was also performed
simultaneously for the translation, language and re-
ordering model (Chen et al, 2008).
A somehow related approach was named lightly-
supervised training (Schwenk, 2008). In that work
an SMT system is used to translate large amounts of
monolingual texts, to filter them and to add them to
the translation model training data. This approach
was reported to obtain interesting improvements
in the translations quality (Schwenk and Senellart,
2009; Bertoldi and Federico, 2009). In comparison
to self enhancing as proposed by Ueffing (2006),
lightly-supervised training does not adapt itself to
the test data, but large amounts of monolingual train-
ing data are translated and a completely new model
is built. This model can be applied to any test data,
including a WEB service.
In this paper we propose to extend this approach
in several ways. First, we argue that the automatic
translations should not be performed from the source
to the target language, but in the opposite direction.
Second, we propose to use the segmentation ob-
tained during translation instead of performing word
alignments with GIZA++ (Och and Ney, 2003) of
the automatic translations. Finally, we propose to
enrich the vocabulary of the adapted system by de-
tecting untranslated words and automatically infer-
ring possible translations from the stemmed form
and the existing translations in the phrase table.
This paper is organized as follows. In the next
section we first describe our approach in detail. Sec-
tion 3 describes the considered task, the available
resources and the baseline PBSMT system. Results
are summarized in section 4 and the paper concludes
with a discussion and perspectives of this work.
2 Architecture of the approach
In this paper we propose to extend in several ways
the translation model adaptation by unsupervised
285
training as proposed by Schwenk (2008). In that
paper the authors propose to first build a PBSMT
system using all available human translated bi-
texts. This system is then used to translate large
amounts of monolingual data in the source language.
These automatic translations are filtered using the
sentence-length normalized log score of Moses, i.e.
the sum of the log-scores of all feature functions.
Putting a threshold on this score, only the most re-
liable translations are kept. This threshold was de-
termined experimentally. The automatic translations
were added to the parallel training data and a new
PBSMT model was build, performing the complete
pipeline of word alignment with GIZA++, phrase
extraction and scoring and tuning the system on
development data with MERT. In Schwenk (2009)
significant improvement were obtained by this ap-
proach when translating from Arabic to French.
2.1 Choice of the translation direction
First, we argue that it should be better to translate
monolingual data in the opposite translation direc-
tion of the system that we want to improve, i.e. from
the target into the source language. When translat-
ing large amounts of monolingual data, the system
will of course produce some wrong translations with
respect to choice of the vocabulary, to word order,
to morphology, etc. If we translate from the source
to the target language, these wrong translations are
added to the phrase table and may be used in future
translations performed by the adapted system. When
we add the automatic translations performed in the
opposite direction to the training data, the possibly
wrong translations will appear on the source side of
the entries in the adapted phrase table. PBSMT sys-
tems segment the source sentence according to the
available entries in the phrase table. Since the source
sentence is usually grammatically and semantically
correct, with the eventual exception of speech trans-
lation, it is unlikely that the wrong entries in the
phrase table will be ever used, e.g. phrases with bad
word choice or wrong morphology.
The question of the choice of the translation di-
rection was already raised by Bertoldi and Fed-
erico (2009). However, when data in the source
language is available they adapt only the translation
model (TM), while they adapt the TM and the lan-
guage model (LM) when data in the target language
is given. Of course the system with adapted LM is
much better, but this doesn?t prove that target mono-
lingual data are better than source monolingual data
for TM adaptation. In our paper, we use the same,
best, LM for all systems and we adapt the baseline
system with bitexts synthesized from source or tar-
get monolingual data.
2.2 Word alignment
In the work of Schwenk (2008), the filtered auto-
matic translation were added to the parallel training
data and the full pipeline to build a PBSMT sys-
tem was performed again, including word alignment
with GIZA++. Word alignment of bitexts of several
hundreds of millions of words is a very time con-
suming step. Therefore we propose to use the seg-
mentation into phrases and words obtained implic-
itly during the translation of the monolingual data
with the moses toolkit. These alignments are simply
added to the previously calculated alignments of the
human translated bitexts and a new phrase table is
built.
This new procedure does not only speed-up the
overall processing, but there are also investigations
that these alignments obtained by decoding are more
suitable to extract phrases than the symmetrized
word alignments produced by GIZA++. For in-
stance, Wuebker et al (2010) proposed to trans-
late the training data, using forced alignment and
a leave-one-out technique, and to use the induced
alignments to extract phrases. They have observed
improvements with respect to word alignment ob-
tained by GIZA++. On the other hand, Bertoldi and
Federico (2009) adapted an SMT system with au-
tomatic translations and trained the translation and
reordering models on the word alignment used by
moses. They reported a very small drop in per-
formance with respect to training word alignments
with GIZA++. Similar ideas were also used in pivot
translation. Bertoldi et al (2008) translated from the
pivot language to the source language to create par-
allel training data for the direct translation.
2.3 Treatment of unknown words
Statistical machine translation systems have some
trouble dealing with morphologically rich lan-
guages. It can happen, in function of the avail-
able training data, that translations of words are only
286
Source language Source language Target language
French stemmed form English
finies fini finished
efface?s efface? erased
hawaienne hawaien Hawaiian
... ... ...
Table 1: Example of translations from French to English
which are automatically extracted from the phrase-table
with the stemmed form.
known in some forms and not in others. For in-
stance, for a user of MT technology it is quite dif-
ficult to understand why the system can translate
the French word ?je pense?1, but not ?tu penses?2.
There have been attempts in the literature to address
this problem, for instance by Habash (2008) to deal
with the Arabic language. It is actually possible to
automatically infer possible translations when trans-
lating from a morphologically rich language, to a
simpler language. In our case we use this approach
to translate from French to English.
Several of the unknown words are actually adjec-
tives, nouns or verbs in a particular form that itself
is not known, but the phrase table would contain the
translation of a different form. As an example we
can mention the French adjective finies which is in
the female plural form. After stemming we may be
able to find the translation in a dictionary which is
automatically extracted from the phrase-table (see
Table 1). This idea was already outlined by (Bo-
jar and Tamchyna, 2011) to translate from Czech to
English.
First, we automatically extract a dictionary from
the phrase table. This is done, be detecting all 1-to-1
entries in the phrase table. When there are multi-
ple entries, all are kept with their lexical translations
probabilities. Our dictionary has about 680k unique
source words with a total of almost 1M translations.
source segment les travaux sont finis
stemmed les travaux sont fini
segment les travaux sont <n translation=?finished||ended?
proposed prob=?0.008||0.0001?>finis</n>
Table 2: Example of the treatment of an unknown French
word and its automatically inferred translation.
The detection of unknown words is performed by
1I think
2you think
comparing the n-grams contained in the phrase ta-
ble and the source segment in order to detect iden-
tical words. Once the unknown word is selected,
we are looking for its stemmed form in the dictio-
nary and propose some translations for the unknown
word based on lexical score of the phrase table (see
Table 2 for some examples). The stemmer used is
the snowball stemmer3. Then the different hypothe-
sis are evaluated with the target language model.
This kind of processing could be done either be-
fore running the Moses decoder, i.e. using the
XML mark-up of Moses, or after decoding by post-
processing the untranslated words. In both cases, we
are unable to differentiate the possible translations
of the same source phrase with meaningful transla-
tion probabilities, and they won?t be added to the
phrase-table, nor put into a context with other words
that may trigger their use.
Therefore, we propose to use this technique to re-
place unknown words during the translation of the
monolingual data that we use to adapt the transla-
tion model. By these means, the automatically in-
duced translations of previously unknown morpho-
logical forms will be put into a context and actually
appear in the new adapted phrase-table. The corre-
sponding translation probabilities will be those cor-
responding to their frequency in the monolingual in-
domain data.
This procedure has been implemented, but we
were not able to obtain improvements in the BLEU
score. However, one can ask if automatic metrics,
evaluated on a test corpus of limited size, are the best
choice to judge this technique. In fact, in our setting
we have observed that less than 0.2% of the words
in the test set are unknown. We argue that the ability
to complement the phrase-table with many morpho-
logical forms of other wise known words, can only
improve the usability of SMT systems.
3 Task Description and resources
In this paper, we consider the translation of news
texts between French and English, in both direc-
tions. In order to allow comparisons, we used ex-
actly the same data as those allowed for the inter-
national evaluation organized in the framework of
the sixth workshop on SMT, to be held in Edinburgh
3http://snowball.tartarus.org/
287
Parallel data Size English/French French/English
[M words] Dev Test Dev Test
Eparl + nc 54 26.20 (0.06) 28.06 (0.2) 26.70 (0.06) 27.41 (0.2)
Eparl + nc + crawled1 168 26.84 (0.09) 29.08 (0.1) 27.96 (0.09) 28.20 (0.04)
Eparl + nc + crawled2 286 26.95 (0.04) 29.29 (0.03) 28.20 (0.03) 28.57 (0.1)
Eparl + nc + un 379 26.57 28.52 - -
Eparl + nc + crawled1 + un 514 26.87 28.99 - -
Eparl + nc + crawled2 + un 631 26.99 29.26 - -
Table 4: Case sensitive BLEU scores as a function of the amount of parallel training data. (Eparl=Europarl, nc=News
Commentary, crawled1/2=sub-sampled crawled bitexts, un=sub-sampled United Nations bitexts).
Corpus English French
Bitexts:
Europarl 50.5M 54.4M
News Commentary 2.9M 3.3M
United Nations 344M 393M
Crawled (109 bitexts) 667M 794M
Development data:
newstest2009 65k 73k
newstest2010 62k 71k
Monolingual data:
LDC Gigaword 4.1G 920M
Crawled news 2.6G 612M
Table 3: Available training data for the translation be-
tween French and English for the translation evaluation
at WMT?11 (number of words after tokenisation).
in July 2011. Preliminary results of this evaluation
are available on the Internet.4 Table 3 summarizes
the available training and development data. We op-
timized our systems on newstest2009 and used
newstest2010 as internal test set. For both cor-
pora, only one reference translations is available.
Scoring was performed with NIST?s implementation
of the BLEU score (?mt-eval? version 13).
3.1 Baseline system
The baseline system is a standard phrase-based SMT
system based on the the Moses SMT toolkit (Koehn
et al, 2007). It uses fourteen features functions
for translation, namely phrase and lexical translation
probabilities in both directions, seven features for
the lexicalized distortion model, a word and a phrase
penalty, and a target language model. It is con-
4http://matrix.statmt.org
structed as follows. First, word alignments in both
directions are calculated. We used a multi-threaded
version of the GIZA++ tool (Gao and Vogel, 2008).
Phrases and lexical reorderings are extracted using
the default settings of the Moses toolkit. All the bi-
texts were concatenated. The parameters of Moses
are tuned on the development data using the MERT
tool. For most of the runs, we performed three op-
timizations using different starting points and report
average results. English and French texts were to-
kenised using a modified version of the tools of the
Moses suite. Punctuation and case were preserved.
The language models were trained on all the avail-
able data, i.e. the target side of the bitexts, the whole
Gigaword corpus and the crawled monolingual data.
We build 4-gram back-off LMs with the SRI LM
toolkit using Modified Kneser-Ney and no cut-off
on all the n-grams. Past experience has shown that
keeping all n-grams slightly improves the perfor-
mance although this produces quite huge models
(10G and 30G of disk space for French and English
respectively).
Table 4 gives the baseline results using various
amounts of bitexts. Starting with the Europarl and
the News Commentary corpora, various amounts of
human translated data were added. The organizers
of the evaluation provide the so called 109 French-
English parallel corpus which contains almost 800
million words of data crawled from Canadian and
European Internet pages. Following works from the
2010 WMT evaluation (Lambert et al, 2010), we
filtered this data using IBM-1 probabilities and lan-
guage model scores to keep only the most reliable
translations. Two subsets were built with 115M and
232M English words respectively (using two differ-
288
alignment Dev Test
BLEU BLEU TER
giza 27.34 (0.01) 29.80 (0.06) 55.34 (0.06)
reused giza 27.40 (0.05) 29.82 (0.10) 55.30 (0.02)
reused moses 27.42 (0.02) 29.77 (0.06) 55.27 (0.03)
Table 5: Results for systems trained via different word alignment configurations. The values are the average over
3 MERT runs performed with different seeds. The numbers in parentheses are the standard deviation of these three
values. Translation was performed from English to French, adding 45M words of automatic translations (translated
from French to English) to the baseline system ?eparl+nc+crawled2?.
ent settings of the filter thresholds). They are re-
ferred to as ?crawled1? and ?crawled2? respectively.
Adding this data improved the BLEU score of al-
most 1 BLEU point (28.30 ? 29.27). This is our
baseline system to be improved by translation model
adaptation. Using the UN data gave no significant
improvement despite its huge size. This is probably
a typical example that it is not necessarily useful to
use all available parallel training data, in particular
when a very specific (out-of domain) jargon is used.
Consequently, the UN data was not used in the sub-
sequent experiments.
We were mainly working on the translation from
English to French. Therefore only one baseline sys-
tem was build for the reverse translation direction.
4 Experimental Evaluation
The system trained on Europarl, News Commen-
tary and the sub-sampled version of the 109 bitexts
(?eparl+nc+crawled2?, in the third line of Table 3),
was used to translate parts of the crawled news in
French and English. Statistics on the translated data
are given in Table 6.
We focused on the most recent data since the
time period of our development and test data was
end of 2008 and 2009 respectively. In the future
we will translate all the available monolingual data
and make it available to the community in order to
ease the widespread use of this kind of translation
model adaptation methods. These automatic trans-
lations were filtered using the sentence normalized
log-score of the decoder, as proposed by (Schwenk,
2008). However, we did not perform systematic ex-
periments to find the optimal threshold on this score,
but simply used a value which seems to be a good
compromise of quality and quantity of the transla-
tions. This gave us about 45M English words of
Corpus French (fe) English (ef)
available filtered available filtered
2009 92 31 121 45
2010 43 12 112 49
2011 8 2 15 6
total 219 45 177 100
Table 6: Monolingual data used to adapt the systems,
given in millions of English words. Under ?French (fe)?,
we indicated the number of translated English words
from French, and under ?English (ef)? we reported the
number of source English words translated into French.
Thus ?fe? and ?ef? refer respectively to French?English
and English?French translation direction of monolingual
data. In the experiments we used the 100M English?
French (ef) filtered monolingual data, as well as a 45M-
word subset (in order to have the same amount of data as
for French?English) and a 65M-word subset.
automatic translations from French, as well as the
translations into French of 100M English words, to
be used to adapt the baseline systems.
4.1 Word alignment
In order to build a phrase table with the translated
data, we re-used the word alignment obtained dur-
ing the translation with the moses toolkit. We com-
pared the system trained via these alignments to
the systems built by running GIZA++ on all the
data. When word alignments of the baseline corpus
(not adapted) are trained together with the translated
data, they could be affected by phrase pairs com-
ing from incorrect translations. To measure this ef-
fect, we trained an additional system, for which the
alignments of the baseline corpus are those trained
without the translated data. For the translated data,
we re-use the GIZA++ alignments trained on all the
data. Results for these three alignment configura-
289
baseline translated bitexts Dev Test
BLEU BLEU TER
Eparl + nc - 26.20 (0.06) 28.06 (0.22) 56.85 (0.09)
news fe 45M 27.18 (0.09) 29.03 (0.07) 55.97 (0.07)
news ef 45M 26.15 (0.04) 28.44 (0.09) 56.56 (0.11)
Eparl + nc + crawled2 - 26.95 (0.04) 29.29 (0.03) 55.77 (0.19)
news fe 45M 27.42 (0.02) 29.77 (0.06) 55.27 (0.03)
news ef 45M 26.75 (0.04) 28.88 (0.10) 56.06 (0.05)
Table 7: Translation results of the English?French systems augmented with a bitext obtained by translating news data
from English to French (ef) and French to English (fe). 45M refers to the number of English running words.
baseline translated bitexts Dev Test
BLEU BLEU TER
Eparl + nc - 26.70 (0.06) 27.41 (0.24) 55.07 (0.17)
news fe 45M 27.47 (0.08) 27.77 (0.23) 54.84 (0.13)
news ef 45M 27.55 (0.05) 28.51 (0.10) 54.12 (0.09)
news ef 65M 27.58 (0.03) 28.70 (0.09) 54.06 (0.17)
news ef 100M 27.63 (0.06) 28.68 (0.06) 54.02 (0.06)
Eparl + nc + crawled2 - 28.20 (0.03) 28.54 (0.12) 54.17 (0.15)
news fe 45M 28.02 (0.11) 28.40 (0.10) 54.45 (0.06)
news ef 45M 28.24 (0.06) 28.93 (0.22) 53.90 (0.08)
news ef 65M 28.16 (0.19) 28.75 (0.06) 54.03 (0.14)
news ef 100M 28.28 (0.09) 28.96 (0.03) 53.79 (0.09)
Table 8: Translation results of the French?English systems augmented with a bitext obtained by translating news data
from English to French (ef) and French to English (fe). 45M/65M/100M refers to the number of English running
words.
tions are presented in Table 5. In these systems
French sources and English translations (45 mil-
lion words) were added to the ?eparl+nc+crawled2?
baseline corpus. According to BLEU and TER met-
rics, reusing Moses alignments to build the adapted
phrase table has no significant impact on the system
performance. We repeated the experiment without
the 109 corpus and with the smaller selection of 109
(crawled1) and arrived to the same conclusion.
However, the re-use of Moses alignments saves time
and resources. On the larger baseline corpus, the
mGiza process lasted 46 hours with two jobs of 4
thread running and a machine with two Intel X5650
quad-core processors.
4.2 Choice of the translation direction
A second point under study in this work is the effect
of the translation direction of the monolingual data
used to adapt the translation model. Tables 7 and
8 present results for, respectively, English?French
and French?English systems adapted with news data
translated from English to French (ef) and French
to English (fe). The experiment was repeated with
two baseline corpora. The results show clearly
that target to source translated data are more use-
ful than source to target translated data. The im-
provement in terms of BLEU score due to the use of
target-to-source translated data instead of source-to-
target translated data ranges from 0.5 to 0.9 for the
French?English and English?French systems. For
instance, when translating from English to French
(Table 7), the baseline system ?eparl+nc? achieves
a BLEU score of 28.06 on the test set. This could
be improved to 29.03 using automatic translations
in the reverse direction (French to English), while
we only achieve a BLEU score of 28.44 when us-
ing automatic translation performed in the same di-
rection as the system to be adapted. The effect is
even clearer when we try to adapt the large system
290
?eparl+nc+crawled2?. Adding automatic transla-
tions translated from English-to-French did actually
lead to a lower BLEU score (29.29 ? 28.88) while
we observe an improvement of nearly 0.5 BLEU in
the other case.
With target-to-source translated news data,
the gain with respect to the baseline corpus
for English-French systems (Table 7) is nearly
1 BLEU for ?Eparl+nc? and 0.5 BLEU for
?Eparl+nc+crawled2?. With the same amount
of translated data (45 million English words),
approximately the same gains are observed in
French?English systems. Due to the larger avail-
ability of English news data, we were able to use
larger sets of target-to-source translated data for
French-English systems, as can be seen in Table 8.
With a bitext containing additionally 20 million
English words, we get a further improvement of
0.2 BLEU for ?Eparl+nc? (28.51 ? 28.70), but no
improvement for ?Eparl+nc+crawled2? (the BLEU
score is even lower, but the scores lie within the
error interval). No further gain on the test data is
achieved if we add again 35 million English words
(total of 100M words) to the system ?Eparl+nc?.
With the ?Eparl+nc+crawle2? baseline, no sig-
nificant improvement is observed if we adapt the
system with 100M words instead of only 45M.
4.3 Result analysis
To get more insight into what happens to the model
when we add the automatic translations, we cal-
culated some statistics of the phrase table, pre-
sented in Table 9. Namely, we calculated the
number of entries in the phrase table, the aver-
age number of translation options of each source
phrase, the average entropy for each source phrase,
the average source phrase length (in words) and
the average target phrase length. The entropy is
calculated over the probabilities of all translation
options for each source phrase. Comparing the
baseline with ?Eparl+nc? and the baseline with
?Eparl+nc+crawl2?, we can observe that the aver-
age number of translation options was nearly mul-
tiplied by 3 with the addition of 230 million words
of human translated bitexts. As a consequence the
average entropy was increased from 1.84 to 2.08.
On the contrary, adding 100 million words of in-
domain automatic translations, the average num-
ber of translation options increased by only 5%
for the ?Eparl+nc? baseline, and decreased for the
?Eparl+nc+crawl2? baseline. A decrease may occur
if new source phrases with less translation options
than the average are added. Furthermore, with the
addition of 45 million words of in-domain data, the
average entropy dropped from 1.84 to 1.33 or 1.60
for the ?Eparl+nc? baseline, and from 2.08 to 1.81 or
1.96 for the ?Eparl+nc+crawl2? baseline. With both
baselines, the more translations are added to the sys-
tem, the lower the entropy, although in some case
the number of translation options increases (this is
the case when we pass from 65M to 100M words
of synthetic data). These results illustrate the fact
that the automatic translations only reinforce some
probabilities in the model, with the subsequent de-
crease in entropy, while human translations add new
vocabulary. Note also that in the corpus using au-
tomatic translations, new words can only occur in
the source side. Thus when translating from French
to English, automatic translations from English to
French are expected to yield more translation op-
tions and a higher entropy than the automatic trans-
lations from French to English. This is what is ef-
fectively observed in Table 9.
5 Conclusion
Unsupervised training is widely used in other ar-
eas, in particular large vocabulary speech recogni-
tion. The statistical models in speech recognition
use a generative approach based on small units, usu-
ally triphones. Each triphone is modeled by a hid-
den Markov model and Gaussian mixture probabil-
ity distributions (plus many improvements like pa-
rameter tying etc). Many methods were developed
to adapt such models. The corresponding model
in statistical machine translation is the phrase table,
a long list of known words with their translations
and probabilities. It seems much more challenging
to adapt this kind of statistical model with unsuper-
vised training, i.e. monolingual data. Nevertheless,
we believe that unsupervised training can be also
very useful in SMT. To the best of our knowledge,
work in this area is very recent and only in its begin-
nings. This paper tries to give additional insights in
this promising method.
Our work is based on the approach initially pro-
291
baseline translated bitexts entries (M) translations entropy src size trg size
Eparl + nc - 7.16 83.83 1.84 1.80 2.81
news fe 45M 7.42 70.00 1.33 1.83 2.80
news ef 45M 8.24 81.58 1.60 1.86 2.79
news ef 65M 8.42 81.58 1.55 1.88 2.79
news ef 100M 9.21 85.93 1.54 1.90 2.79
Eparl + nc + crawl2 - 25.42 235.16 2.08 1.76 2.93
news fe 45M 25.54 217.21 1.81 1.77 2.93
news ef 45M 26.09 228.07 1.96 1.78 2.93
news ef 65M 26.21 226.45 1.91 1.78 2.93
news ef 100M 26.79 227.08 1.89 1.79 2.93
Table 9: Phrase table statistics for French?English systems augmented with bitexts built via automatic translations.
Only the entries useful to translate the development set were present in the considered phrase table.
posed in (Schwenk, 2008): build a first SMT sys-
tem, use it to translate large amounts of monolingual
data, filter the obtained translations, add them to the
bitexts and build a new system from scratch.
We proposed several extensions to this technique
which seem to improve the translations quality in
our experiments. First of all, we have observed that
it is clearly better to add automatically translated
texts to the translations model training data which
were translated from the target to the source lan-
guage. This seems to ensure that potentially wrong
translations are not used in the new model.
Second, we were able to skip the process of per-
forming word alignment of this additional parallel
data without any significant loss in the BLEU score.
Performing word alignments with GIZA++ can eas-
ily take several days when several hundred millions
of bitexts are available. Instead, we directly used the
word alignments produced by Moses when translat-
ing the monolingual data. This resulted in an appre-
ciable speed-up of the procedure, but has also inter-
esting theoretical aspects. Reusing the word align-
ment from the translation process is expected to re-
sult in a phrase extraction process that is more con-
sistent with the use of the phrases.
Finally, we outlined a method to automatically
add new translations without any additional parallel
training data. In fact, when translating from a mor-
phologically rich language to an easier one, in our
case from French to English, it is often possible to
infer the translations of unobserved morphological
forms of nouns, verbs or adjectives. This is obtained
by looking up the stemmed form in an automati-
cally constructed dictionary. This kind of approach
could be also applied to a classical PBSMT system,
by adding various forms to the phrase table, but it
is not obvious to come up with reasonable transla-
tions probabilities for these new entries. In our ap-
proach, the unknown word forms are processed in
large amounts of monolingual data and the induced
translations will appear in the context of complete
sentences. Wrong translations can be blocked by the
language model and the new translations can appear
in phrases of various lengths.
This paper provided a detailed experimental eval-
uation of these methods. We considered the trans-
lation between French and English using the same
data than was made available for the 2011 WMT
evaluation. Improvement of up to 0.5 BLEU were
observed with respect to an already competitive sys-
tem trained on more than 280M words of human
translated parallel data.
Acknowledgments
This work has been partially funded by the French
Government under the project COSMAT (ANR
ANR-09-CORD-004.) and the European Commis-
sion under the project EUROMATRIXPLUS (ICT-
2007.2.2-FP7-231720).
292
References
Nicola Bertoldi and Marcello Federico. 2009. Domain
adaptation for statistical machine translation. In Forth
Workshop on SMT, pages 182?189.
Nicola Bertoldi, Madalina Barbaiani, Marcello Federico,
and Roldano Cattoni. 2008. Phrase-based statistical
machine translation with pivot languages. In IWSLT,
pages 143?149.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for SMT self-
enhancement. In ACL, pages 157?160.
Jorge Civera and Alfons Juan. 2007. Domain adaptation
in statistical machine translation with mixture mod-
elling. In Second Workshop on SMT, pages 177?180,
June.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In EMNLP, pages 128?135.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In ACL 08.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In EMNLP, pages
967?975, Prague, Czech Republic.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Second Workshop on SMT, pages 224?227, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk.
2010. LIUM SMT machine translation system for
WMT 2010. In Workshop on SMT, pages 121?126.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Holger Schwenk and Jean Senellart. 2009. Translation
model adaptation for an Arabic/French news trans-
lation system by lightly-supervised training. In MT
Summit.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In ACL, pages 25?32.
Nicola Ueffing. 2006. Using monolingual source-
language data to improve MT performance. In IWSLT,
pages 174?181.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In ACL, pages 475?484, Uppsala, Sweden, July.
Association for Computational Linguistics.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Coling.
293
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 464?469,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIUM?s SMT Machine Translation Systems for WMT 2011
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Haithem Afli, Sadaf Abdul-Rauf and Kashif Shah
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French statisti-
cal machine translation systems for the 2011
WMT shared task evaluation. Our main sys-
tems were standard phrase-based statistical
systems based on the Moses decoder, trained
on the provided data only, but we also per-
formed initial experiments with hierarchical
systems. Additional, new features this year in-
clude improved translation model adaptation
using monolingual data, a continuous space
language model and the treatment of unknown
words.
1 Introduction
This paper describes the statistical machine trans-
lation systems developed by the Computer Science
laboratory at the University of Le Mans (LIUM) for
the 2011 WMT shared task evaluation. We only
considered the translation between French and En-
glish (in both directions). The main differences
with respect to previous year?s system (Lambert et
al., 2010) are as follows: use of more training data
as provided by the organizers, improved translation
model adaptation by unsupervised training, a con-
tinuous space language model for the translation
into French, some attempts to automatically induce
translations of unknown words and first experiments
with hierarchical systems. These different points are
described in the rest of the paper, together with a
summary of the experimental results showing the
impact of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used to
train the translation and language models of the sys-
tem.
2.1 Bilingual data
Our system was developed in two stages. First,
a baseline system was built to generate automatic
translations of some of the monolingual data avail-
able. These automatic translations were then used
directly with the source texts to create additional bi-
texts. In a second stage, these additional bilingual
data were incorporated into the system (see Sec-
tion 5 and Tables 4 and 5).
The latest version of the News-Commentary (NC)
corpus and of the Europarl (Eparl) corpus (version
6) were used. We also took as training data a sub-
set of the French?English Gigaword (109) corpus.
We applied the same filters as last year to select this
subset. The first one is a lexical filter based on the
IBM model 1 cost (Brown et al, 1993) of each side
of a sentence pair given the other side, normalised
with respect to both sentence lengths. This filter was
trained on a corpus composed of Eparl, NC, and UN
data. The other filter is an n-gram language model
(LM) cost of the target sentence (see Section 3), nor-
malised with respect to its length. This filter was
trained with all monolingual resources available ex-
cept the 109 data. We generated two subsets, both
by selecting sentence pairs with a lexical cost infe-
rior to 4, and an LM cost respectively inferior to 2.3
(1091, 115 million English words) and 2.6 (10
9
2, 232
million English words).
464
2.2 Use of Automatic Translations
Available human translated bitexts such as the Eu-
roparl or 109 corpus seem to be out-of domain for
this task. We used two types of automatically ex-
tracted resources to adapt our system to the task do-
main.
First, we generated automatic translations of the
provided monolingual News corpus and selected the
sentences with a normalised translation cost (re-
turned by the decoder) inferior to a threshold. The
resulting bitext contain no new translations, since
all words of the translation output come from the
translation model, but it contains new combinations
(phrases) of known words, and reinforces the prob-
ability of some phrase pairs (Schwenk, 2008). This
year, we improved this method in the following way.
In the original approach, the automatic translations
are added to the human translated bitexts and a com-
plete new system is build, including time consuming
word alignment with GIZA++. For WMT?11, we
directly used the word-to-word alignments produced
by the decoder at the output instead of GIZA?s align-
ments. This speeds-up the procedure and yields the
same results in our experiments. A detailed compar-
ison is given in (Lambert et al, 2011).
Second, as in last year?s evaluation, we automat-
ically extracted and aligned parallel sentences from
comparable in-domain corpora. We used the AFP
and APW news texts since there are available in the
French and English LDC Gigaword corpora. The
general architecture of our parallel sentence extrac-
tion system is described in detail by Abdul-Rauf and
Schwenk (2009). We first translated 91M words
from French into English using our first stage SMT
system. These English sentences were then used to
search for translations in the English AFP and APW
texts of the Gigaword corpus using information re-
trieval techniques. The Lemur toolkit (Ogilvie and
Callan, 2001) was used for this purpose. Search
was limited to a window of ?5 days of the date of
the French news text. The retrieved candidate sen-
tences were then filtered using the Translation Er-
ror Rate (TER) with respect to the automatic trans-
lations. In this study, sentences with a TER below
75% were kept. Sentences with a large length differ-
ence (French versus English) or containing a large
fraction of numbers were also discarded. By these
means, about 27M words of additional bitexts were
obtained.
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. In
addition, LDC?s Gigaword collection was used for
both languages. Data corresponding to the develop-
ment and test periods were removed from the Giga-
word collections.
2.4 Development data
All development was done on newstest2009, and
newstest2010 was used as internal test set. The de-
fault Moses tokenization was used. However, we
added abbreviations for the French tokenizer. All
our models are case sensitive and include punctua-
tion. The BLEU scores reported in this paper were
calculated with the tool multi-bleu.perl and are case
sensitive.
3 Architecture of the SMT system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sen-
tence f . Our main system is a phrase-based system
(Koehn et al, 2003; Och and Ney, 2003), but we
have also performed some experiments with a hier-
archical system (Chiang, 2007). Both use a log lin-
ear framework in order to introduce several models
explaining the translation process:
e? = argmax p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models
and the ?i weights are typically optimized to maxi-
mize a scoring function on a development set (Och
and Ney, 2002). The phrase-based system uses four-
teen features functions, namely phrase and lexical
translation probabilities in both directions, seven
features for the lexicalized distortion model, a word
and a phrase penalty and a target language model
(LM). The hierarchical system uses only 8 features:
a LM weight, a word penalty and six weights for the
translation model.
Both systems are based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
465
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++ that
can appear with rare words.
Phrases, lexical reorderings or hierarchical rules
are extracted using the default settings of the Moses
toolkit. The parameters of Moses were tuned on
newstest2009, using the ?new? MERT tool. We re-
peated the training process three times, each with a
different seed value for the optimisation algorithm.
In this way we have an rough idea of the error intro-
duced by the tuning process.
4-gram back-off LMs were used. The word list
contains all the words of the bitext used to train the
translation model and all words that appear at least
ten times in the monolingual corpora. Words of the
monolingual corpora containing special characters
or sequences of uppercase characters were not in-
cluded in the word list. Separate LMs were build on
each data source with the SRI LM toolkit (Stolcke,
2002) and then linearly interpolated, optimizing the
coefficients with an EM procedure. The perplexities
of these LMs were 99.4 for French and 129.7 for
English. In addition, we build a 5-gram continuous
space language model for French (Schwenk, 2007).
This model was trained on all the available French
texts using a resampling technique. The continu-
ous space language model is interpolated with the
4-gram back-off model and used to rescore n-best
lists. This reduces the perplexity by about 8% rela-
tive.
4 Treatment of unknown words
Finally, we propose a method to actually add new
translations to the system inspired from (Habash,
2008). For this, we propose to identity unknown
words and propose possible translations.
Moses has two options when encountering an un-
known word in the source language: keep it as it is
or drop it. The first option may be a good choice
for languages that use the same writing system since
the unknown word may be a proper name. The sec-
ond option is usually used when translating between
language based on different scripts, e.g. translating
1The source is available at http://www.cs.cmu.edu/
?qing/
Source language Source language Target language
French stemmed form English
finies fini finished
efface?s efface? erased
hawaienne hawaien Hawaiian
... ... ...
Table 1: Example of translations from French to English
which are automatically extracted from the phrase-table
with the stemmed form.
from Arabic to English. Alternatively, we propose to
infer automatically possible translations when trans-
lating from a morphologically rich language, to a
simpler language. In our case, we use this approach
to translate from French to English.
Several of the unknown words are actually adjec-
tives, nouns or verbs in a particular form that itself
is not known, but the phrase table would contain the
translation of a different form. As an example we
can mention the French adjective finies which is in
the female plural form. After stemming we may be
able to find the translation in a dictionary which is
automatically extracted from the phrase-table (see
Table 1). This idea was already outlined by (Bo-
jar and Tamchyna, 2011) to translate from Czech to
English.
First, we automatically extract a dictionary from
the phrase table. This is done, be detecting all 1-to-1
entries in the phrase table. When there are multi-
ple entries, all are kept with their lexical translations
probabilities. Our dictionary has about 680k unique
source words with a total of almost 1M translations.
source segment les travaux sont finis
target segment works are finis
stemmed word found fini
translations found finished, ended
segment proposed works are finished
works are ended
segment kept works are finished
Table 2: Example of the treatment of an unknown French
word and its automatically inferred translation.
The detection of unknown words is performed by
comparing the source and the target segment in order
to detect identical words. Once the unknown word
is selected, we are looking for its stemmed form in
the dictionary and propose some translations for the
unknown word based on lexical score of the phrase
table (see Table 2 for some examples). The snowball
466
Bitext #Fr Words PT size newstest2009 newstest2010
(M) (M) BLEU BLEU TER METEOR
Eparl+NC 56 7.1 26.74 27.36 (0.19) 55.11 (0.14) 60.13 (0.05)
Eparl+NC+1091 186 16.3 27.96 28.20 (0.04) 54.46 (0.10) 60.88 (0.05)
Eparl+NC+1092 323 25.4 28.20 28.57 (0.10) 54.12 (0.13) 61.20 (0.05)
Eparl+NC+news 140 8.4 27.31 28.41 (0.13) 54.15 (0.14) 61.13 (0.04)
Eparl+NC+1092+news 406 25.5 27.93 28.70 (0.24) 54.12 (0.16) 61.30 (0.20)
Eparl+NC+1092+IR 351 25.3 28.07 28.51 (0.18) 54.07 (0.06) 61.18 (0.07)
Eparl+NC+1092+news+IR 435 26.1 27.99 28.93 (0.02) 53.84 (0.07) 61.46 (0.07)
+larger beam+pruned PT 435 8.2 28.44 29.05 (0.14) 53.74 (0.16) 61.68 (0.09)
Table 4: French?English results: number of French words (in million), number of entries in the filtered phrase-table
(in million) and BLEU scores in the development (newstest2009) and internal test (newstest2010) sets for the different
systems developed. The BLEU scores and the number in parentheses are the average and standard deviation over 3
values (see Section 3)
corpus newstest2010 subtest2010
number of sentences 2489 109
number of words 70522 3586
number of UNK detected 118 118
nbr of sentences containing UNK 109 109
BLEU Score without UNK process 29.43 24.31
BLEU Score with UNK process 29.43 24.33
TER Score without UNK process 53.08 58.54
TER Score with UNK process 53.08 58.59
Table 3: Statistics of the unknown word (UNK) process-
ing algorithm on our internal test (newstest2010) and its
sub-part containing only the processed sentences (sub-
test2010).
stemmer2 was used. Then the different hypothesis
are evaluated with the target language model.
We processed the produced translations with this
method. It can happen that some words are transla-
tions of themselves, e.g. the French word ?duel? can
be translated by the English word ?duel?. If theses
words are present into the extracted dictionary, we
keep them. If we do not find any translation in our
dictionary, we keep the translation. By these means
we hope to keep named entities.
Several statistics made on our internal test (new-
stest2010) are shown in Table 3. Its shows that the
influence of the detected unknown words is minimal.
Only 0.16% of the words in the corpus are actually
unknown. However, the main goal of this process
is to increase the human readability and usefulness
without degrading automatic metrics. We also ex-
pect a larger impact in other tasks for which we have
2http://snowball.tartarus.org/
smaller amounts of parallel training data. In future
versions of this detection process, we will try to de-
tect unknown words before the translation process
and propose alternatives hypothesis to the Moses de-
coder.
5 Results and Discussion
The results of our SMT system for the French?
English and English?French tasks are summarized
in Tables 4 and 5, respectively. The MT metric
scores are the average of three optimisations per-
formed with different seeds (see Section 3). The
numbers in parentheses are the standard deviation
of these three values. The standard deviation gives
a lower bound of the significance of the difference
between two systems. If the difference between two
average scores is less than the sum of the standard
deviations, we can say that this difference is not sig-
nificant. The reverse is not true. Note that most of
the improvements shown in the tables are small and
not significant. However many of the gains are cu-
mulative and the sum of several small gains makes a
significant difference.
Baseline French?English System
The first section of Table 4 shows results of the de-
velopment of the baseline SMT system, used to gen-
erate automatic translations.
Although no French translations were generated,
we did similar experiments in the English?French
direction (first section of Table 5).
467
Bitext #En Words newstest2009 newstest2010
(M) BLEU BLEU TER
Eparl+NC 52 26.20 28.06 (0.22) 56.85 (0.08)
Eparl+NC+1091 167 26.84 29.08 (0.12) 55.83 (0.14)
Eparl+NC+1092 284 26.95 29.29 (0.03) 55.77 (0.19)
Eparl+NC+1092+news 299 27.34 29.56 (0.14) 55.44 (0.18)
Eparl+NC+1092+IR 311 27.14 29.43 (0.12) 55.48 (0.06)
Eparl+NC+1092+news+IR 371 27.32 29.73 (0.21) 55.16 (0.20)
+rescoring with CSLM 371 27.46 30.04 54.79
Table 5: English?French results: number of English words (in million) and BLEU scores in the development (new-
stest2009) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and the number
in parentheses are the average and standard deviation over 3 values (see Section 3.)
In both cases the best system is the one trained
on the Europarl, News-commentary and 1092 cor-
pora. This system was used to generate the auto-
matic translations. We did not observe any gain
when adding the United Nations data, so we dis-
carded this data.
Impact of the Additional Bitexts
With the baseline French?English SMT system (see
above), we translated the French News corpus to
generate an additional bitext (News). We also trans-
lated some parts of the French LDC Gigaword cor-
pus, to serve as queries to our IR system (see section
2.2). The resulting additional bitext is referred to as
IR. The second section of Tables 4 and 5 summarize
the system development including the additional bi-
texts.
With the News additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the baseline system used to generate the
automatic translations, but with less than half of
the data. Adding the News corpus to a larger cor-
pus, such as Eparl+NC+1092, has less impact but
still yields some improvement: 0.1 BLEU point in
French?English and 0.3 in English?French. Thus,
the News bitext translated from French to English
may have more impact when translating from En-
glish to French than in the opposite direction. This
effect is studied in detail in a separate paper (Lam-
bert et al, 2011). With the IR additional bitext added
to Eparl+NC+1092, we observe no improvement in
French to English, and a very small improvement
in English to French. However, added to the base-
line system (Eparl+NC+1092) adapted with the News
data, the IR additional bitexts yield a small (0.2
BLEU) improvement in both translation directions.
Final System
In both translation directions our best system was the
one trained on Eparl+NC+1092+News+IR. We fur-
ther achieved small improvements by pruning the
phrase-table and by increasing the beam size. To
prune the phrase-table, we used the ?sigtest-filter?
available in Moses (Johnson et al, 2007), more pre-
cisely the ??  filter3.
We also build hierarchical systems on the various
human translated corpora, using up to 323M words
(corpora Eparl+NC+1092). The systems yielded sim-
ilar results than the phrase-based approach, but re-
quired much more computational resources, in par-
ticular large amounts of main memory to perform
the translations. Running the decoder was actually
only possible with binarized rule-tables. Therefore,
the hierarchical system was not used in the evalua-
tion system.
3The p-value of two-by-two contingency tables (describing
the degree of association between a source and a target phrase)
is calculated with Fisher exact test. This probability is inter-
preted as the probability of observing by chance an association
that is at least as strong as the given one, and hence as its sig-
nificance. An important special case of a table occurs when a
phrase pair occurs exactly once in the corpus, and each of the
component phrases occurs exactly once in its side of the paral-
lel corpus (1-1-1 phrase pairs). In this case the negative log of
the p-value is ? = logN (N is number of sentence pairs in the
corpus). ? ?  is the largest threshold that results in all of the
1-1-1 phrase pairs being included.
468
6 Conclusions and Further Work
We presented the development of our statistical ma-
chine translation systems for the French?English
and English?French 2011 WMT shared task. In the
official evaluation the English?French system was
ranked first according to the BLEU score and the
French?English system second.
Acknowledgments
This work has been partially funded by the Euro-
pean Union under the EuroMatrixPlus project ICT-
2007.2.2-FP7-231720 and the French government
under the ANR project COSMAT ANR-09-CORD-
004.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 16?
23, Athens, Greece.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In ACL 08.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk.
2010. LIUM SMT machine translation system for
WMT 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 121?126, Uppsala, Sweden, July.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on translation
model adaptation using monolingual data. In Sixth
Workshop on SMT, page this volume.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments using
the Lemur toolkit. In In Proceedings of the Tenth Text
Retrieval Conference (TREC-10), pages 103?108.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proc. of the Int. Conf. on Spoken Lan-
guage Processing, pages 901?904, Denver, CO.
469
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 369?373,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LIUM?s SMT Machine Translation Systems for WMT 2012
Christophe Servan, Patrik Lambert, Anthony Rousseau,
Holger Schwenk and Lo??c Barrault
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French statisti-
cal machine translation systems for the 2012
WMT shared task evaluation. We developed
phrase-based systems based on the Moses de-
coder, trained on the provided data only. Ad-
ditionally, new features this year included im-
proved language and translation model adap-
tation using the cross-entropy score for the
corpus selection.
1 Introduction
This paper describes the statistical machine trans-
lation systems developed by the Computer Science
laboratory at the University of Le Mans (LIUM) for
the 2012 WMT shared task evaluation. We only
considered the translation between French and En-
glish (in both directions). The main differences with
respect to previous year?s system (Schwenk et al,
2011) are as follows: (i) use of more training data as
provided by the organizers and (ii) better selection
of the monolingual and parallel data according to
the domain, using the cross-entropy difference with
respect to in-domain and out-of-domain language
models (Moore and Lewis, 2010). We kept some
previous features: the improvement of the transla-
tion model adaptation by unsupervised training, a
parallel corpus retrieved by Information Retrieval
(IR) techniques and finally, the rescoring with a con-
tinuous space target language model for the trans-
lation into French. These different points are de-
scribed in the rest of the paper, together with a sum-
mary of the experimental results showing the impact
of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used to
train the translation and language models of the sys-
tem.
2.1 Bilingual data
The latest version of the News-Commentary (NC)
corpus and of the Europarl (Eparl) corpus (version
7) were used. We also took as training data a subset
of the French?English Gigaword (109) corpus. This
year we changed the filters applied to select this sub-
set (see Sect. 2.4). We also included in the training
data the test sets from previous shared tasks, that we
called the ntsXX corpus and which was composed
of newstest2008, newstest2009, newssyscomb2009.
2.2 Development data
Development was initially done on newstest2010,
and newstest2011 was used as internal test set (Sec-
tion 3.1). The development and internal test sets
were then (Section 4) switched (tuning was done
on newstest2011 and internal evaluation on new-
stest2010). The default Moses tokenization was
used. However, we added abbreviations for the
French tokenizer. All our models are case sensitive
and include punctuation. The BLEU scores reported
in this paper were calculated with the mteval-v13
tool and are case insensitive.
2.3 Use of Automatic Translations
Available human translated bitexts such as the Eu-
roparl or 109 corpus seem to be out-of domain for
this task. We used two types of automatically ex-
tracted resources to adapt our system to the domain.
369
First, we generated automatic translations of the
provided monolingual News corpus in French and
English, for years 2009, 2010 and 2011, and selected
the sentences with a normalised translation cost (re-
turned by the decoder) inferior to a threshold. The
resulting bitexts contain no new translations, since
all words of the translation output come from the
translation model, but they contain new combina-
tions (phrases) of known words, and reinforce the
probability of some phrase pairs (Schwenk, 2008).
Like last year, we directly used the word-to-word
alignments produced by the decoder at the output
instead of GIZA?s alignments. This speeds-up the
procedure and yields the same results in our experi-
ments. A detailed comparison is given in (Lambert
et al, 2011).
Second, as in last year?s evaluation, we auto-
matically extracted and aligned parallel sentences
from comparable in-domain corpora. We used the
AFP (Agence France Presse) and APW (Associated
Press Worldstream Service) news texts since there
are available in the French and English LDC Giga-
word corpora. The general architecture of our par-
allel sentence extraction system is described in de-
tail by Abdul-Rauf and Schwenk (2009). We first
translated 91M words from French into English us-
ing our first stage SMT system. These English sen-
tences were then used to search for translations in
the English AFP and APW texts of the Gigaword
corpus using information retrieval techniques. The
Lemur toolkit (Ogilvie and Callan, 2001) was used
for this purpose. Search was limited to a window of
?5 days of the date of the French news text. The re-
trieved candidate sentences were then filtered using
the Translation Error Rate (TER) with respect to the
automatic translations. In this study, sentences with
a TER below 75% were kept. Sentences containing
a large fraction of numbers were discarded. By these
means, about 27M words of additional bitexts were
obtained.
2.4 Domain-based Data selection
Before training the target language models, a text se-
lection has been made using the cross-entropy differ-
ence method (Moore and Lewis, 2010). This tech-
nique works by computing the difference between
two cross-entropy values.
We first score an out-of-domain corpus against
a language model trained on a set of in-domain
data and compute the cross-entropy for each sen-
tence. Then, we score the same out-of-domain cor-
pus against a language model trained on a random
sample of itself, with a size roughly equal to the in-
domain corpus. From this point, the difference be-
tween in-domain cross-entropy and out-of-domain
cross-entropy is computed for each sentence, and
these sentences are sorted regarding this score.
By estimating and minimizing on a development
set the perplexity of several percentages of the sorted
out-of-domain corpus, we can then estimate the the-
oretical best point of data size for this specific cor-
pus. According the original paper and given our re-
sults, this leads to better selection than the simple
perplexity sorting (Gao et al, 2002). This way, we
can be assured to discard the vast majority of noise
in the corpora and to select data well-related to the
task.
In this task, the French and English target lan-
guage models were trained on data selected from all
provided monolingual corpora. In addition, LDC?s
Gigaword collection was used for both languages.
Data corresponding to the development and test pe-
riods were removed from the Gigaword collections.
We had time to apply the domain-based data selec-
tion only for French. Thus all data were used for
English.
We used this method to filter the French?English
109 parallel corpus as well, based on the differ-
ence between in-domain cross-entropy and out-of-
domain cross-entropy calculated for each sentence
of the English side of the corpus. We kept 49 mil-
lion words (in the English side) to train our models,
called 109f .
3 Architecture of the SMT system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sentence
f . We have build phrase-based systems (Koehn et
al., 2003; Och and Ney, 2003), using the standard
log linear framework in order to introduce several
models explaining the translation process:
e? = argmax p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
370
The feature functions hi are the system models
and the ?i weights are typically optimized to maxi-
mize a scoring function on a development set (Och,
2003). The phrase-based system uses fourteen fea-
tures functions, namely phrase and lexical transla-
tion probabilities in both directions, seven features
for the lexicalized distortion model, a word and a
phrase penalty and a target language model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and is constructed as follows.
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++ that
can appear with rare words.
Phrases and lexical reorderings are extracted us-
ing the default settings of the Moses toolkit. The
parameters of Moses were tuned using the MERT
tool. We repeated the training process three times,
each with a different seed value for the optimisation
algorithm. In this way we have a rough idea of the
error introduced by the tuning process.
4-gram back-off LMs were used. The word list
contains all the words of the bitext used to train the
translation model and all words that appear at least
ten times in the monolingual corpora. Words of the
monolingual corpora containing special characters
or sequences of uppercase characters were not in-
cluded in the word list. Separate LMs were build
on each data source with the SRI LM toolkit (Stol-
cke, 2002) and then linearly interpolated, optimizing
the coefficients with an EM procedure. The perplex-
ities of these LMs on newstest2011 were 119.1 for
French and 174.8 for English. In addition, we build a
5-gram continuous space language model for French
(Schwenk, 2007). These models were trained on
all the available texts using a resampling technique.
The continuous space language model is interpo-
lated with the 4-gram back-off model and used to
rescore n-best lists. This reduces the perplexity by
about 13% relative.
3.1 Number translation
We have also performed some experiments with
number translation. English and French do not use
1The source is available at http://www.cs.cmu.edu/
?qing/
the same conventions for integer and decimal num-
bers. For example, the English decimal number 0.99
is translated in French by 0,99. In the same way,
the English integer 32,000 is translated in French by
32 000. It should be possible to perform these mod-
ifications by rules.
In this study, we first replaced the numbers by a
tag @@NUM for integer and @@DEC for decimal num-
bers. Integers in the range 1 to 31 were not replaced
since they appear in dates. Then, we created the tar-
get language model using the tagged corpora. Ta-
ble 1 shows results of experiments performed with
and without rule-based number translation.
Corpus NT BLEU TER
NC no 26.57 (0.07) 58.13 (0.06)
NC yes 26.84 (0.15) 57.71 (0.34)
Eparl+NC no 29.28 (0.11) 55.28 (0.13)
Eparl+NC yes 29.26 (0.10) 55.44 (0.29)
Table 1: Results of the study on number translation (NT)
from English to French
We did observe small gains in the translation
quality when only the news-commentary bitexts are
used, but there were no differences when more train-
ing data is available. Due to time constraints, this
procedure was not used in the submitted system.
4 Results and Discussion
The results of our SMT systems are summarized in
Table 2. The MT metric scores for the development
set are the average of three optimisations performed
with different seeds (see Section 3). For the test set,
they are the average of four values: the three val-
ues corresponding to these different optimisations,
plus a fourth value obtained by taking as weight for
each model, the average of the weights obtained in
the three optimisations (Cettolo et al, 2011). The
numbers in parentheses are the standard deviation of
these three or four values. The standard deviation
gives a lower bound of the significance of the differ-
ence between two systems. If the difference between
two average scores is less than the sum of the stan-
dard deviations, we can say that this difference is not
significant. The reverse is not true.
The results of Table 2 show that adding several
adapted corpora (the filtered 109 corpus, the syn-
371
Bitext #Source newstest2011 newstest2010
Words (M) BLEU TER BLEU TER
Translation : En?Fr
Eparl+NC 57 30.91 (0.05) 53.61 (0.12) 28.45 (0.08) 56.29 (0.20)
Eparl+NC+ntsXX 58 31.12 (0.08) 53.67 (0.08) 28.49 (0.04) 56.45 (0.12)
Eparl+NC+ntsXX+109f 107 31.67 (0.06) 53.29 (0.03) 29.38 (0.12) 55.45 (0.15)
Eparl+NC+ntsXX+109f+IR 133 32.41 (0.02) 52.20 (0.02) 29.48 (0.11) 55.33 (0.20)
Eparl+NC+ntsXX+109f+news+IR 162 32.26 (0.04) 52.24 (0.12) 29.79 (0.12) 55.04 (0.20)
Translation : Fr?En
Eparl+NC 64 29.59 (0.12) 51.86 (0.06) 28.12 (0.05) 53.19 (0.06)
Eparl+NC+ntsXX 64 29.59 (0.04) 51.89 (0.14) 28.32 (0.08) 53.22 (0.08)
Eparl+NC+ntsXX+109f 120 30.69 (0.06) 50.77 (0.04) 28.95 (0.14) 52.62 (0.14)
Eparl+NC+ntsXX+109f+IR 149 30.56 (0.02) 50.94 (0.15) 28.67 (0.11) 52.78 (0.06)
Eparl+NC+ntsXX+109f+news+IR 179 30.85 (0.07) 50.72 (0.03) 28.94 (0.05) 52.57 (0.02)
Table 2: English?French and French?English results: number of source words (in million) and scores on the develop-
ment (newstest2011) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and
the number in parentheses are the average and standard deviation over 3 or 4 values when available (see Section 4.)
thetic corpus and the corpus retrieved via IR meth-
ods) to the Eparl+NC+ntsXX baseline, a gain of 1.1
BLEU points and 1.4 TER points was achieved for
the English?French system.
On the other hand, adding the bitexts extracted
from the comparable corpus (IR) does actually hurt
the performance of the French?English system: the
BLEU score decreases from 28.95 to 28.67 on our
internal test set. During the evaluation period, we
added all the corpora at once and we observed this
only in our analysis after the evaluation.
In both translation directions our
best system was the one trained on
Eparl+NC+ntsXX+109f+News+IR. Finally, we
applied a continuous space language model for the
system translating into French.
Acknowledgments
This work has been partially funded by the Euro-
pean Union under the EuroMatrixPlus project ICT-
2007.2.2-FP7-231720 and the French government
under the ANR project COSMAT ANR-09-CORD-
004.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 16?
23, Athens, Greece.
Mauro Cettolo, Nicola Bertoldi, and Marcello Federico.
2011. Methods for smoothing the optimizer instability
in SMT. In Proc. of Machine Translation Summit XIII,
Xiamen, China.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to statistical
language modeling for chinese. In ACM Transactions
on Asian Language Information Processing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on translation
372
model adaptation using monolingual data. In Sixth
Workshop on SMT, pages 284?293.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of the ACL 2010 Conference Short Papers.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
Paul Ogilvie and Jamie Callan. 2001. Experiments using
the Lemur toolkit. In In Proceedings of the Tenth Text
Retrieval Conference (TREC-10), pages 103?108.
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Sadaf Abdul-Rauf, Haithem Afli,
and Kashif Shah. 2011. Lium?s smt machine trans-
lation systems for WMT 2011. In Proceedings of
the Sixth Workshop on Statistical Machine Translation,
pages 464?469, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proc. of the Int. Conf. on Spoken Lan-
guage Processing, pages 901?904, Denver, CO.
373
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 1?6,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Workshop on Hybrid Approaches to Translation:
Overview and Developments
Marta R. Costa-jussa`, Rafael E. Banchs
Institute for Infocomm Research1
Patrik Lambert
Barcelona Media3
Kurt Eberle
Lingenio GmbH4
Reinhard Rapp
Aix-Marseille Universite?, LIF2
Bogdan Babych
University of Leeds5
1{vismrc,rembanchs}@i2r.a-star.edu.sg, 2reinhardrapp@gmx.de,
3patrik.lambert@barcelonamedia.org, 4k.eberle@lingenio.de,
5b.babych@leeds.ac.uk
Abstract
A current increasing trend in machine
translation is to combine data-driven and
rule-based techniques. Such combinations
typically involve the hybridization of dif-
ferent paradigms such as, for instance,
the introduction of linguistic knowledge
into statistical paradigms, the incorpora-
tion of data-driven components into rule-
based paradigms, or the pre- and post-
processing of either sort of translation sys-
tem outputs. Aiming at bringing together
researchers and practitioners from the dif-
ferent multidisciplinary areas working in
these directions, as well as at creating a
brainstorming and discussion venue for
Hybrid Translation approaches, the Hy-
Tra initiative was born. This paper gives
an overview of the Second Workshop on
Hybrid Approaches to Translation (HyTra
2013) concerning its motivation, contents
and outcomes.
1 Introduction
Machine translation (MT) has continuously been
evolving from different perspectives. Early sys-
tems were basically dictionary-based. These ap-
proaches were further developed to more complex
systems based on analysis, transfer and genera-
tion. The objective was to climb up (and down)
in the well-known Vauquois pyramid (see Figure
1) to facilitate the transfer phase or to even mini-
mize the transfer by using an interlingua system.
But then, corpus-based approaches irrupted, gen-
erating a turning point in the field by putting aside
the analysis, generation and transfer phases.
Although there had been such a tendency right
from the beginning (Wilks, 1994), in the last
Figure 1: Vauquois pyramid (image from
Wikipedia).
years, the corpus-based approaches have reached
a point where many researchers assume that rely-
ing exclusively on data might have serious limi-
tations. Therefore, research has focused either on
syntactical/hierarchical-based methods or on try-
ing to augment the popular phrase-based systems
by incorporating linguistic knowledge. In addi-
tion, and given the fact that research on rule-based
has never stopped, there have been several propos-
als of hybrid architectures combining both rule-
based and data-driven approaches.
In summary, there is currently a clear trend to-
wards hybridization, with researchers adding mor-
phological, syntactic and semantic knowledge to
statistical systems, as well as combining data-
driven methods with existing rule-based systems.
In this paper we provide a general overview
of current approaches to hybrid MT within the
context of the Second Workshop on Hybrid Ap-
proaches to Translation (HyTra 2013). In our
overview, we classify hybrid MT approaches ac-
cording to the linguistic levels that they address.
We then briefly summarize the contributions pre-
sented and collected in this volume.
1
The paper is organized as follows. First, we mo-
tivate and summarize the main aspects of the Hy-
Tra initiative. Then, we present a general overview
of the accepted papers and discuss them within
the context of other state-of-the-art research in the
area. Finally, we present our conclusions and dis-
cuss our proposed view of future directions for
Hybrid MT research.
2 Overview of the HyTra Initiative
The HyTra initiative started in response to the in-
creasing interest in hybrid approaches to machine
translation, which is reflected on the substantial
amount of work conducted on this topic. An-
other important motivation was the observation
that, up to now, no single paradigm has been able
to successfully solve to a satisfactory extent all of
the many challenges that the problem of machine
translation poses.
The first HyTra workshop took part in conjunc-
tion with the EACL 2012 conference (Costa-jussa`
et al, 2012). The Second HyTra Workshop, which
was co-organized by the authors of this paper, has
been co-located with the ACL 2013 conference
(Costa-jussa` et al, 2013). The workshop has been
supported by an extensive programme committee
comprising members from over 30 organizations
and representing more than 20 countries. As the
outcome of a comprehensive peer reviewing pro-
cess, and based on the recommendations of the
programme committee, 15 papers were finally se-
lected for either oral or poster presentation at the
workshop.
The workshop also had the privilege to be hon-
ored by two exceptional keynote speeches:
? Controlled Ascent: Imbuing Statistical MT
with Linguistic Knowledge by Will Lewis and
Chris Quirk (2013), Microsoft research. The
intersection of rule-based and statistical ap-
proaches in MT is explored, with a particular
focus on past and current work done at Mi-
crosoft Research. One of their motivations
for a hybrid approach is the observation that
the times are over when huge improvements
in translation quality were possible by sim-
ply adding more data to statistical systems.
The reason is that most of the readily avail-
able parallel data has already been found.
? How much hybridity do we have? by Her-
mann Ney, RWTH Aachen. It is pointed
out that after about 25 years the statistical
approach to MT has been widely accepted
as an alternative to the classical approach
with manually designed rules. But in prac-
tice most statistical MT systems make use
of manually designed rules at least for pre-
processing in order to improve MT quality.
This is exemplified by looking at the RWTH
MT systems.
3 Hybrid Approaches Organized by
Linguistic Levels
?Hybridization? of MT can be understood as com-
bination of several MT systems (possibly of very
different architecture) where the single systems
translate in parallel and compete for the best re-
sult (which is chosen by the integrating meta sys-
tem). The workshop and the papers do not fo-
cus on this ?coarse-grained? hybridization (Eisele
et al, 2008), but on a more ?fine grained? one
where the systems mix information from differ-
ent levels of linguistic representations (see Fig-
ure 2). In the past and mostly in the framework
of rule-based machine translation (RBMT) it has
been experimented with information from nearly
every level including phonetics and phonology
for speech recognition and synthesis in speech-
to-speech systems (Wahlster, 2000) and includ-
ing pragmatics for dialog translation (Batliner et
al., 2000a; Batliner et al, 2000b) and text coher-
ence phenomena (Le Nagard and Koehn, 2010).
With respect to work with emphasis on statisti-
cal machine translation (SMT) and derivations of
it mainly those information levels have been used
that address text in the sense of sets of sentences.
As most of the workshop papers relate to this
perspective - i.e. on hybridization which is de-
fined using SMT as backbone, in this introduc-
tion we can do with distinguishing between ap-
proaches focused on morphology, syntax, and se-
mantics. There are of course approaches which
deal with more than one of these levels in an in-
tegrated manner, which are commonly refered to
as multilevel approaches. As the case of treat-
ing syntax and morphology concurrently is espe-
cially common, we also consider morpho-syntax
as a separate multilevel approach.
3.1 Morphological approaches
The main approaches of statistical MT that ex-
ploit morphology can be classified into segmen-
tation, generation, and enriching approaches. The
2
Figure 2: Major linguistic levels (image from
Wikipedia).
first one attempts to minimize the vocabulary of
highly inflected languages in order to symmetrize
the (lexical granularity of the) source and the tar-
get language. The second one assumes that, due
to data sparseness, not all morphological forms
can be learned from parallel corpora and, there-
fore, proposes techniques to learn new morpho-
logical forms. The last one tries to enrich poorly
inflected languages to compensate for their lack of
morphology. In HyTra 2013, approaches treating
morphology were addressed by the following con-
tributions:
? Toral (2013) explores the selection of data to
train domain-specific language models (LM)
from non-domain specific corpora by means
of simplified morphology forms (such as
lemmas). The benefit of this technique is
tested using automatic metrics in the English-
to-Spanish task. Results show an improve-
ment of up to 8.17% of perplexity reduction
over the baseline system.
? Rios Gonzalez and Goehring (2013) propose
machine learning techniques to decide on the
correct form of a verb depending on the con-
text. Basically they use tree-banks to train the
classifiers. Results show that they are able
to disambiguate up to 89% of the Quechua
verbs.
3.2 Syntactic approaches
Syntax had been addressed originally in SMT in
the form of so called phrase-based SMT with-
out any reference to linguistic structures; during
the last decade (or more) the approach evolved
to or, respectively, was complemented by - work
on syntax-based models in the linguistic sense of
the word. Most such approaches can be classi-
fied into three different types of architecture that
are defined by the type of syntactic analysis used
for the source language and the type of generation
aimed at for the target language: tree-to-tree, tree-
to-string and string-to-tree. Additionally, there
are also the so called hierarchical systems, which
combine the phrase-based and syntax-based ap-
proaches by using phrases as translation-units and
automatically generated context free grammars as
rules. Approaches dealing with the syntactic ap-
proach in HyTra 2013 include the following pa-
pers:
? Green and Zabokrtsky? (2013) study three dif-
ferent ways to ensemble parsing techniques
and provide results in MT. They compute cor-
relations between parsing quality and transla-
tion quality, showing that NIST is more cor-
related than BLEU.
? Han et al (2013) provide a framework for
pre-reordering to make Chinese word order
more similar to Japanese. To this purpose,
they use unlabelled dependency structures of
sentences and POS tags to identify verbal
blocks and move them from after-the-object
positions (SVO) to before-the-object posi-
tions (SOV).
? Nath Patel et al (2013) also propose a pre-
reordering technique, which uses a limited
set of rules based on parse-tree modification
rules and manual revision. The set of rules is
specifically listed in detail.
? Saers et al (2013) report an unsupervised
learning model that induces phrasal ITGs by
breaking rules into smaller ones using mini-
mum description length. The resulting trans-
lation model provides a basis for generaliza-
tion to more abstract transduction grammars
with informative non-terminals.
3.3 Morphosyntactical approaches
In linguistic theories, morphology and syntax are
often considered and represented simultaneously
(not only in unification-based approaches) and the
same is true for MT systems.
3
? Laki et al (2013) combine pre-reordering
rules with morphological and factored mod-
els for English-to-Turkish.
? Li et al (2013) propose pre-reordering rules
to be used for alignment-based reordering,
and corresponding POS-based restructuring
of the input. Basically, they focus on tak-
ing advantage of the fact that Korean has
compound words, which - for the purpose of
alignment - are split and reordered similarly
to Chinese.
? Turki Khemakhem et al (2013) present
work about an English-Arabic SMT sys-
tem that uses morphological decomposition
and morpho-syntactic annotation of the target
language and incorporates the correspond-
ing information in a statistical feature model.
Essentially, the statistical feature language
model replaces words by feature arrays.
3.4 Semantic approaches
The introduction of semantics in statistical MT has
been approached to solve word sense disambigua-
tion challenges covering the area of lexical seman-
tics and, more recently, there have been different
techniques using semantic roles covering shallow
semantics, as well as the use of distributional se-
mantics for improving translation unit selection.
Approaches treating the incorporation of seman-
tics into MT in HyTra 2013 include the following
research work:
? Rudnick et al (2013) present a combina-
tion of Maximum Entropy Markov Models
and HMM to perform lexical selection in
the sense of cross-lingual word sense disam-
biguation (i.e. by choice from the set of trans-
lation alternatives). The system is meant to
be integrated into a RBMT system.
? Boujelbane (2013) proposes to build a bilin-
gual lexicon for the Tunisian dialect us-
ing modern standard Arabic (MSA). The
methodology is based on leveraging the large
available annotated MSA resources by ex-
ploiting MSA-dialect similarities and ad-
dressing the known differences. The author
studies morphological, syntactic and lexical
differences by exploiting Penn Arabic Tree-
bank, and uses the differences to develop
rules and to build dialectal concepts.
? Bouillon et al (2013) presents two method-
ologies to correct homophone confusions.
The first one is based on hand-coded rules
and the second one is based on weighted
graphs derived from a pronunciation re-
source.
3.5 Other multilevel approaches
In a number of linguistic theories information
from the morphological, syntactic and semantic
level is considered conjointly and merged in cor-
responding representations (a RBMT example is
LFG (Lexical Functional Grammars) analysis and
the corresponding XLE translation architecture).
In HyTra 2013 there are three approaches dealing
with multilevel information:
? Pal et al (2013) propose a combination of
aligners: GIZA++, Berkeley and rule-based
for English-Bengali.
? Hsieh et al (2013) use comparable corpora
extracted from Wikipedia to extract parallel
fragments for the purpose of extending an
English-Bengali training corpus.
? Tambouratzis et al (2013) describe a hybrid
MT architecture that uses very few bilingual
corpus and a large monolingual one. The
linguistic information is extracted using
pattern recognition techniques.
Table 1 summarizes the papers that have been
presented in the Second HyTra Workshop. The
papers are arranged into the table according to the
linguistic level they address.
4 Conclusions and further work
The success of the Second HyTra Workshop con-
firms that research in hybrid approaches to MT
systems is a very active and promising area. The
MT community seems to agree that pure data-
driven or rule-based paradigms have strong lim-
itations and that hybrid systems are a promising
direction to overcome most of these limitations.
Considerable progress has been made in this area
recently, as demonstrated by consistent improve-
ments for different language pairs and translation
tasks.
The research community is working hard, with
strong collaborations and with more resources at
hand than ever before. However, it is not clear
4
Morphological (Toral, 2013) Hybrid Selection of LM Training Data Using Linguistic Information and Perplexity
(Gonzales and Goehring, 2013) Machine Learning disambiguation of Quechua verb morphology
Syntax (Green and Zabokrtsky?, 2013) Improvements to SBMT using Ensemble Dependency Parser
(Han et al, 2013) Using unlabeled dependency parsing for pre-reordering for Chinese-to-Japanese SMT
(Patel et al, 2013) Reordering rules for English-Hindi SMT
(Saers et al, 2013) Unsupervised transduction grammar induction via MDL
Morpho-syntactic (Laki et al, 2013) English to Hungarian morpheme-based SMT system with reordering rules
(Li et al, 2013) Experiments with POS-based restructuring and alignment based reordering for SMT
(Khemakhem et al, 2013) Integrating morpho-syntactic feature for English Arabic SMT
Semantic (Rudnick and Gasser, 2013) Lexical Selection for Hybrid MT with Sequence Labeling
(Boujelbane et al, 2013) Building bilingual lexicon to create dialect Tunisian corpora and adapt LM
(Bouillon et al, 2013) Two approaches to correcting homophone confusions in a hybrid SMT based system
Multilevels (Pal et al, 2013) A hybrid Word alignment model for PBSMT
(Hsieh et al, 2013) Uses of monolingual in-domain corpora for cross-domain adaptation with hybrid MT approaches
(Tambouratzis et al, 2013) Overview of a language-independent hybrid MT methodology
Table 1: HyTra 2013 paper overview.
whether technological breakthroughs as in the past
are still possible are still possible, or if MT will be
turning into a research field with only incremen-
tal advances. The question is: have we reached
the point at which only refinements to existing ap-
proaches are needed? Or, on the contrary, do we
need a new turning point?
Our guess is that, similar to the inflection point
giving rise to the statistical MT approach during
the last decade of the twentieth century, once again
there might occur a new discovery which will rev-
olutionize further the research on MT. We cannot
know whether hybrid approaches will be involved;
but, in any case, this seems to be a good and smart
direction as it is open to the full spectrum of ideas
and, thus, it should help to push the field forward.
Acknowledgments
This workshop has been supported by the Sev-
enth Framework Program of the European Com-
mission through the Marie Curie actions HyghTra,
IMTraP, AutoWordNet and CrossLingMind and
the Spanish ?Ministerio de Econom??a y Competi-
tividad? and the European Regional Development
Fund through SpeechTech4all. We would like to
thank the funding institution and all people who
contributed towards making the workshop a suc-
cess. For a more comprehensive list of acknowl-
edgments refer to the preface of this volume.
References
Anton Batliner, J. Buckow, Heinrich Niemann, Elmar
No?th, and Volker Warnke, 2000a. The Prosody
Module, pages 106?121. New York, Berlin.
Anton Batliner, Richard Huber, Heinrich Niemann, El-
mar No?th, Jo?rg Spilker, and K. Fischer, 2000b. The
Recognition of Emotion, pages 122?130. New York,
Berlin.
Pierrette Bouillon, Johanna Gerlach, Ulrich Germann,
Barry Haddow, and Manny Rayner. 2013. Two ap-
proaches to correcting homophone confusions in a
hybrid machine translation system. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Rahma Boujelbane, Mariem Ellouze khemekhem, Si-
war BenAyed, and Lamia HadrichBelguith. 2013.
Building bilingual lexicon to create dialect tunisian
corpora and adapt language model. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Marta R. Costa-jussa`, Patrik Lambert, Rafael E.
Banchs, Reinhard Rapp, and Bogdan Babych, edi-
tors. 2012. Proceedings of the Joint Workshop on
Exploiting Synergies between Information Retrieval
and Machine Translation (ESIRMT) and Hybrid Ap-
proaches to Machine Translation (HyTra). As-
sociation for Computational Linguistics, Avignon,
France, April.
Marta R. Costa-jussa`, Patrik Lambert, Rafael E.
Banchs, Reinhard Rapp, Bogdan Babych, and Kurl
Eberle, editors. 2013. Proceedings of the Sec-
ond Workshop on Hybrid Approaches to Translation
(HyTra). Association for Computational Linguis-
tics, Sofia, Bulgaria, August.
Andreas Eisele, Christian Federmann, Hans Uszkoreit,
Herve? Saint-Amand, Martin Kay, Michael Jelling-
haus, Sabine Hunsicker, Teresa Herrmann, and
Yu Chen. 2008. Hybrid machine translation archi-
tectures within and beyond the euromatrix project.
In John Hutchins and Walther v.Hahn, editors, 12th
annual conference of the European Association for
Machine Translation (EAMT), pages 27?34, Ham-
burg, Germany.
Annette Rios Gonzales and Anne Goehring. 2013.
Machine learning disambiguation of quechua verb
morphology. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Nathan Green and Zdenek Zabokrtsky?. 2013. Im-
provements to syntax-based machine translation us-
ing ensemble dependency parsers. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
5
Dan Han, Pascual Martinez-Gomez, Yusuke Miyao,
Katsuhito Sudoh, and Masaaki NAGATA. 2013.
Using unlabeled dependency parsing for pre-
reordering for chinese-to-japanese statistical ma-
chine translation. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation, Sofia.
An-Chang Hsieh, Hen-Hsen Huang, and Hsin-Hsi
Chen. 2013. Uses of monolingual in-domain cor-
pora for cross-domain adaptation with hybrid mt ap-
proaches. In ACL Workshop on Hybrid Machine Ap-
proaches to Translation, Sofia.
Ines Turki Khemakhem, Salma Jamoussi, and Abdel-
majid Ben Hamadou. 2013. Integrating morpho-
syntactic feature in english-arabic statistical ma-
chine translation. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation, Sofia.
La?szlo? Laki, Attila Novak, and Borba?la Siklo?si. 2013.
English to hungarian morpheme-based statistical
machine translation system with reordering rules. In
ACL Workshop on Hybrid Machine Approaches to
Translation, Sofia.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden, July. Association for
Computational Linguistics.
Will Lewis and Chris Quirk. 2013. Controlled ascent:
Imbuing statistical mt with linguistic knowledge. In
ACL Workshop on Hybrid Machine Approaches to
Translation, Sofia.
Shuo Li, Derek F. Wong, and Lidia S. Chao.
2013. Experiments with pos-based restructuring and
alignment-based reordering for statistical machine
translation. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Santanu Pal, Sudip Naskar, and Sivaji Bandyopadhyay.
2013. A hybrid word alignment model for phrase-
based statistical machine translation. In ACL Work-
shop on Hybrid Machine Approaches to Translation,
Sofia.
Raj Nath Patel, Rohit Gupta, Prakash B. Pimpale, and
Sasikumar M. 2013. Reordering rules for english-
hindi smt. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Alex Rudnick and Michael Gasser. 2013. Lexical se-
lection for hybrid mt with sequence labeling. In ACL
Workshop on Hybrid Machine Approaches to Trans-
lation, Sofia.
Markus Saers, Karteek Addanki, and Dekai Wu. 2013.
Unsupervised transduction grammar induction via
minimum description length. In ACL Workshop on
Hybrid Machine Approaches to Translation, Sofia.
George Tambouratzis, Sokratis Sofianopoulos, and
Marina Vassiliou. 2013. Language-independent hy-
brid mt with presemt. In ACL Workshop on Hybrid
Machine Approaches to Translation, Sofia.
Antonio Toral. 2013. Hybrid selection of language
model training data using linguistic information and
perplexity. In ACL Workshop on Hybrid Machine
Approaches to Translation, Sofia.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of Speech-to-Speech Translation. Springer,
Berlin, Heidelberg, New York.
Yorick Wilks. 1994. Stone soup and the french
room: The empiricist-rationalist debate about ma-
chine translation. Current Issues in Computational
Linguistics: in honor of Don Walker, pages 585?
594. Pisa, Italy: Giardini / Dordrecht, The Nether-
lands: Kluwer Academic.
6
