Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502?512,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Simple Domain-Independent Probabilistic Approach to Generation
Gabor Angeli
UC Berkeley
Berkeley, CA 94720
gangeli@berkeley.edu
Percy Liang
UC Berkeley
Berkeley, CA 94720
pliang@cs.berkeley.edu
Dan Klein
UC Berkeley
Berkeley, CA 94720
klein@cs.berkeley.edu
Abstract
We present a simple, robust generation system
which performs content selection and surface
realization in a unified, domain-independent
framework. In our approach, we break up
the end-to-end generation process into a se-
quence of local decisions, arranged hierar-
chically and each trained discriminatively.
We deployed our system in three different
domains?Robocup sportscasting, technical
weather forecasts, and common weather fore-
casts, obtaining results comparable to state-of-
the-art domain-specific systems both in terms
of BLEU scores and human evaluation.
1 Introduction
In this paper, we focus on the problem of generat-
ing descriptive text given a world state represented
by a set of database records. While existing gen-
eration systems can be engineered to obtain good
performance on particular domains (e.g., Dale et
al. (2003), Green (2006), Turner et al (2009), Re-
iter et al (2005), inter alia), it is often difficult
to adapt them across different domains. Further-
more, content selection (what to say: see Barzilay
and Lee (2004), Foster and White (2004), inter alia)
and surface realization (how to say it: see Ratna-
parkhi (2002), Wong and Mooney (2007), Chen and
Mooney (2008), Lu et al (2009), etc.) are typically
handled separately. Our goal is to build a simple,
flexible system which is domain-independent and
performs content selection and surface realization in
a unified framework.
We operate in a setting in which we are only given
examples consisting of (i) a set of database records
(input) and (ii) example human-generated text de-
scribing some of those records (output). We use the
model of Liang et al (2009) to automatically induce
the correspondences between words in the text and
the actual database records mentioned.
We break up the full generation process into a se-
quence of local decisions, training a log-linear clas-
sifier for each type of decision. We use a simple
but expressive set of domain-independent features,
where each decision is allowed to depend on the en-
tire history of previous decisions, as in the model
of Ratnaparkhi (2002). These long-range contextual
dependencies turn out to be critical for accurate gen-
eration.
More specifically, our model is defined in terms
of three types of decisions. The first type
chooses records from the database (macro content
selection)?for example, wind speed, in the case
of generating weather forecasts. The second type
chooses a subset of fields from a record (micro con-
tent selection)?e.g., the minimum and maximum
temperature. The third type chooses a suitable tem-
plate to render the content (surface realization)?
e.g., winds between [min] and [max] mph; templates
are automatically extracted from training data.
We tested our approach in three domains:
ROBOCUP, for sportscasting (Chen and Mooney,
2008); SUMTIME, for technical weather forecast
generation (Reiter et al, 2005); and WEATHERGOV,
for common weather forecast generation (Liang et
al., 2009). We performed both automatic (BLEU)
and human evaluation. On WEATHERGOV, we
502
s: pass(arg1=purple6, arg2=purple3)kick(arg1=purple3)badPass(arg1=purple3,arg2=pink9)turnover(arg1=purple3,arg2=pink9)
w: purple3 made a bad passthat was picked off by pink9
(a) Robocup
s: temperature(time=5pm-6am,min=48,mean=53,max=61)windSpeed(time=5pm-6am,min=3,mean=6,max=11,mode=0-10)windDir(time=5pm-6am,mode=SSW)gust(time=5pm-6am,min=0,mean=0,max=0)skyCover(time=5pm-9pm,mode=0-25)skyCover(time=2am-6am,mode=75-100)precipPotential(time=5pm-6am,min=2,mean=14,max=20)rainChance(time=5pm-6am,mode=someChance)
w: a 20 percent chance of showers after midnight . increasing clouds ,with a low around 48 southwest wind between 5 and 10 mph
(b) WeatherGov
s: wind10m(time=6am,dir=SW,min=16,max=20,gust min=0,gust max=-)wind10m(time=9pm,dir=SSW,min=28,max=32,gust min=40,gust max=-)wind10m(time=12am,dir=-,min=24,max=28,gust min=36,gust max=-)
w: sw 16 - 20 backing ssw 28 - 32 gusts 40 by mid evening easing 24 - 28 gusts 36 late evening
(c) SumTime
Figure 1: Example scenarios (a scenario is a world state s paired with a text w) for each of the three domains. Each row in the
world state denotes a record. Our generation task is to map a world state s (input) to a text w (output). Note that this mapping
involves both content selection and surface realization.
achieved a BLEU score of 51.5 on the combined task
of content selection and generation, which is more
than a two-fold improvement over a model similar
to that of Liang et al (2009). On ROBOCUP and
SUMTIME, we achieved results comparable to the
state-of-the-art. most importantly, we obtained these
results with a general-purpose approach that we be-
lieve is simpler than current state-of-the-art systems.
2 Setup and Domains
Our goal is to generate a text given a world state.
The world state, denoted s, is represented by a set
of database records. Define T to be a set of record
types, where each record type t ? T is associated
with a set of fields FIELDS(t). Each record r ? s
has a record type r.t ? T and a field value r.v[f ] for
each field f ? FIELDS(t). The text, denoted w, is
represented by a sequence of tokenized words. We
use the term scenario to denote a world state s paired
with a text w.
In this paper, we conducted experiments on three
domains, which are detailed in the following subsec-
tions. Example scenarios for each domain are de-
tailed in Figure 1.
2.1 ROBOCUP: Sportscasting
A world state in the ROBOCUP domain is a set of
event records (meaning representations in the termi-
nology of Chen and Mooney (2008)) generated by
a robot soccer simulator. For example, the record
pass(arg1=pink1,arg2=pink5) denotes a passing
event; records of this type (pass) have two fields:
arg1 (the agent) and arg2 (the recipient). As the
game progresses, human commentators talk about
some of the events in the game, e.g., purple3 made
a bad pass that was picked off by pink9.
We used the dataset created by Chen and Mooney
(2008), which contains 1919 scenarios from the
2001?2004 Robocup finals. Each scenario con-
sists of a single sentence representing a fragment
of a commentary on the game, paired with a set
of candidate records, which were recorded within
five seconds of the commentary. The records in the
ROBOCUP dataset data were aligned by Chen and
Mooney (2008). Each scenario contains on average
|s| = 2.4 records and 5.7 words. See Figure 1(a) for
an example of a scenario. Content selection in this
domain is choosing the single record to talk about,
and surface realization is talking about it.
503
2.2 SUMTIME: Technical Weather Forecasts
Reiter et al (2005) developed a generation system
and created the SUMTIME-METEO corpus, which
consists of marine wind weather forecasts used by
offshore oil rigs, generated by the output of weather
simulators. More specifically, these forecasts de-
scribe various aspects of the wind at different times
during the forecast period.
We used the version of the SUMTIME-METEO
corpus created by Belz (2008). The dataset consists
of 469 scenarios, each containing on average |s| =
2.6 records and 16.2 words. See Figure 1(c) for an
example of a scenario. This task requires no content
selection, only surface realization: The records are
given in some fixed order and the task is to generate
from each of these records in turn; of course, due
to contextual dependencies, these records cannot be
generated independently.
2.3 WEATHERGOV: Common Weather
Forecasts
In the WEATHERGOV domain, the world state con-
tains detailed information about a local weather
forecast (e.g., temperature, rain chance, etc.). The
text is a short forecast report based on this informa-
tion.
We used the dataset created by Liang et al (2009).
The world state is summarized by records which ag-
gregate measurements over selected time intervals.
The dataset consists of 29,528 scenarios, each con-
taining on average |s| = 36 records and 28.7 words.
See Figure 1(b) for an example of a scenario.
While SUMTIME and WEATHERGOV are both
weather domains, there are significant differences
between the two. SUMTIME forecasts are in-
tended to be read by trained meteorologists, and thus
the text is quite abbreviated. On the other hand,
WEATHERGOV texts are intended to be read by the
general public and thus is more English-like. Fur-
thermore, SUMTIME does not require content selec-
tion, whereas content selection is a major focus of
WEATHERGOV. Indeed, on average, only 5 of 36
records are actually mentioned in a WEATHERGOV
scenario. Also, WEATHERGOV is more complex:
The text is more varied, there are multiple record
types, and there are about ten times as many records
in each world state.
Generation Process
for i = 1, 2, . . . :
?choose a record ri ? s
?if ri = STOP: return
?choose a field set Fi ? FIELDS(ri.t)
?choose a template Ti ? TEMPLATES(ri.t, Fi)
Figure 2: Pseudocode for the generation process. The generated
text w is a deterministic function of the decisions.
3 The Generation Process
To model the process of generating a text w from a
world state s, we decompose the generation process
into a sequence of local decisions. There are two as-
pects of this decomposition that we need to specify:
(i) how the decisions are structured; and (ii) what
pieces of information govern the decisions.
The decisions are structured hierarchically into
three types of decisions: (i) record decisions, which
determine which records in the world state to talk
about (macro content selection); (ii) field set deci-
sions, which determine which fields of those records
to mention (micro content selection); and (iii) tem-
plate decisions, which determine the actual words
to use to describe the chosen fields (surface realiza-
tion). Figure 2 shows the pseudocode for the gen-
eration process, while Figure 3 depicts an example
of the generation process on a WEATHERGOV sce-
nario.
Each of these decisions is governed by a set of
feature templates (see Figure 4), which are repre-
sented as functions of the current decision and past
decisions. The feature weights are learned from
training data (see Section 4.3).
We chose a set of generic domain-independent
feature templates, described in the sections below.
These features can, in general, depend on the current
decision and all previous decisions. For example, re-
ferring to Figure 4, R2 features on the record choice
depend on all the previous record decisions, and R5
features depend on the most recent template deci-
sion. This is in contrast with most systems for con-
tent selection (Barzilay and Lee, 2004) and surface
realization (Belz, 2008), where decisions must de-
compose locally according to either a graph or tree.
The ability to use global features in this manner is
504
Worldstate skyCover1: skyCover(time=5pm-6am,mode=50-75)temperature1: temperature(time=5pm-6am,min=44,mean=49,max=60)...
Decisions
Record r1 = skyCover1 r2 = temperature1 r3 = stop
Field set F1 = {mode} F2 = {time,min}
Template T1 = ?mostly cloudy ,? T2 = ?with a low around [min] .?
Text mostly cloudy , with a low around 45 .
Specific active (nonzero) features for highlighted decisions
r2 = temperature1
(R1) Jr2.t = temperature and (r1.t, r0.t) = (skyCover, start)KJr2.t = temperature and (r1.t) = (skyCover)K(R2) Jr2.t = temperature and {r1.t} = {skyCover}K(R3) Jr2.t = temperature and rj .t 6= temperature ?j < 2K(R4) Jr2.t = temperature and r2.v[time] = 5pm-6amKJr2.t = temperature and r2.v[min] = lowKJr2.t = temperature and r2.v[mean] = lowKJr2.t = temperature and r2.v[max] = mediumK
F2 = {time,min} (F1) JF2 = {time,min}K(F2) JF2 = {time,min} and r2.v[time] = 5pm-6amK(F2) JF2 = {time,min} and r2.v[min] = lowK
T2 = ?with a low around [min]?
(W1) JBase(T2) = ?with a low around [min]?KJCoarse(T2) = ?with a [time] around [min]?K(W2) JBase(T2) = ?with a low around [min]? and r2.v[time] = 5pm-6amKJCoarse(T2) = ?with a [time] around [min]? and r2.v[time] = 5pm-6amKJBase(T2) = ?with a low around [min]? and r2.v[min] = lowKJCoarse(T2) = ?with a [time] around [min]? and r2.v[min] = lowK(W3) log plm(with | cloudy ,)
Figure 3: The generation process on an example WEATHERGOV scenario. The figure is divided into two parts: The upper part of
the figure shows the generation of text from the world state via a sequence of seven decisions (in boxes). Three of these decisions
are highlighted and the features that govern these decisions are shown in the lower part of the figure. Note that different decisions
in the generation process would result in different features being active (nonzero).
Feature TemplatesRecord R1? list of last k record types Jri.t = ? and (ri?1.t, . . . , ri?k.t) = ?K for k ? {1, 2}R2 set of previous record types Jri.t = ? and {rj .t : j < i} = ?KR3 record type already generated Jrj .t = ri.t for some j < iKR4 field values Jri.t = ? and ri.v[f ] = ?K for f ? Fields(ri.t)R5? stop under language model (LM) Jri.t = stopK? log plm(stop | previous two words generated)Field set F1? field set JFi = ?KF2 field values JFi = ? and ri.v[f ] = ?K for f ? FiTemplate W1? base/coarse generation template Jh(Ti) = ?K for h ? {Base,Coarse}W2 field values Jh(Ti) = ? and ri.v[f ] = ?K for f ? Fi, h ? {Base,Coarse}W3? first word of template under LM log plm(first word in Ti | previous two words)
Figure 4: Feature templates that govern the record, field set, and template decisions. Each line specifies the name, informal
description, and formal description of a set of features, obtained by ranging ? over possible values (for example, for Jri.t = ?K, ?
ranges over all record types T ). Notation: JeK returns 1 if the expression e is true and 0 if it is false. These feature templates are
domain-independent; that is, they are used to create features automatically across domains. Feature templates marked with ? are
included in our baseline system (Section 5.2).
one of the principal advantages of our approach.
3.1 Record Decisions
Record decisions are responsible for macro content
selection. Each record decision chooses a record ri
from the world state s according to features of the
following types:
R1 captures the discourse coherence aspect of
content selection; for example, we learn that
windSpeed tends to follow windDir (but not al-
505
ways). R2 captures an unordered notion of
coherence?simply which sets of record types are
preferable; for example, we learn that rainChance
is not generated if sleetChance already was men-
tioned. R3 is a coarser version of R2, capturing
how likely it is to propose a record of a type that has
already been generated. R4 captures the important
aspect of content selection that the records chosen
depend on their field values;1 for example, we learn
that snowChance is not chosen unless there is snow.
R5 allows the language model to indicate whether a
STOP record is appropriate; this helps prevent sen-
tences from ending abruptly.
3.2 Field Set Decisions
Field set decisions are responsible for micro con-
tent selection, i.e., which fields of a record are men-
tioned. Each field set decision chooses a subset of
fields Fi from the set of fields FIELDS(ri.t) of the
record ri that was just generated. These decisions
are made based on two types of features:
F1 captures which sets of fields are talked
about together; for example, we learn that {mean}
and {min,max} are preferred field sets for the
windSpeed record. By defining features on the en-
tire field set, we can capture any correlation structure
over the fields; in contrast, Liang et al (2009) gen-
erates a sequence of fields in which a field can only
depend on the previous one.
F2 allows the field set to be chosen based on the
values of the fields, analogously to R4.
3.3 Template Decisions
Template decisions perform surface realization. A
template is a sequence of elements, where each ele-
ment is either a word (e.g., around) or a field (e.g.,
[min]). Given the record ri and field set Fi that we
are generating from, the goal is to choose a template
Ti (Section 4.3.2 describes how we define the set
of possible templates). The features that govern the
choice of Ti are as follows:
W1 captures a priori preferences for generation
templates given field sets. There are two ways
to control this preference, BASE and COARSE.
1We map a numeric field value onto one of five categories
(very-low, low, medium, high, or very-high) based
on its value with respect to the mean and standard deviation of
values of that field in the training data.
BASE(Ti) denotes the template Ti itself, thus allow-
ing us to remember exactly which templates were
useful. To guard against overfitting, we also use
COARSE(Ti), which maps Ti to a coarsened version
of Ti, in which more words are replaced with their
associated fields (see Figure 5 for an example).
W2 captures a dependence on the values of fields
in the field set, and is analogous to R4 and F2. Fi-
nally, W3 contributes a language model probability,
to ensure smooth transitions between templates.
After Ti has been chosen, each field in the tem-
plate is replaced with a word given the correspond-
ing field value in the world state. In particular, a
word is chosen from the parameters learned in the
model of Liang et al (2009). In the example in Fig-
ure 3, the [min] field in T2 has value 44, which is
rendered to the word 45 (rounding and other noisy
deviations are common in the WEATHERGOV do-
main).
4 Learning a Probabilistic Model
Having described all the features, we now present a
conditional probabilistic model over texts w given
world states s (Section 4.1). Section 4.2 describes
how to use the model for generation, and Section 4.3
describes how to learn the model.
4.1 Model
Recall from Section 3 that the generation process
generates r1, F1, T1, r2, F2, T2, . . . , STOP. To unify
notation, denote this sequence of decisions as d =
(d1, . . . , d|d|).
Our probability model is defined as follows:
p(d | s; ?) =
|d|?
j=1
p(dj | d<j ; ?), (1)
where d<j = (d1, . . . , dj?1) is the history of de-
cisions and ? are the model parameters (feature
weights). Note that the text w (the output) is a de-
terministic function of the decisions d. We use the
features described in Section 3 to define a log-linear
model for each decision:
p(dj | d<j , s; ?) =
exp{?j(dj ,d<j , s)>?}
?
d?j?Dj
exp{?j(d?j ,d<j , s)
>?}
, (2)
where ? are all the parameters (feature weights), ?j
is the feature vector for the j-th decision, and Dj is
506
the domain of the j-th decision (either records, field
sets, or templates).
This chaining of log-linear models was used in
Ratnaparkhi (1998) for tagging and parsing, and in
Ratnaparkhi (2002) for surface realization. The abil-
ity to condition on arbitrary histories is a defining
property of these models.
4.2 Using the Model for Generation
Suppose we have learned a model with parameters ?
(how to obtain ? is discussed in Section 4.3). Given
a world state s, we would like to use our model to
generate an output text w via a decision sequence d.
In our experiments, we choose d by sequentially
choosing the best decision in a greedy fashion (until
the STOP record is generated):
dj = argmax
d?j
p(d?j | d<j , s; ?). (3)
Alternatively, instead of choosing the best decision
at each point, we can sample from the distribution:
dj ? p(dj | d<j , s; ?), which provides more diverse
generated texts at the expense of a slight degradation
in quality.
Both greedy search and sampling are very effi-
cient. Another option is to try to find the Viterbi
decision sequence, i.e., the one with the maximum
joint probability: d = argmaxd? p(d
? | s; ?). How-
ever, this computation is intractable due to features
depending arbitrarily on past decisions, making dy-
namic programming infeasible. We tried using beam
search to approximate this optimization, but we ac-
tually found that beam search performed worse than
greedy. Belz (2008) also found that greedy was more
effective than Viterbi for their model.
4.3 Learning
Now we turn our attention to learning the parame-
ters ? of our model. We are given a set of N sce-
narios {(s(i),w(i))}Ni=1 as training data. Note that
our model is defined over the decision sequence d
which contains information not present in w. In Sec-
tions 4.3.1 and 4.3.2, we show how we fill in this
missing information to obtain d(i) for each training
scenario i.
Assuming this missing information is filled, we
end up with a standard supervised learning problem,
which can be solved by maximize the (conditional)
likelihood of the training data:
max
??Rd
?
?
N?
i=1
|d(i)|?
j=1
log p(d(i)j | d
(i)
<j ; ?)
?
???||?||2, (4)
where ? > 0 is a regularization parameter. The ob-
jective function in (4) is optimized using the stan-
dard L-BFGS algorithm (Liu and Nocedal, 1989).
4.3.1 Latent Alignments
As mentioned previously, our training data in-
cludes only the world state s and generated text w,
not the full sequence of decisions d needed for train-
ing. Intuitively, we know what was generated but not
why it was generated.
We use the model of Liang et al (2009) to im-
pute the decisions d. They introduce a generative
model p(a,w|s), where the latent alignment a spec-
ifies (1) the sequence of records that were chosen,
(2) the sequence of fields that were chosen, and (3)
which words in the text were spanned by the chosen
records and fields. The model is learned in an unsu-
pervised manner using EM to produce a observing
only w and s.
An example of an alignment is given in the left
part of Figure 5. This information specifies the
record decisions and a set of fields for each record.
Because the induced alignments can be noisy, we
need to process them to obtain cleaner template de-
cisions. This is the subject of the next section.
4.3.2 Template Extraction
Given an aligned training scenario (Figure 5), we
would like to extract two types of templates.
For each record, an aligned training scenario
specifies a sequence of fields and the text that
is spanned by each field. We create a template
by abstracting fields?that is, replacing the words
spanned by a field by the field itself. We call the
resulting template COARSE. The problem with us-
ing this template directly is that fields can be noisy
due to errors from the unsupervised model.
Therefore, we also create a BASE template which
only abstracts a subset of the fields. In particular,
we define a trigger pattern which specifies a simple
condition under which a field should be abstracted.
For WEATHERGOV, we only abstract fields that
507
Records:Fields:Text:
skyCover1mode=50-75mostly cloudy ,
temperature1xwith a time=17-30low around min=4445 mean=49.
Aligned training scenario
?
skyCover temperatureCoarse ?[mode]? ?with a [time] [min] [mean]?Base ?most cloudy ,? ?with a low around [min] .?
Templates extracted
Figure 5: An example of template extraction from an imperfectly aligned training scenario. Note that these alignments are noisy
(e.g., [mean] aligns to a period). Therefore, for each record (skyCover and temperature in this case), we extract two templates:
(1) a COARSE template, which takes the text spanned by the record and abstracts away all fields in the scenario ([mode], [time],
[min], and [mean] in the example); and (2) a BASE template, which only abstracts away fields whose spanned text matches a simple
pattern (e.g., numbers in WEATHERGOV, corresponding to [min] in the example).
span numbers; for SUMTIME, fields that span num-
bers and wind directions; and for ROBOCUP, fields
that span words starting with purple or pink.
For each record ri, we define Ti so that BASE(Ti)
and COARSE(Ti) are the corresponding two ex-
tracted templates. We restrict Fi to the set of ab-
stracted fields in the COARSE template
5 Experiments
We now present an empirical evaluation of our sys-
tem on our three domains?ROBOCUP, SUMTIME,
and WEATHERGOV.
5.1 Evaluation Metrics
Automatic Evaluation To evaluate surface real-
ization (or, combined content selection and surface
realization), we measured the BLEU score (Papineni
et al, 2002) (the precision of 4-grams with a brevity
penalty) of the system-generated output with respect
to the human-generated output.
To evaluate macro content selection, we measured
the F1 score (the harmonic mean of precision and
recall) of the set of records chosen with respect to
the human-annotated set of records.
Human Evaluation We conducted a human eval-
uation using Amazon Mechanical Turk. For each
domain, we chose 100 scenarios randomly from the
test set. We ran each system under consideration on
each of these scenarios, and presented each resulting
output to 10 evaluators.2 Evaluators were given in-
structions to rank an output on the basis of English
fluency and semantic correctness on the following
scale:
2To minimize bias, we evaluated all the systems at once,
randomly shuffling the outputs of the systems. The evaluators
were not necessarily the same 10 evaluators.
Score English Fluency Semantic Correctness
5 Flawless Perfect
4 Good Near Perfect
3 Non-native Minor Errors
2 Disfluent Major Errors
1 Gibberish Completely Wrong
Evaluators were also given additional domain-
specific information: (1) the background of the
domain (e.g., that SUMTIME reports are techni-
cal weather reports); (2) general properties of the
desired output (e.g., that SUMTIME texts should
mention every record whereas WEATHERGOV texts
need not); and (3) peculiarities of the text (e.g., the
suffix ly in SUMTIME should exist as a separate to-
ken from its stem, or that pink goalie and pink1 have
the same meaning in ROBOCUP).
5.2 Systems
We evaluated the following systems on our three do-
mains:
? HUMAN is the human-generated output.
? OURSYSTEM uses all the features in Figure 4
and is trained according to Section 4.3.
? BASELINE is OURSYSTEM using a subset of
the features (those marked with ? in Fig-
ure 4). In contrast to OURSYSTEM, the in-
cluded features only depend on a local con-
text of decisions in a manner similar to
the generative model of Liang et al (2009)
and the pCRU-greedy system of Belz (2008).
BASELINE also excludes features that depend
on values of the world state.
? The existing state-of-the-art domain-specific
system for each domain.
5.3 ROBOCUP Results
Following the evaluation methodology of Chen and
Mooney (2008), we trained our system on three
508
System F1 BLEU* EnglishFluency
Semantic
Correctness
BASELINE 78.7 24.8 4.28 ? 0.78 4.15 ? 1.14
OURSYSTEM 79.9 28.8 4.34 ? 0.69 4.17 ? 1.21
WASPER-GEN 72.0 28.7 4.43 ? 0.76 4.27 ? 1.15
HUMAN ? ? 4.43 ? 0.69 4.30 ? 1.07
Table 1: ROBOCUP results. WASPER-GEN is described in
Chen and Mooney (2008). The BLEU is reported on systems
that use fixed human-annotated records (in other words, we
evaluate surface realization given perfect content selection).
Human Records:Fields:Text:
pass1arg1=purple10purple10 xpasses back to arg2=purple9purple9
Baseline Records:Fields:Text:
pass1arg1=purple10purple10 xkicks to arg2=purple9purple9
OurSystem Records:Fields:Text:
pass1arg1=purple10purple10 xpasses to arg2=purple9purple9WASPER-GENRecords:Text purple10 passes to purple9
Figure 6: Outputs of systems on an example ROBOCUP sce-
nario. There are some minor differences between the outputs.
Recall that OURSYSTEM differs from BASELINE mostly in
the addition of feature W2, which captures dependencies be-
tween field values (e.g., purple10) and the template chosen
(e.g., [arg1] passes to [arg2]). This allows us to capture value-
dependent preferences for different realizations (e.g., passes to
over kicks to). Also, HUMAN uses passes back to, but this word
choice requires knowledge of passing records in previous sce-
narios, which none of the systems have access to. It would nat-
ural, however, to add features that would capture these longer-
range dependencies in our framework.
Robocup games and tested on the fourth, averaging
over the four train/test splits. We report the average
test accuracy weighted by the number of scenarios
in a game. First, we evaluated macro content selec-
tion. Table 1 shows that OURSYSTEM significantly
outperforms BASELINE and WASPER-GEN on F1.
To compare with Chen and Mooney (2008) on
surface realization, we fixed each system?s record
decisions to the ones given by the annotated data
and enforced that all the fields of that record are
chosen. Table 1 shows that OURSYSTEM sig-
nificantly outperforms BASELINE and is compara-
ble to WASPER-GEN on BLEU. On human eval-
uation, OURSYSTEM outperforms BASELINE, but
WASPER-GEN outperforms OURSYSTEM. See
Figure 6 for example outputs from the various sys-
tems.
BLEU EnglishFluency
Semantic
Correctness
BASELINE 32.9 4.23 ? 0.71 4.26 ? 0.85
OURSYSTEM 55.1 4.25 ? 0.69 4.27 ? 0.82
OURSYSTEM-CUSTOM 62.3 4.12 ? 0.78 4.33 ? 0.91
pCRU-greedy 63.6 4.18 ? 0.71 4.49 ? 0.73
SUMTIME-Hybrid 52.7 ? ?
HUMAN ? 4.09 ? 0.83 4.37 ? 0.87
Table 2: SUMTIME results. The SUMTIME-Hybrid system
is described in (Reiter et al, 2005); pCRU-greedy, in (Belz,
2008).
5.4 SUMTIME Results
The SUMTIME task only requires micro content se-
lection and surface realization because the sequence
of records to be generated is fixed; only these as-
pects are evaluated. Following the methodology of
Belz (2008), we used five-fold cross validation.
We found that using the unsupervised model of
Liang et al (2009) to automatically produce aligned
training scenarios (Section 4.3.1) was less effec-
tive than it was in the other two domains due to
two factors: (i) there are fewer training examples
in SUMTIME and unsupervised learning typically
works better with a large amount of data; and (ii)
the alignment model does not exploit the temporal
structure in the SUMTIME world state. Therefore,
we used a small set of simple regular expressions to
produce aligned training scenarios.
Table 2 shows that OURSYSTEM signif-
icantly outperforms BASELINE as well as
SUMTIME-Hybrid, a hand-crafted system, on
BLEU. Note that OURSYSTEM is domain-
independent and has not been specifically tuned
to SUMTIME. However, OURSYSTEM is outper-
formed by the state-of-the-art statistical system
pCRU-greedy.
Custom Features One of the advantages of our
feature-based approach is that it is straightforward to
incorporate domain-specific features to capture spe-
cific properties of a domain. To this end, we define
the following set of feature templates in place of our
generic feature templates from Figure 4:
? F1?: Value of time
? F2?: Existence of gusts/wind direction/wind
speeds
? W1?: Change in wind direction (clockwise,
counterclockwise, or none)
509
Human Records:Fields:Text:
windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30
windDir2xgradually decreasing min=1010 x- max=1414 time=12amby late evening
Baseline Records:Fields:Text:
windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30
windDir2xincreasing min=1010 x- max=1414
OurSystem-Custom Records:Fields:Text:
windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30
windDir2xgradually decreasing min=1010 x- max=1414 time=12amby late eveningpCRU-greedy Records:Text nne 18 - 22 gusts 30 easing 10 - 14 by late evening
Figure 7: Outputs of systems on an example SUMTIME scenario. Two notable differences between OURSYSTEM-CUSTOM and
BASELINE arise due to OURSYSTEM-CUSTOM?s value-dependent features. For example, OURSYSTEM-CUSTOM can choose
whether to include the time field (windDir2) or not (windDir1), depending on the value of the time (F1?), thereby improving content
selection. OURSYSTEM-CUSTOM also improves surface realization, choosing gradually decreasing over BASELINE?s increasing.
Interestingly, this improvement comes from the joint effort of two features: W2? prefers decreasing over increasing in this case,
and W5? adds the modifier gradually. An important strength of log-linear models is the ability to combine soft preferences from
many features.
? W2?: Change in wind speed
? W3?: Change in wind direction and speed
? W4?: Existence of gust min and/or max
? W5?: Time elapsed since last record
? W6?: Whether wind is a cardinal direction (N,
E, S, W)
The resulting system, which we call
OURSYSTEM-CUSTOM, obtains a BLEU score
which is comparable to pCRU-greedy.
An important aspect of our system that it is flexi-
ble and quick to deploy. According to Belz (2008),
SUMTIME-Hybrid took twelve person-months to
build, while pCRU-greedy took one month. Having
developed OURSYSTEM in a domain-independent
way, we only needed to do simple reformatting upon
receiving the SUMTIME data. Furthermore, it took
only a few days to develop the custom features
above to create OURSYSTEM-CUSTOM, which has
BLEU performance comparable to the state-of-the-
art pCRU-greedy system.
We also conducted human evaluations on the four
systems shown in Table 2. Note that this evalua-
tion is rather difficult for Mechanical Turkers since
SUMTIME texts are rather technical compared to
those in other domains. Interestingly, all systems
outperform HUMAN on English fluency; this result
corroborates the findings of Belz (2008). On se-
mantic correctness, all systems perform comparably
to HUMAN, except pCRU-greedy, which performs
slightly better. See Figure 7 for a comparison of the
outputs generated by the various systems.
F1 BLEU* EnglishFluency
Semantic
Correctness
BASELINE 22.1 22.2 4.07 ? 0.59 3.41 ? 1.16
OURSYSTEM 65.4 51.5 4.12 ? 0.74 4.22 ? 0.89
HUMAN ? ? 4.14 ? 0.71 3.85 ? 0.99
Table 3: WEATHERGOV results. The BLEU score is on joint
content selection and surface realization and is modified to not
penalize numeric deviations of at most 5.
5.5 WEATHERGOV Results
We evaluate the WEATHERGOV corpus on the joint
task of content selection and surface realization.
We split our corpus into 25,000 scenarios for train-
ing, 1,000 for development, and 3,528 for testing.
In WEATHERGOV, numeric field values are often
rounded or noisily perturbed, so it is difficult to gen-
erate precisely matching numbers. Therefore, we
used a modified BLEU score where numbers dif-
fering by at most five are treated as equal. Fur-
thermore, WEATHERGOV is evaluated on the joint
content selection and surface realization task, un-
like ROBOCUP, where content selection and surface
realization were treated separately, and SUMTIME,
where content selection was not applicable.
Table 3 shows the results. We see that
OURSYSTEM substantially outperforms BASELINE,
especially on BLEU score and semantic correctness.
This difference shows that taking non-local context
into account is very important in this domain. This
result is not surprising, since WEATHERGOV is the
most complicated of the three domains, and this
complexity is exactly where non-locality is neces-
510
Human Records:Fields:Text:
skyCover1cover=50-75mostly cloudy x,
temperature1xwith a time=5pm-6amlow xaround min=5957 x.
windDir1mode=ssesouth xwind between
windSpeed1min=75 xand max=1510 xmph .
Baseline Records:Fields:Text:
rainChance2xa chance of showers ,
nonex,
gust1xwith gusts as high as max=2120 xmph .
precipPotential1xchance of precipitation is max=1010 x% .
OurSystem Records:Fields:Text:
skyCover1xmostly cloudy ,
temperature1xwith a low around min=5959 x.
windDir1xsouth wind between
windSpeed1min=77 xand max=1515 xmph .
Figure 8: Outputs of systems on an example WEATHERGOV scenario. Most of the gains of OURSYSTEM over BASELINE come
from improved content selection. For example, BASELINE chooses rainChance because it happens to be the most common first
record type in the training data. However, since OURSYSTEM has features that depend on the value of rainChance (noChance
in this case), it has learned to disprefer talking about rain when there is no rain. Also, OURSYSTEM has additional features on the
entire history of chosen records, which enables it to choose a better sequence of records.
sary. Interestingly, OURSYSTEM even outperforms
HUMAN on semantic correctness, perhaps due to
generating more straightforward renderings of the
world state. Figure 8 describes example outputs for
each system.
6 Related Work
There has been a fair amount of work both on con-
tent selection and surface realization. In content se-
lection, Barzilay and Lee (2004) use an approach
based on local classification with edge-wise scores
between local decisions. Our model, on the other
hand, can capture higher-order constraints to enforce
global coherence.
Liang et al (2009) introduces a generative model
of the text given the world state, and in some ways is
similar in spirit to our model. Although that model
is capable of generation in principle, it was de-
signed for unsupervised induction of hidden align-
ments (which is exactly what we use it for). Even
if combined with a language model, generated text
was much worse than our baseline.
The prominent approach for surface realization
is rendering the text from a grammar. Wong and
Mooney (2007) and Chen and Mooney (2008) use
synchronous grammars that map a logical form, rep-
resented as a tree, into a parse of the text. Soricut
and Marcu (2006) uses tree structures called WIDL-
expressions (the acronym corresponds to four opera-
tions akin to the rewrite rules of a grammar) to repre-
sent the realization process, and, like our approach,
operates in a log-linear framework. Belz (2008) and
Belz and Kow (2009) also perform surface realiza-
tion from a PCFG-like grammar. Lu et al (2009)
uses a conditional random field model over trees.
Other authors have performed surface realization us-
ing various grammar formalisms, for instance CCG
(White et al, 2007), HPSG (Nakanishi et al, 2005),
and LFG (Cahill and van Genabith, 2006).
In each of the above cases, the decomposable
structure of the tree/grammar enables tractability.
However, we saw that it was important to include
features that captured long-range dependencies. Our
model is also similar in spirit to Ratnaparkhi (2002)
in the use of non-local features, but we operate at
three levels of hierarchy to include both content se-
lection and surface realization.
One issue that arises with long-range dependen-
cies is the lack of efficient algorithms for finding the
optimal text. Koller and Striegnitz (2002) perform
surface realization of a flat semantics, which is NP-
hard, so they recast the problem as non-projective
dependency parsing. Ratnaparkhi (2002) uses beam
search to find an approximate solution. We found
that a greedy approach obtained better results than
beam search; Belz (2008) found greedy approaches
to be effective as well.
7 Conclusion
We have developed a simple yet powerful generation
system that combines both content selection and sur-
face realization in a domain independent way. De-
spite our approach being domain-independent, we
were able to obtain performance comparable to the
state-of-the-art across three domains. Additionally,
the feature-based design of our approach makes it
easy to incorporate domain-specific knowledge to
increase performance even further.
511
References
R. Barzilay and L. Lee. 2004. Catching the drift: Prob-
abilistic content models, with applications to genera-
tion and summarization. In Human Language Tech-
nology and North American Association for Computa-
tional Linguistics (HLT/NAACL).
A. Belz and E. Kow. 2009. System building cost vs.
output quality in data-to-text generation. In European
Workshop on Natural Language Generation, pages
16?24.
A. Belz. 2008. Automatic generation of weather forecast
texts using comprehensive probabilistic generation-
space models. Natural Language Engineering,
14(4):1?26.
Aoife Cahill and Josef van Genabith. 2006. Robust pcfg-
based generation using automatically acquired LFG
approximations. In Association for Computational
Linguistics (ACL), pages 1033?1040, Morristown, NJ,
USA. Association for Computational Linguistics.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In International Conference on Machine Learning
(ICML), pages 128?135.
R. Dale, S. Geldof, and J. Prost. 2003. Coral: using natu-
ral language generation for navigational assistance. In
Australasian computer science conference, pages 35?
44.
M. E. Foster and M. White. 2004. Techniques for text
planning with XSLT. In Workshop on NLP and XML:
RDF/RDFS and OWL in Language Technology, pages
1?8.
N. Green. 2006. Generation of biomedical arguments for
lay readers. In International Natural Language Gen-
eration Conference, pages 114?121.
A. Koller and K. Striegnitz. 2002. Generation as de-
pendency parsing. In Association for Computational
Linguistics (ACL), pages 17?24.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP).
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503?528.
W. Lu, H. T. Ng, and W. S. Lee. 2009. Natural lan-
guage generation with tree conditional random fields.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 400?409.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Parsing ?05: Pro-
ceedings of the Ninth International Workshop on Pars-
ing Technology, pages 93?102, Morristown, NJ, USA.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Association for Computational Linguis-
tics (ACL).
A. Ratnaparkhi. 1998. Maximum entropy models for nat-
ural language ambiguity resolution. Ph.D. thesis, Uni-
versity of Pennsylvania.
A. Ratnaparkhi. 2002. Trainable approaches to surface
natural language generation and their application to
conversational dialog systems. Computer, Speech &
Language, 16:435?455.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy. 2005.
Choosing words in computer-generated weather fore-
casts. Artificial Intelligence, 167:137?169.
R. Soricut and D. Marcu. 2006. Stochastic language
generation using WIDL-expressions and its applica-
tion in machine translation and summarization. In As-
sociation for Computational Linguistics (ACL), pages
1105?1112.
R. Turner, Y. Sripada, and E. Reiter. 2009. Gener-
ating approximate geographic descriptions. In Eu-
ropean Workshop on Natural Language Generation,
pages 42?49.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realization
with CCG. In In Proceedings of the Workshop on Us-
ing Corpora for NLG: Language Generation and Ma-
chine Translation (UCNLG+MT).
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960?967.
512
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 534?545,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
NaturalLI: Natural Logic Inference for Common Sense Reasoning
Gabor Angeli
Stanford University
Stanford, CA 94305
angeli@cs.stanford.edu
Christopher D. Manning
Stanford University
Stanford, CA 94305
manning@cs.stanford.edu
Abstract
Common-sense reasoning is important for
AI applications, both in NLP and many
vision and robotics tasks. We propose
NaturalLI: a Natural Logic inference sys-
tem for inferring common sense facts ? for
instance, that cats have tails or tomatoes
are round ? from a very large database
of known facts. In addition to being able
to provide strictly valid derivations, the
system is also able to produce derivations
which are only likely valid, accompanied
by an associated confidence. We both
show that our system is able to capture
strict Natural Logic inferences on the Fra-
CaS test suite, and demonstrate its ability
to predict common sense facts with 49%
recall and 91% precision.
1 Introduction
We approach the task of database completion:
given a database of true facts, we would like to
predict whether an unseen fact is true and should
belong in the database. This is intuitively cast as
an inference problem from a collection of candi-
date premises to the truth of the query. For exam-
ple, we would like to infer that no carnivores eat
animals is false given a database containing the cat
ate a mouse (see Figure 1).
These inferences are difficult to capture in
a principled way while maintaining high recall,
particularly for large scale open-domain tasks.
Learned inference rules are difficult to general-
ize to arbitrary relations, and standard IR methods
easily miss small but semantically important lex-
ical differences. Furthermore, many methods re-
quire explicitly modeling either the database, the
query, or both in a formal meaning representation
(e.g., Freebase tuples).
Although projects like the Abstract Meaning
Representation (Banarescu et al., 2013) have made
No carnivores
eat animals?
The carnivores
eat animals
The cat
eats animals
The cat
ate an animal
The cat
ate a mouse
w
?
w
f
No cats
eat animals
No cats
eat mice
w
. . .
w
. . .
Figure 1: Natural Logic inference cast as search.
The path to the boxed premise the cat ate a mouse
disproves the query no carnivores eat animals, as
it passes through the negation relation (f). This
path is one of many candidates taken; the premise
is one of many known facts in the database. The
edge labels denote Natural Logic inference steps.
headway in providing broad-coverage meaning
representations, it remains appealing to use hu-
man language as the vessel for inference. Fur-
thermore, OpenIE and similar projects have been
very successful at collecting databases of natural
language snippets from an ever-increasing corpus
of unstructured text. These factors motivate our
use of Natural Logic ? a proof system built on the
syntax of human language ? for broad coverage
database completion.
Prior work on Natural Logic has focused on in-
ferences from a single relevant premise, making
use of only formally valid inferences. We improve
upon computational Natural Logic in three ways:
(i) our approach operates over a very large set of
candidate premises simultaneously; (ii) we do not
require explicit alignment between a premise and
the query; and (iii) we allow imprecise inferences
at an associated cost learned from data.
Our approach casts inference as a single uni-
fied search problem from a query to any valid
534
supporting premise. Each transition along the
search denotes a (reverse) inference step in Natu-
ral Logic, and incurs a cost reflecting the system?s
confidence in the validity of that step. This ap-
proach offers two contributions over prior work in
database completion: (i) it allows for unstructured
text as the input database without any assump-
tions about the schema or domain of the text, and
(ii) it proposes Natural Logic for inference, rather
than translating to a formal logic syntax. More-
over, the entire pipeline is implemented in a single
elegant search framework, which scales easily to
large databases.
2 MacCartney?s Natural Logic
Natural Logic aims to capture common logical in-
ferences by appealing directly to the structure of
language, as opposed to running deduction on an
abstract logical form. The logic builds upon tra-
ditional rather than first-order logic: to a first ap-
proximation, Natural Logic can be seen as an en-
hanced version of Aristotle?s syllogistic system
(van Benthem, 2008). A working understanding
of the logic as syllogistic reasoning is sufficient for
understanding the later contributions of the paper.
While some inferences of first-order logic are not
captured by Natural Logic, it nonetheless allows
for a wide range of intuitive inferences in a com-
putationally efficient and conceptually clean way.
We build upon the variant of the logic intro-
duced by the NatLog system (MacCartney and
Manning, 2007; 2008; 2009), based on earlier the-
oretical work on Natural Logic and Monotonicity
Calculus (van Benthem, 1986; Valencia, 1991).
Later work formalizes many aspects of the logic
(Icard, 2012; Djalali, 2013); we adopt the formal
semantics of Icard and Moss (2014), along with
much of their notation.
At a high level, Natural Logic proofs operate by
mutating spans of text to ensure that the mutated
sentence follows from the original ? each step is
much like a syllogistic inference. We construct a
complete proof system in three parts: we define
MacCartney?s atomic relations between lexical en-
tries (Section 2.1), the effect these lexical muta-
tions have on the validity of the sentence (Sec-
tion 2.2), and a practical approach for executing
these proofs. We review MacCartney?s alignment-
based approach in Section 2.3, and show that we
can generalize and simplify this system in Sec-
tion 3.
D
? ? ?
(equivalence)
D
? v ?
(forward entail.)
D
? w ?
(reverse entail.)
D
?f ?
(negation)
D
?  ?
(alternation)
D
? ` ?
(cover)
Figure 2: The model-theoretic interpretation of the
MacCartney relations. The figure shows the re-
lation between the denotation of ? (dark) and ?
(light). The universe is denoted by D.
2.1 Lexical Relations
MacCartney and Manning (2007) introduce seven
set-theoretic relations between the denotations of
any two lexical items. The denotation of a lexical
item is the set of objects in the domain of discourse
D to which that lexical item refers. For instance,
the denotation of cat would be the set of all cats.
Two denotations can then be compared in terms of
set algebra: if we define the set of cats to be ? and
the set of animals to be ?, we can state that ? ? ?.
The six informative relations are summarized in
Figure 2; a seventh relation (#) corresponds to
to the completely uninformative relation. For in-
stance, the example search path in Figure 1 makes
use of the following relations:
No x y f The x y
cat v carnivore
animal ? a animal
animal w mouse
Denotations are not required to be in the
space of predicates (e.g., cat, animal). In
the first example, the denotations of No and
The are in the space of operators p? (p? t):
functions from predicates p to truth values
t. The f relation becomes the conjunction
of two claims: ?x?y ? (no x y ? the x y) and
?x?y (no x y ? the x y). This is analogous to the
construction of the set-theoretic definition of f in
Figure 2: ? ? ? = ? and ? ? ? = D (see Icard
and Moss (2014)).
535
Examples of the last two relations ( and`) and
the complete independence relation (#) include:
cat  dog
animal ` nonhuman
cat # friendly
2.2 Monotonicity and Polarity
The previous section details the relation between
lexical items; however, we still need a theory for
how to ?project? the relation induced by a lexical
mutation as a relation between the two containing
sentences. For example, cat v animal, and some
cat meowsv some animal meows, but no cat barks
6v no animal barks. Despite differing by the same
lexical relation, the first example describes a valid
entailment, while the second does not.
We appeal to two important concepts: mono-
tonicity as a property of arguments to natural lan-
guage operators, and polarity as a property of lexi-
cal items in a sentence. Much like monotone func-
tions in calculus, an [upwards] monotone operator
has an output truth value which is non-decreasing
(i.e., material implication) as the input ?increases?
(i.e., the subset relation). From the example above,
some is upwards monotone in its first argument,
and no is downwards monotone in its first argu-
ment.
Polarity is a property of lexical items in a sen-
tence determined by the operators acting on it. All
lexical items have upward polarity by default; up-
wards monotone operators preserve polarity, and
downwards monotone operators reverse polarity.
For example, mice in no cats eat mice has down-
ward polarity, whereas mice in no cats don?t eat
mice has upward polarity (it is in the scope of two
downward monotone operators). The relation be-
tween two sentences differing by a single lexical
relation is then given by the projection function ?
in Table 1.
1
2.3 Proof By Alignment
MacCartney and Manning (2007) approach the in-
ference task in the context of inferring whether a
single relevant premise entails a query. Their ap-
proach first generates an alignment between the
premise and the query, and then classifies each
aligned segment into one of the lexical relations
in Figure 2. Inference reduces to projecting each
1
Note that this table optimistically assumes every operator
is additive and multiplicative, as defined in Icard (2012).
r ? v w  ` f #
?(r) ? w v `  f #
Table 1: The projection function ?, shown for
downward polarity contexts only. The input r is
the lexical relation between two words in a sen-
tence; the projected relation ?(r) is the relation
between the two sentences differing only by that
word. Note that ? is the identity function in up-
ward polarity contexts.
./ ? v w f  ` #
? ? v w f  ` #
v v v #   # #
w w # w ` # ` #
f f `  ? w v #
  #  v # v #
` ` ` # w w # #
# # # # # # # #
Table 2: The join table as shown in Icard (2012).
Entries in the table are the result of joining a row
with a column.
of these relations according to the projection func-
tion ? (Table 1) and iteratively joining two pro-
jected relations together to get the final entailment
relation. This join relation, denoted as ./, is given
in Table 2.
To illustrate, we can consider MacCartney?s
example inference from Stimpy is a cat to
Stimpy is not a poodle. An alignment of
the two statements would provide three lexical
mutations: r
1
:
= cat? dog, r
2
:
= ? ? not, and
r
3
:
= dog? poodle. Each of these are then pro-
jected with the projection function ?, and are
joined using the join relation:
r
0
./ ?(r
1
) ./ ?(r
2
) ./ ?(r
3
),
where the initial relation r
0
is axiomatically ?. In
MacCartney?s work this style of proof is presented
as a table. The last column (s
i
) is the relation be-
tween the premise and the i
th
step in the proof, and
is constructed inductively as s
i
:
= s
i?1
./ ?(r
i
):
Mutation r
i
?(r
i
) s
i
r
1
cat?dog   
r
2
? ?not f f v
r
3
dog?poodle w v v
In our example, we would conclude that Stimpy
is a cat v Stimpy is not a poodle since s
3
is v;
therefore the inference is valid.
536
?w `
f
v 
f
v

w
`
?
f`
w?
f
v?
f
v?
f`
w?
?? ?
?; ?
?? ??
f
w`
?v
f`
v
?w
any
(a) (b)
Figure 3: (a) Natural logic inference expressed as a finite state automaton. Omitted edges go to the
unknown state (#), with the exception of omitted edges from ?, which go to the state of the edge type.
Green states (?, v) denote valid inferences; red states (, f) denote invalid inferences; blue states (w,
`) denote inferences of unknown validity. (b) The join table collapsed into the three meaningful states
over truth values.
3 Inference as a Finite State Machine
We show that the tabular proof formulation from
Section 2.3 can be viewed as a finite state machine,
and present a novel observation that we can loss-
lessly collapse this finite state machine into only
three intuitive inference states. These observations
allow us to formulate our search problem such that
a search path corresponds to an input to (i.e., path
through) this collapsed state machine.
Taking notation from Section 2.3, we construct
a finite state machine over states s ? {v,w, . . . }.
A machine in state s
i
corresponds to relation s
i
holding between the initial premise and the de-
rived fact so far. States therefore correspond to
states of logical validity. The start state is ?. Out-
going transitions correspond to inference steps.
Each transition is labeled with a projected relation
?(r) ? {v,w, . . . }, and spans from a source state
s to a target s
?
according to the join table. That is,
the transition s
?(r)
??? s
?
exists iff s
?
= s ./ ?(r).
For example, the path in Figure 1 yields the tran-
sitions ?
f
??f
w
??
?
??
w
??. Figure 3a shows the
automaton, with trivial edges omitted for clarity.
Our second contribution is collapsing this au-
tomaton into the three meaningful states we use as
output: valid (? ? ?), invalid (? ? ??), and
unknown validity (? ; ?). We can cluster states
in Figure 3a into these three categories. The rela-
tions ? and v correspond to valid inferences; f
and  correspond to invalid inferences; w, ` and
# correspond to unknown validity. This cluster-
ing mirrors that used by MacCartney for his tex-
tual entailment experiments.
Collapsing the FSA into the form in Figure 3b
becomes straightforward from observing the reg-
ularities in Figure 3a. Nodes in the valid cluster
transition to invalid nodes always and only on the
relations f and . Symmetrically, invalid nodes
transition to valid nodes always and only onf and
`. A similar pattern holds for the other transitions.
Formally, for every relation r and nodes a
1
and a
2
in the same cluster, if we have transitions
a
1
r
?? b
1
and a
2
r
?? b
2
then b
1
and b
2
are neces-
sarily in the same cluster. As a concrete example,
we can take r = f and the two states in the in-
valid cluster: a
1
= f, a
2
=. Although f
f
???
and 
f
??v, both ? and v are in the same cluster
(valid). It is not trivial a priori that the join table
should have this regularity, and it certainly simpli-
fies the logic for inference tasks.
A few observations deserve passing remark.
First, even though the states w and ` appear
meaningful, in fact there is no ?escaping? these
states to either a valid or invalid inference. Sec-
ond, the hierarchy over relations presented in Icard
(2012) becomes apparent ? in particular,f always
behaves as negation, whereas its two ?weaker?
versions ( and `) only behave as negation in cer-
tain contexts. Lastly, with probabilistic inference,
transitioning to the unknown state can be replaced
with staying in the current state at a (potentially
arbitrarily large) cost to the confidence of valid-
ity. This allows us to make use of only two states:
valid and invalid.
537
4 Inference As Search
Natural Logic allows us to formalize our approach
elegantly as a single search problem. Given a
query, we search over the space of possible facts
for a valid premise in our database. The nodes in
our search problem correspond to candidate facts
(Section 4.1); the edges are mutations of these
facts (Section 4.2); the costs over these edges en-
code the confidence that this edge maintains an
informative inference (Section 4.5). This mirrors
the automaton defined in Section 3, except impor-
tantly we are constructing a reversed derivation,
and are therefore ?traversing? the FSA backwards.
This approach is efficient over a large database
of 270 million entries without making use of ex-
plicit queries over the database; nor does the
approach make use of any sort of approximate
matching against the database, beyond lemmatiz-
ing individual lexical items. The motivation in
prior work for approximate matches ? to improve
the recall of candidate premises ? is captured ele-
gantly by relaxing Natural Logic itself. We show
that allowing invalid transitions with appropriate
costs generalizes JC distance (Jiang and Conrath,
1997) ? a common thesaurus-based similarity met-
ric (Section 4.3). Importantly, however, the entire
inference pipeline is done within the framework of
weighted lexical transitions in Natural Logic.
4.1 Nodes
The space of possible nodes in our search is the
set of possible partial derivations. To a first ap-
proximation, this is a pair (w, s) of a surface form
w tagged with word sense and polarity, and an in-
ference state s ? {valid, invalid} in our collapsed
FSA (Figure 3b). For example, the search path in
Figure 1 traverses the nodes:
(No carnivores eat animals, valid)
(The carnivores eat animals, invalid)
(The cat eats animals, invalid)
(The cat eats an animal, invalid)
(The cat ate a mouse, invalid)
During search, we assume that the validity
states s are reversible ? if we know that the cat ate
a mouse is true, we can infer that no carnivores
eat animals is false. In addition, our search keeps
track of some additional information:
Mutation Index Edges between sentences are
most naturally defined to correspond to mutations
of individual lexical items. We therefore maintain
an index of the next item to mutate at each search
state. Importantly, this enforces that each deriva-
tion orders mutations left-to-right; this is compu-
tationally efficient, at the expense of rare search
errors. A similar observation is noted in MacCart-
ney (2009), where prematurely collapsing to # oc-
casionally misses inferences.
Polarity Mutating operators can change the po-
larity on a span in the fact. Since we do not have
the full parse tree at our disposal at search time,
we track a small amount of metadata to guess the
scope of the mutated operator.
4.2 Transitions
We begin by introducing some terminology. A
transition template is a broad class of transitions;
for instance WordNet hypernymy. A transition
(or transition instance) is a particular instantiation
of a transition template. For example, the tran-
sition from cat to feline. Lastly, an edge in the
search space connects two nodes, which are sep-
arated by a single transition instance. For exam-
ple, an edge exists between some felines have tails
and some cats have tails. Transition [instances]
are stored statically in memory, whereas edges are
constructed on demand.
Transition templates provide a means of defin-
ing transitions and subsequently edges in our
search space using existing lexical resources (e.g.,
WordNet, distributional similarity, etc.). We can
then define a mapping from these templates to
Natural Logic lexical relations. This allows us
to map every edge in our search graph back to
the Natural Logic relation it instantiates. The
full table of transition templates is given in Ta-
ble 3, along with the Natural Logic relation that
instances of the template introduce. We include
most relations in WordNet as transitions, and
parametrize insertions and deletions by the part of
speech of the token being inserted/deleted.
Once we have an edge defining a lexical mu-
tation with an associated Natural Logic relation
r, we can construct the corresponding end node
(w
?
, s
?
) such that w
?
is the sentence with the lex-
ical mutation applied, and s
?
is the validity state
obtained from the FSA in Section 3. For instance,
if our edge begins at (w, s), and there exists a tran-
sition in the FSA from s
?
r
?? s, then we define the
end point of the edge to be (w
?
, s
?
). To illustrate
concretely, suppose our search state is:
(some felines have tails, valid)
538
Transition Template Relation
WordNet hypernym v
WordNet hyponym w
WordNet antonym
?

WordNet synonym/pertainym
?
?
Distributional nearest neighbor ?
Delete word
?
v
Add word
?
w
Operator weaken v
Operator strengthen w
Operator negate f
Operator synonym ?
Change word sense ?
Table 3: The edges allowed during inference.
Entries with a dagger (?) are parametrized by
their part-of-speech tag, from the restricted list of
{noun,adjective,verb,other}. The first column de-
scribes the type of the transition. The set-theoretic
relation introduced by each relation is given in the
second column.
The transition template for WordNet hyper-
nymy gives us a transition instance from feline
to cat, corresponding to the Natural Logic infer-
ence cat
v
?? feline. Recall, we are constructing
the inference in reverse, starting from the conse-
quent (query). We then notice that the transition
valid
v
?? valid in the FSA ends in our current
inference state (valid), and set our new inference
state to be the start state of the FSA transition ? in
this case, we maintain validity.
Note that negation is somewhat subtle, as the
transitions are not symmetric from valid to in-
valid and visa versa, and we do not know our true
inference state with respect to the premise yet.
In practice, the search procedure treats all three
of {f, ,`} as negation, and re-scores complete
derivations once their inference states are known.
It should be noted that the mapping from transi-
tion templates to relation types is intentionally im-
precise. For instance, clearly nearest neighbors do
not preserve equivalence (?); more subtly, while
all cats like milk  all cats hate milk, it is not
the case that some cats like milk  some cats hate
milk.
2
We mitigate this imprecision by introducing
a cost for each transition, and learning the appro-
priate value for this cost (see Section 5). The cost
of an edge from fact (w, v) with surface form w
2
The latter example is actually a consequence of the pro-
jection function in Table 1 being overly optimistic.
and validity v to a new fact (w
?
, v
?
), using a transi-
tion instance t
i
of template t and mutating a word
with polarity p, is given by f
t
i
? ?
t,v,p
. We define
this as:
f
t
i
: A value associated with every transition
instance t
i
, intuitively corresponding to how
?far? the endpoints of the transition are.
?
t,v,p
: A learned cost for taking a transition of
template t, if the source of the edge is in a in-
ference state of v and the word being mutated
has polarity p.
The notation for f
t
i
is chosen to evoke an anal-
ogy to features. We set f
t
i
to be 1 in most cases;
the exceptions are the edges over the WordNet hy-
pernym tree and the nearest neighbors edges. In
the first case, taking the hypernymy relation from
w to w
?
to be ?
w?w
?
, we set:
f
?
w?w
?
= log
p(w
?
)
p(w)
= log p(w
?
)? log p(w).
The value f
?
w?w
?
is set analogously. We define
p(w) to be the ?probability? of a concept ? that
is, the normalized frequency of a word w or any
of its hyponyms in the Google N-Grams corpus
(Brants and Franz, 2006). Intuitively, this ensures
that relatively long paths through fine-grained sec-
tions of WordNet are not unduly penalized. For
instance, the path from cat to animal traverses six
intermediate nodes, na??vely yielding a prohibitive
search depth of 6. However, many of these tran-
sitions have low weight: for instance f
?
cat?feline
is
only 0.37.
For nearest neighbors edges, we take Neu-
ral Network embeddings learned in Huang et al.
(2012) corresponding to each vocabulary entry.
We then define f
NN
w?w
?
to be the arc cosine of
the cosine similarity (i.e., the angle) between word
vectors associated with lexical items w and w
?
:
f
NN
w?w
?
= arccos
(
w ? w
?
?w??w
?
?
)
.
For instance, f
NN
cat?dog
= 0.43. In practice, we
explore the 100 nearest neighbors of each word.
We can express f
t
i
as a feature vector by rep-
resenting it as a vector with value f
t
i
at the index
corresponding to (t, v, p) ? the transition template,
the validity of the inference, and the polarity of
the mutated word. Note that the size of this vector
mirrors the number of cost parameters ?
t,v,p
, and
539
is in general smaller than the number of transition
instances.
A search path can then be parametrized by a
sequence of feature vectors f
1
, f
2
, . . . , f
n
, which
in turn can be collapsed into a single vector f =
?
i
f
i
. The cost of a path is defined as ? ? f , where
? is the vector of ?
t,v,p
values. Both f and ? are
constrained to be non-negative, or else the search
problem is misspecified.
4.3 Generalizing Similarities
An elegant property of our definitions of f
t
i
is its
ability to generalize JC distance. Let us assume we
have lexical itemsw
1
andw
2
, with a least common
subsumer lcs. The JC distance dist
jc
(w
1
, w
2
) is:
dist
jc
(w
1
, w
2
) = log
p(lcs)
2
p(w
1
) ? p(w
2
)
. (1)
For simplicity, we simplify ?
?,v,p
and ?
?,v,p
as
simply ?
?
and ?
?
. Without loss of generality, we
also assume that a path in our search is only modi-
fying a single lexical item w
1
, eventually reaching
a mutated form w
2
.
We can factorize the cost of a path, ? ? f , along
the path fromw
1
tow
2
through its lowest common
subsumer (lcs), [w
1
, w
(1)
1
, . . . , lcs, . . . , w
(1)
2
, w
2
],
as follows:
? ? ? = ?
?
([
log p(w
(1)
1
)? log p(w
1
)
]
+ . . .
)
+
?
?
([
log p(lcs)? log p(w
(n)
1
)
]
+ . . .
)
= ?
?
(
log
p(lcs)
p(w
1
)
)
+ ?
?
(
log
p(lcs)
p(w
2
)
)
= log
p(lcs)
?
?
+?
?
p(w
1
)
?
?
? p(w
2
)
?
?
.
Note that setting both ?
?
and ?
?
to 1 exactly
yields Formula (1) for JC distance. This, in addi-
tion to the inclusion of nearest neighbors as tran-
sitions, allows the search to capture the intuition
that similar objects have similar properties (e.g.,
as used in Angeli and Manning (2013)).
4.4 Deletions in Inference
Although inserting lexical items in a derivation
(deleting words from the reversed derivation) is
trivial, the other direction is not. For brevity, we
refer to a deletion in the derivation as an insertion,
since from the perspective of search we are insert-
ing lexical items.
Na??vely, at every node in our search we must
consider every item in the vocabulary as a possi-
ble insertion. We can limit the number of items we
consider by storing the database as a trie. Since
the search mutates the fact left-to-right (as per
Section 4.1), we can consider children of a trie
node as candidate insertions. To illustrate, given
a search state with fact w
0
w
1
. . . w
n
and mutation
index i, we would look up completions w
i+1
for
w
0
w
1
. . . w
i
in our trie of known facts.
Although this approach works well when i is
relatively large, there are too many candidate in-
sertions for small i. We special case the most ex-
treme example for this, where i = 0 ? that is,
when we are inserting into the beginning of the
fact. In this case, rather than taking all possible
lexical items that start any fact, we take all items
which are followed by the first word of our current
fact. To illustrate, given a search state with fact
w
0
w
1
. . . w
n
, we would propose candidate inser-
tions w
?1
such that w
?1
w
0
w
?
1
. . . w
?
k
is a known
fact for some w
?
1
. . . w
?
k
. More concretely, if we
know that fluffy cats have tails, and are at a node
corresponding to cats like boxes, we propose fluffy
as a possible insertion: fluffy cats like boxes.
4.5 Confidence Estimation
The last component in inference is translating a
search path into a probability of truth. We notice
from Section 4.2 that the cost of a path can be rep-
resented as ? ? f . We can normalize this value by
negating every element of the cost vector ? and
passing it through a sigmoid:
confidence =
1
1 + e
?(???f)
.
Importantly, note that the cost vector must be
non-negative for the search to be well-defined, and
therefore the confidence value will be constrained
to be between 0 and
1
2
.
At this point, we have a confidence that the
given path has not violated strict Natural Logic.
However, to translate this value into a probability
we need to incorporate whether the inference path
is confidently valid, or confidently invalid. To il-
lustrate, a fact with a low confidence should trans-
late to a probability of
1
2
, rather than a probability
of 0. We therefore define the probability of valid-
ity as follows: We take v to be 1 if the query is in
the valid state with respect to the premise, and ?1
if the query is in the invalid state. For complete-
ness, if no path is given we can set v = 0. The
540
probability of validity becomes:
p(valid) =
v
2
+
1
1 + e
v??f
. (2)
Note that in the case where v = ?1, the above
expression reduces to
1
2
? confidence; in the case
where v = 0 it reduces to simply
1
2
. Furthermore,
note that the probability of truth makes use of the
same parameters as the cost in the search.
5 Learning Transition Costs
We describe our procedure for learning the transi-
tion costs ?. Our training data D consists of query
facts q and their associated gold truth values y.
Equation (2) gives us a probability that a partic-
ular inference is valid; we axiomatically consider
a valid inference from a known premise to be justi-
fication for the truth of the query. This is at the ex-
pense of the (often incorrect) assumption that our
database is clean and only contains true facts.
We optimize the likelihood of our gold annota-
tions according to this probability, subject to the
constraint that all elements in our cost vector ?
be non-negative. We run the search algorithm de-
scribed in Section 4 on every query q
i
? D. This
produces the highest confidence path x
1
, along
with its inference state v
i
. We now have annotated
tuples: ((x
i
, v
i
), y
i
) for every element in our train-
ing set. Analogous to logistic regression, the log
likelihood of our training data D, subject to costs
?, is:
l
?
(D) =
?
0?i<|D|
[
y
i
log
(
v
i
2
+
1
1 + e
v
i
??f(x
i
)
)
+ (1? y
i
) log
(
?v
i
2
+
1
1 + e
?v
i
??f(x
i
)
)
]
,
where y
i
is 1 if the example is annotated true and
0 otherwise, and f(x
i
) are the features extracted
for path x
i
. The objective function is the negative
log likelihood with an L
2
regularization term and
a log barrier function to prohibit negative costs:
O(D) = ?l
?
(D) +
1
2?
2
???
2
2
?  log(?).
We optimize this objective using conjugate gra-
dient descent. Although the objective is non-
convex, in practice we can find a good initializa-
tion of weights to reduce the risk of arriving at lo-
cal optima.
An elegant property of this formulation is that
the weights we are optimizing correspond directly
? Category Count Precision Recall Accuracy
N M08 N M08 N M07 M08
1 Quantifiers 44 91 95 100 100 95 84 97
2 Plurals 24 80 90 29 64 38 42 75
3 Anaphora 6 100 100 20 60 33 50 50
4 Ellipses 25 100 100 5 5 28 28 24
5 Adjectives 15 80 71 66 83 73 60 80
6 Comparatives 16 90 88 100 89 87 69 81
7 Temporal 36 75 86 53 71 52 61 58
8 Verbs 8 ? 80 0 66 25 63 62
9 Attitudes 9 ? 100 0 83 22 55 89
Applicable (1,5,6) 75 89 89 94 94 89 76 90
Table 4: Results on the FraCaS textual entailment
suite. N is this work; M07 refers to MacCartney
and Manning (2007); M08 refers to MacCartney
and Manning (2008). The relevant sections of the
corpus intended to be handled by this system are
sections 1, 5, and 6 (although not 2 and 9, which
are also included in M08).
to the costs used during search. This creates a pos-
itive feedback loop ? as better weights are learned,
the search algorithm is more likely to find con-
fident paths, and more data is available to train
from. We therefore run this learning step for mul-
tiple epochs, re-running search after each epoch.
The weights for the first epoch are initialized to
an approximation of valid Natural Logic weights.
Subsequent epochs initialize their weights to the
output of the previous epoch.
6 Experiments
We evaluate our system on two tasks: the Fra-
CaS test suite, used by MacCartney and Manning
(2007; 2008), evaluates the system?s ability to cap-
ture Natural Logic inferences even without the ex-
plicit alignments of these previous systems. In
addition, we evaluate the system?s ability to pre-
dict common-sense facts from a large corpus of
OpenIE extractions.
6.1 FraCaS Entailment Corpus
The FraCaS corpus (Cooper et al., 1996) is a small
corpus of entailment problems, aimed at provid-
ing a comprehensive test of a system?s handling of
various entailment patterns. We process the cor-
pus following MacCartney and Manning (2007).
It should be noted that many of the sections of
the corpus are not directly applicable to Natu-
ral Logic inferences; MacCartney and Manning
(2007) identify three sections which are in the
scope of their system, and consequently our sys-
tem as well.
Results on the dataset are given in Table 4.
541
System P R F
1
Accuracy
Lookup 100.0 12.1 21.6 56.0
NaturalLI Only 88.8 40.1 55.2 67.5
NaturalLI + Lookup 90.6 49.1 63.7 72.0
Table 5: Accuracy inferring common-sense facts
on a balanced test set. Lookup queries the lem-
matized lower-case fact directly in the 270M fact
database. NaturalLI Only disallows such lookups,
and infers every query from only distinct premises
in the database. NaturalLI + Lookup takes the
union of the two systems.
Since the corpus is not a blind test set, the re-
sults are presented less as a comparison of perfor-
mance, but rather to validate the expressive power
of our search-based approach against MacCart-
ney?s align-and-classify approach. For the exper-
iments, costs were set to express valid Natural
Logic inference as a hard constraint.
The results show that the system is able to cap-
ture Natural Logic inferences with similar accu-
racy to the state-of-the-art system of MacCartney
and Manning (2008). Note that our system is com-
paratively crippled in this framework along at least
two dimensions: It cannot appeal to the premise
when constructing the search, leading to the intro-
duction of a class of search errors which are en-
tirely absent from prior work. Second, the deriva-
tion process itself does not have access to the full
parse tree of the candidate fact.
Although precision is fairly high even on the
non-applicable sections of FraCaS, recall is sig-
nificantly lower than prior work. This is a direct
consequence of not having alignments to appeal
to. For instance, we can consider two inferences:
Jack saw Jill is playing
?
=? Jill is playing
Jill saw Jack is playing
?
=? Jill is playing
It is clear from the parse of the sentence that
the first is valid and the second is not; however,
from the perspective of the search algorithm both
make the same two edits: inserting Jack and saw.
In order to err on the side of safety, we disallow
deleting the verb saw.
6.2 Common Sense Reasoning
We validate our system?s ability to infer unseen
common sense facts from a large database of
such facts. Whereas evaluation on FraCaS shows
that our search formulation captures applicable in-
ferences as well as prior work, this evaluation
presents a novel use-case for Natural Logic infer-
ence.
For our database of facts, we run the Ol-
lie OpenIE system (Mausam et al., 2012) over
Wikipedia,
3
Simple Wikipedia,
4
and a random 5%
of CommonCrawl. Extractions with confidence
below 0.25 or which contained pronouns were
discarded. This yielded a total of 305 million
unique extractions composed entirely of lexical
items which mapped into our vocabulary (186 707
items). Each of these extracted triples (e
1
, r, e
2
)
was then flattened into a plain-text fact e
1
r e
2
and
lemmatized. This yields 270 million unique lem-
matized premises in our database. In general, each
fact in the database could be arbitrary unstructured
text; our use of Ollie extractions is motivated only
by a desire to extract short, concise facts.
For our evaluation, we infer the top 689 most
confident facts from the ConceptNet project (Tan-
don et al., 2011). To avoid redundancy with Word-
Net, we take facts from eight ConceptNet rela-
tions: MemberOf, HasA, UsedFor, CapableOf,
Causes, HasProperty, Desires, and CreatedBy. We
then treat the surface text field of these facts as
our candidate query. This yields queries like the
following:
not all birds can fly
noses are used to smell
nobody wants to die
music is used for pleasure
For negative examples, we take the 689 ReVerb
extractions (Fader et al., 2011) judged as false
by Mechanical Turk workers (Angeli and Man-
ning, 2013). This provides a set of plausible but
nonetheless incorrect examples, and ensures that
our recall is not due to over-zealous search. Search
costs are tuned from an additional set of 540 true
ConceptNet and 540 false ReVerb extractions.
Results are shown in Table 5. We compare
against the baseline of looking up each fact verba-
tim in the fact database. Note that both the query
and the facts in the database are short snippets, al-
ready lemmatized and lower-cased; therefore, it is
not in principle unreasonable to expect a database
of 270 million extractions to contain these facts.
Nonetheless, only 12% of facts were found via a
direct lookup. We show that NaturalLI (allowing
lookups) improves this recall four-fold, at only an
9.4% drop in precision.
3
http://wikipedia.org/ (2013-07-03)
4
http://simple.wikipedia.org/ (2014-03-25)
542
7 Related Work
A large body of work is devoted to compiling
open-domain knowledge bases. For instance,
OpenIE systems (Yates et al., 2007; Fader et al.,
2011) extract concise facts via surface or depen-
dency patterns. In a similar vein, NELL (Carlson
et al., 2010; Gardner et al., 2013) continuously
learns new high-precision facts from the internet.
Many NLP applications query large knowl-
edge bases. Prominent examples include ques-
tion answering (Voorhees, 2001), semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2007; Kwiatkowski et al., 2013; Berant
and Liang, 2014), and information extraction sys-
tems (Hoffmann et al., 2011; Surdeanu et al.,
2012). A goal of this work is to improve accuracy
on these downstream tasks by providing a proba-
bilistic knowledge base for likely true facts.
A natural alternative to the approach taken in
this paper is to extend knowledge bases by in-
ferring and adding new facts directly. For in-
stance, Snow et al. (2006) present an approach to
enriching the WordNet taxonomy; Tandon et al.
(2011) extend ConceptNet with new facts; Soder-
land et al. (2010) use ReVerb extractions to enrich
a domain-specific ontology. Chen et al. (2013) and
Socher et al. (2013) use Neural Tensor Networks
to predict unseen relation triples in WordNet and
Freebase, following a line of work by Bordes et
al. (2011) and Jenatton et al. (2012). Yao et al.
(2012) and Riedel et al. (2013) present a related
line of work, inferring new relations between Free-
base entities via inference over both Freebase and
OpenIE relations. In contrast, this work runs infer-
ence over arbitrary text, without restricting itself to
a particular set of relations, or even entities.
The goal of tackling common-sense reasoning
is by no means novel in itself. Work by Reiter
and McCarthy (Reiter, 1980; McCarthy, 1980) at-
tempts to reason about the truth of a consequent in
the absence of strict logical entailment. Similarly,
Pearl (1989) presents a framework for assigning
confidences to inferences which can be reason-
ably assumed. Our approach differs from these at-
tempts in part in its use of Natural Logic as the un-
derlying inference engine, and more substantially
in its attempt at creating a broad-coverage sys-
tem. More recently, work by Schubert (2002) and
Van Durme et al. (2009) approach common sense
reasoning with episodic logic; we differ in our fo-
cus on inferring truth from an arbitrary query, and
in making use of longer inferences.
This work is similar in many ways to work
on recognizing textual entailment ? e.g., Schoen-
mackers et al. (2010), Berant et al. (2011). Work
by Lewis and Steedman (2013) is particularly rele-
vant, as they likewise evaluate on the FraCaS suite
(Section 1; 89% accuracy with gold trees). They
approach entailment by constructing a CCG parse
of the query, while mapping questions which are
paraphrases of each other to the same logical form
using distributional relation clustering. However,
their system is unlikely to scale to either our large
database of premises, or our breadth of relations.
Fader et al. (2014) propose a system for ques-
tion answering based on a sequence of paraphrase
rewrites followed by a fuzzy query to a structured
knowledge base. This work can be thought of as
an elegant framework for unifying this two-stage
process, while explicitly tracking the ?risk? taken
with each paraphrase step. Furthermore, our sys-
tem is able to explore mutations which are only
valid in one direction, rather than the bidirectional
entailment of paraphrases, and does not require a
corpus of such paraphrases for training.
8 Conclusion
We have presented NaturalLI, an inference system
over unstructured text intended to infer common
sense facts. We have shown that we can run infer-
ence over a large set of premises while maintain-
ing Natural Logic semantics, and that we can learn
how to infer unseen common sense facts.
Future work will focus on enriching the class
of inferences we can make with Natural Logic.
For example, extending the approach to handle
meronymy and relation entailments. Furthermore,
we hope to learn richer lexicalized parameters, and
use the syntactic structure of a fact during search.
Acknowledgements
We thank the anonymous reviewers for their
thoughtful comments. We gratefully acknowl-
edge the support of the Defense Advanced Re-
search Projects Agency (DARPA) Deep Explo-
ration and Filtering of Text (DEFT) Program un-
der Air Force Research Laboratory (AFRL) con-
tract no. FA8750-13-2-0040. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government.
543
References
Gabor Angeli and Christopher Manning. 2013.
Philosophers are mortal: Inferring the truth of un-
seen facts. In CoNLL.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. Proc. Linguistic Annotation Work-
shop.
J. Berant and P. Liang. 2014. Semantic parsing via
paraphrasing. In Association for Computational
Linguistics (ACL).
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
Antoine Bordes, Jason Weston, Ronan Collobert,
Yoshua Bengio, et al. 2011. Learning structured
embeddings of knowledge bases. In AAAI.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram version 1. Linguistic Data Consortium.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.
Danqi Chen, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2013. Learning new
facts from knowledge bases with neural tensor net-
works and semantic word vectors. arXiv preprint
arXiv:1301.3618.
Robin Cooper, Dick Crouch, Jan Van Eijck, Chris
Fox, Johan Van Genabith, Jan Jaspars, Hans Kamp,
David Milward, Manfred Pinkal, Massimo Poesio,
et al. 1996. Using the framework. Technical report,
The FraCaS Consortium.
Alex J Djalali. 2013. Synthetic logic. Linguistic Issues
in Language Technology, 9:1?18.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In KDD.
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
and Tom Mitchell. 2013. Improving learning and
inference in a large knowledge-base using latent
syntactic cues. EMNLP.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In ACL-HLT.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. ACL.
Thomas Icard, III and Lawrence Moss. 2014. Recent
progress on monotonicity. Linguistic Issues in Lan-
guage Technology.
Thomas Icard, III. 2012. Inclusion and exclusion in
natural language. Studia Logica.
Rodolphe Jenatton, Nicolas L Roux, Antoine Bordes,
and Guillaume R Obozinski. 2012. A latent factor
model for highly multi-relational data. In NIPS.
Jay J Jiang and David W Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. Proceedings of the 10th International Con-
ference on Research on Computational Linguistics.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. TACL, 1:179?
192.
Bill MacCartney and Christopher D Manning. 2007.
Natural logic for textual inference. In ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing.
Bill MacCartney and Christopher D Manning. 2008.
Modeling semantic containment and exclusion in
natural language inference. In Coling.
Bill MacCartney and Christopher D Manning. 2009.
An extended model of natural logic. In Proceedings
of the eighth international conference on computa-
tional semantics.
Bill MacCartney. 2009. Natural Language Inference.
Ph.D. thesis, Stanford.
Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In EMNLP.
John McCarthy. 1980. Circumscription?a form of
non-monotonic reasoning. Artificial intelligence.
Judea Pearl. 1989. Probabilistic semantics for non-
monotonic reasoning: A survey. Principles of
Knowledge Representation and Reasoning.
Raymond Reiter. 1980. A logic for default reasoning.
Artificial intelligence, 13(1):81?132.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL-HLT.
Stefan Schoenmackers, Oren Etzioni, Daniel S Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In EMNLP.
544
Lenhart Schubert. 2002. Can we derive general world
knowledge from texts? In HLT.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In ACL.
Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
NIPS.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Oren Etzioni, et al. 2010. Adapting open infor-
mation extraction to domain-specific relations. AI
Magazine.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-
ati, and Christopher D. Manning. 2012. Multi-
instance multi-label learning for relation extraction.
In EMNLP.
Niket Tandon, Gerard de Melo, and Gerhard Weikum.
2011. Deriving a web-scale common sense fact
database. In AAAI.
V??ctor Manuel S?anchez Valencia. 1991. Studies on
natural logic and categorial grammar. Ph.D. thesis,
University of Amsterdam.
Johan van Benthem. 1986. Essays in logical seman-
tics. Springer.
Johan van Benthem. 2008. A brief history of natural
logic. Technical Report PP-2008-05, University of
Amsterdam.
Benjamin Van Durme, Phillip Michalak, and Lenhart K
Schubert. 2009. Deriving generalized knowledge
from corpora using wordnet abstraction. In EACL.
Ellen M Voorhees. 2001. Question answering in
TREC. In Proceedings of the tenth international
conference on Information and knowledge manage-
ment.
Limin Yao, Sebastian Riedel, and Andrew McCal-
lum. 2012. Probabilistic databases of universal
schema. In Proceedings of the Joint Workshop on
Automatic Knowledge Base Construction and Web-
scale Knowledge Extraction.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. TextRunner: Open information
extraction on the web. In ACL-HLT.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI/IAAI, Portland, OR.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In EMNLP-CoNLL.
545
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1556?1567,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Combining Distant and Partial Supervision for Relation Extraction
Gabor Angeli, Julie Tibshirani, Jean Y. Wu, Christopher D. Manning
Stanford University
Stanford, CA 94305
{angeli, jtibs, jeaneis, manning}@stanford.edu
Abstract
Broad-coverage relation extraction either
requires expensive supervised training
data, or suffers from drawbacks inherent
to distant supervision. We present an ap-
proach for providing partial supervision
to a distantly supervised relation extrac-
tor using a small number of carefully se-
lected examples. We compare against es-
tablished active learning criteria and pro-
pose a novel criterion to sample examples
which are both uncertain and representa-
tive. In this way, we combine the ben-
efits of fine-grained supervision for diffi-
cult examples with the coverage of a large
distantly supervised corpus. Our approach
gives a substantial increase of 3.9% end-
to-end F
1
on the 2013 KBP Slot Filling
evaluation, yielding a net F
1
of 37.7%.
1 Introduction
Fully supervised relation extractors are limited to
relatively small training sets. While able to make
use of much more data, distantly supervised ap-
proaches either make dubious assumptions in or-
der to simulate fully supervised data, or make use
of latent-variable methods which get stuck in local
optima easily. We hope to combine the benefits
of supervised and distantly supervised methods by
annotating a small subset of the available data us-
ing selection criteria inspired by active learning.
To illustrate, our training corpus contains
1 208 524 relation mentions; annotating all of
these mentions for a fully supervised classifier, at
an average of $0.13 per annotation, would cost ap-
proximately $160 000. Distant supervision allows
us to make use of this large corpus without requir-
ing costly annotation. The traditional approach is
based on the assumption that every mention of an
entity pair (e.g., Obama and USA) participates in
the known relation between the two (i.e., born in).
However, this introduces noise, as not every men-
tion expresses the relation we are assigning to it.
We show that by providing annotations for only
10 000 informative examples, combined with a
large corpus of distantly labeled data, we can yield
notable improvements in performance over the
distantly supervised data alone. We report results
on three criteria for selecting examples to anno-
tate: a baseline of sampling examples uniformly
at random, an established active learning criterion,
and a new metric incorporating both the uncer-
tainty and the representativeness of an example.
We show that the choice of metric is important
? yielding as much as a 3% F
1
difference ? and
that our new proposed criterion outperforms the
standard method in many cases. Lastly, we train
a supervised classifier on these collected exam-
ples, and report performance comparable to dis-
tantly supervised methods. Furthermore, we no-
tice that initializing the distantly supervised model
using this supervised classifier is critical for ob-
taining performance improvements.
This work makes a number of concrete contri-
butions. We propose a novel application of active
learning techniques to distantly supervised rela-
tion extraction. To the best of the authors knowl-
edge, we are the first to apply active learning to the
class of latent-variable distantly supervised mod-
els presented in this paper. We show that anno-
tating a proportionally small number of examples
yields improvements in end-to-end accuracy. We
compare various selection criteria, and show that
this decision has a notable impact on the gain in
performance. In many ways this reconciles our
results with the negative results of Zhang et al.
(2012), who show limited gains from na??vely an-
notating examples. Lastly, we make our annota-
tions available to the research community.
1
1
http://nlp.stanford.edu/software/
mimlre.shtml
1556
2 Background
2.1 Relation Extraction
We are interested in extracting a set of relations
y
1
. . . y
k
from a fixed set of possible relations R,
given two entities e
1
and e
2
. For example, we
would like to extract that Barack Obama was born
in Hawaii. The task is decomposed into two steps:
First, sentences containing mentions of both e
1
and e
2
are collected. The set of these sentences
x, marked with the entity mentions for e
1
and e
2
,
becomes the input to the relation extractor, which
then produces a set of relations which hold be-
tween the mentions. We are predominantly in-
terested in the second step ? classifying a set of
pairs of entity mentions into the relations they ex-
press. Figure 1 gives the general setting for re-
lation extraction, with entity pairs Barack Obama
and Hawaii, and Barack Obama and president.
Traditionally, relation extraction has fallen into
one of four broad approaches: supervised classi-
fication, as in the ACE task (Doddington et al.,
2004; GuoDong et al., 2005; Surdeanu and Cia-
ramita, 2007), distant supervision (Craven and
Kumlien, 1999; Wu and Weld, 2007; Mintz et
al., 2009; Sun et al., 2011; Roth and Klakow,
2013) deterministic rule-based systems (Soder-
land, 1997; Grishman and Min, 2010; Chen et al.,
2010), and translation from open domain informa-
tion extraction schema (Riedel et al., 2013). We
focus on the first two of these approaches.
2.2 Supervised Relation Extraction
Relation extraction can be naturally cast as a su-
pervised classification problem. A corpus of rela-
tion mentions is collected, and each mention x is
annotated with the relation y, if any, it expresses.
The classifier?s output is then aggregated to decide
the relations between the two entities.
However, annotating supervised training data
is generally expensive to perform at large scale.
Although resources such as Freebase or the TAC
KBP knowledge base have on the order of millions
of training tuples over entities it is not feasible to
manually annotate the corresponding mentions in
the text. This has led to the rise of distantly su-
pervised methods, which make use of this indirect
supervision, but do not necessitate mention-level
supervision.
Barack Obama was born in Hawaii.
Barack Obama visited Hawaii.
The president grew up in Hawaii.
state of birth
state of residence
Barack Obama met former president Clinton.
Obama became president in 2008. title
Figure 1: The relation extraction setup. For a
pair of entities, we collect sentences which men-
tion both entities. These sentences are then used
to predict one or more relations between those
entities. For instance, the sentences containing
both Barack Obama and Hawaii should support
the state of birth and state of residence relation.
2.3 Distant Supervision
Traditional distant supervision makes the assump-
tion that for every triple (e
1
, y, e
2
) in a knowledge
base, every sentence containing mentions for e
1
and e
2
express the relation y. For instance, tak-
ing Figure 1, we would create a datum for each
of the three sentences containing BARACK OBAMA
and HAWAII labeled with state of birth, and like-
wise with state of residence, creating 6 training
examples overall. Similarly, both sentences in-
volving Barack Obama and president would be
marked as expressing the title relation.
While this allows us to leverage a large database
effectively, it nonetheless makes a number of na??ve
assumptions. First ? explicit in the formulation of
the approach ? it assumes that every mention ex-
presses some relation, and furthermore expresses
the known relation(s). For instance, the sen-
tence Obama visited Hawaii would be erroneously
treated as a positive example of the born in rela-
tion. Second, it implicitly assumes that our knowl-
edge base is complete: entity mentions with no
known relation are treated as negative examples.
The first of these assumptions is addressed by
multi-instance multi-label (MIML) learning, de-
scribed in Section 2.4. Min et al. (2013) address
the second assumption by extending the MIML
model with additional latent variables, while Xu
et al. (2013) allow feedback from a coarse relation
extractor to augment labels from the knowledge
base. These latter two approaches are compatible
with but are not implemented in this work.
2.4 Multi-Instance Multi-Label Learning
The multi-instance multi-label (MIML-RE) model
of Surdeanu et al. (2012), which builds upon work
1557
. . .
. . . . . .
. . .
Figure 2: The MIML-RE model, as shown in Sur-
deanu et al. (2012). The outer plate corresponds to
each of the n entity pairs in our knowledge base.
Each entity pair has a set of mention pairs M
i
, and
a corresponding plate in the diagram for each men-
tion pair in M
i
. The variable x represents the in-
put mention pair, whereas y represents the positive
and negative relations for the given pair of entities.
The latent variable z denotes a mention-level pre-
diction for each input. The weight vector for the
multinomial z classifier is given by w
z
, and there
is a weight vector w
j
for each binary y classifier.
by Hoffmann et al. (2011) and Riedel et al. (2010),
addresses the assumptions of distantly supervised
relations extractors in a principled way by positing
a latent mention-level annotation.
The model groups mentions according to their
entity pair ? for instance, every mention pair with
Obama and Hawaii would be grouped together. A
latent variable z
i
is created for every mention i,
where z
i
? R ? {None} takes a single relation
label, or a no relation marker. We create |R| bi-
nary variables y representing the known positive
and negative relations for the entity pair. A set of
binary classifiers (log-linear factors in the graphi-
cal model) links the latent predictions z
1
. . . z
|M
i
|
and each y
j
. These classifiers include two classes
of features: first, a binary feature which fires if at
least one of the mentions expresses a known rela-
tion between the entity pair, and second, a feature
for each co-occurrence of relations for a given en-
tity pair. Figure 2 describes the model.
2.5 Background on Active Learning
We describe preliminaries and prior work on ac-
tive learning; we use this framework to propose
two sampling schemes in Section 3 which we use
to annotate mention-level labels for MIML-RE.
One way of expressing the generalization error
of a hypothesis
?
h is through its mean-squared error
with the true hypothesis h:
E[(h(x)?
?
h(x))
2
]
= E[E[(h(x)?
?
h(x))
2
|x]]
=
?
x
E[(h(x)?
?
h(x))
2
|x]p(x)dx.
The integrand can be further broken into bias
and variance terms:
E[(h(x)?
?
h(x))
2
] = (E[
?
h(x)]? h(x))
2
+ E[(
?
h(x)? E[
?
h(x)])
2
]
where for simplicity we?ve dropped the condition-
ing on x.
Many traditional sampling strategies, such as
query-by-committee (QBC) (Freund et al., 1992;
Freund et al., 1997) and uncertainty sampling
(Lewis and Gale, 1994), work by decreasing the
variance of the learned model. In QBC, we
first create a ?committee? of classifiers by ran-
domly sampling their parameters from a distribu-
tion based on the training data. These classifiers
then make predictions on the unlabeled examples,
and the examples on which there is the most dis-
agreement are selected for labeling. This strat-
egy can be seen as an attempt to decrease the ver-
sion space ? the set of classifiers that are consis-
tent with the labeled data. Decreasing the version
space should lower variance, since variance is in-
versely related to the size of the hypothesis space.
In most scenarios, active learning does not con-
cern itself with the bias term. If a model is fun-
damentally misspecified, then no amount of ad-
ditional training data can lower its bias. How-
ever, our paradigm differs from the traditional set-
ting, in that we are annotating latent variables in
a model with a non-convex objective. These an-
notations may help increase the convexity of our
objective, leading us to a more accurate optimum
and thereby lowering bias.
The other component to consider is
?
x
? ? ? p(x)dx. This suggests that it is impor-
tant to choose examples that are representative
of the underlying distribution p(x), as we want
to label points that will improve the classifier?s
predictions on as many and as high-probability
examples as possible. Incorporating a repre-
sentativeness metric has been shown to provide
a significant improvement over plain QBC or
1558
uncertainty sampling (McCallum and Nigam,
1998; Settles, 2010).
2.6 Active Learning for Relation Extraction
Several papers have explored active learning for
relation extraction. Fu and Grishman (2013) em-
ploy active learning to create a classifier quickly
for new relations, simulated from the ACE corpus.
Finn and Kushmerick (2003) compare a number
of selection criteria ? including QBC ? for a su-
pervised classifier. To the best of our knowledge,
we are the first to apply active learning to distantly
supervised relation extraction. Furthermore, we
evaluate our selection criteria live in a real-world
setting, collecting new sentences and evaluating
on an end-to-end task.
For latent variable models, McCallum and
Nigam (1998) apply active learning to semi-
supervised document classification. We take in-
spiration from their use of QBC and the choice of
metric for classifier disagreement. However their
model assumes a fully Bayesian set-up, whereas
ours does not require strong assumptions about the
parameter distributions.
Settles et al. (2008) use active learning to im-
prove a multiple-instance classifier. Their model
is simpler in that it does not allow for unobserved
variables or multiple labels, and the authors only
evaluate on image retrieval and synthetic text clas-
sification datasets.
3 Example Selection
We describe three criteria for selection examples
to annotate. The first ? sampling uniformly ? is
a baseline for our hypothesis that intelligently se-
lecting examples is important. For this criterion,
we select mentions uniformly at random from the
training set to annotate. This is the approach used
in Zhang et al. (2012). The other two criteria rely
on a metric for disagreement provided by QBC;
we describe our adaptation of QBC for MIML-RE
as a preliminary to introducing these criteria.
3.1 QBC For MIML-RE
We use a version of QBC based on bootstrap-
ping (Saar-Tsechansky and Provost, 2004). To
create the committee of classifiers, we re-sample
the training set with replacement 7 times and train
a model over each sampled dataset. We mea-
sure disagreement on z-labels among the classi-
fiers using a generalized Jensen-Shannon diver-
gence (McCallum and Nigam, 1998), taking the
average KL divergence of all classifier judgments.
We first calculate the mention-level confi-
dences. Note that z
(m)
i
? M
i
denotes the latent
variable in entity pair i with index m; z
(?m)
i
de-
notes the set of all latent variables except z
(m)
i
:
p(z
(m)
i
|y
i
,x
i
) =
p(y
i
, z
(m)
i
|x
i
)
p(y
i
|x
i
)
=
?
z
(?m)
i
p(y
i
, z
i
|x
i
)
?
z
(m)
i
p(y
i
, z
(m)
i
|x
i
)
.
Notice that the denominator just serves to nor-
malize the probability within a sentence group.
We can rewrite the numerator as follows:
?
z
(?m)
i
p(y
i
, z
i
|x
i
)
=
?
z
(?m)
i
p(y
i
|z
i
)p(z
i
|x
i
)
= p(z
(m)
i
|x
i
)
?
z
(?m)
i
p(y
i
|z
i
)p(z
(?m)
i
|x
i
).
For computational efficiency, we approximate
p(z
(?m)
i
|x
i
) with a point mass at its maximum.
Next, we calculate the Jensen-Shannon (JS) diver-
gence from the k bootstrapped classifiers:
1
k
k
?
c=1
KL(p
c
(z
(m)
i
|y
i
,x
i
)||p
mean
(z
(m)
i
|y
i
,x
i
)) (1)
where p
c
is the probability assigned by each of the
k classifiers to the latent z
(m)
i
, and p
mean
is the av-
erage of these probabilities. We use this metric
to capture the disagreement of our model with re-
spect to a particular latent variable. This is then
used to inform our selection criteria.
We note that QBC may be especially useful in
our situation as our objective is highly nonconvex.
If two committee members disagree on a latent
variable, it is likely because they converged to dif-
ferent local optima; annotating that example could
help bring the classifiers into agreement.
The second selection criterion we consider is
the most straightforward application of QBC ? se-
lecting the examples with the highest JS disagree-
ment. This allows us to compare our criterion, de-
scribed next, against an established criterion from
the active learning literature.
1559
3.2 Sample by JS Disagreement
We propose a novel active learning sampling cri-
terion that incorporates not only disagreement but
also representativeness in selecting examples to
annotate. Prior work has taken a weighted combi-
nation of an example?s disagreement and a score
corresponding to whether the example is drawn
from a dense portion of the feature space (e.g.,
McCallum and Nigam (1998)). However, this re-
quires both selecting a criterion for defining den-
sity (e.g., distance metric in feature space), and
tuning a parameter for the relative weight of dis-
agreement versus representativeness.
Instead, we account for choosing representa-
tive examples by sampling without replacement
proportional to the example?s disagreement. For-
mally, we define the probability of selecting an
example z
(m)
i
to be proportional to the Jensen-
Shannon divergence in (1). Since the training set is
an approximation to the prior distribution over ex-
amples, sampling uniformly over the training set is
an approximation to sampling from the prior prob-
ability of seeing an input x. We can view our crite-
rion as an approximation to sampling proportional
to the product of two densities: a prior over exam-
ples x, and the JS divergence mentioned above.
4 Incorporating Sentence-Level
Annotations
Following Surdeanu et al. (2012), MIML-RE is
trained through hard discriminative Expectation
Maximization, inferring the latent z values in the
E-step and updating the weights for both the z and
y classifiers in the M-step. During the E-step, we
constrain the latent z to match our sentence-level
annotations when available.
It is worth noting that even in the hard-EM
regime, we can in principle incorporate annotator
uncertainty elegantly into the model. At each E
step, each z
i
is set according to
z
i
(m)?
? argmax
z?R
[
p(z | x
(m)
i
,w
z
) ?
?
r
p(y
(r)
i
| z
?
i
,w
(r)
y
)
]
where z
?
i
contains the inferred labels from the
previous iteration, but with its mth component re-
placed by z
(m)
i
.
By setting the distribution p(z | x
(m)
i
,w
z
) to re-
flect uncertainty among annotators, we can leave
open the possibility for the model to choose a re-
lation which annotators deemed unlikely, but the
model nonetheless prefers. For simplicity, how-
ever, we treat our annotations as a hard assign-
ment.
In addition to incorporating annotations during
training, we can also use this data to intelligently
initialize the model. Since the MIML-RE objec-
tive is non-convex, the initialization of the classi-
fier weights w
y
and w
z
is important. The y clas-
sifiers are initialized with the ?at-least-once? as-
sumption of Hoffmann et al. (2011); w
z
can be ini-
tialized either using traditional distant supervision
or from a supervised classifier trained on the an-
notated sentences. If initialized with a supervised
classifier, the model can be viewed as augment-
ing this supervised model with a large distantly
labeled corpus, providing both additional entity
pairs to train from, and additional mentions for an
annotated entity pair.
5 Crowdsourced Example Annotation
Most prior work on active learning is done by sim-
ulation on a fully labeled dataset; such a dataset
doesn?t exist for our case. Furthermore, a key aim
of this paper is to practically improve state-of-the-
art performance in relation extraction in addition
to evaluating active learning criteria. Therefore,
we develop and execute an annotation task for col-
lecting labels for our selected examples.
We utilize Amazon Mechanical Turk to crowd-
source annotations. For each task, the annotator
(Turker) is presented with the task description, fol-
lowed by 15 questions, 2 of which are randomly
placed controls. For each question, we present
Turkers with a relation mention and the top 5 re-
lation predictions from our classifier. The Turker
also has an option to freely specify a relation not
presented in the first five options, or mark that
there is no relation. We attempt to heuristically
match common free-form answers to official rela-
tions.
To maintain the quality of the results, we dis-
card all submissions in which both controls were
answered incorrectly, and additionally discard all
submissions from Turkers who failed the controls
on more than
1
3
of their submissions. Rejected
tasks were republished for other workers to com-
plete. We collect 5 annotations for each example,
and use the most commonly agreed answer as the
ground truth. Ties are broken arbitrarily, except in
1560
Figure 3: The task shown to Amazon Mechanical
Turk workers. A sentence along with the top 5 re-
lation predictions from our classifier are shown to
Turkers, as well as an option to specify a custom
relation or manually enter ?no relation.? The cor-
rect response for this example should be either no
relation or a custom relation.
the case of deciding between a relation and no re-
lation, in which case the relation was always cho-
sen.
A total of 23 725 examples were annotated, cov-
ering 10 000 examples for each of the three selec-
tion criteria. Note that there is overlap between
the examples selected for the three criteria. In ad-
dition, 10 023 examples were annotated during de-
velopment; these are included in the set of all an-
notated examples, but excluded from any of the
three criteria. The compensation per task was 23
cents; the total cost of annotating examples was
$3156, in addition to $204 spent on developing the
task. Informally, Turkers achieved an accuracy of
around 75%, as evaluated by a paper author, per-
forming disproportionately well on identifying the
no relation label.
6 Experiments
We evaluate the three high-level research contri-
butions of this work: we show that we improve
the accuracy of MIML-RE, we validate the effec-
tiveness of our selection criteria, and we provide a
corpus of annotated examples, evaluating a super-
vised classifier trained on this corpus. The train-
ing and testing methodology for evaluating these
contributions is given in Sections 6.1 and 6.2; ex-
periments are given in Section 6.3.
6.1 Training Setup
We adopt the setup of Surdeanu et al. (2012) for
training the MIML-RE model, with minor modifi-
cations. We use both the 2010 and 2013 KBP of-
ficial document collections, as well as a July 2013
dump of Wikipedia as our text corpus. We sub-
sample negatives such that
1
3
of our dataset con-
sists of entity pairs with no known relations. In all
experiments, MIML-RE is trained for 7 iterations
of EM; for efficiency, the z classifier is optimized
using stochastic gradient descent;
2
the y classifiers
are optimized using L-BFGS.
Similarly to Surdeanu et al. (2011), we as-
sign negative relations which are either incompat-
ible with the known positive relations (e.g., re-
lations whose co-occurrence would violate type
constraints); or, are actually functional relations
in which another entity already participates. For
example, if we know that Obama was born in the
United States, we could add born in as a negative
relation to the pair Obama and Kenya.
Our dataset consists of 325 891 entity pairs with
at least one positive relation, and 158 091 entity
pairs with no positive relations. Pairs with at least
one known relation have an average of 4.56 men-
tions per group; groups with no known relations
have an average of 1.55 mentions per group. In to-
tal, 1 208 524 distinct mentions are considered; the
annotated examples are selected from this pool.
6.2 Testing Methodology
We compare against the original MIML-RE model
using the same dataset and evaluation methodol-
ogy as Surdeanu et al. (2012). This allows for an
evaluation where the only free variable between
this and prior work is the predictions of the rela-
tion extractor.
Additionally, we evaluate the relation extractors
in the context of Stanford?s end-to-end KBP sys-
tem (Angeli et al., 2014) using the NIST TAC-
KBP 2013 English Slotfilling evaluation. In the
end-to-end framework, the input to the system is a
query entity and a set of articles, and the output is
a set of slot fills ? each slot fill is a candidate triple
in the knowledge base, the first element of which
is the query entity. This amounts to roughly pop-
ulating a data structure like Wikipedia infoboxes
automatically from a large corpus of text.
Importantly, an end-to-end evaluation in a top-
performing full system gives a more accurate idea
of the expected real-world gain from each model.
Both the information retrieval component provid-
ing candidates to the relation extractor, as well as
2
For the sake of consistency, the supervised classifiers and
those in Mintz++ are trained identically to the z classifiers in
MIML-RE.
1561
Method Init
Active Learning Criterion
Not Used Uniform High JS Sample JS All Available
P R F
1
P R F
1
P R F
1
P R F
1
P R F
1
Mintz++ ? 41.3 28.2 33.5 ? ? ? ?
MIML-RE
Dist 38.0 30.5 33.8 39.2 30.4 34.2 41.7 28.9 34.1 36.6 31.1 33.6 37.5 30.6 33.7
Sup 35.1 35.6 35.4 34.4 35.0 34.7 46.2 30.8 37.0 39.4 36.2 37.7 36.0 37.1 36.5
Supervised ? ? 35.5 28.9 31.9 31.3 33.2 32.2 33.5 35.0 34.2 32.9 33.4 33.2
Table 1: A summary of results on the end-to-end KBP 2013 evaluation for various experiments. The
first column denotes the algorithm used: either traditional distant supervision (Mintz++), MIML-RE, or
a supervised classifier. In the case of MIML-RE, the model may be initialized either using Mintz++, or
the corresponding supervised classifier (the ?Not Used? column is initialized with the ?All? supervised
classifier). One of five active learning scenarios are evaluated: no annotated examples provided, the three
active learning criteria, and all available examples used. The entry in blue denotes the basic MIML-RE
model; entries in gray perform worse than this model. The bold items denote the best performance
among selection criteria.
the consistency and inference performed on the
classifier output introduce bias in this evaluation?s
sensitivity to particular types of errors. Mistakes
which are easy to filter, or are difficult to retrieve
using IR are less important in this evaluation; in
contrast, factors such as providing good confi-
dence scores for consistency become more impor-
tant.
For the end-to-end evaluation, we use the offi-
cial evaluation script with two changes: First, all
systems are evaluated with provenance ignored, so
as not to penalize any system for finding a new
provenance not validated in the official evaluation
key. Second, each system reports its optimal F
1
along its P/R curve, yielding results which are
optimistic when compared against other systems
entered into the competition. However, this also
yields results which are invariant to threshold tun-
ing, and is therefore more appropriate for compar-
ing between systems in this paper.
Development was done on the KBP 2010?2012
queries, and results are reported using the 2013
queries as a simulated test set. Our best system
achieves an F
1
of 37.7; the top two teams at KBP
2013 (of 18 entered) achieved F
1
scores of 40.2
and 37.1 respectively, ignoring provenance.
6.3 Results
Table 1 summarizes all results for the end-to-end
task; relevant features of the table are copied in
subsequent sections to illustrate key trends. Mod-
els which perform worse than the original MIML-
RE model (MIML-RE, initialized with ?Dist,? un-
der ?Not Used?) are denoted in gray. The best per-
System P R F
1
Mintz++ 41.3 28.2 33.5
MIML + Dist 38.0 30.5 33.8
MIML + Sup 35.1 35.6 35.4
MIML + Dist + SampleJS 36.6 31.1 33.6
MIML + Sup + SampleJS 39.4 36.2 37.7
Table 2: A summary of improvements to MIML-
RE on the end-to-end slotfilling task, copied from
Table 1. Mintz++ is the traditional distantly su-
pervised model. The second row corresponds to
the unmodified MIML-RE model. The third row
corresponds to MIML-RE initialized with a su-
pervised classifier (trained on all examples). The
fourth row is MIML-RE with annotated exam-
ples incorporated during training (but not initial-
ization). The last row shows the best results ob-
tained by our model.
forming model improves on the base model by 3.9
F
1
points on the end-to-end task.
We evaluate each of the individual contribu-
tions of the paper: improving the accuracy of
the MIML-RE relation extractor, evaluating our
example selection criteria, and demonstrating the
annotated examples? effectiveness for a fully-
supervised relation extractor.
Improve MIML-RE Accuracy A key goal of
this work is to improve the accuracy of the MIML-
RE model; we show that we improve the model
both on the end-to-end slotfilling task (Table 2) as
well as on a standard evaluation (Figure 5). Sim-
ilar to our work, recent work by Pershina et al.
1562
System P R F
1
MIML + Sup 35.1 35.6 35.4
MIML + Sup + Uniform 34.4 35.0 34.7
MIML + Sup + HighJS 46.2 30.8 37.0
MIML + Sup + SampleJS 39.4 36.2 37.7
MIML + Sup + All 36.0 37.1 36.5
Table 3: A summary of the performance of each
example selection criterion. In each case, the
model was initialized with a supervised classifier.
The first row corresponds to the MIML-RE model
initialized with a supervised classifier. The middle
three rows show performance for the three selec-
tion criteria, used both for initialization and during
training. The last row shows results if all available
annotations are used, independent of their source.
System P R F
1
Mintz++ 41.3 28.2 33.5
MIML + Dist 38.0 30.5 33.8
Supervised + SampleJS 33.5 35.0 34.2
MIML + Sup 35.1 35.6 35.5
MIML + Sup + SampleJS 39.4 36.2 37.7
Table 4: A comparison of the best performing su-
pervised classifier with other systems. The top
section compares the supervised classifier with
prior work. The lower section highlights the im-
provements gained from initializing MIML-RE
with a supervised classifier.
(2014) incorporates labeled data to guide MIML-
RE during training. They make use of labeled data
to extract training guidelines, which are intended
to generalize across many examples. We show that
we can match or outperform their improvements
with our best criterion.
A few interesting trends emerge from the end-
to-end results in Table 2. Using annotated sen-
tences during training alone did not improve per-
formance consistently, even hurting performance
when the SampleJS criterion was used. This
supports an intuition that the initialization of the
model is important, and that it is relatively difficult
to coax the model out of a local optimum if it is
initialized poorly. This is further supported by the
improvement in performance when the model is
initialized with a supervised classifier, even when
no examples are used during training. Similar
trends are reported in prior work, e.g., Smith and
Eisner (2007) Section 4.4.6.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.05  0.1  0.15  0.2  0.25  0.3
Prec
ision
Recall
MIML-RESurdeanu et al. (2012)Mintz++
Figure 4: MIML-RE and Mintz++ evaluated ac-
cording to Surdeanu et al. (2012). The original
model from the paper is plotted for comparison, as
our training methodology is somewhat different.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.05  0.1  0.15  0.2  0.25  0.3
Prec
ision
Recall
Sample JSPershina et al. (2014)MIML-RE
Figure 5: Our best active learning criterion evalu-
ated against our version of MIML-RE, alongside
the best system of Pershina et al. (2014).
Also interesting is the relatively small gain
MIML-RE provides over traditional distant super-
vision (Mintz++) in this setting. We conjecture
that the mistakes made by Mintz++ are often rel-
atively easily filtered by the downstream consis-
tency component. This is supported by Figure 4;
we evaluate our trained MIML-RE model against
Mintz++ and the results reported in Surdeanu et
al. (2012). We show that our model performs as
well or better than the original implementation,
and consistently outperforms Mintz++.
Evaluate Selection Criteria A key objective of
this work is to evaluate how much of an impact
careful selection of annotated examples has on the
overall performance of the system. We evaluate
the three selection criteria from Section 3.2, show-
ing the results for MIML-RE in Table 3; results
for the supervised classifier are given in Table 1.
In both cases, we show that the sampled JS cri-
1563
terion performs comparably to or better than the
other criteria.
At least two interesting trends can be noted from
these results: First, the uniformly sampled crite-
rion performed worse than MIML-RE initialized
with a supervised classifier. This may be due to
noise in the annotation: a small number of an-
notation errors on entity pairs with only a single
corresponding mention could introduce dangerous
noise into training. These singleton mentions will
rarely have disagreement between the committee
of classifiers, and therefore will generally only be
selected in the uniform criterion.
Second, adding in the full set of examples did
not improve performance ? in fact, performance
generally dropped in this scenario. We conjecture
that this is due to the inclusion of the uniformly
sampled examples, with performance dropping for
the same reasons as above.
Both of these results can be reconciled with
the results of Zhang et al. (2012); like this work,
they annotated examples to analyze the trade-off
between adding more data to a distantly super-
vised system, and adding more direct supervi-
sion. They conclude that annotations provide only
a relatively small improvement in performance.
However, their examples were uniformly selected
from the training corpus, and did not make use
of the structure provided by MIML-RE. Our re-
sults agree in that neither the uniform selection
criterion nor the supervised classifier significantly
outperformed the unmodified MIML-RE model;
nonetheless, we show that if care is taken in se-
lecting these labeled examples we can achieve no-
ticeable improvements in accuracy.
We also evaluate our selection criteria on the
evaluation of Surdeanu et al. (2012), both initial-
ized with Mintz++ (Figure 7) and with the super-
vised classifier (Figure 6). These results mirror
those in the end-to-end evaluation; when initial-
ized with the supervised classifier the high dis-
agreement (High JS) and sampling proportional to
disagreement (Sample JS) criteria clearly outper-
form both the base MIML-RE model as well as
the uniformly sampling criterion. Using the an-
notated examples only during training yielded no
perceivable benefit over the base model (Figure 7).
Supervised Relation Extractor The examples
collected can be used to directly train a supervised
classifier, with results summarized in Table 4. The
most salient insight is that the performance of the
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.05  0.1  0.15  0.2  0.25  0.3
Prec
ision
Recall
Sample JSHigh JSUniformMIML-RE
Figure 6: A comparison of models trained with
various selection criteria on the evaluation of Sur-
deanu et al. (2012), all initialized with the corre-
sponding supervised classifier.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.05  0.1  0.15  0.2  0.25  0.3
Prec
ision
Recall
Sample JSHigh JSUniformMIML-RE
Figure 7: A comparison of models trained with
various selection criteria on the evaluation of Sur-
deanu et al. (2012), all initialized with Mintz++.
best supervised classifier is similar to that of the
MIML-RE model, despite being trained on nearly
two orders of magnitude less training data.
More interestingly, however, the supervised
classifier provides a noticeably better initializa-
tion for MIML-RE than Mintz++, yielding better
results even without enforcing the labels during
EM. These results suggest that the power gained
from the the more sophisticated MIML-RE model
is best used in conjunction with a small amount of
training data. That is, using MIML-RE as a princi-
pled model for combining a large distantly labeled
corpus and a small number of careful annotations
yields significant improvement over using either
of the two data sources alone.
1564
Relation # P R F
1
no relation 3073
employee of 1978 29 32 33 46 31 38
countries of res. 1061 30 42 7 40 11 41
states of residence 427 57 33 14 7 23 12
cities of residence 356 31 52 9 30 14 38
(org:)member of 290 0 0 0 0 0 0
country of hq 280 63 62 65 62 64 62
top members 221 36 26 50 60 42 36
country of birth 205 22 0 40 0 29 0
parents 196 10 26 31 54 15 35
city of hq 194 46 52 57 61 51 56
(org:)alt names 184 52 48 39 39 45 43
founded by 180 100 89 29 38 44 53
city of birth 145 17 50 8 17 11 25
state of hq 132 50 64 30 35 38 45
title 121 20 26 28 35 23 30
subsidiaries 105 33 25 6 3 10 5
founded 90 62 82 62 69 62 75
spouse 88 37 54 85 85 51 66
origin 86 42 43 68 70 51 53
state of birth 83 0 50 0 10 0 17
charges 69 54 54 16 16 24 24
cause of death 69 93 93 39 39 55 55
(per:)alt names 69 9 20 2 3 3 6
country of death 65 100 100 10 10 18 18
members 54 0 0 0 0 0 0
children 52 53 62 14 18 22 27
parents 50 64 64 28 28 39 39
city of death 38 42 75 16 19 23 30
dissolved 38 0 0 0 0 0 0
date of death 33 64 64 44 39 52 48
political affiliation 23 7 25 100 100 13 40
state of death 19 0 0 0 0 0 0
shareholders 19 0 0 0 0 0 0
siblings 16 50 50 33 33 40 40
schools attended 14 80 78 41 48 54 60
date of birth 11 100 100 85 85 92 92
other family 9 0 0 0 0 0 0
age 4 94 97 94 90 94 93
# of employees 3 0 0 0 0 0 0
religion 2 100 100 29 29 44 44
website 0 25 0 3 0 6 0
Table 5: A summary of relations annotated, and
end-to-end slotfilling performance by relation.
The first column gives the relation; the second
shows the number of examples annotated. The
subsequent columns show the performance of the
unmodified MIML-RE model and our best per-
forming model (SampleJS). Changes in values are
bolded; positive changes are shown in green and
negative changes in red. The most frequent 10 re-
lations in the evaluation are likewise bolded.
6.4 Analysis By Relation
In this section, we explore which of the KBP rela-
tions were shown to Turkers, and whether the im-
provements in accuracy correspond to these rela-
tions. We compare only the unmodified MIML-
RE model, and our best model (MIML-RE initial-
ized with the supervised classifier, under the Sam-
pleJS criterion). Results are shown in Table 5.
A few interesting trends emerge from this anal-
ysis. We note that annotating even 80+ examples
for a relation seems to provide a consistent boost
in accuracy, whereas relations with fewer anno-
tated examples tended to show little or no change.
However, the gains of our model are not univer-
sal across relation types, even dropping noticeably
on some ? for instance, F
1
drops on both state of
residence and country of birth. This could suggest
systematic noise from Turker judgments; e.g., for
foreign geography (state of residence) or ambigu-
ous relations (top members).
An additional insight from the table is the mis-
match between examples chosen to be annotated,
and the most popular relations in the KBP evalu-
ation. For instance, by far the most popular KBP
relation (title) had only 121 examples annotated.
7 Conclusion
We have shown that providing a relatively small
number of mention-level annotations can improve
the accuracy of MIML-RE, yielding an end-to-end
improvement of 3.9 F
1
on the KBP task. Further-
more, we have introduced a new active learning
criterion, and shown both that the choice of crite-
rion is important, and that our new criterion per-
forms well. Lastly, we make available a dataset of
mention-level annotations for constructing a tradi-
tional supervised relation extractor.
Acknowledgements
We thank the anonymous reviewers for their
thoughtful comments, and Dan Weld for his feed-
back on additional experiments and analysis. We
gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA)
Deep Exploration and Filtering of Text (DEFT)
Program under Air Force Research Laboratory
(AFRL) contract no. FA8750-13-2-0040. Any
opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the view of
the DARPA, AFRL, or the US government.
1565
References
Gabor Angeli, Arun Chaganty, Angel Chang, Kevin
Reschke, Julie Tibshirani, Jean Y. Wu, Osbert Bas-
tani, Keith Siilats, and Christopher D. Manning.
2014. Stanford?s 2013 KBP system. In TAC-KBP.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang
Li, Wen-Pin Lin, Matthew Snover, Javier Artiles,
Marissa Passantino, and Heng Ji. 2010. CUNY-
BLENDER. In TAC-KBP.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In AAAI.
George R Doddington, Alexis Mitchell, Mark A Przy-
bocki, Lance A Ramshaw, Stephanie Strassel, and
Ralph M Weischedel. 2004. The automatic content
extraction (ACE) program?tasks, data, and evalua-
tion. In LREC.
Aidan Finn and Nicolas Kushmerick. 2003. Active
learning selection strategies for information extrac-
tion. In International Workshop on Adaptive Text
Extraction and Mining.
Yoav Freund, H Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1992. Information, prediction, and
query by committee. In NIPS.
Yoav Freund, H Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the
query by committee algorithm. Machine learning,
28(2-3):133?168.
Lisheng Fu and Ralph Grishman. 2013. An efficient
active learning framework for new relation types. In
IJCNLP.
Ralph Grishman and Bonan Min. 2010. New York
University KBP 2010 slot-filling system. In Proc.
TAC 2010 Workshop.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In ACL.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In ACL-HLT.
David D Lewis and William A Gale. 1994. A sequen-
tial algorithm for training text classifiers. In SIGIR.
Andrew McCallum and Kamal Nigam. 1998. Employ-
ing EM and pool-based active learning for text clas-
sification. In ICML.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In NAACL-HLT.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In ACL.
Maria Pershina, Bonan Min, Wei Xu, and Ralph Gr-
ishman. 2014. Infusion of labeled data into distant
supervision for relation extraction. In ACL.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases. Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL-HLT.
Benjamin Roth and Dietrich Klakow. 2013. Feature-
based models for improving the quality of noisy
training data for relation extraction. In CIKM.
Maytal Saar-Tsechansky and Foster Provost. 2004.
Active sampling for class probability estimation and
ranking. Machine Learning, 54(2):153?178.
Burr Settles, Mark Craven, and Soumya Ray. 2008.
Multiple-instance active learning. In Advances in
neural information processing systems, pages 1289?
1296.
Burr Settles. 2010. Active learning literature survey.
University of Wisconsin Madison Technical Report
1648.
Noah Smith and Jason Eisner. 2007. Novel estimation
methods for unsupervised discovery of latent struc-
ture in natural language text. Ph.D. thesis, Johns
Hopkins.
Stephen G Soderland. 1997. Learning text analysis
rules for domain-specific natural language process-
ing. Ph.D. thesis, University of Massachusetts.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP
slot filling. In Proceedings of the Text Analytics
Conference.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
ACE07 Proceedings.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X Chang, Valentin I Spitkovsky, and
Christopher D Manning. 2011. Stanfords distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-
ati, and Christopher D. Manning. 2012. Multi-
instance multi-label learning for relation extraction.
In EMNLP.
Fei Wu and Daniel S Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the six-
teenth ACM conference on information and knowl-
edge management. ACM.
1566
Wei Xu, Le Zhao, Raphael Hoffman, and Ralph Grish-
man. 2013. Filling knowledge base gaps for distant
supervision of relation extraction. In ACL.
Ce Zhang, Feng Niu, Christopher R?e, and Jude Shav-
lik. 2012. Big data versus the crowd: Looking for
relationships in all the right places. In ACL.
1567
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 446?455,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Parsing Time: Learning to Interpret Time Expressions
Gabor Angeli
Stanford University
Stanford, CA 94305
angeli@stanford.edu
Christopher D. Manning
Stanford University
Stanford, CA 94305
manning@stanford.edu
Daniel Jurafsky
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
We present a probabilistic approach for learn-
ing to interpret temporal phrases given only a
corpus of utterances and the times they ref-
erence. While most approaches to the task
have used regular expressions and similar lin-
ear pattern interpretation rules, the possibil-
ity of phrasal embedding and modification in
time expressions motivates our use of a com-
positional grammar of time expressions. This
grammar is used to construct a latent parse
which evaluates to the time the phrase would
represent, as a logical parse might evaluate to
a concrete entity. In this way, we can employ
a loosely supervised EM-style bootstrapping
approach to learn these latent parses while
capturing both syntactic uncertainty and prag-
matic ambiguity in a probabilistic framework.
We achieve an accuracy of 72% on an adapted
TempEval-2 task ? comparable to state of the
art systems.
1 Introduction
Temporal resolution is the task of mapping from
a textual phrase describing a potentially complex
time, date, or duration to a normalized (grounded)
temporal representation. For example, possibly
complex phrases such as the week before last are
often more useful in their grounded form ? e.g.,
January 1 - January 7.
The dominant approach to this problem in previ-
ous work has been to use rule-based methods, gen-
erally a combination of regular-expression matching
followed by hand-written interpretation functions.
In general, it is appealing to learn the interpre-
tation of temporal expressions, rather than hand-
building systems. Moreover, complex hierarchical
temporal expressions, such as the Tuesday before
last or the third Wednesday of each month, and am-
biguous expressions, such as last Friday, are diffi-
cult to handle using deterministic rules and would
benefit from a recursive and probabilistic phrase
structure representation. Therefore, we attempt to
learn a temporal interpretation system where tempo-
ral phrases are parsed by a grammar, but this gram-
mar and its semantic interpretation rules are latent,
with only the input phrase and its grounded interpre-
tation given to the learning system.
Employing probabilistic techniques allows us to
capture ambiguity in temporal phrases in two impor-
tant respects. In part, it captures syntactic ambigu-
ity ? e.g., last Friday the 13th bracketing as either
[last Friday] [the 13th], or last [Friday the 13th].
This also includes examples of lexical ambiguity ?
e.g., two meanings of last in last week of November
versus last week. In addition, temporal expressions
often carry a pragmatic ambiguity. For instance, a
speaker may refer to either the next or previous Fri-
day when he utters Friday on a Sunday. Similarly,
next week can refer to either the coming week or the
week thereafter.
Probabilistic systems furthermore allow propaga-
tion of uncertainty to higher-level components ? for
example recognizing that May could have a num-
ber of non-temporal meanings and allowing a sys-
tem with a broader contextual scope to make the fi-
nal judgment. We implement a CRF to detect tem-
poral expressions, and show our model?s ability to
act as a component in such a system.
We describe our temporal representation, fol-
lowed by the learning algorithm; we conclude with
experimental results showing our approach to be
competitive with state of the art systems.
446
2 Related Work
Our approach draws inspiration from a large body of
work on parsing expressions into a logical form. The
latent parse parallels the formal semantics in previ-
ous work, e.g., Montague semantics. Like these rep-
resentations, a parse ? in conjunction with the refer-
ence time ? defines a set of matching entities, in this
case the grounded time. The matching times can be
thought of as analogous to the entities in a logical
model which satisfy a given expression.
Supervised approaches to logical parsing promi-
nently include Zelle and Mooney (1996), Zettle-
moyer and Collins (2005), Kate et al (2005), Zettle-
moyer and Collins (2007), inter alia. For exam-
ple, Zettlemoyer and Collins (2007) learn a mapping
from textual queries to a logical form. This logical
form importantly contains all the predicates and en-
tities used in their parse. We loosen the supervision
required in these systems by allowing the parse to be
entirely latent; the annotation of the grounded time
neither defines, nor gives any direct cues about the
elements of the parse, since many parses evaluate to
the same grounding. To demonstrate, the grounding
for a week ago could be described by specifying a
month and day, or as a week ago, or as last x ? sub-
stituting today?s day of the week for x. Each of these
correspond to a completely different parse.
Recent work by Clarke et al (2010) and Liang et
al. (2011) similarly relax supervision to require only
annotated answers rather than full logical forms. For
example, Liang et al (2011) constructs a latent parse
similar in structure to a dependency grammar, but
representing a logical form. Our proposed lexi-
cal entries and grammar combination rules can be
thought of as paralleling the lexical entries and pred-
icates, and the implicit combination rules respec-
tively in this framework. Rather than querying from
a finite database, however, our system must com-
pare temporal expression within an infinite timeline.
Furthermore, our system is run using neither lexical
cues nor intelligent initialization.
Related work on interpreting temporal expres-
sions has focused on constructing hand-crafted in-
terpretation rules (Mani and Wilson, 2000; Saquete
et al, 2003; Puscasu, 2004; Grover et al, 2010). Of
these, HeidelTime (Stro?tgen and Gertz, 2010) and
SUTime (Chang and Manning, 2012) provide par-
ticularly strong competition.
Recent probabilistic approaches to temporal reso-
lution include UzZaman and Allen (2010), who em-
ploy a parser to produce deep logical forms, in con-
junction with a CRF classifier. In a similar vein,
Kolomiyets and Moens (2010) employ a maximum
entropy classifier to detect the location and temporal
type of expressions; the grounding is then done via
deterministic rules.
3 Representation
We define a compositional representation of time;
a type system is described in Section 3.1 while the
grammar is outlined in Section 3.2 and described in
detail in Sections 3.3 and 3.4.
3.1 Temporal Expression Types
We represent temporal expressions as either a
Range, Sequence, or Duration. We describe these,
the Function type, and the miscellaneous Number
and Nil types below:
Range [and Instant] A period between two dates
(or times). This includes entities such as Today,
1987, or Now. We denote a range by the variable
r. We maintain a consistent interval-based theory of
time (Allen, 1981) and represent instants as intervals
with zero span.
Sequence A sequence of Ranges, not necessarily
occurring at regular intervals. This includes enti-
ties such as Friday, November 27th, or last
Friday. A Sequence is a tuple of three elements
s = (rs,?s, ?s):
1. rs(i): The ith element of a sequence, of type
Range. In the case of the sequence Friday,
rs(0) corresponds to the Friday in the current
week; rs(1) is the Friday in the following week,
etc.
2. ?s: The distance between two elements in the
sequence ? approximated if this distance is not
constant. In the case of Friday, this distance
would be a week.
3. ?s: The containing unit of an element of a se-
quence. For example, ?Friday would be the
Range corresponding to the current week. The
sequence index i ? Z, from rs(i), is defined
447
relative to rs(0) ? the element in the same con-
taining unit as the reference time.
We define the reference time t (Reichenbach,
1947) to be the instant relative to which times are
evaluated. For the TempEval-2 corpus, we approxi-
mate this as the publication time of the article. While
this is conflating Reichenbach?s reference time with
speech time, it is a useful approximation.
To contrast with Ranges, a Sequence can rep-
resent a number of grounded times. Nonetheless,
pragmatically, not all of these are given equal weight
? an utterance of last Friday may mean either of the
previous two Fridays, but is unlikely to ground to
anything else. We represent this ambiguity by defin-
ing a distribution over the elements of the Sequence.
While this could be any distribution, we chose to ap-
proximate it as a Gaussian.
In order to allow sharing parameters between any
sequence, we define the domain in terms of the index
of the sequence rather than of a constant unit of time
(e.g., seconds). To illustrate, the distribution over
April would have a much larger variance than the
distribution over Sunday, were the domains fixed.
The probability of the ith element of a sequence thus
depends on the beginning of the range rs(i), the ref-
erence time t, and the distance between elements of
the sequence ?s. We summarize this in the equation
below, with learned parameters ? and ?:
Pt(i) =
? 0.5
?=?0.5
N?,?
(
rs(i)? t
?s
+ ?
)
(1)
Figure 1 shows an example of such a distribution;
importantly, note that moving the reference time be-
tween two elements dynamically changes the prob-
ability assigned to each.
Duration A period of time. This includes entities
like Week, Month, and 7 days. We denote a du-
ration with the variable d.
We define a special case of the Duration type to
represent approximate durations, identified by their
canonical unit (week, month, etc). These are used
to represent expressions such as a few years or some
days.
Function A function of arity less than or equal to
two representing some general modification to one
-2
11/13
-1
11/20
-0.3
t
?s
1
12/4
2
12/11
Reference time
P (11/20) = ? ?0.5?1.5 f(x)
0
11/27
ff
ff
Figure 1: An illustration of a temporal distribution, e.g.,
Sunday. The reference time is labeled as time t between
Nov 20 and Nov 27; the probability that this sequence
is referring to Nov 20 is the integral of the marked area.
The domain of the graph are the indices of the sequence;
the distribution is overlaid with mean at the (normalized)
reference time t/?s; in our case ?s is a week. Note
that the probability of an index changes depending on the
exact location of the reference time.
of the above types. This captures semantic entities
such as those implied in last x, the third x [of y],
or x days ago. The particular functions and their
application are enumerated in Table 2.
Other Types Two other types bear auxiliary roles
in representing temporal expressions, though they
are not directly temporal concepts. In the grammar,
these appear as preterminals only.
The first of these types is Number ? denoting
a number without any temporal meaning attached.
This comes into play representing expressions such
as 2 weeks. The other is the Nil type ? denoting
terms which are not directly contributing to the se-
mantic meaning of the expression. This is intended
for words such as a or the, which serve as cues with-
out bearing temporal content themselves. The Nil
type is lexicalized with the word it generates.
Omitted Phenomena The representation de-
scribed is a simplification of the complexities of
time. Notably, a body of work has focused on
reasoning about events or states relative to temporal
expressions. Moens and Steedman (1988) describes
temporal expressions relating to changes of state;
Condoravdi (2010) explores NPI licensing in
temporal expressions. Broader context is also not
448
Range
f(Duration) : Range
catRight
next
Duration
Number
Numn?100
2
Duration
Day
days(a)
catRight(t, 2D )
catRight(t,?)
next
2D
Num(2)
2
1D
days(b)
Figure 2: The grammar ? (a) describes the CFG parse of
the temporal types. Words are tagged with their nontermi-
nal entry, above which only the types of the expressions
are maintained; (b) describes the corresponding combi-
nation of the temporal instances. The parse in (b) is de-
terministic given the grammar combination rules in (a).
directly modeled, but rather left to systems in which
the model would be embedded. Furthermore, vague
times (e.g., in the 90?s) represent a notable chunk
of temporal expressions uttered. In contrast, NLP
evaluations have generally not handled such vague
time expressions.
3.2 Grammar Formalism
Our approach builds on the assumption that natural
language descriptions of time are compositional in
nature. Each word attached to a temporal phrase is
usually compositionally modifying the meaning of
the phrase. To demonstrate, we consider the expres-
sion the week before last week. We can construct a
meaning by applying the modifier last to week ? cre-
ating the previous week; and then applying before to
week and last week.
We construct a paradigm for parsing temporal
phrases consisting of a standard PCFG over tempo-
ral types with each parse rule defining a function to
apply to the child nodes, or the word being gener-
ated. At the root of the tree, we recursively apply
the functions in the parse tree to obtain a final tem-
poral value. One can view this formalism as a rule-
to-rule translation (Bach, 1976; Allen, 1995, p. 263),
or a constrained Synchronous PCFG (Yamada and
Knight, 2001).
Our approach contrasts with common approaches,
such as CCG grammars (Steedman, 2000; Bos et
al., 2004; Kwiatkowski et al, 2011), giving us more
flexibility in the composition rules. Figure 2 shows
an example of the grammar.
Formally, we define our temporal grammar
G = (?, S,V,W,R, ?). The alphabet ? and start
symbol S retain their usual interpretations. We de-
fine a set V to be the set of types, as described in
Section 3.1 ? these act as our nonterminals. For each
v ? V we define an (infinite) set Wv corresponding
to the possible instances of type v. Concretely, if
v = Sequence, our set Wv ? W could contain el-
ements corresponding to Friday, last Friday, Nov.
27th, etc. Each node in the tree defines a pair (v, w)
such that w ? Wv, with combination rules defined
over v and function applications performed on w.
A rule R ? R is defined as a pair
R =
(
vi ? vjvk, f : (Wvj ,Wvk)?Wvi
)
. The
first term is our conventional PCFG rule over the
types V . The second term defines the function to
apply to the values returned recursively by the child
nodes. Note that this definition is trivially adapted
for the case of unary rules.
The last term in our grammar formalism denotes
the rule probabilities ?. In line with the usual in-
terpretation, this defines a probability of applying a
particular rule r ? R. Importantly, note that the
distribution over possible groundings of a temporal
expression are not included in the grammar formal-
ism. The learning of these probabilities is detailed
in Section 4.
3.3 Preterminals
We define a set of preterminals, specifying their
eventual type, as well as the temporal instance it pro-
duces when its function is evaluated on the word it
generates (e.g., f(day) = Day). A distinction is
made in our description between entities with con-
tent roles versus entities with a functional role.
The first ? consisting of Ranges, Sequences, and
Durations ? are listed in Table 1. A total of 62 such
preterminals are defined in the implemented system,
corresponding to primitive entities often appearing
in newswire, although this list is easily adaptable to
449
Function Description Signature(s)
shiftLeft Shift a Range or Sequence left by a Duration f : S,D? S; f : R,D? R
shiftRight Shift a Range or Sequence right by a Duration f : S,D? S; f : R,D? R
shrinkBegin Take the first Duration of a Range/Sequence f : S,D? S; f : R,D? R
shrinkEnd Take the last Duration of a Range/Sequence f : S,D? S; f : R,D? R
catLeft Take Duration units after the end of a Range f : R,D? R
catRight Take Duration units before the start of a Range f : R,D? R
moveLeft1 Move the origin of a sequence left by 1 f : S? S
moveRight1 Move the origin of a sequence right by 1 f : S? S
nth x of y Take the nth Sequence in y (Day of Week, etc) f : Number? S
approximate Make a Duration approximate f : D? D
Table 2: The functional preterminals of the grammar; R, S, and D denote Ranges Sequences and Durations respec-
tively. The name, a brief description, and the type signature of the function (as used in parsing) are given. Described
in more detail in Section 3.4, the functions are most easily interpreted as operations on either an interval or sequence.
Type Instances
Range Past, Future, Yesterday,
Tomorrow, Today, Reference,
Year(n), Century(n)
Sequence Friday, January, . . .
DayOfMonth, DayOfWeek, . . .
EveryDay, EveryWeek, . . .
Duration Second, Minute, Hour,
Day, Week, Month, Quarter,
Year, Decade, Century
Table 1: The content-bearing preterminals of the gram-
mar, arranged by their types. Note that the Sequence
type contains more elements than enumerated here; how-
ever, only a few of each characteristic type are shown here
for brevity.
fit other domains. It should be noted that the expres-
sions, represented in Typewriter, have no a pri-
ori association with words, denoted by italics; this
correspondence must be learned. Furthermore, enti-
ties which are subject to interpretation ? for example
Quarter or Season ? are given a concrete inter-
pretation. The nth quarter is defined by evenly split-
ting a year into four; the seasons are defined in the
same way but with winter beginning in December.
The functional entities are described in Table 2,
and correspond to the Function type. The majority
of these mirror generic operations on intervals on a
timeline, or manipulations of a sequence. Notably,
like intervals, times can be moved (3 weeks ago) or
their size changed (the first two days of the month),
or a new interval can be started from one of the end-
points (the last 2 days). Additionally, a sequence can
be modified by shifting its origin (last Friday), or
taking the nth element of the sequence within some
bound (fourth Sunday in November).
The lexical entry for the Nil type is tagged with the
word it generates, producing entries such as Nil(a),
Nil(November), etc. The lexical entry for the Num-
ber type is parameterized by the order of magnitude
and ordinality of the number; e.g., 27th becomes
Number(101,ordinal).
3.4 Combination Rules
As mentioned earlier, our grammar defines both
combination rules over types (in V) as well as a
method for combining temporal instances (in Wv ?
W). This method is either a function application of
one of the functions in Table 2, a function which is
implicit in the text (intersection and multiplication),
or an identity operation (for Nils). These cases are
detailed below:
? Function application, e.g., last week. We apply
(or partially apply) a function to an argument
on either the left or the right: f(x, y)x or x
f(x, y). Furthermore, for functions of arity 2
taking a Range as an argument, we define a rule
treating it as a unary function with the reference
time taking the place of the second argument.
? Intersecting two ranges or sequences, e.g.,
450
Input (w,t) ( Last Friday the 13 th , May 16 2011 )
LatentparseR
moveLeft1( FRI ) ? 13th
moveLeft1( FRI )
moveLeft1(?)
last
FRI
friday
13th
Nilthe
the
13th
13th
Output ?? May 13 2011
Figure 3: An overview of the system architecture. Note
that the parse is latent ? that is, it is not annotated in the
training data.
November 27th. The intersect function treats
both arguments as intervals, and will return an
interval (Range or Sequence) corresponding to
the overlap between the two.1
? Multiplying a Number with a Duration, e.g., 5
weeks.
? Combining a non-Nil and Nil element with no
change to the temporal expression, e.g., a week.
The lexicalization of the Nil type allows the
algorithm to take hints from these supporting
words.
We proceed to describe learning the parameters of
this grammar.
4 Learning
We present a system architecture, described in Fig-
ure 3. We detail the inference procedure in Sec-
tion 4.1 and training in Section 4.2.
4.1 Inference
To provide a list of candidate expressions with their
associated probabilities, we employ a k-best CKY
parser. Specifically, we implement Algorithm 3 de-
scribed in Huang and Chiang (2005), providing an
O(Gn3k log k) algorithm with respect to the gram-
mar size G, phrase length n, and beam size k. We
set the beam size to 2000.
1In the case of complex sequences (e.g., Friday the 13th) an
A? search is performed to find overlapping ranges in the two
sequences; the origin rs(0) is updated to refer to the closest
such match to the reference time.
Revisiting the notion of pragmatic ambiguity, in
a sense the most semantically complete output of
the system would be a distribution ? an utterance of
Friday would give a distribution over Fridays rather
than a best guess of its grounding. However, it is of-
ten advantageous to ground to a concrete expression
with a corresponding probability. The CKY k-best
beam and the temporal distribution ? capturing syn-
tactic and pragmatic ambiguity ? can be combined to
provide a Viterbi decoding, as well as its associated
probability.
We define the probability of a syntactic parse
y making use of rules R ? R as P (y) =
P (w1, . . . wn;R) =
?
i?j,k?R P (j, k | i). As de-
scribed in Section 3.1, we define the probability of
a grounding relative to reference time t and a par-
ticular syntactic interpretation Pt(i|y). The prod-
uct of these two terms provides the probability of
a grounded temporal interpretation; we can obtain a
Viterbi decoding by maximizing this joint probabil-
ity:
Pt(i, y) = P (y)? Pt(i|y) (2)
This provides us with a framework for obtaining
grounded times from a temporal phrase ? in line with
the annotations provided during training time.
4.2 Training
We present an EM-style bootstrapping approach to
training the parameters of our grammar jointly with
the parameters of our Gaussian temporal distribu-
tion.
Our TimEM algorithm for learning the parame-
ters for the grammar (?), jointly with the temporal
distribution (? and ?) is given in Algorithm 1. The
inputs to the algorithm are the initial parameters ?,
?, and ?, and a set of training instances D. Further-
more, the algorithm makes use of a Dirichlet prior ?
on the grammar parameters ?, as well as a Gaussian
prior N on the mean of the temporal distribution ?.
The algorithm outputs the final parameters ??, ??
and ??.
Each training instance is a tuple consisting of
the words in the temporal phrase w, the annotated
grounded time ??, and the reference time of the ut-
terance t. The input phrase is tokenized according
to Penn Treebank guidelines, except we additionally
451
Algorithm 1: TimEM
Input: Initial parameters ?, ?, ?; data
D = {(w, ??, t)}; Dirichlet prior ?,
Gaussian prior N
Output: Optimal parameters ??, ??, ??
while not converged do1
(M??, M??,?) := E-Step (D,?,?,?)2
(?, ?, ?) := M-Step (M??, M??,?)3
end4
return (?s, ?, ?)5
begin E-Step(D,?,?,?)6
M?? = []; M??,? = []7
for (w, ??, t) ? D do8
m?? = []; m??,? = []9
for y ? k-bestCKY(w, ?) do10
if p = P?,?(?? | y, t) > 0 then11
m?? += (y, p); m??,? += (i, p)12
end13
end14
M? += normalize(m??)15
M??,? += normalize(m??,?)16
end17
return M?18
end19
begin M-Step(M??,M??,?)20
?? := bayesianPosterior(M??, ?)21
?? := mlePosterior(M??,?)22
?? := bayesianPosterior(M??,?, ??, N )23
return (??, ??, ??)24
end25
split on the characters ?-? and ?/,? which often de-
limit a boundary between temporal entities. Beyond
this preprocessing, no language-specific information
about the meanings of the words are introduced, in-
cluding syntactic parses, POS tags, etc.
The algorithm operates similarly to the EM algo-
rithms used for grammar induction (Klein and Man-
ning, 2004; Carroll and Charniak, 1992). How-
ever, unlike grammar induction, we are allowed a
certain amount of supervision by requiring that the
predicted temporal expression match the annotation.
Our expected statistics are therefore more accurately
our normalized expected counts of valid parses.
Note that in conventional grammar induction, the
expected sufficient statistics can be gathered analyt-
ically from reading off the chart scores of a parse.
This does not work in our case for two reasons. In
part, we would like to incorporate the probability
of the temporal grounding in our feedback probabil-
ity. Additionally, we are only using parses which are
valid candidates ? that is, the parses which ground to
the correct time ?? ? which we cannot establish until
the entire expression is parsed. The expected statis-
tics are thus computed non-analytically via a beam
on both the possible parses (line 10) and the pos-
sible temporal groundings of a given interpretation
(line 11).
The particular EM updates are the standard up-
dates for multinomial and Gaussian distributions
given fully observed data. In the multinomial case,
our (unnormalized) parameter updates, with Dirich-
let prior ?, are:
??mn|l = ?+
?
(y,p)?M??
?
vjk|i?y
1
(
vjk|i = vmn|l
)
p (3)
In the Gaussian case, the parameter update for ?
is the maximum likelihood update; while the update
for ? incorporates a Bayesian prior N (?0, ?0):
?? =
?
?
?
?
1
?
(i,p)?M??,?
p
?
(i,p)?M??,?
(i? ??)2 ? p (4)
?? =
??2?0 + ?20
?
(i,p)?M??,? i ? p
??2 + ?20
?
(i,p)?M??,? p
(5)
As the parameters improve, the parser more effi-
ciently prunes incorrect parses and the beam incor-
porates valid parses for longer and longer phrases.
For instance, in the first iteration the model must
learn the meaning of both words in last Friday; once
the parser learns the meaning of one of them ? e.g.,
Friday appears elsewhere in the corpus ? subsequent
iterations focus on proposing candidate meanings
for last. In this way, a progressively larger percent-
age of the data is available to be learned from at each
iteration.
5 Evaluation
We evaluate our model against current state-of-the
art systems for temporal resolution on the English
452
Train Test
System Type Value Type Value
GUTime 0.72 0.46 0.80 0.42
SUTime 0.85 0.69 0.94 0.71
HeidelTime 0.80 0.67 0.85 0.71
OurSystem 0.90 0.72 0.88 0.72
Table 3: TempEval-2 Attribute scores for our system and
three previous systems. The scores are calculated us-
ing gold extents, forcing a guessed interpretation for each
parse.
portion of the TempEval-2 Task A dataset (Verhagen
et al, 2010).
5.1 Dataset
The TempEval-2 dataset is relatively small, contain-
ing 162 documents and 1052 temporal phrases in the
training set and an additional 20 documents and 156
phrases in the evaluation set. Each temporal phrase
was annotated as a TIMEX32 tag around an adver-
bial or prepositional phrase
5.2 Results
In the TempEval-2 A Task, system performance is
evaluated on detection and resolution of expressions.
Since we perform only the second of these, we eval-
uate our system assuming gold detection.
Similarly, the original TempEval-2 scoring
scheme gave a precision and recall for detection,
and an accuracy for only the temporal expressions
attempted. Since our system is able to produce a
guess for every expression, we produce a precision-
recall curve on which competing systems are plotted
(see Figure 4). Note that the downward slope of the
curve indicates that the probabilities returned by the
system are indicative of its confidence ? the prob-
ability of a parse correlates with the probability of
that parse being correct.
Additionally, and perhaps more accurately, we
compare to previous system scores when con-
strained to make a prediction on every example; if
no guess is made, the output is considered incorrect.
This in general yields lower results, as the system
is not allowed to abstain on expressions it does not
2See http://www.timeml.org for details on the
TimeML format and TIMEX3 tag.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Valu
e ac
cura
cy
Extent recall
HeidelTime1
HeidelTime2
SUTime
OurSystem
Figure 4: A precision-recall curve for our system, com-
pared to prior work. The data points are obtained by set-
ting a threshold minimum probability at which to guess
a time creating different extent recall values. The curve
falls below HeidelTime1 and SUTime in part from lack
of context, and in part since our system was not trained
to optimize this curve.
recognize. Results are summarized in Table 3.
We compare to three previous rule-based sys-
tems. GUTime (Mani and Wilson, 2000) presents an
older but widely used baseline.3 More recently, SU-
Time (Chang and Manning, 2012) provides a much
stronger comparison. We also compare to Heidel-
Time (Stro?tgen and Gertz, 2010), which represents
the state-of-the-art system at the TempEval-2 task.
5.3 Detection
One of the advantages of our model is that it can pro-
vide candidate groundings for any expression. We
explore this ability by building a detection model to
find candidate temporal expressions, which we then
ground. The detection model is implemented as a
Conditional Random Field (Lafferty et al, 2001),
with features over the morphology and context. Par-
ticularly, we define the following features:
? The word and lemma within 2 of the current
word.
? The word shape4 and part of speech of the cur-
rent word.
3Due to discrepancies in output formats, the output of
GUTime was heuristically patched and manually checked to
conform to the expected format.
4Word shape is calculated by mapping each character to one
of uppercase, lowercase, number, or punctuation. The first four
characters are mapped verbatim; subsequent sequences of sim-
ilar characters are collapsed.
453
Extent Attribute
System P R F1 Typ Val
GUTime 0.89 0.79 0.84 0.95 0.68
SUTime 0.88 0.96 0.92 0.96 0.82
HeidelTime1 0.90 0.82 0.86 0.96 0.85
HeidelTime2 0.82 0.91 0.86 0.92 0.77
OurSystem 0.89 0.84 0.86 0.91 0.72
Table 4: TempEval-2 Extent scores for our system and
three previous systems. Note that the attribute scores are
now relatively low compared to previous work; unlike
rule-based approaches, our model can guess a temporal
interpretation for any phrase, meaning that a good pro-
portion of the phrases not detected would have been in-
terpreted correctly.
? Whether the current word is a number, along
with its ordinality and order of magnitude
? Prefixes and suffixes up to length 5, along with
their word shape.
We summarize our results in Table 4, noting that
the performance indicates that the CRF and interpre-
tation model find somewhat different phrases hard to
detect and interpret respectively. Many errors made
in detection are attributable to the small size of the
training corpus (63,000 tokens).
5.4 Discussion
Our system performs well above the GUTime base-
line and is competitive with both of the more recent
systems. In part, this is from more sophisticated
modeling of syntactic ambiguity: e.g., the past few
weeks has a clause the past ? which, alone, should
be parsed as PAST ? yet the system correctly dis-
prefers incorporating this interpretation and returns
the approximate duration 1 week. Furthermore,
we often capture cases of pragmatic ambiguity ? for
example, empirically, August tends to refers to the
previous August when mentioned in February.
Compared to rule-based systems, we attribute
most errors the system makes to either data spar-
sity or missing lexical primitives. For example ?
illustrating sparsity ? we have trouble recognizing
Nov. as corresponding to November (e.g., Nov. 13),
since the publication time of the articles happen to
often be near November and we prefer tagging the
word as Nil (analogous to the 13th). Missing lexi-
cal primitives, in turn, include tags for 1990s, or half
(in minute and a half ); as well as missing functions,
such as or (in weeks or months).
Remaining errors can be attributed to causes such
as providing the wrong Viterbi grounding to the
evaluation script (e.g., last rather than this Friday),
differences in annotation (e.g., 24 hours is marked
wrong against a day), or missing context (e.g., the
publication time is not the true reference time),
among others.
6 Conclusion
We present a new approach to resolving temporal ex-
pressions, based on synchronous parsing of a fixed
grammar with learned parameters and a composi-
tional representation of time. The system allows
for output which captures uncertainty both with re-
spect to the syntactic structure of the phrase and the
pragmatic ambiguity of temporal utterances. We
also note that the approach is theoretically better
adapted for phrases more complex than those found
in TempEval-2.
Furthermore, the system makes very few
language-specific assumptions, and the algorithm
could be adapted to domains beyond temporal
resolution. We hope to improve detection and
explore system performance on multilingual and
complex datasets in future work.
Acknowledgements The authors would like to thank Valentin
Spitkovsky, David McClosky, and Angel Chang for valuable
discussion and insights. We gratefully acknowledge the support
of the Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the view of DARPA, AFRL, or the US government.
References
James F. Allen. 1981. An interval-based representa-
tion of temporal knowledge. In Proceedings of the
7th international joint conference on Artificial intelli-
gence, pages 221?226, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
James Allen. 1995. Natural Language Understanding.
Benjamin/Cummings, Redwood City, CA.
454
E. Bach. 1976. An extension of classical transforma-
tional grammar. In Problems of Linguistic Metatheory
(Proceedings of the 1976 Conference), Michigan State
University.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In
Proceedings of Coling, pages 1240?1246, Geneva,
Switzerland. COLING.
Glenn Carroll and Eugene Charniak. 1992. Two experi-
ments on learning probabilistic dependency grammars
from corpora. Technical report, Providence, RI, USA.
Angel Chang and Chris Manning. 2012. SUTIME: a
library for recognizing and normalizing time expres-
sions. In Language Resources and Evaluation.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In CoNLL, pages 18?27, Uppsala,
Sweden.
Cleo Condoravdi. 2010. NPI licensing in temporal
clauses. Natural Language and Linguistic Theory,
28:877?910.
Claire Grover, Richard Tobin, Beatrice Alex, and Kate
Byrne. 2010. Edinburgh-LTG: TempEval-2 system
description. In Proceedings of the 5th International
Workshop on Semantic Evaluation, Sem-Eval, pages
333?336.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology, Parsing, pages 53?
64.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to transform natural to formal lan-
guages. In AAAI, pages 1062?1068, Pittsburgh, PA.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL.
Oleksandr Kolomiyets and Marie-Francine Moens. 2010.
KUL: recognition and normalization of temporal ex-
pressions. In Proceedings of the 5th International
Workshop on Semantic Evaluation, Sem-Eval ?10,
pages 325?328.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generalization
in CCG grammar induction for semantic parsing. In
EMNLP, pages 1512?1523, Edinburgh, Scotland, UK.
J. Lafferty, A. McCallum, and F Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In International
Conference on Machine Learning (ICML).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In ACL.
Inderjeet Mani and George Wilson. 2000. Robust tem-
poral processing of news. In ACL, pages 69?76, Hong
Kong.
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational Lin-
guistics, 14:15?28.
G. Puscasu. 2004. A framework for temporal resolution.
In LREC, pages 1901?1904.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Macmillan, New York.
E. Saquete, R. Muoz, and P. Martnez-Barco. 2003.
Terseo: Temporal expression resolution system ap-
plied to event ordering. In Text, Speech and Dialogue,
pages 220?228.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Jannik Stro?tgen and Michael Gertz. 2010. Heideltime:
High quality rule-based extraction and normalization
of temporal expressions. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation, Sem-
Eval, pages 321?324.
Naushad UzZaman and James F. Allen. 2010. TRIPS
and TRIOS system for TempEval-2: Extracting tem-
poral information from text. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Sem-
Eval, pages 276?283.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
TempEval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62, Up-
psala, Sweden.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL, pages 523?530.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic pro-
gramming. In AAAI/IAAI, pages 1050?1055, Portland,
OR.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
UAI, pages 658?666. AUAI Press.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing to
logical form. In EMNLP-CoNLL, pages 678?687.
455
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 83?92,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Language-Independent Discriminative Parsing of Temporal Expressions
Gabor Angeli
Stanford University
Stanford, CA 94305
angeli@stanford.edu
Jakob Uszkoreit
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94303
uszkoreit@google.com
Abstract
Temporal resolution systems are tradition-
ally tuned to a particular language, re-
quiring significant human effort to trans-
late them to new languages. We present
a language independent semantic parser
for learning the interpretation of tempo-
ral phrases given only a corpus of utter-
ances and the times they reference. We
make use of a latent parse that encodes
a language-flexible representation of time,
and extract rich features over both the
parse and associated temporal semantics.
The parameters of the model are learned
using a weakly supervised bootstrapping
approach, without the need for manually
tuned parameters or any other language
expertise. We achieve state-of-the-art ac-
curacy on all languages in the TempEval-
2 temporal normalization task, reporting
a 4% improvement in both English and
Spanish accuracy, and to our knowledge
the first results for four other languages.
1 Introduction
Temporal resolution is the task of mapping from
a textual phrase describing a potentially complex
time, date, or duration to a normalized (grounded)
temporal representation. For example, possibly
complex phrases such as the week before last1 are
often more useful in their grounded form ? e.g.,
August 4 - August 11.
Many approaches to this problem make
use of rule-based methods, combining regular-
expression matching and hand-written interpreta-
tion functions. In contrast, we would like to learn
the interpretation of a temporal expression proba-
bilistically. This allows propagation of uncertainty
to higher-level components, and the potential to
1Spoken on, for instance, August 20.
dynamically back off to a rule-based system in the
case of low confidence parses. In addition, we
would like to use a representation of time which is
broadly applicable to multiple languages, without
the need for language-specific rules or manually
tuned parameters.
Our system requires annotated data consist-
ing only of an input phrase and an associ-
ated grounded time, relative to some reference
time; the language-flexible parse is entirely latent.
Training data of this weakly-supervised form is
generally easier to collect than the alternative of
manually creating and tuning potentially complex
interpretation rules.
A large number of languages conceptualize time
as lying on a one dimensional line. Although
the surface forms of temporal expressions differ,
the basic operations many languages use can be
mapped to operations on this time line (see Sec-
tion 3). Furthermore, many common languages
share temporal units (hours, weekdays, etc.). By
structuring a latent parse to reflect these seman-
tics, we can define a single model which performs
well on multiple languages.
A discriminative parsing model allows us to de-
fine sparse features over not only lexical cues but
also the temporal value of our prediction. For ex-
ample, it allows us to learn that we are much more
likely to express March 14th than 2pm in March ?
despite the fact that both interpretations are com-
posed of similar types of components. Further-
more, it allows us to define both sparse n-gram and
denser but less informative bag-of-words features
over multi-word phrases, and allows us to handle
numbers in a flexible way.
We briefly describe our temporal representation
and grammar, followed by a description of the
learning algorithm; we conclude with experimen-
tal results on the six languages of the TempEval-2
A task.
83
2 Related Work
Our approach follows the work of Angeli et al
(2012), both in the bootstrapping training method-
ology and the temporal grammar. Our foremost
contributions over this prior work are: (i) the uti-
lization of a discriminative parser trained with rich
features; (ii) simplifications to the temporal gram-
mar which nonetheless maintain high accuracy;
and (iii) experimental results on 6 different lan-
guages, with state-of-the-art performance on both
datasets on which we know of prior work.
As in this previous work, our approach draws
inspiration from work on semantic parsing. The
latent parse parallels the formal semantics in pre-
vious work. Supervised approaches to semantic
parsing prominently include Zelle and Mooney
(1996), Zettlemoyer and Collins (2005), Kate et
al. (2005), Zettlemoyer and Collins (2007), inter
alia. For example, Zettlemoyer and Collins (2007)
learn a mapping from textual queries to a logical
form. Importantly, the logical form of these parses
contain all of the predicates and entities used in
the parse ? unlike the label provided in our case,
where a grounded time can correspond to any of
a number of latent parses. Along this line, re-
cent work by Clarke et al (2010) and Liang et al
(2011) relax supervision to require only annotated
answers rather than full logical forms.
Related work on interpreting temporal expres-
sions has focused on constructing hand-crafted in-
terpretation rules (Mani and Wilson, 2000; Sa-
quete et al, 2003; Puscasu, 2004; Grover et al,
2010). Of these, HeidelTime (Stro?tgen and Gertz,
2010) and SUTime (Chang and Manning, 2012)
provide a strong comparison in English.
Recent probabilistic approaches to temporal
resolution include UzZaman and Allen (2010),
who employ a parser to produce deep logical
forms, in conjunction with a CRF classifier. In a
similar vein, Kolomiyets and Moens (2010) em-
ploy a maximum entropy classifier to detect the
location and temporal type of expressions; the
grounding is then done via deterministic rules.
In addition, there has been work on pars-
ing Spanish expressions; UC3M (Vicente-D??ez et
al., 2010) produce the strongest results on the
TempEval-2 corpus. Of the systems entered in the
original task, TIPSem (Llorens et al, 2010) was
the only system to perform bilingual interpreta-
tion for English and Spanish. Both of the above
systems rely primarily on hand-built rules.
3 Temporal Representation
We define a compositional representation of time,
similar to Angeli et al (2012), but with a greater
focus on efficiency and simplicity. The represen-
tation makes use of a notion of temporal types
and their associated semantic values; a grammar
is constructed over these types, and is grounded
by appealing to the associated values.
A summary of the temporal type system is pro-
vided in Section 3.1; the grammar is described in
Section 3.2; key modifications from previous work
are highlighted in Section 3.3.
3.1 Temporal Types
Temporal expressions are represented either as a
Range, Sequence, or Duration. The root of a parse
tree should be one of these types. In addition,
phrases can be tagged as a Function; or, as a spe-
cial Nil type corresponding to segments without a
direct temporal interpretation. Lastly, a type is al-
located for numbers. We describe each of these
briefly below.
Range [and Instant] A period between two
dates (or times), as per an interval-based theory
of time (Allen, 1981). This includes entities such
as Today, 1987, or Now.
Sequence A sequence of Ranges, occurring at
regular but not necessarily constant intervals. This
includes entities such as Friday, November
27th, or last Friday. A Sequence is de-
fined in terms of a partial completion of calendar
fields. For example, November 27th would de-
fine a Sequence whose year is unspecified, month
is November, and day is the 27th; spanning the en-
tire range of the lower order fields (in this case, a
day). This example is illustrated in Figure 1. Note
that a Sequence implicitly selects a possibly infi-
nite number of possible Ranges.
To select a particular grounded time for a Se-
quence, we appeal to a notion of a reference time
(Reichenbach, 1947). For the TempEval-2 corpus,
we approximate this as the publication time of the
article. While this is conflating Reichenbach?s ref-
erence time with speech time, and comes at the
expense of certain mistakes (see Section 5.3), it is
nonetheless useful in practice.
To a first approximation, grounding a sequence
given a reference time corresponds to filling in the
unspecified fields of the sequence with the fully-
specified fields of the reference time. This pro-
84
Sequence: year?
monNov day27th ? 28th
week? weekday?
hour00 min00 sec00
Reference Time: year2013
monAug day06th
week32 weekdayTue
hour03 min25 sec00
year2013
monNov day27th ? 28th
week? weekday?
hour00 min00 sec00
Figure 1: An illustration of grounding a Sequence. When grounding the Sequence November 27th
with a reference time 2013-08-06 03:25:00, we complete the missing fields in the Sequence (the
year) with the corresponding field in the reference time (2013).
cess has a number of special cases not enumerated
here,2 but the complexity remains constant time.
Duration A period of time. This includes enti-
ties like Week, Month, and 7 days. A special
case of the Duration type is defined to represent ap-
proximate durations, such as a few years or some
days.
Function A function of arity less than or equal
to two representing some general modification to
one of the above types. This captures semantic
entities such as those implied in last x, the third x
[of y], or x days ago. The particular functions are
enumerated in Table 2.
Nil A special Nil type denotes terms which are
not directly contributing to the semantic meaning
of the expression. This is intended for words such
as a or the, which serve as cues without bearing
temporal content themselves.
Number Lastly, a special Number type is defined
for tagging numeric expressions.
3.2 Temporal Grammar
Our approach assumes that natural language de-
scriptions of time are compositional in nature; that
is, each word attached to a temporal phrase is com-
positionally modifying the meaning of the phrase.
We define a grammar jointly over temporal types
and values. The types serve to constrain the parse
and allow for coarse features; the values encode
specific semantics, and allow for finer features.
At the root of a parse tree, we recursively apply
2Some of these special cases are caused by variable days
of the month, daylight savings time, etc. Another class arises
from pragmatically peculiar utterances; e.g., the next Monday
in August uttered in the last week of August should ground to
August of next year (rather than the reference time?s year).
the functions in the tree to obtain a final temporal
value.
This approach can be presented as a rule-to-rule
translation (Bach, 1976; Allen, 1995, p. 263), or
a constrained Synchronous PCFG (Yamada and
Knight, 2001).
Formally, we define our grammar as
G = (?, S,V, T,R). The alphabet ? and start
symbol S retain their usual interpretations. We
define a set V to be the set of types, as described in
Section 3.1. For each v ? V we define an (infinite)
set Tv corresponding to the possible instances of
type v. Each node in the tree defines a pair (v, ?)
such that ? ? Tv. A rule R ? R is defined as
a pair R = (vi ? vjvk, f : (Tvj , Tvk)? Tvi
).
This definition is trivially adapted for the case of
unary rules.
The form of our rules reveals the synchronous
aspect of our grammar. The structure of the tree is
bound by the first part over types v ? these types
are used to populate the chart, and allow for effi-
cient inference. The second part is used to eval-
uate the semantics of the parse, ? ? Tvi , and al-
lows partial derivations to be discriminated based
on richer information than the coarse types.
We adopt the preterminals of Angeli et al
(2012). Each preterminal consists of a type
and a value; neither which are lexically in-
formed. That is, the word week and preterminal
(Week,Duration) are not tied in any way. A total
of 62 preterminals are defined corresponding to in-
stances of Ranges, Sequences, and Durations; these
are summarized in Table 1.
In addition, 10 functions are defined for manip-
ulating temporal expressions (see Table 2). The
majority of these mirror generic operations on in-
tervals on a timeline, or manipulations of a se-
quence. Notably, like intervals, times can be
85
Type Example Instances
Range Past, Future, Yesterday,
Tomorrow, Today, Reference,
Year(n), Century(n)
Sequence Friday, January, . . .
DayOfMonth, DayOfWeek, . . .
EveryDay, EveryWeek, . . .
Duration Second, Minute, Hour,
Day, Week, Month, Quarter,
Year, Decade, Century
Table 1: The content-bearing preterminals of the
grammar, arranged by their types. Note that the
Sequence type contains more elements than enu-
merated here; however, only a few of each charac-
teristic type are shown here for brevity.
Function Description
shiftLeft Shift a Range left by a Duration
shiftRight Shift a Range right by a Duration
shrinkBegin Take the first Duration of a Range
shrinkEnd Take the last Duration of a Range
catLeft Take the Duration after a Range
catRight Take the Duration before a Range
moveLeft1 Shift a Sequence left by 1
moveRight1 Shift a Sequence right by 1
nth x of y Take the nth element in y
approximate Make a Duration approximate
Table 2: The functional preterminals of the gram-
mar. The name and a brief description of the func-
tion are given; the functions are most easily in-
terpreted as operations on either an interval or se-
quence. All operations on Ranges can equivalently
be applied to Sequences.
moved (3 weeks ago) or their size changed (the
first two days of the month), or a new interval can
be started from one of the endpoints (the last 2
days). Additionally, a sequence can be modified
by shifting its origin (last Friday), or taking the
nth element of the sequence within some bound
(fourth Sunday in November).
Combination rules in the grammar mirror type-
checked curried function application. For in-
stance, the function moveLeft1 applied to week
(as in last week) yields a grammar rule:
( EveryWeek -1 , Seq. )
( moveLeft1 , Seq.?Seq. ) ( EveryWeek , Seq. )
In more generality, we create grammar rules for
applying a function on either the left or the right,
for all possible type signatures of f : f(x, y)  x
or x f(x, y).
Additionally, a grammar rule is created for in-
tersecting two Ranges or Sequences, for multiply-
ing a duration by a number, and for absorbing a Nil
span. Each of these can be though of as an implicit
function application (in the last case, the identity
function).
3.3 Differences From Previous Work
While the grammar formalism is strongly inspired
by Angeli et al (2012), a number of key differ-
ences are implemented to both simplify the frame-
work, and make inference more efficient.
Sequence Grounding The most time-
consuming and conceptually nuanced aspect
of temporal inference in Angeli et al (2012)
is intersecting Sequences. In particular, there
are two modes of expressing dates which resist
intersection: a day-of-month-based mode and a
week-based mode. Properly grounding a sequence
which defines both a day of the month and a day
of the week (or week of the year) requires backing
off to an expensive search problem.
To illustrate, consider the example: Friday the
13th. Although both a Friday and a 13th of the
month are easily found, the intersection of the two
requires iterating through elements of one until it
overlaps with an element of the other. At train-
ing time, a number of candidate parses are gen-
erated for each phrase. When considering that
these parses can become both complex and prag-
matically unreasonable, this can result in a notice-
able efficiency hit; e.g., during training a sentence
could have a [likely incorrect] candidate interpre-
tation of: nineteen ninety-six Friday the 13ths from
now.
In our Sequence representation, such intersec-
tions are disallowed, in the same fashion as Febru-
ary 30th would be.
Sequence Pragmatics For the sake of simplicity
the pragmatic distribution over possible ground-
ings of a sequence is replaced with the single most
likely offset, as learned empirically from the En-
glish TempEval-2 corpus by Angeli et al (2012).
No Tag Splitting The Number and Nil types
are no longer split according to their ordinal-
ity/magnitude and subsumed phrase, respectively.
86
More precisely, there is a single nonterminal (Nil),
rather than a nonterminal symbol characterizing
the phrase it is subsuming (Nil-the, Nil-a, etc.). This
information is encoded more elegantly as features.
4 Learning
The system is trained using a discriminative k-
best parser, which is able to incorporate arbi-
trary features over partial derivations. We describe
the parser below, followed by the features imple-
mented.
4.1 Parser
Inference A discriminative k-best parser was
used to allow for arbitrary features in the parse
tree. In the first stage, spans of the input sentence
are tagged as either text or numbers. A rule-based
number recognizer was used for each language
to recognize and ground numeric expressions, in-
cluding information on whether the number was
an ordinal (e.g., two versus second). Note that un-
like conventional parsing, a tag can span multiple
words. Numeric expressions are treated as if the
numeric value replaced the expression.
Each rule of the parse derivation was assigned
a score according to a log-linear factor. Specifi-
cally, each rule R = (vi ? vjvk, f) with features
over the rule and derivation so far ?(R), subject to
parameters ?, is given a probability:
P (vi | vj , vk, f ; ?) ? e?
T?(R) (1)
Na??vely, this parsing algorithm gives us a com-
plexity of O(n3k2), where n is the length of the
sentence, and k is the size of the beam. However,
we can approximate the algorithm inO(n3k log k)
time with cube pruning (Chiang, 2007). With
features which are not context-free, we are not
guaranteed an optimal beam with this approach;
however, empirically the approximation yields a
significant efficiency improvement without notice-
able loss in performance.
Training We adopt an EM-style bootstrapping
approach similar to Angeli et al (2012), in order to
handle the task of parsing the temporal expression
without annotations for the latent parses. Each
training instance is a tuple consisting of the words
in the temporal phrase, the annotated grounded
time ??, and the reference time.
Given an input sentence, our parser will out-
put k possible parses; when grounded to the
reference time these correspond to k candidate
times: ?1 . . . ?k, each with a normalized probabil-
ity P (?i). This corresponds to an approximate E
step in the EM algorithm, where the distribution
over latent parses is approximated by a beam of
size k. Although for long sentences the number
of parses is far greater than the beam size, as the
parameters improve, increasingly longer sentences
will have correct derivations in the beam. In this
way, a progressively larger percentage of the data
is available to be learned from at each iteration.
To approximate the M step, we define a multi-
class hinge loss l(?) over the beam, and optimize
using Stochastic Gradient Descent with AdaGrad
(Duchi et al, 2010):
l(?) = max
0?i<k
1[?i 6= ??] + P?(?i)? P?(??) (2)
We proceed to describe our features.
4.2 Features
Our framework allows us to define arbitrary fea-
tures over partial derivations. Importantly, this al-
lows us to condition not only on the PCFG proba-
bilities over types but also the partial semantics of
the derivation. We describe the features used be-
low; a summary of these features for a short phrase
is illustrated in Figure 2.
Bracketing Features A feature is defined over
every nonterminal combination, consisting of
the pair of children being combined in that
rule. In particular, let us consider a rule
R = (vi ? vjvk, f) corresponding to a CFG rule
vi ? vjvk over types, and a function f over the
semantic values corresponding to vj and vk: ?j
and ?k. Two classes of bracketing features are
extracted: features are extracted over the types
of nonterminals being combined (vj and vk), and
over the top-level semantic derivation of the non-
terminals (f , ?j , and ?k).
Unlike syntactic parsing, child types of a parse
tree uniquely define the parent type of the rule; this
is a direct consequence of our combination rules
being functions with domains defined in terms
of the temporal types, and therefore necessarily
projecting their inputs into a single output type.
Therefore, the first class of bracketing features ?
over types ? reduce to have the exact same expres-
sive power as the nonterminal CFG rules of Angeli
et al (2012). Examples of features in this class are
features 13 and 15 in Figure 2 (b).
87
Input (w,t) ( Friday of this week , August 6 2013 )
Latentparse
FRI ? EveryWeek
FRI
Friday
EveryWeek
Nil
of this
EveryWeek
week
Output ?? August 9 2013
FRI
Friday
1. < FRI , Friday >
Nil
of this
2. < Nil , of >3. < Nil , this >4. < Nil , of this >5. < nil bias >
EveryWeek
week
6. < EveryWeek , week >
EveryWeek
Nil EveryWeek
7. < Nil of , EveryWeek >8. < Nil this , EveryWeek >9. < Nil of this , EveryWeek >10. < Nil of , Sequence >11. < Nil this , Sequence >12. < Nil of this , Sequence >13. < Nil , Sequence >14. < Nil , EveryWeek >
FRI ? EveryWeek
FRI EveryWeek
15. < Sequence , Sequence >16. < Intersect , FRI , EveryWeek >17. < root valid >
(a) (b)
Figure 2: An example parse of Friday of this week, along with the features extracted from the parse.
A summary of the input, latent parse, and output for a particular example is given in (a). The features
extracted for each fragment of the parse are given in (b), and described in detail in Section 4.2.
We now also have the flexibility to extract a sec-
ond class of features from the semantics of the
derivation. We define a feature bracketing the
most recent semantic function applied to each of
the two child derivations; along with the function
being applied in the rule application. If the child
is a preterminal, the semantics of the pretermi-
nal are used; otherwise, the outermost (most re-
cent) function to be applied to the derivation is
used. To illustrate, a tree fragment combining
August and 2013 into August 2013 would
yield the feature<INTERSECT, AUGUST, 2013>.
This can be read as a feature for the rule apply-
ing the intersect function to August and 2013.
Furthermore, intersecting August 2013 with
the 12th of the month would yield a feature
<INTERSECT, INTERSECT, 12th>. This can be
read as applying the intersect function to a subtree
which is the intersection of two terms, and to the
12th of the month. Features 14 and 16 in Figure 2
(b) are examples of such features.
Lexical Features The second large class of fea-
tures extracted are lexicalized features. These are
primarily used for tagging phrases with pretermi-
nals; however, they are also relevant in incorporat-
ing cues from the yield of Nil spans. To illustrate, a
week and the week have very different meanings,
despite differing by only their Nil tagged tokens.
In the first case, a feature is extracted over the
value of the preterminal being extracted, and the
phrase it is subsuming (e.g., features 1?4 and 6 in
Figure 2 (b)). As the type of the preterminal is
deterministic from the value, encoding a feature
on the type of the preterminal would be a coarser
encoding of the same information, and is empir-
ically not useful in this case. Since a multi-word
expression can parse to a single nonterminal, a fea-
ture is extracted for the entire n-gram in addition
to features for each of the individual words. For
example, the phrase of this ? of type Nil ? would
have features extracted: <NIL, of>, <NIL, this>,
and <NIL, of this>.
In the second case ? absorbing Nil-tagged spans
? we extract features over the words under the Nil
span joined with the type and value of the other
derivation (e.g., features 7?12 in Figure 2 (b)).
As above, features are extracted for both n-grams
and for each word in the phrase. For example,
combining of this and week would yield features
88
Train Test
System Type Value Type Value
GUTime 0.72 0.46 0.80 0.42
SUTime 0.85 0.69 0.94 0.71
HeidelTime 0.80 0.67 0.85 0.71
ParsingTime 0.90 0.72 0.88 0.72
OurSystem 0.94 0.81 0.91 0.76
Table 3: English results for TempEval-2 attribute
scores for our system and four previous systems.
The scores are calculated using gold extents, forc-
ing an interpretation for each parse.
Train Test
System Type Value Type Value
UC3M ? ? 0.79 0.72
OurSystem 0.90 0.84 0.92 0.76
Table 4: Spanish results for TempEval-2 attribute
scores for our system and the best known previ-
ous system. The scores are calculated using gold
extents, forcing an interpretation for each parse.
<of, EVERYWEEK>, <this, EVERYWEEK>,
and <of this, EVERYWEEK>.
In both cases, numbers are featurized according
to their order of magnitude, and whether they are
ordinal. Thus, the number tagged from thirty-first
would be featurized as an ordinal number of mag-
nitude 2.
Semantic Validity Although some constraints
can be imposed to help ensure that a top-level
parse will be valid, absolute guarantees are diffi-
cult. For instance, February 30 is never a valid
date; but, it would be difficult to disallow any local
rule in its derivation. To mediate this, an indicator
feature is extracted denoting whether the grounded
semantics of the derivation is valid. This is illus-
trated in Figure 2 (b) by feature 17.
Nil Bias Lastly, an indicator feature is extracted
for each Nil span tagged (feature 5 in Figure 2
(b)). In part, this discourages over-generation of
the type; in another part, it encourages Nil spans to
absorb as many adjacent words as possible.
We proceed to describe our experimental setup
and results.
5 Evaluation
We evaluate our model on all six languages in
the TempEval-2 Task A dataset (Verhagen et al,
2010), comparing against state-of-the-art systems
for English and Spanish. New results are reported
on smaller datasets from the four other languages.
To our knowledge, there has not been any prior
work on these corpora.
We describe the TempEval-2 datasets in Sec-
tion 5.1, present experimental results in Sec-
tion 5.2, and discuss system errors in Section 5.3.
5.1 TempEval-2 Datasets
TempEval-2, from SemEval 2010, focused on re-
trieving and reasoning about temporal information
from newswire. Our system evaluates against Task
A ? detecting and resolving temporal expressions.
Since we perform only the second of these, we
evaluate our system assuming gold detection.
The dataset annotates six languages: English,
Spanish, Italian, French, Chinese, and Korean; of
these, English and Spanish are the most mature.
We describe each of these languages, along with
relevant quirks, below:
English The English dataset consists of 1052
training examples, and 156 test examples. Evalu-
ation was done using the official evaluation script,
which checks for exact match between TIMEX3
tags. Note that this is stricter than our training ob-
jective; for instance, 24 hours and a day have the
same interpretation, but have different TIMEX3
strings. System output was heuristically converted
to the TIMEX3 format; where ambiguities arose,
the convention which maximized training accu-
racy was chosen.
Spanish The Spanish dataset consists of 1092
training examples, and 198 test examples. Evalua-
tion was identical to the English, with the heuristic
TIMEX3 conversion adapted somewhat.
Italian The Italian dataset consists of 523 train-
ing examples, and 126 test examples. Evaluation
was identical to English and Spanish.
Chinese The Chinese dataset consists of 744
training examples, and 190 test examples. Of
these, only 659 training and 143 test examples had
a temporal value marked; the remaining examples
had a type but no value, and are therefore impossi-
ble to predict. Results are also reported on a clean
corpus with these impossible examples omitted.
The Chinese, Korean, and French corpora had
noticeable inconsistencies in the TIMEX3 anno-
tation. Thus, evaluations are reported according
89
Train Test
Language # examples Type Value # examples Type Value
English 1052 0.94 0.81 156 0.91 0.76
Spanish 1092 0.90 0.84 198 0.92 0.76
Italian 523 0.89 0.85 126 0.84 0.38
Chinese? 744 0.95 0.65 190 0.87 0.48
Chinese (clean)? 659 0.97 0.73 143 0.97 0.60
Korean? 247 0.83 0.67 91 0.82 0.42
French? 206 0.78 0.76 83 0.78 0.35
Table 5: Our system?s accuracy on all 6 languages of the TempEval-2 corpus. Chinese is divided into two
results: one for the entire corpus, and one which considers only examples for which a temporal value
is annotated. Languages with a dagger (?) were evaluated based on semantic rather than string-match
correctness.
to the training objective: if two TIMEX3 values
ground to the same grounded time, they are con-
sidered equal. For example, in the example above,
24 hours and a day would be marked identical de-
spite having different TIMEX3 strings.
Most TIMEX3 values convert naturally to
a grounded representation; values with wild-
cards representing Sequences (e.g., 1998-QX or
1998-XX-12) ground to the same value as the
Sequence encoding that value would. For instance,
1998-QX is parsed as every quarter in 1998.
Korean The Korean dataset consists of 287
training examples, and 91 test examples. 40 of
the training examples encoded dates as a long in-
teger For example: 003000000200001131951006
grounds to January 13, 2000 at the time 19:51.
These were removed from the training set, yield-
ing 247 examples; however, all three such exam-
ples were left in the test set. Evaluation was done
identically to the Chinese data.
French Lastly, a dataset for French temporal
expressions was compiled from the TempEval-2
data. Unlike the other 5 languages, the French
data included only the raw TIMEX3 annotated
newswire documents, encoded as XML. These
documents were scraped to recover 206 training
examples and 83 test examples. Evaluation was
done identically to the Chinese and Korean data.
We proceed to describe our experimental results
on these datasets.
5.2 Results
We compare our system with state-of-the-art sys-
tems for both English and Spanish. To the best of
our knowledge, no prior work exists for the other
four languages.
We evaluate in the same framework as Angeli et
al. (2012). We compare to previous system scores
when constrained to make a prediction on every
example; if no guess is made, the output is consid-
ered incorrect. This in general yields lower results
for those systems, as the system is not allowed to
abstain on expressions it does not recognize.
The systems compared against are:
? GUTime (Mani and Wilson, 2000), a widely
used, older rule-based system.
? HeidelTime (Stro?tgen and Gertz, 2010), the
top system at the TempEval-2 task for En-
glish.
? SUTime (Chang and Manning, 2012), a more
recent rule-based system for English.
? ParsingTime (Angeli et al, 2012), a seman-
tic parser for temporal expressions, similar to
this system (see Section 2).
? UC3M (Vicente-D??ez et al, 2010), a rule-
based system for Spanish.
Results for the English corpus are shown in Ta-
ble 3. Results for Spanish are shown in Table 4.
Lastly, a summary of results in all six languages is
shown in Table 5.
A salient trend emerges from the results ? while
training accuracy is consistently high, test accu-
racy drops sharply for the data-impoverished lan-
guages. This is consistent with what would be
expected from a discriminatively trained model
in data-impoverished settings; however, the con-
sistent training accuracy suggests that the model
nonetheless captures the phenomena it sees in
90
Error Class English Spanish
Pragmatics 29% 23%
Type error 16% 5%
Incorrect number 10% 5%
Relative Range 7% 2%
Incorrect parse 19% 36%
Missing context 16% 23%
Bad reference time 3% 6%
Table 6: A summary of errors of our system,
by percentage of incorrect examples for the En-
glish and Spanish datasets. The top section de-
scribes errors which could be handled in our
framework, while the bottom section describes ex-
amples which are either ambiguous (missing con-
text), or are annotated inconsistently relative the
reference time.
training. This suggests the possibility for improv-
ing accuracy further by making use of more data
during training.
5.3 Discussion
We characterize the examples our system parses
incorrectly on the English and Spanish datasets in
Table 6, expanding on each class of error below.
Pragmatics This class of errors is a result of
pragmatic ambiguity over possible groundings of
a sequence ? for instance, it is often ambiguous
whether next weekend refers to the coming or sub-
sequent weekend. These errors manifest in either
dropping a function (next, last), or imagining one
that is not supported by the text (e.g., this week
parsed as next week).
Type error Another large class of errors ? par-
ticularly in the English dataset ? arise from not
matching the annotation?s type, but otherwise pro-
ducing a reasonable response. For instance, the
system may mistake a day on the calendar (a
Range), with a day, the period of time.
Incorrect number A class of mistakes arises
from either omitting numbers from the parse, or
incorrectly parsing numbers ? the second case is
particularly prevalent for written years, such as
seventeen seventy-six.
Relative Range These errors arise from attempt-
ing to parse a grounded Range by applying func-
tions to the reference time. For example, from
a reference time of August 8th, it is possible to
?correctly? parse the phrase August 1 as a week
ago; but, naturally, this parse does not general-
ize well. This class of errors, although relatively
small, merits special designation as it suggests a
class of correct responses which are correct for the
wrong reasons. Future work could explore miti-
gating these errors for domains where the text is
further removed from the events it is describing
than most news stories are.
Incorrect parse Errors in this class are a result
of failing to find the correct parse, for a number of
reasons not individually identified. A small sub-
set of these errors (notably, 6% on the Spanish
dataset) are a result of the grammar being insuf-
ficiently expressive with the preterminals we de-
fined. For instance, we cannot capture fractional
units, such as in half an hour.
Missing context A fairly large percentage of our
errors arise from failing to classify inputs which
express ambiguous or poorly defined times. For
example, from time to time (annotated as the fu-
ture), or that time (annotated as 5 years). Many
of these require either some sort of inference or a
broader understanding of the context in which the
temporal phrase is uttered, which our system does
not attempt to capture.
Bad reference time The last class of errors
cover cases where the temporal phrase is clear,
but annotation differs from our judgment of what
would be reasonable. These are a result of assum-
ing that the reference time of an utterance is the
publication time of the article.
6 Conclusion
We have presented a discriminative, multilingual
approach to resolving temporal expressions, using
a language-flexible latent parse and rich features
on both the types and values of partial derivations
in the parse. We showed state-of-the-art results
on both languages in TempEval-2 with prior work,
and presented results on four additional languages.
Acknowledgments Work was done in the sum-
mer of 2012 while the first author was an intern at
Google. We would like to thank Chris Manning,
and our co-workers at Google for their insight and
help.
91
References
James F. Allen. 1981. An interval-based representa-
tion of temporal knowledge. In Proceedings of the
7th international joint conference on Artificial intel-
ligence, pages 221?226, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
James Allen. 1995. Natural Language Understanding.
Benjamin/Cummings, Redwood City, CA.
Gabor Angeli, Christopher D. Manning, and Daniel Ju-
rafsky. 2012. Parsing time: Learning to interpret
time expressions. In NAACL-HLT.
E. Bach. 1976. An extension of classical transforma-
tional grammar. In Problems of Linguistic Metathe-
ory (Proceedings of the 1976 Conference), Michigan
State University.
Angel Chang and Chris Manning. 2012. SUTIME: a
library for recognizing and normalizing time expres-
sions. In Language Resources and Evaluation.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201?228.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In CoNLL, pages 18?27, Uppsala,
Sweden.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121?2159.
Claire Grover, Richard Tobin, Beatrice Alex, and Kate
Byrne. 2010. Edinburgh-LTG: TempEval-2 system
description. In Proceedings of the 5th International
Workshop on Semantic Evaluation, Sem-Eval, pages
333?336.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to
formal languages. In AAAI, pages 1062?1068, Pitts-
burgh, PA.
Oleksandr Kolomiyets and Marie-Francine Moens.
2010. KUL: recognition and normalization of tem-
poral expressions. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, Sem-Eval
?10, pages 325?328.
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
ACL.
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. Tipsem (english and spanish): Evaluating crfs
and semantic roles in tempeval-2. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, pages 284?291.
Inderjeet Mani and George Wilson. 2000. Robust tem-
poral processing of news. In ACL, pages 69?76,
Hong Kong.
G. Puscasu. 2004. A framework for temporal resolu-
tion. In LREC, pages 1901?1904.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Macmillan, New York.
E. Saquete, R. Muoz, and P. Martnez-Barco. 2003.
Terseo: Temporal expression resolution system ap-
plied to event ordering. In Text, Speech and Dia-
logue, pages 220?228.
Jannik Stro?tgen and Michael Gertz. 2010. Heideltime:
High quality rule-based extraction and normaliza-
tion of temporal expressions. In Proceedings of the
5th International Workshop on Semantic Evaluation,
Sem-Eval, pages 321?324.
Naushad UzZaman and James F. Allen. 2010. TRIPS
and TRIOS system for TempEval-2: Extracting tem-
poral information from text. In Proceedings of the
5th International Workshop on Semantic Evaluation,
Sem-Eval, pages 276?283.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62, Uppsala, Sweden.
Mar??a Teresa Vicente-D??ez, Julia?n Moreno Schneider,
and Paloma Mart??nez. 2010. Uc3m system: De-
termining the extent, type and value of time expres-
sions in tempeval-2. In proceedings of the Semantic
Evaluation?2 (Semeval 2010), ACL Conference, Up-
psala (Sweden).
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In ACL, pages
523?530.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI/IAAI, pages 1050?1055,
Portland, OR.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658?666. AUAI Press.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In EMNLP-CoNLL, pages 678?687.
92
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 133?142,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Philosophers are Mortal: Inferring the Truth of Unseen Facts
Gabor Angeli
Stanford University
Stanford, CA 94305
angeli@stanford.edu
Christopher D. Manning
Stanford University
Stanford, CA 94305
manning@stanford.edu
Abstract
Large databases of facts are prevalent in
many applications. Such databases are
accurate, but as they broaden their scope
they become increasingly incomplete. In
contrast to extending such a database, we
present a system to query whether it con-
tains an arbitrary fact. This work can be
thought of as re-casting open domain in-
formation extraction: rather than growing
a database of known facts, we smooth this
data into a database in which any possi-
ble fact has membership with some confi-
dence. We evaluate our system predicting
held out facts, achieving 74.2% accuracy
and outperforming multiple baselines. We
also evaluate the system as a common-
sense filter for the ReVerb Open IE sys-
tem, and as a method for answer validation
in a Question Answering task.
1 Introduction
Databases of facts, such as Freebase (Bollacker
et al, 2008) or Open Information Extraction
(Open IE) extractions, are useful for a range of
NLP applications from semantic parsing to infor-
mation extraction. However, as the domain of a
database grows, it becomes increasingly impracti-
cal to collect completely, and increasingly unlikely
that all the elements intended for the database are
explicitly mentioned in the source corpus. In par-
ticular, common-sense facts are rarely explicitly
mentioned, despite their abundance. It would be
useful to infer the truth of such unseen facts rather
than assuming them to be implicitly false.
A growing body of work has focused on auto-
matically extending large databases with a finite
set of additional facts. In contrast, we propose
a system to generate the (possibly infinite) com-
pletion of such a database, with a degree of con-
fidence for each unseen fact. This task can be
cast as querying whether an arbitrary element is
a member of the database, with an informative de-
gree of confidence. Since often the facts in these
databases are devoid of context, we refine our no-
tion of truth to reflect whether we would assume
a fact to be true without evidence to the contrary.
In this vein, we can further refine our task as de-
termining whether an arbitrary fact is plausible ?
true in the absence contradictory evidence.
In addition to general applications of such large
databases, our approach can further be integrated
into systems which can make use of probabilis-
tic membership. For example, certain machine
translation errors could be fixed by determining
that the target translation expresses an implausible
fact. Similarly, the system can be used as a soft
feature for semantic compatibility in coreference;
e.g., the types of phenomena expressed in Hobbs?
selectional constraints (Hobbs, 1978). Lastly, it is
useful as a common-sense filter; we evaluate the
system in this role by filtering implausible facts
from Open IE extractions, and filtering incorrect
responses for a question answering system.
Our approach generalizes word similarity met-
rics to a notion of fact similarity, and judges the
membership of an unseen fact based on the aggre-
gate similarity between it and existing members
of the database. For instance, if we have not seen
the fact that philosophers are mortal1 but we know
that Greeks are mortal, and that philosophers and
Greeks are similar, we would like to infer that the
fact is nonetheless plausible.
We implement our approach on both a large
open-domain database of facts extracted from the
Open IE system ReVerb (Fader et al, 2011), and
ConceptNet (Liu and Singh, 2004), a hand curated
database of common sense facts.
1This is an unseen fact in http://openie.cs.
washington.edu.
133
2 Related Work
Many NLP applications make use of a knowl-
edge base of facts. These include semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Kate et al, 2005; Zettlemoyer
and Collins, 2007) question answering (Voorhees,
2001), information extraction (Hoffmann et al,
2011; Surdeanu et al, 2012), and recognizing tex-
tual entailment (Schoenmackers et al, 2010; Be-
rant et al, 2011).
A large body of work has been devoted to creat-
ing such knowledge bases. In particular, Open IE
systems such as TextRunner (Yates et al, 2007),
ReVerb (Fader et al, 2011), Ollie (Mausam et al,
2012), and NELL (Carlson et al, 2010) have tack-
led the task of compiling an open-domain knowl-
edge base. Similarly, the MIT Media Lab?s Con-
ceptNet project (Liu and Singh, 2004) has been
working on creating a large database of common
sense facts.
There have been a number of systems aimed at
automatically extending these databases. That is,
given an existing database, they propose new re-
lations to be added. Snow et al (2006) present
an approach to enriching the WordNet taxonomy;
Tandon et al (2011) extend ConceptNet with new
facts; Soderland et al (2010) use ReVerb extrac-
tions to enrich a domain-specific ontology. We
differ from these approaches in that we aim to pro-
vide an exhaustive completion of the database; we
would like to respond to a query with either mem-
bership or lack of membership, rather than extend-
ing the set of elements which are members.
Yao et al (2012) and Riedel et al (2013) present
a similar task of predicting novel relations be-
tween Freebase entities by appealing to a large col-
lection of Open IE extractions. Our work focuses
on arguments which are not necessarily named
entities, at the expense of leveraging less entity-
specific information.
Work in classical artificial intelligence has tack-
led the related task of loosening the closed world
assumption and monotonicity of logical reason-
ing, allowing for modeling of unseen propositions.
Reiter (1980) presents an approach to leveraging
default propositions in the absence of contradic-
tory evidence; McCarthy (1980) defines a means
of overriding the truth of a proposition in abnor-
mal cases. Perhaps most similar to this work
is Pearl (1989), who proposes approaching non-
monotonicity in a probabilistic framework, and in
particular presents a framework for making infer-
ences which are not strictly entailed but can be
reasonably assumed. Unlike these works, our ap-
proach places a greater emphasis on working with
large corpora of open-domain predicates.
3 Approach
At a high level, we are provided with a large
database of facts which we believe to be true, and
a query fact not in the database. The task is to
output a judgment on whether the fact is plausible
(true unless we have reason to believe otherwise),
with an associated confidence. Although our ap-
proach is robust to unary relations, we evaluate
only against binary relations.
We decompose this decision into three parts, as
illustrated in Figure 1: (i) we find candidate facts
that are similar to our query, (ii) we define a notion
of similarity between these facts and our query,
and (iii) we define a method for aggregating a col-
lection of these similarity values into a single judg-
ment. The first of these parts can be viewed as an
information retrieval component. The second part
can be viewed as an extension of word similarity
to fact similarity. The third part is cast as a classifi-
cation task, where the input is a set of similar facts,
and the decision is the confidence of the query be-
ing plausible.
We define a fact as a triple of two arguments
and a relation. We denote a fact in our database
as f = (a1, r, a2). A fact which we are querying
is denoted by fq ? as our focus is on unseen facts,
this query is generally not in the database.
3.1 Finding Candidate Facts
Na??vely, when determining the correctness of a
query fact, it would be optimal to compare it to
the entire database of known facts. However, this
approach poses significant problems:
1. The computational cost becomes unreason-
able with a large database, and only a small
portion of the database is likely to be relevant.
2. The more candidates we consider the more
opportunities we create for false positives
in finding similar facts. For a sufficiently
large database, even a small false positive rate
could hurt performance.
To address these two problems, we consider
only facts which match the query fact in two
of their three terms. Formally, we define
134
database candidates similarity aggregate
f1f2. . .
fn
(f1, s1). . .
(f1, sn). . .
(fn, sn)
Figure 1: An overview of our approach. A large database of facts is queried for candidate entries that
may be similar to the query fact (see Section 3.1); the similarity of each of these facts to the query fact is
computed using a number of word similarity metrics (see Section 3.2); finally, these similarity judgments
are aggregated into a single judgment per metric, and then a single overall judgment (see Section 3.3).
functions: cand(fq, fi; a1), cand(fq, fi; r), and
cand(fq, fi; a2) for whether the query fq matches
a fact in our database fi on all but one of the argu-
ments (or relation). For efficiency, the total num-
ber of candidates returned by each of these three
functions was limited to 100, creating up to 300
similar facts overall.
The simplest implementation of this cand
function would be exact match (candexact);
however, this is liable to return few re-
sults. As an example, suppose our query
is (private land, be sold to, government). We
would like to consider a fact in our database
(his land, be sold to, United States) as similar ex-
cept for second argument (government versus
United States), despite the first argument not
matching exactly. To account for this, we define
a class of functions which match the head word
of the two phrases, and as many of the follow-
ing stricter criteria as possible while maintaining
at least 40 candidate facts:2
candhead Match the head word of the two
phrases only. Head words were extracted using the
Stanford Parser (Klein and Manning, 2003), treat-
ing each argument and relation as a sentence.
candvn Match all verbs and nouns in the
two phrases; This prunes candidates such as
(land of our ancestors, be sold to, Prussia).
Tagging was done with the Stanford Tagger
(Toutanova et al, 2003).
candopen Match the open-class words be-
tween the two phrases. More precisely, it
matches every word which is not a pro-
noun, determiner, preposition, or form of the
2This threshold is chosen in conjunction with the aggre-
gation threshold in Section 3.3, to allow for at least two facts
in the 95% threshold.
verb be. This prunes candidates such as
(worthless land, be sold to, gullible investors).
We proceed to describe our notion of similarity
between facts, which will be applied to the set of
candidate similar facts retrieved.
3.2 Similarity Between Facts
Determining the similarity between two facts is
in general difficult. For sufficiently complicated
facts, it can be has hard as recognizing textual en-
tailment (RTE); for instance, determining that ev-
ery philosopher is mortal and Socrates is mortal
are similar requires fairly sophisticated inference.
We choose a simple approach, in order to avoid fit-
ting to a particular corpus or weakening our ability
to generalize to arbitrary phrases.
Our approach casts fact similarity in terms of as-
sessing word similarity. The candidate facts from
Section 3.1 differ from the query fact by a single
phrase; we define the similarity between the can-
didate and query fact to be the similarity between
the differing term.
The word similarity metrics are summarized
in Table 1. They fall into two broad classes:
information-theoretic thesaurus based metrics,
and distributional similarity metrics.
Thesaurus Based Metrics We adopt many of
the thesaurus based similarity metrics described
in Budanitsky and Hirst (2006). For each metric,
we use the WordNet ontology (Miller, 1995) com-
bined with n-gram counts retrieved from Google
n-grams (Brants and Franz, 2006). Every word
form was assigned a minimum count of 1; 2265
entries had no counts and were assigned this min-
imum (1.5%). 167 of these were longer than 5
words; the remaining did not appear in the corpus.
Since WordNet is a relatively sparse resource,
135
if a query phrase is not found a number of simple
variants are also tried. These are, in order of pref-
erence: a lemmatized version of the phrase, the
head word of the phrase, and the head lemma of
the phrase. If none of these are found, then the
named entities in the sentence were replaced with
their types. If that fails as well, acronyms3 were
expanded. For words with multiple sense, the
maximum similarity for any pair of word senses
was used.
Distributional Similarity Based Metrics We
define a number of similarity metrics on the 50
dimensional word vectors of Huang et al (2012).
These cover a vocabulary of 100,231 words; a spe-
cial vector is defined for unknown words.
Compound phrases are queried by treating the
phrase as a bag of words and averaging the word
vectors of each word in the phrase, pruning out
unknown words. If the phrase contains no known
words, the same relaxation steps are tried as the
thesaurus based metrics.
3.3 Aggregating Similarity
At this stage, we are presented with a set of candi-
date facts which may be similar to our query, and
a set of similarity judgments for each of these can-
didate facts. Intuitively, we would like to mark a
fact as plausible if it has enough sufficiently simi-
lar candidate facts based on a large number of met-
rics. This is a two-dimensional aggregation task:
(i) we aggregate judgments for a single similarity
metric, and (ii) we aggregate these aggregate judg-
ments across similarity metrics. We accomplish
the first half with a thresholded average similarity;
the second half we accomplish by using the aggre-
gate similarity judgments as features for a logistic
regression model.
Thresholded Average Similarity Given a set
of similarity values, we average the top 5% of
the values and use this as the aggregate similarity
judgment. This approach incorporates the benefit
of two simpler aggregation techniques: averaging
and taking the maximum similarity.
Averaging similarity values has the advantage
of robustness ? given a set of candidate facts, we
would like as many of those facts to be as similar
to the query as possible. To illustrate, we should
be more certain that (philosophers, are, mortal)
36053 acronyms and initialisms were scraped from
http://en.wikipedia.org/wiki/List_of_
acronyms_and_initialisms
Name Formula
Th
esa
uru
sB
ase
d Path ? log len(w1, lcs, w2)Resnik ? logP (lcs)
Lin log(P (lcs)2)log(P (w1)?P (w2))
Jiang-Conrath log
(
P (lcs)2
P (w1)?P (w2)
)?1
Wu-Palmer 2?depth(lcs)2?depth(lcs)+len(w1,lcs,w2)
Di
str
ibu
tio
na
l Cosine
w1?w2
?w1??w2?
Angle arccos
(
w1?w2
?w1??w2?)
)
Jensen-Shannon (KL(p1?p2)+KL(p2?p1))2
Hellinger 1?2?
?p1 ??p2?
Jaccard ?min(w1,w2)?1?max(w1,w2)?1
Dice ?min(w1,w2)?11
2?w1+w2?1
Table 1: A summary of similarity metrics used to
calculate fact similarity. For the thesaurus based
metrics, the two synsets being compared are de-
noted by w1 and w2; the lowest common subsumer
is denoted as lcs. For distributional similarity met-
rics, the two word vectors are denoted by w1 and
w2. For metrics which require a probability distri-
bution, we pass the vectors through a sigmoid to
obtain pi = 11+e?wi .
if we know both that (Greeks, are, mortal) and
(men, are, mortal). However, since the number of
similar facts is likely to be small relative the num-
ber of candidate facts considered, this approach
has the risk of losing the signal in the noise of un-
informative candidates. Taking the maximum sim-
ilarity judgment alleviates this concern, but con-
strains the use of only one element in our aggre-
gate judgment.
If fewer than 20 candidates are returned, our
combination approach reduces to taking the max-
imum similarity value. Note also that the 40 fact
threshold in the candidate selection phase is cho-
sen to provide at least two similarity values to be
averaged together. The threshold was chosen em-
pirically, although varying it does not have a sig-
nificant effect on performance.
Aggregate Similarity Values At this point, we
have a number of distinct notions of similarity:
for each metric, for each differing term, we have
a judgment for whether the query fact is similar
to the list of candidates. We combine these using
136
a simple logistic regression model, treating each
judgment over different metrics and terms as a fea-
ture with weight given by the judgment. For ex-
ample, cosine similarity may judge candidate facts
differing on their first argument to have a similar-
ity of 0.2. As a result, a feature would be created
with weight 0.2 for the pair (cosine, argument 1).
In addition, features are created which are agnostic
to which term differs (e.g., the cosine similarity on
whichever term differs), bringing the total feature
count to 44 for 11 similarity metrics.
Lastly, we define 3 auxiliary feature classes:
? Argument Similarity: We define a feature
for the similarity between the two arguments
in the query fact. Similarity metrics (partic-
ularly distributional similarity metrics) often
capture a notion more akin to relatedness than
similarity (Budanitsky and Hirst, 2006); the
subject and object of a relation are, in many
cases, related in this sense.
? Bias: A single bias feature is included to ac-
count for similarity metrics which do not cen-
ter on zero.
? No Support Bias: A feature is included for
examples which have no candidate facts in
the knowledge base.
4 Data
Our approach is implemented using two datasets.
The first, described in Section 4.1, is built us-
ing facts retrieved from running the University of
Washington?s ReVerb system run over web text.
To showcase the system within a cleaner environ-
ment, we also build a knowledge base from the
MIT Media Lab?s ConceptNet.
4.1 ReVerb
We created a knowledge base of facts by running
ReVerb over ClueWeb09 (Callan et al, 2009). Ex-
tractions rated with a confidence under 0.5 were
discarded; the first billion undiscarded extractions
were used in the final knowledge base. This re-
sulted in approximately 500 million unique facts.
Some examples of facts extracted with ReVerb
are given in Table 2. Note that our notion of plau-
sibility is far more unclear than in the ConceptNet
data; many facts extracted from the internet are ex-
plicitly false, and others are true only in specific
contexts, or are otherwise underspecified.
Argument 1 Relation Argument 2
cat Desires tuna fish
air CapableOf move through
tiny hole
sneeze HasA allergy
person who IsA not wage-slaves
get more sleep
Table 3: Example ConceptNet extractions. The
top rows correspond to characteristic correct ex-
tractions; the bottom rows characterize the types
of noise in the data.
4.2 ConceptNet
We also created a dataset using a subset of Con-
ceptNet. ConceptNet is a hand-curated common
sense database, taking information from multi-
ple sources (including ReVerb) and consolidating
them in a consistent format. We focus on the man-
ually created portion of the database, extracted
from sources such as the Open Mind Common
Sense4 (Singh et al, 2002).
The knowledge base consists of 597,775 facts,
each expressing one of 34 relations. Examples of
facts in the ConceptNet database are given in Ta-
ble 3. While the arguments are generally cleaner
than the ReVerb corpus, there are nonetheless in-
stances of fairly complex facts.
4.3 Training Data
Our training data consists of a set of tuples, each
consisting of a fact f and a database d which
does not contain f . We create artificial negative
training instances in order to leverage the stan-
dard classification framework. We would like neg-
ative examples which are likely to be implausi-
ble, but which are close enough to known facts
that we can learn a reasonable boundary for dis-
criminating between the two. To this end, we
sample negative instances by modifying a sin-
gle argument (or the relation) of a correspond-
ing positive training instance. In more detail: we
take a positive training instance (a1, r, a2) and a
fact from our database (a?1, r?, a?2), and compute
the cosine similarity simcos(a1, a?1), simcos(r, r?),
and simcos(a2, a?2). Our negative instance will be
one of (a?1, r, a2), (a1, r?, a2), or (a1, r, a?2) cor-
responding to the entry whose similarity was the
largest. Negative facts which happen to be in the
database are ignored.
4http://openmind.media.mit.edu/
137
Argument 1 Relation Argument 2
officials contacted students
food riots have recently taken place in many countries
turn left on Front Street
animals have not been performed to evaluate the carcinogenic potential of adenosine
Table 2: Example ReVerb extractions. The top rows correspond to characteristic correct extractions; the
bottom rows shows examples of the types of noise in the data. Note that in general, both the arguments
and the predicate can be largely unconstrained text.
To simulate unseen facts, we construct training
instances by predicting the plausibility of a fact
held out from the database. That is, if our database
consists of d = {f0, f1, . . . fn}we construct train-
ing instances (fi, d\{fi}). Negative examples are
likewise constrained to not occur in the database,
as are the facts used in their construction.
5 Results
We evaluate our system with three experiments.
The first, described in Section 5.2, evaluates the
system?s ability to discriminate plausible facts
from sampled implausible facts, mirroring the
training regime. The second evaluates the system
as a semantic filter for ReVerb extractions, tested
against human evaluations. The third uses our sys-
tem for validating question answering responses.
5.1 Baselines
We define a number of baselines to compare
against. Many of these are subsets of our system,
to justify the inclusion of additional complexity.
Similar Fact Count This baseline judges the
truth of a fact by tuning a threshold on the total
number of similar facts in the database. This base-
line would perform well if our negative facts were
noticeably disconnected from our database.
Argument Similarity A key discriminating fea-
ture may be the similarity between a1 and a2 in
true versus false facts. This baseline thresholds the
cosine similarity between arguments, tuned on the
training data to maximize classification accuracy.
Cosine Similarity At its core, our model judges
the truth of a fact based on its similarity to facts
in the database; we create a baseline to capture
this intuition. For every candidate fact (differing
in either an argument or the relation), we compute
the cosine similarity between the query and the
candidate, evaluated on the differing terms. This
System ReVerb ConceptNetTrain Test Train Test
random 50.0 50.0 50.0 50.0
count 51.9 52.3 51.0 51.6
argsim 52.0 52.6 62.1 60.0
cos 71.4 70.6 71.9 70.5
system 74.3 74.2 76.5 74.3
Table 4: Classification accuracy for ReVerb and
ConceptNet data. The three baselines are de-
scribed above the line as described in Section 5.1;
random chance would get an accuracy of 50%.
baseline outputs the maximum similarity between
a query and any candidate; a threshold on this sim-
ilarity is tuned on the training data to maximize
classification accuracy.
5.2 Automatic Evaluation
A natural way to evaluate our system is to use the
same regime as our training, evaluating on held
out facts. For both domains we train on a balanced
dataset of 20,000 training and 10,000 test exam-
ples. Performance is measured in terms of classi-
fication accuracy, with a random baseline of 50%.
Table 4 summarizes our results. The similar fact
count baseline performs nearly at random chance,
suggesting that our sampled negative facts cannot
be predicted solely on the basis of connectedness
with the rest of the database. Furthermore, we out-
perform the cosine baseline, supporting the intu-
ition that aggregating similarity metrics is useful.
To evaluate the informativeness of the confi-
dence our system produces, we can allow our sys-
tem to abstain from unsure judgments. Recall
refers to the percentage of facts the system chooses
to make a guess on; precision is the percentage of
those facts which are classified correctly. From
this, we can create a precision/recall curve ? pre-
sented in Figure 2 for ReVerb and Figure 3 for
ConceptNet. Our system achieves an area under
138
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
Prec
ision
Recall
systemcos
Figure 2: Accuracy of ReVerb classification, as a
function of the percent of facts answered. The y
axis begins at random chance (50%).
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
Prec
ision
Recall
systemcos
Figure 3: Accuracy of ConceptNet classification,
as a function of the percent of facts answered. The
y axis begins at random chance (50%).
the curve of 0.827 on ConceptNet (compared to
the cosine baseline of 0.751). For ReVerb, we ob-
tain an area of 0.860 (compared to 0.768 for the
cosine baseline).5
5.3 ReVerb Filtering
In order to provide a grounded evaluation metric
we evaluate our system as a confidence estima-
tor for ReVerb extractions. Many ReVerb extrac-
tions are semantically implausible, or clash with
common-sense intuition. We annotate a number
of extractions on Mechanical Turk, and attempt to
predict the extractions? feasibility.
This task is significantly more difficult than the
intrinsic evaluations. Part of the difficulty stems
5Curves begin at the recall value given a system confi-
dence of 1.0. For area under the curve calculations, this value
is extended through to recall 0.
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  0.2  0.4  0.6  0.8  1
Prec
ision
Recall
systemcos
Figure 4: PR curve for ReVerb confidence estima-
tion. The y axis of the graph is truncated at 65% ?
this corresponds to the majority class baseline.
from our database itself (and therefore our can-
didate similar facts) being unfiltered ? our query
facts empirically were and therefore in a sense
should be in the database. Another part stems from
these facts already having been filtered once by
ReVerb?s confidence estimator.
To collect training and test data, we asked work-
ers on Amazon Mechanical Turk to rate facts as
correct, plausible, or implausible. They were in-
structed that they need not research the facts, and
that correct facts may be underspecified. Workers
were given the following descriptions of the three
possible responses:
? Correct: You would accept this fact if you
read it in a reputable source (e.g., Wikipedia)
in an appropriate context.
? Plausible: You would accept this fact if you
read it in a storybook.
? Implausible: The fact is either dubious, or
otherwise nonsense.
Below this, five examples were shown along-
side one control (e.g., (rock, float on, water)).
Workers who answered more than 20% of the con-
trols incorrectly were discarded. In total, 9 work-
ers and 117 of 1200 HITs were discarded.
Each example was shown to three separate
workers; a final judgment was made by taking the
majority vote between correct (corresponding to
our notion of plausibility) and implausible, ignor-
ing votes of plausible. In cases where all the votes
were made for plausible, or there was a tie, the
example was discarded.
The experiment was run twice on 2000 ReVerb
extractions to collect training and test data. The
139
training corpus consists of 1256 positive and 540
negative examples (1796 total; 70% positive). The
test corpus consists of 1286 positive and 689 neg-
ative examples (1975 total; 65% positive)
Our system was retrained with the human eval-
uated training data; to account for class bias, our
system?s classification threshold was then tuned
on the training data, optimizing for area under the
precision/recall curve. Figure 4 illustrates our re-
sults, bounded below by majority choice. Our sys-
tem achieves an area under the curve of 0.721; the
cosine baseline has an area of 0.696.
Our system offers a viable trade-off of recall in
favor of precision. For example, keeping only a
third of the data can reduce the error rate by 25%
? this can be appealing for large corpora where
filtering is frequent anyways.
5.4 Answer Validation Exercise
The Answer Validation Exercise, organized as a
track at CLEF between 2006 and 2008, focuses on
filtering candidate answers from question answer-
ing systems (Pen?as et al, 2007; Pen?as et al, 2008;
Rodrigo et al, 2009). Systems are presented with
a question, and a set of answers along with their
justification. The answers are either validated, re-
jected, or given an annotation of unknown and ig-
nored during scoring. Since the proportion of cor-
rect answers is small (around 10%), the evaluation
measures precision and recall over true answers
predicted by each system.
Many answers in the task are incorrect be-
cause they violate common-sense intuition ? for
instance, one answer to What is leprosy? was
Africa clinic. While any such specific mistake is
easy to fix, our approach can be a means of han-
dling a wide range of such mistakes elegantly.
To adapt our system to the task, we first heuris-
tically converted the question into a query fact us-
ing the subject and object Stanford Dependency
labels (de Marneffe and Manning, 2008). If ei-
ther the subject or object specifies a type (e.g.,
Which party does Bill Clinton belong to?), the
score of the fact encoding this relationship (e.g.,
(Democrat, be, party)) is averaged with the main
query. Next, answers with very little n-gram over-
lap between the justification and either the ques-
tion or answer are filtered; this filters answers
which may be correct, but were not properly justi-
fied. Lastly, our system trained on Turk data (see
Section 5.3), predicts an answer to be correct if it
System 2007 2008P R F1 P R F1
all validated 11 100 19 8 100 14
filter only 16 95 27 14 100 24
median ? ? 35 ? ? 20
best ? ? 55 ? ? 64
system 31 62 41 16 43 23
Table 5: Classification accuracy for the Answer
Validation Exercise task. The baseline is accept-
ing all answers as correct (all validated); a second
baseline (filter only) incorporates only the n-gram
overlap threshold. The median and top performing
scores for both years are provided for comparison.
scores above the 65th percentile of candidate re-
sponse scores. Lastly, as our system has no princi-
pled way of handling numbers, any answer which
is entirely numeric is considered invalid.
Results are shown in Table 5. We evaluate on
the 2007 and 2008 datasets, outperforming the me-
dian score both years. Our system would place
third out of the eight systems that competed in
both the 2007 and 2008 tasks. As we are evaluat-
ing our system as a single component not trained
on the task, we understandably fall well under
the top performing systems; however, our perfor-
mance is nonetheless an indication that the system
provides a valuable signal for the task.
6 Conclusion
We have created a simple yet effective system to
determine the plausibility of an arbitrary fact, both
in terms of an intrinsic measure, and in down-
stream applications. Furthermore we have shown
that the confidences returned by our system are in-
formative, and that high-precision judgments can
be obtained even at reasonable recall. We hope
to devote future work to enriching the notion of
fact similarity, and better handling the noise in the
training data.
Acknowledgements We gratefully acknowledge the sup-
port of the Defense Advanced Research Projects Agency
(DARPA) Deep Exploration and Filtering of Text (DEFT)
Program under Air Force Research Laboratory (AFRL) con-
tract no. FA8750-13-2-0040. Any opinions, findings, and
conclusion or recommendations expressed in this material are
those of the authors and do not necessarily reflect the view of
the DARPA, AFRL, or the US government.
140
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram version 1. Linguistic Data Consortium.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic relat-
edness. Computational Linguistics, pages 13?47.
Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao.
2009. The ClueWeb09 data set.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI, pages 3?3.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP, pages 1535?1545.
Jerry R Hobbs. 1978. Resolving pronoun references.
Lingua, pages 311?338.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In ACL-HLT, pages 541?
550.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. ACL.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to
formal languages. In AAAI, pages 1062?1068, Pitts-
burgh, PA.
Dan Klein and Christopher D Manning. 2003. Accu-
rate unlexicalized parsing. In ACL, pages 423?430.
Hugo Liu and Push Singh. 2004. ConceptNet: a prac-
tical commonsense reasoning toolkit. BT technology
journal, pages 211?226.
Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In EMNLP.
John McCarthy. 1980. Circumscription?a form of
non-monotonic reasoning. Artificial intelligence,
pages 27?39.
George A. Miller. 1995. WordNet: a lexical database
for English. Communications of the ACM, pages
39?41.
Judea Pearl. 1989. Probabilistic semantics for non-
monotonic reasoning: A survey. Knowledge Repre-
sentation and Reasoning.
Anselmo Pen?as, A?lvaro Rodrigo, Valent??n Sama, and
Felisa Verdejo. 2007. Overview of the answer vali-
dation exercise 2006. In Evaluation of Multilingual
and Multi-modal Information Retrieval, pages 257?
264.
Anselmo Pen?as, A?lvaro Rodrigo, and Felisa Verdejo.
2008. Overview of the answer validation exercise
2007. In Advances in Multilingual and Multimodal
Information Retrieval, pages 237?248.
Raymond Reiter. 1980. A logic for default reasoning.
Artificial intelligence, pages 81?132.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL-HLT, pages 74?84.
A?lvaro Rodrigo, Anselmo Pen?as, and Felisa Verdejo.
2009. Overview of the answer validation exercise
2008. In Evaluating Systems for Multilingual and
Multimodal Information Access, pages 296?313.
Stefan Schoenmackers, Oren Etzioni, Daniel S Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In EMNLP, pages 1088?
1098.
Push Singh, Thomas Lin, Erik Mueller, Grace Lim,
Travell Perkins, and Wan Li Zhu. 2002. Open
mind common sense: Knowledge acquisition from
the general public. On the Move to Meaningful In-
ternet Systems 2002: CoopIS, DOA, and ODBASE,
pages 1223?1237.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In ACL, pages 801?808.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Oren Etzioni, et al 2010. Adapting open infor-
mation extraction to domain-specific relations. AI
Magazine, pages 93?102.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-
ati, and Christopher D. Manning. 2012. Multi-
instance multi-label learning for relation extraction.
In EMNLP.
Niket Tandon, Gerard de Melo, and Gerhard Weikum.
2011. Deriving a web-scale common sense fact
database. In AAAI.
141
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL-HLT, pages 173?180.
Ellen M Voorhees. 2001. Question answering in
TREC. In Proceedings of the tenth international
conference on Information and knowledge manage-
ment, pages 535?537.
Limin Yao, Sebastian Riedel, and Andrew McCal-
lum. 2012. Probabilistic databases of universal
schema. In Proceedings of the Joint Workshop on
Automatic Knowledge Base Construction and Web-
scale Knowledge Extraction, pages 116?121.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. TextRunner: Open information
extraction on the web. In ACL-HLT, pages 25?26.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI/IAAI, pages 1050?1055,
Portland, OR.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658?666. AUAI Press.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In EMNLP-CoNLL, pages 678?687.
142
