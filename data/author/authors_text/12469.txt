Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 459?467,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Employing Topic Models for Pattern-based Semantic Class Discovery 
 
 
Huibin Zhang1*     Mingjie Zhu2*     Shuming Shi3     Ji-Rong Wen3 
1Nankai University 
2University of Science and Technology of China 
3Microsoft Research Asia 
{v-huibzh, v-mingjz, shumings, jrwen}@microsoft.com 
 
  
 
Abstract? 
 
A semantic class is a collection of items 
(words or phrases) which have semantically 
peer or sibling relationship. This paper studies 
the employment of topic models to automati-
cally construct semantic classes, taking as the 
source data a collection of raw semantic 
classes (RASCs), which were extracted by ap-
plying predefined patterns to web pages. The 
primary requirement (and challenge) here is 
dealing with multi-membership: An item may 
belong to multiple semantic classes; and we 
need to discover as many as possible the dif-
ferent semantic classes the item belongs to. To 
adopt topic models, we treat RASCs as ?doc-
uments?, items as ?words?, and the final se-
mantic classes as ?topics?. Appropriate 
preprocessing and postprocessing are per-
formed to improve results quality, to reduce 
computation cost, and to tackle the fixed-k 
constraint of a typical topic model. Experi-
ments conducted on 40 million web pages 
show that our approach could yield better re-
sults than alternative approaches. 
1 Introduction 
Semantic class construction (Lin and Pantel, 
2001; Pantel and Lin, 2002; Pasca, 2004; Shinza-
to and Torisawa, 2005; Ohshima et al, 2006) 
tries to discover the peer or sibling relationship 
among terms or phrases by organizing them into 
semantic classes. For example, {red, white, 
black?} is a semantic class consisting of color 
instances. A popular way for semantic class dis-
covery is pattern-based approach, where prede-
fined patterns (Table 1) are applied to a 
                                                   
? This work was performed when the authors were interns at 
Microsoft Research Asia 
collection of web pages or an online web search 
engine to produce some raw semantic classes 
(abbreviated as RASCs, Table 2). RASCs cannot 
be treated as the ultimate semantic classes, be-
cause they are typically noisy and incomplete, as 
shown in Table 2. In addition, the information of 
one real semantic class may be distributed in lots 
of RASCs (R2 and R3 in Table 2). 
 
Type Pattern 
SENT NP {, NP}*{,} (and|or) {other} NP 
TAG <UL>  <LI>item</LI>  ?  <LI>item</LI>  </UL> 
TAG <SELECT> <OPTION>item?<OPTION>item </SELECT> 
* SENT: Sentence structure patterns; TAG: HTML Tag patterns 
Table 1. Sample patterns 
 
R1: {gold, silver, copper, coal, iron, uranium} 
R2: {red, yellow, color, gold, silver, copper} 
R3: {red, green, blue, yellow} 
R4: {HTML, Text, PDF, MS Word, Any file type} 
R5: {Today, Tomorrow, Wednesday, Thursday, Friday, 
Saturday, Sunday} 
R6: {Bush, Iraq, Photos, USA, War} 
Table 2. Sample raw semantic classes (RASCs) 
 
This paper aims to discover high-quality se-
mantic classes from a large collection of noisy 
RASCs. The primary requirement (and chal-
lenge) here is to deal with multi-membership, i.e., 
one item may belong to multiple different seman-
tic classes. For example, the term ?Lincoln? can 
simultaneously represent a person, a place, or a 
car brand name. Multi-membership is more pop-
ular than at a first glance, because quite a lot of 
English common words have also been borrowed 
as company names, places, or product names. 
For a given item (as a query) which belongs to 
multiple semantic classes, we intend to return the 
semantic classes separately, rather than mixing 
all their items together. 
Existing pattern-based approaches only pro-
vide very limited support to multi-membership. 
For example, RASCs with the same labels (or 
hypernyms) are merged in (Pasca, 2004) to gen-
459
erate the ultimate semantic classes. This is prob-
lematic, because RASCs may not have (accurate) 
hypernyms with them. 
In this paper, we propose to use topic models 
to address the problem. In some topic models, a 
document is modeled as a mixture of hidden top-
ics. The words of a document are generated ac-
cording to the word distribution over the topics 
corresponding to the document (see Section 2 for 
details). Given a corpus, the latent topics can be 
obtained by a parameter estimation procedure. 
Topic modeling provides a formal and conve-
nient way of dealing with multi-membership, 
which is our primary motivation of adopting top-
ic models here. To employ topic models, we treat 
RASCs as ?documents?, items as ?words?, and 
the final semantic classes as ?topics?. 
There are, however, several challenges in ap-
plying topic models to our problem. To begin 
with, the computation is intractable for 
processing a large collection of RASCs (our da-
taset for experiments contains 2.7 million unique 
RASCs extracted from 40 million web pages). 
Second, typical topic models require the number 
of topics (k) to be given. But it lacks an easy way 
of acquiring the ideal number of semantic classes 
from the source RASC collection. For the first 
challenge, we choose to apply topic models to 
the RASCs containing an item q, rather than the 
whole RASC collection. In addition, we also per-
form some preprocessing operations in which 
some items are discarded to further improve effi-
ciency. For the second challenge, considering 
that most items only belong to a small number of 
semantic classes, we fix (for all items q) a topic 
number which is slightly larger than the number 
of classes an item could belong to. And then a 
postprocessing operation is performed to merge 
the results of topic models to generate the ulti-
mate semantic classes. 
Experimental results show that, our topic 
model approach is able to generate higher-quality 
semantic classes than popular clustering algo-
rithms (e.g., K-Medoids and DBSCAN). 
We make two contributions in the paper: On 
one hand, we find an effective way of construct-
ing high-quality semantic classes in the pattern-
based category which deals with multi-
membership. On the other hand, we demonstrate, 
for the first time, that topic modeling can be uti-
lized to help mining the peer relationship among 
words. In contrast, the general related relation-
ship between words is extracted in existing topic 
modeling applications. Thus we expand the ap-
plication scope of topic modeling. 
2 Topic Models 
In this section we briefly introduce the two wide-
ly used topic models which are adopted in our 
paper. Both of them model a document as a mix-
ture of hidden topics. The words of every docu-
ment are assumed to be generated via a 
generative probability process. The parameters of 
the model are estimated from a training process 
over a given corpus, by maximizing the likelih-
ood of generating the corpus. Then the model can 
be utilized to inference a new document. 
pLSI: The probabilistic Latent Semantic In-
dexing Model (pLSI) was introduced in Hof-
mann (1999), arose from Latent Semantic 
Indexing (Deerwester et al, 1990). The follow-
ing process illustrates how to generate a docu-
ment d in pLSI: 
1. Pick a topic mixture distribution ?(? |?). 
2. For each word wi in d 
a. Pick a latent topic z with the probabil-
ity ?(?|?) for wi 
b. Generate wi with probability ?(?? |?) 
So with k latent topics, the likelihood of gene-
rating a document d is 
 ?(?) =  ? ?? ? ?(?|?)
??
 (2.1) 
LDA (Blei et al, 2003): In LDA, the topic 
mixture is drawn from a conjugate Dirichlet prior 
that remains the same for all documents (Figure 
1). The generative process for each document in 
the corpus is, 
1. Choose document length N from a Pois-
son distribution Poisson(?). 
2. Choose ?  from a Dirichlet distribution 
with parameter ?. 
3. For each of the N words wi. 
a. Choose a topic z from a Multinomial 
distribution with parameter ?. 
b. Pick a word wi from ? ??  ?,? . 
So the likelihood of generating a document is 
 ?(?) =  ?(?|?)
?
  ?(?|?)? ?? ?,? ??
??
 (2.2) 
 
 
Figure 1. Graphical model representation of LDA, 
from Blei et al (2003) 
 
w? z?
?
N
M
460
3 Our Approach 
The source data of our approach is a collection 
(denoted as CR) of RASCs extracted via applying 
patterns to a large collection of web pages. Given 
an item as an input query, the output of our ap-
proach is one or multiple semantic classes for the 
item. To be applicable in real-world dataset, our 
approach needs to be able to process at least mil-
lions of RASCs. 
3.1 Main Idea 
As reviewed in Section 2, topic modeling pro-
vides a formal and convenient way of grouping 
documents and words to topics. In order to apply 
topic models to our problem, we map RASCs to 
documents, items to words, and treat the output 
topics yielded from topic modeling as our seman-
tic classes (Table 3). The motivation of utilizing 
topic modeling to solve our problem and building 
the above mapping comes from the following 
observations. 
1) In our problem, one item may belong to 
multiple semantic classes; similarly in topic 
modeling, a word can appear in multiple top-
ics. 
2) We observe from our source data that 
some RASCs are comprised of items in mul-
tiple semantic classes. And at the same time, 
one document could be related to multiple 
topics in some topic models (e.g., pLSI and 
LDA). 
 
Topic modeling Semantic class construction 
word item (word or phrase) 
document RASC 
topic semantic class 
Table 3. The mapping from the concepts in topic 
modeling to those in semantic class construction 
 
Due to the above observations, we hope topic 
modeling can be employed to construct semantic 
classes from RASCs, just as it has been used in 
assigning documents and words to topics. 
There are some critical challenges and issues 
which should be properly addressed when topic 
models are adopted here. 
Efficiency: Our RASC collection CR contains 
about 2.7 million unique RASCs and 26 million 
(1 million unique) items. Building topic models 
directly for such a large dataset may be computa-
tionally intractable. To overcome this challenge, 
we choose to apply topic models to the RASCs 
containing a specific item rather than the whole 
RASC collection. Please keep in mind that our 
goal in this paper is to construct the semantic 
classes for an item when the item is given as a 
query. For one item q, we denote CR(q) to be all 
the RASCs in CR containing the item. We believe 
building a topic model over CR(q) is much more 
effective because it contains significantly fewer 
?documents?, ?words?, and ?topics?. To further 
improve efficiency, we also perform preprocess-
ing (refer to Section 3.4 for details) before build-
ing topic models for CR(q), where some low-
frequency items are removed. 
Determine the number of topics: Most topic 
models require the number of topics to be known 
beforehand1. However, it is not an easy task to 
automatically determine the exact number of se-
mantic classes an item q should belong to. Ac-
tually the number may vary for different q. Our 
solution is to set (for all items q) the topic num-
ber to be a fixed value (k=5 in our experiments) 
which is slightly larger than the number of se-
mantic classes most items could belong to. Then 
we perform postprocessing for the k topics to 
produce the final properly semantic classes. 
In summary, our approach contains three 
phases (Figure 2). We build topic models for 
every CR(q), rather than the whole collection CR. 
A preprocessing phase and a postprocessing 
phase are added before and after the topic model-
ing phase to improve efficiency and to overcome 
the fixed-k problem. The details of each phase 
are presented in the following subsections. 
 
 
Figure 2. Main phases of our approach 
 
3.2 Adopting Topic Models 
For an item q, topic modeling is adopted to 
process the RASCs in CR(q) to generate k seman-
tic classes. Here we use LDA as an example to 
                                                   
1 Although there is study of non-parametric Bayesian mod-
els (Li et al, 2007) which need no prior knowledge of topic 
number, the computational complexity seems to exceed our 
efficiency requirement and we shall leave this to future 
work. 
R580 
R1 
R2 
CR 
Item q 
Preprocessing 
?400
?  
?1
? 
?2
? 
T5 
T1 
T2 
C3 
C1 
C2 
Topic  
modeling 
Postprocessing 
T3 
T4 
CR(q) 
461
illustrate the process. The case of other genera-
tive topic models (e.g., pLSI) is very similar. 
According to the assumption of LDA and our 
concept mapping in Table 3, a RASC (?docu-
ment?) is viewed as a mixture of hidden semantic 
classes (?topics?). The generative process for a 
RASC R in the ?corpus? CR(q) is as follows, 
1) Choose a RASC size (i.e., the number of 
items in R): NR ~ Poisson(?). 
2) Choose a k-dimensional vector ??  from a 
Dirichlet distribution with parameter ?. 
3) For each of the NR items an: 
a) Pick a semantic class ??  from a mul-
tinomial distribution with parameter 
?? . 
b) Pick an item an from ?(?? |?? ,?) , 
where the item probabilities are pa-
rameterized by the matrix ?. 
There are three parameters in the model: ? (a 
scalar), ?  (a k-dimensional vector), and ?  (a 
? ? ? matrix where V is the number of distinct 
items in CR(q)). The parameter values can be ob-
tained from a training (or called parameter esti-
mation) process over CR(q), by maximizing the 
likelihood of generating the corpus. Once ?  is 
determined, we are able to compute ?(?|?,?), 
the probability of item a belonging to semantic 
class z. Therefore we can determine the members 
of a semantic class z by selecting those items 
with high ? ? ?,?  values. 
The number of topics k is assumed known and 
fixed in LDA. As has been discussed in Section 
3.1, we set a constant k value for all different 
CR(q). And we rely on the postprocessing phase 
to merge the semantic classes produced by the 
topic model to generate the ultimate semantic 
classes. 
When topic modeling is used in document 
classification, an inference procedure is required 
to determine the topics for a new document. 
Please note that inference is not needed in our 
problem. 
One natural question here is: Considering that 
in most topic modeling applications, the words 
within a resultant topic are typically semantically 
related but may not be in peer relationship, then 
what is the intuition that the resultant topics here 
are semantic classes rather than lists of generally 
related words? The magic lies in the ?docu-
ments? we used in employing topic models. 
Words co-occurred in real documents tend to be 
semantically related; while items co-occurred in 
RASCs tend to be peers. Experimental results 
show that most items in the same output seman-
tic class have peer relationship. 
It might be noteworthy to mention the exchan-
geability or ?bag-of-words? assumption in most 
topic models. Although the order of words in a 
document may be important, standard topic mod-
els neglect the order for simplicity and other rea-
sons2. The order of items in a RASC is clearly 
much weaker than the order of words in an ordi-
nary document. In some sense, topic models are 
more suitable to be used here than in processing 
an ordinary document corpus. 
3.3 Preprocessing and Postprocessing 
Preprocessing is applied to CR(q) before we build 
topic models for it. In this phase, we discard 
from all RASCs the items with frequency (i.e., 
the number of RASCs containing the item) less 
than a threshold h. A RASC itself is discarded 
from CR(q) if it contains less than two items after 
the item-removal operations. We choose to re-
move low-frequency items, because we found 
that low-frequency items are seldom important 
members of any semantic class for q. So the goal 
is to reduce the topic model training time (by 
reducing the training data) without sacrificing 
results quality too much. In the experiments sec-
tion, we compare the approaches with and with-
out preprocessing in terms of results quality and 
efficiency. Interestingly, experimental results 
show that, for some small threshold values, the 
results quality becomes higher after preprocess-
ing is performed. We will give more discussions 
in Section 4. 
In the postprocessing phase, the output seman-
tic classes (?topics?) of topic modeling are 
merged to generate the ultimate semantic classes. 
As indicated in Sections 3.1 and 3.2, we fix the 
number of topics (k=5) for different corpus CR(q) 
in employing topic models. For most items q, 
this is a larger value than the real number of se-
mantic classes the item belongs to. As a result, 
one real semantic class may be divided into mul-
tiple topics. Therefore one core operation in this 
phase is to merge those topics into one semantic 
class. In addition, the items in each semantic 
class need to be properly ordered. Thus main 
operations include, 
1) Merge semantic classes 
2) Sort the items in each semantic class 
Now we illustrate how to perform the opera-
tions. 
Merge semantic classes: The merge process 
is performed by repeatedly calculating the simi-
                                                   
2 There are topic model extensions considering word order 
in documents, such as Griffiths et al (2005). 
462
larity between two semantic classes and merging 
the two ones with the highest similarity until the 
similarity is under a threshold. One simple and 
straightforward similarity measure is the Jaccard 
coefficient, 
 ??? ?1 ,?2 =
 ?1 ? ?2 
 ?1 ? ?2 
 (3.1) 
where ?1 ? ?2  and ?1 ? ?2  are respectively the 
intersection and union of semantic classes C1 and 
C2. This formula might be over-simple, because 
the similarity between two different items is not 
exploited. So we propose the following measure, 
 ??? ?1 ,?2 =
  ???(?, ?)???2???1
 ?1 ?  ?2 
 (3.2) 
where |C| is the number of items in semantic 
class C, and sim(a,b) is the similarity between 
items a and b, which will be discussed shortly. In 
Section 4, we compare the performance of the 
above two formulas by experiments. 
Sort items: We assign an importance score to 
every item in a semantic class and sort them ac-
cording to the importance scores. Intuitively, an 
item should get a high rank if the average simi-
larity between the item and the other items in the 
semantic class is high, and if it has high similari-
ty to the query item q. Thus we calculate the im-
portance of item a in a semantic class C as 
follows, 
 ? ?|? = ? ?sim(a,C)+(1-?) ?sim(a,q) (3.3) 
where ? is a parameter in [0,1], sim(a,q) is the 
similarity between a and the query item q, and 
sim(a,C) is the similarity between a and C, calcu-
lated as, 
 ??? ?,? =
 ???(?, ?)???
 ? 
 (3.4) 
Item similarity calculation: Formulas 3.2, 
3.3, and 3.4 rely on the calculation of the similar-
ity between two items. 
One simple way of estimating item similarity 
is to count the number of RASCs containing both 
of them. We extend such an idea by distinguish-
ing the reliability of different patterns and pu-
nishing term similarity contributions from the 
same site. The resultant similarity formula is, 
 ???(?,?) = log(1 + ?(?(?? ,? ))
??
?=1
)
?
?=1
 (3.5) 
where Ci,j is a RASC containing both a and b, 
P(Ci,j) is the pattern via which the RASC is ex-
tracted, and w(P) is the weight of pattern P. As-
sume all these RASCs belong to m sites with Ci,j 
extracted from a page in site i, and ki being the 
number of RASCs corresponding to site i. To 
determine the weight of every type of pattern, we 
randomly selected 50 RASCs for each pattern 
and labeled their quality. The weight of each 
kind of pattern is then determined by the average 
quality of all labeled RASCs corresponding to it. 
The efficiency of postprocessing is not a prob-
lem, because the time cost of postprocessing is 
much less than that of the topic modeling phase. 
3.4 Discussion 
3.4.1 Efficiency of processing popular items 
Our approach receives a query item q from users 
and returns the semantic classes containing the 
query. The maximal query processing time 
should not be larger than several seconds, be-
cause users would not like to wait more time. 
Although the average query processing time of 
our approach is much shorter than 1 second (see 
Table 4 in Section 4), it takes several minutes to 
process a popular item such as ?Washington?, 
because it is contained in a lot of RASCs. In or-
der to reduce the maximal online processing 
time, our solution is offline processing popular 
items and storing the resultant semantic classes 
on disk. The time cost of offline processing is 
feasible, because we spent about 15 hours on a 4-
core machine to complete the offline processing 
for all the items in our RASC collection. 
3.4.2 Alternative approaches 
One may be able to easily think of other ap-
proaches to address our problem. Here we dis-
cuss some alternative approaches which are 
treated as our baseline in experiments. 
RASC clustering: Given a query item q, run a 
clustering algorithm over CR(q) and merge all 
RASCs in the same cluster as one semantic class. 
Formula 3.1 or 3.2 can be used to compute the 
similarity between RASCs in performing cluster-
ing. We try two clustering algorithms in experi-
ments: K-Medoids and DBSCAN. Please note k-
means cannot be utilized here because coordi-
nates are not available for RASCs. One draw-
back of RASC clustering is that it cannot deal 
with the case of one RASC containing the items 
from multiple semantic classes. 
Item clustering: By Formula 3.5, we are able 
to construct an item graph GI to record the 
neighbors (in terms of similarity) of each item. 
Given a query item q, we first retrieve its neigh-
bors from GI, and then run a clustering algorithm 
over the neighbors. As in the case of RASC clus-
tering, we try two clustering algorithms in expe-
riments: K-Medoids and DBSCAN. The primary 
disadvantage of item clustering is that it cannot 
assign an item (except for the query item q) to 
463
multiple semantic classes. As a result, when we 
input ?gold? as the query, the item ?silver? can 
only be assigned to one semantic class, although 
the term can simultaneously represents a color 
and a chemical element. 
4 Experiments 
4.1 Experimental Setup 
Datasets: By using the Open Directory Project 
(ODP3) URLs as seeds, we crawled about 40 mil-
lion English web pages in a breadth-first way. 
RASCs are extracted via applying a list of sen-
tence structure patterns and HTML tag patterns 
(see Table 1 for some examples). Our RASC col-
lection CR contains about 2.7 million unique 
RASCs and 1 million distinct items. 
Query set and labeling: We have volunteers 
to try Google Sets4, record their queries being 
used, and select overall 55 queries to form our 
query set. For each query, the results of all ap-
proaches are mixed together and labeled by fol-
lowing two steps. In the first step, the standard 
(or ideal) semantic classes (SSCs) for the query 
are manually determined. For example, the ideal 
semantic classes for item ?Georgia? may include 
Countries, and U.S. states. In the second step, 
each item is assigned a label of ?Good?, ?Fair?, 
or ?Bad? with respect to each SSC. For example, 
?silver? is labeled ?Good? with respect to ?col-
ors? and ?chemical elements?. We adopt metric 
MnDCG (Section 4.2) as our evaluation metric. 
Approaches for comparison: We compare 
our approach with the alternative approaches dis-
cussed in Section 3.4.2. 
LDA: Our approach with LDA as the topic 
model. The implementation of LDA is based 
on Blei?s code of variational EM for LDA5. 
pLSI: Our approach with pLSI as the topic 
model. The implementation of pLSI is based 
on Schein, et al (2002). 
KMedoids-RASC: The RASC clustering ap-
proach illustrated in Section 3.4.2, with the 
K-Medoids clustering algorithm utilized. 
DBSCAN-RASC: The RASC clustering ap-
proach with DBSCAN utilized. 
KMedoids-Item: The item clustering ap-
proach with the K-Medoids utilized. 
DBSCAN-Item: The item clustering ap-
proach with the DBSCAN clustering algo-
rithm utilized. 
                                                   
3 http://www.dmoz.org 
4 http://labs.google.com/sets 
5 http://www.cs.princeton.edu/~blei/lda-c/ 
K-Medoids clustering needs to predefine the 
cluster number k. We fix the k value for all dif-
ferent query item q, as has been done for the top-
ic model approach. For fair comparison, the same 
postprocessing is made for all the approaches. 
And the same preprocessing is made for all the 
approaches except for the item clustering ones 
(to which the preprocessing is not applicable). 
4.2 Evaluation Methodology 
Each produced semantic class is an ordered list 
of items. A couple of metrics in the information 
retrieval (IR) community like Precision@10, 
MAP (mean average precision), and nDCG 
(normalized discounted cumulative gain) are 
available for evaluating a single ranked list of 
items per query (Croft et al, 2009). Among the 
metrics, nDCG (Jarvelin and Kekalainen, 2000) 
can handle our three-level judgments (?Good?, 
?Fair?, and ?Bad?, refer to Section 4.1), 
 ????@? =
 ? ? /log(? + 1)??=1
 ?? ? /log(? + 1)??=1
 (4.1) 
where G(i) is the gain value assigned to the i?th 
item, and G*(i) is the gain value assigned to the 
i?th item of an ideal (or perfect) ranking list. 
Here we extend the IR metrics to the evalua-
tion of multiple ordered lists per query. We use 
nDCG as the basic metric and extend it to 
MnDCG. 
Assume labelers have determined m SSCs 
(SSC1~SSCm, refer to Section 4.1) for query q 
and the weight (or importance) of SSCi is wi. As-
sume n semantic classes are generated by an ap-
proach and n1 of them have corresponding SSCs 
(i.e., no appropriate SSC can be found for the 
remaining n-n1 semantic classes). We define the 
MnDCG score of an approach (with respect to 
query q) as, 
 ????? ? =
?1
?
?
 ?? ? ?????(SSC?)
?
i=1
 ??
m
i=1
 (4.2) 
where 
 ????? ???? =  
0                                         ?? ?? = 0
1
??
max
? ?[1, ??]
(???? ?? ,?  )  ?? ?? ? 0
  (4.3) 
In the above formula, nDCG(Gi,j) is the nDCG 
score of semantic class Gi,j; and ki denotes the 
number of semantic classes assigned to SSCi. For 
a list of queries, the MnDCG score of an algo-
rithm is the average of all scores for the queries. 
The metric is designed to properly deal with 
the following cases, 
464
i). One semantic class is wrongly split into 
multiple ones: Punished by dividing ??  in 
Formula 4.3; 
ii). A semantic class is too noisy to be as-
signed to any SSC: Processed by the 
?n1/n? in Formula 4.2; 
iii). Fewer semantic classes (than the number 
of SSCs) are produced: Punished in For-
mula 4.3 by assigning a zero value. 
iv). Wrongly merge multiple semantic 
classes into one: The nDCG score of the 
merged one will be small because it is 
computed with respect to only one single 
SSC. 
The gain values of nDCG for the three relev-
ance levels (?Bad?, ?Fair?, and ?Good?) are re-
spectively -1, 1, and 2 in experiments. 
4.3 Experimental  Results 
4.3.1 Overall performance comparison 
Figure 3 shows the performance comparison be-
tween the approaches listed in Section 4.1, using 
metrics MnDCG@n (n=1?10). Postprocessing 
is performed for all the approaches, where For-
mula 3.2 is adopted to compute the similarity 
between semantic classes. The results show that 
that the topic modeling approaches produce 
higher-quality semantic classes than the other 
approaches. It indicates that the topic mixture 
assumption of topic modeling can handle the 
multi-membership problem very well here. 
Among the alternative approaches, RASC clus-
tering behaves better than item clustering. The 
reason might be that an item cannot belong to 
multiple clusters in the two item clustering ap-
proaches, while RASC clustering allows this. For 
the RASC clustering approaches, although one 
item has the chance to belong to different seman-
tic classes, one RASC can only belong to one 
semantic class. 
 
 
Figure 3. Quality comparison (MnDCG@n) among 
approaches (frequency threshold h = 4 in preprocess-
ing; k = 5 in topic models) 
4.3.2 Preprocessing experiments 
Table 4 shows the average query processing time 
and results quality of the LDA approach, by va-
rying frequency threshold h. Similar results are 
observed for the pLSI approach. In the table, h=1 
means no preprocessing is performed. The aver-
age query processing time is calculated over all 
items in our dataset. As the threshold h increases, 
the processing time decreases as expected, be-
cause the input of topic modeling gets smaller. 
The second column lists the results quality 
(measured by MnDCG@10). Interestingly, we 
get the best results quality when h=4 (i.e., the 
items with frequency less than 4 are discarded). 
The reason may be that most low-frequency 
items are noisy ones. As a result, preprocessing 
can improve both results quality and processing 
efficiency; and h=4 seems a good choice in pre-
processing for our dataset. 
 
h 
Avg. Query Proc. 
Time (seconds) 
Quality 
(MnDCG@10) 
1 0.414 0.281 
2 0.375 0.294 
3 0.320 0.322 
4 0.268 0.331 
5 0.232 0.328 
6 0.210 0.315 
7 0.197 0.315 
8 0.184 0.313 
9 0.173 0.288 
Table 4. Time complexity and quality comparison 
among LDA approaches of different thresholds 
 
4.3.3 Postprocessing experiments 
 
Figure 4. Results quality comparison among topic 
modeling approaches with and without postprocessing 
(metric: MnDCG@10) 
 
The effect of postprocessing is shown in Figure 
4. In the figure, NP means no postprocessing is 
performed. Sim1 and Sim2 respectively mean 
Formula 3.1 and Formula 3.2 are used in post-
processing as the similarity measure between 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
1 2 3 4 5 6 7 8 9 10
pLSI LDA KMedoids-RASC
DBSCAN-RASC KMedoids-Item DBSCAN-Item
n
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
LDA pLSI
NP
Sim1 
Sim2
465
semantic classes. The same preprocessing (h=4) 
is performed in generating the data. It can be 
seen that postprocessing improves results quality. 
Sim2 achieves more performance improvement 
than Sim1, which demonstrates the effectiveness 
of the similarity measure in Formula 3.2. 
4.3.4 Sample results 
Table 5 shows the semantic classes generated by 
our LDA approach for some sample queries in 
which the bad classes or bad members are hig-
hlighted (to save space, 10 items are listed here, 
and the query itself is omitted in the resultant 
semantic classes).  
 
Query Semantic Classes 
apple 
C1: ibm, microsoft, sony, dell, toshiba,  sam-
sung, panasonic, canon, nec, sharp ? 
C2: peach, strawberry, cherry, orange, bana-
na, lemon, pineapple, raspberry, pear, grape 
? 
gold 
C1: silver, copper, platinum, zinc, lead, iron, 
nickel, tin, aluminum, manganese ? 
C2: silver, red, black, white, blue, purple, 
orange, pink, brown, navy ? 
C3: silver, platinum, earrings, diamonds, 
rings, bracelets, necklaces, pendants, jewelry, 
watches ? 
C4: silver, home, money, business, metal, 
furniture, shoes, gypsum, hematite, fluorite 
?  
lincoln 
C1: ford, mazda, toyota, dodge, nissan, hon-
da, bmw, chrysler, mitsubishi, audi ? 
C2: bristol, manchester, birmingham, leeds, 
london, cardiff, nottingham, newcastle, shef-
field, southampton ? 
C3: jefferson, jackson, washington, madison, 
franklin, sacramento, new york city, monroe, 
Louisville, marion ? 
computer 
science 
C1: chemistry, mathematics, physics, biolo-
gy, psychology, education, history, music, 
business, economics ? 
Table 5. Semantic classes generated by our approach 
for some sample queries (topic model = LDA) 
 
5 Related Work 
Several categories of work are related to ours. 
The first category is about set expansion (i.e., 
retrieving one semantic class given one term or a 
couple of terms). Syntactic context information is 
used (Hindle, 1990; Ruge, 1992; Lin, 1998) to 
compute term similarities, based on which simi-
lar words to a particular word can directly be 
returned. Google sets is an online service which, 
given one to five items, predicts other items in 
the set. Ghahramani and Heller (2005) introduce 
a Bayesian Sets algorithm for set expansion. Set 
expansion is performed by feeding queries to 
web search engines in Wang and Cohen (2007) 
and Kozareva (2008). All of the above work only 
yields one semantic class for a given query. 
Second, there are pattern-based approaches in the 
literature which only do limited integration of 
RASCs (Shinzato and Torisawa, 2004; Shinzato 
and Torisawa, 2005; Pasca, 2004), as discussed 
in the introduction section. In Shi et al (2008), 
an ad-hoc approach was proposed to discover the 
multiple semantic classes for one item. The third 
category is distributional similarity approaches 
which provide multi-membership support (Har-
ris, 1985; Lin  and Pantel, 2001; Pantel and Lin, 
2002). Among them, the CBC algorithm (Pantel 
and Lin, 2002) addresses the multi-membership 
problem. But it relies on term vectors and centro-
ids which are not available in pattern-based ap-
proaches. It is therefore not clear whether it can 
be borrowed to deal with multi-membership here. 
Among the various applications of topic 
modeling, maybe the efforts of using topic model 
for Word Sense Disambiguation (WSD) are most 
relevant to our work. In Cai et al(2007), LDA is 
utilized to capture the global context information 
as the topic features for better performing the 
WSD task. In Boyd-Graber et al (2007), Latent 
Dirichlet with WordNet (LDAWN) is developed 
for simultaneously disambiguating a corpus and 
learning the domains in which to consider each 
word. They do not generate semantic classes. 
6 Conclusions 
We presented an approach that employs topic 
modeling for semantic class construction. Given 
an item q, we first retrieve all RASCs containing 
the item to form a collection CR(q). Then we per-
form some preprocessing to CR(q) and build a 
topic model for it. Finally, the output semantic 
classes of topic modeling are post-processed to 
generate the final semantic classes. For the CR(q) 
which contains a lot of RASCs, we perform of-
fline processing according to the above process 
and store the results on disk, in order to reduce 
the online query processing time. 
We also proposed an evaluation methodology 
for measuring the quality of semantic classes. 
We show by experiments that our topic modeling 
approach outperforms the item clustering and 
RASC clustering approaches. 
 
Acknowledgments 
We wish to acknowledge help from Xiaokang 
Liu for mining RASCs from web pages, Chan-
gliang Wang and Zhongkai Fu for data process.  
  
466
References  
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent dirichlet alocation. J. Mach. Learn. 
Res., 3:993?1022. 
Bruce Croft, Donald Metzler, and Trevor Strohman. 
2009. Search Engines: Information Retrieval in 
Practice. Addison Wesley.  
Jordan Boyd-Graber, David Blei, and Xiaojin 
Zhu.2007. A topic model for word sense disambig-
uation. In Proceedings EMNLP-CoNLL 2007, pag-
es 1024?1033, Prague, Czech Republic, June. 
Association for Computational Linguistics. 
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007. 
NUS-ML: Improving word sense disambiguation 
using topic features. In Proceedings of the Interna-
tional Workshop on Semantic Evaluations, volume 
4. 
Scott Deerwester, Susan T. Dumais, GeorgeW. Fur-
nas, Thomas K. Landauer, and Richard Harshman. 
1990. Indexing by latent semantic analysis. Journal 
of the American Society for Information Science, 
41:391?407. 
Zoubin Ghahramani and Katherine A. Heller. 2005. 
Bayesian Sets. In Advances in Neural Information 
Processing Systems (NIPS05). 
Thomas L. Griffiths, Mark Steyvers, David M. 
Blei,and Joshua B. Tenenbaum. 2005. Integrating 
topics and syntax. In Advances in Neural Informa-
tion Processing Systems 17, pages 537?544. MIT 
Press 
Zellig Harris. Distributional Structure. The Philoso-
phy of Linguistics. New York: Oxford University 
Press. 1985. 
Donald Hindle. 1990. Noun Classification from Pre-
dicate-Argument Structures. In Proceedings of 
ACL90, pages 268?275.  
Thomas Hofmann. 1999. Probabilistic latent semantic 
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR99, pages 50?57, New York, 
NY, USA. ACM. 
Kalervo Jarvelin, and Jaana Kekalainen. 2000. IR 
Evaluation Methods for Retrieving Highly Rele-
vant Documents. In Proceedings of the 23rd An-
nual International ACM SIGIR Conference on 
Research and Development in Information Retriev-
al (SIGIR2000). 
Zornitsa Kozareva, Ellen Riloff and Eduard Hovy. 
2008. Semantic Class Learning from the Web with 
Hyponym Pattern Linkage Graphs, In Proceedings 
of ACL-08. 
Wei Li, David M. Blei, and Andrew McCallum. Non-
parametric Bayes Pachinko Allocation. In Proceed-
ings of Conference on Uncertainty in Artificial In-
telligence (UAI), 2007. 
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of COLING-
ACL98, pages 768-774. 
Dekang Lin and Patrick Pantel. 2001. Induction of 
Semantic Classes from Natural Language Text. In 
Proceedings of SIGKDD01, pages 317-322.  
Hiroaki Ohshima, Satoshi Oyama, and Katsumi Tana-
ka. 2006. Searching coordinate terms with their 
context from the web. In WISE06, pages 40?47. 
Patrick Pantel and Dekang Lin. 2002. Discovering 
Word Senses from Text. In Proceedings of 
SIGKDD02.  
Marius Pasca. 2004. Acquisition of Categorized 
Named Entities for Web Search. In Proc. of 2004 
CIKM.  
Gerda Ruge. 1992. Experiments on Linguistically-
Based Term Associations. In Information 
Processing & Management, 28(3), pages 317-32. 
Andrew I. Schein,  Alexandrin Popescul,  Lyle H. 
Ungar and David M. Pennock. 2002. Methods and 
metrics for cold-start recommendations. In Pro-
ceedings of SIGIR02, pages  253-260. 
Shuming Shi, Xiaokang Liu and Ji-Rong Wen. 2008. 
Pattern-based Semantic Class Discovery with Mul-
ti-Membership Support. In CIKM2008, pages 
1453-1454.  
Keiji Shinzato and Kentaro Torisawa. 2004. Acquir-
ing Hyponymy Relations from Web Documents. In 
HLT/NAACL04, pages 73?80. 
Keiji Shinzato and Kentaro Torisawa. 2005. A Simple 
WWW-based Method for Semantic Word Class 
Acquisition. In RANLP05.  
Richard C. Wang and William W. Cohen. 2007. Lan-
gusage-Independent Set Expansion of Named Enti-
ties Using the Web. In ICDM2007. 
 
467
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 993?1001,
Beijing, August 2010
Corpus-based Semantic Class Mining: 
Distributional vs. Pattern-Based Approaches 
Shuming Shi1    Huibin Zhang2*    Xiaojie Yuan2    Ji-Rong Wen1 
1 Microsoft Research Asia 
2 Nankai University 
{shumings, jrwen}@microsoft.com 
zhanghuibin@126.com; yuanxj@nankai.edu.cn 
 
 
Abstract 
Main approaches to corpus-based seman-
tic class mining include distributional 
similarity (DS) and pattern-based (PB). 
In this paper, we perform an empirical 
comparison of them, based on a publicly 
available dataset containing 500 million 
web pages, using various categories of 
queries. We further propose a frequency-
based rule to select appropriate approach-
es for different types of terms. 
1 Introduction1 
Computing the semantic relationship between 
terms, which has wide applications in natural 
language processing and web search, has been a 
hot topic nowadays. This paper focuses on cor-
pus-based semantic class mining (Lin 1998; Pan-
tel and Lin 2002; Pasca 2004; Shinzato and 
Torisawa, 2005; Ohshima, et al, 2006; Zhang et 
al., 2009), where peer terms (or coordinate terms) 
are discovered from a corpus. 
Existing approaches to semantic class mining 
could roughly be divided into two categories: 
distributional similarity (DS), and pattern-based 
(PB). The first type of work (Hindle, 1990; Lin 
1998; Pantel and Lin 2002) is based on the distri-
butional hypothesis (Harris, 1985), saying that 
terms occurring in analogous (lexical or syntactic) 
contexts tend to be similar. DS approaches basi-
cally exploit second-order co-occurrences to dis-
cover strongly associated concepts. In pattern-
based approaches (Hearst 1992; Pasca 2004; 
Shinzato and Torisawa, 2005; Ohshima, et al, 
2006; Zhang et al, 2009), patterns are applied to 
                                                 
* Work done during an internship at Microsoft 
discover specific relationships between terms, 
from the general first-order co-occurrences. For 
example, ?NP such as NP, NP?, and NP? is a 
popular and high-quality pattern for extracting 
peer terms (and also hyponyms). Besides the nat-
ural language patterns, some HTML tag tree pat-
terns (e.g., the drop down list) are also effective 
in semantic class mining. 
It is worth-noting that the word ?pattern? also 
appears in some DS approaches (Pasca et al, 
2006; Tanev and Magnini, 2006; Pennacchiotti 
and Pantel, 2009), to represent the context of a 
term or a term-pair, e.g., ?(invent, subject-of)? 
for the term ?Edison?, and ?- starring -? for the 
term-pair ?(The Terminal, Tom Hanks)?. Alt-
hough ?patterns? are utilized, we categorize them 
as DS approaches rather than PB, because they 
match the DS framework well. In this paper, PB 
only refers to the approaches that utilize patterns 
to exploit first-order co-occurrences. And the 
patterns in DS approaches are called contexts in 
the following part of this paper. 
Progress has been made and promising results 
have been reported in the past years for both DS 
and PB approaches. However, most previous re-
search work (some exceptions are discussed in 
related work) involves solely one category of ap-
proach. And there is little work studying the 
comparison of their performance for different 
types of terms (we use ?term? to represent a sin-
gle word or a phrase). 
In this paper, we make an empirical study of 
this problem, based on a large-scale, publicly 
available dataset containing 500 million web 
pages. For each approach P, we build a term-
similarity graph G(P), with vertices representing 
terms, and edges being the confidence that the 
two terms are peers. Approaches are compared 
by the quality of their corresponding term graphs. 
993
We measure the quality of a term graph by set 
expansion. Two query sets are adopted: One con-
tains 49 semantic classes of named entities and 
20220 trials (queries), collected by Pantel et al 
(2009) from Wikipedia2; and the other contains 
100 queries of five lexical categories (proper 
nouns, common nouns, verbs, adjectives, and 
adverbs), built in this paper for studying the per-
formance comparison on different term types. 
With the dataset and the query sets, we study the 
comparison of DS and PB. Key observations and 
preliminary conclusions are, 
?   DS vs. PB: DS approaches perform much 
better on common nouns, verbs, adjectives, 
and adverbs; while PB generates higher-
quality semantic classes for proper nouns. 
?   Lexical vs. Html-tag patterns: If only lexi-
cal patterns are adopted in PB, the perfor-
mance drops significantly; while the perfor-
mance only becomes slightly worse with only 
Html-tag patterns being included. 
?   Corpus-size: For proper nouns, PB beats 
DS even based on a much smaller corpus; 
similarly, for other term types, DS performs 
better even with a smaller corpus. 
Given these observations, we further study the 
feasibility of selecting appropriate approaches for 
different term types to obtain better results. A 
simple and effective frequency-based rule is pro-
posed for approach-selection. Our online seman-
tic mining system (NeedleSeek)3 adopts both PB 
and DS to build semantic classes. 
2 Related Work 
Existing efforts for semantic class mining has 
been done upon various types of data sources, 
including text-corpora, search-results, and query 
logs. In corpus-based approaches (Lin 1998; Lin 
and Pantel 2001; Pantel and Lin 2002; Pasca 
2004; Zhang et al, 2009), semantic classes are 
obtained by the offline processing of a corpus 
which can be unstructured (e.g., plain text) or 
semi-structured (e.g., web pages). Search-results-
based approaches (Etzioni et al, 2004; Kozareva 
et al, 2008; Wang and Cohen, 2008) assume that 
multiple terms (or, less often, one term) in a se-
mantic class have been provided as seeds. Other 
terms in the class are retrieved by sending queries 
                                                 
2 http://www.wikipedia.org/ 
3 http://needleseek.msra.cn/ 
(constructed according to the seeds) to a web 
search engine and mining the search results. Que-
ry logs are exploited in (Pasca 2007; Komachi 
and Suzuki, 2008; Yamaguchi 2008) for semantic 
class mining. This paper focuses on corpus-based 
approaches. 
As has been mentioned in the introduction 
part, primarily two types of methodologies are 
adopted: DS and PB. Syntactic context infor-
mation is used in (Hindle, 1990; Ruge, 1992; Lin 
1998; Lin and Pantel, 2001; Pantel and Lin, 2002) 
to compute term similarities. The construction of 
syntactic contexts requires sentences to be parsed 
by a dependency parser, which may be extremely 
time-consuming on large corpora. As an alterna-
tive, lexical context (such as text window) has 
been studied (Pantel et al, 2004; Agirre et al, 
2009; Pantel et al, 2009). In the pattern-based 
category, a lot of work has been done to discover 
term relations by sentence lexical patterns 
(Hearst 1992; Pasca 2004), HTML tag patterns 
(Shinzato and Torisawa, 2005), or both (Shi et al, 
2008; Zhang et al, 2009). In this paper, our focus 
is not one specific methodology, but the compari-
son and combination of them. 
A small amount of existing work is related to 
the comparison or combination of multiple meth-
ods. Pennacchiotti and Pantel (2009) proposed a 
feature combination framework (named ensemble 
semantic) to combine features generated by dif-
ferent extractors (distributional and ?pattern-
based?) from various data sources. As has been 
discussed in the introduction, in our terminology, 
their ?pattern-based? approaches are actually DS 
for term-pairs. In addition, their study is based on 
three semantic classes (actors, athletes, and musi-
cians), all of which are proper nouns. Differently, 
we perform the comparison by classifying terms 
according to their lexical categories, based on 
which additional insights are obtained about the 
pros and cons of each methodology. Pantel et al, 
(2004) proposed, in the scenario of extracting is-
a relations, one pattern-based approach and com-
pared it with a baseline syntactic distributional 
similarity method (called syntactic co-occurrence 
in their paper). Differently, we study the compar-
ison in a different scenario (semantic class min-
ing). In addition, they did not differentiate the 
lexical types of terms in the study. The third dif-
ference is that we proposed a rule for method-
selection while they did not. In (Pasca and Durme, 
994
2008), clusters of distributional similar terms 
were adopted to expand the labeled semantic 
classes acquired from the ?such as | including? 
pattern. Although both patterns and distributional 
similarity were used in their paper, they did not 
do any comparison about their performance. 
Agirre et al (2009) compared DS approaches 
with WordNet-based methods in computing word 
similarity and relatedness; and they also studied 
the combination of them. Differently, the meth-
ods for comparison in our paper are DS and PB. 
3 Similarity Graph Construction 
A key operation in corpus-based semantic class 
mining is to build a term similarity graph, with 
vertices representing terms, and edges being the 
similarity (or distance) between terms. Given the 
graph, a clustering algorithm can be adopted to 
generate the final semantic classes. Now we de-
scribe the state-of-the-art DS and PB approaches 
for computing term similarities. 
3.1 Distributional Similarity 
DS approaches are based on the distributional 
hypothesis (Harris, 1985), which says that terms 
appearing in analogous contexts tend to be simi-
lar. In a DS approach, a term is represented by a 
feature vector, with each feature corresponding to 
a context in which the term appears. The similari-
ty between two terms is computed as the similari-
ty between their corresponding feature vectors. 
Different approaches may have different ways of 
1) defining a context, 2) assigning feature values, 
or 3) measuring the similarity between two fea-
ture vectors. 
 
Contexts 
Text window (window size: 2, 4) 
Syntactic 
Feature value PMI 
Similarity measure Cosine, Jaccard 
Table 1. DS approaches implemented in this paper 
 
Mainly two kinds of contexts have been exten-
sively studied: syntactic context and lexical con-
text. The construction of syntactic contexts relies 
on the syntactic parsing trees of sentences, which 
are typically the output of a syntactic parser. Giv-
en a syntactic tree, a syntactic context of a term w 
can be defined as the parent (or one child) of w in 
the tree together with their relationship (Lin, 
1998; Pantel and Lin, 2002; Pantel et al, 2009). 
For instance, in the syntactic tree of sentence 
?this is an interesting read for anyone studying 
logic?, one context of the word ?logic? can be 
defined as ?study V:obj:N?. In this paper, we 
adopt Minipar (Lin, 1994) to parse sentences and 
to construct syntactic trees. 
One popular lexical context is text window, 
where a context c for a term w in a sentence S is 
defined as a substring of the sentence containing 
but removing w. For example, for sentence 
??w1w2w3ww4w5w6??, a text window context 
(with size 4) of w can be ?w2w3w4w5?. It is typi-
cally time-consuming to construct the syntactic 
trees for a large-scale dataset, even with a light-
weight syntactic parser like Minipar. The con-
struction of lexical contexts is much more effi-
cient because it does not require the syntactic 
dependency between terms. Both contexts are 
studied in this paper. 
After defining contexts for a term w, the next 
step is to construct a feature vector for the term: 
F(w)=(fw1, fw2?, fw,m), where m is the number of 
distinct contexts, and fw,c is the feature value of 
context c with respect to term w. Among all the 
existing approaches, the dominant way of assign-
ing feature values (or context values) is compu-
ting the pointwise mutual information (PMI) be-
tween the feature and the term, 
                
             
             
 (3.1) 
where F(w,c) is the frequency of context c occur-
ring for term w, F(w,*) is the total frequency of 
all contexts for term w, F(*,c) is the frequency of 
context c for all terms, and F(*,*) is the total fre-
quency of all context for all terms. They are cal-
culated as follows respectively, 
 
       ?             
       ?             
       ? ?           
 
     
(3.2) 
where m and n are respectively the distinct num-
bers of contexts and terms. 
Following state-of-the-art, we adopt PMI in 
this paper for context weighting. 
Given the feature vectors of terms, the simi-
larity of any two terms is naturally computed as 
the similarity of their corresponding feature vec-
tors. Cosine similarity and Jaccard similarity 
(weighted) are implemented in our experiments, 
         ?  ?  
?      
??   
 
  ??   
 
 
  (3.3) 
995
          ?  ?  
?           
?     ?     ?           
  (3.4) 
Jaccard similarity is finally used in presenting 
our experimental results (in Section 6), because it 
achieves higher performance. 
3.2 Pattern-based Approaches 
In PB approaches, a list of carefully-designed (or 
automatically learned) patterns is exploited and 
applied to a text collection, with the hypothesis 
that the terms extracted by applying each of the 
patterns to a specific piece of text tend to be simi-
lar. Two categories of patterns have been studied 
in the literature: sentence lexical patterns, and 
HTML tag patterns. Table-2 lists some popular 
patterns utilized in existing semantic class mining 
work (Heast 1992; Pasca 2004; Kozareva et al, 
2008; Zhang et al, 2009). In the table, ?T? means 
a term (a word or a phrase). Exactly the same set 
of patterns is employed in implementing our pat-
tern-based approaches in this paper. 
 
Type Pattern 
Lexical 
T {, T}*{,} (and|or) {other} T 
(such as | including) T (and|,|.) 
T, T, T {,T}* 
Tag 
<ul>  <li> T </li>  ?  <li> T </li>  </ul> 
<ol> <li> T </li> ?  <li> T </li> </ol> 
<select> <option> T ?<option> T </select> 
<table>  <tr> <td> T </td> ? <td> T </td> </tr> ... </table> 
Other Html-tag repeat patterns 
Table 2. Patterns employed in this paper (Lexical: 
sentence lexical patterns; Tag: HTML tag patterns) 
We call the set of terms extracted by applying 
a pattern one time as a raw semantic class 
(RASC). The term similarity graph needs to be 
built by aggregating the information of the ex-
tracted RASCs. 
One basic idea of estimating term similarity is 
to count the number of RASCs containing both of 
them. This idea is extended in the state-of-the-art 
approaches (Zhang et al, 2009) to distinguish the 
reliability of different patterns and to punish term 
similarity contributions from the same domain 
(or site), as follows, 
          ?      ?          
  
   
 
 
   
 (3.5) 
where Ci,j is a RASC containing both term a and 
term b, P(Ci,j) is the pattern via which the RASC 
is extracted, and w(P) is the weight of pattern P. 
The above formula assumes all these RASCs be-
long to m sites (or domains) with Ci,j extracted 
from a page in site i, and ki being the number of 
RASCs corresponding to site i. 
In this paper, we adopt an extension of the 
above formula which considers the frequency of 
a single term, as follows, 
 Sim*(a, b) = Sim(a, b)  ?              (3.6) 
where IDF(a)=log(1+N/N(a)), N is the total num-
ber of RASCs, and N(a) is the number of RASCs 
containing a. In the experiments, we simply set 
the weight of every pattern type to be the same 
value (1.0). 
4 Compare PB and DS 
We compare PB and DS by the quality of the 
term similarity graphs they generated. The quali-
ty of a term graph is measured by set expansion: 
Given a list of seed terms (e.g., S={lent, epipha-
ny}) belonging to a semantic class, our task is to 
find other members of this class, such as advent, 
easter, and christmas. 
In this section, we first describe our set expan-
sion algorithm adopted in our study. Then DS 
and PB are compared in terms of their set-
expansion performance. Finally we discuss ways 
of selecting appropriate approaches for different 
types of seeds to get better expansion results. 
4.1 Set Expansion Algorithm 
Having at hand the similarity graph, set expan-
sion can be implemented by selecting the terms 
most similar to the seeds. So given a query 
Q={s1, s2, ?, sk}, the key is to compute       , 
the similarity between a term t and the seed-set 
Q. Naturally, we define it as the weighted aver-
age similarity between t and every seed in Q, 
        ?             
 
     (4.1) 
where   is the weight of seed   , which can be a 
constant value, or a function of the frequency of 
term    in the corpus. Although Formula 3.6 can 
be adopted directly for calculating Sim(t,si), we 
use the following rank-based formula because it 
generate better expansion results. 
           
 
              
 (4.2) 
where         is the rank of term t among the 
neighbors of   . 
In our experiments, we fix  =1 and  =10. 
996
4.2 Compare DS with PB 
In order to have a comprehensive comparison of 
the two approaches, we intentionally choose 
terms of diverse types and do experiments based 
on various data scales. We classify terms into 5 
types by their lexical categories: proper nouns, 
common nouns, verbs, adjectives, and adverbs. 
The data scales for experiments are from one mil-
lion to 500 million web pages. Please refer to 
sections 5.1 and 5.2 for more details about the 
corpora and seeds used for experiments. 
Experimental results (refer to Section 6) will 
show that, for proper nouns, the ranking of ap-
proaches (in terms of performance) is: 
PB > PB-HtmlTag > DS  PB-Lexical 
While for common nouns, verbs, adjectives, 
and adverbs, we have: 
DS > PB 
Here ?PB-lexical? means only the lexical pat-
terns of Table 2 are adopted. Similarly, ?PB-
HtmlTag? represents the PB approach with only 
Html-tag patterns being utilized. 
Please pay attention that this paper by no 
means covers all PB or DS approaches (although 
we have tried our best to include the most popu-
lar ones). For PB, there are of course other kinds 
of patterns (e.g., patterns based on deeper linguis-
tic analysis). For DS, other types of contexts may 
exist in addition to those listed in Table 1. So in 
interpreting experimental results, making obser-
vations, and drawing preliminary conclusions, we 
only means the patterns in Table 2 for PB and 
Table 1 for DS. It will be an interesting future 
work to include more DS and PB approaches in 
the study. 
In order to understand why PB performs so 
well in dealing with proper nouns while so badly 
for other term categories, we calculated the fre-
quency of each seed term in the extracted RASCs, 
the output of the pattern-matching algorithm. We 
define the normalized frequency of a term to be 
its frequency in the RASCs divided by the fre-
quency in the sentences of the original documents 
(with duplicate sentences merged). Then we de-
fine the mean normalized frequency (MNF) of a 
seed set S, as follows, 
        
?            
   
 (4.3) 
where Fnorm(t) is the normalized frequency of t. 
The MNF values for the five seed sets are 
listed in Table 3, where we can see that proper 
nouns have the largest MNF values, followed by 
common nouns. In other words, the patterns in 
Table 2 capture the relations of more proper 
nouns than other term categories. 
 
Seed Categories Terms MNF 
Proper nouns 40 0.2333 
Common nouns 40 0.0716 
Verbs 40 0.0099 
Adjectives 40 0.0126 
Adverbs 40 0.0053 
Table 3. MNF values of different seed categories 
As mentioned in the introduction, the PB and 
DS approaches we studied capture first-order and 
second-order term co-occurrences respectively. 
Some existing work (e.g., Edmonds, 1997) 
showed that second-order co-occurrence leads to 
better results for detecting synonymy. Consider-
ing that a high proportion of coordinate terms of 
verbs, adjectives, and adverbs are their synonyms 
and antonyms, it is reasonable that DS behaves 
better for these term types because it exploits se-
cond-order co-occurrence. For PB, different from 
the standard way of dealing with first-order co-
occurrences where statistics are performed on all 
pairs of near terms, a subset of co-occurred terms 
are selected in PB by specific patterns. The pat-
terns in Table-2 help detecting coordinate proper 
nouns, because they are frequently occurred to-
gether obeying the patterns in sentences or web 
pages. But it is not the case for other term types. 
It will be interesting to study the performance of 
PB when more pattern types are added. 
4.3 Approach Selection 
Having observed that the two approaches per-
form quite differently on every type of queries 
we investigated, we hope we can improve the 
expansion performance by smartly selecting an 
approach for each query. In this section, we pro-
pose and study several approach-selection meth-
ods, by which we hope to gain some insights 
about the possibility and effectiveness of combin-
ing DS and PB for better set expansion. 
Oracle selection: In order to get an insight 
about the upper bound that we could obtain when 
combing the two methods, we implement an ora-
cle that chooses, for each query, the approach 
that generates better expansion results. 
997
Frequency-based selection: It is shown in 
Table 3 that the mean normalized frequency of 
proper nouns is much larger than other terms. 
Motivated by this observation, we select a set 
expansion methodology for each query as fol-
lows: Select PB if the normalized frequency val-
ues of all terms in the query are larger than 0.1; 
otherwise choose DS. 
We demonstrate, in Section 6.3, the effective-
ness of the above selection methods. 
5 Experimental Setup 
5.1 Dataset and Exp. Environment 
We adopt a public-available dataset in our exper-
iments: ClueWeb094. This is a very large dataset 
collected by Carnegie Mellon University in early 
2009 and has been used by several tracks of the 
Text Retrieval Conference (TREC)5. The whole 
dataset consists of 1.04 billion web pages in ten 
languages while only those in English, about 500 
million pages, are used in our experiments. The 
reason for selecting such a dataset is twofold: 
First, it is a corpus large enough for conducting 
web-scale experiments and getting meaningful 
results. Second, since it is publicly available, it is 
possible for other researchers to reproduce the 
experiments in the paper. 
 
Corpora 
Docs 
(millions) 
Sentences 
(millions) 
Description 
Clue500 500 13,000 All En pages in ClueWeb09 
Clue050  50   1,600 ClueWeb09 category B  
Clue010  10      330 Sampling from Clue050 
Clue001   1       42 Sampling from Clue050 
Table 4. Corpora used in experiments 
To test the impact of corpus size on set expan-
sion performance, four corpora are derived from 
the dataset, as outlined in Table 4. The Clue500 
corpus contains all the 500 million English web 
pages in the dataset; while Clue050 is a subset of 
ClueWeb09 (named category B) containing 50 
million English web pages. The remaining two 
corpora are respectively the 1/5 and 1/50 random 
sampling of web pages from Clue050. 
Documents in the corpora are stored and pro-
cessed in a cluster of 40 four-core machines. 
                                                 
4 http://boston.lti.cs.cmu.edu/Data/clueweb09/  
5 http://trec.nist.gov/  
5.2 Query Sets 
We perform our study using two query sets. 
WikiGold: It was collected by Pantel et al 
(2009) from the ?List of? pages in Wikipedia and 
used as the gold standard in their paper. This gold 
standard consists of 49 entity sets, and 20220 tri-
als (used as queries) of various numbers of seeds. 
Most seeds in the query set are named entities. 
Please refer to Pantel et al (2009) for details of 
the gold standard. 
Mix100: This query set consists of 100 queries 
in five categories: verbs, adjectives, adverbs, 
common nouns, and proper nouns. There are 20 
queries in every category and two seeds in every 
query. The query set was built by the following 
steps: First, 20 terms of each category were ran-
domly selected from a term list (which is con-
structed by part-of-speech tagging the Clue050 
corpus and removing low-frequency terms), and 
were treated as the first seed of the each query. 
Then, we manually added one additional seed for 
each query. The reason for utilizing two seeds 
instead of one is the observation that a large por-
tion of the terms selected in the previous step be-
long to multiple categories. For example, ?color-
ful? is both an adjective and a proper noun (a 
Japanese manga). 
5.3 Results Labeling 
No human labeling efforts are needed for the ex-
pansion results of the WikiGold query set. Every 
returned term is automatically judged to be 
?Good? (otherwise ?Bad?) if it appears in the 
corresponding gold standard entity set. 
For Mix100, the search results of various ap-
proaches are merged and labeled by three human 
labelers. Each labeler assigns each term in the 
search results a label of ?Good?, ?Fair? or ?Bad?. 
The labeling agreement values (measured by per-
centage agreement) between labelers I and II, I 
and III, II and III are respectively 0.82, 0.81, and 
0.81. The ultimate judgment of each result term 
is obtained from the three labelers by majority 
voting. In the case of three labelers giving mutu-
ally different results (i.e., one ?Good?, one ?Fair? 
and one ?Bad?), the ultimate judgment is set to 
?Fair? (the average). 
5.4 Evaluation Metrics 
After removing seeds from the expansion results, 
we adopt the following metrics to evaluate the 
998
results of each query. The evaluation score on a 
query set is the average over all the queries. 
Precision@k: The percentage of relevant 
(good or fair) terms in the top-k expansion results 
(terms labeled as ?Fair? are counted as 0.5) 
Recall@k: The ratio of relevant terms in the 
top-k results to the total number of relevant terms 
R-Precision: Precision@R where R is the total 
number of terms labeled as ?Good? 
Mean average precision (MAP): The average 
of precision values at the positions of all good or 
fair results 
6 Experimental Results 
6.1 Overall Performance Comparison 
Table 5 lists the performance (measured by 
MAP, R-precision, and the precisions at ranks 25, 
50, and 100) of some key approaches on corpus 
Clue050 and query set WikiGold. The results of 
query set Mix100 are shown in Table 6. In the 
results, TWn represents the DS approach with 
text-window of size n as contexts, Syntactic is the 
DS approach with syntactic contexts, PB-Lexical 
means only the lexical patterns of Table 2 are 
adopted, and PB-HtmlTag represents the PB ap-
proach with only Html-tag patterns utilized. 
 
Approach MAP R-Prec P@25 P@50 P@100 
TW2 0.218 0.287 0.359 0.278 0.204 
TW4 0.152 0.210 0.325 0.244 0.173 
Syntactic 0.170 0.247 0.314 0.242 0.178 
PB-Lexical 0.227 0.276 0.352 0.272 0.190 
PB-HtmlTag 0.354 0.417 0.513 0.413 0.311 
PB 0.362 0.424 0.520 0.418 0.314 
Pantel-24M N/A 0.264 0.353 0.298 0.239 
Pantel-120M N/A 0.356 0.377 0.319 0.250 
Pantel-600M N/A 0.404 0.407 0.347 0.278 
Table 5. Performance comparison on the Clue050 cor-
pus (query set: WikiGold) 
It is shown that PB gets much higher evalua-
tion scores than other approaches on the WikiG-
old query set and the proper-nouns category of 
Mix100. While for other seed categories in 
Mix100, TW2 return significantly better results. 
We noticed that most seeds in WikiGold are 
proper nouns. So the experimental results tend to 
indicate that the performance comparison be-
tween state-of-the-art DS and PB approaches de-
pends on the types of terms to be mined, specifi-
cally, DS approaches perform better in mining 
semantic classes of common nouns, verbs, adjec-
tives, and adverbs; while state-of-the-art PB ap-
proaches are more suitable for mining semantic 
classes of proper nouns. The performance of PB 
is low in dealing with other types of terms (espe-
cially adverbs). The performance of PB drops 
significantly if only lexical patterns are used; and 
the HtmlTag-only version of PB performs only 
slightly worse than PB. 
The observations are verified by the precision-
recall graph in Figure 1 on Clue500. The results 
of the syntactic approach on Clue500 are not in-
cluded, because it is too time-consuming to parse 
all the 500 million web pages by a dependency 
parser (even using a high-performance parser like 
Minipar). It took overall about 12,000 CPU-hours 
to parse all the sentences in Clue050 by Minipar. 
 
Query types & 
Approaches 
MAP P@5 P@10 P@20 
Proper 
Nouns 
TW2 0.302 0.835 0.810 0.758 
PB 0.336 0.920 0.838 0.813 
Common 
Nouns 
TW2 0.384 0.735 0.668 0.595 
PB 0.212 0.640 0.548 0.485 
Verbs 
TW2 0.273 0.655 0.543 0.465 
PB 0.176 0.415 0.373 0.305 
Adjectives 
TW2 0.350 0.655 0.563 0.473 
PB 0.120 0.335 0.285 0.234 
Adverbs 
TW2 0.432 0.605 0.505 0.454 
PB 0.043 0.100 0.095 0.089 
Table 6. Performance comparison on different query 
types (Corpus: Clue050; query set: Mix100) 
 
Figure 1. Precision and recall of various approaches 
(query set: WikiGold) 
The methods labeled Pantel-24M etc. (in Table 
5 and Figure 1) are the approaches presented in 
(Pantel et al, 2009) on their corpus (called 
Web04, Web20, and Web100 in the paper) con-
taining respectively 24 million, 120 million, and 
600 million web pages. Please pay attention that 
their results and ours may not be directly compa-
rable, because different corpora and set-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
R
e
c
a
ll
 
Precision 
TW221 (Clue500) PB (Clue500)
PB (Clue010) PB (Clue001)
Pantel-600M Pantel-120M
999
expansion algorithms were used. Their results are 
listed here for reference purpose only. 
6.2 Corpus Size Effect 
Table 7 shows the performance (measured by 
MAP) of two approaches on query set Mix100, 
by varying corpus size. We observed that the per-
formance of TW2 improves rapidly along with 
the growth of corpus size from one million to 50 
million documents. From Clue050 to Clue500, 
the performance is slightly improved. 
 
Query types & 
Approaches 
Clue001 Clue010 Clue050 Clue500 
Proper 
Nouns 
TW2 0.209 0.265 0.302 0.311 
PB 0.355 0.351 0.336 0.327 
Common 
Nouns 
TW2 0.259 0.348 0.384 0.393 
PB 0.200 0.234 0.212 0.205 
Verbs 
TW2 0.224 0.268 0.273 0.278 
PB 0.101 0.134 0.176 0.148 
Adjectives 
TW2 0.309 0.326 0.350 0.353 
PB 0.077 0.158 0.120 0.129 
Adverbs 
TW2 0.413 0.423 0.432 0.437 
PB 0.028 0.058 0.043 0.059 
Table 7. Effect of different corpus size (query set: 
Mix100; metric: MAP) 
For PB, however, the performance change is 
not that simple. For proper nouns, the best per-
formance (in terms of MAP) is got on the two 
small corpora Clue001 and Clue010; and the 
score does not increase when corpus size grows. 
Different observations are made on WikiGold 
(see Figure 1), where the performance improves a 
lot with the data growth from Clue001 to 
Clue010, and then stabilizes (from Clue010 to 
Clue500). For other term types, the MAP scores 
do not grow much after Clue010. To our current 
understanding, the reason may be due to the two-
fold effect of incorporating more data in mining: 
bringing useful information as well as noise. 
Clue001 contains enough information, which is 
fully exploited by the PB approach, for expand-
ing the proper-nouns in Mix100. So the perfor-
mance of PB on Clue001 is excellent. The named 
entities in WikiGold are relatively rare, which 
requires a larger corpus (Clue010) for extracting 
peer terms from. But when the corpus gets larger, 
we may not be able to get more useful infor-
mation to further improve results quality. 
Another interesting observation is that, for 
proper nouns, the performance of PB on Clue001 
is even much better than that of TW2 on corpus 
Clue500. Similarly, for other query types (com-
mon nous, verbs, adjectives, and adverbs), TW2 
easily beats PB even with a much smaller corpus. 
6.3 Approach Selection 
Here we demonstrate the experimental results of 
combining DS and PB with the methods we pro-
posed in Section 4.3. Table 8 shows the combina-
tion of PB and TW2 on corpus Clue050 and que-
ry set Mix100. The overall performance relies on 
the number (or percentage) of queries in each 
category. Two ways of mixing the queries are 
tested: avg(4:1:1:1:1) and avg(1:1:1:1:1), where 
the numbers are the proportion of proper nouns, 
common nouns, verbs, adjectives, and adverbs. 
 
Approach 
Avg (1:1:1:1:1) Avg (4:1:1:1:1) 
P@5 P@10 P@20 P@5 P@10 P@20 
TW2 0.697 0.618 0.548 0.749 0.690 0.627 
PB 0.482 0.428 0.385 0.646 0.581 0.545 
Oracle 0.759 0.663 0.591 0.836 0.759 0.695 
Freq-based 0.721 0.633 0.570 0.799 0.723 0.671 
Table 8. Experiments of combining both approaches 
(Corpus: Clue050; query set: Mix100) 
The expansion performance is improved a lot 
with our frequency-based combination method. 
As expected, oracle selection achieves great per-
formance improvement, which shows the large 
potential of combining DS and PB. Similar re-
sults (omitted due to space limitations) are ob-
served on the other corpora. 
Our online semantic mining system (Needle-
Seek, http://needleseek.msra.cn) adopts both PB 
and DS for semantic class construction. 
7 Conclusion 
We compared two mainstream methods (DS and 
PB) for semantic class mining, based on a dataset 
of 500 million pages and using five term types. 
We showed that PB is clearly adept at extracting 
semantic classes of proper nouns; while DS is 
relatively good at dealing with other types of 
terms. In addition, a small corpus is sufficient for 
each approach to generate better semantic classes 
of its ?favorite? term types than those obtained 
by its counterpart on a much larger corpus. Final-
ly, we tried a frequency-based method of com-
bining them and saw apparent performance im-
provement. 
 
1000
References  
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana 
Kravalova, Marius Pasca, Aitor Soroa. A Study on 
Similarity and Relatedness Using Distributional 
and WordNet-based Approaches. NAACL-HLT 
2009. 
Philip Edmonds. 1997. Choosing the Word most Typ-
ical in Context Using a Lexical Co-Occurrence 
Network. ACL'97, pages 507-509. 
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen 
Soderland, Daniel Weld, and Alexander Yates. 
2004. Web-Scale Information Extraction in 
KnowItAll. WWW?04, pages 100?110, New York. 
Zelig S. Harris. 1985. Distributional Structure. The 
Philosophy of Linguistics. New York: Oxford Uni-
versity Press. 
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. COLING?92, 
Nantes, France. 
Donald Hindle. 1990. Noun Classification from Predi-
cate-Argument Structures. In ACL?90, pages 268?
275, Pittsburg, Pennsylvania, June. 
Mamoru Komachi and Hisami Suzuki. Minimally Su-
pervised Learning of Semantic Knowledge from 
Query Logs. IJCNLP 2008, pages 358?365, 2008. 
Zornitsa Kozareva, Ellen Riloff, Eduard Hovy. 2008. 
Semantic Class Learning from the Web with Hypo-
nym Pattern Linkage Graphs. ACL?08: HLT. 
Dekang Lin. 1994. Principar - an Efficient, Broad-
Coverage, Principle-based Parser. COLING?94, pp. 
482-488. 
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING-ACL?98, pages 
768-774. 
Dekang Lin and Patrick Pantel. 2001. Induction of 
Semantic Classes from Natural Language Text. 
SIGKDD?01, pages 317-322. 
Hiroaki Ohshima, Satoshi Oyama and Katsumi 
Tanaka. 2006. Searching Coordinate Terms with 
their Context from the Web. WISE?06. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. 
EMNLP?09. Singapore. 
Patrick Pantel and Dekang Lin. 2002. Discovering 
Word Senses from Text. SIGKDD'02. 
Patric Pantel, Deepak Ravichandran, and Eduard 
Hovy. 2004. Towards Terascale Knowledge Acqui-
sition. COLING?04, Geneva, Switzerland. 
Marius Pasca. 2004. Acquisition of Categorized 
Named Entities for Web Search. CIKM?04. 
Marius Pasca. 2007. Weakly-Supervised Discovery of 
Named Entities Using Web Search Queries. 
CIKM?07. pp. 683-690. 
Marius Pasca and Benjamin Van Durme. 2008. Weak-
ly-supervised Acquisition of Open-Domain Classes 
and Class Attributes from Web Documents and 
Query Logs. ACL?08. 
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei 
Lifchits, and Alpa Jain. 2006. Organizing and 
Searching the World Wide Web of Facts - Step 
One: The One-Million Fact Extraction Challenge. 
In AAAI?06. 
Marco Pennacchiotti and Patrick Pantel. 2009. Entity 
Extraction via Ensemble Semantics.  EMNLP?09. 
Gerda Ruge. 1992. Experiments on Linguistically-
Based Term Associations. Information Processing 
& Management, 28(3): 317-32. 
Keiji Shinzato and Kentaro Torisawa. 2005. A Simple 
WWW-based Method for Semantic Word Class 
Acquisition. Recent Advances in Natural Language 
Processing (RANLP?05), Borovets, Bulgaria. 
Shuming Shi, Xiaokang Liu, Ji-Rong Wen. 2008. Pat-
tern-based Semantic Class Discovery with Multi-
Membership Support. CIKM?08, Napa Valley, Cali-
fornia, USA. 
Hristo Tanev and Bernardo Magnini. 2006. Weakly 
Supervised Approaches for Ontology Population. 
EACL'2006, Trento, Italy. 
Richard C. Wang and William W. Cohen. 2008. Itera-
tive Set Expansion of Named Entities Using the 
Web. ICDM?08, pages 1091?1096. 
Masashi Yamaguchi, Hiroaki Ohshima, Satoshi Oya-
ma, and Katsumi Tanaka. Unsupervised Discovery 
of Coordinate Terms for Multiple Aspects from 
Search Engine Query Logs. The 2008 
IEEE/WIC/ACM International Conference on Web 
Intelligence and Intelligent Agent Technology. 
Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-
Rong Wen. 2009. Employing Topic Models for 
Pattern-based Semantic Class Discovery. ACL?09, 
Singapore. 
 
1001
