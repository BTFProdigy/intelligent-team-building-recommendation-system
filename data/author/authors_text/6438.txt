Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 619?626,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic parsing with Structured SVM Ensemble Classification Models
Le-Minh Nguyen, Akira Shimazu, and Xuan-Hieu Phan
Japan Advanced Institute of Science and Technology (JAIST)
Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan
{nguyenml,shimazu,hieuxuan}@jaist.ac.jp
Abstract
We present a learning framework for struc-
tured support vector models in which
boosting and bagging methods are used to
construct ensemble models. We also pro-
pose a selection method which is based on
a switching model among a set of outputs
of individual classifiers when dealing with
natural language parsing problems. The
switching model uses subtrees mined from
the corpus and a boosting-based algorithm
to select the most appropriate output. The
application of the proposed framework on
the domain of semantic parsing shows ad-
vantages in comparison with the original
large margin methods.
1 Introduction
Natural language semantic parsing is an interest-
ing problem in NLP (Manning and Schutze, 1999)
as it would very likely be part of any interesting
NLP applications (Allen, 1995). For example, the
necessary of semantic parsing for most of NLP ap-
plication and the ability to map natural language to
a formal query or command language is critical for
developing more user-friendly interfaces.
Recent approaches have focused on using struc-
tured prediction for dealing with syntactic parsing
(B. Taskar et. al., 2004) and text chunking prob-
lems (Lafferty et al 2001). For semantic pars-
ing, Zettlemoyer and Collins (2005) proposed a
method for mapping a NL sentence to its logical
form by structured classification using a log-linear
model that represents a distribution over syntac-
tic and semantic analyses conditioned on the in-
put sentence. Taskar et al(B. Taskar et. al.,
2004) present a discriminative approach to pars-
ing inspired by the large-margin criterion under-
lying support vector machines in which the loss
function is factorized analogous to the decoding
process. Tsochantaridis et al(Tsochantaridis et
al., 2004) propose a large-margin models based on
SVMs for structured prediction (SSVM) in gen-
eral and apply it for syntactic parsing problem so
that the models can adapt to overlap features, ker-
nels, and any loss functions.
Following the successes of the SSVM algorithm
to structured prediction, in this paper we exploit
the use of SSVM to the semantic parsing problem
by modifying the loss function, feature representa-
tion, maximization algorithm in the original algo-
rithm for structured outputs (Tsochantaridis et al,
2004).
Beside that, forming committees or ensembles
of learned systems is known to improve accuracy
and bagging and boosting are two popular ensem-
ble methods that typically achieve better accuracy
than a single classifier (Dietterich, 2000). This
leads to employing ensemble learning models for
SSVM is worth to investigate. The first problem of
forming an ensemble learning for semantic pars-
ing is how to obtain individual parsers with re-
spect to the fact that each individual parser per-
forms well enough as well as they make different
types of errors. The second one is that of com-
bining outputs from individual semantic parsers.
The natural way is to use the majority voting strat-
egy that the semantic tree with highest frequency
among the outputs obtained by individual parsers
is selected. However, it is not sure that the ma-
jority voting technique is effective for combining
complex outputs such as a logical form structure.
Thus, a better combination method for semantic
tree output should be investigated.
To deal with these problems, we proposed an
619
ensemble method which consists of learning and
averaging phases in which the learning phases are
either a boosting or a bagging model, and the av-
eraging phase is based on a switching method on
outputs obtained from all individual SSVMs. For
the averaging phase, the switching model is used
subtrees mined from the corpus and a boosting-
based algorithm to select the most appropriate out-
put.
Applications of SSVM ensemble in the seman-
tic parsing problem show that the proposed SSVM
ensemble is better than the SSVM in term of the F-
measure score and accuracy measurements.
The rest of this paper are organized as follows:
Section 2 gives some background about the struc-
tured support vector machine model for structured
predictions and related works. Section 3 proposes
our ensemble method for structured SVMs on the
semantic parsing problem. Section 4 shows exper-
imental results and Section 5 discusses the advan-
tage of our methods and describes future works.
2 Backgrounds
2.1 Related Works
Zelle and Mooney initially proposed the empir-
ically based method using a corpus of NL sen-
tences and their formal representation for learn-
ing by inductive logic programming (Zelle, 1996).
Several extensions for mapping a NL sentence to
its logical form have been addressed by (Tang,
2003). Transforming a natural language sentence
to a logical form was formulated as the task of de-
termining a sequence of actions that would trans-
form the given input sentence to a logical form
(Tang, 2003). The main problem is how to learn a
set of rules from the corpus using the ILP method.
The advantage of the ILP method is that we do not
need to design features for learning a set of rules
from corpus. The disadvantage is that it is quite
complex and slow to acquire parsers for mapping
sentences to logical forms. Kate et alpresented
a method (Kate et al, 2005) that used transfor-
mation rules to transform NL sentences to logi-
cal forms. Those transformation rules are learnt
using the corpus of sentences and their logical
forms. This method is much simpler than the ILP
method, while it can achieve comparable result on
the CLANG (Coach Language) and Query corpus.
The transformation based method has the condi-
tion that the formal language should be in the form
of LR grammar.
Ge and Mooney also presented a statistical
method (Ge and Mooney, 2005) by merging syn-
tactic and semantic information. Their method
relaxed the condition in (Kate et al, 2005) and
achieved a state-of the art performance on the
CLANG and query database corpus. However the
distinction of this method in comparison with the
method presented in (Kate et al, 2005) is that Ge
and Mooney require training data to have SAPTs,
while the transforation based method only needs
the LR grammar for the formal language.
The work proposed by (Zettlemoyer and
Collins, 2005) that maps a NL sentence to its log-
ical form by structured classification, using a log-
linear model that represents a distribution over
syntactic and semantic analyses conditioned on
the input sentence. This work is quite similar to
our work in considering the structured classifica-
tion problem. The difference is that we used the
kernel based method instead of a log-linear model
in order to utilize the advantage of handling a very
large number of features by maximizing the mar-
gin in the learning process.
2.2 Structured Support Vector Models
Structured classification is the problem of predict-
ing y from x in the case where y has a meaningful
internal structure. Elements y ? Y may be, for in-
stance, sequences, strings, labelled trees, lattices,
or graphs.
The approach we pursue is to learn a dis-
criminant function F : X ? Y ? R over <
input, output > pairs from which we can derive
a prediction by maximizing F over the response
variable for a specific given input x. Hence, the
general form of our hypotheses f is
f(x;w) = argmax
y?Y
F (x; y;w)
where w denotes a parameter vector.
As the principle of the maximum-margin pre-
sented in (Vapnik, 1998), in the structured clas-
sification problem, (Tsochantaridis et al, 2004)
proposed several maximum-margin optimization
problems.
For convenience, we define
??i(y) ? ?(xi, yi)? ?(xi, y)
where (xi,yi) is the training data.
The hard-margin optimization problem is:
SVM0 : minw
1
2
?w?2 (1)
620
?i,?y ? Y \yi : ?w, ??i(y)? > 0 (2)
where ?w, ??i(y)? is the linear combination of
feature representation for input and output.
The soft-margin criterion was proposed
(Tsochantaridis et al, 2004) in order to allow
errors in the training set, by introducing slack
variables.
SVM1 : min
1
2
?w?2 +
C
n
n?
i=1
?i,s.t.?i, ?i ? 0
(3)
?i, ?y ? Y \yi : ?w, ??i(y)? ? 1? ?i (4)
Alternatively, using a quadratic term C2n
?
i
?2i to
penalize margin violations, we obtained SVM2.
Here C > 0 is a constant that control the trade-
off between training error minimization and mar-
gin maximization.
To deal with problems in which |Y | is very
large, such as semantic parsing, (Tsochantaridis et
al., 2004) proposed two approaches that general-
ize the formulation SVM0 and SVM1 to the cases
of arbitrary loss function. The first approach is to
re-scale the slack variables according to the loss
incurred in each of the linear constraints.
SVM?s : min????
w,?
1
2
?w?2 +
C
n
n?
i=1
?i,s.t.?i, ?i ? 0
(5)
?i,?y ? Y \yi : ?w, ??i(y)? ?
1? ?i
?(yi, y)
(6)
The second approach to include loss function is to
re-scale the margin as a special case of the Ham-
ming loss. The margin constraints in this setting
take the following form:
?i,?y ? Y \yi : ?w, ??i(y)? ? ?(yi, y)? ?i (7)
This set of constraints yields an optimization prob-
lem, namely SVM?m1 .
2.3 Support Vector Machine Learning
The support vector learning algorithm aims to find
a small set of active constraints that ensures a suf-
ficiently accurate solution. The detailed algorithm,
as presented in (Tsochantaridis et al, 2004) can be
applied to all SVM formulations mentioned above.
The only difference between them is the cost func-
tion in the following optimization problems:
SVM?s1 : H(y) ? (1? ???i(y), w?)?(yi, y)
SVM?s2 : H(y) ? (1? ???i(y), w?)
?
?(yi, y)
SVM?m1 : H(y) ? (?(yi, y)? ???i(y), w?)
SVM?m2 : H(y) ? (
?
?(yi, y)? ???i(y), w?)
Typically, the way to apply structured SVM is to
implement feature mapping ?(x, y), the loss func-
tion ?(yi, y), as well as the maximization algo-
rithm. In the following section, we apply a struc-
tured support vector machine to the problem of se-
mantic parsing in which the mapping function, the
maximization algorithm, and the loss function are
introduced.
3 SSVM Ensemble for Semantic Parsing
Although the bagging and boosting techniques
have known to be effective for improving the
performance of syntactic parsing (Henderson and
Brill, 2000), in this section we focus on our en-
semble learning of SSVM for semantic parsing
and propose a new effective switching model for
either bagging or boosting model.
3.1 SSVM for Semantic Parsing
As discussed in (Tsochantaridis et al, 2004), the
major problem for using the SSVM is to imple-
ment the feature mapping ?(x, y), the loss func-
tion ?(yi, y), as well as the maximization algo-
rithm. For semantic parsing, we describe here
the method of structure representation, the feature
mapping, the loss function, and the maximization
algorithm.
3.1.1 Structure representation
A tree structure representation incorporated
with semantic and syntactic information is named
semantically augmented parse tree (SAPT) (Ge
and Mooney, 2005). As defined in (Ge and
Mooney, 2005), in an SAPT, each internal node in
the parse tree is annotated with a semantic label.
Figure 1 shows the SAPT for a simple sentence in
the CLANG domain. The semantic labels which
are shown after dashes are concepts in the domain.
Some concepts refer to predicates and take an or-
dered list of arguments. Concepts such as ?team?
and ?unum? might not have arguments. A special
semantic label, ?null?, is used for a node that does
not correspond to any concept in the domain.
3.1.2 Feature mapping
For semantic parsing, we can choose a mapping
function to get a model that is isomorphic to a
probabilistic grammar in which each rule within
the grammar consists of both a syntactic rule and
a semantic rule. Each node in a parse tree y for a
sentence x corresponds to a grammar rule gj with
a score wj .
621
Figure 1: An Example of tree representation in
SAPT
All valid parse trees y for a sentence x are
scored by the sum of the wj of their nodes, and the
feature mapping ?(x, y) is a history gram vector
counting how often each grammar rule gj occurs
in the tree y. Note that the grammar rules are lex-
icalized. The example shown in Figure 2 clearly
explains the way features are mapped from an in-
put sentence and a tree structure.
3.1.3 Loss function
Let z and zi be two semantic tree outputs and
|zi| and |zi| be the number of brackets in z and
zi, respectively. Let n be the number of common
brackets in the two trees. The loss function be-
tween zi and z is computed as bellow.
F ? loss(zi, z) = 1?
2? n
|zi|+ |z|
(8)
zero? one(zi, z) =
{
1 if zi 6= z
0 otherwise
(9)
3.1.4 Maximization algorithm
Note that the learning function can be efficiently
computed by finding a structure y ? Y that max-
imizes F (x, y;w)=?w, ??i(y)? via a maximiza-
tion algorithm. Typically we used a variant of
Figure 2: Example of feature mapping using tree
representation
CYK maximization algorithm which is similar to
the one for the syntactic parsing problem (John-
son,1999). There are two phases in our maximiza-
tion algorithm for semantic parsing. The first is
to use a variant of CYK algorithm to generate a
SAPT tree. The second phase then applies a deter-
ministic algorithm to output a logical form. The
score of the maximization algorithm is the same
with the obtained value of the CYK algorithm.
The procedure of generating a logical form us-
ing a SAPT structure originally proposed by (Ge
and Mooney, 2005) and it is expressed as Algo-
rithm 1. It generates a logical form based on a
knowledge database K for given input node N .
The predicate argument knowledge, K, specifies,
for each predicate, the semantic constraints on its
arguments. Constraints are specified in terms of
the concepts that can fill each argument, such as
player(team, unum) and bowner(player).
The GETsemanticHEAD determines which of
node?s children is its semantic head based on they
having matching semantic labels. For example, in
Figure 1N3 is determined to be the semantic head
of the sentence since it matches N8?s semantic la-
bel. ComposeMR assigns their meaning represen-
tation (MR) to fill the arguments in the head?s MR
to construct the complete MR for the node. Figure
1 shows an example of using BuildMR to generate
a semantic tree to a logical form.
622
Input: The root node N of a SAPT
Predicate knowledge K
Notation: XMR is the MR of node X
Output: NMR
Begin
Ci= the ith child node of N
Ch= GETsemanticHEAD(N )
ChMR =BuildMR(Ch,K)
for each other child Ci where i 6= h do
CiMR =BuildMR(Ci,K)
ComposeMR(ChMR ,CiMR ,K)
end
NMR=ChMR
End
Algorithm 1: BuildMR(N,K): Computing a logical
form form an SAPT(Ge and Mooney, 2005)
Input: S = (xi; yi; zi), i = 1, 2, ..., l in which xi is1
the sentence and yi, zi is the pair of tree structure and
its logical form
Output: SSVM model2
repeat3
for i = 1 to n do4
5
SVM?s1 : H(y, z) ? (1? ???i(y), w?)?(zi, z)
SVM?s2 : H(y, z) ? (1? ???i(y), w?)
?
?(zi, z)
SVM?m1 : H(y, z) ? (?(zi, z)? ???i(y), w?)
SVM?m2 : H(y, z) ? (
?
?(zi, z)? ???i(y), w?)
compute < y?, z? >= argmaxy,z?Y,Z H(Y,Z);6
compute ?i = max{0,maxy,z?Si H(y, z)};7
if H(y?, z?) > ?i + ? then8
Si ? Si ? y?, z?;9
solving optimization with SVM;10
end11
end12
until no Si has changed during iteration;13
Algorithm 2: Algorithm of SSVM learning for se-
mantic parsing. The algorithm is based on the original
algorithm(Tsochantaridis et al, 2004)
3.1.5 SSVM learning for semantic parsing
As mentioned above, the proposed maximiza-
tion algorithm includes two parts: the first is to
parse the given input sentence to the SAPT tree
and the second part (BuildMR) is to convert the
SAPT tree to a logical form. Here, the score
of maximization algorithm is the same with the
score to generate a SAPT tree and the loss function
should be the measurement based on two logical
form outputs. Algorithm 2 shows our generation
of SSVM learning for the semantic parsing prob-
lem which the loss function is based on the score
of two logical form outputs.
3.2 SSVM Ensemble for semantic parsing
The structured SVM ensemble consists of a train-
ing and a testing phase. In the training phase, each
individual SSVM is trained independently by its
own replicated training data set via a bootstrap
method. In the testing phase, a test example is ap-
plied to all SSVMs simultaneously and a collec-
tive decision is obtained based on an aggregation
strategy.
3.2.1 Bagging for semantic parsing
The bagging method (Breiman, 1996) is sim-
ply created K bootstrap with sampling m items
from the training data of sentences and their logi-
cal forms with replacement. We then applied the
SSVM learning in the K generated training data
to create K semantic parser. In the testing phase,
a given input sentence is parsed by K semantic
parsers and their outputs are applied a switching
model to obtain an output for the SSVM ensemble
parser.
3.2.2 Boosting for semantic parsing
The representative boosting algorithm is the
AdaBoost algorithm (Schapire, 1999). Each
SSVM is trained using a different training set.
Assuming that we have a training set TR =
(xi; yi)|i = 1, 2, ..., l consisting of l samples and
each sample in the TR is assigned to have the
same value of weight p0(xi) = 1/l. For training
the kth SSVM classifier, we build a set of training
samples
TRboostk = (xi; yi)|i = 1, 2, .., l
? that is ob-
tained by selecting l?(< l) samples among the
whole data set TR according to the weight value
pk?1(xi) at the (k-1)th iteration. This training
samples is used for training the kth SSVM clas-
sifier. Then, we obtained the updated weight val-
ues pk(xi) of the training samples as follows. The
weight values of the incorrectly classified sam-
ples are increased but the weight values of the
correctly classified samples are decreased. This
shows that the samples which are hard to clas-
sify are selected more frequently. These updated
weight values will be used for building the train-
ing samples TRboostk+1 = (xi; yi)|i = 1, 2, ..., l?
of the (k+1)th SSVM classifier. The sampling pro-
cedure will be repeated until K training samples
set has been built for the Kth SSVM classifier.
623
3.2.3 The proposed SSVM ensemble model
We construct a SSVM ensemble model by using
different parameters for each individual SSVM to-
gether with bagging and boosting models. The pa-
rameters we used here including the kernel func-
tion and the loss function as well as features used
in a SSVM. Let N and K be the number of dif-
ferent parameters and individual semantic parsers
in a SSVM ensemble, respectively. The motiva-
tion is to create individual parsers with respect to
the fact that each individual parser performs well
enough as well as they make different types of
errors. We firstly create N ensemble models us-
ing either boosting or bagging models to obtain
N?K individual parsers. We then select the top T
parsers so that their errors on the training data are
minimized and in different types. After forming
an ensemble model of SSVMs, we need a process
for aggregating outputs of individual SSVM clas-
sifiers. Intuitively, a simplest way is to use a vot-
ing method to select the output of a SSVM ensem-
ble. Instead, we propose a switching method using
subtrees mining from the set of trees as follows.
Let t1, t2, ..., tK be a set of candidate parse trees
produced by an ensemble of K parsers. From the
set of tree t1, t2, ..., tK we generated a set of train-
ing data that maps a tree to a label +1 or -1, where
the tree tj received the label +1 if it is an corrected
output. Otherwise tj received the label -1. We
need to define a learning function for classifying a
tree structure to two labels +1 and -1.
For this problem, we can apply a boosting tech-
nique presented in (Kudo and Matsumoto, 2004).
The method is based on a generation of Adaboost
(Schapire, 1999) in which subtrees mined from the
training data are severed as weak decision stump
functions.
The technique for mining these subtrees is pre-
sented in (Zaki, 2002) which is an efficient method
for mining a large corpus of trees. Table 1 shows
an example of mining subtrees on our corpus. One
Table 1: Subtrees mined from the corpus
Frequency Subtree
20 (and(bowner)(bpos))
4 (and(bowner)(bpos(right)))
4 (bpos(circle(pt(playerour11))))
15 (and(bpos)(not(bpos)))
8 (and(bpos(penalty-areaour)))
problem for using the boosting subtrees algorithm
(BT) in our switching models is that we might ob-
tain several outputs with label +1. To solve this,
we evaluate a score for each value +1 obtained by
the BT and select the output with the highest score.
In the case of there is no tree output received the
value +1, the output of the first individual semantic
parser will be the value of our switching model.
4 Experimental Results
For the purpose of testing our SSVM ensem-
bles on semantic parsing, we used the CLANG
corpus which is the RoboCup Coach Language
(www.robocup.org). In the Coach Competition,
teams of agents compete on a simulated soccer
field and receive advice from a team coach in
a formal language. The CLANG consists of 37
non-terminal and 133 productions; the corpus for
CLANG includes 300 sentences and their struc-
tured representation in SAPT (Kate et al, 2005),
then the logical form representations were built
from the trees. Table 2 shows the statistic on the
CLANG corpus.
Table 2: Statistics on CLANG corpus. The average length
of an NL sentence in the CLANG corpus is 22.52 words. This
indicates that CLANG is the hard corpus. The average length
of the MRs is also large in the CLANG corpus.
Statistic CLANG
No.of. Examples 300
Avg. NL sentence length 22.5
Avg. MR length (tokens) 13.42
No. of non-terminals 16
No. of productions 102
No. of unique NL tokens 337
Table 3: Training accuracy on CLANG corpus
Parameter Training Accuracy
linear+F-loss(?s) 83.9%
polynomial(d=2)+F-loss (?m) 90.1%
polynomial(d=2)+F-loss(?s) 98.8%
polynomial(d=2)+F-loss(?m) 90.2%
RBF+F-loss(?s) 86.3%
To create an ensemble learning with SSVM, we
used the following parameters with the linear ker-
nel, the polynomial kernel, and RBF kernel, re-
spectively. Table 3 shows that they obtained dif-
ferent accuracies on the training corpus, and their
accuracies are good enough to form a SSVM en-
semble. The parameters in Table 3 is used to form
our proposed SSVM model.
The following is the performance of the
SSVM1, the boosting model, the bagging model,
and the models with different parameters on the
1The SSVM is obtained via http://svmlight.joachims.org/
624
CLANG corpus2. Note that the numbers of in-
dividual SSVMs in our ensemble models are set
to 10 for boosting and bagging, and each individ-
ual SSVM can be used the zero-one and F1 loss
function. In addition, we also compare the per-
formance of the proposed ensemble SSVM mod-
els and the conventional ensemble models to as-
sert that our models are more effective in forming
SSVM ensemble learning.
We used the standard 10-fold cross validation
test for evaluating the methods. To train a BT
model for the switching phase in each fold test,
we separated the training data into 10-folds. We
keep 9/10 for forming a SSVM ensemble, and
1/10 for producing training data for the switch-
ing model. In addition, we mined a subset of
subtrees in which a frequency of each subtree is
greater than 2, and used them as weak functions
for the boosting tree model. Note that in testing
the whole training data in each fold is formed a
SSVM ensemble model to use the BT model esti-
mated above for selecting outputs obtained by the
SSVM ensemble.
To evaluate the proposed methods in parsing NL
sentences to logical form, we measured the num-
ber of test sentences that produced complete log-
ical forms, and the number of those logical forms
that were correct. For CLANG, a logical form is
correct if it exactly matches the correct representa-
tion, up to reordering of the arguments of commu-
tative operators. We used the evaluation method
presented in (Kate et al, 2005) as the formula be-
low.
precision = #correct?representation#completed?representation
recall = #correct?representation#sentences
Table 4 shows the results of SSVM, the SCSIS-
SOR system (Ge and Mooney, 2005), and the SILT
system (Kate et al, 2005) on the CLANG corpus,
respectively. It shows that SCSISSOR obtained
approximately 89% precision and 72.3% recall
while on the same corpus our best single SSVM
method 3 achieved a recall (74.3%) and lower pre-
cision (84.2%). The SILT system achieved ap-
proximately 83.9% precision and 51.3% recall 4
which is lower than the best single SSVM.
2We set N to 5 and K to 6 for the proposed SSVM.
3The parameter for SSVM is the polynomial(d=2)+(?s)
4Those figures for precision and recall described in
(Kate et al, 2005) showed approximately this precision and
recall of their method in this paper
Table 4: Experiment results with CLANG corpus. Each
SSVM ensemble consists of 10 individual SSVM. SSVM
bagging and SSVM boosting used the voting method. P-
SSVM boosting and P-SSVM bagging used the switching
method (BT) and voting method (VT).
System Methods Precision Recall
1 SSVM 84.2% 74.3%
1 SCSISSOR 89.0% 72.3%
1 SILT 83.9% 51.3%
10 SSVM Bagging 85.7% 72.4%
10 SSVM Boosting 85.7% 72.4%
10 P-SSVM Boosting(BT) 88.4% 79.3%
10 P-SSVM Bagging(BT) 86.5% 79.3%
10 P-SSVM Boosting(VT) 86.5% 75.8%
10 P-SSVM Bagging(VT) 84.6% 75.8%
Table 4 also shows the performance of Bagging,
Boosting, and the proposed SSVM ensemble mod-
els with bagging and boosting models. It is impor-
tant to note that the switching model using a boost-
ing tree method (BT) to learn the outputs of indi-
vidual SSVMs within the SSVM ensemble model.
It clearly indicates that our proposed ensem-
ble method can enhance the performance of the
SSVM model and the proposed methods are more
effective than the conventional ensemble method
for SSVM. This was because the output of each
SSVM is complex (i.e a logical form) so it is not
sure that the voting method can select a corrected
output. In other words, the boosting tree algo-
rithms can utilize subtrees mined from the corpus
to estimate the good weight values for subtrees,
and then combines them to determine whether or
not a tree is selected. In our opinion, with the
boosting tree algorithm we can have a chance to
obtain more accurate outputs. These results in Ta-
ble 4 effectively support for this evidence.
Moreover, Table 4 depicts that the proposed en-
semble method using different parameters for ei-
ther bagging and boosting models can effectively
improve the performance of bagging and boost-
ing in term of precision and recall. This was be-
cause the accuracy of each individual parser in the
model with different parameters is better than each
one in either the boosting or the bagging model.
In addition, when performing SSVM on the test
set, we might obtain some ?NULL? outputs since
the grammar generated by SSVM could not de-
rive this sentence. Forming a number of individual
SSVMs to an ensemble model is the way to handle
this case, but it could make the numbers of com-
pleted outputs and corrected outputs increase. Ta-
625
ble 4 indicates that the proposed SSVM ensemble
model obtained 88.4% precision and 79.3% recall.
Therefore it shows substantially a better F1 score
in comparison with previous work on the CLANG
corpus.
Summarily, our method achieved the best re-
call result and a high precision on CLANG corpus.
The proposed ensemble models outperformed the
original SSVM on CLANG corpus and its perfor-
mances also is better than that of the best pub-
lished result.
5 Conclusions
This paper presents a structured support vector
machine ensemble model for semantic parsing
problem by employing it on the corpus of sen-
tences and their representation in logical form.
We also draw a novel SSVM ensemble model in
which the forming ensemble strategy is based on a
selection method on various parameters of SSVM,
and the aggregation method is based on a switch-
ing model using subtrees mined from the outputs
of a SSVM ensemble model.
Experimental results show substantially that the
proposed ensemble model is better than the con-
ventional ensemble models for SSVM. It can also
effectively improve the performance in term of
precision and recall in comparison with previous
works.
Acknowledgments
The work on this paper was supported by a Mon-
bukagakusho 21st COE Program.
References
J. Allen. 1995. Natural Language Understand-
ing (2nd Edition). Mento Park, CA: Benjam-
ing/Cumming.
L. Breiman. 1996. Bagging predictors. Machine
Learning 24, 123-140.
T.G. Dietterich. 2000. An experimental compari-
son of three methods for constructing ensembles
of decision trees: Bagging, boosting, and ran-
domization. Machine Learning 40 (2) 139-158.
M. Johnson 1999. PCFG models of linguistic tree
representation. Computational Linguistics.
R. Ge and R.J. Mooney. 2005. A Statistical Se-
mantic Parser that Integrates Syntax and Seman-
ics. In proceedings of CONLL 2005.
J.C. Henderson and E. Brill 2000. Bagging and
Boosting a Treebank Parser. In proceedings
ANLP 2000: 34-41
R.J. Kate et al 2005. Learning to Transform Nat-
ural to Formal Languages. Proceedings of AAAI
2005, page 825-830.
T. Kudo, Y. Matsumoto. A Boosting Algorithm
for Classification of Semi-Structured Text. In
proceeding EMNLP 2004.
J. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
Proc. of ICML 2001.
D.C. Manning and H. Schutze. 1999. Founda-
tion of Statistical Natural Language Processing.
Cambridge, MA: MIT Press.
L.S. Zettlemoyer and M. Collins. 2005. Learn-
ing to Map Sentences to Logical Form: Struc-
tured Classification with Probabilistic Catego-
rial Grammars. In Proceedings of UAI, pages
825?830.
I. Tsochantaridis, T. Hofmann, T. Joachims, and
Y. Altun. 2004. Support Vector Machine Learn-
ing for Interdependent and Structured Output
Spaces. In proceedings ICML 2004.
V. Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer, N.Y., 1995.
L.R. Tang. 2003. Integrating Top-down and
Bottom-up Approaches in Inductive Logic Pro-
gramming: Applications in Natural Language
Processing and Relation Data Mining. Ph.D.
Dissertation, University of Texas, Austin, TX,
2003.
B. Taskar, D. Klein, M. Collins, D. Koller, and
C.D. Manning. 2004. Max-Margin Parsing. In
proceedings of EMNLP, 2004.
R.E. Schapire. 1999. A brief introduction to boost-
ing. Proceedings of IJCAI 99
M.J. Zaki. 2002. Efficiently Mining Frequent
Trees in a Forest. In proceedings 8th ACM
SIGKDD 2002.
J.M. Zelle and R.J. Mooney. 1996. Learning
to parse database queries using inductive logic
programming. In Proceedings AAAI-96, 1050-
1055.
626
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1149?1155,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Multilingual Dependency Analysis System using Online
Passive-Aggressive Learning
Le-Minh Nguyen, Akira Shimazu, and Phuong-Thai Nguyen
Japan Advanced Institute of Science and Technology (JAIST)
Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan
{nguyenml,shimazu,thai}@jaist.ac.jp
Xuan-Hieu Phan
Tohoku University
Aobayama 6-3-09, Sendai, 980-8579, Japan
hieuxuan@ecei.tohoku.ac.jp
Abstract
This paper presents an online algorithm for
dependency parsing problems. We propose
an adaptation of the passive and aggressive
online learning algorithm to the dependency
parsing domain. We evaluate the proposed
algorithms on the 2007 CONLL Shared
Task, and report errors analysis. Experimen-
tal results show that the system score is bet-
ter than the average score among the partici-
pating systems.
1 Introduction
Research on dependency parsing is mainly based
on machine learning methods, which can be called
history-based (Yamada and Matsumoto, 2003; Nivre
et al, 2006) and discriminative learning methods
(McDonald et al, 2005a; Corston-Oliver et al,
2006). The learning methods using in discrimina-
tive parsing are Perceptron (Collins, 2002) and on-
line large-margin learning (MIRA) (Crammer and
Singer, 2003).
The difference of MIRA-based parsing in com-
parison with history-based methods is that the
MIRA-based parser were trained to maximize the
accuracy of the overall tree. The MIRA based
parsing is close to maximum-margin parsing as in
Taskar et al (2004) and Tsochantaridis et al (2005)
for parsing. However, unlike maximum-margin
parsing, it is not limited to parsing sentences of 15
words or less due to computation time. The perfor-
mance of MIRA based parsing achieves the state-of-
the-art performance in English data (McDonald et
al., 2005a; McDonald et al, 2006).
In this paper, we propose a new adaptation of on-
line larger-margin learning to the problem of depen-
dency parsing. Unlike the MIRA parser, our method
does not need an optimization procedure in each
learning update, but users only an update equation.
This might lead to faster training time and easier im-
plementation.
The contributions of this paper are two-fold: First,
we present a training algorithm called PA learning
for dependency parsing, which is as easy to im-
plement as Perceptron, yet competitive with large
margin methods. This algorithm has implications
for anyone interested in implementing discrimina-
tive training methods for any application. Second,
we evaluate the proposed algorithm on the multilin-
gual data task as well as the domain adaptation task
(Nivre et al, 2007).
The remaining parts of the paper are organized as
follows: Section 2 proposes our dependency pars-
ing with Passive-Aggressive learning. Section 3
discusses some experimental results and Section 4
gives conclusions and plans for future work.
2 Dependency Parsing with
Passive-Aggressive Learning
This section presents the modification of Passive-
Aggressive Learning (PA) (Crammer et al, 2006)
for dependency parsing. We modify the PA algo-
rithm to deal with structured prediction, in which
our problem is to learn a discriminant function that
maps an input sentence x to a dependency tree y.
Figure 1 shows an example of dependency parsing
which depicts the relation of each word to another
word within a sentence. There are some algorithms
1149
Figure 1: This is an example of dependency tree
to determine these relations of each word to another
words, for instance, the modified CKY algorithm
(Eisner, 1996) is used to define these relations for
a given sentence.
2.1 Parsing Algorithm
Dependency-tree parsing as the search for the maxi-
mum spanning tree (MST) in a graph was proposed
by McDonald et al (2005b). In this subsection,
we briefly describe the parsing algorithms based on
the first-order MST parsing. Due to the limitation
of participation time, we only applied the first-order
decoding parsing algorithm in CONLL-2007. How-
ever, our algorithm can be used for the second order
parsing.
Let the generic sentence be denoted by x ; the
ith word of x is denoted by wi. The generic de-
pendency tree is denoted by y. If y is a dependency
tree for sentence x, we write (i, j) ? y to indicate
that there is a directed edge from word xwi to word
xwj in the tree, that is, xwi is the parent of xwj .
T = {(xt, yt)}nt=1 denotes the training data. We fol-
low the edge based factorization method of Eisner
(Eisner, 1996) and define the score of a dependency
tree as the sum of the score of all edges in the tree,
s(x, y) =
?
(i,j)?y
s(i, j) =
?
(i,j)?y
w ? ?(i, j) (1)
where ?(i, j) is a high-dimensional binary fea-
ture representation of the edge from xwi to xwj .
For example in Figure 1, we can present an example
?(i, j) as follows;
?(i, j) =
{
1 if xwi =? hit? andxwj =? ball?
0 otherwise
The basic question must be answered for models
of this form: how to find the dependency tree y with
the highest score for sentence x? The two algorithms
we employed in our dependency parsing model are
the Eisner parsing (Eisner, 1996) and Chu-Liu?s al-
gorithm (Chu and Liu, 1965). The algorithms are
commonly used in other online-learning dependency
parsing, such as in (McDonald et al, 2005a).
In the next subsection we will address the problem
of how to estimate the weight wi associated with a
feature ?i in the training data using an online PA
learning algorithm.
2.2 Online PA Learning
This section presents a modification of PA algo-
rithm for structured prediction, and its use in de-
pendency parsing. The Perceptron style for natural
language processing problems as initially proposed
by (Collins, 2002) can provide state of the art re-
sults on various domains including text chunking,
syntactic parsing, etc. The main drawback of the
Perceptron style algorithm is that it does not have a
mechanism for attaining the maximize margin of the
training data. It may be difficult to obtain high accu-
racy in dealing with hard learning data. The struc-
tured support vector machine (Tsochantaridis et al,
2005) and the maximize margin model (Taskar et al,
2004) can gain a maximize margin value for given
training data by solving an optimization problem (i.e
quadratic programming). It is obvious that using
such an optimization algorithm requires much com-
putational time. For dependency parsing domain,
McDonald et al(2005a) modified the MIRA learn-
ing algorithm (McDonald et al, 2005a) for struc-
tured domains in which the optimization problem
can be solved by using Hidreth?s algorithm (Censor
and Zenios, 1997), which is faster than the quadratic
programming technique. In contrast to the previous
method, this paper presents an online algorithm for
dependency parsing in which we can attain the max-
imize margin of the training data without using opti-
mization techniques. It is thus much faster and eas-
ier to implement. The details of PA algorithm for
dependency parsing are presented below.
Assume that we are given a set of sentences xi
and their dependency trees yi where i = 1, ..., n. Let
the feature mapping between a sentence x and a tree
y be: ?(x, y) = ?1(x, y),?2(x, y), ...,?d(x, y)
where each feature mapping ?j maps (x, y) to a real
value. We assume that each feature ?(x, y) is asso-
1150
ciated with a weight value. The goal of PA learning
for dependency parsing is to obtain a parameter w
that minimizes the hinge-loss function and the mar-
gin of learning data.
Input:S = {(xi; yi), i = 1, 2, ..., n} in which1
xi is the sentence and yi is a dependency tree
Aggressive parameter C2
Output: the PA learning model3
Initialize: w1 = (0, 0, ..., 0)4
for t=1, 2... do5
Receive an sentence xt6
Predict y?t = argmaxy?Y (wt ? ?(xt, yt))7
Suffer loss: lt =8
wt ??(xt, y?t )?wt ??(xt, yt) +
??(yt, y?t )
Set:9
PA: ?t = lt||?(xt,y?t )??(xt,yt)||2
PA-I: ?t = min{C, lt||?(xt,y?t )??(xt,yt)||2 }
PA-II: ?t = lt||?(xt,y?t )??(xt,yt)||2+ 12C
Update:
wt+1 = wt + ?t(?(xt, yt)? ?(xt, y?t ))
end10
Algorithm 1: The Passive-Aggressive algo-
rithm for dependency parsing.
Algorithm 1 shows the PA learning algorithm for
dependency parsing in which its three variants are
different only in the update formulas. In Algorithm
1, we employ two kinds of argmax algorithms: The
first is the decoding algorithm for projective lan-
guage data and the second one is for non-projective
language data. Algorithm 1 shows (line 8) p(y, yt)
is a real-valued loss for the tree yt relative to the
correct tree y. We define the loss of a dependency
tree as the number of words which have an incorrect
parent. Thus, the largest loss a dependency tree can
have is the length of the sentence. The similar loss
function is designed for the dependency tree with la-
beled. Algorithm 1 returns an averaged weight vec-
tor: an auxiliary weight vector v is maintained that
accumulates the values of w after each iteration, and
the returned weight vector is the average of all the
weight vectors throughout training. Averaging has
been shown to help reduce overfitting (McDonald et
al., 2005a; Collins, 2002). It is easy to see that the
main difference between the PA algorithms and the
Perceptron algorithm (PC) (Collins, 2002) as well as
the MIRA algorithm (McDonald et al, 2005a) is in
line 9. As we can see in the PC algorithm, we do
not need the value ?t and in the MIRA algorithm we
need an optimization algorithm to compute ?t. We
also have three updated formulations for obtaining
?t in Line 9. In the scope of this paper, we only
focus on using the second update formulation (PA-I
method) for training dependency parsing data.
2.3 Feature Set
We denote p-word: word of parent node in depen-
dency tree. c-word: word of child node. p-pos: POS
of parent node. c-pos: POS of child node. p-pos+1:
POS to the right of parent in sentence. p-pos-1: POS
to the left of parent. c-pos+1: POS to the right of
child. c-pos-1: POS to the left of child. b-pos: POS
of a word in between parent and child nodes. The
p-word,p-pos
p-word
p-pos
c-word, c-pos
c-word
c-pos
Table 1: Feature Set 1: Basic Unit-gram features
p-word, p-pos, c-word, c-pos
p-pos, c-word, c-pos
p-word, c-word, c-pos
p-word, p-pos, c-pos
p-word, p-pos, c-word
p-word, c-word
p-pos, c-pos
Table 2: Feature Set 2: Basic bi-gram features
p-pos, b-pos, c-pos
p-pos, p-pos+1, c-pos-1, c-pos
p-pos-1, p-pos, c-pos-1, c-pos
p-pos, p-pos+1, c-pos, c-pos+1
p-pos-1, p-pos, c-pos, c-pos+1
Table 3: Feature Set 3: In Between POS Features
and Surrounding Word POS Features
features used in our system are described below.
? Tables 1 and 2 show our basic features. These
1151
features are added for entire words as well as
for the 5-gram prefix if the word is longer than
5 characters.
? In addition to these features shown in Table 1,
the morphological information for each pair of
words p-word and c-word are represented. In
addition, we also add the conjunction morpho-
logical information of p-word and c-word. We
do not use the LEMMA and CPOSTAG infor-
mation in our set features. The morphological
information are obtained from FEAT informa-
tion.
? Table 3 shows our Feature set 3 which take the
form of a POS trigram: the POS of the par-
ent, of the child, and of a word in between,
for all words linearly between the parent and
the child. This feature was particularly helpful
for nouns identifying their parent (McDonald
et al, 2005a).
? Table 3 also depicts these features taken the
form of a POS 4-gram: The POS of the par-
ent, child, word before/after parent and word
before/after child. The system also used back-
off features with various trigrams where one of
the local context POS tags was removed.
? All features are also conjoined with the direc-
tion of attachment, as well as the distance be-
tween the two words being attached.
3 Experimental Results and Discussion
We test our parsing models on the CONLL-2007
(Hajic? et al, 2004; Aduriz et al, 2003; Mart?? et
al., 2007; Chen et al, 2003; Bo?hmova? et al, 2003;
Marcus et al, 1993; Johansson and Nugues, 2007;
Prokopidis et al, 2005; Csendes et al, 2005; Mon-
temagni et al, 2003; Oflazer et al, 2003) data set on
various languages including Arabic, Basque, Cata-
lan, Chinese, English, Italian, Hungarian, and Turk-
ish. Each word is attached by POS tags for each sen-
tence in both the training and the testing data. Table
4 shows the number of training and testing sentences
for these languages. The table shows that the sen-
tence length in Arabic data is largest and its size of
training data is smallest. These factors might be af-
fected to the accuracy of our proposed algorithm as
we will discuss later.
The training and testing were conducted on a pen-
tium IV at 4.3 GHz. The detailed information about
the data are shown in the CONLL-2007 shared task.
We applied non-projective and projective parsing
along with PA learning for the data in CONLL-2007.
Table 5 reports experimental results by using the
first order decoding method in which an MST pars-
ing algorithm (McDonald et al, 2005b) is applied
for non-projective parsing and the Eisner?s method
is used for projective language data. In fact, in our
method we applied non-projective parsing for the
Italian data, the Turkish data, and the Greek data.
This was because we did not have enough time to
train all training data using both projective and non-
projective parsing. This is the problem of discrimi-
native learning methods when performing on a large
set of training data. In addition, to save time in train-
ing we set the number of best trees k to 1 and the
parameter C is set to 0.05.
Table 5 shows the comparison of the proposed
method with the average, and three top systems on
the CONLL-2007. As a result, our method yields
results above the average score on the CONLL-2007
shared task (Nivre et al, 2007).
Table 5 also indicates that the Basque results ob-
tained a lower score than other data. We obtained
69.11 UA score and 58.16 LA score, respectively.
These are far from the results of the Top3 scores
(81.13 and 75.49). We checked the outputs of the
Basque data to understand the main reason for the
errors. We see that the errors in our methods are
usually mismatched with the gold data at the labels
?ncmod? and ?ncsubj?. The main reason might be
that the application of projective parsing for this data
in both training and testing is not suitable. This was
because the number of sentences with at least 1 non
projective relation in the data is large (26.1).
The Arabic score is lower than the scores of other
data because of some difficulties in our method as
follows. Morphological and sentence length prob-
lems are the main factors which affect the accuracy
of parsing Arabic data. In addition, the training size
in the Arabic is also a problem for obtaining a good
result. Furthermore, since our tasks was focused on
improving the accuracy of English data, it might be
unsuitable for other languages. This is an imbalance
1152
Languages Training size Tokens size tokens-per-sent % of NPR % of-sentence AL-1-NPR
Arabic 2,900 112,000 38.3 0.4 10.1
Basque 3,200 51,000 15.8 2.9 26.2
Catalan 15,000 431,000 28.8 0.1 2.9
Chinese 57,000 337,000 5.9 0.0 0.0
Czech 25,400 432,000 17.0 1.9 23.2
English 18,600 447,000 24.0 0.3 6.7
Greek 2,700 65,000 24.2 1.1 20.3
Hungarian 6,000 132,000 21.8 2.9 26.4
Italian 3,100 71,000 22.9 0.5 7.4
Turkish 5,600 65,000 11.6 0.5 33.3
Table 4: The data used in the multilingual track (Nivre et al, 2007). NPR means non-projective-relations.
AL-1-NPR means at-least-least 1 non-projective relation.
problem in our method. Table 5 also shows the com-
parison of our system to the average score and the
Top3 scores. It depicts that our system is accurate
in English data, while it has low accuracy in Basque
and Arabic data.
We also evaluate our models in the domain adap-
tation tasks. This task is to adapt our model trained
on PennBank data to the test data in the Biomedical
domain. The pchemtb-closed shared task (Marcus
et al, 1993; Johansson and Nugues, 2007; Kulick
et al, 2004) is used to illustrate our models. We do
not use any additional unlabeled data in the Biomed-
ical domain. Only the training data in the PennBank
is used to train our model. Afterward, we selected
carefully a suitable parameter using the development
test set. We set the parameter C to 0.01 and se-
lect the non projective parsing for testing to obtain
the highest result in the development data after per-
forming several experiments. After that, the trained
model was used to test the data in Biomedical do-
main. The score (UA=82.04; LA=79.50) shows that
our method yields results above the average score
(UA=76.42; LA=73.03). In addition, it is officially
coming in 4th place out of 12 teams and within 1.5%
of the top systems.
The good result of performing our model in an-
other domain suggested that the PA learning seems
sensitive to noise. We hope that this problem is
solved in future work.
4 Conclusions
This paper presents an online algorithm for depen-
dency parsing problem which have tested on various
language data in CONLL-2007 shared task. The per-
formance in English data is close to the Top3 score.
We also perform our algorithm on the domain adap-
tation task, in which we only focus on the training of
the source data and select a suitable parameter using
the development set. The result is very good as it
is close to the Top3 score of participating systems.
Future work will also be focused on extending our
method to a version of using semi-supervised learn-
ing that can efficiently be learnt by using labeled and
unlabeled data. We hope that the application of the
PA algorithm to other NLP problems such as seman-
tic parsing will be explored in future work.
Acknowledgments
We would like to thank D. Yuret for his helps in
checking errors of my parser?s outputs. We would
like to thank Vinh-Van Nguyen his helps during the
revision process and Mary Ann Mooradian for cor-
recting the paper.
We would like to thank to anonymous review-
ers for helpful discussions and comments on the
manuscript. Thank also to Sebastian Riedel for
checking the issues raised in the reviews.
The work on this paper was supported by a Mon-
bukagakusho 21st COE Program.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
1153
Languages Unlabled Accuracy Labeled Accuracy NTeams
PA-I Average Top3 Top2 Top1 PA-I Average Top3 Top2 Top1
Arabic 73.46 78.84 84.21 85.81 86.09 68.34 74.75 83.0 75.08 76.52 20
Basque 69.11 75.15 81.13 81.93 81.13 58.16 68.06 75.49 75.73 76.92 20
Catalan 88.12 87.98 93.12 93.34 93.40 83.23 79.85 87.90 88.16 88.70 20
Chinese 84.05 81.98 87.91 88.88 88.94 79.77 76.59 83.51 83.84 84.69 21
Czech 80.91 77.56 84.19 85.16 86.28 72.54 70.12 77.98 78.60 80.19 20
English 88.01 82.67 89.87 90.13 90.63 86.73 80.95 88.41 89.01 89.61 23
Greek 77.56 77.78 81.37 82.04 84.08 70.42 70.22 74.42 74.65 76.31 20
Hungarian 78.13 76.34 82.49 83.51 83.55 68.12 71.49 78.09 79.53 80.27 21
Italian 80.40 82.45 87.68 87.77 87.91 75.06 78.06 78.09 79.53 80.27 20
Turkish 80.19 73.19 85.77 85.77 86.22 67.63 73.19 79.24 79.79 79.81 20
Multilingual-average 79.99 71.13 85.62 85.71 86.55 72.52 65.77 79.90 80.28 80.32 23
pchemtb-closed 82.04 76.42 83.08 83.38 83.42 79.50 73.03 80.22 80.40 81.06 8
Table 5: Dependency accuracy in the CONLL-2007 shared task.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
Y. Censor and S.A. Zenios. 1997. Parallel optimization:
theory, algorithms, and applications. In Oxford Uni-
versity Press.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. In Science Sinica.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of EMNLP.
S. Corston-Oliver, A. Aue, K. Duh, , and E. Ringger.
2006. Multilingual dependency parsing using bayes
point machines. In Proceedings of HLT/NAACL.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research, 3:951?991.
K. Crammer, O. Dekel, J. Keshet, S.Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:581?585.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING 1996, pages 340?345.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald, K. Cramer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 523?530.
R. McDonald, K. Crammer, and F. Pereira. 2006. Multi-
lingual dependency parsing with a two-stage discrim-
inative parser. In Conference on Natural Language
Learning.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
1154
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL), pages 221?225.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
B. Taskar, D. Klein, M. Collins, D. Koller, and C.D. Man-
ning. 2004. Max-margin parsing. In proceedings of
EMNLP.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2005. Support vector machine learning for interde-
pendent and structured output spaces. In proceedings
ICML 2004.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT), pages 195?206.
1155
