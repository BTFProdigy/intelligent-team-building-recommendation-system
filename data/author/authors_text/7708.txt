Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 791?799,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Source-Language Entailment Modeling for Translating Unknown Terms
Shachar Mirkin?, Lucia Specia?, Nicola Cancedda?, Ido Dagan?, Marc Dymetman?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Xerox Research Centre Europe
{mirkins,dagan,szpekti}@cs.biu.ac.il
{lucia.specia,nicola.cancedda,marc.dymetman}@xrce.xerox.com
Abstract
This paper addresses the task of handling
unknown terms in SMT. We propose us-
ing source-language monolingual models
and resources to paraphrase the source text
prior to translation. We further present a
conceptual extension to prior work by al-
lowing translations of entailed texts rather
than paraphrases only. A method for
performing this process efficiently is pre-
sented and applied to some 2500 sentences
with unknown terms. Our experiments
show that the proposed approach substan-
tially increases the number of properly
translated texts.
1 Introduction
Machine Translation systems frequently encounter
terms they are not able to translate due to some
missing knowledge. For instance, a Statistical Ma-
chine Translation (SMT) system translating the
sentence ?Cisco filed a lawsuit against Apple for
patent violation? may lack words like filed and
lawsuit in its phrase table. The problem is espe-
cially severe for languages for which parallel cor-
pora are scarce, or in the common scenario when
the SMT system is used to translate texts of a do-
main different from the one it was trained on.
A previously suggested solution (Callison-
Burch et al, 2006) is to learn paraphrases of
source terms from multilingual (parallel) corpora,
and expand the phrase table with these para-
phrases1. Such solutions could potentially yield a
paraphrased sentence like ?Cisco sued Apple for
patent violation?, although their dependence on
bilingual resources limits their utility.
In this paper we propose an approach that con-
sists in directly replacing unknown source terms,
1As common in the literature, we use the term para-
phrases to refer to texts of equivalent meaning, of any length
from single words (synonyms) up to complete sentences.
using source-language resources and models in or-
der to achieve two goals.
The first goal is coverage increase. The avail-
ability of bilingual corpora, from which para-
phrases can be learnt, is in many cases limited.
On the other hand, monolingual resources and
methods for extracting paraphrases from monolin-
gual corpora are more readily available. These
include manually constructed resources, such as
WordNet (Fellbaum, 1998), and automatic meth-
ods for paraphrases acquisition, such as DIRT (Lin
and Pantel, 2001). However, such resources have
not been applied yet to the problem of substitut-
ing unknown terms in SMT. We suggest that by
using such monolingual resources we could pro-
vide paraphrases for a larger number of texts with
unknown terms, thus increasing the overall cover-
age of the SMT system, i.e. the number of texts it
properly translates.
Even with larger paraphrase resources, we may
encounter texts in which not all unknown terms are
successfully handled through paraphrasing, which
often results in poor translations (see Section 2.1).
To further increase coverage, we therefore pro-
pose to generate and translate texts that convey a
somewhat more general meaning than the original
source text. For example, using such approach,
the following text could be generated: ?Cisco ac-
cused Apple of patent violation?. Although less in-
formative than the original, a translation for such
texts may be useful. Such non-symmetric relation-
ships (as between filed a lawsuit and accused) are
difficult to learn from parallel corpora and there-
fore monolingual resources are more appropriate
for this purpose.
The second goal we wish to accomplish by
employing source-language resources is to rank
the alternative generated texts. This goal can be
achieved by using context-models on the source
language prior to translation. This has two advan-
tages. First, the ranking allows us to prune some
791
candidates before supplying them to the transla-
tion engine, thus improving translation efficiency.
Second, the ranking may be combined with target
language information in order to choose the best
translation, thus improving translation quality.
We position the problem of generating alterna-
tive texts for translation within the Textual Entail-
ment (TE) framework (Giampiccolo et al, 2007).
TE provides a generic way for handling language
variability, identifying when the meaning of one
text is entailed by the other (i.e. the meaning of
the entailed text can be inferred from the mean-
ing of the entailing one). When the meanings of
two texts are equivalent (paraphrase), entailment
is mutual. Typically, a more general version of
a certain text is entailed by it. Hence, through TE
we can formalize the generation of both equivalent
and more general texts for the source text. When
possible, a paraphrase is used. Otherwise, an alter-
native text whose meaning is entailed by the orig-
inal source is generated and translated.
We assess our approach by applying an SMT
system to a text domain that is different from the
one used to train the system. We use WordNet
as a source language resource for entailment rela-
tionships and several common statistical context-
models for selecting the best generated texts to be
sent to translation. We show that the use of source
language resources, and in particular the extension
to non-symmetric textual entailment relationships,
is useful for substantially increasing the amount of
texts that are properly translated. This increase is
observed relative to both using paraphrases pro-
duced by the same resource (WordNet) and us-
ing paraphrases produced from multilingual paral-
lel corpora. We demonstrate that by using simple
context-models on the source, efficiency can be
improved, while translation quality is maintained.
We believe that with the use of more sophisticated
context-models further quality improvement can
be achieved.
2 Background
2.1 Unknown Terms
A very common problem faced by machine trans-
lation systems is the need to translate terms (words
or multi-word expressions) that are not found in
the system?s lexicon or phrase table. The reasons
for such unknown terms in SMT systems include
scarcity of training material and the application
of the system to text domains that differ from the
ones used for training.
In SMT, when unknown terms are found in the
source text, the systems usually omit or copy them
literally into the target. Though copying the source
words can be of some help to the reader if the
unknown word has a cognate in the target lan-
guage, this will not happen in the most general
scenario where, for instance, languages use dif-
ferent scripts. In addition, the presence of a sin-
gle unknown term often affects the translation of
wider portions of text, inducing errors in both lex-
ical selection and ordering. This phenomenon is
demonstrated in the following sentences, where
the translation of the English sentence (1) is ac-
ceptable only when the unknown word (in bold) is
replaced with a translatable paraphrase (3):
1. ?. . . , despite bearing the heavy burden of the
unemployed 10% or more of the labor force.?
2. ?. . . , malgre? la lourde charge de compte le
10% ou plus de cho?meurs labor la force .?
3. ?. . . , malgre? la lourde charge des cho?meurs
de 10% ou plus de la force du travail.?
Several approaches have been proposed to deal
with unknown terms in SMT systems, rather than
omitting or copying the terms. For example, (Eck
et al, 2008) replace the unknown terms in the
source text by their definition in a monolingual
dictionary, which can be useful for gisting. To
translate across languages with different alpha-
bets approaches such as (Knight and Graehl, 1997;
Habash, 2008) use transliteration techniques to
tackle proper nouns and technical terms. For trans-
lation from highly inflected languages, certain ap-
proaches rely on some form of lexical approx-
imation or morphological analysis (Koehn and
Knight, 2003; Yang and Kirchhoff, 2006; Langlais
and Patry, 2007; Arora et al, 2008). Although
these strategies yield gain in coverage and transla-
tion quality, they only account for unknown terms
that should be transliterated or are variations of
known ones.
2.2 Paraphrasing in MT
A recent strategy to broadly deal with the prob-
lem of unknown terms is to paraphrase the source
text with terms whose translation is known to
the system, using paraphrases learnt from multi-
lingual corpora, typically involving at least one
?pivot? language different from the target lan-
guage of immediate interest (Callison-Burch et
792
al., 2006; Cohn and Lapata, 2007; Zhao et al,
2008; Callison-Burch, 2008; Guzma?n and Gar-
rido, 2008). The procedure to extract paraphrases
in these approaches is similar to standard phrase
extraction in SMT systems, and therefore a large
amount of additional parallel corpus is required.
Moreover, as discussed in Section 5, when un-
known texts are not from the same domain as the
SMT training corpus, it is likely that paraphrases
found through such methods will yield misleading
translations.
Bond et al (2008) use grammars to paraphrase
the whole source sentence, covering aspects like
word order and minor lexical variations (tenses
etc.), but not content words. The paraphrases are
added to the source side of the corpus and the cor-
responding target sentences are duplicated. This,
however, may yield distorted probability estimates
in the phrase table, since these were not computed
from parallel data.
The main use of monolingual paraphrases in
MT to date has been for evaluation. For exam-
ple, (Kauchak and Barzilay, 2006) paraphrase ref-
erences to make them closer to the system transla-
tion in order to obtain more reliable results when
using automatic evaluation metrics like BLEU
(Papineni et al, 2002).
2.3 Textual Entailment and Entailment Rules
Textual Entailment (TE) has recently become a
prominent paradigm for modeling semantic infer-
ence, capturing the needs of a broad range of
text understanding applications (Giampiccolo et
al., 2007). Yet, its application to SMT has been so
far limited to MT evaluation (Pado et al, 2009).
TE defines a directional relation between two
texts, where the meaning of the entailed text (hy-
pothesis, h) can be inferred from the meaning of
the entailing text, t. Under this paradigm, para-
phrases are a special case of the entailment rela-
tion, when the relation is symmetric (the texts en-
tail each other). Otherwise, we say that one text
directionally entails the other.
A common practice for proving (or generating)
h from t is to apply entailment rules to t. An
entailment rule, denoted LHS ? RHS, specifies
an entailment relation between two text fragments
(the Left- and Right- Hand Sides), possibly with
variables (e.g. build X in Y ? X is completed
in Y ). A paraphrasing rule is denoted with ?.
When a rule is applied to a text, a new text is in-
ferred, where the matched LHS is replaced with the
RHS. For example, the rule skyscraper? building
is applied to ?The world?s tallest skyscraper was
completed in Taiwan? to infer ?The world?s tallest
building was completed in Taiwan?. In this work,
we employ lexical entailment rules, i.e. rules with-
out variables. Various resources for lexical rules
are available, and the prominent one is WordNet
(Fellbaum, 1998), which has been used in virtu-
ally all TE systems (Giampiccolo et al, 2007).
Typically, a rule application is valid only under
specific contexts. For example, mouse ? rodent
should not be applied to ?Use the mouse to mark
your answers?. Context-models can be exploited
to validate the application of a rule to a text. In
such models, an explicit Word Sense Disambigua-
tion (WSD) is not necessarily required; rather, an
implicit sense-match is sought after (Dagan et al,
2006). Within the scope of our paper, rule ap-
plication is handled similarly to Lexical Substitu-
tion (McCarthy and Navigli, 2007), considering
the contextual relationship between the text and
the rule. However, in general, entailment rule ap-
plication addresses other aspects of context match-
ing as well (Szpektor et al, 2008).
3 Textual Entailment for Statistical
Machine Translation
Previous solutions for handling unknown terms in
a source text s augment the SMT system?s phrase
table based on multilingual corpora. This allows
indirectly paraphrasing s, when the SMT system
chooses to use a paraphrase included in the table
and produces a translation with the corresponding
target phrase for the unknown term.
We propose using monolingual paraphrasing
methods and resources for this task to obtain a
more extensive set of rules for paraphrasing the
source. These rules are then applied to s directly
to produce alternative versions of the source text
prior to the translation step. Moreover, further
coverage increase can be achieved by employing
directional entailment rules, when paraphrasing is
not possible, to generate more general texts for
translation.
Our approach, based on the textual entailment
framework, considers the newly generated texts as
entailed from the original one. Monolingual se-
mantic resources such as WordNet can provide en-
tailment rules required for both these symmetric
and asymmetric entailment relations.
793
Input: A text t with one or more unknown terms;
a monolingual resource of entailment rules;
k - maximal number of source alternatives to produce
Output: A translation of either (in order of preference):
a paraphrase of t OR a text entailed by t OR t itself
1. For each unknown term - fetch entailment rules:
(a) Fetch rules for paraphrasing; disregard rules
whose RHS is not in the phrase table
(b) If the set of rules is empty: fetch directional en-
tailment rules; disregard rules whose RHS is not
in the phrase table
2. Apply a context-model to compute a score for each rule
application
3. Compute total source score for each entailed text as a
combination of individual rule scores
4. Generate and translate the top-k entailed texts
5. If k > 1
(a) Apply target model to score the translation
(b) Compute final source-target score
6. Pick highest scoring translation
Figure 1: Scheme for handling unknown terms by using
monolingual resources through textual entailment
Through the process of applying entailment
rules to the source text, multiple alternatives of
entailed texts are generated. To rank the candi-
date texts we employ monolingual context-models
to provide scores for rule applications over the
source sentence. This can be used to (a) directly
select the text with the highest score, which can
then be translated, or (b) to select a subset of top
candidates to be translated, which will then be
ranked using the target language information as
well. This pruning reduces the load of the SMT
system, and allows for potential improvements in
translation quality by considering both source- and
target-language information.
The general scheme through which we achieve
these goals, which can be implemented using dif-
ferent context-models and scoring techniques, is
detailed in Figure 1. Details of our concrete im-
plementation are given in Section 4.
Preliminary analysis confirmed (as expected)
that readers prefer translations of paraphrases,
when available, over translations of directional en-
tailments. This consideration is therefore taken
into account in the proposed method.
The input is a text unit to be translated, such as a
sentence or paragraph, with one or more unknown
terms. For each unknown term we first fetch a
list of candidate rules for paraphrasing (e.g. syn-
onyms), where the unknown term is the LHS. For
example, if our unknown term is dodge, a possi-
ble candidate might be dodge ? circumvent. We
inflect the RHS to keep the original morphologi-
cal information of the unknown term and filter out
rules where the inflected RHS does not appear in
the phrase table (step 1a in Figure 1).
When no applicable rules for paraphrasing are
available (1b), we fetch directional entailment
rules (e.g. hypernymy rules such as dodge ?
avoid), and filter them in the same way as for para-
phrasing rules. To each set of rules for a given un-
known term we add the ?identity-rule?, to allow
leaving the unknown term unchanged, the correct
choice in cases of proper names, for example.
Next, we apply a context-model to compute an
applicability score of each rule to the source text
(step 2). An entailed text?s total score is the com-
bination (e.g. product, see Section 4) of the scores
of the rules used to produce it (3). A set of the
top-k entailed texts is then generated and sent for
translation (4).
If more than one alternative is produced by the
source model (and k > 1), a target model is ap-
plied on the selected set of translated texts (5a).
The combined source-target model score is a com-
bination of the scores of the source and target
models (5b). The final translation is selected to be
the one that yields the highest combined source-
target score (6). Note that setting k = 1 is equiva-
lent to using the source-language model alone.
Our algorithm validates the application of the
entailment rules at two stages ? before and af-
ter translation, through context-models applied at
each end. As the experiments will show in Sec-
tion 4, a large number of possible combinations of
entailment rules is a common scenario, and there-
fore using the source context models to reduce this
number plays an important role.
4 Experimental Setting
To assess our approach, we conducted a series of
experiments; in each experiment we applied the
scheme described in 3, changing only the mod-
els being used for scoring the generated and trans-
lated texts. The setting of these experiments is de-
scribed in what follows.
SMT data To produce sentences for our experi-
ments, we use Matrax (Simard et al, 2005), a stan-
dard phrase-based SMT system, with the excep-
tion that it allows gaps in phrases. We use approxi-
mately 1M sentence pairs from the English-French
794
Europarl corpus for training, and then translate a
test set of 5,859 English sentences from the News
corpus into French. Both resources are taken
from the shared translation task in WMT-2008
(Callison-Burch et al, 2008). Hence, we compare
our method in a setting where the training and test
data are from different domains, a common sce-
nario in the practical use of MT systems.
Of the 5,859 translated sentences, 2,494 contain
unknown terms (considering only sequences with
alphabetic symbols), summing up to 4,255 occur-
rences of unknown terms. 39% of the 2,494 sen-
tences contain more than a single unknown term.
Entailment resource We use WordNet 3.0 as
a resource for entailment rules. Paraphrases are
generated using synonyms. Directionally entailed
texts are created using hypernyms, which typically
conform with entailment. We do not rely on sense
information in WordNet. Hence, any other seman-
tic resource for entailment rules can be utilized.
Each sentence is tagged using the OpenNLP
POS tagger2. Entailment rules are applied for un-
known terms tagged as nouns, verbs, adjectives
and adverbs. The use of relations from WordNet
results in 1,071 sentences with applicable rules
(with phrase table entries) for the unknown terms
when using synonyms, and 1,643 when using both
synonyms and hypernyms, accounting for 43%
and 66% of the test sentences, respectively.
The number of alternative sentences generated
for each source text varies from 1 to 960 when
paraphrasing rules were applied, and reaches very
large numbers, up to 89,700 at the ?worst case?,
when all TE rules are employed, an average of 456
alternatives per sentence.
Scoring source texts We test our proposed
method using several context-models shown to
perform reasonably well in previous work:
? FREQ: The first model we use is a context-
independent baseline. A common useful
heuristic to pick an entailment rule is to se-
lect the candidate with the highest frequency
in the corpus (Mccarthy et al, 2004). In this
model, a rule?s score is the normalized num-
ber of occurrences of its RHS in the training
corpus, ignoring the context of the LHS.
? LSA: Latent Semantic Analysis (Deerwester
et al, 1990) is a well-known method for rep-
2http://opennlp.sourceforge.net
resenting the contextual usage of words based
on corpus statistics. We represented each
term by a normalized vector of the top 100
SVD dimensions, as described in (Gliozzo,
2005). This model measures the similarity
between the sentence words and the RHS in
the LSA space.
? NB: We implemented the unsupervised
Na??ve Bayes model described in (Glickman
et al, 2006) to estimate the probability that
the unknown term entails the RHS in the
given context. The estimation is based on
corpus co-occurrence statistics of the context
words with the RHS.
? LMS: This model generates the Language
Model probability of the RHS in the source.
We use 3-grams probabilities as produced by
the SRILM toolkit (Stolcke, 2002).
Finally, as a simple baseline, we generated a ran-
dom score for each rule application, RAND.
The score of each rule application by any of
the above models is normalized to the range (0,1].
To combine individual rule applications in a given
sentence, we use the product of their scores. The
monolingual data used for the models above is the
source side of the training parallel corpus.
Target-language scores On the target side we
used either a standard 3-gram language-model, de-
noted LMT, or the score assigned by the com-
plete SMT log-linear model, which includes the
language model as one of its components (SMT).
A pair of a source:target models comprises a
complete model for selecting the best translated
sentence, where the overall score is the product of
the scores of the two models.
We also applied several combinations of source
models, such as LSA combined with LMS, to take
advantage of their complementary strengths. Ad-
ditionally, we assessed our method with source-
only models, by setting the number of sentences to
be selected by the source model to one (k = 1).
5 Results
5.1 Manual Evaluation
To evaluate the translations produced using the
various source and target models and the different
rule-sets, we rely mostly on manual assessment,
since automatic MT evaluation metrics like BLEU
do not capture well the type of semantic variations
795
Model
Precision (%) Coverage (%)
PARAPH. TE PARAPH. TE
1 ?:SMT 75.8 73.1 32.5 48.1
2 NB:SMT 75.2 71.5 32.3 47.1
3 LSA:SMT 74.9 72.4 32.1 47.7
4 NB:? 74.7 71.1 32.1 46.8
5 LMS:LMT 73.8 70.2 31.7 46.3
6 FREQ:? 72.5 68.0 31.2 44.8
7 RAND 57.2 63.4 24.6 41.8
Table 1: Translation acceptance when using only para-
phrases and when using all entailment rules. ?:? indicates
which model is applied to the source (left side) and which to
the target language (right side).
generated in our experiments, particularly at the
sentence level.
In the manual evaluation, two native speakers
of the target language judged whether each trans-
lation preserves the meaning of its reference sen-
tence, marking it as acceptable or unacceptable.
From the sentences for which rules were applica-
ble, we randomly selected a sample of sentences
for each annotator, allowing for some overlap-
ping for agreement analysis. In total, the transla-
tions of 1,014 unique source sentences were man-
ually annotated, of which 453 were produced us-
ing only hypernyms (no paraphrases were appli-
cable). When a sentence was annotated by both
annotators, one annotation was picked randomly.
Inter-annotator agreement was measured by the
percentage of sentences the annotators agreed on,
as well as via the Kappa measure (Cohen, 1960).
For different models, the agreement rate varied
from 67% to 78% (72% overall), and the Kappa
value ranged from 0.34 to 0.55, which is compa-
rable to figures reported for other standard SMT
evaluation metrics (Callison-Burch et al, 2008).
Translation with TE For each model m, we
measured Precisionm, the percentage of accept-
able translations out of all sampled translations.
Precisionm was measured both when using only
paraphrases (PARAPH.) and when using all entail-
ment rules (TE). We also measured Coveragem,
the percentage of sentences with acceptable trans-
lations, Am, out of all sentences (2,494). As
our annotators evaluated only a sample of sen-
tences, Am is estimated as the model?s total num-
ber of sentences with applicable rules, Sm, mul-
tiplied by the model?s Precision (Sm was 1,071
for paraphrases and 1,643 for entailment rules):
Coveragem = Sm?Precisionm2,494 .
Table 1 presents the results of several source-
target combinations when using only paraphrases
and when also using directional entailment rules.
When all rules are used, a substantial improve-
ment in coverage is consistently obtained across
all models, reaching a relative increase of 50%
over paraphrases only, while just a slight decrease
in precision is observed (see Section 5.3 for some
error analysis). This confirms our hypothesis that
directional entailment rules can be very useful for
replacing unknown terms.
For the combination of source-target models,
the value of k is set depending on which rule-set
is used. Preliminary analysis showed that k = 5
is sufficient when only paraphrases are used and
k = 20 when directional entailment rules are also
considered.
We measured statistical significance between
different models for precision of the TE re-
sults according to the Wilcoxon signed ranks test
(Wilcoxon, 1945). Models 1-6 in Table 1 are sig-
nificantly better than the RAND baseline (p <
0.03), and models 1-3 are significantly better than
model 6 (p < 0.05). The difference between
?:SMT and NB:SMT or LSA:SMT is not statisti-
cally significant.
The results in Table 1 therefore suggest that
taking a source model into account preserves the
quality of translation. Furthermore, the quality is
maintained even when source models? selections
are restricted to a rather small top-k ranks, at a
lower computational cost (for the models combin-
ing source and target, like NB:SMT or LSA:SMT).
This is particularly relevant for on-demand MT
systems, where time is an issue. For such systems,
using this source-language based pruning method-
ology will yield significant performance gains as
compared to target-only models.
We also evaluated the baseline strategy where
unknown terms are omitted from the translation,
resulting in 25% precision. Leaving unknown
words untranslated also yielded very poor transla-
tion quality in an analysis performed on a similar
dataset.
Comparison to related work We compared our
algorithm with an implementation of the algo-
rithm proposed by (Callison-Burch et al, 2006)
(see Section 2.2), henceforth CB, using the Span-
ish side of Europarl as the pivot language.
Out of the tested 2,494 sentences with unknown
terms, CB found paraphrases for 706 sentences
(28.3%), while with any of our models, including
796
Model Precision (%) Coverage (%) Better (%)
NB:SMT (TE) 85.3 56.2 72.7
CB 85.3 24.2 12.7
Table 2: Comparison between our top model and the
method by Callison-Burch et al (2006), showing the per-
centage of times translations were considered acceptable, the
model?s coverage and the percentage of times each model
scored better than the other (in the 14% remaining cases, both
models produced unacceptable translations).
NB:SMT , our algorithm found applicable entail-
ment rules for 1,643 sentences (66%).
The quality of the CB translations was manually
assessed for a sample of 150 sentences. Table 2
presents the precision and coverage on this sample
for both CB and NB:SMT , as well as the number
of times each model?s translation was preferred by
the annotators. While both models achieve equally
high precision scores on this sample, the NB:SMT
model?s translations were undoubtedly preferred
by the annotators, with a considerably higher cov-
erage.
With the CB method, given that many of the
phrases added to the phrase table are noisy, the
global quality of the sentences seem to have been
affected, explaining why the judges preferred the
NB:SMT translations. One reason for the lower
coverage of CB is the fact that paraphrases were
acquired from a corpus whose domain is differ-
ent from that of the test sentences. The entail-
ment rules in our models are not limited to para-
phrases and are derived from WordNet, which has
broader applicability. Hence, utilizing monolin-
gual resources has proven beneficial for the task.
5.2 Automatic MT Evaluation
Although automatic MT evaluation metrics are
less appropriate for capturing the variations gen-
erated by our method, to ensure that there was no
degradation in the system-level scores according
to such metrics we also measured the models? per-
formance using BLEU and METEOR (Agarwal
and Lavie, 2007). The version of METEOR we
used on the target language (French) considers the
stems of the words, instead of surface forms only,
but does not make use of WordNet synonyms.
We evaluated the performance of the top mod-
els of Table 1, as well as of a baseline SMT sys-
tem that left unknown terms untranslated, on the
sample of 1,014 manually annotated sentences. As
shown in Table 3, all models resulted in improve-
ment with respect to the original sentences (base-
Model BLEU (TE) METEOR (TE)
?:SMT 15.50 0.1325
NB:SMT 15.37 0.1316
LSA:SMT 15.51 0.1318
NB:? 15.37 0.1311
CB 15.33 0.1299
Baseline SMT 15.29 0.1294
Table 3: Performance of the best models according to auto-
matic MT evaluation metrics at the corpus level. The baseline
refers to translation of the text without applying any entail-
ment rules.
line). The difference in METEOR scores is statis-
tically significant (p < 0.05) for the three top mod-
els against the baseline. The generally low scores
may be attributed to the fact that training and test
sentences are from different domains.
5.3 Discussion
The use of entailed texts produced using our ap-
proach clearly improves the quality of translations,
as compared to leaving unknown terms untrans-
lated or omitting them altogether. While it is clear
that textual entailment is useful for increasing cov-
erage in translation, further research is required to
identify the amount of information loss incurred
when non-symmetric entailment relations are be-
ing used, and thus to identify the cases where such
relations are detrimental to translation.
Consider, for example, the sentence: ?Conven-
tional military models are geared to decapitate
something that, in this case, has no head.?. In this
sentence, the unknown term was replaced by kill,
which results in missing the point originally con-
veyed in the text. Accordingly, the produced trans-
lation does not preserve the meaning of the source,
and was considered unacceptable: ?Les mode`les
militaires visent a` faire quelque chose que, dans
ce cas, n?est pas responsable.?.
In other cases, the selected hypernyms were too
generic words, such as entity or attribute, which
also fail to preserve the sentence?s meaning. On
the other hand, when the unknown term was a
very specific word, hypernyms played an impor-
tant role. For example, ?Bulgaria is the most
sought-after east European real estate target, with
its low-cost ski chalets and oceanfront homes?.
Here, chalets are replaced by houses or units (de-
pending on the model), providing a translation that
would be acceptable by most readers.
Other incorrect translations occurred when the
unknown term was part of a phrase, for exam-
ple, troughs replaced with depressions in peaks
797
and troughs, a problem that also strongly affects
paraphrasing. In another case, movement was the
hypernym chosen to replace labor in labor move-
ment, yielding an awkward text for translation.
Many of the cases which involved ambiguity
were resolved by the applied context-models, and
can be further addressed, together with the above
mentioned problems, with better source-language
context models.
We suggest that other types of entailment rules
could be useful for the task beyond the straight-
forward generalization using hypernyms, which
was demonstrated in this work. This includes
other types of lexical entailment relations, such as
holonymy (e.g. Singapore ? Southeast Asia) as
well as lexical syntactic rules (X cure Y ? treat
Y with X). Even syntactic rules, such as clause re-
moval, can be recruited for the task: ?Obama, the
44th president, declared Monday . . . ?? ?Obama
declared Monday . . . ?. When the system is un-
able to translate a term found in the embedded
clause, the translation of the less informative sen-
tence may still be acceptable by readers.
6 Conclusions and Future Work
In this paper we propose a new entailment-based
approach for addressing the problem of unknown
terms in machine translation. Applying this ap-
proach with lexical entailment rules from Word-
Net, we show that using monolingual resources
and textual entailment relationships allows sub-
stantially increasing the quality of translations
produced by an SMT system. Our experiments
also show that it is possible to perform the process
efficiently by relying on source language context-
models as a filter prior to translation. This pipeline
maintains translation quality, as assessed by both
human annotators and standard automatic mea-
sures.
For future work we suggest generating entailed
texts with a more extensive set of rules, in particu-
lar lexical-syntactic ones. Combining rules from
monolingual and bilingual resources seems ap-
pealing as well. Developing better context-models
to be applied on the source is expected to further
improve our method?s performance. Specifically,
we suggest taking into account the prior likelihood
that a rule is correct as part of the model score.
Finally, some researchers have advocated re-
cently the use of shared structures such as parse
forests (Mi and Huang, 2008) or word lattices
(Dyer et al, 2008) in order to allow a compact rep-
resentation of alternative inputs to an SMT system.
This is an approach that we intend to explore in
future work, as a way to efficiently handle the dif-
ferent source language alternatives generated by
entailment rules. However, since most current MT
systems do not accept such type of inputs, we con-
sider the results on pruning by source-side context
models as broadly relevant.
Acknowledgments
This work was supported in part by the ICT Pro-
gramme of the European Community, under the
PASCAL 2 Network of Excellence, ICT-216886
and The Israel Science Foundation (grant No.
1112/08). We wish to thank Roy Bar-Haim and
the anonymous reviewers of this paper for their
useful feedback. This publication only reflects the
authors? views.
References
Abhaya Agarwal and Alon Lavie. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proceedings of WMT-08.
Karunesh Arora, Michael Paul, and Eiichiro Sumita.
2008. Translation of Unknown Words in Phrase-
Based Statistical Machine Translation for Lan-
guages of Rich Morphology. In Proceedings of
SLTU.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving Statistical Machine
Translation by Paraphrasing the Training Data. In
Proceedings of IWSLT.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further Meta-Evaluation of Machine Translation. In
Proceedings of WMT.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
Trevor Cohn and Mirella Lapata. 2007. Machine
Translation by Triangulation: Making Effective Use
of Multi-Parallel Corpora. In Proceedings of ACL.
798
Ido Dagan, Oren Glickman, Alfio Massimiliano
Gliozzo, Efrat Marmorshtein, and Carlo Strappar-
ava. 2006. Direct Word Sense Matching for Lexical
Substitution. In Proceedings of ACL.
Scott Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
dauer, and R.A. Harshman. 1990. Indexing by La-
tent Semantic Analysis. Journal of the American So-
ciety for Information Science, 41.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing Word Lattice Trans-
lation. In Proceedings of ACL-HLT.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2008.
Communicating Unknown Words in Machine Trans-
lation. In Proceedings of LREC.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nising Textual Entailment Challenge. In Proceed-
ings of ACL-WTEP Workshop.
Oren Glickman, Ido Dagan, Mikaela Keller, Samy
Bengio, and Walter Daelemans. 2006. Investigat-
ing Lexical Substitution Scoring for Subtitle Gener-
ation. In Proceedings of CoNLL.
Alfio Massimiliano Gliozzo. 2005. Semantic Domains
in Computational Linguistics. Ph.D. thesis, Univer-
sity of Trento.
Francisco Guzma?n and Leonardo Garrido. 2008.
Translation Paraphrases in Phrase-Based Machine
Translation. In Proceedings of CICLing.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-HLT.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of
HLT-NAACL.
Kevin Knight and Jonathan Graehl. 1997. Machine
Transliteration. In Proceedings of ACL.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of EACL.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating Unknown Words by Analogical Learning. In
Proceedings of EMNLP-CoNLL.
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of
SIGKDD.
Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 Task 10: English Lexical Substitu-
tion Task. In Proceedings of SemEval.
Diana Mccarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding Predominant Word Senses
in Untagged Text. In Proceedings of ACL.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. In Proceedings of
EMNLP.
Sebastian Pado, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2009. Textual Entail-
ment Features for Machine Translation Evaluation.
In Proceedings of WMT.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL.
M. Simard, N. Cancedda, B. Cavestro, M. Dymet-
man, E. Gaussier, C. Goutte, and K. Yamada. 2005.
Translating with Non-contiguous Phrases. In Pro-
ceedings of HLT-EMNLP.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual Preferences. In Pro-
ceedings of ACL-HLT.
Frank Wilcoxon. 1945. Individual Comparisons by
Ranking Methods. Biometrics Bulletin, 1(6):80?83.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-Based
Backoff Models for Machine Translation of Highly
Inflected Languages. In Proceedings of EACL.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-HLT.
799
Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 55?60,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Hybrid Relational Approach for WSD ? First Results 
 
 
Lucia Specia 
N?cleo Interinstitucional de Ling??stica Computational ? ICMC ? University of S?o Paulo 
Caixa Postal 668, 13560-970, S?o Carlos, SP, Brazil 
lspecia@icmc.usp.br 
 
  
 
 
 
Abstract 
We present a novel hybrid approach for 
Word Sense Disambiguation (WSD) 
which makes use of a relational formalism 
to represent instances and background 
knowledge. It is built using Inductive 
Logic Programming techniques to com-
bine evidence coming from both sources 
during the learning process, producing a 
rule-based WSD model. We experimented 
with this approach to disambiguate 7 
highly ambiguous verbs in English-
Portuguese translation. Results showed 
that the approach is promising, achieving 
an average accuracy of 75%, which out-
performs the other machine learning tech-
niques investigated (66%). 
1 Introduction 
Word Sense Disambiguation (WSD) is concerned 
with the identification of the correct sense of an 
ambiguous word given its context. Although it can 
be thought of as an independent task, its importance 
is more easily realized when it is applied to particu-
lar tasks, such as Information Retrieval or Machine 
Translation (MT). In MT, the application we are 
focusing on, a WSD (or translation disambigua-
tion) module should identify the correct translation 
for a source word when options with different 
meanings are available.  
As shown by Vickrey et al (2005), we believe 
that a WSD module can significantly improve the 
performance of MT systems, provided that such 
module is developed following specific require-
ments of MT, e.g., employing multilingual sense 
repositories. Differences between monolingual and 
multilingual WSD are very significant for MT, 
since it is concerned only with the ambiguities that 
appear in the translation (Hutchins and Sommers, 
1992). 
In this paper we present a novel approach for 
WSD, designed focusing on MT. It follows a hy-
brid strategy, i.e., knowledge and corpus-based, 
and employs a highly expressive relational for-
malism to represent both the examples and back-
ground knowledge. This approach allows the 
exploitation of several knowledge sources, to-
gether with evidences provided by examples of 
disambiguation, both automatically extracted 
from lexical resources and sense tagged corpora. 
This is achieved using Inductive Logic Pro-
gramming (Muggleton, 1991), which has not 
been exploited for WSD so far. In this paper we 
investigate the disambiguation of 7 highly am-
biguous verbs in English-Portuguese MT, using 
knowledge from 7 syntactic, semantic and prag-
matic sources.  
In what follows, we first present some related 
approaches on WSD for MT, focusing oh their 
limitations (Section 2). We then give some basic 
concepts on Inductive Logic Programming and de-
scribe our approach (Section 3). Finally, we present 
our initial experiments and the results achieved 
(Section 4).  
2 Related work 
Many approaches have been proposed for WSD, 
but only a few are designed for specific applica-
tions, such as MT. Existing multilingual approaches 
can be classified as (a) knowledge-based ap-
proaches, which make use of linguistic knowledge 
manually codified or extracted from lexical re-
sources (Pedersen, 1997; Dorr and Katsova, 1998); 
(b) corpus-based approaches, which make use of 
knowledge automatically acquired from text using 
machine learning algorithms (Lee, 2002; Vickrey et 
al., 2005); and (c) hybrid approaches, which em-
ploy techniques from the two other approaches (Zi-
novjeva, 2000).  
55
Hybrid approaches potentially explore the ad-
vantages of both other strategies, yielding accurate 
and comprehensive systems. However, they are 
quite rare, even in monolingual contexts (Stevenson 
and Wilks, 2001, e.g.), and they are not able to in-
tegrate and use knowledge coming from corpus and 
other resources during the learning process.  
In fact, current hybrid approaches usually em-
ploy knowledge sources in pre-processing steps, 
and then use machine learning algorithms to com-
bine disambiguation evidence from those sources. 
This strategy is necessary due to the limitations of 
the formalism used to represent examples in the 
machine learning process: the propositional formal-
ism, which structures data in attribute-value vectors.  
Even though it is known that great part of the 
knowledge regarding to languages is relational 
(e.g., syntactic or semantic relations among words 
in a sentence) (Mooney, 1997), the propositional 
formalism traditionally employed makes unfeasible 
the representation of substantial relational knowl-
edge and the use of this knowledge during the 
learning process.  
According to the attribute-value representation, 
one attribute has to be created for every feature, and 
the same structure has to be used to characterize all 
the examples. In order to represent the syntactic 
relations between every pair of words in a sentence, 
e.g., it will be necessary to create at least one attrib-
ute for each possible relation (Figure 1). This would 
result in an enormous number of attributes, since 
the possibilities can be many in distinct sentences. 
Also, there could be more than one pair with the 
same relation. 
 
Sentence: John gave to Mary a big cake. 
verb1-subj1 verb1-obj1 mod1-obj1 ? 
give-john give-cake big-cake ? 
Figure 1. Attribute-value vector for syntactic relations  
 
Given that some types of information are not avail-
able for certain instances, many attributes will have 
null values. Consequently, the representation of the 
sample data set tends to become highly sparse. It is 
well-known that sparseness on data ensue serious 
problems to the machine learning process in general 
(Brown and Kros, 2003). Certainly, data will be-
come sparser as more knowledge about the exam-
ples is considered, and the problem will be even 
more critical if relational knowledge is used.  
Therefore, at least three relevant problems arise 
from the use of a propositional representation in 
corpus-based and hybrid approaches: (a) the limita-
tion on its expressiveness power, making it difficult 
to represent relational and other more complex 
knowledge; (b) the sparseness in data; and (c) the 
lack of integration of the evidences provided by 
examples and linguistic knowledge. 
3 A hybrid relational approach for WSD 
We propose a novel hybrid approach for WSD 
based on a relational representation of both exam-
ples and linguistic knowledge. This representation 
is considerably more expressive, avoids sparseness 
in data, and allows the use of these two types of 
evidence during the learning process.  
3.1 Sample data 
We address the disambiguation of 7 verbs selected 
according to the results of a corpus study (Specia, 
2005). To build our sample corpus, we collected 
200 English sentences containing each of the verbs 
from a corpus comprising fiction books. In a previ-
ous step, each sentence was automatically tagged 
with the translation of the verb, part-of-speech and 
lemmas of all words, and subject-object syntactic 
relations with respect to the verb (Specia et al, 
2005). The set of verbs, their possible translations, 
and the accuracy of the most frequent translation 
are shown in Table 1.  
 
Verb # Translations Most frequent 
translation - % 
come 11 50.3 
get 17 21 
give 5 88.8 
go 11 68.5 
look 7 50.3 
make 11 70 
take 13 28.5 
Table 1. Verbs and their possible senses in our corpus 
3.2 Inductive Logic Programming  
We utilize Inductive Logic Programming (ILP) 
(Muggleton, 1991) to explore relational machine 
learning. ILP employs techniques of both Machine 
Learning and Logic Programming to build first-
order logic theories from examples and background 
knowledge, which are also represented by means of 
first-order logic clauses. It allows the efficient rep-
resentation of substantial knowledge about the 
problem, and allows this knowledge to be used dur-
ing the learning process. The general idea underly-
ing ILP is: 
Given: 
-  a set of positive and negative examples E = 
E+ ? E- 
- a predicate p specifying the target relation to 
be learned 
56
- knowledge ? of a certain domain, described 
according to a language Lk, which specifies which 
other predicates qi can be part of the definition of p. 
The goal is: to induce a hypothesis (or theory) h 
for p, with relation to E and ?, which covers most 
of the E+, without covering the E-, that is, K ? h  
E+ and K ? h  E-.  
To implement our approach we chose Aleph 
(Srinivasan, 2000), an ILP system which provides a 
complete relational learning inference engine and 
various customization options. We used the follow-
ing options, which correspond to the Progol mode 
(Muggleton, 1995): bottom-up search, non-
incremental and non-interactive learning, and learn-
ing based only on positive examples. Fundamen-
tally, the default inference engine induces a theory 
iteratively by means of the following steps: 
1. One instance is randomly selected to be gen-
eralized.  
2. A more specific clause (bottom clause) ex-
plaining the selected example is built. It consists of 
the representation of all knowledge about that ex-
ample.  
3. A clause that is more generic than the bottom 
clause is searched, by means of search and gener-
alization strategies (best first search, e.g.).  
4. The best clause found is added to the theory 
and the examples covered by such clause are re-
moved from the sample set. If there are more in-
stances in the sample set, return to step 1. 
3.3 Knowledge sources 
The choice, acquisition, and representation of syn-
tactic, semantic, and pragmatic knowledge sources 
(KSs) were our main concerns at this stage. The 
general architecture of the system, showing our 7 
groups of KSs, is illustrated in Figure 2.  
Several of our KSs have been traditionally em-
ployed in monolingual WSD (e.g., Agirre and Ste-
venson, 2006), while other are specific for MT. 
Some of them were extracted from our sample cor-
pus (Section 3.1), while others were automatically 
extracted from lexical resources1. In what follows, 
we briefly describe, give the generic definition and 
examples of each KS, taking sentence (1), for the 
?to come?, as example. 
(1) ?If there is such a thing as reincarnation, I 
would not mind coming back as a squirrel?. 
 
KS1: Bag-of-words ? a list of ?5 words (lem-
mas) surrounding the verb for every sentence 
(sent_id). 
                                                          
1
 Michaelis? and Password? English-Portuguese Dictionar-
ies, LDOCE (Procter, 1978), and WordNet (Miller, 1990). 
 
 
 
 
 
KS2: Part-of-speech (POS) tags of content 
words in a ?5 word window surrounding the verb. 
 
 
 
 
 
KS3: Subject and object syntactic relations with 
respect to the verb under consideration. 
 
 
 
 
KS4: Context words represented by 11 colloca-
tions with respect to the verb: 1st preposition to the 
right, 1st and 2nd words to the left and right, 1st 
noun, 1st adjective, and 1st verb to the left and 
right. 
 
 
 
 
KS5: Selectional restrictions of verbs and se-
mantic features of their arguments, given by 
LDOCE. Verb restrictions are expressed by lists of 
semantic features required for their subject and ob-
ject, while these arguments are represented with 
their features. 
 
 
 
 
 
 
 
 
The hierarchy for LDOCE feature types defined 
by Bruce and Guthrie (1992) is used to account for 
restrictions established by the verb for features that 
are more generic than the features describing the 
words in the subject / object roles in the sentence. 
Ontological relations extracted from WordNet 
(Miller, 1990) are also used: if the restrictions im-
posed by the verb are not part of the description of 
its arguments, synonyms or hypernyms of those 
arguments that meet the restrictions are considered. 
 
 
 
 
 
 
KS6: Idioms and phrasal verbs, indicating that 
the verb occurring in a given context could have a 
specific translation.  
bag(sent_id, list_of_words). 
bag(sent1,[mind, not, will, i, reincarnation, back, as, a, 
squirrel]) 
has_pos(sent_id, word_position, pos). 
has_pos(sent1, first_content_word_left, nn).    
has_pos(sent1, second_content_word_left, vbp). 
 ...  
has_rel(sent_id, subject_word, object_word). 
has_rel(sent1, i, nil). 
 
rest(verb, subj_restrition, obj_ restriction ,translation) 
rest(come, [], nil, voltar). 
rest(come, [animal,human], nil, vir).  ... 
feature(noun, sense_id, features). 
feature(reincarnation, 0_1, [abstract]). 
feature(squirrel, 0_0, [animal]). 
 
has_collocation(sent_id, collocation_type, collocation) 
has_collocation(sent1, word_right_1, back). 
has_collocation(sent1, word_left_1, mind). ? 
 
relation(word1, sense_id1, word2 ,sense_id2). 
hyper(reincarnation, 1, avatar, 1). 
synon(rebirth, 2, reincarnation, -1). 
 
57
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. System architecture 
 
 
 
 
 
 
KS7: A count of the overlapping words in dic-
tionary definitions for the possible translations of 
the verb and the words surrounding it in the sen-
tence, relative to the total number of words.  
 
 
 
 
 
The representation of all KSs for each example 
is independent of the other examples. Therefore, the 
number of features can be different for different 
sentences, without resulting in sparseness in data.  
In order to use the KSs, we created a set of rules 
for each KS. These rules are not dependent on par-
ticular words or instances. They can be very simple, 
as in the example shown below for bag-of-words, 
or more complex, e.g., for selectional restrictions. 
Therefore, KSs are represented by means of rules 
and facts (rules without conditions), which can be 
intensional, i.e., it can contain variables, making the 
representation more expressive.  
 
 
 
 
Besides the KSs, the other main input to the sys-
tem is the set of examples. Since all knowledge 
about them is expressed by the KSs, the representa-
tion of examples is very simple, containing only the 
example identifier (of the sentence, in our case, 
such as, ?sent1?), and the class of that example (in 
 
 
 
 
 
 
 
 
KS4 
 
 
 
 
 
 
 
 
KS7 
 
 
 
 
 
 
 
 
 
 
 
 
KS6 
 
 
 
 
 
 
 
 
KS1 
ILP Inference  
Engine 
Rules to use Bag-
of-words (10) 
Rules to use Collo-
cations 
 
 
 
 
KS2
 
POS of the Narrow 
Context (10) 
Rules to use POS 
 
 
 
 
KS3 
Subject-object syn-
tactic relations 
Rules to use syntac-
tic relations 
Rules to use context 
with phrasal verbs 
and idioms 
 
 
 
 
 
 
 
 
 
 
 
 
KS5 
 
Verbs selectional 
restrictions 
Rules to use selec-
tional restrictions 
Subject-object syn-
tactic relations 
Nouns semantic 
features 
Rules to use defini-
tions overlapping 
Overlapping count-
ing 
Rule-based 
model 
Instances 
Bag-of-words (10) 
 
POS 
tagger 
LDOCE Wordnet 
Hierarchical rela-
tions 
Feature types 
hierarchy 
Bilingual MRDs 
Definitions over-
lapping 
 
Bag-of-words (200) 
 
Bag-of-words (10) 
Mode + type + 
general definitions 
 
Phrasal verbs and 
idioms 
Bag-of-words (10) 
 
11 Collocations 
Parser 
Verb definitions 
and examples 
LDOCE + Pass-
word 
exp(verbal_expression, translation) 
exp('come about', acontecer). 
exp('come about', chegar).   ? 
highest_overlap(sent_id, translation, overlapping). 
highest_overlap(sent1, voltar, 0.222222). 
highest_overlap(sent2, chegar, 0.0857143). 
 
has_bag(Sent,Word) :-   
bag(Sent,List), member(Word,List).  
 
58
our case, the translation of the verb in that sen-
tence). 
 
 
 
 
 
In Aleph?s default induction mode, the order of 
the training examples plays an important role. One 
example is taken at a time, according to its order in 
the training set, and a rule can be produced based 
on that example. Since examples covered by a cer-
tain rule are removed from the training set, certain 
examples will not be used to produce rules. Induc-
tion methods employing different strategies in 
which the order is irrelevant will be exploited in 
future work. 
In order to produce a theory, Aleph also requires 
?mode definitions?, i.e., the specification of the 
predicates p and q (Section 3.2). For example, the 
first mode definition below states that the predicate 
p to be learned will consist of a clause 
sense(sent_id, translation), which can be instanti-
ated only once (1). The other two definitions state 
the predicates q, has_colloc(sent_id, colloc_id, col-
loc), with at most 11 instantiations, and 
has_bag(sent_id, word), with at most 10 instantia-
tions. That is, the predicates in the conditional piece 
of the rules in the theory can consist of up to 11 
collocations and a bag of up to 10 words. One mode 
definition must be created for each KS. 
 
 
 
 
 
Based on the examples and background knowl-
edge, the inference engine will produce a set of 
symbolic rules. Some of the rules induced for the 
verb ?to come?, e.g., are illustrated in the box be-
low.  
 
 
 
 
 
 
 
 
 
The first rule checks if the first preposition to 
the right of the verb is ?out?, assigning the transla-
tion ?sair? if so. The second rule verifies if the sub-
ject-object arguments satisfy the verb restrictions, 
i.e, if the subject has the features ?animal? or ?hu-
man?, and the object has the feature ?concrete?. 
Alternatively, it verifies if the sentence contains the 
phrasal verb ?come at?.  Rule 3 also tests the verb 
selectional restrictions and the first word to the right 
of the verb.  
4 Experiments and results 
In order to assess the accuracy of our approach, we 
ran a set of initial experiments with our sample cor-
pus. For each verb, we ran Aleph in the default 
mode, except for the following parameters: 
 
 
 
 
 
 
 
The accuracy was calculated by applying the 
rules to classify the new examples in the test set 
according to the order these rules appeared in the 
theory, eliminating the examples (correctly or 
incorrectly) covered by a certain rule from the 
test set. In order to cover 100% of the examples, 
we relied on the existence of a rule without con-
ditions, which generally is induced by Aleph and 
points out to the most frequent translation in the 
training data. When this rule was not generated by 
Aleph, we add it to the end of theory. For all the 
verbs, however, this rule only classified a few ex-
amples (form 1 to 6). 
In Table 2 we show the accuracy of the theory 
learned for each verb, as well as accuracy 
achieved by two propositional machine learning 
algorithms on the same data: Decision Trees 
(C4.5) and Support Vector Machine (SVM), all 
according to a 10-fold cross-validation strategy. 
Since it is rather impractical to represent certain 
KSs using attribute-value vectors, in the experi-
ments with SVM and C4.5 only low level fea-
tures were considered, corresponding to KS1, KS2, 
KS3, and KS4. On average, Our approach outper-
forms the two other algorithms. Moreover, its accu-
racy is by far better than the accuracy of the most 
frequent sense baseline (Table 1).  
For all verbs, theories with a small number of 
rules were produced (from 19 to 33 rules). By 
looking at these rules, it becomes clear that all KSs 
are being explored by the ILP system and thus are 
potentially useful for the disambiguation of verbs. 
5 Conclusion and future work 
We presented a hybrid relational approach for 
WSD designed for MT. One important character-
istic of our approach is that all the KSs were 
sense(sent_id,translation). 
sense(sent1,voltar). 
sense(sent2,ir). 
 
:- modeh(1,sense(sent,translation)). 
:- modeb(11,has_colloc(sent,colloc_id,colloc)). 
:- modeb(10,has_bag(sent,word)).  ? 
1. sense(A, sair) :- 
    has_collocation(A, preposition_right, out). 
2. sense(A, chegar) :- 
    satisfy_restrictions(A, [animal,human],[concrete]); 
    has_expression(A, 'come at'). 
3. sense(A, vir) :- 
    satisfy_restriction(A, [human],[abstract]),  
    has_collocation(A, word_right_1, from). 
set(evalfn, posonly): learns from positive examples. 
set(search, heuristic): turns the search strategy heuristic. 
set(minpos, 2): establishes as 2 the minimum number of 
positive examples covered by each rule in the theory.  
set(gsamplesize, 1000): defines the number of randomly 
generated negative examples to prune the search space. 
59
 Verb Aleph 
Accuracy 
C4.5 
Accuracy 
SVM 
Accuracy 
come 0.82 0.55 0.6 
Get 0.51 0.36 0.45 
Give 0.96 0.88 0.88 
Go 0.73 0.73 0.72 
look 0.83 0.66 0.84 
make 0.74 0.76 0.76 
Take 0.66 0.35 0.41 
Average 0.75 0.61 0.67 
Table 2. Results of the experiments with Aleph 
 
automatically extracted, either from the corpus or 
machine-readable lexical resources. Therefore, the 
work could be easily extended to other words and 
languages. 
In future work we intend to carry out experi-
ments with different settings: (a) combinations of 
certain KSs; (b) other sample corpora, of different 
sizes, genres / domains; and (c) different parameters 
in Aleph regarding search strategies, evaluation 
functions, etc. We also intend to compare our ap-
proach with other machine learning algorithms us-
ing all the KSs employed in Aleph, by pre-
processing the KSs in order to extract binary fea-
tures that can be represented by means of attribute-
value vectors. After that, we intend to adapt our 
approach to evaluate it with standard WSD data 
sets, such as the ones used in Senseval2. 
References  
E. Agirre and M. Stevenson. 2006 (to appear). Knowl-
edge Sources for Word Sense Disambiguation. In 
Word Sense Disambiguation: Algorithms, Applica-
tions and Trends, Agirre, E. and Edmonds, P. (Eds.), 
Kluwer. 
M.L. Brown, J.F. Kros. 2003. Data Mining and the Im-
pact of Missing Data. Industrial Management and 
Data Systems, 103(8):611-621. 
R. Bruce and L. Guthrie. 1992. Genus disambiguation: A 
study in weighted performance. In Proceedings of the 
14th COLING, Nantes, pp. 1187-1191.  
B.J. Dorr and M. Katsova. 1998. Lexical Selection for 
Cross-Language Applications: Combining LCS with 
WordNet. In Proceedings of AMTA?1998, Langhorne, 
pp. 438-447. 
W.J. Hutchins and H.L. Somers. 1992. An Introduction 
to Machine Translation. Academic Press, Great Brit-
ain. 
H. Lee. 2002. Classification Approach to Word Selection 
in Machine Translation. In Proceedings of 
AMTA?2002, Berlin, pp. 114-123. 
                                                          
2
 http://www.senseval.org/ 
G.A. Miller, R.T. Beckwith, C.D. Fellbaum, D. Gross, K. 
Miller. 1990. WordNet: An On-line Lexical Database. 
International Journal of Lexicography, 3(4):235-244. 
R.J. Mooney. 1997. Inductive Logic Programming for 
Natural Language Processing. In Proceedings of the 
6th International ILP Workshop, Berlin, pp. 3-24. 
S. Muggleton. 1991. Inductive Logic Programming. New 
Generation Computing, 8 (4):295-318. 
S. Muggleton. 1995. Inverse Entailment and Progol.  
New Generation Computing Journal, 13: 245-286. 
B.S. Pedersen. 1997. Lexical Ambiguity in Machine 
Translation: Expressing Regularities in the Polysemy 
of Danish Motion Verbs. PhD Thesis, Center for 
Sprogteknologi, Copenhagen.  
P. Procter (editor). 1978. Longman Dictionary of Con-
temporary English. Longman Group, Essex, England. 
L. Specia. 2005. A Hybrid Model for Word Sense Dis-
ambiguation in English-Portuguese MT. In Proceed-
ings of the 8th CLUK, Manchester, pp. 71-78. 
L. Specia, M.G.V Nunes, M. Stevenson. 2005. Exploit-
ing Parallel Texts to Produce a Multilingual Sense-
tagged Corpus for Word Sense Disambiguation. In 
Proceedings of RANLP-05, Borovets, pp. 525-531.   
A. Srinivasan. 2000. The Aleph Manual. Technical Re-
port. Computing Laboratory, Oxford University. 
URL: 
http://web.comlab.ox.ac.uk/oucl/research/areas/machl
earn/Aleph/aleph_toc.html. 
M. Stevenson and Y. Wilks. 2001 The Interaction of 
Knowledge Sources for Word Sense Disambiguation. 
Computational Linguistics, 27(3):321-349. 
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 
2005. Word-Sense Disambiguation for Machine 
Translation. In Proceedings of HLT/EMNLP-05, Van-
couver. 
N. Zinovjeva. 2000. Learning Sense Disambiguation 
Rules for Machine Translation. Master?s Thesis, De-
partment of Linguistics, Uppsala University. 
60
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 41?48,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning Expressive Models for Word Sense Disambiguation 
Lucia Specia 
NILC/ICMC 
University of S?o Paulo 
Caixa Postal 668, 13560-970 
S?o Carlos, SP, Brazil 
lspecia@icmc.usp.br 
Mark Stevenson 
Department of Computer Science 
University of Sheffield 
Regent Court, 211 Portobello St. 
Sheffield, S1 4DP, UK  
marks@dcs.shef.ac.uk 
Maria das Gra?as V. Nunes 
NILC/ICMC 
University of S?o Paulo 
Caixa Postal 668, 13560-970  
S?o Carlos, SP, Brazil 
gracan@icmc.usp.br 
 
 
Abstract 
We present a novel approach to the word 
sense disambiguation problem which 
makes use of corpus-based evidence com-
bined with background knowledge. Em-
ploying an inductive logic programming 
algorithm, the approach generates expres-
sive disambiguation rules which exploit 
several knowledge sources and can also 
model relations between them. The ap-
proach is evaluated in two tasks: identifica-
tion of the correct translation for a set of 
highly ambiguous verbs in English-
Portuguese translation and disambiguation 
of verbs from the Senseval-3 lexical sam-
ple task. The average accuracy obtained for 
the multilingual task outperforms the other 
machine learning techniques investigated. 
In the monolingual task, the approach per-
forms as well as the state-of-the-art sys-
tems which reported results for the same 
set of verbs. 
1 Introduction 
Word Sense Disambiguation (WSD) is concerned 
with the identification of the meaning of ambi-
guous words in context. For example, among the 
possible senses of the verb ?run? are ?to move fast 
by using one's feet? and ?to direct or control?. 
WSD can be useful for many applications, includ-
ing information retrieval, information extraction 
and machine translation. Sense ambiguity has been 
recognized as one of the most important obstacles 
to successful language understanding since the ear-
ly 1960?s and many techniques have been pro-
posed to solve the problem. Recent approaches 
focus on the use of various lexical resources and 
corpus-based techniques in order to avoid the sub-
stantial effort required to codify linguistic know-
ledge. These approaches have shown good results; 
particularly those using supervised learning (see 
Mihalcea et al, 2004 for an overview of state-of-
the-art systems). However, current approaches rely 
on limited knowledge representation and modeling 
techniques: traditional machine learning algorithms 
and attribute-value vectors to represent disambigu-
ation instances. This has made it difficult to exploit 
deep knowledge sources in the generation of the 
disambiguation models, that is, knowledge that 
goes beyond simple features extracted directly 
from the corpus, like bags-of-words and colloca-
tions, or provided by shallow natural language 
tools like part-of-speech taggers.  
In this paper we present a novel approach for 
WSD that follows a hybrid strategy, i.e. combines 
knowledge and corpus-based evidence, and em-
ploys a first-order formalism to allow the represen-
tation of deep knowledge about disambiguation 
examples together with a powerful modeling tech-
nique to induce theories based on the examples and 
background knowledge. This is achieved using 
Inductive Logic Programming (ILP) (Muggleton, 
1991), which has not yet been applied to WSD.  
Our hypothesis is that by using a very expres-
sive representation formalism, a range of (shallow 
and deep) knowledge sources and ILP as learning 
technique, it is possible to generate models that, 
when compared to models produced by machine 
learning algorithms conventionally applied to 
41
WSD, are both more accurate for fine-grained dis-
tinctions, and ?interesting?, from a knowledge ac-
quisition point of view (i.e., convey potentially 
new knowledge that can be easily interpreted by 
humans).  
WSD systems have generally been more suc-
cessful in the disambiguation of nouns than other 
grammatical categories (Mihalcea et al, 2004). A 
common approach to the disambiguation of nouns 
has been to consider a wide context around the 
ambiguous word and treat it as a bag of words or 
limited set of collocates. However, disambiguation 
of verbs generally benefits from more specific 
knowledge sources, such as the verb?s relation to 
other items in the sentence (for example, by ana-
lysing the semantic type of its subject and object). 
Consequently, we believe that the disambiguation 
of verbs is task to which ILP is particularly well-
suited. Therefore, this paper focuses on the disam-
biguation of verbs, which is an interesting task 
since much of the previous work on WSD has con-
centrated on the disambiguation of nouns.  
WSD is usually approached as an independent 
task, however, it has been argued that different 
applications may have specific requirements (Res-
nik and Yarowsky, 1997). For example, in machine 
translation, WSD, or translation disambiguation, is 
responsible for identifying the correct translation 
for an ambiguous source word. There is not always 
a direct relation between the possible senses for a 
word in a (monolingual) lexicon and its transla-
tions to a particular language, so this represents a 
different task to WSD against a (monolingual) 
lexicon (Hutchins and Somers, 1992). Although it 
has been argued that WSD does not yield better 
translation quality than a machine translation 
system alone, it has been recently shown that a 
WSD module that is developed following specific 
multilingual requirements can significantly im-
prove the performance of a machine translation 
system (Carpuat et al, 2006). 
This paper focuses on the application of our ap-
proach to the translation of verbs in English to Por-
tuguese translation, specifically for a set of 10 
mainly light and highly ambiguous verbs. We also 
experiment with a monolingual task by using the 
verbs from Senseval-3 lexical sample task. We 
explore knowledge from 12 syntactic, semantic 
and pragmatic sources. In principle, the proposed 
approach could also be applied to any lexical dis-
ambiguation task by customizing the sense reposi-
tory and knowledge sources. 
In the remainder of this paper we first present 
related approaches to WSD and discuss their limi-
tations (Section 2). We then describe some basic 
concepts on ILP and our application of this tech-
nique to WSD (Section 3). Finally, we described 
our experiments and their results (Section 4).  
2 Related Work 
WSD approaches can be classified as (a) know-
ledge-based approaches, which make use of lin-
guistic knowledge, manually coded or extracted 
from lexical resources (Agirre and Rigau, 1996; 
Lesk 1986); (b) corpus-based approaches, which 
make use of shallow knowledge automatically ac-
quired from corpus and statistical or machine 
learning algorithms to induce disambiguation 
models (Yarowsky, 1995; Sch?tze 1998); and (c) 
hybrid approaches, which mix characteristics from 
the two other approaches to automatically acquire 
disambiguation models from corpus supported by 
linguistic knowledge (Ng and Lee 1996; Stevenson 
and Wilks, 2001). 
Hybrid approaches can combine advantages 
from both strategies, potentially yielding accurate 
and comprehensive systems, particularly when 
deep knowledge is explored. Linguistic knowledge 
is available in electronic resources suitable for 
practical use, such as WordNet (Fellbaum, 1998), 
dictionaries and parsers. However, the use of this 
information has been hampered by the limitations 
of the modeling techniques that have been ex-
plored so far: using deep sources of domain know-
ledge is beyond the capabilities of such techniques, 
which are in general based on attribute-value vec-
tor representations. 
Attribute-value vectors consist of a set of 
attributes intended to represent properties of the 
examples. Each attribute has a type (its name) and 
a single value for a given example. Therefore, 
attribute-value vectors have the same expressive-
ness as propositional formalisms, that is, they only 
allow the representation of atomic propositions and 
constants. These are the representations used by 
most of the machine learning algorithms conven-
tionally employed to WSD, for example Na?ve 
Bayes and decision-trees. First-order logic, a more 
expressive formalism which is employed by ILP, 
allows the representation of variables and n-ary 
predicates, i.e., relational knowledge.  
42
In the hybrid approaches that have been ex-
plored so far, deep knowledge, like selectional pre-
ferences, is either pre-processed into a vector 
representation to accommodate machine learning 
algorithms, or used in previous steps to filter out 
possible senses e.g. (Stevenson and Wilks, 2001). 
This may cause information to be lost and, in addi-
tion, deep knowledge sources cannot interact in the 
learning process. As a consequence, the models 
produced reflect only the shallow knowledge that 
is provided to the learning algorithm.  
Another limitation of attribute-value vectors is 
the need for a unique representation for all the ex-
amples: one attribute is created for every knowl-
edge feature and the same structure is used to 
characterize all the examples. This usually results 
in a very sparse representation of the data, given 
that values for certain features will not be available 
for many examples. The problem of data sparse-
ness increases as more knowledge is exploited and 
this can cause problems for the machine learning 
algorithms. 
A final disadvantage of attribute-value vectors 
is that equivalent features may have to be bounded 
to distinct identifiers. An example of this occurs 
when the syntactic relations between words in a 
sentence are represented by attributes for each pos-
sible relation, sentences in which there is more 
than one instantiation for a particular grammatical 
role cannot be easily represented.  For example, the 
sentence ?John and Anna gave Mary a present.? 
contains a coordinate subject and, since each fea-
ture requires a unique identifier, two are required 
(subj1-verb1, subj2-verb1). These will be treated as 
two independent pieces of knowledge by the learn-
ing algorithm.  
First-order formalisms allow a generic predicate 
to be created for every possible syntactic role, re-
lating two or more elements. For example 
has_subject(verb, subject), which could then have 
two instantiations: has_subject(give, john) and 
has_subject(give, anna). Since each example is 
represented independently from the others, the data 
sparseness problem is minimized. Therefore, ILP 
seems to provide the most general-purpose frame-
work for dealing with such data: it does not suffer 
from the limitations mentioned above since there 
are explicit provisions made for the inclusion of 
background knowledge of any form, and the repre-
sentation language is powerful enough to capture 
contextual relationships. 
3 A hybrid relational approach to WSD 
In what follows we provide an introduction to ILP 
and then outline how it is applied to WSD by pre-
senting the sample corpus and knowledge sources 
used in our experiments. 
3.1 Inductive Logic Programming 
Inductive Logic Programming (Muggleton, 1991) 
employs techniques from Machine Learning and 
Logic Programming to build first-order theories 
from examples and background knowledge, which 
are also represented by first-order clauses. It allows 
the efficient representation of substantial know-
ledge about the problem, which is used during the 
learning process, and produces disambiguation 
models that can make use of this knowledge. The 
general approach underlying ILP can be outlined 
as follows:  
Given: 
-  a set of positive and negative examples E = 
E+ ? E- 
- a predicate p specifying the target relation to 
be learned 
- knowledge ? of the domain, described ac-
cording to a language Lk, which specifies which 
predicates qi can be part of the definition of p. 
The goal is: to induce a hypothesis (or theory) 
h for p, with relation to E and ?, which covers 
most of the E+, without covering the E-, i.e., K ? h 
 E+ and K ? h  E-.  
 
We use the Aleph ILP system (Srinivasan, 2000), 
which provides a complete inference engine and 
can be customized in various ways. The default 
inference engine induces a theory iteratively using 
the following steps: 
1. One instance is randomly selected to be gen-
eralized.  
2. A more specific clause (the bottom clause) is 
built using inverse entailment (Muggleton, 1995), 
generally consisting of the representation of all the 
knowledge about that example. 
3. A clause that is more generic than the bottom 
clause is searched for using a given search (e.g., 
best-first) and evaluation strategy (e.g., number of 
positive examples covered). 
4. The best clause is added to the theory and the 
examples covered by that clause are removed from 
the sample set. Stop if there are more no examples 
in the training set, otherwise return to step 1. 
43
3.2 Sample data 
This approach was evaluated using two scenarios: 
(1) an English-Portuguese multilingual setting ad-
dressing 10 very frequent and problematic verbs 
selected in a previous study (Specia et. al., 2005); 
and (2) an English setting consisting of 32 verbs 
from Senseval-3 lexical sample task (Mihalcea et. 
al. 2004). 
For the first scenario a corpus containing 500 
sentences for each of the 10 verbs was constructed. 
The text was randomly selected from corpora of 
different domains and genres, including literary 
fiction, Bible, computer science dissertation ab-
stracts, operational system user manuals, newspa-
pers and European Parliament proceedings. This 
corpus was automatically annotated with the trans-
lation of the verb using a tagging system based on 
parallel corpus, statistical information and transla-
tion dictionaries (Specia et al, 2005), followed by 
a manual revision. For each verb, the sense reposi-
tory was defined as the set of all the possible trans-
lations of that verb in the corpus. 80% of the 
corpus was randomly selected and used for train-
ing, with the remainder retained for testing. The 10 
verbs, number of possible translations and the per-
centage of sentences for each verb which use the 
most frequent translation are shown in Table 1. 
For the monolingual scenario, we use the sense 
tagged corpus and sense repositories provided for 
verbs in Senseval-3. There are 32 verbs with be-
tween 40 and 398 examples each. The number of 
senses varies between 3 and 10 and the average 
percentage of examples with the majority (most 
frequent) sense is 55%.  
 
 Verb # Translations Most frequent 
translation - % 
ask 7 53 
come 29 36 
get 41 13 
give 22 72 
go 30 53 
live 8 66 
look 12 41 
make 21 70 
take 32 25 
tell 8 66 
Table 1. Verbs and possible senses in our corpus 
 
Both corpora were lemmatized and part-of-speech 
(POS) tagged using Minipar (Lin, 1993) and 
Mxpost (Ratnaparkhi, 1996), respectivelly. Addi-
tionally, proper nouns identified by the tagger were 
replaced by a single identifier (proper_noun) and 
pronouns replaced by identifiers representing 
classes of pronouns (relative_pronoun, etc.).  
3.3 Knowledge sources 
We now describe the background knowledge 
sources used by the learning algorithm, having as 
an example sentence (1), in which the word ?com-
ing? is the target verb being disambiguated. 
 
(1) "If there is such a thing as reincarnation, I 
would not mind coming back as a squirrel". 
 
KS1. Bag-of-words consisting of 5 words to the 
right and left of the verb (excluding stop words), 
represented using definitions of the form 
has_bag(snt, word): 
has_bag(snt1, mind). 
has_bag(snt1, not). ? 
 
KS2. Frequent bigrams consisting of pairs of adja-
cent words in a sentence (other than the target 
verb) which occur more than 10 times in the cor-
pus, represented by has_bigram(snt, word1, 
word2): 
has_bigram(snt1, back, as). 
has_bigram(snt1, such, a). ? 
 
KS3. Narrow context containing 5 content words to 
the right and left of the verb, identified using POS 
tags, represented by has_narrow(snt, 
word_position, word): 
has_narrow(snt1, 1st_word_left, mind). 
has_narrow(snt1, 1st_word_right, back). ? 
 
KS4. POS tags of 5 words to the right and left of 
the verb, represented by has_pos(snt, 
word_position, pos): 
has pos(snt1, 1st_word_left, nn). 
has pos(snt1, 1st_word_right, rb). ? 
 
KS5. 11 collocations of the verb: 1st preposition to 
the right, 1st and 2nd words to the left and right, 
1st noun, 1st adjective, and 1st verb to the left and 
right. These are represented using definitions of the 
form has_collocation(snt, type, collocation): 
has_collocation(snt1, 1st_prep_right, back). 
has_collocation(snt1, 1st_noun_left, mind).? 
44
KS6. Subject and object of the verb obtained using 
Minipar and represented by has_rel(snt, type, 
word): 
has_rel(snt1, subject, i). 
has_rel(snt1, object, nil). ? 
 
KS7. Grammatical relations not including the tar-
get verb also identified using Minipar. The rela-
tions (verb-subject, verb-object, verb-modifier, 
subject-modifier, and object-modifier) occurring 
more than 10 times in the corpus are represented 
by has_related_pair(snt, word1, word2): 
has_related_pair(snt1, there, be). ? 
 
KS8. The sense with the highest count of overlap-
ping words in its dictionary definition and in the 
sentence containing the target verb (excluding stop 
words) (Lesk, 1986), represented by 
has_overlapping(sentence, translation): 
has_overlapping(snt1, voltar). 
 
KS9. Selectional restrictions of the verbs defined 
using LDOCE (Procter, 1978). WordNet is used 
when the restrictions imposed by the verb are not 
part of the description of its arguments, but can be 
satisfied by synonyms or hyperonyms of those ar-
guments. A hierarchy of feature types is used to 
account for restrictions established by the verb that 
are more generic than the features describing its 
arguments in the sentence. This information is 
represented by definitions of the form satis-
fy_restriction(snt, rest_subject, rest_object): 
satisfy_restriction(snt1, [human], nil). 
satisfy_restriction(snt1, [animal, human], nil). 
 
KS1-KS9 can be applied to both multilingual and 
monolingual disambiguation tasks. The following 
knowledge sources were specifically designed for 
multilingual applications: 
 
KS10. Phrasal verbs in the sentence identified using 
a list extracted from various dictionaries. (This 
information was not used in the monolingual task 
because phrasal constructions are not considered 
verb senses in Senseval data.) These are 
represented by definitions of the form 
has_expression(snt, verbal_expression):  
has_expression(snt1, ?come back?). 
 
KS11. Five words to the right and left of the target 
verb in the Portuguese translation. This could be 
obtained using a machine translation system that 
would first translate the non-ambiguous words in 
the sentence. In our experiments it was extracted 
using a parallel corpus and represented using defi-
nitions of the form has_bag_trns(snt, portu-
guese_word): 
has_bag_trns(snt1, coelho). 
has_bag_trns(snt1, reincarna??o). ? 
 
KS12. Narrow context consisting of 5 collocations 
of the verb in the Portuguese translation, which 
take into account the positions of the words, 
represented by has_narrow_trns(snt, 
word_position, portuguese_word): 
has_narrow_trns(snt1, 1st_word_right, como). 
has_narrow_trns(snt1, 2nd_word_right, um). ? 
 
In addition to background knowledge, the system 
learns from a set of examples. Since all knowledge 
about them is expressed as background knowledge, 
their representation is very simple, containing only 
the sentence identifier and the sense of the verb in 
that sentence, i.e. sense(snt, sense): 
sense(snt1,voltar).  
sense(snt2,ir). ? 
 
Based on the examples, background knowledge 
and a series of settings specifying the predicate to 
be learned (i.e., the heads of the rules), the predi-
cates that can be in the conditional part of the 
rules, how the arguments can be shared among dif-
ferent  predicates and several other parameters, the 
inference engine produces a set of symbolic rules. 
Figure 1 shows examples of the rules induced for 
the verb ?to come? in the multilingual task.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Examples of rules produced for the verb 
?come? in the multilingual task 
 
Rule_1. sense(A, voltar) :- 
    has_collocation(A, 1st_prep_right, back). 
Rule_2. sense(A, chegar) :- 
   has_rel(A, subj, B), has_bigram(A, today, B), 
   has_bag_trans(A, hoje). 
Rule_3. sense(A, chegar) :- 
    satisfy_restriction(A, [animal, human], [concrete]); 
    has_expression(A, 'come at'). 
Rule_4. sense(A, vir) :- 
    satisfy_restriction(A, [animate], nil);  
    (has_rel(A, subj, B), 
    (has_pos(A, B, nnp); has_pos(A, B, prp))). 
 
45
Models learned with ILP are symbolic and can be 
easily interpreted. Additionally, innovative knowl-
edge about the problem can emerge from the rules 
learned by the system. Although some rules simply 
test shallow features such as collocates, others pose 
conditions on sets of knowledge sources, including 
relational sources, and allow non-instantiated ar-
guments to be shared amongst them by means of 
variables. For example, in Figure 1, Rule_1 states 
that the translation of the verb in a sentence A will 
be ?voltar? (return) if the first preposition to the 
right of the verb in that sentence is ?back?. Rule_2 
states that the translation of the verb will be 
?chegar? (arrive) if it has a certain subject B, 
which occurs frequently with the word ?today? as a 
bigram, and if the partially translated sentence con-
tains the word ?hoje? (the translation of ?today?). 
Rule_3 says that the translation of the verb will be 
?chegar? (reach) if the subject of the verb has the 
features ?animal? or ?human? and the object has 
the feature ?concrete?, or if the verb occurs in the 
expression ?come at?. Rule_4 states that the trans-
lation of the verb will be ?vir? (move toward) if the 
subject of the verb has the feature ?animate? and 
there is no object, or if the verb has a subject B that 
is a proper noun (nnp) or a personal pronoun (prp). 
4 Experiments and results 
To assess the performance of the approach the 
model produced for each verb was tested on the 
corresponding set of test cases by applying the 
rules in a decision-list like approach, i.e., retaining 
the order in which they were produced and backing 
off to the most frequent sense in the training set to 
classify cases that were not covered by any of the 
rules. All the knowledge sources were made avail-
able to be used by the inference engine, since pre-
vious experiments showed that they are all relevant 
(Specia, 2006). In what follows we present the re-
sults and discuss each task.  
4.1 Multilingual task 
Table 2 shows the accuracies (in terms of percen-
tage of corpus instances which were correctly dis-
ambiguated) obtained by the Aleph models. 
Results are compared against the accuracy that 
would be obtained by using the most frequent 
translation in the training set to classify all the ex-
amples of the test set (in the column labeled ?Ma-
jority sense?). For comparison, we ran experiments 
with three learning algorithms frequently used for 
WSD, which rely on knowledge represented as 
attribute-value vectors: C4.5 (decision-trees), 
Naive Bayes and Support Vector Machine (SVM)1. 
In order to represent all knowledge sources in 
attribute-value vectors, KS2, KS7, KS9 and KS10 
had to be pre-processed to be transformed into bi-
nary attributes. For example, in the case of selec-
tional restrictions (KS9), one attribute was created 
for each possible sense of the verb and a true/false 
value was assigned to it depending on whether the 
arguments of the verb satisfied any restrictions re-
ferring to that sense. Results for each of these algo-
rithms are also shown in Table 2. 
As we can see in Table 2, the accuracy of the 
ILP approach is considerably better than the most 
frequent sense baseline and also outperforms the 
other learning algorithms. This improvement is 
statistically significant (paired t-test; p < 0.05). As 
expected, accuracy is generally higher for verbs 
with fewer possible translations.  
The models produced by Aleph for all the verbs 
are reasonably compact, containing 50 to 96 rules. 
In those models the various knowledge sources 
appear in different rules and all are used. This 
demonstrates that they are all useful for the disam-
biguation of verbs. 
 
Verb Majori- 
ty sense 
C4.5 Na?ve  
Bayes 
SVM Aleph 
ask 0.68 0.68 0.82 0.88 0.92 
come 0.46 0.57 0.61 0.68 0.73 
get 0.03 0.25 0.46 0.47 0.49 
give 0.72 0.71 0.74 0.74 0.74 
go 0.49 0.61 0.66 0.66 0.66 
live 0.71 0.72 0.64 0.73 0.87 
look 0.48 0.69 0.81 0.83 0.93 
make 0.64 0.62 0.60 0.64 0.68 
take 0.14 0.41 0.50 0.51 0.59 
tell 0.65 0.67 0.66 0.68 0.82 
Average 0.50 0.59 0.65 0.68 0.74 
Table 2. Accuracies obtained by Aleph and other 
learning algorithms in the multilingual task 
 
These results are very positive, particularly if we 
consider the characteristics of the multilingual sce-
nario: (1) the verbs addressed are highly ambi-
guous; (2) the corpus was automatically tagged and 
thus distinct synonym translations were sometimes 
                                                          
1
 The implementations provided by Weka were used. Weka is 
available from http://www.cs.waikato.ac.nz/ml/weka/ 
46
used to annotate different examples (these count as 
different senses for the inference engine); and (3) 
certain translations occur very infrequently (just 1 
or 2 examples in the whole corpus). It is likely that 
a less strict evaluation regime, such as one which 
takes account of synonym translations, would re-
sult in higher accuracies. 
It is worth noticing that we experimented with a 
few relevant parameters for both Aleph and the 
other learning algorithms. Values that yielded the 
best average predictive accuracy in the training 
sets were assumed to be optimal and used to eva-
luate the test sets.  
4.2 Monolingual task 
Table 3 shows the average accuracy obtained by 
Aleph in the monolingual task (Senseval-3 verbs 
with fine-grained sense distinctions and using the 
evaluation system provided by Senseval). It also 
shows the average accuracy of the most frequent 
sense and accuracies reported on the same set of 
verbs by the best systems submitted by the sites 
which participated in this task. Syntalex-3 (Mo-
hammad and Pedersen, 2004) is based on an en-
semble of bagged decision trees with narrow 
context part-of-speech features and bigrams. 
CLaC1 (Lamjiri et al, 2004) uses a Naive Bayes 
algorithm with a dynamically adjusted context 
window around the target word. Finally, MC-WSD 
(Ciaramita and Johnson, 2004) is a multi-class av-
eraged perceptron classifier using syntactic and 
narrow context features, with one component 
trained on the data provided by Senseval and other 
trained on WordNet glosses.  
 
System % Average accuracy 
Majority sense 0.56 
Syntalex-3 0.67 
CLaC1 0.67 
MC-WSD 0.72 
Aleph 0.72 
Table 3. Accuracies obtained by Aleph and other 
approaches in the monolingual task 
 
As we can see in Table 3, results are very encour-
aging: even without being particularly customized 
for this monolingual task, the ILP approach signif-
icantly outperforms the majority sense baseline and 
performs as well as the state-of-the-art system re-
porting results for the same set of verbs. As with 
the multilingual task, the models produced contain 
a small number of rules (from 6, for verbs with a 
few examples, to 88) and all knowledge sources 
are used across different rules and verbs. 
In general, results from both multilingual and 
monolingual tasks demonstrate that the hypothesis 
put forward in Section 1, that ILP?s ability to gen-
erate expressive rules which combine and integrate 
a wide range of knowledge sources is beneficial for 
WSD systems, is correct.  
5 Conclusion 
We have introduced a new hybrid approach to 
WSD which uses ILP to combine deep and shallow 
knowledge sources. ILP induces expressive disam-
biguation models which include relations between 
knowledge sources. It is an interesting approach to 
learning which has been considered promising for 
several applications in natural language processing 
and has been explored for a few of them, namely 
POS-tagging, grammar acquisition and semantic 
parsing (Cussens et al, 1997; Mooney, 1997). This 
paper has demonstrated that ILP also yields good 
results for WSD, in particular for the disambigua-
tion of verbs.  
We plan to further evaluate our approach for 
other sets of words, including other parts-of-speech 
to allow further comparisons with other approach-
es. For example, Dang and Palmer (2005) also use 
a rich set of features with a traditional learning al-
gorithm (maximum entropy). Currently, we are 
evaluating the role of the WSD models for the 10 
verbs of the multilingual task in an English-
Portuguese statistical machine translation system. 
References 
Eneko Agirre and German Rigau. 1996. Word Sense 
Disambiguation using Conceptual Density. Proceed-
ings of the 15th Conference on Computational Lin-
guistics (COLING-96). Copenhagen, pages 16-22. 
Marine Carpuat, Yihai Shen, Xiaofeng Yu, and Dekai 
WU. 2006. Toward Integrating Word Sense and Enti-
ty Disambiguation into Statistical Machine Transla-
tion. Proceedings of the Third International 
Workshop on Spoken Language Translation,. Kyoto, 
pages 37-44. 
Massimiliano Ciaramita and Mark Johnson. 2004. Mul-
ti-component Word Sense Disambiguation. Proceed-
ings of Senseval-3: 3rd International Workshop on 
the Evaluation of Systems for the Semantic Analysis 
of Text, Barcelona, pages 97-100. 
47
James Cussens, David Page, Stephen Muggleton, and 
Ashwin Srinivasan. 1997. Using Inductive Logic 
Programming for Natural Language Processing. 
Workshop Notes on Empirical Learning of Natural 
Language Tasks, Prague, pages 25-34. 
Hoa T. Dang and Martha Palmer. 2005. The Role of 
Semantic Roles in Disambiguating Verb Senses. 
Proceedings of the 43rd Meeting of the Association 
for Computational Linguistics (ACL-05), Ann Arbor, 
pages 42?49. 
Christiane Fellbaum. 1998. WordNet: An Electronic 
Lexical Database. MIT Press, Massachusetts.  
W. John Hutchins and Harold L. Somers. 1992. An In-
troduction to Machine Translation. Academic Press, 
Great Britain. 
Abolfazl K. Lamjiri, Osama El Demerdash, Leila Kos-
seim. 2004. Simple features for statistical Word 
Sense Disambiguation. Proceedings of Senseval-3: 
3rd International Workshop on the Evaluation of Sys-
tems for the Semantic Analysis of Text, Barcelona, 
pages 133-136. 
Michael Lesk. 1986. Automatic sense disambiguation 
using machine readable dictionaries: how to tell a 
pine cone from an ice cream cone. ACM SIGDOC 
Conference, Toronto, pages 24-26. 
Dekang Lin. 1993. Principle based parsing without 
overgeneration. Proceedings of the 31st Meeting of 
the Association for Computational Linguistics (ACL-
93), Columbus, pages 112-120. 
Rada Mihalcea, Timothy Chklovski and Adam Kilga-
riff. 2004. The Senseval-3 English Lexical Sample 
Task. Proceedings of Senseval-3: 3rd International 
Workshop on the Evaluation of Systems for Semantic 
Analysis of Text, Barcelona, pages 25-28. 
Saif Mohammad and Ted Pedersen. 2004. Complemen-
tarity of Lexical and Simple Syntactic Features: The 
SyntaLex Approach to Senseval-3. Proceedings of 
Senseval-3: 3rd International Workshop on the Eval-
uation of Systems for the Semantic Analysis of Text, 
Barcelona, pages 159-162. 
Raymond J. Mooney. 1997. Inductive Logic Program-
ming for Natural Language Processing. Proceedings 
of the 6th International Workshop on ILP, LNAI 
1314, Stockolm, pages 3-24. 
Stephen Muggleton. 1991. Inductive Logic Program-
ming. New Generation Computing, 8(4):295-318. 
Stephen Muggleton. 1995. Inverse Entailment and Pro-
gol. New Generation Computing, 13:245-286. 
 
Hwee T. Ng and Hian B. Lee. 1996. Integrating mul-
tiple knowledge sources to disambiguate word sense: 
an exemplar-based approach. Proceedings of the 34th 
Meeting of the Association for Computational 
Linguistics (ACL-96), Santa Cruz, CA, pages 40-47. 
Paul Procter (editor). 1978. Longman Dictionary of 
Contemporary English. Longman Group, Essex. 
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
Of-Speech Tagger. Proceedings of the Conference on 
Empirical Methods in Natural Language Processing, 
New Jersey, pages 133-142. 
Phillip Resnik and David Yarowsky. 1997. A Perspec-
tive on Word Sense Disambiguation Methods and 
their Evaluating. Proceedings of the ACL-SIGLEX 
Workshop Tagging Texts with Lexical Semantics: 
Why, What and How?, Washington. 
Hinrich Sch?tze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1): 97-123. 
Lucia Specia, Maria G.V. Nunes, and Mark Stevenson. 
2005. Exploiting Parallel Texts to Produce a 
Multilingual Sense Tagged Corpus for Word Sense 
Disambiguation. Proceedings of the Conference on 
Recent Advances on Natural Language Processing 
(RANLP-2005), Borovets, pages 525-531. 
Lucia Specia. 2006. A Hybrid Relational Approach for 
WSD - First Results. Proceedings of the 
COLING/ACL 06 Student Research Workshop, Syd-
ney, pages 55-60.  
Ashwin Srinivasan. 2000. The Aleph Manual. Technical 
Report. Computing Laboratory, Oxford University. 
Mark Stevenson and Yorick Wilks. 2001. The Interaction 
of Knowledge Sources for Word Sense Disambiguation. 
Computational Linguistics, 27(3):321-349. 
Yorick Wilks and Mark Stevenson. 1998. The Grammar 
of Sense: Using Part-of-speech Tags as a First Step in 
Semantic Disambiguation. Journal of Natural Lan-
guage Engineering, 4(1):1-9 
David Yarowsky. 1995. Unsupervised Word-Sense Dis-
ambiguation Rivaling Supervised Methods. 
Proceedings of the 33rd Meeting of the Association 
for Computational Linguistics (ACL-05), Cambridge, 
MA, pages 189-196.  
 
48
Multilingual versus Monolingual WSD 
Lucia Specia 
ICMC ? University of S?o Paulo 
Av. do Trabalhador S?o-Carlense, 400 
S?o Carlos, 13560-970, Brazil 
lspecia@icmc.usp.br
Maria das Gra?asVolpe Nunes 
ICMC ? University of S?o Paulo 
Av. do Trabalhador S?o-Carlense, 400 
S?o Carlos, 13560-970, Brazil 
gracan@icmc.usp.br
Mark Stevenson 
Computer Science ? University of Sheffield 
Regent Court, 211 Portobello Street 
Sheffield, S1 4DP, UK  
M.Stevenson@dcs.shef.ac.uk  
Gabriela Castelo Branco Ribeiro 
DL - Pontificial Catholic University - Rio 
R. Marqu?s de S?o Vicente, 225 - G?vea 
Rio de Janeiro, RJ, Brazil. CEP: 22.453-900 
gabrielacastelo@globo.com
  
Abstract 
Although it is generally agreed that Word 
Sense Disambiguation (WSD) is an ap-
plication dependent task, the great major-
ity of the efforts has aimed at the devel-
opment of WSD systems without consid-
ering their application. We argue that this 
strategy is not appropriate, since some 
aspects, such as the sense repository and 
the disambiguation process itself, vary 
according to the application. Taking Ma-
chine Translation (MT) as application 
and focusing on the sense repository, we 
present evidence for this argument by ex-
amining WSD in English-Portuguese MT 
of eight sample verbs. By showing that 
the traditional monolingual WSD strate-
gies are not suitable for multilingual ap-
plications, we intend to motivate the de-
velopment of WSD methods for particu-
lar applications.  
1 Introduction 
Word Sense Disambiguation (WSD) is con-
cerned with the choice of the most appropriate 
sense of an ambiguous word given its context. 
The applications for which WSD has been 
thought to be helpful include Information Re-
trieval, Information Extraction, and Machine 
Translation (MT) (Ide and Ver?nis, 1998). The 
usefulness of WSD for MT, particularly, has 
been recently subject of debate, with conflicting 
results. Vickrey et al (2005), e.g., show that the 
inclusion of a WSD module significantly im-
proves the performance of their statistical MT 
system. Conversely, Carpuat and Wu (2005) 
found that WSD does not yield significantly bet-
ter translation quality than a statistical MT sys-
tem alone. In this latter work, however, the WSD 
module was not specifically designed for MT: it 
is based on the use of monolingual methods to 
identify the source language senses, which are 
then mapped into the target language transla-
tions. 
In fact, although it has been agreed that WSD 
is more useful when it is meant for a specific ap-
plication (Wilks and Stevenson, 1998; Kilgarriff, 
1997; Resnik and Yarowsky, 1997), little has 
been done on the development of WSD modules 
specifically for particular applications. WSD 
models in general are application independent, 
and focus on monolingual contexts, particularly 
English. 
Approaches to WSD as an application-
independent task usually apply standardised 
sense repositories, such as WordNet (Miller, 
1990). For multilingual applications, a popular 
approach is to carry out monolingual WSD and 
then map the source language senses into the cor-
responding target word translations (Carpuat and 
Wu, 2005; Montoyo et al, 2002). Although this 
strategy can yield reasonable results for certain 
pairs of languages, especially those which have a 
common sense repository, such as EuroWordNet 
(Vossen, 1998), mapping senses between lan-
guages is a very complex issue (cf. Section 2).  
33
We believe that WSD is an intermediate, applica-
tion dependent task, and thus WSD modules for 
particular applications must be developed fol-
lowing the requirements of such applications. 
Many key factors of the process are application-
dependent. The main factor is the sense inven-
tory. As emphasized by Kilgarriff (1997), no 
sense inventory is suitable for all applications. 
Even for the same application there is often little 
consensus about the most appropriate sense in-
ventory. For example, the use of WordNet, al-
though very frequent, has been criticized due to 
characteristics such as the level sense granularity 
and the abstract criteria used for the sense dis-
tinctions in that resource (e.g., Palmer 1998). In 
particular, it is generally agreed that the granular-
ity in WordNet is too refined for MT.  
In addition to requiring different sense inven-
tories (Hutchins and Somers, 1992), the disam-
biguation process itself often can be varied ac-
cording to the application. For instance, in mono-
lingual WSD, the main information source is the 
context of the ambiguous word, that is, the sur-
rounding words in a sentence or paragraph. For 
MT purposes, the context can be also that of the 
translation in the target language, i.e., words 
which have been already translated.  
In this paper we focus on the differences in the 
sense inventory, contrasting the WordNet inven-
tory for English disambiguation, which was cre-
ated according to psycholinguistics principles, 
with the Portuguese translations assigned to a set 
of eight verbs in a corpus, simulating MT as a 
Computational Linguistics application.  
We show that the relation between the number 
of senses and translations is not a one-to-one, 
and that it is not only a matter of the level of re-
finement of WordNet. The number of transla-
tions can be either smaller or larger, i.e., either 
two or more senses can be translated as the same 
word, or the same sense can be translated using 
different words. With that, we present evidence 
that employing a monolingual WSD method for 
the task of MT is not appropriate, since monolin-
gual information offers little help to multilingual 
disambiguation. In other words, we argue that 
multilingual WSD is different from monolingual 
WSD, and thus requires specific strategies. We 
start by presenting approaches that show cognate 
results for different pairs of languages, and also 
approaches developed with the reverse goal of 
using multilingual information to help monolin-
gual WSD (Section 2). We then present our ex-
periments (Sections 3 and 4) and their results 
(Section 5).  
2 Related work  
Recently, others have also investigated the dif-
ferences between sense repositories for monolin-
gual and multilingual WSD. Chatterjee et al 
(2005), e.g., investigated the ambiguity in the 
translation of the English verb ?to have? into 
Hindi. 11 translation patterns were identified for 
the 19 senses of the verb, according to the vari-
ous target syntactic structures and/or target 
words for the verb. They argued that differences 
in both these aspects do not depend only on the 
sense of the verb. Out of the 14 senses analyzed, 
six had 2-5 different translations each.  
Bentivogli et al (2004) proposed an approach 
to create an Italian sense tagged corpus (Mul-
tiSemCor) based on the transference of the anno-
tations from the English sense tagged corpus 
SemCor (Miller et al, 1994), by means of word-
alignment methods. A gold standard corpus was 
created by manually transferring senses in Sem-
Cor to the Italian words in a translated version of 
that corpus. From a total of 1,054 English words, 
155 annotations were considered non-
transferable to their corresponding Italian words, 
mainly due to the lack of synonymy at the lexical 
level.  
Mih?ltz (2005) manually mapped senses from 
the English in a sense tagged corpus to Hungar-
ian translations, in order to carry out WSD be-
tween these languages. Out of 43 ambiguous 
nouns, 38 had all or most of their English senses 
mapped into the same Hungarian translation. 
Some senses of the remaining nouns had to be 
split into different Hungarian translations. On 
average, the sense mapping decreased the ambi-
guity from 3.97 English senses to 2.49 Hungar-
ian translations. 
As we intend to show with this work, differ-
ences like those mentioned above in the sense 
inventories make it inappropriate to use mono-
lingual WSD strategies for multilingual disam-
biguation. Nevertheless, some approaches have 
successfully employed multilingual information, 
especially parallel corpora, to support monolin-
gual WSD. They are motivated by the argument 
that the senses of a word should be determined 
based on the distinctions that are lexicalized in a 
second language (Resnik and Yarowsky, 1997). 
In general, the assumptions behind these ap-
proaches are the following:  
(1) If a source language word is translated dif-
ferently into a second language, it might be am-
biguous and the different translations can indi-
cate the senses in the source language.  
34
(2) If two distinct source language words are 
translated as the same word into a second lan-
guage, it often indicates that the two are being 
used with similar senses.  
Ide (1999), for example, analyzes translations 
of English words into four different languages, in 
order to check if the different senses of an Eng-
lish word are lexicalized by different words in all 
the other languages. A parallel aligned corpus is 
used and the translated senses are mapped into 
WordNet senses. She uses this information to 
determine a set of monolingual sense distinctions 
that is potentially useful for NLP applications. In 
subsequent work (Ide et al, 2002), seven lan-
guages and clustering techniques are employed 
to create sense groups based on the translations.  
Diab and Resnik (2002) use multilingual in-
formation to create an English sense tagged cor-
pus to train a monolingual WSD approach. An 
English sense inventory and a parallel corpus 
automatically produced by an MT system are 
employed. Sentence and word alignment systems 
are used to assign the word correspondences be-
tween the two languages. After grouping all the 
words that correspond to translations of a single 
word in the target language, all their possible 
senses are considered as candidates. The sense 
that maximizes the semantic similarity of the 
word with the others in the group is chosen.  
Similarly, Ng et al (2003) employ English-
Chinese parallel word aligned corpora to identify 
a repository of senses for English. The English 
word senses are manually defined, based on the 
WordNet senses, and then revised in the light of 
the Chinese translations. For example, if two oc-
currences of a word with two different senses in 
WordNet are translated into the same Chinese 
word, they will be considered to have the same 
English sense.  
In general, these approaches rely on the two 
previously mentioned assumptions about the in-
teraction between translations and word senses. 
Although these assumptions can be useful when 
using cross-language information as an approxi-
mation to monolingual disambiguation, they are 
not very helpful in the opposite direction, i.e., 
using monolingual information for cross-
language disambiguation, as we will show in 
Section 4.  
3 Experimental setting  
We focused our experiments on verbs, which 
represent difficult cases for WSD. In particular, 
we experimented with five frequent and highly 
ambiguous verbs identified as problematic for 
MT systems in a previous study (Specia, 2005): 
?to come?, ?to get?, ?to give?, ?to look?, and ?to 
make?; and other three frequent verbs that are 
not so ambiguous: ?to ask?, ?to live?, and ?to 
tell?. The inclusion of the additional verbs allows 
us to analyze the effect of the ambiguity level in 
the experiment. These verbs will then be trans-
lated into Portuguese so that the resulting transla-
tions can be contrasted to the English senses. 
3.1 Corpus selection 
We collected all the sentences containing one of 
the eight verbs and their corresponding phrasal 
verbs from SemCor, Senseval-2 and Senseval-3 
corpora1. These corpora were chosen because 
they are both widely used and easily available. In 
each of these corpora, ambiguous words are an-
notated with WordNet 2.0 senses. Occurrences 
which did not identify a unique sense were not 
used. The numbers of sentences selected for each 
verb and its phrasal verbs are shown in Table 1. 
Verb # Verb  
Occurrences
# Phrasal Verb 
Occurrences 
ask 414 8
come 674 330
get 683 267
give 740 79
live 242 5 
look 370 213 
make 1463 105 
tell 509 3 
Table 1. Number of verbs and phrasal verbs ex-
tracted from SemCor and Senseval corpora 
It is worth mentioning that the phrasal verbs in-
clude simple verb-particle constructions, such as 
?give up?, and more complex multi-word expres-
sions, e.g., ?get in touch with?, ?make up for?, 
?come to mind?, etc. 
In order to avoid biasing the experiment due to 
possible misunderstandings of the verb uses, and 
to make the experiment feasible, with a reason-
able number of occurrences to be analyzed, we 
selected a subset of the total number of sentences 
in Table 1, which were distributed among five 
professional English-Portuguese translators (T1, 
T2, T3, T4, T5), according to the following crite-
ria:  
- The meaning of the verb/phrasal verb in the 
context of the sentence should be understandable 
and non-ambiguous (for human translators). 
                                                
1
 Available at http://www.cs.unt.edu/~rada/downloads.html. 
35
- The experiment should be the most compre-
hensive possible, with the largest possible num-
ber of senses for each verb/phrasal. 
- Each translator should be given two occur-
rences (when available) of all the distinct senses 
of each verb/phrasal verb, in order to make it 
possible to contrast different uses of the verb.  
- The translators should not be given any in-
formation other than the sentence to select the 
translation.  
To meet these criteria, a professional translator, 
who was not involved in the translation task, 
post-processed the selected sentences, filtering 
them according to the criteria specified above. 
Due to both the scarce number of occurrences of 
each phrasal verb sense and the large number of 
different phrasal verbs for certain verbs, the post-
selection of phrasal verbs was different from the 
post-selection of verbs. In the case of verbs, the 
translator scanned the sentences in order to get 
10 distinct occurrences of each sense (two for 
each translator), eliminating those sentences 
which were too complex to understand or used 
the verb in an ambiguous way. This process did 
not eliminate any senses, and thus did not reduce 
the coverage of the experiment. When there were 
fewer than 10 occurrences of a given sense, sen-
tences were repeated among translators to guar-
antee that each translator would be given exam-
ples of all the senses of the verb. For instance, if 
a sense had only four occurrences, the first two 
occurrences were given to T1, T3 and T5, while 
the other two occurrences were given to T2 and 
T4. If a sense occurred only once for a verb, it 
was repeated for all five translators. 
For phrasal verbs, the same process was used 
to eliminate the complex and ambiguous sen-
tences. Two occurrences (when available) of 
each sense of a phrasal verb were then selected. 
Due to the large number of different phrasal 
verbs for certain verbs, they were divided among 
translators, so that each translator was given two 
occurrences of only some phrasal verbs of each 
verb. Sentences were distributed so that all trans-
lators had a similar number of cases, as shown in 
Table 2. 
In order to avoid biasing the translations ac-
cording to the English senses, the original sense 
annotations were not shown to the translators and 
the sentences for each of the verbs, together with 
their phrasal verbs, were randomly ordered. 
Additionally, we gave the same set of selected 
sentences to another group of five translators, so 
that we could analyze the reliability of the ex-
periment by investigating the agreement between 
the groups of translators on the same data. 
Translator
Verb 
# T1 # T2 # T3 # T4 # T5
ask 13 13 13 10 10
come 53 52 52 51 47
get 59 59 56 59 57
give 46 50 48 47 48
live 11 11 11 16 16
look 15 19 17 19 14
make 47 45 44 46 41
tell 14 12 12 15 10
Total  258 261 253 263 243
Table 2. Number of selected sentences and its 
distribution among the five translators 
3.2 English senses and Portuguese transla-
tions 
As mentioned above, the corpora used are tagged 
with WordNet senses. Although this may not be 
the optimal sense inventory for many purposes, it 
is the best option in terms of availability and 
comprehensiveness. Moreover, it is the most fre-
quently used repository for monolingual WSD 
systems, making it possible to generalize, to a 
certain level, our results to most of the monolin-
gual work. The number of senses for the eight 
selected verbs (and their phrasal verbs) in 
WordNet 2.0, along with the number of their 
possible translations in bilingual dictionaries2, is 
shown in Table 3. 
Verb # Senses # Translations 
ask  12 16
come 108 226
get  147 242
give  92 128
live  15 15
look  34 63
make 96 239
tell  12 28
Table 3. Verbs, possible senses and translations 
As we can see, the number of possible transla-
tions is different from the number of possible 
senses, which already shows that there is not a 
one-to-one correspondence between senses and 
translations (although there is a high correlation 
between the number of senses and translations: 
Pearson?s Correlation = 0.955). In general, the 
number of possible translations is greater than 
                                                
2
 For example, DIC Pratico Michaelis?, version 5.1. 
36
the number of possible senses, in part because 
synonyms are considered as different transla-
tions. As we will show in Section 5 (Table 4), we 
eliminate the use of synonyms as possible trans-
lations. Moreover, we are dealing with a limited 
set of possible senses, provided by the SemCor 
and Senseval data. As a consequence, the num-
ber of translations pointed out by the human 
translators for our corpus will be considerably 
smaller than the total number of possible transla-
tions. 
4 Contrasting senses and translations 
In order to contrast the English senses with the 
Portuguese translations, we submitted the se-
lected sentences (cf. Section 3.1) to two groups 
of five translators (T1, T2, T3, T4, and T5), all 
native speakers of Portuguese. We asked the 
translators to assign the appropriate translation to 
each of the verb occurrences, which we would 
then compare to the original English senses. 
They were not told what their translations were 
going to be used for. 
The translators were provided with entire sen-
tences, but for practical reasons they were asked 
to translate only the verb and were allowed to 
use any bilingual resource to search for possible 
translations, if needed. They were asked to avoid 
considering synonyms as different translations.  
The following procedure was defined to ana-
lyze the results returned by the translators, for 
each verb and its phrasal verbs separately: 
1) We grouped all the occurrences of an Eng-
lish sense and looked at all the translations used 
by the translators in order to identify synonyms 
(in those specific uses), using a dictionary of 
Portuguese synonyms. Synonyms were consid-
ered as unique translations.  
2) We then analyzed the sentences which had 
been given to multiple translators of the same 
group (when there were not enough occurrences 
of certain senses, as mentioned in Section 3.1), in 
order to identify a single translation for the oc-
currence and eliminate redundancies. The trans-
lation chosen was the one pointed out by the ma-
jority of the translators. When it was not possible 
to elect only one translation, the n equally most 
used were kept, and thus the sentence was re-
peated n times. 
3) Finally, we examined the relation between 
senses and translations, focusing on two cases: 
(1) if a sense had only one or many translations; 
and (2) if a translation referred to only one or 
many senses, i.e., whether the sense was shared 
by many translations. We placed each sense into 
two of the following categories, explained be-
low: (a) or (b), mutually exclusive, representing 
the first case; and (c), (d) or (e), also mutually 
exclusive, representing the second case. 
(a) 1 sense  1 translation: all the occur-
rences of the same sense being translated as 
the same Portuguese word. For example, ?to 
ask?, in the sense of ?inquire, enquire?, is al-
ways translated as ?perguntar?. 
(b) 1 sense  n translations: different oc-
currences of the same sense being translated as 
different, non-synonyms, Portuguese words. 
For example, ?to look?, in the sense of ?per-
ceive with attention; direct one's gaze to-
wards? can be translated as ?olhar?, ?assistir?, 
and ?voltar-se?. 
(c) n senses  1 translation (ambiguous): 
Different senses of a word being translated as 
the same Portuguese word, which encom-
passes all the English senses. For example, 
?make?, in the sense of ?engage in?, ?create?, 
and ?give certain properties to something?, is 
translated as ?fazer?, which carries the three 
senses. 
(d) n senses  1 translation (non-
ambiguous): different senses of a word being 
translated using the same Portuguese word, 
which has only one sense. For example, ?take 
advantage? in both the senses of ?draw advan-
tages from? and ?make excessive use of?, be-
ing translated as ?aproveitar-se?.  
(e) n senses  n translations: different 
senses of a word being translated as different 
Portuguese words. For example, the ?move 
fast? and ?carry out a process or program? 
senses of the verb ?run? being translated re-
spectively as ?correr? and ?executar?. 
Items (a) and (e) represent cases where multilin-
gual ambiguity only reflects the monolingual 
one, that is, to all the occurrences of every sense 
of an English word corresponds a specific Portu-
guese translation. On the other hand, items (b), 
(c) and (d) provide evidence that multilingual 
ambiguity is different from monolingual ambigu-
ity. Item (b) means that different criteria are 
needed for the disambiguation, as ambiguity 
arises only during the translation, due to specific 
principles used to distinguish senses in Portu-
guese. Items (c) and (d) mean that disambigua-
tion is not necessary, as either the Portuguese 
37
translation is also ambiguous, embracing the 
same senses of the English word, or Portuguese 
has a less refined sense distinction. 
5 Results and discussion 
Table 4 presents the number of different sen-
tences analyzed for each of the verbs (after 
grouping and eliminating the repeated sen-
tences), the English (E) senses and (non-
synonyms) Portuguese (P) translations in our 
corpus, followed by the percentage of occur-
rences of each of the categories outlined in Sec-
tion 4 (a ? e) with respect to the number of 
senses (# Senses) for that verb. Items (c) and (d) 
were grouped, since for practical purposes it is 
not important to tell if the P word translating the 
various E senses encompasses one or many 
senses. For items (b) and (c&d) we also present 
the average of P translations per E sense ((b) av-
erage), and the average of E senses per P transla-
tion, respectively ((c&d) average).  
We divided the analysis of these results ac-
cording to our two cases (cf. Section 4): the first 
covers items (c&d) and (e) (light grey in Table 
4), while the second covers items (a) and (b) 
(dark grey in Table 4). 
1) Items (c), (d) and (e): n senses ? ? transla-
tion(s) 
The number of senses in the corpus is almost 
always greater than the number of translations, 
suggesting that the level of sense distinctions in 
WordNet can be too fine-grained for translation 
applications The numbers of senses and transla-
tions are in an opposite relation comparing to the 
one shown in Table 3, where the number of pos-
sible translations was larger than the number of 
possible senses. This shows that indeed many of 
the possible translations are synonyms.  
On average, the level of ambiguity decreased 
from 40.3 (possible senses) to 24.4 (possible 
translations), if the monolingual and multilingual 
ambiguity are compared in the corpus. If we con-
sider the five most ambiguous verbs, the level of 
ambiguity decreased from 58.8 to 35. For the 
other three less ambiguous verbs, the level of 
ambiguity decreased from 9.3 to 6.7.  
Column % (c&d) shows the percentage of 
senses, with respect to the total shown in the 
third column (# Senses), which share translations 
with other senses. A shared translation means 
that several senses of the verb have the same 
translation. (c&d) average indicates the average 
number of E senses per P translation, for those 
cases where translations are shared. For all verbs, 
on average translations cover more than two 
senses. The level of variation in the number of 
shared translations among senses is high, e.g., 
from 2 (translation = ?organizar?) to 27 (transla-
tion = ?dar?) for the verb ?to give?. Contrasting 
the percentage of senses that share translations, 
in % (c), with the percentages in % (d), which 
refers to the senses for which translations are not 
shared, we can see that the great majority of 
senses have translations in common with other 
senses, and thus the disambiguation among these 
senses would not be necessary in most of the 
cases. In fact, it could result in errors, since an 
incorrect sense could be chosen. 
2) Items (a) and (b): 1 sense ? ? translation(s) 
As previously mentioned, the differences in the 
sense inventory for monolingual and multilingual 
WSD are not only due to the fact that sense dis-
tinctions in WordNet are too refined. That would 
only indicate that using monolingual WSD for 
multilingual purposes implies unnecessary work. 
However, we consider that the most important 
problem is the one evidenced by item (b) in the 
sixth column in Table 4. For all the verbs except 
?to ask? (the least ambiguous), there were cases 
in which different occurrences of the same sense 
were translated into different, non-synonyms 
words. Although the proportion of senses with 
only one translation is greater, as shown by item 
(a) in the fifth column, the percentage of senses 
with more than one translation is impressive, 
especially for the five most ambiguous verbs. In 
face of this, the lack of disambiguation of a word 
during translation based on the fact that the word 
is not ambiguous in the source language can re-
sult in very serious translation errors when 
monolingual methods are employed for multilin-
gual WSD. Therefore, this also shows that, for 
these verbs, sense inventories that are specific to 
the translation between the pair of languages un-
der consideration would be more appropriate to 
achieve effective WSD. 
5.1 Agreement between translators 
In an attempt to quantify the agreement between 
the two groups of translators, we computed the 
Kappa coefficient for annotation tasks, as de-
fined by Carletta (1996). Kappa was calculated 
separately for our two areas of inquiry, i.e., cases 
(1) and (2) discussed in Section 5.  
In the experiment referring to case (1), groups 
were considered to agree about a sense of a verb 
if they both judged that the translation of such 
38
Verb # Sen-
tences
# Senses # Transla-
tions 
% (a)  % (b) (b) av-
erage 
%  
(c&d) 
(c&d) av-
erage 
% (e) 
ask 83 8 3 100 0 0 87.5 3.5 12.5 
come 202 68 42 62 38 3.1 73.2 6.3 26.8 
get 226 90 61 70 30 2.6 61.1 3.4 38.9 
give 241 57 12 48.7 51.3 3.3 84.2 6.3 15.8 
live 55 10 7 83.3 16.7 3.0 70 2.7 30 
look 82 26 18 63.2 36.8 2.4 84.6 2.7 15.4 
make 225 53 42 51.4 48.6 2.9 77.4 4.1 22.6 
tell 73 10 10 37.5 62.5 2.8 60 4.0 40 
Table 4. Results of the procedure contrasting senses and translations 
verb was or was not shared by other senses. For 
example, both groups agreed that the word 
?fazer? should be used to translate occurrences 
of many senses of the verb ?to make?, including 
?engage in?, ?give certain properties to some-
thing?, and ?make or cause to be or to become?. 
On the other hand, the groups disagreed about 
the sense ?go off or discharge? of the phrasal 
verb ?to go off?: the first group found that the 
translation of that sense, ?disparar?, did not refer 
to any other sense, while the second group used 
that word to translate also the sense ?be dis-
charged or activated? of the same phrasal verb.  
In the experiment with case (2), groups were 
considered to agree about a sense if they both 
judged that the sense had or had not more than 
one translation. For example, both groups agreed 
that the sense ?reach a state, relation, or condi-
tion? of the verb ?to come? should be translated 
by more than one Portuguese word, including 
?terminar?, ?vir?, and ?chegar?. They also 
agreed that the sense ?move toward, travel to-
ward something or somebody or approach some-
thing or somebody? of the same verb had only 
one translation, namely ?vir?.  
The average Kappa coefficient obtained was 
0.66 for item (1), and 0.65 for item (2). There is 
not a reference value for this particular annota-
tion task (translation annotation), but the levels 
of agreement pointed by Kappa here can be con-
sidered satisfactory. The agreement levels are 
close to the coefficient suggested by Carletta as 
indicative of a good agreement level for dis-
course annotation (0.67), and which has been 
adopted as a cutoff in Computational Linguistics. 
6 Conclusions and future work 
We presented experiments contrasting monolin-
gual and multilingual WSD. It was found that, in 
fact, monolingual and multilingual disambigua-
tion differ in many respects, particularly the 
sense repository, and therefore specific strategies 
could be more appropriate to achieve effective 
multilingual WSD. We investigated the differ-
ences in sense repositories considering English-
Portuguese translation, using a set of eight am-
biguous verbs collected from sentences in Sem-
Cor and Senseval corpora. The English sense 
tags given by WordNet were compared to the 
Portuguese translations assigned by two groups 
of five human translators.  
Results corroborate previous cognate work, 
showing that there is not a one-to-one mapping 
between the English senses and their translations 
(to Portuguese, in this study). In most of the 
cases, many different senses were translated into 
the same Portuguese word. In many other cases, 
different, non-synonymous, words were neces-
sary to translate occurrences of the same sense of 
the source language, showing that differences 
between monolingual and multilingual WSD are 
not only a matter of the highly refined sense dis-
tinction criterion adopted in WordNet. Therefore, 
these results reinforce our argument that apply-
ing monolingual methods for multilingual WSD 
can either imply unnecessary work, or result in 
disambiguation errors.  
As future work we plan to carry out further in-
vestigation of the differences between monolin-
gual and multilingual WSD contrasting the Eng-
lish senses and translations into other languages, 
and analyzing other grammatical categories, par-
ticularly nouns.  
References  
Bentivogli, L., Forner, P., and Pianta, E. (2004). 
Evaluating Cross-Language Annotation Trans-
fer in the MultiSemCor Corpus. COLING-
2004, Geneva, pp. 364-370. 
Carletta, J. (1996). Assessing agreement on clas-
sification tasks: the kappa statistic. Computa-
tional Linguistics, 22(2), pp. 249-254. 
Carpuat, M. and Wu, D. (2005). Word sense dis-
ambiguation vs. statistical machine translation. 
43rd ACL Meeting, Ann Arbor, pp. 387?394. 
39
 Chatterjee, N., Goyal, S., and Naithani, A. 
(2005). Pattern Ambiguity and its Resolution 
in English to Hindi Translation. RANLP-2005, 
Borovets, pp. 152-156. 
Diab, M. and Resnik, P. (2002). An Unsuper-
vised Method for Word Sense Tagging using 
Parallel Corpora. 40th ACL Meeting, Philadel-
phia. 
Hutchins, W.J. and Somers H.L. (1992) An In-
troduction to Machine Translation. Academic 
Press, Great Britain. 
Ide, N. and V?ronis, J. (1998). Word Sense Dis-
ambiguation: The State of the Art. Computa-
tional Linguistics, 24 (1).  
Ide, N. (1999). Parallel Translations as Sense 
Discriminators. SIGLEX99 Workshop: Stan-
dardizing Lexical Resources, Maryland, pp. 
52-61. 
Ide, N., Erjavec, T., and Tufi, D. (2002). Sense 
Discrimination with Parallel Corpora. ACL'02 
Workshop on Word Sense Disambiguation: 
Recent Successes and Future Directions, 
Philadelphia, pp. 54-60.  
Kilgarriff, A. (1997). I Don't Believe in Word 
Senses. Computers and the Humanities, 31 
(2):91-113. 
Mih?ltz, M. (2005). Towards A Hybrid Ap-
proach to Word-Sense Disambiguation in Ma-
chine Translation. RANLP-2005 Workshop: 
Modern Approaches in Translation Technolo-
gies, Borovets.  
Miller, G.A., Beckwith, R.T., Fellbaum, C.D., 
Gross, D., and Miller, K. (1990). WordNet: 
An On-line Lexical Database. International 
Journal of Lexicography, 3(4):235-244. 
Miller, G.A., Chorodow, M., Landes, S., Lea-
cock, C., and Thomas, R.G. (1994). Using a 
Semantic Concordancer for Sense Identifica-
tion. ARPA Human Language Technology
Workshop - ACL, Washington, pp. 240-243. 
Montoyo, A., Romero, R., Vazquez, S., Calle, 
M., and Soler, S. (2002). The Role of WSD 
for Multilingual Natural Language Applica-
tions.  TSD?2002, Czech Republic, pp. 41-48. 
Ng, H.T., Wang, B., and Chan, Y.S. (2003). Ex-
ploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. 41st ACL 
Meeting, Sapporo, pp. 455-462.  
Palmer, M. (1998). Are WordNet sense distinc-
tions appropriate for computational lexicons? 
Senseval, Siglex98, Brighton.  
Resnik, P. and Yarowsky, D. (1997). A Perspec-
tive on Word Sense Disambiguation Methods 
and their Evaluating. ACL-SIGLEX Workshop 
Tagging Texts with Lexical Semantics: Why, 
What and How?, Washington. 
Specia, L. (2005). A Hybrid Model for Word 
Sense Disambiguation in English-Portuguese 
Machine Translation. 8th CLUK, Manchester, 
pp. 71-78. 
Vickrey, D., Biewald, L., Teyssier, M., and 
Koller, D. (2005). Word-Sense Disambigua-
tion for Machine Translation. HLT/EMNLP, 
Vancouver.
Vossen, P. (1998). EuroWordNet: Building a 
Multilingual Database with WordNets for 
European Languages. The ELRA Newsletter, 
3(1). 
Wilks, Y. and Stevenson, M. (1998). The Gram-
mar of Sense: Using Part-of-speech Tags as a 
First Step in Semantic Disambiguation. Natu-
ral Language Engineering, 4(1):1-9.  
40
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 442?445,
Prague, June 2007. c?2007 Association for Computational Linguistics
USP-IBM-1 and USP-IBM-2: The ILP-based Systems for Lexical Sample
WSD in SemEval-2007
Lucia Specia, Maria das Grac?as Volpe Nunes
ICMC - University of Sa?o Paulo
Trabalhador Sa?o-Carlense, 400, Sa?o Carlos, 13560-970, Brazil
{lspecia, gracan}@icmc.usp.br
Ashwin Srinivasan, Ganesh Ramakrishnan
IBM India Research Laboratory
Block 1, Indian Institute of Technology, New Delhi 110016, India
{ashwin.srinivasan, ganramkr}@in.ibm.com
Abstract
We describe two systems participating of the
English Lexical Sample task in SemEval-
2007. The systems make use of Inductive
Logic Programming for supervised learning
in two different ways: (a) to build Word
Sense Disambiguation (WSD) models from
a rich set of background knowledge sources;
and (b) to build interesting features from
the same knowledge sources, which are then
used by a standard model-builder for WSD,
namely, Support Vector Machines. Both sys-
tems achieved comparable accuracy (0.851
and 0.857), which outperforms considerably
the most frequent sense baseline (0.787).
1 Introduction
Word Sense Disambiguation (WSD) aims to iden-
tify the correct sense of ambiguous words in context.
Results from the last edition of the Senseval com-
petition (Mihalcea et al, 2004) have shown that, for
supervised learning, the best accuracies are obtained
with a combination of various types of features, to-
gether with traditional machine learning algorithms
based on feature-value vectors, such as Support Vec-
tor Machines (SVMs) and Naive Bayes. While the
features employed by these approaches are mostly
considered to be ?shallow?, that is, extracted from
corpus or provided by shallow syntactic tools like
part-of-speech taggers, it is generally thought that
significant progress in automatic WSD would re-
quire a ?deep? approach in which access to substan-
tial body of linguistic and world knowledge could
assist in resolving ambiguities. Although the ac-
cess to large amounts of knowledge is now possi-
ble due to the availability of lexicons like WordNet,
parsers, etc., the incorporation of such knowledge
has been hampered by the limitations of the mod-
elling techniques usually employed for WSD. Using
certain sources of information, mainly relational in-
formation, is beyond the capabilities of such tech-
niques, which are based on feature-value vectors.
Arguably, Inductive Logic Programming (ILP) sys-
tems provide an appropriate framework for dealing
with such data: they make explicit provisions for the
inclusion of background knowledge of any form; the
richer representation language used, based on first-
order logic, is powerful enough to capture contextual
relationships; and the modelling is not restricted to
being of a particular form (e.g., classification).
We describe the investigation of the use of ILP
for WSD in the Lexical Sample task of SemEval-
2007 in two different ways: (a) the construction of
models that can be used directly to disambiguate
words; and (b) the construction of interesting fea-
tures to be used by a standard feature-based algo-
rithm, namely, SVMs, to build disambiguation mod-
els. We call the systems resulting of the two differ-
ent approaches ?USP-IBM-1? and ?USP-IBM-2?,
respectively. The background knowledge is from 10
different sources of information extracted from cor-
pus, lexical resources and NLP tools.
In the rest of this paper we first present the spec-
ification of ILP implementations that construct ILP
models and features (Section 2) and then describe
the experimental evaluation on the SemEval-2007
Lexical Sample task data (Section 3).
442
2 Inductive Logic Programming
Inductive Logic Programming (ILP) (Muggleton,
1991) employs techniques from Machine Learning
and Logic Programming to build first-order theo-
ries or descriptions from examples and background
knowledge, which are also represented by first-order
clauses. Functionally, ILP can be characterised by
two classes of programs. The first, predictive ILP,
is concerned with constructing models (in this case,
sets of rules) for discriminating accurately amongst
positive and negative examples. The partial spec-
ifications provided by (Muggleton, 1994) form the
basis for deriving programs in this class:
? B is background knowledge consisting of a fi-
nite set of clauses = {C1, C2, . . .}
? E is a finite set of examples = E+?E? where:
? Positive Examples. E+ = {e1, e2, . . .} is
a non-empty set of definite clauses
? Negative Examples. E? = {f1, f2 . . .} is
a set of Horn clauses (this may be empty)
? H , the output of the algorithm given B and E,
is acceptable if these conditions are met:
? Prior Satisfiability. B ? E? 6|= 2
? Posterior Satisfiability. B ?H ?E? 6|= 2
? Prior Necessity. B 6|= E+
? Posterior Sufficiency. B ? H |= e1 ? e2 ?
. . .
The second category of ILP programs, descriptive
ILP, is concerned with identifying relationships that
hold amongst the background knowledge and exam-
ples, without a view of discrimination. The partial
specifications for programs in this class are based
on the description in (Muggleton and Raedt, 1994):
? B is background knowledge
? E is a finite set of examples (this may be
empty)
? H , the output of the algorithm given B and E
is acceptable if the following condition is met:
? Posterior Sufficiency. B ? H ? E 6|= 2
The intuition behind the idea of exploiting a
feature-based model constructor that uses first-order
features is that certain sources of structured infor-
mation that cannot be represented by feature vectors
can, by a process of ?propositionalization?, be iden-
tified and converted in a way that they can be accom-
modated in such vectors, allowing for traditional
learning techniques to be employed. Essentially, this
involve two steps: (1) a feature-construction step
that identifies all the features, that is, a set of clauses
H , that are consistent with the constraints provided
by the background knowledge B (descriptive ILP);
and (2) a feature-selection step that retains some of
the features based on their utility in classifying the
examples, for example, each clause must entail at
least one positive example (predictive ILP). In order
to be used by SVMs, each clause hi in H is con-
verted into a boolean feature fi that takes the value
1 (or 0) for any individual for which the body of
the clause is true (if the body is false). Thus, the
set of clauses H gives rise to a boolean vector for
each individual in the set of examples. The fea-
tures constructed may express conjunctions on dif-
ferent knowledge sources. For example, the follow-
ing boolean feature built from a clause for the verb
?ask? tests whether the sentence contains the expres-
sion ?ask out? and the word ?dinner?. More details
on the specifications of predictive and descriptive
ILP for WSD can be found in (Specia et al, 2007):
f1(X) =
{
1 expr(X, ?ask out?) ? bag(X,dinner) = 1
0 otherwise
3 Experiments
We investigate the performance of two kinds of ILP-
based models for WSD:
1. ILP models (USP-IBM-1 system): models con-
structed by an ILP system for predicting the
correct sense of a word.
2. ILP-assisted models (USP-IBM-2 system):
models constructed by SVMs for predicting the
correct sense of a word that, in addition to ex-
isting shallow features, use features built by an
ILP system according to the specification for
feature construction in Section 2.
443
The data for the English Lexical Sample task in
SemEval-2007 consists of 65 verbs and 35 nouns.
Examples containing those words were extracted
from the WSJ Penn Treebank II and Brown corpus.
The number of training / test examples varies from
19 / 2 to 2,536 / 541 (average = 222.8 / 48.5). The
senses of the examples were annotated according to
OntoNotes tags, which are groupings of WordNet
senses, and therefore are more coarse-grained. The
number of senses used in the training examples for
a given word varies from 1 to 13 (average = 3.6).
First-order clauses representing the following
background knowledge sources, which were au-
tomatically extracted from corpus and lexical re-
sources or provided by NLP tools, were used to de-
scribe the target words in both systems:
B1. Unigrams consisting of the 5 words to the
right and left of the target word.
B2. 5 content words to the right and left of the
target word.
B3. Part-of-speech tags of 5 words to the right and
left of the target word.
B4. Syntactic relations with respect to the target
word. If that word is a verb, subject and object syn-
tactic relations are represented. If it is a noun, the
representation includes the verb of which it is a sub-
ject or object, and the verb / noun it modifies.
B5. 12 collocations with respect to the target
word: the target word itself, 1st preposition to the
right, 1st and 2nd words to the left and right, 1st
noun, 1st adjective, and 1st verb to the left and right.
B6. A relative count of the overlapping words in
the sense inventory definitions of each of the pos-
sible senses of the target word and the words sur-
rounding that target word in the sentence, according
to the sense inventories provided.
B7. If the target word is a verb, its selectional
restrictions, defined in terms of the semantic fea-
tures of its arguments in the sentence, as given by
LDOCE. WordNet relations are used to make the
verification more generic and a hierarchy of feature
types is used to account for different levels of speci-
ficity in the restrictions.
B8. If the target word is a verb, the phrasal verbs
possibly occurring in a sentence, according to the
list of phrasal verbs given by dictionaries.
B9. Pairs of words in the sentence that occur fre-
quently in the corpus related by verb-subject/object
or subject/verb/object-modifier relations.
B10. Bigrams consisting of adjacent words in a
sentence occurring frequently in the corpus.
Of these 10 sources, B1?B6 correspond to the so
called ?shallow features?, in the sense that they can
be straightforwardly represented by feature vectors.
A feature vector representation of these sources is
built to be used by the feature-based model construc-
tor. Clausal definitions for B1?B10 are directly used
by the ILP system.
We use the Aleph ILP system (Srinivasan, 1999)
to construct disambiguation models in USP-IBM-1
and to construct features to be used in USP-IBM-
2. Feature-based model construction in USP-IBM-
2 system is performed by a linear SVM (the SMO
implementation in WEKA).
In the USP-IBM-1 system, for each target word,
equipped with examples and background knowl-
edge definitions (B1?B10), Aleph constructs a set
of clauses in line with the specifications for predic-
tive ILP described in Section 2. Positive examples
are provided by the correct sense of the target word.
Negative examples are generated automatically us-
ing all the other senses. 3-fold cross-validation on
the training data was used to obtain unbiased esti-
mates of the predictive accuracy of the models for a
set of relevant parameters. The best average accura-
cies were obtained with the greedy induction strat-
egy, in conjunction with a minimal clause accuracy
of 2. The constructed clauses were used to predict
the senses in the test data following the order of their
production, in a decision-list like manner, with the
addition to the end of a default rule assigning the
majority sense for those cases which are not covered
by any other rule.
In the USP-IBM-2 system, for constructing the
?good? features for each target word from B1?
B10 (the ?ILP-based features?), we first selected, in
Aleph, the clauses covering at least 1 positive exam-
ple. 3-fold cross-validation on the training data was
performed in order to obtain the best model possi-
ble using SVM with features in B1?B6 and the ILP-
based features. A feature selection method based
on information gain with various percentages of fea-
tures to be selected (1/64, ..., 1/2) was used, which
resulted in different numbers of features for each tar-
get word.
444
Baseline USP-IBM-1 USP-IBM-2
Nouns 0.809 0.882 0.882
Verbs 0.762 0.817 0.828
All 0.787 0.851 0.857
Table 1: Average accuracies of the ILP-based mod-
els for different part-of-speeches
Table 1 shows the average accuracy of a base-
line classifier that simply votes for the most frequent
sense of each word in the training data against the
accuracy of our ILP-based systems, USP-IBM-1 and
USP-IBM-2, according to the part-of-speech of the
target word, and for all words. Clearly, the ?ma-
jority class? classifier performs poorest, on average.
The difference between both ILP-based systems and
the baseline is statistically significant according to
a paired t-test with p < 0.01. The two ILP-based
models appear to be comparable in their average ac-
curacy. Discarding ties, IBM-USP-2 outperforms
IBM-USP-1 for 31 of the words, but the advantage
is not statistically significant (cf. paired t-test).
The low accuracy of the ILP-based systems for
certain words may be consequence of some charac-
teristics of the data. In particular, the sense distri-
butions are very skewed in many cases, with differ-
ent distributions in the training and test data. For
example, in the case of ?care? (accuracy = 0.428),
the majority sense in the training data is 1 (78.3%),
while in the test data the majority sense is 2 (71%).
In cases like this, many of the test examples remain
uncovered by the rules produced by the ILP system
and backing off to the majority sense also results in
a mistake, since the majority sense in the training
data does not apply for most of the test examples.
The same goes for the feature-based system: fea-
tures which are relevant for the test examples will
not be built or selected.
One relevant feature of ILP is its ability to pro-
duce expressive symbolic models. These models
can reproduce any kind of background knowledge
using sets of rules testing conjunctions of different
types of knowledge, which may include variables
(intensional clauses). This is valid both for the con-
struction of predictive models and for the construc-
tion of features (which are derived from the clauses).
Examples of rules induced for the verb ?come? are
given in Figure 1. The first rule states that the sense
sense(X, 3) :-
expr(X, ?come to?).
sense(X, 1) :-
satisfy restrictions(X, [animate], nil);
(relation(X, subj, B), pos(X, B, nnp)).
Figure 1: Examples of rules learned for ?come?
of the verb in a sentence X will be 3 (progress to a
state) if that sentence contains the expression ?come
to?. The second rule states that the sense of the verb
will be 1 (move, travel, arrive) if its subject is ?ani-
mate? and there is no object, or if it has has a subject
B that is a proper noun (nnp).
4 Concluding Remarks
We have investigated the use of ILP as a mech-
anism for incorporating shallow and deep knowl-
edge sources into the construction of WSD mod-
els for the Semeval-2007 Lexical Sample Task data.
Results consistently outperform the most frequent
sense baseline. It is worth noticing that the knowl-
edge sources used here were initially designed for
the disambiguation of verbs (Specia et al, 2007)
and therefore we believe that further improvements
could be achieved with the identification and speci-
fication of other sources which are more appropriate
for the disambiguation of nouns.
References
R. Mihalcea, T. Chklovski, A. Kilgariff. 2004.
The SENSEVAL-3 English Lexical Sample Task.
SENSEVAL-3: 3rd Int. Workshop on the Evaluation of
Systems for Semantic Analysis of Text, 25?28.
S. Muggleton. 1991. Inductive Logic Program-ming.
New Generation Computing, 8(4):29-5-318.
S. Muggleton. 1994. Inductive Logic Programming:
derivations, successes and shortcomings. SIGART Bul-
letin, 5(1):5?11.
S. Muggleton and L. D. Raedt. 1994. Inductive logic
programming: Theory and methods. Journal of Logic
Programming, 19,20:629?679.
L. Specia, M.G.V. Nunes, A. Srinivasan, G. Ramakrish-
nan. 2007. Word Sense Disambiguation using Induc-
tive Logic Programming. Proceedings of the 16th In-
ternational Conference on ILP, Springer-Verlag.
A. Srinivasan. 1999. The Aleph Manual. Computing
Laboratory, Oxford University.
445
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 34?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Supporting the Adaptation of Texts for Poor Literacy Readers: a Text 
Simplification Editor for Brazilian Portuguese 
 
Arnaldo Candido Jr., Erick Maziero, Caroline Gasperin, Thiago A. S. Pardo, Lucia Specia, and Sandra M. Aluisio  
 
Center of Computational Linguistics (NILC) / Department of Computer Sciences, University of S?o Paulo 
Av. Trabalhador S?o-Carlense, 400. 13560-970 - S?o Carlos/SP, Brazil 
arnaldoc@icmc.usp.br, egmaziero@gmail.com, {cgasperin,taspardo,lspecia,sandra}@icmc.usp.br 
 
Abstract 
In this paper we investigate the task of text 
simplification for Brazilian Portuguese. Our 
purpose is three-fold: to introduce a 
simplification tool for such language and its 
underlying development methodology, to 
present an on-line authoring system of 
simplified text based on the previous tool, and 
finally to discuss the potentialities of such 
technology for education. The resources and 
tools we present are new for Portuguese and 
innovative in many aspects with respect to 
previous initiatives for other languages.  
1 Introduction 
In Brazil, according to the index used to measure 
the literacy level of the population (INAF - National 
Indicator of Functional Literacy), a vast number of 
people belong to the so called rudimentary and basic 
literacy levels. These people are only able to find 
explicit information in short texts (rudimentary 
level) or process slightly longer texts and make 
simple inferences (basic level). INAF reports that 
68% of the 30.6 million Brazilians between 15 and 
64 years who have studied up to 4 years remain at 
the rudimentary literacy level, and 75% of the 31.1 
million who studied up to 8 years remain at the 
rudimentary or basic levels. 
Reading comprehension entails three elements: 
the reader who is meant to comprehend; the text that 
is to be comprehended and the activity in which 
comprehension is a part of (Snow, 2002). In 
addition to the content presented in the text, the 
vocabulary load of the text and its linguistic 
structure, discourse style, and genre interact with the 
reader?s knowledge. When these factors do not 
match the reader?s knowledge and experience, the 
text becomes too complex for the comprehension to 
occur. In this paper we will focus on the text and the 
aspects of it that make reading difficult or easy. One 
solution to ease the syntactic structure of a text is 
via Text Simplification (TS) facilities.  
TS aims to maximize the comprehension of 
written texts through the simplification of their 
linguistic structure. This may involve simplifying 
lexical and syntactic phenomena, by substituting 
words that are only understood by a few people with 
words that are more usual, and by breaking down 
and changing the syntactic structure of the sentence, 
respectively. As a result, it is expected that the text 
can be more easily understood both by humans and 
computer systems (Mapleson, 2006; Siddharthan, 
2003, Max, 2006). TS may also involve dropping 
parts or full sentences and adding some extra 
material to explain a difficult point. This is the case, 
for example, of the approach presented by Petersen 
and Ostendorf (2007), in which abridged versions of 
articles are used in adult literacy learning. 
It has already been shown that long sentences, 
conjoined sentences, embedded clauses, passives, 
non-canonical word order, and use of low-frequency 
words, among other things, increase text complexity 
for language-impaired readers (Siddharthan, 2002; 
Klebanov et al, 2004; Devlin and Unthank, 2006). 
The Plain English initiative makes available 
guidelines to make texts easier to comprehend: the 
Plain Language1. In principle, its recommendations 
can be applied to any language. Although some of 
them are directly useful for TS systems (e.g., 
subject-verb-object order and active voice), others 
are difficult to specify (e.g., how simple each 
syntactic construction is and which words are 
simple). 
In this paper we present the results of a study of 
syntactic simplification for Brazilian Portuguese 
(BP) and a rule-based syntactic simplification 
system for this language that was developed based 
on this study ? the first of this kind for BP. We also 
present an on-line authoring tool for creating 
simplified texts. One possible application of this 
tool is to help teachers to produce instructional texts 
                                                 
1
 http://www.plainlanguage.gov 
34
to be used in classrooms. The study is part of the 
PorSimples project2 (Simplification of Portuguese 
Text for Digital Inclusion and Accessibility), which 
aims at producing text simplification tools for 
promoting digital inclusion and accessibility for 
people with different levels of literacy, and possibly 
other kinds of reading disabilities. 
This paper is organized as follows. In Section 2 
we present related approaches for text simplification 
with educational purposes. In Section 3 we describe 
the proposed approach for syntactic simplification, 
which is used within an authoring tool described in 
Section 4. In Section 5 we discuss possible uses of 
text simplification for educational purposes.  
2 Related work 
Burstein (2009) presents an NLP-based application 
for educational purposes, named Text Adaptor, 
which resembles our authoring tool. It includes 
complex sentence highlighting, text elaboration 
(word substitutions by easier ones), text 
summarization and translation. The system does not 
perform syntactic simplification, but simply 
suggests, using a shallow parser, that some 
sentences might be too complex. Specific hints on 
the actual source of complexity are not provided. 
Petersen (2007) addresses the task of text 
simplification in the context of second-language 
learning. A data-driven approach to simplification is 
proposed using a corpus of paired articles in which 
each original sentence does not necessarily have a 
corresponding simplified sentence, making it 
possible to learn where writers have dropped or 
simplified sentences. A classifier is used to select 
the sentences to simplify, and Siddharthan?s 
syntactic simplification system (Siddharthan, 2003) 
is used to split the selected sentences. In our 
approach, we do not drop sentences, since we 
believe that all the content must be kept in the text. 
Siddharthan proposes a syntactic simplification 
architecture that relies on shallow text analysis and 
favors time performance. The general goal of the 
architecture is to make texts more accessible to a 
broader audience; it has not targeted any particular 
application. The system treats apposition, relative 
clauses, coordination and subordination. Our 
method, on the other hand, relies on deep parsing 
(Bick, 2000). We treat the same phenomena as 
                                                 
2
 http://caravelas.icmc.usp.br/wiki/index.php/Principal 
Siddharthan, but also deal with Subject-Verb-Object 
ordering (in Portuguese sentences can be written in 
different orders) and passive to active voice 
conversion. Siddharthan's system deals with non-
finite clauses which are not handled by our system 
at this stage. 
Lal and Ruger?s (2002) created a bayesian 
summarizer with a built-in lexical simplification 
module, based on WordNet and MRC psycho-
linguistic database3. The system focuses on 
schoolchildren and provides background 
information about people and locations in the text, 
which are retrieved from databases. Our rule-based 
simplification system only replaces discourse 
markers for more common ones using lexical 
resources built in our project, instead of inserting 
additional information in the text. 
Max (2005, 2006) applies text simplification in 
the writing process by embedding an interactive text 
simplification system into a word processor. At the 
user?s request, an automatic parser analyzes an 
individual sentence and the system applies 
handcrafted rewriting rules. The resulting suggested 
simplifications are ranked by a score of syntactic 
complexity and potential change of meaning. The 
writer then chooses their preferred simplification. 
This system ensures accurate output, but requires 
human intervention at every step. Our system, on 
the other hand, is autonomous, even though the user 
is able to undo any undesirable simplification or to 
choose alternative simplifications. These alternative 
simplifications may be produced in two cases: i) to 
compose a new subject in simplifications involving 
relatives and appositions and ii) to choose among 
one of the coordinate or subordinate simplifications 
when there is ambiguity regarding to conjunctions. 
Inui et al (2003) proposes a rule-based system 
for text simplification aimed at deaf people. The 
authors create readability assessments based on 
questionnaires answered by teachers about the deaf. 
With approximately one thousand manually created 
rules, the authors generate several paraphrases for 
each sentence and train a classifier to select the 
simpler ones. Promising results are obtained, 
although different types of errors on the paraphrase 
generation are encountered, such as problems with 
verb conjugation and regency. In our work we 
produce alternative simplifications only in the two 
cases explained above. 
                                                 
3
 http://www.psych.rl.ac.uk/ 
35
Caseli et al (2009) developed an annotation 
editor to support the building of parallel corpora of 
original and simplified texts in Brazilian 
Portuguese. The tool was used to build a corpus of 
simplified texts aimed at people with rudimentary 
and basic literacy levels. We have used the parallel 
corpus to evaluate our rule-based simplification 
system. The on-line authoring system presented in 
this paper evolved from this annotation editor. 
There are also commercial systems like Simplus4 
and StyleWriter5, which aim to support Plain 
English writing.  
3 A rule-based syntactic simplification 
system 
Our text simplification system comprises seven 
operations (see Sections 3.1 and 3.2), which are 
applied to a text in order to make its syntactic 
structure simpler. These operations are applied 
sentence by sentence, following the 3-stage 
architecture proposed by Siddharthan (2002), which 
includes stages of analysis, transformation and 
regeneration. In Siddharthan?s work, the analysis 
stage performs the necessary linguistic analyses of 
the input sentences, such as POS tagging and 
chunking; the transformation stage applies 
simplification rules, producing simplified versions 
of the sentences; the regeneration stage performs 
operations on the simplified sentences to make them 
readable, like referring expressions generation, cue 
words rearrangement, and sentence ordering. 
Differently from such architecture, currently our 
regeneration stage only includes the treatment of 
cue words and a surface forms (GSF) generator, 
which is used to adjust the verb conjugation and 
regency after some simplification operations. 
As a single sentence may contain more than 
one complex linguistic phenomenon, simplification 
operations are applied in cascade to a sentence, as 
described in what follows. 
3.1 Simplification cases and operations 
As result of a study on which linguistic phenomena 
make BP text complex to read and how these 
phenomena could be simplified, we elaborated a 
manual of BP syntactic simplification (Aluisio et al, 
2008). The rule-based text simplification system 
                                                 
4
 http://www.linguatechnologies.com/english/home.html  
5
 http://www.editorsoftware.com/writing-software 
developed here is based on the specifications in this 
manual. According to this manual, simplification 
operations should be applied when any of the 22 
linguistic phenomena presented in Table 1 is 
detected. 
The possible operations suggested to be applied 
in order to simplify these phenomena are: (a) split 
the sentence, (b) change a discourse marker by a 
simpler and/or more frequent one (the indication is 
to avoid the ambiguous ones), (c) change passive to 
active voice, (d) invert the order of the clauses, (e) 
convert to subject-verb-object ordering, (f) change 
topicalization and detopicalization of adverbial 
phrases and (g) non-simplification.  
Table 1 shows the list of all simplification 
phenomena covered by our manual, the clues used 
to identify the phenomena, the simplification 
operations that should be applied in each case, the 
expected order of clauses in the resulting sentence, 
and the cue phrases (translated here from 
Portuguese) used to replace complex discourse 
markers or to glue two sentences. In column 2, we 
consider the following clues: syntactic information 
(S), punctuation (P), and lexicalized clues, such as 
conjunctions (Cj), relative pronouns (Pr) and 
discourse markers (M), and semantic information 
(Sm, and NE for named entities). 
3.2 Identifying simplification cases and 
applying simplification rules 
Each sentence is parsed in order to identify cases for 
simplification. We use parser PALAVRAS (Bick, 
2000) for Portuguese. This parser provides lexical 
information (morphology, lemma, part-of-speech, 
and semantic information) and the syntactic trees for 
each sentence. For some operations, surface 
information (such as punctuation or lexicalized cue 
phrases) is used to identify the simplification cases, 
as well as to assist simplification process. For 
example, to detect and simplify subjective non-
restrictive relative clauses (where the relative 
pronoun is the subject of the relative clause), the 
following steps are performed: 
1. The presence of a relative pronoun is verified. 
2. Punctuation is verified in order to distinguish it 
from restrictive relative clauses: check if the 
pronoun occurs after a comma or semicolon.  
3. Based on the position of the pronoun, the next 
punctuation symbol is searched to define the 
boundaries of the relative clause. 
36
4. The first part of the simplified text is generated, 
consisting of the original sentence without the 
embedded relative clause. 
5. The noun phrase in the original sentence to 
which the relative clause refers is identified. 
6. A second simplified sentence is generated, 
consisting of the noun phrase (as subject) and 
the relative clause (without the pronoun). 
The identification of the phenomena and the 
application of the operations are prone to errors 
though. Some of the clues that indicate the 
occurrence of the phenomena may be ambiguous. 
For example, some of the discourse markers that are 
used to identify subordinate clauses can indicate 
more than one type of these: for instance, ?como? 
(in English ?like?, ?how? or ?as?) can indicate 
reason, conformative or concessive subordinate 
clauses. Since there is no other clue that can help us 
disambiguate among those, we always select the 
case that occurs more frequently according to a 
corpus study of discourse markers and the rhetoric 
relations that they entitle (Pardo and Nunes, 2008). 
However, we can also treat all cases and let the user 
decide the simplifications that is most appropriate. 
 
Phenomenon Clues Op Clause Order Cue phrase Comments 
1.Passive voice S c   Verb may have to be adapted 
2.Embedded appositive S a Original/ 
App. 
 Appositive: Subject is the head of original + 
to be in present tense + apposition 
3.Asyndetic coordinate clause S a Keep order   New sentences: Subjects are the head of the 
original subject 
4.Additive coordinate clause S, Cj a Keep order Keep marker Marker appears in the beginning of the new 
sentence 
5.Adversative coordinate clause M a, b Keep order But  
6.Correlated coordinate clause M a, b Keep order Also Original markers disappear 
7.Result coordinate clause S, M a, b Keep order As a result  
8.Reason coordinate clause S, M a, b Keep order This happens 
because 
May need some changes in verb 
9.Reason subordinate clause M a, b, 
d 
Sub/Main With this To keep the ordering cause, result 
M a, b Main/Sub Also Rule for such ... as, so ... as markers  10.Comparative subordinate clause 
M g   Rule for the other markers or short sentences 
M a, b, 
d 
Sub/Main But ?Clause 1 although clause 2? is changed to 
?Clause 2. But clause 1? 
11.Concessive subordinate clause 
M a, b Main/Sub This happens 
even if 
Rule for hypothetical sentences 
12.Conditional subordinate clause S, M d Sub/Main  Pervasive use in simple accounts 
13. Result subordinate clause M a, b Main/Sub Thus May need some changes in verb 
14.Final/Purpose subordinate clause S, M a, b Main/Sub The goal is  
15.Confirmative subordinate clause M a, b, 
d 
Sub/Main Confirms 
that 
May need some changes in verb 
M a Sub/Main  May need some changes in verb 16.Time subordinate clause 
M a, b  Then Rule for markers: after that, as soon as  
17. Proportional Subordinate Clause M g    
18. Non-finite subordinate clause S g    
19.Non-restrictive relative clause S, P, Pr a Original/ 
Relative 
 Relative: Subject is the head of original + 
relative (subjective relative clause) 
20.Restrictive relative clause S, Pr a Relative/ 
Original 
 Relative: Subject is the head of original + 
relative  (subjective relative clause) 
21.Non Subject-Verb-Object order S e   Rewrite in Subject-Verb-Object order 
22. Adverbial phrases in theme 
position 
S, NE, 
Sm  
f In study  In study 
Table 1: Cases, operations, order and cue phrases 
Every phenomenon has one or more 
simplification steps associated with it, which are 
applied to perform the simplification operations. 
Below we detail each operation and discuss the 
challenges involved and our current limitations in 
their implementing. 
a) Splitting the sentence - This operation is the 
most frequent one. It requires finding the split point 
37
in the original sentence (such as the boundaries of 
relative clauses and appositions, the position of 
coordinate or subordinate conjunctions) and the 
creation of a new sentence, whose subject 
corresponds to the replication of a noun phrase in 
the original sentence. This operation increases the 
text length, but decreases the length of the 
sentences. With the duplication of the term from the 
original sentence (as subject of the new sentence), 
the resulting text contains redundant information, 
but it is very helpful for people at the rudimentary 
literacy level. 
When splitting sentences due to the presence of 
apposition, we need to choose the element in the 
original sentence to which it is referring, so that this 
element can be the subject of the new sentence. At 
the moment we analyze all NPs that precede the 
apposition and check for gender and number 
agreement. If more than one candidate passes the 
agreement test, we choose the closest one among 
these; if none does, we choose the closest among all 
candidates. In both cases we can also pass the 
decision on to the user, which we do in our 
authoring tool described in Section 4. 
For treating relative clauses we have the same 
problem as for apposition (finding the NP to which 
the relative clause is anchored) and an additional 
one: we need to choose if the referent found should 
be considered the subject or the object of the new 
sentence. Currently, the parser indicates the 
syntactic function of the relative pronoun and that 
serves as a clue. 
b) Changing discourse marker - In most cases 
of subordination and coordination, discourse 
markers are replaced by most commonly used ones, 
which are more easily understood. The selection of 
discourse markers to be replaced and the choice of 
new markers (shown in Table 1, col. 4) are done 
based on the study of Pardo and Nunes (2008). 
c) Transformation to active voice - Clauses in 
the passive voice are turned into active voice, with 
the reordering of the elements in the clause and the 
modification of the tense and form of the verb. Any 
other phrases attached to the object of the original 
sentence have to be carried with it when it moves to 
the subject position, since the voice changing 
operation is the first to be performed. For instance, 
the sentence: 
?More than 20 people have been bitten by gold piranhas 
(Serrasalmus Spilopleura), which live in the waters of the 
Sanchuri dam, next to the BR-720 highway, 40 km from 
the city.? 
is simplified to: 
?Gold piranhas (Serrasalmus Spilopleura), which live in 
the waters of the Sanchuri dam, next to the BR-720 
highway, 40 km from the city, have bitten more than 20 
people.? 
After simplification of the relative clause and 
apposition, the final sentence is: 
?Gold piranhas have bitten more than 20 people. Gold 
piranhas live in the waters of the Sanchuri dam, next to 
the BR-720 highway, 40 km from the city. Gold piranhas 
are Serrasalmus Spilopleura.? 
d) Inversion of clause ordering - This operation 
was primarily designed to handle subordinate 
clauses, by moving the main clause to the beginning 
of the sentence, in order to help the reader 
processing it on their working memory (Graesser et 
al., 2004). Each of the subordination cases has a 
more appropriate order for main and subordinate 
clauses (as shown in Table 1, col. 3), so that 
?independent? information is placed before the 
information that depends on it. In the case of 
concessive subordinate clauses, for example, the 
subordinate clause is placed before the main clause. 
This gives the sentence a logical order of the 
expressed ideas. See the example below, in which 
there is also a change of discourse marker and 
sentence splitting, all operations assigned to 
concessive subordinate clauses:  
?The building hosting the Brazilian Consulate was also 
evacuated, although the diplomats have obtained 
permission to carry on working.? 
Its simplified version becomes:  
?The diplomats have obtained permission to carry on 
working. But the building hosting the Brazilian Consulate 
was also evacuated.? 
e) Subject-Verb-Object ordering - If a sentence 
is not in the form of subject-verb-object, it should be 
rearranged. This operation is based only on 
information from the syntactic parser. The example 
below shows a case in which the subject is after the 
verb (translated literally from Portuguese, 
preserving the order of the elements): 
?On the 9th of November of 1989, fell the wall that for 
almost three decades divided Germany.? 
Its simplified version is: 
?On the 9th of November of 1989, the wall that for almost 
three decades divided Germany fell.? 
Currently the only case we are treating is the non-
canonical order Verb-Object-Subject. We plan to 
treat other non-canonical orderings in the near 
future. Besides that, we still have to define how to 
deal with elliptic subjects and impersonal verbs 
(which in Portuguese do not require a subject). 
38
When performing this operation and the previous 
one, a generator of surface forms (GSF) is used to 
adjust the verb conjugation and regency. The GSF is 
compiled from the Apertium morphological 
dictionaries enhanced with the entries of Unitex-BP 
(Muniz et al, 2005), with an extra processing to 
map the tags of the parser to those existing in 
morphological dictionaries (Caseli et al, 2007) to 
obtain an adjusted verb in the modified sentence. 
f) Topicalization and detopicalization - This 
operation is used to topicalize or detopicalize an 
adverbial phrase. We have not implemented this 
operation yet, but have observed that moving 
adverbial phrases to the end or to the front of 
sentences can make them simpler in some cases. For 
instance, the sentence in the last example would 
become: 
?The wall that for almost three decades divided Germany fell 
on the 9th of November of 1989.? 
We are still investigating how this operation 
could be applied, that is, which situations require 
(de)topicalization. 
3.3 The cascaded application of the rules 
As previously mentioned, one sentence may contain 
several phenomena that could be simplified, and we 
established the order in which they are treated. The 
first phenomenon to be treated is passive voice. 
Secondly, embedded appositive clauses are 
resolved, since they are easy to simplify and less 
prone to errors. Thirdly, subordinate, non-restrictive 
and restrictive relative clauses are treated, and only 
then the coordinate clauses are dealt with.  
As the rules were designed to treat each case 
individually, it is necessary to apply the operations 
in cascade, in order to complete the simplification 
process for each sentence. At each iteration, we (1) 
verify the phenomenon to be simplified following 
the standard order indicated above; (2) when a 
phenomenon is identified, its simplification is 
executed; and (3) the resulting simplified sentence 
goes through a new iteration. This process continues 
until there are no more phenomena. The cascade 
nature of the process is crucial because the 
simplified sentence presents a new syntactic 
structure and needs to be reparsed, so that the 
further simplification operations can be properly 
applied. However, this process consumes time and 
is considered the bottleneck of the system.  
3.4 Simplification evaluation 
We have so far evaluated the capacity of our rule-
based simplifier to identify the phenomena present 
in each sentence, and to recommend the correct 
simplification operation. We compared the 
operations recommended by the system with the 
ones performed manually by an annotator in a 
corpus of 104 news articles from the Zero Hora 
newspaper, which can be seen in our Portal of 
Parallel Corpora of Simplified Texts6. Table 2 
presents the number of occurrences of each 
simplification operation in this corpus. 
Simplification Operations # Sentences 
Non-simplification 2638 
Subject-verb-object ordering 44 
Transformation to active voice 154 
Inversion of clause ordering 265 
Splitting sentences 1103 
Table 2. Statistics on the simplification operations 
The performance of the system for this task is 
presented in Table 3 in terms of precision, recall, 
and F-measure for each simplification operation.  
Operation P R F 
Splitting sentences 64.07 82.63 72.17 
Inversion of clause ordering 15.40 18.91 16.97 
Transformation to active voice 44.29 44.00 44.14 
Subject-verb-object ordering 1.12 4.65 1.81 
ALL 51.64 65.19 57.62 
Non-simplification 64.69 53.58 58.61 
Table 3. Performance on defining simplification 
operations according to syntactic phenomena 
These results are preliminary, since we are still 
refining our rules. Most of the recall errors on the 
inversion of clause ordering are due to the absence 
of a few discourse markers in the list of markers that 
we use to identify such cases. The majority of recall 
errors on sentence splitting are due to mistakes on 
the output of the syntactic parser and to the number 
of ordering cases considered and implemented so 
far. The poor performance for subject-verb-object 
ordering, despite suffering from mistakes of the 
parser, indicates that our rules for this operation 
need to be refined. The same applies to inversion of 
clause ordering. 
We did not report performance scores related to 
the ?changing discourse marker? operation because 
in our evaluation corpus this operation is merged 
with other types of lexical substitution. However, in  
                                                 
6
 http://caravelas.icmc.usp.br/portal/index.php 
39
order to assess if the sentences were correctly 
simplified, it is necessary to do a manual evaluation, 
since it is not possible to automatically compare the 
output of the rule-based simplifier with the 
annotated corpus, as the sentences in the corpus 
have gone through operations that are not performed 
by the simplifier (such as lexical substitution). We 
are in the process of performing such manual 
evaluation. 
4 Simplifica editor: supporting authors 
We developed Simplifica7 (Figure 1), an authoring 
system to help writers to produce simplified texts. It 
employs the simplification technology described in 
the previous section. It is a web-based WYSIWYG 
editor, based on TinyMCE web editor8.  
The user inputs a text in the editor, customizes 
the simplification settings where one or more 
simplifications can be chosen to be applied in the 
text and click on the ?simplify? button. This triggers 
the syntactic simplification system, which returns an 
XML file containing the resulting text and tags 
indicating the performed simplification operations. 
After that, the simplified version of the text is 
shown to the user, and he/she can revise the 
automatic simplification. 
4.1 The XML representation of simplification 
operations 
Our simplification system generates an XML file 
                                                 
7
 http://www.nilc.icmc.usp.br/porsimples/simplifica/ 
8
 http://tinymce.moxiecode.com/ 
describing all simplification operations applied to a 
text. This file can be easily parsed using standard 
XML parsers. Table 5 presents the XML annotation 
to the ?gold piranhas? example in Section 3.2. 
  
<simplification type="passive"> 
<simplification type="appositive"> 
<simplification type="relative"> 
Gold piranhas have bitten more than 20 people. Gold 
piranhas live in the waters of the Sanchuri dam, next to 
the BR-720 highway, 40 km from the city. 
</simplification> 
Gold piranhas are Serrasalmus Spilopleura. 
</simplification> 
</simplification> 
Table 5. XML representation of a simplified text 
In our annotation, each sentence receives a 
<simplification> tag which describes the simplified 
phenomena (if any); sentences that did not need 
simplification are indicated with a <simplification 
type=?no?> tag. The other simplification types refer 
to the eighteen simplification cases presented in 
Table 1. Nested tags indicate multiple operations 
applied to the same sentence. 
4.2 Revising the automatic simplification 
Once the automatic simplification is done, a review 
screen shows the user the simplified text so that 
he/she can visualize all the modifications applied 
and approve or reject them, or select alternative 
simplifications. Figure 1 shows the reviewing screen 
and a message related to the simplification 
performed below the text simplified. 
The user can revise simplified sentences one at a 
time; the selected sentence is automatically 
highlighted. The user can accept or reject a 
 
Figure 1: Interface of the Simplifica system 
40
simplified sentence using the buttons below the text. 
In the beginning of the screen ?Mais op??es?, 
alternative simplifications for the sentence are 
shown: this facility gives the user the possibility to 
resolve cases known to be ambiguous (as detailed in 
Sections 2 and 3.2) for which the automatic 
simplification may have made a mistake. In the 
bottom of the same screen we can see the original 
sentence (?Senten?a original?) to which the 
highlighted sentence refers.  
For the example in Figure 1, the tool presents 
alternative simplifications containing different 
subjects, since selecting the correct noun phrase to 
which an appositive clause was originally linked 
(which becomes the subject of the new sentence) 
based on gender and number information was not 
possible.  
At the end of the process, the user returns to the 
initial screen and can freely continue editing the text 
or adding new information to it. 
5 Text Simplification for education 
Text simplification can be used in several 
applications. Journalists can use it to write simple 
and straightforward news texts. Government 
agencies can create more accessible texts to a large 
number of people. Authors of manuals and technical 
documents can also benefit from the simplification 
technology. Simplification techniques can also be 
used in an educational setting, for example, by a 
teacher who is creating simplified texts to students. 
Classic literature books, for example, can be quite 
hard even to experienced readers. Some genres of 
texts already have simplified versions, even though 
the simplification level can be inadequate to a 
specific target audience. For instance, 3rd and 7th 
grade students have distinct comprehension levels. 
In our approach, the number and type of 
simplification operations applied to sentences 
determine its appropriateness to a given literacy 
level, allowing the creation of multiple versions of 
the same text, with different levels of complexity, 
targeting special student needs. 
The Simplifica editor allows the teacher to adopt 
any particular texts to be used in the class, for 
example, the teacher may wish to talk about current 
news events with his/her students, which would not 
be available via any repository of simplified texts. 
The teacher can customize the text generating 
process and gradually increase the text complexity 
as his/her students comprehension skills evolve. The 
use of the editor also helps the teacher to develop a 
special awareness of the language, which can 
improve his/her interaction with the students.  
Students can also use the system whenever they 
have difficulties to understand a text given in the 
classroom. After a student reads the simplified text, 
the reading of the original text becomes easier, as a 
result of the comprehension of the simplified text. In 
this scenario, reading the original text can also help 
the students to learn new and more complex words 
and syntactic structures, which would be harder for 
them without reading of the simplified text. 
6 Conclusions 
The potentialities of text simplification systems for 
education are evident. For students, it is a first step 
for more effective learning. Under another 
perspective, given the Brazilian population literacy 
levels, we consider text simplification a necessity. 
For poor literacy people, we see text simplification 
as a first step towards social inclusion, facilitating 
and developing reading and writing skills for people 
to interact in society. The social impact of text 
simplification is undeniable. 
In terms of language technology, we not only 
introduced simplification tools in this paper, but also 
investigated which linguistic phenomena should be 
simplified and how to simplify them. We also 
developed a representation schema and designed an 
on-line authoring system. Although some aspects of 
the research are language dependent, most of what 
we propose may be adapted to other languages. 
Next steps in this research include practical 
applications of such technology and the 
measurement of its impact for both education and 
social inclusion. 
Acknowledgments 
We thank the Brazilian Science Foundation FAPESP 
and Microsoft Research for financial support. 
References 
Alu?sio, S.M., Specia, L., Pardo, T.A.S., Maziero, E.G., 
Fortes, R. 2008. Towards Brazilian Portuguese 
Automatic Text Simplification Systems. In the 
Proceedings of the 8th ACM Symposium on Document 
Engineering, pp. 240-248. 
Bick, E. 2000. The parsing system ?Palavras?: 
41
Automatic grammatical analysis of Portuguese in a 
constraint grammar framework. PhD Thesis 
University of ?rhus, Denmark. 
Burstein, J. 2009. Opportunities for Natural Language 
Processing Research in Education. In the  Proceedings 
of CICLing, pp. 6-27.  
Caseli, H., Pereira, T.F., Specia, L., Pardo, T.A.S., 
Gasperin, C., Aluisio, S. 2009. Building a Brazilian 
Portuguese Parallel Corpus of Original and Simplified 
Texts. In the Proceedings of CICLing. 
Caseli, H.M.; Nunes, M.G.V.; Forcada, M.L. 2008. 
Automatic induction of bilingual resources from 
aligned parallel corpora: application to shallow-
transfer machine translation. Machine Translation, V. 
1, p. 227-245. 
Devlin, S., Unthank, G. 2006. Helping aphasic people 
process online information. In the Proceedings of the 
ACM SIGACCESS Conference on Computers and 
Accessibility, pp. 225-226. 
Graesser, A., McNamara, D. S., Louwerse, M., Cai, Z. 
2004. Coh-Metrix: Analysis of text on cohesion and 
language. Behavioral Research Methods, Instruments, 
and Computers, V. 36, pp. 193-202. 
Inui, K., Fujita, A., Takahashi, T., Iida, R., Iwakura, T. 
2003. Text Simplification for Reading Assistance: A 
Project Note. In the Proceedings of the Second 
International Workshop on Paraphrasing, 9 -16.  
Klebanov, B., Knight, K., Marcu, D. 2004. Text 
Simplification for Information-Seeking Applications. 
On the Move to Meaningful Internet Systems. LNCS, 
V.. 3290, pp. 735-747. 
Lal, P., Ruger, S. 2002. Extract-based summarization with 
simplification. In the Proceedings of DUC.  
Mapleson, D.L. 2006. Post-Grammatical Processing for 
Discourse Segmentation. PhD Thesis. School of 
Computing Sciences, University of East Anglia, 
Norwich. 
Max, A. 2005. Simplification interactive pour la 
production de textes adapt es aux personnes souffrant 
de troubles de la compr ehension. In the Proceedings 
of Traitement Automatique des Langues Naturelles 
(TALN).  
Max, A. 2006. Writing for language-impaired readers. In 
the  Proceedings of CICLing, pp. 567-570.  
Muniz, M.C., Laporte, E. Nunes, M.G.V. 2005. UNITEX-
PB, a set of flexible language resources for Brazilian 
Portuguese. In Anais do III Workshop em Tecnologia 
da Informa??o e da Linguagem Humana, V. 1, pp. 1-
10. 
Pardo, T.A.S.  and Nunes, M.G.V. 2008. On the 
Development and Evaluation of a Brazilian 
Portuguese Discourse Parser. Journal of Theoretical 
and Applied Computing, V. 15, N. 2, pp. 43-64. 
Petersen, S.E. 2007. Natural Language Processing Tools 
for Reading Level Assessment and Text Simplification 
for Bilingual Education. PhD Thesis, University of 
Washington.  
Petersen, S.E. and Ostendorf, M. 2007. Text 
Simplification for Language Learners: A Corpus 
Analysis. In the Proceedings of the Speech and 
Language Technology for Education Workshop, pp. 
69-72. 
Specia, L., Alu?sio, S.M., Pardo, T.A.S. 2008. Manual de 
simplifica??o sint?tica para o portugu?s. Technical 
Report NILC-TR-08-06, NILC. 
Siddharthan, A. 2002. An Architecture for a Text 
Simplification System. In the Proceedings of the 
Language Engineering Conference, pp. 64-71. 
Siddharthan, A. 2003. Syntactic Simplification and Text 
Cohesion. PhD Thesis. University of Cambridge. 
Snow, C. 2002. Reading for understanding: Toward an 
R&D program in reading comprehension. Santa 
Monica, CA. 
 
42
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 57?64,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A hybrid approach for extracting semantic relations from texts 
 
 
Lucia Specia and Enrico Motta 
Knowledge Media Institute & Centre for Research in Computing 
The Open University, Walton Hall, MK7 6AA, Milton Keynes, UK 
{L.Specia,E.Motta}@open.ac.uk 
 
  
 
Abstract 
We present an approach for extracting re-
lations from texts that exploits linguistic 
and empirical strategies, by means of a 
pipeline method involving a parser, part-
of-speech tagger, named entity recogni-
tion system, pattern-based classification 
and word sense disambiguation models, 
and resources such as ontology, knowl-
edge base and lexical databases. The rela-
tions extracted can be used for various 
tasks, including semantic web annotation 
and ontology learning. We suggest that 
the use of knowledge intensive strategies 
to process the input text and corpus-
based techniques to deal with unpredicted 
cases and ambiguity problems allows to 
accurately discover the relevant relations 
between pairs of entities in that text. 
1 Introduction 
Semantic relations extracted from texts are useful 
for several applications, including question an-
swering, information retrieval, semantic web an-
notation, and construction and extension of lexi-
cal resources and ontologies. In this paper we 
present an approach for relation extraction de-
veloped to semantically annotate relational 
knowledge coming from raw text, within a 
framework aiming to automatically acquire high 
quality semantic metadata for the Semantic Web.  
In that framework, applications such as se-
mantic web portals (Lei et al, 2006) analyze data 
from texts, databases, domain ontologies, and 
knowledge bases in order to extract the semantic 
knowledge in an integrated way. Known entities 
occurring in the text, i.e., entities that are in-
cluded in the knowledge base, are semantically 
annotated with their properties, also provided by 
the knowledge base and by databases. New enti-
ties, as given by a named entity recognition sys-
tem according to the possible types of entities in 
the ontology, are annotated without any addi-
tional information. In this context, the goal of the 
relation extraction approach presented here is to 
extract relational knowledge about entities, i.e., 
to identify the semantic relations between pairs 
of entities in the input texts. Entities can be both 
known and new, since named entity recognition 
is also carried out. Relations include those al-
ready existent in the knowledge base, new rela-
tions predicted as possible by the domain ontol-
ogy, or completely new (unpredicted) relations.  
The approach makes use of a domain ontol-
ogy, a knowledge base, and lexical databases, 
along with knowledge-based and empirical re-
sources and strategies for linguistic processing. 
These include a lemmatizer, syntactic parser, 
part-of-speech tagger, named entity recognition 
system, and pattern matching and word sense 
disambiguation models. The input data used in 
the experiments with our approach consists of 
English texts from the Knowledge Media Insti-
tute (KMi)1 newsletters. We believe that by inte-
grating corpus and knowledge-based techniques 
and using rich linguistic processing strategies in 
a completely automated fashion, the approach 
can achieve effective results, in terms of both 
accuracy and coverage.  
With relational knowledge, a richer represen-
tation of the input data can be produced. More-
over, by identifying new entities, the relation 
extraction approach can also be applied to ontol-
ogy population. Finally, since it extracts new 
relations, it can also be used as a first step for 
ontology learning. 
In the remaining of this paper we first describe 
some cognate work on relation extraction, par-
ticularly those exploring empirical methods, for 
various applications (Section 2). We then present 
                                                 
1
 http://kmi.open.ac.uk/ 
57
our approach, showing its architecture and de-
scribing each of its main components (Section 3). 
Finally, we present the next steps (Section 4). 
2 Related Work 
Several approaches have been proposed for the 
extraction of relations from unstructured sources. 
Recently, they have focused on the use of super-
vised or unsupervised corpus-based techniques in 
order to automate the task. A very common ap-
proach is based on pattern matching, with pat-
terns composed by subject-verb-object (SVO) 
tuples. Interesting work has been done on the 
unsupervised automatic detection of relations 
from a small number of seed patterns. These are 
used as a starting point to bootstrap the pattern 
learning process, by means of semantic similarity 
measures (Yangarber, 2000; Stevenson, 2004).  
Most of the approaches for relation extraction 
rely on the mapping of syntactic dependencies, 
such as SVO, onto semantic relations, using ei-
ther pattern matching or other strategies, such as 
probabilistic parsing for trees augmented with 
annotations for entities and relations (Miller et al 
2000), or clustering of semantically similar syn-
tactic dependencies, according to their selec-
tional restrictions (Gamallo et al, 2002).  
In corpus-based approaches, many variations 
are found concerning the machine learning tech-
niques used to produce classifiers to judge rela-
tion as relevant or non-relevant. (Roth and Yih, 
2002), e.g., use probabilistic classifiers with con-
straints induced between relations and entities, 
such as selectional restrictions. Based on in-
stances represented by a pair of entities and their 
position in a shallow parse tree, (Zelenko et al, 
2003) use support vector machines and voted 
perceptron algorithms with a specialized kernel 
model. Also using kernel methods and support 
vector machines, (Zhao and Grishman, 2005) 
combine clues from different levels of syntactic 
information and applies composite kernels to 
integrate the individual kernels.  
Similarly to our proposal, the framework pre-
sented by (Iria and Ciravegna, 2005) aims at the 
automation of semantic annotations according to 
ontologies. Several supervised algorithms can be 
used on the training data represented through a 
canonical graph-based data model. The frame-
work includes a shallow linguistic processing 
step, in which corpora are analyzed and a repre-
sentation is produced according to the data 
model, and a classification step, where classifiers 
run on the datasets produced by the linguistic 
processing step.  
Several relation extraction approaches have 
been proposed focusing on the task of ontology 
learning (Reinberger and Spyns, 2004; Schutz 
and Buitelaar, 2005; Ciaramita et al, 2005). 
More comprehensive reviews can be found in 
(Maedche, 2002) and (Gomez-Perez and Man-
zano-Macho, 2003). These approaches aim to 
learn non-taxonomic relations between concepts, 
instead of lexical items. However, in essence, 
they can employ similar techniques to extract the 
relations. Additional strategies can be applied to 
determine whether the relations can be lifted 
from lexical items to concepts, as well as to de-
termine the most appropriate level of abstraction 
to describe a relation (e.g. Maedche, 2002). 
In the next section we describe our relation ex-
traction approach, which merges features that 
have shown to be effective in several of the pre-
vious works, in order to achieve more compre-
hensive and accurate results. 
3 A hybrid approach for relation ex-
traction 
The proposed approach for relation extraction is 
illustrated in Figure 1. It employs knowledge-
based and (supervised and unsupervised) corpus-
based techniques. The core strategy consists of 
mapping linguistic components with some syn-
tactic relationship (a linguistic triple) into their 
corresponding semantic components. This in-
cludes mapping not only the relations, but also 
the terms linked by those relations. The detection 
of the linguistic triples involves a series of lin-
guistic processing steps. The mapping between 
terms and concepts is guided by a domain ontol-
ogy and a named entity recognition system. The 
identification of the relations relies on the 
knowledge available in the domain ontology and 
in a lexical database, and on pattern-based classi-
fication and sense disambiguation models. 
The main goal of this approach is to provide 
rich semantic annotations for the Semantic Web. 
Other potential applications include:  
1) Ontology population: terms are mapped 
into new instances of concepts of an ontology, 
and relations between them are identified, ac-
cording to the possible relations in that ontology.  
3) Ontology learning: new relations between 
existent concepts are identified, and can be used 
as a first step to extend an existent ontology. A 
subsequent step to lift relations between in-
stances to an adequate level of abstraction may 
be necessary. 
58
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Architecture of the proposed approach 
 
3.1 Context and resources 
The input to our experiments consists of elec-
tronic Newsletter Texts2. These are short texts 
describing news of several natures related to 
members of a research group: projects, publica-
tions, events, awards, etc. The domain Ontology 
used (KMi-basic-portal-ontology) was designed 
based on the AKT reference ontology3 to include 
concepts relevant to our domain. The instantia-
tions of concepts in this ontology are stored in 
the knowledge base (KB) KMi-basic-portal-kb. 
The other two resources used in our architecture 
are the lexical database WordNet (Fellbaum, 
1998) and a repository of Patterns of relations, 
described in Section 3.4. 
3.2 Identifying linguistic triples 
Given a newsletter text, the first step of the rela-
tion extraction approach is to process the natural 
language text in order to identify linguistic tri-
ples, that is, sets of three elements with a syntac-
tic relationship, which can indicate potentially 
relevant semantic relations. In our architecture, 
                                                 
2
 http://news.kmi.open.ac.uk/kmiplanet/ 
3
 http://kmi.open.ac.uk/projects/akt/ref-onto/ 
 
this is accomplished by the Linguistic Compo-
nent module, and adaptation of the linguistic 
component designed in Aqualog (Lopez et al, 
2005), a question answering system.  
The linguistic component uses the infrastruc-
ture and the following resources from GATE 
(Cunningham et al, 2002): tokenizer, sentence 
splitter, part-of-speech tagger, morphological 
analyzer and VP chunker. On the top of these 
resources, which produce syntactic annotations 
for the input text, the linguistic component uses a 
grammar to identify linguistic triples. This 
grammar was implemented in Jape (Cunningham 
et al, 2000), which allows the definition of pat-
terns to recognize regular expressions using the 
annotations provided by GATE.  
The main type of construction aimed to be 
identified by our grammar involves a verbal ex-
pression as indicative of a potential relation and 
two noun phrases as terms linked by that rela-
tion. However, our patterns also account for 
other types of constructions, including, e.g., the 
use of comma to implicitly indicate a relation, as 
in sentence (1). In this case, when mapping the 
terms into entities (Section 3.3), having identi-
fied that ?KMi? is an organization and ?Enrico 
ESpotter++ Linguistic 
Component 
yes 
 
yes 
Pattern-based 
classification 
POS + 
Lemmatizer 
 
WSD 
module 
no, n rela-
tions 
Annotate & 
add to patterns 
Patterns 
 
no 
yes 
RSS_2 
 
 
WordNet 
Ontology 
yes no, 0 rela-
tions 
yes 
 
no 
WordNet Patterns 
no, case 1  
with n  
relations 
 
no 
yes 
Annotate & 
add to patterns 
 
Patterns  
 
yes 
no 
RSS_1 
Linguistic 
triple 
Newsletter Texts 
Case (1) Case (2) 
Case (3) 
Types 
identified 
1 relation  
matches 
Ontology  
+ KB 
 
Classific
ation 
 
no 
 
Disambigu
ated 
WordNet 
 
59
Motta? is a person, it is possible to guess the re-
lation indicated by the comma (e.g., work). Some 
examples triples identified by our patterns for the 
newsletter in Figure 2 are given in Figure 3. 
(1) ?Enrico Motta, at KMi now, is leading a 
project on ?.?.  
 
 
 
 
 
 
 
Figure 2. Example of newsletter 
 
 
 
Figure 3. Examples of linguistic triples for the 
newsletter in Figure 2 
Jape patterns are based on shallow syntactic in-
formation only, and therefore they are not able to 
capture certain potentially relevant triples. To 
overcome this limitation, we employ a parser as 
a complementary resource to produce linguistic 
triples. We use Minipar (Lin, 1993), which pro-
duces functional relations for the components in 
a sentence, including subject and object relations 
with respect to a verb. This allows capturing 
some implicit relations, such as indirect objects 
and long distance dependence relations.  
Minipar?s representation is converted into a 
triple format and therefore the intermediate rep-
resentation provided by both GATE and Minipar 
consists of triples of the type: <noun_phrase, 
verbal_expression, noun_phrase>. 
3.3 Identifying entities and relations 
Given a linguistic triple, the next step is to verify 
whether the verbal expression in that triple con-
veys a relevant semantic relationship between 
entities (given by the terms) potentially belong-
ing to an ontology. This is the most important 
phase of our approach and is represented by a 
series of modules in our architecture in Figure 1. 
As first step we try to map the linguistic triple 
into an ontology triple, by using an adaptation of 
Aqualog?s Relation Similarity Service (RSS).  
RSS tries to make sense of the linguistic triple 
by looking at the structure of the domain ontol-
ogy and the information stored in the KB. In or-
der to map a linguistic triple into an ontology 
triple, besides looking for an exact matching be-
tween the components of the two triples, RSS 
considers partial matching by using a set of re-
sources in order to account for minor lexical or 
conceptual discrepancies between these two ele-
ments. These resources include metrics for string 
similarity matching, synonym relations given by 
WordNet, and a lexicon of previous mappings 
between the two types of triples. Different strate-
gies are employed to identify a matching for 
terms and relations, as we describe below.  
Since we do not consider any interaction with 
the user in order to achieve a fully automated 
annotation process, other modules were devel-
oped to complete the mapping process even if 
there is no matching (Section 3.4) or if there is 
ambiguity (Section 3.5), according to RSS. 
Strategies for mapping terms 
 
To map terms into entities, the following at-
tempts are accomplished (in the given order): 
1) Search the KB for an exact matching of the 
term with any instance. 
2) Apply string similarity metrics4 to calculate 
the similarity between the given term and each 
instance of the KB. A hybrid scheme combining 
three metrics is used: jaro-Winkler, jlevelDis-
tance a wlevelDistance. Different combinations 
of threshold values for the metrics are consid-
ered. The elements in the linguistic triples are 
lemmatized in order to avoid problems which 
could be incorrectly handled by the string simi-
larity metrics (e.g., past tense). 
2.1) If there is more that one possible match-
ing, check whether any of them is a substring 
of the term. For example, the instance name 
for ?Enrico Motta? is a substring of the term 
?Motta?, and thus it should be preferred.  
 
For example, the similarity values returned for 
the term ?vanessa? with instances potentially 
relevant for the mapping are given in Figure 4. 
The combination of thresholds is met for the in-
stance ?Vanessa Lopez?, and thus the mapping is 
accomplished. If there is still more than one pos-
sible mapping, we assume there is not enough 
evidence to map that term and discard the triple. 
 
 
 
 
 
Figure 4. String similarity measures for the term 
?vanessa? and the instance ?Vanessa Lopez? 
                                                 
4
 http://sourceforge.net/projects/simmetrics/ 
Nobel Summit on ICT and public services 
 
Peter Scott attended the Public Services Summit in Stock-
holm, during Nobel Week 2005. The theme this year was 
Responsive Citizen Centered Public Services. The event 
was hosted by the City of Stockholm and Cisco Systems 
Thursday 8 December - Sunday 11 December 2005. 
? 
<peter-scott,attend,public-services-summit> 
<public-services-summit,located,stockholm> 
<theme,is,responsive-citizen-centered-public-services> 
<city-of-stockholm-and-cisco-systems,host,event> 
 
jaroDistance for ?vanessa? and ?vanessa-lopez? = 
0.8461538461538461wlevel for ?vanessa? and ?vanessa-
lopez? = 1.0jWinklerDistance for ?vanessa? and ?vanessa-
lopez? = 0.9076923076923077 
60
Strategies for mapping relations 
 
In order to map the verbal expression into a con-
ceptual relation, we assume that the terms of the 
triple have already been mapped either into in-
stances of classes in the KB by RSS, or into po-
tential new instances, by a named entity recogni-
tion system (as we explain in the next section). 
The following attempts are then made for the 
verb-relation mapping: 
1) Search the KB for an exact matching of the 
verbal expression with any existent relation for 
the instances under consideration or any possible 
relation between the classes (and superclasses) of 
the instances under consideration. 
2) Apply the string similarity metrics to calcu-
late the similarity between the given verbal ex-
pression and the possible relations between in-
stances (or their classes) corresponding to the 
terms in the linguistic triple. 
3) Search for similar mappings for the 
types/classes of entities under consideration in a 
lexicon of mappings automatically created ac-
cording to users? choices in the question answer-
ing system Aqualog. This lexicon contains on-
tology triples along with the original verbal ex-
pression, as illustrated in Table 1. The use of this 
lexicon represents a simplified form of pattern 
matching in which only exact matching is con-
sidered. 
 
given_relation class_1 conceptual relation class_2 
works project has-project-member person 
cite project has-publication publication 
Table 1. Examples of lexicon patterns 
4) Search for synonyms of the given verbal 
expression in WordNet, in order to verify if there 
is a synonym that matches (complete or partially, 
using string similarity metrics) any existent rela-
tion for the instances under consideration, or any 
possible relation between the classes (or super-
classes) of those instances (likewise in step 1). 
If there is no possible mapping for the term, 
the pattern-based classification model is trig-
gered (Section 3.4). Conversely, if there is more 
than one possible mapping, the disambiguation 
model is called (Section 3.5). 
The application of these strategies to map the 
linguistic triples into existent or new instances 
and relations is described in what follows. 
Applying RSS to map entities and relations 
 
In our architecture, RSS is represented by mod-
ules RSS_1 and RSS_2. RSS_1 first checks if 
the terms in the linguistic triple are instances of a 
KB (cf. strategies for mapping terms). If the 
terms can be mapped to instances, it checks 
whether the relation given in the triple matches 
any already existent relation between for those 
instances, or, alternatively, if that relation 
matches any of the possible relations for the 
classes (and superclasses) of the two instances in 
the domain ontology (cf. strategies for mapping 
relations). Three situations may arise from this 
attempt to map the linguistic triple into an ontol-
ogy triple (Cases (1), (2), and (3) in Fig. 1): 
Case (1): complete matching with instances of 
the KB and a relation of the KB or ontology, 
with possibly more than one valid conceptual 
relation being identified: 
<instance1, (conceptual_relation)+, instance2>. 
 
Case (2): no matching or partial matching 
with instances of the ontology (the relation is not 
analyzed (na) when there is not a matching for 
instances): 
<instance1, na , ?>   or   <?, na, instance2>   or    
<?, na, ?> 
 
Case (3): matching with instances of the KB, 
but no matching with a relation of the KB or on-
tology:  
<instance1, ?, instance2> 
 
If the matching attempt results in Case (1) with 
only one conceptual relation, then the triple can 
be formalized into a semantic annotation. This 
yields the annotation of an already existent rela-
tion for two instances of the KB, as well as a new 
relation for two instances of the KB, although 
this relation was already predicted in the ontol-
ogy as possible between the classes of those in-
stances. The generalization of the produced triple 
for classes/types of entities, i.e., <class, concep-
tual_relation, class>, is added to the repository of 
Patterns. 
On the other hand, if there is more than one 
possible conceptual relation in case (1), the sys-
tem tries to find the correct one by means of a 
sense disambiguation model, described in Sec-
tion 3.5. Conversely, if there is no matching for 
the relation (Case (3)), the system tries an alter-
native strategy: the pattern-based classification 
model (Section 3.4). Finally, if there is no com-
plete matching of the terms with instances of the 
KB (Case (2)), it means that the entities can be 
new to the KB. 
In order to check if the terms in the linguistic 
triple express new entities, the system first iden-
61
tifies to what classes of the ontology they belong. 
This is accomplished by means of ESpotter++, 
and extension of the named entity recognition 
system ESpotter (Zhu et al 2005).  
ESpotter is based on a mixture of lexicon 
(gazetteers) and patterns. We extended ESpotter 
by including new entities (extracted from other 
gazetteers), a few relevant new types of entities, 
and a small set of efficient patterns. All types of 
entities correspond to generic classes of our do-
main ontology, including: person, organization, 
event, publication, location, project, research-
area, technology, etc.  
In our architecture, if ESpotter++ is not able to 
identify the types of the entities, the process is 
aborted and no annotation is produced. This may 
be either because the terms do not have any con-
ceptual mapping (for example ?it?), or because 
the conceptual mapping is not part of our domain 
ontology. Otherwise, if ESpotter++ succeeds, 
RSS is triggered again (RSS_2) in order to verify 
whether the verbal expression encompasses a 
semantic relation. Since at least one of the two 
entities is recognized by Espotter++, and there-
fore at least one entity is new, it is only possible 
to check if the relation matches the possible rela-
tions between the classes of the recognized enti-
ties (cf. strategies for mapping relations).  
If the matching attempt results in only one 
conceptual relation, then the triple will be for-
malized into a semantic annotation. This repre-
sents the annotation of a new (although pre-
dicted) relation and two or at least one new en-
tity/instance. The produced triple of the type 
<class, conceptual_relation, class> is added to 
the repository of Patterns. 
Again, if there are multiple valid conceptual 
relations, the system tries to find the correct one 
by means of a disambiguation model (Section 
3.5). Conversely, if it there is no matching for the 
relation, the pattern-based classification model is 
triggered (Section 3.4).  
3.4 Identifying new relations 
The process described in Section 3.3 for the 
identification of relations accounts only for the 
relations already predicted as possible in the do-
main ontology. However, we are also interested 
in the additional information that can be pro-
vided by the text, in the form of new types of 
relations for known or new entities. In order to 
discover these relations, we employ a pattern 
matching strategy to identify relevant relations 
between types of terms. 
 
The pattern matching strategy has proved to be 
an efficient way to extract semantic relations, but 
in general has the drawback of requiring the pos-
sible relations to be previously defined. In order 
to overcome this limitation, we employ a Pat-
tern-based classification model that can identify 
similar patterns based on a very small initial 
number of patterns. 
We consider patterns of relations between 
types of entities, instead of the entities them-
selves, since we believe that it would be impos-
sible to accurately judge the similarity for the 
kinds of entities we are addressing (names of 
people, locations, etc). Thus, our patterns consist 
of triples of the type <class, conceptual_relation, 
class>, which are compared against a given triple 
using its classes (already provided by the linguis-
tic component or by ESpotter++) in order to clas-
sify relations in that triple as relevant or non-
relevant. 
The classification model is based on the ap-
proach presented in (Stevenson, 2004). It is an 
unsupervised corpus-based module which takes 
as examples a small set of relevant SVO patterns, 
called seed patterns, and uses a WordNet-based 
semantic similarity measure to compare the pat-
tern to be classified against the relevant ones. 
Our initial seed patterns (see examples in Table 
2) mixes patterns extracted from the lexicon gen-
erated by Aqualog?s users (cf. Section 3.3) and a 
small number of manually defined relevant pat-
terns. This set of patterns is expected to be en-
riched with new patterns as our system annotates 
relevant relations, since the system adds new tri-
ples to the initial set of patterns. 
 
class_1 conceptual relation class_2 
project has-project-member person 
project has-publication publication 
person develop technology 
person attend event 
Table 2. Examples of seed patterns 
 
Likewise (Stevenson, 2004), we use a semantic 
similarity metric based on the information con-
tent of the words in WordNet hierarchy, derived 
from corpus probabilities. It scores the similarity 
between two patterns by computing the similarity 
for each pair of words in those patterns. A 
threshold of 0.90 for this score was used here to 
classify two patterns as similar. In that case, a 
new annotation is produced for the input triple 
and it is added to the set of patterns. 
It is important to notice that, although Word-
Net is also used in the RSS module, in that case 
62
only synonyms are checked, while here the simi-
larity metric explores deeper information in 
WordNet, considering the meaning (senses) of 
the words. It is also important to distinguish the 
semantic similarity metrics employed here from 
the string metrics used in RSS. String similarity 
metrics simply try to capture minor variations on 
the strings representing terms/relations, they do 
not account for the meaning of those strings.  
3.5 Disambiguating relations 
The ambiguity arising when more than one pos-
sible relation exists for a pair of entities is a 
problem neglected in most of the current work on 
relation extraction. In our architecture, when the 
RSS finds more than one possible relation, we 
choose one relation by using the word sense dis-
ambiguation (WSD) system SenseLearner (Mi-
halcea and Csomai, 2005).  
SenseLearner is supervised WSD system to 
disambiguate all open class words in any given 
text, after being trained on a small data set, ac-
cording to global models for word categories. 
The current distribution includes two default 
models for verbs, which were trained on a corpus 
containing 200,000 content words of journalistic 
texts tagged with their WordNet senses. Since 
SenseLeaner requires a sense tagged corpus in 
order to be trained to specific domains and there 
is not such a corpus for our domain, we use one 
of the default training models. This is a contex-
tual model that relies on the first word before and 
after the verb, and its POS tags. To disambiguate 
new cases, it requires only that the words are an-
notated with POS tags. The use of lemmas of the 
words instead of the words yields better results, 
since the models were generated for lemmas. In 
our architecture, these annotations are produced 
by the component POS + Lemmatizer.  
Since the WSD module disambiguates among 
WordNet senses, it is employed only after the 
use of the WordNet subcomponent by RSS. This 
subcomponent finds all the synonyms for the 
verb in a linguistic triple and checks which of 
them matches existent or possible relations for 
the terms in that triple. In some cases, however, 
there is a matching for more than one synonym. 
Since in WordNet synonyms usually represent 
different uses of the verb, the WSD module can 
identify in which sense the verb is being used in 
the sentence, allowing the system to choose one 
among all the matching options. 
For example, given the linguistic triple <en-
rico_motta, head, kmi>, RSS is able to identify 
that ?enrico_motta? is a person, and that ?kmi? is 
an organization. However, it cannot find an ex-
act or partial matching (using string metrics), or 
even a matching (given by the user lexicon) for 
the relation ?head?. After getting all its syno-
nyms in WordNet, RSS verifies that two of them 
match possible relations in the ontology between 
a person and an organization: ?direct? and 
?lead?. In this case, the WSD module disam-
biguates the sense of ?head? as ?direct?. 
3.6 Example of extracted relations 
As an example of the relations that can be ex-
tracted in our approach, consider the representa-
tion of the entity ?Enrico Motta? and all the rela-
tions involving this entity in Figure 5. The rela-
tions were extracted from the text in Figure 6. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 5. Example of newsletter 
 
 
 
 
 
Figure 6. Semantic annotations produced for the 
news in Figure 5 
In this case, ?Enrico-Motta? is an instance of 
kmi-academic-staff-member, a subclass of person 
in the domain ontology. The mapped relation 
?works-in? ?knowledge-media-institute? already 
existed in the KB. The new relations pointed out 
by our approach are the ones referring to the 
award received from the ?European Commis-
sion? (an organization, here), for three projects: 
?NeOn?, ?XMEDIA?, and ?OK?. 
4 Conclusions and future work 
We presented a hybrid approach for the extrac-
tion of semantic relations from text. It was de-
KMi awarded ?4M for Semantic Web Research 
 
Professor Enrico Motta and Dr John Domingue of the 
Knowledge Media Institute have received a set of record-
breaking awards totalling ?4m from the European Commis-
sion's Framework 6 Information Society Technologies (IST) 
programme. This is the largest ever combined award ob-
tained by KMi associated with a single funding programme. 
The awards include three Integrated Projects (IPs) and 
three Specific Targeted Research Projects (STREPs) and 
they consolidate KMi?s position as one of the leading inter-
national research centers in semantic technologies. Specifi-
cally Professor Motta has been awarded:  
 
a.. ?1.55M for the project NeOn: Lifecycle Support for Net-
worked Ontologies  
b.. ?565K for XMEDIA: Knowledge Sharing and Reuse 
across Media and  
c.. ?391K for OK: Openknowledge - Open, coordinated 
knowledge sharing architecture. ? 
(def-instance Enrico-Motta kmi-academic-staff-member 
 ((works-in knowledge-media-institute) 
  (award-from european-commission) 
  (award-for NeOn) 
  (award-for XMEDIA) 
  (award-for OK))) 
63
signed mainly to enrich the annotations produced 
by a semantic web portal, but can be used for 
other domains and applications, such as ontology 
population and development. Currently we are 
concluding the integration of the several modules 
composing our architecture. We will then carry 
experiments with our corpus of newsletters in 
order to evaluate the approach. Subsequently, we 
will incorporate the architecture to a semantic 
web portal and accomplish an extrinsic evalua-
tion in the context of that application. Since the 
approach uses deep linguistic processing and 
corpus-based strategies not requiring any manual 
annotation, we expect it will accurately discover 
most of the relevant relations in the text.  
Acknowledgement 
This research was supported by the Advanced 
Knowledge Technologies (AKT) project. AKT is 
an Interdisciplinary Research Collaboration 
(IRC), which is sponsored by the UK Engineer-
ing and Physical Sciences Research Council un-
der grant number GR/N15764/01.  
References 
Massimiliano Ciaramita, Aldo Gangemi, Esther 
Ratsch, Jasmim Saric, Isabel Rojas. 2005. Unsu-
pervised learning of semantic relations between 
concepts of a molecular biology ontology. 19th 
IJCAI, pp. 659-664 
Hamish Cunningham, Diana Maynard, Kalina 
Bontcheva, and Valentin Tablan. 2002. GATE: A 
Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. 40th 
ACL Meeting, Philadelphia. 
Hamish Cunningham, Diana Maynard, and Valentin 
Tablan. 2000. JAPE: a Java Annotation Patterns 
Engine. Tech. Report CS--00--10, University of 
Sheffield, Department of Computer Science. 
Christiane D. Fellbaum (ed). 1998. Wordnet: An Elec-
tronic Lexical Database. The MIT Press. 
Pablo Gamallo, Marco Gonzalez, Alexandre Agustini, 
Gabriel Lopes, and Vera S. de Lima. 2002. Map-
ping syntactic dependencies onto semantic rela-
tions. ECAI Workshop on Machine Learning and 
Natural Language Processing for Ontology Engi-
neering, Lyon, France. 
Asuncion Gomez-Perez and David Manzano-Macho. 
2003. A Survey of Ontology Learning Methods and 
Techniques. Deliverable 1.5, OntoWeb Project. 
Jose Iria and Fabio Ciravegna. 2005. Relation Extrac-
tion for Mining the Semantic Web. Dagstuhl Semi-
nar on Machine Learning for the Semantic Web, 
Dagstuhl, Germany. 
Yuangui Lei, Marta Sabou, Vanessa Lopez, Jianhan 
Zhu, Victoria Uren, and Enrico Motta. 2006. An 
infrastructure for Acquiring High Quality Semantic 
Metadata. To appear in the 3rd ESWC, Budva. 
Dekang Lin. 1993. Principle based parsing without 
overgeneration. 31st ACL, Columbus, pp. 112-120. 
Vanessa Lopez, Michele Pasin, and Enrico Motta. 
2005. AquaLog: An Ontology-portable Question 
Answering System for the Semantic Web. 2nd 
ESWC, Creete, Grece. 
Alexander D. Maedche. 2002. Ontology Learning for 
the Semantic Web, Kluwer Academic Publishers, 
Norwell, MA. 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph 
Weischedel. 2000. A novel use of statistical pars-
ing to extract information from text. 6th ANLP-
NAACL, Seattle, pp. 226-233. 
Rada Mihalcea and Andras Csomai. 2005. Sense-
Learner: Word Sense Disambiguation for All 
Words in Unrestricted Text. 43rd ACL Meeting, 
Ann Arbor. 
Marie-Laure Reinberger and Peter Spyns. 2004. Dis-
covering knowledge in texts for the learning of 
DOGMA inspired ontologies. ECAI 2004 Work-
shop on Ontology Learning and Population, Va-
lencia, pp. 19-24. 
Dan Roth and Wen-tau Yih. 2002. Probabilistic rea-
soning for entity & relation recognition. 19th COL-
ING, Taipei, Taiwan, pp. 1-7. 
Alexander Schutz and Paul Buitelaar. 2005. RelExt: A 
Tool for Relation Extraction from Text in Ontology 
Extension. 4th ISWC, pp. 593-606. 
Mark Stevenson. 2004. An Unsupervised WordNet-
based Algorithm for Relation Extraction. 4th LREC 
Workshop Beyond Named Entity: Semantic Label-
ing for NLP Tasks, Lisbon. 
Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-
ardella. 2003. Kernel Methods for Relation Extrac-
tion. Journal of Machine Learning Research, 
(3):1083-1106. 
Shubin Zhao and Ralph Grishman. 2005. Extracting 
Relations with Integrated Information Using Ker-
nel Methods. 43d ACL Meeting, Ann Arbor. 
Jianhan Zhu, Victoria Uren, and Enrico Motta. 2005. 
ESpotter: Adaptive Named Entity Recognition for 
Web Browsing. 3rd Conf. on Professional Knowl-
edge Management, Kaiserslautern, pp. 518-529. 
Roman Yangarber, Ralph Grishman and Pasi Tapana-
inen, P. 2000. Unsupervised Discovery of Sce-
nario-Level Patterns for Information Extraction. 
6th ANLP, pp. 282-289. 
64
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1237?1249,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Exact Decoding for Phrase-Based Statistical Machine Translation
Wilker Aziz
?
Marc Dymetman
?
Lucia Specia
?
?
Department of Computer Science, University of Sheffield, UK
W.Aziz@sheffield.ac.uk
L.Specia@sheffield.ac.uk
?
Xerox Research Centre Europe, Grenoble, France
Marc.Dymetman@xrce.xerox.com
Abstract
The combinatorial space of translation
derivations in phrase-based statistical ma-
chine translation is given by the intersec-
tion between a translation lattice and a tar-
get language model. We replace this in-
tractable intersection by a tractable relax-
ation which incorporates a low-order up-
perbound on the language model. Exact
optimisation is achieved through a coarse-
to-fine strategy with connections to adap-
tive rejection sampling. We perform ex-
act optimisation with unpruned language
models of order 3 to 5 and show search-
error curves for beam search and cube
pruning on standard test sets. This is the
first work to tractably tackle exact opti-
misation with language models of orders
higher than 3.
1 Introduction
In Statistical Machine Translation (SMT), the task
of producing a translation for an input string x =
?x
1
, x
2
, . . . , x
I
? is typically associated with find-
ing the best derivation d
?
compatible with the in-
put under a linear model. In this view, a derivation
is a structured output that represents a sequence of
steps that covers the input producing a translation.
Equation 1 illustrates this decoding process.
d
?
= argmax
d?D(x)
f(d) (1)
The set D(x) is the space of all derivations com-
patible with x and supported by a model of trans-
lational equivalences (Lopez, 2008). The func-
tion f(d) = ? ? H(d) is a linear parameteri-
sation of the model (Och, 2003). It assigns a
real-valued score (or weight) to every derivation
d ? D(x), where ? ? R
m
assigns a relative
importance to different aspects of the derivation
independently captured by m feature functions
H(d) = ?H
1
(d), . . . ,H
m
(d)? ? R
m
.
The fully parameterised model can be seen as
a discrete weighted set such that feature func-
tions factorise over the steps in a derivation. That
is, H
k
(d) =
?
e?d
h
k
(e), where h
k
is a (local)
feature function that assesses steps independently
and d = ?e
1
, e
2
, . . . , e
l
? is a sequence of l steps.
Under this assumption, each step is assigned the
weightw(e) = ? ??h
1
(e), h
2
(e), . . . , h
m
(e)?. The
setD is typically finite, however, it contains a very
large number of structures ? exponential (or even
factorial, see ?2) with the size of x ? making
exhaustive enumeration prohibitively slow. Only
in very restricted cases combinatorial optimisation
techniques are directly applicable (Tillmann et al.,
1997; Och et al., 2001), thus it is common to resort
to heuristic techniques in order to find an approxi-
mation to d
?
(Koehn et al., 2003; Chiang, 2007).
Evaluation exercises indicate that approximate
search algorithms work well in practice (Bojar
et al., 2013). The most popular algorithms pro-
vide solutions with unbounded error, thus pre-
cisely quantifying their performance requires the
development of a tractable exact decoder. To
date, most attempts were limited to short sentences
and/or somewhat toy models trained with artifi-
cially small datasets (Germann et al., 2001; Igle-
sias et al., 2009; Aziz et al., 2013). Other work
has employed less common approximations to the
model reducing its search space complexity (Ku-
mar et al., 2006; Chang and Collins, 2011; Rush
and Collins, 2011). These do not answer whether
or not current decoding algorithms perform well at
real translation tasks with state-of-the-art models.
We propose an exact decoder for phrase-based
SMT based on a coarse-to-fine search strategy
(Dymetman et al., 2012). In a nutshell, we re-
lax the decoding problem with respect to the Lan-
guage Model (LM) component. This coarse view
is incrementally refined based on evidence col-
1237
lected via maximisation. A refinement increases
the complexity of the model only slightly, hence
dynamic programming remains feasible through-
out the search until convergence. We test our de-
coding strategy with realistic models using stan-
dard data sets. We also contribute with optimum
derivations which can be used to assess future im-
provements to approximate decoders. In the re-
maining sections we present the general model
(?2), survey contributions to exact optimisation
(?3), formalise our novel approach (?4), present
experiments (?5) and conclude (?6).
2 Phrase-based SMT
In phrase-based SMT (Koehn et al., 2003), the
building blocks of translation are pairs of phrases
(or biphrases). A translation derivation d is an
ordered sequence of non-overlapping biphrases
which covers the input text in arbitrary order gen-
erating the output from left to right.
1
f(d) = ?(y) +
l
?
i=1
?(e
i
) +
l?1
?
i=1
?(e
i
, e
i?1
) (2)
Equation 2 illustrates a standard phrase-based
model (Koehn et al., 2003): ? is a weighted tar-
get n-gram LM component, where y is the yield
of d; ? is a linear combination of features that
decompose over phrase pairs directly (e.g. back-
ward and forward translation probabilities, lexi-
cal smoothing, and word and phrase penalties);
and ? is an unlexicalised penalty on the num-
ber of skipped input words between two adjacent
biphrases. The weighted logic program in Figure
1 specifies the fully parameterised weighted set of
solutions, which we denote ?D(x), f(d)?.
2
A weighted logic program starts from its ax-
ioms and follows exhaustively deducing new items
by combination of existing ones and no deduction
happens twice. In Figure 1, a nonteminal item
summarises partial derivation (or hypotheses). It is
denoted by [C, r, ?] (also known as carry), where:
C is a coverage vector, necessary to impose the
non-overlapping constraint; r is the rightmost po-
sition most recently covered, necessary for the
computation of ?; and ? is the last n ? 1 words
1
Preventing phrases from overlapping requires an expo-
nential number of constraints (the powerset of x) rendering
the problem NP-complete (Knight, 1999).
2
Weighted logics have been extensively used to describe
weighted sets (Lopez, 2009), operations over weighted sets
(Chiang, 2007; Dyer and Resnik, 2010), and a variety of dy-
namic programming algorithms (Cohen et al., 2008).
ITEM
[
{0, 1}
I
, [0, I + 1],?
n?1
]
GOAL
[
1
I
, I + 1, EOS
]
AXIOM
?BOS? BOS?
[0
I
, 0, BOS] : ?(BOS)
EXPAND
[
C, r, y
j?1
j?n+1
] ?
x
i
?
i
?
r
??? y
j
?
j
?
[
C
?
, i
?
, y
j
?
j
?
?n+2
]
: w
?
i
?
k=i
c
k
=
?
0
where c
?
k
= c
k
if k < i or k > i
?
else
?
1
w = ?
r
? ?(r, i)? ?(y
j
?
j
|y
j?1
j?n+1
)
ACCEPT [
1
I
, r, ?
]
[1
I
, I + 1, EOS] : ?(r, I + 1)? ?(EOS|?)
r ? I
Figure 1: Specification for the weighted set of
translation derivations in phrase-based SMT with
unconstrained reordering.
in the yield, necessary for the LM component. The
program expands partial derivations by concatena-
tion with a translation rule
?
x
i
?
i
?
r
??? y
j
?
j
?
, that is, an
instantiated biphrase which covers the span x
i
?
i
and
yields y
j
?
j
with weight ?
r
. The side condition im-
poses the non-overlapping constraint (c
k
is the kth
bit in C). The antecedents are used to compute the
weight of the deduction, and the carry is updated
in the consequent (item below the horizontal line).
Finally, the rule ACCEPT incorporates the end-of-
sentence boundary to complete items.
3
It is perhaps illustrative to understand the set of
weighted translation derivations as the intersection
between two components. One that is only locally
parameterised and contains all translation deriva-
tions (a translation lattice or forest), and one that
re-ranks the first as a function of the interactions
between translation steps. The model of transla-
tional equivalences parameterised only with ? is
an instance of the former. An n-gram LM compo-
nent is an instance of the latter.
2.1 Hypergraphs
A backward-hypergraph, or simply hypergraph,
is a generalisation of a graph where edges have
multiple origins and one destination (Gallo et al.,
1993). They can represent both finite-state and
context-free weighted sets and they have been
widely used in SMT (Huang and Chiang, 2007).
A hypergraph is defined by a set of nodes (or ver-
3
Figure 1 can be seen as a specification for a weighted
acyclic finite-state automaton whose states are indexed by
[l, C, r] and transitions are labelled with biphrases. However,
for generality of representation, we opt for using acyclic hy-
pergraphs instead of automata (see ?2.1).
1238
tices) V and a weighted set of edges ?E,w?. An
edge e connects a sequence of nodes in its tail
t[e] ? V
?
under a head node h[e] ? V and has
weight w(e). A node v is a terminal node if it
has no incoming edges, otherwise it is a nontermi-
nal node. The node that has no outgoing edges,
is called root, with no loss of generality we can
assume hypergraphs to have a single root node.
Hypergraphs can be seen as instantiated logic
programs. In this view, an item is a template
for the creation of nodes, and a weighted deduc-
tion rule is a template for edges. The tail of
an edge is the sequence of nodes associated with
the antecedents, and the head is the node associ-
ated with the consequent. Even though the space
of weighted derivations in phrase-based SMT is
finite-state, using a hypergraph as opposed to a
finite-state automaton makes it natural to encode
multi-word phrases using tails. We opt for rep-
resenting the target side of the biphrase as a se-
quence of terminals nodes, each of which repre-
sents a target word.
3 Related Work
3.1 Beam filling algorithms
Beam search (Koehn et al., 2003) and cube prun-
ing (Chiang, 2007) are examples of state-of-the-art
approximate search algorithms. They approximate
the intersection between the translation forest and
the language model by expanding a limited beam
of hypotheses from each nonterminal node. Hy-
potheses are organised in priority queues accord-
ing to common traits and a fast-to-compute heuris-
tic view of outside weights (cheapest way to com-
plete a hypothesis) puts them to compete at a fairer
level. Beam search exhausts a node?s possible ex-
pansions, scores them, and discards all but the k
highest-scoring ones. This process is wasteful in
that k is typically much smaller than the number of
possible expansions. Cube pruning employs a pri-
ority queue at beam filling and computes k high-
scoring expansions directly in near best-first order.
The parameter k is known as beam size and it con-
trols the time-accuracy trade-off of the algorithm.
Heafield et al. (2013a) move away from us-
ing the language model as a black-box and build
a more involved beam filling algorithm. Even
though they target approximate search, some of
their ideas have interesting connections to ours
(see ?4). They group hypotheses that share partial
language model state (Li and Khudanpur, 2008)
reasoning over multiple hypotheses at once. They
fill a beam in best-first order by iteratively vis-
iting groups using a priority queue: if the top
group contains a single hypothesis, the hypothesis
is added to the beam, otherwise the group is parti-
tioned and the parts are pushed back to the queue.
More recently, Heafield et al. (2014) applied their
beam filling algorithm to phrase-based decoding.
3.2 Exact optimisation
Exact optimisation for monotone translation has
been done using A
?
search (Tillmann et al., 1997)
and finite-state operations (Kumar et al., 2006).
Och et al. (2001) design near-admissible heuris-
tics for A
?
and decode very short sentences (6-
14 words) for a word-based model (Brown et al.,
1993) with a maximum distortion strategy (d = 3).
Zaslavskiy et al. (2009) frame phrase-based de-
coding as an instance of a generalised Travel-
ling Salesman Problem (TSP) and rely on ro-
bust solvers to perform decoding. In this view,
a salesman graph encodes the translation options,
with each node representing a biphrase. Non-
overlapping constraints are imposed by the TSP
solver, rather than encoded directly in the sales-
man graph. They decode only short sentences
(17 words on average) using a 2-gram LM due to
salesman graphs growing too large.
4
Chang and Collins (2011) relax phrase-based
models w.r.t. the non-overlapping constraints,
which are replaced by soft penalties through La-
grangian multipliers, and intersect the LM com-
ponent exhaustively. They do employ a maximum
distortion limit (d = 4), thus the problem they
tackle is no longer NP-complete. Rush and Collins
(2011) relax a hierarchical phrase-based model
(Chiang, 2005)
5
w.r.t. the LM component. The
translation forest and the language model trade
their weights (through Lagrangian multipliers) so
as to ensure agreement on what each component
believes to be the maximum. In both approaches,
when the dual converges to a compliant solution,
the solution is guaranteed to be optimal. Other-
4
Exact decoding had been similarly addressed with Inte-
ger Linear Programming (ILP) in the context of word-based
models for very short sentences using a 2-gram LM (Ger-
mann et al., 2001). Riedel and Clarke (2009) revisit that for-
mulation and employ a cutting-plane algorithm (Dantzig et
al., 1954) reaching 30 words.
5
In hierarchical translation, reordering is governed by a
synchronous context-free grammar and the underlying prob-
lem is no longer NP-complete. Exact decoding remains in-
feasible because the intersection between the translation for-
est and the target LM is prohibitively slow.
1239
wise, a subset of the constraints is explicitly added
and the dual optimisation is repeated. They handle
sentences above average length, however, resort-
ing to compact rulesets (10 translation options per
input segment) and using only 3-gram LMs.
In the context of hierarchical models, Aziz et
al. (2013) work with unpruned forests using up-
perbounds. Their approach is the closest to ours.
They also employ a coarse-to-fine strategy with
the OS
?
framework (Dymetman et al., 2012), and
investigate unbiased sampling in addition to op-
timisation. However, they start from a coarser
upperbound with unigram probabilities, and their
refinement strategies are based on exhaustive in-
tersections with small n-gram matching automata.
These refinements make forests grow unmanage-
able too quickly. Because of that, they only deal
with very short sentences (up to 10 words) and
even then decoding is very slow. We design bet-
ter upperbounds and a more efficient refinement
strategy. Moreover, we decode long sentences us-
ing language models of order 3 to 5.
6
4 Approach
4.1 Exact optimisation with OS
?
Dymetman et al. (2012) introduced OS
?
, a unified
view of optimisation and sampling which can be
seen as a cross between adaptive rejection sam-
pling (Robert and Casella, 2004) and A
?
optimisa-
tion (Hart et al., 1968). In this framework, a com-
plex goal distribution is upperbounded by a sim-
pler proposal distribution for which optimisation
(and sampling) is feasible. This proposal is incre-
mentally refined to be closer to the goal until the
maximum is found (or until the sampling perfor-
mance exceeds a certain level).
Figure 2 illustrates exact optimisation with OS
?
.
Suppose f is a complex target goal distribution,
such that we cannot optimise f , but we can as-
sess f(d) for a given d. Let g
(0)
be an upper-
bound to f , i.e., g
(0)
(d) ? f(d) for all d ? D(x).
Moreover, suppose that g
(0)
is simple enough to
be optimised efficiently. The algorithm proceeds
by solving d
0
= argmax
d
g
(0)
(d) and comput-
6
The intuition that a full intersection is wasteful is also
present in (Petrov et al., 2008) in the context of approximate
search. They start from a coarse distribution based on au-
tomatic word clustering which is refined in multiple passes.
At each pass, hypotheses are pruned a posteriori on the basis
of their marginal probabilities, and word clusters are further
split. We work with upperbounds, rather than word clusters,
with unpruned distributions, and perform exact optimisation.
f
g
(0)
d
0
D(x)
g
(1)
d
1
d
*
f
1
f
0
f
*
Figure 2: Sequence of incrementally refined up-
perbound proposals.
ing the quantity r
0
=
f(d
0
)
/g
(0)
(d
0
). If r
0
were
sufficiently close to 1, then g
(0)
(d
0
) would be
sufficiently close to f(d
0
) and we would have
found the optimum. However, in the illustration
g
(0)
(d
0
)  f(d
0
), thus r
0
 1. At this point
the algorithm has concrete evidence to motivate
a refinement of g
(0)
that can lower its maximum,
bringing it closer to f
?
= max
d
f(d) at the cost
of some small increase in complexity. The re-
fined proposal must remain an upperbound to f .
To continue with the illustration, suppose g
(1)
is
obtained. The process is repeated until eventually
g
(t)
(d
t
) = f(d
t
), where d
t
= argmax
d
g
(t)
(d),
for some finite t. At which point d
t
is the opti-
mum derivation d
?
from f and the sequence of
upperbounds provides a proof of optimality.
7
4.2 Model
We work with phrase-based models in a standard
parameterisation (Equation 2). However, to avoid
having to deal with NP-completeness, we con-
strain reordering to happen only within a limited
window given by a notion of distortion limit. We
require that the last source word covered by any
biphrase must be within d words from the leftmost
uncovered source position (Lopez, 2009). This is
a widely used strategy and it is in use in the Moses
toolkit (Koehn et al., 2007).
8
Nevertheless, the problem of finding the best
7
If d is a maximum from g and g(d) = f(d), then it is
easy to show by contradiction that d is the actual maximum
from f : if there existed d
?
such that f(d
?
) > f(d), then it
follows that g(d
?
) ? f(d
?
) > f(d) = g(d), and hence d
would not be a maximum for g.
8
A distortion limit characterises a form of pruning that
acts directly in the generative capacity of the model leading
to induction errors (Auli et al., 2009). Limiting reordering
like that lowers complexity to a polynomial function of I and
an exponential function of the distortion limit.
1240
derivation under the model remains impractica-
ble due to nonlocal parameterisation (namely,
the n-gram LM component). The weighted set
?D(x), f(d)?, which represents the objective, is
a complex hypergraph which we cannot afford
to construct. We propose to construct instead a
simpler hypergraph for which optimisation by dy-
namic programming is feasible. This proxy rep-
resents the weighted set
?
D(x), g
(0)
(d)
?
, where
g
(0)
(d) ? f(d) for every d ? D(x). Note that
this proposal contains exactly the same translation
options as in the original decoding problem. The
simplification happens only with respect to the pa-
rameterisation. Instead of intersecting the com-
plete n-gram LM distribution explicitly, we im-
plicitly intersect a simpler upperbound view of it,
where by simpler we mean lower-order.
g
(0)
(d) =
l?
i=1
?(y[e
i
]) +
l?
i=1
?(e
i
) +
l?1?
i=1
?(e
i
, e
i?1
) (3)
Equation 3 shows the model we use as a proxy
to perform exact optimisation over f . In compar-
ison to Equation 2, the term
?
l
i=1
?(y[e
i
]) replaces
?(y) = ?
?
p
LM
(y). While ? weights the yield y
taking into account all n-grams (including those
crossing the boundaries of phrases), ? weights
edges in isolation. Particularly, ?(y[e
i
]) =
?
?
q
LM
(y[e
i
]), where y[e
i
] returns the sequence of
target words (a target phrase) associated with the
edge, and q
LM
(?) is an upperbound on the true LM
probability p
LM
(?) (see ?4.3). It is obvious from
Equation 3 that our proxy model is much simpler
than the original ? the only form of nonlocal pa-
rameterisation left is the distortion penalty, which
is simple enough to represent exactly.
The program in Figure 3 illustrates the con-
struction of
?
D(x), g
(0)
(d)
?
. A nonterminal item
[l, C, r] stores: the leftmost uncovered position l
and a truncated coverage vector C (together they
track d input positions); and the rightmost position
r most recently translated (necessary for the com-
putation of the distortion penalty). Observe how
nonterminal items do not store the LM state.
9
The
rule ADJACENT expands derivations by concate-
nation with a biphrase
?
x
i
?
i
? y
j
?
j
?
starting at the
leftmost uncovered position i = l. That causes
the coverage window to move ahead to the next
leftmost uncovered position: l
?
= l + ?
1
(C) + 1,
9
Drawing a parallel to (Heafield et al., 2013a), a nontermi-
nal node in our hypergraph groups derivations while exposing
only an empty LM state.
ITEM
[
[1, I + 1], {0, 1}
d?1
, [0, I + 1]
]
GOAL [I, ?, I + 1]
AXIOMS
?BOS? BOS?
[1, 0
d?1
, 0] : ?(BOS)
ADJACENT
[l, C, r]
?
x
i
?
i
?
r
??? y
j
?
j
?
[l
?
, C
?
, i
?
] : ?
r
? ?(r, i
?
)? ?(y
j
?
j
)
i = l
?
i
?
?l
k=i?l
c
k
=
?
0
where l
?
= l + ?
1
(C) + 1
C
?
 ?
1
(C) + 1
NON-ADJACENT
[l, C, r]
?
x
i
?
i
?
r
??? y
j
?
j
?
[l, C
?
, i
?
] : ?
r
? ?(r, i
?
)? ?(y
j
?
j
)
i > l
?
i
?
?l
k=i?l
c
k
=
?
0
|r ? i+ 1| ? d
|i
?
? l + 1| ? d
where c
?
k
= c
k
if k < i? l or k > i
?
? l else
?
1
ACCEPT
[I + 1, C, r]
[I + 1, ?, I + 1] : ?(r, I + 1)? ?(EOS)
r ? I
Figure 3: Specification of the initial proposal hy-
pergraph. This program allows the same reorder-
ings as (Lopez, 2009) (see logic WLd), however,
it does not store LM state information and it uses
the upperbound LM distribution ?(?).
where ?
1
(C) returns the number of leading 1s in
C, and C
?
 ?
1
(C) + 1 represents a left-shift.
The rule NON-ADJACENT handles the remaining
cases i > l provided that the expansion skips at
most d input words |r ? i+ 1| ? d. In the conse-
quent, the window C is simply updated to record
the translation of the input span i..i
?
. In the non-
adjacent case, a gap constraint imposes that the
resulting item will require skipping no more than
d positions before the leftmost uncovered word is
translated |i
?
? l + 1| ? d.
10
Finally, note that
deductions incorporate the weighted upperbound
?(?), rather than the true LM component ?(?).
11
4.3 LM upperbound and Max-ARPA
Following Carter et al. (2012) we compute an
upperbound on n-gram conditional probabilities
by precomputing max-backoff weights stored in
a ?Max-ARPA? table, an extension of the ARPA
format (Jurafsky and Martin, 2000).
A standard ARPA table T stores entries
10
This constraint prevents items from becoming dead-ends
where incomplete derivations require a reordering step larger
than d. This is known to prevent many search errors in beam
search (Chang and Collins, 2011).
11
Unlike Aziz et al. (2013), rather than unigrams only, we
score all n-grams within a translation rule (including incom-
plete ones).
1241
?Z,Z.p,Z.b?, where Z is an n-gram equal to the
concatenation Pz of a prefix P with a word z, Z.p
is the conditional probability p(z|P), and Z.b is
a so-called ?backoff? weight associated with Z.
The conditional probability of an arbitrary n-gram
p(z|P), whether listed or not, can then be recov-
ered from T by the simple recursive procedure
shown in Equation 4, where tail deletes the first
word of the string P.
p(z|P) =
?
?
?
p(z| tail(P)) Pz 6? T and P 6? T
p(z| tail(P))? P.b Pz 6? T and P ? T
Pz.p Pz ? T
(4)
The optimistic version (or ?max-backoff?) q of
p is defined as q(z|P) ? max
H
p(z|HP), where
H varies over all possible contexts extending the
prefix P to the left. The Max-ARPA table allows to
compute q(z|P) for arbitrary values of z and P. It
is constructed on the basis of the ARPA table T by
adding two columns to T : a column Z.q that stores
the value q(z|P) and a column Z.m that stores an
optimistic version of the backoff weight.
These columns are computed offline in two
passes by first sorting T in descending order of
n-gram length.
12
In the first pass (Algorithm 1),
we compute for every entry in the table an opti-
mistic backoff weight m. In the second pass (Algo-
rithm 2), we compute for every entry an optimistic
conditional probability q by maximising over 1-
word history extensions (whose .q fields are al-
ready known due to the sorting of T ).
The following Theorem holds (see proof be-
low): For an arbitrary n-gram Z = Pz, the prob-
ability q(z|P) can be recovered through the proce-
dure shown in Equation 5.
q(z|P) =
?
?
?
p(z|P) Pz 6? T and P 6? T
p(z|P)? P.m Pz 6? T and P ? T
Pz.q Pz ? T
(5)
Note that, if Z is listed in the table, we return its
upperbound probability q directly. When the n-
gram is unknown, but its prefix is known, we take
into account the optimistic backoff weight m of the
prefix. On the other hand, if both the n-gram and
its prefix are unknown, then no additional context
could change the score of the n-gram, in which
case q(z|P) = p(z|P).
In the sequel, we will need the following defini-
tions. Suppose ? = y
J
I
is a substring of y = y
M
1
.
12
If an n-gram is listed in T , then all its substrings must
also be listed. Certain pruning strategies may corrupt this
property, in which case we make missing substrings explicit.
Then p
LM
(?) ?
?
J
k=I
p(y
k
|y
k?1
1
) is the contribu-
tion of ? to the true LM score of y. We then ob-
tain an upperbound q
LM
(?) to this contribution by
defining q
LM
(?) ? q(y
I
|)
?
J
k=I+1
q(y
k
|y
k?1
I
).
Proof of Theorem. Let us first suppose that the length
of P is strictly larger than the order n of the language
model. Then for any H, p(z|HP) = p(z|P); this is be-
cause HP /? T and P /? T , along with all intermedi-
ary strings, hence, by (4), p(z|HP) = p(z| tail(HP)) =
p(z| tail(tail(HP))) = . . . = p(z|P). Hence q(z|P) =
p(z|P), and, because Pz /? T and P /? T , the theorem
is satisfied in this case.
Having established the theorem for |P| > n, we
now assume that it is true for |P| > m and prove by
induction that it is true for |P| = m. We use the
fact that, by the definition of q, we have q(z|P) =
max
x??
q(z|xP). We have three cases to consider.
First, suppose that Pz /? T and P /? T . Then
xPz /? T and xP /? T , hence by induction q(z|xP) =
p(z|xP) = p(z|P) for any x, therefore q(z|P) =
p(z|P). We have thus proven the first case.
Second, suppose that Pz /? T and P ? T . Then, for
any x, we have xPz /? T , and:
q(z|P) = max
x??
q(z|xP)
= max( max
x??, xP/?T
q(z|xP), max
x??, xP?T
q(z|xP)).
For xP /? T , by induction, q(z|xP) = p(z|xP) =
p(z|P), and therefore max
x??, xP/?T
q(z|xP) =
p(z|P). For xP ? T , we have q(z|xP) = p(z|xP) ?
xP.m = p(z|P)? xP.b? xP.m. Thus, we have:
max
x??, xP?T
q(z|xP) = p(z|P)? max
x??, xP?T
xP.b?xP.m.
But now, because of lines 3 and 4 of Algorithm
1, P.m = max
x??, xP?T
xP.b ? xP.m, hence
max
x??, xP?T
q(z|xP) = p(z|P) ? P.m. Therefore,
q(z|P) = max(p(z|P), p(z|P)?P.m) = p(z|P)?P.m,
where we have used the fact that P.m ? 1 due to line 1
of Algorithm 1. We have thus proven the second case.
Finally, suppose that Pz ? T . Then, again,
q(z|P) = max
x??
q(z|xP)
= max(
max
x??, xPz/?T, xP/?T
q(z|xP),
max
x??, xPz/?T, xP?T
q(z|xP),
max
x??, xPz?T
q(z|xP) ).
For xPz /? T, xP /? T , we have q(z|xP) =
p(z|xP) = p(z|P) = Pz.p, where the last equality is
due to the fact that Pz ? T . For xPz /? T, xP ? T , we
have q(z|xP) = p(z|xP)? xP.m = p(z|P)? xP.b?
xP.m = Pz.p? xP.b? xP.m. For xPz ? T , we have
q(z|xP) = xPz.q. Overall, we thus have:
q(z|P) = max( Pz.p,
max
x??, xPz/?T, xP?T
Pz.p? xP.b? xP.m,
max
x??, xPz?T
xPz.q ).
Note that xPz ? T ? xP ? T , and then one can
check that Algorithm 2 exactly computes Pz.q as this
maximum over three maxima, hence Pz.q = q(z|P).
1242
Algorithm 1 Max-ARPA: first pass
1: for Z ? T do
2: Z.m? 1
3: for x ? ? s.t xZ ? T do
4: Z.m? max(Z.m,xZ.b? xZ.m)
5: end for
6: end for
Algorithm 2 Max-ARPA: second pass
1: for Z = Pz ? T do
2: Pz.q? Pz.p
3: for x ? ? s.t xP ? T do
4: if xPz ? T then
5: Pz.q? max(Pz.q,xPz.q)
6: else
7: Pz.q? max(Pz.q,Pz.p? xP.b? xP.m)
8: end if
9: end for
10: end for
4.4 Search
The search for the true optimum derivation is il-
lustrated in Algorithm 3. The algorithm takes as
input the initial proposal distribution g
(0)
(d) (see
?4.2, Figure 3) and a maximum error  (which we
set to a small constant 0.001 rather than zero, to
avoid problems with floating point precision). In
line 3 we find the optimum derivation d in g
(0)
(see ?4.5). The variable g
?
stores the maximum
score w.r.t. the current proposal, while the vari-
able f
?
stores the maximum score observed thus
far w.r.t. the true model (note that in line 5 we as-
sess the true score of d). In line 6 we start a loop
that runs until the error falls below . This error is
the difference (in log-domain) between the proxy
maximum g
?
and the best true score observed thus
far f
?
.
13
In line 7, we refine the current proposal
using evidence from d (see ?4.6). In line 9, we
update the maximum derivation searching through
the refined proposal. In line 11, we keep track of
the best score so far according to the true model,
in order to compute the updated gap in line 6.
4.5 Dynamic Programming
Finding the best derivation in a proposal hyper-
graph is straightforward with standard dynamic
programming. We can compute inside weights
in the max-times semiring in time proportional
13
Because g
(t)
upperbounds f everywhere, in optimisation
we have a guarantee that the maximum of f must lie in the
interval [f
?
, g
?
) (see Figure 2) and the quantity g
?
? f
?
is
an upperbound on the error that we incur if we early-stop the
search at any given time t. This bound provides a principled
criterion in trading accuracy for performance (a direction that
we leave for future work). Note that most algorithms for ap-
proximate search produce solutions with unbounded error.
Algorithm 3 Exact decoding
1: function OPTIMISE(g
(0)
, )
2: t? 0 . step
3: d? argmax
d
g
(t)
(d)
4: g
?
? g
(t)
(d)
5: f
?
? f(d)
6: while (q
?
? f
?
? ) do .  is the maximum error
7: g
(t+1)
? refine(g
(t)
,d) . update proposal
8: t? t+ 1
9: d? argmax
d
g
(t)
(d) . update argmax
10: g
?
? g
(t)
(d)
11: f
?
? max(f
?
, f(d)) . update ?best so far?
12: end while
13: return g
(t)
, d
14: end function
to O(|V | + |E|) (Goodman, 1999). Once inside
weights have been computed, finding the Viterbi-
derivation starting from the root is straightforward.
A simple, though important, optimisation con-
cerns the computation of inside weights. The in-
side algorithm (Baker, 1979) requires a bottom-up
traverse of the nodes in V . To do that, we topolog-
ically sort the nodes in V at time t = 0 and main-
tain a sorted list of nodes as we refine g throughout
the search ? thus avoiding having to recompute the
partial ordering of the nodes at every iteration.
4.6 Refinement
If a derivation d = argmax
d
g
(t)
(d) is such that
g
(t)
(d) f(d), there must be in d at least one n-
gram whose upperbound LM weight is far above
its true LM weight. We then lower g
(t)
locally by
refining only nonterminal nodes that participate in
d. Nonterminal nodes are refined by having their
LM states extended one word at a time.
14
For an illustration, assume we are perform-
ing optimisation with a bigram LM. Suppose
that in the first iteration a derivation d
0
=
argmax
d
g
(0)
(d) is obtained. Now consider an
edge in d
0
[l, C, r, ] ?y
1
w
?? [l
0
, C
0
, r
0
, ]
where an empty LM state is made explicit (with an
empty string ) and ?y
1
represents a target phrase.
We refine the edge?s head [l
0
, C
0
, r
0
, ] by creating
a node based on it, however, with an extended LM
state, i.e., [l
0
, C
0
, r
0
, y
1
]. This motivates a split
of the set of incoming edges to the original node,
such that, if the target projection of an incoming
14
The refinement operation is a special case of a general
finite-state intersection. However, keeping its effect local to
derivations going through a specific node is non-trivial using
the general mechanism and justifies a tailored operation.
1243
edge ends in y
1
, that edge is reconnected to the
new node as below.
[l, C, r, ] ?y
1
w
?? [l
0
, C
0
, r
0
, y
1
]
The outgoing edges from the new node are
reweighted copies of those leaving the original
node. That is, outgoing edges such as
[l
0
, C
0
, r
0
, ] y
2
?
w
??
[
l
?
, C
?
, r
?
, ?
?
]
motivate edges such as
[l
0
, C
0
, r
0
, y
1
] y
2
?
w?w
?
????
[
l
?
, C
?
, r
?
, ?
?
]
where w
?
= ?
?
q
LM
(y
1
y
2
)
/q
LM
(y
2
) is a change in LM
probability due to an extended context.
Figure 4 is the logic program that constructs the
refined hypergraph in the general case. In com-
parison to Figure 3, items are now extended to
store an LM state. The input is the original hy-
pergraph G = ?V,E? and a node v
0
? V to be
refined by left-extending its LM state ?
0
with the
word y. In the program,
?
u?
w
?? v
?
with u,v ? V
and ? ? ?
?
represents an edge in E. An item
[l, C, r, ?]
v
(annotated with a state v ? V ) rep-
resents a node (in the refined hypergraph) whose
signature is equivalent to v (in the input hyper-
graph). We start with AXIOMS by copying the
nodes in G. In COPY, edges from G are copied
unless they are headed by v
0
and their target pro-
jections end in y?
0
(the extended context). Such
edges are processed by REFINE, which instead of
copying them, creates new ones headed by a re-
fined version of v
0
. Finally, REWEIGHT contin-
ues from the refined node with reweighted copies
of the edges leaving v
0
. The weight update repre-
sents a change in LM probability (w.r.t. the upper-
bound distribution) due to an extended context.
5 Experiments
We used the dataset made available by the Work-
shop on Statistical Machine Translation (WMT)
(Bojar et al., 2013) to train a German-English
phrase-based system using the Moses toolkit
(Koehn et al., 2007) in a standard setup. For
phrase extraction, we used both Europarl (Koehn,
2005) and News Commentaries (NC) totalling
about 2.2M sentences.
15
For language modelling,
in addition to the monolingual parts of Europarl
15
Pre-processing: tokenisation, truecasing and automatic
compound-splitting (German only). Following Durrani et al.
(2013), we set the maximum phrase length to 5.
INPUT
G = ?V,E?
v
0
= [l
0
, C
0
, r
0
, ?
0
] ? V where ?
0
? ?
?
y ? ?
ITEM [l, C, r, ? ? ?
?
]
AXIOMS
[l, C, r, ?]
v
v ? V
COPY
[l, C, r, ?]
u
?
u?
w
?? v
?
[l
?
, C
?
, r
?
, ?
?
]
v
: w
v 6= v
0
? ?? 6= ?y?
0
?, ?
?
, ?, ? ? ?
?
REFINE
[l, C,R, ?]
u
?
u?
w
?? v
0
?
[l
0
, C
0
, r
0
, y?
0
] : w
?? = ?y?
0
?, ?, ? ? ?
?
REWEIGHT
[l
0
, C
0
, r
0
, y?
0
]
?
v
0
?
w
?? v
?
[l, C, r, ?]
v
: w ? w
?
?, ? ? ?
?
where w
?
= ?
?
q
LM
(y?
0
)
q
LM
(?
0
)
Figure 4: Local intersection via LM right state re-
finement. The input is a hypergraph G = ?V,E?,
a node v
0
? V singly identified by its carry
[l
0
, C
0
, r
0
, ?
0
] and a left-extension y for its LM
context ?
0
. The program copies most of the edges?
u?
w
?? v
?
? E. If a derivation goes through v
0
and the string under v
0
ends in y?
0
, the program
refines and reweights it.
and NC, we added News-2013 totalling about 25M
sentences. We performed language model interpo-
lation and batch-mira tuning (Cherry and Foster,
2012) using newstest2010 (2,849 sentence pairs).
For tuning we used cube pruning with a large beam
size (k = 5000) and a distortion limit d = 4. Un-
pruned language models were trained using lmplz
(Heafield et al., 2013b) which employs modified
Kneser-Ney smoothing (Kneser and Ney, 1995).
We report results on newstest2012.
Our exact decoder produces optimal translation
derivations for all the 3,003 sentences in the test
set. Table 1 summarises the performance of our
novel decoder for language models of order n = 3
to n = 5. For 3-gram LMs we also varied the dis-
tortion limit d (from 4 to 6). We report the average
time (in seconds) to build the initial proposal, the
total run time of the algorithm, the number of it-
erations N before convergence, and the size of the
hypergraph in the end of the search (in thousands
of nodes and thousands of edges).
16
16
The size of the initial proposal does not depend on LM
order, but rather on distortion limit (see Figure 3): on aver-
age (in thousands) |V
0
| = 0.6 and |E
0
| = 27 with d = 4,
|V
0
| = 1.3 and |E
0
| = 70 with d = 5, and |V
0
| = 2.5 and
1244
n d build (s) total (s) N |V | |E|
3 4 1.5 21 190 2.5 159
3 5 3.5 55 303 4.4 343
3 6 10 162 484 8 725
4 4 1.5 50 350 4 288
5 4 1.5 106 555 6.1 450
Table 1: Performance of the exact decoder in
terms of: time to build g
(0)
, total decoding time in-
cluding build, number of iterations (N), and num-
ber of nodes and edges (in thousands) at the end of
the search.
It is insightful to understand how different as-
pects of the initial proposal impact on perfor-
mance. Increasing the translation option limit (tol)
leads to g
(0)
having more edges (this dependency
is linear with tol). In this case, the number of
nodes is only minimally affected ? due to the pos-
sibility of a few new segmentations. The maxi-
mum phrase length (mpl) introduces in g
(0)
more
configurations of reordering constraints ([l, C] in
Figure 3). However, not many more, due to C
being limited by the distortion limit d. In prac-
tice, we observe little impact on time performance.
Increasing d introduces many more permutations
of the input leading to exponentially many more
nodes and edges. Increasing the order n of the LM
has no impact on g
(0)
and its impact on the overall
search is expressed in terms of a higher number of
nodes being locally intersected.
An increased hypergraph, be it due to addi-
tional nodes or additional edges, necessarily leads
to slower iterations because at each iteration we
must compute inside weights in timeO(|V |+|E|).
The number of nodes has the larger impact on the
number of iterations. OS
?
is very efficient in ig-
noring hypotheses (edges) that cannot compete for
an optimum. For instance, we observe that run-
ning time depends linearly on tol only through the
computation of inside weights, while the number
of iterations is only minimally affected.
17
An in-
|E
0
| = 178 with d = 6. Observe the exponential depen-
dency on distortion limit, which also leads to exponentially
longer running times.
17
It is possible to reduce the size of the hypergraph
throughout the search using the upperbound on the search
error g
?
? f
?
to prune hypotheses that surely do not stand
a chance of competing for the optimum (Graehl, 2005). An-
other direction is to group edges connecting the same nonter-
minal nodes into one partial edge (Heafield et al., 2013a) ?
this is particularly convenient due to our method only visiting
the 1-best derivation from g(d) at each iteration.
n
Nodes at level m LM states at level m
0 1 2 3 4 1 2 3 4
3 0.4 1.2 0.5 - - 113 263 - -
4 0.4 1.6 1.4 0.3 - 132 544 212 -
5 0.4 2.1 2.4 0.7 0.1 142 790 479 103
Table 2: Average number of nodes (in thousands)
whose LM state encode an m-gram, and average
number of unique LM states of order m in the fi-
nal hypergraph for different n-gram LMs (d = 4
everywhere).
creased LM order, for a fixed distortion limit, im-
pacts much more on the number of iterations than
on the average running time of a single iteration.
Fixing d = 4, the average time per iteration is 0.1
(n = 3), 0.13 (n = 4) and 0.18 (n = 5). Fixing a
3-gram LM, we observe 0.1 (d = 4), 0.17 (d = 5)
and 0.31 (d = 6). Note the exponential growth
of the latter, due to a proposal encoding exponen-
tially many more permutations.
Table 2 shows the average degree of refine-
ment of the nodes in the final proposal. Nodes
are shown by level of refinement, where m indi-
cates that they store m words in their carry. The
table also shows the number of unique m-grams
ever incorporated to the proposal. This table il-
lustrates well how our decoding algorithm moves
from a coarse upperbound where every node stores
an empty string to a variable-order representation
which is sufficient to prove an optimum derivation.
In our approach a complete derivation is opti-
mised from the proxy model at each iteration. We
observe that over 99% of these derivations project
onto distinct strings. In addition, while the opti-
mum solution may be found early in the search, a
certificate of optimality requires refining the proxy
until convergence (see ?4.1). It turns out that most
of the solutions are first encountered as late as in
the last 6-10% of the iterations.
We use the optimum derivations obtained with
our exact decoder to measure the number of search
errors made by beam search and cube pruning with
increasing beam sizes (see Table 3). Beam search
reaches optimum derivations with beam sizes k ?
500 for all language models tested. Cube prun-
ing, on the other hand, still makes mistakes at
k = 1000. Table 4 shows translation quality
achieved with different beam sizes for cube prun-
ing and compares it to exact decoding. Note that
for k ? 10
4
cube pruning converges to optimum
1245
kBeam search Cube pruning
3 4 5 3 4 5
10 938 1294 1475 2168 2347 2377
10
2
19 60 112 613 999 1126
10
3
0 0 0 29 102 167
10
4
0 0 0 0 4 7
Table 3: Beam search and cube pruning search er-
rors (out of 3,003 test samples) by beam size using
LMs of order 3 to 5 (d = 4).
order 3 4 5
k d = 4 d = 5 d = 6 d = 4 d = 4
10 20.47 20.13 19.97 20.71 20.69
10
2
21.14 21.18 21.08 21.73 21.76
10
3
21.27 21.34 21.32 21.89 21.91
10
4
21.29 21.37 21.37 21.92 21.93
OS
?
21.29 21.37 21.37 21.92 21.93
Table 4: Translation quality in terms of BLEU as
a function of beam size in cube pruning with lan-
guage models of order 3 to 5. The bottom row
shows BLEU for our exact decoder.
derivations in the vast majority of the cases (100%
with a 3-gram LM) and translation quality in terms
of BLEU is no different from OS
?
. However, with
k < 10
4
both model scores and translation quality
can be improved. Figure 5 shows a finer view on
search errors as a function of beam size for LMs
of order 3 to 5 (fixed d = 4). In Figure 6, we fix
a 3-gram LM and vary the distortion limit (from 4
to 6). Dotted lines correspond to beam search and
dashed lines correspond to cube pruning.
6 Conclusions and Future Work
We have presented an approach to decoding with
unpruned hypergraphs using upperbounds on the
language model distribution. The algorithm is an
instance of a coarse-to-fine strategy with connec-
tions to A
?
and adaptive rejection sampling known
as OS
?
. We have tested our search algorithm us-
ing state-of-the-art phrase-based models employ-
ing robust language models. Our algorithm is able
to decode all sentences of a standard test set in
manageable time consuming very little memory.
We have performed an analysis of search errors
made by beam search and cube pruning and found
that both algorithms perform remarkably well for
phrase-based decoding. In the case of cube prun-
ing, we show that model score and translation
102 103 104[log] Beam size
100
101
102
103
104
[log
] Se
arch
 err
ors
Search errors in newstest2012
CP 3-gramCP 4-gramCP 5-gramBS 3-gramBS 4-gramBS 5-gram
Figure 5: Search errors made by beam search and
cube pruning as a function of beam-size.
102 103 104[log] Beam size
100
101
102
103
104
[log
] Se
arch
 err
ors
Search errors in newstest2012 (3-gram LM)
CP d=4CP d=5CP d=6BS d=4BS d=5BS d=6
Figure 6: Search errors made by beam search and
cube pruning as a function of the distortion limit
(decoding with a 3-gram LM).
quality can be improved for beams k < 10, 000.
There are a number of directions that we intend
to investigate to speed up our decoder, such as: (1)
error-safe pruning based on search error bounds;
(2) use of reinforcement learning to guide the de-
coder in choosing which n-gram contexts to ex-
tend; and (3) grouping edges into partial edges,
effectively reducing the size of the hypergraph and
ultimately computing inside weights in less time.
Acknowledgments
The work of Wilker Aziz and Lucia Specia was
supported by EPSRC (grant EP/K024272/1).
1246
References
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of transla-
tion model search spaces. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 224?232, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Wilker Aziz, Marc Dymetman, and Sriram Venkatapa-
thy. 2013. Investigations in exact inference for hi-
erarchical translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, pages
472?483, Sofia, Bulgaria, August. Association for
Computational Linguistics.
James K. Baker. 1979. Trainable grammars for speech
recognition. In Proceedings of the Spring Confer-
ence of the Acoustical Society of America, pages
547?550, Boston, MA, June.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263?311, June.
Simon Carter, Marc Dymetman, and Guillaume
Bouchard. 2012. Exact sampling and decoding in
high-order hidden Markov models. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1125?1134, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
Lagrangian relaxation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?11, pages 26?37, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 427?436, Stroudsburg, PA,
USA. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05, pages 263?
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33:201?228.
Shay B. Cohen, Robert J. Simmons, and Noah A.
Smith. 2008. Dynamic programming algorithms as
products of weighted logic programs. In Maria Gar-
cia de la Banda and Enrico Pontelli, editors, Logic
Programming, volume 5366 of Lecture Notes in
Computer Science, pages 114?129. Springer Berlin
Heidelberg.
G Dantzig, R Fulkerson, and S Johnson. 1954. So-
lution of a large-scale traveling-salesman problem.
Operations Research, 2:393?410.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine trans-
lation systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 114?121, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 858?
866, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Marc Dymetman, Guillaume Bouchard, and Simon
Carter. 2012. Optimization and sampling for NLP
from a unified viewpoint. In Proceedings of the
First International Workshop on Optimization Tech-
niques for Human Language Technology, pages 79?
94, Mumbai, India, December. The COLING 2012
Organizing Committee.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and
applications. Discrete Applied Mathematics, 42(2-
3):177?201, April.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, ACL ?01, pages 228?
235, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Joshua Goodman. 1999. Semiring parsing. Comput.
Linguist., 25(4):573?605, December.
Jonathan Graehl. 2005. Relatively useless pruning.
Technical report, USC Information Sciences Insti-
tute.
Peter E. Hart, Nils J. Nilsson, and Bertram Raphael.
1968. A formal basis for the heuristic determina-
tion of minimum cost paths. IEEE Transactions On
Systems Science And Cybernetics, 4(2):100?107.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013a. Grouping language model boundary words
1247
to speed k-best extraction from hypergraphs. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
958?968, Atlanta, Georgia, USA, June.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013b. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 690?696, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Kenneth Heafield, Michael Kayser, and Christopher D.
Manning. 2014. Faster Phrase-Based decoding by
refining feature state. In Proceedings of the Associa-
tion for Computational Linguistics, Baltimore, MD,
USA, June.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL 2009), pages 380?388, Athens,
Greece, March. Association for Computational Lin-
guistics.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Series in Artificial In-
telligence. Prentice Hall, 1 edition.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. Ac-
coustics, Speech, and Signal Processing, 1:181?184.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Comput. Linguist.,
25(4):607?615, December.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit, pages 79?86.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine transla-
tion. Natural Language Engineering, 12(1):35?75,
March.
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine translation
with equivalent language model state maintenance.
In Proceedings of the Second Workshop on Syntax
and Structure in Statistical Translation, SSST ?08,
pages 10?18, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):8:1?8:49, August.
Adam Lopez. 2009. Translation as weighted de-
duction. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, EACL ?09, pages 532?540,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statisti-
cal machine translation. In Proceedings of the work-
shop on Data-driven methods in machine translation
- Volume 14, DMMT ?01, pages 1?8, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, volume 1 of ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 108?116, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sebastian Riedel and James Clarke. 2009. Revisit-
ing optimal decoding for machine translation IBM
model 4. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Short Pa-
pers, NAACL-Short ?09, pages 5?8, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
1248
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods (Springer Texts in Statis-
tics). Springer-Verlag New York, Inc., Secaucus,
NJ, USA.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
Lagrangian relaxation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
- Volume 1, HLT ?11, pages 72?82, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
and A. Zubiaga. 1997. A DP based search using
monotone alignments in statistical translation. In
Proceedings of the eighth conference on European
chapter of the Association for Computational Lin-
guistics, EACL ?97, pages 289?296, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-
cedda. 2009. Phrase-based statistical machine
translation as a traveling salesman problem. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1 - Volume 1, ACL ?09, pages 333?
341, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
1249
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1798?1803,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Joint Emotion Analysis via Multi-task Gaussian Processes
Daniel Beck
?
Trevor Cohn
?
Lucia Specia
?
?
Department of Computer Science, University of Sheffield, United Kingdom
{debeck1,l.specia}@sheffield.ac.uk
?
Computing and Information Systems, University of Melbourne, Australia
t.cohn@unimelb.edu.au
Abstract
We propose a model for jointly predicting
multiple emotions in natural language sen-
tences. Our model is based on a low-rank
coregionalisation approach, which com-
bines a vector-valued Gaussian Process
with a rich parameterisation scheme. We
show that our approach is able to learn
correlations and anti-correlations between
emotions on a news headlines dataset. The
proposed model outperforms both single-
task baselines and other multi-task ap-
proaches.
1 Introduction
Multi-task learning (Caruana, 1997) has been
widely used in Natural Language Processing.
Most of these learning methods are aimed for Do-
main Adaptation (Daum?e III, 2007; Finkel and
Manning, 2009), where we hypothesize that we
can learn from multiple domains by assuming sim-
ilarities between them. A more recent use of
multi-task learning is to model annotator bias and
noise for datasets labelled by multiple annotators
(Cohn and Specia, 2013).
The settings mentioned above have one aspect
in common: they assume some degree of posi-
tive correlation between tasks. In Domain Adap-
tation, we assume that some ?general?, domain-
independent knowledge exists in the data. For an-
notator noise modelling, we assume that a ?ground
truth? exists and that annotations are some noisy
deviations from this truth. However, for some set-
tings these assumptions do not necessarily hold
and often tasks can be anti-correlated. For these
cases, we need to employ multi-task methods that
are able to learn these relations from data and
correctly employ them when making predictions,
avoiding negative knowledge transfer.
An example of a problem that shows this be-
haviour is Emotion Analysis, where the goal is to
automatically detect emotions in a text (Strappa-
rava and Mihalcea, 2008; Mihalcea and Strappa-
rava, 2012). This problem is closely related to
Opinion Mining (Pang and Lee, 2008), with sim-
ilar applications, but it is usually done at a more
fine-grained level and involves the prediction of a
set of labels (one for each emotion) instead of a
single label. While we expect some emotions to
have some degree of correlation, this is usually not
the case for all possible emotions. For instance, we
expect sadness and joy to be anti-correlated.
We propose a multi-task setting for Emotion
Analysis based on a vector-valued Gaussian Pro-
cess (GP) approach known as coregionalisation
(
?
Alvarez et al., 2012). The idea is to combine a GP
with a low-rank matrix which encodes task corre-
lations. Our motivation to employ this model is
three-fold:
? Datasets for this task are scarce and small
so we hypothesize that a multi-task approach
will results in better models by allowing a
task to borrow statistical strength from other
tasks;
? The annotation scheme is subjective and very
fine-grained, and is therefore heavily prone to
bias and noise, both which can be modelled
easily using GPs;
? Finally, we also have the goal to learn a
model that shows sound and interpretable
correlations between emotions.
2 Multi-task Gaussian Process
Regression
Gaussian Processes (GPs) (Rasmussen and
Williams, 2006) are a Bayesian kernelised
framework considered the state-of-the-art for
regression. They have been recently used success-
fully for translation quality prediction (Cohn and
Specia, 2013; Beck et al., 2013; Shah et al., 2013)
1798
and modelling text periodicities (Preotiuc-Pietro
and Cohn, 2013). In the following we give a
brief description on how GPs are applied in a
regression setting.
Given an input x, the GP regression assumes
that its output y is a noise corrupted version of a
latent function evaluation, y = f(x) + ?, where
? ? N (0, ?
2
n
) is the added white noise and the
function f is drawn from a GP prior:
f(x) ? GP(?(x), k(x,x
?
)), (1)
where ?(x) is the mean function, which is usually
the 0 constant, and k(x,x
?
) is the kernel or co-
variance function, which describes the covariance
between values of f at locations x and x
?
.
To predict the value for an unseen input x
?
, we
compute the Bayesian posterior, which can be cal-
culated analytically, resulting in a Gaussian distri-
bution over the output y
?
:
1
y
?
? N (k
?
(K + ?
n
I)
?1
y
T
, (2)
k(x
?
,x
?
)? k
T
?
(K + ?
n
I)
?1
k
?
),
where K is the Gram matrix corre-
sponding to the covariance kernel evalu-
ated at every pair of training inputs and
k
?
= [?x
1
,x
?
?, ?x
2
,x
?
?, . . . , ?x
n
,x
?
?] is the
vector of kernel evaluations between the test input
and each training input.
2.1 The Intrinsic Coregionalisation Model
By extending the GP regression framework to
vector-valued outputs we obtain the so-called
coregionalisation models. Specifically, we employ
a separable vector-valued kernel known as Intrin-
sic Coregionalisation Model (ICM) (
?
Alvarez et al.,
2012). Considering a set of D tasks, we define the
corresponding vector-valued kernel as:
k((x, d), (x
?
, d
?
)) = k
data
(x,x
?
)?B
d,d
?
, (3)
where k
data
is a kernel on the input points (here
a Radial Basis Function, RBF), d and d
?
are task
or metadata information for each input and B ?
R
D?D
is the coregionalisation matrix, which en-
codes task covariances and is symmetric and posi-
tive semi-definite.
A key advantage of GP-based modelling is its
ability to learn hyperparameters directly from data
1
We refer the reader to Rasmussen and Williams (2006,
Chap. 2) for an in-depth explanation of GP regression.
by maximising the marginal likelihood:
p(y|X,?) =
?
f
p(y|X,?, f)p(f). (4)
This process is usually performed to learn the
noise variance and kernel hyperparameters, in-
cluding the coregionalisation matrix. In order to
do this, we need to consider how B is parame-
terised.
Cohn and Specia (2013) treat the diagonal val-
ues of B as hyperparameters, and as a conse-
quence are able to leverage the inter-task trans-
fer between each independent task and the global
?pooled? task. They however fix non-diagonal val-
ues to 1, which in practice is equivalent to assum-
ing equal correlation across tasks. This can be lim-
iting, in that this formulation cannot model anti-
correlations between tasks.
In this work we lift this restriction by adopting
a different parameterisation of B that allows the
learning of all task correlations. A straightforward
way to do that would be to consider every corre-
lation as an hyperparameter, but this can result in
a matrix which is not positive semi-definite (and
therefore, not a valid covariance matrix). To en-
sure this property, we follow the method proposed
by Bonilla et al. (2008), which decomposes B us-
ing Probabilistic Principal Component Analysis:
B = U?U
T
+ diag(?), (5)
where U is an D ? R matrix containing the R
principal eigenvectors and ? is a R ? R diago-
nal matrix containing the corresponding eigenval-
ues. The choice of R defines the rank of U?U
T
,
which can be understood as the capacity of the
manifold with which we model the D tasks. The
vector ? allows for each task to behave more or
less independently with respect to the global task.
The final rank of B depends on both terms in
Equation 5.
For numerical stability, we use the incomplete-
Cholesky decomposition over the matrix U?U
T
,
resulting in the following parameterisation for B:
B =
?
L
?
L
T
+ diag(?), (6)
where
?
L is a D ?R matrix. In this setting, we
treat all elements of
?
L as hyperparameters. Set-
ting a larger rank allows more flexibility in mod-
elling task correlations. However, a higher number
of hyperparameters may lead to overfitting prob-
lems or otherwise cause issues in optimisation due
1799
to additional non-convexities in the log likelihood
objective. In our experiments we evaluate this be-
haviour empirically by testing a range of ranks for
each setting.
The low-rank model can subsume the ones pro-
posed by Cohn and Specia (2013) by fixing and
tying some of the hyperparameters:
Independent: fixing
?
L = 0 and ? = 1;
Pooled: fixing
?
L = 1 and ? = 0;
Combined: fixing
?
L = 1 and tying all compo-
nents of ?;
Combined+: fixing
?
L = 1.
These formulations allow us to easily replicate
their modelling approach, which we evaluate as
competitive baselines in our experiments.
3 Experimental Setup
To address the feasibility of our approach, we pro-
pose a set of experiments with three goals in mind:
? To find our whether the ICM is able to learn
sensible emotion correlations;
? To check if these correlations are able to im-
prove predictions for unseen texts;
? To investigate the behaviour of the ICM
model as we increase the training set size.
Dataset We use the dataset provided by the ?Af-
fective Text? shared task in SemEval-2007 (Strap-
parava and Mihalcea, 2007), which is composed
of 1000 news headlines annotated in terms of six
emotions: Anger, Disgust, Fear, Joy, Sadness and
Surprise. For each emotion, a score between 0 and
100 is given, 0 meaning total lack of emotion and
100 maximum emotional load. We use 100 sen-
tences for training and the remaining 900 for test-
ing.
Model For all experiments, we use a Radial Ba-
sis Function (RBF) data kernel over a bag-of-
words feature representation. Words were down-
cased and lemmatized using the WordNet lemma-
tizer in the NLTK
2
toolkit (Bird et al., 2009). We
then use the GPy toolkit
3
to combine this kernel
with a coregionalisation model over the six emo-
tions, comparing a number of low-rank approxi-
mations.
2
http://www.nltk.org
3
http://github.com/SheffieldML/GPy
Baselines and Evaluation We compare predic-
tion results with a set of single-task baselines: a
Support Vector Machine (SVM) using an RBF
kernel with hyperparameters optimised via cross-
validation and a single-task GP, optimised via like-
lihood maximisation. The SVM models were
trained using the Scikit-learn toolkit
4
(Pedregosa
et al., 2011). We also compare our results against
the ones obtained by employing the ?Combined?
and ?Combined+? models proposed by Cohn and
Specia (2013). Following previous work in this
area, we use Pearson?s correlation coefficient as
evaluation metric.
4 Results and Discussion
4.1 Learned Task Correlations
Figure 1 shows the learned coregionalisation ma-
trix setting the initial rank as 1, reordering the
emotions to emphasize the learned structure. We
can see that the matrix follows a block structure,
clustering some of the emotions. This picture
shows two interesting behaviours:
? Sadness and fear are highly correlated. Anger
and disgust also correlate with them, al-
though to a lesser extent, and could be con-
sidered as belonging to the same cluster. We
can also see correlation between surprise and
joy. These are intuitively sound clusters
based on the polarity of these emotions.
? In addition to correlations, the model
learns anti-correlations, especially between
joy/surprise and the other emotions. We also
note that joy has the highest diagonal value,
meaning that it gives preference to indepen-
dent modelling (instead of pooling over the
remaining tasks).
Inspecting the eigenvalues of the learned ma-
trix allows us to empirically determine its result-
ing rank. In this case we find that the model has
learned a matrix of rank 3, which indicates that
our initial assumption of a rank 1 coregionalisa-
tion matrix may be too small in terms of modelling
capacity
5
. This suggests that a higher rank is
justified, although care must be taken due to the
local optima and overfitting issues cited in ?2.1.
4
http://scikit-learn.org
5
The eigenvalues were 592, 62, 86, 4, 3 ? 10
?3
and 9 ?
10
?5
.
1800
Anger Disgust Fear Joy Sadness Surprise All
SVM 0.3084 0.2135 0.3525 0.0905 0.3330 0.1148 0.2603
Single GP 0.1683 0.0035 0.3462 0.2035 0.3011 0.1599 0.3659
ICM GP (Combined) 0.2301 0.1230 0.2913 0.2202 0.2303 0.1744 0.3295
ICM GP (Combined+) 0.1539 0.1240 0.3438 0.2466 0.2850 0.2027 0.3723
ICM GP (Rank 1) 0.2133 0.1075 0.3623 0.2810 0.3137 0.2415 0.3988
ICM GP (Rank 5) 0.2542 0.1799 0.3727 0.2711 0.3157 0.2446 0.3957
Table 1: Prediction results in terms of Pearson?s correlation coefficient (higher is better). Boldface values
show the best performing model for each emotion. The scores for the ?All? column were calculated over
the predictions for all emotions concatenated (instead of just averaging over the scores for each emotion).
Figure 1: Heatmap showing a learned coregional-
isation matrix over the emotions.
4.2 Prediction Results
Table 1 shows the Pearson?s scores obtained in
our experiments. The low-rank models outper-
formed the baselines for the full task (predicting
all emotions) and for fear, joy and surprise sub-
tasks. The rank 5 models were also able to out-
perform all GP baselines for the remaining emo-
tions, but could not beat the SVM baseline. As
expected, the ?Combined? and ?Combined+? per-
formed worse than the low-rank models, probably
due to their inability to model anti-correlations.
4.3 Error analysis
To check why SVM performs better than GPs for
some emotions, we analysed their gold-standard
score distributions. Figure 2 shows the smoothed
distributions for disgust and fear, comparing the
gold-standard scores to predictions from the SVM
and GP models. The distributions for the training
set follow similar shapes.
We can see that GP obtains better matching
score distributions in the case when the gold-
Figure 2: Test score distributions for disgust and
fear. For clarity, only scores between 0 and 50 are
shown. SVM performs better on disgust, while GP
performs better on fear.
standard scores are more spread over the full sup-
port of response values, i.e., [0, 100]. Since our GP
model employs a Gaussian likelihood, it is effec-
tively minimising a squared-error loss. The SVM
model, on the other hand, uses hinge loss, which
is linear beyond the margin envelope constraints.
This affects the treatment of outlier points, which
attract quadratic cf. linear penalties for the GP
and SVM respectively. Therefore, when train-
ing scores are more uniformly distributed (which
is the case for fear), the GP model has to take the
high scores into account, resulting in broader cov-
erage of the full support. For disgust, the scores
are much more peaked near zero, favouring the
1801
more narrow coverage of the SVM.
More importantly, Figure 2 also shows that both
SVM and GP predictions tend to exhibit a Gaus-
sian shape, while the true scores show an expo-
nential behaviour. This suggests that both mod-
els are making wrong prior assumptions about the
underlying score distribution. For SVMs, this is
a non-trivial issue to address, although it is much
easier for GPs, where we can use a different like-
lihood distribution, e.g., a Beta distribution to re-
flect that the outputs are only valid over a bounded
range. Note that non-Gaussian likelihoods mean
that exact inference is no longer tractable, due to
the lack of conjugacy between the prior and likeli-
hood. However a number of approximate infer-
ence methods are appropriate which are already
widely used in the GP literature for use with non-
Gaussian likelihoods, including expectation prop-
agation (Jyl?anki et al., 2011), the Laplace approx-
imation (Williams and Barber, 1998) and Markov
Chain Monte Carlo sampling (Adams et al., 2009).
4.4 Training Set Influence
We expect multi-task models to perform better for
smaller datasets, when compared to single-task
models. This stems from the fact that with small
datasets often there is more uncertainty associated
with each task, a problem which can be alleviated
using statistics from the other tasks. To measure
this behaviour, we performed an additional exper-
iment varying the size of the training sets, while
using 100 sentences for testing.
Figure 3 shows the scores obtained. As ex-
pected, for smaller datasets the single-task mod-
els are outperformed by ICM, but their perfor-
mance become equivalent as the training set size
increases. SVM performance tends to be slightly
worse for most sizes. To study why we obtained
an outlier for the single-task model with 200 sen-
tences, we inspected the prediction values. We
found that, in this case, predictions for joy, sur-
prise and disgust were all around the same value.
6
For larger datasets, this effect disappears and the
single-task models yield good predictions.
5 Conclusions and Future Work
This paper proposed an multi-task approach for
Emotion Analysis that is able to learn correlations
6
Looking at the predictions for smaller datasets, we found
the same behaviour, but because the values found were near
the mean they did not hurt the Pearson?s score as much.
Figure 3: Pearson?s correlation score according to
training set size (in number of sentences).
and anti-correlations between emotions. Our for-
mulation is based on a combination of a Gaussian
Process and a low-rank coregionalisation model,
using a richer parameterisation that allows the
learning of fine-grained task similarities. The pro-
posed model outperformed strong baselines when
applied to a news headline dataset.
As it was discussed in Section 4.3, we plan
to further explore the possibility of using non-
Gaussian likelihoods with the GP models. An-
other research avenue we intend to explore is to
employ multiple layers of metadata, similar to the
model proposed by Cohn and Specia (2013). An
example is to incorporate the dataset provided by
Snow et al. (2008), which provides multiple non-
expert emotion annotations for each sentence, ob-
tained via crowdsourcing. Finally, another possi-
ble extension comes from more advanced vector-
valued GP models, such as the linear model of
coregionalisation (
?
Alvarez et al., 2012) or hierar-
chical kernels (Hensman et al., 2013). These mod-
els can be specially useful when we want to em-
ploy multiple kernels to explain the relation be-
tween the input data and the labels.
Acknowledgements
Daniel Beck was supported by funding from
CNPq/Brazil (No. 237999/2012-9). Dr.
Cohn is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105).
References
Ryan Prescott Adams, Iain Murray, and David J. C.
MacKay. 2009. Tractable Nonparametric Bayesian
1802
Inference in Poisson Processes with Gaussian Pro-
cess Intensities. In Proceedings of ICML, pages 1?8,
New York, New York, USA. ACM Press.
Mauricio A.
?
Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for Vector-Valued Func-
tions: a Review. Foundations and Trends in Ma-
chine Learning, pages 1?37.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite : When Less is More for
Translation Quality Estimation. In Proceedings of
WMT13, pages 337?342.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Edwin V. Bonilla, Kian Ming A. Chai, and Christopher
K. I. Williams. 2008. Multi-task Gaussian Process
Prediction. Advances in Neural Information Pro-
cessing Systems.
Rich Caruana. 1997. Multitask Learning. Machine
Learning, 28:41?75.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian Domain Adaptation. In Pro-
ceedings of NAACL.
James Hensman, Neil D Lawrence, and Magnus Rat-
tray. 2013. Hierarchical Bayesian modelling of
gene expression time series across irregularly sam-
pled replicates and clusters. BMC Bioinformatics,
14:252.
Pasi Jyl?anki, Jarno Vanhatalo, and Aki Vehtari. 2011.
Robust Gaussian Process Regression with a Student-
t Likelihood. Journal of Machine Learning Re-
search, 12:3227?3257.
Rada Mihalcea and Carlo Strapparava. 2012. Lyrics,
Music, and Emotions. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 590?599.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Duborg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Daniel Preotiuc-Pietro and Trevor Cohn. 2013. A tem-
poral model of text periodicities using Gaussian Pro-
cesses. In Proceedings of EMNLP.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and Fast - But
is it Good?: Evaluating Non-Expert Annotations
for Natural Language Tasks. In Proceedings of
EMNLP.
Carlo Strapparava and Rada Mihalcea. 2007.
SemEval-2007 Task 14 : Affective Text. In Pro-
ceedings of SEMEVAL.
Carlo Strapparava and Rada Mihalcea. 2008. Learning
to identify emotions in text. In Proceedings of the
2008 ACM Symposium on Applied Computing.
Christopher K. I. Williams and David Barber. 1998.
Bayesian Classification with Gaussian Processes.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1342?1351.
1803
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 32?42,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modelling Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Estimation
Trevor Cohn and Lucia Specia
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{t.cohn,l.specia}@sheffield.ac.uk
Abstract
Annotating linguistic data is often a com-
plex, time consuming and expensive en-
deavour. Even with strict annotation
guidelines, human subjects often deviate
in their analyses, each bringing different
biases, interpretations of the task and lev-
els of consistency. We present novel tech-
niques for learning from the outputs of
multiple annotators while accounting for
annotator specific behaviour. These tech-
niques use multi-task Gaussian Processes
to learn jointly a series of annotator and
metadata specific models, while explicitly
representing correlations between models
which can be learned directly from data.
Our experiments on two machine trans-
lation quality estimation datasets show
uniform significant accuracy gains from
multi-task learning, and consistently out-
perform strong baselines.
1 Introduction
Most empirical work in Natural Language Pro-
cessing (NLP) is based on supervised machine
learning techniques which rely on human anno-
tated data of some form or another. The annota-
tion process is often time consuming, expensive,
and prone to errors; moreover there is often con-
siderable disagreement amongst annotators.
In general, the predominant perspective to deal
with these data annotation issues in previous work
has been that there is a single underlying ground
truth, and that the annotations collected are noisy
and/or biased samples of this. The challenge is
then one of quality control, in order to process
the data by filtering, averaging or similar to dis-
til the truth. We posit that this perspective is
too limiting, especially with respect to linguis-
tic data, where each individual?s idiolect and lin-
guistic background can give rise to many different
? and yet equally valid ? truths. Particularly in
highly subjective annotation tasks, the differences
between annotators cannot be captured by simple
models such as scaling all instances of a certain
annotator by a factor. They can originate from
a number of nuanced aspects. This is the case,
for example, of annotations on the quality of sen-
tences generated using machine translation (MT)
systems, which are often used to build quality es-
timation models (Blatz et al, 2004; Specia et al,
2009) ? our application of interest.
In addition to annotators? own perceptions and
expectations with respect to translation quality, a
number of factors can affect their judgements on
specific sentences. For example, certain anno-
tators may prefer translations produced by rule-
based systems as these tend to be more grammati-
cal, while others would prefer sentences produced
by statistical systems with more adequate lexical
choices. Likewise, some annotators can be biased
by the complexity of the source sentence: lengthy
sentences are often (subconsciously) assumed to
be of low quality by some annotators. An ex-
treme case is the judgement of quality through
post-editing time: annotators have different typing
speeds, as well as levels of expertise in the task
of post-editing, proficiency levels in the language
pair, and knowledge of the terminology used in
particular sentences. These variations result in
time measurements that are not comparable across
annotators. Thus far, the use of post-editing time
has been done on an per-annotator basis (Specia,
2011), or simply averaged across multiple transla-
tors (Plitt and Masselot, 2010), both strategies far
from ideal.
Overall, these myriad of factors affecting qual-
ity judgements make the modelling of multiple
annotators a very challenging problem. This
problem is exacerbated when annotations are
provided by non-professional annotators, e.g.,
through crowdsourcing ? a common strategy used
32
to make annotation cheaper and faster, however at
the cost of less reliable outcomes.
Most related work on quality assurance for data
annotation has been developed in the context of
crowdsourcing. Common practices include fil-
tering out annotators who substantially deviate
from a gold-standard set or present unexpected
behaviours (Raykar et al, 2010; Raykar and Yu,
2012), or who disagree with others using, e.g., ma-
jority or consensus labelling (Snow et al, 2008;
Sheng et al, 2008). Another relevant strand of
work aims to model legitimate, systematic biases
in annotators (including both non-experts and ex-
perts), such as the fact that some annotators tend
to be more negative than others, and that some
annotators use a wider or narrower range of val-
ues (Flach et al, 2010; Ipeirotis et al, 2010).
However, with a few exceptions in Computer Vi-
sion (e.g., Whitehill et al (2009), Welinder et al
(2010)), existing work disregard metadata and its
impact on labelling.
In this paper we model the task of predicting the
quality of sentence translations using datasets that
have been annotated by several judges with differ-
ent levels of expertise and reliability, containing
translations from a variety of MT systems and on
a range of different types of sentences. We ad-
dress this problem using multi-task learning in
which we learn individual models for each context
(the task, incorporating the annotator and other
metadata: translation system and the source sen-
tence) while also modelling correlations between
tasks such that related tasks can mutually inform
one another. Our use of multi-task learning allows
the modelling of a diversity of truths, while also
recognising that they are rarely independent of one
another (annotators often agree) by explicitly ac-
counting for inter-annotator correlations.
Our approach is based on Gaussian Processes
(GPs) (Rasmussen and Williams, 2006), a ker-
nelised Bayesian non-parametric learning frame-
work. We develop multi-task learning models by
representing intra-task transfer simply and explic-
itly as part of a parameterised kernel function. GPs
are an extremely flexible probabilistic framework
and have been successfully adapted for multi-task
learning in a number of ways, e.g., by learning
multi-task correlations (Bonilla et al, 2008), mod-
elling per-task variance (Groot et al, 2011) or per-
annotator biases (Rogers et al, 2010). Our method
builds on the work of Bonilla et al (2008) by
explicitly modelling intra-task transfer, which is
learned automatically from the data, in order to ro-
bustly handle outlier tasks and task variances. We
show in our experiments on two translation qual-
ity datasets that these multi-task learning strate-
gies are far superior to training individual per-task
models or a single pooled model, and moreover
that our multi-task learning approach can achieve
similar performance to these baselines using only
a fraction of the training data.
In addition to showing empirical performance
gains on quality estimation applications, an im-
portant contribution of this paper is in introduc-
ing Gaussian Processes to the NLP community,1
a technique that has great potential to further per-
formance in a wider range of NLP applications.
Moreover, the algorithms proposed herein can be
adapted to improve future annotation efforts, and
subsequent use of noisy crowd-sourced data.
2 Quality Estimation
Quality estimation (QE) for MT aims at providing
an estimate on the quality of each translated seg-
ment ? typically a sentence ? without access to ref-
erence translations. Work in this area has become
increasingly popular in recent years as a conse-
quence of the widespread use of MT among real-
world users such as professional translators. Ex-
amples of applications of QE include improving
post-editing efficiency by filtering out low qual-
ity segments which would require more effort and
time to correct than translating from scratch (Spe-
cia et al, 2009), selecting high quality segments
to be published as they are, without post-editing
(Soricut and Echihabi, 2010), selecting a trans-
lation from either an MT system or a translation
memory for post-editing (He et al, 2010), select-
ing the best translation from multiple MT sys-
tems (Specia et al, 2010), and highlighting sub-
segments that need revision (Bach et al, 2011).
QE is generally addressed as a machine learn-
ing task using a variety of linear and kernel-based
regression or classification algorithms to induce
models from examples of translations described
through a number of features and annotated for
quality. For an overview of various algorithms and
features we refer the reader to the WMT12 shared
task on QE (Callison-Burch et al, 2012).
While initial work used annotations derived
1We are not strictly the first, Polajnar et al (2011) used
GPs for text classification.
33
from automatic MT evaluation metrics (Blatz et
al., 2004) such as BLEU (Papineni et al, 2002)
at training time, it soon became clear that human
labels result in significantly better models (Quirk,
2004). Current work at sentence level is thus based
on some form of human supervision.
As typical of subjective annotation tasks, QE
datasets should contain multiple annotators to lead
to models that are representative. Therefore, work
in QE faces all common issues regarding variabil-
ity in annotators? judgements. The following are a
few other features that make our datasets particu-
larly interesting:
? In order to minimise annotation costs, trans-
lation instances are often spread among anno-
tators, such that each instance is only labelled
by one or a few judges. In fact, for a sizeable
dataset (thousands of instances), the annota-
tion of a complete dataset by a single judge
may become infeasible.
? It is often desirable to include alternative
translations of source sentences produced by
multiple MT systems, which requires multi-
ple annotators for unbiased judgements, par-
ticularly for labels such as post-editing time
(a translation seen a second time will require
less editing effort).
? For crowd-sourced annotations it is often im-
possible to ensure that the same annotators
will label the same subset of cases.
These features ? which are also typical of many
other linguistic annotation tasks ? make the learn-
ing process extremely challenging. Learning mod-
els from datasets annotated by multiple annotators
remains an open challenge in QE, as we show in
Section 4. In what follows, we present our QE
datasets in more detail.
2.1 Datasets
We use two freely available QE datasets to experi-
ment with the techniques proposed in this paper:2
WMT12: This dataset was distributed as part of
the WMT12 shared task on QE (Callison-Burch et
al., 2012). It contains 1, 832 instances for train-
ing, and 422 for test. The English source sen-
tences are a subset of WMT09-12 test sets. The
Spanish MT outputs were created using a standard
PBSMT Moses engine. Each instance was anno-
tated with post-editing effort scores from highest
2Both datasets can be downloaded from http://www.
dcs.shef.ac.uk/?lucia/resources.html.
effort (score 1) to lowest effort (score 5), where
each score identifies an estimated percentage of
the MT output that needs to be corrected. The
post-editing effort scores were produced indepen-
dently by three professional translators based on
a previously post-edited translation by a fourth
translator. In an attempt to accommodate for sys-
tematic biases among annotators, the final effort
score was computed as the weighted average be-
tween the three PE-effort scores, with more weight
given to the judges with higher standard deviation
from their own mean score. This resulted in scores
spread more evenly in the [1, 5] range.
WPTP12: This dataset was distributed by Ko-
ponen et al (2012). It contains 299 English sen-
tences translated into Spanish using two or more
of eight MT systems randomly selected from all
system submissions for WMT11 (Callison-Burch
et al, 2011). These MT systems range from on-
line and customised SMT systems to commercial
rule-based systems. Translations were post-edited
by humans while time was recorded. The labels
are the number of seconds spent by a translator
editing a sentence normalised by source sentence
length. The post-editing was done by eight na-
tive speakers of Spanish, including five profes-
sional translators and three translation students.
Only 20 translations were edited by all eight an-
notators, with the remaining translations randomly
distributed amongst them. The resulting dataset
contains 1, 624 instances, which were randomly
split into 1, 300 for training and 300 for test. Ac-
cording to the analysis in (Koponen et al, 2012),
while on average certain translators were found to
be faster than others, their speed in post-editing
individual sentences varies considerably, i.e., cer-
tain translators are faster at certain sentences. To
our knowledge, no previous work has managed to
successfully model the prediction of post-editing
time from datasets with multiple annotators.
3 Gaussian Process Regression
Machine learning models for quality estimation
typically treat the problem as regression, seeking
to model the relationship between features of the
text input and the human quality judgement as a
continuous response variable. Popular choices in-
clude Support Vector Machines (SVMs), which
have been shown to perform well for quality es-
timation (Callison-Burch et al, 2012) using non-
linear kernel functions such as radial basis func-
34
tions. In this paper we consider Gaussian Pro-
cesses (GP) (Rasmussen and Williams, 2006), a
probabilistic machine learning framework incor-
porating kernels and Bayesian non-parametrics,
widely considered state-of-the-art for regression.
Despite this GPs have not been used widely to date
in statistical NLP. GPs are particularly suitable for
modelling QE for a number of reasons: 1) they
explicitly model uncertainty, which is rife in QE
datasets; 2) they allow fitting of expressive kernels
to data, in order to modulate the effect of features
of varying usefulness; and 3) they can naturally
be extended to model correlated tasks using multi-
task kernels. We now give a brief overview of GPs,
following Rasmussen and Williams (2006).
In our regression task3 the data consists of n
pairs D = {(xi, yi)}, where xi ? RF is a F -
dimensional feature vector and yi ? R is the re-
sponse variable. Each instance is a translation and
the feature vector encodes its linguistic features;
the response variable is a numerical quality judge-
ment: post editing time or likert score. As usual,
the modelling challenge is to automatically predict
the value of y based on the x for unseen test input.
GP regression assumes the presence of a la-
tent function, f : RF ? R, which maps from
the input space of feature vectors x to a scalar.
Each response value is then generated from the
function evaluated at the corresponding data point,
yi = f(xi) + ?, where ? ? N (0, ?2n) is added
white-noise. Formally f is drawn from a GP prior,
f(x) ? GP
(
0, k(x,x?)
)
,
which is parameterised by a mean (here, 0) and
a covariance kernel function k(x,x?). The ker-
nel function represents the covariance (i.e., sim-
ilarities in the response) between pairs of data
points. Intuitively, points that are in close proxim-
ity should have high covariance compared to those
that are further apart, which constrains f to be a
smoothly varying function of its inputs. This intu-
ition is embodied in the squared exponential ker-
nel (a.k.a. radial basis function or Gaussian),
k(x,x?) = ?2f exp
(
?12(x? x
?)TA?1(x? x?)
)
(1)
where ?2f is a scaling factor describing the overall
levels of variance, and A = diag(a) is a diagonal
3Our approach generalises to classification, ranking (ordi-
nal regression) or various other training objectives, including
mixtures of objectives. In this paper we use regression for
simplicity of exposition and implementation.
matrix of length scales, encoding the smoothness
of functions f with respect to each feature. Non-
uniform length scales allow for different degrees
of smoothness of f in each dimension, such that
e.g., for unimportant features f is relatively flat
whereas for very important features f is jagged,
such that a small change in the feature value has
a large effect. When the values of a are learned
automatically from data, as we do herein, this is
referred to as the automatic relevance determina-
tion (ARD) kernel.
Given the generative process defined above, we
formulate prediction as Bayesian inference under
the posterior, namely
p(y?|x?,D) =
?
f
p(y?|x?, f)p(f |D)
where x? is a test input and y? is its response
value. The posterior p(f |D) reflects our updated
belief over possible functions after observing the
training set D, i.e., f should pass close to the re-
sponse values for each training instance (but need
not fit exactly due to additive noise). This is bal-
anced against the smoothness constraints that arise
from the GP prior. The predictive posterior can be
solved analytically, resulting in
y? ? N
(
kT? (K + ?2nI)?1y, (2)
k(x?,x?)? kT? (K + ?2nI)?1k?
)
where k? = [k(x?,x1) k(x?,x2) ? ? ? k(x?,xn)]T
are the kernel evaluations between the test point
and the training set, and {Kij = k(xi,xj)} is
the kernel (gram) matrix over the training points.
Note that the posterior in Eq. 2 includes not only
the expected response (the mean) but also the vari-
ance, encoding the model?s uncertainty, which is
important for integration into subsequent process-
ing, e.g., as part of a larger probabilistic model.
GP regression also permits an analytic for-
mulation of the marginal likelihood, p(y|X) =?
f p(y|X, f)p(f), which can be used for modeltraining (X are the training inputs). Specifically,
we can derive the gradient of the (log) marginal
likelihood with respect to the model hyperparam-
eters (i.e., a, ?n, ?s etc.) and thereby find the type
II maximum likelihood estimate using gradient as-
cent. Note that in general the marginal likelihood
is non-convex in the hyperparameter values, and
consequently the solutions may only be locally op-
timal. Here we bootstrap the learning of complex
models with many hyperparameters by initialising
35
with the (good) solutions found for simpler mod-
els, thereby avoiding poor local optima. We refer
the reader to Rasmussen and Williams (2006) for
further details.
At first glance GPs resemble SVMs, which also
admit kernels such as the popular squared expo-
nential kernel in Eq. 1. The key differences are
that GPs are probabilistic models and support ex-
act Bayesian inference in the case of regression
(approximate inference is required for classifica-
tion (Rasmussen and Williams, 2006)). Moreover
GPs provide greater flexibility in fitting the ker-
nel hyperparameters even for complex composite
kernels. In typical usage, the kernel hyperparam-
eters for an SVM are fit using held-out estima-
tion, which is inefficient and often involves ty-
ing together parameters to limit the search com-
plexity (e.g., using a single scale parameter in
the squared exponential). Multiple-kernel learning
(Go?nen and Alpayd?n, 2011) goes some way to ad-
dressing this problem within the SVM framework,
however this technique is limited to reweighting
linear combinations of kernels and has high com-
putational complexity.
3.1 Multi-task Gaussian Process Models
Until now we have considered a standard regres-
sion scenario, where each training point is labelled
with a single output variable. In order to model
multiple different annotators jointly, i.e., multi-
task learning, we need to extend the model to han-
dle many tasks. Conceptually, we can consider
the multi-task model drawing a latent function for
each task, fm(x), where m ? 1, ...,M is the task
identifier. This function is then used to explain
the response values for all the instances for that
task (subject to noise). Importantly, for multi-task
learning to be of benefit, the prior over {fm} must
correlate the functions over different tasks, e.g., by
imposing similarity constraints between the values
for fm(x) and fm?(x).
We can consider two alternative perspectives
for framing the multi-task learning problem: ei-
ther isotopic where we associate each input point
x with a vector of outputs, y ? RM , one for
each of the M tasks; or heterotopic where some
of the outputs are missing, i.e., tasks are not con-
strained to share the same data points (Alvarez et
al., 2011). Given the nature of our datasets, we
opted for the heterotopic approach, which can han-
dle both singly annotated and multiply annotated
data. This can be implemented by augmenting
each input point with an additional task identity
feature, which is paired with a single y response,
and integrated into a GP model with the standard
training and inference algorithms.4
In moving to a task-augmented data representa-
tion, we need to revise our kernel function. We use
a separable multi-task kernel (Bonilla et al, 2008;
Alvarez et al, 2011) of the form
k
(
(x, d), (x?, d?)
)
= kdata(x,x?)Bd,d? , (3)
where kdata(x,x?) is a standard kernel over the in-
put points, typically a squared exponential (see
Eq. 1), and B ? RD?D is a positive semi-definite
matrix encoding task covariances. We develop
a series of increasingly complex choices for B,
which we compare empirically in Section 4.2:
Independent The simplest case is whereB = I ,
i.e., all pairs of different tasks have zero covari-
ance. This corresponds to independent modelling
of each task, although all models share the same
data kernel, so this setting is not strictly equiva-
lent to independent training with independent per-
task data kernels (with different hyperparameters).
Similarly, we might choose to use a single noise
variance, ?2n, or an independent noise variance hy-
perparameter per task.
Pooled Another extreme is B = 1, which ig-
nores the task identity, corresponding to pooling
the multi-task data into one large set. Groot et
al. (2011) present a method for applying GPs for
modelling multi-annotator data using this pool-
ing kernel with independent per-task noise terms.
They show on synthetic data experiments that this
approach works well at extracting the signal from
noise-corrupted inputs.
Combined A simple approach for B is a
weighted combination of Independent and Pool,
i.e., B = 1+ aI , where the hyperparameter a ? 0
controls the amount of inter-task transfer between
each task and the global ?pooled? task.5 For dis-
similar tasks, a high value of a allows each task to
be modelled independently, while for more simi-
lar tasks low a allows the use of a large pool of
4Note that the separable kernel (Eq. 3) gives rise to block
structured kernel matrices which permit various optimisa-
tions (Bonilla et al, 2008) to reduce the computational com-
plexity of inference, e.g., the matrix inversion in Eq. 2.
5Note that larger values of a need not affect the overall
magnitude of k, which can be down-scaled by the ?2f factorin the data kernel (Eq. 1).
36
similar data. A scaled version of this kernel has
been shown to correspond to mean regularisation
in SVMs when combined with a linear data ker-
nel (Evgeniou et al, 2006). A similar multi-task
kernel was proposed by Daume? III (2007), using
a linear data kernel and a = 1, which has shown
to result in excellent performance across a range
of NLP problems. In contrast to these earlier ap-
proaches, we learn the hyperparameter a directly,
fitting the relative amounts of inter- versus intra-
task transfer to the dataset.
Combined+ We consider an extension to the
Combined kernel, B = 1 + diag(a), ad ? 0
in which each task has a different hyperparameter
modulating its independence from the global pool.
This additional flexibility can be used, e.g., to al-
low individual outlier annotators to be modelled
independently of the others, by assigning a high
value to ad. In contrast, Combined ties together
the parameters for all tasks, i.e., all annotators are
assumed to have similar quality in that they devi-
ate from the mean to the same degree.
3.2 Integrating metadata
The approaches above assume that the data is split
into an unstructured set of M tasks, e.g., by anno-
tator. However, it is often the case that we have
additional information about each data instance in
the form of metadata. In our quality estimation
experiments we consider as metadata the MT sys-
tem which produced the translation, and the iden-
tity of the source sentence being translated. Many
other types of metadata, such as the level of expe-
rience of the annotator, could also be used. One
way of integrating such metadata would be to de-
fine a separate task for every observed combina-
tion of metadata values, in which case we treat the
metadata as a task descriptor. Doing so naively
would however incur a significant penalty, as each
task will have very few training instances result-
ing in inaccurate models, even with the inter-task
kernel approaches defined above.
We instead extend the task-level kernels to use
the task descriptors directly to represent task cor-
relations. Let B(i) be a square covariance matrix
for the ith task descriptor ofM , with a column and
row for each value (e.g., annotator identity, trans-
lation system, etc.). We redefine the task level ker-
nel using paired inputs (x,m), where m are the
task descriptors,
k
(
(x,m), (x?,m?)
)
= kdata(x,x?)
M?
i=1
B(i)mi,m?i .
This is equivalent to using a structured task-kernel
B = B(1) ? B(3) ? ? ? ? ? B(M) where ? is the
Kronecker product. Using this formulation we can
consider any of the above choices for B applied
to each task descriptor. In our experiments we
consider the Combined and Combined+ kernels,
which allow the model to learn the relative impor-
tance of each descriptor in terms of independent
modelling versus pooling the data.
4 Multi-task Quality Estimation
4.1 Experimental Setup
Feature sets: In all experiments we use 17 shal-
low QE features that have been shown to perform
well in previous work. These were used by a
highly competitive baseline entry in the WMT12
shared task, and were extracted here using the sys-
tem provided by that shared task.6 They include
simple counts, e.g., the tokens in sentences, as
well as source and target language model proba-
bilities. Each feature was scaled to have zero mean
and unit standard deviation on the training set.
Baselines: The baselines use the SVM regres-
sion algorithm with radial basis function kernel
and parameters ?,  and C optimised through grid-
search and 5-fold cross validation on the training
set. This is generally a very strong baseline: in
the WMT12 QE shared task, only five out of 19
submissions were able to significantly outperform
it, and only by including many complex additional
features, tree kernels, etc. We also present ?, a
trivial baseline based on predicting for each test
instance the training mean (overall, and for spe-
cific tasks).
GP: All GP models were implemented using the
GPML Matlab toolbox.7 Hyperparameter optimi-
sation was performed using conjugate gradient as-
cent of the log marginal likelihood function, with
up to 100 iterations. The simpler models were ini-
tialised with all hyperparameters set to one, while
more complex models were initialised using the
6The software used to extract these (and other) fea-
tures can be downloaded from http://www.quest.
dcs.shef.ac.uk/
7http://www.gaussianprocess.org/gpml/
code
37
Model MAE RMSE
? 0.8279 0.9899
SVM 0.6889 0.8201
Linear ARD 0.7063 0.8480
Squared exp. Isotropic 0.6813 0.8146
Squared exp. ARD 0.6680 0.8098
Rational quadratic ARD 0.6773 0.8238
Matern(5,2) 0.6772 0.8124
Neural network 0.6727 0.8103
Table 1: Single-task learning results on the
WMT12 dataset, trained and evaluated against
the weighted averaged response variable. ? is a
baseline which predicts the training mean, SVM
uses the same system as the WMT12 QE task, and
the remainder are GP regression models with dif-
ferent kernels (all include additive noise).
solution for a simpler model. For instance, mod-
els using ARD kernels were initialised from an
equivalent isotropic kernel (which ties all the hy-
perparameters together), and independent per-task
noise models were initialised from a single noise
model. This approach was more reliable than ran-
dom restarts in terms of accuracy and runtime ef-
ficiency.
Evaluation: We evaluate predictive accuracy
using two measures: mean absolute error,
MAE = 1N
?N
i=1 |yi ? y?i| and root mean square
error, RMSE =
?
1
N
?N
i=1 (yi ? y?i)2, where yi
are the gold standard response values and y?i are
the model predictions.
4.2 Results
Our experiments aim to demonstrate the efficacy
of GP regression, both the single task and multi-
task settings, compared to competitive baselines.
WMT12: Single task We start by comparing
GP regression with alternative approaches using
the WMT12 dataset on the standard task of pre-
dicting a weighted mean quality rating (as it was
done in the WMT12 QE shared task). Table 1
shows the results for baseline approaches and the
GP models, using a variety of different kernels
(see Rasmussen and Williams (2006) for details of
the kernel functions). From this we can see that all
models do much better than the mean baseline and
that most of the GP models have lower error than
the state-of-the-art SVM. In terms of kernels, the
linear kernel performs comparatively worse than
non-linear kernels. Overall the squared exponen-
Model MAE RMSE
? 0.8541 1.0119
Independent SVMs 0.7967 0.9673
EasyAdapt SVM 0.7655 0.9105
Independent 0.7061 0.8534
Pooled 0.7252 0.8754
Pooled & {N} 0.7050 0.8497
Combined 0.6966 0.8448
Combined & {N} 0.6975 0.8476
Combined+ 0.6975 0.8463
Combined+ & {N} 0.7046 0.8595
Table 2: Results on the WMT12 dataset, trained
and evaluated over all three annotator?s judge-
ments. Shown above are the training mean base-
line ?, single-task learning approaches, and multi-
task learning models, with the columns showing
macro average error rates over all three response
values. All systems use a squared exponential
ARD kernel in a product with the named task-
kernel, and with added noise (per-task noise is de-
noted {N}, otherwise has shared noise).
tial ARD kernel has the best performance under
both measures of error, and for this reason we use
this kernel in our subsequent experiments.
WMT12: Multi-task We now turn to the multi-
task setting, where we seek to model each of the
three annotators? predictions. Table 2 presents
the results. Note that here error rates are mea-
sured over all of the three annotators? judgements,
and consequently are higher than those measured
against their average response in Table 1. For com-
parison, taking the predictions of the best model,
Combined, in Table 2 and evaluating its averaged
prediction has a MAE of 0.6588 vs. the averaged
gold standard, significantly outperforming the best
model in Table 1.
There are a number of important findings in Ta-
ble 2. First, the independently trained models do
well, outperforming the pooled model with fixed
noise, indicating that naively pooling the data is
counter-productive and that there are annotator-
specific biases. Including per-annotator noise to
the pooled model provides a boost in performance,
however the best results are obtained using the
Combined kernel which brings the strengths of
both the independent and pooled settings. There
are only minor differences between the different
multi-task kernels, and in this case per-annotator
noise made little difference. An explanation for
the contradictory findings about the importance
38
of independent noise is that differences between
annotators can already be explained by the MTL
model using the multi-task kernel, and need not be
explained as noise.
The GP models significantly improve over
the baselines, including an SVM trained inde-
pendently and using the EasyAdapt method for
multi-task learning (Daume? III, 2007). While
EasyAdapt showed an improvement over the in-
dependent SVM, it was a long way short of the
GP models. A possible explanation is that in
EasyAdapt the multi-task sharing parameter is set
at a = 1, which may not be appropriate for the
task. In contrast the Combined GP model learned
a value of a = 0.01, weighting the value of pool-
ing much more highly than independent training.
A remaining question is how these approaches
cope with smaller datasets, where issues of data
sparsity become more prevalent. To test this, we
trained single-task, pooled and multi-task models
on randomly sub-sampled training sets of differ-
ent sizes, and plot their error rates in Figure 1.
As expected, for very small datasets pooling out-
performs single task learning, however for modest
sized datasets of ? 90 training instances pooling
was inferior. For all dataset sizes multi-task learn-
ing is superior to the other approaches, making
much better use of small and large training sets.
The MTL model trained on 500 samples had an
MAE of 0.7082? 0.0042, close to the best results
from the full dataset in Table 2, despite using 19as much data: here we use 13 as many traininginstances where each is singly (cf. triply) anno-
tated. The same experiments run with multiply-
annotated instances showed much weaker perfor-
mance, presumably due to the more limited sam-
ple of input points and poorer fit of the ARD ker-
nel hyperparameters. This finding suggests that
our multi-task learning approach could be used to
streamline annotation efforts by reducing the need
for extensive multiple annotations.
WPTP12 This dataset involves predicting the
post-editing time for eight annotators, where we
seek to test our model?s capability to use addi-
tional metadata. We model the logarithm of the
per-word post-editing time, in order to make the
response variable more comparable between an-
notators and across sentences, and generally more
Gaussian in shape. In Table 3 immediately we
can see that the baseline of predicting the train-
ing mean is very difficult to beat, and the trained
50 100 150 200 250 300 350 400 450 5000.7
0.72
0.74
0.76
0.78
0.8
0.82
Training examples 
 
STLMTLPooled
Figure 1: Learning curve comparing MAE for dif-
ferent training methods on the WMT12 dataset,
all using a squared exponential ARD data kernel
and tied noise parameter. The MTL model uses the
Combined task kernel. Each point is the average
of 5 runs, and the error bars show ?1 s.d.
systems often do worse. Partitioning the data
by annotator (?A) gives the best baseline result,
while there is less information from the MT sys-
tem or sentence identity. Single-task learning per-
forms only a little better than these baselines, al-
though some approaches such as the naive pool-
ing perform terribly. This suggests that the tasks
are highly different to one another. Interestingly,
adding the per-task noise models to the pooling ap-
proach greatly improves its performance.
The multi-task learning methods performed best
when using the annotator identity as the task de-
scriptor, and less well for the MT system and sen-
tence pair, where they only slightly improved over
the baseline. However, making use of all these lay-
ers of metadata together gives substantial further
improvements, reaching the best result with Com-
binedA,S,T . The effect of adding per-task noise to
these models was less marked than for the pooled
models, as in the WMT12 experiments. Inspecting
the learned hyperparameters, the combined mod-
els learned a large bias towards independent learn-
ing over pooling, in contrast to the WMT12 exper-
iments. This may explain the poor performance of
EasyAdapt on this dataset.
5 Conclusion
This paper presented a novel approach for learning
from human linguistic annotations by explicitly
training models of individual annotators (and pos-
sibly additional metadata) using multi-task learn-
ing. Our method using Gaussian Processes is flex-
ible, allowing easy learning of inter-dependences
between different annotators and other task meta-
39
Model MAE RMSE
? 0.5596 0.7053
?A 0.5184 0.6367
?S 0.5888 0.7588
?T 0.6300 0.8270
Pooled SVM 0.5823 0.7472
IndependentA SVM 0.5058 0.6351
EasyAdapt SVM 0.7027 0.8816
SINGLE-TASK LEARNING
IndependentA 0.5091 0.6362
IndependentS 0.5980 0.7729
Pooled 0.5834 0.7494
Pooled & {N} 0.4932 0.6275
MULTI-TASK LEARNING: Annotator
CombinedA 0.4815 0.6174
CombinedA & {N} 0.4909 0.6268
Combined+A 0.4855 0.6203
Combined+A & {N} 0.4833 0.6102
MULTI-TASK LEARNING: Translation system
CombinedS 0.5825 0.7482
MULTI-TASK LEARNING: Sentence pair
CombinedT 0.5813 0.7410
MULTI-TASK LEARNING: Combinations
CombinedA,S 0.4988 0.6490
CombinedA,S & {NA,S} 0.4707 0.6003
Combined+A,S 0.4772 0.6094
CombinedA,S,T 0.4588 0.5852
CombinedA,S,T & {NA,S} 0.4723 0.6023
Table 3: Results on the WPTP12 dataset, using
the log of the post-editing time per word as the
response variable. Shown above are the training
mean and SVM baselines, single-task learning and
multi-task learning results (micro average). The
subscripts denote the task split: annotator (A), MT
system (S) and sentence identity (T).
data. Our experiments showed how our approach
outperformed competitive baselines on two ma-
chine translation quality regression problems, in-
cluding the highly challenging problem of predict-
ing post-editing time.
In future work we plan to apply these techniques
to new datasets, particularly noisy crowd-sourced
data with much large numbers of annotators, as
well as a wider range of task types and mixtures
thereof (regression, ordinal regression, ranking,
classification). We also have preliminary positive
results for more advanced multi-task kernels, e.g.,
general dense matrices, which can induce clusters
of related tasks.
Our multi-task learning approach has much
wider application. Models of individual annota-
tors could be used to train machine translation
systems to optimise an annotator-specific quality
measure, or in active learning for corpus annota-
tion, where the model can suggest the most ap-
propriate instances for each annotator or the best
annotator for a given instance. Further, our ap-
proach contributes to work based on cheap and fast
crowdsourcing of linguistic annotation by min-
imising the need for careful data curation and
quality control.
Acknowledgements
This work was funded by PASCAL2 Harvest Pro-
gramme, as part of the QuEst project: http:
//www.quest.dcs.shef.ac.uk/. The au-
thors would like to thank Neil Lawerence and
James Hensman for advice on Gaussian Processes,
the QuEst participants, particularly Jose? Guil-
herme Camargo de Souza and Eva Hassler, and the
three anonymous reviewers.
References
Mauricio A. Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2011. Kernels for vector-valued func-
tions: A review. Foundations and Trends in Machine
Learning, 4(3):195?266.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: a method for measuring machine
translation confidence. In the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 211?
219, Portland, Oregon.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
40
Sanchis, and Nicola Ueffing. 2004. Confidence Es-
timation for Machine Translation. In the 20th Inter-
national Conference on Computational Linguistics
(Coling 2004), pages 315?321, Geneva.
Edwin Bonilla, Kian Ming Chai, and Christopher
Williams. 2008. Multi-task gaussian process pre-
diction. In Advances in Neural Information Process-
ing Systems (NIPS).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In the Sixth
Workshop on Statistical Machine Translation, pages
22?64, Edinburgh, Scotland.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In the Seventh Workshop
on Statistical Machine Translation, pages 10?51,
Montre?al, Canada.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, Prague, Czech
Republic.
Theodoros Evgeniou, Charles A. Micchelli, and Massi-
miliano Pontil. 2006. Learning multiple tasks with
kernel methods. Journal of Machine Learning Re-
search, 6(1):615.
Peter A. Flach, Sebastian Spiegler, Bruno Gole?nia, Si-
mon Price, John Guiver, Ralf Herbrich, Thore Grae-
pel, and Mohammed J. Zaki. 2010. Novel tools
to streamline the conference review process: experi-
ences from SIGKDD?09. SIGKDD Explor. Newsl.,
11(2):63?67, May.
Mehmet Go?nen and Ethem Alpayd?n. 2011. Multi-
ple kernel learning algorithms. Journal of Machine
Learning Research, 12:2211?2268.
Perry Groot, Adriana Birlutiu, and Tom Heskes. 2011.
Learning from multiple annotators with gaussian
processes. In Proceedings of the 21st international
conference on Artificial neural networks - Volume
Part II, ICANN?11, pages 159?164, Espoo, Finland.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with transla-
tion recommendation. In the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden.
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010. Quality management on amazon mechanical
turk. In Proceedings of the ACM SIGKDD Work-
shop on Human Computation, HCOMP ?10, pages
64?67, Washington DC.
Maarit Koponen, Wilker Aziz, Luciana Ramos, and
Lucia Specia. 2012. Post-editing time as a mea-
sure of cognitive effort. In Proceedings of the
AMTA 2012 Workshop on Post-editing Technology
and Practice, WPTP 2012, San Diego, CA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In the 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsyl-
vania.
Mirko Plitt and Franc?ois Masselot. 2010. A productiv-
ity test of statistical machine translation post-editing
in a typical localisation context. Prague Bull. Math.
Linguistics, 93:7?16.
Tamara Polajnar, Simon Rogers, and Mark Girolami.
2011. Protein interaction detection in sentences via
gaussian processes; a preliminary evaluation. Int. J.
Data Min. Bioinformatics, 5(1):52?72, February.
Christopher B. Quirk. 2004. Training a sentence-level
machine translation confidence metric. In Proceed-
ings of the International Conference on Language
Resources and Evaluation, volume 4 of LREC 2004,
pages 825?828, Lisbon, Portugal.
Carl E. Rasmussen and Christopher K.I. Williams.
2006. Gaussian processes for machine learning,
volume 1. MIT press Cambridge, MA.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminating
spammers and ranking annotators for crowdsourced
labeling tasks. J. Mach. Learn. Res., 13:491?518.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
J. Mach. Learn. Res., 99:1297?1322.
Simon Rogers, Mark Girolami, and Tamara Polajnar.
2010. Semi-parametric analysis of multi-rater data.
Statistics and Computing, 20(3):317?334.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? Improving data
quality and data mining using multiple, noisy la-
belers. In Proceedings of the 14th ACM SIGKDD,
KDD?08, pages 614?622, Las Vegas, Nevada.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
254?263, Honolulu, Hawaii.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 612?621, Uppsala, Swe-
den, July.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the Sentence-Level Quality of Machine Trans-
lation Systems. In the 13th Annual Meeting of
the European Association for Machine Translation
(EAMT?2009), pages 28?37, Barcelona.
41
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine translation evaluation versus quality esti-
mation. Machine Translation, pages 39?50.
Lucia Specia. 2011. Exploiting Objective Annotations
for Measuring Translation Post-editing Effort. In the
15th Annual Meeting of the European Association
for Machine Translation (EAMT?2011), pages 73?
80, Leuven.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The Multidimensional Wis-
dom of Crowds. In Advances in Neural Information
Processing Systems, volume 23, pages 2424?2432.
Jacob Whitehill, Paul Ruvolo, Ting-fan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. Advances in
Neural Information Processing Systems, 22:2035?
2043.
42
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 543?548,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Reducing Annotation Effort for Quality Estimation via Active Learning
Daniel Beck and Lucia Specia and Trevor Cohn
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{debeck1,l.specia,t.cohn}@sheffield.ac.uk
Abstract
Quality estimation models provide feed-
back on the quality of machine translated
texts. They are usually trained on human-
annotated datasets, which are very costly
due to its task-specific nature. We in-
vestigate active learning techniques to re-
duce the size of these datasets and thus
annotation effort. Experiments on a num-
ber of datasets show that with as little as
25% of the training instances it is possible
to obtain similar or superior performance
compared to that of the complete datasets.
In other words, our active learning query
strategies can not only reduce annotation
effort but can also result in better quality
predictors.
1 Introduction
The purpose of machine translation (MT) qual-
ity estimation (QE) is to provide a quality pre-
diction for new, unseen machine translated texts,
without relying on reference translations (Blatz et
al., 2004; Specia et al, 2009; Callison-Burch et
al., 2012). This task is usually addressed with
machine learning models trained on datasets com-
posed of source sentences, their machine transla-
tions, and a quality label assigned by humans. A
common use of quality predictions is the decision
between post-editing a given machine translated
sentence and translating its source from scratch,
based on whether its post-editing effort is esti-
mated to be lower than the effort of translating the
source sentence.
Since quality scores for the training of QE mod-
els are given by human experts, the annotation pro-
cess is costly and subject to inconsistencies due to
the subjectivity of the task. To avoid inconsisten-
cies because of disagreements among annotators,
it is often recommended that a QE model is trained
for each translator, based on labels given by such
a translator (Specia, 2011). This further increases
the annotation costs because different datasets are
needed for different tasks. Therefore, strategies to
reduce the demand for annotated data are needed.
Such strategies can also bring the possibility of se-
lecting data that is less prone to inconsistent anno-
tations, resulting in more robust and accurate pre-
dictions.
In this paper we investigate Active Learning
(AL) techniques to reduce the size of the dataset
while keeping the performance of the resulting
QE models. AL provides methods to select in-
formative data points from a large pool which,
if labelled, can potentially improve the perfor-
mance of a machine learning algorithm (Settles,
2010). The rationale behind these methods is to
help the learning algorithm achieve satisfactory re-
sults from only on a subset of the available data,
thus incurring less annotation effort.
2 Related Work
Most research work on QE for machine transla-
tion is focused on feature engineering and feature
selection, with some recent work on devising more
reliable and less subjective quality labels. Blatz et
al. (2004) present the first comprehensive study on
QE for MT: 91 features were proposed and used
to train predictors based on an automatic metric
(e.g. NIST (Doddington, 2002)) as the quality la-
bel. Quirk (2004) showed that small datasets man-
ually annotated by humans for quality can result
in models that outperform those trained on much
larger, automatically labelled sets.
Since quality labels are subjective to the anno-
tators? judgements, Specia and Farzindar (2010)
evaluated the performance of QE models using
HTER (Snover et al, 2006) as the quality score,
i.e., the edit distance between the MT output and
its post-edited version. Specia (2011) compared
the performance of models based on labels for
543
post-editing effort, post-editing time, and HTER.
In terms of learning algorithms, by and large
most approaches use Support Vector Machines,
particularly regression-based approaches. For an
overview on various feature sets and machine
learning algorithms, we refer the reader to a re-
cent shared task on the topic (Callison-Burch et
al., 2012).
Previous work use supervised learning methods
(?passive learning? following the AL terminol-
ogy) to train QE models. On the other hand, AL
has been successfully used in a number of natural
language applications such as text classification
(Lewis and Gale, 1994), named entity recognition
(Vlachos, 2006) and parsing (Baldridge and Os-
borne, 2004). See Olsson (2009) for an overview
on AL for natural language processing as well as
a comprehensive list of previous work.
3 Experimental Settings
3.1 Datasets
We perform experiments using four MT datasets
manually annotated for quality:
English-Spanish (en-es): 2, 254 sentences
translated by Moses (Koehn et al, 2007), as pro-
vided by the WMT12 Quality Estimation shared
task (Callison-Burch et al, 2012). Effort scores
range from 1 (too bad to be post-edited) to 5 (no
post-editing needed). Three expert post-editors
evaluated each sentence and the final score was
obtained by a weighted average between the three
scores. We use the default split given in the shared
task: 1, 832 sentences for training and 432 for
test.
French-English (fr-en): 2, 525 sentences trans-
lated by Moses as provided in Specia (2011), an-
notated by a single translator. Human labels in-
dicate post-editing effort ranging from 1 (too bad
to be post-edited) to 4 (little or no post-editing
needed). We use a random split of 90% sentences
for training and 10% for test.
Arabic-English (ar-en): 2, 585 sentences trans-
lated by two state-of-the-art SMT systems (de-
noted ar-en-1 and ar-en-2), as provided in (Specia
et al, 2011). A random split of 90% sentences for
training and 10% for test is used. Human labels in-
dicate the adequacy of the translation ranging from
1 (completely inadequate) to 4 (adequate). These
datasets were annotated by two expert translators.
3.2 Query Methods
The core of an AL setting is how the learner will
gather new instances to add to its training data. In
our setting, we use a pool-based strategy, where
the learner queries an instance pool and selects
the best instance according to an informativeness
measure. The learner then asks an ?oracle? (in this
case, the human expert) for the true label of the in-
stance and adds it to the training data.
Query methods use different criteria to predict
how informative an instance is. We experiment
with two of them: Uncertainty Sampling (US)
(Lewis and Gale, 1994) and Information Density
(ID) (Settles and Craven, 2008). In the following,
we denote M(x) the query score with respect to
method M .
According to the US method, the learner selects
the instance that has the highest labelling variance
according to its model:
US(x) = V ar(y|x)
The ID method considers that more dense regions
of the query space bring more useful information,
leveraging the instance uncertainty and its similar-
ity to all the other instances in the pool:
ID(x) = V ar(y|x)?
(
1
U
U?
u=1
sim(x, x(u))
)?
The ? parameter controls the relative importance
of the density term. In our experiments, we set it
to 1, giving equal weights to variance and density.
The U term is the number of instances in the query
pool. As similarity measure sim(x, x(u)), we use
the cosine distance between the feature vectors.
With each method, we choose the instance that
maximises its respective equation.
3.3 Experiments
To build our QE models, we extracted the 17 fea-
tures used by the baseline approach in the WMT12
QE shared task.1 These features were used with a
Support Vector Regressor (SVR) with radial basis
function and fixed hyperparameters (C=5, ?=0.01,
=0.5), using the Scikit-learn toolkit (Pedregosa
et al, 2011). For each dataset and each query
method, we performed 20 active learning simu-
lation experiments and averaged the results. We
1We refer the reader to (Callison-Burch et al, 2012) for
a detailed description of the feature set, but this was a very
strong baseline, with only five out of 19 participating systems
outperforming it.
544
started with 50 randomly selected sentences from
the training set and used all the remaining train-
ing sentences as our query pool, adding one new
sentence to the training set at each iteration.
Results were evaluated by measuring Mean Ab-
solute Error (MAE) scores on the test set. We
also performed an ?oracle? experiment: at each it-
eration, it selects the instance that minimises the
MAE on the test set. The oracle results give an
upper bound in performance for each test set.
Since an SVR does not supply variance values
for its predictions, we employ a technique known
as query-by-bagging (Abe and Mamitsuka, 1998).
The idea is to build an ensemble of N SVRs
trained on sub-samples of the training data. When
selecting a new query, the ensemble is able to re-
turnN predictions for each instance, from where a
variance value can be inferred. We used 20 SVRs
as our ensemble and 20 as the size of each training
sub-sample.2 The variance values are then used
as-is in the case of US strategy and combined with
query densities in case of the ID strategy.
4 Results and Discussion
Figure 1 shows the learning curves for all query
methods and all datasets. The ?random? curves
are our baseline since they are equivalent to pas-
sive learning (with various numbers of instances).
We first evaluated our methods in terms of how
many instances they needed to achieve 99% of the
MAE score on the full dataset. For three datasets,
the AL methods significantly outperformed the
random selection baseline, while no improvement
was observed on the ar-en-1 dataset. Results are
summarised in Table 1.
The learning curves in Figure 1 show an inter-
esting behaviour for most AL methods: some of
them were able to yield lower MAE scores than
models trained on the full dataset. This is par-
ticularly interesting in the fr-en case, where both
methods were able to obtain better scores using
only ?25% of the available instances, with the
US method resulting in 0.03 improvement. The
random selection strategy performs surprisingly
well (for some datasets it is better than the AL
strategies with certain number of instances), pro-
viding extra evidence that much smaller annotated
2We also tried sub-samples with the same size of the cur-
rent training data but this had a large impact in the query
methods running time while not yielding significantly better
results.
Figure 1: Learning curves for different query se-
lection strategies in the four datasets. The horizon-
tal axis shows the number of instances in the train-
ing set and the vertical axis shows MAE scores.
545
US ID Random Full dataset#instances MAE #instances MAE #instances MAE
en-es 959 (52%) 0.6818 549 (30%) 0.6816 1079 (59%) 0.6818 0.6750
fr-en 79 (3%) 0.5072 134 (6%) 0.5077 325 (14%) 0.5070 0.5027
ar-en-1 51 (2%) 0.6067 51 (2%) 0.6052 51 (2%) 0.6061 0.6058
ar-en-2 209 (9%) 0.6288 148 (6%) 0.6289 532 (23%) 0.6288 0.6290
Table 1: Number (proportion) of instances needed to achieve 99% of the performance of the full dataset.
Bold-faced values indicate the best performing datasets.
Best MAE US Best MAE ID Full dataset#instances MAE US MAE Random #instances MAE ID MAE Random
en-es 1832 (100%) 0.6750 0.6750 1122 (61%) 0.6722 0.6807 0.6750
fr-en 559 (25%) 0.4708 0.5010 582 (26%) 0.4843 0.5008 0.5027
ar-en-1 610 (26%) 0.5956 0.6042 351 (15%) 0.5987 0.6102 0.6058
ar-en-2 1782 (77%) 0.6212 0.6242 190 (8%) 0.6170 0.6357 0.6227
Table 2: Best MAE scores obtained in the AL experiments. For each method, the first column shows the
number (proportion) of instances used to obtain the best MAE, the second column shows the MAE score
obtained and the third column shows the MAE score for random instance selection at the same number
of instances. The last column shows the MAE obtained using the full dataset. Best scores are shown in
bold and are significantly better (paired t-test, p < 0.05) than both their randomly selected counterparts
and the full dataset MAE.
datasets than those used currently can be sufficient
for machine translation QE.
The best MAE scores achieved for each dataset
are shown in Table 2. The figures were tested for
significance using pairwise t-test with 95% confi-
dence,3 with bold-faced values in the table indicat-
ing significantly better results.
The lower bounds in MAE given by the ora-
cle curves show that AL methods can indeed im-
prove the performance of QE models: an ideal
query method would achieve a very large improve-
ment in MAE using fewer than 200 instances in all
datasets. The fact that different datasets present
similar oracle curves suggests that this is not re-
lated for a specific dataset but actually a common
behaviour in QE. Although some of this gain in
MAE may be due to overfitting to the test set, the
results obtained with the fr-en and ar-en-2 datasets
are very promising, and therefore we believe that
it is possible to use AL to improve QE results in
other cases, as long as more effective query tech-
niques are designed.
5 Further analysis on the oracle
behaviour
By analysing the oracle curves we can observe an-
other interesting phenomenon which is the rapid
increase in error when reaching the last ?200 in-
stances of the training data. A possible explana-
3We took the average of the MAE scores obtained from
the 20 runs with each query method for that.
tion for this behaviour is the existence of erro-
neous, inconsistent or contradictory labels in the
datasets. Quality annotation is a subjective task by
nature, and it is thus subject to noise, e.g., due to
misinterpretations or disagreements. Our hypothe-
sis is that these last sentences are the most difficult
to annotate and therefore more prone to disagree-
ments.
To investigate this phenomenon, we performed
an additional experiment with the en-es dataset,
the only dataset for which multiple annotations
are available (from three judges). We measure the
Kappa agreement index (Cohen, 1960) between all
pairs of judges in the subset containing the first
300 instances (the 50 initial random instances plus
250 instances chosen by the oracle). We then mea-
sured Kappa in windows of 300 instances until the
last instance of the training set is selected by the
oracle method. We also measure variances in sen-
tence length using windows of 300 instances. The
idea of this experiment is to test whether sentences
that are more difficult to annotate (because of their
length or subjectivity, generating more disagree-
ment between the judges) add noise to the dataset.
The resulting Kappa curves are shown in Fig-
ure 2: the agreement between judges is high for
the initial set of sentences selected, tends to de-
crease until it reaches ?1000 instances, and then
starts to increase again. Figure 3 shows the results
for source sentence length, which follow the same
trend (in a reversed manner). Contrary to our hy-
546
Figure 2: Kappa curves for the en-es dataset. The
horizontal axis shows the number of instances and
the vertical axis shows the kappa values. Each
point in the curves shows the kappa index for a
window containing the last 300 sentences chosen
by the oracle.
pothesis, these results suggest that the most diffi-
cult sentences chosen by the oracle are those in the
middle range instead of the last ones. If we com-
pare this trend against the oracle curve in Figure 1,
we can see that those middle instances are the ones
that do not change the performance of the oracle.
The resulting trends are interesting because they
give evidence that sentences that are difficult to an-
notate do not contribute much to QE performance
(although not hurting it either). However, they do
not confirm our hypothesis about the oracle be-
haviour. Another possible source of disagreement
is the feature set: the features may not be discrim-
inative enough to distinguish among different in-
stances, i.e., instances with very similar features
but different labels might be genuinely different,
but the current features are not sufficient to indi-
cate that. In future work we plan to further inves-
tigate this by hypothesis by using other feature sets
and analysing their behaviour.
6 Conclusions and Future Work
We have presented the first known experiments us-
ing active learning for the task of estimating ma-
chine translation quality. The results are promis-
ing: we were able to reduce the number of in-
stances needed to train the models in three of the
four datasets. In addition, in some of the datasets
active learning yielded significantly better models
using only a small subset of the training instances.
Figure 3: Average source and target sentence
lengths for the en-es dataset. The horizontal axis
shows the number of instances and the vertical
axis shows the length values. Each point in the
curves shows the average length for a window con-
taining the last 300 sentences chosen by the oracle.
The oracle results give evidence that it is possi-
ble to go beyond these encouraging results by em-
ploying better selection strategies in active learn-
ing. In future work we will investigate more
advanced query techniques that consider features
other than variance and density of the data points.
We also plan to further investigate the behaviour
of the oracle curves using not only different fea-
ture sets but also different quality scores such as
HTER and post-editing time. We believe that a
better understanding of this behaviour can guide
further developments not only for instance selec-
tion techniques but also for the design of better
quality features and quality annotation schemes.
Acknowledgments
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from the EU FP7-ICT QTLaunchPad project
(No. 296347, Lucia Specia).
References
Naoki Abe and Hiroshi Mamitsuka. 1998. Query
learning strategies using boosting and bagging. In
Proceedings of the Fifteenth International Confer-
ence on Machine Learning, pages 1?9.
Jason Baldridge and Miles Osborne. 2004. Active
learning and the total cost of annotation. In Pro-
ceedings of EMNLP, pages 9?16.
547
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315?321.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of 7th Workshop
on Statistical Machine Translation.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and psychological
measurement, 20(1):37?46.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, pages 128?132.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180.
David D. Lewis and Willian A. Gale. 1994. A sequen-
tial algorithm for training text classifiers. In Pro-
ceedings of the ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 1?
10.
Fredrik Olsson. 2009. A literature survey of active
machine learning in the context of natural language
processing. Technical report.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Duborg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and E?douard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Chris Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of
LREC, pages 825?828.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1070?1079.
Burr Settles. 2010. Active learning literature survey.
Technical report.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia and Atefeh Farzindar. 2010. Estimating
machine translation post-editing effort with HTER.
In Proceedings of AMTA Workshop Bringing MT to
the User: MT Research and the Translation Indus-
try.
Lucia Specia, M Turchi, Zhuoran Wang, and J Shawe-
Taylor. 2009. Improving the confidence of machine
translation quality estimates. In Proceedings of MT
Summit XII.
Lucia Specia, Najeh Hajlaoui, Catalina Hallett, and
Wilker Aziz. 2011. Predicting machine translation
adequacy. In Proceedings of MT Summit XIII.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of EAMT.
Andreas Vlachos. 2006. Active annotation. In Pro-
ceedings of the Workshop on Adaptive Text Extrac-
tion and Mining at EACL.
548
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79?84,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
QuEst - A translation quality estimation framework
Lucia Specia?, Kashif Shah?, Jose G. C. de Souza? and Trevor Cohn?
?Department of Computer Science
University of Sheffield, UK
{l.specia,kashif.shah,t.cohn}@sheffield.ac.uk
?Fondazione Bruno Kessler
University of Trento, Italy
desouza@fbk.eu
Abstract
We describe QUEST, an open source
framework for machine translation quality
estimation. The framework allows the ex-
traction of several quality indicators from
source segments, their translations, exter-
nal resources (corpora, language models,
topic models, etc.), as well as language
tools (parsers, part-of-speech tags, etc.). It
also provides machine learning algorithms
to build quality estimation models. We
benchmark the framework on a number of
datasets and discuss the efficacy of fea-
tures and algorithms.
1 Introduction
As Machine Translation (MT) systems become
widely adopted both for gisting purposes and to
produce professional quality translations, auto-
matic methods are needed for predicting the qual-
ity of a translated segment. This is referred to as
Quality Estimation (QE). Different from standard
MT evaluation metrics, QE metrics do not have
access to reference (human) translations; they are
aimed at MT systems in use. QE has a number of
applications, including:
? Deciding which segments need revision by a
translator (quality assurance);
? Deciding whether a reader gets a reliable gist
of the text;
? Estimating how much effort it will be needed
to post-edit a segment;
? Selecting among alternative translations pro-
duced by different MT systems;
? Deciding whether the translation can be used
for self-training of MT systems.
Work in QE for MT started in the early 2000?s,
inspired by the confidence scores used in Speech
Recognition: mostly the estimation of word pos-
terior probabilities. Back then it was called confi-
dence estimation, which we believe is a narrower
term. A 6-week workshop on the topic at John
Hopkins University in 2003 (Blatz et al, 2004)
had as goal to estimate automatic metrics such as
BLEU (Papineni et al, 2002) and WER. These
metrics are difficult to interpret, particularly at the
sentence-level, and results of their very many trials
proved unsuccessful. The overall quality of MT
was considerably lower at the time, and therefore
pinpointing the very few good quality segments
was a hard problem. No software nor datasets
were made available after the workshop.
A new surge of interest in the field started re-
cently, motivated by the widespread used of MT
systems in the translation industry, as a conse-
quence of better translation quality, more user-
friendly tools, and higher demand for translation.
In order to make MT maximally useful in this
scenario, a quantification of the quality of trans-
lated segments similar to ?fuzzy match scores?
from translation memory systems is needed. QE
work addresses this problem by using more com-
plex metrics that go beyond matching the source
segment with previously translated data. QE can
also be useful for end-users reading translations
for gisting, particularly those who cannot read the
source language.
QE nowadays focuses on estimating more inter-
pretable metrics. ?Quality? is defined according to
the application: post-editing, gisting, etc. A num-
ber of positive results have been reported. Exam-
ples include improving post-editing efficiency by
filtering out low quality segments which would re-
quire more effort or time to correct than translating
from scratch (Specia et al, 2009; Specia, 2011),
selecting high quality segments to be published as
they are, without post-editing (Soricut and Echi-
habi, 2010), selecting a translation from either
an MT system or a translation memory for post-
editing (He et al, 2010), selecting the best trans-
lation from multiple MT systems (Specia et al,
79
2010), and highlighting sub-segments that need re-
vision (Bach et al, 2011).
QE is generally addressed as a supervised ma-
chine learning task using a variety of algorithms to
induce models from examples of translations de-
scribed through a number of features and anno-
tated for quality. For an overview of various al-
gorithms and features we refer the reader to the
WMT12 shared task on QE (Callison-Burch et
al., 2012). Most of the research work lies on
deciding which aspects of quality are more rel-
evant for a given task and designing feature ex-
tractors for them. While simple features such as
counts of tokens and language model scores can be
easily extracted, feature engineering for more ad-
vanced and useful information can be quite labour-
intensive. Different language pairs or optimisation
against specific quality scores (e.g., post-editing
time vs translation adequacy) can benefit from
very different feature sets.
QUEST, our framework for quality estimation,
provides a wide range of feature extractors from
source and translation texts and external resources
and tools (Section 2). These go from simple,
language-independent features, to advanced, lin-
guistically motivated features. They include fea-
tures that rely on information from the MT sys-
tem that generated the translations, and features
that are oblivious to the way translations were
produced (Section 2.1). In addition, by inte-
grating a well-known machine learning toolkit,
scikit-learn,1 and algorithms that are known
to perform well on this task, QUEST provides a
simple and effective way of experimenting with
techniques for feature selection and model build-
ing, as well as parameter optimisation through grid
search (Section 2.2). In Section 3 we present
experiments using the framework with nine QE
datasets.
In addition to providing a practical platform
for quality estimation, by freeing researchers from
feature engineering, QUEST will facilitate work
on the learning aspect of the problem. Quality
estimation poses several machine learning chal-
lenges, such as the fact that it can exploit a large,
diverse, but often noisy set of information sources,
with a relatively small number of annotated data
points, and it relies on human annotations that are
often inconsistent due to the subjectivity of the
task (quality judgements). Moreover, QE is highly
1http://scikit-learn.org/
non-linear: unlike many other problems in lan-
guage processing, considerable improvements can
be achieved using non-linear kernel techniques.
Also, different applications for the quality predic-
tions may benefit from different machine learn-
ing techniques, an aspect that has been mostly ne-
glected so far. Finally, the framework will also
facilitate research on ways of using quality predic-
tions in novel extrinsic tasks, such as self-training
of statistical machine translation systems, and for
estimating quality in other text output applications
such as text summarisation.
2 The QUEST framework
QUEST consists of two main modules: a feature
extraction module and a machine learning mod-
ule. The first module provides a number of feature
extractors, including the most commonly used fea-
tures in the literature and by systems submitted to
the WMT12 shared task on QE (Callison-Burch et
al., 2012). More than 15 researchers from 10 in-
stitutions contributed to it as part of the QUEST
project.2 It is implemented in Java and provides
abstract classes for features, resources and pre-
processing steps so that extractors for new features
can be easily added.
The basic functioning of the feature extraction
module requires raw text files with the source and
translation texts, and a few resources (where avail-
able) such as the source MT training corpus and
language models of source and target. Configura-
tion files are used to indicate the resources avail-
able and a list of features that should be extracted.
The machine learning module provides
scripts connecting the feature files with the
scikit-learn toolkit. It also uses GPy, a
Python toolkit for Gaussian Processes regression,
which outperformed algorithms commonly used
for the task such as SVM regressors.
2.1 Feature sets
In Figure 1 we show the types of features that
can be extracted in QUEST. Although the text
unit for which features are extracted can be of any
length, most features are more suitable for sen-
tences. Therefore, a ?segment? here denotes a sen-
tence.
From the source segments QUEST can extract
features that attempt to quantify the complexity
2http://www.dcs.shef.ac.uk/?lucia/
projects/quest.html
80
Confidence indicatorsComplexity indicators Fluency indicators
Adequacyindicators
Source text TranslationMT system
Figure 1: Families of features in QUEST.
of translating those segments, or how unexpected
they are given what is known to the MT system.
Examples of features include:
? number of tokens in the source segment;
? language model (LM) probability of source
segment using the source side of the parallel
corpus used to train the MT system as LM;
? percentage of source 1?3-grams observed in
different frequency quartiles of the source
side of the MT training corpus;
? average number of translations per source
word in the segment as given by IBM 1
model with probabilities thresholded in dif-
ferent ways.
From the translated segments QUEST can ex-
tract features that attempt to measure the fluency
of such translations. Examples of features include:
? number of tokens in the target segment;
? average number of occurrences of the target
word within the target segment;
? LM probability of target segment using a
large corpus of the target language to build
the LM.
From the comparison between the source and
target segments, QUEST can extract adequacy
features, which attempt to measure whether the
structure and meaning of the source are pre-
served in the translation. Some of these are based
on word-alignment information as provided by
GIZA++. Features include:
? ratio of number of tokens in source and target
segments;
? ratio of brackets and punctuation symbols in
source and target segments;
? ratio of percentages of numbers, content- /
non-content words in the source & target seg-
ments;
? ratio of percentage of nouns/verbs/etc in the
source and target segments;
? proportion of dependency relations between
(aligned) constituents in source and target
segments;
? difference between the depth of the syntactic
trees of the source and target segments;
? difference between the number of
PP/NP/VP/ADJP/ADVP/CONJP phrases in
the source and target;
? difference between the number of per-
son/location/organization entities in source
and target sentences;
? proportion of person/location/organization
entities in source aligned to the same type of
entities in target segment;
? percentage of direct object personal or pos-
sessive pronouns incorrectly translated.
When available, information from the MT sys-
tem used to produce the translations can be very
useful, particularly for statistical machine transla-
tion (SMT). These features can provide an indi-
cation of the confidence of the MT system in the
translations. They are called ?glass-box? features,
to distinguish them from MT system-independent,
?black-box? features. To extract these features,
QUEST assumes the output of Moses-like SMT
systems, taking into account word- and phrase-
alignment information, a dump of the decoder?s
standard output (search graph information), global
model score and feature values, n-best lists, etc.
For other SMT systems, it can also take an XML
file with relevant information. Examples of glass-
box features include:
? features and global score of the SMT system;
? number of distinct hypotheses in the n-best
list;
? 1?3-gram LM probabilities using translations
in the n-best to train the LM;
? average size of the target phrases;
? proportion of pruned search graph nodes;
? proportion of recombined graph nodes.
We note that some of these features are
language-independent by definition (such as the
confidence features), while others can be depen-
dent on linguistic resources (such as POS taggers),
or very language-specific, such as the incorrect
translation of pronouns, which was designed for
Arabic-English QE.
Some word-level features have also been im-
plemented: they include standard word posterior
probabilities and n-gram probabilities for each tar-
81
get word. These can also be averaged across the
whole sentence to provide sentence-level value.
The complete list of features available is given
as part of QUEST?s documentation. At the current
stage, the number of BB features varies from 80
to 123 depending on the language pair, while GB
features go from 39 to 48 depending on the SMT
system used (see Section 3).
2.2 Machine learning
QUEST provides a command-line interface mod-
ule for the scikit-learn library implemented
in Python. This module is completely indepen-
dent from the feature extraction code and it uses
the extracted feature sets to build QE models.
The dependencies are the scikit-learn li-
brary and all its dependencies (such as NumPy3
and SciPy4). The module can be configured to
run different regression and classification algo-
rithms, feature selection methods and grid search
for hyper-parameter optimisation.
The pipeline with feature selection and hyper-
parameter optimisation can be set using a con-
figuration file. Currently, the module has an
interface for Support Vector Regression (SVR),
Support Vector Classification, and Lasso learn-
ing algorithms. They can be used in conjunction
with the feature selection algorithms (Randomised
Lasso and Randomised decision trees) and the grid
search implementation of scikit-learn to fit
an optimal model of a given dataset.
Additionally, QUEST includes Gaussian Pro-
cess (GP) regression (Rasmussen and Williams,
2006) using the GPy toolkit.5 GPs are an ad-
vanced machine learning framework incorporating
Bayesian non-parametrics and kernel machines,
and are widely regarded as state of the art for
regression. Empirically we found the perfor-
mance to be similar to SVR on most datasets,
with slightly worse MAE and better RMSE.6 In
contrast to SVR, inference in GP regression can
be expressed analytically and the model hyper-
parameters optimised directly using gradient as-
cent, thus avoiding the need for costly grid search.
This also makes the method very suitable for fea-
ture selection.
3http://www.numpy.org/
4http://www.scipy.org/
5https://github.com/SheffieldML/GPy
6This follows from the optimisation objective: GPs use a
quadratic loss (the log-likelihood of a Gaussian) compared to
SVR which penalises absolute margin violations.
Data Training Test
WMT12 (en-es) 1,832 422
EAMT11 (en-es) 900 64
EAMT11 (fr-en) 2,300 225
EAMT09-s1-s4 (en-es) 3,095 906
GALE11-s1-s2 (ar-en) 2,198 387
Table 1: Number of sentences used for training
and testing in our datasets.
3 Benchmarking
In this section we benchmark QUEST on nine ex-
isting datasets using feature selection and learning
algorithms known to perform well in the task.
3.1 Datasets
The statistics of the datasets used in the experi-
ments are shown in Table 1.7
WMT12 English-Spanish sentence translations
produced by an SMT system and judged for
post-editing effort in 1-5 (worst-best), taking a
weighted average of three annotators.
EAMT11 English-Spanish (EAMT11-en-es)
and French-English (EAMT11-fr-en) sentence
translations judged for post-editing effort in 1-4.
EAMT09 English sentences translated by four
SMT systems into Spanish and scored for post-
editing effort in 1-4. Systems are denoted by s1-s4.
GALE11 Arabic sentences translated by two
SMT systems into English and scored for ade-
quacy in 1-4. Systems are denoted by s1-s2.
3.2 Settings
Amongst the various learning algorithms available
in QUEST, to make our results comparable we se-
lected SVR with radial basis function (RBF) ker-
nel, which has been shown to perform very well
in this task (Callison-Burch et al, 2012). The op-
timisation of parameters is done with grid search
using the following ranges of values:
? penalty parameter C: [1, 10, 10]
? ?: [0.0001, 0.1, 10]
? : [0.1, 0.2, 10]
where elements in list denote beginning, end and
number of samples to generate, respectively.
For feature selection, we have experimented
with two techniques: Randomised Lasso and
7The datasets can be downloaded from http://www.
dcs.shef.ac.uk/?lucia/resources.html
82
Gaussian Processes. Randomised Lasso (Mein-
shausen and Bu?hlmann, 2010) repeatedly resam-
ples the training data and fits a Lasso regression
model on each sample. A feature is said to be se-
lected if it was selected (i.e., assigned a non-zero
weight) in at least 25% of the samples (we do this
1000 times). This strategy improves the robust-
ness of Lasso in the presence of high dimensional
and correlated inputs.
Feature selection with Gaussian Processes is
done by fitting per-feature RBF widths (also
known as the automatic relevance determination
kernel). The RBF width denotes the importance
of a feature, the narrower the RBF the more impor-
tant a change in the feature value is to the model
prediction. To make the results comparable with
our baseline systems we select the 17 top ranked
features and then train a SVR on these features.8
As feature sets, we select all features available
in QUEST for each of our datasets. We differen-
tiate between black-box (BB) and glass-box (GB)
features, as only BB are available for all datasets
(we did not have access to the MT systems that
produced the other datasets). For the WMT12 and
GALE11 datasets, we experimented with both BB
and GB features. For each dataset we build four
systems:
? BL: 17 baseline features that performed well
across languages in previous work and were
used as baseline in the WMT12 QE task.
? AF: All features available for dataset.
? FS: Feature selection for automatic ranking
and selection of top features with:
? RL: Randomised Lasso.
? GP: Gaussian Process.
Mean Absolute Error (MAE) and Root Mean
Squared Error (RMSE) are used to evaluate the
models.
3.3 Results
The error scores for all datasets with BB features
are reported in Table 2, while Table 3 shows the re-
sults with GB features, and Table 4 the results with
BB and GB features together. For each table and
dataset, bold-faced figures are significantly better
than all others (paired t-test with p ? 0.05).
It can be seen from the results that adding more
BB features (systems AF) improves the results in
most cases as compared to the baseline systems
8More features resulted in further performance gains on
most tasks, with 25?35 features giving the best results.
Dataset System #feats. MAE RMSE
WMT12
BL 17 0.6802 0.8192
AF 80 0.6703 0.8373
FS(RL) 69 0.6628 0.8107
FS(GP) 17 0.6537 0.8014
EAMT11(en-es)
BL 17 0.4867 0.6288
AF 80 0.4696 0.5438
FS(RL) 29 0.4657 0.5424
FS(GP) 17 0.4640 0.5420
EAMT11(fr-en)
BL 17 0.4387 0.6357
AF 80 0.4275 0.6211
FS(RL) 65 0.4266 0.6196
FS(GP) 17 0.4240 0.6189
EAMT09-s1
BL 17 0.5294 0.6643
AF 80 0.5235 0.6558
FS(RL) 73 0.5190 0.6516
FS(GP) 17 0.5195 0.6511
EAMT09-s2
BL 17 0.4604 0.5856
AF 80 0.4734 0.5973
FS(RL) 59 0.4601 0.5837
FS(GP) 17 0.4610 0.5825
EAMT09-s3
BL 17 0.5321 0.6643
AF 80 0.5437 0.6827
FS(RL) 67 0.5338 0.6627
FS(GP) 17 0.5320 0.6630
EAMT09-s4
BL 17 0.3583 0.4953
AF 80 0.3569 0.5000
FS(RL) 40 0.3554 0.4995
FS(GP) 17 0.3560 0.4949
GALE11-s1
BL 17 0.5456 0.6905
AF 123 0.5359 0.6665
FS(RL) 56 0.5358 0.6649
FS(GP) 17 0.5410 0.6721
GALE11-s2
BL 17 0.5532 0.7177
AF 123 0.5381 0.6933
FS(RL) 54 0.5369 0.6955
FS(GP) 17 0.5424 0.6999
Table 2: Results with BB features.
Dataset System #feats. MAE RMSE
WMT12 AF 47 0.7036 0.8476FS(RL) 26 0.6821 0.8388
FS(GP) 17 0.6771 0.8308
GALE11-s1 AF 39 0.5720 0.7392FS(RL) 46 0.5691 0.7388
FS(GP) 17 0.5711 0.7378
GALE11-s2
AF 48 0.5510 0.6977
FS(RL) 46 0.5512 0.6970
FS(GP) 17 0.5501 0.6978
Table 3: Results with GB features.
Dataset System #feats. MAE RMSE
WMT12 AF 127 0.7165 0.8476FS(RL) 26 0.6601 0.8098
FS(GP) 17 0.6501 0.7989
GALE11-s1 AF 162 0.5437 0.6741FS(RL) 69 0.5310 0.6681
FS(GP) 17 0.5370 0.6701
GALE11-s2
AF 171 0.5222 0.6499
FS(RL) 82 0.5152 0.6421
FS(GP) 17 0.5121 0.6384
Table 4: Results with BB and GB features.
83
BL, however, in some cases the improvements are
not significant. This behaviour is to be expected
as adding more features may bring more relevant
information, but at the same time it makes the rep-
resentation more sparse and the learning prone to
overfitting. In most cases, feature selection with
both or either RL and GP improves over all fea-
tures (AF). It should be noted that RL automati-
cally selects the number of features used for train-
ing while FS(GP) was limited to selecting the top
17 features in order to make the results compara-
ble with our baseline feature set. It is interesting
to note that system FS(GP) outperformed the other
systems in spite of using fewer features. This tech-
nique is promising as it reduces the time require-
ments and overall computational complexity for
training the model, while achieving similar results
compared to systems with many more features.
Another interesting question is whether these
feature selection techniques identify a common
subset of features from the various datasets. The
overall top ranked features are:
? LM perplexities and log probabilities for
source and target;
? size of source and target sentences;
? average number of possible translations of
source words (IBM 1 with thresholds);
? ratio of target by source lengths in words;
? percentage of numbers in the target sentence;
? percentage of distinct unigrams seen in the
MT source training corpus.
Interestingly, not all top ranked features are
among the baseline 17 features which are report-
edly best in literature.
GB features on their own perform worse than
BB features, but in all three datasets, the combi-
nation of GB and BB followed by feature selec-
tion resulted in significantly lower errors than us-
ing only BB features with feature selection, show-
ing that the two features sets are complementary.
4 Remarks
The source code for the framework, the datasets
and extra resources can be downloaded from
http://www.quest.dcs.shef.ac.uk/.
The project is also set to receive contribution from
interested researchers using a GitHub repository:
https://github.com/lspecia/quest.
The license for the Java code, Python and shell
scripts is BSD, a permissive license with no re-
strictions on the use or extensions of the software
for any purposes, including commercial. For pre-
existing code and resources, e.g., scikit-learn, GPy
and Berkeley parser, their licenses apply, but fea-
tures relying on these resources can be easily dis-
carded if necessary.
Acknowledgments
This work was supported by the QuEst (EU
FP7 PASCAL2 NoE, Harvest program) and QT-
LaunchPad (EU FP7 CSA No. 296347) projects.
References
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Good-
ness: a method for measuring machine translation
confidence. In ACL11, pages 211?219, Portland.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2004. Confidence Estimation for Machine Transla-
tion. In Coling04, pages 315?321, Geneva.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 workshop on statistical machine translation. In
WMT12, pages 10?51, Montre?al.
Y. He, Y. Ma, J. van Genabith, and A. Way. 2010.
Bridging SMT and TM with Translation Recom-
mendation. In ACL10, pages 622?630, Uppsala.
N. Meinshausen and P. Bu?hlmann. 2010. Stability se-
lection. Journal of the Royal Statistical Society: Se-
ries B (Statistical Methodology), 72:417?473.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL02, pages 311?318,
Philadelphia.
C.E. Rasmussen and C.K.I. Williams. 2006. Gaus-
sian processes for machine learning, volume 1. MIT
Press, Cambridge.
R. Soricut and A. Echihabi. 2010. Trustrank: Induc-
ing trust in automatic translations via ranking. In
ACL11, pages 612?621, Uppsala.
L. Specia, M. Turchi, N. Cancedda, M. Dymetman,
and N. Cristianini. 2009. Estimating the Sentence-
Level Quality of Machine Translation Systems. In
EAMT09, pages 28?37, Barcelona.
L. Specia, D. Raj, and M. Turchi. 2010. Ma-
chine translation evaluation versus quality estima-
tion. Machine Translation, 24(1):39?50.
L. Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In
EAMT11, pages 73?80, Leuven.
84
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 117?122,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
USPwlv and WLVusp: Combining Dictionaries and Contextual
Information for Cross-Lingual Lexical Substitution
Wilker Aziz
University of S?ao Paulo
S?ao Carlos, SP, Brazil
wilker.aziz@usp.br
Lucia Specia
University of Wolverhampton
Wolverhampton, UK
l.specia@wlv.ac.uk
Abstract
We describe two systems participating
in Semeval-2010?s Cross-Lingual Lexical
Substitution task: USPwlv and WLVusp.
Both systems are based on two main com-
ponents: (i) a dictionary to provide a num-
ber of possible translations for each source
word, and (ii) a contextual model to select
the best translation according to the con-
text where the source word occurs. These
components and the way they are inte-
grated are different in the two systems:
they exploit corpus-based and linguistic
resources, and supervised and unsuper-
vised learning methods. Among the 14
participants in the subtask to identify the
best translation, our systems were ranked
2nd and 4th in terms of recall, 3rd and 4th
in terms of precision. Both systems out-
performed the baselines in all subtasks ac-
cording to all metrics used.
1 Introduction
The goal of the Cross-Lingual Lexical Substitu-
tion task in Semeval-2010 (Mihalcea et al, 2010)
is to find the best (best subtask) Spanish transla-
tion or the 10-best (oot subtask) translations for
100 different English source words depending on
their context of occurrence. Source words include
nouns, adjectives, adverbs and verbs. 1, 000 oc-
currences of such words are given along with a
short context (a sentence).
This task resembles that of Word Sense Dis-
ambiguation (WSD) within Machine Translation
(MT). A few approaches have recently been pro-
posed using standard WSD features to learn mod-
els using translations instead of senses (Specia et
al., 2007; Carpuat and Wu, 2007; Chan and Ng,
2007). In such approaches, the global WSD score
is added as a feature to statistical MT systems,
along with additional features, to help the system
on its choice for the best translation of a source
word or phrase.
We exploit contextual information in alternative
ways to standard WSD features and supervised ap-
proaches. Our two systems - USPwlv and WLV
usp - use two main components: (i) a list of pos-
sible translations for the source word regardless of
its context; and (ii) a contextual model that ranks
such translations for each occurrence of the source
word given its context.
While these components constitute the core of
most WSD systems, the way they are created and
integrated in our systems differs from standard ap-
proaches. Our systems do not require a model
to disambiguate / translate each particular source
word, but instead use general models. We experi-
mented with both corpus-based and standard dic-
tionaries, and different learning methodologies to
rank the candidate translations. Our main goal was
to maximize the accuracy of the system in choos-
ing the best translation.
WLVusp is a very simple system based es-
sentially on (i) a Statistical Machine Translation
(SMT) system trained using a large parallel cor-
pus to generate the n-best translations for each
occurrence of the source words and (ii) a stan-
dard English-Spanish dictionary to filter out noisy
translations and provide additional translations in
case the SMT system was not able to produce a
large enough number of legitimate translations,
particularly for the oot subtask.
USPwlv uses a dictionary built from a large par-
allel corpus using inter-language information the-
ory metrics and an online-learning supervised al-
gorithm to rank the options from the dictionary.
The ranking is based on global and local contex-
tual features, such as the mutual information be-
tween the translation and the words in the source
context, which are trained using human annotation
on the trial dataset.
117
2 Resources
2.1 Parallel corpus
The English-Spanish part of Europarl (Koehn,
2005), a parallel corpus from the European Par-
liament proceedings, was used as a source of sen-
tence level aligned data. The nearly 1.7M sentence
pairs of English-Spanish translations, as provided
by the Fourth Workshop on Machine Translation
(WMT09
1
), sum up to approximately 48M tokens
in each language. Europarl was used both to train
the SMT system and to generate dictionaries based
on inter-language mutual information.
2.2 Dictionaries
The dictionary used by WLVusp was extracted us-
ing the free online service Word Reference
2
, which
provides two dictionaries: Espasa Concise and
Pocket Oxford Spanish Dictionary. Regular ex-
pressions were used to extract the content of the
webpages, keeping only the translations of the
words or phrasal expressions, and the outcome
was manually revised. The manual revision was
necessary to remove translations of long idiomatic
expressions which were only defined through ex-
amples, for example, for the verb check: ?we
checked up and found out he was lying ? hicimos
averiguaciones y comprobamos que ment??a?. The
resulting dictionary contains a number of open do-
main (single or multi-word) translations for each
of the 100 source words. This number varies from
3 to 91, with an average of 12.87 translations per
word. For example:
? yet.r = todav??a, a?un, ya, hasta ahora, sin em-
bargo
? paper.n = art??culo, papel, envoltorio, diario,
peri?odico, trabajo, ponencia, examen, parte,
documento, libro
Any other dictionary can in principle be used to
produce the list of translations, possibly without
manual intervention. More comprehensive dictio-
naries could result in better results, particularly
those with explicit information about the frequen-
cies of different translations. Automatic metrics
based on parallel corpus to learn the dictionary can
also be used, but we would expect the accuracy of
the system to drop in that case.
1
http://www.statmt.org/wmt09/
translation-task.html
2
http://www.wordreference.com/
The process to generate the corpus-based dic-
tionary for USPwlv is described in Section 4.
2.3 Pre-processing techniques
The Europarl parallel corpus was tokenized and
lowercased using standard tools provided by the
WMT09 competition. Additionally, the sentences
that were longer than 100 tokens after tokenization
were discarded.
Since the task specifies that translations should
be given in their basic forms, and also in order to
decrease the sparsity due to the rich morphology
of Spanish, the parallel corpus was lemmatized us-
ing TreeTagger (Schmid, 2006), a freely available
part-of-speech (POS) tagger and lemmatizer. Two
different versions of the parallel corpus were built
using both lemmatized words and their POS tags:
Lemma Words are represented by their lemma-
tized form. In case of ambiguity, the original
form was kept, in order to avoid incorrect choices.
Words that could not be lemmatized were also kept
as in their original form.
Lemma.pos Words are represented by their lem-
matized form followed by their POS tags. POS
tags representing content words are generalized
into four groups: verbs, nouns, adjectives and ad-
verbs. When the system could not identify a POS
tag, a dummy tag was used.
The same techniques were used to pre-process
the trial and test data.
2.4 Training samples
The trial data available for this task was used as a
training set for the USPwlv system, which uses a
supervised learning algorithm to learn the weights
of a number of global features. For the 300 oc-
currences of 30 words in the trial data, the ex-
pected lexical substitutions were given by the task
organizers, and therefore the feature weights could
be optimized in a way to make the system result
in good translations. These sentences were pre-
processed in the same way the parallel corpus.
3 WLVusp system
This system is based on a combination of the
Statistical Machine Translation (SMT) frame-
work using the English-Spanish Europarl data
and an English-Spanish dictionary built semi-
automatically (Section 2.2). The parallel corpus
118
was lowercased, tokenized and lemmatized (Sec-
tion 2.3) and then used to train the standard SMT
system Moses (Koehn et al, 2007) and translate
the trial/test sentences, producing the 1000-best
translations for each input sentence.
Moses produces its own dictionary from the
parallel corpus by using a word alignment tool
and heuristics to build parallel phrases of up to
seven source words and their corresponding target
words, to which are assigned translation probabil-
ities using frequency counts in the corpus. This
methodology provides some very localized con-
textual information, which can help guiding the
system towards choosing a correct translation. Ad-
ditional contextual information is used by the lan-
guage model component in Moses, which con-
siders how likely the sentence translation is in
the Spanish language (with a 5-gram language
model).
Using the phrase alignment information, the
translation of each occurrence of a source word
is identified in the output of Moses. Since the
phrase translations are learned using the Europarl
corpus, some translations are very specific to that
domain. Moreover, translations can be very noisy,
given that the process is unsupervised. We there-
fore filter the translations given by Moses to keep
only those also given as possible Spanish trans-
lations according to the semi-automatically built
English-Spanish dictionary (Section 2.2). This is
a general-domain dictionary, but it is less likely to
contain noise.
For best results, only the top translation pro-
duced by Moses is considered. If the actual trans-
lation does not belong to the dictionary, the first
translation in that dictionary is used. Although
there is no information about the order of the
translations in the dictionaries used, by looking at
the translations provided, we believe that the first
translation is in general one of the most frequent.
For oot results, the alternative translations pro-
vided by the 1000-best translations are consid-
ered. In cases where fewer than 10 translations
are found, we extract the remaining ones from the
handcrafted dictionary following their given order
until 10 translations (when available) are found,
without repetition.
WLVusp system therefore combines contextual
information as provided by Moses (via its phrases
and language model) and general translation infor-
mation as provided by a dictionary.
4 USPwlv System
For each source word occurring in the context of
a specific sentence, this system uses a linear com-
bination of features to rank the options from an
automatically built English-Spanish dictionary.
For the best subtask, the translation ranked first
is chosen, while for the oot subtask, the 10 best
ranked translations are used without repetition.
The building of the dictionary, the features used
and the learning scheme are described in what fol-
lows.
Dictionary Building The dictionary building is
based on the concept of inter-language Mutual In-
formation (MI) (Raybaud et al, 2009). It consists
in detecting which words in a source-language
sentence trigger the appearance of other words in
its target-language translation. The inter-language
MI in Equation 3 can be defined for pairs of source
(s) and target (t) words by observing their occur-
rences at the sentence level in a parallel, sentence
aligned corpus. Both simple (Equation 1) and
joint distributions (Equation 2) were built based
on the English-Spanish Europarl corpus using its
Lemma.pos version (Section 2.3).
p
l
(x) =
count
l
(x)
Total
(1)
p
en,es
(s, t) =
f
en,es
(s, t)
Total
(2)
MI(s, t) = p
en,es
(s, t)log
(
p
en,es
(s, t)
p
en
(s)p
es
(t)
)
(3)
Avg
MI
(t
j
) =
?
l
i=1
w(|i? j|)MI(s
i
, t
j
)
?
l
i=1
w(|i? j|)
(4)
In the equations, count
l
(x) is the number of sen-
tences in which the word x appear in a corpus of
l-language texts; count
en,es
(s, t) is the number of
sentences in which source and target words co-
occur in the parallel corpus; and Total is the to-
tal number of sentences in the corpus of the lan-
guage(s) under consideration. The distributions
p
en
and p
es
are monolingual and can been ex-
tracted from any monolingual corpus.
To prevent discontinuities in Equation 3, we
used a smoothing technique to avoid null proba-
bilities. We assume that any monolingual event
occurs at least once and the joint distribution is
smoothed by a Guo?s factor ? = 0.1 (Guo et al,
2004):
p
en,es
(s, t)?
p
en,es
(s, t) + ?p
en
(s)p
es
(t)
1 + ?
119
For each English source word, a list of Span-
ish translations was produced and ranked accord-
ing to inter-language MI. From the resulting list,
the 50-best translations constrained by the POS of
the original English word were selected.
Features The inter-language MI is a feature
which indicates the global suitability of translat-
ing a source token s into a target one t. However,
inter-language MI is not able to provide local con-
textual information, since it does not take into ac-
count the source context sentence c. The following
features were defined to achieve such capability:
Weighted Average MI (aMI) consists in averag-
ing the inter-language MI between the target
word t
j
and every source word s in the con-
text sentence c (Raybaud et al, 2009). The
MI component is scaled in a way that long
range dependencies are considered less im-
portant, as shown in Equation 4. The scaling
factor w(?) is assigned 1 for verbs, nouns, ad-
jectives and adverbs up to five positions from
the source word, and 0 otherwise. This fea-
ture gives an idea of how well the elements in
a window centered in the source word head
(s
j
) align to the target word t
j
, representing
the suitability of t
j
translating s
j
in the given
context.
Modified Weighted Average MI (mMI) takes
the average MI as previously defined, except
that the source word head is not taken into
account. In other words, the scaling function
in Equation 4 equals 0 also when |i? j| = 0.
It gives an idea of how well the source words
align to the target word t
j
without the strong
influence of its source translation s
j
. This
should provide less biased information to the
learning.
Best from WLVusp (B) consists in a flag that in-
dicates whether a candidate t is taken as the
best ranked option according to the WLVusp
system. The goal is to exploit the informa-
tion from the SMT system and handcrafted
dictionary used by that system.
10-best from WLVusp (T) this feature is a flag
which indicates whether a candidate t was
among the 10 best ranked translations pro-
vided by the WLVusp system.
Online Learning In order to train a binary rank-
ing system based on the trial dataset as our train-
ing set, we used the online passive-aggressive al-
gorithm MIRA (Crammer et al, 2006). MIRA is
said to be passive-aggressive because it updates
the parameters only when a misprediction is de-
tected. At training time, for each sentence a set
of pairs of candidate translations is retrieved. For
each of these pairs, the rank given by the system
with the current parameters is compared to the cor-
rect rank
h
(?). A loss function loss(?) controls the
updates attributing non 0 values only for mispre-
dictions. In our implementation, it equals 1 for
any mistake made by the model.
Each element of the kind (c, s, t) = (source
context sentence, source head, translation can-
didate) is assigned a feature vector f(c, s, t) =
?MI, aMI,mMI,B, T ?, which is modeled by a
vector of parameters w ? R
5
.
The binary ranking is defined as the task of find-
ing the best parameters w which maximize the
number of successful predictions. A successful
prediction happens when the system is able to rank
two translation candidates as expected. For do-
ing so, we define an oriented pair x = (a, b) of
candidate translations of s in the context of c and
a feature vector F (x) = f(c, s, a) ? f(c, s, b).
signal(w?F (x)) is the orientation the model gives
to x, that is, whether the system believes a is bet-
ter than b or vice versa. Based on whether or not
that orientation is the same as that of the reference
3
, the algorithm takes the decision between updat-
ing or not the parameters. When an update occurs,
it is the one that results in the minimal changes in
the parameters leading to correct labeling x, that
is, guaranteeing that after the update the system
will rank (a, b) correctly. Algorithm 1 presents
the general method, as proposed in (Crammer et
al., 2006).
In the case of this binary ranking, the minimiza-
tion problem has an analytic solution well defined
as long as f(c, s, a) 6= f(c, s, b) and rank
h
(a) 6=
rank
h
(b), otherwise signal(w ? F (x)) or the hu-
man label would not be defined, respectively.
These conditions have an impact on the content of
Pairs(c), the set of training points built upon the
system outputs for c, which can only contain pairs
of differently ranked translations.
The learning scheme was initialized with a uni-
3
Given s in the context of c and (a, b) a pair of candidate
translations of s, the reference produces 1 if rank
h
(a) >
rank
h
(b) and ?1 if rank
h
(b) > rank
h
(a).
120
Algorithm 1 MIRA
1: for c ? Training Set do
2: for x = (a, b) ? Pairs(c) do
3: y? ? signal(w ? F (x))
4: z ? correct label(x)
5: w = argmax
u
1
2
||w ? u||
2
6: s.t. u ? F (x) ? loss(y?, z)
7: v ? v + w
8: T ? T + 1
9: end for
10: end for
11: return
1
T
v
form vector. The average parameters after N = 5
iterations over the training set was taken.
5 Results
5.1 Official results
Tables 1 and 2 show the main results obtained by
our two systems in the official competition. We
contrast our systems? results against the best base-
line provided by the organizers, DIC, which con-
siders translations from a dictionary and frequency
information from WordNet, and show the relative
position of the system among the 14 participants.
The metrics are defined in (Mihalcea et al, 2010).
Subtask Metric Baseline WLVusp Position
Best
R 24.34 25.27 4
th
P 24.34 25.27 3
rd
Mode R 50.34 52.81 3
rd
Mode P 50.34 52.81 4
th
OOT
R 44.04 48.48 6
th
P 44.04 48.48 6
th
Mode R 73.53 77.91 5
th
Mode P 73.53 77.91 5
th
Table 1: Official results for WLVusp on the test set, com-
pared to the highest baseline, DICT. P = precision, R = recall.
The last column shows the relative position of the system.
Subtask Metric Baseline USPwlv Position
Best
R 24.34 26.81 2
nd
P 24.34 26.81 3
rd
Mode R 50.34 58.85 1
st
Mode P 50.34 58.85 2
nd
OOT
R 44.04 47.60 8
th
P 44.04 47.60 8
th
Mode R 73.53 79.84 3
rd
Mode P 73.53 79.84 3
rd
Table 2: Official results for USPwlv on the test set, com-
pared to the highest baseline, DICT. The last column shows
the relative position of the system.
In the oot subtask, the original systems were
able to output the mode translation approximately
80% of the times. From those translations, nearly
50% were actually considered as best options ac-
cording to human annotators. It is worth noticing
that we focused on the best subtask. Therefore,
for the oot subtask we did not exploit the fact that
translations could be repeated to form the set of 10
best translations. For certain source words, our re-
sulting set of translations is smaller than 10. For
example, in the WLVusp system, whenever the
set of alternative translations identified in Moses?
top 1000-best list did not contain 10 legitimate
translations, that is, 10 translations also found in
the handcrafted dictionary, we simply copied other
translations from that dictionary to amount 10 dif-
ferent translations. If they did not sum to 10 be-
cause the list of translations in the dictionary was
too short, we left the set as it was. As a result, 58%
of the 1000 test cases had fewer than 10 transla-
tions, many of them with as few as two or three
translations. In fact, the list of oot results for the
complete test set resulted in only 1, 950 transla-
tions, when there could be 10, 000 (1, 000 test case
occurrences ? 10 translations). In the next section
we describe some additional experiments to take
this issue into account.
5.2 Additional results
After receiving the gold-standard data, we com-
puted the scores for a number of variations of our
two systems. For example, we checked whether
the performance of USPwlv is too dependent on
the handcrafted dictionary, via the features B and
T. Table 3 presents the performance of two varia-
tions of USPwlv: MI-aMI-mMI was trained with-
out the two contextual flag features which depend
on WLVusp. MI-B-T was trained without the mu-
tual information contextual features. The variation
MI-aMI-mMI of USPwlv performs well even in
the absence of the features coming from WLVusp,
although the scores are lower. These results show
the effectiveness of the learning scheme, since
USPwlv achieves better performance by combin-
ing these feature variations, as compared to their
individual performance.
To provide an intuition on the contribution
of the two different components in the system
WLVusp, we checked the proportion of times a
translation was provided by each of the compo-
nents. In the best subtask, 48% of the translations
came from Moses, while the remaining 52% pro-
121
Subtask Metric Baseline MI-aMI-mMI MI-B-T
Best
R 24.34 22.59 20.50
P 24.34 22.59 20.50
Mode R 50.34 50.21 44.01
Mode P 50.34 50.21 44.01
OOT
R 39.65 47.60 32.75
P 44.04 39.65 32.75
Mode R 73.53 74.19 56.70
Mode P 73.53 74.19 56.70
Table 3: Comparing between variations of the system
USPwlv on the test set and the highest baseline, DICT. The
variations are different sources of contextual knowledge: MI
(MI?aMI?mMI) and the WLVusp (MI?B?T) system.
vided by Moses were not found in the dictionary.
In those cases, the first translation in the dictio-
nary was used. In the oot subtask, only 12% (246)
of the translations came from Moses, while the re-
maining (1, 704) came from the dictionary. This
can be explained by the little variation in the n-
best lists produced by Moses: most of the varia-
tions account for word-order, punctuation, etc.
Finally, we performed additional experiments in
order to exploit the possibility of replicating well
ranked translations for the oot subtask. Table 4
presents the results of some strategies arbitrarily
chosen for such replications. For example, in the
colums labelled ?5? we show the scores for re-
peating (once) the 5 top translations. Notice that
precision and recall increase as we take fewer top
translation and repeat them more times. In terms
of mode metrics, by reducing the number of dis-
tinct translations from 10 to 5, USPwlv still out-
performs (marginally) the baseline. In general, the
new systems outperform the baseline and our pre-
vious results (see Table 1 and 2) in terms of pre-
cision and recall. However, according to the other
mode metrics, they are below our official systems.
System Metric 5 4 3 2
WLVusp
R 69.09 88.36 105.32 122.29
P 69.09 88.36 105.32 122.29
Mode R 68.27 63.05 63.05 52.47
Mode P 68.27 63.05 63.05 52.47
USPwlv
R 73.50 94.78 102.96 129.09
P 73.50 94.78 102.96 129.09
Mode R 73.77 68.27 62.62 57.40
Mode P 73.77 68.27 62.62 57.40
Table 4: Comparison between different strategies for dupli-
cating answers in the task oot. The systems output a number
of distinct guesses and through arbitrarily schemes replicate
them in order to complete a list of 10 translations.
6 Discussion and future work
We have presented two systems combining con-
textual information and a pre-defined set of trans-
lations for cross-lingual lexical substitution. Both
systems performed particularly well in the best
subtask. A handcrafted dictionary has shown to be
essential for the WLVusp system and also helpful
for the USPwlv system, which uses an additional
dictionary automatically build from a parallel cor-
pus. We plan to investigate how such systems can
be improved by enhancing the corpus-based re-
sources to further minimize the dependency on the
handcrafted dictionary.
References
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 61?72.
Yee Seng Chan and Hwee Tou Ng. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In 45th Annual Meeting of the Association for
Computational Linguistics, pages 33?40.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-agressive algorithms. Jornal of Machine
Learning Research, 7:551?585.
Gang Guo, Chao Huang, Hui Jiang, and Ren-Hua
Wang. 2004. A comparative study on various con-
fidence measures in large vocabulary speech recog-
nition. In International Symposium on Chinese Spo-
ken Language Processing, pages 9?12.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. Semeval-2010 task 2: Cross-lingual lexical
substitution. In SemEval-2010: 5th International
Workshop on Semantic Evaluations.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Smaili. 2009. Word- and sentence-
level confidence measures for machine translation.
In 13th Annual Conference of the European Associ-
ation for Machine Translation, pages 104?111.
Helmut Schmid. 2006. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Natural Language Pro-
cessing, pages 44?49.
Lucia Specia, Mark Stevenson, and Maria das Grac?as
Volpe Nunes. 2007. Learning expressive models for
word sense disambiguation. In 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 41?148.
122
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347?355,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 1: English Lexical Simplification
Lucia Specia
Department of Computer Science
University of Sheffield
L.Specia@sheffield.ac.uk
Sujay Kumar Jauhar
Research Group in Computational Linguistics
University of Wolverhampton
Sujay.KumarJauhar@wlv.ac.uk
Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
rada@cs.unt.edu
Abstract
We describe the English Lexical Simplifica-
tion task at SemEval-2012. This is the first
time such a shared task has been organized
and its goal is to provide a framework for the
evaluation of systems for lexical simplification
and foster research on context-aware lexical
simplification approaches. The task requires
that annotators and systems rank a number of
alternative substitutes ? all deemed adequate ?
for a target word in context, according to how
?simple? these substitutes are. The notion of
simplicity is biased towards non-native speak-
ers of English. Out of nine participating sys-
tems, the best scoring ones combine context-
dependent and context-independent informa-
tion, with the strongest individual contribution
given by the frequency of the substitute re-
gardless of its context.
1 Introduction
Lexical Simplification is a subtask of Text Simpli-
fication (Siddharthan, 2006) concerned with replac-
ing words or short phrases by simpler variants in a
context aware fashion (generally synonyms), which
can be understood by a wider range of readers. It
generally envisages a certain human target audience
that may find it difficult or impossible to understand
complex words or phrases, e.g., children, people
with poor literacy levels or cognitive disabilities, or
second language learners. It is similar in many re-
spects to the task of Lexical Substitution (McCarthy
and Navigli, 2007) in that it involves determining
adequate substitutes in context, but in this case on
the basis of a predefined criterion: simplicity.
A common pipeline for a Lexical Simplification
system includes at least three major components: (i)
complexity analysis: selection of words or phrases
in a text that are considered complex for the reader
and/or task at hand; (ii) substitute lookup: search
for adequate replacement words or phrases deemed
complex in context, e.g., taking synonyms (with
the same sense) from a thesaurus or finding similar
words/phrases in a corpus using distributional simi-
larity metrics; and (iii) context-based ranking: rank-
ing of substitutes according to how simple they are
to the reader/task at hand.
As an example take the sentence: ?Hitler com-
mitted terrible atrocities during the second World
War.? The system would first identify complex
words, e.g. atrocities, then search for substitutes
that might adequately replace it. A thesaurus lookup
would yield the following synonyms: abomination,
cruelty, enormity and violation, but enormity should
be dropped as it does not fit the context appropri-
ately. Finally, the system would determine the sim-
plest of these substitutes, e.g., cruelty, and use it
to replace the complex word, yielding the sentence:
?Hitler committed terrible cruelties during the sec-
ond World War.?.
Different from other subtasks of Text Simplifica-
tion like Syntactic Simplification, which have been
relatively well studied, Lexical Simplification has
received less attention. Although a few recent at-
tempts explicitly address dependency on context (de
Belder et al, 2010; Yatskar et al, 2010; Biran et al,
2011; Specia, 2010), most approaches are context-
independent (Candido et al, 2009; Devlin and Tait,
1998). In addition, a general deeper understanding
347
of the problem is yet to be gained. As a first attempt
to address this problem in the shape of a shared task,
the English Simplification task at SemEval-2012 fo-
cuses on the third component, which we believe is
the core of the Lexical Simplification problem.
The SemEval-2012 shared task on English Lexi-
cal Simplification has been conceived with the fol-
lowing main purposes: advancing the state-of-the-
art Lexical Simplification approaches, and provid-
ing a common framework for evaluation of Lexical
Simplification systems for participants and other re-
searchers interested in the field. Another central mo-
tive of such a shared task is to bring awareness to the
general vagueness associated with the notion of lex-
ical simplicity. Our hypothesis is that in addition to
the notion of a target application/reader, the notion
of simplicity is highly context-dependent. In other
words, given the same list of substitutes for a given
target word with the same sense, we expect different
orderings of these substitutes in different contexts.
We hope that participation in this shared task will
help discover some underlying traits of lexical sim-
plicity and furthermore shed some light on how this
may be leveraged in future work.
2 Task definition
Given a short context, a target word in English,
and several substitutes for the target word that are
deemed adequate for that context, the goal of the
English Simplification task at SemEval-2012 is to
rank these substitutes according to how ?simple?
they are, allowing ties. Simple words/phrases are
loosely defined as those which can be understood by
a wide range of people, including those with low lit-
eracy levels or some cognitive disability, children,
and non-native speakers of English. In particular,
the data provided as part of the task is annotated by
fluent but non-native speakers of English.
The task thus essentially involves comparing
words or phrases and determining their order of
complexity. By ranking the candidates, as opposed
to categorizing them into specific labels (simple,
moderate, complex, etc.), we avoid the need for a
fixed number of categories and for more subjective
judgments. Also ranking enables a more natural and
intuitive way for humans (and systems) to perform
annotations by preventing them from treating each
individual case in isolation, as opposed to relative
to each other. However, the inherent subjectivity
introduced by ranking entails higher disagreement
among human annotators, and more complexity for
systems to tackle.
3 Corpus compilation
The trial and test corpora were created from the cor-
pus of SemEval-2007 shared task on Lexical Sub-
stitution (McCarthy and Navigli, 2007). This de-
cision was motivated by the similarity between the
two tasks. Moreover the existing corpus provided an
adequate solution given time and cost constraints for
our corpus creation. Given existing contexts with the
original target word replaced by a placeholder and
the lists of substitutes (including the target word),
annotators (and systems) are required to rank substi-
tutes in order of simplicity for each context.
3.1 SemEval-2007 - LS corpus
The corpus from the shared task on Lexical Substi-
tution (LS) at SemEval-2007 is a selection of sen-
tences, or contexts, extracted from the English Inter-
net Corpus of English (Sharoff, 2006). It contains
samples of English texts crawled from the web.
This selection makes up the dataset of a total of
2, 010 contexts which are divided into Trial and Test
sets, consisting of 300 and 1710 contexts respec-
tively. It covers a total of 201 (mostly polysemous)
target words, including nouns, verbs, adjectives and
adverbs, and each of the target words is shown in
10 different contexts. Annotators had been asked to
suggest up to three different substitutes (words or
short phrases) for each of the target words within
their contexts. The substitutes were lemmatized un-
less it was deemed that the lemmatization would al-
ter the meaning of the substitute. Annotators were
all native English speakers and each annotated the
entire dataset. Here is an example of a context for
the target word ?bright?:
<lexelt item="bright.a">
<instance id="1">
<context>During the siege, George
Robertson had appointed Shuja-ul-Mulk,
who was a <head>bright</head> boy
only 12 years old and the youngest surviv-
ing son of Aman-ul-Mulk, as the ruler of
Chitral.</context>
348
</instance> ... </lexelt>
The gold-standard document contains each target
word along with a ranked list of its possible substi-
tutes, e.g., for the context above, three annotators
suggested ?intelligent? and ?clever? as substitutes
for ?bright?, while only one annotator came up with
?smart?:
bright.a 1:: intelligent 3; clever 3; smart 1;
3.2 SemEval-2012 Lexical Simplification
corpus
Given the list of contexts and each respective list
of substitutes we asked annotators to rank substi-
tutes for each individual context in ascending order
of complexity. Since the notion of textual simplic-
ity varies from individual to individual, we carefully
chose a group of annotators in an attempt to cap-
ture as much of a common notion of simplicity as
possible. For practical reasons, we selected annota-
tors with high proficiency levels in English as sec-
ond language learners - all with a university first de-
gree in different subjects.
The Trial dataset was annotated by four people
while the Test dataset was annotated by five peo-
ple. In both cases each annotator tagged the com-
plete dataset.
Inter-annotator agreement was computed using an
adaptation of the kappa index with pairwise rank
comparisons (Callison-Burch et al, 2011). This is
also the primary evaluation metric for participating
systems in the shared task, and it is covered in more
detail in Section 4.
The inter-annotator agreement was computed for
each pair of annotators and averaged over all possi-
ble pairs for a final agreement score. On the Trial
dataset, a kappa index of 0.386 was found, while
for the Test dataset, a kappa index of 0.398 was
found. It may be noted that certain annotators dis-
agreed considerably with all others. For example,
on the Test set, if annotations from one judge are re-
moved, the average inter-annotator agreement rises
to 0.443. While these scores are apparently low, the
highly subjective nature of the annotation task must
be taken into account. According to the reference
values for other tasks, this level of agreement is con-
sidered ?moderate? (Callison-Burch et al, 2011).
It is interesting to note that higher inter-annotator
agreement scores were achieved between annota-
tors with similar language and/or educational back-
grounds. The highest of any pairwise annotator
agreement (0.52) was achieved between annotators
of identical language and educational background,
as well as very similar levels of English proficiency.
High agreement scores were also achieved between
annotators with first languages belonging to the
same language family.
Finally, it is also worth noticing that this agree-
ment metric is highly sensitive to small differences
in annotation, thus leading to overly pessimistic
scores. A brief analysis reveals that annotators often
agree on clusters of simplicity and the source of the
disagreement comes from the rankings within these
clusters.
Finally, the gold-standard annotations for the
Trial and Test datasets ? against which systems are
to be evaluated ? were generated by averaging the
annotations from all annotators. This was done
context by context where each substitution was at-
tributed a score based upon the average of the rank-
ings it was ascribed. The substitutions were then
sorted in ascending order of scores, i.e., lowest score
(highest average ranking) first. Tied scores were
grouped together to form a single rank. For exam-
ple, assume that for a certain context, four annota-
tors provided rankings as given below, where multi-
ple candidates between {} indicate ties:
Annotator 1: {clear} {light} {bright} {lumi-
nous} {well-lit}
Annotator 2: {well-lit} {clear} {light}
{bright} {luminous}
Annotator 3: {clear} {bright} {light} {lumi-
nous} {well-lit}
Annotator 4: {bright} {well-lit} {luminous}
{clear} {light}
Thus the word ?clear?, having been ranked 1st,
2nd, 1st and 4th by each of the annotators respec-
tively is given an averaged ranking score of 2. Sim-
ilarly ?light? = 3.25, ?bright? = 2.5, ?luminous? =
4 and ?well-lit? = 3.25. Consequently the gold-
standard ranking for this context is:
Gold: {clear} {bright} {light, well-lit} {lumi-
nous}
349
3.3 Context-dependency
As mentioned in Section 1, one of our hypothe-
ses was that the notion of simplicity is context-
dependent. In other words, that the ordering of sub-
stitutes for different occurrences of a target word
with a given sense is highly dependent on the con-
texts in which such a target word appears. In order
to verify this hypothesis quantitatively, we further
analyzed the gold-standard annotations of the Trial
and Test datasets. We assume that identical lists of
substitutes for different occurrences of a given tar-
get word ensure that such a target word has the same
sense in all these occurrences. For every target word,
we then generate all pairs of contexts containing the
exact same initial list of substitutes and check the
proportion of these contexts for which human an-
notators ranked the substitutes differently. We also
check for cases where only the top-ranked substitute
is different. The numbers obtained are shown in Ta-
ble 1.
Trial Test
1) # context pairs 1350 7695
2) # 1) with same list 60 242
3) # 2) with different rankings 24 139
4) # 2) with different top substitute 19 38
Table 1: Analysis on the context-dependency of the no-
tion of simplicity.
Although the proportion of pairs of contexts with
the same list of substitutes is very low (less than
5%), it is likely that there are many other occur-
rences of a target word with the same sense and
slightly different lists of substitutes. Further man-
ual inspection is necessary to determine the actual
numbers. Nevertheless, from the observed sample
it is possible to conclude that humans will, in fact,
rank the same set of words (with the same sense)
differently depending on the context (on an average
in 40-57% of the instances).
4 Evaluation metric
No standard metric has yet been defined for eval-
uating Lexical Simplification systems. Evaluating
such systems is a challenging problem due to the
aforementioned subjectivity of the task. Since this
is a ranking task, rank correlation metrics are desir-
able. However, metrics such as Spearman?s Rank
Correlation are not reliable on the limited number of
data points available for comparison on each rank-
ing (note that the nature of the problem enforces a
context-by-context ranking, as opposed to a global
score), Other metrics for localized, pairwise rank
correlation, such as Kendall?s Tau, disregard ties, ?
which are important for our purposes ? and are thus
not suitable.
The main evaluation metric proposed for this
shared task is in fact a measure of inter-annotator
agreement, which is used for both contrasting two
human annotators (Section 3.2) and contrasting a
system output to the average of human annotations
that together forms the gold-standard.
Out metric is based on the kappa index (Cohen,
1960) which in spite of many criticisms is widely
used for its simplicity and adaptability for different
applications. The generalized form of the kappa in-
dex is
? =
P (A)? P (E)
1? P (E)
where P (A) denotes the proportion of times two
annotators agree and P (E) gives the probability of
agreement by chance between them.
In order to apply the kappa index for a ranking
task, we follow the method proposed by (Callison-
Burch et al, 2011) for measuring agreement over
judgments of translation quality. This method de-
fines P (A) and P (E) in such a way that it now
counts agreement whenever annotators concur upon
the order of pairwise ranks. Thus, if one annotator
ranked two given words 1 and 3, and the second an-
notator ranked them 3 and 7 respectively, they are
still in agreement. Formally, assume that two anno-
tators A1 and A2 rank two instance a and b. Then
P (A) = the proportion of times A1 and A2 agree
on a ranking, where an occurrence of agreement is
counted whenever rank(a < b) or rank(a = b) or
rank(a > b).
P (E) (the likelihood that annotators A1 and A2
agree by chance) is based upon the probability that
both of them assign the same ranking order to a and
b. Given that the probability of getting rank(a <
b) by any annotator is P (a < b), the probability
that both annotators get rank(a < b) is P (a < b)2
(agreement is achieved when A1 assigns a < b by
chance and A2 also assigns a < b). Similarly, the
350
probability of chance agreement for rank(a = b)
and rank(a > b) are P (a = b)2 and P (a > b)2
respectively. Thus:
P (E) = P (a < b)2 + P (a = b)2 + P (a > b)2
However, the counts of rank(a < b) and
rank(a > b) are inextricably linked, since for any
particular case of a1 < b1, it follows that b1 >
a1, and thus the two counts must be incremented
equally. Therefore, over the entire space of ranked
pairs, the probabilities remain exactly the same. In
essence, after counting for P (a = b), the remaining
probability mass is equally split between P (a < b)
and P (a > b). Therefore:
P (a < b) = P (a > b) =
1? P (a = b)
2
Kappa is calculated for every pair of ranked items
for a given context, and then averaged to get an over-
all kappa score:
? =
|N |?
n=1
Pn(A)? Pn(E)
1? Pn(E)
|N |
where N is the total number of contexts, and Pn(A)
and Pn(E) are calculated based on counts extracted
from the data on the particular context n.
The functioning of this evaluation metric is illus-
trated by the following example:
Context: During the siege, George Robert-
son had appointed Shuja-ul-Mulk, who was a
_____ boy only 12 years old and the youngest
surviving son of Aman-ul-Mulk, as the ruler
of Chitral.
Gold: {intelligent} {clever} {smart} {bright}
System: {intelligent} {bright} {clever,
smart}
Out of the 6 distinct unordered pairs of lexical
items, system and gold agreed 3 times. Conse-
quently, Pn(A) = 36 . In addition, count(a =
b) = 1. Thus, Pn(a = b) = 112 . Which gives a
P (E) = 4196 and the final kappa score for this partic-
ular context of 0.13.
The statistical significance of the results from two
systems A and B is measured using the method
of Approximate Randomization, which has been
shown to be a robust approach for several NLP tasks
(Noreen, 1989). The randomization is run 1, 000
times and if the p-value is ? 0.05 the difference be-
tween systems A and B is asserted as being statisti-
cally significance.
5 Baselines
We defined three baseline lexical simplification sys-
tems for this task, as follows.
L-Sub Gold: This baseline uses the gold-standard
annotations from the Lexical Substitution cor-
pus of SemEval-2007 as is. In other words, the
ranking is based on the goodness of fit of sub-
stitutes for a context, as judged by human anno-
tators. This method also serves to show that the
Lexical Substitution and Lexical Simplification
tasks are indeed different.
Random: This baseline provides a randomized or-
der of the substitutes for every context. The
process of randomization is such that is allows
the occurrence of ties.
Simple Freq.: This simple frequency baseline uses
the frequency of the substitutes as extracted
from the Google Web 1T Corpus (Brants and
Franz, 2006) to rank candidate substitutes
within each context.
The results in Table 2 show that the ?L-Sub Gold?
and ?Random? baselines perform very poorly on
both Trial and Test sets. In particular, the reason for
the poor scores for ?L-Sub Gold? can be attributed
to the fact that it yields many ties, whereas the gold-
standard presents almost no ties. Our kappa met-
ric tends to penalize system outputs with too many
ties, since the probability of agreement by chance is
primarily computed on the basis of the number of
ties present in the two rankings being compared (see
Section 4).
The ?Simple Freq.? baseline, on the other hand,
performs very strongly, in spite of its simplistic ap-
proach, which is entirely agnostic to context. In fact
it surpasses the average inter-annotator agreement
on both Trial and Test datasets. Indeed, the scores on
the Test set approach the best inter-annotator agree-
ment scores between any two annotators.
351
Trial Test
L-Sub Gold 0.050 0.106
Random 0.016 0.012
Simple Freq. 0.397 0.471
Table 2: Baseline kappa scores on trial and test sets
6 Results and Discussion
6.1 Participants
Five sites submitted one or more systems to the task,
totaling nine systems:
ANNLOR-lmbing: This system (Ligozat et al,
2012) relies on language models probabili-
ties, and builds on the principle of the Sim-
ple Frequency baseline. While the baseline
uses Google n-grams to rank substitutes, this
approach uses Microsoft Web n-grams in the
same way. Additionally characteristics, such
as the contexts of each term to be substituted,
were integrated into the system. Microsoft Web
N-gram Service was used to obtain log likeli-
hood probabilities for text units, composed of
the lexical item and 4 words to the left and right
from the surrounding context.
ANNLOR-simple: The system (Ligozat et al,
2012) is based on Simple English Wikipedia
frequencies, with the motivation that the lan-
guage used in this version of Wikipedia is
targeted towards people who are not first-
language English speakers. Word n-grams (n =
1-3) and their frequencies were extracted from
this corpus using the Text-NSP Perl module
and a ranking of the possible substitutes of a
target word according to these frequencies in
descending order was produced.
EMNLPCPH-ORD1: The system performs a se-
ries of pairwise comparisons between candi-
dates. A binary classifier is learned purpose
using the Trial dataset and artificial unlabeled
data extracted based on Wordnet and a corpus
in a semi-supervised fashion. A co-training
procedure that lets each classifier increase the
other classifier?s training set with selected in-
stances from the unlabeled dataset is used. The
features include word and character n-gram
probabilities of candidates and contexts using
web corpora, distributional differences of can-
didate in a corpus of ?easy? sentences and a
corpus of normal sentences, syntactic complex-
ity of documents that are similar to the given
context, candidate length, and letter-wise rec-
ognizability of candidate as measured by a tri-
gram LM. The first feature sets for co-training
combines the syntactic complexity, character
trigram LM and basic word length features, re-
sulting in 29 features against the remaining 21.
EMNLPCPH-ORD2: This is a variant of the
EMNLPCPH-ORD1 system where the first fea-
ture set pools all syntactic complexity fea-
tures and Wikipedia-based features (28 fea-
tures) against all the remaining 22 features in
the second group.
SB-mmSystem: The approach (Amoia and Ro-
manelli, 2012) builds on the baseline defini-
tion of simplicity using word frequencies but
attempt at defining a more linguistically mo-
tivated notion of simplicity based on lexical
semantics considerations. It adopts different
strategies depending on the syntactic complex-
ity of the substitute. For one-word substitutes
or common collocations, the system uses its
frequency from Wordnet as a metric. In the
case of multi-words substitutes the system uses
?relevance? rules that apply (de)compositional
semantic criteria and attempts to identify a
unique content word in the substitute that might
better approximate the whole expression. The
expression is then assigned the frequency asso-
ciated to this content word for the ranking. Af-
ter POS tagging and sense disambiguating all
substitutes, hand-written rules are used to de-
compose the meaning of a complex phrase and
identify the most relevant word conveying the
semantics of the whole.
UNT-SimpRank: The system (Sinha, 2012) uses
external resources, including the Simple En-
glish Wikipedia corpus, a set of Spoken En-
glish dialogues, transcribed into machine read-
able form, WordNet, and unigram frequencies
(Google Web1T data). SimpRank scores each
substitute by a sum of its unigram frequency, its
352
frequency in the Simple English Wikipedia, its
frequency in the spoken corpus, the inverse of
its length, and the number of senses the sub-
stitute has in WordNet. For a given context,
the substitutes are then reverse-ranked based on
their simplicity scores.
UNT-SimpRankLight: This is a variant of Sim-
pRank which does not use unigram frequen-
cies. The goal of this system is to check
whether a memory and time-intensive and non-
free resource such as the Web1T corpus makes
a difference over other free and lightweight re-
sources.
UNT-SaLSA: The only resource SaLSA depends
on is the Web1T data, and in particular only
3-grams from this corpus. It leverages the con-
text provided with the dataset by replacing the
target placeholder one by one with each of the
substitutes and their inflections thus building
sets of 3-grams for each substitute in a given
instance. The score of any substitute is then the
sum of the 3-gram frequencies of all the gener-
ated 3-grams for that substitute.
UOW-SHEF-SimpLex: The system (Jauhar and
Specia, 2012) uses a linear weighted ranking
function composed of three features to pro-
duce a ranking. These include a context sen-
sitive n-gram frequency model, a bag-of-words
model and a feature composed of simplicity
oriented psycholinguistic features. These three
features are combined using an SVM ranker
that is trained and tuned on the Trial dataset.
6.2 Pairwise kappa
The official task results and the ranking of the sys-
tems are shown in Table 3.
Firstly, it is worthwhile to note that all the top
ranking systems include features that use frequency
as a surrogate measure for lexical simplicity. This
indicates a very high correlation between distribu-
tional frequency of a given word and its perceived
complexity level. Additionally, the top two systems
involve context-dependent and context-independent
features, thus supporting our hypothesis of the com-
posite nature of the lexical simplification problem.
Rank Team - System Kappa
1 UOW-SHEF-SimpLex 0.496
2
UNT-SimpRank 0.471
Baseline-Simple Freq. 0.471
ANNLOR-simple 0.465
3 UNT-SimpRankL 0.449
4 EMNLPCPH-ORD1 0.405
5 EMNLPCPH-ORD2 0.393
6 SB-mmSystem 0.289
7 ANNLOR-lmbing 0.199
8 Baseline-L-Sub Gold 0.106
9 Baseline-Random 0.013
10 UNT-SaLSA -0.082
Table 3: Official results and ranking according to the pair-
wise kappa metric. Systems are ranked together when the
difference in their kappa score is not statistically signifi-
cant.
Few of the systems opted to use some form of
supervised learning for the task, due to the limited
number of training examples given. As pointed out
by some participants who checked learning curves
for their systems, the performance is likely to im-
prove with larger training sets. Without enough
training data, context agnostic approaches such as
the ?Simple Freq.? baseline become very hard to
beat.
We speculate that the reason why the effects of
context-aware approaches are somewhat mitigated is
because of the isolated setup of the shared task. In
practice, humans produce language at an even level
of complexity, i.e. consistently simple, or consis-
tently complex. In the shared task?s setup, systems
are expected to simplify a single target word in a
context, ignoring the possibility that sometimes sim-
ple words may not be contextually associated with
complex surrounding words. This not only explains
why context-aware approaches are less successful
than was originally expected, but also gives a reason
for the good performance of context-agnostic sys-
tems.
6.3 Recall and top-rank
As previously noted, the primary evaluation met-
ric is very susceptible to penalize slight changes,
making it overly pessimistic about systems? perfor-
mance. Hence, while it may be an efficient way to
compare and rank systems within the framework of
353
a shared task, it may be unnecessarily devaluing the
practical viability of approaches. We performed two
post hoc evaluations that assess system output from
a practical point of view. We check how well the
top-ranked substitute, i.e., the simplest substitute ac-
cording to a given system (which is most likely to
be used in a real simplification task) compares to the
top-ranked candidate from the gold standard. This is
reported in the TRnk column of Table 4: the percent-
age of contexts in which the intersection between the
simplest substitute set from a system?s output and
the gold standard contained at least one element.
We note that while ties are virtually inexistent in the
gold standard data, ties in the system output can af-
fect this metric: a system that naively predicts all
substitutes as the simplest (i.e., a single tie includ-
ing all candidates) will score 100% in this metric.
We also measured the ?recall-at-n" values for 1 ?
n ? 3, which gives the ratio of candidates from the
top n substitute sets to those from the gold-standard.
For a given n, we only consider contexts that have
at least n+1 candidates in the gold-standard (so that
there is some ranking to be done). Table 4 shows the
results of this additional analysis.
Team - System TRnk n=1 n=2 n=3
UOW-SHEF-SimpLex 0.602 0.575 0.689 0.769
UNT-SimpRank 0.585 0.559 0.681 0.760
Baseline-Simple Freq. 0.585 0.559 0.681 0.760
ANNLOR-simple 0.564 0.538 0.674 0.768
UNT-SimpRankL 0.567 0.541 0.674 0.753
EMNLPCPH-ORD1 0.539 0.513 0.645 0.727
EMNLPCPH-ORD2 0.530 0.503 0.637 0.722
SB-mmSystem 0.477 0.452 0.632 0.748
ANNLOR-lmbing 0.336 0.316 0.494 0.647
Baseline-L-Sub Gold 0.454 0.427 0.667 0.959
Baseline-Random 0.340 0.321 0.612 0.825
UNT-SaLSA 0.146 0.137 0.364 0.532
Table 4: Additional results according to the top-rank
(TRnk) and recall-at-n metrics.
These evaluation metrics favour systems that pro-
duce many ties. Consequently the baselines ?L-Sub
Gold" and ?Random" yield overly high scores for
recall-at-n for n=2 and n= 3. Nevertheless the rest
of the results are by and large consistent with the
rankings from the kappa metric.
The results for recall-at-2, e.g., show that most
systems, on average 70% of the time, are able to
find the simplest 2 substitute sets that correspond
to the gold standard. This indicates that most ap-
proaches are reasonably good at distinguishing very
simple substitutes from very complex ones, and that
the top few substitutes will most often produce ef-
fective simplifications.
These results correspond to our experience from
the comparison of human annotators, who are easily
able to form clusters of simplicity with high agree-
ment, but who strongly disagree (based on personal
biases towards perceptions of lexical simplicity) on
the internal rankings of these clusters.
7 Conclusions
We have presented the organization and findings of
the first English Lexical Simplification shared task.
This was a first attempt at garnering interest in the
NLP community for research focused on the lexical
aspects of Text Simplification.
Our analysis has shown that there is a very strong
relation between distributional frequency of words
and their perceived simplicity. The best systems on
the shared task were those that relied on this asso-
ciation, and integrated both context-dependent and
context-independent features. Further analysis re-
vealed that while context-dependent features are im-
portant in principle, their applied efficacy is some-
what lessened due to the setup of the shared task,
which treats simplification as an isolated problem.
Future work would involve evaluating the im-
portance of context for lexical simplification in the
scope of a simultaneous simplification to all the
words in a context. In addition, the annotation of
the gold-standard datasets could be re-done taking
into consideration some of the features that are now
known to have clearly influenced the large variance
observed in the rankings of different annotators,
such as their background language and the educa-
tion level. One option would be to select annotators
that conform a specific instantiation of these fea-
tures. This should result in a higher inter-annotator
agreement and hence a simpler task for simplifica-
tion systems.
Acknowledgments
We would like to thank the annotators for their hard
work in delivering the corpus on time.
354
References
Marilisa Amoia and Massimo Romanelli. 2012. SB-
mmSystem: Using Decompositional Semantics for
Lexical Simplification. In English Lexical Simplifica-
tion. Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012), Montreal,
Canada.
Or Biran, Samuel Brody, and Noemie Elhadad. 2011.
Putting it simply: a context-aware approach to lexi-
cal simplification. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 496?501,
Portland, Oregon.
Thorsten Brants and Alex Franz. 2006. The google web
1t 5-gram corpus version 1.1.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland.
Arnaldo Candido, Jr., Erick Maziero, Caroline Gasperin,
Thiago A. S. Pardo, Lucia Specia, and Sandra M.
Aluisio. 2009. Supporting the adaptation of texts for
poor literacy readers: a text simplification editor for
Brazilian Portuguese. In Proceedings of the Fourth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 34?42, Boulder, Col-
orado.
J Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46, April.
Jan de Belder, Koen Deschacht, and Marie-Francine
Moens. 2010. Lexical simplification. In Proceedings
of Itec2010: 1st International Conference on Inter-
disciplinary Research on Technology, Education and
Communication, Kortrijk, Belgium.
Siobhan Devlin and John Tait. 1998. The use of a psy-
cholinguistic database in the simplification of text for
aphasic readers. Linguistic Databases, pages 161?
173.
Sujay Kumar Jauhar and Lucia Specia. 2012. UOW-
SHEF: SimpLex - Lexical Simplicity Ranking based
on Contextual and Psycholinguistic Features. In En-
glish Lexical Simplification. Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval 2012), Montreal, Canada.
Anne-Laure Ligozat, Cyril Grouin, Anne Garcia-
Fernandez, and Delphine Bernhard. 2012. ANNLOR:
A Naive Notation-system for Lexical Outputs Rank-
ing. In English Lexical Simplification. Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012), Montreal, Canada.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), Prague, Czech Re-
public, pages 48?53.
E. Noreen. 1989. Computer-intensive methods for test-
ing hypotheses. New York: Wiley.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435?462.
Advaith Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language and Computa-
tion, 4:77?109.
Ravi Sinha. 2012. UNT-SimpRank: Systems for Lex-
ical Simplification Ranking. In English Lexical Sim-
plification. Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
Lucia Specia. 2010. Translating from complex to simpli-
fied sentences. In Proceedings of the 9th international
conference on Computational Processing of the Por-
tuguese Language, PROPOR?10, pages 30?39, Berlin,
Heidelberg. Springer-Verlag.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 365?368, Los Angeles, California.
355
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 477?481,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UOW-SHEF: SimpLex ? Lexical Simplicity Ranking based on Contextual
and Psycholinguistic Features
Sujay Kumar Jauhar
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton
WV1 1SB, UK
Sujay.KumarJauhar@wlv.ac.uk
Lucia Specia
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
L.Specia@dcs.shef.ac.uk
Abstract
This paper describes SimpLex,1 a Lexical
Simplification system that participated in the
English Lexical Simplification shared task at
SemEval-2012. It operates on the basis of
a linear weighted ranking function composed
of context sensitive and psycholinguistic fea-
tures. The system outperforms a very strong
baseline, and ranked first on the shared task.
1 Introduction
Lexical Simplification revolves around replacing
words by their simplest synonym in a context aware
fashion. It is similar in many respects to the task of
Lexical Substitution (McCarthy and Navigli, 2007)
in that it involves elements of selectional preference
on the basis of a central predefined criterion (sim-
plicity in the current case), as well as sensitivity to
context.
Lexical Simplification envisages principally a hu-
man target audience, and can greatly benefit chil-
dren, second language learners, people with low lit-
eracy levels or cognitive disabilities, and in general
facilitate the dissemination of knowledge to wider
audiences.
We experimented with a number of features that
we posited might be inherently linked with tex-
tual simplicity and selected the three that seemed
the most promising on an evaluation with the trial
dataset. These include contextual and psycholin-
guistic components. When combined using an SVM
1Developed by co-organizers of the shared task
ranker to build a model, such a model provides re-
sults that offer a statistically significant improve-
ment over a very strong context-independent base-
line. The system ranked first overall on the Lexical
Simplification task.
2 Related Work
Lexical Simplification has received considerably
less interest in the NLP community as compared
with Syntactic Simplification. However, there are
a number of notable works related to the topic.
In particular Yatskar et al (2010) leverage the
relations between Simple Wikipedia and English
Wikipedia to extract simplification pairs. Biran et al
(2011) extend this base methodology to apply lexi-
cal simplification to input sentences. De Belder and
Moens (2010), in contrast, provide a more general
architecture for the task, with scope for possible ex-
tension to other languages.
These studies and others have envisaged a range
of different target user groups including children
(De Belder and Moens, 2010), people with low liter-
acy levels (Aluisio et al, 2008) and aphasic readers
(Carroll et al, 1998).
The current work differs from previous research
in that it envisages a stand-alone lexical simpli-
fication system based on linguistically motivated
and cognitive principles within the framework of a
shared task. Its core methodology remains open to
integration into a larger Text Simplification system.
3 Task Setup
The English Lexical Simplification shared task at
SemEval-2012 (Specia et al, 2012) required sys-
477
tems to rank a number of candidate substitutes
(which were provided beforehand) based on their
simplicity of usage in a given context. For example,
given the following context with an empty place-
holder, and its candidate substitutes:
Context: During the siege , George
Robertson had appointed Shuja-ul-Mulk,
who was a boy only 12 years old and
the youngest surviving son of Aman-ul-
Mulk, as the ruler of Chitral.
Candidates: {clever} {smart}
{intelligent} {bright}
a system is required to produce a ranking, e.g.:
System: {intelligent} {bright} {clever,
smart}
Note that ties were permitted and that all candi-
dates needed to be included in the system rankings.
4 The SimpLex Lexical Simplification
System
In an approach similar to what Hassan et al (2007)
used for Lexical Substitution, SimpLex ranks can-
didates based on a weighted linear scoring function,
which has the generalized form:
s (cn,i) =
?
m?M
1
rm (cn,i)
where cn,i is the candidate substitute to be scored,
and each rm is a standalone ranking function that
attributes to each candidate its rank based on its
uniquely associated features. Based on this scoring,
candidates for context are ranked in descending or-
der of scores.
In the development of the system we experi-
mented with a number of these features including
ranking based on word length, number of syllables,
scoring with a 2-step cluster and rank architecture,
latent semantic analysis, and average point-wise mu-
tual information between the candidate and neigh-
boring words in the context.
However, the features which were intuitively the
simplest proved, in the end, to give the best results.
They were selected based on their superior perfor-
mance on the trial dataset and their competitiveness
with the strong Simple Frequency baseline. These
stand-alone features are described in what follows.
4.1 Adapted N-Gram Model
The motivation behind an n-gram model for Lexical
Simplification is that the task involves an inherent
WSD problem. This is because the same word may
be used with different senses (and consequently dif-
ferent levels of complexity) in different contexts.
A blind application of n-gram frequency search-
ing on the shared task?s dataset, however, gives sub-
optimal results because of two main factors:
1. Inconsistently lemmatized candidates.
2. Blind replacement of even correctly lemma-
tized forms in context producing ungrammat-
ical results.
We infer the correct inflection of all candidates for
a given context based on the appearance of the orig-
inal target word (which is also one of the candidate
substitutes) in context. To do this we run a part-of-
speech (POS) tagger on the source text and note the
POS of the target word. Then handcrafted rules are
used to correctly inflect the other candidates based
on this POS tag.
To resolve the issue of ungrammatical textual out-
put, we further use a simple approach of popping
words in close proximity to the placeholder and per-
forming n-gram searches on all possible query com-
binations. Take for instance the following example:
Context: He was away.
Candidates: {going} {leaving}
where ?going? is evidently the original word in con-
text, but ?leaving? has also been suggested as a sub-
stitute (there are many such cases in the datasets).
One of the possible outcomes of popping context
words leads to the correct sequence for the latter
substitute, i.e. ?He was leaving? with the word
?away? having been popped.
The rationale behind this approach is that if one of
the combinations is grammatically correct, the num-
ber of n-gram hits it returns will far exceed those
returned by ungrammatical ones.
The n-gram (2 ? n ? 5) searches are performed
on the Google Web 1T corpus (Brants and Franz,
2006), and the number of hits is weighted by the
length of the n-gram search (such that longer se-
quences obtain higher weight). This may seem like
478
a simplistic approach, especially when the candidate
words appear in long-distance dependency relations
to other parts of the sentence. However, it should be
noted that since the Web 1T corpus only consists of
n-grams with n ? 5, structures that contain longer
dependencies than this are in any case not consid-
ered, and hence do not interfere with local context.
4.2 Bag-of-Words Model
The limitations of performing queries on the Google
Web 1T are that n-grams hits must be in strict lin-
ear order of appearance. To overcome this diffi-
culty, we further mimic the functioning of a bag-
of-words model by taking all possible ordering of
words of a given n-gram sequence. This approach,
to some extent, gives the possibility of observing co-
occurrences of candidate and context words in vari-
ous orderings of appearance. This results in a num-
ber of inadequate query strings, but possibly a few
(as opposed to one in a linear n-gram search) good
word orderings with high hits as well.
As with the previous model, only n-grams with
2 ? n ? 5 are taken. For a given substitute the total
number of hits for all possible queries involving that
substitute are summed (with each hit being weighted
by the length of its corresponding query in words).
To obtain the final score, this sum is normalized by
the actual number of queries.
4.3 Psycholinguistic Feature Model
The MRC Psycholinguistic Database (Wilson, 1988)
and the Bristol Norms (Stadthagen-Gonzalez and
Davis, 2006) are knowledge repositories that asso-
ciate scores to words based on a number of psy-
cholinguistic features. The ones that we felt were
most pertinent to our study are:
1. Concreteness - the level of abstraction associ-
ated with the concept a word describes.
2. Imageability - the ability of a given word to
arouse mental images.
3. Familiarity - the frequency of exposure to a
word.
4. Age of Acquisition - the age at which a given
word is appropriated by a speaker.
We combined both databases and compiled a sin-
gle resource consisting of all the words from both
sources that list at least one of these features. It may
be noted that these attributes were compiled in simi-
lar fashion in both databases and were normalized to
the same scale of scores falling in the range of 100
to 700.
In spite of a combined compilation, the coverage
of the resource was poor, with more than half the
candidate substitutes on both trial and test sets sim-
ply not being listed in the databases. To overcome
this difficulty we introduced a fifth frequency feature
that essentially simulates the ?Simple Frequency?
baseline, 2 but with scores that were normalized to
the same scale of the other psycholinguistic features.
This composite of features was used in a linear
weighted function with weights tuned to best perfor-
mance values on the trial dataset. This function sums
the weighted scores for each candidate, and normal-
izes this sum by the number of non-zero features (in
the worst-case scenario, ? when no psycholinguistic
features are found ? the scorer is equivalent to the
?Simple Frequency? baseline). It is interesting to
note that the frequency feature did not dominate the
linear combination; rather there was a nice interplay
of features with Concreteness, Imageability, Famil-
iarity, Age of Acquisition and Simple Frequency be-
ing weighted (on a scale of -1 to +1) as 0.72, -0.22,
0.87, 0.36 and 0.36, respectively.
4.4 Feature Combination
We combined the three standalone models using
the ranking function of the SVM-light package
(Joachims, 2006) for building SVM rankers. The pa-
rameters of the SVM were tuned on the trial dataset,
which consisted of only 300 example contexts. To
avoid overfitting, instead of taking the single best
parameters, we took parameter values that were the
average of the top 10 distinct runs.
It may be noted that the resulting model makes no
attempt to tie candidates, although actual ties may be
produced by chance. But since ties are rarely used
in the gold standard for the trial dataset, we reasoned
that this should not affect the system performance in
any significant way.
2The ?Simple Frequency? baseline scores each substitute
based on the number of hits it produces in the Google Web 1T
479
bline-SFreq w-ln n-syll psycho a-n-gram b-o-w pmi lsa SimpLex
Trial 0.398 0.176 0.118 0.388 0.397 0.395 0.340 0.089 ?
Test 0.471 0.236 0.163 0.432 0.460 0.460 0.404 0.054 0.496
Table 1: Comparison of Models? Scores
5 Results and Discussion
The results of the SimpLex system trained and tuned
on the trial set, in comparison with the Simple Fre-
quency baseline and the other stand-alone features
we experimented with are presented in Table 1. The
scores are computed through a version of the Kappa
index over pairwise rankings, and therefore repre-
sent the average agreement between the system and
the gold-standard annotation in the ranking of pairs
of candidate substitutes.
Table 1 shows that while in isolation the features
are unable to beat the Simple Frequency model, to-
gether they form a combination which outperforms
the baseline. The improvement of SimpLex over
the other models is statistically significant (statisti-
cal significance was established using a randomiza-
tion test with 1000 iterations and p-value ? 0.05).
We believe that the reason why the context aware
features were still unable to score better than the
context-independent baseline is the isolated focus
on simplifying a single target word. People tend
to produce language that contains words of roughly
equal levels of complexity. Hence in some cases
the surrounding context, instead of helping to dis-
ambiguate the target word, introduces further noise
to queries, especially when its individual component
words have skewed complexity factors. A simul-
taneous simplification of all the content words in a
context could be a possible solution to this problem.
As an additional experiment to assess the impor-
tance of the size of the training data in our simplifi-
cation system, we pooled together the trial and test
datasets, and ran several iterations of the combina-
tion algorithm with a regular increment of number of
training examples and noted the effects it produced
on eventual score. Three hundred examples were ap-
portioned consistently to a test set to maintain com-
parability between experiments. Note that this time,
no optimization of the SVM parameters was made.
The results were inconclusive, and contrary to ex-
pectation, revealed that there is no general improve-
ment with additional training data. This could be
because of the difficulty of the learning problem, for
which the scope of the combined dataset is still very
limited. A more detailed study with a corpus that is
orders of magnitude larger than the current one may
be necessary to establish conclusive evidence.
6 Conclusion
This paper presented our system SimpLex which
participated in the English Lexical Simplification
shared-task at SemEval-2012 and ranked first out of
9 participating systems.
Our findings showed that while a context agnostic
frequency approach to lexical simplification seems
to effectively model the problem of assessing word
complexity to a relatively decent level of accuracy,
as evidenced by the strong baseline of the shared
task, other elements, such as interplay of context
awareness with humanly perceived psycholinguistic
features can produce better results, in spite of very
limited training data.
Finally, a more global approach to lexical sim-
plification that concurrently addresses all the words
in a context to normalize simplicity levels, may be
a more realistic proposition for target applications,
and also help context aware features perform better.
Acknowledgment
This work was supported by the European Com-
mission, Education & Training, Erasmus Mundus:
EMMC 2008-0083, Erasmus Mundus Masters in
NLP & HLT program.
References
Sandra M. Aluisio, Lucia Specia, Thiago A.S. Pardo, Er-
ick G. Maziero, and Renata P.M. Fortes. 2008. To-
wards brazilian portuguese automatic text simplifica-
tion systems. In Proceeding of the eighth ACM sym-
480
posium on Document engineering, DocEng ?08, pages
240?248, Sao Paulo, Brazil. ACM.
Or Biran, Samuel Brody, and Noemie Elhadad. 2011.
Putting it simply: a context-aware approach to lexi-
cal simplification. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 496?501,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Thorsten Brants and Alex Franz. 2006. The google web
1t 5-gram corpus version 1.1 ldc2006t13.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
Proceedings of AAAI - 98 Workshop on Integrating
Artificial Intelligence and Assistive Technology, Madi-
son, Wisconsin, July.
Jan De Belder and Marie-Francine Moens. 2010. Text
simplification for children. In Proceedings of the SI-
GIR workshop on Accessible Search Systems, pages
19?26. ACM, July.
S. Hassan, A. Csomai, C. Banea, R. Sinha, and R. Mi-
halcea. 2007. Unt: Subfinder: Combining knowledge
sources for automatic lexical substitution. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 410 ? 413. Asso-
ciation for Computational Linguistics, Prague, Czech
Republic.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ?06, pages 217?226, New York,
NY, USA. ACM.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), Prague, Czech Re-
public, pages 48?53.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea.
2012. Semeval-2012 task 1: English lexical simplifi-
cation. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
Hans Stadthagen-Gonzalez and Colin Davis. 2006. The
bristol norms for age of acquisition, imageability, and
familiarity. Behavior Research Methods, 38:598?605.
Michael Wilson. 1988. Mrc psycholinguistic database:
Machine-usable dictionary, version 2.00. Behavior
Research Methods, 20:6?10.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ?10, pages 365?368, Stroudsburg, PA, USA.
Association for Computational Linguistics.
481
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 673?678,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UOW: Semantically Informed Text Similarity
Miguel Rios and Wilker Aziz
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton,
WV1 1SB, UK
{M.Rios, W.Aziz}@wlv.ac.uk
Lucia Specia
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello,
Sheffield, S1 4DP, UK
L.Specia@sheffield.ac.uk
Abstract
The UOW submissions to the Semantic Tex-
tual Similarity task at SemEval-2012 use a
supervised machine learning algorithm along
with features based on lexical, syntactic and
semantic similarity metrics to predict the se-
mantic equivalence between a pair of sen-
tences. The lexical metrics are based on word-
overlap. A shallow syntactic metric is based
on the overlap of base-phrase labels. The
semantically informed metrics are based on
the preservation of named entities and on the
alignment of verb predicates and the overlap
of argument roles using inexact matching. Our
submissions outperformed the official base-
line, with our best system ranked above aver-
age, but the contribution of the semantic met-
rics was not conclusive.
1 Introduction
We describe the UOW submissions to the Semantic
Textual Similarity (STS) task at SemEval-2012. Our
systems are based on combining similarity scores as
features using a regression algorithm to predict the
degree of semantic equivalence between a pair of
sentences. We train the regression algorithm with
different classes of similarity metrics: i) lexical,
ii) syntactic and iii) semantic. The lexical similar-
ity metrics are: i) cosine similarity using a bag-of-
words representation, and ii) precision, recall and
F-measure of content words. The syntactic metric
computes BLEU (Papineni et al, 2002), a machine
translation evaluation metric, over a labels of base-
phrases (chunks). Two semantic metrics are used: a
metric based on the preservation of Named Entities
and TINE (Rios et al, 2011). Named entities are
matched by type and content: while the type has to
match exactly, the content is compared with the as-
sistance of a distributional thesaurus. TINE is a met-
ric proposed to measure adequacy in machine trans-
lation and favors similar semantic frames. TINE
attempts to align verb predicates, assuming a one-
to-one correspondence between semantic roles, and
considering ontologies for inexact alignment. The
surface realization of the arguments is compared us-
ing a distributional thesaurus and the cosine similar-
ity metric. Finally, we use METEOR (Denkowski
and Lavie, 2010), also a common metric for ma-
chine translation evaluation, that also computes in-
exact word overlap as at way of measuring the im-
pact of our semantic metrics.
The lexical and syntactic metrics complement the
semantic metrics in dealing with the phenomena ob-
served in the task?s dataset. For instance, from the
MSRvid dataset:
S1 Two men are playing football.
S2 Two men are practicing football.
In this case, as typical of paraphrasing, the situa-
tion and participants are the same while the surface
realization differs, but playing can be considered
similar to practicing. From the SMT-eur dataset:
S3 The Council of Europe, along with the Court of
Human Rights, has a wealth of experience of
such forms of supervision, and we can build on
these.
673
S4 Just as the European Court of Human Rights, the
Council of Europe has also considerable expe-
rience with regard to these forms of control; we
can take as a basis.
Similarly, here although with different realiza-
tions, the Court of Human Rights and the European
Court of Human Rights represent the same entity.
Semantic metrics based on predicate-argument
structure can play a role in cases when different re-
alization have similar semantic roles:
S5 The right of a government arbitrarily to set aside
its own constitution is the defining characteris-
tic of a tyranny.
S6 The right for a government to draw aside its con-
stitution arbitrarily is the definition character-
istic of a tyranny.
In this work we attempt to exploit the fact that su-
perficial variations such the ones in these examples
should still render very similarity scores.
In Section 2 we describe the similarity metrics in
more detail. In Section 3 we show the results of our
three systems. In Section 4 we discuss these results
and in Section 5 we present some conclusions.
2 Similarity Metrics
The metrics used in this work are as follows:
2.1 Lexical metrics
All our lexical metrics use the same surface repre-
sentation: words. However, the cosine metric uses
bag-of-words, while all the other metrics use only
content words. We thus first represent the sentences
as bag-of-words. For example, given the pair of sen-
tences S7 and S8:
S7 A man is riding a bicycle.
S8 A man is riding a bike.
the bag-of-words are S7 = {A, man, is, riding, a,
bicycle,.} and S8 = {A, man, is, riding, a, bike, .},
and the bag-of-content-words are S7 = {man, riding,
bicycle} and S8 = {man, riding, bike}.
We compute similarity scores using the following
metrics between a pair of sentencesA andB: cosine
distance (Equation 1), precision (Equation 2), recall
(Equation 3) and F-measure (Equation 4).
cosine(A,B) =
|A
?
B|
?
|A| ? |B|
(1)
precision(A,B) =
|A
?
B|
|B|
(2)
recall(A,B) =
|A
?
B|
|A|
(3)
F (A,B) = 2 ?
precision(A,B) ? recall(A,B)
precision(A,B) + recall(A,B)
(4)
2.2 BLEU over base-phrases
The BLEU metric is used for the automatic evalua-
tion of Machine Translation. The metric computes
the precision of exact matching of n-grams between
a hypothesis and reference translations. This sim-
ple procedure has limitations such as: the matching
of non-content words mixed with the counts of con-
tent words affects in a perfect matching that can hap-
pen even if the order of sequences of n-grams in the
hypothesis and reference translation are very differ-
ent, changing completely the meaning of the trans-
lation. To account for similarity in word order we
use BLEU over base-phrase labels instead of words,
leaving the lexical matching for other lexical and se-
mantic metrics. We compute the matchings of 1-
4-grams of base-phrase labels. This metric favors
similar syntactic order.
2.3 Named Entities metric
The goal of the metric is to deal with synonym enti-
ties. First, named entities are grouped by class (e.g.
Organization), and then the content of the named en-
tities within the same classes is compared through
cosine similarity. If the surface realization is differ-
ent, we retrieve words that share the same context
with the named entity using Dekang Lin?s distribu-
tional thesaurus (Lin, 1998). Therefore, the cosine
similarity will have more information than just the
named entities themselves. For example, from the
sentence pair S9 and S10:
S9 Companies include IBM Corp. ...
674
S10 Companies include International Business Ma-
chines ...
The entity from S9: IBM Corp. and the entity
from S10: International Business Machines have
the same tag Organization. The metric groups
them and adds words from the thesaurus result-
ing in the following bag-of-words. S9: {IBM
Corp.,... Microsoft, Intel, Sun Microsystems, Mo-
torola/Motorola, Hewlett-Packard/Hewlett-Packard,
Novell, Apple Computer...} and S10: {International
Business Machines,... Apple Computer, Yahoo, Mi-
crosoft, Alcoa...}. The metric then computes the co-
sine similarity between this expanded pair of bag-of-
words.
2.4 METEOR
This metric is also a lexical metric based on uni-
gram matching between two sentences. However,
matches can be exact, using stems, synonyms, or
paraphrases of unigrams. The synonym matching is
computed using WordNet (Fellbaum, 1998) and the
paraphrase matching is computed using paraphrase
tables (Callison-Burch et al, 2010). The structure of
the sentences is not not directly considered, but sim-
ilar word orders are rewarded through higher scores
for the matching of longer fragments.
2.5 Semantic Role Label metric
Rios et al (2011) propose TINE, an automatic met-
ric based on the use semantic roles to align predi-
cates and their respective arguments in a pair of sen-
tences. The metric complements lexical matching
with a shallow semantic component to better address
adequacy in machine translation evaluation. The
main contribution of such a metric is to provide a
more flexible way of measuring the overlap between
shallow semantic representations (semantic role la-
bels) that considers both the semantic structure of
the sentence and the content of the semantic compo-
nents.
This metric allows to match synonym predicates
by using verb ontologies such as VerbNet (Schuler,
2006) and VerbOcean (Chklovski and Pantel, 2004)
and distributional semantics similarity metrics, such
as Dekang Lin?s thesaurus (Lin, 1998), where pre-
vious semantic metrics only perform exact match of
predicate structures and arguments. For example, in
VerbNet the verbs spook and terrify share the same
class amuse-31.1, and in VerbOcean the verb dress
is related to the verb wear, so these are considered
matches in TINE.
The main sources of errors in this metric are the
matching of unrelated verbs and the lack of coverage
of the ontologies. For example, for S11 and S12,
remain and say are (incorrectly) related as given by
VerbOcean.
S11 If snow falls on the slopes this week, Christmas
will sell out too, says Schiefert.
S12 If the roads remain snowfall during the week,
the dates of Christmas will dry up, said
Schiefert.
For this work the matching of unrelated verbs is
a particularly crucial issue, since the sentences to be
compared are not necessarily similar, as it is the gen-
eral case in machine translation. We have thus mod-
ified the metric with a preliminary optimization step
which aligns the verb predicates by measuring two
degrees of similarity: i) how similar their arguments
are, and ii) how related the predicates? realizations
are. Both scores are combined as shown in Equation
5 to score the similarity between the two predicates
(Av, Bv) from a pair of sentences (A,B).
sim(Av,Bv) = (wlex ? lexScore(Av, Bv))
+(warg ? argScore(Aarg, Barg))
(5)
where wlex and warg are the weights for each
component, argScore(Aarg, Barg) is the similarity,
which is computed as in Equation 7, of the argu-
ments between the predicates being compared and
lexScore(Av, Bv) is the similarity score extracted
from the Dekang Lin?s thesaurus between the predi-
cates being compared. The Dekang Lin?s thesaurus
is an automatically built thesaurus, and for each
word it has an entry with the most similar words and
their similarity scores. If the verbs are related in the
thesaurus we use their similarity score as lexScore
otherwise lexScore = 0. The pair of predicates
with the maximum sim score is aligned. The align-
ment is an optimization problem where predicates
are aligned 1-1: we search for all 1-1 alignments that
lead to the maximum average sim for the pair of sen-
tences. For example, S13 and S14 have the follow-
ing list of predicates: S13 = {loaded, rose, ending}
675
and S14 = {laced, climbed}. The metric compares
each pair of predicates and it aligns the predicates
rose and climbed because they are related in the the-
saurus with a similarity score lexScore = 0.796
and a argScore = 0.185 given that the weights are
set to 0.5 and sum up to 1 the predicates reach the
maximum sim = 0.429 score. The output of this
step results in a set of aligned verbs between a pair
of sentences.
S13 The tech - loaded Nasdaq composite rose 0
points to 0 , ending at its highest level for 0
months.
S14 The technology - laced Nasdaq Composite In-
dex IXIC climbed 0 points , or 0 percent , to
0.
The SRL similarity metric semanticRole be-
tween two sentences A and B is then defined as:
semanticRole(A,B) =
?
v?V verbScore(Av, Bv)
|VB |
(6)
The verbScore in Equation 6 is computed over
the set of aligned predicates from the previous opti-
mization step and for each aligned predicate the ar-
gument similarity is computed by Equation 7.
verbScore(Av, Bv) =
?
arg?ArgA?ArgB
argScore(Aarg, Barg)
|ArgB |
(7)
In Equation 6, V is the set of verbs aligned between
the two sentences A and B, and |VB| is the num-
ber of verbs in one of the sentences.1 The similar-
ity between the arguments of a verb pair (Av, Bv)
in V is measured as defined in Equation 7, where
ArgA and ArgB are the sets of labeled arguments
of the first and the second sentences and |ArgB| is
the number of arguments of the verb in B.2 The
argScore(Aarg, Barg) computation is based on the
cosine similarity as in Equation 1. We treat the to-
kens in the argument as a bag-of-words.
1This is inherited from the use of the metric focusing on re-
call in machine translation, where the B is the reference trans-
lation. In this work a better approach could be to compute this
metric twice, in both directions.
2Again, from the analogy of a recall metric for machine
translation.
3 Experiments and Results
We use the following state-of-the-art tools to pre-
process the data for feature extraction: i) Tree-
Tagger3 for lemmas and ii) SENNA (Collobert et
al., 2011)4 for Part-of-Speech tagging, Chunking,
Named Entity Recognition and Semantic Role La-
beling. SENNA has been reported to achieve an F-
measure of 75.79% for tagging semantic roles on the
CoNLL-2005 2 benchmark. The final feature set in-
cludes:
? Lexical metrics
? Cosine metric over bag-of-words
? Precision over content words
? Recall over content words
? F-measure over content words
? BLEU metric over chunks
? METEOR metric over words (with stems, syn-
onyms and paraphrases)
? Named Entity metric
? Semantic Role Labeling metric
The Machine Learning algorithm used for re-
gression is the LIBSVM5 Support Vector Machine
(SVM) implementation using the radial basis kernel
function. We used a simple genetic algorithm (Back
et al, 1999) to tune the parameters of the SVM. The
configuration of the genetic algorithm is as follows:
? Fitness function: minimize the mean squared
error found by cross-validation
? Chromosome: real numbers for SVM parame-
ters ?, cost and 
? Number of individuals: 80
? Number of generations: 100
? Selection method: roulette
? Crossover probability: 0.9
3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
4http://ml.nec-labs.com/senna/
2http://www.lsi.upc.edu/ srlconll/
5http://www.csie.ntu.edu.tw/ cjlin/libsvm/
676
? Mutation probability: 0.01
We submitted three system runs, each is a varia-
tion of the above feature set. For the official submis-
sion we used the systems with optimized SVM pa-
rameters. We trained SVM models with each of the
following task datasets: MSRpar, MSRvid, SMT-
eur and the combination of MSRpar+MSRvid. For
each test dataset we applied their respective training
models, except for the new test sets, not covered by
any training set: for On-WN we used the combina-
tion MSRpar+MSRvid, and for SMT-news we used
SMT-eur.
Tables 1 to 3 focus on the Pearson correlation
of our three systems/runs for individual datasets of
the predicted scores against human annotation, com-
pared against the official baseline, which uses a sim-
ple word overlap metric. Table 4 shows the aver-
age results over all five datasets, where ALL stands
for the Pearson correlation with the gold standard
for the five dataset, Rank is the absolute rank among
all submissions, ALLnrm is the Pearson correlation
when each dataset is fitted to the gold standard us-
ing least squares, RankNrm is the corresponding
rank and Mean is the weighted mean across the five
datasets, where the weight depends on the number
of sentence pairs in the dataset.
3.1 Run 1: All except SRL features
Our first run uses the lexical, BLEU, METEOR and
Named Entities features, without the SRL feature.
Table 1 shows the results over the test set, where
Run 1-A is the version without SVM parameter op-
timization and Run 1-B are the official results with
optimized parameters for SVM.
Task Run 1-A Run 1-B Baseline
MSRpar 0.455 0.455 0.433
MSRvid 0.706 0.362 0.300
SMT-eur 0.461 0.307 0.454
On-WN 0.514 0.281 0.586
SMT-news 0.386 0.208 0.390
Table 1: Results for Run 1 using lexical, chunking,
named entities and METEOR as features. A is the non-
optimized version, B are the official results
3.2 Run 2: SRL feature
In this run we use only the SRL feature in order to
analyze whether this feature on its own could be suf-
ficient or lexical and other simpler features are im-
portant. Table 2 shows the results over the test set
without parameter optimization (Run 2-A) and the
official results with optimized parameters for SVM
(Run 2-B).
Task Run 2-A Run 2-B Baseline
MSRpar 0.335 0.300 0.433
MSRvid 0.264 0.291 0.300
SMT-eur 0.264 0.161 0.454
On-WN 0.281 0.257 0.586
SMT-news 0.189 0.221 0.390
Table 2: Results for Run 2 using the SRL feature only. A
is the non-optimized version, B are the official results
3.3 Run 3: All features
In the last run we use all features. Table 3 shows
the results over the test set without parameter opti-
mization (Run 3-A) and the official results with op-
timized parameters for SVM (Run 3-B).
Task Run 3-A Run 3-B Baseline
MSRpar 0.472 0.353 0.433
MSRvid 0.705 0.572 0.300
SMT-eur 0.471 0.307 0.454
On-WN 0.511 0.264 0.586
SMT-news 0.410 0.116 0.390
Table 3: Results for Run 3 using all features. A is the
non-optimized version, B are the official results
4 Discussion
Table 4 shows the ranking and normalized offi-
cial scores of our submissions compared against the
baseline. Our submissions outperform the official
baseline but significantly underperform the top sys-
tems in the shared task. The best system (Run 1)
achieved an above average ranking, but disappoint-
ingly the performance of our most complete system
(Run 3) using the semantic metric is poorer. Sur-
prisingly, the results of the non-optimized versions
outperform the optimized versions used in our offi-
cial submission. One possible reason for that is the
overfitting of the optimized models to the training
sets.
Run 1 and Run 3 have very similar results: the
overall correlation between all datasets of these two
systems is 0.98. One of the reasons for these results
is that the SRL metric is compromised by the length
677
System ALL Rank ALLnrm RankNrm Mean RankMean
Run 1 0.640 36 0.719 71 0.382 80
Run 2 0.536 59 0.629 88 0.257 88
Run 3 0.598 49 0.696 82 0.347 84
Baseline 0.311 87 0.673 85 0.436 70
Table 4: Official results and ranking over the test set for Runs 1-3 with SVM parameters optimized
of the sentences. In the MSRvid dataset, where the
sentences are simple such as ?Someone is drawing?,
resulting in a good semantic parsing, a high per-
formance for this metric is achieved. However, in
the SMT datasets, sentences are much longer (and
often ungrammatical, since they are produced by a
machine translation system) and the performance of
the metric drops. In addition, the SRL metric makes
mistakes such as judging as highly similar sentences
such as ?A man is peeling a potato? and ?A man is
slicing a potato?, where the arguments are the same
but the situations are different.
5 Conclusions
We have presented our systems based on similar-
ity scores as features to train a regression algorithm
to predict the semantic similarity between a pair
of sentences. Our official submissions outperform
the baseline method, but have lower performance
than most participants, and a simpler version of the
systems without any parameter optimization proved
more robust. Disappointingly, our main contribu-
tion, the addition of a metric based on Semantic Role
Labels shows no improvement as compared to sim-
pler metrics.
Acknowledgments
This work was supported by the Mexican National
Council for Science and Technology (CONACYT),
scholarship reference 309261.
References
Thomas Back, David B. Fogel, and Zbigniew
Michalewicz, editors. 1999. Evolutionary Com-
putation 1, Basic Algorithms and Operators. IOP
Publishing Ltd., Bristol, UK, 1st edition.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic Verb
Relations. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 33?40, Barcelona,
Spain, July.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural Language Processing (almost) from Scratch.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: Improved evaluation
support for five target languages. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 339?342, July.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. Cambridge, MA ; London,
May.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ?98, pages 768?
774, Stroudsburg, PA, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA.
Miguel Rios, Wilker Aziz, and Lucia Specia. 2011. Tine:
A metric to assess mt adequacy. Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
678
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?9,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Readability Assessment for Text Simplification  
 
Sandra Aluisio1, Lucia Specia2, Caroline Gasperin1 and Carolina Scarton1 
1Center of Computational Linguistics (NILC) 2Research Group in Computational Linguistics 
University of S?o Paulo University of Wolverhampton 
S?o Carlos - SP, Brazil Wolverhampton, UK 
{sandra,cgasperin}@icmc.usp.br, 
carol.scarton@gmail.com 
L.Specia@wlv.ac.uk 
 
 
 
 
Abstract 
We describe a readability assessment ap-
proach to support the process of text simplifi-
cation for poor literacy readers. Given an in-
put text, the goal is to predict its readability 
level, which corresponds to the literacy level 
that is expected from the target reader: rudi-
mentary, basic or advanced. We complement 
features traditionally used for readability as-
sessment with a number of new features, and 
experiment with alternative ways to model 
this problem using machine learning methods, 
namely classification, regression and ranking. 
The best resulting model is embedded in an 
authoring tool for Text Simplification. 
1 Introduction 
In Brazil, the National Indicator of Functional Lite-
racy (INAF) index has been computed annually 
since 2001 to measure the levels of literacy of the 
Brazilian population. The 2009 report presented a 
worrying scenario: 7% of the individuals are illite-
rate; 21% are literate at the rudimentary level; 47% 
are literate at the basic level; only 25% are literate 
at the advanced level (INAF, 2009). These literacy 
levels are defined as:  
(1) Illiterate: individuals who cannot perform 
simple tasks such as reading words and phrases;  
(2) Rudimentary: individuals who can find ex-
plicit information in short and familiar texts (such 
as an advertisement or a short letter);  
(3) Basic: individuals who are functionally lite-
rate, i.e., they can read and understand texts of av-
erage length, and find information even when it is 
necessary to make some inference; and  
(4) Advanced: fully literate individuals, who can 
read longer texts, relating their parts, comparing 
and interpreting information, distinguish fact from 
opinion, make inferences and synthesize.   
In order to promote digital inclusion and acces-
sibility for people with low levels of literacy, par-
ticularly to documents available on the web, it is 
important to provide text in a simple and easy-to- 
read way. This is a requirement of the Web Con-
tent Accessibility Guidelines 2.0?s principle of 
comprehensibility and accessibility of Web con-
tent1. It states that for texts which demand reading 
skills more advanced than that of individuals with 
lower secondary education, one should offer an al-
ternative version of the same content suitable for 
those individuals. While readability formulas for 
English have a long history ? 200 formulas have 
been reported from 1920 to 1980s (Dubay, 2004) ? 
the only tool available for Portuguese is an adapta-
tion of the Flesch Reading Ease index.  It evaluates 
the complexity of texts in a 4-level scale corres-
ponding to grade levels (Martins et al, 1996).  
In the PorSimples project (Alu?sio et al, 2008) 
we develop text adaptation methods (via text sim-
plification and elaboration approaches) to improve 
the comprehensibility of texts published on gov-
ernment websites or by renowned news agencies, 
which are expected to be relevant to a large au-
dience with various literacy levels. The project 
provides automatic simplification tools to aid (1) 
poorly literate readers to understand online content 
? a browser plug-in for automatically simplifying 
websites ? and (2) authors producing texts for this 
audience ? an authoring tool for guiding the crea-
tion of simplified versions of texts.  
This paper focuses on a readability assessment 
approach to assist the simplification process in the 
authoring tool, SIMPLIFICA. The current version 
of SIMPLIFICA offers simplification operations 
addressing a number of lexical and syntactic phe-
nomena to make the text more readable. The au-
                                                          
1 http://www.w3.org/TR/WCAG20/ 
1
thor has the freedom to choose when and whether 
to apply the available simplification operations, a 
decision based on the level of complexity of the 
current text and on the target reader.  
A method for automatically identifying such 
level of complexity is therefore of great value. 
With our readability assessment tool, the author is 
able to automatically check the complexi-
ty/readability level of the original text, as well as 
modified versions of such text produced as he/she 
applies simplification operations offered by 
SIMPLIFICA, until the text reaches the expected 
level, adequate for the target reader. 
In this paper we present such readability as-
sessment tool, developed as part of the PorSimples 
project, and discuss its application within the au-
thoring tool. Different from previous work, the tool 
does not model text difficulty according to linear 
grade levels (e.g., Heilman et al, 2008), but in-
stead maps the text into the three levels of literacy 
defined by INAF: rudimentary, basic or advanced. 
Moreover, it uses a more comprehensive set of fea-
tures, different learning techniques and targets a 
new language and application, as we discuss in 
Section 4. More specifically, we address the fol-
lowing research questions: 
 
1. Given some training material, is it possible to 
detect the complexity level of Portuguese texts, 
which corresponds to the different literacy levels 
defined by INAF? 
2. What is the best way to model this problem 
and which features are relevant? 
 
We experiment with nominal, ordinal and interval-
based modeling techniques and exploit a number 
of the cognitively motivated features proposed by 
Coh-Metrix 2.0 (Graesser et al, 2004) and adapted 
to Portuguese (called Coh-Metrix-PORT), along 
with a set of new features, including syntactic fea-
tures to capture simplification operations and n-
gram language model features.  
In the remainder of this paper, we first provide 
some background information on the need for a 
readability assessment tool within our text simpli-
fication system (Section 2) and discuss prior work 
on readability assessment (Section 3), to then 
present our features and modeling techniques (Sec-
tion 4) and the experiments performed to answer 
our research questions (Section 5). 
2. Text Simplification in PorSimples 
Text Simplification (TS) aims to maximize reading 
comprehension of written texts through their sim-
plification. Simplification usually involves substi-
tuting complex by simpler words and breaking 
down and changing the syntax of complex, long 
sentences (Max, 2006; Siddharthan, 2003).   
To meet the needs of people with different le-
vels of literacy, in the PorSimples project we pro-
pose two types of simplification: natural and 
strong. The first type results in texts adequate for 
people with a basic literacy level and the second, 
rudimentary level. The difference between these 
two is the degree of application of simplification 
operations to complex sentences. In strong simpli-
fication, operations are applied to all complex syn-
tactic phenomena present in the text in order to 
make it as simple as possible, while in natural sim-
plification these operations are applied selectively, 
only when the resulting text remains ?natural?. 
One example of original text (a), along with its 
natural (b) and strong (c) manual simplifications, is 
given in Table 1. 
 
(a) The cinema theaters around the world were show-
ing a production by director Joe Dante in which a 
shoal of piranhas escaped from a military laborato-
ry and attacked participants of an aquatic show. 
(...) More than 20 people were bitten by palometas 
(Serrasalmus spilopleura, a species of piranhas) 
that live in the waters of the Sanchuri dam. 
(b) The cinema theaters around the world were show-
ing a production by director Joe Dante. In the pro-
duction a shoal of piranhas escaped from a military 
laboratory and attacked participants of an aquatic 
show. (?) More than 20 people were bitten by pa-
lometas that live in the waters of the Sanchuri dam. 
Palometas are Serrasalmus spilopleura, a species 
of piranhas. 
(c) The cinema theaters around the world were show-
ing a movie by director Joe Dante. In the movie a 
shoal of piranhas escaped from a military laborato-
ry. The shoal of piranhas attacked participants of 
an aquatic show. (...). Palometas have bitten more 
than 20 people. Palometas live in the waters of the 
Sanchuri dam. Palometas are Serrasalmus spilop-
leura, a species of piranhas. 
Table 1: Example of original and simplified texts 
 
The association between these two types of simpli-
fication and the literacy levels was identified by 
means of a corpus study. We have manually built a 
corpus of simplified texts at both natural and 
2
strong levels and analyzed their linguistic struc-
tures according to the description of the two litera-
cy levels. We verified that strong simplified sen-
tences are more adequate for rudimentary level 
readers, and natural ones for basic level readers. 
This claim is supported by several studies which 
relate capabilities and performance of the working 
memory with reading levels (Siddharthan, 2003; 
McNamara et al, 2002). 
2.1 The Rule-based Simplification System 
The association between simplification operations 
and the syntactic phenomena they address is im-
plemented within a rule-based syntactic simplifica-
tion system (Candido Jr. et al, 2009). This system 
is able to identify complex syntactic phenomena in 
a sentence and perform the appropriate operations 
to simplify each phenomenon.  
The simplification rules follow a manual for 
syntactic simplification in Portuguese also devel-
oped in PorSimples. They cover syntactic con-
structions such as apposition, relative clauses, 
coordination and subordination, which had already 
been addressed by previous work on text simplifi-
cation (Siddharthan, 2003). Additionally, they ad-
dress the transformation of sentences from passive 
into active voice, normalization of sentences into 
the Subject-Verb-Object order, and simplification 
of adverbial phrases. The simplification operations 
available are: sentence splitting, changing particu-
lar discourse markers by simpler ones, transform-
ing passive into active voice, inverting the order of 
clauses, converting to subject-verb-object order, 
relocating long adverbial phrases.  
2.2 The SIMPLIFICA Tool 
The rule-based simplification system is part of 
SIMPLIFICA, an authoring tool for writers to 
adapt original texts into simplified texts. Within 
SIMPLIFICA, the author plays an active role in 
generating natural or strong simplified texts by ac-
cepting or rejecting the simplifications offered by 
the system on a sentence basis and post-editing 
them if necessary. 
 Despite the ability to make such choices at the 
sentence level, it is not straightforward for the au-
thor to judge the complexity level of the text as 
whole in order to decide whether it is ready for a 
certain audience. This is the main motivation for 
the development of a readability assessment tool.  
The readability assessment tool automatically 
detects the level of complexity of a text at any 
moment of the authoring process, and therefore 
guides the author towards producing the adequate 
simplification level according to the type of reader. 
It classifies a text in one of three levels: rudimenta-
ry, basic or advanced.  
Figure 1 shows the interface of SIMPLIFICA, 
where the complexity level of the current text as 
given by the readability assessment tool is shown 
at the bottom, in red (in this case, ?N?vel Pleno?, 
which corresponds to advanced). To update the 
readability assessment of a text the author can 
choose ?N?vel de Inteligibilidade? (readability lev-
el) at any moment.  
The text shown in Figure 1 is composed of 13 
sentences, 218 words. The lexical simplification 
module (not shown in the Figure 1) finds 10 candi-
date words for simplification in this text, and the 
syntactic simplification module selects 10 sen-
tences to be simplified (highlighted in gray).  
When the author selects a highlighted sentence, 
he/she is presented with all possible simplifications 
proposed by the rule-based system for this sen-
tence. Figure 2 shows the options for the first sen-
tence in Figure 1. The first two options cover non-
finite clause and adverbial adjuncts, respectively, 
while the third option covers both phenomena in 
one single step. The original sentence is also given 
as an option.  
It is possible that certain suggestions of auto-
matic simplifications result in ungrammatical or 
inadequate sentences (mainly due to parsing er-
rors). The author can choose not to use such sug-
gestions as well as manually edit the original or 
automatically simplified versions. The impact of 
the author?s choice on the overall readability level 
of the text is not always clear to the author. The 
goal of the readability assessment function is to 
provide such information. 
Simplified texts are usually longer than the 
original ones, due to sentence  splittings and 
repetition of information to connect such 
sentences.  We  acknowledge  that  low literacy 
readers prefer short texts, but in this tool the 
shortening of the text is a responsibility of the 
author. Our focus is on the linguistic structure of 
the texts; the length of the text actually is a feature 
considered by our readability assessment system. 
3
Figure 1: SIMPLIFICA interface 
Figure 2. Simplification options available for the first sentence of the text presented in Figure 1
3. Readability Assessment 
Recent work on readability assessment for the 
English language focus on: (i) the feature set used 
to capture the various aspects of readability, to 
evaluate the contribution of lexical, syntactic, se-
mantic and discursive features; (ii) the audience of 
the texts the readability measurement is intended 
to; (iii) the genre effects on the calculation of text 
difficult; (iv) the type of learning technique 
which is more appropriate: those producing nomi-
nal, ordinal or interval scales of measurement, and 
(v) providing an application for the automatic as-
sessment of reading difficulty.  
Pitler and Nenkova (2008) propose a unified 
framework composed of vocabulary, syntactic, 
elements of lexical cohesion, entity coherence and 
discourse relations to measure text quality, which 
resembles the composition of rubrics in the area of 
essay scoring (Burstein et al, 2003).  
 The following studies address readability as-
sessment for specific audiences: learners of Eng-
lish as second language (Schwarm and Ostendorf, 
2005; Heilman et al, 2007), people with intellec-
tual disabilities (Feng et al, 2009), and people with 
cognitive impairment caused by Alzheimer (Roark 
at al, 2007). 
Sheehan et al (2007) focus on models for 
literary and expository texts, given that traditional 
metrics like Flesch-Kincaid Level score tend to 
overpredict the difficulty of literary texts and 
underpredict the difficulty of expository texts.  
Heilman et al (2008) investigate an appropriate 
scale of measurement for reading difficulty ? 
nominal, ordinal, or interval ? by comparing the 
effectiveness of statistical models for each type of 
data. Petersen and Ostendorf (2009) use 
classification and regression techniques to predict a 
readability score. 
Miltsakali and Troutt (2007; 2008) propose an 
automatic tool to evaluate reading difficulty of 
Web texts in real time, addressing teenagers and 
adults with low literacy levels. Using machine 
learning, Gl?ckner et al (2006) present a tool for 
automatically rating the readability of German 
texts using several linguistic information sources 
and a global readability score similar to the Flesch 
Reading Ease.   
4
4. A Tool for Readability Assessment 
In this section we present our approach to readabil-
ity assessment.  It differs from previous work in 
the following aspects: (i) it uses a feature set with 
cognitively-motivated metrics and a number of ad-
ditional features to provide a better explanation of 
the complexity of a text; (ii) it targets a new audi-
ence: people with different literacy levels; (iii) it 
investigates different statistical models for non- 
linear data scales: the levels of literacy defined by 
INAF, (iv) it focus on a new application: the use of 
readability assessment for text simplification sys-
tems; and (v) it is aimed at Portuguese. 
4.1 Features for Assessing Readability 
Our feature set (Table 2) consists of 3 groups of 
features. The first group contains cognitively-
motivated features (features 1-42), derived from 
the Coh-Metrix-PORT tool (see Section 4.1.1). 
The second group contains features that reflect the 
incidence of particular syntactic constructions 
which we target in our text simplification system 
(features 43-49). The third group (the remaining 
features in Table 2) contains features derived from 
n-gram language models built considering uni-
grams, bigrams and trigrams probability and per-
plexity plus out-of-vocabulary rate scores. We later 
refer to a set of basic features, which consist of 
simple counts that do not require any linguistic tool 
or external resources to be computed. This set cor-
responds to features 1-3 and 9-11. 
4.1.1 Coh-Metrix-Port 
The Coh-Metrix tool was developed to compute 
features potentially relevant to the comprehension 
of English texts through a number of measures in-
formed by linguistics, psychology and cognitive 
studies. The main aspects covered by the measures 
are cohesion and coherence (Graesser et al, 2004). 
Coh-Metrix 2.0, the free version of the tool, con-
tains 60 readability metrics. The Coh-Metrix-
PORT tool (Scarton et al, 2009) computes similar 
metrics for texts in Brazilian Portuguese. The ma-
jor challenge to create such tool is the lack of some 
of the necessary linguistic resources. The follow-
ing metrics are currently available in the tool (we 
refer to Table 2 for details): 
1. Readability metric: feature 12. 
 
2. Words and textual information:  
 Basic counts: features 1 to 11. 
1 Number of words 
2 Number of sentences 
3 Number of paragraphs 
4 Number of verbs 
5 Number of nouns 
6 Number of adjectives 
7 Number of adverbs 
8 Number of pronouns 
9 Average number of words per sentence 
10 Average number of sentences per paragraph 
11 Average number of syllables per word 
12 Flesch index for Portuguese 
13 Incidence of content words 
14 Incidence of functional words  
15 Raw Frequency of content words  
16 Minimal frequency of content words  
17 Average number of verb hypernyms 
18 Incidence of NPs 
19 Number of NP modifiers 
20 Number of words before the main verb 
21 Number of high level constituents 
22 Number of personal pronouns 
23 Type-token ratio 
24 Pronoun-NP ratio 
25 Number of ?e? (and) 
26 Number of ?ou? (or)  
27 Number of ?se? (if) 
28 Number of negations 
29 Number of logic operators 
30 Number of connectives  
31 Number of positive additive connectives 
32 Number of negative additive connectives 
33 Number of positive temporal connectives 
34 Number of negative temporal connectives 
35 Number of positive causal connectives 
36 Number of negative causal connectives 
37 Number of positive logic connectives 
38 Number of negative logic connectives 
39 Verb ambiguity ratio 
40 Noun ambiguity ratio 
41 Adverb ambiguity ratio 
42 Adjective ambiguity ratio 
43 Incidence of clauses 
44 Incidence of adverbial phrases 
45 Incidence of apposition 
46 Incidence of passive voice 
47 Incidence of relative clauses 
48 Incidence of coordination 
49 Incidence of subordination 
50 Out-of-vocabulary words  
51 LM probability of unigrams  
52 LM perplexity of unigrams  
53 LM perplexity of unigrams, without line break  
54 LM probability of bigrams  
55 LM perplexity of bigrams  
56 LM perplexity of bigrams, without line break  
57 LM probability of trigrams  
58 LM perplexity of trigrams  
59 LM perplexity of trigrams, without line break  
Table 2. Feature set 
5
 Frequencies: features 15 to 16. 
 Hypernymy: feature 17. 
 
3. Syntactic information:  
 Constituents: features 18 to 20. 
 Pronouns: feature 22 
 Types and Tokens: features 23 to 24. 
 Connectives: features 30 to 38. 
 
4. Logical operators: features 25 to 29. 
 
The following resources for Portuguese were used: 
the MXPOST POS tagger (Ratnaparkhi, 1996), a 
word frequency list compiled from a 700 million-
token corpus2, a tool to identify reduced noun 
phrases (Oliveira et al, 2006), a list of connectives 
classified as positives/negatives and according to 
cohesion type (causal, temporal, additive or logi-
cal), a list of logical operators and WordNet.Br 
(Dias-da-Silva et al, 2008).  
In this paper we include seven new metrics to 
Coh-Metrix-PORT: features 13, 14, 21, and 39 to 
42. We used TEP3 (Dias-da-Silva et al, 2003) to 
obtain the number of senses of words (and thus 
their ambiguity level), and the Palavras parser 
(Bick, 2000) to identify the higher level constitu-
ents. The remaining metrics were computed based 
on the POS tags. 
According to a report on the performance of 
each Coh-Metrix-PORT metric (Scarton et al, 
2009), no individual feature provides sufficient in-
dication to measure text complexity, and therefore 
the need to exploit their combination, and also to 
combine them with the other types of features de-
scribed in this section. 
4.1.2 Language-model Features 
Language model features were derived from a 
large corpus composed of a sample of the Brazilian 
newspaper Folha de S?o Paulo containing issues 
from 12 months taken at random from 1994 to 
2005. The corpus contains 96,868 texts and 
26,425,483 tokens. SRILM (Stolcke, 2002), a 
standard language modelling toolkit, was used to 
produce the language model features.  
4.2 Learning Techniques 
Given that the boundaries of literacy level classes 
are one of the subjects of our study, we exploit 
three different types of models in order to check 
                                                          
2 http://www2.lael.pucsp.br/corpora/bp/index.htm 
3 http://www.nilc.icmc.usp.br/tep2/index.htm 
which of them can better distinguish among the 
three literacy levels. We therefore experiment with 
three types of machine learning algorithms: a stan-
dard classifier, an ordinal (ranking) classifier and a 
regressor. Each algorithm assumes different rela-
tions among the groups: the classifier assumes no 
relation, the ordinal classifier assumes that the 
groups are ordered, and the regressor assumes that 
the groups are continuous.  
As classifier we use the Support Vector Ma-
chines (SVM) implementation in the Weka4 toolkit 
(SMO). As ordinal classifier we use a meta clas-
sifier in Weka which takes SMO as the base classi-
fication algorithm and performs pairwise classifi-
cations (OrdinalClassClassifier). For regression we 
use the SVM regression implementation in Weka 
(SMO-reg). We use the linear versions of the algo-
rithms for classification, ordinal classification and 
regression, and also experiment with a radial basis 
function (RBF) kernel for regression. 
5. Experiments 
5.1 Corpora 
In order to train (and test) the different machine 
learning algorithms to automatically identify the 
readability level of the texts we make use of ma-
nually simplified corpora created in the PorSimples 
project. Seven corpora covering our three literacy 
levels (advanced, basic and rudimentary) and two 
different genres were compiled. The first corpus is 
composed of general news articles from the Brazil-
ian newspaper Zero Hora (ZH original). These ar-
ticles were manually simplified by a linguist, ex-
pert in text simplification, according to the two 
levels of simplification: natural (ZH natural) and 
strong (ZH strong). The remaining corpora are 
composed of popular science articles from differ-
ent sources: (a) the Caderno Ci?ncia section of the 
Brazilian newspaper Folha de S?o Paulo, a main-
stream newspaper in Brazil (CC original) and a 
manually simplified version of this corpus using 
the natural (CC natural) and strong (CC strong) 
levels; and (b) advanced level texts from a popular 
science magazine called Ci?ncia Hoje (CH). Table 
3 shows a few statistics about these seven corpora. 
5.2 Feature Analysis 
As a simple way to check the contribution of dif-
ferent features to our three literacy levels, we com- 
                                                          
4 http://www.cs.waikato.ac.nz/ml/weka/ 
6
  
Corpus Doc Sent Words Avg. words 
per text (std. 
deviation) 
Avg. 
words p. 
sentence 
ZH original 104 2184 46190 444.1 (133.7) 21.1 
ZH natural 104 3234 47296 454.7 (134.2) 14.6 
ZH strong 104 3668 47938 460.9 (137.5) 13.0 
CC original 50 882 20263 405.2 (175.6) 22.9 
CC natural 50 975 19603 392.0 (176.0) 20.1 
CC strong 50 1454 20518 410.3 (169.6) 14.1 
CH 130 3624 95866 737.4 (226.1) 26.4 
Table 3. Corpus statistics 
 
puted the (absolute) Pearson correlation between 
our features and the expected literacy level for the 
two sets of corpora that contain versions of the 
three classes of interest (original, natural and 
strong). Table 4 lists the most highly correlated 
features. 
 
 Feature Corr. 
1 Words per sentence 0.693 
2 Incidence of apposition 0.688 
3 Incidence of clauses 0.614 
4 Flesch index  0.580 
5 Words before main verb  0.516 
6 Sentences per paragraph  0.509 
7 Incidence of relative clauses  0.417 
8 Syllables per word 0.414 
9 Number of positive additive connectives  0.397 
10 Number of negative causal connectives 0.388 
Table 4: Correlation between features and literacy levels 
 
Among the top features are mostly basic and syn-
tactic features representing the number of apposi-
tive and relative clauses and clauses in general, and 
also features from Coh-Metrix-PORT. This shows 
that traditional cognitively-motivated features can 
be complemented with more superficial features 
for readability assessment. 
5.3 Predicting Complexity Levels 
As previously discussed, the goal is to predict the 
complexity level of a text as original, naturally or 
strongly simplified, which correspond to the three 
literacy levels of INAF: rudimentary, basic and ad-
vanced level.  
Tables 5-7 show the results of our experiments 
using 10-fold cross-validation and standard classi-
fication (Table 5), ordinal classification (Table 6) 
and regression (Table 7), in terms of F-measure 
(F), Pearson correlation with true score (Corr.) and 
mean absolute error (MAE). Results using our 
complete feature set (All) and different subsets of 
it are shown so that we can analyze the 
performance of each group of features. We also 
experiment with the Flesch index on its own as a 
feature. 
 
Features Class F Corr. MAE 
All original 0.913 0.84 0.276 
natural 0.483 
strong 0.732 
Language 
Model 
original 0.669 0.25 0.381 
natural 0.025 
strong 0.221 
Basic original 0.846 0.76 0.302 
natural 0.149 
strong 0.707 
Syntactic original 0.891 0.82 0.285 
natural 0.32 
strong 0.74 
Coh-
Metrix-
PORT 
original 0.873 0.79 0.290 
natural 0.381 
strong 0.712 
Flesch original 0.751 0.52 0.348 
natural 0.152 
strong 0.546 
Table 5: Standard Classification 
 
Features Class F Corr. MAE 
All original 0.904 0.83 0.163 
natural 0.484 
strong 0.731 
Language 
Model 
original 0.634 0.49 0.344 
natural 0.497 
strong 0.05 
Basic original 0.83 0.73 0.231 
natural 0.334 
strong 0.637 
Syntactic original 0.891 0.81 0.180 
natural 0.382 
strong 0.714 
Coh-
Metrix-
PORT 
original 0.878 0.8 0.183 
natural 0.432 
strong 0.709 
Flesch original 0.746 0.56 0.310 
natural 0.489 
strong 0 
Table 6: Ordinal classification 
 
The results of the standard and ordinal classifica-
tion are comparable in terms of F-measure and cor-
relation, but the mean absolute error is lower for 
the ordinal classification. This indicates that ordi-
nal classification is more adequate to handle our 
classes, similarly to the results found in (Heilman 
et al, 2008). Results also show that distinguishing 
between natural and strong simplifications is a 
harder problem than distinguishing between these 
and original texts. This was expected, since these 
two levels of simplification share many features. 
However, the average performance achieved is 
considered satisfactory. 
Concerning the regression model (Table 7), the 
RBF kernel reaches the best correlation scores 
7
among all models. However, its mean error rates 
are above the ones found for classification. A lin-
ear SVM (not shown here) achieves very poor re-
sults across all metrics. 
   
Features Corr. MAE 
All 0.8502 0.3478 
Language Model 0.6245 0.5448 
Basic 0.7266 0.4538 
Syntactic 0.8063 0.3878 
Coh-Metrix-PORT 0.8051 0.3895 
Flesch 0.5772 0.5492 
Table 7: Regression with RBF kernel 
 
With respect to the different feature sets, we can 
observe that the combination of all features consis-
tently yields better results according to all metrics 
across all our models. The performances obtained 
with the subsets of features vary considerably from 
model to model, which shows that the combination 
of features is more robust across different learning 
techniques. Considering each feature set independ-
ently, the syntactic features, followed by Coh-
Metrix-PORT, achieve the best correlation scores, 
while the language model features performed the 
poorest. 
These results show that it is possible to predict 
with satisfactory accuracy the readability level of 
texts according to our three classes of interest: 
original, naturally simplified and strongly simpli-
fied texts. Given such results we embedded the 
classification model (Table 5) as a tool for read-
ability assessment into our text simplification au-
thoring system. The linear classification is our 
simplest model, has achieved the highest F-
measure and its correlation scores are comparable 
to those of the other models.  
6. Conclusions 
We have experimented with different machine 
learning algorithms and features in order to verify 
whether it was possible to automatically distin-
guish among the three readability levels: original 
texts aimed at advanced readers, naturally simpli-
fied texts aimed at people with basic literacy level, 
and strongly simplified texts aimed at people with 
rudimentary literacy level. All algorithms achieved 
satisfactory performance with the combination of 
all features and we embedded the simplest model 
into our authoring tool. 
As future work, we plan to investigate the con-
tribution of deeper cognitive features to this prob-
lem, more specifically, semantic, co-reference and 
mental model dimensions metrics. Having this ca-
pacity for readability assessment is useful not only 
to inform authors preparing simplified material 
about the complexity of the current material, but 
also to guide automatic simplification systems to 
produce simplifications with the adequate level of 
complexity according to the target user.  
The authoring tool, as well as its text simplifica-
tion and readability assessment systems, can be 
used not only for improving text accessibility, but 
also for educational purposes: the author can pre-
pare texts that are adequate according to the level 
of the reader and it will also allow them to improve 
their reading skills. 
References  
Sandra M. Alu?sio, Lucia Specia, Thiago A. S. Pardo, 
Erick G. Maziero, Renata P. M. Fortes (2008). To-
wards Brazilian Portuguese Automatic Text Simpli-
fication Systems. In the Proceedings of the 8th ACM 
Symposium on Document Engineering, pp. 240-248. 
Eckhard Bick (2000). The Parsing System "Palavras": 
Automatic Grammatical Analysis of Portuguese in a 
Constraint Grammar Framework. PhD Thesis. Uni-
versity of ?rhus, Denmark. 
Jill Burstein, Martin Chodorow and Claudia Leacock 
(2003). CriterionSM Online Essay Evaluation: An 
Application for Automated Evaluation of Student 
Essays. In the Proceedings of the Fifteenth Annual 
Conference on Innovative Applications of Artificial 
Intelligence, Acapulco, Mexico.  
Arnaldo Candido Jr., Erick Maziero, Caroline Gasperin, 
Thiago A. S. Pardo, Lucia Specia, and Sandra M. 
Aluisio (2009). Supporting the Adaptation of Texts 
for Poor Literacy Readers: a Text Simplification 
Editor for Brazilian Portuguese. In NAACL-HLT 
Workshop on Innovative Use of NLP for Building 
Educational Applications, pages 34?42, Boulder?.  
Helena de M. Caseli, Tiago de F. Pereira, L?cia Specia, 
Thiago A. S. Pardo, Caroline Gasperin and Sandra 
Maria Alu?sio (2009). Building a Brazilian Portu-
guese Parallel Corpus of Original and Simplified 
Texts. In the Proceedings of CICLing. 
Max Coltheart (1981). The MRC psycholinguistic data-
base. In Quartely Jounal of Experimental Psycholo-
gy, 33A, pages 497-505. 
Scott Deerwester, Susan T. Dumais, George W. Furnas, 
Thomas K. Landauer e Richard Harshman (1990). 
Indexing By Latent Semantic Analysis. In Journal of 
the American Society For Information Science, V. 
41, pages 391-407. 
Bento C. Dias-da-Silva and Helio R. Moraes (2003). A 
constru??o de um thesaurus eletr?nico para o portu-
gu?s do Brasil. In ALFA- Revista de Ling??stica, V. 
8
47, N. 2, pages 101-115.    
Bento C Dias-da-Silva, Ariani Di Felippo and Maria das 
Gra?as V. Nunes (2008). The automatic mapping of 
Princeton WordNet lexical conceptual relations onto 
the Brazilian Portuguese WordNet database. In Pro-
ceedings of the 6th LREC, Marrakech, Morocco. 
William H. DuBay (2004). The principles of readability. 
Costa Mesa, CA: Impact Information: http://www.i 
mpact-information.com/impactinfo/readability02.pdf 
Christiane Fellbaum (1998). WordNet: An electronic 
lexical database. Cambridge, MA: MIT Press. 
Lijun Feng, No?mie Elhadad and Matt Huenerfauth 
(2009). Cognitively Motivated Features for Reada-
bility Assessment. In the Proceedings of EACL 
2009, pages 229-237. 
Ingo Gl?ckner, Sven Hartrumpf, Hermann Helbig, Jo-
hannes Leveling and Rainer Osswald (2006b). An 
architecture for rating and controlling text readabili-
ty. In Proceedings of KONVENS 2006, pages 32-35. 
Konstanz, Germany.  
Arthur C. Graesser, Danielle S. McNamara, Max M. 
Louwerse and Zhiqiang Cai (2004). Coh-Metrix: 
Analysis of text on cohesion and language. In Beha-
vioral Research Methods, Instruments, and Comput-
ers, V. 36, pages 193-202. 
Ronald K. Hambleton, H. Swaminathan and H. Jane 
Rogers (1991). Fundamentals of item response 
theory. Newbury Park, CA: Sage Press. 
Michael Heilman, Kevyn Collins-Thompson, Jamie 
Callan and Max Eskenazi (2007). Combining lexical 
and grammatical features to improve readability 
measures for first and second language texts. In the 
Proceedings of NAACL HLT 2007, pages 460-467. 
Michael Heilman, Kevyn Collins-Thompson and Max-
ine Eskenazi (2008). An Analysis of Statistical 
Models and Features for Reading Difficulty Predic-
tion. In Proceedings of the 3rd Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 71-79. 
INAF (2009). Instituto P. Montenegro and A??o Educa-
tiva. INAF Brasil - Indicador de Alfabetismo Funcio-
nal - 2009. Available online at http://www. ibope. 
com.br/ipm/relatorios/relatorio_inaf_2009.pdf  
Teresa B. F. Martins, Claudete M. Ghiraldelo, Maria 
das Gra?as V. Nunes e Osvaldo N. de Oliveira Jr. 
(1996). Readability formulas applied to textbooks in 
brazilian portuguese. ICMC Technical Report, N. 
28, 11p.  
Aur?lien Max (2006). Writing for Language-impaired 
Readers. In Proceedings of CICLing, pages 567-570. 
Danielle McNamara, Max Louwerse, and Art Graesser, 
2002. Coh-Metrix: Automated cohesion and coher-
ence scores to predict text readability and facilitate 
comprehension. Grant proposal. http://cohmetrix. 
memphis.edu/cohmetrixpr/publications.html 
Eleni Miltsakaki and Audrey Troutt (2007). Read-X: 
Automatic Evaluation of Reading Difficulty of Web 
Text. In the Proceedings of E-Learn 2007, Quebec, 
Canada. 
Eleni Miltsakaki and Audrey Troutt (2008). Real Time 
Web Text Classification and Analysis of Reading 
Difficulty. In the Proceedings of the 3rd Workshop 
on Innovative Use of NLP for Building Educational 
Applications, Columbus, OH. 
Cl?udia Oliveira, Maria C. Freitas, Violeta Quental, C?-
cero N. dos Santos, Renato P. L. and Lucas Souza 
(2006). A Set of NP-extraction rules for Portuguese: 
defining and learning. In 7th Workshop on Computa-
tional Processing of Written and Spoken Portuguese, 
Itatiaia, Brazil.  
Sarah E. Petersen and Mari Ostendorf (2009). A ma-
chine learning approach to reading level assess-
ment. Computer Speech and Language 23, 89-106. 
Emily Pitler and Ani Nenkova (2008). Revisiting reada-
bility: A unified framework for predicting text quali-
ty. In Proceedings of EMNLP, 2008. 
Adwait Ratnaparkhi (1996). A Maximum Entropy Part-
of-Speech Tagger. In Proceedings of the First Em-
pirical Methods in Natural Language Processing 
Conference, pages133-142. 
Brian Roark, Margaret Mitchell and Kristy Holling-
shead (2007). Syntactic complexity measures for de-
tecting mild cognitive impairment. In the Proceed-
ings of the Workshop on BioNLP 2007: Biological, 
Translational, and Clinical Language Processing, 
Prague, Czech Republic. 
Caroline E. Scarton, Daniel M. Almeida, Sandra M. A-
lu?sio (2009). An?lise da Inteligibilidade de textos 
via ferramentas de Processamento de L?ngua Natu-
ral: adaptando as m?tricas do Coh-Metrix para o 
Portugu?s. In Proceedings of STIL-2009, S?o Carlos, 
Brazil.   
Sarah E. Schwarm and Mari Ostendorf (2005). Reading 
Level Assessment Using Support Vector Machines 
and Statistical Language Models. In the Proceedings 
of the 43rd Annual Meeting of the ACL, pp 523?530. 
Kathleen M. Sheehan, Irene Kostin and Yoko Futagi 
(2007). Reading Level Assessment for Literary and 
Expository Texts. In D. S. McNamara and J. G. 
Trafton (Eds.), Proceedings of the 29th Annual Cog-
nitive Science Society, page 1853. Austin, TX: Cog-
nitive Science Society. 
Advaith Siddharthan (2003). Syntactic Simplification 
and Text Cohesion. PhD Thesis. University of Cam-
bridge. 
Andreas Stolcke. SRILM -- an extensible language 
modeling toolkit. In Proceedings of the International 
Conference on Spoken Language Processing, 2002. 
9
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 116?122,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
TINE: A Metric to Assess MT Adequacy
Miguel Rios, Wilker Aziz and Lucia Specia
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton, WV1 1SB, UK
{m.rios, w.aziz, l.specia}@wlv.ac.uk
Abstract
We describe TINE, a new automatic evalua-
tion metric for Machine Translation that aims
at assessing segment-level adequacy. Lexical
similarity and shallow-semantics are used as
indicators of adequacy between machine and
reference translations. The metric is based on
the combination of a lexical matching com-
ponent and an adequacy component. Lexi-
cal matching is performed comparing bags-
of-words without any linguistic annotation.
The adequacy component consists in: i) us-
ing ontologies to align predicates (verbs), ii)
using semantic roles to align predicate argu-
ments (core arguments and modifiers), and
iii) matching predicate arguments using dis-
tributional semantics. TINE?s performance
is comparable to that of previous metrics
at segment level for several language pairs,
with average Kendall?s tau correlation from
0.26 to 0.29. We show that the addition of
the shallow-semantic component improves the
performance of simple lexical matching strate-
gies and metrics such as BLEU.
1 Introduction
The automatic evaluation of Machine Translation
(MT) is a long-standing problem. A number of met-
rics have been proposed in the last two decades,
mostly measuring some form of matching between
the MT output (hypothesis) and one or more human
(reference) translations. However, most of these
metrics focus on fluency aspects, as opposed to ad-
equacy. Therefore, measuring whether the meaning
of the hypothesis and reference translation are the
same or similar is still an understudied problem.
The most commonly used metrics, BLEU (Pap-
ineni et al, 2002) and alike, perform simple exact
matching of n-grams between hypothesis and refer-
ence translations. Such a simple matching proce-
dure has well known limitations, including that the
matching of non-content words counts as much as
the matching of content words, that variations of
words with the same meaning are disregarded, and
that a perfect matching can happen even if the order
of sequences of n-grams in the hypothesis and ref-
erence translation are very different, changing com-
pletely the meaning of the translation.
A number of other metrics have been proposed
to address these limitations, for example, by allow-
ing for the matching of synonyms or paraphrases
of content words, such as in METEOR (Denkowski
and Lavie, 2010). Other attempts have been made
to capture whether the reference translation and hy-
pothesis translations share the same meaning us-
ing shallow semantics, i.e., Semantic Role Labeling
(Gime?nez and Ma?rquez, 2007). However, these are
limited to the exact matching of semantic roles and
their fillers.
We propose TINE, a new metric that comple-
ments lexical matching with a shallow semantic
component to better address adequacy. The main
contribution of such a metric is to provide a more
flexible way of measuring the overlap between shal-
low semantic representations that considers both the
semantic structure of the sentence and the content
of the semantic elements. The metric uses SRLs
such as in (Gime?nez and Ma?rquez, 2007). However,
it analyses the content of predicates and arguments
seeking for either exact or ?similar? matches. The
116
inexact matching is based on the use of ontologies
such as VerbNet (Schuler, 2006) and distributional
semantics similarity metrics, such as Dekang Lin?s
thesaurus (Lin, 1998) .
In the remainder of this paper we describe some
related work (Section 2), present our metric - TINE
- (Section 3) and its performance compared to pre-
vious work (Section 4) as well as some further im-
provements. We then provide an analysis of these
results and discuss the limitations of the metric (Sec-
tion 5) and present conclusions and future work
(Section 6).
2 Related Work
A few metrics have been proposed in recent years
to address the problem of measuring whether a hy-
pothesis and a reference translation share the same
meaning. The most well-know metric is probably
METEOR (Banerjee and Lavie, 2005; Denkowski
and Lavie, 2010). METEOR is based on a general-
ized concept of unigram matching between the hy-
pothesis and the reference translation. Alignments
are based on exact, stem, synonym, and paraphrase
matches between words and phrases. However, the
structure of the sentences is not considered.
Wong and Kit (2010) measure word choice and
word order by the matching of words based on
surface forms, stems, senses and semantic similar-
ity. The informativeness of matched and unmatched
words is also weighted.
Liu et al (2010) propose to match bags of uni-
grams, bigrams and trigrams considering both recall
and precision and F-measure giving more impor-
tance to recall, but also using WordNet synonyms.
Tratz and Hovy (2008) use transformations in or-
der to match short syntactic units defined as Ba-
sic Elements (BE). The BE are minimal-length
syntactically well defined units. For example,
nouns, verbs, adjectives and adverbs can be con-
sidered BE-Unigrams, while a BE-Bigram could be
formed from a syntactic relation (e.g. subject+verb,
verb+object). BEs can be lexically different, but se-
mantically similar.
Pado? et al (2009) uses Textual Entailment fea-
tures extracted from the Standford Entailment Rec-
ognizer (MacCartney et al, 2006). The Textual En-
tailment Recognizer computes matching and mis-
matching features over dependency parses. The met-
ric then predicts the MT quality with a regression
model. The alignment is improved using ontologies.
He et al (2010) measure the similarity between
hypothesis and reference translation in terms of
the Lexical Functional Grammar (LFG) represen-
tation. The representation uses dependency graphs
to generate unordered sets of dependency triples.
Calculating precision, recall, and F-score on the
sets of triples corresponding to the hypothesis and
reference segments allows measuring similarity at
the lexical and syntactic levels. The measure also
matches WordNet synonyms.
The closest related metric to the one proposed in
this paper is that by Gime?nez and Ma?rquez (2007)
and Gime?nez et al (2010), which also uses shallow
semantic representations. Such a metric combines a
number of components, including lexical matching
metrics like BLEU and METEOR, as well as com-
ponents that compute the matching of constituent
and dependency parses, named entities, discourse
representations and semantic roles. However, the se-
mantic role matching is based on exact matching of
roles and role fillers. Moreover, it is not clear what
the contribution of this specific information is for the
overall performance of the metric.
We propose a metric that uses a lexical similar-
ity component and a semantic component in order
to deal with both word choice and semantic struc-
ture. The semantic component is based on seman-
tic roles, but instead of simply matching the surface
forms (i.e. arguments and predicates) it is able to
match similar words.
3 Metric Description
The rationale behind TINE is that an adequacy-
oriented metric should go beyond measuring the
matching of lexical items to incorporate information
about the semantic structure of the sentence, as in
(Gime?nez et al, 2010). However, the metric should
also be flexible to consider inexact matches of se-
mantic components, similar to what is done with lex-
ical metrics like METEOR (Denkowski and Lavie,
2010). We experiment with TINE having English
as target language because of the availability of lin-
guistic processing tools for this language. The met-
ric is particularly dependent on semantic role label-
117
ing systems, which have reached satisfactory perfor-
mance for English (Carreras and Ma?rquez, 2005).
TINE uses semantic role labels (SRL) and lexical se-
mantics to fulfill two requirements by: (i) compare
both the semantic structure and its content across
matching arguments in the hypothesis and refer-
ence translations; and (ii) propose alternative ways
of measuring inexact matches for both predicates
and role fillers. Additionally, it uses an exact lexi-
cal matching component to reward hypotheses that
present the same lexical choices as the reference
translation. The overall score s is defined using the
simple weighted average model in Equation (1):
s(H,R) = max
{
?L(H,R) + ?A(H,R)
?+ ?
}
R?R
(1)
where H represents the hypothesis translation, R
represents a reference translation contained in the set
of available references R; L defines the (exact) lex-
ical match component in Equation (2), A defines the
adequacy component in Equation (3); and ? and ?
are tunable weights for these two components. If
multiple references are provided, the score of the
segment is the maximum score achieved by compar-
ing the segment to each available reference.
L(H,R) =
|H
?
R|
?
|H| ? |R|
(2)
The lexical match component measures the over-
lap between the two representations in terms of the
cosine similarity metric. A segment, either a hypoth-
esis or a reference, is represented as a bag of tokens
extracted from an unstructured representation, that
is, bag of unigrams (words or stems). Cosine sim-
ilarity was chosen, as opposed to simply checking
the percentage of overlapping words (POW) because
cosine does not penalize differences in the length of
the hypothesis and reference translation as much as
POW. Cosine similarity normalizes the cardinality
of the intersection |H?R| using the geometric mean?
|H| ? |R| instead of the union |H?R|. This is par-
ticularly important for the matching of arguments -
which is also based on cosine similarity. If an hy-
pothesized argument has the same meaning as its
reference translation, but differs from it in length,
cosine will penalize less the matching than POW.
That is specially interesting when core arguments
get merged with modifiers due to bad semantic role
labeling (e.g. [A0 I] [T bought] [A1 something to eat
yesterday] instead of [A0 I] [T bought] [A1 some-
thing to eat] [AM-TMP yesterday]).
A(H,R) =
?
v?V verb score(Hv, Rv)
|Vr|
(3)
In the adequacy component, V is the set of verbs
aligned between H and R, and |Vr| is the number of
verbs in R. Hereafter the indexes h and r stand for
hypothesis and reference translations, respectively.
Verbs are aligned using VerbNet (Schuler, 2006) and
VerbOcean (Chklovski and Pantel, 2004). A verb in
the hypothesis vh is aligned to a verb in the refer-
ence vr if they are related according to the follow-
ing heuristics: (i) the pair of verbs share at least one
class in VerbNet; or (ii) the pair of verbs holds a re-
lation in VerbOcean.
For example, in VerbNet the verbs spook and ter-
rify share the same class amuse-31.1, and in VerbO-
cean the verb dress is related to the verb wear.
verb score(Hv, Rv) =
?
a?Ar?At
arg score(Ha, Ra)
|Ar|
(4)
The similarity between the arguments of a verb
pair (vh, vr) in V is measured as defined in Equa-
tion (4), where Ah and At are the sets of labeled
arguments of the hypothesis and the reference re-
spectively and |Ar| is the number of arguments of
the verb in R. In other words, we only measure the
similarity of arguments in a pair of sentences that are
annotated with the same role. This ensures that the
structure of the sentence is taken into account (for
example, an argument in the role of agent would not
be compared against an argument in a role of experi-
encer). Additionally, by restricting the comparison
to arguments of a given verb pair, we avoid argument
confusion in sentences with multiple verbs.
The arg score(Ha, Ra) computation is based on
the cosine similarity as in Equation (2). We treat
the tokens in the argument as a bag-of-words. How-
ever, in this case we change the representation of
the segments. If the two sets do not match exactly,
we expand both of them by adding similar words.
For every mismatch in a segment, we retrieve the
118
20-most similar words from Dekang Lin?s distribu-
tional thesaurus (Lin, 1998), resulting in sets with
richer lexical variety.
The following example shows how the computa-
tion of A(H,R) is performed, considering the fol-
lowing hypothesis and reference translations:
H: The lack of snow discourages people from ordering
ski stays in hotels and boarding houses.
R: The lack of snow is putting people off booking ski
holidays in hotels and guest houses.
1. extract verbs from H: Vh = {discourages, ordering}
2. extract verbs from R: Vr = {putting, booking}
3. similar verbs aligned with VerbNet (shared class
get-13.5.1): V = {(vh = order,vr = book)}
4. compare arguments of (vh = order,vr = book):
Ah = {A0, A1, AM-LOC}
Ar = {A0, A1, AM-LOC}
5. Ah ?Ar = {A0, A1, AM-LOC}
6. exact matches:
HA0 = {people} and RA0 = {people}
argument score = 1
7. different word forms: expand the representation:
HA1 = {ski, stays} and RA1 = {ski, holidays}
expand to:
HA1 = {{ski},{stays, remain... journey...}}
RA1 = {{ski},{holidays, vacations, trips... jour-
ney...}}
argument score = 0.5
8. similarly to HAM?LOC and RAM?LOC
argument score = 0.72
9. verb score (order, book) = 1+0.5+0.723 = 0.74
10. A(H,R) = 0.742 = 0.37
Different from previous work, we have not used
WordNet to measure lexical similarity for two main
reasons: problems with lexical ambiguity and lim-
ited coverage in WordNet (instances of named enti-
ties are not in WordNet, e.g. Barack Obama). For
example, in WordNet the aligned verbs (order/book)
from the previous hypothesis and reference trans-
lations have: 9 senses - order (e.g. give instruc-
tions to or direct somebody to do something with
authority, make a request for something, etc.) - and
4 senses - book (engage for a performance, arrange
for and reserve (something for someone else) in ad-
vance, etc.). Thus, a WordNet-based similarity mea-
sure would require disambiguating segments, an ad-
ditional step and a possible source of errors. Second,
a thresholds would need to be set to determine when
a pair of verbs is aligned. In contrast, the structure of
VerbNet (i.e. clusters of verbs) allows a binary deci-
sion, although the VerbNet heuristic results in some
errors, as we discuss in Section 5.
4 Results
We set the weights ? and ? by experimental test-
ing to ? = 1 and ? = 0.25. The lexical component
weight is prioritized because it has shown a good av-
erage Kendall?s tau correlation (0.23) on a develop-
ment dataset (Callison-Burch et al, 2010). Table 1
shows the correlation of the lexical component with
human judgments for a number of language pairs.
Table 1: Kendall?s tau segment-level correlation of the
lexical component with human judgments
Metric cz-en fr-en de-en es-en avg
Lexical 0.27 0.21 0.26 0.19 0.23
We use the SENNA1 SRL system to tag the
dataset with semantic roles. SENNA has shown to
have achieved an F-measure of 75.79% for tagging
semantic roles over the CoNLL 2005 2 benchmark.
We compare our metric against standard BLEU
(Papineni et al, 2002), METEOR (Denkowski and
Lavie, 2010) and other previous metrics reported in
(Callison-Burch et al, 2010) which also claim to use
some form of semantic information (see Section 2
for their description). The comparison is made in
terms of Kendall?s tau correlation against the human
judgments at a segment-level. For our submission to
the shared evaluation task, system-level scores are
obtained by averaging the segment-level scores.
TINE achieves the same average correlation with
BLUE, but outperforms it for some language pairs.
Additionally, TINE outperforms some of the previ-
ous which use WordNet to deal with synonyms as
part of the lexical matching.
The closest metric to TINE (Gime?nez et al,
2010), which also uses semantic roles as one of its
1http://ml.nec-labs.com/senna/
2http://www.lsi.upc.edu/ srlconll/
119
Table 2: Comparison with previous semantically-
oriented metrics using segment-level Kendall?s tau cor-
relation with human judgments
Metric cz-en fr-en de-en es-en avg
(Liu et al,
2010)
0.34 0.34 0.38 0.34 0.35
(Gime?nez
et al, 2010)
0.34 0.33 0.34 0.33 0.33
(Wong and
Kit, 2010)
0.33 0.27 0.37 0.32 0.32
METEOR 0.33 0.27 0.36 0.33 0.32
TINE 0.28 0.25 0.30 0.22 0.26
BLEU 0.26 0.22 0.27 0.28 0.26
(He et al,
2010)
0.15 0.14 0.17 0.21 0.17
(Tratz
and Hovy,
2008)
0.05 0.0 0.12 0.05 0.05
components, achieves better performance. However,
this metric is a rather complex combination of a
number of other metrics to deal with different lin-
guistic phenomena.
4.1 Further Improvements
As an additional experiment, we use BLEU as the
lexical component L(H,R) in order to test if the
shallow-semantic component can contribute to the
performance of this standard evaluation metric. Ta-
ble 3 shows the results of the combination of BLEU
and the shallow-semantic component using the same
parameter configuration as in Section 4. The addi-
tion of the shallow-semantic component increased
the average correlation of BLEU from 0.26 to 0.28.
Table 3: TINE-B: Combination of BLEU and the
shallow-semantic component
Metric cz-en fr-en de-en es-en avg
TINE-B 0.27 0.25 0.30 0.30 0.28
Finally, we improve the tuning of the weights of
the components (? and ? parameters) by using a
simple genetic algorithm (Back et al, 1999) to se-
lect the weights that maximize the correlation with
human scores on a development set (we use the de-
velopment sets from WMT10 (Callison-Burch et al,
2010)). The configuration of the genetic algorithm
is as follows:
? Fitness function: Kendall?s tau correlation
? Chromosome: two real numbers, ? and ?
? Number of individuals: 80
? Number of generations: 100
? Selection method: roulette
? Crossover probability: 0.9
? Mutation probability: 0.01
Table 4 shows the parameter values obtaining
from tuning for each language pair and the corre-
lation achieved by the metric with such parameters.
With such an optimization step the average correla-
tion of the metric increases to 0.29.
Table 4: Optimized values of the parameters using a ge-
netic algorithm and Kendall?s tau and final correlation of
the metric on the test sets
Language pair Correlation ? ?
cz-en 0.28 0.62 0.02
fr-en 0.25 0.91 0.03
de-en 0.30 0.72 0.1
es-en 0.31 0.57 0.02
avg 0.29 ? ?
5 Discussion
In what follows we discuss with a few examples
some of the common errors made by TINE. Over-
all, we consider the following categories of errors:
1. Lack of coverage of the ontologies.
R: This year, women were awarded the Nobel Prize in all
fields except physics
H: This year the women received the Nobel prizes in all
categories less physical
The lack of coverage in VerbNet prevented the
detection of the similarity between receive and
award.
2. Matching of unrelated verbs.
R: If snow falls on the slopes this week, Christmas will
sell out too, says Schiefert.
H: If the roads remain snowfall during the week, the dates
of Christmas will dry up, said Schiefert.
In VerbOcean remain and say are incorrectly
120
said to be related. VerbOcean was cre-
ated by a semi-automatic extraction algorithm
(Chklovski and Pantel, 2004) with an average
accuracy of 65.5%.
3. Incorrect tagging of the semantic roles by
SENNA.
R: Colder weather is forecast for Thursday, so if anything
falls, it should be snow.
H: On Thursday , must fall temperatures and, if there is
rain, in the mountains should.
The position of the predicates affects the SRL
tagging. The predicate fall has the following
roles (A1, V, and S-A1) in the reference, and
the following roles (AM-ADV, A0, AM-MOD,
and AM-DIS) in the hypothesis. As a con-
sequence, the metric cannot attempt to match
the fillers. Also, SRL systems do not detect
phrasal verbs such as in the example of Section
3, where the action putting people off is similar
to discourages.
6 Conclusions and Future Work
We have presented an MT evaluation metric based
on the alignment of semantic roles and flexible
matching of role fillers between hypothesis and ref-
erence translations. To deal with inexact matches,
the metric uses ontologies and distributional seman-
tics, as opposed to lexical databases like WordNet,
in order to minimize ambiguity and lack of cover-
age. The metric also uses an exact lexical matching
component to reward hypotheses that present lexical
choices similar to those of the reference translation.
Given the simplicity of the metric, it has achieved
competitive results. We have shown that the addition
of the shallow-semantic component into a lexical
component yields absolute improvements in the cor-
relation of 3%-6% on average, depending on the lex-
ical component used (cosine similarity or BLEU).
In future work, in order to improve the perfor-
mance of the metric we plan to add components to
address a few other linguistic phenomena such as
in (Gime?nez and Ma?rquez, 2007; Gime?nez et al,
2010). In order to deal with the coverage problem
of an ontology, we plan to use distributional seman-
tics (i.e. word space models) also to align the pred-
icates. We consider using a backoff model for the
shallow-semantic component to deal with the very
frequent cases where there are no comparable pred-
icates between the reference and hypothesis transla-
tions, which result in a 0 score from the semantic
component. Finally, we plan to improve the lexical
component to better tackle fluency, for example, by
adding information about the word order.
References
Thomas Back, David B. Fogel, and Zbigniew
Michalewicz, editors. 1999. Evolutionary Com-
putation 1, Basic Algorithms and Operators. IOP
Publishing Ltd., Bristol, UK, 1st edition.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July.
Xavier Carreras and Llu??s Ma?rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
In Proceedings of the 9th Conference on Natural Lan-
guage Learning, CoNLL-2005, Ann Arbor, MI USA.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic Verb
Relations. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 33?40, Barcelona,
Spain, July.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: Improved evaluation
support for five target languages. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 339?342, July.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2007. Linguistic fea-
tures for automatic evaluation of heterogenous mt sys-
tems. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT ?07, pages 256?
264, Stroudsburg, PA, USA.
Jesu?s Gime?nez, Llu??s Ma?rquez, Elisabet Comelles, Irene
Castello?n, and Victoria Arranz. 2010. Document-
level automatic mt evaluation based on discourse rep-
resentations. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, WMT ?10, pages 333?338, Stroudsburg, PA,
USA.
121
Yifan He, Jinhua Du, Andy Way, and Josef van Gen-
abith. 2010. The dcu dependency-based metric in
wmt-metricsmatr 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 349?353, Strouds-
burg, PA, USA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ?98, pages 768?
774, Stroudsburg, PA, USA.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
Tesla: translation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, WMT ?10, pages 354?359,
Stroudsburg, PA, USA.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the Human Language
Technology Conference of the NAACL, pages 41?48,
New York City, USA, June.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23:181?193, September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
Stephen Tratz and Eduard Hovy. 2008. Summarisation
evaluation using transformed basic elements. In Pro-
ceedings TAC 2008.
Billy T.-M. Wong and Chunyu Kit. 2010. The parameter-
optimized atec metric for mt evaluation. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, WMT ?10, pages 360?
364, Stroudsburg, PA, USA.
122
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 316?322,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Shallow Semantic Trees for SMT
Wilker Aziz, Miguel Rios and Lucia Specia
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton, WV1 1SB, UK
{w.aziz, m.rios, l.specia}@wlv.ac.uk
Abstract
We present a translation model enriched with
shallow syntactic and semantic information
about the source language. Base-phrase la-
bels and semantic role labels are incorporated
into an hierarchical model by creating shal-
low semantic ?trees?. Results show an in-
crease in performance of up to 6% in BLEU
scores for English-Spanish translation over a
standard phrase-based SMT baseline.
1 Introduction
The use of semantic information to improve Statis-
tical Machine Translation (SMT) is a very recent re-
search topic that has been attracting significant at-
tention. In this paper we describe our participation
in the shared translation task of the 6th Workshop on
Statistical Machine Translation (WMT) with a sys-
tem that incorporates shallow syntactic and semantic
information into hierarchical SMT models.
The system is based on the Moses toolkit (Hoang
et al, 2009; Koehn et al, 2007) using hierarchi-
cal models informed with shallow syntactic (chunks)
and semantic (semantic role labels) information for
the source language. The toolkit SENNA (Collobert
et al, 2011) is used to provide base-phrases (chunks)
and semantic role labels.
Experiments with English-Spanish and English-
German news datasets show promising results and
highlight important issues about the use of seman-
tic information in hierarchical models as well as a
number of possible directions for further research.
The remaining of the paper is organized as fol-
lows: Section 2 presents related work; Section 3 de-
scribes the method; Section 4 presents the results ob-
tained for the English-Spanish and English-German
translation tasks; and Section 5 brings some conclu-
sions and directions for further research.
2 Related Work
In hierarchical SMT (Chiang, 2005), a Synchronous
Context Free Grammar (SCFG) is learned from a
parallel corpus.The model capitalizes on the recur-
sive nature of language replacing sub-phrases by
an unlabeled nonterminal. Hierarchical models are
known to produce high coverage rules, once they are
only constrained by the word alignment. Neverthe-
less the lack of specialized vocabulary also leads to
spurious ambiguity (Chiang, 2005).
Syntax-based models are hierarchical models
whose rules are constrained by syntactic informa-
tion.The syntactic constraints have an impact in
the rule extraction process, reducing drastically the
number of rules available to the system. While this
may be helpful to reduce ambiguity, it can lead to
poorer performance (Ambati and Lavie, 2008).
Motivated by the fact that syntactically constrain-
ing a hierarchical model can decrease translation
quality, some attempts to overcome the problems
at rule extraction time have been made. Venugopal
and Zollmann (2006) propose a heuristic method to
relax parse trees known as Syntax Augmented Ma-
chine Translation (SAMT). Significant gains are ob-
tained by grouping nonterminals under categories
when they do not span across syntactic constituents.
Hoang and Koehn (2010) propose a soft syntax-
based model which combines the precision of a
syntax-constrained model with the coverage of an
316
unconstrained hierarchical model. Instead of hav-
ing heuristic strategies to combine nonterminals in a
parse tree, whenever a rule cannot be retrieved be-
cause it does not span a constituent, the extraction
procedure falls back to the hierarchical approach, re-
trieving a rule with unlabeled nonterminals. Perfor-
mance gains are reported over standard hierarchical
models using both full parse trees and shallow syn-
tax.
Moving beyond syntactic information, some at-
tempts have recently been made to add semantic an-
notations to SMT. Wu and Fung (2009) present a
two-pass model to incorporate semantic information
to the phrase-based SMT pipeline. The method per-
forms conventional translation in a first step, fol-
lowed by a constituent reordering step seeking to
maximize the cross-lingual match of the semantic
role labels of the translation and source sentences.
Liu and Gildea (2010) add features extracted from
the source sentences annotated with semantic role
labels in a tree-to-string SMT model. They mod-
ify a syntax-based SMT system in order to penal-
ize/reward role reordering and role deletion. The
input sentence is parsed for semantic roles and the
roles are then projected onto the target side using
word alignment information at decoding time. They
assume that a one-to-one mapping between source
and target roles is desirable.
Baker et al (2010) propose to graft semantic in-
formation, namely named entities and modalities, to
syntactic tags in a syntax-based model. The vocab-
ulary of nonterminals is specialized using the se-
mantic categories, for instance, a noun phrase (NP)
whose head is a geopolitical entity (GPE) will be
tagged as NPGPE, making the rule table less am-
biguous.
Similar to (Baker et al, 2010) we specialize a vo-
cabulary of syntactic nonterminals with semantic in-
formation, however we use shallow syntax (base-
phrases) and semantic role labels instead of con-
stituent parse and named entities. The resulting shal-
low trees are relaxed following SAMT (Venugopal
and Zollmann, 2006). Different from previous work
we add the semantic knowledge at the level of the
corpus annotation. As a consequence, instead of bi-
asing deletion and reordering through additional fea-
tures (Liu and Gildea, 2010), we learn hierarchical
rules that encode those phenomena, taking also into
account the semantic role of base-phrases.
3 Proposed Method
The proposed method is based on an extension of the
hierarchical models in Moses using source language
information. Our submission included systems for
two language pairs: English-Spanish (en-es) and
English-German (en-de) and was constrained to us-
ing data provided by WMT11. Phrase and rule ex-
traction were performed using the entire en-es and
en-de portions of Europarl. Model parameters were
tuned using the news-test2008 dataset. Three 5-
gram Spanish and German language models were
trained using SRILM1 with the News Commentaries
(? 160K sentences), Europarl (? 2M sentences)
and News (? 5M sentences) corpora. These models
were interpolated using scripts provided in Moses
(Koehn and Schroeder, 2007).
At pre-processing stage, sentences longer than 80
tokens were filtered from the training/development
corpus. The parallel corpus was then tokenized and
truecased. Additionally, for en-de, compound split-
ting of the German side of the corpus was performed
using a frequency based method described in (Koehn
and Knight, 2003). This method helps alleviate spar-
sity, reducing the size of the vocabulary by decom-
posing compounds into their base words. Recas-
ing and detokenization, along with compound merg-
ing of the translations into German, were handled
at post-processing stage. Compound merging was
performed by finding the most likely sequences of
words to be merged into previously seen compounds
(Stymne, 2009).
3.1 Source Language Annotation
For rule extraction, training and test, the English side
of the corpus was annotated with Semantic Role La-
bels (SRL) using the toolkit SENNA2, which also
outputs POS and base-phrase (without prepositional
attachment) tags. The resulting source language an-
notation was used to produce trees in order to build
a tree-to-string model in Moses.
1http://www.speech.sri.com/projects/
srilm/
2http://ml.nec-labs.com/senna/
317
S
NP VP NP PP NP O O NP VP NP ADVP
PRP VBZ TO VB DT NN TO NN PUNC CC PRP VBZ RB VBD WDT RB
he intends to donate this money to charity , but he has not decided which yet
Figure 1: Example of POS tags and base-phrase annotation. Base-phrases: noun-phrase (NP), verb-phrase
(VP), prepositional-phrase (PP), adverbial-phrase (ADVP), outside-of-a-phrase (O)
In order to derive trees for the source side of the
corpus from this annotation, a new level is created to
add the POS tags for each word form. Syntactic tags
are then added by grouping words and POS tags into
base phrases using linguistic information as given
by SENNA. Figure 1 shows an example of an input
sentence annotated with POS and base-phrase infor-
mation. Additionally, SRLs are used to enrich the
POS and base-phrase annotation levels. Semantic
roles are assigned to each predicate independently.
As a consequence, the resulting annotation cannot
be considered a tree and there is not an obvious hi-
erarchy of predicates in a sentence. For example,
Figure 2 shows the SRL annotation for the example
in Figure 1.
[A0 He] [T intends] [A1 to donate this money to charity],
but he has not decided which yet
[A0 He] intends to [T donate] [A1 this money] [A2 to
charity], but he has not decided which yet
He intends to donate this money to charity, but [A0 he]
has [AM-NEG not] [T decided] [A1 which] [AM-TMP
yet]
Figure 2: SRL for sentence in Figure 1
Arguments of a single predicate never overlap,
however in longer sentences, the occurrence of mul-
tiple verbs increases the chances that arguments of
different predicates overlap, that is, the argument of
a verb might contain or even coincide with the argu-
ment of another verb and depending on the verb the
argument role might change. For example, in Fig-
ure 2: i) He is both the agent of intend and donate;
ii) this money is the donated thing and also part of
the chunk which express the intention (to donate this
money to charity). In a different example we can see
that arguments might overlap and their roles change
completely depending on their target predicates (e.g
in I gave you something to eat, you is the recipient
of the verb give and the agent of the verb eat). For
this reason, why semantic role labels are usually an-
notated individually in different structures, as shown
in Figure 2, each annotation focusing on a single tar-
get verb. In order to convert the predicates and argu-
ments of a sentence into a single tree, we enrich the
POS-tags and base-phrase annotation as follows:
? Semantic labels are directly grafted to the base-
phrase annotation whenever possible, that is,
if a predicate argument coincides with a sin-
gle base-phrase, the base-phrase type is spe-
cialized with the argument role. In Figure 3,
the noun-phrase (NP) the money is specialized
into NP:A1:donate, since that single NP is the
argument A1 of donate.
? If a predicate argument groups multiple base-
phrases, the semantic label applies to a node in
a new level of the tree subsuming all these base-
phrases. In Figure 3, the base-phrases to (PP)
and charity (NP) are grouped by A2:donate.
? We add the labels sequentially from the short-
est chunks to the largest ones. If two la-
bels spanning the same number of tokens: i)
overlap completely, we merge them so that
no hierarchy is imposed between their targets
(e.g. in Figure 3, the noun-phrase He is spe-
cialized into NP:A0:donate,intend); ii) over-
lap partially, we merge them so that the re-
sulting label will compete against other labels
in a different length category. If a label span-
ning a larger chunk overlaps partially with a
label spanning a shorter chunk, or contains it,
we stack them in a way that the first subsumes
the second (e.g in Figure 3, A1:intend sub-
sumes VP:T:donate, NP:A1:donate,intend and
A2:donate).
? Verb phrases might get split if they contain
multiple target predicates (e.g. in Figure 3,
the VP intends to donate is split into two verb-
318
phrases, each specialized with its own role la-
bel).
? Finally, tags are lexicalized, that is, semantic
labels are composed by their type (e.g. A0) and
target predicate lemma (verb).
Figure 3 shows and example of how semantic la-
bels are combined with shallow syntax in order to
produce the input tree for the sentence in Figure
1. The argument A1 of intend subsumes the target
verb donate and its arguments A1 and A2; A2:donate
groups base-phrases so as to attach the preposition to
the noun phrase.
Finally, following the method for syntactic trees
by Venugopal and Zollmann (2006), the input trees
are relaxed in order to alleviate the impact of the
linguistic constraints on rule extraction. We relax
trees3 by combining any pairs of neighboring nodes.
For example, NP:A0:donate,intend+VP:T:intend
and NP:A1:donate+A2:donate are created for the
tree in Figure 3.
4 Results
As a baseline to compare against our proposed ap-
proach (srl), we took a phrase-based SMT system
(pb) built using the Moses toolkit with the same
datasets and training conditions described in Sec-
tion 3. The results are reported in terms of standard
BLEU (Papineni et al, 2002) (and its case sensitive
version, BLEU-c) and tested for statistical signifi-
cance using an approximate randomization test (Rie-
zler and Maxwell, 2005) with 100 iterations.
In addition, we included an intermediate model
between these two: a hierarchical model in-
formed with source-language base-phrase informa-
tion (chunk). For the English-Spanish task we also
built a purely hierarchical model (hier) using Moses
and the same datasets and training conditions. For
the English-German task, hierarchical models have
not been shown to outperform standard phrase-based
models in previous work (Koehn et al, 2010).
Table 1 shows the performance achieved for the
English-Spanish translation task test set, where (srl)
is our official submission. One can notice a signifi-
cant gain in performance (up to 6% BLEU) in using
tree-based models (with or without source language
3Using the Moses implementation relax-parse for SAMT 2
annotation) as opposed to using standard phrase-
based models.
Model BLEU BLEU-c
pb 0.2429 0.2340
srl 0.2901 0.2805
hier 0.3029 0.2933
chunk 0.3034 0.2935
Table 1: English-Spanish experiments - differences
between all pairs of models are statistically signifi-
cant with 99% confidence, except for the pair (hier,
chunk)
The purely hierarchical approach performs as
well as our linguistically informed tree-based mod-
els (chunk and srl). On the one hand this finding
is somewhat disappointing as we expected that tree-
based models would benefit from linguistic annota-
tion. On the other hand it shows that the linguistic
annotation yields a significant reduction in the num-
ber of unnecessary productions: the linguistically in-
formed models are much smaller than hier (Table
5), but perform just as well. Whether the linguistic
annotation significantly helps make the productions
less ambiguous or not is still a question to be ad-
dressed in further experimentation.
Table 2 shows the performance achieved for the
English-German translation task test set. These re-
sults indicate that the linguistic information did not
lead to any significant gains in terms of automatic
metrics. An in-depth comparative analysis based on
a manual inspection of the translations remains to be
done.
Model BLEU BLEU-c
pb 0.1398 0.1360
srl 0.1381 0.1344
chunk 0.1403 0.1367
Table 2: English-German experiments - differences
between pairs of models are not statistically signifi-
cant
In Table 3 we also show the impact of three com-
pound merging strategies as post-processing for en-
de: i) no compound merging (nm), ii) frequency-
based compound merging (fb), and iii) frequency-
319
SNP:A0:donate,intend
PRP
He
VP:T:intend
VBZ
intends
A1:intend
VP:T:donate
TO
to
VB
donate
NP:A1:donate
DT
this
NN
money
A2:donate
PP
TO
to
NP
NN
charity
...
Figure 3: Tree for example in Figure 1
based compound merging constrained by POS4
(cfb). Applying both frequency-based compound
merging strategies (Stymne, 2009) resulted in sig-
nificant improvements of nearly 0.5% in BLEU.
Model BLEU BLEU-c
nm 0.1334 0.1298
fb 0.1369 0.1332
cfb 0.1381 0.1344
Table 3: English-German compound merging - dif-
ferences between all pairs of models are statistically
significant with 99% confidence
Another somewhat disappoint result is the perfor-
mance of srl when compared to chunk. We believe
the main reason why the chunk models outperform
the srl models is data sparsity. The semantic infor-
mation, and particularly the way it was used in this
paper, with lexicalized roles, led to a very sparse
model. As an attempt to make the srl model less
sparse, we tested a version of this model without
lexicalizing the semantic tags, in other words, us-
ing the semantic role labels only, for example, A1
instead of A1:intend in Figure 3. Table 4 shows that
models with lexicalized semantic roles (lex) consis-
tently outperform the alternative version (non lex),
although the differences were only statistically sig-
nificant for the en-de dataset. One reason for that
may be that non-lexicalized rules do not help mak-
4POS tagging was performed using the TreeTagger toolkit:
http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/
ing the chunk rules less ambiguous.
Model BLEU BLEU-c
en-esnon lex 0.2891 0.2795
en-eslex 0.2901 0.2805
en-denon lex 0.1319 0.1284
en-delex 0.1381 0.1344
Table 4: Alternative model with non-lexicalized tags
- differences are statistically significant with 99%
confidence for en-de only
Table 5 shows how the additional annotation con-
strains the rule extraction (for the en-es dataset). The
unconstrained model hier presents the largest rule
table, followed by the chunk model, which is only
constrained by syntactic information. The models
enriched with semantic labels, both the lexicalized
or non-lexicalized versions, contain a comparable
number of rules. They are at least half the size of
the chunk model and about 9 times smaller than the
hier model. However, the number of nonterminals
in the lexicalized models highlights the sparsity of
such models.
Model Rules Nonterminals
hier 962,996,167 1
chunk 235,910,731 3,390
srlnon lex 92,512,493 44,095
srllex 117,563,878 3,350,145
Table 5: Statistics from the rule table
In order to exemplify the importance of having
320
some form of lexicalized information as part of the
semantic models, Figure 4 shows two predicates
which present different semantic roles, even though
they have nearly the same shallow syntactic struc-
ture. In this case, unless lexicalized, rules map-
ping semantic roles into base-phrases become am-
biguous. Besides, the same role might appear sev-
eral times in the same sentence (Figure 2). In this
case, if the semantic roles are not annotated with
their target lemma, they bring additional confusion.
Therefore, the model needs the lexical information
to distinguish role deletion and reordering phenom-
ena across predicates.
Figure 4: Different SRL for similar chunks
[NP:A0 I] [VP:T gave] [NP:A2 you] [NP:A1 a car]
[NP:A0 I] [VP:T dropped] [NP:A1 the glass] [AM-LOC
[PP on] [NP the floor]]
In WMT11?s official manual evaluation, our sys-
tem submissions (srl) were ranked 10th out of 15
systems in the English-Spanish task, and 18th out
of 22 systems participating in the English-German
task. For detailed results refer to the overview paper
of the Shared Translation Task of the Sixth Work-
shop on Machine Translation (WMT11).
5 Conclusions
We have presented an effort towards using shal-
low syntactic and semantic information for SMT.
The model based on shallow syntactic information
(chunk annotation) has significantly outperformed a
baseline phrase-based model and performed as well
as a hierarchical phrase-based model with a signifi-
cantly smaller number of translation rules.
While annotating base-phrases with semantic la-
bels is intuitively a promising research direction, the
current model suffers from sparsity and representa-
tion issues resulting from the fact that multiple pred-
icates share arguments within a given sentence. As
a consequence, shallow semantics has not yet shown
improvements with respect to the chunk-based mod-
els.
In future work, we will address the sparsity is-
sues in the lexicalized semantic models by cluster-
ing predicates in a way that semantic roles can be
specialized with semantic categories, instead of the
verb lemmas.
References
Vamshi Ambati and Alon Lavie. 2008. Improving syntax
driven translation models by re-structuring divergent
and non-isomorphic parse tree structures. In The Eight
Conference of the Association for Machine Translation
in the Americas (AMTA).
Kathryn Baker, Michael Bloodgood, Chris Callison-
burch, Bonnie J. Dorr, Nathaniel W. Filardo, Lori
Levin, Scott Miller, and Christine Piatko. 2010.
Semantically-informed syntactic machine translation:
A tree-grafting approach.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceeding of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
arXiv:1103.0398v1.
Hieu Hoang and Philipp Koehn. 2010. Improved trans-
lation with source syntax labels. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 409?417.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A
unified framework for phrase-based, hierarchical, and
syntax-based statistical machine translation. In Pro-
ceedings of International Workshop on Spoken Lan-
guage Translation, pages 152 ? 159.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the
tenth conference on European chapter of the Associ-
ation for Computational Linguistics - Volume 1, pages
187?193.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 224?227.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Cal-
lison Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In 45th An-
nual Meeting of the Association for Computational
Linguistics.
Philipp Koehn, Barry Haddow, Philip Williams, and Hieu
Hoang. 2010. More linguistic annotation for statis-
tical machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, pages 115?120.
321
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 716?724.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for mt. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, Work-
shop in Intrinsic and Extrinsic Evaluation Measures
for MT and Summarization.
Sara Stymne. 2009. A comparison of merging strate-
gies for translation of german compounds. In Proceed-
ings of the 12th Conference of the European Chapter
of the Association for Computational Linguistics: Stu-
dent Research Workshop, pages 61?69.
Ashish Venugopal and Andreas Zollmann. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: a hybrid two-pass model. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 13?16.
322
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 137?147,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Towards an on-demand Simple Portuguese Wikipedia
Arnaldo Candido Junior Ann Copestake
Institute of Mathematics and Computer Sciences Computer Laboratory 
University of S?o Paulo University of Cambridge 
arnaldoc at icmc.usp.br Ann.Copestake at cl.cam.ac.uk 
Lucia Specia Sandra Maria Alu?sio
Research Group in Computational Linguistics Institute of Mathematics and Computer Sciences 
University of Wolverhampton University of S?o Paulo
l.specia at wlv.ac.uk sandra at icmc.usp.br
Abstract
The  Simple  English Wikipedia  provides  a 
simplified  version  of  Wikipedia's  English 
articles  for  readers  with  special  needs. 
However,  there are fewer efforts  to make 
information  in  Wikipedia  in  other 
languages  accessible  to  a  large  audience. 
This work proposes the use of a syntactic 
simplification  engine  with  high  precision 
rules  to  automatically  generate  a  Simple 
Portuguese Wikipedia on demand, based on 
user interactions with the main Portuguese 
Wikipedia.  Our  estimates  indicated that  a 
human  can  simplify  about  28,000 
occurrences  of  analysed  patterns  per 
million  words,  while  our  system  can 
correctly simplify 22,200 occurrences, with 
estimated f-measure 77.2%. 
1 Introduction
The Simple English Wikipedia1 is an effort to make 
information  in  Wikipedia2 accessible  for  less 
competent  readers  of  English  by  using  simple 
words and grammar.  Examples of  intended users 
include  children  and  readers  with  special  needs, 
such as users with learning disabilities and learners 
of English as a second language. 
Simple English (or Plain English), used in this 
version  of  Wikipedia,  is  a  result  from the  Plain 
English movement that occurred in Britain and the 
United States in the late 1970?s as a reaction to the 
unclear language used in government and business 
forms and documents. Some recommendations on 
how  to  write  and  organize  information  in  Plain 
1 http://simple.wikipedia.org/
2 http://www.wikipedia.org/
Language (the set of guidelines to write simplified 
texts) are related to both syntax and lexical levels: 
use short sentences; avoid hidden verbs; use active 
voice; use concrete, short, simple words. 
A number of resources, such as lists of common 
words3,  are available for the English language to 
help users write in Simple English. These include 
lexical  resources  like  the  MRC  Psycholinguistic 
Database4 which  helps  identify  difficult  words 
using  psycholinguistic  measures.  However, 
resources as such do not exist for Portuguese. An 
exception is a small list of simple words compiled 
as part  of  the  PorSimples project  (Aluisio et  al., 
2008).
Although  the  guidelines  from  the  Plain 
Language  can  in  principle  be  applied  for  many 
languages and text genres, for Portuguese there are 
very  few  efforts  using  Plain  Language  to  make 
information accessible to a large audience. To the 
best  of  our  knowledge,  the  solution  offered  by 
Portugues  Claro5 to  help  organizations  produce 
European  Portuguese  (EP)  documents  in  simple 
language is the only commercial option in such a 
direction.  For  Brazilian  Portuguese  (BP),  a 
Brazilian  Law (10098/2000)  tries  to  ensure  that 
content  in  e-Gov sites  and  services  is  written  in 
simple  and  direct  language  in  order  to  remove 
barriers in communication and to ensure citizens' 
rights  to  information  and communication  access. 
However,  as  it  has  been  shown  in  Martins  and 
Filgueiras  (2007),  content  in  such  websites  still 
needs  considerable  rewriting  to  follow the  Plain 
Language guidelines. 
A few efforts from the research community have 
recently  resulted  in  natural  language  processing 
3 http://simple.wiktionary.org/
4 http://www2.let.vu.nl/resources/elw/resource/mrc.html
5  http://www.portuguesclaro.pt/
137
systems to simplify and make Portuguese language 
clearer. ReEscreve (Barreiro and Cabral, 2009) is a 
multi-purpose  paraphraser that  helps  users  to 
simplify their EP texts by reducing its ambiguity, 
number  of  words  and  complexity.  The  current 
linguistic phenomena paraphrased are support verb 
constructions,  which  are  replaced  by  stylistic 
variants.  In  the  case  of  BP,  the  lack  of 
simplification  systems  led  to  development  of 
PorSimples project (Alu?sio and Gasperin, 2010). 
This  project  uses  simplification  in  different 
linguistic levels to provide simplified text to poor 
literacy readers.
For  English,  automatic  text  simplification  has 
been  exploited  for  helping  readers  with  poor 
literacy (Max, 2006) and readers with other special 
needs,  such  as  aphasic  people  (Devlin  and 
Unthank,  2006;  Carroll  et  al.  1999).  It  has  also 
been used in bilingual education (Petersen, 2007) 
and  for  improving  the  accuracy  of  Natural 
Language Processing (NLP) tasks (Klebanov et al, 
2004; Vickrey and Koller, 2008).
Given the general scarcity of human resources to 
manually simplify large content  repositories such 
as Wikipedia, simplifying texts automatically can 
be  the  only  feasible  option.  The  Portuguese 
Wikipedia,  for  example,  is  the  tenth  largest 
Wikipedia (as of May 2011), with 683,215 articles 
and approximately 860,242 contributors6. 
In  this  paper  we  propose  a  new  rule-based 
syntactic simplification system to create a Simple 
Portuguese Wikipedia  on demand,  based on user 
interactions with the main Portuguese Wikipedia. 
We use a simplification engine to change passive 
into active voice and to break down and change the 
syntax of subordinate clauses.  We focus on these 
operations  because  they  are  more  difficult  to 
process  by  readers  with  learning  disabilities  as 
compared  to  others  such  as  coordination  and 
complex noun phrases (Abedi et al, 2011; Jones et 
al.,  2006;  Chappell,  1985).  User  interaction with 
Wikipedia can be performed by a system like the 
Facilita7 (Watanabe et al, 2009), a browser plug-in 
developed  in  the  PorSimples  project  to  allow 
automatic adaptation (summarization and syntactic 
simplification) of any web page in BP.
This  paper  is  organized  as  follows.  Section  2 
presents related work on syntactic  simplification. 
6 http://meta.wikimedia.org/wiki/List_of_Wikipedias#
Grand Total
7 http://nilc.icmc.usp.br/porsimples/facilita/
Section  3 presents the methodology to build and 
evaluate the simplification engine for BP. Section 4 
presents  the  results  of  the  engine  evaluation. 
Section  5 presents  an  analysis  on  simplification 
issues  and  discusses  possible  improvements. 
Section 6 contains some final remarks.
2 Related work
Given the dependence of  syntactic  simplification 
on  linguistic  information,  successful  approaches 
are  mostly  based  on  rule-based  systems. 
Approaches using operations learned from corpus 
have  not  shown  to  be  able  to  perform  complex 
operations  such  the  splitting  of  sentences  with 
relative clauses (Chandrasekar and Srinivas, 1997; 
Daelemans  et  al.,  2004;  Specia,  2010).  On  the 
other hand. the use of machine learning techniques 
to predict when to simplify a sentence, i.e. learning 
the properties of language that distinguish simple 
from normal  texts,  has  achieved relative  success 
(Napoles and Dredze, 2010). Therefore, most work 
on syntactic simplification still relies on rule-based 
systems to simplify a set of syntactic constructions. 
This is also the approach we follow in this paper. 
In what follows we review some relevant and work 
on syntactic simplification.
The seminal work of Chandrasekar and Srinivas 
(1997) investigated the induction of syntactic rules 
from a corpus annotated with part-of-speech tags 
augmented  by  agreement  and  subcategorization 
information.  They  extracted  syntactic 
correspondences  and  generated  rules  aiming  to 
speed up parsing and improving its accuracy, but 
not  working  on  naturally  occurring  texts. 
Daelemans et  al.  (2004)  compared both machine 
learning  and  rule-based  approaches  for  the 
automatic generation of TV subtitles for hearing-
impaired  people.  In  their  machine  learning 
approach,  a  simplification model  is  learned from 
parallel  corpora  with  TV  programme  transcripts 
and the associated subtitles. Their method used a 
memory-based learner and features such as words, 
lemmas,  POS tags,  chunk tags,  relation tags  and 
proper  name  tags,  among  others  features  (30  in 
total). However, this approach did not perform as 
well  as  the  authors  expected,  making errors  like 
removing sentence subjects or deleting a part of a 
multi-word  unit.   More  recently,  Specia  (2010) 
presented a new approach for text simplification, 
based  on  the  framework  of  Statistical  Machine 
138
Translation. Although the results are promising for 
lexical simplification, syntactic rewriting was not 
captured  by  the  model  to  address  long-distance 
operations,  since  syntactic  information  was  not 
included into the framework.
Inui et al (2003) proposed a rule-based system 
for text simplification aimed at deaf people. Using 
about  one  thousand  manually  created  rules,  the 
authors  generate  several  paraphrases  for  each 
sentence and train a classifier to select the simpler 
ones.  Promising  results  were  obtained,  although 
different  types  of  errors  on  the  paraphrase 
generation are encountered, such as problems with 
verb conjugation and regency.  Our work aims at 
making  Portuguese  Wikipedia  information 
accessible  to  a  large  audience  and  instead  of 
generating  several  possible  outputs  we  generate 
only one based on rules taken from a manual of 
simplification for BP.
Siddharthan  (2006)  proposed  a  syntactic 
simplification  architecture  that  relies  on  shallow 
parsing. The general goal of the architecture is to 
make texts more accessible to a broader audience 
instead of targeting any particular application. The 
system  simplifies  apposition,  relative  clauses, 
coordination  and  subordination.  Our  method,  on 
the other hand, relies on deep parsing (Bick, 2000) 
and focuses  on  changing passive  to  active voice 
and  changing  the  syntax  of  relative  clauses  and 
subordinate sentences.
Max  (2006)  applied  text  simplification  in  the 
writing process by embedding the simplifier into a 
word  processor.  Although  this  system  ensures 
accurate  output,  it  requires  manual  choices.  The 
suggested simplifications are ranked by a score of 
syntactic  complexity  and  potential  change  of 
meaning.  The writer  then chooses their  preferred 
simplification.  Our  method,  on  the  other  hand, 
offers the user only one simplification since it uses 
several  rules  to  better  capture  each  complex 
phenomenon. 
Inspired  by  Siddharthan  (2006),  Jonnalagadda 
and  Gonzalez  (2009)  present  an  approach  to 
syntactic  simplification  addressing  also  the 
problem of accurately determining the grammatical 
correctness  of  the  simplified  sentences.  They 
propose  the  combination  of  the  number  of  null 
links  and  disjunct  cost  (the  level  of 
inappropriateness,  caused  by  using  less  frequent 
rules in the linkage) from the cost vector returned 
by a Link Grammar8 parser. Their motivation is to 
improve the performance of systems for extracting 
Protein-Protein  Interactions  automatically  from 
biomedical  articles  by  automatically  simplifying 
sentences.  Besides  treating  the  syntactic 
phenomena described in Siddharthan (2006), they 
remove  describing  phrases  occurring  at  the 
beginning  of  the  sentences,  like  ?These  results 
suggest that? and ?As reported previously?. While 
they  focus  on  the  scientific  genre,  our  work  is 
focused on the encyclopedic genre.
In order to obtain a text easier to understand by 
children,  De  Belder  and  Moens  (2010)  use  the 
Stanford parser9 to select the following phenomena 
to syntactically simplify the sentences: appositions, 
relative  clauses,  prefix  subordination  and  infix 
subordination  and  coordination.  After  sentence 
splitting, they try to apply the simplification rules 
again to both of the new sentences. However, they 
conclude that  with the set  of  simplification rules 
used,  it  was  not  possible  to  reduce  the  reading 
difficulty for children and foresee the use of other 
techniques for this purpose, such as summarization 
and elaborations for difficult words.
3 Simplification engine
3.1 Engine development
The  development  of  a  syntactic  simplification 
engine  for  a  specific  task  and  audience  can  be 
divided  into  five  distinct  phases:  (a)  target 
audience analysis; (b) review of complex syntactic 
phenomena for such an audience; (c) formulation 
of simplification guidelines; (d) refinement of rules 
based  on  evidence  from  corpora;  and  (e) 
programming and evaluation of rules.
In this paper we focus on the last two phases. 
We  use  the  simplification  guidelines  from  the 
PorSimples  project,  but  these  are  based  on 
grammar  studies  and  corpora  analysis  for  a 
different  text  genre  (news).  Therefore  additional 
corpora  evidence  proved  to  be  necessary.  This 
resulted  in  the  further  refinement  of  the  rules, 
covering  different  cases  for  each  syntactic 
phenomenon.
The Simplification engine relies on the output of 
the  Palavras  Parser  (Bick,  2000)  to  perform 
constituent tree transformations (for example, tree 
8 http://www.abisource.com/projects/link-grammar/
9 http://nlp.stanford.edu/software/lex-parser.shtml
139
splitting).  Each  node  of  a  sentence  tree  is  fed 
(breadth-first  order)  to  the  simplification 
algorithms,  which can simplify the node (and its 
sub-tree) or skip it when the node does not meet 
the simplification prerequisites. Breadth-first order 
is chosen because several operations affect the root 
of a (sub)tree, while none of them affect leaves.
A development  corpus containing examples  of 
cases analysed for each syntactic phenomenon is 
used  to  test  and  refine  the  rules.  The  current 
version of the corpus has 156 sentences extracted 
from news text. The corpus includes negative and 
positive  examples  for  each  rule.  Negative 
examples  should  not  be  simplified.  They  were 
inserted  into  the  corpus  to  avoid  unnecessary 
simplifications. Each rule is first tested against its 
own positive and negative examples.  This test  is 
called  local  test.  After reaching a good precision 
on the local test, the rule is then tested against all 
the  sentences  in  the  corpus,  global  test.  In  the 
current corpus, the global test identified sentences 
correctly   simplified by  at  least  one  rule  (66%), 
sentences incorrectly simplified due to major errors 
in  parsing/rules  (7%)  (ungrammatical  sentences) 
and  non-simplified  sentences  (27%).  The  last 
includes  mainly  negative  examples,  but  also 
includes  sentences  not  selected  due  to  parsing 
errors, sentences from cases not yet implemented, 
and sentences from cases ignored due to ambiguity.
3.2 Passive voice
The default case for dealing with passive voice in 
our simplification engine is illustrated by the pair 
of  original-simplified sentences  in  example10 (1). 
Sentences  belonging  to  this  case  have  a  non-
pronominal subject and a passive agent. Also, the 
predicator has two verbs, the verb  to be followed 
by  a  verb  in  the  past  participle  tense.  The 
simplification consists  in  reordering the sentence 
components,  turning  the  agent  into  subject 
(removing the  by preposition), turning the subject 
into direct  object and adjusting the predicator by 
removing the verb to be and re-inflecting the main 
verb. The new tense of the main verb is the same 
as  the  one  of  the  to  be  verb  and  its  number  is 
defined according to the new subject.
10 Literal translations from Portuguese result in some 
sentences appearing ungrammatical in English. 
O: As[The] transfer?ncias[transfers] 
foram[were:plural] feitas[made] pela[by the] 
empresa[company]. (1)
S: A[The] empresa[company] fez[made:sing] 
as[the] transfer?ncias[transfers].
Other correctly processed cases vary according 
the  number  of  verbs  (three  or  four),  special 
subjects, and special agents. For cases comprising 
three or four verbs, the simplification rule must re-
inflect11 two verbs (2) (one of them should agree 
with  the  subject  and  the  other  receives  its  tense 
from  the  verb  to  be).   There  are  two  cases  of 
special subjects. In the first case, a hidden subject 
is turned into a pronominal direct object (3). In the 
second  case,  a  pronominal  subject  must  be 
transformed to oblique case pronoun and then to 
direct  object.  Special  agents  also  represent  two 
cases. In the first one, oblique case pronouns must 
be  transformed before  turning  the  agent  into  the 
subject. In the second case (4), a non-existent agent 
is turned into an undetermined subject (represented 
here by ?they?).
O: A[The] porta[door] deveria[should] ter[have] 
sido[been] trancada[locked:fem] por[by] John. (2)S: John deveria[should] ter[have] 
trancado[locked:masc] a[the] porta[door].
O: [I] fui[was] encarregado[entrusted] por[by] 
minha[my] fam?lia[family]. (3)S: Minha[My] fam?lia[family] 
encarregou[entrusted] me[me].
O: O[The] ladr?o[thief] foi[was] pego[caught]. (4)
S: [They] pegaram[caught] o[the] ladr?o[thief].
Two cases  are  not  processed because they are 
already considered easy enough: the syndetic voice 
and passive in non-root sentences. In those cases, 
the  proposed  simplification  is  generally  less 
understandable  than  the  original  sentence. 
Sentences  with  split  predicator  (as  in  ?the 
politician was very criticized by his electors?) are 
not  processed  for  the  time  being,  but  should  be 
incorporated in the pipeline in the future.
Table  1 presents the algorithm used to process 
the  default  case  rule  and  verb  case  rules. 
Simplification rules are applied against all nodes in 
constituent tree, one node at a time, using breadth-
first traversing.
11 Some reinflections may not be visible on example 
translation.
140
Step Description
1 Validate these prerequisites or give up:
1.1     Node must be root
1.2     Predictor must have an inflection of auxiliary   
    verb to be
1.3     Main verb has to be in past participle
2 Transform subject into direct object
3 Fix the predicator
3.1 If main verb is finite then:
    main verb gets mode and tense from to be
    main verb gets person according to agent
3.2 Else:
    main verb gets mode and tense from verb to be
    finite verb gets person according to agent
3.3 Remove verb to be
4 Transform passive agent into a new subject
Table 1: Algorithm for default and verb cases
3.3 Subordination
Types of subordinate clauses are presented in Table 
2. Two clauses are not processed: comparative and 
proportional.  Comparative  and  proportional 
clauses will be addressed in future work.
id Clause type Processed
d Relative Restrictive ?
e Relative  Non-restrictive ?
f Reason ?
g Comparative  
h Concessive ?
i Conditional ?
j Result ?
k Confirmative ?
l Final Purpose ?
m Time ?
w Proportional
Table 2: Subordinate clauses
Specific  rules  are  used  for  groups  of  related 
subordinate cases. At least one of two operations 
can  be  found  in  all  rules:  component  reordering 
and sentence splitting. Below, letter codes are used 
to  describe  rules  involving  these  two  and  other 
common operations:
A additional processing
M splitting-order main-subordinate 
P Also processes non-clause phrases and/or non-
finite clauses
R component reordering 
S splitting-order subordinate-main 
c clone subject or turn object of a clause into 
subject in another if it is necessary
d marker deletion
m marker replacement
v verb reinflection
[na] not simplified due ambiguity
[nf] not simplified, future case
[np] not simplified due parsing problems
2...8 covered cases (when more than one applies)
Table  3 presents the marker information. They 
are used to select sentences for simplification, and 
several  of  them  are  replaced  by  easier  markers. 
Cases  themselves  are  not  detailed since they are 
too  numerous  (more  than  40  distinct  cases). 
Operation  codes  used  for  each  marker  are 
described in column ?Op?. It is important to notice 
that  multi-lexeme  markers  also  face  ambiguities 
due to co-occurrence of its component lexemes12. 
The  list  does  not  cover  all  possible  cases,  since 
there  may  be  additional  cases  not  seen  in  the 
corpus. As relative clauses (d and e) require almost 
the same processing, they are grouped together.
Several  clauses  require  additional  processing. 
For  example,  some  conditional  clauses  require 
negating the main clause. Other examples include 
noun phrases replacing clause markers and clause 
reordering, both for relative clauses, as showed in 
(5). The marker  cujo (whose) in the example can 
refer to Northbridge or to the building. Additional 
processing  is  performed  to  try  to  solve  this 
anaphora13,  mostly  using  number  agreement 
between the each possible co-referent and the main 
verb in the subordinate clause. The simplification 
engine can give up in ambiguous cases (focusing 
on  precision)  or  elect  a  coreferent  (focusing  on 
recall),  depending  on  the  number  of  possible 
coreferents  and  on  a  confidence  threshold 
parameter, which was not used in this paper.
O: Ele[He] deve[should] visitar[visit] o[the] 
pr?dio[building] em[in] Northbridge 
cujo[whose] desabamento[landslide] 
matou[killed] 16 pessoas[people].
(5)S: Ele[He] deve[should] visitar[visit] o[the] 
pr?dio[building] em[in] Northbridge. O[The] 
desabamento[landslide] do[of  the] 
pr?dio[building] em[in] Northbridge 
matou[killed] 16 pessoas[people].
12 For example, words ?de?, ?sorte? and ?que? can be 
adjacent  to each other without the meaning of ?de sorte 
que? marker (?so that?).
13 We opted to solve this kind of anaphora instead of using 
pronoun insertion in order to facilitate the reading of the 
text.
141
3.4 Evaluation in the development corpus
Figure 1 provides statistics from the of processing 
all  identified  cases  in  the  development  corpus. 
These statistics cover number of cases rather than 
the  number  of  sentences  containing  cases.  The 
cases  ?incorrect  selection?  and  ?incorrect 
simplification?  affect  precision  by  generating 
ungrammatical  sentences.  The  former  refers  to 
sentences  that  should  not  be  selected  for  the 
simplification  process,  while  the  latter  refers  to 
sentences  correctly  selected  but  wrongly 
simplified.  There  are  three  categories  affecting 
recall, classified according to their priority in the 
simplification  engine.  Pending cases  are 
considered  to  be  representative,  with  higher 
priority.  Possible cases  are  considered  to  be 
unrepresentative. Having less priority, they can be 
handled in future versions of the engine.  Finally, 
Skipped cases  will  not  be  implemented,  mainly 
because  of  ambiguity,  but  also  due  to  low 
representativeness.  It  is  possible  to  observe  that 
categories  reducing  precision  (incorrect  selection 
and simplification) represent a smaller number of 
cases (5%) than categories reducing recall (45%). 
It  is  worth  noticing  that  our  approach  focus  on 
precision  in  order  to  make  the  simplification  as 
automatic  as  possible,  minimizing  the  need  for 
human interaction.
Figure 1: Performance on the development 
corpus
There are some important remarks regarding the 
development corpus used during the programming 
phase.  First,  some  cases  are  not  representative, 
therefore  the  results  are  expected  to  vary 
significantly in real texts. Second, a few cases are 
not orthogonal: i.e.,  there are sentences that can be 
classified  in  more  than  one  case.  Third,  several 
errors  refer  to  sub-cases  of  cases  being  mostly 
correctly  processed,  which are  expected to occur 
less frequently. Fourth, incorrect parsed sentences 
were not take in account in this phase. Although 
there may exist other cases not identified yet, it is 
plausible to estimate that only 5% of known cases 
are affecting the precision negatively.
id Marker Op id Marker Op id Markers Op
de que [that/which] 8MRAdv h se bem que [albeit] Mmv j tanto ? que [so ? that] [nf]
de o qual [which]* 8MRAdv h ainda que [even if] 2Mm j tal ? que [such ? that] [nf]
de como [as] [na] h mesmo que [even if] 2Mm j tamanho ? que [so ? that]* [nf]
de onde [where] [nf] h nem que [even if] 2Mm k conforme [as/according] 3PRAcm
de quando [when] [na] h por mais que [whatever] 2Mm k consoante [as/according] 3PRAcm
de quem [who/whom] [nf] h mas [but] [np] k segundo [as/according] 3PRAcm
de quanto [how much] [nf] i contanto que [provided that] 2Rmv k como [as] [na]
de cujo [whose]* MAd i caso [case] 2Rmv l a fim de [in order to] 2PMcm
de o que [what/which] Sd i se [if/whether] 2Rmv l a fim de que [in order that] 2PMcm
f j? que [since] Scm i a menos que [unless] 2RAmv l para que [so that] 2PMcm
f porquanto [in view of] Scm i a n?o ser que [unless] 2RAmv l porque [because] [na]
f uma vez que [since] Scm i exceto se [unless] 2RAmv m assim que [as soon as] 5PMAcvr
f visto que [since] Scm i salvo se [unless] 2RAmv m depois de [after] 5PMAcvr
f como [for] [na] i antes que [before] Rmv m depois que [after] 5PMAcvr
f porque [because] [na] i sem que [without] Rmv m logo que [once] 5PMAcvr
f posto que [since] [na] i desde que [since] RAmv m antes que [before] PSAcvr
f visto como [seen as] [na] j de forma que [so] 5Mmv m apenas [only] [na]
f pois que [since] [nf] j de modo que [so] 5Mmv m at? que [until] [na]
h apesar de que [although] Mmv j de sorte que [so that] 5Mmv m desde que [since] [na]
h apesar que [despite] Mmv j tamanho que [so that]* 5Mmv m cada vez que [every time] [nf]
h conquanto [although] Mmv j tal que [such that] 5Mmv m sempre que [whenever] [nf]
h embora [albeit] Mmv j tanto que [so that] (1)* [na] m enquanto [while] [nf]
h posto que [since] Mmv j tanto que [so that] (2) [na] m mal [just] [na]
h por muito que [although] Mmv j t?o ? que [so ? that] [nf] m quando [when] [na]
* gender and/or number variation
Table 3: Marker processing
correct 45%
incorrect
simplification 4% incorrectselection 1%
pending 17%
possible 7%
skipped 25%
142
4 Engine evaluation
4.1 Evaluation patterns
The  evaluation  was  performed  on  a  sample  of 
sentences  extracted  from Wikipedia's  texts  using 
lexical patterns. These patterns allows to filter the 
texts,  extracting  only  relevant  sentences  for 
precision and recall evaluation. They were created 
to  cover  both  positive  and  negative  sentences. 
They are applied before parsing or Part of Speech 
(PoS)  analysis.  For  passive  voice  detection,  the 
pattern is  defined as a sequence of  two or  more 
possible verbs (no PoS in use) in which at least one 
of them could be an inflection of verb to be. For 
subordination detection, the pattern is equivalent to 
the  discourse  markers  associated  with  each 
subordination type, as shown in Table 3. 
The  patterns  were  applied  against  featured 
articles  appearing  in  Wikipedia's  front  page  in 
2010 and 2011, including featured articles planned 
to be featured, but not featured yet. A maximum of 
30 sentences resulting from each pattern matching 
were then submitted to the simplification engine. 
Table 4 presents statistics from featured articles. 
texts 165
sentences 83,656
words 1,226,880
applied patterns 57,735
matched sentences 31,080
Table 4: Wikipedia's featured articles (2010/2011)
The number of applied patterns represents both 
patterns to be simplified (s-patterns) and patterns 
not  to  be  simplified  (n-patterns).  N-patterns 
represent both non-processable patterns due to high 
ambiguity (a-patterns) and pattern extraction false 
negatives. We observed a few, but very frequent, 
ambiguous patterns introducing noise, particularly 
se and  como.  In  fact,  these  two  markers  are  so 
noisy that  we were not  be able  to  provide good 
estimations  on  their  true  positives  distribution 
given the 30 sentences limit per pattern. Similarly 
to the number of applied patterns, the number of 
matched sentences correspond to both sentences to 
be simplified and not to be simplified.
Table  5 presents  additional  statistics  about 
characters,  words  and  sentences  calculated  in  a 
sample of 32 articles where the 12 domains of the 
Portuguese Wikipedia are balanced. The number of 
automatic simplified sentence is also presented. In 
Table  5,  simple  words refers  to  percentage  of 
words  which  are  listed  on  our  simple  word  list, 
supposed to be common to youngsters,  extracted 
from the dictionary described in (Biderman, 2005), 
containing 5,900 entries.  Figure 2 presents clause 
distribution per sentence in  the balanced sample. 
Zero  clauses refers  to  titles,  references,  figure 
labels, and other pieces of text without a verb. We 
observed  60%  of  multi-clause  sentences  in  the 
sample.
characters per word 5.22
words per sentence 21.17
words per text 8,476
simple words 75.52%
sentences per text 400.34
passive voice 15.11%
total sentences 13,091
simplified sentences 16,71%
Table 5: Statistics from the balanced text sample
Figure 2: Clauses per sentence in the sample
4.2 Simplification analysis
We manually analysed and annotated all sentences 
in  our  samples.  These  samples  were  used  to 
estimate  several statistics, including the number of 
patterns  per  million  words,  the  system precision 
and  recall  and  the  noise  rate.  We  opted  for 
analysing  simplified  patterns  per  million  words 
instead of  per simplified sentences. First, because 
an analysis based on sentences can be misleading, 
since  there  are  cases  of  long  coordinations  with 
many patterns, as well as succinct sentences with 
no patterns.  Moreover,  one incorrectly  simplified 
marker in a sentence could hide useful statistics of 
correctly  simplified  patterns  and  even  of  other 
incorrectly simplified patterns. 
The samples are composed by s-patterns and n-
patterns  (including  a-patterns).  In  total  1,243 
patterns were annotated.  Table  6 presents pattern 
estimates per million words. 
0 1 2 3 4 5 6 7 8
0,0000
0,0500
0,1000
0,1500
0,2000
0,2500
0,3000
clauses
dis
trib
uti
on
 [0
-1]
>7
143
Total patterns 70,834
Human s-patterns 33,906
Selection s-patterns 27,714
Perfect parser s-patterns 23,969
Obtained s-patterns 22,222
Table 6: Patterns per million words
Total patterns refers to the expected occurrences 
of  s-patterns  and  n-patterns  in  a  corpus  of  one 
million  words.  This  is  the  only  information 
extracted from the full corpus, while the remaining 
figures are estimates from the sample corpus.
Human s-patterns is an estimate of the number 
patterns that a human could simplify in the corpus. 
Unlike  other  s-pattern  estimates,  a-patterns  are 
included, since a human can disambiguate them. In 
other words, this is the total of positive patterns. 
The estimate  does  not  include very rare  (sample 
size equals to zero) or very noisy markers (patterns 
presenting 30 noisy sentences in its sample).
Selection  s-patterns are  an  estimate  of  the 
number  of  patterns  correctly  selected  for 
simplification,  regardless  of  whether  the  pattern 
simplification is correct or incorrect.  Precision and 
recall derived from this measure (Table 7) consider 
incorrectly simplified patterns,  and do not include 
patterns with parsing problems.  Its  purpose is  to 
evaluate how well the selection for simplification 
is performed. Rare or noisy patterns, whose human 
s-patterns  per  sample  is  lower  than  7,  are  not 
included.
Perfect  parser  s-patterns is  an  estimate  very 
similar to selection s-patterns, but considering only 
correctly  simplified  patterns.  As  in  selection  s-
patterns,  incorrect  parsed  sentences  are  not 
included in calculations. This is useful to analyse 
incorrect simplifications due to simplification rule 
problems, ignoring errors originating from parsing.
Finally,  obtained  s-patterns refers  to  the 
estimate of correct simplified patterns,  similar to 
perfect  parser  s-patterns,  but  including 
simplification  problems  caused  by  parsing.  This 
estimate  represents  the  real  performance  to  be 
expected from the system on Wikipedia's texts.
It is important to note that the real numbers of 
selection  s-patterns,  perfect  s-patterns  and 
obtained s-patterns  is expected to be bigger than 
the estimates,  since noisy and rare  pattern could 
not used be used in calculations (due the threshold 
of  7  human  s-patterns  per  sample).  The  data 
presented on Table 6 is calculated using estimated 
local precisions for each pattern. Table  7 presents 
global  precision,  recall  and  f-measure  related  to 
selection,  perfect  parser  and  obtained s-patterns. 
The  real  values  of  the  estimates  are  expected to 
variate up to +/- 2.48% .
Measures Precision Recall F-measure
Selection 99.05% 82.24% 89.86%
Perfect parser 85.66% 82.24% 83.92%
Obtained 79.42% 75.09% 77.20%
Table 7: Global estimated measures
Although the precision of the selection seems to 
be  impressive,  this  result  is  expected,  since  our 
approach  focus  on  the  processing  of  mostly 
unambiguous  markers,  with  sufficient  syntactic 
information. It is also due to the the threshold of 7 
human s-patterns  and the fact  that  a-patterns  are 
not  included.  Due to  these two restrictions,  only 
approximately 31.5% of unique patterns could be 
used for the calculations in Table  7. Interestingly, 
these unique patterns correspond to 82.5% of the 
total estimated human s-patterns. The majority of 
the 17.5%  remaining s-patterns refers to patterns 
too  noisy  to  be  analysed  and  to  a-patterns  (not 
processed  due  ambiguity),  and  also  others  n-
patterns which presented a low representativeness 
in  the  corpus.  The  results  indicate  good 
performance in rule formulation, covering the most 
important (and non-ambiguous) markers, which is 
also confirmed by the ratio between both selection 
s-patterns  and  human  s-patterns  previously 
presented on Table 6. 
An  alternative  analysis,  including  a-patterns, 
lowers recall and f-measure, but not precision (our 
focus in this work). In this case, recall drops from 
75.09% to  62.18%,  while  f-measure  drops  from 
77.20% to 70.18%.
Figure 3: Pattern distribution
Figure  3 presents  the  distribution  of  patterns 
according to their frequency per million words and 
their purity (1 - noisy rate). This data is useful to 
2,0 20,0 200,0 2000,0 20000,0 200000,0
0,0000
0,2000
0,4000
0,6000
0,8000
1,0000
1,2000
b-passiva
de-que
l-a_fim_de
j-tal_que
J-tanto_*_que
Frequency PMW
Pu
rity
144
identify  most  frequent  patterns  (such  as  passive 
voice in  b-passiva)  and patterns with medium to 
high  frequency,  which  are  easy  to  process  (not 
ambiguous), such as l-a_fim_de.
5 Issues on simplification quality
This analysis aims at identifying factors affecting 
the quality of simplifications considered as correct. 
Hence, factors affecting the overall simplified text 
quality  are  also  presented.  In  contrast,  the 
quantitative  analysis  presented  on  Section  4.2 
covered  the  ratio  between  incorrect  and  correct 
simplifications.
Three cases of clause disposition were identified 
as  important  factors  affecting  the  simplified 
sentence  readability.  These  cases  are  presented 
using  the  following  notation:  clauses  are 
represented  in  uppercase  letters;  clause 
concatenation represents coordination; parentheses 
represent  subordination;  c1 and  c2 represent 
clause/sentence  connectors  (including  markers); 
the  entailment  operator  (?)  represents  the 
simplification rule transforming clauses.
? ?A(B(c1 C)) ? A(B). c2 C?: the vertical case. 
In this scenario it is more natural to read c2 as 
connecting  C to the main clause  A, while  c1 
connects  C to  B,  as seen in (6). This is still 
acceptable for several sentences analysed, but 
we  are  considering to  simplify only level  2 
clauses in the future, splitting C  from B only 
if another rule splits A and B first.
? ?A(B)CD  ?  ACD.  c1 B?:  the  horizontal 
case. In this scenario, c1 correctly connects A 
and B, but long coordinations following A can 
impact  negatively on text  reading,  since the 
target  audience  may  forget  about  A when 
starting  to  read  B.  In  this  scenario, 
coordination  compromise  subordination 
simplification,  showing  the  importance  of 
simplifying coordination as well, even though 
they  are  considered  easier  to  read  than 
subordination.
? Mixed  case:  this  scenario  combines  the 
potential problems of horizontal and vertical 
cases.  It  may  occur  in  extremely  long 
sentences.
Besides  clause  disposition  factors,  clause 
inversions can also lead to  problems in sentence 
readability.  In  our  current  system,  inversion  is 
mainly used to produce simplified sentences in the 
cause-effect  order  or  condition-action  order. 
Reordering, despite using more natural orders, can 
transform  anaphors  into  cataphors.  A  good 
anaphora resolution system would be necessary to 
avoid  this  issue.  Another  problem  is  moving 
sentence connectors as in ?A. c1 BC. ? A. B. c2 c1 
C?,  while  ?A.  c1 B.  c2 C?  is  more  natural 
(maintaining c1 position). 
O: Ela[She] dissertou[talked] sobre[about] 
como[how] motivar[to motive] o[the] 
grupo[group] de_modo_que[so that] seu[their] 
desempenho[performance] melhore[improves] (6)S: [He/She] dissertou[talked] sobre[about] 
como[how] motivar[to motive] o[the] 
grupo[group]. Thus, seu[their] 
desempenho[performance] melhore[improves]
We  have  observed  some  errors  in  sentence 
parsing,  related  to  clause  attachment,  generating 
truncated ungrammatical text. As a result, a badly 
simplified key sentence can compromise the text 
readability more than several  correctly simplified 
sentences  can  improve  it,  reinforcing  the 
importance  of  precision  rather  than  recall  in 
automated text simplification.
Experienced  readers  analysed  the  simplified 
versions of the articles and considered them easier 
to read than the original ones in most cases, despite 
simplification  errors.  Particularly,  the  readers 
considered  that  the  readability  would  improve 
significantly  if  cataphor  and horizontal  problems 
were  addressed.  Evaluating  the  simplifications 
with readers from the target audience is left  as a 
future work, after  improvements in the identified 
issues.
6 Conclusions
We  have  presented  a  simplification  engine  to 
process texts from the Portuguese Wikipedia. Our 
quantitative  analysis  indicated  a  good  precision 
(79.42%),  and  reasonable  number  of  correct 
simplifications  per  million  words  (22,222). 
Although our focus was on the encyclopedic genre 
evaluation,  the  proposed  system  can  be  used  in 
other genres as well.
Acknowledgements
We thank FAPESP (p.  2008/08963-4)  and CNPq 
(p. 201407/2010-8) for supporting this work.
145
References
J.  Abedi,  S.  Leon,  J.  Kao,  R.  Bayley,  N.  Ewers,  J. 
Herman and K. Mundhenk. 2011. Accessible Reading 
Assessments for Students with Disabilities: The Role 
of  Cognitive,  Grammatical,  Lexical,  and 
Textual/Visual  Features.  CRESST  Report  785. 
National  Center  for  Research  on  Evaluation, 
Standards,  and  Student  Testing,  University  of 
California, Los Angeles.
S.  M.  Alu?sio,   C.  Gasperin.  2010.  Fostering  Digital 
Inclusion and Accessibility: The PorSimples project 
for Simplification of Portuguese Texts.  Proceedings 
of  the  NAACL  HLT  2010  Young  Investigators 
Workshop  on  Computational  Approaches  to 
Languages of the Americas. : ACL, New York, USA. 
v. 1. p. 46-53.
A.  Barreiro,  L.  M.  Cabral.  2009.  ReEscreve:  a 
translator-friendly  multi-purpose  paraphrasing 
software  tool.  The  Proceedings  of  the  Workshop 
Beyond  Translation  Memories:  New  Tools  for 
Translators,  The  Twelfth  Machine  Translation 
Summit. Ontario, Canada, pp. 1-8. 
E.  Bick.  2006.   The  parsing  system  ?Palavras?: 
Automatic grammatical  analysis of  Portuguese in a 
constraint  grammar  framework.  Thesis  (PhD). 
University of ?rhus, Aarhus, Denmark.
M.  T.  C.  Biderman.   2005.  Dicion?rio  Ilustrado  de 
Portugu?s. Editora ?tica. 1a. ed. S?o Paulo 
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin 
and  J.  Tait.  1999.  Simplifying  Text  for  Language-
Impaired  Readers,.  In  Proceedings  of  the  9th 
Conference  of  the  European  Chapter  of  the 
Association for Computational  Linguistics  (EACL), 
269-270.
R.  Chandrasekar  and  B.  Srinivas.  1997.  Automatic 
Induction  of  Rules  for  Text  Simplification. 
Knowledge-Based Systems, 10, 183-190.
G. E. Chappell.  1985. Description and assessment of 
language disabilities of junior high school students. 
In:  Communication  skills  and  classroom  success: 
Assessment  of  language-learning  disabled  students. 
College- Hill Press,  San Diego,  pp. 207-239.
W.  Daelemans,  A.  Hothker  and  E.  T.  K.  Sang.  2004. 
Automatic Sentence Simplification for Subtitling in 
Dutch  and  English.  In:  Proceedings  of  the  4th 
Conference on Language Resources and Evaluation, 
Lisbon, Portugal 1045-1048.
J. De Belder and M. Moens. 2010.  Text simplification 
for children. Proceedings of the SIGIR Workshop on 
Accessible Search Systems, pp.19-26.
S.  Devlin  and  G.  Unthank.  2006.  Helping  aphasic 
people process online information. In: Proceedings of 
the  ACM  SIGACCESS  2006,  Conference  on 
Computers  and  Accessibility.  Portland,  Oregon, 
USA , 225-226.
K. Inui, A. Fujita, T. Takahashi, R. Iida and T. Iwakura. 
2003.  Text Simplification for Reading Assistance: A 
Project  Note.  In  the  Proceedings  of  the  Second 
International Workshop on Paraphrasing, 9-16.
S.  Jonnalagadda  and  G.  Gonzalez.  2009.  Sentence 
Simplification  Aids  Protein-Protein  Interaction 
Extraction.  Proceedings  of  the  3rd  International 
Symposium on Languages in Biology and Medicine, 
Short  Papers,  pages  109-114,  Jeju  Island,  South 
Korea, 8-10 November 2009.
F.  W.  Jones,  K.  Long  and  W.  M.  L.  Finlay.  2006. 
Assessing the reading comprehension of adults with 
learning disabilities. Journal of Intellectual Disability 
Research, 50(6), 410-418. 
B.  Klebanov,  K.  Knight  and  D.  Marcu.  2004.  Text 
Simplification for Information-Seeking Applications. 
In:  On  the  Move  to  Meaningful  Internet  Systems. 
Volume  3290,  Springer-Verlag,  Berlin  Heidelberg 
New York, 735-747.
S. Martins, L. Filgueiras. 2007. M?todos de Avalia??o 
de Apreensibilidade das Informa??es Textuais:  uma 
Aplica??o  em  S?tios  de  Governo  Eletr?nico.  In 
proceeding  of  Latin  American  Conference  on 
Human-Computer Interaction (CLIHC 2007). Rio de 
Janeiro, Brazil.
A. Max. 2006. Writing for Language-impaired Readers. 
In: Proceedings of Seventh International Conference 
on  Intelligent  Text  Processing  and  Computational 
Linguistics. Mexico City, Mexico. Berlin Heidelberg 
New York, Springer-Verlag, 567-570.
C.  Napoles  and  M.  Dredze.  2010.  Learning  simple 
Wikipedia:  a  cogitation  in  ascertaining  abecedarian 
language. In the  Proceedings of the NAACL HLT 
2010  Workshop  on  Computational  Linguistics  and 
Writing:  Writing  Processes  and  Authoring  Aids 
(CL&W '10), 42-50.
S.  E.  Petersen.  2007.  Natural  Language  Processing 
Tools  for  Reading  Level  Assessment  and  Text 
Simplification for  Bilingual  Education.  PhD thesis. 
University of Washington.
A. Siddharthan. 2006. Syntactic simplification and text 
cohesion.  Research  on  Language  &  Computation, 
4(1):77-109.
L.  Specia.   2010.  Translating  from  Complex  to 
Simplified  Sentences.  9th  International  Conference 
146
on  Computational  Processing  of  the  Portuguese 
Language.  Lecture  Notes  in  Artificial  Intelligence, 
Vol. 6001, Springer, pp. 30-39.
D. Vickrey and D. Koller. 2008. Sentence Simplification 
for Semantic Role Labelling. In: Proceedings of the 
ACL-HLT. 344-352.
W. M. Watanabe,  A. Candido Jr, V. R. Uzeda, R. P. M. 
Fortes,  T.  A.  S.  Pardo  and  S.  M.  Alu?sio.  2009. 
Facilita:  Reading  Assistance  for  Low-literacy 
Readers.  In:  ACM  International  Conference  on 
Design of Communication (SIGDOC 2009), volume 
1, Bloomington, US,   29-36. 
147
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 10?51,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Findings of the 2012 Workshop on Statistical Machine Translation
Chris Callison-Burch
Johns Hopkins University
Philipp Koehn
University of Edinburgh
Christof Monz
University of Amsterdam
Matt Post
Johns Hopkins University
Radu Soricut
SDL Language Weaver
Lucia Specia
University of Sheffield
Abstract
This paper presents the results of the WMT12
shared tasks, which included a translation
task, a task for machine translation evaluation
metrics, and a task for run-time estimation of
machine translation quality. We conducted a
large-scale manual evaluation of 103 machine
translation systems submitted by 34 teams.
We used the ranking of these systems to mea-
sure how strongly automatic metrics correlate
with human judgments of translation quality
for 12 evaluation metrics. We introduced a
new quality estimation task this year, and eval-
uated submissions from 11 teams.
1 Introduction
This paper presents the results of the shared tasks
of the Workshop on statistical Machine Translation
(WMT), which was held at NAACL 2012. This
workshop builds on six previous WMT workshops
(Koehn and Monz, 2006; Callison-Burch et al,
2007; Callison-Burch et al, 2008; Callison-Burch
et al, 2009; Callison-Burch et al, 2010; Callison-
Burch et al, 2011). In the past, the workshops have
featured a number of shared tasks: a translation task
between English and other languages, a task for au-
tomatic evaluation metrics to predict human judg-
ments of translation quality, and a system combina-
tion task to get better translation quality by combin-
ing the outputs of multiple translation systems. This
year we discontinued the system combination task,
and introduced a new task in its place:
? Quality estimation task ? Structured predic-
tion tasks like MT are difficult, but the dif-
ficulty is not uniform across all input types.
It would thus be useful to have some mea-
sure of confidence in the quality of the output,
which has potential usefulness in a range of set-
tings, such as deciding whether output needs
human post-editing or selecting the best trans-
lation from outputs from a number of systems.
This shared task focused on sentence-level es-
timation, and challenged participants to rate
the quality of sentences produced by a stan-
dard Moses translation system on an English-
Spanish news corpus in one of two tasks:
ranking and scoring. Predictions were scored
against a blind test set manually annotated with
relevant quality judgments.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dissem-
inate common test sets and public training data with
published performance numbers, and to refine eval-
uation methodologies for machine translation. As
with previous workshops, all of the data, transla-
tions, and collected human judgments are publicly
available.1 We hope these datasets form a valuable
resource for research into statistical machine transla-
tion, system combination, and automatic evaluation
or automatic prediction of translation quality.
2 Overview of the Shared Translation Task
The recurring task of the workshop examines trans-
lation between English and four other languages:
German, Spanish, French, and Czech. We created a
1http://statmt.org/wmt12/results.html
10
test set for each language pair by translating newspa-
per articles. We additionally provided training data
and two baseline systems.
2.1 Test data
The test data for this year?s task was created by hir-
ing people to translate news articles that were drawn
from a variety of sources from November 15, 2011.
A total of 99 articles were selected, in roughly equal
amounts from a variety of Czech, English, French,
German, and Spanish news sites:2
Czech: Blesk (1), CTK (1), E15 (1), den??k (4),
iDNES.cz (3), iHNed.cz (3), Ukacko (2),
Zheny (1)
French: Canoe (3), Croix (3), Le Devoir (3), Les
Echos (3), Equipe (2), Le Figaro (3), Libera-
tion (3)
Spanish: ABC.es (4), Milenio (4), Noroeste (4),
Nacion (3), El Pais (3), El Periodico (3), Prensa
Libre (3), El Universal (4)
English: CNN (3), Fox News (2), Los Angeles
Times (3), New York Times (3), Newsweek (1),
Time (3), Washington Post (3)
German: Berliner Kurier (1), FAZ (3), Giessener
Allgemeine (2), Morgenpost (3), Spiegel (3),
Welt (3)
The translations were created by the professional
translation agency CEET.3 All of the translations
were done directly, and not via an intermediate lan-
guage.
Although the translations were done profession-
ally, we observed a number of errors. These errors
ranged from minor typographical mistakes (I was
terrible. . . instead of It was terrible. . . ) to more
serious errors of incorrect verb choices and nonsen-
sical constructions. An example of the latter is the
French sentence (translated from German):
Il a gratte? une planche de be?ton, perdit des
pie`ces du ve?hicule.
(He scraped against a concrete crash bar-
rier and lost parts of the car.)
2For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
3http://www.ceet.eu/
Here, the French verb gratter is incorrect, and the
phrase planche de be?ton does not make any sense.
We did not quantify errors, but collected a number
of examples during the course of the manual evalua-
tion. These errors were present in the data available
to all the systems and therefore did not bias the re-
sults, but we suggest that next year a manual review
of the professionally-collected translations be taken
prior to releasing the data in order to correct mis-
takes and provide feedback to the translation agency.
2.2 Training data
As in past years we provided parallel corpora to train
translation models, monolingual corpora to train lan-
guage models, and development sets to tune system
parameters. Some statistics about the training mate-
rials are given in Figure 1.
2.3 Submitted systems
We received submissions from 34 groups across 18
institutions. The participants are listed in Table 1.
We also included two commercial off-the-shelf MT
systems, three online statistical MT systems, and
three online rule-based MT systems. Not all systems
supported all language pairs. We note that the eight
companies that developed these systems did not sub-
mit entries themselves, but were instead gathered by
translating the test data via their interfaces (web or
PC).4 They are therefore anonymized in this paper.
The data used to construct these systems is not sub-
ject to the same constraints as the shared task partic-
ipants. It is possible that part of the reference trans-
lations that were taken from online news sites could
have been included in the systems? models, for in-
stance. We therefore categorize all commercial sys-
tems as unconstrained when evaluating the results.
3 Human Evaluation
As with past workshops, we placed greater empha-
sis on the human evaluation than on the automatic
evaluation metric scores. It is our contention that
automatic measures are an imperfect substitute for
human assessment of translation quality. Therefore,
we define the manual evaluation to be primary, and
4We would like to thank Ondr?ej Bojar for harvesting the
commercial entries, Christian Federmann for the statistical MT
entries, and Herve? Saint-Amand for the rule-based MT entries.
11
Europarl Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,965,734 2,007,723 1,920,209 646,605
Words 56,895,229 54,420,026 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 176,258 117,481 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 157,302 137,097 158,840 136,151
Words 4,449,786 3,903,339 3,915,218 3,403,043 3,950,394 3,856,795 2,938,308 3,264,812
Distinct words 78,383 57,711 63,805 53,978 130,026 57,464 136,392 52,488
United Nations Training Corpus
Spanish? English French? English
Sentences 11,196,913 12,886,831
Words 318,788,686 365,127,098 411,916,781 360,341,450
Distinct words 593,567 581,339 565,553 666,077
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,218,201 2,123,835 2,190,579 2,176,537 668,595
Words 59,848,044 60,476,282 63,439,791 53,534,167 14,946,399
Distinct words 123,059 181,837 145,496 394,781 172,461
News Language Model Data
English Spanish French German Czech
Sentence 51,827,706 8,627,438 16,708,622 30,663,107 18,931,106
Words 1,249,883,955 247,722,726 410,581,568 576,833,910 315,167,472
Distinct words 2,265,254 926,999 1,267,582 3,336,078 2,304,933
News Test Set
English Spanish French German Czech
Sentences 3003
Words 73,785 78,965 81,478 73,433 65,501
Distinct words 9,881 12,137 11,441 14,252 17,149
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of
distinct words (case-insensitive) is based on the provided tokenizer.
12
ID Participant
CMU Carnegie Mellon University (Denkowski et al, 2012)
CU-BOJAR Charles University - Bojar (Bojar et al, 2012)
CU-DEPFIX Charles University - DEPFIX (Rosa et al, 2012)
CU-POOR-COMB Charles University - Bojar (Bojar et al, 2012)
CU-TAMCH Charles University - Tamchyna (Tamchyna et al, 2012)
CU-TECTOMT Charles University - TectoMT (Dus?ek et al, 2012)
DFKI-BERLIN German Research Center for Artificial Intelligence (Vilar, 2012)
DFKI-HUNSICKER German Research Center for Artificial Intelligence - Hunsicker (Hunsicker et al, 2012)
GTH-UPM Technical University of Madrid (Lo?pez-Luden?a et al, 2012)
ITS-LATL Language Technology Laboratory @ University of Geneva (Wehrli et al, 2009)
JHU Johns Hopkins University (Ganitkevitch et al, 2012)
KIT Karlsruhe Institute of Technology (Niehues et al, 2012)
LIMSI LIMSI (Le et al, 2012)
LIUM University of Le Mans (Servan et al, 2012)
PROMT ProMT (Molchanov, 2012)
QCRI Qatar Computing Research Institute (Guzman et al, 2012)
QUAERO The QUAERO Project (Markus et al, 2012)
RWTH RWTH Aachen (Huck et al, 2012)
SFU Simon Fraser University (Razmara et al, 2012)
UEDIN-WILLIAMS University of Edinburgh - Williams (Williams and Koehn, 2012)
UEDIN University of Edinburgh (Koehn and Haddow, 2012)
UG University of Toronto (Germann, 2012)
UK Charles University - Zeman (Zeman, 2012)
UPC Technical University of Catalonia (Formiga et al, 2012)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C] Three online statistical machine translation systems
RBMT-[1,3,4] Three rule-based statistical machine translation systems
Table 1: Participants in the shared translation task. Not all teams participated in all language pairs. The translations
from the commercial, online, and rule-based systems were crawled by us, not submitted by the respective companies,
and are therefore anonymized. Anonymized identifiers were chosen so as to correspond with the WMT11 systems.
13
Language Pair Num Label Labels per
Systems Count System
Czech-English 6 6,470 1,078.3
English-Czech 13 11,540 887.6
German-English 16 7,135 445.9
English-German 15 8,760 584.0
Spanish-English 12 5,705 475.4
English-Spanish 11 7,375 670.4
French-English 15 6,975 465.0
English-French 15 7,735 515.6
Overall 103 61,695 598
Table 2: A summary of the WMT12 ranking task, show-
ing the number of systems and number of labels (rank-
ings) collected for each of the language translation tasks.
use the human judgments to validate automatic met-
rics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct on the scale of our
workshop. We distributed the workload across a
number of people, beginning with shared-task par-
ticipants and interested volunteers. This year, we
also opened up the evaluation to non-expert anno-
tators hired on Amazon Mechanical Turk (Callison-
Burch, 2009). To ensure that the Turkers provided
high quality annotations, we used controls con-
structed from the machine translation ranking tasks
from prior years. Control items were selected such
that there was high agreement across the system de-
velopers who completed that item. In all, there were
229 people who participated in the manual evalua-
tion, with 91 workers putting in more than an hour?s
worth of effort, and 21 putting in more than four
hours. After filtering Turker rankings against the
controls to discard Turkers who fell below a thresh-
old level of agreement on the control questions,
there was a collective total of 336 hours of usable
labor. This is similar to the total of 361 hours of
labor collected for WMT11.
We asked annotators to evaluate system outputs
by ranking translated sentences relative to each
other. This was our official determinant of trans-
lation quality. The total number of judgments col-
lected for each of the language pairs is given in Ta-
ble 2.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the instruc-
tions simple:
You are shown a source sentence followed
by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions, and asked to rank them.
We refer to each of these as ranking tasks or some-
times blocks.
Every language task had more than five partici-
pating systems ? up to a maximum of 16 for the
German-English task. Rather than attempting to get
a complete ordering over the systems in each rank-
ing task, we instead relied on random selection and
a reasonably large sample size to make the compar-
isons fair.
We use the collected rank labels to assign each
system a score that reflects how highly that system
was usually ranked by the annotators. The score for
some systemA reflects how frequently it was judged
to be better than other systems. Specifically, each
block in whichA appears includes four implicit pair-
wise comparisons (against the other presented sys-
tems). A is rewarded once for each of the four com-
parisons in which A wins, and its score is the num-
ber of such winning pairwise comparisons, divided
by the total number of non-tying pairwise compar-
isons involving A.
This scoring metric is different from that used in
prior years in two ways. First, the score previously
included ties between system rankings. In that case,
the score for A reflected how often A was rated as
better than or equal to other systems, and was nor-
malized by all comparisons involving A. However,
this approach unfairly rewards systems that are sim-
ilar (and likely to be ranked as tied). This is prob-
lematic since many of the systems use variations of
the same underlying decoder (Bojar et al, 2011).
A second difference is that this year we no longer
include comparisons against reference translations.
In the past, reference translations were included
14
among the systems to be ranked as controls, and
the pairwise comparisons were used in determin-
ing the best system. However, workers have a very
clear preference for reference translations, so includ-
ing them unduly penalized systems that, through
(un)luck of the draw, were pitted against the ref-
erences more often. These changes are part of a
broader discussion of the best way to produce the
system ranking, which we discuss at length in Sec-
tion 4.
The system scores are reported in Section 3.3.
Appendix A provides detailed tables that contain
pairwise head-to-head comparisons between pairs of
systems.
3.2 Inter- and Intra-annotator agreement in
the ranking task
Each year we calculate the inter- and intra-annotator
agreement for the human evaluation, since a reason-
able degree of agreement must exist to support our
process as a valid evaluation setup. To ensure we
had enough data to measure agreement, we occa-
sionally showed annotators items that were repeated
from previously completed items. These repeated
items were drawn from ones completed by the same
annotator and from different annotators.
We measured pairwise agreement among anno-
tators using Cohen?s kappa coefficient (?) (Cohen,
1960), which is defined as
? =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the anno-
tators agree, and P (E) is the proportion of time that
they would agree by chance. Note that ? is basically
a normalized version of P (A), one which takes into
account how meaningful it is for annotators to agree
with each other, by incorporating P (E). Note also
that ? has a value of at most 1 (and could possibly
be negative), with higher rates of agreement result-
ing in higher ?.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate at
which annotators agree, in the context of pairwise
comparisons. P (A) is computed similarly for intra-
annotator agreement (i.e. self-consistency), but over
pairwise comparisons that were annotated more than
once by a single annotator.
As for P (E), it should capture the probability that
two annotators would agree randomly. Therefore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is com-
puted empirically, by observing how often annota-
tors actually rank two systems as being tied. We
note here that this empirical computation is a depar-
ture from previous years? analyses, where we had
assumed that the three categories are equally likely
(yielding P (E) = 19 +
1
9 +
1
9 =
1
3 ). We believe that
this is a more principled approach, which faithfully
reflects the motivation of accounting for P (E) in the
first place.
Table 3 gives ? values for inter-annotator and
intra-annotator agreement. These give an indica-
tion of how often different judges agree, and how
often single judges are consistent for repeated judg-
ments, respectively. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0 ? 0.2 is slight, 0.2 ? 0.4
is fair, 0.4 ? 0.6 is moderate, 0.6 ? 0.8 is sub-
stantial, and 0.8 ? 1.0 is almost perfect. Based on
these interpretations, the agreement for sentence-
level ranking is fair for inter-annotator and moder-
ate for intra-annotator agreement. Consistent with
previous years, intra-annotator agreement is higher
than inter-annotator agreement, except for English?
Czech.
An important difference from last year is that the
evaluations were not constrained only to workshop
participants, but were made available to all Turk-
ers. The workshop participants were trusted to com-
plete the tasks in good faith, and we have multiple
years of data establishing general levels of inter- and
intra-annotator agreement. Their HITs were unpaid,
and access was limited with the use of a qualifica-
tion. The Turkers completed paid tasks, and we used
controls to filter out fraudulent and unconscientious
workers.
15
INTER-ANNOTATOR AGREEMENT INTRA-ANNOTATOR AGREEMENT
LANGUAGE PAIRS P (A) P (E) ? P (A) P (E) ?
Czech-English 0.567 0.405 0.272 0.660 0.405 0.428
English-Czech 0.576 0.383 0.312 0.566 0.383 0.296
German-English 0.595 0.401 0.323 0.733 0.401 0.554
English-German 0.598 0.394 0.336 0.732 0.394 0.557
Spanish-English 0.540 0.408 0.222 0.792 0.408 0.648
English-Spanish 0.504 0.398 0.176 0.566 0.398 0.279
French-English 0.568 0.406 0.272 0.719 0.406 0.526
English-French 0.519 0.388 0.214 0.634 0.388 0.401
WMT12 0.568 0.396 0.284 0.671 0.396 0.455
WMT11 0.601 0.362 0.375 0.722 0.362 0.564
Table 3: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11
rows contain the results from the European languages individual systems task (Callison-Burch et al (2011), Table 7).
Agreement rates vary widely across languages.
For inter-annotator agreements, the range is 0.176 to
0.336, while intra-annotator agreement ranges from
0.279 to 0.648. We note in particular the low agree-
ment rates among judgments in the English-Spanish
task, which is reflected in the relative lack of statis-
tical significance Table 4. The agreement rates for
this year were somewhat lower than last year.
3.3 Results of the Translation Task
We used the results of the manual evaluation to an-
alyze the translation quality of the different systems
that were submitted to the workshop. In our analy-
sis, we aimed to address the following questions:
? Which systems produced the best translation
quality for each language pair?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 4 shows the system ranking for each of the
translation tasks. For each language pair, we define
a system as ?winning? if no other system was found
statistically significantly better (using the Sign Test,
at p ? 0.10). In some cases, multiple systems are
listed as winners, either due to a large number of par-
ticipants or a low number of judgments per system
pair, both of which are factors that make it difficult
to achieve statistical significance.
As in prior years, unconstrained online systems
A and B are among the best for many tasks, with
a few notable exceptions. CU-DEPFIX, which post-
processes the output of ONLINE-B, was judged as
the best system for English-Czech. For the French-
English and English-French tasks, constrained sys-
tems came out on top, with LIMSI appearing both
times. Consistent with prior years, the rule-based
systems performed very well on the English-German
task. A rule-based system also had a good showing
for English-Spanish, but not really anywhere else.
Among the systems competing in all tasks, no sin-
gle system consistently appeared among the top en-
trants. Participants that competed in all tasks tended
to fair worse, with the exception of UEDIN. Addi-
tionally, KIT appeared in four tasks and was a con-
strained winner each time.
4 Methods for Overall Ranking
Last year one of the long papers published at WMT
criticized our method for compiling the overall rank-
ing for systems in the translation task (Bojar et
al., 2011). This year another paper shows some
additional potential inconsistencies in the rankings
(Lopez, 2012). In this section we delve into a de-
tailed analysis of a variety of methods that use the
human evaluation to create an overall ranking of sys-
tems.
In the human evaluation, we collect ranking judg-
ments for output from five systems at a time. We in-
terpret them as 10 ?
(
5?4
2
)
pairwise judgments over
systems and use these to analyze how each system
faired compared against each of the others. Not all
16
Czech-English
3,603?3,718 comparisons/system
System C? >others
ONLINE-B ? N 0.65
UEDIN ? Y 0.60
CU-BOJAR Y 0.53
ONLINE-A N 0.53
UK Y 0.37
JHU Y 0.32
Spanish-English
1,527?1,775 comparisons/system
System C? >others
ONLINE-A ? N 0.62
ONLINE-B ? N 0.61
QCRI ? Y 0.60
UEDIN ?? Y 0.58
UPC Y 0.57
GTH-UPM Y 0.52
RBMT-3 N 0.51
JHU Y 0.48
RBMT-4 N 0.46
RBMT-1 N 0.42
ONLINE-C N 0.42
UK Y 0.19
French-English
1,437?1,701 comparisons/system
System C? >others
LIMSI ?? Y 0.63
KIT ?? Y 0.61
ONLINE-A ? N 0.59
CMU ?? Y 0.57
ONLINE-B ? N 0.57
UEDIN Y 0.55
LIUM Y 0.52
RWTH Y 0.52
RBMT-1 N 0.46
RBMT-3 N 0.46
UK Y 0.44
SFU Y 0.44
RBMT-4 N 0.43
JHU Y 0.41
ONLINE-C N 0.32
English-Czech
2,652?3,146 comparisons/system
System C? >others
CU-DEPFIX ? N 0.66
ONLINE-B N 0.63
UEDIN ? Y 0.56
CU-TAMCH N 0.56
CU-BOJAR ? Y 0.54
CU-TECTOMT ? Y 0.53
ONLINE-A N 0.53
COMMERCIAL-1 N 0.48
COMMERCIAL-2 N 0.46
CU-POOR-COMB Y 0.44
UK Y 0.44
SFU Y 0.36
JHU Y 0.32
English-Spanish
2,013?2,294 comparisons/system
System C? >others
ONLINE-B ? N 0.65
RBMT-3 N 0.58
ONLINE-A ? N 0.56
PROMT N 0.55
UPC ? Y 0.52
UEDIN ? Y 0.52
RBMT-4 N 0.46
RBMT-1 N 0.45
ONLINE-C N 0.43
UK Y 0.41
JHU Y 0.36
English-French
1,410?1,697 comparisons/system
System C? >others
LIMSI ?? Y 0.66
RWTH Y 0.62
ONLINE-B N 0.60
KIT ?? Y 0.59
LIUM Y 0.55
UEDIN Y 0.53
RBMT-3 N 0.52
ONLINE-A N 0.51
PROMT N 0.51
RBMT-1 N 0.48
JHU Y 0.44
UK Y 0.40
RBMT-4 N 0.39
ONLINE-C N 0.39
ITS-LATL N 0.36
German-English
1,386?1,567 comparisons/system
System C? >others
ONLINE-A ? N 0.65
ONLINE-B ? N 0.65
QUAERO Y 0.61
RBMT-3 N 0.60
UEDIN ? Y 0.60
RWTH ? Y 0.56
KIT ? Y 0.55
LIMSI Y 0.54
QCRI Y 0.52
RBMT-1 N 0.51
RBMT-4 N 0.50
ONLINE-C N 0.43
DFKI-BERLIN Y 0.40
UK Y 0.37
JHU Y 0.34
UG Y 0.17
English-German
1,777?2,160 comparisons/system
System C? >others
ONLINE-B ? N 0.64
RBMT-3 N 0.63
RBMT-4 ? N 0.58
RBMT-1 N 0.56
LIMSI ? Y 0.55
ONLINE-A N 0.54
UEDIN-WILLIAMS ? Y 0.51
KIT ? Y 0.50
DFKI-HUNSICKER N 0.48
UEDIN ? Y 0.47
RWTH ? Y 0.47
ONLINE-C N 0.47
UK Y 0.45
JHU Y 0.43
DFKI-BERLIN Y 0.25
C? indicates whether system is constrained (unhighlighted rows): trained only using supplied training data, standard
monolingual linguistic tools, and, optionally, LDC?s English Gigaword.
? indicates a win: no other system is statistically significantly better at p-level ? 0.10 in pairwise comparison.
? indicates a constrained win: no other constrained system is statistically better.
Table 4: Official results for the WMT12 translation task. Systems are ordered by their > others score, reflecting how
often their translations won in pairwise comparisons. For detailed head-to-head comparisons, see Appendix A.
17
pairwise comparisons detect statistical significantly
superior quality of either system, and we note this
accordingly.
It is desirable to additionally produce an overall
ranking. In the past evaluation campaigns, we used
two different methods to obtain such a ranking, and
this year we use yet another one. In this section, we
discuss each of these overall ranking methods and a
few more.
4.1 Rank Ranges
In the first human evaluation, we use fluency and
adequacy judgments on a scale from 1 to 5 (Koehn
and Monz, 2006). We normalized the scores on a
per-sentence basis, thus converting them to a rela-
tive ranking in a 5-system comparison. We listed
systems by the average of these scores over all sen-
tences, in which they were judged.
We did not report ranks, but rank ranges. To
give an example: if a system scored neither sta-
tistically significantly better nor statistically signif-
icantly worse than 3 other systems, we assign it the
rank range 1?4. The given evidence is not sufficient
to rank it exactly, but it does rank somewhere in the
top 4.
In subsequent years, we did not continue the re-
porting of rank ranges (although they can be ob-
tained by examining the pairwise comparison ta-
bles), but we continued to report systems as win-
ners whenever there was not statistically signifi-
cantly outperformed by any other system.
4.2 Ratio of Wins and Ties
In the following years (Callison-Burch et al, 2007;
Callison-Burch et al, 2008; Callison-Burch et al,
2009; Callison-Burch et al, 2010; Callison-Burch et
al., 2011), we abandoned the idea of using fluency
and adequacy judgments, since they showed to be
less reliable than simple ranking of system transla-
tions. We also started to interpret the 5-system com-
parison as a set of pairwise comparisons.
Systems were then ranked by the ratio of how of-
ten they were ranked better or equal to any of the
other systems.
Given a set J of sentence-level judgments
(s1, s2, c) where s1 ? S and s2 ? S are two sys-
tems and
c =
?
??
??
win if s1 better than s2
tie if s1 equal to s2
loss if s1 worse than s2
(1)
then we can count the total number of wins and ties
of a system s as
win(s) = |{(s1, s2, c) ? J : s = s1, c = win}|+
|{(s1, s2, c) ? J : s = s2, c = loss}|
loss(s) = |{(s1, s2, c) ? J : s = s1, c = loss}|+
|{(s1, s2, c) ? J : s = s2, c = win}|
tie(s) = |{(s1, s2, c) ? J : s = s1, c = tie}|+
|{(s1, s2, c) ? J : s = s2, c = tie}|
(2)
and rank systems by the ratio
score(s) =
win(s) + tie(s)
win(s) + loss(s) + tie(s)
(3)
This ratio was used for the official rankings over
the last five years.
4.3 Ratio of Wins (Ignoring Ties)
Bojar et al (2011) present a persuasive argument
that our ranking scheme is biased towards systems
that are similar to many other systems. Given that
most of the systems are based on phrase-based mod-
els trained on the same training data, this is indeed a
valid concern.
They suggest ignoring ties, and using as ranking
score instead the following ratio:
score(s) =
win(s)
win(s) + loss(s)
(4)
This ratio is used for the official ranking this year.
4.4 Minimizing Pairwise Ranking Violations
Lopez (2012, in this volume) argues against using
aggregate statistics over a set of very diverse judg-
ments. Instead, a ranking that has the least number
of pairwise ranking violations is said to be preferred.
If we define the number of pairwise wins as
win(s1, s2) = |{(s1, s2, c) ? J : c = win}|+
|{(s2, s1, c) ? J : c = loss}|
(5)
then we define a count function for pairwise order
violations as
18
score(s1, s2) = max(0,win(s2, s1)? win(s1, s2))
(6)
Given a bijective ranking function R(s)? i with
the codomain of consecutive integers starting at 1,
the total number of pairwise ranking violations is de-
fined as
score(R) =
?
R(si)<R(sj)
score(si, sj) (7)
Finding the optimal rankingR that minimizes this
score is not trivial, but given the number of systems
involved in this evaluation campaign, it is quite man-
ageable.
4.5 Most Probable Ranking
We now introduce a variant to Lopez?s ranking
method. We motivate it first.
Consider the following scenario:
win(A,B) = 20 win(B,A) = 0
win(B,C) = 40 win(C,B) = 20
win(C,A) = 60 win(A,C) = 40
Since this constitutes a circle, there are three
rankings with the minimum number of 20 violation
(ABC, BCA, CAB).
However, we may want to take the ratio of wins
and losses for each pairwise ranking into account.
Using maximum likelihood estimation, we can de-
fine the probability that system s1 is better than sys-
tem s2 on a randomly drawn sentence as
p(s1 > s2) =
win(s1, s2)
win(s1, s2) + win(s2, s1)
(8)
We can then go on to define5 the probability of a
5Sketch of derivation:
p(s1 > s2 > s3) = p(s1 first)p(s2 second|s1 first)
(chain rule)
p(s1 first) = p(s1 > s2 and s1 > s3)
= p(s1 > s2)p(s1 > s3)
(independence assumption)
p(s2 sec.|s1 first) = p(s2 second)
(independence assumption)
= p(s2 > s3)
ranking of three systems as:
p(s1 > s2 > s3) = p(s1 > s2)p(s1 > s3)p(s2 > s3)
(9)
This function scores the three rankings in the ex-
ample above as follows:
p(A > B > C) = 2020
40
100
40
60 = 0.27
p(B > C > A) = 4060
0
20
60
100 = 0
p(C > A > B) = 60100
20
60
20
20 = 0.20
One disadvantage of this and the previous rank-
ing method is that they do not take advantage of all
available evidence. Consider the example:
win(A,B) = 100 win(B,A) = 0
win(A,C) = 60 win(C,A) = 40
win(B,C) = 50 win(C,B) = 50
Here, system A is clearly ahead, but how about B
and C? They are tied in their pairwise comparison.
So, both ABC and ACB have no pairwise ranking
violations and their most probable ranking score, as
defined above, is the same.
B is clearly worse than A, but C has a fighting
chance, and this should be reflected in the ranking.
The following two overall ranking methods over-
come this problem.
4.6 Monte Carlo Playoffs
The sports world is accustomed to the problem of
finding a ranking of sports teams, but being only able
to have pairwise competitions (think basketball or
football). One strategy is to stage playoffs.
Let?s say there are 4 systems: A,B, C, andD. As
in well-known play-off fashion, they are first seeded.
In our case, this happens randomly, say, 1:A, 2:B,
3:C, 4:D (for simplicity?s sake).
First round: A plays against D, B plays against
C. How do they play? We randomly select a sen-
tence on which they were compared (no ties). If A
is better according to human judgment than D, then
A wins.
Let?s say, A wins against D, and B loses against
C. This leads us to the final A against C and the
3rd place game D against B, in which, say, A and D
win. The resulting final ranking is ACDB.
We repeat this a million times with a different ran-
dom seeding every time, and compute the average
rank, which is then used for overall ranking.
19
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.641: ONLINE-B RBMT-4 RBMT-4 6.16: ONLINE-B 0.640 (1-2): ONLINE-B
2 0.627: RBMT-3 ONLINE-B ONLINE-B 6.39: RBMT-3 0.622 (1-2): RBMT-3
3 0.577: RBMT-4 RBMT-3 RBMT-3 6.98: RBMT-4 0.578 (3-5): RBMT-4
4 0.557: RBMT-1 RBMT-1 RBMT-1 7.32: RBMT-1 0.553 (3-6): RBMT-1
5 0.547: LIMSI ONLINE-A ONLINE-A 7.46: LIMSI 0.543 (3-7): LIMSI
6 0.537: ONLINE-A UEDIN-WILLIAMS LIMSI 7.57: ONLINE-A 0.534 (4-8): ONLINE-A
7 0.509: UEDIN-WILLIAMS LIMSI UEDIN-WILLIAMS 7.87: UEDIN-WILLIAMS 0.511 (5-9): UEDIN-WILLIAMS
8 0.503: KIT KIT KIT 7.98: KIT 0.503 (6-11): KIT
9 0.476: DFKI-HUNSICKER DFKI-HUNSICKER DFKI-HUNSICKER 8.32: UEDIN 0.477 (7-13): UEDIN
10 0.475: UEDIN ONLINE-C ONLINE-C 8.38: DFKI-HUNSICKER 0.472 (8-13): DFKI-HUNSICKER
11 0.470: RWTH UEDIN UEDIN 8.41: ONLINE-C 0.470 (8-13): ONLINE-C
12 0.470: ONLINE-C UK UK 8.44: RWTH 0.468 (8-13): RWTH
13 0.448: UK RWTH RWTH 8.72: UK 0.447 (10-14): UK
14 0.435: JHU JHU JHU 8.87: JHU 0.434 (12-14): JHU
15 0.249: DFKI-BERLIN DFKI-BERLIN DFKI-BERLIN 11.15: DFKI-BERLIN 0.249 (15): DFKI-BERLIN
Table 5: Overall ranking with different methods (English?German)
4.7 Expected Wins
In European national football competitions, each
team plays against each other team, and at the end
the number of wins decides the rankings.6 We can
simulate this type of tournament as well with Monte
Carlo methods. However, in the limit, each team will
be on average ranked based on its expected number
of wins in the competition. We can compute the ex-
pected number of wins straightforward as
score(si) =
1
|S| ? 1
?
j,j 6=i
p(si > sj) (10)
Note that this is very similar to Bojar?s method of
ranking systems, with one additional and important
twist. We can rewrite Equation 4, the variant that
ignores ties, as:
score(si) =
win(si)
win(si)+loss(si)
(11)
=
?
j,j 6=i win(si,sj)?
j,j 6=i win(si,sj)+loss(si,sj)
(12)
This section?s Equation 10 can be rewritten as:
score(si) =
1
|S|
?
j,j 6=i
win(si, sj)
win(si, sj) + loss(si, sj)
(13)
The difference is that the new overall ranking
method normalizes the win ratios per pairwise rank-
ing. And this makes sense, since it overcomes one
6They actually play twice against each other, to balance out
home field advantage, which is not a concern here.
problem with our traditional and Bojar?s ranking
method.
Previously, some systems were put at an dis-
advantage, if they are compared more frequently
against good systems than against bad systems. This
could happen, if participants were not allowed to
rank their own systems (a constraint we enforced
in the past, but no longer). This was noticed by
judges a few years ago, when we had instant re-
porting of rankings during the evaluation period. If
you have one of the best systems and carry out a lot
of human judgments, then competitors? systems will
creep up higher, since they are not compared against
your own (very good) system anymore, but more fre-
quently against bad systems.
4.8 Comparison
Table 5 shows the different rankings for English?
German, a rather typical example. The table dis-
plays the ranking of the systems according to five
different methods, alongside with system scores ac-
cording to the ranking method: the win ratio (Bo-
jar), the average rank (MC Playoffs), and the ex-
pected win ratio (Expected Wins). For the latter, we
performed bootstrap resampling and computed rank
ranges that lie in a 95% confidence interval. You
can find the tables for the other language pairs in the
annex.
The win-based methods (Bojar, MC Playoffs, Ex-
pected Wins) give very similar rankings ? exhibit-
ing mostly just the occasional pairwise flip or for
20
many language pairs the ranking is identical. The
same is true for the two methods based on pairwise
rankings (Lopez, Most Probable). However, the two
types of ranking lead to significantly different out-
comes.
For instance, the win-based methods are pretty
sure that ONLINE-B and RBMT-3 are the two top
performers. Bootstrap resampling of rankings ac-
cording to Expected Wins ranking draws a clear
line between them and the rest. However, Lopez?s
method ranks RBMT-4 first. Why? In direct com-
parison of the three systems, RBMT-4 beats statis-
tically insignificantly ONLINE-B 45% wins against
42% wins and essentially ties with RBMT-3 41%
wins against 41% wins (ONLINE-B beats RBMT-3
49%?35%, p ? 0.01).
We use Bojar?s method as our official method for
ranking in Table 4 and as the human judgments that
we used when calculating how well automatic eval-
uation metrics correlate with human judgments.
4.9 Number of Judgments Needed
In general, there are not enough judgments to rank
systems unambiguously. How many judgments do
we need?
We may extrapolate this number from the num-
ber of judgments we have. Figure 2 provides some
hints. The outlier is Czech?English, for which only
6 systems were submitted and we can separate them
almost completely even at p-level 0.01. For all the
other language pairs, we can only draw for around
40% of the pairwise comparisons conclusions with
that level of statistical significance.
Since the plots also contains the ratio of signifi-
cant conclusions when sub-sampling the number of
judgments, we obtain curves with a clear upward
slope. For English?Czech, for which we were able
to collect much more judgments, we can draw over
60% significant conclusions. The curve for this lan-
guage pair does not look much different than the
other languages, suggesting that doubling the num-
ber of judgments should allow similar levels for
them as well.
5 Metrics Task
In addition to allowing us to analyze the translation
quality of different systems, the data gathered during
p-level 0.01
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
p-level 0.05
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
p-level 0.10
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
Figure 2: Ratio of statistically significant pairwise com-
parisons at different p-levels, based on number of pair-
wise judgments collected.
21
Metric IDs Participant
AMBER National Research Council Canada (Chen et al, 2012)
METEOR CMU (Denkowski and Lavie, 2011)
SAGAN-STS FaMAF, UNC, Argentina (Castillo and Estrella, 2012)
SEMPOS Charles University (Macha?c?ek and Bojar, 2011)
SIMBLEU University of Sheffield (Song and Cohn, 2011)
SPEDE Stanford University (Wang and Manning, 2012)
TERRORCAT University of Zurich, DFKI, Charles U (Fishel et al, 2012)
BLOCKERRCATS, ENXERRCATS, WORD-
BLOCKERRCATS, XENERRCATS, POSF
DFKI (Popovic, 2012)
Table 6: Participants in the metrics task.
the manual evaluation is useful for validating auto-
matic evaluation metrics. Table 6 lists the partici-
pants in this task, along with their metrics.
A total of 12 metrics and their variants were sub-
mitted to the metrics task by 8 research groups. We
provided BLEU and TER scores as baselines. We
asked metrics developers to score the outputs of
the machine translation systems and system com-
binations at the system-level and at the segment-
level. The system-level metrics scores are given in
the Appendix in Tables 29?36. The main goal of
the metrics shared task is not to score the systems,
but instead to validate the use of automatic metrics
by measuring how strongly they correlate with hu-
man judgments. We used the human judgments col-
lected during the manual evaluation for the transla-
tion task and the system combination task to calcu-
late how well metrics correlate at system-level and
at the segment-level.
5.1 System-Level Metric Analysis
We measured the correlation of the automatic met-
rics with the human judgments of translation qual-
ity at the system-level using Spearman?s rank cor-
relation coefficient ?. We converted the raw scores
assigned to each system into ranks. We assigned a
human ranking to the systems based on the percent
of time that their translations were judged to be bet-
ter than the translations of any other system in the
manual evaluation (Equation 4).
When there are no ties, ? can be calculated using
the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
C
S
-E
N
-
6
S
Y
S
T
E
M
S
D
E
-E
N
-
16
S
Y
S
T
E
M
S
E
S
-E
N
-
12
S
Y
S
T
E
M
S
F
R
-E
N
-
15
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
System-level correlation for translations into English
SEMPOS .94 .92 .94 .80 .90
AMBER .83 .79 .97 .85 .86
METEOR .66 .89 .95 .84 .83
TERRORCAT .71 .76 .97 .88 .83
SIMPBLEU .89 .70 .89 .82 .82
TER -.89 -.62 -.92 -.82 .81
BLEU .89 .67 .87 .81 .81
POSF .66 .66 .87 .83 .75
BLOCKERRCATS -.64 -.75 -.88 -.74 .75
WORDBLOCKEC -.66 -.67 -.85 -.77 .74
XENERRCATS -.66 -.64 -.87 -.77 .74
SAGAN-STS .66 n/a .91 n/a n/a
Table 7: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average absolute
value.
22
E
N
-C
Z
-
10
S
Y
S
T
E
M
S
E
N
-D
E
-
22
S
Y
S
T
E
M
S
E
N
-E
S
-
15
S
Y
S
T
E
M
S
E
N
-F
R
-
17
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
System-level correlation for translations out of English
SIMPBLEU .83 .46 .42 .94 .66
BLOCKERRCATS -.65 -.53 -.47 -.93 .64
ENXERRCATS -.74 -.38 -.47 -.93 .63
POSF .80 .54 .37 .69 .60
WORDBLOCKEC -.71 -.37 -.47 -.81 .59
TERRORCAT .65 .48 .58 .53 .56
AMBER .71 .25 .50 .75 .55
TER -.69 -.41 -.45 -.66 .55
METEOR .73 .18 .45 .82 .54
BLEU .80 .22 .40 .71 .53
SEMPOS .52 n/a n/a n/a n/a
Table 8: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average abso-
lute value.
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher absolute value
for ? is making predictions that are more similar to
the human judgments than an automatic evaluation
metric with a lower absolute ?.
The system-level correlations are shown in Ta-
ble 7 for translations into English, and Table 8 out
of English, sorted by average correlation across the
language pairs. The highest correlation for each
language pair and the highest overall average are
bolded. Once again this year, many of the metrics
had stronger correlation with human judgments than
BLEU. The metrics that had the strongest correlation
this year were SEMPOS for the into English direc-
tion and SIMPBLEU for the out of English direc-
tion.
5.2 Segment-Level Metric Analysis
We measured the metrics? segment-level scores with
the human rankings using Kendall?s tau rank corre-
F
R
-E
N
(1
15
94
PA
IR
S
)
D
E
-E
N
(1
19
34
PA
IR
S
)
E
S
-E
N
(9
79
6
PA
IR
S
)
C
S
-E
N
(1
10
21
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations into English
SPEDE07-PP .26 .28 .26 .21 .25
METEOR .25 .27 .25 .21 .25
AMBER .24 .25 .23 .19 .23
SIMPBLEU .19 .17 .19 .13 .17
TERRORCAT .18 .19 .18 .19 .19
XENERRCATS .17 .18 .18 .13 .17
POSF .16 .18 .15 .12 .15
WORDBLOCKEC .15 .16 .17 .13 .15
BLOCKERRCATS .07 .08 .08 .06 .07
SAGAN-STS n/a n/a .21 .20 n/a
Table 9: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average correla-
tion.
E
N
-F
R
(1
15
62
PA
IR
S
)
E
N
-D
E
(1
45
53
PA
IR
S
)
E
N
-E
S
(1
18
34
PA
IR
S
)
E
N
-C
S
(1
88
05
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations out of English
METEOR .26 .18 .21 .16 .20
AMBER .23 .17 .22 .15 .19
TERRORCAT .18 .19 .18 .18 .18
SIMPBLEU .2 .13 .18 .10 .15
ENXERRCATS .20 .11 .17 .09 .14
POSF .15 .13 .15 .13 .14
WORDBLOCKEC .19 .1 .17 .1 .14
BLOCKERRCATS .13 .04 .12 .01 .08
Table 10: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average corre-
lation.
23
lation coefficient. We calculated Kendall?s tau as:
? =
num concordant pairs - num discordant pairs
total pairs
where a concordant pair is a pair of two translations
of the same segment in which the ranks calculated
from the same human ranking task and from the cor-
responding metric scores agree; in a discordant pair,
they disagree. In order to account for accuracy- vs.
error-based metrics correctly, counts of concordant
vs. discordant pairs were calculated specific to these
two metric types. The possible values of ? range
between 1 (where all pairs are concordant) and ?1
(where all pairs are discordant). Thus an automatic
evaluation metric with a higher value for ? is mak-
ing predictions that are more similar to the human
judgments than an automatic evaluation metric with
a lower ? .
We did not include cases where the human rank-
ing was tied for two systems. As the metrics produce
absolute scores, compared to five relative ranks in
the human assessment, it would be potentially un-
fair to the metric to count a slightly different met-
ric score as discordant with a tie in the relative hu-
man rankings. A tie in automatic metric rank for
two translations was counted as discordant with two
corresponding non-tied human judgments.
The correlations are shown in Table 9 for trans-
lations into English, and Table 10 out of English,
sorted by average correlation across the four lan-
guage pairs. The highest correlation for each lan-
guage pair and the highest overall average are
bolded. For the into English direction SPEDE and
METEOR tied for the highest segment-level correla-
tion. METEOR performed the best for the out of En-
glish direction, with AMBER doing admirably well
in both the into- and the out-of-English directions.
6 Quality Estimation task
Quality estimation aims to provide a quality indica-
tor for machine translated sentences at various gran-
ularity levels. It differs from MT evaluation, because
quality estimation techniques do not rely on refer-
ence translations. Instead, quality estimation is gen-
erally addressed using machine learning techniques
to predict quality scores. Potential applications of
quality estimation include:
? Deciding whether a given translation is good
enough for publishing as is
? Informing readers of the target language only
whether or not they can rely on a translation
? Filtering out sentences that are not good
enough even for post-editing by professional
translators
? Selecting the best translation among options
from multiple systems.
This shared-task provides a first common ground
for development and comparison of quality estima-
tion systems, focusing on sentence-level estimation.
It provides training and test datasets, along with
evaluation metrics and a baseline system. The goals
of this shared task are:
? To identify new and effective quality indicators
(features)
? To identify alternative machine learning tech-
niques for the problem
? To test the suitability of the proposed evalua-
tion metrics for quality estimation systems
? To establish the state of the art performance in
the field
? To contrast the performance of regression and
ranking techniques.
The task provides datasets for a single language
pair, text domain and MT system: English-Spanish
news texts produced by a phrase-based SMT sys-
tem (Moses) trained on Europarl and News Com-
mentaries corpora provided in the WMT10 transla-
tion task. As training data, translations were man-
ually annotated for quality in terms of post-editing
effort (1-5 scores) and were provided together with
their source sentences, reference translations, and
post-edited translations (Section 6.1). The shared-
task consisted on automatically producing quality-
estimations for a blind test-set, where English source
sentences and their MT-translations were used as in-
puts. Hidden (and subsequently publicly-released)
manual effort-annotations of those translations (ob-
tained in the same fashion as for the training data)
24
were used as reference labels to evaluate the per-
formance of the participating systems (Section 6.1).
Participants also had full access to the translation
engine-related resources (Section 6.1) and could use
any additional external resources. We have also pro-
vided a software package to extract baseline quality
estimation features (Section 6.3).
Participants could submit up to two systems for
two variations of the task: ranking, where par-
ticipants submit a ranking of translations (no ties
allowed), without necessarily giving any explicit
scores for translations, and scoring, where partici-
pants submit a score for each sentence (in the [1,5]
range). Each of these subtasks is evaluated using
specific metrics (Section 6.2).
6.1 Datasets and resources
Training data
The training data used was selected from data
available from previous WMT shared-tasks for
machine-translation: a subset of the WMT10
English-Spanish test set, and a subset of the WMT09
English-Spanish test set, for a total of 1832 sen-
tences.
The training data consists of the following re-
sources:
? English source sentences
? Spanish machine-translation outputs, created
using the SMT Moses engine
? Effort scores, created by using three profes-
sional post-editors using guidelines describ-
ing Post-Editing (PE) effort from highest effort
(score 1) to lowest effort (score 5)
? Post-Editing output, created by a pool of pro-
fessional post-editors starting from the source
sentences and the Moses translations; these PE
outputs were created before the effort scores
were elicited, and were shown to the PE-effort
judges to facilitate their effort estimates
? Spanish translation outputs, created as part of
the WMT machine-translation shared-task as
reference translations for the English source
sentences (independent of any MT output).
The guidelines used by the PE-effort judges to as-
sign scores 1-5 for each of the ?source, MT-output,
PE-output? triplets are the following:
[1] The MT output is incomprehensible, with lit-
tle or no information transferred accurately. It
cannot be edited, needs to be translated from
scratch.
[2] About 50-70% of the MT output needs to be
edited. It requires a significant editing effort in
order to reach publishable level.
[3] About 25-50% of the MT output needs to be
edited. It contains different errors and mis-
translations that need to be corrected.
[4] About 10-25% of the MT output needs to be
edited. It is generally clear and intelligible.
[5] The MT output is perfectly clear and intelligi-
ble. It is not necessarily a perfect translation,
but requires little or no editing.
Providing reliable effort estimates turned out to
be a difficult task for the PE-effort judges, even in
the current set-up (with post edited outputs available
for consultation). To eliminate some of the noise
from these judgments, we performed an intermedi-
ate cleaning step, in which we eliminated the sen-
tences for which the difference between the max-
imum score and the minimum score assigned be-
tween the three judges was > 1. We started the
data-creation process from a total of 2000 sentences
for the training set, and the final 1832 sentences we
selected as training data were the ones that passed
through this intermediate cleaning step.
Besides score disagreement, we noticed another
trend on the human judgements of PE-effort. Some
judges tend to give more moderate scores (in the
middle of available range), while others like to com-
mit also to scores that are more in the extremes of
the available range. Since the quality estimation task
would be negatively influenced by having most of
the scores in the middle of the range, we have chosen
to compute the final effort scores as an weighted av-
erage between the three PE-effort scores, with more
weight given to the judges with higher standard de-
viation from their own mean score. We have used
25
weights 3, 2, and 1 for the three PE-effort judges ac-
cording to this criterion. There is an additional ad-
vantage resulting from this weighted average score:
instead of obtaining average numbers only at val-
ues x.0, x.33, and x.66 (for unweighted average)7,
the weighted averages are spread more evenly in the
range [1, 5].
A few variations of the training data were pro-
vided, including version with cases restored and a
version detokenized. In addition, engine-internal
information from Moses such as phrase and word
alignments, detailed model scores, etc. (parameter
-trace), n-best lists and stack information from the
search graph as a word graph (parameter -output-
word-graph) as produced by the Moses engine were
provided.
The rationale behind releasing this engine-
internal data was to make it possible for this shared-
task to address quality estimation using a glass-box
approach, that is, making use of information from
the internal workings of the MT engine.
Test data
The test data was a subset of the WMT12 English-
Spanish test set, consisting of 442 sentences. The
test data consists of the following files:
? English source sentences
? Spanish machine-translation outputs, created
using the same SMT Moses engine used to cre-
ate the training data
? Effort scores, created by using three profes-
sional post-editors8 using guidelines describing
PE effort from highest effort (score 1) to lowest
effort (score 5)
The first two files were the input for the quality-
estimation shared-task participating systems. Since
the Moses engine used to create the MT outputs was
the same as the one used for generating the train-
ing data, the engine-internal resources are the same
7These three values are the only ones possible given the
cleaning step we perform prior to averaging the scores, which
ensures that the difference between the maximum score and the
minimum score is at most 1.
8The same post-editors that were used to create the training
data were used to create the test data.
as the ones we released as part of the training data
package.
The effort scores were released after the partic-
ipants submitted their shared-task submission, and
were solely used to evaluate the submissions accord-
ing to the established metrics. The guidelines used
by the PE-effort judges to assign 1-5 scores were the
same as the ones used for creating the training data.
We have used the same criteria to ensure the con-
sistency of the human judgments. The initial set of
candidates consisted of 604 sentences, of which only
442 met this criteria. The final scores used as gold-
values have been obtained using the same weighted-
average scheme as for the training data.
Resources
In addition to the training and test materials, we
made several additional resources that were used for
the baseline QE system and/or the SMT system that
produced the training and test datasets:
? The SMT training corpus: source and target
sides of the corpus used to train the Moses en-
gine. These are a concatenation of the Eu-
roparl and the news-commentary data sets from
WMT10 that were tokenized, cleaned (remov-
ing sentences longer than 80 tokens) and true-
cased.
? Two Language models: 5-gram LM generated
from the interpolation of the two target cor-
pora after tokenization and truecasing (used
by Moses) and a trigram LM generated from
the two source corpora and filtered to remove
singletons (used by the baseline QE system).
We also provided unigram, bigram and trigram
counts (used in the baseline QE system).
? An IBM Model 1 table that generated by
Giza++ using the SMT training corpora.
? A word-alignment file as produced by the
grow-diag-final heuristic in Moses for the SMT
training set.
? A phrase table with word alignment informa-
tion generated from the parallel corpora.
? The Moses configuration file used for decod-
ing.
26
6.2 Evaluation metrics
Ranking metrics
For the ranking task, we defined a novel met-
ric that provides some advantages over a more tra-
ditional ranking metrics like Spearman correlation.
Our metric, called DeltaAvg, assumes that the refer-
ence test set has a number associated with each en-
try that represents its extrinsic value. For instance,
using the effort scale we described in Section 6.1,
we associate a value between 1 and 5 with each
sentence, representing the quality of that sentence.
Given these values, our metric does not need an ex-
plicit reference ranking, the way the Spearman rank-
ing correlation does.9 The goal of the DeltaAvg met-
ric is to measure how valuable a proposed ranking
(which we call a hypothesis ranking) is according to
the extrinsic values associated with the test entries.
We first define a parameterized version of this
metric, called DeltaAvg[n]. The following notations
are used: for a given entry sentence s, V (s) repre-
sents the function that associates an extrinsic value
to that entry; we extend this notation to a set S, with
V (S) representing the average of all V (s), s ? S.
Intuitively, V (S) is a quantitative measure of the
?quality? of the set S, as induced by the extrinsic
values associated with the entries in S. For a set
of ranked entries S and a parameter n, we denote
by S1 the first quantile of set S (the highest-ranked
entries), S2 the second quantile, and so on, for n
quantiles of equal sizes.10 We also use the notation
Si,j =
?j
k=i Sk. Using these notations, we define:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1
? V (S) (14)
When the valuation function V is clear from the con-
text, we write DeltaAvg[n] for DeltaAvgV [n]. The
parameter n represents the number of quantiles we
want to split the set S into. For instance, n = 2
gives DeltaAvg[2] = V (S1) ? V (S), hence it mea-
sures the difference between the quality of the top
9A reference ranking can be implicitly induced according to
these values; if, as in our case, higher values mean better sen-
tences, then the reference ranking is defined such that higher-
scored sentences rank higher than lower-scored sentences.
10If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
quantile (top half) S1 and the overall quality (rep-
resented by V (S)). For n = 3, DeltaAvg[3] =
(V (S1)+V (S1,2)/2?V (S) = ((V (S1)?V (S))+
(V (S1,2 ? V (S)))/2, hence it measures an aver-
age difference across two cases: between the quality
of the top quantile (top third) and the overall qual-
ity, and between the quality of the top two quan-
tiles (S1?S2, top two-thirds) and the overall quality.
In general, DeltaAvg[n] measures an average differ-
ence in quality across n ? 1 cases, with each case
measuring the impact in quality of adding an addi-
tional quantile, from top to bottom. Finally, we de-
fine:
DeltaAvgV =
?N
n=2 DeltaAvgV [n]
N ? 1
(15)
whereN = |S|/2. As before, we write DeltaAvg for
DeltaAvgV when the valuation function V is clear
from the context. The DeltaAvg metric is an aver-
age across all DeltaAvg[n] values, for those n values
for which the resulting quantiles have at least 2 en-
tries (no singleton quantiles). The DeltaAvg metric
has some important properties that are desired for a
ranking metric (see Section 6.4 for the results of the
shared-task that substantiate these claims):
? it is non-parametric (i.e., it does not depend on
setting particular parameters)
? it is automatic and deterministic (and therefore
consistent)
? it measures the quality of a hypothesis rank-
ing from an extrinsic perspective (as offered by
function V )
? its values are interpretable: for a given set of
ranked entries, a value DeltaAvg of 0.5 means
that, on average, the difference in quality be-
tween the top-ranked quantiles and the overall
quality is 0.5
? it has a high correlation with the Spearman rank
correlation coefficient, which makes it as use-
ful as the Spearman correlation, with the added
advantage of its values being extrinsically in-
terpretable.
27
In the rest of this paper, we present results for
DeltaAvg using as valuation function V the Post-
Editing effort scores, as defined in Section 6.1.
We also report the results of the ranking task using
the more-traditional Spearman correlation.
Scoring metrics
For the scoring task, we use two metrics that have
been traditionally used for measuring performance
for regression tasks: Mean Absolute Error (MAE) as
a primary metric, and Root of Mean Squared Error
(RMSE) as a secondary metric. For a given test set
S with entries si, 1 ? i ? |S|, we denote by H(si)
the proposed score for entry si (hypothesis), and by
V (si) the reference value for entry si (gold-standard
value). We formally define our metrics as follows:
MAE =
?N
i=1 |H(si)? V (si)|
N
(16)
RMSE =
?
?N
i=1(H(si)? V (si))
2
N
(17)
where N = |S|. Both these metrics are non-
parametric, automatic and deterministic (and there-
fore consistent), and extrinsically interpretable. For
instance, a MAE value of 0.5 means that, on aver-
age, the absolute difference between the hypothe-
sized score and the reference score value is 0.5. The
interpretation of RMSE is similar, with the differ-
ence that RMSE penalizes larger errors more (via
the square function).
6.3 Participants
Eleven teams (listed in Table 11) submitted one or
more systems to the shared task, with most teams
submitting for both ranking and scoring subtasks.
Each team was allowed up to two submissions (for
each subtask). In the descriptions below participa-
tion in the ranking is denoted (R) and scoring is de-
noted (S).
Baseline system (R, S): the baseline system used
the feature extraction software (also provided
to all participants). It analyzed the source and
translation files and the SMT training corpus
to extract the following 17 system-independent
features that were found to be relevant in previ-
ous work (Specia et al, 2009):
? number of tokens in the source and target
sentences
? average source token length
? average number of occurrences of the tar-
get word within the target sentence
? number of punctuation marks in source
and target sentences
? LM probability of source and target sen-
tences using language models described in
Section 6.1
? average number of translations per source
word in the sentence: as given by IBM 1
model thresholded so that P (t|s) > 0.2,
and so that P (t|s) > 0.01 weighted by
the inverse frequency of each word in the
source side of the SMT training corpus
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower fre-
quency words) and 4 (higher frequency
words) in the source side of the SMT train-
ing corpus
? percentage of unigrams in the source sen-
tence seen in the source side of the SMT
training corpus
These features are used to train a Support Vec-
tor Machine (SVM) regression algorithm using
a radial basis function kernel with the LIBSVM
package (Chang and Lin, 2011). The ?,  and C
parameters were optimized using a grid-search
and 5-fold cross validation on the training set.
We note that although the system is referred to
as a ?baseline?, it is in fact a strong system.
Although it is simple it has proved to be ro-
bust across a range of language pairs, MT sys-
tems, and text domains. It is a simpler variant
of the system used in (Specia, 2011). The ratio-
nale behind having such a strong baseline was
to push systems to exploit alternative sources
of information and combination / learning ap-
proaches.
SDLLW (R, S): Both systems use 3 sets of fea-
tures: the 17 baseline features, 8 system-
dependent features from the decoder logs of
Moses, and 20 features developed internally.
Some of these features made use of additional
data and/or resources, such as a secondary
28
ID Participating team
PRHLT-UPV Universitat Politecnica de Valencia, Spain (Gonza?lez-Rubio et al, 2012)
UU Uppsala University, Sweden (Hardmeier et al, 2012)
SDLLW SDL Language Weaver, USA (Soricut et al, 2012)
Loria LORIA Institute, France (Langlois et al, 2012)
UPC Universitat Politecnica de Catalunya, Spain (Pighin et al, 2012)
DFKI DFKI, Germany (Avramidis, 2012)
WLV-SHEF University of Wolverhampton & University of Sheffield, UK (Felice and Specia, 2012)
SJTU Shanghai Jiao Tong University, China (Wu and Zhao, 2012)
DCU-SYMC Dublin City University, Ireland & Symantec, Ireland (Rubino et al, 2012)
UEdin University of Edinburgh, UK (Buck, 2012)
TCD Trinity College Dublin, Ireland (Moreau and Vogel, 2012)
Table 11: Participants in the WMT12 Quality Evaluation shared task.
MT system that was used as pseudo-reference
for the hypothesis, and POS taggers for both
languages. Feature-selection algorithms were
used to select subsets of features that directly
optimize the metrics used in the task. System
?SDLLW M5PbestAvgDelta? uses a resulting
15-feature set optimized towards the AvgDelta
metric. It employs an M5P model to learn a
decision-tree with only two linear equations.
System ?SDLLW SVM? uses a 20-feature set
and an SVM epsilon regression model with ra-
dial basis function kernel with parameters C,
gamma, and epsilon tuned on a development
set (305 training instances). The model was
trained with 10-fold cross validation and the
tuning process was restarted several times us-
ing different starting points and step sizes to
avoid overfitting. The final model was selected
based on its performance on the development
set and the number of support vectors.
UU (R, S): System ?UU best? uses the 17 base-
line features, plus 82 features from Hardmeier
(2011) (with some redundancy and some over-
lap with baseline features), and constituency
trees over input sentences generated by the
Stanford parser and dependency trees over both
input and output sentences generated by the
MaltParser. System ?UU bltk? uses only the
17 baseline features plus constituency and de-
pendency trees as above. The machine learn-
ing component in both cases is SVM regres-
sion (SVMlight software). For the ranking task,
the ranking induced by the regression output
is used. The system uses polynomial kernels
of degree 2 (UU best) and 3 (UU bltk) as well
as two different types of tree kernels for con-
stituency and dependency trees, respectively.
The SVM margin/error trade-off, the mixture
proportion between tree kernels and polyno-
mial kernels and the degree of the polynomial
kernels were optimised using grid search with
5-fold cross-validation over the training set.
TCD (R, S): ?TCD M5P-resources-only? uses
only the baseline features, while ?TCD M5P-
all? uses the baseline and additional features.
A number of metrics (used as features in
TCD M5P-all) were proposed which work in
the following way: given a sentence to eval-
uate (source sentence for complexity or target
sentence for fluency), it is compared against
some reference data using similarity mea-
sures (various metrics which compare distri-
butions of n-grams). The training data was
used as reference, along with the Google n-
grams dataset. Several learning methods were
tested using Weka on the training data (10-
fold cross-validation). The system submission
uses the M5P (regression with decision trees)
algorithm which performed best. Contrary to
what had been observed on the training data
using cross-validation, ?TCD M5P-resources-
only? performs better than ?TCD M5P-all? on
the test data.
29
PRHLT-UPV (R, S): The system addresses the
task using a regression algorithm with 475 fea-
tures, including the 17 the baseline features.
Most of the features are defined as word scores.
Among them, the features obtained form a
smoothed naive Bayes classifier have shown to
be particularly interesting. Different methods
to combine word-level scores into sentence-
level features were investigated. For model
building, SVM regression was used. Given
the large number of features, the training data
provided as part of the task was insufficient
yielding unstable systems with not so good per-
formance. Different feature selection methods
were implemented to determine a subset of rel-
evant features. The final submission used these
relevant features to train an SVM system whose
parameters were optimized with respect to the
final evaluation metrics.
UEDIN (R, S): The system uses the baseline fea-
tures along with some additional features: bi-
nary features for named entities in source using
Stanford NER Tagger; binary indicators for oc-
currence of quotes or parenthetical segments,
words in upper case and numbers; geometric
mean of target word probabilities and proba-
bility of worst scoring word under a Discrim-
inative Word Lexicon Model; Sparse Neural
Network directly mapping from source to tar-
get (using the vector space model) with source
and target side either filtered to relevant words
or hashed to reduce dimensionality; number of
times at least a 3-gram is seen normalized by
sentence length; and Levenshtein distance of
either source or translation to closest entry of
the SMT training corpus on word or character
level. An ensemble of neural networks opti-
mized for RMSE was used for prediction (scor-
ing) and ranking. The contribution of new fea-
tures was tested by adding them to the baseline
features using 5-fold cross-validation. Most
features did not result in any improvement over
the baseline. The final submission was a com-
bination of all feature sets that showed im-
provement.
SJTU (R, S): The task is treated as a regression
problem using the epsilon-SVM method. All
features are extracted from the official data, in-
volving no external NLP tools/resources. Most
of them come from the phrase table, decod-
ing data and SMT training data. The focus
is on special word relations and special phrase
patterns, thus several feature templates on this
topic are extracted. Since the training data is
not large enough to assign weights to all fea-
tures, methods for estimating common strings
or sequences of words are used. The training
data is divided in 3/4 for training and 1/4 for
development to filter ineffective features. Be-
sides the baseline features, the final submission
contains 18 feature templates and about 4 mil-
lion features in total.
WLV-SHEF (R, S): The systems integrates novel
linguistic features from the source and target
texts in an attempt to overcome the limitations
of existing shallow features for quality estima-
tion. These linguistically-informed features in-
clude part-of-speech information, phrase con-
stituency, subject-verb agreement and target
lexicon analysis, which are extracted using
parsers, corpora and auxiliary resources. Sys-
tems are built using epsilon-SVM regression
with parameters optimised using 5-fold cross-
validation on the training set and two differ-
ent feature sets: ?WLV-SHEF BL? uses the 17
baseline features plus 70 linguistically inspired
features, while ?WLV-SHEF FS? uses a larger
set of 70 linguistic plus 77 shallow features (in-
cluding the baseline). Although results indicate
that the models fall slightly below the baseline,
further analysis shows that linguistic informa-
tion is indeed informative and complementary
to shallow indicators.
DFKI (R, S): ?DFKI morphPOSibm1LM? (R) is
a simple linear interpolation of POS 6-gram
language model scores, morpheme 6-gram lan-
guage model scores, IBM 1 scores (both ?di-
rect? and ?inverse?) for POS 4-grams and for
morphemes. The parallel News corpora from
WMT10 is used as extra data to train the lan-
guage model and the IBM 1 model. ?DFKI cfs-
30
plsreg? and ?DFKI grcfs-mars? (S) use a col-
lection of 264 features generated containing
the baseline features and additional resources.
Numerous methods of feature selection were
tested using 10-fold cross validation on the
training data, reducing these to 23 feature sets.
Several regression and (discretized) classifica-
tion algorithms were employed to train predic-
tion models. The best-performing models in-
cluded features derived from PCFG parsing,
language quality checking and LM scoring, of
both source and target, besides features from
the SMT search graph and a few baseline fea-
tures. ?DFKI cfs-plsreg? uses a Best First
correlation-based feature selection technique,
trained with Partial Least Squares Regression,
while ?DFKI grcfs-mars? uses a Greedy Step-
wise correlation-based feature selection tech-
nique, trained with multivariate adaptive re-
gression splines.
DCU-SYMC (R, S): Systems are based on a clas-
sification approach using a set of features that
includes the baseline features. The manually
assigned quality scores provided for each MT
output in the training set were rounded in or-
der to apply classification algorithms on a lim-
ited set of classes (integer values from 1 to 5).
Three classifiers were combined by averaging
the predicted classes: SVM using sequential
minimal optimization and RBF kernel (parame-
ters optimized by grid search), Naive Bayes and
Random Forest. ?DCU-SYMC constrained? is
based on a set of 70 features derived only from
the data provided for the task. These include
a set of features which attempt to model trans-
lation adequacy using a bilingual topic model
built using Latent Dirichlet Allocation. ?DCU-
SYMC unconstrained? is based on 308 fea-
tures including the constrained ones and oth-
ers extracted using external tools: grammatical-
ity features extracted from the source segments
using the TreeTagger part-of-speech tagger, an
English precision grammar, the XLE parser and
the Brown re-ranking parser and features based
on part-of-speech tag counts extracted from the
MT output using a Spanish TreeTagger model.
Loria (S): Several numerical or boolean features
are computed from the source and target sen-
tences and used to train an SVM regression al-
gorithm with linear (?Loria SVMlinear?) and
radial basis function (?Loria SVMrbf?) as ker-
nel. For the radial basis function, a grid search
is performed to optimise the parameter ?. The
official submission use the baseline features
and a number of features proposed in previous
work (Raybaud et al, 2011), amounting to 66
features. A feature selection algorithm is used
in order to remove non-informative features.
No additional data other than that provided for
the shared task is considered. The training data
is split into a training part (1000 sentences) and
a development part (832 sentences) to learn the
regression model and optimise the parameters
of the regression and for feature selection.
UPC (R, S): The systems use several features on
top of the baseline features. These are mostly
based on different language models estimated
on reference and automatic Spanish transla-
tions of the news-v7 corpus. The automatic
translations are generated by the system used
for the shared task. N-gram LMs are esti-
mated on word forms, POS tags, stop words
interleaved by POS tags, stop-word patterns,
plus variants in which the POS tags are re-
placed with the stem or root of each target
word. The POS tags on the target side are ob-
tained by projecting source side annotations via
automatic alignments. The resulting features
are: the perplexity of each additional language
model, according to the two translations, and
the ratio between the two perplexities. Addi-
tionally, features that estimate the likelihood
of the projection of dependency parses on the
two translations are encoded. For learning, lin-
ear SVM regression is used. Optimization was
done via 5-fold cross-validation on a develop-
ment data. Features are encoded by means of
their z-scores, i.e. how many standard devia-
tions the observed value is above or below the
mean. A variant of the system, ?UPC-2? uses
an option of SVMLight that removes inconsis-
tent points from the training set and retrains the
model until convergence.
31
6.4 Results
Here we give the official results for the ranking and
scoring subtasks followed by a discussion that high-
lights the main findings of the task.
Ranking subtask
Table 12 gives the results for the ranking sub-
task. The table is sorted from best to worse using
the DeltaAvg metric scores (Equation 15) as pri-
mary key and the Spearman correlation scores as
secondary key.
The winning submissions for the ranking subtask
are SDLLW?s M5PbestDeltaAvg and SVM entries,
which have DeltaAvg scores of 0.63 and 0.61, re-
spectively. The difference with respect to all the
other submissions is statistically significant at p =
0.05, using pairwise bootstrap resampling (Koehn,
2004). The state-of-the-art baseline system has a
DeltaAvg score of 0.55 (Spearman rank correla-
tion of 0.58). Five other submissions have perfor-
mances that are not different from the baseline at a
statistically-significant level (p = 0.05), as shown
by the gray area in the middle of Table 12. Three
submissions scored higher than the baseline system
at p = 0.05 (systems above the middle gray area),
which indicates that this shared-task succeeded in
pushing the state-of-the-art performance to new lev-
els. The range of performance for the submissions
in the ranking task varies from a DeltaAvg of 0.65
down to a DeltaAvg of 0.15 (with Spearman values
varying from 0.64 down to 0.19).
In addition to the performance of the official sub-
mission, we report here results obtained by var-
ious oracle methods. The oracle methods make
use of various metrics that are associated in a or-
acle manner to the test input: the gold-label Ef-
fort metric for ?Oracle Effort?, the HTER metric
computed against the post-edited translations as ref-
erence for ?Oracle HTER?, and the BLEU metric
computed against the same post-edited translations
as reference for ?Oracle (H)BLEU?.11 The ?Oracle
Effort? DeltaAvg score of 0.95 gives an upperbound
in terms of DeltaAvg for the test set used in this
evaluation. It basically indicates that, for this set,
11We use the (H)BLEU notation to underscore the use of
Post-Edited translations as reference, as opposed to using ref-
erences that are not the product of a Post-Editing process, as for
the traditional BLEU metric.
the difference in PE effort between the top-quality
quantiles and the overall quality is 0.95 on average.
We would like to emphasize here that the DeltaAvg
metric does not have any a-priori range for its values.
The upperbound, for instance, is test-dependent, and
therefore an ?Oracle Effort? score is useful for un-
derstanding the performance level of real system-
submissions. The ?Oracle HTER? DeltaAvg score
of 0.77 is a more realistic upperbound for the cur-
rent set. Since the HTER metric is considered a
good approximation for the effort required in post-
editing, ranking the test set based on the HTER
scores (from lowest HTER to highest HTER) pro-
vides a good oracle comparison point. The oracle
based on (H)BLEU gives a lower DeltaAvg score,
which can be interpreted to mean that the BLEU
metric provides a lower correlation to post-editing
effort compared to HTER. We also note here that
there is room for improvement between the highest-
scoring submission (at DeltaAvg 0.63) and the ?Ora-
cle HTER? DeltaAvg score of 0.77. We are not sure
if this difference can be bridged completely, but hav-
ing measured a quantitative difference between the
current best-performance and a realistic upperbound
is an important achievement of this shared-task.
Scoring subtask
The results for the scoring task are presented in
Table 13, sorted from best to worse by using the
MAE metric scores (Equation 16) as primary key
and the RMSE metric scores (Equation 17) as sec-
ondary key.
The winning submission is SDLLW?s
M5PbestDeltaAvg, with an MAE of 0.61 and
an RMSE of 0.75 (the difference with respect to
all the other submissions is statistically significant
at p = 0.05, using pairwise bootstrap resam-
pling (Koehn, 2004)). The strong, state-of-the-art
quality-estimation baseline system is measured to
have an MAE of 0.69 and RMSE of 0.82, with six
other submissions having performances that are
not different from the baseline at a statistically-
significant level (p = 0.05), as shown by the gray
area in the middle of Table 13). Five submissions
scored higher than the baseline system at p = 0.05
(systems above the middle gray area), which
indicates that this shared-task also succeeded in
pushing the state-of-the-art performance to new
32
System ID DeltaAvg Spearman Corr
? SDLLW M5PbestDeltaAvg 0.63 0.64
? SDLLW SVM 0.61 0.60
UU bltk 0.58 0.61
UU best 0.56 0.62
TCD M5P-resources-only* 0.56 0.56
Baseline (17FFs SVM) 0.55 0.58
PRHLT-UPV 0.55 0.55
UEdin 0.54 0.58
SJTU 0.53 0.53
WLV-SHEF FS 0.51 0.52
WLV-SHEF BL 0.50 0.49
DFKI morphPOSibm1LM 0.46 0.46
DCU-SYMC unconstrained 0.44 0.41
DCU-SYMC constrained 0.43 0.41
TCD M5P-all* 0.42 0.41
UPC 1 0.22 0.26
UPC 2 0.15 0.19
Oracle Effort 0.95 1.00
Oracle HTER 0.77 0.70
Oracle (H)BLEU 0.71 0.62
Table 12: Official results for the ranking subtask of the WMT12 Quality Evaluation shared-task. The winning submis-
sions are indicated by a ? (the difference with respect to other systems is statistically significant with p = 0.05). The
systems in the gray area are not significantly different from the baseline system. Entries with * represent submissions
for which a bug-fix was applied after the submission deadline.
33
System ID MAE RMSE
? SDLLW M5PbestDeltaAvg 0.61 0.75
UU best 0.64 0.79
SDLLW SVM 0.64 0.78
UU bltk 0.64 0.79
Loria SVMlinear 0.68 0.82
UEdin 0.68 0.82
TCD M5P-resources-only* 0.68 0.82
Baseline (17FFs SVM) 0.69 0.82
Loria SVMrbf 0.69 0.83
SJTU 0.69 0.83
WLV-SHEF FS 0.69 0.85
PRHLT-UPV 0.70 0.85
WLV-SHEF BL 0.72 0.86
DCU-SYMC unconstrained 0.75 0.97
DFKI grcfs-mars 0.82 0.98
DFKI cfs-plsreg 0.82 0.99
UPC 1 0.84 1.01
DCU-SYMC constrained 0.86 1.12
UPC 2 0.87 1.04
TCD M5P-all 2.09 2.32
Oracle Effort 0.00 0.00
Oracle HTER (linear mapping into [1.5-5.0]) 0.56 0.73
Oracle (H)BLEU (linear mapping into [1.5-5.0]) 0.61 0.84
Table 13: Official results for the scoring subtask of the WMT12 Quality Evaluation shared-task. The winning submis-
sion is indicated by a ? (the difference with respect to the other submissions is statistically significant at p = 0.05).
The systems in the gray area are not different from the baseline system at a statistically significant level (p = 0.05).
Entries with * represent submissions for which a bug-fix was applied after the submission deadline.
34
levels in terms of absolute scoring. The range of
performance for the submissions in the scoring task
varies from an MAE of 0.61 up to an MAE of 0.87
(the outlier MAE of 2.09 is reportedly due to bugs).
We also calculate scoring Oracles using the meth-
ods used for the ranking Oracles. The difference is
that the HTER and (H)BLEU oracles need a way
of mapping their scores (which are usually in the
[0, 100] range) into the [1, 5] range. For the compar-
ison here, we did the mapping by excluding the 5%
top and bottom outlier scores, and then linearly map-
ping the remaining range into the [1.5, 5] range. The
?Oracle Effort? scores are not very indicative in this
case. However, the ?Oracle HTER? MAE score of
0.56 is a somewhat realistic lowerbound for the cur-
rent set (although the score could be decreased by a
smarter mapping from the HTER range to the Effort
range). We argue that since the HTER metric is con-
sidered a good approximation for the effort required
in post-editing, effort-like scores derived from the
HTER score provide a good way to compute oracle
scores in a deterministic manner. Note that again
the oracle based on (H)BLEU gives a worse MAE
score at 0.61, which support the interpretation that
the (H)BLEU metric provides a lower correlation
to post-editing effort compared to (H)TER. Over-
all, we consider the MAE values for these HTER
and (H)BLEU-based oracles to indicate high error
margins. Most notably the performance of the best
system gets the same MAE score as the (H)BLEU
oracle, at 0.61 MAE. We take this to mean that the
scoring task is more difficult compared to the rank-
ing task, since even oracle-based solutions get high
error scores.
6.5 Discussion
When looking back at the goals that we identified for
this shared-task, most of them have been success-
fully accomplished. In addition, we have achieved
additional ones that were not explicitly stated from
the beginning. In this section, we discuss the accom-
plishments of this shared-task in more detail, start-
ing from the defined goals and beyond.
Identify new and effective quality indicators
The vast majority of the participating systems use
external resources in addition to those provided for
the task, such as parsers, part-of-speech taggers,
named entity recognizers, etc. This has resulted in
a wide variety of features being used. Many of the
novel features have tried to exploit linguistically-
oriented features. While some systems did not
achieve improvements over the baseline while ex-
ploiting such features, others have (the ?UU? sub-
missions, for instance, exploiting both constituency
and dependency trees).
Another significant set of features that has been
previously overlooked is the feature set of the MT
decoder. Considering statistical engines, these fea-
tures are immediately available for quality predic-
tion from the internal trace of the MT decoder (in
a glass-box prediction scenario), and its contribu-
tion is significant. These features, which reflect the
?confidence? of the SMT system on the translations
it produces, have been shown to be complemen-
tary to other, system-independent (black-box) fea-
tures. For example, the ?SDLLW? submissions in-
corporate these features, and their feature selection
strategy consistently favored this feature set. The
power of this set of features alone is enough to yield
(when used with an M5P model) outputs that would
have been placed 4th in the ranking task and 5th
in the scoring task, a remarkable achievement. An-
other interesting feature used by the ?SDLLW? sub-
missions rely on pseudo-references, i.e., translations
produced by other MT systems for the same input
sentence.
Identify alternative machine learning techniques
Although SVM regression was used to compute the
baseline performance, the baseline ?system? pro-
vided for the task consisted solely of a software to
extract features, as opposed to a model built us-
ing the regression algorithm. The rationale behind
this decision was to encourage participants to exper-
iment with alternative methods for combining differ-
ent quality indicators. This was achieved to a large
extent.
The best-performing machine learning techniques
were found to be the M5P Regression Trees and the
SVM Regression (SVR) models. The merit of the
M5P Regression Trees is that it provides compact
models that are less prone to overfitting. In contrast,
the SVR models can easily overfit given the small
amount of training data available and the large num-
bers of features commonly used. Indeed, many of
35
the submissions that fell below the baseline perfor-
mance can blame overfitting for (part of) their sub-
optimal performance. However, SVR models can
achieve high performance through the use of tun-
ing and feature selection techniques to avoid overfit-
ting. Structured learning techniques were success-
fully used by the ?UU? submissions ? the second
best performing team ? to represent parse trees. This
seems an interesting direction to encode other sorts
of linguistic information about source and trans-
lation texts. Other interesting learning techniques
have been tried, such as Neural Networks, Par-
tial Least Squares Regression, or multivariate adap-
tive regression splines, but their performance does
not suggest they are strong candidates for learning
highly-performing quality-estimation models.
Test the suitability of evaluation metrics for qual-
ity estimation DeltaAvg, our proposed metric for
measuring ranking performance, proved suitable for
scoring the ranking subtask. Its high correlation with
the Spearman ranking metric, coupled with its ex-
trinsic interpretability, makes it a preferred choice
for future measurements. It is also versatile, in the
sense that the its valuation function V can change to
reflect different extrinsic measures of quality.
Establish the state of the art performance The
results on both the ranking and the scoring subtasks
established new state of the art levels on the test set
used in this shared task. In addition to these lev-
els, the oracle performance numbers also help under-
stand the current performance level, and how much
of a gap in performance there still exists. Addi-
tional data points regarding quality estimation per-
formance are needed to establish how stable this
measure of the performance gap is.
Contrast the performance of regression and
ranking techniques Most of the submissions in
the ranking task used the results provided by a re-
gression solution (submitted for the scoring task) to
infer the rankings. Also, optimizing for ranking per-
formance via a regression solution seems to result in
regression models that perform very well, as in the
case of the top-ranked submission.
6.6 Quality Estimation Conclusions
There appear to be significant differences between
considering the quality estimation task as a ranking
problem versus a scoring problem. The ranking-
based approach appears to be somewhat simpler
and more easily amenable to automatic solutions,
and at the same time provides immediate benefits
when integrated into larger applications (see, for in-
stance, the post-editing application described in Spe-
cia (2011)). The scoring-based approach is more dif-
ficult, as the high error rate even of oracle-based so-
lutions indicates. It is also well-known from human
evaluations of MT outputs that human judges also
have a difficult time agreeing on absolute-number
judgements to translations.
Our experience in creating the current datasets
confirms that, even with highly-trained profession-
als, it is difficult to arrive at consistent judge-
ments. We plan to have future investigations on
how to achieve more consistent ways of generating
absolute-number scores that reflect the quality of au-
tomated translations.
7 Summary
As in previous incarnations of this workshop we car-
ried out an extensive manual and automatic evalu-
ation of machine translation performance, and we
used the human judgements that we collected to val-
idate automatic metrics of translation quality. This
year was also the debut of a new quality estimation
task, which tries to predict the effort involved in hav-
ing post editors correct MT output. The quality es-
timation task differs from the metrics task in that it
does not involve reference translations.
As in previous years, all data sets generated by
this workshop, including the human judgments, sys-
tem translations and automatic scores, are publicly
available for other researchers to analyze.12
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-0022,
12http://statmt.org/wmt12/results.html
36
the US National Science Foundation under grant
IIS-0713448, and the CoSyne project FP7-ICT-4-
248531 funded by the European Commission. The
views and findings are the authors? alone. Thanks
for Adam Lopez for discussions about alternative
ways of ranking the overall system scores. The
Quality Estimation shared task organizers thank
Wilker Aziz for his help with the SMT models and
resources, and Mariano Felice for his help with the
system for the extraction of baseline features.
References
Eleftherios Avramidis. 2012. Quality estimation for
machine translation output using linguistic analysis
and decoding features. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A grain of salt for the wmt man-
ual evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 1?11, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ondrej Bojar, Bushra Jawaid, and Amir Kamran. 2012.
Probes in a taxonomy of factored phrase-based mod-
els. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Christian Buck. 2012. Black box features for the WMT
2012 quality estimation shared task. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion (WMT07), Prague, Czech Republic.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation (WMT09), Athens, Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT10), Uppsala, Swe-
den.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2009), Singapore.
Julio Castillo and Paula Estrella. 2012. Semantic tex-
tual similarity for MT evaluation. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27. Software available at http://www.
csie.ntu.edu.tw/?cjlin/libsvm.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving amber, an MT evaluation metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measur-
ment, 20(1):37?46.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic metric for reliable optimization and evalu-
ation of machine translation systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The CMU-avenue French-English translation
system. In Proceedings of the Seventh Workshop on
Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Ondr?ej Dus?ek, Zdene?k Z?abokrtsky?, Martin Popel, Mar-
tin Majlis?, Michal Nova?k, and David Marec?ek. 2012.
Formemes in English-Czech deep syntactic mt. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Mariano Felice and Lucia Specia. 2012. Linguistic fea-
tures for quality estimation. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
37
Mark Fishel, Rico Sennrich, Maja Popovic?, and Ondr?ej
Bojar. 2012. TerrorCat: a translation error
categorization-based MT quality metric. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Lluis Formiga, Carlos A. Henr??quez Q., Adolfo
Herna?ndez, Jose? B. Marin?o, Enric Monte, and Jose?
A. R. Fonollosa. 2012. The TALP-UPC phrase-based
translation systems for WMT12: Morphology simpli-
fication and domain adaptation. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
and Chris Callison-Burch. 2012. Joshua 4.0: Packing,
PRO, and paraphrases. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Ulrich Germann. 2012. Syntax-aware phrase-based sta-
tistical machine translation: System description. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Jesu?s Gonza?lez-Rubio, Alberto Sanch??s, and Francisco
Casacuberta. 2012. PRHLT submission to the
WMT12 quality estimation task. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
Francisco Guzman, Preslav Nakov, Ahmed Thabet, and
Stephan Vogel. 2012. QCRI at WMT12: Exper-
iments in Spanish-English and German-English ma-
chine translation of news text. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiedemann.
2012. Tree kernels for machine translation quality
estimation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Christian Hardmeier. 2011. Improving machine transla-
tion quality prediction with syntactic tree kernels. In
Proceedings of the 15th conference of the European
Association for Machine Translation, pages 233?240,
Leuven, Belgium.
Matthias Huck, Stephan Peitz, Markus Freitag, Malte
Nuhn, and Hermann Ney. 2012. The RWTH aachen
machine translation system for WMT 2012. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Sabine Hunsicker, Chen Yu, and Christian Federmann.
2012. Machine learning for hybrid machine transla-
tion. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards effec-
tive use of training data in statistical machine transla-
tion. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation, New
York, New York.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the Empirical Methods in Natural Language Process-
ing Conference.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
David Langlois, Sylvain Raybaud, and Kamel Sma??li.
2012. LORIA system for the WMT12 quality esti-
mation shared task. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,
Marianna Apidianaki, Li Gong, Aure?lien Max, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois Yvon.
2012. LIMSI @ WMT12. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Vero?nica Lo?pez-Luden?a, Rube?n San-Segundo, and
Juan M. Montero. 2012. UPM system for WMT 2012.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Adam Lopez. 2012. Putting human assessments of ma-
chine translation systems in order. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Matous? Macha?c?ek and Ondej Bojar. 2011. Approxi-
mating a deep-syntactic metric for mt evaluation and
tuning. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 373?379, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Freitag Markus, Peitz Stephan, Huck Matthias, Ney Her-
mann, Niehues Jan, Herrmann Teresa, Waibel Alex,
38
Hai-son Le, Lavergne Thomas, Allauzen Alexandre,
Buschbeck Bianka, Crego Joseph Maria, and Senel-
lart Jean. 2012. Joint WMT 2012 submission of
the QUAERO project. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Alexander Molchanov. 2012. PROMT deephybrid sys-
tem for WMT12 shared translation task. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Erwan Moreau and Carl Vogel. 2012. Quality estima-
tion: an experimental study using unsupervised simi-
larity measures. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, Montreal,
Canada, June. Association for Computational Linguis-
tics.
Jan Niehues, Yuqi Zhang, Mohammed Mediani, Teresa
Herrmann, Eunah Cho, and Alex Waibel. 2012. The
karlsruhe institute of technology translation systems
for the WMT 2012. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Daniele Pighin, Meritxell Gonza?lez, and Llu??s Ma`rquez.
2012. The upc submission to the WMT 2012 shared
task on quality estimation. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Sylvain Raybaud, David Langlois, and Kamel Sma??li.
2011. ?This sentence is wrong.? Detecting errors in
machine-translated sentences. Machine Translation,
25(1):1?34.
Majid Razmara, Baskaran Sankaran, Ann Clifton, and
Anoop Sarkar. 2012. Kriya - the SFU system for
translation task at WMT-12. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek. 2012.
DEPFIX: A system for automatic correction of czech
MT outputs. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. DCU-Symantec submission for the
WMT 2012 quality estimation task. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Christophe Servan, Patrik Lambert, Anthony Rousseau,
Holger Schwenk, and Lo??c Barrault. 2012. LIUM?s
smt machine translation systems for WMT 2012. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence level MT eval-
uation. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver systems in the WMT12
quality estimation shared task. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the Sentence-Level Quality of Machine Transla-
tion Systems. In Proceedings of the 13th Conference
of the European Association for Machine Translation,
pages 28?37, Barcelona.
Lucia Specia. 2011. Exploiting Objective Annotations
for Measuring Translation Post-editing Effort. In Pro-
ceedings of the 15th Conference of the European As-
sociation for Machine Translation, pages 73?80, Leu-
ven.
Ales? Tamchyna, Petra Galus?c?a?kova?, Amir Kamran, Milos?
Stanojevic?, and Ondr?ej Bojar. 2012. Selecting data for
English-to-Czech machine translation. In Proceedings
of the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
David Vilar. 2012. DFKI?s smt system for WMT 2012.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Mengqiu Wang and Christopher Manning. 2012.
SPEDE: Probabilistic edit distance metrics for MT
evaluation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Eric Wehrli, Luka Nerima, and Yves Scherrer. 2009.
Deep linguistic multilingual translation and bilingual
dictionaries. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 90?94.
Philip Williams and Philipp Koehn. 2012. GHKM rule
extraction and scope-3 parsing in Moses. In Proceed-
ings of the Seventh Workshop on Statistical Machine
39
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Chunyang Wu and Hai Zhao. 2012. Regression with
phrase indicators for estimating MT quality. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Daniel Zeman. 2012. Data issues of the multilingual
translation matrix. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
40
C
U
-B
O
JA
R
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
U
E
D
IN
U
K
CU-BOJAR ? .29? .43 .53? .47? .31?
JHU .59? ? .59? .67? .65? .44?
ONLINE-A .44 .28? ? .52? .46? .32?
ONLINE-B .36? .23? .34? ? .38? .25?
UEDIN .36? .23? .36? .48? ? .27?
UK .56? .33? .56? .63? .60? ?
> others 0.53 0.32 0.53 0.65 0.60 0.37
Table 14: Head to head comparison for Czech-English systems
A Pairwise System Comparisons by Human Judges
Tables 14?21 show pairwise comparisons between systems for each language pair. The numbers in each of
the tables? cells indicate the percentage of times that the system in that column was judged to be better than
the system in that row. Bolding indicates the winner of the two systems. The difference between 100 and
the sum of the complementary cells is the percent of time that the two systems were judged to be equal.
Because there were so many systems and data conditions the significance of each pairwise comparison
needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differences
(rather than differences that are attributable to chance). In the following tables ? indicates statistical signif-
icance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical significance at
p ? 0.01, according to the Sign Test.
Each table contains a final row showing how often a system was ranked to be > than the others. As
suggested by Bojar et al (2011) present, this is calculated ignoring ties as:
score(s) =
win(s)
win(s) + loss(s)
(18)
B Automatic Scores
Tables 29?36 give the automatic scores for each of the systems.
41
C
O
M
M
E
R
C
IA
L
2
C
U
-B
O
JA
R
C
U
-D
E
P
F
IX
C
U
-P
O
O
R
-C
O
M
B
C
U
-T
A
M
C
H
C
U
-T
E
C
T
O
M
T
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
C
O
M
M
E
R
C
IA
L
1
S
F
U
U
E
D
IN
U
K
COMMERCIAL2 ? .48? .56? .43 .49? .50? .32? .49? .54? .36 .38? .50? .42
CU-BOJAR .33? ? .49? .29? .26 .39 .26? .40 .51? .37? .27? .43 .33?
CU-DEPFIX .28? .36? ? .26? .30? .32? .18? .31? .13? .33? .21? .31? .25?
CU-POOR-COMB .42 .40? .59? ? .41? .51? .34? .49? .57? .45 .33? .47? .42
CU-TAMCH .38? .24 .51? .27? ? .39 .22? .42 .47? .38? .28? .39 .28?
CU-TECTOMT .32? .42 .49? .33? .47 ? .24? .42 .46? .36? .33? .46 .40
JHU .54? .59? .69? .50? .62? .60? ? .59? .61? .52? .44 .62? .48?
ONLINE-A .36? .41 .51? .36? .43 .43 .24? ? .51? .40 .26? .45 .32?
ONLINE-B .32? .34? .24? .28? .35? .35? .22? .33? ? .31? .23? .33? .22?
COMMERCIAL1 .41 .48? .55? .41 .50? .49? .36? .46 .54? ? .30? .48 .41
SFU .47? .56? .64? .47? .55? .52? .36 .53? .64? .56? ? .58? .48?
UEDIN .36? .36 .50? .29? .38 .43 .24? .37 .48? .40 .25? ? .30?
UK .43 .47? .59? .43 .52? .44 .26? .50? .59? .47 .35? .52? ?
> others 0.46 0.54 0.66 0.44 0.56 0.53 0.32 0.53 0.63 0.48 0.36 0.56 0.44
Table 15: Head to head comparison for English-Czech systems
IT
S
-L
A
T
L
JH
U
K
IT
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
P
R
O
M
T
R
W
T
H
U
E
D
IN
U
K
ITS-LATL ? .49? .54? .55? .53? .59? .58? .38 .47? .32 .45? .47? .62? .53? .48
JHU .35? ? .47? .55? .42 .45 .55? .36 .49? .37 .46 .46? .47? .46? .29
KIT .25? .25? ? .37 .29? .28? .39 .27? .35 .30? .32? .33 .36 .24? .22?
LIMSI .23? .23? .34 ? .26 .21? .29? .25? .29? .19? .19? .32? .22? .29 .16?
LIUM .25? .36 .42? .34 ? .27? .46? .21? .40 .25? .37 .34 .35 .29 .30?
ONLINE-A .22? .33 .40? .45? .42? ? .44? .26? .43 .33? .38 .33 .47? .35 .30?
ONLINE-B .20? .22? .33 .43? .32? .29? ? .27? .36 .26? .33 .34 .39 .29? .24?
RBMT-4 .37 .47 .56? .60? .60? .55? .52? ? .41 .36 .39 .40 .58? .51? .42
RBMT-3 .30? .35? .43 .45? .40 .39 .37 .34 ? .27? .29 .23 .55? .42 .34?
ONLINE-C .36 .46 .46? .55? .49? .50? .58? .38 .48? ? .45? .43 .62? .45 .39
RBMT-1 .28? .36 .49? .58? .40 .42 .44 .35 .38 .31? ? .41 .45 .37 .30?
PROMT .20? .34? .41 .50? .46 .40 .40 .34 .22 .33 .32 ? .48? .41 .27?
RWTH .22? .28? .34 .37? .31 .28? .32 .27? .26? .22? .34 .31? ? .29 .17?
UEDIN .28? .29? .40? .39 .34 .35 .42? .31? .39 .34 .36 .34 .34 ? .27?
UK .37 .36 .53? .53? .44? .43? .48? .38 .52? .39 .44? .46? .52? .46? ?
> others 0.36 0.44 0.59 0.66 0.55 0.51 0.6 0.39 0.52 0.39 0.48 0.51 0.62 0.53 0.4
Table 16: Head to head comparison for English-French systems
42
D
F
K
I-
B
E
R
L
IN
D
F
K
I-
H
U
N
S
IC
K
E
R
JH
U
K
IT
L
IM
S
I
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
R
W
T
H
U
E
D
IN
-W
IL
L
IA
M
S
U
E
D
IN
U
K
DFKI-BERLIN ? .62? .58? .64? .71? .68? .80? .68? .71? .58? .65? .62? .64? .61? .60?
DFKI-HUNSICKER .28? ? .42 .48 .51? .47 .52? .49? .57? .38 .53? .39 .39 .41 .39
JHU .24? .45 ? .43? .43 .47? .62? .56? .60? .46 .47? .46? .47? .39 .42
KIT .22? .41 .27? ? .39 .45 .60? .54? .58? .37 .47 .33 .43 .39 .26?
LIMSI .15? .37? .34 .36 ? .47 .49? .43 .43 .35 .48 .36 .37 .32 .31?
ONLINE-A .20? .37 .35? .41 .39 ? .45? .42 .51? .38 .49 .42 .40 .37 .36?
ONLINE-B .15? .35? .26? .27? .35? .30? ? .45 .35? .29? .41 .30? .34? .30? .18?
RBMT-4 .25? .22? .31? .31? .45 .45 .42 ? .41 .38 .40 .44 .35? .36? .36?
RBMT-3 .18? .27? .24? .28? .38 .36? .49? .41 ? .33? .26? .29? .28? .31? .34?
ONLINE-C .27? .47 .35 .49 .46 .44 .63? .48 .55? ? .49? .40 .43 .43 .46
RBMT-1 .19? .30? .33? .41 .41 .39 .45 .45 .50? .32? ? .34? .40 .39 .39
RWTH .20? .43 .30? .45 .45 .44 .58? .50 .58? .43 .53? ? .41 .40 .41
UEDIN-WILLIAMS .20? .46 .30? .36 .36 .45 .54? .52? .54? .41 .46 .38 ? .32 .30?
UEDIN .20? .45 .40 .38 .43 .48 .56? .56? .53? .47 .48 .29 .39 ? .35
UK .25? .49 .40 .45? .51? .49? .64? .51? .52? .44 .47 .34 .48? .40 ?
> others 0.25 0.48 0.43 0.50 0.55 0.54 0.64 0.58 0.63 0.47 0.56 0.47 0.51 0.47 0.45
Table 17: Head to head comparison for English-German systems
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
P
R
O
M
T
U
E
D
IN
U
K
U
P
C
JHU ? .52? .59? .50? .58? .48? .49? .56? .48? .44? .52?
ONLINE-A .27? ? .45 .34? .44 .31? .31? .44 .37 .28? .37
ONLINE-B .21? .37 ? .28? .35? .25? .28? .31? .30? .23? .31?
RBMT-4 .35? .52? .56? ? .49? .39 .40 .46? .45 .38? .45
RBMT-3 .26? .39 .46? .34? ? .32? .28? .24 .34? .32? .37
ONLINE-C .33? .54? .61? .40 .47? ? .43 .50? .50? .42 .48
RBMT-1 .39? .51? .61? .39 .49? .34 ? .47? .50? .39 .46
PROMT .28? .41 .51? .33? .29 .33? .34? ? .42 .32? .40
UEDIN .25? .41 .48? .38 .47? .30? .35? .43 ? .28? .39
UK .31? .52? .57? .48? .53? .42 .44 .52? .42? ? .50?
UPC .24? .40 .53? .40 .43 .39 .39 .46 .36 .28? ?
> others 0.36 0.56 0.65 0.46 0.58 0.43 0.45 0.55 0.52 0.41 0.52
Table 18: Head to head comparison for English-Spanish systems
43
C
M
U
JH
U
K
IT
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
R
W
T
H
S
F
U
U
E
D
IN
U
K
CMU ? .34? .32 .46 .35 .41 .39 .30? .36 .29? .35? .32 .28? .45 .33?
JHU .50? ? .63? .55? .53? .63? .57? .43 .42 .31? .46 .52? .43 .53? .43
KIT .40 .21? ? .36 .30 .35 .44 .33? .33? .23? .31? .25? .28? .23? .30?
LIMSI .35 .26? .37 ? .31? .35 .40 .29? .32? .23? .33? .29? .28? .29 .23?
LIUM .47 .25? .43 .53? ? .44 .42 .36 .43 .28? .38 .38 .32? .40 .42
ONLINE-A .45 .22? .41 .47 .40 ? .41 .30? .25? .28? .23? .40 .27? .40 .25?
ONLINE-B .45 .32? .38 .42 .41 .39 ? .34? .39 .30? .33? .30? .34? .44 .32?
RBMT-4 .56? .40 .54? .61? .48 .54? .54? ? .43 .31? .48? .45 .42 .52? .46
RBMT-3 .50 .46 .53? .53? .46 .54? .47 .33 ? .28? .40 .53? .52 .50 .48
ONLINE-C .59? .57? .72? .66? .59? .60? .61? .45? .54? ? .58? .65? .53? .66? .58?
RBMT-1 .54? .43 .58? .54? .48 .62? .55? .31? .44 .20? ? .47 .41 .56? .38
RWTH .39 .35? .50? .52? .43 .50 .55? .42 .37? .23? .40 ? .34? .36 .29?
SFU .57? .38 .55? .54? .48? .55? .51? .42 .38 .35? .45 .50? ? .41 .46
UEDIN .37 .32? .42? .42 .40 .43 .40 .34? .40 .24? .36? .39 .41 ? .29?
UK .50? .40 .48? .59? .44 .58? .50? .42 .41 .35? .49 .53? .35 .51? ?
> others 0.57 0.41 0.61 0.63 0.52 0.59 0.57 0.43 0.46 0.32 0.46 0.52 0.44 0.55 0.44
Table 19: Head to head comparison for French-English systems
D
F
K
I-
B
E
R
L
IN
JH
U
K
IT
L
IM
S
I
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
Q
C
R
I
Q
U
A
E
R
O
R
W
T
H
U
E
D
IN
U
G
U
K
DFKI-BERLIN ? .38 .49 .52? .57? .65? .55? .62? .50 .49 .51? .66? .53? .61? .17? .37
JHU .45 ? .60? .66? .66? .69? .57? .60? .52 .62? .58? .67? .59? .62? .21? .37
KIT .36 .16? ? .47 .60? .50 .41 .50 .31? .39 .32 .36 .32 .39 .15? .26?
LIMSI .30? .14? .35 ? .49? .57? .49 .54 .34? .33? .43 .31 .44 .49? .14? .30?
ONLINE-A .32? .20? .22? .32? ? .39 .30? .44 .20? .30? .37 .35? .32? .31? .16? .29?
ONLINE-B .25? .21? .38 .29? .38 ? .27? .39 .31? .37 .30? .43 .34 .33? .12? .24?
RBMT-4 .33? .33? .49 .44 .57? .63? ? .46 .26? .40 .53? .51? .56? .48 .21? .32?
RBMT-3 .26? .30? .39 .40 .45 .45 .32 ? .35 .36 .34? .48 .33? .41 .13? .23?
ONLINE-C .36 .37 .58? .54? .70? .62? .57? .50 ? .53? .48 .57? .55? .58? .14? .45
RBMT-1 .41 .32? .48 .55? .64? .52 .42 .47 .34? ? .51 .49 .48 .45 .15? .25?
QCRI .31? .26? .43 .37 .48 .51? .36? .52? .43 .38 ? .48? .48? .45? .14? .23?
QUAERO .18? .19? .29 .33 .51? .43 .33? .42 .31? .37 .23? ? .34 .48? .09? .16?
RWTH .29? .25? .38 .34 .51? .48 .37? .58? .38? .40 .29? .39 ? .44 .20? .24?
UEDIN .24? .20? .38 .30? .55? .52? .42 .44 .35? .37 .29? .32? .38 ? .08? .22?
UG .68? .61? .72? .76? .76? .82? .72? .80? .70? .76? .73? .76? .73? .84? ? .57?
UK .43 .37 .48? .48? .54? .62? .57? .64? .44 .59? .49? .58? .51? .56? .20? ?
> others 0.40 0.34 0.55 0.54 0.65 0.65 0.50 0.60 0.43 0.51 0.52 0.61 0.56 0.6 0.17 0.37
Table 20: Head to head comparison for German-English systems
44
G
T
H
-U
P
M
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
Q
C
R
I
U
E
D
IN
U
K
U
P
C
GTH-UPM ? .41 .50? .52? .38 .46 .32? .35? .44? .46 .17? .41
JHU .37 ? .54? .56? .44 .48 .39 .39 .47? .50? .15? .47?
ONLINE-A .34? .31? ? .43 .28? .38? .29? .29? .40 .39 .16? .41
ONLINE-B .36? .30? .44 ? .34? .38 .30? .32? .37? .38 .18? .41
RBMT-4 .50 .45 .61? .57? ? .46 .41 .40 .53? .57? .21? .56?
RBMT-3 .42 .40 .53? .51 .36 ? .36? .31? .60? .54? .14? .54?
ONLINE-C .54? .48 .58? .62? .49 .50? ? .40 .58? .59? .23? .55?
RBMT-1 .56? .50 .59? .57? .40 .53? .41 ? .57? .59? .23? .58?
QCRI .28? .31? .45 .50? .38? .32? .29? .34? ? .31 .12? .33?
UEDIN .39 .27? .49 .49 .33? .38? .31? .31? .34 ? .15? .38
UK .74? .71? .81? .76? .73? .76? .69? .66? .76? .75? ? .77?
UPC .42 .32? .49 .49 .38? .36? .33? .35? .44? .36 .14? ?
> others 0.52 0.48 0.62 0.61 0.46 0.51 0.42 0.42 0.60 0.58 0.19 0.57
Table 21: Head to head comparison for Spanish-English systems
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.643: ONLINE-B ONLINE-B ONLINE-B 2.88: ONLINE-B 0.642 (1): ONLINE-B
2 0.606: UEDIN UEDIN UEDIN 3.07: UEDIN 0.603 (2): UEDIN
3 0.530: ONLINE-A CU-BOJAR CU-BOJAR 3.40: CU-BOJAR 0.528 (3-4): ONLINE-A
4 0.530: CU-BOJAR ONLINE-A ONLINE-A 3.40: ONLINE-A 0.528 (3-4): CU-BOJAR
5 0.375: UK UK UK 4.01: UK 0.379 (5): UK
6 0.318: JHU JHU JHU 4.24: JHU 0.320 (6): JHU
Table 22: Overall ranking with different methods (Czech?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.646: ONLINE-A ONLINE-B ONLINE-B 6.35: ONLINE-A 0.647 (1-3): ONLINE-A
2 0.645: ONLINE-B ONLINE-A ONLINE-A 6.44: ONLINE-B 0.642 (1-3): ONLINE-B
3 0.612: QUAERO UEDIN UEDIN 6.94: QUAERO 0.609 (2-5): QUAERO
4 0.599: RBMT-3 QUAERO QUAERO 7.04: RBMT-3 0.600 (2-6): RBMT-3
5 0.597: UEDIN RBMT-3 RBMT-3 7.16: UEDIN 0.593 (3-6): UEDIN
6 0.558: RWTH KIT KIT 7.76: RWTH 0.551 (5-9): RWTH
7 0.545: LIMSI RWTH RWTH 7.83: KIT 0.547 (5-10): KIT
8 0.544: KIT QCRI QCRI 7.85: LIMSI 0.545 (6-10): LIMSI
9 0.524: QCRI RBMT-4 RBMT-4 8.20: QCRI 0.521 (7-11): QCRI
10 0.505: RBMT-1 LIMSI LIMSI 8.40: RBMT-4 0.506 (8-11): RBMT-1
11 0.502: RBMT-4 RBMT-1 RBMT-1 8.42: RBMT-1 0.506 (8-11): RBMT-4
12 0.434: ONLINE-C ONLINE-C ONLINE-C 9.43: ONLINE-C 0.434 (12-13): ONLINE-C
13 0.402: DFKI-BERLIN DFKI-BERLIN DFKI-BERLIN 9.86: DFKI-BERLIN 0.405 (12-14): DFKI-BERLIN
14 0.374: UK UK UK 10.25: UK 0.377 (13-15): UK
15 0.337: JHU JHU JHU 10.81: JHU 0.338 (14-15): JHU
16 0.179: UG UG UG 13.26: UG 0.180 (16): UG
Table 23: Overall ranking with different methods (German?English)
45
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.630: LIMSI LIMSI LIMSI 6.33: LIMSI 0.626 (1-3): LIMSI
2 0.613: KIT CMU CMU 6.55: KIT 0.610 (1-4): KIT
3 0.593: ONLINE-A ONLINE-B ONLINE-B 6.80: ONLINE-A 0.592 (1-5): ONLINE-A
4 0.573: CMU KIT KIT 7.06: CMU 0.571 (2-6): CMU
5 0.569: ONLINE-B ONLINE-A ONLINE-A 7.12: ONLINE-B 0.567 (3-7): ONLINE-B
6 0.546: UEDIN LIUM LIUM 7.51: UEDIN 0.538 (5-8): UEDIN
7 0.523: LIUM RWTH RWTH 7.73: LIUM 0.522 (5-8): LIUM
8 0.515: RWTH UEDIN UEDIN 7.88: RWTH 0.510 (6-9): RWTH
9 0.459: RBMT-1 RBMT-1 RBMT-1 8.51: RBMT-1 0.463 (8-12): RBMT-1
10 0.457: RBMT-3 UK UK 8.56: RBMT-3 0.458 (9-13): RBMT-3
11 0.444: UK SFU SFU 8.75: SFU 0.444 (9-14): SFU
12 0.444: SFU RBMT-3 RBMT-3 8.78: UK 0.441 (9-14): UK
13 0.429: RBMT-4 RBMT-4 RBMT-4 8.92: RBMT-4 0.430 (10-14): RBMT-4
14 0.412: JHU JHU JHU 9.19: JHU 0.409 (12-14): JHU
15 0.321: ONLINE-C ONLINE-C ONLINE-C 10.31: ONLINE-C 0.319 (15): ONLINE-C
Table 24: Overall ranking with different methods (French?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.617: ONLINE-A ONLINE-A ONLINE-A 5.38: ONLINE-A 0.617 (1-4): ONLINE-A
2 0.612: ONLINE-B ONLINE-B ONLINE-B 5.43: ONLINE-B 0.611 (1-4): ONLINE-B
3 0.603: QCRI QCRI QCRI 5.56: QCRI 0.600 (1-4): QCRI
4 0.585: UEDIN UPC UPC 5.75: UEDIN 0.581 (2-5): UEDIN
5 0.565: UPC UEDIN UEDIN 5.89: UPC 0.567 (3-6): UPC
6 0.528: GTH-UPM RBMT-3 RBMT-3 6.29: GTH-UPM 0.526 (5-7): GTH-UPM
7 0.512: RBMT-3 JHU JHU 6.37: RBMT-3 0.518 (6-8): RBMT-3
8 0.477: JHU GTH-UPM GTH-UPM 6.73: JHU 0.480 (7-9): JHU
9 0.461: RBMT-4 RBMT-4 RBMT-4 6.92: RBMT-4 0.460 (8-10): RBMT-4
10 0.423: RBMT-1 ONLINE-C ONLINE-C 7.19: RBMT-1 0.429 (9-11): RBMT-1
11 0.420: ONLINE-C RBMT-1 RBMT-1 7.24: ONLINE-C 0.423 (9-11): ONLINE-C
12 0.189: UK UK UK 9.25: UK 0.188 (12): UK
Table 25: Overall ranking with different methods (Spanish?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.662: CU-DEPFIX CU-DEPFIX CU-DEPFIX 5.25: CU-DEPFIX 0.660 (1): CU-DEPFIX
2 0.628: ONLINE-B ONLINE-B ONLINE-B 5.78: ONLINE-B 0.616 (2): ONLINE-B
3 0.557: UEDIN UEDIN UEDIN 6.42: UEDIN 0.557 (3-6): UEDIN
4 0.555: CU-TAMCH CU-TAMCH CU-TAMCH 6.45: CU-TAMCH 0.555 (3-6): CU-TAMCH
5 0.543: CU-BOJAR CU-BOJAR CU-BOJAR 6.58: CU-BOJAR 0.541 (3-7): CU-BOJAR
6 0.531: CU-TECTOMT CU-TECTOMT CU-TECTOMT 6.69: CU-TECTOMT 0.532 (4-7): CU-TECTOMT
7 0.528: ONLINE-A ONLINE-A ONLINE-A 6.72: ONLINE-A 0.529 (4-7): ONLINE-A
8 0.478: COMMERCIAL1 COMMERCIAL2 COMMERCIAL2 7.27: COMMERCIAL1 0.477 (8-10): COMMERCIAL1
9 0.459: COMMERCIAL2 COMMERCIAL1 COMMERCIAL1 7.46: COMMERCIAL2 0.459 (8-11): COMMERCIAL2
10 0.442: CU-POOR-COMB CU-POOR-COMB CU-POOR-COMB 7.61: CU-POOR-COMB 0.443 (9-11): CU-POOR-COMB
11 0.437: UK UK UK 7.65: UK 0.440 (9-11): UK
12 0.360: SFU SFU SFU 8.40: SFU 0.362 (12): SFU
13 0.326: JHU JHU JHU 8.72: JHU 0.328 (13): JHU
Table 26: Overall ranking with different methods (English?Czech)
46
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.655: LIMSI LIMSI LIMSI 5.98: LIMSI 0.651 (1-2): LIMSI
2 0.615: RWTH RWTH RWTH 6.57: RWTH 0.609 (2-4): RWTH
3 0.595: ONLINE-B ONLINE-B ONLINE-B 6.84: ONLINE-B 0.589 (2-5): ONLINE-B
4 0.590: KIT KIT KIT 6.86: KIT 0.587 (2-5): KIT
5 0.554: LIUM LIUM LIUM 7.36: LIUM 0.550 (4-8): LIUM
6 0.534: UEDIN UEDIN UEDIN 7.67: UEDIN 0.526 (5-9): UEDIN
7 0.516: RBMT-3 RBMT-3 RBMT-3 7.85: RBMT-3 0.514 (5-10): RBMT-3
8 0.513: ONLINE-A ONLINE-A ONLINE-A 7.92: PROMT 0.507 (6-10): ONLINE-A
9 0.506: PROMT PROMT PROMT 7.92: ONLINE-A 0.507 (6-10): PROMT
10 0.483: RBMT-1 RBMT-1 RBMT-1 8.23: RBMT-1 0.483 (8-11): RBMT-1
11 0.436: JHU JHU JHU 8.85: JHU 0.436 (10-12): JHU
12 0.396: UK UK RBMT-4 9.34: RBMT-4 0.397 (11-15): RBMT-4
13 0.394: ONLINE-C RBMT-4 ITS-LATL 9.38: ONLINE-C 0.393 (12-15): ONLINE-C
14 0.394: RBMT-4 ITS-LATL ONLINE-C 9.41: UK 0.391 (12-15): UK
15 0.360: ITS-LATL ONLINE-C UK 9.81: ITS-LATL 0.360 (13-15): ITS-LATL
Table 27: Overall ranking with different methods (English?French)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.648: ONLINE-B ONLINE-B ONLINE-B 4.70: ONLINE-B 0.646 (1): ONLINE-B
2 0.579: RBMT-3 RBMT-3 RBMT-3 5.35: RBMT-3 0.577 (2-4): RBMT-3
3 0.561: ONLINE-A PROMT PROMT 5.49: ONLINE-A 0.561 (2-5): ONLINE-A
4 0.545: PROMT ONLINE-A ONLINE-A 5.66: PROMT 0.542 (3-6): PROMT
5 0.526: UEDIN UPC UPC 5.78: UEDIN 0.528 (4-6): UEDIN
6 0.524: UPC UEDIN UEDIN 5.81: UPC 0.525 (4-6): UPC
7 0.463: RBMT-4 RBMT-1 RBMT-1 6.33: RBMT-4 0.464 (7-9): RBMT-4
8 0.452: RBMT-1 RBMT-4 RBMT-4 6.42: RBMT-1 0.452 (7-9): RBMT-1
9 0.430: ONLINE-C UK ONLINE-C 6.57: ONLINE-C 0.434 (8-10): ONLINE-C
10 0.412: UK ONLINE-C UK 6.73: UK 0.415 (9-10): UK
11 0.357: JHU JHU JHU 7.17: JHU 0.357 (11): JHU
Table 28: Overall ranking with different methods (English?Spanish)
47
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
A
G
A
N
-S
T
S
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
Czech-English News Task
CU-BOJAR 0.17 0.2 39 0.31 44 0.66 0.50 0.21 0.65 0.2 50 639
JHU 0.16 0.18 41 0.28 41 0.63 0.47 0.19 0.65 0.10 53 692
ONLINE-A 0.18 0.21 40 0.31 43 0.68 0.51 0.21 0.62 0.22 50 648
ONLINE-B 0.18 0.23 40 0.30 42 0.67 0.53 0.23 0.59 0.20 52 660
UEDIN 0.18 0.22 39 0.32 45 0.69 0.53 0.23 0.60 0.25 49 627
UK 0.16 0.18 41 0.29 41 0.63 0.49 0.19 0.67 0.17 53 682
Table 29: Automatic evaluation metric scores for systems in the WMT12 Czech-English News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
German-English News Task
DFKI-BERLIN 0.17 0.21 40 0.3 43 0.46 0.18 0.61 0.25 50 653
JHU 0.17 0.2 41 0.29 42 0.42 0.21 0.61 0.20 52 672
KIT 0.18 0.23 39 0.31 45 0.46 0.23 0.58 0.28 49 630
LIMSI 0.18 0.23 39 0.31 45 0.48 0.23 0.6 0.30 49 628
ONLINE-A 0.18 0.21 40 0.32 44 0.50 0.22 0.6 0.27 50 645
ONLINE-B 0.19 0.24 39 0.31 44 0.53 0.24 0.59 0.29 50 636
RBMT-4 0.16 0.16 41 0.29 42 0.44 0.18 0.68 0.24 53 690
RBMT-3 0.16 0.17 40 0.3 42 0.47 0.19 0.66 0.29 52 677
ONLINE-C 0.15 0.14 42 0.28 40 0.43 0.17 0.70 0.26 54 711
RBMT-1 0.15 0.15 43 0.29 40 0.45 0.17 0.69 0.24 54 711
QCRI 0.18 0.23 40 0.31 44 0.46 0.23 0.59 0.26 50 639
QUAERO 0.19 0.24 38 0.32 46 0.49 0.24 0.57 0.3 48 613
RWTH 0.18 0.23 39 0.31 45 0.48 0.24 0.58 0.27 49 626
UEDIN 0.18 0.23 39 0.31 46 0.51 0.23 0.59 0.32 49 630
UG 0.11 0.11 45 0.24 35 0.38 0.14 0.77 0.10 59 768
UK 0.16 0.18 42 0.29 40 0.42 0.2 0.65 0.27 53 683
Table 30: Automatic evaluation metric scores for systems in the WMT12 German-English News Task
48
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
French-English News Task
CMU 0.20 0.29 36 0.34 51 0.54 0.29 0.52 0.25 44 561
JHU 0.19 0.26 37 0.33 47 0.50 0.26 0.54 0.20 46 596
KIT 0.21 0.30 35 0.34 51 0.54 0.3 0.51 0.25 43 551
LIMSI 0.21 0.30 35 0.34 52 0.55 0.3 0.51 0.25 43 546
LIUM 0.20 0.29 36 0.34 50 0.54 0.29 0.52 0.24 44 558
ONLINE-A 0.2 0.27 37 0.34 48 0.52 0.27 0.53 0.24 45 584
ONLINE-B 0.20 0.30 36 0.33 48 0.55 0.29 0.51 0.22 46 582
RBMT-4 0.18 0.20 38 0.32 45 0.49 0.21 0.64 0.15 48 622
RBMT-3 0.18 0.21 39 0.31 46 0.49 0.22 0.61 0.15 48 637
ONLINE-C 0.18 0.19 38 0.31 45 0.45 0.21 0.64 0.10 48 633
RBMT-1 0.18 0.21 39 0.32 47 0.5 0.22 0.62 0.15 48 626
RWTH 0.20 0.29 36 0.34 50 0.53 0.28 0.53 0.20 44 563
SFU 0.2 0.25 37 0.33 48 0.51 0.26 0.54 0.17 46 596
UEDIN 0.20 0.30 35 0.34 51 0.54 0.3 0.51 0.25 43 549
UK 0.19 0.25 38 0.33 47 0.52 0.25 0.57 0.17 47 602
Table 31: Automatic evaluation metric scores for systems in the WMT12 French-English News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
A
G
A
N
-S
T
S
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
Spanish-English News Task
GTH-UPM 0.21 0.29 35 0.35 51 0.7 0.55 0.29 0.51 0.31 43 565
JHU 0.21 0.29 35 0.35 51 0.7 0.56 0.29 0.51 0.31 43 560
ONLINE-A 0.22 0.31 34 0.36 52 0.72 0.58 0.31 0.49 0.36 42 535
ONLINE-B 0.22 0.38 33 0.36 53 0.70 0.60 0.35 0.45 0.35 41 523
RBMT-4 0.19 0.23 36 0.33 49 0.69 0.54 0.24 0.60 0.29 45 591
RBMT-3 0.19 0.23 36 0.33 49 0.69 0.54 0.23 0.60 0.29 45 590
ONLINE-C 0.19 0.22 37 0.33 47 0.68 0.5 0.23 0.61 0.24 46 598
RBMT-1 0.18 0.22 38 0.33 48 0.67 0.52 0.23 0.62 0.23 47 607
QCRI 0.22 0.33 33 0.36 54 0.71 0.6 0.32 0.49 0.32 40 523
UEDIN 0.22 0.33 33 0.36 54 0.71 0.59 0.32 0.48 0.32 40 519
UK 0.18 0.22 37 0.30 44 0.6 0.48 0.23 0.60 0.10 48 634
UPC 0.22 0.32 34 0.36 54 0.71 0.57 0.31 0.49 0.33 41 531
Table 32: Automatic evaluation metric scores for systems in the WMT12 Spanish-English News Task
49
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-Czech News Task
COMMERCIAL-2 0.01 0.08 47 693 0.17 23 0.38 0.1 0.76 0.17 61
CU-BOJAR 0.17 0.13 45 644 0.21 28 0.4 0.13 0.69 0.26 57
CU-DEPFIX 0.19 0.16 44 623 0.22 28 0.45 0.15 0.66 0.30 55
CU-POOR-COMB 0.14 0.12 48 710 0.19 27 0.35 0.12 0.67 0.23 60
CU-TAMCH 0.17 0.13 45 647 0.21 28 0.38 0.13 0.69 0.29 57
CU-TECTOMT 0.16 0.12 48 690 0.19 26 0.36 0.12 0.68 0.22 60
JHU 0.16 0.1 47 691 0.2 23 0.39 0.11 0.69 0.10 60
ONLINE-A 0.17 0.13 n/a n/a 0.21 n/a 0.42 0.13 0.67 0.25 n/a
ONLINE-B 0.19 0.16 44 623 0.21 28 0.45 0.15 0.66 0.30 55
COMMERCIAL-1 0.11 0.09 48 692 0.18 22 0.38 0.10 0.74 0.21 61
SFU 0.15 0.11 47 674 0.19 23 0.39 0.11 0.71 0.21 60
UEDIN 0.18 0.15 45 639 0.21 27 0.41 0.14 0.66 0.40 56
UK 0.15 0.11 47 669 0.19 25 0.39 0.12 0.71 0.35 59
Table 33: Automatic evaluation metric scores for systems in the WMT12 English-Czech News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-German News Task
DFKI-BERLIN 0.18 0.14 46 628 0.35 41 0.13 0.69 0.10 57
DFKI-HUNSICKER 0.18 0.14 45 621 0.35 42 0.15 0.69 0.17 57
JHU 0.2 0.15 45 618 0.37 42 0.16 0.68 0.17 56
KIT 0.20 0.17 45 606 0.38 43 0.17 0.66 0.14 55
LIMSI 0.2 0.17 45 615 0.37 43 0.17 0.65 0.15 56
ONLINE-A 0.20 0.16 45 617 0.38 43 0.17 0.65 0.36 55
ONLINE-B 0.22 0.18 43 589 0.38 42 0.18 0.64 0.35 55
RBMT-4 0.18 0.14 45 623 0.35 42 0.15 0.69 0.35 57
RBMT-3 0.19 0.15 44 608 0.36 44 0.16 0.68 0.37 56
ONLINE-C 0.16 0.11 47 655 0.32 39 0.13 0.74 0.37 60
RBMT-1 0.17 0.13 47 643 0.34 42 0.15 0.70 0.36 58
RWTH 0.2 0.16 44 609 0.37 43 0.16 0.67 0.25 56
UEDIN-WILLIAMS 0.19 0.16 45 628 0.37 43 0.17 0.66 0.33 57
UEDIN 0.20 0.16 45 611 0.37 43 0.17 0.66 0.29 55
UK 0.18 0.14 46 632 0.36 40 0.15 0.71 0.27 58
Table 34: Automatic evaluation metric scores for systems in the WMT12 English-German News Task
50
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-French News Task
ITS-LATL 0.24 0.21 41 548 0.45 48 0.21 0.61 0.15 50
JHU 0.26 0.25 38 511 0.49 51 0.25 0.57 0.15 47
KIT 0.28 0.28 36 480 0.52 55 0.28 0.54 0.22 44
LIMSI 0.28 0.29 36 472 0.52 55 0.28 0.54 0.22 44
LIUM 0.28 0.28 37 480 0.51 54 0.28 0.55 0.20 45
ONLINE-A 0.26 0.25 39 512 0.5 52 0.26 0.57 0.17 47
ONLINE-B 0.24 0.21 36 473 0.48 45 0.26 0.77 0.10 49
RBMT-4 0.24 0.21 40 539 0.46 48 0.22 0.60 0.10 49
RBMT-3 0.26 0.24 39 511 0.48 52 0.24 0.58 0.14 47
ONLINE-C 0.23 0.2 41 550 0.45 50 0.21 0.62 0.10 50
RBMT-1 0.25 0.22 40 531 0.47 51 0.23 0.6 0.13 49
PROMT 0.26 0.24 38 502 0.49 52 0.25 0.58 0.18 46
RWTH 0.28 0.29 36 478 0.52 54 0.28 0.54 0.22 44
UEDIN 0.28 0.28 36 479 0.52 54 0.28 0.55 0.27 45
UK 0.25 0.23 39 523 0.48 51 0.24 0.6 0.17 48
Table 35: Automatic evaluation metric scores for systems in the WMT12 English-French News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-Spanish News Task
JHU 0.29 0.29 37 494 0.54 52 0.29 0.51 0.14 45
ONLINE-A 0.31 0.31 36 475 0.56 54 0.31 0.48 0.2 43
ONLINE-B 0.33 0.36 34 431 0.57 54 0.34 0.48 0.25 42
RBMT-4 0.27 0.24 39 528 0.5 50 0.25 0.55 0.14 48
RBMT-3 0.28 0.26 39 510 0.51 51 0.26 0.54 0.13 46
ONLINE-C 0.26 0.24 40 532 0.5 49 0.25 0.55 0.10 48
RBMT-1 0.26 0.23 40 534 0.50 49 0.25 0.57 0.13 49
PROMT 0.29 0.27 38 497 0.52 52 0.28 0.53 0.18 45
UEDIN 0.31 0.32 35 466 0.56 55 0.32 0.49 0.19 42
UK 0.29 0.28 38 510 0.54 51 0.28 0.52 0.17 46
UPC 0.31 0.32 36 476 0.56 54 0.31 0.49 0.19 43
Table 36: Automatic evaluation metric scores for systems in the WMT12 English-Spanish News Task
51
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 96?103,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Linguistic Features for Quality Estimation
Mariano Felice
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street
Wolverhampton, WV1 1SB, UK
Mariano.Felice@wlv.ac.uk
Lucia Specia
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
L.Specia@dcs.shef.ac.uk
Abstract
This paper describes a study on the contribu-
tion of linguistically-informed features to the
task of quality estimation for machine trans-
lation at sentence level. A standard regression
algorithm is used to build models using a com-
bination of linguistic and non-linguistic fea-
tures extracted from the input text and its ma-
chine translation. Experiments with English-
Spanish translations show that linguistic fea-
tures, although informative on their own, are
not yet able to outperform shallower features
based on statistics from the input text, its
translation and additional corpora. However,
further analysis suggests that linguistic infor-
mation is actually useful but needs to be care-
fully combined with other features in order to
produce better results.
1 Introduction
Estimating the quality of automatic translations is
becoming a subject of increasing interest within the
Machine Translation (MT) community for a num-
ber of reasons, such as helping human translators
post-editing MT, warning users about non-reliable
translations or combining output from multiple MT
systems. Different from most classic approaches for
measuring the progress of an MT system or compar-
ing MT systems, which assess quality by contrast-
ing system output to reference translations such as
BLEU (Papineni et al, 2002), Quality Estimation
(QE) is a more challenging task, aimed at MT sys-
tems in use, and therefore without access to refer-
ence translations.
From the findings of previous work on reference-
dependent MT evaluation, it is clear that metrics
exploiting linguistic information can achieve sig-
nificantly better correlation with human judgments
on quality, particularly at the level of sentences
(Gime?nez and Ma`rquez, 2010). Intuitively, this
should also apply for quality estimation metrics:
while evaluation metrics compare linguistic repre-
sentations of the system output and reference trans-
lations (e.g. matching of n-grams of part-of-speech
tags or predicate-argument structures), quality esti-
mation metrics would perform the (more complex)
comparison og linguistic representations of the input
and translation texts. The hypothesis put forward in
this paper is therefore that using linguistic informa-
tion to somehow contrast the input and translation
texts can be beneficial for quality estimation.
We test this hypothesis as part of the WMT-12
shared task on quality estimation. The system sub-
mitted to this task (WLV-SHEF) integrates linguis-
tic information to a strong baseline system using
only shallow statistics from the input and transla-
tion texts, with no explicit information from the MT
system that produced the translations. A variant
also tests the addition of linguistic information to
a larger set of shallow features. The quality esti-
mation problem is modelled as a supervised regres-
sion task using Support Vector Machines (SVM),
which has been shown to achieve good performance
in previous work (Specia, 2011). Linguistic features
are computed using a number of auxiliary resources
such as parsers and monolingual corpora.
The remainder of this paper is organised as fol-
lows. Section 2 gives an overview of previous work
96
on quality estimation, Section 3 describes the set of
linguistic features proposed in this paper, along with
general experimental settings, Section 4 presents our
evaluation and Section 5 provides conclusions and a
brief discussion of future work.
2 Related Work
Reference-free MT quality assessment was ini-
tially approached as a Confidence Estimation task,
strongly biased towards exploiting data from a Sta-
tistical MT (SMT) system and the translation pro-
cess to model the confidence of the system in the
produced translation. Blatz et al (2004) attempted
sentence-level assessment using a set of 91 features
(from the SMT system input and translation texts)
and automatic annotations such as NIST and WER.
Experiments on classification and regression using
different machine learning techniques produced not
very encouraging results. More successful experi-
ments were later run by Quirk (2004) in a similar
setting but using a smaller dataset with human qual-
ity judgments.
Specia et al (2009a) used Partial Least Squares
regression to jointly address feature selection and
model learning using a similar set of features and
datasets annotated with both automatic and human
scores. Black-box features (i.e. those extracted from
the input and translation texts only) were as discrim-
inative as glass-box features (i.e. those from the MT
system). Later work using black-box features only
focused on finding an appropriate threshold for dis-
criminating ?good? from ?bad? translations for post-
editing purposes (Specia et al, 2009b) and investi-
gating more objective ways of obtaining human an-
notation, such as post-editing time (Specia, 2011).
Recent approaches have started exploiting lin-
guistic information with promising results. Specia
et al (2011), for instance, used part-of-speech (PoS)
tagging, chunking, dependency relations and named
entities for English-Arabic quality estimation. Hard-
meier (2011) explored the use of constituency
and dependency trees for English-Swedish/Spanish
quality estimation. Focusing on word-error detec-
tion through the estimation of WER, Xiong et al
(2010) used PoS tags of neighbouring words and a
link grammar parser to detect words that are not con-
nected to the rest of the sentence. Work by Bach et
al. (2011) focused on learning patterns of linguis-
tic information (such as sequences of part-of-speech
tags) to predict sub-sentence errors. Finally, Pighin
and Ma`rquez (2011) modelled the expected projec-
tions of semantic roles from the input text into the
translations.
3 Method
Our work focuses on the use of a wide range of
linguistic information for representing different as-
pects of translation quality to complement shallow,
system-independent features that have been proved
to perform well in previous work.
3.1 Linguistic features
Non-linguistic features, such as sentence length or
n-gram statistics, are limited in their scope since
they can only account for very shallow aspects of
a translation. They convey no notion of meaning,
grammar or content and as a result they could be
very biased towards describing only superficial as-
pects. For this reason, we introduce linguistic fea-
tures that account for richer aspects of translations
and are in closer relation to the way humans make
their judgments. All of the proposed features, lin-
guistic or not, are MT-system independent.
The proposal of linguistic features was guided by
three main aspects of translation: fidelity, fluency
and coherence. The number of features that were
eventually extracted was inevitably limited by the
availability of suitable tools for the language pair
at hand, mainly for Spanish. As a result, many of
the features that were initially devised could not be
implemented (e.g. grammar checking). A total of
70 linguistic features were extracted, as summarised
below, where S and T indicate whether they refer to
the source/input or translation texts respectively:
? Sentence 3-gram log-probability and perplexity
using a language model (LM) of PoS tags [T]
? Number, percentage and ratio of content words
(N, V, ADJ) and function words (DET, PRON,
PREP, ADV) [S & T]
? Width and depth of constituency and depen-
dency trees for the input and translation texts
and their differences [S & T]
97
? Percentage of nouns, verbs and pronouns in the
sentence and their ratios between [S & T]
? Number and difference in deictic elements in
[S & T]
? Number and difference in specific types of
named entities (person, organisation, location,
other) and the total of named entities [S & T]
? Number and difference in noun, verb and
prepositional phrases [S & T]
? Number of ?dangling? (i.e. unlinked) deter-
miners [T]
? Number of explicit (pronominal, non-
pronominal) and implicit (zero pronoun)
subjects [T]
? Number of split contractions in Spanish (i.e.
al=a el, del=de el) [T]
? Number and percentage of subject-verb dis-
agreement cases [T]
? Number of unknown words estimated using a
spell checker [T]
While many of these features attempt to check
for general errors (e.g. subject verb disagreement),
others are targeted at usual MT errors (e.g. ?dan-
gling? determiners, which are commonly introduced
by SMT systems and are not linked to any words) or
target language peculiarities (e.g. Spanish contrac-
tions, zero subjects). In particular, studying deeper
aspects such as different types of subjects can pro-
vide a good indication of how natural a translation
is in Spanish, which is a pro-drop language. Such a
distinction is expected to spot unnatural expressions,
such as those caused by unnecessary pronoun repe-
tition.1
For subject classification, we identified all VPs
and categorised them according to their preceding
1E.g. (1) The girl beside me was smiling rather brightly.
She thought it was an honor that the exchange student should
be seated next to her. ? *La nin?a a mi lado estaba sonriente
bastante bien. Ella penso? que era un honor que el intercambio
de estudiantes se encuentra pro?ximo a ella. (superfluous)
(2) She is thought to have killed herself through suffocation us-
ing a plastic bag.? *Ella se cree que han matado a ella medi-
ante asfixia utilizando una bolsa de pla?stico. (confusing)
NPs. Thus, explicit subjects were classified as
pronominal (PRON+VP) or non-pronominal (NON-
PRON-NP+VP) while implicit subjects only in-
cluded elided (zero) subjects (i.e. a VP not preceded
by an NP).
Subject-verb agreement cases were estimated by
rules analysing person, number and gender matches
in explicit subject cases, considering also inter-
nal NP agreement between determiners, nouns, ad-
jectives and pronouns.2 Deictics, common coher-
ence indicators (Halliday and Hasan, 1976), were
checked against manually compiled lists.3 Unknown
words were estimated using the JMySpell4 spell
checker with the publicly available Spanish (es ES)
OpenOffice5 dictionary. In order to avoid incorrect
estimates, all named entities were filtered out before
spell-checking.
TreeTagger (Schmid, 1995) was used for PoS tag-
ging of English texts, while Freeling (Padro? et al,
2010) was used for PoS tagging in Spanish and
for constituency parsing, dependency parsing and
named entity recognition in both languages.
In order to compute n-gram statistics over PoS
tags, two language models of general and more
detailed morphosyntactic PoS were built using the
SRILM toolkit (Stolcke, 2002) on the PoS-tagged
AnCora corpus (Taule? et al, 2008).
3.2 Shallow features
In a variant of our system, the linguistic features
were complemented by a set of 77 non-linguistic
features:
? Number and proportion of unique tokens and
numbers in the sentence [S & T]
? Sentence length ratios [S & T]
? Number of non-alphabetical tokens and their
ratios [S & T]
? Sentence 3-gram perplexity [S & T]
2E.g. *Algunas de estas personas se convertira? en he?roes.
(number mismatch), *Barricadas fueron creados en la calle
Cortlandt. (gender mismatch), *Buena mentirosos esta?n cuali-
ficados en lectura. (internal NP gender and number mismatch).
3These included common deictic terms compiled from vari-
ous sources, such as hoy, all??, tu? (Spanish) or that, now or there
(English).
4http://kenai.com/projects/jmyspell
5http://www.openoffice.org/
98
? Type/Token Ratio variations: corrected TTR
(Carroll, 1964), Log TTR (Herdan, 1960),
Guiraud Index (Guiraud, 1954), Uber Index
(Dugast, 1980) and Jarvis TTR (Jarvis, 2002)
[S & T]
? Average token frequency from a monolingual
corpus [S]
? Mismatches in opening and closing brackets
and quotation marks [S & T]
? Differences in brackets, quotation marks, punc-
tuation marks and numbers [S & T]
? Average number of occurrences of all words
within the sentence [T]
? Alignment score (IBM-4) and percentage of
different types of word alignments by GIZA++
(from the SMT training alignment model pro-
vided)
Our basis for comparison is the set of 17 baseline
features, which are shallow MT system-independent
features provided by the WMT-12 QE shared task
organizers.
3.3 Building QE models
We created two main feature sets from the features
listed above for the WMT-12 QE shared task:
WLV-SHEF FS: all features, that is, baseline fea-
tures, shallow features (Section 3.2) and lin-
guistic features (Section 3.1).
WLV-SHEF BL: baseline features and linguistic
features (Section 3.1).
Additionally, we experimented with other variants
of these feature sets using 3-fold cross validation on
the training set, such as only linguistic features and
only non-linguistic features, but these yielded poorer
results and are not reported in this paper.
We address the QE problem as a regression task
by building SVM models with an epsilon regressor
and a radial basis function kernel using the LibSVM
toolkit (Chang and Lin, 2011). Values for the cost,
epsilon and gamma parameters were optimized us-
ing 5-fold cross validation on the training set.
MAE ? RMSE ? Pearson ?
Baseline 0.69 0.82 0.562
WLV-SHEF FS 0.69 0.85 0.514
WLV-SHEF BL 0.72 0.86 0.490
Table 1: Scoring performance
The training sets distributed for the shared task
comprised 1, 832 English sentences taken from news
texts and their Spanish translations produced by an
SMT system, Moses (Koehn et al, 2007), which
had been trained on a concatenation of Europarl and
news-commentaries data (from WMT-10). Transla-
tions were accompanied by a quality score derived
from an average of three human judgments of post-
editing effort using a 1-5 scale.
The models built for each of these two feature
sets were evaluated using the official test set of 422
sentences produced in the same fashion as the train-
ing set. Two sub-tasks were considered: (i) scor-
ing translations using the 1-5 quality scores, and
(ii) ranking translations from best to worse. While
quality scores were directly predicted by our mod-
els, sentence rankings were defined by ordering the
translations according to their predicted scores in de-
scending order, with no additional criteria to resolve
ties other than the natural ordering given by the sort-
ing algorithm.
4 Results and Evaluation
Table 1 shows the official results of our systems in
the scoring task in terms of Mean Absolute Error
(MAE) and Root Mean Squared Error (RMSE), the
metrics used in the shared task, as well as in terms
of Pearson correlation.
Results reveal that our models fall slightly be-
low the baseline, although this drop is not statisti-
cally significant in any of the cases (paired t-tests for
Baseline vs WLV-SHEF FS and Baseline vs WLV-
SHEF BL yield p > 0.05). This may suggest that
for this particular dataset the baseline features al-
ready cover all relevant aspects of quality on their
own, or simply that the representation of the lin-
guistic features is not appropriate for the task. The
quality of the resources used to extract the linguistic
features may also have been an issue. However, a
feature selection method may find a different com-
99
Figure 1: Comparison of true versus predicted scores
bination of features that outperforms the baseline, as
is later described in this section.
A correlation analysis between our predicted
scores and the gold standard (Figure 1) shows some
dispersion, especially for the WLV-SHEF FS set,
with lower Pearson coefficients when compared to
the baseline. The fluctuation of predicted values for
a single score is also very noticeable, spanning more
than one score band in some cases. However, if we
consider the RMSE achieved by our models, we find
that, on average, predictions deviate less than 0.9 ab-
solute points.
A closer look at the score distribution (Figure 2)
reveals our models had some difficulty predicting
scores in the 1-2 range, possibly affected by the
lower proportion of these cases in the training data.
In addition, it is interesting to see that the only sen-
tence with a true score of 1 is predicted as a very
good translation (with a score greater than 3.5). The
reason for this is that the translation has isolated
grammatical segments that our features might regard
as good but it is actually not faithful to the original.6
Although the cause for this behaviour can be traced
to inaccurate tokenisation, this reveals that our fea-
tures assess fidelity only superficially and deeper
semantically-aware indicators should be explored.
Results for the ranking task also fall below the
baseline as shown in Table 2, according to the two
official metrics: DeltaAvg and Spearman rank cor-
relation coefficient.
4.1 Further analysis
At first glance, the performance of our models seems
to indicate that the integration of linguistic infor-
6I won?t give it away. ? *He ganado ? t darle.
Figure 2: Scatter plot of true versus predicted scores
DeltaAvg ? Spearman ?
Baseline 0.55 0.58
WLV-SHEF FS 0.51 0.52
WLV-SHEF BL 0.50 0.49
Table 2: Ranking performance
mation is not beneficial, since both linguistically-
informed feature sets lead to poorer performance as
compared to the baseline feature set, which contains
only shallow, language-independent features. How-
ever, there could be many factors affecting perfor-
mance so further analysis was necessary to assess
their contribution.
Our first analysis focuses on the performance of
individual features. To this end, we built and tested
models using only one feature at a time and repeated
the process afterwards using the full WLV-SHEF FS
set without one feature at a time. In Table 3 we re-
port the 5-best and 5-worst performing features. Al-
though purely statistical features lead the rank, lin-
guistic features also appear among the top five (as
indicated by L?), showing that they can be as good
as other shallow features. It is interesting to note that
a few features appear as the top performing in both
columns (e.g. source bigrams in 4th frequency quar-
tile and target LM probability). These constitute the
truly top performing features.
Our second analysis studies the optimal subset of
features that would yield the best performance on the
test set, from which we could draw further conclu-
sions. Since this analysis requires training and test-
ing models using all the possible partitions of the
100
Rank One feature All but one feature
1 Source bigrams in 4th freq. quartile Source average token length
2 Source LM probability Source bigrams in 4th freq. quartile
3 Target LM probability Unknown words in target L?
4 Number of source bigrams Target LM probability
5 Target PoS LM probability L? Difference in constituency tree width L?
143 Percentage of target S-V agreement L? Difference in number of periods
144 Source trigrams in 2nd freq. quartile Number of source bigrams
145 Target location entities L? Target person entities L?
146 Source trigrams in 3rd freq. quartile Target Corrected TTR
147 Source average translations by inv. freq. Source trigrams in 3rd freq. quartile
Table 3: List of best and worst performing features
full feature set,7 it is infeasible in practice so we
adopted the Sequential Forward Selection method
instead (Alpaydin, 2010). Using this method, we
start from an empty set and add one feature at a time,
keeping in the set only the features that decrease the
error until no further improvement is possible. This
strategy decreases the number of iterations substan-
tially8 but it does not guarantee finding a global op-
timum. Still, a local optimum was acceptable for
our purpose. The optimal feature set found by our
selection algorithm is shown in Table 4.
Error rates are lower when using this optimal fea-
ture set (MAE=0.62 and RMSE=0.76) but the differ-
ence is only statistically significant when compared
to the baseline with 93% confidence level (paired t-
test with p <= 0.07). However, this analysis allows
us to see how many linguistic features get selected
for the optimal feature set.
Out of the total 37 features in the optimal set,
15 are linguistic (40.5%), showing that they are in
fact informative when strategically combined with
other shallow indicators. This also reveals that fea-
ture selection is a key issue for building a quality
estimation system that combines linguistic and shal-
low information. Using a sequential forward selec-
tion method, the optimal set is composed of both lin-
guistic and shallow features, reinforcing the idea that
they account for different aspects of quality and are
not interchangeable but actually complementary.
7For 147 features: 2147
8For 147 features, worst case is 147 ? (147 + 1)/2 =
10, 878.
5 Conclusions and Future Work
We have explored the use of linguistic informa-
tion for quality estimation of machine translations.
Our approach was not able to outperform a baseline
with only shallow features. However, further feature
analysis revealed that linguistic features are comple-
mentary to shallow features and must be strategi-
cally combined in order to be exploited efficiently.
The availability of linguistic tools for processing
Spanish is limited, and thus the linguistic features
used here only account for a few of the many aspects
involved in translation quality. In addition, comput-
ing linguistic information is a challenging process
for a number of reasons, mainly the fact that trans-
lations are often ungrammatical, and thus linguistic
processors may return inaccurate results, leading to
further errors.
In future work we plan to integrate more global
linguistic features such as grammar checkers, along
with deeper features such as semantic roles, hybrid
n-grams, etc. In addition, we have noticed that rep-
resenting information for input and translation texts
independently seems more appropriate than con-
trasting input and translation information within the
same feature. This representation issue is somehow
counter-intuitive and is yet to be investigated.
Acknowledgements
This research was supported by the European Com-
mission, Education & Training, Eramus Mundus:
EMMC 2008-0083, Erasmus Mundus Masters in
NLP & HLT programme.
101
Iter. Feature
1 Source bigrams in 4th frequency quartile
2 Target PoS LM probability L?
3 Source average token length
4 Guiraud Index of T
5 Unknown words in T L?
6 Difference in number of VPs between S and T L?
7 Diff. in constituency trees width of S and T L?
8 Non-alphabetical tokens in T
9 Ratio of length between S and T
10 Source trigrams in 4th frequency quartile
11 Number of content words in S L?
12 Source 3-gram perplexity
13 Ratio of PRON percentages in S and T L?
14 Number of NPs in T L?
15 Average number of source token translations with
p > 0.05 weighted by frequency
16 Source 3-gram LM probability
17 Target simple PoS LM probability L?
18 Difference in dependency trees depth of S and T L?
19 Number of NPs in S L?
20 Number of tokens in S
21 Number of content words in T L?
22 Source unigrams in 3rd frequency quartile
23 Source unigrams in 1st frequency quartile
24 Source unigrams in 2nd frequency quartile
25 Average number of source token translations with
p > 0.01 weighted by frequency
26 Ratio of non-alpha tokens in S and T
27 Difference of question marks between S and T nor-
malised by T length
28 Percentage of pron subjects in T L?
29 Percentage of verbs in T L?
30 Constituency trees width for S L?
31 Absolute diff. of question marks between S and T
32 Average num. of source token trans. with p > 0.2
33 Diff. of person entities between S and T L?
34 Diff. of periods between S and T norm. by T length
35 Diff. of semicolons between S and T normalised by
T length
36 Source 3-gram perplexity without end-of-sentence
markers
37 Absolute difference of periods between S and T
Table 4: An optimal set of features for the test set. The
number of iteration indicates the order in which features
were selected, giving a rough ranking of features by their
performance.
References
Ethem Alpaydin. 2010. Introduction to Machine Learn-
ing. Adaptive Computation and Machine Learning.
The MIT Press, Cambridge, MA, 2nd edition.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011.
Goodness: A method for measuring machine transla-
tion confidence. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 211?219,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation
for machine translation. Final Report of Johns Hop-
kins 2003 Summer Workshop on Speech and Lan-
guage Engineering, Johns Hopkins University, Balti-
more, Maryland, USA, March.
John Bissell Carroll. 1964. Language and Thought.
Prentice-Hall, Englewood Cliffs, NJ.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):1?27, May.
Daniel Dugast. 1980. La statistique lexicale. Slatkine,
Gene`ve.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Linguistic
measures for automatic machine translation evalua-
tion. Machine Translation, 24(3):209?240.
Pierre Guiraud. 1954. Les Caracte`res Statistiques du
Vocabulaire. Presses Universitaires de France, Paris.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Christian Hardmeier. 2011. Improving machine transla-
tion quality prediction with syntactic tree kernels. In
Proceedings of the 15th conference of the European
Association for Machine Translation (EAMT 2011),
pages 233?240, Leuven, Belgium.
Gustav Herdan. 1960. Type-token Mathematics: A Text-
book of Mathematical Linguistics. Mouton & Co., The
Hague.
Scott Jarvis. 2002. Short texts, best-fitting curves and
new measures of lexical diversity. Language Testing,
19(1):57?84, January.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
102
Llus Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling
2.1: Five years of open-source language process-
ing tools. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Daniele Pighin and Llu??s Ma`rquez. 2011. Automatic
projection of semantic structures: an application to
pairwise translation ranking. In Fifth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation (SSST-5), Portland, Oregon.
Christopher B. Quirk. 2004. Training a sentence-level
machine translation confidence metric. In Proceed-
ings of the International Conference on Language Re-
sources and Evaluation, volume 4 of LREC 2004,
pages 825?828, Lisbon, Portugal.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to german. In In Pro-
ceedings of the ACL SIGDAT-Workshop, pages 47?50,
Dublin, Ireland, August.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009a. Estimating
the sentence-level quality of machine translation sys-
tems. In Proceedings of the 13th Annual Conference
of the European Association for Machine Translation
(EAMT), pages 28?35, Barcelona, Spain, May.
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009b. Improv-
ing the confidence of machine translation quality esti-
mates. In Proceedings of the Twelfth Machine Transla-
tion Summit (MT Summit XII), pages 136?143, Ottawa,
Canada, August.
Lucia Specia, Najeh Hajlaoui, Catalina Hallett, and
Wilker Aziz. 2011. Predicting machine translation
adequacy. In Machine Translation Summit XIII, pages
19?23, Xiamen, China, September.
Lucia Specia. 2011. Exploiting objective annotations for
measuring translation post-editing effort. In Proceed-
ings of the 15th Conference of the European Associa-
tion for Machine Translation, pages 73?80, Leuven.
Andreas Stolcke. 2002. Srilman extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Process-
ing (ICSLP 2002), volume 2, pages 901?904, Denver,
USA, November.
Mariona Taule?, M. Antnia Mart??, and Marta Recasens.
2008. Ancora: Multilevel annotated corpora for cata-
lan and spanish. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC?08), Marrakech, Morocco, May.
European Language Resources Association (ELRA).
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Er-
ror detection for statistical machine translation using
linguistic features. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 604?611, Uppsala, Sweden.
103
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Findings of the 2013 Workshop on Statistical Machine Translation
Ondr?ej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Chris Callison-Burch
University of Pennsylvania
Christian Federmann
Saarland University
Barry Haddow
University of Edinburgh
Philipp Koehn
University of Edinburgh
Christof Monz
University of Amsterdam
Matt Post
Johns Hopkins University
Radu Soricut
Google
Lucia Specia
University of Sheffield
Abstract
We present the results of the WMT13
shared tasks, which included a translation
task, a task for run-time estimation of ma-
chine translation quality, and an unoffi-
cial metrics task. This year, 143 machine
translation systems were submitted to the
ten translation tasks from 23 institutions.
An additional 6 anonymized systems were
included, and were then evaluated both au-
tomatically and manually, in our largest
manual evaluation to date. The quality es-
timation task had four subtasks, with a to-
tal of 14 teams, submitting 55 entries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2013. This workshop builds
on seven previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al, 2007, 2008,
2009, 2010, 2011, 2012).
This year we conducted three official tasks: a
translation task, a human evaluation of transla-
tion results, and a quality estimation task.1 In
the translation task (?2), participants were asked
to translate a shared test set, optionally restrict-
ing themselves to the provided training data. We
held ten translation tasks this year, between En-
glish and each of Czech, French, German, Span-
ish, and Russian. The Russian translation tasks
were new this year, and were also the most popu-
lar. The system outputs for each task were evalu-
ated both automatically and manually.
The human evaluation task (?3) involves ask-
ing human judges to rank sentences output by
anonymized systems. We obtained large numbers
of rankings from two groups: researchers (who
1The traditional metrics task is evaluated in a separate pa-
per (Macha?c?ek and Bojar, 2013).
contributed evaluations proportional to the number
of tasks they entered) and workers on Amazon?s
Mechanical Turk (who were paid). This year?s ef-
fort was our largest yet by a wide margin; we man-
aged to collect an order of magnitude more judg-
ments than in the past, allowing us to achieve sta-
tistical significance on the majority of the pairwise
system rankings. This year, we are also clustering
the systems according to these significance results,
instead of presenting a total ordering over systems.
The focus of the quality estimation task (?6)
is to produce real-time estimates of sentence- or
word-level machine translation quality. This task
has potential usefulness in a range of settings, such
as prioritizing output for human post-editing, or
selecting the best translations from a number of
systems. This year the following subtasks were
proposed: prediction of percentage of word edits
necessary to fix a sentence, ranking of up to five al-
ternative translations for a given source sentence,
prediction of post-editing time for a sentence, and
prediction of word-level scores for a given trans-
lation (correct/incorrect and types of edits). The
datasets included English-Spanish and German-
English news translations produced by a number
of machine translation systems. This marks the
second year we have conducted this task.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation methodologies for machine trans-
lation. As before, all of the data, translations,
and collected human judgments are publicly avail-
able.2 We hope these datasets serve as a valu-
able resource for research into statistical machine
translation, system combination, and automatic
evaluation or prediction of translation quality.
2http://statmt.org/wmt13/results.html
1
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and five other lan-
guages: German, Spanish, French, Czech, and ?
new this year ? Russian. We created a test set for
each language pair by translating newspaper arti-
cles and provided training data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources. A total of 52
articles were selected, in roughly equal amounts
from a variety of Czech, English, French, German,
Spanish, and Russian news sites:3
Czech: aktua?lne?.cz (1), CTK (1), den??k (1),
iDNES.cz (3), lidovky.cz (1), Novinky.cz (2)
French: Cyber Presse (3), Le Devoir (1), Le
Monde (3), Liberation (2)
Spanish: ABC.es (2), BBC Spanish (1), El Peri-
odico (1), Milenio (3), Noroeste (1), Primera
Hora (3)
English: BBC (2), CNN (2), Economist (1),
Guardian (1), New York Times (2), The Tele-
graph (1)
German: Der Standard (1), Deutsche Welle (1),
FAZ (1), Frankfurter Rundschau (2), Welt (2)
Russian: AIF (2), BBC Russian (2), Izvestiya (1),
Rosbalt (1), Vesti (1)
The stories were translated by the professional
translation agency Capita, funded by the EU
Framework Programme 7 project MosesCore, and
by Yandex, a Russian search engine.4 All of the
translations were done directly, and not via an in-
termediate language.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
train language models, and development sets to
tune system parameters. Some training corpora
were identical from last year (Europarl5, United
Nations, French-English 109 corpus, CzEng),
some were updated (News Commentary, mono-
lingual data), and new corpora were added (Com-
mon Crawl (Smith et al, 2013), Russian-English
3For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
4http://www.yandex.com/
5As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
parallel data provided by Yandex, Russian-English
Wikipedia Headlines provided by CMU).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their en-
try names are listed in Table 1; each system did
not necessarily appear in all translation tasks. We
also included three commercial off-the-shelf MT
systems and three online statistical MT systems,6
which we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
We run the evaluation campaign using an up-
dated version of Appraise (Federmann, 2012); the
tool has been extended to support collecting judg-
ments using Amazon?s Mechanical Turk, replac-
ing the annotation system used in previous WMTs.
The software, including all changes made for this
year?s workshop, is available from GitHub.7
This year differs from prior years in a few im-
portant ways:
? We collected about ten times more judgments
that we have in the past, using judgments
from both participants in the shared task and
non-experts hired on Amazon?s Mechanical
Turk.
? Instead of presenting a total ordering of sys-
tems for each pair, we cluster them and report
a ranking over the clusters.
6Thanks to Herve? Saint-Amand and Martin Popel for har-
vesting these entries.
7https://github.com/cfedermann/Appraise
2
Europarl Parallel Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,965,734 2,007,723 1,920,209 646,605
Words 56,895,229 54,420,026 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 176,258 117,481 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 174,441 157,168 178,221 140,324 150,217
Words 5,116,388 4,520,796 4,928,135 4,066,721 4,597,904 4,541,058 3,206,423 3,507,249 3,841,950 4,008,949
Distinct words 84,273 61,693 69,028 58,295 142,461 61,761 138,991 54,270 145,997 57,991
Common Crawl Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 1,845,286 3,244,152 2,399,123 161,838 878,386
Words 49,561,060 46,861,758 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 710,755 640,778 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
Spanish? English French? English
Sentences 11,196,913 12,886,831
Words 318,788,686 365,127,098 411,916,781 360,341,450
Distinct words 593,567 581,339 565,553 666,077
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English
Sentences 514,859
Words 1,191,474 1,230,644
Distinct words 282,989 251,328
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,218,201 2,123,835 2,190,579 2,176,537 668,595
Words 59,848,044 60,476,282 63,439,791 53,534,167 14,946,399
Distinct words 123,059 181,837 145,496 394,781 172,461
News Language Model Data
English Spanish French German Czech Russian
Sentence 68,521,621 13,384,314 21,195,476 54,619,789 27,540,749 19,912,911
Words 1,613,778,461 386,014,234 524,541,570 983,818,841 456,271,247 351,595,790
Distinct words 3,392,137 1,163,825 1,590,187 6,814,953 2,655,813 2,195,112
News Test Set
English Spanish French German Czech Russian
Sentences 3000
Words 64,810 73,659 73,659 63,412 57,050 58,327
Distinct words 8,935 10,601 11,441 12,189 15,324 15,736
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
3
ID Institution
BALAGUR Yandex School of Data Analysis (Borisov et al, 2013)
CMU
CMU-TREE-TO-TREE
Carnegie Mellon University (Ammar et al, 2013)
CU-BOJAR,
CU-DEPFIX,
CU-TAMCHYNA
Charles University in Prague (Bojar et al, 2013)
CU-KAREL, CU-ZEMAN Charles University in Prague (B??lek and Zeman, 2013)
CU-PHRASEFIX,
CU-TECTOMT
Charles University in Prague (Galus?c?a?kova? et al, 2013)
DCU Dublin City University (Rubino et al, 2013a)
DCU-FDA Dublin City University (Bicici, 2013a)
DCU-OKITA Dublin City University (Okita et al, 2013)
DESRT Universita` di Pisa (Miceli Barone and Attardi, 2013)
ITS-LATL University of Geneva
JHU Johns Hopkins University (Post et al, 2013)
KIT Karlsruhe Institute of Technology (Cho et al, 2013)
LIA Universite? d?Avignon (Huet et al, 2013)
LIMSI LIMSI (Allauzen et al, 2013)
MES-* Munich / Edinburgh / Stuttgart (Durrani et al, 2013a; Weller et al, 2013)
OMNIFLUENT SAIC (Matusov and Leusch, 2013)
PROMT PROMT Automated Translations Solutions
QCRI-MES Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al, 2013)
QUAERO QUAERO (Peitz et al, 2013a)
RWTH RWTH Aachen (Peitz et al, 2013b)
SHEF University of Sheffield
STANFORD Stanford University (Green et al, 2013)
TALP-UPC TALP Research Centre (Formiga et al, 2013a)
TUBITAK TU?BI?TAK-BI?LGEM (Durgar El-Kahlout and Mermer, 2013)
UCAM University of Cambridge (Pino et al, 2013)
UEDIN,
UEDIN-HEAFIELD
University of Edinburgh (Durrani et al, 2013b)
UEDIN-SYNTAX University of Edinburgh (Nadejde et al, 2013)
UMD University of Maryland (Eidelman et al, 2013)
UU Uppsala University (Stymne et al, 2013)
COMMERCIAL-1,2,3 Anonymized commercial systems
ONLINE-A,B,G Anonymized online systems
Table 1: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
4
3.1 Ranking translations of sentences
The ranking among systems is produced by col-
lecting a large number of rankings between the
systems? translations. Every language task had
many participating systems (the largest was 19,
for the Russian-English task). Rather than asking
judges to provide a complete ordering over all the
translations of a source segment, we instead ran-
domly select five systems and ask the judge to rank
just those. We call each of these a ranking task.
A screenshot of the ranking interface is shown in
Figure 2.
For each ranking task, the judge is presented
with a source segment, a reference translation,
and the outputs of five systems (anonymized and
randomly-ordered). The following simple instruc-
tions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
The rankings of the systems are numbered from 1
to 5, with 1 being the best translation and 5 be-
ing the worst. Each ranking task has the potential
to provide 10 pairwise rankings, and fewer if the
judge chooses any ties. For example, the ranking
{A:1, B:2, C:4, D:3, E:5}
provides 10 pairwise rankings, while the ranking
{A:3, B:3, C:4, D:3, E:1}
provides just 7. The absolute value of the ranking
or the degree of difference is not considered.
We use the collected pairwise rankings to assign
each system a score that reflects how highly that
system was usually ranked by the annotators. The
score for some system A reflects how frequently it
was judged to be better than other systems when
compared on the same segment; its score is the
number of pairwise rankings where it was judged
to be better, divided by the total number of non-
tying pairwise comparisons. These scores were
used to compute clusters of systems and rankings
between them (?3.4).
3.2 Collecting the data
A goal this year was to collect enough data to
achieve statistical significance in the rankings. We
distributed the workload among two groups of
judges: researchers and Turkers. The researcher
group comprised partipants in the shared task, who
were asked to contribute judgments on 300 sen-
tences for each system they contributed. The re-
searcher evaluation was held over three weeks
from May 17?June 7, and yielded about 280k pair-
wise rankings.
The Turker group was composed of non-expert
annotators hired on Amazon?s Mechanical Turk
(MTurk). A basic unit of work on MTurk is called
a Human Intelligence Task (HIT) and included
three ranking tasks, for which we paid $0.25. To
ensure that the Turkers provided high quality an-
notations, this portion of the evaluation was be-
gun after the researcher portion had completed,
enabling us to embed controls in the form of high-
consensus pairwise rankings in the Turker HITs.
To build these controls, we collected ranking tasks
containing pairwise rankings with a high degree of
researcher consensus. An example task is here:
SENTENCE 504
SOURCE Vor den heiligen Sta?tten verbeugen
REFERENCE Let?s worship the holy places
SYSTEM A Before the holy sites curtain
SYSTEM B Before we bow to the Holy Places
SYSTEM C To the holy sites bow
SYSTEM D Bow down to the holy sites
SYSTEM E Before the holy sites pay
MATRIX
A B C D E
A - 0 0 0 3
B 5 - 0 1 5
C 6 6 - 0 6
D 6 8 5 - 6
E 0 0 0 0 -
Matrix entry Mi,j records the number of re-
searchers who judged System i to be better than
System j. We use as controls pairwise judgments
for which |Mi,j?Mj,i| > 5, i.e., judgments where
the researcher consensus ran strongly in one direc-
tion. We rejected HITs from Turkers who encoun-
tered at least 10 of these controls and failed more
than 50% of them.
There were 463 people who participated in the
Turker portion of the manual evaluation, contribut-
ing 664k pairwise rankings from Turkers who
passed the controls. Together with the researcher
judgments, we collected close to a million pair-
wise rankings, compared to 101k collected last
year: a ten-fold increase. Table 2 contains more
detail.
5
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly-ordered) and has to rank
these according to their translation quality, ties are allowed. For technical reasons, annotators on Amazon?s Mechanical Turk
received all three ranking tasks for a single HIT on a single page, one upon the other.
3.3 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960), which is de-
fined as
? = P (A)? P (E)1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. Note that ? is ba-
sically a normalized version of P (A), one which
takes into account how meaningful it is for anno-
tators to agree with each other, by incorporating
P (E). The values for ? range from 0 to 1, with
zero indicating no agreement and 1 perfect agree-
ment.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it should capture the probability
that two annotators would agree randomly. There-
fore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 3 gives ? values for inter-annotator agree-
ment for WMT11?WMT13 while Table 4 de-
tails intra-annotator agreement scores. Due to the
change of annotation software, we used a slightly
different way of computing annotator agreement
scores. Therefore, we chose to re-compute values
for previous WMTs to allow for a fair comparison.
The exact interpretation of the kappa coefficient is
difficult, but according to Landis and Koch (1977),
0?0.2 is slight, 0.2?0.4 is fair, 0.4?0.6 is moderate,
6
LANGUAGE PAIR Systems Rankings Average
Czech-English 11 85,469 7,769.91
English-Czech 12 102,842 8,570.17
German-English 17 128,668 7,568.71
English-German 15 77,286 5,152.40
Spanish-English 12 67,832 5,652.67
English-Spanish 13 60,464 4,651.08
French-English 13 80,741 6,210.85
English-French 17 100,783 5,928.41
Russian-English 19 151,422 7,969.58
English-Russian 14 87,323 6,237.36
Total 148 942,840 6,370.54
WMT12 103 101,969 999.69
WMT11 133 63,045 474.02
Table 2: Amount of data collected in the WMT13 manual evaluation. The final two rows report summary information from the
previous two workshops.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.400 0.311 0.244 0.342 0.279
English-Czech 0.460 0.359 0.168 0.408 0.075
German-English 0.324 0.385 0.299 0.443 0.324
English-German 0.378 0.356 0.267 0.457 0.239
Spanish-English 0.494 0.298 0.277 0.415 0.295
English-Spanish 0.367 0.254 0.206 0.333 0.249
French-English 0.402 0.272 0.275 0.405 0.321
English-French 0.406 0.296 0.231 0.434 0.237
Russian-English ? ? 0.278 0.315 0.324
English-Russian ? ? 0.243 0.416 0.207
Table 3: ? scores measuring inter-annotator agreement. The WMT13r and WMT13m columns provide breakdowns for re-
searcher annotations and MTurk annotations, respectively. See Table 4 for corresponding intra-annotator agreement scores.
0.6?0.8 is substantial, and 0.8?1.0 is almost per-
fect. We find that the agreement rates are more or
less the same as in prior years.
The WMT13 column contains both researcher
and Turker annotations at a roughly 1:2 ratio. The
final two columns break out agreement numbers
between these two groups. The researcher agree-
ment rates are similar to agreement rates from past
years, while the Turker agreement are well below
researcher agreement rates, varying widely, but of-
ten comparable to WMT11 and WMT12. Clearly,
researchers are providing us with more consistent
opinions, but whether these differences are ex-
plained by Turkers racing through jobs, the partic-
ularities that inform researchers judging systems
they know well, or something else, is hard to tell.
Intra-annotator agreement scores are also on par
from last year?s level, and are often much better.
We observe better intra-annotator agreement for
researchers compared to Turkers.
As a small test, we varied the threshold of ac-
ceptance against the controls for the Turker data
alone and computed inter-annotator agreement
scores on the datasets for the Russian?English task
(the only language pair where we had enough data
at high thresholds). Table 5 shows that higher
thresholds do indeed give us better agreements,
but not monotonically. The increasing ?s sug-
gests that we can find a segment of Turkers who
do a better job and that perhaps a slightly higher
threshold of 0.6 would serve us better, while the
remaining difference against the researchers sug-
gests there may be different mindsets informing
the decisions. In any case, getting the best perfor-
mance out of the Turkers remains difficult.
3.4 System Score
Given the multitude of pairwise comparisons, we
would like to rank the systems according to a
single score computed for each system. In re-
7
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.597 0.454 0.479 0.483 0.478
English-Czech 0.601 0.390 0.290 0.547 0.242
German-English 0.576 0.392 0.535 0.643 0.515
English-German 0.528 0.433 0.498 0.649 0.452
Spanish-English 0.574 1.000 0.575 0.605 0.537
English-Spanish 0.426 0.329 0.492 0.468 0.492
French-English 0.673 0.360 0.578 0.585 0.565
English-French 0.524 0.414 0.495 0.630 0.486
Russian-English ? ? 0.450 0.363 0.477
English-Russian ? ? 0.513 0.582 0.500
Table 4: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation. The WMT13r and WMT13m columns provide breakdowns for researcher annotations and MTurk annota-
tions, respectively. The perfect inter-annotator agreement for Spanish-English is a result of there being very little data for that
language pair.
thresh. rankings ?
0.5 16,605 0.234
0.6 9,999 0.337
0.7 3,219 0.360
0.8 1,851 0.395
0.9 849 0.336
Table 5: Agreement as a function of threshold for Turkers on
the Russian?English task. The threshold is the percentage of
controls a Turker must pass for her rankings to be accepted.
cent evaluation campaigns, we tweaked the metric
and now arrived at a intuitive score that has been
demonstrated to be accurate in ranking systems ac-
cording to their true quality (Koehn, 2012).
The score, which we call EXPECTED WINS, has
an intuitive explanation. If the system is compared
against a randomly picked opposing system, on a
randomly picked sentence, by a randomly picked
judge, what is the probability that its translation is
ranked higher?
Formally, the score for a system Si among a set
of systems {Sj} given a pool of pairwise rankings
summarized as win(A,B) ? the number of times
system A is ranked higher than system B ? is
defined as follows:
score(Si) = 1|{Sj}|
?
j,j 6=i
win(Si, Sj)
win(Si, Sj) + win(Sj , Si)
Note that this score ignores ties.
3.5 Rank Ranges and Clusters
Given the scores, we would like to rank the sys-
tems, which is straightforward. But we would also
like to know, if the obtained system ranking is
statistically significant. Typically, given the large
number of systems that participate, and the simi-
larity of the systems given a common training data
condition and often common toolsets, there will be
some systems that will be very close in quality.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute the expected wins score for each system
based on this sample, and rank each system. By
repeating this procedure a 1,000 times, we can de-
termine a range of ranks, into which system falls
at least 95% of the time (i.e., at least 950 times) ?
corresponding to a p-level of p ? 0.05.
Furthermore, given the rank ranges for each sys-
tem, we can cluster systems with overlapping rank
ranges.8
For all language pairs and all systems, Table 6
reports all system scores, rank ranges, and clus-
ters. The official interpretation of these results
is that systems in the same cluster are considered
tied. Given the large number of judgements that
we collected, it was possible to group on average
about two systems in a cluster, even though the
systems in the middle are typically in larger clus-
ters.
8Formally, given ranges defined by start(Si) and end(Si),
we seek the largest set of clusters {Cc} that satisfies:
?S ?C : S ? C
S ? Ca, S ? Cb ? Ca = Cb
Ca 6= Cb ? ?Si ? Ca, Sj ? Cb :
start(Si) > end(Sj) or start(Sj) > end(Si)
8
Czech-English
# score range system
1 0.607 1 UEDIN-HEAFIELD
2 0.582 2-3 ONLINE-B
0.573 2-4 MES
0.562 3-5 UEDIN
0.547 4-7 ONLINE-A
0.542 5-7 UEDIN-SYNTAX
0.534 6-7 CU-ZEMAN
8 0.482 8 CU-TAMCHYNA
9 0.458 9 DCU-FDA
10 0.321 10 JHU
11 0.297 11 SHEF-WPROA
English-Czech
# score range system
1 0.580 1-2 CU-BOJAR
0.578 1-2 CU-DEPFIX
3 0.562 3 ONLINE-B
4 0.525 4 UEDIN
5 0.505 5-7 CU-ZEMAN
0.502 5-7 MES
0.499 5-8 ONLINE-A
0.484 7-9 CU-PHRASEFIX
0.476 8-9 CU-TECTOMT
10 0.457 10-11 COMMERCIAL-1
0.450 10-11 COMMERCIAL-2
12 0.389 12 SHEF-WPROA
Spanish-English
# score range system
1 0.624 1 UEDIN-HEAFIELD
2 0.595 2 ONLINE-B
3 0.570 3-5 UEDIN
0.570 3-5 ONLINE-A
0.567 3-5 MES
6 0.537 6 LIMSI-SOUL
7 0.514 7 DCU
8 0.488 8-9 DCU-OKITA
0.484 8-9 DCU-FDA
10 0.462 10 CU-ZEMAN
11 0.425 11 JHU
12 0.169 12 SHEF-WPROA
English-Spanish
# rank range system
1 0.637 1 ONLINE-B
2 0.582 2-4 ONLINE-A
0.578 2-4 UEDIN
0.567 3-4 PROMT
5 0.535 5-6 MES
0.528 5-6 TALP-UPC
7 0.491 7-8 LIMSI
0.474 7-9 DCU
0.472 8-10 DCU-FDA
0.455 9-11 DCU-OKITA
0.446 10-11 CU-ZEMAN
12 0.417 12 JHU
13 0.324 13 SHEF-WPROA
German-English
# rank range system
1 0.660 1 ONLINE-B
2 0.620 2-3 ONLINE-A
0.608 2-3 UEDIN-SYNTAX
4 0.586 4-5 UEDIN
0.584 4-5 QUAERO
0.571 5-7 KIT
0.562 6-7 MES
8 0.543 8-9 RWTH-JANE
0.533 8-10 MES-REORDER
0.526 9-10 LIMSI-SOUL
11 0.480 11 TUBITAK
12 0.462 12-13 UMD
0.462 12-13 DCU
14 0.396 14 CU-ZEMAN
15 0.367 15 JHU
16 0.311 16 SHEF-WPROA
17 0.238 17 DESRT
English-German
# rank range system
1 0.637 1-2 ONLINE-B
0.636 1-2 PROMT
3 0.614 3 UEDIN-SYNTAX
0.587 3-5 ONLINE-A
0.571 4-6 UEDIN
0.554 5-6 KIT
7 0.523 7 STANFORD
8 0.507 8 LIMSI-SOUL
9 0.477 9-11 MES-REORDER
0.476 9-11 JHU
0.460 10-12 CU-ZEMAN
0.453 11-12 TUBITAK
13 0.361 13 UU
14 0.329 14-15 SHEF-WPROA
0.323 14-15 RWTH-JANE
English-Russian
# rank range system
1 0.641 1 PROMT
2 0.623 2 ONLINE-B
3 0.556 3-4 CMU
0.542 3-6 ONLINE-G
0.538 3-7 ONLINE-A
0.531 4-7 UEDIN
0.520 5-7 QCRI-MES
8 0.498 8 CU-KAREL
9 0.478 9-10 MES-QCRI
0.469 9-10 JHU
11 0.434 11-12 COMMERCIAL-3
0.426 11-13 LIA
0.419 12-13 BALAGUR
14 0.331 14 CU-ZEMAN
French-English
# rank range system
1 0.638 1 UEDIN-HEAFIELD
2 0.604 2-3 UEDIN
0.591 2-3 ONLINE-B
4 0.573 4-5 LIMSI-SOUL
0.562 4-5 KIT
0.541 5-6 ONLINE-A
7 0.512 7 MES-SIMPLIFIED
8 0.486 8 DCU
9 0.439 9-10 RWTH
0.429 9-11 CMU-T2T
0.420 10-11 CU-ZEMAN
12 0.389 12 JHU
13 0.322 13 SHEF-WPROA
English-French
# rank range system
1 0.607 1-2 UEDIN
0.600 1-3 ONLINE-B
0.588 2-4 LIMSI-SOUL
0.584 3-4 KIT
5 0.553 5-7 PROMT
0.551 5-8 STANFORD
0.547 5-8 MES
0.537 6-9 MES-INFLECTION
0.533 7-10 RWTH-PB
0.516 9-11 ONLINE-A
0.499 10-11 DCU
12 0.427 12 CU-ZEMAN
13 0.408 13 JHU
14 0.382 14 OMNIFLUENT
15 0.350 15 ITS-LATL
16 0.326 16 ITS-LATL-PE
Russian-English
# rank range system
1 0.657 1 ONLINE-B
2 0.604 2-3 CMU
0.588 2-3 ONLINE-A
4 0.562 4-6 ONLINE-G
0.561 4-6 PROMT
0.550 5-7 QCRI-MES
0.546 5-7 UCAM
8 0.527 8-9 BALAGUR
0.519 8-10 MES-QCRI
0.507 9-11 UEDIN
0.497 10-12 OMNIFLUENT
0.492 11-14 LIA
0.483 12-15 OMNIFLUENT-C
0.481 12-15 UMD
0.476 13-15 CU-KAREL
16 0.432 16 COMMERCIAL-3
17 0.417 17 UEDIN-SYNTAX
18 0.396 18 JHU
19 0.215 19 CU-ZEMAN
Table 6: Official results for the WMT13 translation task. Systems are ordered by the expected win score. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05. This method is also used to determine the
range of ranks into which system falls. Systems with grey background indicate use of resources that fall outside the constraints
provided for the shared task.
9
4 Understandability of English?Czech
For the English-to-Czech translation, we con-
ducted a variation of the ?understandability? test
as introduced in WMT09 (Callison-Burch et al,
2009) and used in WMT10. In order to obtain
additional reference translations, we conflated this
test with post-editing. The procedure was as fol-
lows:
1. Monolingual editing (also called blind edit-
ing). The first annotator is given just the MT
output and requested to correct it. Given er-
rors in MT outputs, some guessing of the
original meaning is often inevitable and the
annotators are welcome to try. If unable, they
can mark the sentences as incomprehensible.
2. Review. A second annotator is asked to
validate the monolingual edit given both the
source and reference translations. Our in-
structions specify three options:
(a) If the monolingual edit is an adequate
translation and acceptably fluent Czech,
confirm it without changes.
(b) If the monolingual edit is adequate but
needs polishing, modify the sentence
and prefix it with the label ?OK:?.
(c) If the monolingual edit is wrong, cor-
rect it. You may start from the origi-
nal unedited MT output, if that is eas-
ier. Avoid using the reference directly,
prefer words from MT output whenever
possible.
The motivation behind this procedure is that we
want to save the time necessary for reading the
sentence. If the reviewer has already considered
whether the sentence is an acceptable translation,
they do not need to read the MT output again in
order to post-edit it. Our approach is thus some-
what the converse of Aziz et al (2013) who ana-
lyze post-editing effort to obtain rankings of MT
systems. We want to measure the understandabil-
ity of MT outputs and obtain post-edits at the same
time.
Both annotation steps were carried out in
the CASMACAT/Matecat post-editing user inter-
face.9, modified to provide the relevant variants of
the sentence next to the main edit box. Screen-
shots of the two annotation phases are given in
Figure 3 and Figure 4.
9http://www.casmacat.eu/index.php?n=Workbench
Occurrence GOOD ALMOST BAD EMPTY Total
First 34.7 0.1 42.3 11.0 4082
Repeated 41.1 0.1 41.0 6.1 805
Overall 35.8 0.1 42.1 10.2 4887
Table 7: Distribution of review statuses.
Similarly to the traditional ranking task, we pro-
vided three consecutive sentences from the origi-
nal text, each translated with a different MT sys-
tem. The annotators are free to use this contex-
tual information when guessing the meaning or re-
viewing the monolingual edits. Each ?annotation
HIT? consists of 24 sentences, i.e. 8 snippets of 3
consecutive sentences.
4.1 Basic Statistics on Editing
In total, 21 annotators took part in the exercise, 20
of them contributed to monolingual editing and 19
contributed to the reviews.
Connecting each review with the monolingual
edit (some edits received multiple reviews), we ob-
tain one data row. We collected 4887 data rows
(i.e. sentence revisions) for 3538 monolingual ed-
its, covering 1468 source sentences as translated
by 12 MT systems (including the reference).
Not all MT systems were considered for each
sentence, we preferred to obtain judgments for
more source sentences.
Based on the annotation instructions, each data
row has one of the four possible statuses: GOOD,
ALMOST, BAD, and EMPTY. GOOD rows are
those where the reviewer accepted the monolin-
gual edit without changes, ALMOST edits were
modified by the reviewer but they were marked as
?OK?. BAD edits were changed by the reviewer
and no ?OK? mark was given. Finally, the sta-
tus EMPTY is assigned to rows where the mono-
lingual editor refused to edit the sentence. The
EMPTY rows nevertheless contain the (?regular?)
post-edit of the reviewer, so they still provide a
new reference translation for the sentence.
Table 7 summarizes the distribution of row sta-
tuses depending on one more significant distinc-
tion: whether the monolingual editor has seen the
sentence before or not. We see that EMPTY and
BAD monolingual edits together drop by about
6% absolute when the sentence is not new to the
monolingual editor. The occurrence is counted as
?repeated? regardless whether the annotator has
previously seen the sentence in an editing or re-
viewing task. Unless stated otherwise, we exclude
repeated edits from our calculations.
10
Figure 3: In this screen, the annotator is expected to correct the MT output given only the context of at most two neighbouring
machine-translated sentences.
ALMOST Pairwise
treated Comparisons Agreement ?
inter
separate 2690 56.0 0.270
as BAD 2690 67.9 0.351
as GOOD 2690 65.2 0.289
intra
separate 170 65.3 0.410
as BAD 170 69.4 0.386
as GOOD 170 71.8 0.422
Table 8: Annotator agreement when reviewing monolingual
edits.
4.2 Agreement on Understandability
Before looking at individual system results, we
consider annotator agreement in the review step.
Details are given in Table 8. Given a (non-
EMPTY) string from a monolingual edit, we
would like to know how often two acceptability
judgments by two different reviewers (inter-) or
the same reviewer (intra-) agree. The repeated ed-
its remain in this analysis because we are not in-
terested in the origin of the string.
Our annotation setup leads to three possible la-
bels: GOOD, ALMOST, and BAD. The agree-
ment on one of three classes is bound to be lower
than the agreement on two classes, so we also re-
interpret ALMOST as either GOOD or BAD. Gen-
erally speaking, ALMOST is a positive judgment,
so it would be natural to treat it as GOOD. How-
ever, in our particular setup, when the reviewer
modified the sentence and forgot to add the label
?OK:?, the item ended up in the BAD class. We
conclude that this is indeed the case: the inter-
annotator agreement appears higher if ALMOST
is treated as BAD. Future versions of the review-
ing interface should perhaps first ask for the yes/no
judgment and only then allow to post-edit.
The ? values in Table 8 are the Fleiss?
kappa (Fleiss, 1971), accounting for agreement by
chance given the observed label distributions.
In WMT09, the agreements for this task were
higher: 77.4 for inter-AA and 86.6 for intra-AA.
(In 2010, the agreements for this task were not re-
ported.) It is difficult to say whether the differ-
ence lies in the particular language pair, the dif-
ferent set of annotators, or the different user in-
terface for our reviewing task. In 2009 and 2010,
the reviewers were shown 5 monolingual edits at
once and they were asked to judge each as accept-
able or not acceptable. We show just one segment
and they have probably set their minds on the post-
editing rather than acceptability judgment. We be-
lieve that higher agreements can be reached if the
reviewers first validate one or more of the edits and
only then are allowed to post-edit it.
4.3 Understandability of English?Czech
Table 9 brings about the first main result of our
post-editing effort. For each system (including
the reference translation), we check how often a
monolingual edit was marked OK or ALMOST
by the subsequent reviewer. The average under-
standability across all MT systems into Czech is
44.2?1.6%. This is a considerable improvement
compared to 2009 where the best systems pro-
duced about 32% understandable sentences. In
11
Figure 4: In this screen, the annotator is expected to validate the monolingual edit, correcting it if necessary. The annotator is
expected to add the prefix ?OK:? if the correction was more or less cosmetic.
Rank System Total Observations % Understandable
Overall incl. ref. 4082 46.7?1.6
Overall without ref. 3808 44.2?1.6
1 Reference 274?31 80.3?4.8
2-6 CU-ZEMAN 348?34 51.7?5.1
2-6 UEDIN 332?33 51.5?5.4
2-6 ONLINE-B 337?34 50.7?5.3
2-6 CU-BOJAR 341?35 50.7?5.2
2-7 CU-DEPFIX 350?34 48.0?5.3
6-10 COMMERCIAL-2 358?36 43.6?5.2
6-11 COMMERCIAL-1 316?34 41.5?5.5
7-12 CU-TECTOMT 338?34 39.4?5.2
8-12 MES 346?36 38.4?5.2
8-12 CU-PHRASEFIX 394?40 38.1?4.8
10-12 SHEF-WPROA 348?32 34.2?5.1
2009 Reference 91
2009 Best System 32
2010 Reference 97
2010 Best System 58
Table 9: Understandability of English?Czech systems. The
? values indicate empirical confidence bounds at 95%. Rank
ranges were also obtained in the same resampling: in 95% of
observations, the system was ranked in the given range.
2010, the best systems or system combinations
reached 55%?58%. The test set across years and
the quality of references and judgments also play a
role. In our annotation setup, the references appear
to be correctly understandable only to 80.3?4.8%.
To estimate the variance of these results due
to the particular sentences chosen, we draw 1000
random samples from the dataset, preserving the
dataset size and repeating some. The exact num-
ber of judgments per system can thus vary. We
report the 95% empirical confidence interval after
the ??? signs in Table 9 (the systems range from
?4.8 to?5.5). When we drop individual blind ed-
itors or reviewers, the understandability judgments
differ by about ?2 to ?4. In other words, the de-
pendence on the test set appears higher than the
dependence on the annotators.
The limited size of our dataset alows us only
to separate two main groups of systems: those
ranking 2?6 and those ranking worse. This rough
grouping vaguely matches with WMT13 ranking
results as given in Table 6. A somewhat surpris-
ing observation is that two automatic corrections
ranked better in WMT13 ranking but score worse
in understandability: CU-DEPFIX fixes some lost
negation and some agreement errors of CU-BOJAR
and CU-PHRASEFIX is a standard statistical post-
editing of a transfer-based system CU-TECTOMT.
A detailed inspection of the data is necessary to
explain this.
5 More Reference Translations for Czech
Our annotation procedure described in Section 4
allowed us to obtain a considerable number of ad-
ditional reference translations on top of official
single reference.
12
Refs 1 2 3 4 5 6 7 8 9 10-16
Sents 233 709 174 123 60 48 40 27 25 29
Table 10: Number of source sentences with the given number
of distinct reference translations.
In total, our edits cover 1468 source sentences,
i.e. about a half of the official test set size, and pro-
vide 4311 unique references. On average, one sen-
tence in our set has 2.94?2.17 unique reference
translations. Table 10 provides a histogram.
It is well known that automatic MT evalua-
tion methods perform better with more references,
because a single one may not confirm a correct
part of MT output. This issue is more severe
for morphologically rich languages like Czech
where about 1/3 of MT output was correct but not
confirmed by the reference (Bojar et al, 2010).
Advanced evaluation methods apply paraphras-
ing to smooth out some of the lexical divergence
(Kauchak and Barzilay, 2006; Snover et al, 2009;
Denkowski and Lavie, 2010). Simpler techniques
such as lemmatizing are effective for morphologi-
cally rich languages (Tantug et al, 2008; Kos and
Bojar, 2009) but they will lose resolution once the
systems start performing generally well.
WMTs have taken the stance that a big enough
test set with just a single reference should compen-
sate for the lack of other references. We use our
post-edited reference translations to check this as-
sumption for BLEU and NIST as implemented in
mteval-13a (international tokenization switched
on, which is not the default setting).
We run many probes, randomly picking the test
set size (number of distinct sentences) and the
number of distinct references per sentence. Note
that such test sets are somewhat artificially more
diverse; in narrow domains, source sentences can
repeat and even appear verbatim in the training
data, and in natural test sets with multiple refer-
ences, short sentences can receive several identical
translations.
For each probe, we measure the Spearman?s
rank correlation coefficient ? of the ranks pro-
posed by BLEU or NIST and the manual ranks.
We use the same implementation as applied in the
WMT13 Shared Metrics Task (Macha?c?ek and Bo-
jar, 2013). Note that the WMT13 metrics task still
uses the WMT12 evaluation method ignoring ties,
not the expected wins. As Koehn (2012) shows,
the two methods do not differ much.
Overall, the correlation is strongly impacted by
Figure 5: Correlation of BLEU and WMT13 manual ranks
for English?Czech translation
Figure 6: Correlation of NIST and WMT13 manual ranks
for English?Czech translation
the particular choice of test sentences and refer-
ence translations. By picking sentences randomly,
similarly or equally sized test sets can reach dif-
ferent correlations. Indeed, e.g. for a test set of
about 1500 distinct sentences selected from the
3000-sentence official test set (1 reference trans-
lation), we obtain correlations for BLEU between
0.86 and 0.94.
Figure 5 plots the correlations of BLEU and the
system rankings, Figure 6 provides the same pic-
ture for NIST. The upper triangular part of the plot
contains samples from our post-edited reference
translations, the lower rectangular part contains
probes from the official test set of 3000 sentences
with 1 reference translation.
To interpret the observations, we also calculate
the average and standard deviation of correlations
for each cell in Figures 5 and 6. Figures 7 and
8 plot the values for 1, 6, 7 and 8 references for
13
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
B
L
E
U
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 7: Projections from Figure 5 of BLEU and WMT13
manual ranks for English?Czech translation
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
N
I
S
T
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 8: Projections from Figure 6 of NIST and WMT13
manual ranks for English?Czech translation
BLEU and NIST, resp. The projections confirm
that the average correlations grow with test set
size, the growth is however sub-logarithmic.
Starting from as few as a dozen of sentences, we
see that using more references is better than using
a larger test set. For BLEU, we however already
seem to reach false positives at 7 references for
one or two hundred sentences: larger sets with just
one reference may correlate slightly better.
Using one reference obtained by post-editing
seems better than using the official (independent)
reference translations. BLEU is more affected
than NIST by this difference even at relatively
large test set size. Note that our post-edits are in-
spired by all MT systems, the good as well as the
bad ones. This probably provides our set with a
certain balance.
Overall, the best balance between the test set
size and the number of references seems to lie
somewhere around 7 references and 100 or 200
sentences. Creating such a test set could be even
cheaper than the standard 3000 sentences with just
one reference. However, the wide error bars re-
mind us that even this setting can lead to correla-
tions anywhere between 0.86 and 0.96. For other
languages, data sets types or other MT evaluation
methods, the best setting can be quite different and
has to be sought for.
6 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
second edition of the WMT shared task on qual-
ity estimation builds on the previous edition of the
task (Callison-Burch et al, 2012), with variants to
this previous task, including both sentence-level
and word-level estimation, with new training and
test datasets, along with evaluation metrics and
baseline systems.
The motivation to include both sentence- and
word-level estimation come from the different po-
tential applications of these variants. Some inter-
esting uses of sentence-level quality estimation are
the following:
? Decide whether a given translation is good
enough for publishing as is.
? Inform readers of the target language only
whether or not they can rely on a translation.
? Filter out sentences that are not good enough
for post-editing by professional translators.
? Select the best translation among options
from multiple MT and/or translation memory
systems.
Some interesting uses of word-level quality es-
timation are the following:
? Highlight words that need editing in post-
editing tasks.
? Inform readers of portions of the sentence
which are not reliable.
? Select the best segments among options from
multiple translation systems for MT system
combination.
The goals of this year?s shared task were:
14
? To explore various granularity levels for the
task (sentence-level and word-level).
? To explore the prediction of more objective
scores such as edit distance and post-editing
time.
? To explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics in the task of ranking alternative
translations generated by different MT sys-
tems.
? To identify new and effective quality indica-
tors (features) for all variants of the quality
estimation task.
? To identify effective machine learning tech-
niques for all variants of the quality estima-
tion task.
? To establish the state of the art performance
in the field.
Four subtasks were proposed, as we discuss in
Sections 6.1 and 6.2. Each subtask provides spe-
cific datasets, annotated for quality according to
the subtask (Section 6.3), and evaluates the system
submissions using specific metrics (Section 6.6).
When available, external resources (e.g. SMT
training corpus) and translation engine-related re-
sources were given to participants (Section 6.4),
who could also use any additional external re-
sources (no distinction between open and close
tracks is made). Participants were also provided
with a software package to extract quality esti-
mation features and perform model learning (Sec-
tion 6.5), with a suggested list of baseline features
and learning method (Section 6.7). Participants
could submit up to two systems for each subtask.
6.1 Sentence-level Quality Estimation
Task 1.1 Predicting Post-editing Distance This
task is similar to the quality estimation task in
WMT12, but with one important difference in the
scoring variant: instead of using the post-editing
effort scores in the [1-5] range, we use HTER
(Snover et al, 2006) as quality score. This score
is to be interpreted as the minimum edit distance
between the machine translation and its manually
post-edited version, and its range is [0, 1] (0 when
no edit needs to be made, and 1 when all words
need to be edited). Two variants of the results
could be submitted in the shared task:
? Scoring: A quality score for each sentence
translation in [0,1], to be interpreted as an
HTER score; lower scores mean better trans-
lations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning). The reference ranking is de-
fined based on the true HTER scores.
Task 1.2 Selecting Best Translation This task
consists in ranking up to five alternative transla-
tions for the same source sentence produced by
multiple MT systems. We use essentially the same
data provided to participants of previous years
WMT?s evaluation metrics task ? where MT eval-
uation metrics are assessed according to how well
they correlate with human rankings. However, ref-
erence translations produced by humans are not be
used in this task.
Task 1.3 Predicting Post-editing Time For this
task systems are required to produce, for each
translation, the expected time (in seconds) it
would take a translator to post-edit such an MT
output. The main application for predictions of
this type is in computer-aided translation where
the predicted time can be used to select among dif-
ferent hypotheses or even to omit any MT output
in cases where no good suggestion is available.
6.2 Word-level Quality Estimation
Based on the data of Task 1.3, we define Task 2, a
word-level annotation task for which participants
are asked to produce a label for each token that
indicates whether the word should be changed by
a post-editor or kept in the final translation. We
consider the following two sets of labels for pre-
diction:
? Binary classification: a keep/change label,
the latter meaning that the token should be
corrected in the post-editing process.
? Multi-class classification: a label specifying
the edit action that should be performed on
the token (keep as is, delete, or substitute).
6.3 Datasets
Task 1.1 Predicting post-editing distance For
the training of models, we provided the WMT12
15
quality estimation dataset: 2,254 English-
Spanish news sentences extracted from previous
WMT translation task English-Spanish test sets
(WMT09, WMT10, and WMT12). These were
translated by a phrase-based SMT Moses system
trained on Europarl and News Commentaries cor-
pora as provided by WMT, along with their source
sentences, reference translations, post-edited
translations, and HTER scores. We used TERp
(default settings: tokenised, case insensitive,
etc., but capped to 1)10 to compute the HTER
scores. Likert scores in [1,5] were also provided,
as participants may choose to use them for the
ranking variant.
As test data, we use a subset of the WMT13
English-Spanish news test set with 500 sentences,
whose translations were produced by the same
SMT system used for the training set. To com-
pute the true HTER labels, the translations were
post-edited under the same conditions as those on
the training set. As in any blind shared task, the
HTER scores were solely used to evaluate the sub-
missions, and were only released to participants
after they submitted their systems.
A few variations of the training and test data
were provided, including a version with cases re-
stored and a version detokenized. In addition,
we provided a number of engine-internal informa-
tion from Moses for glass-box feature extraction,
such as phrase and word alignments, model scores,
word graph, n-best lists and information from the
decoder?s search graph.
Task 1.2 Selecting best translation As training
data, we provided a large set of up to five alter-
native machine translations produced by different
MT systems for each source sentence and ranked
for quality by humans. This was the outcome of
the manual evaluation of the translation task from
WMT09-WMT12. It includes two language pairs:
German-English and English-Spanish, with 7,098
and 4,592 source sentences and up to five ranked
translations, totalling 32,922 and 22,447 transla-
tions, respectively.
As test data, a set of up to five alternative ma-
chine translations per source sentence from the
WMT08 test sets was provided, with 365 (1,810)
and 264 (1,315) source sentences (translations)
for German-English and English-Spanish, respec-
tively. We note that there was some overlap be-
tween the MT systems used in the training data
10http://www.umiacs.umd.edu/?snover/terp/
and test datasets, but not all systems were the
same, as different systems participate in WMT
over the years.
Task 1.3 and Task 2 Predicting post-editing
time and word-level edits For Tasks 1.3 and 2
we provides a new dataset consisting of 22 English
news articles which were translated into Span-
ish using Moses and post-edited during a CAS-
MACAT11 field trial. Of these, 15 documents have
been processed repeatedly by at least 2 out of 5
translators, resulting in a total of 1,087 segments.
For each segment we provided:
? English source and Spanish translation.
? Spanish MT output which was used as basis
for post-editing.
? Document and translator ID.
? Position of the segment within the document.
The metadata about translator and document was
made available as we expect that translator perfor-
mance and normalisation over document complex-
ity can be helpful when predicting the time spend
on a given segment.
For the training portion of the data we also pro-
vided:
? Time to post-edit in seconds (Task 1.3).
? Binary (Keep, Change) and multiclass (Keep,
Substitute, Delete) labels on word level along
with explicit tokenization (Task 2).
The labels in Task 2 are derived by comput-
ing WER between the original machine translation
and its post-edited version.
6.4 Resources
For all tasks, we provided resources to extract
quality estimation features when these were avail-
able:
? The SMT training corpus (WMT News and
Europarl): source and target sides of the cor-
pus used to train the SMT engines for Tasks
1.1, 1.3, and 2, and truecase models gener-
ated from these. These corpora can also be
used for Task 1.2, but we note that some of
the MT systems used in the datasets of this
task were not statistical or did not use (only)
the training corpus provided by WMT.
11http://casmacat.eu/
16
? Language models: n-gram language models
of source and target languages generated us-
ing the SMT training corpora and standard
toolkits such as SRILM Stolcke (2002), and
a language model of POS tags for the target
language. We also provided unigram, bigram
and trigram counts.
? IBM Model 1 lexical tables generated by
GIZA++ using the SMT training corpora.
? Phrase tables with word alignment informa-
tion generated by scripts provided by Moses
from the parallel corpora.
? For Tasks 1.1, 1.3 and 2, the Moses config-
uration file used for decoding or the code to
re-run the entire Moses system.
? For Task 1.1, both English and Spanish re-
sources for a number of advanced features
such as pre-generated PCFG parsing models,
topic models, global lexicon models and mu-
tual information trigger models.
We refer the reader to the QUEST website12 for
a detailed list of resources provided for each task.
6.5 QUEST Framework
QUEST (Specia et al, 2013) is an open source
framework for quality estimation which provides a
wide variety of feature extractors from source and
translation texts and external resources and tools.
These range from simple, language-independent
features, to advanced, linguistically motivated fea-
tures. They include features that rely on informa-
tion from the MT system that generated the trans-
lations (glass-box features), and features that are
oblivious to the way translations were produced
(black-box features).
QUEST also integrates a well-known machine
learning toolkit, scikit-learn,13 and other algo-
rithms that are known to perform well on this task
(e.g. Gaussian Processes), providing a simple and
effective way of experimenting with techniques
for feature selection and model building, as well
as parameter optimisation through grid search.
From QUEST, a subset of 17 features and an
SVM regression implementation were used as
baseline for Tasks 1.1, 1.2 and 1.3. The software
was made available to all participants.
12http://www.quest.dcs.shef.ac.uk/
13http://scikit-learn.org/
6.6 Evaluation Metrics
Task 1.1 Predicting post-editing distance
Evaluation is performed against the HTER and/or
ranking of translations using the same metrics as
in WMT12. For the scoring variant of the task,
we use two standard metrics for regression tasks:
Mean Absolute Error (MAE) as a primary metric,
and Root of Mean Squared Error (RMSE) as a
secondary metric. To improve readability, we
report these error numbers by first mapping the
HTER values to the [0, 100] interval, to be read
as percentage-points of the HTER metric. For a
given test set S with entries si, 1 ? i ? |S|, we
denote by H(si) the proposed score for entry si
(hypothesis), and by V (si) the reference value for
entry si (gold-standard value):
MAE =
?N
i=1 |H(si)? V (si)|
|S|
RMSE =
??N
i=1(H(si)? V (si))2
|S|
Both these metrics are non-parametric, auto-
matic and deterministic (and therefore consistent),
and extrinsically interpretable. For instance, a
MAE value of 10 means that, on average, the ab-
solute difference between the hypothesized score
and the reference score value is 10 percentage
points (i.e., 0.10 difference in HTER scores). The
interpretation of RMSE is similar, with the differ-
ence that RMSE penalises larger errors more (via
the square function).
For the ranking variant of the task, we use the
DeltaAvg metric proposed in the 2012 edition of
the task (Callison-Burch et al, 2012) as our main
metric. This metric assumes that each reference
test instance has an extrinsic number associated
with it that represents its ranking with respect to
the other test instances. For completeness, we
present here again the definition of DeltaAvg.
The goal of the DeltaAvg metric is to measure
how valuable a proposed ranking (which we call a
hypothesis ranking) is, according to the true rank-
ing values associated with the test instances. We
first define a parametrised version of this metric,
called DeltaAvg[n]. The following notations are
used: for a given entry sentence s, V (s) represents
the function that associates an extrinsic value to
that entry; we extend this notation to a set S, with
V (S) representing the average of all V (s), s ? S.
17
Intuitively, V (S) is a quantitative measure of the
?quality? of the set S, as induced by the extrinsic
values associated with the entries in S. For a set
of ranked entries S and a parameter n, we denote
by S1 the first quantile of set S (the highest-ranked
entries), S2 the second quantile, and so on, for n
quantiles of equal sizes.14 We also use the nota-
tion Si,j = ?jk=i Sk. Using these notations, wedefine:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1 ? V (S)
When the valuation function V is clear from the
context, we write DeltaAvg[n] for DeltaAvgV [n].
The parameter n represents the number of quan-
tiles we want to split the set S into. For instance,
n = 2 gives DeltaAvg[2] = V (S1)?V (S), hence it
measures the difference between the quality of the
top quantile (top half) S1 and the overall quality
(represented by V (S)). For n = 3, DeltaAvg[3] =
(V (S1)+V (S1,2)/2?V (S) = ((V (S1)?V (S))+
(V (S1,2?V (S)))/2, hence it measures an average
difference across two cases: between the quality of
the top quantile (top third) and the overall quality,
and between the quality of the top two quantiles
(S1 ? S2, top two-thirds) and the overall quality.
In general, DeltaAvg[n] measures an average dif-
ference in quality across n ? 1 cases, with each
case measuring the impact in quality of adding an
additional quantile, from top to bottom. Finally,
we define:
DeltaAvgV =
?N
n=2 DeltaAvgV [n]
N ? 1
where N = |S|/2. As before, we write DeltaAvg
for DeltaAvgV when the valuation function V is
clear from the context. The DeltaAvg metric is an
average across all DeltaAvg[n] values, for those
n values for which the resulting quantiles have at
least 2 entries (no singleton quantiles).
We present results for DeltaAvg using as valu-
ation function V the HTER scores, as defined in
Section 6.3. We also use Spearman?s rank correla-
tion coefficient ? as a secondary metric.
Task 1.2 Selecting best translation The perfor-
mance on the task of selecting the best transla-
tion from a pool of translation candidates is mea-
14If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
sured by comparing proposed (hypothesis) rank-
ings against human-produced rankings. The met-
ric used is Kendall?s ? rank correlation coefficient,
computed as follows:
? = |concordant pairs| ? |discordant pairs||total pairs|
where a concordant pair is a pair of two transla-
tions for the same source segment in which the
ranking order proposed by a human annotator and
the ranking order of the hypothesis agree; in a dis-
cordant pair, they disagree. The possible values of
? range between 1 (where all pairs are concordant)
and ?1 (where all pairs are discordant). Thus a
system with ranking predictions having a higher
? value makes predictions that are more similar
to human judgements than a system with ranking
predictions having a lower ? . Note that, in general,
being able to predict rankings with an accuracy
of ? = ?1 is as difficult as predicting rankings
with an accuracy of ? = 1, whereas a completely
random ranking would have an expected value of
? = 0. The range is therefore said to be symmet-
ric.
However, there are two distinct ways of mea-
suring rank correlation using Kendall?s ? , related
to the way ties are treated. They greatly affect how
Kendall?s ? numbers are to be interpreted, and es-
pecially the symmetry property. We explain the
difference in detail in what follows.
Kendall?s ? with ties penalised If the goal is
to measure to what extent the difference in qual-
ity visible to a human annotator has been captured
by an automatically produced hypothesis (recall-
oriented view), then proposing a tie between t1
and t2 (t1-equal-to-t2) when the pair was judged
(in the reference) as t1-better-than-t2 is treated as
a failure-to-recall. In other words, it is as bad as
proposing t1-worse-than-t2. Henceforth, we call
this recall-oriented measure ?Kendall?s ? with ties
penalised?. This metric has the following proper-
ties:
? it is completely fair when comparing differ-
ent methods to produce ranking hypotheses,
because the denominator (number of total
pairs) is the same (it is the number of non-
tied pairs under the human judgements).
? it is non-symmetric, in the sense that a value
of ? = ?1 is not as difficult to obtain as ? =
18
1 (simply proposing only ties gets a ? = ?1);
hence, the sign of the ? value matters.
? the expected value of a completely random
ranking is not necessarily ? = 0, but rather
depends on the number of ties in the refer-
ence rankings (i.e., it is test set dependent).
Kendall?s ? with ties ignored If the goal
is to measure to what extent the difference in
quality signalled by an automatically produced
hypothesis is reflected in the human annota-
tion (precision-oriented view), then proposing t1-
equal-to-t2 when the pair was judged differently
in the reference does no harm the metric.
Henceforth, we call this precision-oriented
measure ?Kendall?s ? with ties ignored?. This
metric has the following properties:
? it is not completely fair when comparing dif-
ferent methods to produce ranking hypothe-
ses, because the denominator (number of to-
tal pairs) may not be the same (it is the num-
ber of non-tied pairs under each system?s pro-
posal).
? it is symmetric, in the sense that a value of
? = ?1 is as difficult to obtain as ? = 1;
hence, the sign of the ? value may not mat-
ter. 15
? the expected value of a completely random
ranking is ? = 0 (test-set independent).
The first property is the most worrisome from
the perspective of reporting the results of a shared
task, because a system may fare very well on this
metric simply because it choses not to commit
(proposes ties) most of the time. Therefore, to
give a better understanding of the systems? perfor-
mance, for Kendall?s ? with ties ignored we also
provide the number of non-ties proposed by each
system.
Task 1.3 Predicting post-editing time Submis-
sions are evaluated in terms of Mean Average Er-
ror (MAE) against the actual time spent by post-
editors (in seconds). By using a linear error mea-
sure we limit the influence of outliers: sentences
that took very long to edit or where the measure-
ment taken is questionable.
15In real life applications this distinction matters. Even
if, from a computational perspective, it is as hard to get ?
close to?1 as it is to get it close to 1, knowing the sign is the
difference between selecting the best or the worse translation.
To further analyse the influence of extreme val-
ues, we also compute Spearman?s rank correlation
? coefficient which does not depend on the abso-
lute values of the predictions.
We also give RMSE and Pearson?s correlation
coefficient r for reference.
Task 2 Predicting word-level scores The word-
level task is primarily evaluated by macro-
averaged F-measure. Because the class distribu-
tion is skewed ? in the test data about one third
of the tokens are marked as correct ? we compute
precision and recall and F1 for each class individ-
ually. Consider the following confusion matrix for
the two classes Keep and Change:
predicted
(K)eep (C)hange
expected (K)eep 10 20(C)hange 30 40
For the given example we derive true-positive
(tp), true-negative (tn), false-positive (fp), and
false-negative (fn) counts:
tpK = 10 fpK = 30 fnK = 20
tpC = 40 fpC = 20 fnC = 30
precisionK =
tpK
tpK + fpK
= 10/40
recallK =
tpK
tpK + fnK
= 10/30
F1,K =
2 ? precisionK ? recallK
precisionK +recallK
A single cumulative statistic can be computed
by averaging the resulting F-measures (macro av-
eraging) or by micro averaging in which case pre-
cision and recall are first computed by accumulat-
ing the relevant values for all classes (O?zgu?r et al,
2005), e.g.
precision = tpK + tpC(tpK + fpK) + (tpC + fpC)
The latter gives equal weight to each exam-
ple and is therefore dominated by performance on
the largest class while macro-averaged F-measure
gives equal weight to each class.
The same setup is used to evaluate the perfor-
mance in the multiclass setting. Please note that
here the test data only contains 4% examples for
class (D)elete.
19
ID Participating team
CMU Carnegie Mellon University, USA (Hildebrand and Vogel, 2013)
CNGL Centre for Next Generation Localization, Ireland (Bicici, 2013b)
DCU Dublin City University, Ireland (Almaghout and Specia, 2013)
DCU-SYMC Dublin City University & Symantec, Ireland (Rubino et al, 2013b)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis and
Popovic, 2013)
FBK-UEdin Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de
Souza et al, 2013)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al, 2013)
LIMSI Laboratoire d?Informatique pour la Me?canique et les Sciences de l?Inge?nieur,
France (Singh et al, 2013)
LORIA Lorraine Laboratory of Research in Computer Science and its Applications,
France (Langlois and Smaili, 2013)
SHEF University of Sheffield, UK (Beck et al, 2013)
TCD-CNGL Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013)
TCD-DCU-CNGL Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and
Rubino, 2013)
UMAC University of Macau, China (Han et al, 2013)
UPC Universitat Politecnica de Catalunya, Spain (Formiga et al, 2013b)
Table 11: Participants in the WMT13 Quality Estimation shared task.
6.7 Participants
Table 11 lists all participating teams submitting
systems to any subtask in this shared task. Each
team was allowed up to two submissions for each
subtask. In the descriptions below participation in
specific tasks is denoted by a task identifier: T1.1,
T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.3):
QUEST was used to extract 17 system-
independent features from the source and
translation files and the SMT training cor-
pus that were found to be relevant in previous
work (same features as in the WMT12 shared
task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? Language model probability of source
and target sentences using language
models provided by the task.
? average number of translations per
source word in the sentence: as given
by IBM 1 model thresholded so that
P (t|s) > 0.2, and so that P (t|s) > 0.01
weighted by the inverse frequency of
each word in the source side of the SMT
training corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source side of the
SMT training corpus
? percentage of unigrams in the source
sentence seen in the source side of the
SMT training corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within the
SCIKIT-LEARN toolkit. The ?,  and C pa-
rameters were optimized using a grid-search
and 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as a ?baseline?, it is in fact a strong
system. For tasks of the same type as 1.1
and 1.3, it has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting post-editing effort, as it
has also been shown in the previous edition
of the task (Callison-Burch et al, 2012).
The same features could be useful for a base-
line system for Task 1.2. In our official re-
20
sults, however, the baseline for Task 1.2 is
simpler than that: it proposes random ranks
for each pair of alternative translations for a
given source sentence, as we will discuss in
Section 6.8.
CMU (T1.1, T1.2, T1.3): The CMU quality
estimation system was trained on features
based on language models, the MT sys-
tem?s distortion model and phrase table fea-
tures, statistical word lexica, several sentence
length statistics, source language word and
bi-gram frequency statistics, n-best list agree-
ment and diversity, source language parse,
source-target word alignment and a depen-
dency parse based cohesion penalty. These
features were extracted using GIZA++, a
forced alignment algorithm and the Stanford
parser (de Marneffe et al, 2006). The pre-
diction models were trained using four clas-
sifiers in the Weka toolkit (Hall et al, 2009):
linear regression, M5P trees, multi layer per-
ceptron and SVM regression. In addition to
main system submission, a classic n-best list
re-ranking approach was used for Task 1.2.
CNGL (T1.1, T1.2, T1.3, T2): CNGL systems
are based on referential translation machines
(RTM) (Bic?ici and van Genabith, 2013), par-
allel feature decay algorithms (FDA) (Bicici,
2013a), and machine translation performance
predictor (MTPP) (Bic?ici et al, 2013), all
of which allow to obtain language and MT
system-independent predictions. For each
task, RTM models were developed using the
parallel corpora and the language model cor-
pora distributed by the WMT13 translation
task and the language model corpora pro-
vided by LDC for English and Spanish.
The sentence-level features are described in
MTPP (Bic?ici et al, 2013); they include
monolingual or bilingual features using n-
grams defined over text or common cover
link (CCL) (Seginer, 2007) structures as the
basic units of information over which sim-
ilarity calculations are made. RTMs use
308 features about coverage and diversity,
IBM1, and sentence translation performance,
retrieval closeness and minimum Bayes re-
trieval risk, distributional similarity and en-
tropy, IBM2 alignment, character n-grams,
and sentence readability. The learning mod-
els are Support Vector Machines (SVR) and
SVR with partial least squares (SVRPLS).
The word-level features include CCL links,
word length, location, prefix, suffix, form,
context, and alignment, totalling 511K fea-
tures for binary classification, and 637K for
multiclass classification. Generalised lin-
ear models (GLM) (Collins, 2002) and GLM
with dynamic learning (GLMd) were used.
DCU (T1.2): The main German-English submis-
sion uses six Combinatory Categorial Gram-
mar (CCG) features: CCG supertag lan-
guage model perplexity and log probability,
the number of maximal CCG constituents in
the translation output which are the highest-
probability minimum number of CCG con-
stituents that span the translation output, the
percentage of CCG argument mismatches be-
tween each subsequent CCG supertags, the
percentage of CCG argument mismatches be-
tween each subsequent CCG maximal cate-
gories and the minimum number of phrases
detected in the translation output. A second
submission uses the aforementioned CCG
features combined with 80 features from
QUEST as described in (Specia, 2011). For
the CCG features, the C&C parser was used
to parse the translation output. Moses was
used to build the phrase table from the SMT
training corpus with maximum phrase length
set to 7. The language model of supertags
was built using the SRILM toolkit. As learn-
ing algorithm, Logistic Regression as pro-
vided by the SCIKIT-LEARN toolkit was used.
The training data was prepared by converting
each ranking of translation outputs to a set
of pairwise comparisons according to the ap-
proach proposed by Avramidis et al (2011).
The rankings were generated back from pair-
wise comparisons predicted by the model.
DCU-SYMC (T1.1): The DCU-Symantec team
employed a wide set of features which in-
cluded language model, n-gram counts and
word-alignment features as well as syntac-
tic features, topic model features and pseudo-
reference features. The main learning algo-
rithm was SVR, but regression tree learning
was used to perform feature selection, re-
ducing the initial set of 442 features to 96
features (DCU-Symantec alltypes) and 134
21
(DCU-Symantec combine). Two methods
for feature selection were used: a best-first
search in the feature space using regression
trees to evaluate the subsets, and reading bi-
narised features directly from the nodes of
pruned regression trees.
The following NLP tools were used in feature
extraction: the Brown English Wall-Street-
Journal-trained statistical parser (Charniak
and Johnson, 2005), a Lexical Functional
Grammar parser (XLE), together with a
hand-crafted Lexical Functional Grammar,
the English ParGram grammar (Kaplan et al,
2004), and the TreeTagger part-of-speech
tagger (Schmidt, 1994) with off-the-shelf
publicly available pre-trained tagging mod-
els for English and Spanish. For pseudo-
reference features, the Bing, Moses and Sys-
tran translation systems were used. The Mal-
let toolkit (McCallum, 2002) was used to
build the topic models and features based on
a grammar checker were extracted with Lan-
guageTool.16
DFKI (T1.2, T1.3): DFKI?s submission for Task
1.2 was based on decomposing rankings into
pairs (Avramidis, 2012), where the best sys-
tem for each pair was predicted with Lo-
gistic Regression (LogReg). For German-
English, LogReg was trained with Stepwise
Feature Selection (Hosmer, 1989) on two
feature sets: Feature Set 24 includes ba-
sic counts augmented with PCFG parsing
features (number of VPs, alternative parses,
parse probability) on both source and tar-
get sentences (Avramidis et al, 2011), and
pseudo-reference METEOR score; the most
successful set, Feature Set 33 combines those
24 features with the 17 baseline features. For
English-Spanish, LogReg was used with L2
Regularisation (Lin et al, 2007) and two fea-
ture sets were devised after scoring features
with ReliefF (Kononenko, 1994) and Infor-
mation Gain (Hunt et al, 1966). Feature Set
431 combines 30 features with highest abso-
lute Relief-F and Information Gain (15 from
each). features with the highest
Task 1.3 was modelled using feature sets
selected after Relief-F scoring of external
black-box and glass-box features extracted
16http://www.languagetool.org/
from the SMT decoding process. The most
successful submission (linear6) was trained
with Linear Regression including the 17 fea-
tures with highest positive Relief-F. Most
prominent features include the alternative
possible parses of the source and target sen-
tence, the positions of the phrases with the
lowest and highest probability and future
cost estimate in the translation, the counts of
phrases in the decoding graph whose prob-
ability or whether the future cost estimate
is higher/lower than their standard deviation,
counts of verbs and determiners, etc. The
second submission (pls8) was trained with
Partial Least Squares regression (Stone and
Brooks, 1990) including more glass-box fea-
tures.
FBK-Uedin (T1.1, T1.3):
The submissions explored features built on
MT engine resources including automatic
word alignment, n-best candidate translation
lists, back-translations and word posterior
probabilities. Information about word align-
ments is used to extract quantitative (amount
and distribution of the alignments) and qual-
itative (importance of the aligned terms) fea-
tures under the assumption that alignment
information can help tasks where sentence-
level semantic relations need to be identified
(Souza et al, 2013). Three similar English-
Spanish systems are built and used to provide
pseudo-references (Soricut et al, 2012) and
back-translations, from which automatic MT
evaluation metrics could be computed and
used as features.
All features were computed over a concatena-
tion of several publicly available parallel cor-
pora for the English-Spanish language pair
such as Europarl, News Commentary, and
MultiUN. The models were developed using
supervised learning algorithms: SVMs (with
feature selection step prior to model learning)
and extremely randomized trees.
LIG (T2): The LIG systems are designed to
deal with both binary and multiclass variants
of the word level task. They integrate sev-
eral features including: system-based (graph
topology, language model, alignment con-
text, etc.), lexical (Part-of-Speech tags), syn-
tactic (constituent label, distance to the con-
22
stituent tree root) and semantic (target and
source polysemy count). Besides the exist-
ing components of the SMT system, feature
extraction requires further external tools and
resources, such as: TreeTagger (for POS tag-
ging), Bekerley Parser trained with AnCora
treebank (for generating constituent trees in
Spanish), WordNet and BabelNet (for pol-
ysemy count), Google Translate. The fea-
ture set is then combined and trained using
a Conditional Random Fields (CRF) learn-
ing method. During the labelling phase, the
optimal threshold is tuned using a small de-
velopment set split from the original training
set. In order to retain the most informative
features and eliminate the redundant ones, a
Sequential Backward Selection algorithm is
employed over the all-feature systems. With
the binary classifier, the Boosting technique
is applied to allow a number of sub feature
sets to complement each other, resulting in
the ?stronger? combined system.
LIMSI (T1.1, T1.3): The two tasks were treated
as regression problems using a simple elas-
tic regression, a linear model trained with L1
and L2 regularisers. Regarding features, the
submissions mainly aimed at evaluating the
usefulness for quality estimation of n-gram
posterior probabilities (Gispert et al, 2013)
that quantify the probability for a given n-
gram to be part of the system output. Their
computation relies on all the hypotheses con-
sidered by a SMT system during decoding:
intuitively, the more hypotheses a n-gram ap-
pears in, the more confident the system is
that this n-gram is part of the correct trans-
lation, and the higher its posterior probabil-
ity is. The feature set contains 395 other fea-
tures that differs, in two ways, from the tra-
ditional features used in quality estimation.
First, it includes several features based on
large span continuous space language mod-
els (Le et al, 2011) that have already proved
their efficiency both for the translation task
and the quality estimation task. Second, each
feature was expanded into two ?normalized
forms? in which their value was divided ei-
ther by the source length or the target length
and, when relevant, into a ?ratio form? in
which the feature value computed on the tar-
get sentence is divided by its value computed
in the source sentence.
LORIA (T1.1): The system uses the 17 baseline
features, plus several numerical and boolean
features computed from the source and target
sentences (Langlois et al, 2012). These are
based on language model information (per-
plexity, level of back-off, intra-lingual trig-
gers), translation table (IBM1 table, inter-
lingual triggers). For language models, for-
ward and backward models are built. Each
feature gives a score to each word in the sen-
tence, and the score of the sentence is the av-
erage of word scores. For several features,
the score of a word depends on the score of its
neighbours. This leads to 66 features. Sup-
port Vector Machines are used to learn a re-
gression model. In training is done in a multi-
stage procedure aimed at increasing the size
of the training corpus. Initially, the train-
ing corpus with machine translated sentences
provided by the task is used to train an SVM
model. Then this model is applied to the post-
edited and reference sentences (also provided
as part of the task). These are added to the
quality estimation training corpus using as la-
bels the SVM predictions. An algorithm to
tune the predicted scores on a development
corpus is used.
SHEF (T1.1, T1.3): These submissions use
Gaussian Processes, a non-parametric prob-
abilistic learning framework for regression,
along with two techniques to improve predic-
tion performance and minimise the amount
of resources needed for the problem: feature
selection based on optimised hyperparame-
ters and active learning to reduce the training
set size (and therefore the annotation effort).
The initial set features contains all black box
and glass box features available within the
QUEST framework (Specia et al, 2013) for
the dataset at hand (160 in total for Task 1.1,
and 80 for Task 1.3). The query selection
strategy for active learning is based on the
informativeness of the instances using Infor-
mation Density, a measure that leverages be-
tween the variance among instances and how
dense the region (in the feature space) where
the instance is located is. To perform fea-
ture selection, following (Shah et al, 2013)
features are ranked by the Gaussian Process
23
algorithm according to their learned length
scales, which can be interpreted as the rel-
evance of such feature for the model. This
information was used for feature selection
by discarding the lowest ranked (least use-
ful) ones. based on empirical results found
in (Shah et al, 2013), the top 25 features for
both models were selected and used to retrain
the same regression algorithm.
UPC (T1.2): The methodology used a broad set
of features, mainly available through the last
version of the Asiya toolkit for MT evalua-
tion (Gonza`lez et al, 2012)17. Concretely,
86 features were derived for the German-to-
English and 97 features for the English-to-
Spanish tasks. These features cover differ-
ent approaches and include standard qual-
ity estimation features, as provided by the
above mentioned Asiya and QUEST toolk-
its, but also a variety of features based on
pseudo-references, explicit semantic analy-
sis and specialised language models trained
on the parallel and monolingual corpora pro-
vided by the WMT Translation Task.
The system selection task is approached by
means of pairwise ranking decisions. It uses
Random Forest classifiers with ties, expand-
ing the work of 402013cFormiga et al), from
which a full ranking can be derived and the
best system per sentence is identified. Once
the classes are given by the Random Forest,
one can build a graph by means of the adja-
cency matrix of the pairwise decision. The fi-
nal ranking is assigned through a dominance
scheme similar to Pighin et al (2012).
An important remark of the methodology is
the feature selection process, since it was no-
ticed that the learner was sensitive to the fea-
tures used. Selecting the appropriate set of
features was crucial to achieve a good per-
formance. The best feature combination was
composed of: i) a baseline quality estimation
feature set (Asiya or Quest) but not both of
them, ii) Length Model, iii) Pseudo-reference
aligned based features, and iv) adapted lan-
guage models. However, within the de-en
task, substituting Length Model and Aligned
Pseudo-references by the features based on
17http://asiya.lsi.upc.edu/
Semantic Roles could bring marginally bet-
ter accuracy.
TCD-CNGL (T1.1) and TCD-DCU-CNGL
(T1.3): The system is based on features
which are commonly used for style classifi-
cation (e.g. author identification). The as-
sumption is that low/high quality translations
can be characterised by some patterns which
are frequent and/or differ significantly from
the opposite category. Such features are in-
tended to focus on striking patterns rather
than to capture the global quality in a sen-
tence, but they are used in conjunction with
classical features for quality estimation (lan-
guage modelling, etc.). This requires two
steps in the training process: first the refer-
ence categories against which sentences will
be compared are built, then the standard qual-
ity estimation model training stage is per-
formed. Both datasets (Tasks 1.1 and 1.3)
were used for both tasks. Since the number
of features can be very high (up to 65,000),
a combination of various heuristics for se-
lecting features was used before the training
stage (the submitted systems were trained us-
ing SVM with RBF kernels).
UMAC (T1.1, T1.2, T2): For Task 1.1, the fea-
ture set consists in POS sequences of the
source and target languages, using 12 uni-
versal tags that are common in both lan-
guages. The algorithm is an enhanced ver-
sion of the BLEU metric (EBLEU) designed
with a modified length penalty and added re-
call factor, and having the precision and re-
call components grouped using the harmonic
mean. For Task 1.2, in addition to the uni-
versal POS sequences of the source and tar-
get languages, features include the scores of
length penalty, precision, recall and rank.
Variants of EBLEU with different strategies
for alignment are used, as well as a Na??ve
Bayes classification algorithm. For Task 2,
the features used are unigrams (from previous
4th to following 3rd tokens), bigrams (from
previous 2nd to following 2nd tokens), skip
bigrams (previous and next token), trigrams
(from previous 2nd to following 2nd tokens).
The learning algorithms are Conditional Ran-
dom Fields and Na??ve Bayes.
24
6.8 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing distance
Table 12 summarises the results for the ranking
variant of the task. They are sorted from best to
worse using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are CNGL SVRPLS, with a
DeltaAvg score of 11.09, and DCU-SYMC all-
types, with a DeltaAvg score of 10.13. While the
former holds the higher score, the difference is not
significant at the p ? 0.05 level as estimated by a
bootstrap resampling test.
Both submissions are better than the baseline
system by a very wide margin, a larger relative im-
provement than that obtained in the corresponding
WMT12 task. In addition, five submissions (out
of 12 systems) scored significantly higher than the
baseline system (systems above the middle gray
area), which is a larger proportion than that in last
year?s task (only 3 out of 16 systems), indicat-
ing that this shared task succeeded in pushing the
state-of-the-art performance to new levels.
In addition to the performance of the official
submission, we report results obtained by two or-
acle methods: the gold-label HTER metric com-
puted against the post-edited translations as ref-
erence (Oracle HTER), and the BLEU metric (1-
BLEU to obtain the same range as HTER) com-
puted against the same post-edited translations as
reference (Oracle HBLEU). The ?Oracle HTER?
DeltaAvg score of 16.38 gives an upperbound in
terms of DeltaAvg for the test set used in this eval-
uation. It indicates that, for this set, the differ-
ence in post-editing effort between the top quality
quantiles and the overall quality is 16.38 on aver-
age. The oracle based on HBLEU gives a lower
DeltaAvg score, which is expected since HTER
was our actual gold label. However, it is still
significantly higher than the score of the winning
submission, which shows that there is significant
room for improvement even by the highest scor-
ing submissions.
The results for the scoring variant of the task
are presented in Table 13, sorted from best to
worse by using the MAE metric scores as primary
key and the RMSE metric scores as secondary key.
According to MAE scores, the winning submis-
sion is SHEF FS (MAE = 12.42), which uses fea-
ture selection and a novel learning algorithm for
the task, Gaussian Processes. The baseline sys-
tem is measured to have an MAE of 14.81, with
six other submissions having performances that
are not different from the baseline at a statisti-
cally significant level, as shown by the gray area
in the middle of Table 13). Nine submissions (out
of 16) scored significantly higher than the base-
line system (systems above the middle gray area),
a considerably higher proportion of submissions
as compared to last year (5 out of 19), which indi-
cates that this shared task also succeeded in push-
ing the state-of-the-art performance to new levels
in terms of absolute scoring. Only one (6%) sys-
tem scored significantly lower than the baseline,
as opposed to 8 (42%) in last year?s task.
For the sake of completeness, we also show or-
acles figures using the same methods as for the
ranking variant of the task. Here the lowerbound
in error (Oracle HTER) will clearly be zero, as
both MAE and RMSE are measured against the
same gold label used for the oracle computation.
?Oracle HBLEU? is also not indicative in this
case, as the although the values for the two metrics
(HTER and HBLEU) are within the same ranges,
they are not directly comparable. This explains the
larger MAE/RMSE figures for ?Oracle HBLEU?
than those for most submissions.
Task 1.2 Selecting the best translation
Below we present the results for this task for each
of the two Kendall?s ? flavours presented in Sec-
tion 6.6, for the German-English test set (Tables 14
and 16) and the English-Spanish test set (Tables 15
and 17). The results are sorted from best to worse
using each of the Kendall?s ? metric flavours.
For German-English, the winning submission is
DFKI?s logRegFss33 entry, for both Kendall?s ?
with ties penalised and ties ignored, with ? = 0.31
(since this submission has no ties, the two met-
rics give the same ? value). A trivial baseline that
proposes random ranks (with ties allowed) has a
Kendall?s ? with ties penalised of -0.12 (as this
metric penalises the system?s ties that were non-
ties in the reference), and a Kendall?s ? with ties
ignored of 0.08. Most of the submissions per-
formed better than this simple baseline. More in-
terestingly perhaps is the comparison between the
best submission and the performance by an ora-
25
System ID DeltaAvg Spearman ?
? CNGL SVRPLS 11.09 0.55
? DCU-SYMC alltypes 10.13 0.59
SHEF FS 9.76 0.57
CNGL SVR 9.88 0.51
DCU-SYMC combine 9.84 0.59
CMU noB 8.98 0.57
SHEF FS-AL 8.85 0.50
Baseline bb17 SVR 8.52 0.46
CMU full 8.23 0.54
LIMSI 8.15 0.44
TCD-CNGL open 6.03 0.33
TCD-CNGL restricted 5.85 0.31
UMAC 2.74 0.11
Oracle HTER 16.38 1.00
Oracle HBLEU 15.74 0.93
Table 12: Official results for the ranking variant of the WMT13 Quality Estimation Task 1.1. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test. Oracle results that use human-references are also shown for comparison purposes.
System ID MAE RMSE
? SHEF FS 12.42 15.74
SHEF FS-AL 13.02 17.03
CNGL SVRPLS 13.26 16.82
LIMSI 13.32 17.22
DCU-SYMC combine 13.45 16.64
DCU-SYMC alltypes 13.51 17.14
CMU noB 13.84 17.46
CNGL SVR 13.85 17.28
FBK-UEdin extra 14.38 17.68
FBK-UEdin rand-svr 14.50 17.73
LORIA inctrain 14.79 18.34
Baseline bb17 SVR 14.81 18.22
TCD-CNGL open 14.81 19.00
LORIA inctraincont 14.83 18.17
TCD-CNGL restricted 15.20 19.59
CMU full 15.25 18.97
UMAC 16.97 21.94
Oracle HTER 0.00 0.00
Oracle HBLEU (1-HBLEU) 16.85 19.72
Table 13: Official results for the scoring variant of the WMT13 Quality Estimation Task 1.1. The winning submission is
indicated by a ? (it is significantly better than the other submissions according to bootstrap resampling (10k times) with 95%
confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant level
according to the same test. Oracle results that use human-references are also shown for comparison purposes.
26
German-English System ID Kendall?s ? with ties penalised
? DFKI logRegFss33 0.31
DFKI logRegFss24 0.28
CNGL SVRPLSF1 0.17
CNGL SVRF1 0.17
DCU CCG 0.15
UPC AQE+SEM+LM 0.11
UPC AQE+LeM+ALGPR+LM 0.10
DCU baseline+CCG 0.00
Baseline Random-ranks-with-ties -0.12
UMAC EBLEU-I -0.39
UMAC NB-LPR -0.49
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.19
Oracle BLEU (margin 0.01) 0.05
Oracle METEOR-ex (margin 0.00) 0.23
Oracle METEOR-ex (margin 0.01) 0.06
Table 14: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties penalised
? CNGL SVRPLSF1 0.15
CNGL SVRF1 0.13
DFKI logRegL2-411 0.09
DFKI logRegL2-431 0.04
UPC QQE+LeM+ALGPR+LM -0.03
UPC AQE+LeM+ALGPR+LM -0.06
CMU BLEUopt -0.11
Baseline Random-ranks-with-ties -0.23
UMAC EBLEU-A -0.27
UMAC EBLEU-I -0.35
CMU cls -0.63
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.17
Oracle BLEU (margin 0.02) -0.06
Oracle METEOR-ex (margin 0.00) 0.19
Oracle METEOR-ex (margin 0.02) 0.05
Table 15: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
27
German-English System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? DFKI logRegFss33 0.31 882/882
DFKI logRegFss24 0.28 882/882
UPC AQE+SEM+LM 0.27 768/882
UPC AQE+LeM+ALGPR+LM 0.24 788/882
DCU CCG 0.18 862/882
CNGL SVRPLSF1 0.17 882/882
CNGL SVRF1 0.17 881/882
Baseline Random-ranks-with-ties 0.08 718/882
DCU baseline+CCG 0.01 874/882
UMAC NB-LPR 0.01 447/882
UMAC EBLEU-I -0.03 558/882
Oracle Human 1.00 882/882
Oracle BLEU (margin 0.00) 0.22 859/882
Oracle BLEU (margin 0.01) 0.27 728/882
Oracle METEOR-ex (margin 0.00) 0.20 869/882
Oracle METEOR-ex (margin 0.01) 0.24 757/882
Table 16: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? CMU cls 0.23 192/633
CNGL SVRPLSF1 0.16 632/633
CNGL SVRF1 0.13 631/633
DFKI logRegL2-411 0.13 610/633
UPC QQE+LeM+ALGPR+LM 0.11 554/633
UPC AQE+LeM+ALGPR+LM 0.08 554/633
UMAC EBLEU-A 0.07 430/633
DFKI logRegL2-431 0.04 633/633
Baseline Random-ranks-with-ties 0.03 507/633
UMAC EBLEU-I 0.02 407/633
CMU BLEUopt -0.11 633/633
Oracle Human 1.00 633/633
Oracle BLEU (margin 0.00) 0.19 621/633
Oracle BLEU (margin 0.02) 0.26 474/633
Oracle METEOR-ex (margin 0.00) 0.25 623/633
Oracle METEOR-ex (margin 0.02) 0.28 517/633
Table 17: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
28
cle method that has access to human-created refer-
ences. This oracle uses human references to com-
pute BLEU and METEOR scores for each trans-
lation segment, and consequently computes rank-
ings for the competing translations based on these
scores. To reflect the impact of ties on the two
versions of Kendall?s ? metric we use, we allow
these ranks to be tied if the difference between the
oracle BLEU or METEOR scores is smaller than
a margin (see lower section of Tables 14 and 16,
with margins of 0 and 0.01 for the scores). For ex-
ample, under a regime of BLEU with margin 0.01,
a translation with BLEU score of 0.172 would get
the same rank as a translation with BLEU score of
0.164 (difference of 0.008), but a higher rank than
a translation with BLEU score of 0.158 (difference
of 0.014). Not surprisingly, under the Kendall?s
? with ties penalised the best Oracle BLEU or
METEOR performance happens for a 0.0 mar-
gin (which makes ties possible only for exactly-
matching scores), for a value of ? = 0.19 and
? = 0.23, respectively. Under the Kendall?s ? with
ties ignored, the Oracle BLEU performance for a
0.01 margin (i.e, translations under 1 BLEU point
should be considered as having the same rank)
achieves ? = 0.27, while Oracle METEOR for a
0.01 margin achieves ? = 0.24. These values are
lower than the ? = 0.31 of the winning submis-
sion without access to reference translations, sug-
gesting that quality estimation models are capable
of better modelling translation differences com-
pared to traditional, human reference-based MT
evaluation metrics.
For English-Spanish, under Kendall?s ? with
ties penalised the winning submission is CNGL?s
SVRPLSF1, with ? = 0.15. Under Kendall?s ?
with ties ignored, the best scoring submission is
CMU?s cls with ? = 0.23, but this is achieved
by offering non-tie judgements only for 192 of the
633 total judgements (30% of them). As we dis-
cussed in Section 6.6, the ?Kendall?s ? with ties
ignored? metric is weak with respect to compar-
ing different submissions, since it favours systems
that are do not commit to a given rank and rather
produce a large number of ties. This becomes even
clearer when we look at the performance of the or-
acle methods (Tables 15 and 17). Under Kendall?s
? with ties penalised, ?Oracle BLEU? (margin
0.00) achieves ? = 0.17, while under Kendall?s
? with ties ignored, ?Oracle BLEU? (margin 0.02)
has a ? = 0.26. This results in 474 non-tie deci-
sions (75% of them), and a better ? value com-
pared to ?Oracle BLEU? (margin 0.00), with a
? = 0.19 under the same metric. The oracle values
for both BLEU and METEOR are close to the ?
values of the winning submissions, supporting the
conclusion that quality estimation techniques can
successfully replace traditional, human reference-
based MT evaluation metrics.
Task 1.3 Predicting post-editing time
Results for this task are presented in Table 18.
A third of the submissions was able to beat the
baseline. Among these FBK-UEDIN?s submission
ranked best in terms of MAE, our main metric for
this task, and also achieved the lowest RMSE.
Only three systems were able to beat our base-
line in terms of MAE. Please note that while all
features were available to the participants, our
baseline is actually a competitive system.
The second-best entry, CNGL SVR, reached
the highest Spearman?s rank correlation, our sec-
ondary metric. Furthermore, in terms of this met-
ric all four top-ranking entries, two by CNGL and
FBK-UEDIN respectively, are significantly better
than the baseline (10k bootstrap resampling test
with 95% confidence intervals). As high ranking
submissions also yield strong rank correlation to
the observed post-editing time, we can be confi-
dent that improvements in MAE are not only due
to better handling of extreme cases.
Many participants submitted two variants of
their systems with different numbers of features
and/or machine learning approaches. In Table 18
we can see these are grouped closely together giv-
ing rise to the assumption that the general pool of
available features and thereby the used resources
and strongest features are most relevant for a sys-
tem?s performance. Another hint in that direction
is the observation the top-ranked systems rely on
additional data and resources to generate their fea-
tures.
Task 2 Predicting word-level scores
Results for this task are presented in Table 19 and
20, sorted by macro average F1. Since this is a
new task, we have yet to establish a strong base-
line. For reference we provide a trivial baseline
that predicts the dominant class ? (K)eep ? for ev-
ery token.
The first observation in Table 19 is that this triv-
ial baseline is difficult to beat in terms of accuracy.
However, considering our main metric ? macro-
29
System ID MAE RMSE Pearson?s r Spearman?s ?
? FBK-UEDIN Extra 47.5 82.6 0.65 0.75
? FBK-UEDIN Rand-SVR 47.9 86.7 0.66 0.74
CNGL SVR 49.2 90.4 0.67 0.76
CNGL SVRPLS 49.6 86.6 0.68 0.74
CMU slim 51.6 84.7 0.63 0.68
Baseline bb17 SVR 51.9 93.4 0.61 0.70
DFKI linear6 52.4 84.3 0.64 0.68
CMU full 53.6 92.2 0.58 0.60
DFKI pls8 53.6 88.3 0.59 0.67
TCD-DCU-CNGL SVM2 55.8 98.9 0.47 0.60
TCD-DCU-CNGL SVM1 55.9 99.4 0.48 0.60
SHEF FS 55.9 103.1 0.42 0.61
SHEF FS-AL 64.6 99.1 0.57 0.60
LIMSI elastic 70.6 114.4 0.58 0.64
Table 18: Official results for the Task 1.3 of the WMT13 Quality Estimation shared-task. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test.
Keep Change
System ID Accuracy Prec. Recall F1 Prec. Recall F1 Macro F1
? LIG FS BIN 0.74 0.79 0.86 0.82 0.56 0.43 0.48 0.65
? LIG BOOST BIN 0.74 0.78 0.88 0.83 0.57 0.37 0.45 0.64
CNGL GLM 0.70 0.76 0.86 0.80 0.47 0.31 0.38 0.59
UMAC NB 0.56 0.82 0.49 0.62 0.37 0.73 0.49 0.55
CNGL GLMd 0.71 0.74 0.93 0.82 0.51 0.19 0.28 0.55
UMAC CRF 0.71 0.72 0.98 0.83 0.49 0.04 0.07 0.45
Baseline (one class) 0.71 0.71 1.00 0.83 0.00 0.00 0.00 0.42
Table 19: Official results for Task 2: binary classification on word level of the WMT13 Quality Estimation shared-task. The
winning submissions are indicated by a ?.
System ID F1 Keep F1 Substitute F1 Delete Micro-F1 Macro-F1
? LIG FS MULT 0.83 0.44 0.072 0.72 0.45
? LIG ALL MULT 0.83 0.45 0.064 0.72 0.45
UMAC NB 0.62 0.43 0.042 0.52 0.36
CNGL GLM 0.83 0.18 0.028 0.71 0.35
CNGL GLMd 0.83 0.14 0.034 0.72 0.34
UMAC CRF 0.83 0.04 0.012 0.71 0.29
Baseline (one class) 0.83 0.00 0.000 0.71 0.28
Table 20: Official results for Task 2: multiclass classification on word level of the WMT13 Quality Estimation shared-task.
The winning submissions are indicated by a ?.
30
average F1 ? it is clear that all systems outperform
the baseline. The winning systems by LIG for the
binary task are also the top ranking systems on the
multiclass task.
While promising results are found for the bi-
nary variant of the task where systems are able to
achieve an F1 of almost 0.5 for the relevant class
? Change, the multiclass prediction variant of the
task seem to suffer from its severe class imbalance.
In fact, none of the systems shows good perfor-
mance when predicting deletions.
6.9 Discussion
In what follows, we discuss the main accomplish-
ments of this shared task starting from the goals
we had previously identified for it.
Explore various granularity levels for the
quality-prediction task The decision on which
level of granularity quality estimation is applied
depends strongly on the intended application. In
Task 2 we tested binary word-level classification
in a post-editing setting. If such annotation is pre-
sented through a user interface we imagine that
words marked as incorrect would be hidden from
the editor, highlighted as possibly wrong or that a
list of alternatives would we generated.
With respect to the poor improvements over
trivial baselines, we consider that the results for
word-level prediction could be mostly connected
to limitations of the datasets provided, which are
very small for word-level prediction, as compared
to successful previous work such as (Bach et al,
2011). Despite the limited amount of training
data, several systems were able to predict dubious
words (binary variant of the task), showing that
this can be a promising task. Extending the granu-
larity even further by predicting the actual editing
action necessary for a word yielded less positive
results than the binary setting.
We cannot directly compare sentence- and
word-level results. However, since sentence-level
predictions can benefit from more information
available and therefore more signal on which the
prediction is based, the natural conclusion is that,
if there is a choice in the prediction granularity,
to opt for the coarser one possible (i.e., sentence-
level over word-level). But certain applications
may require finer granularity levels, and therefore
word-level predictions can still be very valuable.
Explore the prediction of more objective scores
Given the multitude of possible applications for
quality estimation we must decide which predicted
values are both useful and accurate. In this year?s
task we have attempted to address the useful-
ness criterion by moving from the subjective, hu-
man judgement-based scores, to the prediction of
scores that can be more easily interpreted for prac-
tical applications: post-editing distance or types of
edits (word-level), post-editing time, and ranking
of alternative translations.
The general promise of using objective scores is
that predicting a value that is related to the use case
will make quality estimation more applicable and
yield lower deviance compared to the use of proxy
metrics. The magnitude of this benefit should be
sufficient to account for the possible additional ef-
fort related to collecting such scores.
While a direct comparison between the differ-
ent types of scores used for this year?s tasks is not
possible as they are based on different datasets, if
we compare last year?s task on predicting 1-5 lik-
ert scores (and generating an overall ranking of all
translations in the test set) with this year?s Task
1.1, which is virtually the same, but using post-
editing distance as gold-label, we see that the num-
ber of systems that outperform the baseline 18 is
proportionally larger this year. We can also notice
a higher relative improvement of these submis-
sions over the baseline system. While this could
simply be a consequence of progress in the field, it
may also provide an indication that objective met-
rics are more suitable for the problem.
Particularly with respect to post-editing time,
given that this label has a long tailed distribution
and is not trivial to measure even in a controlled
environment, the results of Task 1.3 are encour-
aging. Comparison with the better results seen
on Tasks 1.1 and 1.2, however, suggests that, for
Task 1.3, additional data processing, filtering, and
modelling (including modelling translator-specific
traits such as their variance in time) is required, as
evidenced in (Cohn and Specia, 2013).
Explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics When it comes to the task of au-
tomatically ranking alternative translations gener-
ated by different MT systems, the traditional use
of reference-based MT evaluation metrics is chal-
lenged by the findings of this task.
The top ranking quality estimation submissions
18The two baselines are exactly the same, and therefore the
comparison is meaningful.
31
to Task 1.2 have performances that outperform or
are at least at the same level with the ones that
involve the use of human references. The most in-
teresting property of these techniques is that, be-
ing reference-free, they can be used for any source
sentences, and therefore are ready to be deployed
for arbitrary texts.
An immediate application for this capability is
a procedure by which MT system-selection is per-
formed, based on the output of such quality esti-
mators. Additional measurements are needed to
determine the level of improvement in translation
quality that the current performance of these tech-
niques can achieve in a system-selection scenario.
Identify new and effective quality indicators
Quality indicators, or features, are core to the
problem of quality estimation. One significant dif-
ference this year with respect to previous year was
the availability of QUEST, a framework for the ex-
traction of a large number of features. A few sub-
missions used these larger sets ? as opposed to the
17 baseline features used in the 2012 edition ? as
their starting point, to which they added other fea-
tures. Most features available in this framework,
however, had already been used in previous work.
Novel families of features used this year which
seems to have played an important role are those
proposed by CNGL. They include a number of
language and MT-system independent monolin-
gual and bilingual similarity metrics between the
sentences for prediction and corpora of the lan-
guage pair under consideration. Based on standard
regression algorithm (the same used by the base-
line system), the submissions from CNGL using
such feature families topped many of the tasks.
Another interesting family of features is that
used by TCD-CNGL and TCD-DCU-CNGL for
Tasks 1.1 and 1.3. These were borrowed from
work on style or authorship identification. The as-
sumption is that low/high quality translations can
be characterised by some patterns which are fre-
quent and/or differ significantly from patterns be-
longing to the opposite category.
Like in last year?s task, the vast majority of
the participating systems used external resources
in addition to those provided for the task, par-
ticularly for linguistically-oriented features, such
as parsers, part-of-speech taggers, named entity
recognizers, etc. A novel set of syntactic fea-
tures based on Combinatory Categorial Grammar
(CCG) performed reasonably well in Task 1.2:
with six CCG-based features and no additional
features, the system outperformed the baseline
system and also a second submission where the
17 baseline features were added. This highlights
the potential of linguistically-motivated features
for the problem.
As expected, different feature sets were used
for different tasks. This is essential for Task 2,
where word-level features are certainly necessary.
For example, LIG used a number of lexical fea-
tures such as part-of-speech tag, word-posterior
probabilities, syntactic (constituent label, distance
to the constituent tree root, and target and source
polysemy count). For submissions where a se-
quence labelling algorithm such as a Conditional
Random Fields was used for prediction, the inter-
dependencies between adjacent words and labels
was also modelled though features.
Pseudo-references, i.e., scores from standard
evaluation metrics such as BLEU based on trans-
lations generated by an alternative MT system as
?reference?, featured in more than half of the sub-
missions for sentence-level tasks. This is not sur-
prising given their performance in previous work
on quality estimation.
Identify effective machine learning techniques
for all variants of the quality estimation task
For the sentence-level tasks, standard regression
methods such as SVR performed well as in the
previous edition of the shared task, topping the
results for the ranking variant of Task 1.1, both
first and second place. In fact this algorithm was
used by most submissions that outperformed the
baseline. An alternative algorithm to SVR with
very promising results and which was introduced
for the problem this year is that of Gaussian Pro-
cesses. It was used by SHEF, the winning submis-
sion in the scoring variant of Task 1.1, which also
performed well in the ranking variant, despite its
hyperparameters having been optimised for scor-
ing only. Algorithms behave similarly for Task
1.3, with SVR performing particularly well.
For Task 1.2, logistic regression performed the
best or among the best, along with SVR. One of
the most effective approach for this task, however,
appears to be one that is better tailored for the
task, namely pair-wise decomposition for ranking.
This approach benefits from transforming a k-way
ranking problem into a series of simpler, 2-way
ranking problems, which can be more accurately
solved. Another approach that shows promise is
32
that of ensemble of regressors, in which the output
is the results combining the predictions of differ-
ent regression models.
Linear-chain Conditional Random Fields are a
popular model of choice for sequence labelling
tasks and have been successfully used by several
participants in Task 2, along with discriminatively
trained Hidden Markov Models and Na??ve Bayes.
As in the previous edition, feature engineer-
ing and feature selection prior to model learning
were important components in many submissions.
However, the role of individual features is hard
to judge separately from the role of the machine
learning techniques employed.
Establish the state of the art performance All
four tasks addressed in this shared task have
achieved a dual role that is important for the re-
search community: (i) to make publicly available
new data sets that can serve to compare different
approaches and contributions; and (ii) to estab-
lish the present state-of-the-art performance in the
field, so that progress can be easily measured and
tracked. In addition, the public availability of the
scoring scripts makes evaluation and direct com-
parison straightforward.
Many participants submitted predictions for
several tasks. Comparison of the results shows
that there is little overlap between the best sys-
tems when the predicted value is varied. While
we did not formally require the participants to use
similar systems across tasks, these results indicate
that specialised systems with features selected de-
pending on the predicted variable can in fact be
beneficial.
As we mentioned before, compared to the pre-
vious edition of the task, we noticed (for Task
1.1) a larger relative improvement of scores over
the baseline system, as well as a larger propor-
tion of systems outperforming the baseline sys-
tems, which are a good indication that the field is
progressing over the years. For example, in the
scoring variant of Task 1.1, last year only 5 out of
20 systems (i.e. 25% of the systems) were able to
significantly outperform the baseline. This year, 9
out 16 systems (i.e. 56%) outperformed the same
baseline. Last year, the relative improvement of
the winning submission with respect to the base-
line system was 13%, while this year the relative
improvement is of 19%.
Overall, the tables of results presented in Sec-
tion 6.8 give a comprehensive view of the current
state-of-the-art on the data sets used for this shared
task, as well as indications on how much room
there still is for improvement via figures from ora-
cle methods. As a result, people interested in con-
tributing to research in these machine translation
quality estimation tasks will be able to do so in a
principled way, with clearly established state-of-
the-art levels and straightforward means of com-
parison.
7 Summary
As in previous incarnations of this workshop we
carried out an extensive manual and automatic
evaluation of machine translation performance,
and we used the human judgements that we col-
lected to validate automatic metrics of translation
quality. We also refined last year?s quality estima-
tion task, asking for methods that predict sentence-
level post-editing effort and time, rank translations
from alternative systems, and pinpoint words in
the output that are more likely to be wrong.
As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.19
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Google, Microsoft and Yandex.
We would also like to thank our colleagues Ma-
tous? Macha?c?ek and Martin Popel for detailed dis-
cussions.
References
Allauzen, A., Pe?cheux, N., Do, Q. K., Dinarelli,
M., Lavergne, T., Max, A., Le, H.-S., and Yvon,
F. (2013). LIMSI @ WMT13. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 60?67, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Almaghout, H. and Specia, L. (2013). A CCG-
based Quality Estimation Metric for Statistical
Machine Translation. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Ammar, W., Chahuneau, V., Denkowski, M., Han-
neman, G., Ling, W., Matthews, A., Murray,
19http://statmt.org/wmt13/results.html
33
K., Segall, N., Lavie, A., and Dyer, C. (2013).
The CMU machine translation systems at WMT
2013: Syntax, synthetic translation options, and
pseudo-references. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 68?75, Sofia, Bulgaria. Association for
Computational Linguistics.
Avramidis, E. (2012). Comparative Quality Es-
timation: Automatic Sentence-Level Ranking
of Multiple Machine Translation Outputs. In
Proceedings of 24th International Conference
on Computational Linguistics, pages 115?132,
Mumbai, India.
Avramidis, E. and Popovic, M. (2013). Selecting
feature sets for comparative and time-oriented
quality estimation of machine translation out-
put. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 327?
334, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Avramidis, E., Popovic?, M., Vilar, D., and Bur-
chardt, A. (2011). Evaluate with confidence es-
timation: Machine ranking of translation out-
puts using grammatical features. In Proceed-
ings of the Sixth Workshop on Statistical Ma-
chine Translation.
Aziz, W., Mitkov, R., and Specia, L. (2013).
Ranking Machine Translation Systems via Post-
Editing. In Proc. of Text, Speech and Dialogue
(TSD), Lecture Notes in Artificial Intelligence,
Berlin / Heidelberg. Za?padoc?eska? univerzita v
Plzni, Springer Verlag.
Bach, N., Huang, F., and Al-Onaizan, Y. (2011).
Goodness: A method for measuring machine
translation confidence. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 211?219, Portland, Ore-
gon, USA.
Beck, D., Shah, K., Cohn, T., and Specia, L.
(2013). SHEF-Lite: When less is more for
translation quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 335?340, Sofia, Bulgaria.
Association for Computational Linguistics.
Bic?ici, E., Groves, D., and van Genabith, J. (2013).
Predicting sentence translation quality using ex-
trinsic and language independent features. Ma-
chine Translation.
Bic?ici, E. and van Genabith, J. (2013). CNGL-
CORE: Referential translation machines for
measuring semantic similarity. In *SEM 2013:
The Second Joint Conference on Lexical and
Computational Semantics, Atlanta, Georgia,
USA. Association for Computational Linguis-
tics.
Bicici, E. (2013a). Feature decay algorithms
for fast deployment of accurate statistical ma-
chine translation systems. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 76?82, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Bicici, E. (2013b). Referential translation ma-
chines for quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 341?349, Sofia, Bulgaria.
Association for Computational Linguistics.
B??lek, K. and Zeman, D. (2013). CUni multilin-
gual matrix in the WMT 2013 shared task. In
Proceedings of the Eighth Workshop on Statis-
tical Machine Translation, pages 83?89, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Bojar, O., Kos, K., and Marec?ek, D. (2010). Tack-
ling Sparse Data Issue in Machine Translation
Evaluation. In Proceedings of the ACL 2010
Conference Short Papers, pages 86?91, Upp-
sala, Sweden. Association for Computational
Linguistics.
Bojar, O., Rosa, R., and Tamchyna, A. (2013).
Chimera ? three heads for English-to-Czech
translation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
90?96, Sofia, Bulgaria. Association for Compu-
tational Linguistics.
Borisov, A., Dlougach, J., and Galinskaya, I.
(2013). Yandex school of data analysis ma-
chine translation systems for WMT13. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, pages 97?101, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
34
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montre?al, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Buck, C., Turchi, M.,
and Negri, M. (2013). FBK-UEdin participa-
tion to the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 350?
356, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Charniak, E. and Johnson, M. (2005). Coarse-to-
fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual
Meeting on Association for Computational Lin-
guistics, pages 173?180. Association for Com-
putational Linguistics.
Cho, E., Ha, T.-L., Mediani, M., Niehues, J., Her-
rmann, T., Slawik, I., and Waibel, A. (2013).
The Karlsruhe Institute of Technology transla-
tion systems for the WMT 2013. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 102?106, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling Anno-
tator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality
Estimation. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (to appear).
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
de Marneffe, M.-C., MacCartney, B., and Man-
ning, C. D. (2006). Generating typed depen-
dency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Denkowski, M. and Lavie, A. (2010). Meteor-next
and the meteor paraphrase tables: improved
evaluation support for five target languages. In
Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR,
WMT ?10, pages 339?342, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.
Durgar El-Kahlout, I. and Mermer, C. (2013).
TU?btak-blgem german-english machine trans-
lation systems for w13. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 107?111, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Durrani, N., Fraser, A., Schmid, H., Sajjad, H.,
and Farkas, R. (2013a). Munich-Edinburgh-
Stuttgart submissions of OSM systems at
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
120?125, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Durrani, N., Haddow, B., Heafield, K., and Koehn,
P. (2013b). Edinburgh?s machine translation
systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 112?119, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
35
Eidelman, V., Wu, K., Ture, F., Resnik, P., and Lin,
J. (2013). Towards efficient large-scale feature-
rich statistical machine translation. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 126?131, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Fleiss, J. L. (1971). Measuring nominal scale
agreement among many raters. Psychological
Bulletin, 76(5):378?382.
Formiga, L., Costa-jussa`, M. R., Marin?o, J. B.,
Fonollosa, J. A. R., Barro?n-Ceden?o, A., and
Marquez, L. (2013a). The TALP-UPC phrase-
based translation systems for WMT13: System
combination with morphology generation, do-
main adaptation and corpus filtering. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 132?138, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Formiga, L., Gonza`lez, M., Barro?n-Ceden?o, A.,
Fonollosa, J. A. R., and Marquez, L. (2013b).
The TALP-UPC approach to system selection:
Asiya features and pairwise classification using
random forests. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 357?362, Sofia, Bulgaria. Association for
Computational Linguistics.
Formiga, L., Ma`rquez, L., and Pujantell, J.
(2013c). Real-life translation quality estimation
for mt system selection. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Galus?c?a?kova?, P., Popel, M., and Bojar, O. (2013).
PhraseFix: Statistical post-editing of TectoMT.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 139?145,
Sofia, Bulgaria. Association for Computational
Linguistics.
Gispert, A., Blackwood, G., Iglesias, G., and
Byrne, W. (2013). N-gram posterior probabil-
ity confidence measures for statistical machine
translation: an empirical study. Machine Trans-
lation, 27:85?114.
Gonza`lez, M., Gime?nez, J., and Ma`rquez, L.
(2012). A graphical interface for mt evaluation
and error analysis. In Proceedings of the ACL
2012 System Demonstrations, pages 139?144,
Jeju Island, Korea.
Green, S., Cer, D., Reschke, K., Voigt, R., Bauer,
J., Wang, S., Silveira, N., Neidert, J., and Man-
ning, C. D. (2013). Feature-rich phrase-based
translation: Stanford University?s submission to
the WMT 2013 translation task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 146?151, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hall, M., Frank, E., Holmes, G., Pfahringer,
B., Reutemann, P., and Witten, I. H. (2009).
The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Han, A. L.-F., Wong, D. F., Chao, L. S., Lu, Y., He,
L., Wang, Y., and Zhou, J. (2013). A descrip-
tion of tunable machine translation evaluation
systems in WMT13 metrics task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 412?419, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hildebrand, S. and Vogel, S. (2013). MT quality
estimation: The CMU system for WMT?13. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 371?377, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Hosmer, D. (1989). Applied logistic regression.
Wiley, New York, 8th edition.
Huet, S., Manishina, E., and Lefe`vre, F.
(2013). Factored machine translation systems
for Russian-English. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 152?155, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Hunt, E., Martin, J., and Stone, P. (1966). Experi-
ments in Induction. Academic Press, New York.
Kaplan, R., Riezler, S., King, T., Maxwell, J.,
Vasserman, A., and Crouch, R. (2004). Speed
and accuracy in shallow and deep stochastic
parsing. In Proceedings of the Human Lan-
guage Technology Conference and the 4th An-
nual Meeting of the North American Chapter of
the Association for Computational Linguistics
(HLT/NAACL 04).
36
Kauchak, D. and Barzilay, R. (2006). Paraphras-
ing for automatic evaluation. In Proceedings
of the main conference on Human Language
Technology Conference of the North American
Chapter of the Association of Computational
Linguistics, HLT-NAACL ?06, pages 455?462,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Koehn, P. (2012). Simulating human judgment in
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Kononenko, I. (1994). Estimating attributes: anal-
ysis and extensions of RELIEF. In Proceedings
of the European conference on machine learn-
ing on Machine Learning, pages 171?182, Se-
caucus, NJ, USA. Springer-Verlag New York,
Inc.
Kos, K. and Bojar, O. (2009). Evaluation of Ma-
chine Translation Metrics for Czech as the Tar-
get Language. Prague Bulletin of Mathematical
Linguistics, 92:135?147.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Langlois, D., Raybaud, S., and Sma??li, K. (2012).
Loria system for the wmt12 quality estimation
shared task. In Proceedings of the Seventh
Workshop on Statistical Machine Translation,
pages 114?119, Montre?al, Canada.
Langlois, D. and Smaili, K. (2013). LORIA sys-
tem for the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 378?
383, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Le, H. S., Oparin, I., Allauzen, A., Gauvain, J.-L.,
and Yvon, F. (2011). Structured output layer
neural network language model. In ICASSP,
pages 5524?5527.
Lin, C.-J., Weng, R. C., and Keerthi, S. S. (2007).
Trust region Newton methods for large-scale lo-
gistic regression. In Proceedings of the 24th
international conference on Machine learning
- ICML ?07, pages 561?568, New York, New
York, USA. ACM Press.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Macha?c?ek, M. and Bojar, O. (2013). Results of the
WMT13 metrics shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 43?49, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Matusov, E. and Leusch, G. (2013). Omnifluent
English-to-French and Russian-to-English sys-
tems for the 2013 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 156?161, Sofia, Bulgaria. Association for
Computational Linguistics.
McCallum, A. K. (2002). MALLET: a machine
learning for language toolkit.
Miceli Barone, A. V. and Attardi, G. (2013).
Pre-reordering for machine translation using
transition-based walks on dependency parse
trees. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 162?
167, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Moreau, E. and Rubino, R. (2013). An approach
using style classification features for quality es-
timation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
427?432, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Nadejde, M., Williams, P., and Koehn, P. (2013).
Edinburgh?s syntax-based machine translation
systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
168?174, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Okita, T., Liu, Q., and van Genabith, J. (2013).
Shallow semantically-informed PBSMT and
HPBSMT. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
175?182, Sofia, Bulgaria. Association for Com-
putational Linguistics.
O?zgu?r, A., O?zgu?r, L., and Gu?ngo?r, T. (2005). Text
37
categorization with class-based and corpus-
based keyword selection. In Proceedings of
the 20th International Conference on Computer
and Information Sciences, ISCIS?05, pages
606?615, Berlin, Heidelberg. Springer.
Peitz, S., Mansour, S., Huck, M., Freitag, M.,
Ney, H., Cho, E., Herrmann, T., Mediani,
M., Niehues, J., Waibel, A., Allauzen, A.,
Khanh Do, Q., Buschbeck, B., and Wand-
macher, T. (2013a). Joint WMT 2013 submis-
sion of the QUAERO project. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 183?190, Sofia, Bulgaria.
Association for Computational Linguistics.
Peitz, S., Mansour, S., Peter, J.-T., Schmidt, C.,
Wuebker, J., Huck, M., Freitag, M., and Ney,
H. (2013b). The RWTH aachen machine trans-
lation system for WMT 2013. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 191?197, Sofia, Bulgaria.
Association for Computational Linguistics.
Pighin, D., Formiga, L., and Ma`rquez, L.
(2012). A graph-based strategy to streamline
translation quality assessments. In Proceed-
ings of the Tenth Conference of the Associa-
tion for Machine Translation in the Americas
(AMTA?2012), San Diego, USA.
Pino, J., Waite, A., Xiao, T., de Gispert, A., Flego,
F., and Byrne, W. (2013). The University of
Cambridge Russian-English system at WMT13.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 198?203,
Sofia, Bulgaria. Association for Computational
Linguistics.
Post, M., Ganitkevitch, J., Orland, L., Weese, J.,
Cao, Y., and Callison-Burch, C. (2013). Joshua
5.0: Sparser, better, faster, server. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 204?210, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Rubino, R., Toral, A., Corte?s Va??llo, S., Xie, J.,
Wu, X., Doherty, S., and Liu, Q. (2013a). The
CNGL-DCU-Prompsit translation systems for
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
211?216, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Rubino, R., Wagner, J., Foster, J., Roturier, J.,
Samad Zadeh Kaljahi, R., and Hollowood, F.
(2013b). DCU-Symantec at the WMT 2013
quality estimation shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 390?395, Sofia, Bulgaria.
Association for Computational Linguistics.
Sajjad, H., Smekalova, S., Durrani, N., Fraser, A.,
and Schmid, H. (2013). QCRI-MES submis-
sion at WMT13: Using transliteration mining
to improve statistical machine translation. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 217?222, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Schmidt, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods
in Natural Language Processing.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An In-
vestigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings
of MT Summit XIV (to appear), Nice, France.
Singh, A. K., Wisniewski, G., and Yvon, F.
(2013). LIMSI submission for the WMT?13
quality estimation task: an experiment with n-
gram posteriors. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 396?402, Sofia, Bulgaria. Association for
Computational Linguistics.
Smith, J., Saint-Amand, H., Plamada, M., Koehn,
P., Callison-Burch, C., and Lopez, A. (2013).
Dirt cheap web-scale parallel text from the
Common Crawl. In Proceedings of the 2013
Conference of the Association for Computa-
tional Linguistics (ACL 2013), Sofia, Bulgaria.
Association for Computational Linguistics.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Snover, M., Madnani, N., Dorr, B. J., and
Schwartz, R. (2009). Fluency, adequacy, or
hter?: exploring different human judgments
with a tunable mt metric. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
38
lation, StatMT ?09, pages 259?268, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Soricut, R., Bach, N., and Wang, Z. (2012). The
SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceed-
ings of the 7th Workshop on Statistical Machine
Translation, pages 145?151.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L. (2011). Exploiting Objective Annota-
tions for Measuring Translation Post-editing Ef-
fort. In Proceedings of the 15th Conference of
the European Association for Machine Transla-
tion, pages 73?80, Leuven.
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Stolcke, A. (2002). SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the
7th International Conference on Spoken Lan-
guage Processing (ICSLP 2002), pages 901?
904.
Stone, M. and Brooks, R. J. (1990). Contin-
uum regression: cross-validated sequentially
constructed prediction embracing ordinary least
squares, partial least squares and principal com-
ponents regression. Journal of the Royal
Statistical Society Series B Methodological,
52(2):237?269.
Stymne, S., Hardmeier, C., Tiedemann, J., and
Nivre, J. (2013). Tunable distortion limits and
corpus cleaning for SMT. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 223?229, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Tantug, A. C., Oflazer, K., and El-Kahlout, I. D.
(2008). BLEU+: a Tool for Fine-Grained BLEU
Computation. In (ELRA), E. L. R. A., edi-
tor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08),
Marrakech, Morocco.
Weller, M., Kisselew, M., Smekalova, S., Fraser,
A., Schmid, H., Durrani, N., Sajjad, H., and
Farkas, R. (2013). Munich-Edinburgh-Stuttgart
submissions at WMT13: Morphological and
syntactic processing for SMT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 230?237, Sofia, Bulgaria.
Association for Computational Linguistics.
39
A Pairwise System Comparisons by Human Judges
Tables 21?30 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according bootstrap resampling (p ?
0.05). Gray lines separate clusters based on non-overlapping rank ranges.
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
M
ES
UE
DI
N
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
CU
-ZE
M
AN
CU
-TA
M
CH
YN
A
DC
U-
FD
A
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .50 .48? .43? .47? .43? .44? .38? .32? .25? .26?
ONLINE-B .50 ? .46? .48? .47? .49 .44? .40? .39? .29? .27?
MES .52? .54? ? .49 .47? .44? .45? .42? .41? .27? .25?
UEDIN .57? .52? .51 ? .51 .48? .47? .42? .39? .28? .25?
ONLINE-A .53? .53? .53? .49 ? .48 .51 .44? .42? .31? .30?
UEDIN-SYNTAX .57? .51 .56? .52? .52 ? .51 .43? .41? .29? .26?
CU-ZEMAN .56? .56? .55? .53? .49 .49 ? .45? .42? .32? .29?
CU-TAMCHYNA .62? .60? .58? .58? .56? .57? .55? ? .46? .35? .32?
DCU-FDA .68? .61? .59? .61? .58? .59? .58? .54? ? .32? .32?
JHU .75? .71? .73? .72? .69? .71? .68? .65? .68? ? .46?
SHEF-WPROA .74? .73? .75? .75? .70? .74? .71? .68? .68? .54? ?
score .60 .58 .57 .56 .54 .54 .53 .48 .45 .32 .29
rank 1 2-3 2-4 3-5 4-7 5-7 6-7 8 9 10 11
Table 21: Head to head comparison, ignoring ties, for Czech-English systems
CU
-B
OJ
AR
CU
-D
EP
FI
X
ON
LI
NE
-B
UE
DI
N
CU
-ZE
M
AN
M
ES
ON
LI
NE
-A
CU
-PH
RA
SE
FI
X
CU
-TE
CT
OM
T
CO
M
M
ER
CI
AL
-1
CO
M
M
ER
CI
AL
-2
SH
EF
-W
PR
OA
CU-BOJAR ? .51 .47? .44? .42? .43? .48 .41? .37? .39? .38? .33?
CU-DEPFIX .49 ? .48? .42? .43? .41? .47? .42? .40? .40? .39? .34?
ONLINE-B .53? .52? ? .47? .44? .44? .44? .44? .44? .41? .36? .34?
UEDIN .56? .58? .53? ? .47? .47? .48 .45? .44? .42? .43? .38?
CU-ZEMAN .58? .57? .56? .53? ? .49 .49 .48? .46? .47? .47? .35?
MES .57? .59? .56? .53? .51 ? .50 .47? .46? .43? .44? .42?
ONLINE-A .52 .53? .56? .52 .51 .50 ? .52 .47? .47? .47? .46?
CU-PHRASEFIX .59? .58? .56? .55? .52? .53? .48 ? .49 .48? .49 .42?
CU-TECTOMT .63? .60? .56? .56? .54? .54? .53? .51 ? .46? .46? .40?
COMMERCIAL-1 .61? .60? .59? .58? .53? .57? .53? .52? .54? ? .49 .42?
COMMERCIAL-2 .62? .61? .64? .57? .53? .56? .53? .51 .54? .51 ? .43?
SHEF-WPROA .67? .66? .66? .62? .65? .58? .54? .58? .60? .58? .57? ?
score .58 .57 .56 .52 .50 .50 .49 .48 .47 .45 .45 .38
rank 1-2 1-2 3 4 5-7 5-7 5-8 7-9 8-9 10-11 10-11 12
Table 22: Head to head comparison, ignoring ties, for English-Czech systems
40
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
UE
DI
N
QU
AE
RO
KI
T
M
ES
RW
TH
-JA
NE
M
ES
-SZ
EG
ED
-R
EO
RD
ER
-SP
LI
T
LI
M
SI
-N
CO
DE
-SO
UL
TU
BI
TA
K
UM
D
DC
U
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
DE
SR
T
ONLINE-B ? .48 .44? .37? .44? .41? .42? .40? .35? .37? .32? .31? .31? .27? .23? .18? .16?
ONLINE-A .52 ? .47 .45? .47 .43? .42? .41? .44? .40? .35? .36? .34? .31? .27? .25? .21?
UEDIN-SYNTAX .56? .53 ? .48 .46? .48? .46? .46? .45? .45? .35? .35? .34? .28? .25? .20? .19?
UEDIN .63? .55? .52 ? .51 .46? .47? .49 .44? .43? .39? .34? .35? .32? .28? .24? .22?
QUAERO .56? .53 .54? .49 ? .49 .52 .44? .46? .44? .39? .38? .37? .30? .31? .25? .21?
KIT .59? .57? .52? .54? .51 ? .45? .51 .43? .46? .37? .38? .41? .35? .31? .25? .21?
MES .58? .58? .54? .53? .48 .55? ? .49 .49 .46? .44? .37? .40? .34? .30? .26? .20?
RWTH-JANE .60? .59? .54? .51 .56? .49 .51 ? .46? .50 .45? .46? .47? .38? .33? .28? .20?
MES-SZEGED-REORDER-SPLIT .65? .56? .55? .56? .54? .57? .51 .54? ? .53? .44? .41? .41? .36? .34? .31? .21?
LIMSI-NCODE-SOUL .63? .60? .55? .57? .56? .54? .54? .50 .47? ? .51 .45? .43? .37? .34? .30? .22?
TUBITAK .68? .65? .65? .61? .61? .63? .56? .55? .56? .49 ? .48? .49 .39? .41? .30? .25?
UMD .69? .64? .65? .66? .62? .62? .63? .54? .59? .55? .52? ? .48? .41? .40? .33? .27?
DCU .69? .66? .66? .65? .63? .59? .60? .53? .59? .57? .51 .52? ? .41? .38? .37? .25?
CU-ZEMAN .73? .69? .72? .68? .70? .65? .66? .62? .64? .63? .61? .59? .59? ? .44? .43? .29?
JHU .77? .73? .75? .72? .69? .69? .70? .67? .66? .66? .59? .60? .62? .56? ? .43? .30?
SHEF-WPROA .82? .75? .80? .76? .75? .75? .74? .72? .69? .70? .70? .67? .63? .57? .57? ? .41?
DESRT .84? .79? .81? .78? .79? .79? .80? .80? .79? .78? .75? .73? .75? .71? .70? .59? ?
score .66 .62 .60 .58 .58 .57 .56 .54 .53 .52 .48 .46 .46 .39 .36 .31 .23
rank 1 2-3 2-3 4-5 4-5 5-7 6-7 8-9 8-10 9-10 11 12-13 12-13 14 15 16 17
Table 23: Head to head comparison, ignoring ties, for German-English systems
ON
LI
NE
-B
PR
OM
T
UE
DI
N-
SY
NT
AX
ON
LI
NE
-A
UE
DI
N
KI
T
ST
AN
FO
RD
LI
M
SI
-N
CO
DE
-SO
UL
M
ES
-R
EO
RD
ER
JH
U
CU
-ZE
M
AN
TU
BI
TA
K
UU SH
EF
-W
PR
OA
RW
TH
-JA
NE
ONLINE-B ? .55? .50 .45? .45? .34? .37? .37? .37? .32? .32? .33? .24? .21? .26?
PROMT .45? ? .48? .50 .43? .40? .39? .36? .37? .31? .31? .32? .27? .24? .27?
UEDIN-SYNTAX .50 .52? ? .57? .45? .43? .38? .41? .39? .38? .33? .33? .26? .25? .22?
ONLINE-A .55? .50 .43? ? .51 .42? .48 .41? .36? .44? .44? .38? .32? .27? .29?
UEDIN .55? .57? .55? .49 ? .52 .45? .45? .42? .43? .37? .34? .29? .27? .31?
KIT .66? .60? .57? .58? .48 ? .48 .45? .42? .36? .39? .40? .30? .29? .26?
STANFORD .63? .61? .62? .52 .55? .52 ? .50 .44? .48 .44? .43? .34? .29? .32?
LIMSI-NCODE-SOUL .63? .64? .59? .59? .55? .55? .50 ? .44? .44? .44? .47? .40? .34? .33?
MES-REORDER .63? .63? .61? .64? .58? .58? .56? .56? ? .50 .46? .49 .38? .37? .34?
JHU .68? .69? .62? .56? .57? .64? .52 .56? .50 ? .48? .45? .36? .37? .34?
CU-ZEMAN .68? .69? .67? .56? .63? .61? .56? .56? .54? .52? ? .48 .40? .33? .34?
TUBITAK .67? .68? .67? .62? .66? .60? .57? .53? .51 .55? .52 ? .38? .40? .32?
UU .76? .73? .74? .68? .71? .70? .66? .60? .62? .64? .60? .62? ? .44? .46?
SHEF-WPROA .79? .76? .75? .73? .73? .71? .71? .66? .63? .63? .67? .60? .56? ? .47?
RWTH-JANE .74? .73? .78? .71? .69? .74? .68? .67? .66? .66? .66? .68? .54? .53? ?
score .63 .63 .61 .58 .57 .55 .52 .50 .47 .47 .46 .45 .36 .32 .32
rank 1-2 1-2 3 3-5 4-6 5-6 7 8 9-11 9-11 10-12 11-12 13 14-15 14-15
Table 24: Head to head comparison, ignoring ties, for English-German systems
41
UE
DI
N-
HE
AF
IE
LD
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
ON
LI
NE
-A
M
ES
-SI
M
PL
IF
IE
DF
RE
NC
H
DC
U
RW
TH
CM
U-
TR
EE
-TO
-TR
EE
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .45? .46? .46? .42? .42? .34? .34? .29? .33? .31? .28? .24?
UEDIN .55? ? .52? .43? .45? .46? .40? .38? .33? .36? .33? .32? .23?
ONLINE-B .54? .48? ? .49 .46? .44? .45? .40? .38? .34? .36? .31? .26?
LIMSI-NCODE-SOUL .54? .57? .51 ? .52? .47 .45? .42? .38? .36? .34? .31? .28?
KIT .58? .55? .54? .48? ? .47 .46? .44? .39? .38? .37? .33? .28?
ONLINE-A .58? .54? .56? .53 .53 ? .47 .45? .40? .40? .39? .34? .32?
MES-SIMPLIFIEDFRENCH .66? .60? .55? .55? .54? .53 ? .48? .44? .40? .39? .39? .32?
DCU .66? .62? .60? .58? .56? .55? .52? ? .45? .45? .42? .41? .36?
RWTH .71? .67? .62? .62? .61? .60? .56? .55? ? .48? .47? .47? .38?
CMU-TREE-TO-TREE .67? .64? .66? .64? .62? .60? .60? .55? .52? ? .50 .48 .37?
CU-ZEMAN .69? .67? .64? .66? .63? .61? .61? .58? .53? .50 ? .47? .39?
JHU .72? .68? .69? .69? .67? .66? .61? .59? .53? .52 .53? ? .45?
SHEF-WPROA .76? .77? .74? .72? .72? .68? .68? .64? .62? .63? .61? .55? ?
score .63 .60 .59 .57 .56 .54 .51 .48 .43 .42 .42 .38 .32
rank 1 2-3 2-3 4-5 4-5 5-6 7 8 9-10 9-11 10-11 12 13
Table 25: Head to head comparison, ignoring ties, for French-English systems
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
PR
OM
T
ST
AN
FO
RD
M
ES
M
ES
-IN
FL
EC
TI
ON
RW
TH
-PH
RA
SE
-B
AS
ED
-JA
NE
ON
LI
NE
-A
DC
U
CU
-ZE
M
AN
JH
U
OM
NI
FL
UE
NT
IT
S-L
AT
L
IT
S-L
AT
L-P
E
UEDIN ? .49 .47? .48 .50 .44? .41? .40? .47? .39? .41? .35? .29? .30? .27? .24?
ONLINE-B .51 ? .46? .47? .47? .44? .49 .43? .43? .43? .38? .35? .36? .28? .25? .25?
LIMSI-NCODE-SOUL .53? .54? ? .45? .48 .48 .45? .43? .44? .45? .41? .32? .34? .30? .27? .27?
KIT .52 .53? .55? ? .48 .46? .45? .43? .45? .46? .38? .30? .33? .31? .29? .29?
PROMT .50 .53? .52 .52 ? .50 .48 .52? .45? .47 .48? .38? .36? .36? .34? .31?
STANFORD .56? .56? .52 .54? .50 ? .52 .48 .44? .49 .44? .39? .34? .36? .30? .29?
MES .59? .51 .55? .55? .52 .48 ? .52 .51 .45? .45? .36? .37? .34? .29? .29?
MES-INFLECTION .60? .57? .57? .57? .48? .52 .48 ? .54? .51 .46? .37? .35? .31? .33? .31?
RWTH-PHRASE-BASED-JANE .53? .57? .56? .55? .55? .56? .49 .46? ? .53 .49 .38? .36? .34? .35? .31?
ONLINE-A .61? .57? .55? .54? .53 .51 .55? .49 .47 ? .50 .45? .38? .38? .39? .35?
DCU .59? .62? .59? .62? .52? .56? .55? .54? .51 .50 ? .42? .40? .40? .36? .35?
CU-ZEMAN .65? .65? .68? .70? .62? .61? .64? .63? .62? .55? .58? ? .50 .42? .41? .37?
JHU .71? .64? .66? .67? .64? .66? .63? .65? .64? .62? .60? .50 ? .47? .42? .38?
OMNIFLUENT .70? .72? .70? .69? .64? .64? .66? .69? .66? .62? .60? .58? .53? ? .43? .42?
ITS-LATL .73? .75? .72? .71? .66? .70? .71? .67? .65? .61? .64? .59? .58? .57? ? .45?
ITS-LATL-PE .76? .75? .73? .71? .69? .71? .71? .69? .69? .65? .65? .63? .62? .58? .55? ?
score .60 .60 .58 .58 .55 .55 .54 .53 .53 .51 .49 .42 .40 .38 .35 .32
rank 1-2 1-3 2-4 3-4 5-7 5-8 5-8 6-9 7-10 9-11 10-11 12 13 14 15 16
Table 26: Head to head comparison, ignoring ties, for English-French systems
42
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
UE
DI
N
ON
LI
NE
-A
M
ES
LI
M
SI
-N
CO
DE
-SO
UL
DC
U
DC
U-
OK
IT
A
DC
U-
FD
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .49 .42? .45? .43? .40? .34? .43? .37? .34? .31? .15?
ONLINE-B .51 ? .49 .44? .46? .47? .42? .39? .40? .37? .37? .16?
UEDIN .58? .51 ? .55? .50 .47? .43? .42? .39? .39? .35? .14?
ONLINE-A .55? .56? .45? ? .50 .44? .45? .42? .42? .41? .37? .18?
MES .57? .54? .50 .50 ? .47? .45? .41? .41? .40? .38? .15?
LIMSI-NCODE-SOUL .60? .53? .53? .56? .53? ? .46? .45? .44? .43? .38? .18?
DCU .66? .58? .57? .55? .55? .54? ? .44? .47? .42? .41? .16?
DCU-OKITA .57? .61? .58? .58? .59? .55? .56? ? .49 .46? .46? .18?
DCU-FDA .63? .60? .61? .58? .59? .56? .53? .51 ? .48? .43? .18?
CU-ZEMAN .66? .63? .61? .59? .60? .57? .58? .54? .52? ? .43? .18?
JHU .69? .63? .65? .63? .62? .62? .59? .54? .57? .57? ? .22?
SHEF-WPROA .85? .84? .86? .82? .85? .82? .84? .82? .82? .82? .78? ?
score .62 .59 .57 .57 .56 .53 .51 .48 .48 .46 .42 .16
rank 1 2 3-5 3-5 3-5 6 7 8-9 8-9 10 11 12
Table 27: Head to head comparison, ignoring ties, for Spanish-English systems
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N
PR
OM
T
M
ES
TA
LP
-U
PC
LI
M
SI
-N
CO
DE
DC
U
DC
U-
FD
A
DC
U-
OK
IT
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
ONLINE-B ? .49 .45? .43? .38? .35? .34? .35? .37? .34? .33? .32? .23?
ONLINE-A .51 ? .49 .48 .38? .46? .42? .41? .43? .38? .38? .37? .31?
UEDIN .55? .51 ? .49 .46? .45? .43? .42? .36? .38? .38? .38? .26?
PROMT .57? .52 .51 ? .46? .48 .43? .43? .40? .37? .39? .34? .29?
MES .62? .62? .54? .54? ? .46? .44? .44? .41? .40? .43? .36? .32?
TALP-UPC .65? .54? .55? .52 .54? ? .50 .45? .44? .40? .40? .37? .32?
LIMSI-NCODE .66? .58? .57? .57? .56? .50 ? .46? .51 .48 .44? .45? .35?
DCU .65? .59? .58? .57? .56? .55? .54? ? .50 .48 .48 .45? .36?
DCU-FDA .63? .57? .64? .60? .59? .56? .49 .50 ? .53? .49 .42? .32?
DCU-OKITA .66? .62? .62? .63? .60? .60? .52 .52 .47? ? .50 .47? .36?
CU-ZEMAN .67? .62? .62? .61? .57? .60? .56? .52 .51 .50 ? .46? .40?
JHU .68? .63? .62? .66? .64? .63? .55? .55? .58? .53? .54? ? .37?
SHEF-WPROA .77? .69? .74? .71? .68? .68? .65? .64? .68? .64? .60? .63? ?
score .63 .58 .57 .56 .53 .52 .49 .47 .47 .45 .44 .41 .32
rank 1 2-4 2-4 3-4 5-6 5-6 7-8 7-9 8-10 9-11 10-11 12 13
Table 28: Head to head comparison, ignoring ties, for English-Spanish systems
43
ON
LI
NE
-B
CM
U
ON
LI
NE
-A
ON
LI
NE
-G
PR
OM
T
QC
RI
-M
ES
UC
AM
-M
UL
TI
FR
ON
TE
ND
BA
LA
GU
R
M
ES
-Q
CR
I
UE
DI
N
OM
NI
FL
UE
NT
-U
NC
NS
TR
LI
A
OM
NI
FL
UE
NT
-C
NS
TR
UM
D
CU
-K
AR
EL
CO
M
M
ER
CI
AL
-3
UE
DI
N-
SY
NT
AX
JH
U
CU
-ZE
M
AN
ONLINE-B ? .40? .42? .41? .37? .37? .41? .33? .33? .37? .33? .33? .35? .38? .34? .33? .29? .28? .14?
CMU .60? ? .50 .46? .43? .47? .42? .42? .39? .43? .41? .41? .40? .38? .36? .30? .30? .29? .17?
ONLINE-A .58? .50 ? .50 .51 .43? .47? .44? .40? .41? .43? .38? .40? .38? .38? .39? .34? .30? .19?
ONLINE-G .59? .54? .50 ? .55? .50 .51 .48 .42? .41? .44? .43? .46? .40? .44? .36? .34? .33? .19?
PROMT .63? .57? .49 .45? ? .43? .47? .43? .47? .47? .43? .39? .44? .43? .37? .41? .40? .38? .25?
QCRI-MES .63? .53? .57? .50 .57? ? .48 .46? .47? .45? .43? .45? .45? .38? .42? .37? .33? .40? .19?
UCAM-MULTIFRONTEND .59? .58? .53? .49 .53? .52 ? .47? .48 .46? .46? .42? .45? .46? .45? .40? .39? .33? .17?
BALAGUR .67? .58? .56? .52 .57? .54? .53? ? .47? .49 .45? .53? .40? .44? .44? .41? .36? .33? .23?
MES-QCRI .67? .61? .60? .58? .53? .53? .52 .53? ? .49 .47? .47? .43? .43? .44? .38? .42? .39? .17?
UEDIN .63? .57? .59? .59? .53? .55? .54? .51 .51 ? .48 .52 .44? .52 .49 .42? .43? .35? .21?
OMNIFLUENT-UNCNSTR .67? .59? .57? .56? .57? .57? .54? .55? .53? .52 ? .51 .46? .48 .48 .44? .40? .39? .25?
LIA .67? .59? .62? .57? .61? .55? .58? .47? .53? .48 .49 ? .51 .49 .48 .50 .41? .39? .20?
OMNIFLUENT-CNSTR .65? .60? .60? .54? .56? .55? .55? .60? .57? .56? .54? .49 ? .51 .48 .47? .40? .40? .25?
UMD .62? .62? .62? .60? .57? .62? .54? .56? .57? .48 .52 .51 .49 ? .53? .42? .46? .42? .19?
CU-KAREL .66? .64? .62? .56? .63? .58? .55? .56? .56? .51 .52 .52 .52 .47? ? .44? .40? .47? .24?
COMMERCIAL-3 .67? .70? .61? .64? .59? .63? .60? .59? .62? .58? .56? .50 .53? .58? .56? ? .51 .44? .32?
UEDIN-SYNTAX .71? .70? .66? .66? .60? .67? .61? .64? .58? .57? .60? .59? .60? .54? .60? .49 ? .45? .25?
JHU .72? .71? .70? .67? .62? .60? .67? .67? .61? .65? .61? .61? .60? .58? .53? .56? .55? ? .24?
CU-ZEMAN .86? .83? .81? .81? .75? .81? .83? .77? .83? .79? .75? .80? .75? .81? .76? .68? .75? .76? ?
score .65 .60 .58 .56 .56 .55 .54 .52 .51 .50 .49 .49 .48 .48 .47 .43 .41 .39 .21
rank 1 2-3 2-3 4-6 4-6 5-7 5-7 8-9 8-10 9-11 10-12 11-14 12-15 12-15 13-15 16 17 18 19
Table 29: Head to head comparison, ignoring ties, for Russian-English systems
PR
OM
T
ON
LI
NE
-B
CM
U
ON
LI
NE
-G
ON
LI
NE
-A
UE
DI
N
QC
RI
-M
ES
CU
-K
AR
EL
M
ES
-Q
CR
I
JH
U
CO
M
M
ER
CI
AL
-3
LI
A
BA
LA
GU
R
CU
-ZE
M
AN
PROMT ? .44? .39? .47 .46? .36? .37? .37? .32? .35? .28? .30? .32? .24?
ONLINE-B .56? ? .44? .41? .44? .38? .37? .35? .33? .39? .33? .31? .35? .24?
CMU .61? .56? ? .52 .49 .47? .43? .41? .39? .44? .44? .40? .35? .28?
ONLINE-G .53 .59? .48 ? .48 .50 .48 .46 .46? .42? .38? .43? .38? .36?
ONLINE-A .54? .56? .51 .52 ? .47 .49 .49 .48 .44? .38? .40? .40? .34?
UEDIN .64? .62? .53? .50 .53 ? .49 .46? .42? .39? .44? .41? .38? .29?
QCRI-MES .63? .63? .57? .52 .51 .51 ? .48 .45? .44? .42? .39? .40? .29?
CU-KAREL .63? .65? .59? .54 .51 .54? .52 ? .50 .46? .43? .40? .42? .34?
MES-QCRI .68? .67? .61? .54? .52 .58? .55? .50 ? .48? .47? .43? .45? .34?
JHU .65? .61? .56? .58? .56? .61? .56? .54? .52? ? .51 .44? .44? .33?
COMMERCIAL-3 .72? .67? .56? .62? .62? .56? .58? .57? .53? .49 ? .52 .48 .44?
LIA .70? .69? .60? .57? .60? .59? .61? .60? .57? .56? .48 ? .47? .41?
BALAGUR .68? .65? .65? .62? .60? .62? .60? .58? .55? .56? .52 .53? ? .41?
CU-ZEMAN .76? .76? .72? .64? .66? .71? .71? .66? .66? .67? .56? .59? .59? ?
score .64 .62 .55 .54 .53 .53 .52 .49 .47 .46 .43 .42 .41 .33
rank 1 2 3-4 3-6 3-7 4-7 5-7 8 9-10 9-10 11-12 11-13 12-13 14
Table 30: Head to head comparison, ignoring ties, for English-Russian systems
44
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 337?342,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
SHEF-Lite: When Less is More for Translation Quality Estimation
Daniel Beck and Kashif Shah and Trevor Cohn and Lucia Specia
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{debeck1,kashif.shah,t.cohn,l.specia}@sheffield.ac.uk
Abstract
We describe the results of our submissions
to the WMT13 Shared Task on Quality
Estimation (subtasks 1.1 and 1.3). Our
submissions use the framework of Gaus-
sian Processes to investigate lightweight
approaches for this problem. We focus on
two approaches, one based on feature se-
lection and another based on active learn-
ing. Using only 25 (out of 160) fea-
tures, our model resulting from feature
selection ranked 1st place in the scoring
variant of subtask 1.1 and 3rd place in
the ranking variant of the subtask, while
the active learning model reached 2nd
place in the scoring variant using only
?25% of the available instances for train-
ing. These results give evidence that
Gaussian Processes achieve the state of
the art performance as a modelling ap-
proach for translation quality estimation,
and that carefully selecting features and
instances for the problem can further im-
prove or at least maintain the same per-
formance levels while making the problem
less resource-intensive.
1 Introduction
The purpose of machine translation (MT) quality
estimation (QE) is to provide a quality prediction
for new, unseen machine translated texts, with-
out relying on reference translations (Blatz et al,
2004; Specia et al, 2009; Callison-burch et al,
2012). A common use of quality predictions is
the decision between post-editing a given machine
translated sentence and translating its source from
scratch, based on whether its post-editing effort is
estimated to be lower than the effort of translating
the source sentence.
The WMT13 QE shared task defined a group
of tasks related to QE. In this paper, we present
the submissions by the University of Sheffield
team. Our models are based on Gaussian Pro-
cesses (GP) (Rasmussen and Williams, 2006), a
non-parametric probabilistic framework. We ex-
plore the application of GP models in two con-
texts: 1) improving the prediction performance by
applying a feature selection step based on opti-
mised hyperparameters and 2) reducing the dataset
size (and therefore the annotation effort) by per-
forming Active Learning (AL). We submitted en-
tries for two of the four proposed tasks.
Task 1.1 focused on predicting HTER scores
(Human Translation Error Rate) (Snover et al,
2006) using a dataset composed of 2254 English-
Spanish news sentences translated by Moses
(Koehn et al, 2007) and post-edited by a profes-
sional translator. The evaluation used a blind test
set, measuring MAE (Mean Absolute Error) and
RMSE (Root Mean Square Error), in the case of
the scoring variant, and DeltaAvg and Spearman?s
rank correlation in the case of the ranking vari-
ant. Our submissions reached 1st (feature selec-
tion) and 2nd (active learning) places in the scor-
ing variant, the task the models were optimised
for, and outperformed the baseline by a large mar-
gin in the ranking variant.
The aim of task 1.3 aimed at predicting post-
editing time using a dataset composed of 800
English-Spanish news sentences also translated by
Moses but post-edited by five expert translators.
Evaluation was done based on MAE and RMSE
on a blind test set. For this task our models were
not able to beat the baseline system, showing that
more advanced modelling techniques should have
been used for challenging quality annotation types
and datasets such as this.
2 Features
In our experiments, we used a set of 160 features
which are grouped into black box (BB) and glass
box (GB) features. They were extracted using the
337
open source toolkit QuEst1 (Specia et al, 2013).
We briefly describe them here, for a detailed de-
scription we refer the reader to the lists available
on the QuEst website.
The 112 BB features are based on source and
target segments and attempt to quantify the source
complexity, the target fluency and the source-
target adequacy. Examples of them include:
? Word and n-gram based features:
? Number of tokens in source and target
segments;
? Language model (LM) probability of
source and target segments;
? Percentage of source 1?3-grams ob-
served in different frequency quartiles of
the source side of the MT training cor-
pus;
? Average number of translations per
source word in the segment as given by
IBM 1 model with probabilities thresh-
olded in different ways.
? POS-based features:
? Ratio of percentage of nouns/verbs/etc
in the source and target segments;
? Ratio of punctuation symbols in source
and target segments;
? Percentage of direct object personal or
possessive pronouns incorrectly trans-
lated.
? Syntactic features:
? Source and target Probabilistic Context-
free Grammar (PCFG) parse log-
likelihood;
? Source and target PCFG average confi-
dence of all possible parse trees in the
parser?s n-best list;
? Difference between the number of
PP/NP/VP/ADJP/ADVP/CONJP
phrases in the source and target;
? Other features:
? Kullback-Leibler divergence of source
and target topic model distributions;
? Jensen-Shannon divergence of source
and target topic model distributions;
1http://www.quest.dcs.shef.ac.uk
? Source and target sentence intra-lingual
mutual information;
? Source-target sentence inter-lingual mu-
tual information;
? Geometric average of target word prob-
abilities under a global lexicon model.
The 48 GB features are based on information
provided by the Moses decoder, and attempt to in-
dicate the confidence of the system in producing
the translation. They include:
? Features and global score of the SMT model;
? Number of distinct hypotheses in the n-best
list;
? 1?3-gram LM probabilities using translations
in the n-best to train the LM;
? Average size of the target phrases;
? Relative frequency of the words in the trans-
lation in the n-best list;
? Ratio of SMT model score of the top transla-
tion to the sum of the scores of all hypothesis
in the n-best list;
? Average size of hypotheses in the n-best list;
? N-best list density (vocabulary size / average
sentence length);
? Fertility of the words in the source sentence
compared to the n-best list in terms of words
(vocabulary size / source sentence length);
? Edit distance of the current hypothesis to the
centre hypothesis;
? Proportion of pruned search graph nodes;
? Proportion of recombined graph nodes.
3 Model
Gaussian Processes are a Bayesian non-parametric
machine learning framework considered the state-
of-the-art for regression. They assume the pres-
ence of a latent function f : RF ? R, which maps
a vector x from feature space F to a scalar value.
Formally, this function is drawn from a GP prior:
f(x) ? GP(0, k(x,x?))
which is parameterized by a mean function (here,
0) and a covariance kernel function k(x,x?). Each
338
response value is then generated from the function
evaluated at the corresponding input, yi = f(xi)+
?, where ? ? N (0, ?2n) is added white-noise.
Prediction is formulated as a Bayesian inference
under the posterior:
p(y?|x?,D) =
?
f
p(y?|x?, f)p(f |D)
where x? is a test input, y? is the test response
value andD is the training set. The predictive pos-
terior can be solved analitically, resulting in:
y? ? N (kT? (K + ?2nI)?1y,
k(x?, x?)? kT? (K + ?2nI)?1k?)
where k? = [k(x?,x1)k(x?,x2) . . . k(x?,xd)]T
is the vector of kernel evaluations between the
training set and the test input and K is the kernel
matrix over the training inputs.
A nice property of this formulation is that y?
is actually a probability distribution, encoding the
model uncertainty and making it possible to inte-
grate it into subsequent processing. In this work,
we used the variance values given by the model in
an active learning setting, as explained in Section
4.
The kernel function encodes the covariance
(similarity) between each input pair. While a vari-
ety of kernel functions are available, here we fol-
lowed previous work on QE using GP (Cohn and
Specia, 2013; Shah et al, 2013) and employed
a squared exponential (SE) kernel with automatic
relevance determination (ARD):
k(x,x?) = ?2f exp
(
?12
F?
i=1
xi ? x?i
li
)
where F is the number of features, ?2f is the co-
variance magnitude and li > 0 are the feature
length scales.
The resulting model hyperparameters (SE vari-
ance ?2f , noise variance ?2n and SE length scales li)
were learned from data by maximising the model
likelihood. In general, the likelihood function is
non-convex and the optimisation procedure may
lead to local optima. To avoid poor hyperparam-
eter values due to this, we performed a two-step
procedure where we first optimise a model with all
the SE length scales tied to the same value (which
is equivalent to an isotropic model) and we used
the resulting values as starting point for the ARD
optimisation.
All our models were trained using the GPy2
toolkit, an open source implementation of GPs
written in Python.
3.1 Feature Selection
To perform feature selection, we followed the ap-
proach used in Shah et al (2013) and ranked the
features according to their learned length scales
(from the lowest to the highest). The length scales
of a feature can be interpreted as the relevance of
such feature for the model. Therefore, the out-
come of a GP model using an ARD kernel can be
viewed as a list of features ranked by relevance,
and this information can be used for feature selec-
tion by discarding the lowest ranked (least useful)
ones.
For task 1.1, we performed this feature selection
over all 160 features mentioned in Section 2. For
task 1.3, we used a subset of the 80 most general
BB features as in (Shah et al, 2013), for which we
had all the necessary resources available for the
extraction. We selected the top 25 features for both
models, based on empirical results found by Shah
et al (2013) for a number of datasets, and then
retrained the GP using only the selected features.
4 Active Learning
Active Learning (AL) is a machine learning
paradigm that let the learner decide which data it
wants to learn from (Settles, 2010). The main goal
of AL is to reduce the size of the dataset while
keeping similar model performance (therefore re-
ducing annotation costs). In previous work with
17 baseline features, we have shown that with only
?30% of instances it is possible to achieve 99%
of the full dataset performance in the case of the
WMT12 QE dataset (Beck et al, 2013).
To investigate if a reduced dataset can achieve
competitive performance in a blind evaluation set-
ting, we submitted an entry for both tasks 1.1 and
1.3 composed of models trained on a subset of in-
stances selected using AL, and paired with fea-
ture selection. Our AL procedure starts with a
model trained on a small number of randomly se-
lected instances from the training set and then uses
this model to query the remaining instances in the
training set (our query pool). At every iteration,
the model selects the more ?informative? instance,
asks an oracle for its true label (which in our case
is already given in the dataset, and therefore we
2http://sheffieldml.github.io/GPy/
339
only simulate AL) and then adds it to the training
set. Our procedure started with 50 instances for
task 1.1 and 20 instances for task 1.3, given its re-
duced training set size. We optimised the Gaussian
Process hyperparameters every 20 new instances,
for both tasks.
As a measure of informativeness we used Infor-
mation Density (ID) (Settles and Craven, 2008).
This measure leverages between the variance
among instances and how dense the region (in the
feature space) where the instance is located is:
ID(x) = V ar(y|x)?
(
1
U
U?
u=1
sim(x,x(u))
)?
The ? parameter controls the relative impor-
tance of the density term. In our experiments, we
set it to 1, giving equal weights to variance and
density. The U term is the number of instances
in the query pool. The variance values V ar(y|x)
are given by the GP prediction while the similar-
ity measure sim(x,x(u)) is defined as the cosine
distance between the feature vectors.
In a real annotation setting, it is important to
decide when to stop adding new instances to the
training set. In this work, we used the confidence
method proposed by Vlachos (2008). This is an
method that measures the model?s confidence on
a held-out non-annotated dataset every time a new
instance is added to the training set and stops the
AL procedure when this confidence starts to drop.
In our experiments, we used the average test set
variance as the confidence measure.
In his work, Vlachos (2008) showed a correla-
tion between the confidence and test error, which
motivates its use as a stop criterion. To check if
this correlation also occurs in our task, we measure
the confidence and test set error for task 1.1 using
the WMT12 split (1832/422 instances). However,
we observed a different behaviour in our experi-
ments: Figure 1 shows that the confidence does
not raise or drop according to the test error but it
stabilises around a fixed value at the same point as
the test error also stabilises. Therefore, instead of
using the confidence drop as a stop criterion, we
use the point where the confidence stabilises. In
Figure 2 we can observe that the confidence curve
for the WMT13 test set stabilises after ?580 in-
stances. We took that point as our stop criterion
and used the first 580 selected instances as the AL
dataset.
Figure 1: Test error and test confidence curves
for HTER prediction (task 1.1) using the WMT12
training and test sets.
Figure 2: Test confidence for HTER prediction
(task 1.1) using the official WMT13 training and
test sets.
We repeated the experiment with task 1.3, mea-
suring the relationship between test confidence
and error using a 700/100 instances split (shown
on Figure 3). For this task, the curves did not fol-
low the same behaviour: the confidence do not
seem to stabilise at any point in the AL proce-
dure. The same occurred when using the official
training and test sets (shown on Figure 4). How-
ever, the MAE curve is quite flat, stabilising after
about 100 sentences. This may simply be a conse-
quence of the fact that our model is too simple for
post-editing time prediction. Nevertheless, in or-
der to analyse the performance of AL for this task
we submitted an entry using the first 100 instances
chosen by the AL procedure for training.
The observed peaks in the confidence curves re-
340
Task 1.1 - Ranking Task 1.1 - Scoring Task 1.3
DeltaAvg ? Spearman ? MAE ? RMSE ? MAE ? RMSE ?
SHEF-Lite-FULL 9.76 0.57 12.42 15.74 55.91 103.11
SHEF-Lite-AL 8.85 0.50 13.02 17.03 64.62 99.09
Baseline 8.52 0.46 14.81 18.22 51.93 93.36
Table 1: Submission results for tasks 1.1 and 1.3. The bold value shows a winning entry in the shared
task.
Figure 3: Test error and test confidence curves
for post-editing time prediction (task 1.3) using a
700/100 split on the WMT13 training set.
Figure 4: Test confidence for post-editing time
prediction (task 1.3) using the official WMT13
training and test sets.
sult from steps where the hyperparameter optimi-
sation got stuck at bad local optima. These de-
generated results set the variances (?2f , ?2n) to very
high values, resulting in a model that considers all
data as pure noise. Since this behaviour tends to
disappear as more instances are added to the train-
ing set, we believe that increasing the dataset size
helps to tackle this problem. We plan to investi-
gate this issue in more depth in future work.
For both AL datasets we repeated the feature se-
lection procedure explained in Section 3.1, retrain-
ing the models on the selected features.
5 Results
Table 1 shows the results for both tasks. SHEF-
Lite-FULL represents GP models trained on the
full dataset (relative to each task) with a feature
selection step. SHEF-Lite-AL corresponds to the
same models trained on datasets obtained from
each active learning procedure and followed by
feature selection.
For task 1.1, our submission SHEF-Lite-FULL
was the winning system in the scoring subtask, and
ranked third in the ranking subtask. These results
show that GP models achieve the state of the art
performance in QE. These are particularly positive
results considering that there is room for improve-
ment in the feature selection procedure to identify
the optimal number of features to be selected. Re-
sults for task 1.3 were below the baseline, once
again evidencing the fact that the noise model used
in our experiments is probably too simple for post-
editing time prediction. Post-editing time is gener-
ally more prone to large variations and noise than
HTER, especially when annotations are produced
by multiple post-editors. Therefore we believe that
kernels that encode more advanced noise models
(such as the multi-task kernel used by Cohn and
Specia (2013)) should be used for better perfor-
mance. Another possible reason for that is the
smaller set of features used for this task (black-
box features only).
Our SHEF-Lite-AL submissions performed bet-
ter than the baseline in both scoring and ranking
in task 1.1, ranking 2nd place in the scoring sub-
task. Considering that the dataset is composed by
only ?25% of the full training set, these are very
encouraging results in terms of reducing data an-
341
notation needs. We note however that these results
are below those obtained with the full training set,
but Figure 1 shows that it is possible to achieve
the same or even better results with an AL dataset.
Since the curves shown in Figure 1 were obtained
using the full feature set, we believe that advanced
feature selection strategies can help AL datasets to
achieve better results.
6 Conclusions
The results obtained by our submissions confirm
the potential of Gaussian Processes to become the
state of the art approach for Quality Estimation.
Our models were able to achieve the best perfor-
mance in predicting HTER. They also offer the ad-
vantage of inferring a probability distribution for
each prediction. These distributions provide richer
information (like variance values) that can be use-
ful, for example, in active learning settings.
In the future, we plan to further investigate these
models by devising more advanced kernels and
feature selection methods. Specifically, we want
to employ our feature set in a multi-task kernel set-
ting, similar to the one proposed by Cohn and Spe-
cia (2013). These kernels have the power to model
inter-annotator variance and noise, which can lead
to better results in the prediction of post-editing
time.
We also plan to pursue better active learning
procedures by investigating query methods specif-
ically tailored for QE, as well as a better stop cri-
teria. Our goal is to be able to reduce the dataset
size significantly without hurting the performance
of the model. This is specially interesting in the
case of QE, since it is a very task-specific problem
that may demand a large annotation effort.
Acknowledgments
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from the EU FP7-ICT QTLaunchPad project
(No. 296347, Kashif Shah and Lucia Specia).
References
Daniel Beck, Lucia Specia, and Trevor Cohn. 2013.
Reducing Annotation Effort for Quality Estimation
via Active Learning. In Proceedings of ACL (to ap-
pear).
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315?321.
Chris Callison-burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of 7th Workshop
on Statistical Machine Translation.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL (to appear).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1070?1079.
Burr Settles. 2010. Active learning literature survey.
Technical report.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV (to appear).
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving
the confidence of machine translation quality esti-
mates. In Proceedings of MT Summit XII.
Lucia Specia, Kashif Shah, Jose? G. C. De Souza, and
Trevor Cohn. 2013. QuEst - A translation qual-
ity estimation framework. In Proceedings of ACL
Demo Session (to appear).
Andreas Vlachos. 2008. A stopping criterion for
active learning. Computer Speech & Language,
22(3):295?312, July.
342
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 202?211,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Multilingual WSD-like Constraints for Paraphrase Extraction
Wilker Aziz
Research Group in Computational Linguistics
University of Wolverhampton, UK
W.Aziz@wlv.ac.uk
Lucia Specia
Department of Computer Science
University of Sheffield, UK
L.Specia@sheffield.ac.uk
Abstract
The use of pivot languages and word-
alignment techniques over bilingual cor-
pora has proved an effective approach for
extracting paraphrases of words and short
phrases. However, inherent ambiguities in
the pivot language(s) can lead to inade-
quate paraphrases. We propose a novel ap-
proach that is able to extract paraphrases
by pivoting through multiple languages
while discriminating word senses in the in-
put language, i.e., the language to be para-
phrased. Text in the input language is an-
notated with ?senses? in the form of for-
eign phrases obtained from bilingual par-
allel data and automatic word-alignment.
This approach shows 62% relative im-
provement over previous work in generat-
ing paraphrases that are judged both more
accurate and more fluent.
1 Introduction
Paraphrases are alternative ways of expressing a
given meaning. Generating paraphrases that go
beyond morphological variants of the original text
is a challenging problem and has been shown to
be useful in many natural language applications.
These include i) expanding the set of reference
translations for Machine Translation (MT) eval-
uation (Denkowski and Lavie, 2010; Liu et al,
2010) and parameter optimisation (Madnani et al,
2007), where multiple reference translations are
important to accommodate for valid variations of
system translations; ii) addressing the problem of
out-of-vocabulary words or phrases in MT, either
by replacing these by paraphrases that are known
to the MT system (Mirkin et al, 2009) or by ex-
panding the phrase table with new translation al-
ternatives (Callison-Burch et al, 2006); and iii)
expanding queries for improved coverage in ques-
tion answering (Riezler et al, 2007).
Bannard and Callison-Burch (2005) introduced
an approach to paraphrasing which has shown par-
ticularly promising results by pivoting through dif-
ferent languages for which bilingual parallel data
is available. The approach consists in aligning
phrases in the bilingual parallel corpus to find
pairs of phrases (e1, e2) in the input language, i.e.,
the language to be paraphrased, which typically
align to the same foreign phrases F = {f : e1 ?
f ? e2}. This intermediate language is called
pivot language and the phrases f ? F that support
the equivalence (e1, e2) are called pivot phrases.
If there exists a non-empty set of pivots connect-
ing e1 to e2, e2 is said to be a paraphrase of e1. The
paraphrase is scored in terms of the conditional
probabilities observed in the parallel corpus1 by
marginalising out the pivot phrases that support
the alignment (e1, e2) as shown in Equation 1.
p(e2|e1) =
?
f?F
p(f |e1)p(e2|f) (1)
Equation 1 allows paraphrases to be extracted
by using multiple pivot languages such that these
languages help discard inadequate paraphrases re-
sulting from ambiguous pivot phrases. However
in this formulation all senses of the input phrase
are mixed together in a single distribution. For ex-
ample, for the Spanish input phrase acabar con,
both paraphrases superar (overcome) and elim-
inar (eliminate) may be adequate depending on
the context, however they are not generally in-
terchangeable. In (Bannard and Callison-Burch,
1The distributions p(f |e) and p(e|f) are extracted from
relative counts in word-aligned parallel corpus.
202
2005), the distributions learnt from different bilin-
gual corpora are combined through a simple av-
erage. This makes the model naturally favour
the most frequent senses of the phrases, assigning
very low probabilities to less frequent senses. Sec-
tion 5 shows evidence of how this limitation makes
paraphrases with certain senses unreachable.
We propose a novel formulation of the problem
of generating paraphrases that is constrained by
sense information in the form of foreign phrases,
which can be thought of as a quasi-sense annota-
tion. Using a bilingual parallel corpus to annotate
phrases with their quasi-senses has proved help-
ful in building word-sense disambiguation (WSD)
models for MT (Carpuat and Wu, 2007; Chan et
al., 2007): instead of monolingual senses, pos-
sible translations of phrases obtained with word-
alignment were used as senses. Our approach per-
forms paraphrase extraction by pivoting through
multiple languages while penalising senses of the
input that are not supported by these pivots.
Our experiments show that the proposed ap-
proach can effectively eliminate inadequate para-
phrases for polysemous phrases, with a significant
improvement over previous approaches. We ob-
serve absolute gains of 15-25% in precision and
recall in generating paraphrases that are judged
fluent and meaning preserving in context.
This paper is structured as follows: Section 2
describes additional previous work on paraphrase
extraction and pivoting. Section 3 presents the
proposed model. Section 4 introduces our experi-
mental settings, while Section 5 shows the results
of a series of experiments.
2 Related work
In addition to the well-known approach by (Ban-
nard and Callison-Burch, 2005), the following
previous approaches using pivot languages for
paraphrasing can be mentioned. For a recent
and comprehensive survey on a number of data-
driven paraphrase generation methods, we refer
the reader to (Madnani and Dorr, 2010).
Cohn and Lapata (2007) make use of multi-
ple parallel corpora to improve Statistical Ma-
chine Translation (SMT) by triangulation for lan-
guages with little or no source-target parallel data
available. Translation tables are learnt by pivot-
ing through languages for which source-pivot and
pivot-target bilingual corpora can be found. Multi-
ple pivot languages were found useful to preserve
the meaning of the source in the triangulated trans-
lation, as different languages are likely to realise
ambiguities differently. Although their findings
apply to generating translation candidates, the in-
put phrases are not constrained to specific senses,
and as a consequence multiple translations, which
are valid in different contexts but not generally
interchangeable, are mixed together in the same
distribution. In SMT the target Language Model
(LM) helps selecting the adequate translation can-
didate in context.
Callison-Burch (2008) extends (Bannard and
Callison-Burch, 2005) by adding syntactic con-
straints to the model. Paraphrase extraction is
done by pivoting using word-alignment informa-
tion, as before, but sentences are syntactically
annotated and paraphrases are restricted to those
with the same syntactic category. This addresses
categorial ambiguity by preventing that words
with a given category (e.g. a noun) are para-
phrased by words with other categories (e.g., a
verb). However, the approach does not solve the
more complex issue of polysemous paraphrases:
words with the same category but different mean-
ings, such as the noun bank as financial institution
and land alongside a river/lake.
Marton et al (2009) derive paraphrases from
monolingual data using distributional similarity
metrics. The approach has the advantage of not re-
quiring bilingual parallel data, but it suffers from
issues typical of distributional similarity metrics.
In particular, it produces paraphrases that share the
same or similar contexts but are related in ways
that do not always characterise paraphrasing, such
as antonymy.
3 Paraphrasing through multilingual
constraints
Our approach to paraphrasing can be applied to
both individual words or sequences of words of
any length, conditioned only on sufficient evi-
dence of these segments in a parallel corpus. We
use segments as provided by the standard phrase
extraction process from phrase-based SMT ap-
proaches (see Section 4), which in most cases
range from individual words to short sequences of
words (up to seven words in our case). Hereafter,
we refer to these segments simply as phrases.
A model for paraphrasing under a constrained
set of senses should take into account both the
input phrase and the sense tag while selecting
203
Paired with en de nl da sv fi fr it pt el
es 1.78 1.56 1.62 1.61 1.51 1.58 1.65 1.51 1.60 5.68
en - 1.73 1.82 1.78 1.67 1.74 1.82 1.73 1.78 1.06
Table 1: Size of the bilingual parallel corpora in millions of sentence pairs
the pivot phrases that will lead to adequate para-
phrases. In our approach a sense tag consists in a
phrase in a foreign language, that is, a valid trans-
lation of the input phrase in a language of interest,
here referred to as target language. Treating the
target language vocabulary as a sense repository is
a good strategy from both theoretical and practi-
cal perspectives: it has been shown that monolin-
gual sense distinctions can be effectively captured
by translations into second languages, especially
as language family distance increases (Resnik and
Yarowsky, 1999; Specia et al, 2006). These trans-
lations can be easily captured given the avail-
ability of bilingual parallel data and robust au-
tomatic word-alignment techniques (Carpuat and
Wu, 2007; Chan et al, 2007).
Figure 1 illustrates the proposed model to pro-
duce sense tagged paraphrases. We start the pro-
cess at e1 and we need to make sure that the pivot
phrases f ? F align back to the input language,
producing the paraphrase e2, and to the target lan-
guage, producing the sense tag q. To avoid com-
puting the distribution p(e2, q|f) ? which would
require a trilingual parallel corpus ? we assume
that e2 and q are conditionally independent on f :
p(e2, q|f)
e2??q|f= p(e2|f)p(q|f)
In other words, we assume that pivot phrases gen-
erate paraphrases and sense tags independently.
Equation 2 shows how paraphrase probabilities are
computed by marginalising out the pivot phrases
under this assumption.
GFED@ABCe1 //GFED@ABCf

// GFED@ABCe2
?>=<89:;q
Figure 1: Pivot phrases must align back to target
phrases (sense annotation).
p(e2|e1, q) =
1
z
?
f?F
p(e2|f)p(q|f)p(f |e1) (2)
In order to constrain the extraction of para-
phrases such that it complies with a sense repos-
itory, in addition to bilingual parallel corpora be-
tween the input language and the pivot languages,
our model requires bilingual parallel corpora be-
tween the pivot languages and the language that is
used for sense annotation.
Callison-Burch (2007) discusses factors affect-
ing paraphrase quality, one of which is word
senses. Paraphrasing through pivoting essentially
relies on the hypothesis that different pivot phrases
can be used to identify synonymy, rather than pol-
ysemy (an assumption made in the WSD liter-
ature). Callison-Burch (2007) also proposes an
extraction procedure that may be conditioned on
specific contexts of the input phrase (Bannard
and Callison-Burch, 2005), where the context is
a given pivot phrase.2 However, that model is un-
able to pivot through multiple languages. As we
show in Section 5, this makes the model extremely
sensitive to ambiguities of the one phrase used as
both sense tag and pivot.
The model we propose attempts to perform
sense-disambiguated paraphrase extraction, that
is, paraphrases are discovered in the context of
translation candidates of the input phrases. In ad-
dition, it allows the use of multiple pivot languages
in the process, capitalising on both the WSD
and the paraphrase assumption. While the target
phrases discriminate different senses of the input
phrases, the pivot phrases coming from multiple
languages bring extra evidence to jointly capture
the ambiguities introduced by the target phrases
themselves.
To illustrate the impact of this contribution, con-
sider the polysemous Spanish word forma, and
some of its translations into English extracted
from our corpus (Section 4): kind, way, means
and form. The English words distinguish three
possible senses of forma: (a) means/way of do-
ing/achieving something, (b) shape, and (c) type
or group sharing common traits. The model pre-
sented in (Bannard and Callison-Burch, 2005)
cannot discriminate these senses. It mixes valid
senses of forma and (correctly) proposes the para-
phrases manera and modo for sense (a), and tipo
2A paraphrase is scored in the context of a given pivot
phrase f : p(e2|e1, f) = p(e2|f)p(f |e1).
204
for sense (c). However, paraphrases for sense (b)
are over penalised and account for very little of the
probability mass of the candidate paraphrases of
forma. Their extension which conditions extrac-
tion on a given pivot phrase is highly sensitive to
the ambiguities of the phrase used as sense anno-
tation. Table 5 shows how this model (CB-wsd in
the Table) makes mistakes for most senses of the
input due to the ambiguities of the English context
kind, way, means and form. Our approach (multi
in the Table) on the other hand successfully sep-
arates paraphrases according to the sense annota-
tion provided.
4 Experimental settings
4.1 Resources
The source of bilingual data used in the experi-
ments is the Europarl collection (Koehn, 2005).
We paraphrase Spanish (es) phrases using their
corresponding English (en) phrases as sense tags
and nine European languages as pivots: Ger-
man (de), Dutch (nl), Danish (da), Swedish (sv),
Finnish (fi), French (fr), Italian (it), Portuguese
(pt) and Greek (el). The tools provided along
with the corpus were used to extract the sentence
aligned parallel data as shown in Table 1.
The sentence aligned parallel data is first word-
aligned using GIZA++ in both source-target and
target-source directions, followed by the applica-
tion of traditional symmetrisation heuristics (Och
and Ney, 2003). These aligned corpora are used
for paraphrase extraction, except for a subset of
them used in the creation of a test set (Section 4.2).
4.2 Test set creation
Since we are interested in showing the ability
of our approach to find adequate paraphrases in
the presence of a foreign phrase (the sense tag),
it is important that our test set contains polyse-
mous phrases. Like in (Bannard and Callison-
Burch, 2005), we use the Spanish WordNet3 to
bias our selection of phrases to paraphrase to con-
tain ambiguous cases. However, rather than bi-
asing selection towards having more multi-word
expressions, we chose to have more polysemous
cases. From the Spanish WordNet, we selected 50
phrases (with at least one content word) to be para-
phrased such that 80% of the samples (40 phrases)
had at least 2 senses (with a given part-of-speech
3http://nlp.lsi.upc.edu/freeling/
Unambiguous Ambiguous
concreto,
pol??tica, fon-
dos, regular,
haber, amor
proprio, sangre
fr??a, dar a luz,
dar con, tomar
el pelo
derecho, comercial, real, particular, le-
gal, justo, comu?n, cerca, esencial, es-
pecial, fuerte, puesto, oficial, figura,
informe, parte, cuenta, forma, claro,
clave, tiempo, seguro, respuesta, traba-
jar, responder, garantizar, volver, au-
mentar, incluir, tratar, ofrecer, estable-
cer, pasar, dejar, realizar, punto de vista,
llevar a cabo, dar vueltas, tener que,
acabar con
Figure 2: Words and phrases selected to be para-
phrased. Ambiguity is determined on the basis of
the number of synsets in the Spanish WordNet. We
note that this information was only used to bias the
selection of the phrases, i.e., WordNet is not used
in the proposed approach.
La idea de conceder a la Unio?n Europea su propia compe-
tencia fiscal - la palabra clave es el ?impuesto por Europa?
- esta? siendo debatida.
The idea of granting the EU its own tax competence - the
keyword is the ?Europe tax? - is being discussed.
Figure 3: Example of context selected for the
phrase clave.
tag to avoid selecting simpler, categorial ambigui-
ties). Figure 2 lists the selected words and phrases
in their base forms.
The bilingual corpus was queried for sentences
containing at least one of the 50 phrases listed in
Figure 2, or any of their morphological variants.
The resulting sentences were then grouped on the
basis of whether or not they shared the same En-
glish translation. To find the English phrase (i.e.,
our sense tag) which constrains the sense of the
Spanish phrase, we followed the heuristics used in
phrase-based SMT to extract the minimal phrase
pair that includes the Spanish phrase and is con-
sistent with the word-alignment4 (Koehn et al,
2003). We discarded groups containing fewer than
five sentence pairs and randomly sampled 2-6 con-
texts per Spanish phrase. The resulting test set is
made of 258 Spanish phrases in context such as
the one exemplified in Figure 3.
4.3 Paraphrasing
Nine pivot languages were used to constrain para-
phrase extraction following the approach pre-
sented in Section 3. The conditional probabil-
ity distributions over phrase pairs in Equation 2
are estimated using relative frequencies. For each
Spanish phrase in the test set, we retrieve their
4Note that we did not use gold-standard word-alignments.
205
paraphrase candidates grouped by sense (English
translation) and rank them based on the evidence
collected from all bilingual corpora. Evidence
from different pivot languages is combined using
their average. English itself was not used as a pivot
language. It was used only to provide sense tags.
The rationale behind this choice is that if the lan-
guage used to provide sense tags is also used as
pivot language, there is no obvious way of esti-
mating p(q|f) in Equation 2. Note that in this case
this probability would represent the likelihood of
the English phrase aligning to itself.
Similar to (Bannard and Callison-Burch, 2005),
we weight our paraphrase probabilities using an
LM to adjust it to the context of the input sentence.
We use a 5-gram LM trained on the Spanish part of
Europarl with the SRILM toolkit (Stolcke, 2002).
Paraphrases are re-ranked in context by multiply-
ing the paraphrase probability and the LM score of
the sentence.5
In order to assess the performance of our model,
we compare it to two variants of the models pro-
posed by Bannard and Callison-Burch (2005).
multi: the paraphrasing model with multilingual
constraints introduced in this paper.
CCB: the model in (Bannard and Callison-
Burch, 2005) which does not explicitly per-
form any sense disambiguation.
CCB-wsd: an extended model in (Bannard and
Callison-Burch, 2005) using English phrases
as sense tags for pivoting.
Using each of these three models, we para-
phrased the 258 samples in our test set, retrieving
the 3-best paraphrases in context for each model.
CCB is used with 10 pivot languages (English is
included as a pivot) to generate paraphrase candi-
dates. Note that CCB relies solely on the LM com-
ponent to fit the paraphrase candidate to the con-
text. On the other hand, CCB-wsd and multi both
have access to sense annotation, but while multi
is able to benefit from multiple pivot languages,
CCB-wsd can only pivot through the one English
phrase provided as sense annotation.
5Given the localised effect of the phrase replacement
within a given context in terms of n-gram language mod-
elling, a neighbourhood of n-1 words on each side of the
selected phrase is sufficient to re-rank paraphrase candidates:
p(w?4 . . . w?1e2w+1 . . . w+4) for our 5-gram LM.
4.4 Evaluation
To assess whether the proposed model effectively
disambiguates senses of candidate paraphrases,
we perform experiments using similar settings
to those in (Bannard and Callison-Burch, 2005).
Paraphrases are evaluated in context (a sentence)
using binary human judgements in terms of the
following components:
Meaning (M): whether or not the candidate con-
veys the meaning of the original phrase; and
Grammar (G): whether or not the candidate pre-
serves the fluency of the sentence.
These two components are assessed separately and
a paraphrase candidate is considered to be cor-
rect only when it is judged to be both meaning
preserving and grammatical. Our evaluators were
presented with one pair of sentences at a time, the
original one and its paraphrased version. For ev-
ery test sample we selected the 3-best paraphrases
of each method and distributed them amongst the
evaluators. We considered two evaluation scenar-
ios:
Gold-standard translations: the English trans-
lation as found in Europarl was taken as
sense tag, using automatic word-alignments
to identify the English phrase that constrains
the sense of the Spanish phrase.
SMT translations: a phrase-based SMT system
built using the Moses toolkit (Koehn et al,
2007) and the whole Spanish-English dataset
(except the sentences in the test set) was
used to translated the Spanish sentences. In-
stead of gold-standard translations as a quasi-
perfect sense annotation (quasi because the
word-alignment is still automatic and thus
prone to errors), the phrase-based SMT sys-
tem plays the role of a sense annotation mod-
ule predicting the ?sense? tags.
Note that models may not be able to produce
a paraphrase for certain input phrases, e.g. when
the input phrase is not found in the bilingual cor-
pora. Therefore, we assess precision (P) and re-
call (R) as the number of paraphrases in context
that are judged correct out of the number of cases
for which a candidate paraphrase was proposed,
and out of the total number of test samples, re-
spectively. To summarise the results, accuracy is
expressed in terms of F1.
206
Method Top M G CorrectF1 F1 P R F1
CCB 1 32 28 25 25 25
CCB-wsd 1 61 38 34 28 30
multi 1 62 55 59 42 49
CCB 2 41 37 33 33 33
CCB-wsd 2 68 44 40 33 36
multi 2 71 64 66 47 55
CCB 3 46 42 37 37 37
CCB-wsd 3 71 47 45 36 40
multi 3 74 67 71 50 59
Table 2: Performance in retrieving paraphrases in
context using gold-standard translations for sense
tags and a 5-gram LM component.
In the following section we present results on
whether the best candidate (Top-1) or at least one
of the two (Top-2) or three (Top-3) best candidates
satisfies the criterion under consideration (mean-
ing/grammar).
5 Results
The evaluation was performed by seven native
speakers of Spanish who judged a total of 5, 110
sentences containing one paraphrased input phrase
each. We used 40 overlapping judgements across
annotators to measure inter-annotator agreement.
The average inter-annotator agreement in terms
of Cohen?s Kappa (Cohen, 1960) is 0.54 ? 0.15
for meaning judgements, 0.63 ? 0.16 for gram-
mar judgements and 0.62 ? 0.20 for correctness
judgements. These figures are similar or superior
to those reported in (Bannard and Callison-Burch,
2005; Callison-Burch, 2008), which we consider
particularly encouraging as in our case we have
seven instead of only two annotators. In Tables
2, 3 and 4 we report the performance of the three
models in terms of precision, recall and F1, with
p-values < 0.01 based on the t-test for statistical
significance.
5.1 Paraphrasing from human translations
We first assess the paraphrasing models us-
ing gold-standard translations, that is, the En-
glish phrases were selected via automatic word-
alignments between the input text and its corre-
sponding human translation from Europarl. Ta-
ble 2 shows the performance in terms of F1 for
our three criteria: meaning preservation, grammat-
icality, and correctness. Our method (multi) out-
performs the best performing alternative (CCB-
wsd) by a large margin. It is 19% more effective
at selecting the 1-best candidate in terms of cor-
Method M G Correct
CCB 33 23 22
CCB-wsd 19 9 8
multi 64 43 37
Table 3: Performance (F1) in correctly retrieving
the best paraphrase in context using gold-standard
translations without the 5-gram LM component.
rectness. A consistent gain is also observed when
more guesses are allowed (top 2?3), showing that
our model is better at ranking the top candidates
as well. CCB-wsd and multi are close in terms of
paraphrases that are meaning preserving, however
their differences become more obvious as more
guesses are allowed, again showing that multi is
better at ranking more adequate paraphrases first.
Moreover, multi consistently chooses more gram-
matical paraphrases.
Table 2 also shows that our model consistently
improves both the precision and recall of the pre-
dictions. Recall improves by 14% w.r.t. CCB-wsd
because multi is able to find more paraphrases,
which we believe are only reachable through the
additional pivots. For example, in our data the
paraphrase forma ? medio in the sense of way
(see Table 5) is only found through the Dutch
pivot middel, which is not accessible to CCB-
wsd. Recall is much lower in CCB because of
the model?s strong bias towards the most frequent
senses: other senses receive very little of the prob-
ability mass and thus rarely feature amongst the
top ranked paraphrases. Our multilingual disam-
biguation model also shows a 25% increase in pre-
cision, which must be due to the stronger contri-
bution of the sense discrimination over the LM
component in getting the senses of the paraphrases
right.
To show the impact of the LM re-ranking com-
ponent, in Table 3 we remove this component from
all models, such that the ranking of paraphrases is
done purely based on the paraphrase probabilities.
All models are harmed by the absence of the LM
component, but to different extents and for differ-
ent reasons. CCB typically ranks at the top para-
phrases that convey the most frequent sense and
the LM is the only component with information
about the input context. CCB-wsd is impacted the
most: typically invalid paraphrases are produced
from unrelated senses of the foreign phrase used
as sense tag, they do not represent any valid sense
of the input but still get ranked at the top. For
207
this model, the LM component is crucial to prune
such unrelated paraphrases. Back to Table 2, the
superior performance of CCB-wsd over CCB in
the presence of the LM component suggest that
CCB-wsd assigns less negligible probabilities to
the paraphrases that convey a valid sense of the
input. Finally, multi?s performance is only truly
harmed in terms of grammaticality: sense discrim-
ination is the main responsible for selecting the
appropriate sense, while the LM component is re-
sponsible for selecting the candidate that makes
the sentence more fluent. Further investigation
showed that in some cases the most meaning pre-
serving option was down-weighted due to low flu-
ency, and a less adequate option was chosen, ex-
plaining the slight improvement under the mean-
ing preservation criterion when no LM re-ranking
is performed.
Table 5 lists the 5-best paraphrases of the Span-
ish phrase forma in its different senses. The para-
phrases are ranked by CCB-wsd and multi out of
context, that is, without LM re-ranking. Note that,
because the sense tags are themselves ambiguous
in English, most of the top-ranked paraphrases
from CCB-wsd are inadequate, that is, they do not
convey any valid sense of forma.
It is also interesting to observe the impact of the
different pivot languages on the performance of
our proposed approach. Figure 4 shows CCB-wsd
and multi, both using LM re-ranking. For multi
we can see the impact of the pivot languages indi-
vidually and in groups.6 Except for Finnish when
used on its own as pivot all other setups are supe-
rior to CCB-wsd. We can also see that putting to-
gether languages of different families has a strong
positive impact, probably due to the fact that am-
biguities are realised differently in languages that
are farther from each other, emphasising the po-
tential of sense discrimination by pivoting through
multiple languages.
5.2 Paraphrasing from machine translations
Finally, we assessed the paraphrasing models us-
ing machine translations instead of gold-standard
translations from Europarl. In order to have an
idea of the quality of the SMT model beforehand,
we evaluated the machine translations in terms of
BLEU scores (Papineni et al, 2002) using a single
reference from Europarl. Our phrase-based SMT
6For a larger version of this figure, we refer the reader
to: http://pers-www.wlv.ac.uk/?in1676/
publications/2013/conll2013pivots.pdf
Method Top M G CorrectF1 F1 P R F1
CCB-wsd 1 71 39 34 32 33
multi 1 69 55 50 45 48
CCB-wsd 2 79 46 40 38 39
multi 2 82 69 63 57 60
CCB-wsd 3 83 50 44 41 42
multi 3 85 74 69 62 65
Table 4: Performance in retrieving paraphrases in
context using machine translations for sense tags
and a 5-gram LM component.
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
en fi el da de fr nl pt sv it sv,fi el,fi sv,el
it,sv it,fi it,el sv,el,fi
it,sv,el
it,sv,fi
it,el,fi
it,sv,el,fi
sv sv,nl
sv,nl,de
sv,nl,de,da
it it,pt it,pt,fr
R,sv R,sv,nl
R,sv,nl,de
R,D R,D,G
R,D,G,F
Co
rre
ctn
ess
Pivot Languages
CCB-wsd
1 pivot
2 families
3 families
4 families
Germanic (1-4)
Romance (1-3)
Romance-All (4-9)
Figure 4: Impact of pivot languages on correct-
ness. Language codes follow the convention pre-
sented in Section 4.1. Additionally R stands for
Romance languages, D for Germanic languages,
G for Greek and F for Finnish.
model achieved 48.9 BLEU, which can be con-
sidered a high score for Europarl data (in-domain
evaluation). Table 4 is analogous to Table 2, but
with paraphrases extracted from machine trans-
lated sentences as opposed to human translations.
We observe that multi still outperforms CCB-
wsd by a large margin. On the one hand there is a
drop in precision of about 9% for correctness with
multi. On the other hand there is an improvement
in recall: multi improves from 3% (top-1 guess)
to 12% (top-3 guesses). Manual inspection re-
vealed that the tags predicted by the SMT model
are more frequent translation options, reducing the
chance of finding rare target phrases as sense an-
notation, for which significant statistics cannot be
computed. However, with respect to correctness,
the differences between this setting and that with
gold-standard translations are not statistically sig-
nificant.
208
multi: English as sense annotation and nine other pivot languages
forma ? way forma ? form forma ? means forma ? kind
forma 0.34 forma 0.64 medio 0.64 tipo 0.37
manera 0.24 tipo 0.10 trave?s 0.23 forma 0.23
modo 0.23 forma de 0.05 instrumento 0.13 especie 0.06
forma de 0.02 formas 0.03 especie de 0.03
medio 0.02 modo 0.02 tipo de 0.03
CCB-wsd: English as sense annotation and sole evidence for pivoting
forma ? way forma ? form forma ? means forma ? kind
?way 0.08 ?formulario 0.18 ?significa contar 0.07 ?amables 0.16
?v??a por 0.08 de sus formas 0.10 medios que tiene 0.07 ?kind 0.12
?camino que hay 0.07 ?formulario de 0.07 ?significa 0.06 especie 0.09
?camino que hay que 0.07 modalidad 0.06 ?significa contar con 0.06 ?amable 0.08
?v??a por la 0.07 aspecto formal 0.05 ?anterior significa 0.06 tipo 0.07
Table 5: Top paraphrases of forma annotated by the English words way, form, means and kind. Starred
phrases denote inadequate candidates.
5.3 Potential applications
In what follows we discuss two applications which
we believe could directly benefit from the para-
phrase extraction approach proposed in this paper.
MT evaluation metrics such as METEOR
(Denkowski and Lavie, 2010) and TESLA (Liu
et al, 2010) already use paraphrases of n-grams
in the machine translated sentence in an attempt
to match more of the reference translation?s n-
grams. TESLA, in particular, uses paraphrases
constrained by a single pivot language as sense tag
as originally proposed in (Bannard and Callison-
Burch, 2005). Metrics like METEOR, which use
paraphrases simply as a repository with extra op-
tions for the n-gram matching, could be extended
to use the word-alignment between the source sen-
tence and the translation to constrain the translated
phrases while paraphrasing them with multilingual
constraints. In this case the model would attempt
to paraphrase the MT, which is not necessarily
fluent, therefore potentially compromising its LM
component. However, even after completely disre-
garding the LM re-ranking (see context-insensitive
model multi in Table 3), we may be able to im-
prove n-gram matching by paraphrasing.
Handling out-of-vocabulary words in SMT by
expanding the bilingual phrase-tables (Callison-
Burch et al, 2006) is a direct application of the
sense constrained paraphrases. We can add trans-
lations for a given unknown phrase f1, whose
paraphrase f2 is present in the phrase-table and
is aligned to the target phrase e (sense tag). We
basically expand the phrase table to translate the
out-of-vocabulary word f1 using the knowledge
associated to its paraphrase f2 in the context of the
known translation e: (f2, e) ? (f1, e). The mul-
tilingual constraints offer more control over ambi-
guities, therefore potentially leading to more accu-
rate phrase pairs added to the phrase-table.
6 Conclusions and future work
We have proposed a new formulation of the prob-
lem of generating ?sense? tagged paraphrases for
words and short phrases using bilingual corpora
and multiple pivot languages to jointly disam-
biguate the input phrase and the sense tag. Sense
tags are phrases in a foreign language of interest,
for instance the target language of a phrase-based
SMT system.
The approach was evaluated against the state of
the art method for paraphrase extraction. Signif-
icant improvements were found in particular with
respect to two aspects: i) the proposed model has
higher recall, since it has access to paraphrases
that would receive a negligible probability mass
and therefore would never be selected in previ-
ous formulations, and ii) the proposed model has
higher precision, since it is able to filter out or rank
down paraphrases with incorrect senses.
In future work we plan to further evaluate the
approach in the two scenarios discussed in Sec-
tion 5.3: i) to expand the phrase table of SMT sys-
tems to address issues such as out-of-vocabulary
words and phrases; and ii) to evaluate and opti-
mise parameters of SMT systems using metrics
that can accommodate sense disambiguated para-
phrases. We also plan to integrate syntactic con-
straints, as proposed in (Callison-Burch, 2008), to
our model to investigate the complementarities be-
tween these two ways of constraining paraphras-
ing.
209
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 597?604, Ann
Arbor, Michigan.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, pages 17?24,
New York, New York.
Chris Callison-Burch. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, University of Edinburgh, Edin-
burgh, Scotland.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ?08, pages
196?205, Honolulu, Hawaii.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense dis-
ambiguation. In The 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?07, pages 61?72, Prague, Czech
Republic.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 33?40, Prague, Czech Republic.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46, April.
Trevor Cohn and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use of
multi-parallel corpora. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics, Prague, Czech Republic.
Michael Denkowski and Alon Lavie. 2010.
METEOR-NEXT and the METEOR Paraphrase Ta-
bles: Improved Evaluation Support For Five Target
Languages. In Proceedings of the ACL 2010 Joint
Workshop on Statistical Machine Translation and
Metrics MATR.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 48?54, Edmonton,
Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics: Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In The Proceedings
of the Tenth Machine Translation Summit, pages 79?
86, Phuket, Thailand. AAMT, AAMT.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. Tesla: Translation evaluation of sentences
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 354?
359, Uppsala, Sweden.
Nitin Madnani and Bonnie J. Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?387.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for param-
eter tuning in statistical machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 120?127, Prague, Czech
Republic.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
381?390, Suntec, Singapore.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translat-
ing unknown terms. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 791?
799, Suntec, Singapore.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical align-
ment models. Computational Linguistics, 29:19?51,
March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: new evalua-
tion methods for word sense disambiguation. Nat.
Lang. Eng., 5(2):113?133.
210
Stefan Riezler, Er Vasserman, Ioannis Tsochantaridis,
Vibhu Mittal, and Yi Liu. 2007. Statistical machine
translation for query expansion in answer retrieval.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, pages 464?
471, Prague, Czech Republic.
Lucia Specia, Mark Stevenson, Maria das Grac?as
Volpe Nunes, and Gabriela C.B. Ribeiro. 2006.
Multilingual versus monolingual WSD. In Pro-
ceedings of the EACL Workshop ?Making Sense of
Sense: Bringing Psycholinguistics and Computa-
tional Linguistics Together?, pages 33?40, Trento,
Italy.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language, vol-
ume 2, pages 901?904, Denver, CO.
211
Workshop on Humans and Computer-assisted Translation, pages 78?83,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
Confidence-based Active Learning Methods for Machine Translation
Varvara Logacheva
University of Sheffield
Sheffield, United Kingdom
v.logacheva@sheffield.ac.uk
Lucia Specia
University of Sheffield
Sheffield, United Kingdom
l.specia@sheffield.ac.uk
Abstract
The paper presents experiments with ac-
tive learning methods for the acquisition
of training data in the context of machine
translation. We propose a confidence-
based method which is superior to the
state-of-the-art method both in terms of
quality and complexity. Additionally,
we discovered that oracle selection tech-
niques that use real quality scores lead to
poor results, making the effectiveness of
confidence-driven methods of active learn-
ing for machine translation questionable.
1 Introduction
Active learning (AL) is a technique for the auto-
matic selection of data which is most useful for
model building. In the context of machine trans-
lation (MT), AL is particularly important as the
acquisition of data often has a high cost, i.e. new
source texts need to be translated manually. Thus
it is beneficial to select for manual translation sen-
tences which can lead to better translation quality.
The majority of AL methods for MT is based
on the (dis)similarity of sentences with respect to
the training data, with particular focus on domain
adaptation. Eck et al. (2005) suggest a TF-IDF
metric to choose sentences with words absent in
the training corpus. Ambati et al. (2010) propose
a metric of informativeness relying on unseen n-
grams.
Bloodgood and Callison-Burch (2010) use n-
gram frequency and coverage of the additional
data as selection criteria. Their technique solic-
its translations for phrases instead of entire sen-
tences, which saves user effort and leads to quality
improvements even if the initial dataset is already
sizeable.
A recent trend is to select source sentences
based on an estimate of the quality of their trans-
lation by a baseline MT system. It is assumed
that if a sentence has been translated well with the
existing data, it will not contribute to improving
the translation quality. If however a sentence has
been translated erroneously, it might have words
or phrases that are absent or incorrectly repre-
sented. Haffari et al. (2009) train a classifier to
define the sentences to select. The classifier uses
a set of features of the source sentences and their
automatic translations: n-grams and phrases fre-
quency, MT model score, etc. Ananthakrishnan et
al. (2010) build a pairwise classifier that ranks sen-
tences according to the proportion of n-grams they
contain that can cause errors. For quality estima-
tion, Banerjee et al. (2013) train language models
of well and badly translated sentences. The use-
fulness of a sentence is measured as the difference
of its perplexities in these two language models.
In this research we also explore a quality-based
AL technique. Compared to its predecessors, our
method is based on a more complex and therefore
potentially more reliable quality estimation frame-
work. It uses wider range of features, which go
beyond those used in previous work, covering in-
formation from both source and target sentences.
Another important novel feature in our work is
the addition of real post-editions to the MT train-
ing data, as opposed to simulated post-editions
(human reference translations) as in previous work
on AL for MT. As we show in section 3.2, adding
post-editions leads to superior translation quality
improvements. Additionally, this is a suitable so-
lution for ?human in the loop? settings, as post-
editing automatically translated sentences tends to
be faster and easier than translation from scratch
(Koehn and Haddow, 2009). Also, different from
previous work, we do not focus on domain adapta-
tion: our experiments involve only in-domain data.
Compared to previous work on confidence-
driven AL, our approach has led to better results,
but these proved to be highly dependent on a sen-
tence length bias. However, an oracle-based selec-
78
tion using true quality scores has not been shown
to perform well. This indicates that the usefulness
of quality scores as AL selection criterion in the
context of MT needs to be further investigated.
2 Active selection strategy
Our AL sentence selection strategy relies on qual-
ity estimation (QE). QE is aimed at predicting the
quality of a translated text (in this case, a sen-
tence) without resorting to reference translations.
It considers features of the source and machine
translated texts, and an often small number (a few
hundreds) of examples of translations labelled for
quality by humans to train a machine learning al-
gorithm to predict such quality labels for new data.
We use the open source QE framework QuEst
(Specia et al., 2013). In our settings it was trained
to predict an HTER score (Snover et al., 2006) for
each sentence, i.e., the edit distance between the
automatic translation and its human post-edited
version. QuEst can extract a wide range of fea-
tures. In our experiments we use only the 17 so-
called baseline features, which have been shown
to perform well in evaluation campaigns (Bojar
et al., 2013): number of tokens in sentences, av-
erage token length, language model probabilities
for source and target sentences, average number of
translations per source word, percentage of higher
and lower frequency n-grams in source sentence
based on MT training corpus, number of punctua-
tion marks in source and target sentences.
Similarly to Ananthakrishnan et al. (2010), we
assume that the most useful sentences are those
that lead to larger translation errors. However,
instead of looking at the n-grams that caused er-
rors ? a very sparse indicator requiring signifi-
cantly larger amounts of training data, we account
for errors in a more general way: the (QuEst pre-
dicted) percentage of edits (HTER) that would be
necessary to transform the MT output into a cor-
rect sentence.
3 Experiments and results
3.1 Datasets and MT settings
For the AL data selection experiment, two datasets
are necessary: parallel sentences to train an ini-
tial, baseline MT system, and an additional pool
of parallel sentences to select from. Our goal
was to study potential improvements in the base-
line MT system in a realistic ?human in the loop?
scenario, where source sentences are translated by
the baseline system and post-edited by humans be-
fore they are added to the system. As it has been
shown in (Potet et al., 2012), post-editions tend to
be closer to source sentences than freely created
translations. One of our research questions was to
investigate whether they would be more useful to
improve MT quality.
We chose the biggest corpus with machine
translations and post-editions available to date: the
LIG French?English post-editions corpus (Potet
et al., 2012). It contains 10,881 quadruples of
the type: <source sentence, reference transla-
tion, automatic translation, post-edited automatic
translation>. Out of these, we selected 9,000 as
the pool to be added to be baseline MT system,
and the remaining 1,881 to train the QE system for
the experiments with AL. For QE training, we use
the HTER scores between MT and its post-edited
version as computed by the TERp tool.
1
We use the Moses toolkit with standard set-
tings
2
to build the (baseline) statistical MT sys-
tems. As training data, we use the French?
English News Commentary corpus released by the
WMT13 shared task (Bojar et al., 2013). For the
AL experiments, the size of the pool of additional
data (10,000) poses a limitation. To examine im-
provements obtained by adding fractions of up to
only 9,000 sentences, we took a small random sub-
set of the WMT13 data for these experiments (Ta-
ble 1). Although these figures may seem small, the
settings are realistic for many language pairs and
text domains where larger data sets are simply not
available.
We should also note that all the data used in our
experiments belongs to the same domain: the LIG
SMT system which produced sentences for the
post-editions corpus was trained on Europarl and
News commentary datasets (Potet et al., 2010), but
the post-edited sentences themselves were taken
from news test sets released for WMT shared tasks
in different years. Our baseline system is trained
on a fraction of the news commentary corpus. Fi-
nally, we tune and test all our systems on WMT
shared task news news datasets (those which do
not overlap with the post-editions corpus).
1
http://www.umiacs.umd.edu/
?
snover/
terp/
2
http://www.statmt.org/moses/?n=Moses.
Baseline
79
Corpora Size
(sentences)
Initial data (baseline MT system)
Training - subset of 10, 000
News Commentary corpus
Tuning - WMT newstest-2012 3, 000
Test - WMT newstest-2013 3, 000
Additional data (AL data)
Post-editions corpus: 10, 881
- Training QE system 1, 881
- AL pool 9, 000
Table 1: Datasets
3.2 Post-editions versus references
In order to compare the impact of post-editions
and reference translations on MT quality, we
added these two variants of translations to base-
line MT systems of different sizes, including the
entire News Commentary corpus. The figures for
BLEU (Papineni et al., 2002) scores in Table 2
show that adding post-editions results in signifi-
cantly better quality than adding the same number
of reference translations
3
. This effect can be seen
even when the additional data corresponds to only
a small fraction of the training data.
In addition, it does not seem to matter which
MT system produced the translations which were
then post-edited in the post-edition corpus. Even if
the output of a third-party system was used (as in
our case), it improves the quality of machine trans-
lations for unseen data. We assume that since post-
editions tend to be closer to original sentences than
free translations (Potet et al., 2012), they gener-
ally help produce better source-target alignments,
leading to the extraction of good quality phrases.
Baseline corpus Results (BLEU)
(sentences) Baseline Ref PE
150,000 22.41 22.95 23.21
50,000 20.22 20.91 22.01
10,000 15.09 18.65 20.44
Table 2: Influence of post-edited and reference
translations on MT quality. Ref: baseline system
with added free references, PE: baseline system
with added post-editions.
3
These systems use the whole post-editions set (10,881
sentences) as opposed to 9,000-sentence subset which we use
further in our AL experiments. Therefore the figures reported
in this table are higher than those in subsequent sections.
3.3 AL settings
The experimental settings for all methods are as
follows. First, a baseline MT system is trained.
Then a batch of 1,000 sentences is selected from
the data pool with an AL strategy, and the selected
data is removed from the pool. The MT system is
rebuilt using a concatenation of the initial training
data and the new batch. The process is repeated
until the pool is empty, with subsequent steps us-
ing the MT system trained on the previous step as
a baseline. The performance of each MT system
is measured in terms of BLEU scores. We use the
following AL strategies:
? QuEst: our method described in section 2.
? Random: random selection of sentences.
? HTER: oracle-based selection based on true
HTER scores of sentences in the pool, instead
of the QuEst estimated HTER scores.
? Ranking: AL strategy described in (Anan-
thakrishnan et al., 2010) for comparison.
3.4 AL results
Our initial results in Figure 1 show that our selec-
tion strategy (QuEst) consistently outperforms the
Random selection baseline.
Figure 1: Performance of MT systems enhanced
with data selected by different AL strategies
In comparison with previous work, we found
that the error-based Ranking strategy performs
closely to Random selection, although (Anan-
thakrishnan et al., 2010) reports it to be better.
80
Compared to QuEst, we believe the lower figures
of the Ranking strategy are due to the fact that the
latter considers features of only one type (source
n-grams), whereas QuEst uses a range of different
features of the source and translation sentences.
Interestingly, the Oracle method under-
performs our QE-based method, although we
expected the use of real HTER scores to be more
effective. In order to understand the reasons
behind such behaviour, we examined the batches
selected by QuEst and Oracle strategies more
closely. We found that the distribution of sentence
lengths in batches by the two strategies is very
different (see Figure 2). While in batches selected
by QuEst the average sentence length steadily
decreases as more data is added, in Oracle
batches the average length was almost uniform for
all batches, except the first one, which contains
shorter sentences.
This is explained by HTER formulation: HTER
is computed as the number of edits over the sen-
tence length, and therefore in shorter sentences ev-
ery edit is given more weight. For example, the
HTER score of a 5-word sentence with one error
is 0.2, whereas a sentence of 20 words with the
same single error has a score of 0.05. However, it
is doubtful that the former sentence will be more
useful for an MT system than the latter. Regarding
the nature of length bias in the predictions done by
QuEst system, sentence length is used there as a
feature, and longer sentences tend to be estimated
as having higher HTER scores (i.e., lower transla-
tion quality).
Therefore, sentences with the highest HTER
may not actually be the most useful, which makes
the Oracle strategy inferior to QuEst. Moreover,
longer sentences chosen by our strategy simply
provide more data, so their addition might be more
useful even regardless of the amount of errors.
This seems to indicate that the success of our
strategy might not be related to the quality of the
translations only, but to their length. Another pos-
sibility is that sentences selected by QuEst might
have more errors, which means that they can con-
tribute more to the MT system.
3.5 Additional experiments
In order to check the two hypotheses put forward
in the previous section, we conduct two other sets
of AL experiments: (i) a selection strategy that
chooses longer sentences first (denoted as Length)
Figure 2: Number of words in batches selected by
different AL strategies
and (ii) a selection strategy that chooses sentences
with larger numbers of errors first (Errors).
Figure 3 shows that a simple length-based strat-
egy yields better results than any of the other
tested strategies. Therefore, in cases when the
corpus has sufficient variation in sentence length,
length-based selection might perform at least as
well as other more sophisticated criteria. The
experiments with confidence-based selection de-
scribed in (Ananthakrishnan et al., 2010) were free
of this length bias, as sentences much longer or
shorter than average were deliberately filtered out.
Interestingly, results for the Errors strategy are
slightly worse than those for QuEst, although the
former is guaranteed to choose sentences with the
largest number of errors and has even stronger
length bias than QuEst (see figure 2). Therefore,
the reasons hypothesised to be behind the superi-
ority of QuEst over Oracle (longer sentences and
larger number of errors) are actually not the only
factors that influence the quality of an AL strategy.
3.6 Length-independent results
Despite the success of the length-based strategy,
we do not believe that it is enough for an effective
AL technique. First of all, the experiment with
the Errors strategy demonstrated that more data
does not always lead to better results. Further-
more, our aim is to reduce the translator?s effort in
cases when the additional data needs to be trans-
lated or post-edited manually. However, longer
sentences usually take more time to translate or
edit, so choosing the longest sentences from a pool
of sentences will not reduce translator?s effort.
81
Figure 3: Comparison of our QuEst-based selec-
tion with a length-based selection
Therefore, we would like to study the effec-
tiveness of our strategy by isolating the sentence
length bias. One option is to filter out long sen-
tences, as it was done in (Ananthakrishnan et al.,
2010). However, our pool is already too small.
Therefore, we plot the performance improvements
with respect to training data size in words, in-
stead of sentences. As it was already noted by
Bloodgood and Callison-Burch (2010), measuring
the amount of added data in sentences can signifi-
cantly contort the real annotation cost (the cost of
acquisition of new translations). So we switch to
length-independent representation.
Figure 4: Active learning quality plotted with re-
spect to data size in words: QuEst vs Oracle
strategies.
Figure 4 shows that the Oracle strategy in
Figure 5: AL quality plotted with respect to data
size in words: QuEst vs Length and Errors
strategies.
length-independent representation can still be seen
to perform worse than both our strategy and ran-
dom selection. Results of Length and Error
strategies (plotted separately in figure 5 for read-
ability) are very close and both underperform our
QuEst-based strategy and random selection of
data.
Here our experience echoes the results of (Mo-
hit and Hwa, 2007), where the authors propose the
idea of difficult to translate phrases. It is assumed
that extending an MT system with phrases that can
cause difficulties during translation is more effec-
tive than simply adding new data and re-building
the system. Due to the lack of time and human
annotators, the authors extracted difficult phrases
automatically using a set of features: alignment
features, syntactic features, model score, etc. Con-
versely, we had the human-generated information
on what segments have been translated incorrectly.
We assumed that the use of this knowledge as part
of our AL strategy would give us an upper bound
for our AL method results. However, it turned out
that prediction based on multiple features is more
reliable than precise information on quality, which
accounts for only one aspect of data.
4 Conclusions
We presented experiments with an active learning
strategy for machine translation based on quality
predictions. This strategy performs well compared
to another quality-driven strategy and a random
baseline. However, we found that it was success-
82
ful mostly due to its tendency to rate long sen-
tences as having lower quality. Consequently, the
AL application that chooses the longest sentences
is not less successful when selecting from corpora
with large variation in sentence length. A length-
independent representation of the results showed
that an oracle selection is less effective than our
quality-based strategy, which we believe to be due
to the nature of corrections and small size of the
post-edition corpus. In addition to that, another
oracle selection based on the amount of errors and
length-based selection show poor results when dis-
played in length-independent mode.
We believe that the quality estimation strategy
benefits from other features that reflect the useful-
ness of a sentence better than its HTER score and
the amount of user corrections. In future work we
will examine the influence of individual features
of the quality estimation model (such as language
model scores) as active learning selection strategy.
References
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active Learning and Crowd-Sourcing for Ma-
chine Translation. LREC 2010: Proceedings of the
seventh international conference on Language Re-
sources and Evaluation, 17-23 May 2010, Valletta,
Malta, pages 2169?2174.
Sankaranarayanan Ananthakrishnan, Rohit Prasad,
David Stallard, and Prem Natarajan. 2010. Dis-
criminative Sample Selection for Statistical Machine
Translation. EMNLP-2010: Proceedings of the
2010 Conference on Empirical Methods in Natu-
ral Language Processing, October 9-11, 2010, MIT,
Massachusetts, USA, (October):626?635.
Pratyush Banerjee, Raphael Rubino, Johann Roturier,
and Josef van Genabith. 2013. Quality Estimation-
guided Data Selection for Domain Adaptation of
SMT. MT Summit XIV: proceedings of the four-
teenth Machine Translation Summit, September 2-6,
2013, Nice, France, pages 101?108.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the Trend: Large-Scale Cost-Focused Ac-
tive Learning for Statistical Machine Translation.
ACL 2010: the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, Uppsala, Swe-
den, July 11-16, 2010, pages 854?864.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low Cost Portability for Statistical Machine Trans-
lation based on N-gram Frequency and TF-IDF.
IWSLT 2005: Proceedings of the International
Workshop on Spoken Language Translation. Octo-
ber 24-25, 2005, Pittsburgh, PA.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics on - NAACL ?09.
Philipp Koehn and Barry Haddow. 2009. Interactive
Assistance to Human Translators using Statistical
Machine Translation Methods. MT Summit XII: pro-
ceedings of the twelfth Machine Translation Sum-
mit, August 26-30, 2009, Ottawa, Ontario, Canada,
pages 73?80.
Behrang Mohit and Rebecca Hwa. 2007. Localiza-
tion of Difficult-to-Translate Phrases. ACL 2007:
proceedings of the Second Workshop on Statistical
Machine Translation, June 23, 2007, Prague, Czech
Republic, pages 248?255.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. ACL 2002: 40th
Annual Meeting of the Association for Computa-
tional Linguistics, July 2002, Philadelphia, pages
311?318.
Marion Potet, Laurent Besacier, and Herv?e Blanchon.
2010. The LIG machine translation system for
WMT 2010. ACL 2010: Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 161?166.
Marion Potet, Emmanuelle Esperanc?a-Rodier, Laurent
Besacier, and Herv?e Blanchon. 2012. Collection
of a Large Database of French-English SMT Output
Corrections. LREC 2012: Eighth international con-
ference on Language Resources and Evaluation, 21-
27 May 2012, Istanbul, Turkey, pages 4043?4048.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. AMTA 2006: Proceedings of the 7th Con-
ference of the Association for Machine Translation
in the Americas, Visions for the Future of Machine
Translation, August 8-12, 2006, Cambridge, Mas-
sachusetts, USA, pages 223?231.
Lucia Specia, Kashif Shah, Jose G C de Souza, and
Trevor Cohn. 2013. QuEst - A translation quality
estimation framework. ACL 2013: Annual Meet-
ing of the Association for Computational Linguis-
tics, Demo session, August 2013, Sofia, Bulgaria.
83
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 123?130,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
An Analysis of Crowdsourced Text Simplifications
Marcelo Adriano Amancio
Department of Computer Science
University of Sheffield
Sheffield, UK
acp12maa@sheffield.ac.uk
Lucia Specia
Department of Computer Science
University of Sheffield
Sheffield, UK
l.specia@sheffield.ac.uk
Abstract
We present a study on the text simplifica-
tion operations undertaken collaboratively
by Simple English Wikipedia contribu-
tors. The aim is to understand whether
a complex-simple parallel corpus involv-
ing this version of Wikipedia is appropri-
ate as data source to induce simplifica-
tion rules, and whether we can automat-
ically categorise the different operations
performed by humans. A subset of the cor-
pus was first manually analysed to iden-
tify its transformation operations. We then
built machine learning models to attempt
to automatically classify segments based
on such transformations. This classifica-
tion could be used, e.g., to filter out po-
tentially noisy transformations. Our re-
sults show that the most common transfor-
mation operations performed by humans
are paraphrasing (39.80%) and drop of in-
formation (26.76%), which are some of
the most difficult operations to generalise
from data. They are also the most diffi-
cult operations to identify automatically,
with the lowest overall classifier accuracy
among all operations (73% and 59%, re-
spectively).
1 Introduction
Understanding written texts in a variety of forms
(newspapers, educational books, etc.) can be a
challenge for certain groups of readers (Paciello,
2000). Among these readers we can cite second
language learners, language-impaired people (e.g.
aphasic and dyslexic), and the elderly. Sentences
with multiple clauses, unusual word order and rare
vocabulary are some of the linguistic phenomena
that should be avoided in texts written for these au-
diences. Although initiatives like the Plain English
(Flesch, 1979) have long advocated for the use of
clear and concise language, these have only been
adopted in limited cases (UK government bodies,
for example). The vast majority of texts which are
aimed at the broad population, such as news, are
often too complex to be processed by a large pro-
portion of the population.
Adapting texts into their simpler variants is an
expensive task. Work on automating this process
only started in recent years. However, already in
the 1920?s Lively and Pressey (1923) created a
method to distinguish simple from complex texts
based on readability measures. Using such mea-
sures, publishers were able to grade texts accord-
ing to reading levels (Klare and Buck, 1954) so
that readers could focus on texts that were appro-
priate to them. The first attempt to automate the
process of simplification of texts was devised by
Chandrasekar et al. (1996). This pioneer work has
shown that it was possible to simplify texts auto-
matically through hand-crafted linguistic rules. In
further work, Chandrasekar et al. (1997) devel-
oped a method to extract these rules from data.
Siddharthan (2002) defines Text Simplification
as any method or process that simplifies text while
maintaining its information. Instead of hand-
crafted rules, recent methodologies are mostly
data-driven, i.e., based on the induction of sim-
plification rules from parallel corpora of complex
segments and their corresponding simpler vari-
ants. Specia (2010) and Zhu et al. (2010) model
the task using the Statistical Machine Translation
framework, where simplified sentences are consid-
ered the ?target language?. Yatskar et al. (2010)
construct a simplification model based on edits in
the Simple English Wikipedia. Woodsend and La-
pata (2011) adopt a quasi-synchronous grammar
with optimisation via integer linear programming.
This research focuses the corpus used by most of
123
previous data-driven Text Simplification work: the
parallel corpus of the main and simple English
Wikipedia.
Following the collaborative nature of
Wikipedia, a subset of the Main English
Wikipedia (MainEW) has been edited by
volunteers to make the texts more readable to a
broader audience. This resulted in the Simple
English Wikipedia (SimpleEW)
1
, which we con-
sider a crowdsourced text simplification corpus.
Coster and Kauchak (2011) paired articles from
these two versions and automatically extracted
parallel paragraphs and sentences from them
(ParallelSEW). The first task was accomplished in
a straightforward way, given that corresponding
articles have the same title as unique identifica-
tion. The paragraph alignment was performed
selecting paragraphs when their normalised TF-
IDF weighted cosine distance reached a minimum
threshold. Sentence alignment was performed us-
ing monolingual alignment techniques (Barzilay
and Elhadad, 2003) based on a dynamic pro-
gramming algorithm. In total, 137, 000 sentences
were found to be parallel. The resulting parallel
corpora contains transformation operations of
various types, including rewording, reordering,
insertion and deletion. In our experiments we
analyse the distribution of these operations and
perform some further analysis on their nature.
Most studies on data-driven Text Simplification
have focused on the learning of the operations,
with no or little qualitative analysis of the Text
Simplification corpora used (Yasseri et al., 2012).
As in any other area, the quality of machine learn-
ing models for Text Simplification will depend on
the size and quality of the training dataset. Our
study takes a step back to carefully look at the
most common simplification corpus and: (i) un-
derstand the most common transformation oper-
ations performed by humans and judge whether
this corpus is adequate to induce simplification
rules from, and (ii) automatically categorise trans-
formation operations such as to further process
and ?clean? the corpus, for example to allow the
modelling of specific simplification phenomena or
groups of phenomena individually. After review-
ing some of the relevant related work (Section 2),
in Section 3, we present the manual analysis of a
subset of the ParallelSEW corpus. In Section 4 we
1
http://simple.wikipedia.org/wiki/
Main_Page
present a classification experiments to label this
corpus according to different simplification oper-
ations. Finally, we present a discussion of the re-
sults in section 5.
2 Literature Review
The closest work to ours is that of Yasseri et al.
(2012). They present a statistical analysis of lin-
guistic features that can indicate language com-
plexity in both MainEW and SimpleEW. Differ-
ent from our work, their analysis was automatic,
and therefore more superficial by nature (mostly
counts based on pattern matching and simple read-
ability metrics). They have found equivalent vo-
cabulary complexity in both versions of Wikipedia,
although one could expect simpler vocabulary in
SimpleEW. They have also demonstrated that Sim-
pleEW is considered simpler mainly because it
presents shorter sentences, as opposed to sim-
pler grammar. Additionally, they found a high
interdependence between topicality and language
complexity. Conceptual wikipages were found to
be linguistically more complex than biographical
ones, for example. For measuring language com-
plexity, the Gunning readability index (Gunning,
1969) was used. As in Besten and Dalle (2008),
additional complexity metrics are said to be nec-
essary to better assess readability issues in Sim-
pleEW.
(Petersen and Ostendorf, 2007)?s work is in the
context of bilingual education. A corpus of 104
news parallel texts, original and simplified ver-
sions of the Literacyworks corpus (Petersen and
Ostendorf, 2007), was used. The goal was to iden-
tify which simplification operations were more
frequent and provide a classifier (using machine
learning) as an aiding tool for teachers to deter-
mine which sentences should be (manually) sim-
plified. For the classification of sentences that
should be split, attributes such as sentence length,
POS tags, average length of specific phrases (e.g.
S, SBAR, NP) were used. For the classification
of sentences that should be dropped, the features
used included the position of the sentence in the
document, its paragraph position, the presence of
quotation marks, rate of stop words in the sen-
tence, and percentage of content words. It was
reported that the simplified versions of texts had
30% fewer words, and that sentences were 27%
shorter, with the elimination of adjectives, adverbs
and coordinating conjunctions, and the increase of
124
nouns (22%) and pronouns (33%). In the experi-
ments in this paper, we use similar features to clas-
sify a broader set of text simplification operations.
With similar goal and methodology, (Gasperin
et al., 2009) use a parallel corpus containing origi-
nal and simple news sentences in Portuguese. A
binary classifier was built to decide which sen-
tences to split, reaching precision of above 73%.
The feature set used was rich, including surface
sentence cues (e.g. number of words, number of
verbs, numbers of coordinative conjunctions), lex-
icalized cue phrases and rhetoric relations (e.g.
conclusions, contrast), among others.
Medero and Ostendorf (2011) work was moti-
vated by language-learning contexts, where teach-
ers often find themselves editing texts such that
they are adequate to readers with certain native
languages. In order to develop aiding tools for
this task, a number of attributes that lead to dif-
ferent operations were identified. Attributes lead-
ing to sentences splitting include sentence length
and POS tags frequency. Attributed that lead to
sentences being dropped include position of a sen-
tence in a document, paragraph number, presence
of a direct quotation, percentage of stop words,
etc. Based on these attributes, a classifier was
built to make splitting and dropping decisions au-
tomatically, reaching average error rates of 29%
and 15%, respectively.
Stajner et al. (2013) focus on selecting can-
didates for simplification in a parallel corpus of
original and simplified Spanish sentences. A clas-
sifier is built to decide over the following opera-
tions: sentence splitting, deletion and reduction.
The features are similar to those in (Petersen and
Ostendorf, 2007; Gasperin et al., 2009), with addi-
tional complexity features, such as sentence com-
plexity index, lexical density, and lexical richness.
They achieve an F-measure of 92%.
3 Corpus Annotation and Statistics
Our first study was exploratory. We randomly ex-
tracted 143 sentence pairs from the ParallelSWE
corpus. We then annotated each sentence in the
simplified version for the transformation opera-
tions (TOs) undertaken by Simple Wikipedia con-
tributors on the Main English Wikipedia to gener-
ate this version. We refer to this corpus as Paral-
lel143. These annotations will be used as labels
for the classification experiments in Section 4.
We start our analysis by looking at the number
of transformations that have been applied to each
sentence: on average, 2.1. More detailed statistics
are shown in Table 1 .
# Sentences 143
# TOs 299
Avg. TOs/sentence 2.10
Table 1: Counts of transformation operations in
the Parallel143 corpus
A more interesting way to look at these num-
bers is the mode of the operations, as shown in
Table 2. From this table we can notice that most
sentences had only one transformation operation
(about 48.2% of the corpus). Two to three opera-
tions together were found in 36.4% of the corpus.
Four or more operations in only about 11.8%.
N. of TOs. N. of sent. % of sent.
1 69 0.48
2 30 0.21
3 22 0.15
4 12 0.08
5 6 0.03
6 3 0.02
7 0 0.00
8 1 0.01
Table 2: Mode of transformation operations in the
Parallel143 corpus
The 299 operations found in the corpus were
classified into five main transformation operations,
which are also common in the previous work men-
tioned in Section 2: Sentence Splitting (SS); Para-
phrasing (PR); Drop of Information (DI); Sen-
tence Reordering (SR); Information Insertion (II);
and a label for ?Not a Parallel Sentence? (NPS).
Paraphrasing is often not considered as an opera-
tion on itself. Here we use it to refer to transfor-
mations that involve rewriting the sentence, be it
of a single word or of the entire sentence. In Ta-
ble 3 we show the distribution these operations in
the corpus. We can observe that the most common
operations were paraphrasing and drop of infor-
mation. Also, it is interesting to notice that more
than 7% of the corpus contains sentences that are
not actually parallel (NPS), that is, where the sim-
plified version does not correspond, in meaning, to
the original version.
125
TO Frequency of TO % of TO
PR 119 39.80
DI 80 26.76
II 38 12.71
NPS 23 7.69
SS 21 7.02
SR 18 6.02
Table 3: Main transformation operations found in
the Parallel143 corpus
Different from previous work, we further cate-
gorise each of these five main transformation oper-
ations into more specific operations. These subcat-
egorisation allowed us to further study the trans-
formation phenomena that can occur in the Paral-
lelSWE corpus. In the following sections we de-
scribe the main operations and their subcategories
in detail and provide examples.
3.1 Sentence Splitting (SS)
Sentence Splitting (SS) is the rewriting of a sen-
tence by breaking it into two or more sentences,
mostly in order avoid to embedded sentences. This
is overall the most common operation modelled in
automatic Text Simplification systems, as it is rel-
atively simple if a good syntactic parser is avail-
able. It has been found to be the most common
operation in other corpora. For example, in the
study in (Caseli et al., 2009) it accounts for 34%
of the operations. Nevertheless, it was found to be
relatively rare in the Parallel143 corpus, account-
ing for only 7% of the operations. One possible
reason for this low number is the automatic align-
ment of our corpus according to similarity metrics.
This matching algorithm could occasionally fail in
matching sentences that have been split. Within
the SS categories, we have identified three subcat-
egories: (1) simple sentence splitting (59.01%),
where the splitting does not alter the discourse
structure considerably; (2) complex sentence split-
ting (36.36%), where sentence splitting is associ-
ated with strong paraphrasing, and (3) inverse sen-
tence splitting (4.63%), i.e., the joining of two or
more sentences into one.
Sentences 1 and 2 show an example of com-
plex sentence splitting. In this case, the splitting
separates the information about the Birmingham
Symphony Orchestra?s origin from where it is lo-
cated into two different sentences. The operation
also includes paraphrasing and adding information
to complement the original sentence.
Sentence 1 ? MainEW:
?The City of Birmingham Symphony
Orchestra is a British orchestra based in
Birmingham, England.?
Sentence 2 ? SimpleEW:
?The City of Birmingham Symphony
Orchestra is one of the leading British
orchestras. It is based in the Symphony
Hall, Birmingham, England.?
3.2 Drop of Information (DI)
In the Parallel143 corpus we have observed that
the second most frequent operation is dropping
parts of the segment. We have sub-classified
the information removal into three classes: (1)
drop of redundant words (11.25%), for cases
when dropped words have not altered the sen-
tence meaning, (2) drop of auxiliary information
(12.50%), where the auxiliary information in the
original sentence adds extra information that can
elicit and reinforce its meaning, and (3) drop of
phrases (76.25 %), when phrases with important
nuclear information are dropped, incurring in in-
formation loss.
Sentences 3 and 4 show an example of par-
allel sentence with two occurrences of DI cases.
The phrases At an elevation of 887m and in the
Kingdom of are dropped, with the first phrase rep-
resenting a loss of information, which the second
could be considered redundant.
Sentence 3 ? MainEW:
?At an elevation of 877m, it is the
highest point in the Kingdom of the
Netherlands.?
Sentence 4 ? SimpleEW:
?It is the highest point in the Nether-
lands.?
3.3 Information Insertion (II)
Information Insertion represents the adding of in-
formation to the text. During the corpus analy-
sis we have found different sub-categories of this
operation: (1) eliciting information (78.95%), in
cases when some grammatical construct or aux-
iliary phrase is inserted enriching the main in-
formation already in the text, or making it more
explicit, (2) complementary external information
(18.42%), for cases when external information is
126
inserted to complement the existing information,
and (3) spurious information (2.63%), for when
new information is inserted but it does not relate
with the original text. We assume that latter case
happens due to errors in the sentence alignment
algorithm used to build the corpus.
In sentences 5 and 6, we show an example of
external information insertion. In this case, the op-
eration made the information more specific.
Sentence 5 ? MainEW:
?The 14 generators in the north side of
the dam have already been installed.?
Sentence 6 ? SimpleEW:
?The 14 main generators in the north
side were installed from 2003 to 2005.?
3.4 Sentence Reordering (RE)
Some of the transformation operations results in
the reordering of parts of the sentence. We
have classified reordering as (1) reorder individ-
ual phrases (33.33%), when a phrase is moved
within the sentence; and (2) invert pairs of phrases
(66.67%), when two phrases have their position
swapped in the sentence. In sentences 7 and 8
we can see an example moving the phrase June
20, 2003 to the end of the SimpleEW sentence.
Sentence 7 ? MainEW:
?The creation of the foundation was of-
ficially announced on June 20, 2003
by Wikipedia co-founder Jimmy Wales
, who had been operating Wikipedia un-
der the aegis of his company Bomis.?
Sentence 8 ? SimpleEW:
?The foundations creation was offi-
cially announced by Wikipedia co-
founder Jimmy Wales, who was running
Wikipedia within his company Bomis,
on June 20, 2003.?
3.5 Paraphrasing (PR)
Paraphrase operations are the most common mod-
ification found in the Parallel143 corpus. We fur-
ther classified it into 12 types:
? Specific to generic (21.01%): some specific
information is substituted by a broader and
more generic concept;
? Generic to specific (5.88%): the opposite of
the above operation;
? Noun to pronoun (3.36%): a noun is substi-
tuted by a pronoun;
? Pronoun instantiation (2.52%): a pronoun is
substituted by its referring noun;
? Word synonym (14.29%): a word is substi-
tuted by a synonym;
? Discourse marker (0.84%): a discourse
marker is altered;
? Word definition (0.84%): a word is substi-
tuted by its dictionary description;
? Writing style (7.56%): the writing style of the
word, e.g. hyphenation, changes;
? Preposition (3.36%): a proposition is substi-
tuted;
? Verb substitution (5.04%): a verb is replaced
by another verb;
? Verb tense (2.52%): the verb tense is
changed; and
? Abstract change (32.78%): paraphrase
substitution that contains abstract, non-
systematic changes, usually depending on
external information and human reasoning,
resulting in considerable modifications in the
content of the simplified sentence.
In sentences 9 and 10 we can observe a case of
abstract change. The MainEW sentence has de-
scriptive historical details of the city of Prague.
The SimpleEW version is shorter, containing less
factual information when compared to the first
sentence.
Sentence 9 ? MainEW:
?In 1993, after the split of Czechoslo-
vakia, Prague became the capital city of
the new Czech Republic.?
Sentence 10 ? SimpleEW:
?Prague is the capital and the biggest
city of the Czech Republic.?
Another common operation is shown in Sen-
tences 11 and 12. The substitution of the word
hidden by put represents a change of specific to
generic.
127
Sentence 11 ? MainEW:
?The bells were transported north to
Northampton-Towne, and hidden in the
basement of the Old Zion Reformed
Church, in what is now center city Al-
lentown.?
Sentence 12 ? SimpleEW:
?The bells were moved north to
Northampton-Towne, and put in the
basement of the Old Zion Reformed
Church, in what is now center of
Allentown.?
The outcome of this study that is of most
relevance to our work is the high percentage
of sentences that have undergone paraphras-
ing/rewriting, and in special the ones that suffered
abstract changes. These cases are very hard to
generalise, and any learning method applied to a
corpus with a high percentage of these cases is
likely to fail or to induce noisy or spurious opera-
tions.
4 Classification Experiments
Our ultimate goal of this experiment is to select
parts of the ParallelSWE corpus that are more ad-
equate for the learning of certain simplification
rules. While it may seem that simplification opera-
tions comprise a small set which is already known
based on previous work, we would like to focus
on the learning of fine-grained, lexicalized rules.
In other words, we are interested in the learning of
more specific rules based on lexical items in ad-
dition to more general information such as POS
tags and syntactic structures. The learning of such
rules could benefit from a high quality corpus that
is not only noise-free, but also for which one al-
ready has some information about the general op-
eration(s) covered. In an ideal scenario, one could
for example use a subset of the corpus that con-
tains only sentence splitting operations to learn
very specific and accurate rules to perform dif-
ferent types of sentence splitting in unseen data.
Selecting a subset of the corpus that contain only
one transformation operation per segment is also
appealing as it would facilitate the learning. The
process of manually annotating the corpus with the
corresponding transformation operations is how-
ever a laborious task. For this reason, we have
trained classifiers on the labelled data described in
the previous section with two purposes:
? Decide over the six main transformation op-
erations presented in the previous section;
and
? Decide whether a sentence was simplified by
one operation only, or by more than one op-
eration.
The features used in both experiments are de-
scribed in Section 4.1 and the algorithms and re-
sults are presented in Section 4.2.
4.1 Features
We extract simple features from the source (orig-
inal, complex) and target (simplified) sentences.
These were inspired by previous work, including
(Medero and Ostendorf, 2011; Petersen and Os-
tendorf, 2007; Gasperin et al., 2009;
?
Stajner et al.,
2013):
? Size of the source sentence: how many words
there are in the source sentence;
? Size of the target sentence: how many words
there are in the target sentence;
? Target/source size ratio: the number of words
in the target sentence divided by the number
of words in the source sentence;
? Number of sequences of words dropped in
the target sentence;
? Number of sequences of words inserted in the
target sentence; and
? Occurrence of lexical substitution (true or
false).
4.2 Machine Learning Models
Our experiments are divided in two parts. In the
first part, we train six binary classifiers to test the
presence of the following transformation opera-
tions: Information Insertion (II); Drop of Informa-
tion (DI); Paraphrasing (PR); Sentence Reordering
(SR); Sentence Splitting (SS); Not a Parallel Sen-
tence (NPS).
The second experiment evaluated whether the
simplification operation performed in the segment
was simple or complex (S/C). We consider simple
a transformation that has only one operation, and
complex when it has two or more operations.
A few popular classifiers from the Weka pack-
age (Hall et al., 2009) with default parameters
128
were selected. The experiments were devised us-
ing the 10-fold cross validation. The results ?
measured in terms of accuracy ? for each of these
classifiers with the best machine learning algo-
rithm are shown in Table 4. These are compared
to the accuracy of the majority class baseline (i.e.,
the class with the highest frequency in the train-
ing set). Table 5 shows the best machine learning
algorithm for each classification problem.
TO Baseline (%) Model (%)
NPS 83.3 90.2
SR 89 90
SS 86 87
II 79 86
PR 61 73
DI 59 69
S/C 51 81
Table 4: Baselines and classifiers accuracy of the
transformation operations
According to Table 4, the identification of non-
parallel sentences (NPS) and sentence reordering
(SR) achieved the highest accuracies of 90.2% and
90%, followed by syntactic simplification (SS)
and Information Insertion (II) with values of 87%
and 86%, respectively. Paraphrases (PR) and drop
information (DI) have scored last, although they
yielded a significant gain of 12% and 10% ab-
solute points, respectively, when compared with
baseline. The decision between simple and com-
plex transformations was the task with best rel-
ative gain in accuracy compared to the baseline
(30%).
TO Best algorithm
NPS Bayesian Logistic
SR SMO
SS Simple Logistic
II Simple Logistic
PR Logistic
DI Simple Logistic
S/C Bayes Net
Table 5: Best machine learning algorithm for each
operation/task
The difference in the performance of different
algorithms for each operation requires further ex-
amination. For different classifiers on the same
dataset, the accuracy figures varied from 2 to 10
points, which is quite significant.
We found the results of these experiments
promising, particularly for the classifiers NPS and
S/C. The outcome of the classifier for NPS, for
example, means that with an accuracy of over
90% we can filter out sentences from the Simple
Wikipedia Corpus which are not entirely parallel,
and therefore would only add noisy to any rule in-
duction algorithm. The positive outcome of S/C
means that with 80% accuracy one could select
parallel sentences where the target contain only
one operation to simplify the rule induction pro-
cess.
Overall, these results are even more promising
given two factors: the very small size of our la-
belled corpus (143 sentences) and the very simple
set of features used. Improvements on both fronts
are likely to lead to better results.
5 Conclusion
This research has focused on studying the paral-
lel corpus of the Main English Wikipedia and its
Simple English Wikipedia corresponding version.
Most current data-driven methods for text simpli-
fication are based on this resource. Our exper-
iments include the identification and quantifica-
tion of the transformation operations undertaken
by contributors generating the simplified version
of the corpus, and the construction of classifiers to
categorise these automatically.
Particularly interesting outcomes of our experi-
ments include: (i) the high proportion of complex
paraphrasing cases observed in the corpus (?40%
of the operations), which is important since para-
phrase generation is a difficult task to automate,
particularly via machine learning algorithms; and
(ii) the relatively high accuracy of our classi-
fiers on the categorisation of certain phenomena,
namely the identification of segment pairs which
are not parallel in meaning, and the filtering of the
corpus to select sentences that have undergone a
single transformation operation. These classifiers
can be used as filtering steps to improve the qual-
ity of text simplification corpora, which we believe
can in turn lead to better performance of learning
algorithms inducing rules from such corpora.
Acknowledgements
This research was supported by the CAPES PhD
research grant n. 31983594814, process n.
5475/10-4.
129
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25?
32. Association for Computational Linguistics.
Matthijs Den Besten and Jean-Michel Dalle. 2008.
Keep it simple: A companion for simple wikipedia?
Industry and Innovation, 15(2):169?178.
Helena M. Caseli, Tiago F. Pereira, Lucia Specia, Thi-
ago A.S. Pardo, Caroline Gasperin, and Sandra M.
Alu??sio. 2009. Building a brazilian portuguese par-
allel corpus of original and simplified texts. Ad-
vances in Computational Linguistics, Research in
Computer Science, 41:59?70.
Raman Chandrasekar and Bangalore Srinivas. 1997.
Automatic induction of rules for text simplification.
Knowledge-Based Systems, 10(3):183?190.
Raman Chandrasekar, Christine Doran, and Bangalore
Srinivas. 1996. Motivations and methods for text
simplification. In Proceedings of the 16th confer-
ence on Computational linguistics-Volume 2, pages
1041?1044. Association for Computational Linguis-
tics.
William Coster and David Kauchak. 2011. Simple
english wikipedia: a new text simplification task.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics. Strouds-
burg, PA: Association for Computational Linguis-
tics, pages 665?669.
Rudolf Flesch. 1979. How to write plain
english. URL: http://www. mang. canterbury.
ac. nz/courseinfo/AcademicWriting/Flesch. htm [ac-
cessed 2003 Oct 13][WebCite Cache].
Caroline Gasperin, Lucia Specia, Tiago Pereira, and
Sandra Alu??sio. 2009. Learning when to simplify
sentences for natural text simplification. Proceed-
ings of ENIA, pages 809?818.
Robert Gunning. 1969. The fog index after twenty
years. Journal of Business Communication, 6(2):3?
13.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
George Roger Klare and Byron Buck. 1954. Know
your reader: The scientific approach to readability.
Hermitage House.
Bertha A Lively and Sidney L Pressey. 1923. A
method for measuring the vocabulary burden of text-
books. Educational administration and supervision,
9(389-398):73.
Julie Medero and Mari Ostendorf. 2011. Identifying
targets for syntactic simplification. In Proceedings
of the SLaTE 2011 workshop.
Michael Paciello. 2000. Web accessibility for people
with disabilities. Taylor & Francis US.
Sarah E Petersen and Mari Ostendorf. 2007. Text sim-
plification for language learners: a corpus analysis.
In In Proc. of Workshop on Speech and Language
Technology for Education.
Advaith Siddharthan. 2002. An architecture for a
text simplification system. In Language Engineer-
ing Conference, 2002. Proceedings, pages 64?71.
IEEE.
Lucia Specia. 2010. Translating from complex to sim-
plified sentences. In Computational Processing of
the Portuguese Language, pages 30?39. Springer.
Sanja
?
Stajner, Biljana Drndarevic, and Horacio Sag-
gion. 2013. Corpus-based sentence deletion and
split decisions for spanish text simplification.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 409?420. Association
for Computational Linguistics.
Taha Yasseri, Andr?as Kornai, and J?anos Kert?esz. 2012.
A practical approach to language complexity: a
wikipedia case study. PloS one, 7(11):e48386.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 365?368. Association for
Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd international conference on computational lin-
guistics, pages 1353?1361. Association for Compu-
tational Linguistics.
130
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12?58,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Findings of the 2014 Workshop on Statistical Machine Translation
Ond
?
rej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Christian Federmann
Microsoft Research
Barry Haddow
University of Edinburgh
Philipp Koehn
JHU / Edinburgh
Johannes Leveling
Dublin City University
Christof Monz
University of Amsterdam
Pavel Pecina
Charles University in Prague
Matt Post
Johns Hopkins University
Herve Saint-Amand
University of Edinburgh
Radu Soricut
Google
Lucia Specia
University of Sheffield
Ale
?
s Tamchyna
Charles University in Prague
Abstract
This paper presents the results of the
WMT14 shared tasks, which included a
standard news translation task, a sepa-
rate medical translation task, a task for
run-time estimation of machine translation
quality, and a metrics task. This year, 143
machine translation systems from 23 insti-
tutions were submitted to the ten transla-
tion directions in the standard translation
task. An additional 6 anonymized sys-
tems were included, and were then evalu-
ated both automatically and manually. The
quality estimation task had four subtasks,
with a total of 10 teams, submitting 57 en-
tries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2014. This workshop builds
on eight previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al., 2007, 2008,
2009, 2010, 2011, 2012; Bojar et al., 2013).
This year we conducted four official tasks: a
translation task, a quality estimation task, a met-
rics task
1
and a medical translation task. In the
translation task (?2), participants were asked to
translate a shared test set, optionally restricting
themselves to the provided training data. We held
ten translation tasks this year, between English and
each of Czech, French, German, Hindi, and Rus-
sian. The Hindi translation tasks were new this
year, providing a lesser resourced data condition
on a challenging language pair. The system out-
puts for each task were evaluated both automati-
cally and manually.
1
The metrics task is reported in a separate paper
(Mach?a?cek and Bojar, 2014).
The human evaluation (?3) involves asking
human judges to rank sentences output by
anonymized systems. We obtained large num-
bers of rankings from researchers who contributed
evaluations proportional to the number of tasks
they entered. Last year, we dramatically increased
the number of judgments, achieving much more
meaningful rankings. This year, we developed a
new ranking method that allows us to achieve the
same with fewer judgments.
The quality estimation task (?4) this year
included sentence- and word-level subtasks:
sentence-level prediction of 1-3 likert scores,
sentence-level prediction of percentage of word
edits necessary to fix a sentence, sentence-level
prediction of post-editing time, and word-level
prediction of scores at different levels of granular-
ity (correct/incorrect, accuracy/fluency errors, and
specific types of errors). Datasets were released
with English-Spanish, English-German, Spanish-
English and German-English news translations
produced by 2-3 machine translation systems and,
for some subtasks, a human translation.
The medical translation task (?5) was intro-
duced this year. Unlike the ?standard? translation
task, the test sets come from the very specialized
domain of medical texts. The aim of this task was
not only domain adaptation but also the utilization
of translation systems in a larger scenario, namely
cross-lingual information retrieval (IR). Extrinsic
evaluation in an IR setting was a part of this task
(on the other hand, manual evaluation of transla-
tion quality was not carried out).
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation and estimation methodologies for
machine translation. As before, all of the data,
12
translations, and collected human judgments are
publicly available.
2
We hope these datasets serve
as a valuable resource for research into statistical
machine translation and automatic evaluation or
prediction of translation quality.
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and other languages.
As in the previous years, the other languages in-
clude German, French, Czech and Russian.
We dropped Spanish and added Hindi this year.
From a linguistic point of view, Spanish poses
similar problems as French, making its prior in-
clusion less valuable. Hindi is not only interest-
ing since it is a more distant language than the
European languages we include, but also because
we have much less training data, thus forcing re-
searchers to deal with low resource conditions, but
also providing them with a language pair that does
not suffer from the computational complexities of
having to deal with massive amounts of training
data.
We created a test set for each language pair by
translating newspaper articles and provided train-
ing data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources, as before. How-
ever, we changed our method to create the test sets.
In previous years, we took equal amounts of
source sentences from all six languages involved
(around 500 sentences each), and translated them
into all other languages. While this produced a
multi-parallel test corpus that could be also used
for language pairs (such as Czech-Russian) that
we did not include in the evaluation, it did suf-
fer from artifacts from the larger distance between
source and target sentences. Most test sentences
involved the translation a source sentence that
was translated from a their language into a tar-
get sentence (which was compared against a trans-
lation from that third language as well). Ques-
tions have been raised, if the evaluation of, say,
French-English translation is best served when
testing on sentences that have been originally writ-
ten in, say, Czech. For discussions about trans-
lationese please for instance refer to Koppel and
Ordan (2011).
2
http://statmt.org/wmt14/results.html
This year, we took about 1500 English sen-
tences and translated them into the other 5 lan-
guages, and then additional 1500 sentences from
each of the other languages and translated them
into English. This gave us test sets of about 3000
sentences for our English-X language pairs, which
have been either written originally written in En-
glish and translated into X, or vice versa.
The composition of the test documents is shown
in Table 1. The stories were translated by the pro-
fessional translation agency Capita, funded by the
EU Framework Programme 7 project MosesCore,
and by Yandex, a Russian search engine com-
pany.
3
All of the translations were done directly,
and not via an intermediate language.
2.2 Training data
As in past years we provided parallel corpora
to train translation models, monolingual cor-
pora to train language models, and development
sets to tune system parameters. Some train-
ing corpora were identical from last year (Eu-
roparl
4
, United Nations, French-English 10
9
cor-
pus, CzEng, Common Crawl, Russian-English
Wikipedia Headlines provided by CMU), some
were updated (Russian-English parallel data pro-
vided by Yandex, News Commentary, monolin-
gual data), and a new corpus was added (Hindi-
English corpus, Bojar et al. (2014)), Hindi-English
Wikipedia Headline corpus).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their entry
names are listed in Table 2; each system did not
necessarily appear in all translation tasks. We also
included four commercial off-the-shelf MT sys-
tems and four online statistical MT systems, which
we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3
http://www.yandex.com/
4
As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
13
Europarl Parallel Corpus
French? English German? English Czech? English
Sentences 2,007,723 1,920,209 646,605
Words 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 183,251 201,288 146,549 165,602
Words 5,688,656 4,659,619 5,105,101 5,046,157 3,288,645 3,590,287 4,153,847 4,339,974
Distinct words 72,863 62,673 150,760 65,520 139,477 55,547 151,101 60,801
Common Crawl Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 3,244,152 2,399,123 161,838 878,386
Words 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
French? English
Sentences 12,886,831
Words 411,916,781 360,341,450
Distinct words 565,553 666,077
10
9
Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Hindi-English Parallel Corpus
Hindi? English
Sentences 287,202
Words 6,002,418 3,953,851
Distinct words 121,236 105,330
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English Hindi? English
Sentences 514,859 32,863
Words 1,191,474 1,230,644 141,042 70,075
Distinct words 282,989 251,328 25,678 26,989
Europarl Language Model Data
English French German Czech
Sentence 2,218,201 2,190,579 2,176,537 668,595
Words 59,848,044 63,439,791 53,534,167 14,946,399
Distinct words 123,059 145,496 394,781 172,461
News Language Model Data
English French German Czech Russian Hindi
Sentence 90,209,983 30,451,749 89,634,193 36,426,900 32,245,651 1,275,921
Words 2,109,603,244 748,852,739 1,606,506,785 602,950,410 575,423,682 36,297,394
Distinct words 4,089,792 1,906,470 10,248,707 3,101,846 2,860,837 258,759
News Test Set
French? English German? English Czech? English Russian? English Hindi? English
Sentences 3003 3003 3003 3003 2507
Words 81,194 71,147 63,078 67,624 60,240 68,866 62,107 69,329 86,974 55,822
Distinct words 11,715 10,610 13,930 10,458 16,774 9,893 17,009 9,938 8,292 9,217
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
14
Language Sources (Number of Documents)
Czech aktu?aln?e.cz (2), blesk.cz (3), blisty.cz (1), den??k.cz (9), e15.cz (1), iDNES.cz (17), ihned.cz (14), lidovky.cz (8), medi-
afax.cz (2), metro.cz (1), Novinky.cz (5), pravo.novinky.cz (6), reflex.cz (2), tyden.cz (1), zdn.cz (1).
French BBC French Africa (1), Canoe (9), Croix (4), Cyber Presse (12), Dernieres Nouvelles (1), dhnet.be (5), Equipe (1),
Euronews (6), Journal Metro.com (1), La Libre.be (2), La Meuse.be (2), Le Devoir (3), Le Figaro (8), Le Monde (3),
Les Echos (15), Lexpress.fr (3), Liberation (1), L?independant (2), Metro France (1), Nice-Matin (6), Le Nouvel Ob-
servateur (3), Radio Canada (6), Reuters (7).
English ABC News (5), BBC (5), CBS News (5), CNN (5), Daily Mail (5), Financial Times (5), Fox News (2), Globe and
Mail (1), Independent (1), Los Angeles Times (1), New Yorker (1), News.com Australia (16), Reuters (3), Scotsman (2),
smh.com.au (2), stv.tv (1), Telegraph (6), UPI (2).
German Abendzeitung N?urnberg (1), all-in.de (2), Augsburger Allgemeine (1), AZ Online (1), B?orsenzeitung (1), come-
on.de (1), Der Westen (2), DZ Online (1), Reutlinger General-Anzeiger (1), Generalanzeiger Bonn (1), Giessener
Anzeiger (1), Goslarsche Zeitung (1), Hersfelder Zeitung (1), J?udische Allgemeine (1), Kreisanzeiger (2),
Kreiszeitung (2), Krone (1), Lampertheimer Zeitung (2), Lausitzer Rundschau (1), Mittelbayerische (1), Morgen-
post (1), nachrichten.at (1), Neue Presse (1), OP Online (1), Potsdamer Neueste Nachrichten (1), Passauer Neue
Presse (1), Recklingh?auser Zeitung (1), Rhein Zeitung (1), salzburg.com (1), Schwarzw?alder Bote (29), Segeberger
Zeitung (1), Soester Anzeiger (1), S?udkurier (17), svz.de (1), Tagesspiegel (1), Usinger Anzeiger (3), Volksblatt.li (1),
Westf?alischen Anzeiger (3), Wiener Zeitung (1), Wiesbadener Kurier (1), Westdeutsche Zeitung (1), Wilhelmshavener
Zeitung (1), Yahoo Deutschland (1).
Hindi Bhaskar (24), Jagran (61), Navbharat Times / India Times (4), ndtv (2).
Russian 168.ru (1), aif (3), altapress.ru (2), argumenti.ru (2), BBC Russian (3), belta.by (2), communa.ru (1), dp.ru (1), eg-
online.ru (1), Euronews (2), fakty.ua (2), gazeta.ru (1), inotv.rt.com (1), interfax (1), Izvestiya (1), Kommersant (7),
kp (2), lenta.ru (4), lgng (1), litrossia.ru (1), mirnov.ru (5), mk (8), mn.ru (2), newizv (2), nov-pravda.ru (1), no-
vayagazeta (1), nr2.ru (8), pnp.ru (1), rbc.ru (3), ria.ru (4), rosbalt.ru (1), sovsport.ru (6), Sport Express (10), trud.ru (4),
tumentoday.ru (1), vesti.ru (10), zr.ru (1).
Table 1: Composition of the test set. For more details see the XML test files. The docid tag gives the source and the date for
each document in the test set, and the origlang tag indicates the original source language.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
This year?s evaluation was conducted a bit dif-
ferently. The main differences are:
? In contrast to the past two years, we collected
judgments entirely from researchers partici-
pating in the shared tasks and trusted friends
of the community. Last year, about two thirds
of the data were solicited from random volun-
teers on the Amazon Mechanical Turk. For
some language pairs, the Turkers data had
much lower inter-annotator agreement com-
pared to the researchers.
? As a result, we collected about seventy-five
percent less data, but were able to obtain
good confidence intervals on the clusters with
the use of new approaches to ranking.
? We compared three different ranking method-
ologies, selecting the one with the highest ac-
curacy on held-out data.
We also maintain many of our customs from
prior years, including the presentation of the re-
sults in terms of a partial ordering (clustering) of
the systems. Systems in the same cluster could not
be meaningfully distinguished and should be con-
sidered ties.
3.1 Data collection
The system ranking is produced from a large set of
pairwise annotations between system pairs. These
pairwise annotations are collected in an evaluation
campaign that enlists participants in the shared
task to contribute one hundred ?Human Intelli-
gence Tasks? (HITs) per system submitted. Each
HIT consists of three ranking tasks. In a rank-
ing task, an annotator is presented with a source
segment, a human reference translation, and the
outputs of five anonymized systems, randomly se-
lected from the set of participating systems, and
randomly ordered.
To run the evaluation, we use Appraise
5
(Fe-
dermann, 2012), an open-source tool built on
Python?s Django framework. At the top of each
HIT, the following instructions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
5
https://github.com/cfedermann/Appraise
15
ID Institution
AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014)
CIMS University of Stuttgart / University of Munich (Cap et al., 2014)
CMU Carnegie Mellon University (Matthews et al., 2014)
CU-* Charles University, Prague (Tamchyna et al., 2014)
DCU-FDA Dublin City University (Bicici et al., 2014)
DCU-ICTCAS Dublin City University (Li et al., 2014b)
DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014)
EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014)
KIT Karlsruhe Institute of Technology (Herrmann et al., 2014)
IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014)
IIIT-HYDERABAD IIIT Hyderabad
IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014)
IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014)
KAZNU Amandyk Kartbayev, FBK
LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014)
MANAWI-* Universit?at des Saarlandes (Tan and Pal, 2014)
MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014)
PROMT-RULE,
PROMT-HYBRID
PROMT
RWTH RWTH Aachen (Peitz et al., 2014)
STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014)
UA-* University of Alicante (S?anchez-Cartagena et al., 2014)
UEDIN-PHRASE,
UEDIN-UNCNSTR
University of Edinburgh (Durrani et al., 2014b)
UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014)
UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014)
YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C,G] Four online statistical machine translation systems
RBMT-[1,4] Two rule-based statistical machine translation systems
Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
16
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly ordered), and is asked to
rank these according to their translation quality, with ties allowed.
A screenshot of the ranking interface is shown in
Figure 2. Annotators are asked to rank the sys-
tems from 1 (best) to 5 (worst), with ties permit-
ted. Note that a lower rank is better. The rankings
provided by a ranking task are then reduced to a
set of ten pairwise rankings produced by consider-
ing all
(
5
2
)
combinations of systems in the ranking
task. For example, consider the following annota-
tion provided among systems A,B, F,H , and J :
1 2 3 4 5
F ?
A ?
B ?
J ?
H ?
This is reduced to the following set of pairwise
judgments:
A > B,A = F,A > H,A < J
B < F,B < H,B < J
F > H,F < J
H < J
Here,A > B should be read is ?A is ranked higher
than (worse than) B?. Note that by this procedure,
the absolute value of ranks and the magnitude of
their differences are discarded.
For WMT13, nearly a million pairwise anno-
tations were collected from both researchers and
paid workers on Amazon?s Mechanical Turk, in
a roughly 1:2 ratio. This year, we collected data
from researchers only, an ability that was enabled
by the use of a new technique for producing the
partial ranking for each task (?3.3.3). Table 3 con-
tains more detail.
3.2 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960). If P (A) be
the proportion of times that the annotators agree,
and P (E) is the proportion of time that they would
17
LANGUAGE PAIR Systems Rankings Average
Czech?English 5 21,130 4,226.0
English?Czech 10 55,900 5,590.0
German?English 13 25,260 1,943.0
English?German 18 54,660 3,036.6
French?English 8 26,090 3,261.2
English?French 13 33,350 2,565.3
Russian?English 13 34,460 2,650.7
English?Russian 9 28,960 3,217.7
Hindi?English 9 20,900 2,322.2
English?Hindi 12 28,120 2,343.3
TOTAL WMT 14 110 328,830 2,989.3
WMT13 148 942,840 6,370.5
WMT12 103 101,969 999.6
WMT11 133 63,045 474.0
Table 3: Amount of data collected in the WMT14 manual evaluation. The final three rows report summary information from
the previous two workshops.
agree by chance, then Cohen?s kappa is:
? =
P (A)? P (E)
1? P (E)
Note that ? is basically a normalized version of
P (A), one which takes into account how mean-
ingful it is for annotators to agree with each other
by incorporating P (E). The values for ? range
from 0 to 1, with zero indicating no agreement and
1 perfect agreement.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A < B, A = B, or A > B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it captures the probability that two
annotators would agree randomly. Therefore:
P (E) = P (A<B)
2
+ P (A=B)
2
+ P (A>B)
2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 4 gives ? values for inter-annotator agree-
ment for WMT11?WMT14 while Table 5 de-
tails intra-annotator agreement scores, including
the division of researchers (WMT13
r
) and MTurk
(WMT13
m
) data. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0?0.2 is slight, 0.2?0.4 is
fair, 0.4?0.6 is moderate, 0.6?0.8 is substantial,
and 0.8?1.0 is almost perfect. The agreement rates
are more or less in line with prior years: worse for
some tasks, better for others, and on average, the
best since WMT11 (where agreement scores were
likely inflated due to inclusion of reference trans-
lations in the comparisons).
3.3 Models of System Rankings
The collected pairwise rankings are used to pro-
duce a ranking of the systems. Machine transla-
tion evaluation has always been a subject of con-
tention, and no exception to this rule exists for the
WMT manual evaluation. While the precise met-
ric has varied over the years, it has always shared
a common idea of computing the average num-
ber of times each system was judged better than
other systems, and ranking from highest to low-
est. For example, in WMT11 Callison-Burch et al.
(2011), the metric computed the percentage of the
time each system was ranked better than or equal
to other systems, and included comparisons to hu-
man references. In WMT12 Callison-Burch et al.
(2012), comparisons to references were dropped.
In WMT13, rankings were produced over 1,000
bootstrap-resampled sets of the training data. A
rank range was collected for each system across
these folds; the average value was used to order
the systems, and a 95% confidence interval across
these ranks was used to organize the systems into
equivalence classes containing systems with over-
18
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.400 0.311 0.244 0.342 0.279 0.305
English?Czech 0.460 0.359 0.168 0.408 0.075 0.360
German?English 0.324 0.385 0.299 0.443 0.324 0.368
English?German 0.378 0.356 0.267 0.457 0.239 0.427
French?English 0.402 0.272 0.275 0.405 0.321 0.357
English?French 0.406 0.296 0.231 0.434 0.237 0.302
Hindi?English ? ? ? ? ? 0.400
English?Hindi ? ? ? ? ? 0.413
Russian?English ? ? 0.278 0.315 0.324 0.324
English?Russian ? ? 0.243 0.416 0.207 0.418
MEAN 0.395 0.330 0.260 0.367
Table 4: ? scores measuring inter-annotator agreement. See Table 5 for corresponding intra-annotator agreement scores.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.597 0.454 0.479 0.483 0.478 0.382
English?Czech 0.601 0.390 0.290 0.547 0.242 0.448
German?English 0.576 0.392 0.535 0.643 0.515 0.344
English?German 0.528 0.433 0.498 0.649 0.452 0.576
French?English 0.673 0.360 0.578 0.585 0.565 0.629
English?French 0.524 0.414 0.495 0.630 0.486 0.507
Hindi?English ? ? ? ? ? 0.605
English?Hindi ? ? ? ? ? 0.535
Russian?English ? ? 0.450 0.363 0.477 0.629
English?Russian ? ? 0.513 0.582 0.500 0.570
MEAN 0.583 0.407 0.479 0.522
Table 5: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation.
lapping ranges.
This year, we introduce two new changes. First,
we pit the WMT13 method against two new ap-
proaches: that of Hopkins and May (2013, ?3.3.2),
and another based on TrueSkill (Sakaguchi et al.,
2014, ?3.3.3). Second, we compare these two
methods against WMT13?s ?Expected Wins? ap-
proach, and then select among them by determin-
ing which of them has the highest accuracy in
terms of predicting annotations on a held-out set
of pairwise judgments.
3.3.1 Method 1: Expected Wins (EW)
Introduced for WMT13, the EXPECTED WINS has
an intuitive score demonstrated to be accurate in
ranking systems according to an underlying model
of ?relative ability? (Koehn, 2012a). The idea is
to gauge the probability that a system S
i
will be
ranked better than another system randomly cho-
sen from a pool of opponents {S
j
: j 6= i}. If
we define the function win(A,B) as the number
of times system A is ranked better than system B,
then we can define this as follows:
score
EW
(S
i
) =
1
|{S
j
}|
?
j,j 6=i
win(S
i
, S
j
)
win(S
i
, S
j
) + win(S
j
, S
i
)
Note that this score ignores ties.
3.3.2 Method 2: Hopkins and May (HM)
Hopkins and May (2013) introduced a graphical
model formulation of the task, which makes the
notion of underlying system ability even more ex-
plicit. Each system S
J
in the pool {S
j
} is repre-
sented by an associated relative ability ?
j
and a
variance ?
2
a
(fixed across all systems) which serve
as the parameters of a Gaussian distribution. Sam-
ples from this distribution represent the quality
of sentence translations, with higher quality sam-
ples having higher values. Pairwise annotations
(S
1
, S
2
, pi) are generated according to the follow-
ing process:
19
1. Select two systems S
1
and S
2
from the pool
of systems {S
j
}
2. Draw two ?translations?, adding random
Gaussian noise with variance ?
2
obs
to simulate
the subjectivity of the task and the differences
among annotators:
q
1
? N (?
S
1
, ?
2
a
) +N (0, ?
2
obs
)
q
2
? N (?
S
2
, ?
2
a
) +N (0, ?
2
obs
)
3. Let d be a nonzero real number that defines
a fixed decision radius. Produce a rating pi
according to:
pi =
?
?
?
< q
1
? q
2
> d
> q
2
? q
1
> d
= otherwise
Hopkins and May use Gibbs sampling to infer
the set of system means from an annotated dataset.
Details of this inference procedure can be found in
Sakaguchi et al. (2014). The score used to produce
the rankings is simply the system mean associated
with each system:
score
HM
(S
i
) = ?
S
i
3.3.3 Method 3: TrueSkill (TS)
TrueSkill is an adaptive, online system that em-
ploys a similar model of relative ability Herbrich
et al. (2006). It was initially developed for Xbox
Live?s online player community, where it is used
to model player ability, assign levels, and select
competitive matches. Each player S
j
is modeled
by two parameters: TrueSkill?s current estimate
of each system?s relative ability, ?
S
j
, and a per-
system measure of TrueSkill?s uncertainty of those
estimates, ?
2
S
j
. When the outcome of a match is
observed, TrueSkill uses the relative status of the
two systems to update these estimates. If a trans-
lation from a system with a high mean is judged
better than a system with a greatly lower mean, the
result is not surprising, and the update size for the
corresponding system means will be small. On the
other hand, when an upset occurs in a competition,
the means will receive larger updates. Sakaguchi
et al. (2014) provide an adaptation of this approach
to the WMT manual evaluation, and showed that
it performed well on WMT13 data.
Similar to the Hopkins and May model,
TrueSkill scores systems by their inferred means:
score
TS
(S
i
) = ?
S
i
This score is then used to sort the systems and pro-
duce the ranking.
3.4 Method Selection
We have three methods which, provided with the
collected data, produce different rankings of the
systems. Which of them is correct? More imme-
diately, which one of them should we publish as
the official ranking for the WMT14 manual eval-
uation? As discussed, the method used to com-
pute the ranking has been tweaked a bit each year
over the past few years in response to criticisms
(e.g., Lopez (2012); Bojar et al. (2011)). While the
changes were reasonable (and later corroborated),
Hopkins and May (2013) pointed out that this task
of model selection should be driven by empirical
evaluation on held-out data, and suggested per-
plexity as the metric of choice.
We choose instead a more direct gold-standard
evaluation metric: the accuracy of the rankings
produced by each method in predicting pairwise
judgments. We use each method to produce a par-
tial ordering of the systems, grouping them into
equivalence classes. This partial ordering unam-
biguously assigns a prediction pi
P
between any
pair of systems (S
i
, S
j
). By comparing the pre-
dicted relationship pi
P
to the actual annotation for
each pairwise judgment in the test data (by token),
we can compute an accuracy score for each model.
We predict accuracy in this manner using 100-
fold cross-validation. For each task, we split the
data into a fixed set of 100 randomly-selected
folds. Each fold serves as a test set, with the
remaining ninety-nine folds available as training
data for each method. Note that the total order-
ing over systems provided by the score
?
functions
defined do not predict ties. In order to do enable
the models to predict ties, we produce equivalence
classes using the following procedure:
? Assign S
1
to a cluster
? For each system S
i
, assign it to the current
cluster if score(S
i?1
) ? score(S
i
) ? r; oth-
erwise, assign it to a new cluster
The value of r (the decision radius for ties)
is tuned using accuracy on the entire training
data using grid search over the values r ?
0, 0.01, 0.02, . . . , .25 (26 values in total). This
value is tuned separately for each method on each
fold. Table 6 contains an example partial ordering.
20
System Score Rank
B 0.60 1
D 0.44 2
E 0.39 2
A 0.25 2
F -0.09 3
C -0.22 3
Table 6: The partial ordering computed with the provided
scores when r = 0.15.
Task EW HM TS Oracle
Czech?English 40.4 41.1 41.1 41.2
English?Czech 45.3 45.6 45.9 46.8
French?English 49.0 49.4 49.3 50.3
English?French 44.6 44.4 44.7 46.0
German?English 43.5 43.7 43.7 45.2
English?German 47.3 47.4 47.2 48.2
Hindi?English 62.5 62.2 62.5 62.6
English?Hindi 53.3 53.7 53.5 55.7
Russian?English 47.6 47.7 47.7 50.6
English?Russian 46.5 46.1 46.4 48.2
MEAN 48.0 48.1 48.2 49.2
Table 7: Accuracies for each method across 100 folds, for
each translation task. The oracle uses the most frequent out-
come between each pair of systems, and therefore might not
constitute a feasible ranking.
After training, each model has defined a partial
ordering over systems.
6
This is then used to com-
pute accuracy on all the pairwise judgments in the
test fold. This process yields 100 accuracies for
each method; the average accuracy across all the
folds can then be used to compute the best method.
Table 7 contains accuracy results for the three
methods on the WMT14 tasks. On average, there
is a small improvement in accuracy moving from
Expected Wins to the H&M model, and then again
to the TrueSkill model; however, there is no pat-
tern to the best model for each class. The Oracle
column is computed by selecting the most prob-
able outcome (pi ? {<,=, >}) for each system
pair, and provides an upper bound on accuracy
when predicting outcomes using only system-level
information. Furthermore, this method of oracle
computation might not represent a feasible rank-
ing or clustering,
7
.
The TrueSkill approach was best overall, so we
used it to produce the official rankings for all lan-
6
It is a total ordering when r = 0, or when all the system
scores are outside the decision radius.
7
For example, if there were a cycle of ?better than? judg-
ments among a set of systems.
guage pairs.
3.5 Rank Ranges and Clusters
Above we saw how to produce system scores for
each method, which provides a total ordering of
the systems. But we would also like to know if the
obtained system ranking is statistically significant.
Given the large number of systems that participate,
and the similarity of the underlying systems result-
ing from the common training data condition and
(often) toolsets, there will be some systems that
will be very close in quality. These systems should
be grouped together in equivalence classes.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute a TrueSkill model score for each system
based on this sample, and then rank the systems
from 1..|{S
j
}|. By repeating this procedure 1,000
times, we can determine a range of ranks, into
which system falls at least 95% of the time (i.e.,
at least 950 times) ? corresponding to a p-level
of p ? 0.05. Furthermore, given the rank ranges
for each system, we can cluster systems with over-
lapping rank ranges.
8
Table 8 reports all system scores, rank ranges,
and clusters for all language pairs and all systems.
The official interpretation of these results is that
systems in the same cluster are considered tied.
Given the large number of judgments that we col-
lected, it was possible to group on average about
two systems in a cluster, even though the systems
in the middle are typically in larger clusters.
3.6 Cluster analysis
The official ranking results for English-German
produced clusters compute at the 90% confidence
level due to the presence of a very large cluster
(of nine systems). While there is always the pos-
sibility that this cluster reflects a true ambiguity, it
is more likely due to the fact that we didn?t have
enough data: English?German had the most sys-
8
Formally, given ranges defined by start(S
i
) and end(S
i
),
we seek the largest set of clusters {C
c
} that satisfies:
?S ?C : S ? C
S ? C
a
, S ? C
b
? C
a
= C
b
C
a
6= C
b
? ?S
i
? C
a
, S
j
? C
b
:
start(S
i
) > end(S
j
) or start(S
j
) > end(S
i
)
21
Czech?English
# score range system
1 0.591 1 ONLINE-B
2 0.290 2 UEDIN-PHRASE
3 -0.171 3-4 UEDIN-SYNTAX
-0.243 3-4 ONLINE-A
4 -0.468 5 CU-MOSES
English?Czech
# score range system
1 0.371 1-3 CU-DEPFIX
0.356 1-3 UEDIN-UNCNSTR
0.333 1-4 CU-BOJAR
0.287 3-4 CU-FUNKY
2 0.169 5-6 ONLINE-B
0.113 5-6 UEDIN-PHRASE
3 0.030 7 ONLINE-A
4 -0.175 8 CU-TECTO
5 -0.534 9 COMMERCIAL1
6 -0.950 10 COMMERCIAL2
Russian?English
# score range system
1 0.583 1 AFRL-PE
2 0.299 2 ONLINE-B
3 0.190 3-5 ONLINE-A
0.178 3-5 PROMT-HYBRID
0.123 4-7 PROMT-RULE
0.104 5-8 UEDIN-PHRASE
0.069 5-8 YANDEX
0.066 5-8 ONLINE-G
4 -0.017 9 AFRL
5 -0.159 10 UEDIN-SYNTAX
6 -0.306 11 KAZNU
7 -0.487 12 RBMT1
8 -0.642 13 RBMT4
English?Russian
# score range system
1 0.575 1-2 PROMT-RULE
0.547 1-2 ONLINE-B
2 0.426 3 PROMT-HYBRID
3 0.305 4-5 UEDIN-UNCNSTR
0.231 4-5 ONLINE-G
4 0.089 6-7 ONLINE-A
0.031 6-7 UEDIN-PHRASE
5 -0.920 8 RBMT4
6 -1.284 9 RBMT1
German?English
# score range system
1 0.451 1 ONLINE-B
2 0.267 2-3 UEDIN-SYNTAX
0.258 2-3 ONLINE-A
3 0.147 4-6 LIMSI-KIT
0.146 4-6 UEDIN-PHRASE
0.138 4-6 EU-BRIDGE
4 0.026 7-8 KIT
-0.049 7-8 RWTH
5 -0.125 9-11 DCU-ICTCAS
-0.157 9-11 CMU
-0.192 9-11 RBMT4
6 -0.306 12 RBMT1
7 -0.604 13 ONLINE-C
French?English
# score range system
1 0.608 1 UEDIN-PHRASE
2 0.479 2-4 KIT
0.475 2-4 ONLINE-B
0.428 2-4 STANFORD
3 0.331 5 ONLINE-A
4 -0.389 6 RBMT1
5 -0.648 7 RBMT4
6 -1.284 8 ONLINE-C
English?French
# score range system
1 0.327 1 ONLINE-B
2 0.232 2-4 UEDIN-PHRASE
0.194 2-5 KIT
0.185 2-5 MATRAN
0.142 4-6 MATRAN-RULES
0.120 4-6 ONLINE-A
3 0.003 7-9 UU-DOCENT
-0.019 7-10 PROMT-HYBRID
-0.033 7-10 UA
-0.069 8-10 PROMT-RULE
4 -0.215 11 RBMT1
5 -0.328 12 RBMT4
6 -0.540 13 ONLINE-C
English?German
# score range system
1 0.264 1-2 UEDIN-SYNTAX
0.242 1-2 ONLINE-B
2 0.167 3-6 ONLINE-A
0.156 3-6 PROMT-HYBRID
0.155 3-6 PROMT-RULE
0.155 3-6 UEDIN-STANFORD
3 0.094 7 EU-BRIDGE
4 0.033 8-10 RBMT4
0.031 8-10 UEDIN-PHRASE
0.012 8-10 RBMT1
5 -0.032 11-12 KIT
-0.069 11-13 STANFORD-UNC
-0.100 12-14 CIMS
-0.126 13-15 STANFORD
-0.158 14-16 UU
-0.191 15-16 ONLINE-C
6 -0.307 17-18 IMS-TTT
-0.325 17-18 UU-DOCENT
Hindi?English
# score range system
1 1.326 1 ONLINE-B
2 0.559 2-3 ONLINE-A
0.476 2-4 UEDIN-SYNTAX
0.434 3-4 CMU
3 0.323 5 UEDIN-PHRASE
4 -0.198 6-7 AFRL
-0.280 6-7 IIT-BOMBAY
5 -0.549 8 DCU-LINGO24
6 -2.092 9 IIIT-HYDERABAD
English?Hindi
# score range system
1 1.008 1 ONLINE-B
2 0.915 2 ONLINE-A
3 0.214 3 UEDIN-UNCNSTR
4 0.120 4-5 UEDIN-PHRASE
0.054 4-5 CU-MOSES
5 -0.111 6-7 IIT-BOMBAY
-0.142 6-7 IPN-UPV-CNTXT
6 -0.233 8-9 DCU-LINGO24
-0.261 8-9 IPN-UPV-NODEV
7 -0.449 10-11 MANAWI-H1
-0.494 10-11 MANAWI
8 -0.622 12 MANAWI-RMOOV
Table 8: Official results for the WMT14 translation task. Systems are ordered by their inferred system means. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05, except for English?German, where p ? 0.1.
This method is also used to determine the range of ranks into which system falls. Systems with grey background indicate use
of resources that fall outside the constraints provided for the shared task.
22
tems (18, compared to 13 for the next languages),
yet only an average amount of per-system data.
Here, we look at this language pair in more detail,
in order to justify this decision, and to shed light
on the differences between the ranking methods.
Table 9 presents the 95% confidence-level clus-
terings for English?German computed with each
of the three methods, along with lines that show
the reorderings of the systems between them. Re-
orderings of this type have been used to argue
against the reliability of the official WMT rank-
ing (Lopez, 2012; Hopkins and May, 2013). This
table shows that these reorderings are captured en-
tirely by the clustering approach we used. This rel-
ative consensus of these independently-computed
and somewhat different models suggests that the
published ranking is approaching the true ambigu-
ity underlying systems within the same cluster.
Looking across all language pairs, we find that
the total ordering predicted by EW and TS is ex-
actly the same for eight of the ten language pair
tasks, and is constrained to reorderings within
the official cluster for the other two (German?
English ? just one adjacent swap ? and English?
German, depicted in Table 9).
3.7 Conclusions
The official ranking method employed by WMT
over the past few years has changed a few times as
a result of error analysis and introspection. Until
this year, these results were largely based on the
intuitions of the community and organizers about
deficiencies in the models. In addition to their in-
tuitive appeal, many of these changes (such as the
decision to throw out comparisons against refer-
ences) have been empirically validated Hopkins
and May (2013). The actual effect of the refine-
ments in the ranking metric has been minor pertur-
bations in the permutation of systems. The cluster-
ing method of Koehn (2012b), in which the official
rankings are presented as a partial (instead of to-
tal) ordering, alleviated many of the problems ob-
served by Lopez (2012), and also capture all the
variance across the new systems introduced this
year. In addition, presenting systems as clusters
appeals to intuition. As such, we disagree with
claims that there is a problem with irreproducibil-
ity of the results of the workshop evaluation task,
and especially disagree that there is anything ap-
proaching a ?crisis of confidence? (Hopkins and
May, 2013). These claims seem to us to be over-
stated.
Conducting proper model selection by compar-
ison on held-out data, however, is a welcome sug-
gestion, and our inclusion of this process supports
improved confidence in the ranking results. That
said, it is notable that the different methods com-
pute very similar orderings. This avoids hallu-
cinating distinctions among systems that are not
really there, and captures the intuition that some
systems are basically equivalent. The chief ben-
efit of the TrueSkill model is not in outputting a
better complete ranking of the systems, but lies in
its reduced variance, which allow us to cluster the
systems with less data. There is also the unex-
plored avenue of using TrueSkill to drive the data
collection, steering the annotations of judges to-
wards evenly matched systems during the collec-
tion phase, potentially allowing confident results
to be presented while collecting even less data.
There is, of course, more work to be done.
We have produced this year statistically significant
clusters with a third of the data required last year,
which is an improvement. Models of relative abil-
ity are a natural fit for the manual evaluation, and
the introduction of an online Bayesian approach
to data collection present further opportunities to
reduce the amount of data needed. These methods
also provide a framework for extending the models
in a variety of potentially useful ways, including
modeling annotator bias, incorporating sentence
metadata (such as length, difficulty, or subtopic),
and adding features of the sentence pairs.
4 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
third edition of the WMT shared task on qual-
ity estimation builds on the previous editions of
the task (Callison-Burch et al., 2012; Bojar et al.,
2013), with tasks including both sentence-level
and word-level estimation, with new training and
test datasets.
The goals of this year?s shared task were:
? To investigate the effectiveness of different
quality labels.
? To explore word-level quality prediction at
23
Expected Wins Hopkins & May TrueSkill
UEDIN-SYNTAX UEDIN-SYNTAX UEDIN-SYNTAX
ONLINE-B ONLINE-B ONLINE-B
ONLINE-A UEDIN-STANFORD ONLINE-A
UEDIN-STANFORD PROMT-HYBRID PROMT-HYBRID
PROMT-RULE ONLINE-A PROMT-RULE
PROMT-HYBRID PROMT-RULE UEDIN-STANFORD
EU-BRIDGE EU-BRIDGE EU-BRIDGE
RBMT4 UEDIN-PHRASE RBMT4
UEDIN-PHRASE RBMT4 UEDIN-PHRASE
RBMT1 RBMT1 RBMT1
KIT KIT KIT
STANFORD-UNC STANFORD-UNC STANFORD-UNC
CIMS CIMS CIMS
STANFORD STANFORD STANFORD
UU UU UU
ONLINE-C ONLINE-C ONLINE-C
IMS-TTT UU-DOCENT IMS-TTT
UU-DOCENT IMS-TTT UU-DOCENT
Table 9: A comparison of the rankings produced by Expected Wins, Hopkins & May, and TrueSkill for English?German (the
task with the most systems and the largest cluster). The lines extending all the way across mark the official English?German
clustering (computed from TrueSkill with 90% confidence intervals), while bold entries mark the start of new clusters within
each method or column (computed at the 95% confidence level). The TrueSkill clusterings contain all the system reorderings
across the other two ranking methods.
different levels of granularity.
? To study the effects of training and test
datasets with mixed domains, language pairs
and MT systems.
? To examine the effectiveness of quality pre-
diction methods on human translations.
Four tasks were proposed: Tasks 1.1, 1.2, 1.3
are defined at the sentence-level (Sections 4.1),
while Task 2, at the word-level (Section 4.2). Each
task provides one or more datasets with up to four
language pairs each: English-Spanish, English-
German, German-English, Spanish-English, and
up to four alternative translations generated by:
a statistical MT system (SMT), a rule-based MT
system (RBMT), a hybrid MT system, and a hu-
man. These datasets were annotated with differ-
ent labels for quality by professional translators as
part of the QTLaunchPad
9
project. External re-
sources (e.g. parallel corpora) were provided to
participants. Any additional resources, including
additional quality estimation training data, could
9
http://www.qt21.eu/launchpad/
be used by participants (no distinction between
open and close tracks is made). Participants were
also provided with a software package to extract
quality estimation features and perform model
learning, with a suggested list of baseline features
and learning method for sentence-level prediction.
Participants, described in Section 4.3, could sub-
mit up to two systems for each task.
Data used for building specific MT systems or
internal system information (such as n-best lists)
were not made available this year as multiple MT
systems were used to produced the datasets, in-
cluding rule-based systems. In addition, part of
the translations were produced by humans. Infor-
mation on the sources of translations was not pro-
vided either. Therefore, as a general rule, partici-
pants were only allowed to use black-box features.
4.1 Sentence-level Quality Estimation
For the sentence-level tasks, two variants of the
results could be submitted for each task and lan-
guage pair:
? Scoring: An absolute quality score for each
sentence translation according to the type of
24
prediction, to be interpreted as an error met-
ric: lower scores mean better translations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning).
Evaluation was performed against the true label
and/or HTER ranking using the same metrics as in
previous years:
? Scoring: Mean Average Error (MAE) (pri-
mary metric), Root Mean Squared Error
(RMSE).
? Ranking: DeltaAvg (primary metric) (Bojar
et al., 2013) and Spearman?s rank correlation.
For all sentence-level these tasks, the same 17
features as in WMT12-13 were used to build base-
line systems. The SVM regression algorithm
within QUEST (Specia et al., 2013)
10
was applied
for that with RBF kernel and grid search for pa-
rameter optimisation.
Task 1.1 Predicting post-editing effort
Data in this task is labelled with discrete and
absolute scores for perceived post-editing effort,
where:
? 1 = Perfect translation, no post-editing
needed at all.
? 2 = Near miss translation: translation con-
tains maximum of 2-3 errors, and possibly
additional errors that can be easily fixed (cap-
italisation, punctuation, etc.).
? 3 = Very low quality translation, cannot be
easily fixed.
The datasets were annotated in a ?triage? phase
aimed at selecting translations of type ?2? (near
miss) that could be annotated for errors at the
word-level using the MQM metric (see Task 2, be-
low) for a more fine-grained and systematic trans-
lation quality analysis. Word-level errors in trans-
lations of type ?3? are too difficult if not impos-
sible to annotate and classify, particularly as they
often contain inter-related errors in contiguous or
overlapping word spans.
10
http://www.quest.dcs.shef.ac.uk/
For the training of prediction models, we pro-
vide a new dataset consisting of source sen-
tences and their human translations, as well as
two-three versions of machine translations (by an
SMT system, an RBMT system and, for English-
Spanish/German only, a hybrid system), all in the
news domain, extracted from tests sets of various
WMT years and MT systems that participated in
the translation shared task:
# Source sentences # Target sentences
954 English 3,816 Spanish
350 English 1,400 German
350 German 1,050 English
350 Spanish 1,050 English
As test data, for each language pair and MT sys-
tem (or human translation) we provide a new set
of translations produced by the same MT systems
(and humans) as those used for the training data:
# Source sentences # Target sentences
150 English 600 Spanish
150 English 600 German
150 German 450 English
150 Spanish 450 English
The distribution of true scores in both training
and test sets for each language pair is given in Fig-
ures 3.
0%#
10%#
20%#
30%#
40%#
50%#
60%#
{en-
de-1
}#
{en-
de-2
}#
{en-
de-3
}#
{de-
en-1
}#
{de-
en-2
}#
{de-
en-3
}#
{en-
es-1
}#
{en-
es-2
}##
{en-
es-3
}##
{es-
en-1
}#
{es-
en-2
}#
{es-
en-3
}#
#Training##### #Test####
Figure 3: Distribution of true 1-3 scores by langauge pair.
Additionally, we provide some out of domain
test data. These translations were annotated in
the same way as above, each dataset by one Lan-
guage Service Provider (LSP), i.e, one profes-
sional translator, with two LPSs producing data in-
dependently for English-Spanish. They were gen-
erated using the LSPs? own source data (a different
domain from news), and own MT system (differ-
ent from the three used for the official datasets).
The results on these datasets were not considered
25
for the official ranking of the participating sys-
tems:
# Source sentences # Target sentences
971 English 971 Spanish
297 English 297 German
388 Spanish 388 English
Task 1.2 Predicting percentage of edits
In this task we use HTER (Snover et al., 2006) as
quality score. This score is to be interpreted as
the minimum edit distance between the machine
translation and its manually post-edited version,
and its range is [0, 1] (0 when no edit needs to
be made, and 1 when all words need to be edited).
We used TERp (default settings: tokenised, case
insensitive, etc., but capped to 1)
11
to compute the
HTER scores.
For practical reasons, the data is a subset of
Task 1.1?s dataset: only translations produced
by the SMT system English-Spanish. As train-
ing data, we provide 896 English-Spanish trans-
lation suggestions and their post-editions. As
test data, we provide a new set of 208 English-
Spanish translations produced by the same SMT
system. Each of the training and test translations
was post-edited by a professional translator using
the CASMACAT
12
web-based tool, which also col-
lects post-editing time on a sentence-basis.
Task 1.3 Predicting post-editing time
For this task systems are required to produce, for
each translation, a real valued estimate of the time
(in milliseconds) it takes a translator to post-edit
the translation. The training and test sets are a sub-
set of that uses in Task 1.2 (subject to filtering of
outliers). The difference is that the labels are now
the number of milliseconds that were necessary to
post-edit each translation.
As training data, we provide 650 English-
Spanish translation suggestions and their post-
editions. As test data, we provide a new set of 208
English-Spanish translations (same test data as for
Task 1.2).
4.2 Word-level Quality Estimation
The data for this task is based on a subset of the
datasets used for Task 1.1, for all language pairs,
11
http://www.umiacs.umd.edu/
?
snover/terp/
12
http://casmacat.eu/
human and machine translations: those transla-
tions labelled ?2? (near misses), plus additional
data provided by industry (either on the news do-
main or on other domains, such as technical doc-
umentation, produced using their own MT sys-
tems, and also pre-labelled as ?2?). All seg-
ments were annotated with word-level labels by
professional translators using the core categories
in MQM (Multidimensional Quality Metrics)
13
as
error typology (see Figure 4). Each word or se-
quence of words was annotated with a single error.
For (supposedly rare) cases where a decision be-
tween multiple fine-grained error types could not
be made, annotators were requested to choose a
coarser error category in the hierarchy.
Participants are asked to produce a label for
each token that indicates quality at different lev-
els of granularity:
? Binary classification: an OK / bad label,
where bad indicates the need for editing the
token.
? Level 1 classification: an OK / accuracy /
fluency label, specifying coarser level cate-
gories of errors for each token, or ?OK? for
tokens with no error.
? Multi-class classification: one of the labels
specifying the error type for the token (termi-
nology, mistranslation, missing word, etc.) in
Figure 4, or ?OK? for tokens with no error.
As training data, we provide tokenised transla-
tion output for all language pairs, human and ma-
chine translations, with tokens annotated with all
issue types listed above, or ?OK?. The annotation
was performed manually by professional transla-
tors as part of the QTLaunchPad project. For
the coarser variants, fine-grained errors are gen-
eralised to Accuracy or Fluency, or ?bad? for the
binary variant. The amount of available training
data varies by language pair:
# Source sentences # Target sentences
1,957 English 1,957 Spanish
715 English 715 German
350 German 350 English
900 Spanish 900 English
13
http://www.qt21.eu/launchpad/content/
training
26
Figure 4: MQM metric as error typology.
As test data, we provide additional data points
for all language pairs, human and machine trans-
lations:
# Source sentences # Target sentences
382 English 382 Spanish
150 English 150 German
100 German 100 English
150 Spanish 150 English
In contrast to Tasks 1.1?1.3, no baseline feature
set is provided to the participants.
Similar to last year (Bojar et al., 2013), the
word-level task is primarily evaluated by macro-
averaged F-measure (in %). Because the class dis-
tribution is skewed ? in the test data about 78% of
the tokens are marked as ?OK? ? we compute pre-
cision, recall, and F
1
for each class individually,
weighting F
1
scores by the frequency of the class
in the test data. This avoids giving undue impor-
tance to less frequent classes. Consider the follow-
ing confusion matrix for Level 1 annotation, i.e.
the three classes (O)K, (F)luency, and (A)ccuracy:
reference
O F A
predicted
O 4172 1482 193
F 1819 1333 214
A 198 133 69
For each of the three classes we assume a binary
setting (one-vs-all) and derive true-positive (tp),
false-positive (fp), and false-negative (fn) counts
from the rows and columns of the confusion ma-
trix as follows:
tp
O
= 4172
fp
O
= 1482 + 193 = 1675
fn
O
= 1819 + 198 = 2017
tp
F
= 1333
fp
F
= 1819 + 214 = 2033
fn
F
= 1482 + 133 = 1615
tp
A
= 69
fp
A
= 198 + 133 = 331
fn
A
= 193 + 214 = 407
We continue to compute F
1
scores for each
class c ? {O,F,A}:
precision
c
= tp
c
/(tp
c
+ fp
c
)
recall
c
= tp
c
/(tp
c
+ fn
c
)
F
1,c
=
2 ? precision
c
? recall
c
precision
c
+recall
c
yielding:
precision
O
= 4172/(4172 + 1675) = 0.7135
recall
O
= 4172/(4172 + 2017) = 0.6741
F
1,O
=
2 ? 0.7135 ? 0.6741
0.7135 + 0.6741
= 0.6932
? ? ?
F
1,F
= 0.4222
F
1,A
= 0.1575
Finally, we compute the average of F
1,c
scores
weighted by the occurrence count N(c) of c:
weightedF
1,ALL
=
1
?
c
N(c)
?
c
N
c
? F
1,c
weightedF
1,ERR
=
1
?
c:c 6=O
N(c)
?
c:c 6=O
N
c
? F
1,c
27
which for the above example gives:
weightedF
1,ALL
=
1
6189 + 2948 + 476
?
(6189 ? 0.6932 + 2948 ? 0.4222
+476 ? 0.1575) = 0.5836
weightedF
1,ERR
=
1
2948 + 476
?
(2948 ? 0.4222 + 476 ? 0.1575)
= 0.3854
We choose F
1,ERR
as our primary evaluation mea-
sure because it most closely mimics the common
application of F
1
scores in binary classification:
one is interested in the performance in detecting a
positive class, which in this case would be erro-
neous words. This does, however, ignore the num-
ber of correctly classified words of the OK class,
which is why we also report F
1,ALL
. In addition,
we follow Powers (2011) and report Matthews
Correlation Coefficient (MCC), averaged in the
same way as F
1
, as our secondary metric. Finally,
for contrast we also report Accuracy (ACC).
4.3 Participants
Table 10 lists all participating teams. Each team
was allowed up to two submissions for each task
and language pair. In the descriptions below, par-
ticipation in specific tasks is denoted by a task
identifier: T1.1, T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.2,
T1.3): QUEST is used to extract 17 system-
independent features from source and trans-
lation sentences and parallel corpora (same
features as in the WMT12 shared task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? language model (LM) probability of
source and target sentences based on
models for the WMT News Commen-
tary corpus.
? average number of translations per
source word in the sentence as given by
IBM Model 1 extracted from the WMT
News Commentary parallel corpus, and
thresholded so that P (t|s) > 0.2, or
so that P (t|s) > 0.01 weighted by the
inverse frequency of each word in the
source side of the parallel corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source language
extracted from the WMT News Com-
mentary corpus.
? percentage of unigrams in the source
sentence seen in the source side of the
WMT News Commentary corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within
the SCIKIT-LEARN toolkit. The ?,  and C
parameters were optimised via grid search
with 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as ?baseline?, it is in fact a strong
system. It has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting various forms of post-
editing effort (Callison-Burch et al., 2012;
Bojar et al., 2013).
DCU (T1.1): DCU-MIXED and DCU-SVR use
a selection of features available in QUEST,
such as punctuation statistics, LM perplex-
ity, n-gram frequency quartile statistics and
coarse-grained POS frequency ratios, and
four additional feature types: combined POS
and stop word LM features, source-side
pseudo-reference features, inverse glass-box
features for translating the translation and er-
ror grammar parsing features. For machine
learning, the QUEST framework is expanded
to combine logistic regression and support
vector regression and to handle cross- valida-
tion and randomisation in a way that training
items with the same source side are kept to-
gether. External resources are monolingual
corpora taken from the WMT 2014 transla-
tion task for LMs, the MT system used for the
inverse glass-box features (Li et al., 2014b)
and, for error grammar parsing, the Penn-
Treebank and an error grammar derived from
it (Foster, 2007).
28
ID Participating team
DCU Dublin City University Team 1, Ireland (Hokamp et al., 2014)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis,
2014)
FBK-UPV-UEDIN Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia,
Spain & University of Edinburgh, UK (Camargo de Souza et al., 2014)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al., 2014)
LIMSI Laboratoire d?Informatique pour la M?ecanique et les Sciences de l?Ing?enieur,
France (Wisniewski et al., 2014)
MULTILIZER Multilizer, Finland
RTM-DCU Dublin City University Team 2, Ireland (Bicici and Way, 2014)
SHEF-lite University of Sheffield Team 1, UK (Beck et al., 2014)
USHEFF University of Sheffield Team 2, UK (Scarton and Specia, 2014)
YANDEX Yandex, Russia
Table 10: Participants in the WMT14 Quality Estimation shared task.
DFKI (T1.2): DFKI/SVR builds upon the base-
line system (above) by adding non-redundant
data from the WMT13 task for predicting
the same label (HTER) and additional fea-
tures such as (a) rule-based language cor-
rections (language tool) (b), PCFG parsing
statistics and counts of tree labels, (c) po-
sition statistics of parsing labels, (d) posi-
tion statistics of trigrams with low probabil-
ity. DFKI/SVRxdata uses a similar setting,
with the addition of more training data from
non-minimally post-edited translation out-
puts (references), filtered based on a thresh-
old on the edit distance between the MT out-
put and the freely-translated reference.
FBK-UPV-UEDIN (T1.2, T1.3, T2): The sub-
missions for the word-level task (T2) use fea-
tures extracted from word posterior probabil-
ities and confusion network descriptors com-
puted over the 100k-best hypothesis transla-
tions generated by a phrase-based SMT sys-
tem. They also use features from word lexi-
cons, and POS tags of each word for source
and translation sentences. The predictions of
the Binary model are used as a feature for the
Level 1 and Multi-class settings. Both condi-
tional random fields (CRF) and bidirectional
long short-term memory recurrent neural net-
works (BLSTM-RNNs) are used for the Bi-
nary setting, and BLSTM-RNNs only for the
Level 1 and Multi-class settings.
The sentence-level QE submissions (T1.2
and T1.3) are trained on black-box features
extracted using QUEST in addition to fea-
tures based on word alignments, word poste-
rior probabilities and diversity scores (Souza
et al., 2013). These features are computed
over 100k-best hypothesis translations also
used for task 2. In addition, a set of ratios
computed from the word-level predictions of
the model trained on the binary setting of
task 2 is used. A total of 221 features and
the extremely randomised trees (Geurts et al.,
2006) learning algorithm are used to train re-
gression models.
LIG (T2): Conditional Random Fields classi-
fiers are trained with features used in LIG?s
WMT13 systems (Luong et al., 2013): tar-
get and source words, alignment informa-
tion, source and target alignment context,
LM scores, target and source POS tags,
lexical categorisations (stopword, punctua-
tion, proper name, numerical), constituent
label, depth in the constituent tree, target
polysemy count, pseudo reference. These
are combined with novel features: word
occurrence in multiple translation systems
and POS tag-based LM scores (longest tar-
get/source n-gram length and backoff score
for POS tag). These features require external
NLP tools and resources such as: TreeTag-
ger, GIZA++, Bekerley parser, Link Gram-
mar parser, WordNet and BabelNet, Google
Translate (pseudo-reference). For the binary
task, the optimal classification threshold is
tuned based on a development set split from
the original training set. Feature selection is
employed over the all features (for the binary
29
task only), with the Sequential Backward Se-
lection algorithm. The best performing fea-
ture set is then also used for the Level 1 and
Multi-class variants.
LIMSI (T2): The submission relies on a ran-
dom forest classifier and considers only 16
dense and continuous features. To prevent
sparsity issues, lexicalised information such
as the word or the previous word identities
is not included. The features considered are
mostly classic MT features and can be cat-
egorised into two classes: association fea-
tures, which describe the quality of the as-
sociation between the source sentence and
each target word, and fluency features, which
describe the ?quality? of the translation hy-
potheses. The latter rely on different lan-
guage models (either on POS or on words)
and the former on IBM Model 1 translation
probabilities and on pseudo- references, i.e.
translation produced by an independent MT
system. Random forests are known to per-
form well in tasks like this one, in which
only a few dense and continuous features are
available, possibly because of their ability to
take into account complex interactions be-
tween features and to automatically partition
the continuous feature values into a discrete
set of intervals that achieves the best classifi-
cation performance. Since they predict the
class probabilities, it is possible to directly
optimize the F
1
score during training by find-
ing, with a grid search method, the decision
threshold that achieved the best F
1
score on
the training set.
MULTILIZER (T1.2, T1.3): The 80 black-box
features from QUEST are used in addition to
new features based on using other MT en-
gines for forward and backward translations.
In forward translations, the idea is that dif-
ferent MT engines make different mistakes.
Therefore, when several forward translations
are similar to each other, these translations
are more likely to be correct. This is con-
firmed by the Pearson correlation of similar-
ities between the forward translations against
the true scores (above 0.5). A backward
translation is very error-prone and therefore
it has to be used in combination with for-
ward translations. A single back-translation
similar to original source segment does not
bring much information. Instead, when sev-
eral MT engines give back-translations simi-
lar to this source segment, one can conclude
that the translation is reliable. Those transla-
tions where similarities both in forward trans-
lation and backward translation are high are
intuitively more likely to be good. A simple
feature selection method that omits all fea-
tures with Pearson correlation against the true
scores below 0.2 is used. The systems sub-
mitted are obtained using linear regression
models.
RTM-DCU (T1.1, T1.2, T1.3, T2): RTM-DCU
systems are based on referential translation
machines (RTM) (Bic?ici, 2013) and parallel
feature decay algorithms (ParFDA5) (Bic?ici
et al., 2014), which allow language and MT
system-independent predictions. For each
task, individual RTM models are developed
using the parallel corpora and the language
model corpora distributed by the WMT14
translation task and the language model cor-
pora provided by LDC for English and Span-
ish. RTMs use 337 to 437 sentence-level fea-
tures for coverage and diversity, IBM1 and
sentence translation performance, retrieval
closeness and minimum Bayes retrieval risk,
distributional similarity and entropy, IBM2
alignment, character n-grams, sentence read-
ability, and parse output tree structures. The
features use ngrams defined over text or com-
mon cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over
which similarity calculations are performed.
Learning models include ridge regression
(RR), support vector machines (SVR), and
regression trees (TREE), which are applied
after partial least squares (PLS) or feature
selection (FS). For word-level prediction,
generalised linear models (GLM) (Collins,
2002) and GLM with dynamic learning
(GLMd) (Bic?ici, 2013) are used with word-
level features including CCL links, word
length, location, prefix, suffix, form, context,
and alignment, totalling up to a couple of mil-
lion features.
SHEF-lite (T1.1, T1.2, T1.3): These submis-
sions use the framework of Multi-task Gaus-
sian Processes, where multiple datasets are
30
combined in a multi-task setting similar to
the one used by Cohn and Specia (2013).
For T1.1, data for all language pairs is put
together, and each language is considered a
task. For T1.2 and T1.3, additional datasets
from previous shared task years are used,
each encoded as a different task. For all tasks,
the QUEST framework is used to extract a set
of 80 black-box features (a superset of the 17
baseline features). To cope with the large size
of the datasets, the SHEF-lite-sparse submis-
sion uses Sparse Gaussian Processes, which
provide sensible sparse approximations using
only a subset of instances (inducing inputs)
to speed up training and prediction. For this
?sparse? submission, feature selection is per-
formed following the approach of Shah et al.
(2013) by ranking features according to their
learned length-scales and selecting the top 40
features.
USHEFF (T1.1, T1.2, T1.3): USHEFF submis-
sions exploit the use of consensus among
MT systems by comparing the MT sys-
tem output to several alternative translations
generated by other MT systems (pseudo-
references). The comparison is done using
standard evaluation metrics (BLEU, TER,
METEOR, ROUGE for all tasks, and two
metrics based on syntactic similarities from
shallow and dependency parser information
for T1.2 and T1.3). Figures extracted from
such metrics are used as features to com-
plement prediction models trained on the 17
baseline features. Different from the standard
use of pseudo-reference features, these fea-
tures do not assume that the alternative MT
systems are better than the system of inter-
est. A more realistic scenario is considered
where the quality of the pseudo-references is
not known. For T1, no external systems in
addition to those provided for the shared task
are used: for a given translation, all alter-
native translations for the same source seg-
ment (two or three, depending on the lan-
guage pair) are used as pseudo-references.
For T1.2 and T1.3, for each source sentence,
all alternative translations produced by MT
systems on the same data (WMT12/13) are
used as pseudo-references. The hypothesis
is that by using translations from several MT
systems one can find consensual information
and this can smooth out the effect of ?coinci-
dences? in the similarities between systems?
translations. SVM regression with radial ba-
sis function kernel and hyper-parameters op-
timised via grid search is used to build the
models.
YANDEX (T1.1): Both submissions are based
on the the 80 black-box features, plus an
LM score from a larger language model,
a pseudo-reference, and several additional
features based on POS tags and syntactic
parsers. The first attempt uses an extract
of the top 5 features selected with a greedy
search from the set of all features. SVM re-
gression is used as machine learning algo-
rithm. The second attempt uses the same
features processed with Yandex? implemen-
tation of the gradient tree boosting (Ma-
trixNet).
4.4 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing effort
Table 11 summarises the results for the ranking
variant of Task 1.1. They are sorted from best to
worst using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are as follows: for English-Spanish
it is RTM-DCU/RTM-TREE, with a DeltaAvg
score of 0.26; for Spanish-English it is USH-
EFF, with a DeltaAvg score of 0.23; for English-
German it is again RTM-DCU/RTM-TREE, with a
DeltaAvg score of 0.39; and for German-English it
is RTM-DCU/RTM-RR, with a DeltaAvg score of
0.38. These winning submissions are better than
the baseline system by a large margin, which indi-
cates that current best performance in MT quality
estimation has reached levels that are clearly be-
yond what the baseline system can produce. As for
the other systems, according to DeltaAvg, com-
pared to the previous year results a smaller per-
centage of systems is able to beat the baseline.
This might be a consequence of the use of the met-
ric for the prediction of only three discrete labels.
The results for the scoring task are presented in
Table 12, sorted from best to worst using the MAE
31
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.26 0.38
? RTM-DCU/RTM-TREE 0.26 0.41
? YANDEX/SHAD BOOSTEDTREES2 0.23 0.35
USHEFF 0.21 0.33
SHEFF-lite 0.21 0.33
YANDEX/SHAD SVR1 0.18 0.29
SHEFF-lite-sparse 0.17 0.27
Baseline SVM 0.14 0.22
Spanish-English
? USHEFF 0.23 0.30
? RTM-DCU/RTM-PLS-RR 0.20 0.35
? RTM-DCU/RTM-FS-RR 0.19 0.36
Baseline SVM 0.12 0.21
SHEFF-lite-sparse 0.12 0.17
SHEFF-lite 0.11 0.15
English-German
? RTM-DCU/RTM-TREE 0.39 0.54
RTM-DCU/RTM-PLS-TREE 0.33 0.42
USHEFF 0.26 0.41
SHEFF-lite 0.26 0.36
Baseline SVM 0.23 0.34
SHEFF-lite-sparse 0.23 0.33
German-English
? RTM-DCU/RTM-RR 0.38 0.51
? RTM-DCU/RTM-PLS-RR 0.35 0.45
USHEFF 0.28 0.30
SHEFF-lite 0.24 0.27
Baseline SVM 0.21 0.25
SHEFF-lite-sparse 0.14 0.17
Table 11: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
32
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.49 0.61
? SHEFF-lite 0.49 0.63
? USHEFF 0.49 0.63
? SHEFF-lite/sparse 0.49 0.69
? RTM-DCU/RTM-TREE 0.49 0.61
Baseline SVM 0.52 0.66
YANDEX/SHAD BOOSTEDTREES2 0.56 0.68
YANDEX/SHAD SVR1 0.64 0.81
DCU-Chris/SVR 0.66 0.88
DCU-Chris/MIXED 0.94 1.14
Spanish-English
? RTM-DCU/RTM-FS-RR 0.53 0.64
? SHEFF-lite/sparse 0.54 0.69
? RTM-DCU/RTM-PLS-RR 0.55 0.71
USHEFF 0.57 0.67
Baseline SVM 0.57 0.68
SHEFF-lite 0.62 0.77
DCU-Chris/MIXED 0.65 0.91
English-German
? RTM-DCU/RTM-TREE 0.58 0.68
RTM-DCU/RTM-PLS-TREE 0.60 0.71
SHEFF-lite 0.63 0.74
USHEFF 0.64 0.75
SHEFF-lite/sparse 0.64 0.75
Baseline SVM 0.64 0.76
DCU-Chris/MIXED 0.69 0.98
German-English
? RTM-DCU/RTM-RR 0.55 0.67
? RTM-DCU/RTM-PLS-RR 0.57 0.74
USHEFF 0.63 0.76
SHEFF-lite 0.65 0.77
Baseline SVM 0.65 0.78
Table 12: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
33
metric scores as primary key and the RMSE metric
scores as secondary key.
The winning submissions for the scoring variant
of Task 1.1 are as follows: for English-Spanish it
is RTM-DCU/RTM-TREE with a MAE of 0.49;
for Spanish-English it is RTM-DCU/RTM-FS-
RR with a MAE of 0.53; for English-German
it is again RTM-DCU/RTM-TREE, with a MAE
of 0.58; and for German-English it is RTM-
DCU/RTM-RR with a MAE of 0.55. These sub-
missions are again much better than the baseline
system, which under the scoring variant seems
to perform at a middle-of-the-pack level or lower
compared to the overall pool of submissions.
Overall, more systems are able to outperform the
baseline according to the scoring metric.
The top system for most language pairs are
essentially based on the same core techniques
(RTM-DCU) according to both the DeltaAvg and
MAE metrics. The ranking of other systems, how-
ever, can be substantially different according to the
two metrics.
Task 1.2 Predicting percentage of edits
Table 13 summarises the results for the ranking
variant of Task 1.2. For readability purposes we
have used a multiplication-factor of 100 in the
scoring script, which makes the HTER numbers
(both predicted and gold) to be in the [0, 100]
range. They are sorted from best to worst using
the DeltaAvg metric scores as primary key and the
Spearman?s rank correlation scores as secondary
key.
The winning submission for the ranking vari-
ant of Task 1.2 is RTM-DCU/RTM-SVR, with a
DeltaAvg score of 9.31. There is a large mar-
gin between this score and the baseline score of
DeltaAvg 5.08, which indicates again that current
best performance has reached levels that are much
beyond what this baseline system can produce.
The vast majority of the submissions perform bet-
ter than the baseline (the only exception is the sub-
mission from SHEFF-lite, for which the authors
report a major issue with the learning algorithm).
The results for the scoring variant are presented
in Table 14, sorted from best to worst by using the
MAE metric scores as primary key and the RMSE
metric scores as secondary key.
The winning submission for the scoring variant
of Task 1.2 is FBK-UPV-UEDIN/WP with a MAE
of 12.89, while the baseline system has a MAE
of 15.23. Most of the submissions perform better
than the baseline.
Task 1.3 Predicting post-editing time
Table 15 summarises the results for the ranking
variant of Task 1.3. For readability purposes, we
have used a multiplication-factor of 0.001 in the
scoring script, which makes the time (both pre-
dicted and gold) to be measured in seconds. They
are sorted from best to worst using the DeltaAvg
metric scores as primary key and the Spearman?s
rank correlation scores as secondary key.
The winning submission for the ranking vari-
ant of Task 1.3 is RTM-DCU/RTM-RR, with a
DeltaAvg score of 17.02 (when predicting sec-
onds). The interesting aspect of these results is
that the DeltaAvg numbers have a direct real-
world interpretation, in terms of time spent (or
saved, depending on one?s view-point) for post-
editing machine-produced translations. A more
elaborate discussion on this point can be found in
Section 4.5.
The winning submission for the scoring variant
of Task 1.3 is RTM-DCU/RTM-SVR, with a MAE
of 16.77. Note that all of the submissions perform
significantly better than the baseline, which has a
MAE of 21.49, and that the majority is not signif-
icantly worse than the top scoring submission.
Task 2 Predicting word-level edits
The results for Task 2 are summarised in Tables
17?19. The results are ordered by F
1
score for
the Error (BAD) class. For comparison, two triv-
ial baselines are included, one that marks every
word as correct and that marks every word with
the most common error class found in the training
data. Both baselines are clearly useless for any ap-
plication, but help put the results in perspective.
Most teams submitted systems for a single lan-
guage pair: English-Spanish; only a single team
produced predictions for all four pairs.
Table 17 gives the results of the binary (OK vs.
BAD) classification variant of Task 2. The win-
ning submissions for this variant are as follows:
for English-Spanish it is FBK-UPV-UEDIN/RNN
with a weighted F
1
of 48.73; for Spanish-
English it is RTM-DCU/RTM-GLMd with a
weighted F
1
of 29.14; for English-German it is
RTM-DCU/RTM-GLM with a weighted F
1
of
45.30; and for German-English it is again RTM-
DCU/RTM-GLM with a weighted F
1
of 26.13.
Remarkably, for three out of four language
pairs, the systems fail to beat our trivial baseline of
34
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-SVR 9.31 0.53
? RTM-DCU/RTM-TREE 8.57 0.48
? USHEFF 7.93 0.45
SHEFF-lite/sparse 7.69 0.43
Baseline 5.08 0.31
SHEFF-lite 0.72 0.09
Table 13: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (100k times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
System ID MAE RMSE
English-Spanish
? FBK-UPV-UEDIN/WP 12.89 16.74
? RTM-DCU/RTM-SVR 13.40 16.69
? USHEFF 13.61 17.84
RTM-DCU/RTM-TREE 14.03 17.48
DFKI/SVR 14.32 17.74
FBK-UPV-UEDIN/NOWP 14.38 18.10
SHEFF-lite/sparse 15.04 18.38
MULTILIZER 15.04 20.86
Baseline 15.23 19.48
DFKI/SVRxdata 16.01 19.52
SHEFF-lite 18.15 23.41
Table 14: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-RR 17.02 0.68
? RTM-DCU/RTM-SVR 16.60 0.67
SHEFF-lite/sparse 16.33 0.63
SHEFF-lite 16.08 0.64
USHEFF 14.98 0.59
Baseline 14.71 0.57
Table 15: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
35
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-SVR 16.77 26.17
?MULTILIZER/MLZ2 17.07 25.83
? SHEFF-lite 17.13 27.33
?MULTILIZER/MLZ1 17.31 25.51
? SHEFF-lite/sparse 17.42 27.35
? FBK-UPV-UEDIN/WP 17.48 25.31
RTM-DCU/RTM-RR 17.50 25.97
FBK-UPV-UEDIN/NOWP 18.69 26.58
USHEFF 21.48 34.28
Baseline 21.49 34.28
Table 16: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
weighted F
1
F
1
System ID All Bad ? MCC ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 64.38
Baseline (always Bad) 18.71 52.53 0.00 35.62
? FBK-UPV-UEDIN/RNN 62.00 48.73 18.23 61.62
LIMSI/RF 60.55 47.32 15.44 60.09
LIG/FS 63.55 44.47 19.41 64.67
LIG/BL ALL 63.77 44.11 19.91 65.12
FBK-UPV-UEDIN/RNN+tandem+crf 62.17 42.63 16.32 63.26
RTM-DCU/RTM-GLM 60.68 35.08 13.45 63.74
RTM-DCU/RTM-GLMd 60.24 32.89 12.98 63.97
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 82.37
Baseline (always Bad) 5.28 29.98 0.00 17.63
? RTM-DCU/RTM-GLMd 79.54 29.14 25.47 82.98
RTM-DCU/RTM-GLM 79.42 26.91 25.93 83.43
English-German
Baseline (always OK) 59.39 0.00 0.00 71.33
Baseline (always Bad) 12.78 44.57 0.00 28.67
? RTM-DCU/RTM-GLM 71.51 45.30 28.61 72.97
RTM-DCU/RTM-GLMd 68.73 36.91 21.32 71.41
German-English
Baseline (always OK) 67.82 0.00 0.00 77.60
Baseline (always Bad) 8.20 36.60 0.00 22.40
? RTM-DCU/RTM-GLM 72.41 26.13 16.08 76.14
RTM-DCU/RTM-GLMd 71.42 22.97 12.63 75.46
Table 17: Official results for the binary part of the WMT14 Quality Evaluation Task 2. The winning submissions are indicated
by a ?. All values are given as percentages.
36
marking all the words as wrong. This may either
indicate that the predictions themselves are of low
quality or the chosen evaluation approach is mis-
leading. On the other hand F
1
scores are a com-
mon measure of binary classification performance
and no averaging is performed here.
Table 18 gives the results of the Level 1
classification (OK, Fluency, Accuracy) variant
of Task 2. Here the second baseline is to
always predict Fluency errors, as this is the
most common error category in the training
data. The winning submissions of this vari-
ant are as follows: for English-Spanish it
is FBK-UPV-UEDIN/RNN+tandem+crf with a
weighted F
1
of 23.94 and for Spanish-English,
English-German, and German-English it is RTM-
DCU/RTM-GLMd with weighted F
1
scores of
23.94, 21.94, and 8.57 respectively.
As before, all systems fail to outperform the
single-class baseline for the Spanish-English lan-
guage pair according to our primary metric. How-
ever, for Spanish-English and English-German
both submissions are able to beat the baseline by
large margin. We also observe that the absolute
numbers vary greatly between language pairs.
Table 19 gives the results of the Multi-class
classification variant of Task 2. Again, the sec-
ond baseline is to always predict the most common
error category in the training data, which varies
depending on language pair and produces and in-
creasingly weak baseline as the number of classes
rises.
The winning submissions of this variant are
as follows: for English-Spanish, Spanish-English,
and English-German it is RTM-DCU/RTM-GLM
with weighted F
1
scores of 26.84, 8.75, and 15.02
respectively and and for German-English it is
RTM-DCU/RTM-GLMd with a weighted F
1
of
3.08. Not only do these systems perform above
our baselines for all but the German-English lan-
guage pair, they also outperform all other sub-
missions for English-Spanish. Remarkably, RTM-
DCU/RTM-GLM wins English-Spanish for all of
the proposed metrics by a sizeable margin.
4.5 Discussion
In what follows, we discuss the main accomplish-
ments of this year?s shared task starting from the
goals we had previously identified for it.
Investigating the effectiveness of different
quality labels
For the sentence-level tasks, the results of this
year?s shared task allow us to investigate the ef-
fectiveness of predicting translation quality using
three very different quality labels: perceived post-
editing effort on a scale of [1-3] (Task 1.1); HTER
scores (Task 1.2); and the time that a translator
takes to post-edit the translation (Task 1.3). One of
the ways one can compare the effectiveness across
all these different labels is to look at how well
the models can produce predictions that correlate
with the gold label that we have at our disposal.
A measure of correlation that does not depend
on the value of the labels is Spearman?s ranking
correlation. From this perspective, the label that
seems the most effective appears to be post-editing
time (Task 1.3), with the best system (RTM-
DCU/RTM-RR) producing a Spearman?s ? of 0.68
(English-Spanish translations, see Table 15). In
comparison, when perceived post-editing effort la-
bels are used (Task 1.1), the best systems achieve
a Spearman?s ? of 0.38 and 0.30 for English-
Spanish and Spanish-English translations, respec-
tively, and ? of 0.54 and 0.51 for English-German
and German-English, respectively (Table 11); for
HTER scores (Task 1.2) the best systems achieve
a Spearman?s ? of 0.53 for English-Spanish trans-
lations (Table 13).
This comparison across tasks seems to indicate
that, among the three labels we have proposed,
post-editing time seems to be the most learnable,
in the sense that automatic predictions can vest
match the gold labels (in this case, with respect
to the rankings they induce). A possible reason
for this is that post-editing time correlates with the
length of the source sentence whereas HTER is a
normalised measure.
Compared to the results regarding time predic-
tion in the Quality Evaluation shared task from
2013 (Bojar et al., 2013), we note that this time
all submissions were able to beat the baseline sys-
tem (compared to only 1/3 of the submissions in
2013). In addition, better handling of the data
acquisition reduced the number of outliers in this
year?s dataset allowing for numbers that are more
reliably interpretable. As an example of its in-
terpretability, consider the following: the winning
submission for the ranking variant of Task 1.3 is
RTM-DCU/RTM-RR, with a a Spearman?s ? of
0.68 and a DeltaAvg score of 17.02 (when predict-
37
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always fluency) 14.39 40.41 0.00 0.00 30.67
? FBK-UPV-UEDIN/RNN+tandem+crf 58.36 38.54 16.63 13.89 57.98
FBK-UPV-UEDIN/RNN 60.32 37.25 18.22 15.51 61.75
LIG/BL ALL 58.97 31.79 14.95 11.48 61.13
LIG/FS 58.95 31.78 14.92 11.46 61.10
RTM-DCU/RTM-GLMd 58.23 26.62 12.60 12.76 62.94
RTM-DCU/RTM-GLM 56.47 29.91 8.11 7.96 58.56
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always fluency) 2.67 15.13 0.00 0.00 12.24
? RTM-DCU/RTM-GLMd 78.89 23.94 25.41 25.45 83.17
RTM-DCU/RTM-GLM 78.78 21.96 26.31 26.99 83.69
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always fluency) 3.83 13.35 0.00 0.00 14.82
? RTM-DCU/RTM-GLMd 64.58 21.94 17.69 15.92 69.26
RTM-DCU/RTM-GLM 64.43 21.10 16.99 14.93 69.34
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always fluency) 3.34 14.92 0.00 0.00 13.79
? RTM-DCU/RTM-GLMd 69.17 8.57 10.61 5.76 75.91
RTM-DCU/RTM-GLM 69.09 8.26 9.95 5.76 75.97
Table 18: Official results for the Level 1 classification part of the WMT14 Quality Evaluation Task 2. The winning submissions
are indicated by a ?. All values are given as percentages.
38
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always unintelligible) 7.93 22.26 0.00 0.00 21.99
? RTM-DCU/RTM-GLM 60.52 26.84 23.77 21.45 66.83
FBK-UPV-UEDIN/RNN+tandem+crf 52.96 23.07 15.17 10.74 52.13
LIG/BL ALL 56.66 20.50 18.56 13.39 60.39
LIG/FS 56.66 20.50 18.56 13.39 60.39
FBK-UPV-UEDIN/RNN 52.84 17.09 7.66 4.24 57.18
RTM-DCU/RTM-GLMd 51.87 3.22 10.16 4.04 64.42
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always word order) 0.34 1.96 0.00 0.00 4.24
? RTM-DCU/RTM-GLM 76.34 8.75 19.82 13.43 83.27
RTM-DCU/RTM-GLMd 76.21 8.19 19.35 15.32 83.17
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always mistranslation) 2.48 8.66 0.00 0.00 11.78
? RTM-DCU/RTM-GLM 63.57 15.02 17.57 15.08 70.82
RTM-DCU/RTM-GLMd 63.33 12.48 18.70 13.20 71.45
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always word order) 1.56 6.96 0.00 0.00 9.23
? RTM-DCU/RTM-GLMd 67.62 3.08 7.19 1.48 74.73
RTM-DCU/RTM-GLM 67.86 2.36 7.55 1.79 75.75
Table 19: Official results for the Multi-class classification part of the WMT14 Quality Evaluation Task 2. The winning
submissions are indicated by a ?. All values are given as percentages.
39
ing seconds). This number has a direct real-world
interpretation: using the order proposed by this
system, a human translator would spend, on av-
erage, about 17 seconds less on a sentence taken
from the top of the ranking compared to a sen-
tence picked randomly from the set.
14
To put this
number into perspective, for this dataset the av-
erage time to complete a sentence post-editing is
39 seconds. As such, one has an immediate inter-
pretation for the usefulness of using such a rank-
ing: translating around 100 sentences taken from
the top of the rankings would take around 36min
(at about 22 seconds/sentence), while translating
the same number of sentences extracted randomly
from the same dataset would take around 1h5min
(at about 39 seconds/sentence). It is in this sense
that we consider post-editing time an interpretable
label.
Another desirable property of label predictions
is usefulness; this property, however, it highly
task-dependent and therefore cannot be judged in
the absence of a specific task. For instance, an in-
terpretable label like post-editing time may not be
that useful in a task the requires one to place the
machine translations into ?ready to publish? and
?not ready to publish? bins. For such an appli-
cation, labels such as the ones used by Task 1.1
are clearly more useful, and also very much inter-
pretable within the scope of the task. Our attempt
at presenting the Quality Prediction task with a va-
riety of prediction labels illustrates a good range
of properties for the proposed labels and enables
one to draw certain conclusions depending on the
needs of the specific task at hand.
For the word-level tasks, different quality labels
equate with using different levels of granularity for
the predictions, which we discuss next.
Exploring word-level quality prediction at
different levels of granularity
Previous work on word-level predictions, e.g. (Bo-
jar et al., 2013) has focused on prediction of auto-
matically derived labels, generally due to practical
considerations as the manual annotation is labour
intensive. While easily applicable, automatic an-
notations, using for example TER alignment be-
tween the machine translation and reference (or
post-edition), face the same problems as automatic
14
Note that the 17.02 seconds figure is a difference in real-
time, not predicted time; what is considered in this variant of
Task 1.3 is only the predicted ranking of data points, not the
absolute values of the predictions.
MT evaluation metrics as they fail to account for
different word choices and lack the ability to re-
liably distinguish meaning preserving reorderings
from those that change the semantics of the out-
put. Furthermore, previous automatic annotation
for word-level quality estimation has focused on
binary labels: correct / incorrect, or at most, the
main edit operations that can be captured by align-
ment metrics like TER: correct, insertion, dele-
tion, substitution.
In this year?s task we were able to provide
manual fine-grained annotations at the word-level
produced by humans irrespective of references or
post-editions. Error categories range from fre-
quent ones, such as unintelligible, mistranslation,
and terminology, to rare ones such as additions or
omissions. For example, only 10 out of more than
3,400 errors in the English-Spanish test set fall
into the latter categories, while over 2,000 words
are marked as unintelligible. By hierarchically
grouping errors into coarser categories we aimed
to find a compromise between data sparsity and
the expressiveness of the labels. What marks a
good compromise depends on the use case, which
we do not specify here, and the quality of the finer
grained predictions: if a system is able to predict
even rare errors these may be grouped later if nec-
essary.
Overall, word-level error prediction seems to re-
main a challenging task as evidenced by the fact
that many submissions were unable to beat a triv-
ial baseline. We hypothesise that this is at least
partially due to a mismatch in loss-functions used
in training and testing. We know from the sys-
tem descriptions that some systems were tuned to
optimise squared error or accuracy, while evalua-
tion was performed using weighted F
1
scores. On
the other hand, even a comparison of just accuracy
shows that systems struggle to obtain a lower error
rates than the ?all-OK? baseline.
Such performance problems are consistent over
the three levels of granularity, contrary to the in-
tuition that binary classification would be easier.
A notable exception is the RTM-DCU/RTM-GLM
system, which is able to beat both the baseline and
all other systems on the Multi-Class variant of the
English-Spanish task ? cf. Table 19 ? with regard
to all metrics. For this and most other submis-
sions we observe that labels are not consistent for
different granularities, i.e. at token marked with a
specific error in the multi-class variant may still
40
carry an ?OK? label in binary annotation. Thus,
additional coarse grained annotations may be de-
rived by automatic means. For example, mapping
the multi-class predictions of the above system to
coarser categories improves the F
1,ERR
score in
Table 17 from 35.08 to 37.02 but does not change
the rank with respect to the other entries.
The fact that coarse grained predictions seem
not to be derived from the fine-grained ones leads
us to believe that most participants treated the
different granularities as independent classifica-
tion tasks. The FBK-UPV-UEDIN team trans-
fers information in the opposite direction by using
their binary predictions as features for Level-1 and
multi-class.
Given the current quality of word-level predic-
tion it remains unclear if these systems can already
be employed in a practical setting, e.g. to focus the
attention of post-editors.
Studying the effects of training and test
datasets with mixed domains, language pairs
and MT systems
This year?s shared task made available datasets for
more than one language pair with the same or dif-
ferent types of annotation, 2-3 multiple MT sys-
tems (plus a human translation) per language pair,
and out-of-domain test data (Tasks 1.1 and 2). In-
stances for each language pair were kept in sep-
arate datasets and thus the ?language pair? vari-
able can be analysed independently. However, for
a given language pair, datasets mix translation sys-
tems (and humans) in Task 1.1, and also text do-
mains in Task 2.
Directly comparing the performance across lan-
guage pairs is not possible, given that their
datasets have different numbers of instances (pro-
duced by 3 or 4 systems) and/or different true
score distributions (see Figure 3). For a relative
comparison (although not all systems submitted
results for all language pairs, which is especially
true in Task 2), we observe in Task 1.1 that for all
language pairs generally at least half of the sys-
tems did better than the baseline. To our surprise,
only one submission combined data for multiple
languages together for Task 1.1: SHEF-lite, treat-
ing each language pair data as a different task in
a multi-task learning setting. However, only for
the ?sparse? variant of the submission significant
gains were reported over modelling each task in-
dependently (with the tasks still sharing the same
data kernel and the same hyperparameters).
The interpretation of the results for Task 2 is
very dependent on the evaluation metric used,
but generally speaking a large variation in per-
formance was found between different languages,
with English-Spanish performing the best, possi-
bly given the much larger number of training in-
stances. Data for Task 2 also presented varied true
score distributions (as shown by the performance
of the baseline (e.g. always ?OK?) in Tables 17-
19.
One of the main goals with Task 1.1 (and Task 2
to some extent) was to test the robustness of mod-
els in a blind setting where multiple MT systems
(and human translations) are put together and their
identifiers are now known. All submissions for
these tasks were therefore translation system ag-
nostic, with no submission attempting to perform
meta-identification of the origins of the transla-
tions. For Task 1.1, data from multiple MT sys-
tems was explicitly used by USHEFF though the
idea of consensus translations. Translations from
all but the system of interest for the same source
segment were used as pseudo-references. The
submission significantly outperformed the base-
line for all language pairs and did particularly well
for Spanish-English and English-Spanish.
An in depth analysis of Task 1.1?s datasets on
the difference in prediction performance between
models built and applied for individual transla-
tion systems and models built and tested for all
translations pooled together is presented in (Shah
and Specia, 2014). Not surprisingly, the former
models perform significantly better, with MAE
scores ranging between 0.35 and 0.5 for differ-
ent language pairs and MT systems, and signifi-
cantly lower scores for models trained and tested
on human translations only (MAE scores between
0.2 and 0.35 for different language pairs), against
MAE scores ranging between 0.5 and 0.65 for
models with pooled data.
For Tasks 1.2 and 1.3, two submissions included
English-Spanish data which had been produced by
yet different MT systems (SHEF-lite and DFKI).
While using these additional instances seemed at-
tractive given the small number of instances avail-
able for these tasks, it is not clear what their contri-
bution was. For example, with a reduced set of in-
stances (only 400) from the combined sets, SHEF-
lite/sparse performed significantly better than its
variant SHEF-lite.
Finally, with respect to out-of-domain (different
41
text domain and MT system) test data, for Task
1.1, none of the papers submitted included experi-
ments. (Shah and Specia, 2014) applied the mod-
els trained on pooled datasets (as explained above)
for each language pair to the out-of-domain test
sets. The results were surprisingly positive, with
average MAE score of 0.5, compared to the 0.5-
0.65 range for in-domain data (see above). Further
analysis is necessary to understand the reasons for
that.
In Task 2, the official training and test sets al-
ready include out-of-domain data because of the
very small amount of in-domain data available,
and thus is is hard to isolate the effect of this data
on the results.
Examining the effectiveness of quality
prediction methods on human translations
Datasets for Tasks 1.1 and 2 contain human trans-
lations, in addition to the automatic translations
from various MT systems. Predicting human
translation quality is an area that has been largely
unexplored. Previous work has looked into dis-
tinguishing human from machine translations (e.g.
(Gamon et al., 2005)), but this problem setting is
somehow artificial, and moreover arguably harder
to solve nowadays given the higher general qual-
ity of current MT systems (Shah and Specia,
2014). Although human translations are obviously
of higher quality in general, many segments are
translated by MT systems with the same or similar
levels of quality as human translation. This is par-
ticularly true for Task 2, since data had been pre-
viously categorised and only ?near misses? were
selected for the word-level annotation, i.e., human
and machine translations that were both nearly
perfect in this case.
While no distinction was made between human
and machine translations in our tasks, we believe
the mix of these two types of translations has had
a negative impact in prediction performance. Intu-
itively, one can expect errors in human translation
to be more subtle, and hence more difficult to cap-
ture via standard quality estimation features. For
example, an incorrect lexical choice (due to, e.g.,
ambiguity) which still fits the context and does not
make the translation ungrammatical is unlikely to
be captured. We hoped that participants would de-
sign features for this particular type of translation,
but although linguistically motivated features have
been exploited, they did not seem appropriate for
human translations.
It is interesting to mention the indirect use of
human translations by USHEFF for Tasks 1.1-1.3:
given a translation for a source segment, all other
translations for the same segment were used as
pseudo-references. Apart from when this transla-
tion was actually the human translation, the hu-
man translation was effectively used as a refer-
ence. While this reference was mixed with 2-
3 other pseudo-references (other machine transla-
tions) for the feature computations, these features
led to significant gains in performance over the
baseline features Scarton and Specia (2014).
We believe that more investigation is needed for
human translation quality prediction. Tasks ded-
icated to this type of data at both sentence- and
word-level in the next editions of this shared task
would be a possible starting point. The acquisi-
tion of such data is however much more costly, as
it is arguably hard to find examples of low quality
human translation, unless specific settings, such as
translation learner corpora, are considered.
5 Medical Translation Task
The Medical Translation Task addresses the prob-
lem of domain-specific and genre-specific ma-
chine translation. The task is split into two sub-
tasks: summary translation, focused on transla-
tion of sentences from summaries of medical ar-
ticles, and query translation, focused on transla-
tion of queries entered by users into medical infor-
mation search engines.
In general, texts of specific domains and gen-
res are characterized by the occurrence of special
vocabulary and syntactic constructions which are
rare or even absent in traditional (general-domain)
training data and therefore difficult for MT. Spe-
cific training data (containing such vocabulary and
syntactic constructions) is usually scarce or not
available at all. Medicine, however, is an exam-
ple of a domain for which in-domain training data
(both parallel and monolingual) is publicly avail-
able in amounts which allow to train a complete
SMT system or to adapt an existing one.
5.1 Task Description
In the Medical Translation Task, we provided links
to various medical-domain training resources and
asked participants to use the data to train or adapt
their systems to translate unseen test sets for both
subtasks between English and Czech (CS), Ger-
man (DE), and French (FR), in both directions.
42
The summary translation test data is domain-
specific, but otherwise can be considered as ordi-
nary sentences. On the other hand, the query trans-
lation test data is also specific for its genre (gen-
eral style) ? it contains short sequences of (more
or less) of independent terms rather than complete
and grammatical sentences, the usual target of cur-
rent MT systems.
Similarly to the standard Translation Task, the
participants of the Medical Translation Task were
allowed to use only the provided resources in the
constrained task (in addition to data allowed in
the constrained standard Translation Task), but
could exploit any additional resources in the un-
constrained task. The submissions were expected
with true letter casing and detokenized. The trans-
lation quality was measured using automatic eval-
uation metrics, manual evaluation was not per-
formed.
5.2 Test and Development Data
The test and development data sets for this task
were provided by the EU FP7 project Khres-
moi.
15
This projects develops a multi-lingual
multi-modal search and access system for biomed-
ical information and documents and its MT com-
ponent allows users to use non-English queries to
search in English documents and see summaries
of retrieved documents in their preferred language
(Czech, German, or French). The statistics of the
data sets are presented in Tables 20 and 21.
For the summary translation subtask, 1,000
and 500 sentences were provided for test devel-
opment purposes, respectively. The sentences
were randomly sampled from automatically gen-
erated summaries (extracts) of English documents
(web pages) containing medical information rel-
evant to 50 topics provided for the CLEF 2013
eHealth Task 3.
16
Out-of-domain and ungram-
matical sentences were manually removed. The
sentences were then translated by medical experts
into Czech, German and French, and the transla-
tions were reviewed. Each sentence was provided
with the corresponding document ID and topic ID.
The set also included a description for each of the
50 topics. The data package (Khresmoi Summary
Translation Test Data 1.1) is now available from
the LINDAT/CLARIN repository
17
and more de-
15
http://khresmoi.eu/
16
https://sites.google.com/site/
shareclefehealth/
17
http://hdl.handle.net/11858/
tails can be found in Zde?nka Ure?sov?a and Pecina
(2014).
For the query translation subtask, the main
test set contains 1,000 queries for test and 508
queries for development purposes. The original
English queries were extracted at random from
real user query logs provided by the Health on the
Net foundation
18
(queries by general public) and
the Trip database
19
(queries by medical experts).
Each query was translated into Czech, German,
and French by medical experts and the transla-
tions were reviewed. The data package (Khresmoi
Query Translation Test Data 1.0) is available from
the LINDAT/CLARIN repository.
20
An additional test set for the query translation
subtask was adopted from the CLEF 2013 eHealth
Task 3 (Pecina et al., 2014). It contains 50 queries
constructed from titles of the test topics (originally
in English) translated into Czech, German, and
French by medical experts. The participants were
asked to translate the queries back to English and
the resulting translations were used in an informa-
tion retrieval (IR) experiment for extrinsic evalua-
tion.
5.3 Training Data
This section reviews the in-domain resources
which were allowed for the constrained Medical
Translation Task in addition to resources for the
constrained standard Translation Task (see Section
2). Most of the corpora are available for direct
download, others can be obtained upon registra-
tion. The corpora usually employ their own, more
or less complex data format. To lower the entry
barrier, we provided a set of easy-to-use scripts to
convert the data to a plain text format suitable for
MT training.
5.3.1 Parallel Training Data
The medical-domain parallel data includes the fol-
lowing corpora (see Table 22 for statistics): The
EMEA corpus (Tiedemann, 2009) contains doc-
uments from the European Medicines Agency,
automatically processed and aligned on sentence
level. It is available for many language pairs, in-
cluding those relevant to this task. UMLS is a
multilingual metathesaurus of health and biomed-
00-097C-0000-0023-866E-1
18
http://www.hon.ch/
19
http://www.tripdatabase.com/
20
http://hdl.handle.net/11858/
00-097C-0000-0022-D9BF-5
43
sents tokens
total Czech German French English
dev 500 9,209 9,924 12,369 10,350
test 1,000 19,191 20,831 26,183 21,423
Table 20: Statistics of summary test data.
queries tokens
total general expert Czech German French English
dev 508 249 259 1,128 1,041 1,335 1,084
test 1,000 500 500 2,121 1,951 2,490 2,067
Table 21: Statistics of query test data.
L1?L2 Czech?English DE?EN FR?EN
data set sents L1 tokens L2 tokens sents L1 tokens L2 tokens sents L1 tokens L2 tokens
EMEA 1,053 13,872 14,378 1,108 13,946 14,953 1,092 17,605 14,786
UMLS 1,441 4,248 5,579 2,001 6,613 8,153 2,171 8,505 8,524
Wiki 3 5 6 10 19 22 8 19 17
MuchMore 29 688 740
PatTr 1,848 102,418 106,727 2,201 127,098 108,665
COPPA 664 49,016 39,933
Table 22: Statistics of the in-domain parallel training data allowed for the constrained task (in thousands).
data set English Czech German French
PatTR 121,592 53,242 54,608
UMLS 7,991 63 24 37
Wiki 26,945 1,784 10,232 8,376
AACT 13,341
DrugBank 953
FMA 884
GENIA 557
GREC 62
PIL 662
Table 23: Sizes of monolingual training data allowed for the
constrained tasks (in thousands of tokens).
ical vocabularies and standards (U.S. National Li-
brary of Medicine, 2009). The UMLS dataset
was constructed by selecting the concepts which
have translations in the respective languages. The
Wiki dataset contains bilingual pairs of titles of
Wikipedia articles belonging to the categories
identified to be medical-domain within the Khres-
moi project. It is available for all three lan-
guage pairs. The MuchMore Springer Corpus
is a German?English parallel corpus of medical
journals abstracts published by Springer (Buitelaar
et al., 2003). PatTR is a parallel corpus extracted
from the MAREC patent collection (W?aschle and
Riezler, 2012). It is available for German?English
and French?English. For the medical domain,
we only consider text from patents indicated to
be from the medicine-related categories (A61,
C12N, C12P). COPPA (Corpus of Parallel Patent
Applications (Pouliquen and Mazenc, 2011) is a
French?English parallel corpus extracted from the
MAREC patent collection (W?aschle and Riezler,
2012). The medical-domain subset is identified by
the same categories as in PatTR.
5.3.2 Monolingual Training Data
The medical-domain monolingual data consists of
the following corpora (statistics are presented in
Table 23): The monolingual UMLS dataset con-
tains concept descriptions in CS, DE, and FR ex-
tracted from the UMLS Metathesaurus (see Sec-
tion 5.3.1). The monolingual Wiki dataset con-
sists of articles belonging to the categories iden-
tified to be medical-domain within the Khresmoi
project. The PatTR dataset contains non-parallel
data extracted from the medical patents included
in the PatTR corpus (see Section 5.3.1). AACT is a
collection of restructured and reformatted English
texts publicly available and downloadable from
ClinicalTrials.gov, containing clinical studies con-
ducted around the world. DrugBank is a bioin-
formatics and cheminformatics resource contain-
ing drug descriptions (Knox et al., 2011). GENIA
is a corpus of biomedical literature compiled and
annotated within the GENIA project (Kim et al.,
2003). FMA stands for the Foundational Model
of Anatomy Ontology, a knowledge source for
biomedical informatics concerned with symbolic
representation of the phenotypic structure of the
human body (Rosse and Mejino Jr., 2008). GREC
(Gene Regulation Event Corpus) is a semantically
annotated English corpus of abstracts of biomedi-
cal papers (Thompson et al., 2009). The PIL cor-
pus is a collection of documents giving instruc-
tions to patients about their medication (Bouayad-
Agha et al., 2000).
5.4 Participants
A total of eight teams participated in the Medical
Translation Task by submitting their systems to at
least one subtask for one or more translation direc-
tions. A list of the participants is given in Table 24;
we provide short descriptions of their systems in
the following.
CUNI was involved in the organization of the task,
and their primary goal was to set up a baseline for
both the subtasks and for all translation directions.
44
ID Participating team
CUNI Charles University in Prague (Du?sek et al., 2014)
DCU-Q Dublin City University (Okita et al., 2014)
DCU-S Dublin City University (Zhang et al., 2014)
LIMSI Laboratoire dInformatique pour la Mecanique et les Sciences de lIng?enieur (P?echeux et al., 2014)
POSTECH Pohang University of Science and Technology (Li et al., 2014a)
UEDIN University of Edinburgh (Durrani et al., 2014a)
UM-DA University of Macau (Wang et al., 2014)
UM-WDA University of Macau (Lu et al., 2014)
Table 24: Participants in the WMT14 Medical Translation Task.
Their systems are based on the Moses phrase-
based toolkit and linear interpolation of in-domain
and out-of-domain language models and phrase ta-
bles. The constrained/unconstrained systems dif-
fer in the training data only. The constrained
ones are built using all allowed training data; the
unconstrained ones take advantage of additional
web-crawled monolingual data used for training of
the language models, and additional parallel non-
medical data from the PatTr and COPPA patent
collections.
DCU-Q submitted a system designed specifically
for terminology translation in the query translation
task for EN?FR and FR?EN. This system supports
six terminology extraction methods and is able to
detect rare word pairs including zero-appearance
word pairs. It uses monotonic decoding with lat-
tice inputs, avoiding unnecessary hypothesis ex-
pansions by the reordering model.
DCU-S submitted a system to the FR?EN sum-
mary translation subtask only. The system is
similar to DCU?s system for patent translation
(phrased-based using Moses) but adapted to trans-
late medical summaries and reports.
LIMSI took part in the summary translation sub-
task for English to French.Their primary submis-
sion uses a combination of two translation sys-
tems: NCODE, based on bilingual n-gram trans-
lation models; and an on-the-fly estimation of
the parameters of Moses along with a vector
space model to perform domain adaptation. A
continuous-space language model is also used in
a post-processing step for each system.
POSTECH submitted a phrase-based SMT sys-
tem and query translation system for the DE?EN
language pair in both subtasks. They analysed
three types of query formation, generated query
translation candidates using term-to-term dictio-
naries and a phrase-based system, and then scored
them using a co-occurrence word frequency mea-
sure to select the best candidate.
UEDIN applied the Moses phrase-based system to
all language pairs and both subtasks. They used
the hierarchical reordering model and the OSM
feature, same as in UEDIN?s news translation sys-
tem, and applied compound splitting to German
input. They used separate language models built
on in-domain and out-of-domain data with linear
interpolation. For all language pairs except CS-
EN and DE-EN, they selected data for the transla-
tion model using modified Moore-Lewis filtering.
For DE-EN and CS-EN, they concatenated all the
supplied parallel training data.
UM-DA submitted systems for all language pairs
in the summary translation subtask based on a
combination of different adaptation steps, namely
domain-specific pre-processing, language model
adaptation, translation model adaptation, numeric
adaptation, and hyphenated word adaptation. Data
for the domain-adapted language and translation
models were selected using various data selection
techniques.
UM-WDA submitted systems for all language
pairs in the summary translation subtask. Their
systems are domain-adapted using web-crawled
in-domain resources: bilingual dictionaries and
monolingual data. The translation model and lan-
guage model trained on the crawled data were in-
terpolated with the best-performing language and
translation model employed in the UM-DA sys-
tems.
5.5 Results
MT quality in the Medical Translation Task
is evaluated using automatic evaluation metrics:
BLEU (Papineni et al., 2002), TER (Snover et al.,
2006), PER (Tillmann et al., 1997), and CDER
(Leusch et al., 2006). BLEU scores are reported as
percentage and all error rates are reported as one
minus the original value, also as percentage, so
that all metrics are in the 0-100 range, and higher
scores indicate better translations.
The main reason for not conducting human
evaluation, as it happens in the standard Trans-
45
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 29.64 29.79
?
1.07 47.45
?
1.15 61.64
?
1.06 52.18
?
0.98 31.68
?
1.14 49.84
?
1.10 64.38
?
1.06 54.10
?
0.96
CUNI 22.44 22.57
?
0.95 41.43
?
1.16 55.46
?
1.09 46.42
?
0.96 32.34
?
1.12 50.24
?
1.20 65.07
?
1.10 54.42
?
0.96
UEDIN 36.65 36.87
?
1.23 54.35
?
1.19 67.16
?
1.00 57.61
?
1.01 38.02
?
1.24 56.14
?
1.17 69.24
?
1.01 58.96
?
0.96
UM-DA 37.62 37.79
?
1.26 54.55
?
1.20 68.29
?
0.88 57.28
?
1.03 38.81
?
1.28 56.04
?
1.20 70.06
?
0.82 58.45
?
1.05
CUNI 22.92 23.06
?
0.97 42.49
?
1.10 56.10
?
1.12 47.13
?
0.95 33.18
?
1.15 51.48
?
1.15 66.00
?
1.03 55.30
?
0.96
CUNI 22.69 22.84
?
0.98 42.21
?
1.14 56.01
?
1.11 46.79
?
0.94 32.84
?
1.13 51.10
?
1.11 65.79
?
1.07 54.81
?
0.96
UM-WDA 37.35 37.53
?
1.26 54.39
?
1.19 68.21
?
0.83 57.16
?
1.07 38.61
?
1.27 55.92
?
1.17 70.02
?
0.81 58.36
?
1.07
ONLINE 39.57
?
1.21 58.24
?
1.14 70.16
?
0.78 60.04
?
1.02 40.62
?
1.23 59.72
?
1.11 71.94
?
0.74 61.26
?
1.01
German?English
CUNI 28.20 28.34
?
1.12 46.66
?
1.13 61.53
?
1.03 50.57
?
0.93 30.69
?
1.19 48.91
?
1.16 64.12
?
1.04 52.52
?
0.95
CUNI 28.85 28.99
?
1.15 47.12
?
1.15 61.98
?
1.07 50.72
?
0.98 31.37
?
1.21 49.29
?
1.13 64.53
?
1.05 52.64
?
0.98
POSTECH 25.92 25.99
?
1.06 43.66
?
1.14 59.62
?
0.92 47.13
?
0.90 26.97
?
1.06 45.13
?
1.12 61.53
?
0.89 48.37
?
0.88
UEDIN 37.31 37.53
?
1.19 55.72
?
1.14 68.82
?
0.99 58.35
?
0.95 38.60
?
1.25 57.18
?
1.12 70.46
?
0.98 59.53
?
0.94
UM-DA 35.71 35.81
?
1.23 53.08
?
1.16 66.82
?
0.98 55.91
?
0.96 36.55
?
1.27 54.01
?
1.13 68.05
?
0.97 56.78
?
0.95
CUNI 30.58 30.71
?
1.10 48.68
?
1.09 63.19
?
1.08 52.72
?
0.94 33.14
?
1.19 50.98
?
1.06 65.88
?
1.04 54.74
?
0.94
CUNI 30.22 30.32
?
1.12 47.71
?
1.18 62.20
?
1.10 52.17
?
0.91 32.75
?
1.20 50.00
?
1.14 64.87
?
1.06 54.19
?
0.92
UM-WDA 32.70 32.88
?
1.19 49.60
?
1.18 63.74
?
1.01 53.50
?
0.96 33.95
?
1.23 51.05
?
1.19 65.54
?
0.98 54.73
?
0.96
ONLINE 41.18
?
1.24 59.33
?
1.09 70.95
?
0.92 61.92
?
1.01 42.29
?
1.23 60.76
?
1.08 72.51
?
0.88 63.06
?
0.96
French?English
CUNI 34.42 34.55
?
1.20 52.24
?
1.17 64.52
?
1.03 56.48
?
0.91 36.52
?
1.23 54.35
?
1.12 67.07
?
1.00 58.34
?
0.91
CUNI 33.67 33.59
?
1.16 50.39
?
1.23 61.75
?
1.16 56.74
?
0.97 35.55
?
1.21 52.55
?
1.26 64.45
?
1.13 58.63
?
0.91
DCU-B 44.85 45.01
?
1.24 62.57
?
1.12 74.11
?
0.78 64.33
?
0.99 46.12
?
1.26 64.04
?
1.06 75.84
?
0.74 65.55
?
0.94
UEDIN 46.44 46.68
?
1.26 64.12
?
1.16 74.47
?
0.87 66.40
?
0.96 48.01
?
1.29 65.70
?
1.15 76.30
?
0.86 67.76
?
0.91
UM-DA 47.08 47.22
?
1.33 64.08
?
1.16 75.41
?
0.88 66.15
?
0.96 48.23
?
1.31 65.36
?
1.10 76.95
?
0.89 67.18
?
0.93
CUNI 34.74 34.89
?
1.12 52.39
?
1.16 63.76
?
1.09 57.29
?
0.94 36.84
?
1.17 54.56
?
1.13 66.43
?
1.07 59.14
?
0.90
CUNI 35.04 34.99
?
1.18 52.11
?
1.24 63.24
?
1.09 57.51
?
0.97 37.04
?
1.18 54.38
?
1.17 66.02
?
1.05 59.55
?
0.93
UM-WDA 43.84 44.06
?
1.32 61.14
?
1.18 73.13
?
0.87 63.09
?
1.00 45.17
?
1.36 62.63
?
1.15 74.94
?
0.84 64.37
?
0.99
ONLINE 46.99
?
1.35 64.31
?
1.12 76.07
?
0.78 66.09
?
1.00 47.99
?
1.33 65.65
?
1.07 77.65
?
0.75 67.20
?
0.96
English?Czech
CUNI 17.36 17.65
?
0.96 37.17
?
1.02 49.13
?
0.98 40.31
?
0.95 18.75
?
0.96 38.32
?
1.02 50.82
?
0.91 41.39
?
0.94
CUNI 16.64 16.89
?
0.93 36.57
?
1.05 48.79
?
0.98 39.46
?
0.90 17.94
?
0.96 37.74
?
1.03 50.50
?
0.97 40.59
?
0.91
UEDIN 23.45 23.74
?
1.00 44.20
?
1.10 55.38
?
0.88 46.23
?
0.99 24.20
?
1.00 44.92
?
1.08 56.38
?
0.90 46.78
?
1.00
UM-DA 22.61 22.72
?
0.98 42.73
?
1.16 54.12
?
0.93 44.73
?
1.01 23.12
?
1.01 43.41
?
1.14 55.11
?
0.93 45.32
?
1.02
CUNI 20.56 20.84
?
1.01 39.98
?
1.09 51.98
?
0.99 42.86
?
1.00 22.03
?
1.05 41.19
?
1.08 53.66
?
0.97 43.93
?
1.01
CUNI 19.50 19.72
?
0.97 38.09
?
1.10 50.12
?
1.06 41.50
?
0.96 20.91
?
1.02 39.26
?
1.12 51.79
?
1.04 42.59
?
0.96
UM-WDA 22.14 22.33
?
0.96 42.30
?
1.11 53.89
?
0.92 44.48
?
1.01 22.72
?
0.97 43.02
?
1.09 54.89
?
0.95 45.08
?
0.99
ONLINE 33.45
?
1.28 51.64
?
1.28 61.82
?
1.10 53.97
?
1.18 34.02
?
1.31 52.35
?
1.22 62.84
?
1.08 54.52
?
1.18
English?German
CUNI 12.52 12.64
?
0.77 29.84
?
0.99 45.38
?
1.14 34.69
?
0.81 16.63
?
0.91 33.63
?
1.07 50.03
?
1.24 38.43
?
0.87
CUNI 12.42 12.53
?
0.77 29.02
?
1.05 44.27
?
1.16 34.62
?
0.78 16.41
?
0.91 32.87
?
1.08 48.99
?
1.21 38.37
?
0.86
POSTECH 15.46 15.59
?
0.91 34.41
?
1.01 49.00
?
0.83 37.11
?
0.90 15.98
?
0.92 34.98
?
1.00 49.94
?
0.81 37.60
?
0.87
UEDIN 20.88 21.01
?
1.03 40.03
?
1.08 55.54
?
0.91 42.95
?
0.90 21.40
?
1.03 40.55
?
1.08 56.33
?
0.92 43.41
?
0.90
UM-DA 20.89 21.09
?
1.07 40.76
?
1.03 55.45
?
0.89 43.02
?
0.93 21.52
?
1.08 41.31
?
1.01 56.38
?
0.90 43.58
?
0.91
CUNI 14.29 14.42
?
0.81 31.82
?
1.03 47.01
?
1.13 36.81
?
0.79 18.87
?
0.90 35.76
?
1.11 51.76
?
1.17 40.65
?
0.87
CUNI 13.44 13.58
?
0.75 30.37
?
1.03 45.80
?
1.14 35.80
?
0.76 17.84
?
0.89 34.41
?
1.13 50.75
?
1.18 39.85
?
0.78
UM-WDA 18.77 18.91
?
1.00 37.92
?
1.02 53.59
?
0.85 40.90
?
0.86 19.30
?
1.02 38.42
?
1.01 54.40
?
0.85 41.34
?
0.86
ONLINE 23.92
?
1.06 44.33
?
0.97 57.47
?
0.80 46.35
?
0.91 24.29
?
1.07 44.83
?
0.98 58.20
?
0.80 46.71
?
0.92
English?French
CUNI 30.30 30.67
?
1.11 46.59
?
1.09 59.83
?
1.04 50.51
?
0.93 32.06
?
1.12 48.01
?
1.09 61.66
?
1.00 51.83
?
0.94
CUNI 29.35 29.71
?
1.10 45.84
?
1.07 58.81
?
1.04 50.00
?
0.96 31.02
?
1.10 47.24
?
1.09 60.57
?
1.02 51.31
?
0.94
LIMSI 40.14 43.54
?
1.22 59.70
?
1.04 69.45
?
0.86 61.35
?
0.96 44.04
?
1.22 60.32
?
1.03 70.20
?
0.85 61.90
?
0.94
LIMSI 38.83 42.21
?
1.13 58.88
?
1.01 68.70
?
0.81 60.59
?
0.93 42.69
?
1.12 59.53
?
0.98 69.50
?
0.80 61.17
?
0.91
UEDIN 40.74 44.24
?
1.16 60.66
?
1.07 70.35
?
0.82 62.28
?
0.95 44.85
?
1.17 61.43
?
1.05 71.27
?
0.81 62.94
?
0.91
UM-DA 41.24 41.68
?
1.12 58.72
?
1.06 69.37
?
0.78 60.12
?
0.95 42.16
?
1.11 59.39
?
1.05 70.21
?
0.77 60.71
?
0.92
CUNI 32.23 32.61
?
1.09 48.48
?
1.08 61.13
?
1.01 52.24
?
0.93 34.08
?
1.10 49.93
?
1.11 62.92
?
0.99 53.65
?
0.92
CUNI 32.45 32.84
?
1.06 48.68
?
1.06 61.32
?
0.98 52.35
?
0.94 34.22
?
1.07 50.09
?
1.04 63.04
?
0.96 53.67
?
0.91
UM-WDA 40.78 41.16
?
1.13 58.20
?
0.99 68.93
?
0.84 59.64
?
0.94 41.79
?
1.12 59.10
?
0.96 70.01
?
0.84 60.39
?
0.91
ONLINE 58.63
?
1.26 70.70
?
1.12 78.22
?
0.81 71.89
?
0.96 59.27
?
1.26 71.50
?
1.10 79.16
?
0.81 72.63
?
0.94
Table 25: Official results of translation quality evaluation in the medical summary translation subtask.
46
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 10.71 10.57
?
3.42 15.72
?
2.77 23.37
?
3.03 18.68
?
2.42 30.13
?
4.85 53.38
?
3.01 62.53
?
2.84 55.44
?
2.87
CUNI 9.92 9.78
?
3.04 16.84
?
2.84 23.80
?
3.08 19.85
?
2.40 28.21
?
4.56 54.15
?
3.04 62.56
?
2.99 55.91
?
2.79
UEDIN 24.66 24.68
?
4.52 39.88
?
3.05 49.97
?
3.29 41.81
?
2.80 28.25
?
4.94 45.31
?
3.14 55.66
?
3.06 46.67
?
2.77
CUNI 12.00 11.86
?
3.42 18.49
?
2.74 24.67
?
2.85 21.08
?
2.29 31.91
?
4.81 57.61
?
3.13 65.02
?
2.99 59.24
?
2.69
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
ONLINE 28.88
?
4.96 47.31
?
3.35 55.19
?
3.21 49.88
?
2.89 35.33
?
5.20 55.80
?
3.20 64.05
?
2.97 57.94
?
2.85
German?English
CUNI 10.90 10.74
?
3.41 18.89
?
2.39 26.09
?
2.00 20.29
?
2.07 32.15
?
5.23 55.56
?
2.90 63.68
?
2.34 56.45
?
2.62
CUNI 10.71 10.55
?
3.47 18.40
?
2.35 25.45
?
2.04 19.84
?
2.07 32.06
?
5.19 54.85
?
2.91 62.87
?
2.39 55.52
?
2.61
POSTECH 18.06 17.97
?
4.38 28.57
?
3.30 40.38
?
2.77 31.79
?
2.80 21.99
?
4.65 35.76
?
3.35 47.84
?
2.82 38.84
?
2.92
POSTECH 17.99 17.88
?
4.72 29.79
?
3.04 41.15
?
2.48 32.49
?
2.63 24.41
?
4.83 41.72
?
3.19 53.33
?
2.55 44.06
?
2.88
UEDIN 23.33 23.39
?
4.37 38.55
?
3.65 48.21
?
3.43 40.75
?
3.05 27.17
?
4.63 43.87
?
3.52 53.76
?
3.48 45.72
?
3.03
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
CUNI 8.75 8.49
?
3.60 19.10
?
2.27 24.98
?
1.95 19.95
?
2.02 30.00
?
5.59 56.07
?
2.92 62.92
?
2.32 56.27
?
2.56
ONLINE 19.97
?
4.46 37.03
?
3.26 43.91
?
3.22 40.95
?
2.93 33.86
?
4.87 53.28
?
3.28 60.86
?
3.22 56.33
?
2.98
French?English
CUNI 13.90 13.79
?
3.61 18.49
?
2.55 28.35
?
2.81 20.36
?
2.20 34.97
?
5.34 59.54
?
2.94 72.30
?
2.63 58.86
?
2.76
CUNI 12.10 11.95
?
3.41 17.23
?
2.57 27.12
?
2.88 19.15
?
2.28 33.74
?
5.01 58.95
?
2.96 71.25
?
2.76 58.20
?
2.81
DCU-Q 30.85 31.24
?
5.08 58.88
?
2.97 67.94
?
2.62 59.19
?
2.62 36.88
?
5.07 66.38
?
2.85 75.86
?
2.37 66.29
?
2.55
DCU-Q 26.51 26.16
?
4.40 48.02
?
3.72 57.34
?
3.24 53.56
?
2.79 28.61
?
4.52 53.65
?
3.73 63.51
?
3.21 59.07
?
2.79
UEDIN 27.20 27.60
?
3.98 38.54
?
3.22 48.81
?
3.26 39.77
?
2.95 32.23
?
4.27 43.66
?
3.20 54.31
?
3.17 44.53
?
2.79
CUNI 14.03 14.00
?
3.30 20.11
?
2.38 29.00
?
2.71 21.62
?
2.22 38.98
?
5.08 62.90
?
2.87 74.49
?
2.45 62.12
?
2.64
CUNI 13.38 13.16
?
3.52 17.79
?
2.56 28.84
?
2.81 19.17
?
2.23 35.00
?
5.20 59.52
?
2.98 73.08
?
2.57 58.41
?
2.68
ONLINE 32.96
?
5.04 53.68
?
3.21 64.27
?
2.80 54.40
?
2.66 38.09
?
5.52 61.44
?
3.08 72.59
?
2.61 61.60
?
2.78
English?Czech
CUNI 8.37 8.00
?
3.65 17.74
?
2.23 26.46
?
1.96 19.48
?
2.10 19.49
?
4.60 41.53
?
2.94 51.34
?
2.51 42.54
?
2.74
CUNI 9.04 8.75
?
3.64 18.25
?
2.27 26.97
?
1.92 19.69
?
2.11 21.46
?
5.05 42.36
?
3.09 51.99
?
2.40 43.18
?
2.68
UEDIN 12.57 12.40
?
3.61 21.15
?
2.96 33.56
?
2.80 22.30
?
2.67 14.06
?
3.80 24.92
?
2.90 37.85
?
2.72 25.58
?
2.70
UEDIN 6.64 6.21
?
4.73 -2.35
?
3.06 5.95
?
3.48 -0.97
?
3.12 14.35
?
3.52 14.51
?
3.19 24.96
?
3.50 15.11
?
3.10
CUNI 9.06 8.64
?
3.82 19.92
?
2.24 26.97
?
1.94 20.82
?
2.06 22.42
?
5.24 44.89
?
2.94 52.89
?
2.40 45.36
?
2.78
CUNI 8.49 8.01
?
6.05 18.13
?
2.28 25.19
?
1.86 19.19
?
2.01 21.04
?
4.80 42.66
?
2.87 50.34
?
2.47 43.30
?
2.74
ONLINE 21.09
?
4.60 48.56
?
2.82 54.72
?
2.51 48.30
?
2.83 24.37
?
4.80 51.93
?
2.74 58.10
?
2.50 51.62
?
2.80
English?German
CUNI 10.17 10.01
?
3.92 26.48
?
3.24 36.71
?
3.37 29.26
?
2.96 13.02
?
4.17 31.96
?
3.41 42.39
?
3.21 34.61
?
2.95
CUNI 9.98 9.69
?
3.94 26.16
?
3.19 35.50
?
3.23 28.86
?
2.94 12.90
?
4.28 31.75
?
3.33 41.24
?
3.21 34.38
?
3.05
POSTECH 13.43 13.01
?
5.91 26.38
?
3.09 35.75
?
3.16 27.86
?
2.82 15.05
?
5.71 30.45
?
3.10 39.89
?
3.14 31.79
?
3.00
POSTECH 13.41 13.15
?
5.21 22.18
?
3.09 30.89
?
3.31 24.17
?
3.06 14.96
?
5.15 26.13
?
3.19 34.92
?
3.40 27.98
?
3.12
UEDIN 10.45 10.14
?
3.86 23.44
?
3.43 34.55
?
3.34 25.46
?
3.17 11.91
?
4.42 27.91
?
3.45 39.08
?
3.42 29.63
?
3.31
CUNI 8.91 7.72
?
6.48 30.05
?
3.22 40.65
?
2.71 31.91
?
2.88 13.66
?
5.37 35.51
?
3.28 46.12
?
2.74 37.27
?
3.01
CUNI 9.14 8.69
?
6.44 27.66
?
3.31 37.95
?
3.45 31.00
?
2.82 14.03
?
5.92 33.53
?
3.45 44.03
?
3.53 36.73
?
3.00
ONLINE 20.07
?
6.06 41.07
?
3.23 47.41
?
2.86 41.61
?
3.02 21.67
?
6.23 43.78
?
3.23 50.18
?
2.95 44.26
?
3.06
English?French
CUNI 13.12 12.92
?
2.84 21.95
?
2.41 33.19
?
2.09 23.70
?
2.24 28.42
?
3.98 51.43
?
2.90 63.74
?
2.35 52.64
?
2.58
CUNI 12.80 12.65
?
2.81 19.16
?
2.61 31.61
?
2.21 21.91
?
2.32 27.52
?
4.05 47.47
?
3.08 61.43
?
2.37 49.82
?
2.72
DCU-Q 27.69 27.84
?
4.11 48.97
?
3.06 60.90
?
2.55 51.84
?
2.83 28.98
?
4.16 51.73
?
3.10 63.84
?
2.47 54.43
?
2.76
UEDIN 20.16 21.76
?
3.42 31.66
?
4.23 44.37
?
4.13 44.29
?
2.73 23.25
?
3.49 35.38
?
4.19 48.52
?
4.07 47.94
?
2.75
CUNI 13.78 13.57
?
3.00 21.92
?
2.51 33.47
?
2.03 24.16
?
2.32 30.07
?
4.10 51.12
?
3.08 63.61
?
2.45 52.96
?
2.67
CUNI 15.27 15.24
?
3.12 23.58
?
2.54 34.39
?
2.54 25.79
?
2.32 31.40
?
4.15 53.60
?
2.96 65.39
?
2.57 55.47
?
2.69
ONLINE 28.93
?
3.66 49.20
?
3.08 60.85
?
2.69 51.68
?
2.78 30.88
?
3.66 52.25
?
3.08 64.06
?
2.62 54.59
?
2.68
Table 26: Official results of translation quality evaluation in the medical query translation subtask.
source lang. ID P@5 P@10 NDCG@5 NDCG@10 MAP Rprec bpref rel
Czech?English CUNI 0.3280 0.3340 0.2873 0.2936 0.2217 0.2362 0.3473 1461
German?English CUNI 0.2800 0.3000 0.2467 0.2630 0.2057 0.2077 0.3310 1426
French?English CUNI 0.3280 0.3380 0.2811 0.2882 0.2206 0.2284 0.3504 1481
DCU-Q 0.3480 0.3460 0.3060 0.3072 0.2252 0.2358 0.3659 1524
UEDIN 0.4440 0.4300 0.3793 0.3826 0.2843 0.2935 0.3936 1544
English (monolingual) 0.4600 0.4700 0.4091 0.4205 0.3035 0.3198 0.3858 1638
Table 27: Official results of retrieval evaluation in the query translation subtask.
47
lation Task, was the lack of domain expertise of
prospective raters. While in the standard task, the
only requirement for the raters was to be a na-
tive speaker of the target language, in the Med-
ical Translation Task, a very good knowledge of
the domain would be necessary to provide reli-
able judgements and the raters with such an ex-
pertise (medical doctors and native speakers) were
not available.
The complete results of the task are presented
in Table 25 (for summary translation) and Ta-
bles 26 and 27 (for query translation). Partici-
pant IDs given in bold indicate primary submis-
sions, IDs in normal font refer to contrastive sub-
missions. The first section for each translation di-
rection (white background) refers to constrained
submissions and the second one (light-gray back-
ground) to unconstrained submissions. The col-
umn denoted as ?original? contains BLEU scores
as reported by the Matrix submission system ob-
tained on the original submitted translations. Due
to punctuation inconsistency in the original refer-
ence translations, we decided to perform punctu-
ation normalization before calculating the official
scores. The columns denoted as ?normalized true-
cased? contain scores obtained on the submitted
translations after punctuation normalization and
the columns denoted as ?normalized lowercased?
contain scores obtained after punctuation normal-
ization and lowercasing. The normalization script
is available in the package with summary transla-
tion test data. The confidence intervals were ob-
tained by bootstrap resampling with a confidence
level of 95%. Figures in bold denote the best con-
strained system and, if its score is higher, the best
unconstrained system for each translation direc-
tion and each metric. For comparison, we also
present results of a major on-line translation sys-
tem (denoted as ONLINE).
The results of the extrinsic evaluation of query
translation submissions are given in 27. We used
the CLEF 2013 eHealth Task 3 test collection con-
taining about 1 million web pages (in English),
50 test queries (originally in English and trans-
lated to Czech, German, and French), and their
relevance assessments. Some of the participants
of the WMT Medical Task (three teams with five
submissions in total) submitted translations of the
queries (from Czech, German, and French) into
English and these translations were used to query
the CLEF 2013 eHealth Task 3 test collection us-
ing a state-of-the-art system based on a BM25
model, described in Pecina et al. (2014). Origi-
nally, we asked for 10 best translations for each
query, but only the best one were used for the
evaluation. The results are provided in terms of
standard IR evaluation measures: precision at a
cut-off of 5 and 10 documents (P@5, P@10),
normalized discounted cumulative gain (J?arvelin
and Kek?al?ainen, 2002) at 5 and 10 documents
(NDCG@5, NDCG@10), mean average precision
(MAP) (Voorhees and Harman, 2005), precision
reached after R documents retrieved, where R in-
dicates the number of the relevant documents for
each query in the entire collection (Rprec), binary
preference (bpref) (Buckley and Voorhees, 2004),
and number or relevant documents retrieved (rel).
The cross-lingual results are also compared with
the monolingual one (obtained by using the refer-
ence (English) translations of the test topics) to see
how the system would perform if the queries were
translated perfectly.
5.6 Discussion and Conclusion
Both the subtasks turned out to be quite challeng-
ing not only because of the specific domain ? in
summary sentences, we can observe much higher
density of terminology than in ordinary sentences;
the queries, which are also rich in terminology, do
not form sentences at all.
Most submissions were based on systems par-
ticipating in the standard Translation Task and
trained on the provided data or its subsets CUNI
provided baseline systems for all language pairs in
both subtasks, which turned to be relatively strong
for the query translation task, especially in trans-
lation to English, but only in terms of scores ob-
tained on normalized and lowercased translations
since their truecasing component did not perform
well.
In the summary translation subtask, the best
overall results were achieved by the UEDIN team
which won for DE?EN, EN?CS, and EN?FR, fol-
lowed by the UM-DA team, which performed on
par with UEDIN in all other translation.
The unconstrained submissions in almost all
cases did not outperform the results of the con-
strained submissions. Some improvements were
observed in the query translations subtasks by the
CUNI?s unconstrained system with language mod-
els trained on larger in-domain data.
The ONLINE system outperforms all other sub-
48
missions with only two exceptions ? the UM-DA?s
and UEDIN?s systems for the summary translation
in the FR?EN direction, though the score differ-
ences are within the 95% confidence interval.
In the query translation subtask, DCU-Q built
a system designed specifically for terminology
translation between French and English and out-
performed all other participants in translation into
English; however, the confidence intervals in the
query translation task are much wider and most of
the differences in scores of the automatic metrics
are not statistically significant.
The extrinsic evaluation in the cross-lingual in-
formation retrieval was conducted for translations
into English only. CUNI provided the baselines
for all directions, but other submissions were done
for FR?EN only. Here, the winner is UEDIN, who
outperformed both CUNI and DCU-Q, and their
scores are very close to those obtained using the
reference English translations.
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Yandex.
We would also like to thank our colleagues Ma-
tou?s Mach?a?cek and Martin Popel for detailed dis-
cussions.
References
Avramidis, E. (2014). Efforts on machine learning
over human-mediated translation edit rate. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Beck, D., Shah, K., and Specia, L. (2014). Shef-
lite 2.0: Sparse multi-task gaussian processes
for translation quality estimation. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Bic?ici, E. (2013). Referential translation machines
for quality estimation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, Sofia, Bulgaria.
Bic?ici, E., Liu, Q., and Way, A. (2014). Parallel
FDA5 for fast deployment of accurate statisti-
cal machine translation systems. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, USA. Association
for Computational Linguistics.
Bicici, E., Liu, Q., and Way, A. (2014). Parallel
fda5 for fast deployment of accurate statistical
machine translation systems. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bicici, E. and Way, A. (2014). Referential transla-
tion machines for predicting translation quality.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Bojar, O., Buck, C., Callison-Burch, C., Feder-
mann, C., Haddow, B., Koehn, P., Monz, C.,
Post, M., Soricut, R., and Specia, L. (2013).
Findings of the 2013 Workshop on Statistical
Machine Translation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 1?42, Sofia, Bulgaria. Association
for Computational Linguistics.
Bojar, O., Diatka, V., Rychl?y, P., Stra?n?ak, P.,
Tamchyna, A., and Zeman, D. (2014). Hindi-
English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth Inter-
national Language Resources and Evaluation
Conference, Reykjavik, Iceland. ELRA.
Bojar, O., Ercegov?cevi?c, M., Popel, M., and
Zaidan, O. (2011). A grain of salt for the WMT
manual evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation,
pages 1?11, Edinburgh, Scotland. Association
for Computational Linguistics.
Borisov, A. and Galinskaya, I. (2014). Yandex
school of data analysis russian-english machine
translation system for wmt14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bouayad-Agha, N., Scott, D. R., and Power, R.
(2000). Integrating content and style in doc-
uments: A case study of patient information
leaflets. Information Design Journal, 9(2?
3):161?176.
Buckley, C. and Voorhees, E. M. (2004). Re-
trieval evaluation with incomplete information.
49
In Proceedings of the 27th Annual International
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 25?
32, Sheffield, United Kingdom.
Buitelaar, P., Sacaleanu, B.,
?
Spela Vintar, Stef-
fen, D., Volk, M., Dejean, H., Gaussier, E.,
Widdows, D., Weiser, O., and Frederking, R.
(2003). Multilingual concept hierarchies for
medical information organization and retrieval.
Public deliverable, MuchMore project.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Gonz?alez-Rubio, J.,
Buck, C., Turchi, M., and Negri, M. (2014).
Fbk-upv-uedin participation in the wmt14 qual-
ity estimation shared-task. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Cap, F., Weller, M., Ramm, A., and Fraser, A.
(2014). Cims ? the cis and ims joint submis-
sion to wmt 2014 translating from english into
german. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling an-
notator bias with multi-task gaussian processes:
An application to machine translation quality
estimation. In Proceedings of the 51st An-
nual Meeting of the Association for Compu-
tational Linguistics, ACL-2013, pages 32?42,
Sofia, Bulgaria.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Costa-juss`a, M. R., Gupta, P., Rosso, P., and
Banchs, R. E. (2014). English-to-hindi sys-
tem description for wmt 2014: Deep source-
context features for moses. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Do, Q. K., Herrmann, T., Niehues, J., Allauzen,
A., Yvon, F., and Waibel, A. (2014). The
kit-limsi translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Dungarwal, P., Chatterjee, R., Mishra, A.,
Kunchukuttan, A., Shah, R., and Bhattacharyya,
P. (2014). The iit bombay hindi-english transla-
tion system at wmt 2014. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
50
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014a). Edinburgh?s phrase-based machine
translation systems for wmt-14. In Proceedings
of the ACL 2014 Ninth Workshop of Statistical
Machine Translation, Baltimore, USA.
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014b). Edinburghs phrase-based machine
translation systems for wmt-14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Du?sek, O., Haji?c, J., Hlav?a?cov?a, J., Nov?ak, M.,
Pecina, P., Rosa, R., Tamchyna, A., Ure?sov?a,
Z., and Zeman, D. (2014). Machine transla-
tion of medical texts in the khresmoi project. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Foster, J. (2007). Treebanks gone bad: Parser eval-
uation and retraining using a treebank of un-
grammatical sentences. International Journal
on Document Analysis and Recognition, 10(3-
4):129?145.
Freitag, M., Peitz, S., Wuebker, J., Ney, H., Huck,
M., Sennrich, R., Durrani, N., Nadejde, M.,
Williams, P., Koehn, P., Herrmann, T., Cho,
E., and Waibel, A. (2014). Eu-bridge mt:
Combined machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Gamon, M., Aue, A., and Smets, M. (2005).
Sentence-level MT evaluation without reference
translations: beyond language modeling. In
Proceedings of the Annual Conference of the
European Association for Machine Translation,
Budapest.
Geurts, P., Ernst, D., and Wehenkel, L. (2006). Ex-
tremely randomized trees. Machine Learning,
63(1):3?42.
Green, S., Cer, D., and Manning, C. (2014).
Phrasal: A toolkit for new directions in statis-
tical machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hardmeier, C., Stymne, S., Tiedemann, J., Smith,
A., and Nivre, J. (2014). Anaphora models and
reordering for phrase-based smt. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Herbrich, R., Minka, T., and Graepel, T. (2006).
TrueSkill
TM
: A Bayesian Skill Rating Sys-
tem. In Proceedings of the Twentieth Annual
Conference on Neural Information Processing
Systems, pages 569?576, Vancouver, British
Columbia, Canada. MIT Press.
Herrmann, T., Mediani, M., Cho, E., Ha, T.-L.,
Niehues, J., Slawik, I., Zhang, Y., and Waibel,
A. (2014). The karlsruhe institute of technol-
ogy translation systems for the wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Hokamp, C., Calixto, I., Wagner, J., and Zhang,
J. (2014). Target-centric features for transla-
tion quality estimation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hopkins, M. and May, J. (2013). Models of trans-
lation competitions. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1416?1424, Sofia, Bulgaria.
J?arvelin, K. and Kek?al?ainen, J. (2002). Cumu-
lated gain-based evaluation of ir techniques.
ACM Transactions on Information Systems,
20(4):422?446.
Kim, J.-D., Ohta, T., Tateisi, Y., and Tsujii, J.
(2003). GENIA corpus ? a semantically anno-
tated corpus for bio-textmining. Bioinformatics,
19(suppl 1):i180?i182.
Knox, C., Law, V., Jewison, T., Liu, P., Ly,
S., Frolkis, A., Pon, A., Banco, K., Mak, C.,
Neveu, V., Djoumbou, Y., Eisner, R., Guo,
A. C., and Wishart, D. S. (2011). DrugBank 3.0:
a comprehensive resource for Omics research
on drugs. Nucleic acids research, 39(suppl
1):D1035?D1041.
Koehn, P. (2012a). Simulating human judgment in
51
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. (2012b). Simulating Human Judgment
in Machine Translation Evaluation Campaigns.
In Proceedings of the Ninth International Work-
shop on Spoken Language Translation, pages
179?184, Hong Kong, China.
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Koppel, M. and Ordan, N. (2011). Translationese
and its dialects. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Techolo-
gies, pages 1318?1326, Portland, Oregon.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Leusch, G., Ueffing, N., and Ney, H. (2006). Cder:
Efficient mt evaluation using block movements.
In Proceedings of the 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 241?248, Trento,
Italy.
Li, J., Kim, S.-J., Na, H., and Lee, J.-H. (2014a).
Postech?s system description for medical text
translation task. In Proceedings of the ACL
2014 Ninth Workshop of Statistical Machine
Translation, Baltimore, USA.
Li, L., Wu, X., Vaillo, S. C., Xie, J., Way, A., and
Liu, Q. (2014b). The dcu-ictcas mt system at
wmt 2014 on german-english translation task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Lopez, A. (2012). Putting Human Assessments of
Machine Translation Systems in Order. In Pro-
ceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 1?9, Montr?eal,
Canada. Association for Computational Lin-
guistics.
Lu, Y., Wang, L., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira, F. (2014). Domain adapta-
tion for medical text translation using web re-
sources. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
Luong, N. Q., Besacier, L., and Lecouteux, B.
(2014). Lig system for word level qe task at
wmt14. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Mach?a?cek, M. and Bojar, O. (2014). Results of
the wmt14 metrics shared task. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Matthews, A., Ammar, W., Bhatia, A., Feely, W.,
Hanneman, G., Schlinger, E., Swayamdipta, S.,
Tsvetkov, Y., Lavie, A., and Dyer, C. (2014).
The cmu machine translation systems at wmt
2014. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Neidert, J., Schuster, S., Green, S., Heafield, K.,
and Manning, C. (2014). Stanford universitys
submissions to the wmt 2014 translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Okita, T., Vahid, A. H., Way, A., and Liu, Q.
(2014). Dcu terminology translation system for
medical query subtask at wmt14. In Proceed-
ings of the ACL 2014 Ninth Workshop of Statis-
tical Machine Translation, Baltimore, USA.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a method for automatic eval-
uation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 311?318,
Philadelphia, PA, USA. Association for Com-
putational Linguistics.
P?echeux, N., Gong, L., Do, Q. K., Marie, B.,
Ivanishcheva, Y., Allauzen, A., Lavergne, T.,
52
Niehues, J., Max, A., and Yvon, Y. (2014).
LIMSI @ WMT?14 Medical Translation Task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, USA.
Pecina, P., Du?sek, O., Goeuriot, L., Haji?c, J.,
Hlav?a?cov?a, J., Jones, G., Kelly, L., Leveling, J.,
Mare?cek, D., Nov?ak, M., Popel, M., Rosa, R.,
Tamchyna, A., and Ure?sov?a, Z. (2014). Adapta-
tion of machine translation for multilingual in-
formation retrieval in the medical domain. Arti-
ficial Intelligence in Medicine, (0):?.
Peitz, S., Wuebker, J., Freitag, M., and Ney, H.
(2014). The rwth aachen german-english ma-
chine translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Pouliquen, B. and Mazenc, C. (2011). COPPA,
CLIR and TAPTA: three tools to assist in over-
coming the patent barrier at WIPO. In Pro-
ceedings of the Thirteenth Machine Translation
Summit, pages 24?30, Xiamen, China. Asia-
Pacific Association for Machine Translation.
Powers, D. M. W. (2011). Evaluation: from preci-
sion, recall and f-measure to roc, informedness,
markedness & correlation. Journal of Machine
Learning Technologies.
Quernheim, D. and Cap, F. (2014). Large-scale ex-
act decoding: The ims-ttt submission to wmt14.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Rosse, C. and Mejino Jr., J. L. V. (2008). The
foundational model of anatomy ontology. In
Burger, A., Davidson, D., and Baldock, R., ed-
itors, Anatomy Ontologies for Bioinformatics,
volume 6 of Computational Biology, pages 59?
117. Springer London.
Rubino, R., Toral, A., S?anchez-Cartagena, V. M.,
Ferr?andez-Tordera, J., Ortiz Rojas, S., Ram??rez-
S?anchez, G., S?anchez-Mart??nez, F., and Way,
A. (2014). Abu-matran at wmt 2014 transla-
tion task: Two-step data selection and rbmt-
style synthetic rules. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Sakaguchi, K., Post, M., and Van Durme, B.
(2014). Efficient elicitation of annotations for
human evaluation of machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland.
S?anchez-Cartagena, V. M., P?erez-Ortiz, J. A., and
S?anchez-Mart??nez, F. (2014). The ua-prompsit
hybrid machine translation system for the 2014
workshop on statistical machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Scarton, C. and Specia, L. (2014). Exploring con-
sensus in machine translation for quality esti-
mation. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Schwartz, L., Anderson, T., Gwinnup, J., and
Young, K. (2014). Machine translation and
monolingual postediting: The afrl wmt-14 sys-
tem. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An
investigation on the effectiveness of features for
translation quality estimation. In Proceedings
of the Machine Translation Summit XIV, pages
167?174, Nice, France.
Shah, K. and Specia, L. (2014). Quality estimation
for translation selection. In Proceedings of the
17th Annual Conference of the European As-
sociation for Machine Translation, Dubrovnik,
Croatia.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
53
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria.
Tamchyna, A., Popel, M., Rosa, R., and Bojar, O.
(2014). Cuni in wmt14: Chimera still awaits
bellerophon. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, Balti-
more, Maryland, USA. Association for Compu-
tational Linguistics.
Tan, L. and Pal, S. (2014). Manawi: Using
multi-word expressions and named entities to
improve machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Thompson, P., Iqbal, S., McNaught, J., and Ana-
niadou, S. (2009). Construction of an annotated
corpus to support biomedical information ex-
traction. BMC bioinformatics, 10(1):349.
Tiedemann, J. (2009). News from OPUS ? a
collection of multilingual parallel corpora with
tools and interfaces. In Recent Advances in
Natural Language Processing, volume 5, pages
237?248, Borovets, Bulgaria. John Benjamins.
Tillmann, C., Vogel, S., Ney, H., Zubiaga, A.,
and Sawaf, H. (1997). Accelerated DP based
search for statistical translation. In Kokki-
nakis, G., Fakotakis, N., and Dermatas, E., edi-
tors, Proceedings of the Fifth European Confer-
ence on Speech Communication and Technol-
ogy, pages 2667?2670, Rhodes, Greece. Inter-
national Speech Communication Association.
U.S. National Library of Medicine (2009). UMLS
reference manual. Metathesaurus. Bethesda,
MD, USA.
Voorhees, E. M. and Harman, D. K., editors
(2005). TREC: Experiment and evaluation in
information retrieval, volume 63 of Digital li-
braries and electronic publishing series. MIT
press Cambridge, Cambridge, MA, USA.
Wang, L., Lu, Y., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira., F. (2014). Combining domain
adaptation approaches for medical text transla-
tion. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
W?aschle, K. and Riezler, S. (2012). Analyz-
ing parallelism and domain similarities in the
MAREC patent corpus. In Salampasis, M. and
Larsen, B., editors, Multidisciplinary Informa-
tion Retrieval, volume 7356 of Lecture Notes
in Computer Science, pages 12?27. Springer
Berlin Heidelberg.
Williams, P., Sennrich, R., Nadejde, M., Huck, M.,
Hasler, E., and Koehn, P. (2014). Edinburghs
syntax-based systems at wmt 2014. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Wisniewski, G., P?echeux, N., Allauzen, A., and
Yvon, F. (2014). Limsi submission for wmt?14
qe task. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
wu, x., Haque, R., Okita, T., Arora, P., Way, A.,
and Liu, Q. (2014). Dcu-lingo24 participation
in wmt 2014 hindi-english translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Zde?nka Ure?sov?a, Ond?rej Du?sek, J. H. and Pecina,
P. (2014). Multilingual test sets for machine
translation of search queries for cross-lingual
information retrieval in the medical domain. In
To appear in Proceedings of the Ninth Interna-
tional Conference on Language Resources and
Evaluation, Reykjavik, Iceland.
Zhang, J., Wu, X., Calixto, I., Vahid, A. H., Zhang,
X., Way, A., and Liu, Q. (2014). Experiments in
medical translation shared task at wmt 2014. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
54
A Pairwise System Comparisons by Human Judges
Tables 28?37 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according the official method used in
Table 8. Gray lines separate clusters based on non-overlapping rank ranges.
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
C
U
-
M
O
S
E
S
ONLINE-B ? .47? .43? .42? .39?
UEDIN-PHRASE .53? ? .44? .44? .41?
UEDIN-SYNTAX .57? .56? ? .49 .48?
ONLINE-A .58? .56? .51 ? .48?
CU-MOSES .61? .59? .52? .52? ?
score .57 .54 .47 .46 .44
rank 1 2 3-4 3-4 5
Table 28: Head to head comparison, ignoring ties, for Czech-English systems
C
U
-
D
E
P
F
I
X
U
E
D
I
N
-
U
N
C
N
S
T
R
C
U
-
B
O
J
A
R
C
U
-
F
U
N
K
Y
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
O
N
L
I
N
E
-
A
C
U
-
T
E
C
T
O
C
O
M
M
E
R
C
I
A
L
1
C
O
M
M
E
R
C
I
A
L
2
CU-DEPFIX ? .50 .42? .48 .44? .43? .41? .35? .30? .24?
UEDIN-UNCNSTR .50 ? .51 .48 .42? .37? .42? .39? .31? .26?
CU-BOJAR .58? .49 ? .49 .45? .44? .40? .36? .32? .24?
CU-FUNKY .52 .52 .51 ? .48 .47? .44? .34? .33? .26?
ONLINE-B .56? .58? .55? .52 ? .48 .47? .41? .31? .26?
UEDIN-PHRASE .57? .63? .56? .53? .52 ? .48 .44? .32? .27?
ONLINE-A .59? .58? .60? .56? .53? .52 ? .45? .37? .30?
CU-TECTO .65? .61? .64? .66? .59? .56? .55? ? .42? .30?
COMMERCIAL1 .70? .69? .68? .67? .69? .68? .63? .58? ? .40?
COMMERCIAL2 .76? .74? .76? .74? .74? .73? .70? .70? .60? ?
score .60 .59 .58 .57 .54 .52 .50 .44 .36 .28
rank 1-3 1-3 1-4 3-4 5-6 5-6 7 8 9 10
Table 29: Head to head comparison, ignoring ties, for English-Czech systems
55
O
N
L
I
N
E
-
B
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
L
I
M
S
I
-
K
I
T
E
U
-
B
R
I
D
G
E
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
R
W
T
H
D
C
U
-
I
C
T
C
A
S
C
M
U
R
B
M
T
4
R
B
M
T
1
O
N
L
I
N
E
-
C
ONLINE-B ? .46 .40? .41? .35? .42? .38? .35? .40? .31? .33? .32? .22?
UEDIN-SYNTAX .54 ? .51 .47 .47 .45 .45? .39? .36? .38? .35? .34? .27?
ONLINE-A .60? .49 ? .42? .44? .51 .41? .38? .44? .42? .38? .31? .20?
LIMSI-KIT .59? .53 .58? ? .55 .53 .31? .45? .39? .41? .37? .35? .29?
EU-BRIDGE .65? .53 .56? .45 ? .45 .44? .48 .40? .37? .39? .37? .30?
UEDIN-PHRASE .58? .55 .49 .47 .55 ? .48 .39? .34? .45? .40? .40? .34?
KIT .62? .55? .59? .69? .56? .52 ? .45? .41? .45? .47 .40? .31?
RWTH .65? .61? .62? .55? .52 .61? .55? ? .54 .44? .44? .38? .37?
DCU-ICTCAS .60? .64? .56? .61? .60? .66? .59? .46 ? .51 .49 .46? .40?
CMU .69? .62? .58? .59? .63? .55? .55? .56? .49 ? .53 .42? .43?
RBMT4 .67? .65? .62? .63? .61? .60? .53 .56? .51 .47 ? .51 .37?
RBMT1 .68? .66? .69? .65? .63? .60? .60? .62? .54? .58? .49 ? .38?
ONLINE-C .78? .73? .80? .71? .70? .66? .69? .63? .60? .57? .63? .62? ?
score .63 .58 .58 .55 .55 .54 .49 .47 .45 .44 .44 .40 .32
rank 1 2-3 2-3 4-6 4-6 4-6 7-8 7-8 9-11 9-11 9-11 12 13
Table 30: Head to head comparison, ignoring ties, for German-English systems
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
S
T
A
N
F
O
R
D
E
U
-
B
R
I
D
G
E
R
B
M
T
4
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
1
K
I
T
S
T
A
N
F
O
R
D
-
U
N
C
C
I
M
S
S
T
A
N
F
O
R
D
U
U
O
N
L
I
N
E
-
C
I
M
S
-
T
T
T
U
U
-
D
O
C
E
N
T
UEDIN-SYNTAX ? .55? .46? .45? .46? .44? .41? .45? .43? .41? .38? .38? .36? .33? .38? .30? .30? .25?
ONLINE-B .45? ? .50 .48 .50 .47 .43? .46? .41? .45? .39? .39? .37? .32? .35? .34? .30? .29?
ONLINE-A .54? .50 ? .44? .52 .50 .45? .43? .43? .42? .39? .41? .42? .42? .37? .44? .38? .33?
PROMT-HYBRID .55? .52 .56? ? .45? .47 .47 .46? .50 .44? .42? .40? .41? .38? .39? .39? .33? .34?
PROMT-RULE .54? .50 .48 .55? ? .51 .47 .47 .45? .38? .42? .40? .43? .41? .43? .38? .35? .29?
UEDIN-STANFORD .56? .53 .50 .53 .49 ? .48 .50 .47 .44? .46 .36? .36? .36? .36? .35? .30? .32?
EU-BRIDGE .59? .57? .55? .53 .53 .52 ? .46? .43? .52 .42? .42? .45? .35? .36? .41? .38? .30?
RBMT4 .55? .54? .57? .54? .53 .50 .54? ? .53 .49 .44? .49 .50 .47 .40? .42? .38? .40?
UEDIN-PHRASE .57? .59? .57? .50 .55? .53 .57? .47 ? .50 .55? .47 .45? .44? .43? .42? .37? .34?
RBMT1 .59? .55? .58? .56? .62? .56? .48 .51 .50 ? .47 .47 .45? .47 .43? .42? .38? .41?
KIT .62? .61? .61? .58? .58? .54 .58? .56? .45? .53 ? .47 .49 .46 .43? .48 .34? .37?
STANFORD-UNC .62? .61? .59? .60? .60? .64? .58? .51 .53 .53 .53 ? .48 .47 .45? .45? .39? .41?
CIMS .64? .63? .58? .59? .57? .64? .55? .50 .55? .55? .51 .52 ? .53 .42? .52 .47 .42?
STANFORD .67? .68? .58? .62? .59? .64? .65? .53 .56? .53 .54 .53 .47 ? .53 .42? .39? .48
UU .62? .65? .62? .61? .57? .64? .64? .60? .57? .57? .57? .55? .58? .47 ? .46? .45? .38?
ONLINE-C .70? .66? .56? .61? .62? .65? .59? .58? .58? .58? .52 .55? .48 .58? .54? ? .48 .47
IMS-TTT .70? .70? .62? .67? .65? .70? .62? .62? .63? .62? .66? .61? .53 .61? .55? .52 ? .49
UU-DOCENT .75? .71? .67? .66? .71? .68? .70? .60? .66? .59? .63? .59? .58? .52 .62? .53 .51 ?
score .60 .59 .56 .56 .56 .56 .54 .51 .51 .50 .48 .47 .46 .44 .43 .42 .38 .37
rank 1-2 1-2 3-6 3-6 3-6 3-6 7 8-10 8-10 8-10 11-12 11-13 12-14 13-15 14-16 15-16 17-18 17-18
Table 31: Head to head comparison, ignoring ties, for English-German systems
56
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
O
N
L
I
N
E
-
B
S
T
A
N
F
O
R
D
O
N
L
I
N
E
-
A
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
UEDIN-PHRASE ? .48 .48 .45? .43? .28? .28? .19?
KIT .52 ? .54? .48 .44? .31? .29? .21?
ONLINE-B .52 .46? ? .51 .47 .31? .30? .24?
STANFORD .55? .52 .49 ? .46? .34? .30? .23?
ONLINE-A .57? .56? .53 .54? ? .32? .29? .21?
RBMT1 .72? .69? .69? .66? .68? ? .42? .33?
RBMT4 .72? .71? .70? .70? .71? .58? ? .39?
ONLINE-C .81? .79? .76? .77? .79? .67? .61? ?
score .63 .60 .59 .58 .57 .40 .35 .25
rank 1 2-4 2-4 2-4 5 6 7 8
Table 32: Head to head comparison, ignoring ties, for French-English systems
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
M
A
T
R
A
N
M
A
T
R
A
N
-
R
U
L
E
S
O
N
L
I
N
E
-
A
U
U
-
D
O
C
E
N
T
P
R
O
M
T
-
H
Y
B
R
I
D
U
A
P
R
O
M
T
-
R
U
L
E
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
ONLINE-B ? .46? .48 .46? .50 .41? .39? .39? .37? .38? .37? .35? .27?
UEDIN-PHRASE .54? ? .50 .47 .46 .46? .42? .41? .46? .42? .35? .34? .33?
KIT .52 .50 ? .53 .51 .50 .43? .49 .41? .42? .35? .37? .29?
MATRAN .54? .53 .47 ? .49 .50 .43? .43? .38? .48 .40? .34? .32?
MATRAN-RULES .50 .54 .49 .51 ? .53 .40? .45? .46? .42? .44? .40? .34?
ONLINE-A .59? .54? .50 .50 .47 ? .44? .49 .47 .45? .42? .37? .34?
UU-DOCENT .61? .58? .57? .57? .60? .56? ? .43? .52 .46? .39? .44? .33?
PROMT-HYBRID .61? .59? .51 .57? .55? .51 .57? ? .50 .41? .46? .44? .35?
UA .63? .54? .59? .62? .54? .53 .48 .50 ? .49 .46? .43? .34?
PROMT-RULE .62? .58? .58? .52 .58? .55? .54? .59? .51 ? .47 .39? .37?
RBMT1 .63? .65? .65? .60? .56? .58? .61? .54? .54? .53 ? .46? .45?
RBMT4 .65? .66? .63? .66? .60? .63? .56? .56? .57? .61? .54? ? .45?
ONLINE-C .73? .67? .71? .67? .66? .66? .67? .65? .66? .63? .55? .55? ?
score .59 .57 .55 .55 .54 .53 .49 .49 .48 .47 .43 .40 .34
rank 1 2-4 2-5 2-5 4-6 4-6 7-9 7-10 7-10 8-10 11 12 13
Table 33: Head to head comparison, ignoring ties, for English-French systems
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
S
Y
N
T
A
X
C
M
U
U
E
D
I
N
-
P
H
R
A
S
E
A
F
R
L
I
I
T
-
B
O
M
B
A
Y
D
C
U
-
L
I
N
G
O
2
4
I
I
I
T
-
H
Y
D
E
R
A
B
A
D
ONLINE-B ? .36? .33? .37? .31? .21? .20? .14? .00
ONLINE-A .64? ? .48 .47? .44? .31? .30? .24? .12?
UEDIN-SYNTAX .67? .52 ? .47 .46? .33? .29? .24? .12?
CMU .63? .53? .53 ? .47 .37? .31? .26? .11?
UEDIN-PHRASE .69? .56? .54? .53 ? .40? .33? .25? .11?
AFRL .79? .69? .67? .63? .60? ? .53 .40? .16?
IIT-BOMBAY .80? .70? .71? .69? .67? .47 ? .44? .19?
DCU-LINGO24 .86? .76? .76? .74? .75? .60? .56? ? .19?
IIIT-HYDERABAD .94? .88? .88? .89? .89? .84? .81? .81? ?
score .75 .62 .61 .60 .57 .44 .41 .34 .13
rank 1 2-3 2-4 3-4 5 6-7 6-7 8 9
Table 34: Head to head comparison, ignoring ties, for Hindi-English systems
57
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
U
N
C
N
S
T
R
U
E
D
I
N
-
P
H
R
A
S
E
C
U
-
M
O
S
E
S
I
I
T
-
B
O
M
B
A
Y
I
P
N
-
U
P
V
-
C
N
T
X
T
D
C
U
-
L
I
N
G
O
2
4
I
P
N
-
U
P
V
-
N
O
D
E
V
M
A
N
A
W
I
-
H
1
M
A
N
A
W
I
M
A
N
A
W
I
-
R
M
O
O
V
ONLINE-B ? .49 .28? .29? .27? .23? .22? .20? .17? .12? .13? .13?
ONLINE-A .51 ? .31? .29? .27? .25? .20? .20? .21? .19? .16? .15?
UEDIN-UNCNSTR .72? .69? ? .44? .49 .39? .40? .34? .39? .29? .30? .27?
UEDIN-PHRASE .71? .71? .56? ? .48 .45? .44? .39? .37? .31? .31? .32?
CU-MOSES .73? .73? .51 .52 ? .47 .42? .40? .45? .36? .35? .33?
IIT-BOMBAY .77? .75? .61? .55? .53 ? .50 .47 .45? .41? .40? .36?
IPN-UPV-CNTXT .78? .80? .60? .56? .58? .50 ? .51 .41? .40? .40? .37?
DCU-LINGO24 .80? .80? .66? .61? .60? .53 .49 ? .52 .41? .41? .39?
IPN-UPV-NODEV .83? .79? .61? .63? .55? .55? .59? .48 ? .46? .44? .38?
MANAWI-H1 .88? .81? .71? .69? .64? .59? .60? .59? .54? ? .35? .34?
MANAWI .87? .84? .70? .69? .65? .60? .60? .59? .56? .65? ? .39?
MANAWI-RMOOV .87? .85? .73? .68? .67? .64? .63? .61? .62? .66? .61? ?
score .77 .75 .57 .54 .52 .47 .46 .43 .42 .38 .35 .31
rank 1 2 3 4-5 4-5 6-7 6-7 8-9 8-9 10-11 10-11 12
Table 35: Head to head comparison, ignoring ties, for English-Hindi systems
A
F
R
L
-
P
E
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
P
H
R
A
S
E
Y
A
N
D
E
X
O
N
L
I
N
E
-
G
A
F
R
L
U
E
D
I
N
-
S
Y
N
T
A
X
K
A
Z
N
U
R
B
M
T
1
R
B
M
T
4
AFRL-PE ? .42? .40? .39? .39? .41? .35? .39? .28? .26? .26? .29? .21?
ONLINE-B .58? ? .42? .43? .45? .45? .42? .43? .46? .37? .33? .29? .31?
ONLINE-A .60? .58? ? .50 .45? .51 .47 .45? .42? .40? .33? .32? .30?
PROMT-HYBRID .61? .57? .50 ? .47 .45? .49 .44? .43? .44? .39? .31? .27?
PROMT-RULE .61? .55? .55? .53 ? .46? .47 .49 .48 .42? .36? .34? .30?
UEDIN-PHRASE .59? .55? .49 .55? .54? ? .49 .50 .47 .44? .32? .37? .29?
YANDEX .65? .58? .53 .51 .53 .51 ? .48 .50 .43? .34? .36? .34?
ONLINE-G .61? .57? .55? .56? .51 .50 .52 ? .48 .43? .39? .35? .30?
AFRL .72? .54? .58? .57? .52 .53 .50 .52 ? .44? .41? .41? .37?
UEDIN-SYNTAX .74? .63? .60? .56? .58? .56? .57? .57? .56? ? .51 .36? .37?
KAZNU .74? .67? .67? .61? .64? .68? .66? .61? .59? .49 ? .44? .38?
RBMT1 .71? .71? .68? .69? .66? .63? .64? .65? .59? .64? .56? ? .47
RBMT4 .79? .69? .70? .73? .70? .71? .66? .70? .63? .63? .62? .53 ?
score .66 .58 .55 .55 .53 .53 .52 .51 .49 .45 .40 .36 .32
rank 1 2 3-5 3-5 4-7 5-8 5-8 5-8 9 10 11 12 13
Table 36: Head to head comparison, ignoring ties, for Russian-English systems
P
R
O
M
T
-
R
U
L
E
O
N
L
I
N
E
-
B
P
R
O
M
T
-
H
Y
B
R
I
D
U
E
D
I
N
-
U
N
C
N
S
T
R
O
N
L
I
N
E
-
G
O
N
L
I
N
E
-
A
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
4
R
B
M
T
1
PROMT-RULE ? .51 .45? .43? .43? .39? .38? .15? .00
ONLINE-B .49 ? .50 .47? .38? .36? .38? .16? .13?
PROMT-HYBRID .55? .50 ? .49 .47 .39? .40? .18? .15?
UEDIN-UNCNSTR .57? .53? .51 ? .50 .44? .36? .25? .18?
ONLINE-G .57? .62? .53 .50 ? .46? .44? .23? .18?
ONLINE-A .61? .64? .61? .56? .54? ? .49 .24? .18?
UEDIN-PHRASE .62? .62? .60? .64? .56? .51 ? .30? .21?
RBMT4 .85? .84? .82? .75? .77? .76? .70? ? .42?
RBMT1 .91? .87? .85? .82? .82? .82? .79? .58? ?
score .64 .64 .61 .58 .55 .51 .49 .26 .19
rank 1-2 1-2 3 4-5 4-5 6-7 6-7 8 9
Table 37: Head to head comparison, ignoring ties, for English-Russian systems
58
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 307?312,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
SHEF-Lite 2.0: Sparse Multi-task Gaussian Processes for Translation
Quality Estimation
Daniel Beck and Kashif Shah and Lucia Specia
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
{debeck1,kashif.shah,l.specia}@sheffield.ac.uk
Abstract
We describe our systems for the WMT14
Shared Task on Quality Estimation (sub-
tasks 1.1, 1.2 and 1.3). Our submissions
use the framework of Multi-task Gaus-
sian Processes, where we combine multi-
ple datasets in a multi-task setting. Due to
the large size of our datasets we also ex-
periment with Sparse Gaussian Processes,
which aim to speed up training and predic-
tion by providing sensible sparse approxi-
mations.
1 Introduction
The purpose of machine translation (MT) quality
estimation (QE) is to provide a quality prediction
for new, unseen machine translated texts, with-
out relying on reference translations (Blatz et al.,
2004; Specia et al., 2009; Bojar et al., 2013). A
common use of quality predictions is the decision
between post-editing a given machine translated
sentence and translating its source from scratch,
based on whether its post-editing effort is esti-
mated to be lower than the effort of translating the
source sentence.
The WMT 2014 QE shared task defined a group
of tasks related to QE. In this paper, we de-
scribe our submissions for subtasks 1.1, 1.2 and
1.3. Our models are based on Gaussian Pro-
cesses (GPs) (Rasmussen and Williams, 2006),
a non-parametric kernelised probabilistic frame-
work. We propose to combine multiple datasets
to improve our QE models by applying GPs in
a multi-task setting. Our hypothesis is that us-
ing sensible multi-task learning settings gives im-
provements over simply pooling all datasets to-
gether.
Task 1.1 focuses on predicting post-editing ef-
fort for four language pairs: English-Spanish
(en-es), Spanish-English (es-en), English-German
(en-de), and German-English (de-en). Each con-
tains a different number of source sentences and
their human translations, as well as 2-3 versions
of machine translations: by a statistical (SMT)
system, a rule-based system (RBMT) system and,
for en-es/de only, a hybrid system. Source sen-
tences were extracted from tests sets of WMT13
and WMT12, and the translations were produced
by top MT systems of each type and a human
translator. Labels range from 1 to 3, with 1 in-
dicating a perfect translation and 3, a low quality
translation.
The purpose of task 1.2 is to predict HTER
scores (Human Translation Error Rate) (Snover
et al., 2006) using a dataset composed of 896
English-Spanish sentences translated by a MT sys-
tem and post-edited by a professional translator.
Finally, task 1.3 aims at predicting post-editing
time, using a subset of 650 sentences from the
Task 1.2 dataset.
For each task, participants can submit two types
of results: scoring and ranking. For scoring, eval-
uation is made in terms of Mean Absolute Error
(MAE) and Root Mean Square Error (RMSE). For
ranking, DeltaAvg and Spearman?s rank correla-
tion were used as evaluation metrics.
2 Model
Gaussian Processes are a Bayesian non-parametric
machine learning framework considered the state-
of-the-art for regression. They assume the pres-
ence of a latent function f : R
F
? R, which maps
a vector x from feature space F to a scalar value.
Formally, this function is drawn from a GP prior:
f(x) ? GP(0, k(x,x
?
)),
which is parameterised by a mean function (here,
0) and a covariance kernel function k(x,x
?
). Each
response value is then generated from the function
evaluated at the corresponding input, y
i
= f(x
i
)+
?, where ? ? N (0, ?
2
n
) is added white-noise.
307
Prediction is formulated as a Bayesian inference
under the posterior:
p(y
?
|x
?
,D) =
?
f
p(y
?
|x
?
, f)p(f |D),
where x
?
is a test input, y
?
is the test response
value andD is the training set. The predictive pos-
terior can be solved analitically, resulting in:
y
?
? N (k
T
?
(K + ?
2
n
I)
?1
y,
k(x
?
, x
?
)? k
T
?
(K + ?
2
n
I)
?1
k
?
),
where k
?
= [k(x
?
,x
1
)k(x
?
,x
2
) . . . k(x
?
,x
n
)]
T
is the vector of kernel evaluations between the
training set and the test input and K is the kernel
matrix over the training inputs (the Gram matrix).
The kernel function encodes the covariance
(similarity) between each input pair. While a vari-
ety of kernel functions are available, here we fol-
lowed previous work in QE using GP (Cohn and
Specia, 2013; Shah et al., 2013) and employed
a squared exponential (SE) kernel with automatic
relevance determination (ARD):
k(x,x
?
) = ?
2
f
exp
(
?
1
2
F
?
i=1
x
i
? x
?
i
l
i
)
,
where F is the number of features, ?
2
f
is the co-
variance magnitude and l
i
> 0 are the feature
lengthscales.
The resulting model hyperparameters (SE vari-
ance ?
2
f
, noise variance ?
2
n
and SE lengthscales l
i
)
were learned from data by maximising the model
likelihood. All our models were trained using the
GPy
1
toolkit, an open source implementation of
GPs written in Python.
2.1 Multi-task learning
The GP regression framework can be extended to
multiple outputs by assuming f(x) to be a vec-
tor valued function. These models are commonly
referred as coregionalization models in the GP lit-
erature (
?
Alvarez et al., 2012). Here we refer to
them as multi-task kernels, to emphasize our ap-
plication.
In this work, we employ a separable multi-task
kernel, similar to the one used by Bonilla et al.
(2008) and Cohn and Specia (2013). Consider-
ing a set of D tasks, we define the corresponding
multi-task kernel as:
k((x, d), (x
?
, d
?
)) = k
data
(x,x
?
)?B
d,d
?
, (1)
1
http://sheffieldml.github.io/GPy/
where k
data
is a kernel on the input points, d and
d
?
are task or metadata information for each input
and B ? R
D?D
is the multi-task matrix, which
encodes task covariances. For task 1.1, we con-
sider each language pair as a different task, while
for tasks 1.2 and 1.3 we use additional datasets
for the same language pair (en-es), treating each
dataset as a different task.
To perform the learning procedure the multi-
task matrix should be parameterised in a sensible
way. We follow the parameterisations proposed
by Cohn and Specia (2013), which we briefly de-
scribe here:
Independent: B = I. In this setting each task is
modelled independently. This is not strictly
equivalent to independent model training be-
cause the tasks share the same data kernel
(and the same hyperparameters);
Pooled: B = 1. Here the task identity is ignored.
This is equivalent to pooling all datasets in a
single task model;
Combined: B = 1 + ?I. This setting lever-
ages between independent and pooled mod-
els. Here, ? > 0 is treated as an hyperparam-
eter;
Combined+: B = 1 + diag(?). Same as ?com-
bined?, but allowing one different ? value per
task.
2.2 Sparse Gaussian Processes
The performance bottleneck for GP models is the
Gram matrix inversion, which is O(n
3
) for stan-
dard GPs, with n being the number of training in-
stances. For multi-task settings this can be a po-
tential issue because these models replicate the in-
stances for each task and the resulting Gram ma-
trix has dimensionality nd ? nd, where d is the
number of tasks.
Sparse GPs tackle this problem by approximat-
ing the Gram matrix using only a subset of m in-
ducing inputs. Without loss of generalisation, con-
sider these m points as the first instances in the
training data. We can then expand the Gram ma-
trix in the following way:
K =
[
K
mm
K
m(n?m)
K
(n?m)m
K
(n?m)(n?m)
]
.
Following the notation in (Rasmussen and
Williams, 2006), we refer K
m(n?m)
as K
mn
and
308
its transpose as K
nm
. The block structure of K
forms the basis of the so-called Nystr?om approxi-
mation:
?
K = K
nm
K
?1
mm
K
mn
, (2)
which results in the following predictive posterior:
y
?
? N (k
T
m?
?
G
?1
K
mn
y, (3)
k(x
?
,x
?
)? k
T
m?
K
?1
mm
k
m?
+
?
2
n
k
T
m?
?
G
?1
k
m?
),
where
?
G = ?
2
n
K
mm
+ K
mn
K
nm
and k
m?
is the
vector of kernel evaluations between test input x
?
and the m inducing inputs. The resulting training
complexity is O(m
2
n).
The remaining question is how to choose the in-
ducing inputs. We follow the approach of Snelson
and Ghahramani (2006), which note that these in-
ducing inputs do not need to be a subset of the
training data. Their method considers each in-
put as a hyperparameter, which is then optimised
jointly with the kernel hyperparameters.
2.3 Features
For all tasks we used the QuEst framework (Spe-
cia et al., 2013) to extract a set of 80 black-box
features as in Shah et al. (2013), for which we had
all the necessary resources available. Examples of
the features extracted include:
? N-gram-based features:
? Number of tokens in source and target
segments;
? Language model (LM) probability of
source and target segments;
? Percentage of source 1?3-grams ob-
served in different frequency quartiles of
a large corpus of the source language;
? Average number of translations per
source word in the segment as given by
IBM 1 model from a large parallel cor-
pus of the language, with probabilities
thresholded in different ways.
? POS-based features:
? Ratio of percentage of nouns/verbs/etc
in the source and target segments;
? Ratio of punctuation symbols in source
and target segments;
? Percentage of direct object personal or
possessive pronouns incorrectly trans-
lated.
For the full set of features we refer readers to
QuEst website.
2
To perform feature selection, we followed the
approach used in Shah et al. (2013) and ranked
the features according to their learned lengthscales
(from the lowest to the highest). The lengthscale
of a feature can be interpreted as the relevance of
such feature for the model. Therefore, the out-
come of a GP model using an ARD kernel can be
viewed as a list of features ranked by relevance,
and this information can be used for feature selec-
tion by discarding the lowest ranked (least useful)
ones.
3 Preliminary Experiments
Our submissions are based on multi-task settings.
For task 1.1, we consider each language pair as a
different task, training one model for all pairs. For
tasks 1.2 and 1.3, we used additional datasets and
encoded each one as a different task (totalling 3
tasks):
WMT13: these are the datasets provided in last
year?s QE shared task (Bojar et al., 2013).
We combined training and test sets, totalling
2, 754 sentences for HTER prediction and
1, 003 sentences for post-editing time predic-
tion, both for English-Spanish.
EAMT11: this dataset is provided by Specia
(2011) and is composed of 1, 000 English-
Spanish sentences annotated in terms of
HTER and post-editing time.
For each task we prepared two submissions: one
trained on a standard GP with the full 80 features
set and another one trained on a sparse GP with
a subset of 40 features. The features were chosen
by training a smaller model on a subset of 400 in-
stances and following the procedure explained in
Section 2.3 for feature selection, with a pre-define
cutoff point on the number of features (40), based
on previous experiments. The sparse models were
trained using 400 inducing inputs.
To select an appropriate multi-task setting for
our submissions we performed preliminary exper-
iments using a 90%/10% split on the correspond-
ing training set for each task. The resulting MAE
scores are shown in Tables 1 and 2, for standard
and sparse GPs, respectively. The boldface fig-
ures correspond to the settings we choose for the
2
http://www.quest.dcs.shef.ac.uk/
quest_files/features_blackbox
309
Task 1.1 Task 1.2 Task 1.3
en-es es-en en-de de-en en-es en-es
Independent 0.4905 0.5325 0.5962 0.5452 0.2047 0.4486
Pooled 0.4957 0.5171 0.6012 0.5612 0.2036 0.8599
Combined 0.4939 0.5162 0.6007 0.5550 0.2321 0.7489
Combined+ 0.4932 0.5182 0.5990 0.5514 0.2296 0.4472
Table 1: MAE results for preliminary experiments on standard GPs. Post-editing time scores for task 1.3
are shown on log time per word.
Task 1.1 Task 1.2 Task 1.3
en-es es-en en-de de-en en-es en-es
Independent 0.5036 0.5274 0.6002 0.5532 0.3432 0.3906
Pooled 0.4890 0.5131 0.5927 0.5532 0.1597 0.6410
Combined 0.4872 0.5183 0.5871 0.5451 0.2871 0.6449
Combined+ 0.4935 0.5255 0.5864 0.5458 0.1659 0.4040
Table 2: MAE results for preliminary experiments on sparse GPs. Post-editing time scores for task 1.3
are shown on log time per word.
official submissions, after re-training on the corre-
sponding full training sets.
To check the speed-ups obtained from using
sparse GPs, we measured wall clock times for
training and prediction in Task 1.1 using the ?In-
dependent? multi-task setting. Table 3 shows the
resulting times and the corresponding speed-ups
when comparing to the standard GP. For compar-
ison, we also trained a model using 200 inducing
inputs, although we did not use the results of this
model in our submissions.
Time (secs) Speed-up
Standard GP 12122 ?
Sparse GP (m=400) 3376 3.59x
Sparse GP (m=200) 978 12.39x
Table 3: Wall clock times and speed-ups for GPs
training and prediction: full versus sparse GPs.
4 Official Results and Discussion
Table 4 shows the results for Task 1.1. Us-
ing standard GPs we obtained improved results
over the baseline for English-Spanish and English-
German only, with particularly substantial im-
provements for English-Spanish, which also hap-
pens for sparse GPs. This may be related to the
larger size of this dataset when compared to the
others. Our results here are mostly inconclusive
though and we plan to investigate this setting more
in depth in the future. Specifically, due to the
coarse behaviour of the labels, ordinal regression
GP models (like the one proposed in (Chu et al.,
2005)) could be useful for this task.
Results for Task 1.2 are shown in Table 5. The
standard GP model performed unusually poorly
when compared to the baseline or the sparse GP
model. To investigate this, we inspected the re-
sulting model hyperparameters. We found out that
the noise ?
2
n
was optimised to a very low value,
close to zero, which characterises overfitting. The
same behaviour was not observed with the sparse
model, even though it had a much higher number
of hyperparameters to optimise, and was therefore
more prone to overfitting. We plan to investigate
this issue further but a possible cause could be bad
starting values for the hyperparameters.
Table 6 shows results for Task 1.3. In this task,
the standard GP model outperformed the base-
line, with the sparse GP model following very
closely. These figures represent significant im-
provements compared to our submission to the
same task in last year?s shared task (Beck et al.,
2013), where we were not able to beat the baseline.
The main differences between last year?s and this
year?s models are the use of additional datasets
and a higher number of features (25 vs. 40). The
competitive results for the sparse GP models are
very promising because they show we can com-
bine multiple datasets to improve post-editing time
prediction while employing a sparse model to cope
with speed issues.
310
en-es es-en en-de de-en
? ? ? ? ? ? ? ?
Standard GP 0.21 -0.33 0.11 -0.15 0.26 -0.36 0.24 -0.27
Sparse GP 0.17 0.27 0.12 -0.17 0.23 -0.33 0.14 -0.17
Baseline 0.14 -0.22 0.12 -0.21 0.23 -0.34 0.21 -0.25
en-es es-en en-de de-en
MAE RMSE MAE RMSE MAE RMSE MAE RMSE
Standard GP 0.49 0.63 0.62 0.77 0.63 0.74 0.65 0.77
Sparse GP 0.54 0.69 0.54 0.69 0.64 0.75 0.66 0.79
Baseline 0.52 0.66 0.57 0.68 0.64 0.76 0.65 0.78
Table 4: Official results for task 1.1. The top table shows results for the ranking subtask (?: DeltaAvg;
?: Spearman?s correlation). The bottom table shows results for the scoring subtask.
Ranking Scoring
? ? MAE RMSE
Standard GP 0.72 0.09 18.15 23.41
Sparse GP 7.69 0.43 15.04 18.38
Baseline 5.08 0.31 15.23 19.48
Table 5: Official results for task 1.2.
Ranking Scoring
? ? MAE RMSE
Standard GP 16.08 0.64 17.13 27.33
Sparse GP 16.33 0.63 17.42 27.35
Baseline 14.71 0.57 21.49 34.28
Table 6: Official results for task 1.3.
5 Conclusions
We proposed a new setting for training QE mod-
els based on Multi-task Gaussian Processes. Our
settings combined different datasets in a sensible
way, by considering each dataset as a different
task and learning task covariances. We also pro-
posed to speed-up training and prediction times
by employing sparse GPs, which becomes crucial
in multi-task settings. The results obtained are
specially promising in the post-editing time task,
where we obtained the same results as with stan-
dard GPs and improved over our models from the
last evaluation campaign.
In the future, we plan to employ our multi-task
models in large-scale settings, like datasets an-
notated through crowdsourcing platforms. These
datasets are usually labelled by dozens of annota-
tors and multi-task GPs have proved an interest-
ing framework for learning the annotation noise
(Cohn and Specia, 2013). However, multiple tasks
can easily make training and prediction times pro-
hibitive, and thus another direction if work is to
use recent advances in sparse GPs, like the one
proposed by Hensman et al. (2013). We believe
that the combination of these approaches could
further improve the state-of-the-art performance in
these tasks.
Acknowledgments
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9, Daniel Beck)
and from European Union?s Seventh Framework
Programme for research, technological develop-
ment and demonstration under grant agreement
no. 296347 (QTLaunchPad).
References
Mauricio A.
?
Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for Vector-Valued Func-
tions: a Review. Foundations and Trends in Ma-
chine Learning, pages 1?37.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite : When Less is More for
Translation Quality Estimation. In Proceedings of
WMT13, pages 337?342.
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315?321.
Ondej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proceedings
of WMT13, pages 1?44.
311
Edwin V. Bonilla, Kian Ming A. Chai, and Christopher
K. I. Williams. 2008. Multi-task Gaussian Process
Prediction. Advances in Neural Information Pro-
cessing Systems.
Wei Chu, Zoubin Ghahramani, Francesco Falciani, and
David L Wild. 2005. Biomarker discovery in mi-
croarray gene expression data with Gaussian pro-
cesses. Bioinformatics, 21(16):3385?93, August.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL.
James Hensman, Nicol`o Fusi, and Neil D. Lawrence.
2013. Gaussian Processes for Big Data. In Pro-
ceedings of UAI.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
MT Summit XIV.
Edward Snelson and Zoubin Ghahramani. 2006.
Sparse Gaussian Processes using Pseudo-inputs. In
Proceedings of NIPS.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving
the confidence of machine translation quality esti-
mates. In Proceedings of MT Summit XII.
Lucia Specia, Kashif Shah, Jos?e G. C. De Souza, and
Trevor Cohn. 2013. QuEst - A translation qual-
ity estimation framework. In Proceedings of ACL
Demo Session.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of EAMT.
312
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 342?347,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Exploring Consensus in Machine Translation for Quality Estimation
Carolina Scarton and Lucia Specia
Department of Computer Science, University of Sheffield
Regent Court, 211 Portobello, Sheffield, S1 4DP, UK
{c.scarton,l.specia}@sheffield.ac.uk
Abstract
This paper presents the use of consensus
among Machine Translation (MT) systems
for the WMT14 Quality Estimation shared
task. Consensus is explored here by com-
paring the MT system output against sev-
eral alternative machine translations using
standard evaluation metrics. Figures ex-
tracted from such metrics are used as fea-
tures to complement baseline prediction
models. The hypothesis is that knowing
whether the translation of interest is simi-
lar or dissimilar to translations from multi-
ple different MT systems can provide use-
ful information regarding the quality of
such a translation.
1 Introduction
While Machine Translation (MT) evaluation met-
rics can rely on the similarity of the MT system
output to reference (human) translations as a proxy
to quality assessment, this is not possible for MT
systems in use, translating unseen texts. Quality
Estimation (QE) metrics are used in such settings
as a way of predicting translation quality. While
reference translations are not available for QE,
previous work has explored the so called pseudo-
references (Soricut and Echihabi, 2010; Soricut et
al., 2012; Soricut and Narsale, 2012; Shah et al.,
2013). Pseudo-references are alternative transla-
tions produced by MT systems different from the
system that we intend to predict quality for (Al-
brecht and Hwa, 2008). These can be used to pro-
vide additional features to train QE models. Such
features are normally figures resulting from au-
tomatic metrics (such as BLEU, Papineni et al.
(2002)) computed between pseudo-references and
the output of the given MT system.
Soricut and Echihabi (2010) explore pseudo-
references for document-level QE prediction to
rank outputs from an MT system. The pseudo-
references-based features are BLEU scores ex-
tracted by comparing the output of the MT sys-
tem under investigation and the output of an off-
the-shelf MT system, for both the target and the
source texts. The statistical MT system training
data is also used as pseudo-references to compute
training data-based features. The use of pseudo-
references has been shown to outperform strong
baseline results. Soricut and Narsale (2012) pro-
pose a method that uses sentence-level prediction
models for document-level QE. They also use a
pseudo-references-based feature (based in BLEU)
and claim that this feature is one of the most pow-
erful in the framework.
For QE at sentence-level, Soricut et al. (2012)
use BLEU based on pseudo-references combined
with other features to build the best QE system of
the WMT12 QE shared task.
1
Shah et al. (2013)
use pseudo-references in the same way to ex-
tract a BLEU feature for sentence-level prediction.
Feature analysis on a number of datasets showed
that this feature contributed the most across all
datasets.
Louis and Nenkova (2013) apply pseudo-
references for summary evaluation. They use six
systems classified as ?best systems?, ?mediocre
systems? or ?worst systems? to make the compar-
ison, with ROUGE (Lin and Och, 2004) as quality
score. They also experiment with a combination of
the ?best systems? and the ?worst systems?. The
use of only ?best systems? led to the best results.
Examples of ?bad summaries? are said not to be
very useful because a summary close to the worst
systems outputs can mean that either it is bad or
it is too different from the best systems outputs in
terms of content. Albrecht and Hwa (2008) use
pseudo-references to improve MT evaluation by
combining them with a single human reference.
They show that the use of pseudo-references im-
1
http://www.statmt.org/wmt12/
342
proves the correlation with human judgements.
Soricut and Echihabi (2010) claim that pseudo-
references should be produced by systems as dif-
ferent as possible from the MT system of interest.
This ensures that the similarities found among the
systems? translations are not related to the similar-
ities of the systems themselves. Therefore, the as-
sumption that a translation from system X shares
some characteristics with a translation from sys-
tem Y is not a mere coincidence. Another way to
make the most of pseudo-references is to use an
MT system known as generally better (or worse)
than the MT system of interest. In that case, the
comparison will lead to whether the MT system of
interest is similar to a good (or bad) MT system.
However, in most scenarios it is difficult to rely
on the average translation quality of a given sys-
tem as an absolute indicator of its quality. This
is particularly true for sentence-level QE, where
the quality of a given system can vary signifi-
cantly across sentences. Finding translations from
MT systems that are considerably different can
also be a challenge. In this paper we exploit
pseudo-references in a different way: measuring
the consensus among different MT systems in the
translations they produce. As sources of pseudo-
references, we use translations given in a multi-
translation dataset or those produced by the par-
ticipants in the WMT translation task for the same
data. While some MT systems can be similar
to each other, for some language pairs, such as
English-Spanish, a wide range of MT systems
with different average qualities are available. Our
hypothesis is that by using translations from sev-
eral MT systems we can find consensual infor-
mation (even if some of the systems are similar
to the one of interest). The use of more than one
MT system is expected to smooth out the effect
of ?coincidences? in the similarities between sys-
tems? translations.
This paper describes the use of consensual
information for the WMT14 QE shared task
(USHEFF-consensus system), simulating a sce-
nario where we do not know the quality of the
pseudo-references, nor the characteristics of any
MT systems (the system of interest or the systems
which generated the pseudo-references). We par-
ticipated in all variants of Task 1, sentence-level
QE, for both for scoring and ranking. Section 2
explains how we extracted consensual information
for all tasks. Section 3 shows our official results
compared to the baselines provided. Section 4
presents some conclusions.
2 Consensual information extraction
The consensual information is exploited in two
different ways in Task 1. Task 1.1 used?perceived?
post-editing effort labels as quality scores for scor-
ing and ranking in four languages pairs. These la-
bels vary within [1-3], where:
? 1 = perfect translation
? 2 = near miss translation (sentences with 2-3
errors that are easy to fix)
? 3 = very low quality sentence.
The training and test sets for each language
pair in Task 1.1 contain 3-4 translations of the
same source sentences. The language pairs are
German-English (DE-EN) with 150 source sen-
tences for test and 350 source sentences for train-
ing, English-German (EN-DE) with 150 source
sentences for test and 350 source sentences
for training, English-Spanish (EN-ES) with 150
source sentences for test and 954 source sentences
for training, and Spanish-English (ES-EN) with
150 source sentences for test and 350 source sen-
tences for training. The translations for each lan-
guage pair include a human translation and trans-
lations produced by a statistical MT (SMT) sys-
tem, a rule-based MT (RBMT) system, and a hy-
brid system (for the EN-DE and EN-ES language
pairs only).
By inspecting the source side of the training set,
we noticed that the translations were ordered per
systems, since the source file had sentences re-
peated in batches. For example, the EN-ES lan-
guage pair had 954 English sentences and 3,816
Spanish sentences. In the source file, the English
sentences were repeated in batches of 954 sen-
tences. Based on that, we assumed that in the tar-
get file each set of 954 translations in sequence
corresponded to a given MT system (or human).
For each system (human translation is consid-
ered as a system, since we do not know the or-
der of the translations), we calculate the consen-
sual information considering the other 2-3 systems
available as pseudo-references.
The quality scores for Task 1.2 and Task 1.3
were computed as HTER (Human Translation Er-
ror Rate (Snover et al., 2006)) and post-editing
time, respectively, for both scoring and ranking.
343
The datasets were a mixture of test sets from the
WMT13 and WMT12 translation shared tasks for
the EN-ES language pair only. In this case, the
consensual information was extracted by using
systems submitted to the WMT translation shared
tasks of both years. Therefore, for each source
sentence in the WMT12/13 data, all translations
produced by the participating MT systems of that
year were used as pseudo-references. The uedin
system outputs for both WMT13 and WMT12
were not considered, since the datasets in Tasks
1.2 and 1.3 were created from translations gener-
ated by this system.
2
The Asyia Toolkit
3
(Gim?enez and M`arquez,
2010) was used to extract the automatic metrics
considered as features. BLEU, TER (Snover et
al., 2006), METEOR (Banerjee and Lavie, 2005)
and ROUGE (Lin and Och, 2004) are used in
all task variants. For Tasks 1.2 and 1.3 we also
use metrics based on syntactic similarities from
shallow and dependency parser information (met-
rics SPOc(*) and DPmHWCM c1, respectively, in
Asyia). BLEU is a precision-oriented metric that
compares n-grams (n=1-4 in our case) from refer-
ence documents against n-grams of the MT out-
put, measuring how close the output of a system
is to one or more references. TER (Translation
Error Rate) measures the minimum number of ed-
its required to transform the MT output into the
closest reference document. METEOR (Metric
for Evaluation of Translation with Explicit OR-
dering) scores MT outputs by aligning them with
given references. This alignment can be done by
exact, stem, synonym and paraphrases matching
(here, exact matching was used). ROUGE is a
recall-oriented metric that measures similarity be-
tween sentences by considering the longest com-
mon n-gram statistics between a translation sen-
tence and the corresponding reference sentence.
SPOc(*) measures the lexical overlap according to
the chunk types of the syntactic realisation. The
?*? means that an average of all chunk types is
computed. DPmHWCM c1 is based on the match-
ing of head-word chains. We considered the match
of grammatical categories of only one head-word.
These consensual features are combined with
the 17 QuEst baseline features provided by the
shared task organisers.
2
WMT14 QE shared task organisers, personal communi-
cation, March 2014.
3
http://asiya.lsi.upc.edu/
3 Experiments and Results
The results reported herein are the official shared
task results, that is, they were computed using the
true scores of the test set made available by the
organisers after our submission.
For training the QE models, we used Sup-
port Vector Machines (SVM) regression algorithm
with a radial basis function (RBF) kernel with
the hyperparameters optimised via grid search.
The scikit-learn algorithm available in the QuEst
Framework
4
(Specia et al., 2013) was used for
that.
We compared the results obtained against using
only the QuEst baseline (BL) features, which is
the same system used as the official baseline for
the shared task. For the scoring variant we also
compare our results against a baseline that ?pre-
dicts? the average of the true scores of the train-
ing set as scores for each sentence of the test set
(Mean ? each sentence has the same predicted
score).
For all language pairs in Task 1.1, Table 1 shows
the average results for the scoring variant using
MAE (Mean Absolute Error) as evaluation met-
ric, while Table 2 shows the results for the ranking
variant using DeltaAvg.
The results for scoring improved over the base-
lines with the use of consensual information for
language pairs DE-EN and EN-ES. For EN-DE
and ES-EN the consensual features achieved simi-
lar results to BL. The best result for consensual in-
formation features was achieved with EN-ES (0.03
of MAE difference from BL).
For the ranking variant, the consensual informa-
tion improved the results for all language pairs.
The largest improvement from consensual-based
features was achieved for ES-EN, with a differ-
ence of 0.11 from the baseline. It is worth men-
tioning that for ES-EN our system achieved the
best ranking result in Task 1.1.
Since the results varied for different languages
pairs, we further inspected them for each language
pair. First, we looked at the true scores distribution
and realised that the first batch of translations for
each language pair was probably the human refer-
ence since the percentage of 1s ? the best quality
score ? was much higher for this system (see Fig-
ure 1 for EN-DE as an example). By using this
human translation as a reference for the other MT
systems, we computed BLEU for each sentence
4
http://www.quest.dcs.shef.ac.uk/
344
DE-EN EN-DE EN-ES ES-EN
Mean 0.67 0.68 0.46 0.58
BL 0.65 0.64 0.52 0.57
BL+Consensus 0.63 0.64 0.49 0.57
Table 1: Scoring results for Task 1.1 in terms of MAE
DE-EN EN-DE EN-ES ES-EN
BL 0.21 0.23 0.14 0.12
BL+Consensus 0.28 0.26 0.21 0.23
Table 2: Ranking results for Task 1.1 in terms of DeltaAvg
and averaged these values. The results are shown
in Table 3.
For DE-EN, EN-DE and EN-ES, the various
systems appeared to be less dissimilar in terms
of BLEU, when compared to ES-EN. For ES-EN,
the difference between the two MT systems was
higher than for other language pairs (0.12 for the
test set and 0.11 for the training set). Moreover,
for DE-EN, EN-DE and EN-ES, the difference be-
tween the averaged BLEU score of the training set
and the average BLEU score of the test set is very
small (smaller than 0.01). For ES-EN, however,
the difference between the scores for the training
and test sets was also higher (0.04 for System1 and
0.03 for System2). This can be one reason why the
consensual features did not show improvements
for this language pair. Since the systems are con-
siderably different and also there is a considerable
difference between training and test sets, the data
can be too noisy to be used as pseudo-references.
For EN-DE, the reasons for the bad perfor-
mance of consensual features are not clear. This
language pair showed the worst average quality
scores for all systems. Reasons for this can include
characteristics of German language, such as com-
pound words which are not well treated in MT, and
complex grammar. One hypothesis is that these
low BLEU scores (as Table 3 shows) introduce
noise instead of useful information for QE. An-
other difference that appeared only in EN-DE was
the distributions of the scores across the different
systems. As Figure 1 shows, System1 has a dis-
tribution considerably different from the other two
systems. For the other language pairs, the distribu-
tions across different systems were more uniform.
This difference can be another factor influencing
the results for this language pair.
Table 4 shows the results for scoring (MAE) and
Table 5 shows the results for ranking (DeltaAvg)
for Tasks 1.2 and 1.3.
Task 1.2 Task 1.3
Mean 16.93 23.34
BL 15.23 21.49
BL+Consensus 13.61 21.48
Table 4: Scoring results of Tasks 1.2 and 1.3 in
terms of MAE
Task 1.2 Task 1.3
BL 5.08 14.71
BL+Consensus 7.93 14.98
Table 5: Ranking results of Tasks 1.2 and 1.3 in
terms of DeltaAvg
For Tasks 1.2 and 1.3 the use of consensual
information only slightly improved the baseline
results for scoring. For the ranking variant,
BL+Consensus achieved better results, but only
significantly so for Task 1.2. Therefore, consen-
sual information seems useful to rank sentences
according to predicted HTER, its contribution to
predicting actual HTER is not noticeable. For
post-editing time as quality labels, the improve-
ment achieved with the use of consensual infor-
mation was marginal.
4 Conclusions
The use of consensual information of MT systems
can be useful to improve state-of-the-art results for
QE. For some scenarios, it is possible to acquire
several translations for a given source segment,
but with no additional information on the qual-
ity or type of MT systems used to produce them.
Therefore, these translations could not be used as
pseudo-references in the same way as in (Soricut
and Echihabi, 2010).
345
DE-EN EN-DE EN-ES ES-EN
Sys1 Sys2 Sys1 Sys2 Sys3 Sys1 Sys2 Sys3 Sys1 Sys2
Average BLEU
(test) 0.31 0.25 0.20 0.19 0.21 0.36 0.29 0.32 0.44 0.32
Average BLEU
(training) 0.31 0.26 0.21 0.18 0.22 0.35 0.29 0.31 0.40 0.29
Table 3: Average BLEU of systems in Task 1.1
Figure 1: Distribution of true quality scores for the EN-DE language pair
The use of several references with the hypoth-
esis that they share consensual information has
been shown useful in some settings, particularly
in Task 1.1. In others, the results were inconclu-
sive. In particular, the approach does not seem ap-
propriate for scenarios where the MT systems are
considerably different (as shown in Table 3). In
those cases, better ways to exploit consensual in-
formation need to be investigated further.
Acknowledgements: This work was supported
by the EXPERT (EU Marie Curie ITN No.
317471) project.
References
Joshua S. Albrecht and Rebecca Hwa. 2008. The
role of pseudo references in mt evaluation. In Pro-
ceedings of WMT 2008, pages 187?190, Columbus,
Ohio, USA.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the ACL 2005Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Jes?us Gim?enez and Llu??s M`arquez. 2010. Asiya: An
Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
Chin-Yew Lin and Franz J. Och. 2004. Automatic
Evaluation of Machine Translation Quality Using
Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of ACL 2004, Barcelona,
Spain.
Annie Louis and Ani Nenkova. 2013. Automatically
assessing machine summary content without a gold
standard. Computational Linguistics, 39(2):267?
300, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL
2002, pages 311?318, Philadelphia, USA.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings of
the XIV MT Summit, pages 167?174, Nice, France.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA 2006, pages 223?231.
Radu Soricut and Abdessamad Echihabi. 2010.
TrustRank: Inducing Trust in Automatic Transla-
346
tions via Ranking. In Proceedings of the ACL 2010,
pages 612?621, Uppsala, Sweden.
Radu Soricut and Sushant Narsale. 2012. Combin-
ing Quality Prediction and System Selection for Im-
proved Automatic Translation Output. In Proceed-
ings of WMT 2012, Montreal, Canada.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
WMT 2012, Montreal, Canada.
Lucia Specia, Kashif Shah, Jose G.C. de Souza, and
Trevor Cohn. 2013. Quest - a translation quality es-
timation framework. In Proceedings of WMT 2013:
System Demonstrations, ACL-2013, pages 79?84,
Sofia, Bulgaria.
347
