REES: A Large-Scale Relation and Event Extraction System 
Chinatsu Aone 
SRA International, Inc. 
4300 Fair Lakes Court 
Fairfax, VA 22033 
aonec@verdi.sra.com 
Mila Ramos-Santacruz 
SRA International, Inc. 
4300 Fair Lakes Court 
Fairfax, VA 22033 
mila@verdi.sra.com 
Abstract 
This paper reports on a large-scale, end-to- 
end relation and event extraction system. At 
present, the system extracts a total of 100 
types of relations and events, which 
represents a much wider coverage than is 
typical of extraction systems. The system 
consists of three specialized pattem-based 
tagging modules, a high-precision co- 
reference resolution module, and a 
configurable template generation module. 
We report quantitative valuation results, 
analyze the results in detail, and discuss 
future directions. 
Introduction 
One major goal of information extraction (IE) 
technology is to help users quickly identify a 
variety of relations and events and their key 
players in a large volume of documents. In 
contrast with this goal, state-of-the-art 
information extraction systems, as shown in the 
various Message Understanding Conferences 
(MUCs), extract a small number of relations and 
events. For instance, the most recent MUC, 
MUC-7, called for the extraction of 3 relations 
(person-employer, maker-product, and 
organization-location) and 1 event (spacecraft 
launches). Our goal is to develop an IE system 
which scales up to extract as many types of 
relations and events as possible with a minimum 
amount of porting effort combined with high 
accuracy. Currently, REES handles 100 types of 
relations and events, and it does so in a modular, 
configurable, and scalable manner. 
Below, Section 1 presents the ontologies of 
relations and events that we have developed. 
Section 2 describes REES' system architecture. 
Section 3 evaluates the system's performance, 
and offers a qualitative analysis of system errors. 
Section 4 discusses future directions. 
1 Relation and Event Ontologies 
As the first step in building a large-scale relation 
and event extraction system, we developed 
ontologies of the relations and events to be 
extracted. These ontologies represent a wide 
variety of domains: political, financial, business, 
military, and life-related events and relations. 
"Relations" covers what in MUC-7 are called 
Template Elements (TEs) and Template 
Relations (TRs). There are 39 types of relations. 
While MUC TE's only dealt with singular 
entities, REES extracts both singular and plural 
entities (e.g., "five executives"). The TR 
relations are shown in italic in the table below. 
Relations 
Place Relations 'Artifact Relations 
Place-Name&Aliases 
Place-Type 
Place-Subtype 
Place-Descriptor 
Place-Country 
Artifact-Name&Aliases 
Artifact-Type 
Artifact-Subtype 
Artifact-Descriptor 
Artifact-Maker 
Artifact-Owner 
Organization Relations Person Relations 
Org-Name&Aliases 
Org-Descriptor 
Org-FoundationDate 
Org-Nationality 
Org-TickerSymbol 
Org-Location 
Org-P arentOrg 
Org-Owner 
Org-Founder 
Org-StockMarket 
Person-Name&Aliases 
Person-Type 
Person-Subtype 
Person-Descriptor 
Person-Honorific 
Person-Age 
Person-PhoneNumber 
Person-Nationality 
Person-Affiliation 
Person-Sibling 
Person-Spouse 
Person-Parent 
Person-Grandparent 
76 
Person-OtherRelative 
Person-BirthPlace 
Person-BirthDate 
Table 1: Relation Ontology 
"Events" are extracted along with their event 
participants, e.g., "who did what to whom when 
and where?" For example, for a BUYING 
event, REES extracts the buyer, the artifact, the 
seller, and the time and location of the BUYING 
event. REES currently covers 61 types of 
events, as shown below. 
Events 
Vehicle Transaction 
Vehicle departs 
Vehicle arrives 
Spacecraft launch 
Vehicle crash 
Personnel Change 
Hire 
Terminate contract 
Promote 
Succeed 
Start office 
Buy artifact 
Sell artifact 
Import artifact 
Export artifact 
Give money 
Business 
Start business 
Close business 
Make artifact 
Acquire company 
Sell company 
Sue organization 
Merge company 
Crime Financial 
Sexual assault 
Steal money 
Seize drug 
Indict 
Arrest 
Try 
Convict 
Sentence 
Jail 
Currency moves up 
Currency moves down 
Stock moves up 
Stock moves down 
Stock market moves up 
Stock market moves down 
Stock index moves up 
Stock index moves down 
Political Conflict 
Nominate 
Appoint 
Elect 
Expel person 
Reach agreement 
Hold meeting 
Impose mbargo 
Topple 
Family 
Die 
Marry 
Kill 
Injure 
Hijack vehicle 
Hold hostages 
Attack target 
Fire weapon 
Weapon hit 
Invade land 
Move forces 
Retreat 
Surrender 
Evacuate 
Table 2: Event Ontology 
Figures 1 and 2 show sample relation and event 
templates. Figure 1 shows a Person-Affiliation 
relation template for "Frank Ashley, a 
spokesman for Occidental Petroleum Corp.'" 
<PERSON AFFILIATION-AP8802230207-54> := 
TYPE: PERSON AFFILIATION 
PERSON: \[TE for"Frank Ashley"\] 
ORG: \[TE for "Occidental Petroleum"\] 
Figure 1: Example of Relation Template 
Figure 2 shows an Attack Target event template 
for the sentence "an Iraqi warplane attacked the 
frigate Stark with missiles May 17, 1987. " 
<ATTACK TARGET-AP8804160078-12>: = 
i 
TYPE: CONFLICT 
SUBTYPE: ATTACK TARGET 
ATTACKER: \[TE for "an Iraqi warplane"\] 
TARGET: \[TE for "the frigate Stark"\] 
WEAPON: \[TE for "missiles"\] 
TIME: "May 17, 1987" 
PLACE: \[TE for "the gulf'\] 
COMMENT: "attacked" 
Figure 2: Example of Event Template 
2 System Architecture and Components 
Figure 3 illustrates the REES system 
architecture. REES consists of three main 
components: a tagging component (cf. Section 
2.1), a co-reference resolution module (cf. 
Section 2.2), and a template generation module 
(cf. Section 2.3). Figure 3 also illustrates that 
the user may run REES from a Graphical User 
Interface (GUI) called TemplateTool (cf. 
Section 2.4). 
2.1 Tagging Modules 
The tagging component consists of three 
modules as shown in Figure 3: NameTagger, 
NPTagger and EventTagger. Each module relies 
on the same pattern-based xtraction engine, but 
uses different sets of patterns. The NameTagger 
recognizes names of people, organizations, 
places, and artifacts (currently only vehicles). 
77  
remplateroot / /v  
- ' : . v "  . . . . . . . .  
GUI interaction 
? 
Figure 3: The REES System Architecture 
The NPTagger then takes the XML-tagged 
output of the NameTagger through two phases. 
First, it recognizes non-recursive Base Noun 
Phrase (BNP) (our specifications for BNP 
resemble those in Ramshaw and Marcus 1995). 
Second, it recognizes complex NPs for only 
the four main semantic types of NPs, i.e., 
Person, Organization, Location, and Artifact 
(vehicle, drug and weapon). It makes post- 
modifier attachment decisions only for those 
NPs that are crucial to the extraction at hand. 
During this second phase, relations which can 
be recognized locally (e.g., Age, Affiliation, 
Maker) are also recognized and stored using 
the XML attributes for the NPs. For instance, 
the XML tag for "President of XYZ Corp." 
below holds an AFFILIATION attribute with 
the ID for "XYZ Corp." 
<PNP ID="03" AFFILIATION="O4">President of 
<ENTITY ID="04">XYZ Corp.</ENTITY> 
</PNP> 
Building upon the XML output of the 
NPTagger, the EventTagger ecognizes 
events applying its lexicon-driven, 
syntactically-based generic patterns. These 
patterns tag events in the presence of at 
least one of the arguments specified in the 
lexical entry for a predicate. Subsequent 
pattems try to find additional arguments as 
well as place and time adjunct information 
for the tagged event. As an example of the 
EventTagger's generic patterns, consider 
the simplified pattern below. This pattem 
matches on an event-denoting verb that 
requires a direct object of type weapon 
(e.g., "fire a gun") 
(& 
{AND $VP {ARG2_SYN=DO} 
{ARG2_SEM=WEAPON } } 
{AND $ARTIFACT {SUBTYPE=WEAPON} })1 
The important aspect of REES is its 
declarative, lexicon-driven approach. This 
approach requires a lexicon entry for each 
event-denoting word, which is generally a 
I &=concatenation, AND=Boolean operator, $VP 
and SARTIFACT are macro references for complex 
phrases. 
71:1 
verb. The lexicon entry specifies the syntactic 
and semantic restrictions on the verb's 
arguments. For instance, the following lexicon 
entry is for the verb "attack." It indicates that 
the verb "attack" belongs to the CONFLICT 
ontology and to the ATTACK_TARGET type. 
The first argument for the verb "attack" is 
semantically an organization, location, person, 
or artifact (ARGI_SEM), and syntactically a 
subject (ARGI_SYN). The second argument 
is semantically an organization, location, 
person or artifact, and syntactically a direct 
object. The third argument is semantically a 
weapon and syntactically a prepositional 
phrase introduced by the preposition "with". 
ATTACK { { {CATEGORY VERB} 
{ONTOLOGY CONFLICT} 
{TYPE ATTACK_TARGET} 
{ARGI_SEM {ORGANIZATION LOCATION 
PERSON ARTIFACT} } 
{ARGI_SYN {SUBJECT} } 
{ARG2_SEM {ORGANIZATION LOCATION 
PERSON ARTIFACT} } 
{ARG2_SYN {DO}  
{ARG3_SEM{WEAPON}  
{ARG3_SYN {WITH} } } } 
About 50 generic event extraction patterns, 
supported by lexical information as shown 
above, allow extraction of events and their 
arguments in cases like: 
An lraqi warplane attacked the frigate Stark 
with missiles May 17, 1987. 
This generic, lexicon-driven event extraction 
approach makes REES easily portable because 
new types of events can be extracted by just 
adding new verb entries to the lexicon. No 
new patterns are required. Moreover, this 
approach allows for easy customization 
capability: a person with no knowledge of the 
pattern language would be able to configure 
the system to extract new events. 
While the tagging component is similar to 
other pattern-based IE systems (e.g., Appelt et 
al. 1995; Aone et al 1998, Yangarber and 
Grishman 1998), our EventTagger is more 
portable through a lexicon-driven approach. 
2.2 Co-reference Resolution 
After the tagging phase, REES sends the XML 
output through a rule-based co-reference 
resolution module that resolves: 
? definite noun phrases of Organization, 
Person, and Location types, and 
? singular person pronouns: he and she. 
Only "high-precision" rules are currently 
applied to selected types of anaphora. That is, 
we resolve only those cases of anaphora whose 
antecedents the module can identify with high 
confidence. For example, the pronoun rules 
look for the antecedents only within 3 
sentences, and the definite NP rules rely 
heavily on the head noun matches. Our high- 
precision approach results from our 
observation that unless the module is very 
accurate (above 80% precision), the co- 
reference module can hurt the overall 
extraction results by over-merging templates. 
2.3 Template Generation Module 
A typical template generation module is a 
hard-coded post-processing module which has 
to be written for each type of template. By 
contrast, our Template Generation module is 
unique as it uses declarative rules to generate 
and merge templates automatically so as to 
achieve portability. 
2.3.1 Declarative Template Generation 
REES outputs the extracted information in the 
form of either MUC-style templates, as 
illustrated in Figure 1 and 2, or XML. A 
crucial part of a portable, scalable system is to 
be able to output different ypes of relations 
and events without changing the template 
generation code. REES maps XML-tagged 
output of the co-reference module to templates 
using declarative template definitions, which 
specifies the template label (e.g., 
ATTACK_TARGET), XML attribute names 
(e.g., ARGUMENT l), corresponding template 
slot names (e.g., ATTACKER), and the type 
restrictions on slot values (e.g., string). 
79 
2.3.2 Event Merging 
One of the challenges of event extraction is to 
be able to recognize and merge those event 
descriptions which refer to the same event. 
The Template Generation module uses a set of 
declarative, customizable rules to merge co- 
referring events into a single event. Often, the 
rules reflect pragmatic knowledge of the world. 
For example, consider the rule below for the 
DYING event ype. This rule establishes that 
if two die events have the same subject, then 
they refer to the same event (i.e., a person 
cannot die more than once). 
{merge 
{EVENT 1 {AND {SUBTYPE DIE} {PERSON 
$foo}} 
{EVENT 2 {AND {SUBTYPE DIE} {PERSON 
$foo}}} 
2.4 Graphical User Interface (GUI) 
For some applications such as database 
population, the user may want to validate the 
system output. REES is provided with a Java- 
based Graphical User Interface that allows the 
user to run REES and display, delete, or 
modify the system output. As illustrated in 
Figure 4, the tool displays the templates on the 
bottom half of the screen, and the user can 
choose which template to display. The top half 
of the screen displays the input document with 
extracted phrases in different colors. The user 
can select any slot value, and the tool will 
highlight the portion of the input text 
responsible for the slot value. This feature is 
very useful in efficiently verifying system 
output. Once the system's output has been 
verified, the resulting templates can be saved 
and used to populate adatabase. 
3 System Evaluat ion 
The table below shows the system's recall, 
precision, and F-Measure scores for the 
training set (200 texts) and the blind set (208 
texts) from about a dozen news sources. Each 
set contains at least 3 examples of each type of 
relations and events. As we mentioned earlier, 
"relations" includes MUC-style TEs and TRs. 
Text Task Templates R P F-M 
Set in keys 
Rel. 9955 76 74 75.35 
Train Events 2525 57 74 64.57 
Rel. & 10707 74 74 73.95 
Events 
Rel. 8938 74 74 73.74 
Blind Events 2020 42 75 53.75 
Rel. & 9526 69 74 71.39 
Events 
Table 3: Evaluation Results 
The blind set F-Measure for 31 types of 
relations (73.95%) exceeded our initial goal of 
70%. While the blind set F-Measure for 61 
types of events was 53.75%, it is significant to 
note that 26 types of events achieved an F- 
Measure over 70%, and 37 types over 60% (cf. 
Table 4). For reference, though not exactly 
comparable, the best-performing MUC-7 
system achieved 87% in TE, 76% in TR, and 
51% in event extraction. 
F-M in Event types 
blind set 
90-100 2 : Buy artifact. Marry 
80-89 9 : Succeed, Merge company, Kill, 
Surrender, Arrest, Convict, Sentence, 
Nominate, Expel. 
70-79 15 : Die, Sell artif~/ct, Export 
Artifact, Hire, Start office, Make 
artifact, Acquire company, Sue 
organization, Stock Index moves 
down, Steal money, Indict, Jail, 
Vehicle crash, Elect, Hold meeting. 
Table 4: Top-performing Event Types 
80  
Figure 4: TemplateTool 
Regarding relation extraction, the difference in 
the score between the training and blind sets 
was very small. In fact, the total F-Measure on 
the blind set is less than 2 points lower than 
that of the training set. It is also interesting to 
note that for 8 of the 12 relation types where 
the F-Measure dropped more than 10 points, 
the training set includes less than 20 instances. 
In other words, there seems to be a natural 
correlation between low number of instances in 
the training set and low performance in the 
blind set. 
There was a significant drop between the 
training and blind sets in event extraction: 11 
points. We believe that the main reason is that 
the total number of events in the training set is 
fairly low: 801 instances of 61 types of events 
(an average of 13/event), where 35 of the event 
types had fewer than 10 instances. In fact, 9 
out of the 14 event types which scored lower 
than 40% F-Measure had fewer than I0 
examples. In comparison, there were 34,000 
instances of 39 types of relations in the training 
set. 
The contribution of the co-reference module is 
illustrated in the table below. Co-reference 
resolution consistently improves F-Measures 
both in training and blind sets. Its impact is 
larger in relation than event extraction. 
Text set Task Co- No co- 
reference reference 
rules rules 
Relations 75.35 72.54 
Training Events 64.57 63.62 
Relations 73.95 71.34 
& Events 
Relations 73.74 72.03 
Blind Events 53.75 53.22 
71.39 69.86 Relations 
& Events 
Table 5: Comparative results with and without 
co-reference rules 
In the next two sections, we analyze both false 
positives and false negatives. 
81 
3.1 False Positives (or Precision Errors) 
REES produced precision errors 
following cases: 
? Most of the errors were due 
in the 
to over- 
generation of templates. These are mostly 
cases of co-referring noun phrases that the 
system failed to resolve. For example: 
"Panama ... the nation ... this country.., his 
country" 
Rules for the co-reference module are still 
under development, and at present REES 
handles only limited types of plural noun 
phrase anaphora. 
Spurious events resulted from verbs in 
conditional constructions (e.g., "if ... 
then...") or from ambiguous predicates. 
For instance, "appoint" as a POLITICAL 
event vs. a PERSONNEL CHANGE 
event. 
The subject of a verb was misidentified. 
This is particularly frequent in reduced 
relative clauses. 
Kabul radio said the latest deaths brought 
to 38 the number of  people killed in the 
three car bomb explosions, 
(Wrong subject: "the number of people" as 
the KILLER instead of the victim) 
3.2 False Negatives (or Recall Errors) 
Below, we list the most frequent recall errors 
in the training set. 
? Some event arguments are mentioned with 
event nouns instead of event verbs. The 
current system does not handle noun-based 
event extraction. 
India's acquisition last month of the 
nuclear submarine from the Soviet 
Union... 
(SELLER="Soviet Union" and 
TIME="last month'" come with the noun- 
based event "acquisition.") 
? Pronouns "it" and "they," which carry 
little semantic information, are currently 
not resolved by the co-reference module. 
It also has bought hree late-1970s vintage 
ICilo class Soviet submarines and two West 
German HDW 209 subs 
(Missed BUYER=India because of 
unresolved it.) 
? Verb arguments are a conjunction of noun 
phrases. The current system does not 
handle coordination of verb arguments. 
Hezbollah killed 21 lsraelis and 43 of 
Lahad's oldiers 
(The system gets only the first object: 21 
Israelis. )
? Ellipsis cases. The current system does not 
handle ellipsis. 
The two were sentenced to five-year prison 
terms with hard labor by the state security 
court... 
(Missed PERSON_SENTENCED fill 
because of unresolved the two.) 
? The subject of the event is relatively far 
from the event-denoting verb: 
Vladislav Listyev, 38, who brought 
television interview shows in the style of 
Phil Donahue or Larry King to Russian 
viewers and pioneered hard-hitting 
television journalism in the 1980s, was 
shot in the heart by unknown assailants 
and died immediately... 
(The system missed subject Vladislav 
Listyev for attack event shot) 
? Missed ORG LOCATION relations for 
locations that are part of the organization's 
name. 
Larnaca General Hospital 
(Missed ORG_LOCATION TR for this 
and Larnaca. ) 
We asked a person who is not involved in the 
development of REES to review the event 
extraction output for the blind set. This person 
reported that: 
? In 35% of the cases where the REES 
system completely missed an event, it was 
because the lexicon was missing the 
predicate. REES's event predicate lexicon 
is rather small at present (a total of 140 
verbs for 61 event types) and is mostly 
based on the examples found in the 
training set, 
? In 30% of the cases, the subject or object 
was elliptical. The system does not 
currently handle ellipsis. 
82 
? In 25% of the cases, syntactic/semantic 
argument structures were missing from 
existing lexical entries. 
It is quite encouraging that simply adding 
additional predicates and predicate argument 
structures to the lexicon could significantly 
increase the blind set performance. 
4 Future Directions 
We believe that improving co-reference 
resolution and adding noun-based event 
extraction capability are critical to achieving 
our ultimate goal of at least 80% F-Measure 
for relations and 70% for events. 
4.1 Co-reference Resolution 
As discussed in Section 3.1 and 3.2, accurate 
co-reference r solution is crucial to improving 
the accuracy of extraction, both in terms of 
recall and precision. In particular, we 
identified two types of high-payoff co- 
reference r solution: 
? definite noun phrase resolution, especially 
plural noun phrases 
? 3 rd person neutral pronouns "it" and 
"they." 
4.2 Noun-based Event Extraction 
REES currently handles only verb-based 
events. Noun-based event extraction adds 
more complexity because: 
Nouns are often used in a generic, non- 
referential manner (e.g., "We see a merger 
as being in the consumer's interest"), and 
When referential, nouns often refer to 
verb-based events, thus requiring noun- 
verb co-reference resolution ("An F-14 
crashed shortly after takeoff... The crash"). 
However, noun-based events are crucial 
because they often introduce additional key 
information, as the underlined phrases below 
indicate: 
While Bush's meetings with prominent anti- 
apartheid leaders uch as Archbishop 
Desmond Tutu and Albertina Sisulu are 
important... 
We plan to develop a generic set of patterns for 
noun-based event extraction to complement the 
set of generic verb-based extraction patterns. 
5 Conclusions 
In this paper, we reported on a fast, portable, 
large-scale event and relation extraction system 
REES. To the best of our knowledge, this is 
the first attempt to develop an IE system which 
can extract such a wide range of relations and 
events with high accuracy. It performs 
particularly well on relation extraction, and it 
achieves 70% or higher F-Measure for 26 types 
of events already. In addition, the design of 
REES is highly portable for future addition of 
new relations and events. 
Acknowledgements 
This project would have not been possible 
without the contributions of Arcel Castillo, 
Lauren Halverson, and Sandy Shinn. Our 
thanks also to Brandon Kennedy, who 
prepared the hand-tagged data. 
References 
Aone, Chinatsu, Lauren Halverson, Tom Hampton, 
and Mila Ramos-Santacruz. 1998. "SRA: 
Description of the IE 2 System Used for MUC-7." 
In Proceedings ofthe 7thMessage Understanding 
Conference (MUC-7). 
Appelt, Douglas E., Jerry R Hobbs, John Bear, 
David Israel, Megumi Kameyama, Andy Kehler, 
David Martin, Karen Myers, and Mabry Tyson. 
1995. "SRI International FASTUS System: MUC- 
6 Test Results and Analysis." In Proceedings of 
the 6 th Message Understanding Conference 
(MUC-6). 
Ramshaw, Lance A., and Mitchell P. Marcus. 1995. 
"Text Chunking Using Transformation-Based 
Learning". In Proceedings of the 3 rd ACL 
Workshop on Very Large Corpora (WVLC95). 
Yangarber, Roman and Ralph Grishman. 1998. 
"NYU: Description of the Proteus~PET System as 
Used for MUC-7 ST." In Proceedings of the 6 th 
Message Understanding Conference (MUC-7). 
83 
 	
ff  	
   
  	
   	 	   	 
 	 
 
  
	   
 
	   Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 612?617,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discriminative Methods for Transliteration 
 
Dmitry Zelenko 
SRA International 
4300 Fair Lakes Ct. 
Fairfax VA 22033 
dmitry_zelenko@sra.com 
Chinatsu Aone 
SRA International 
4300 Fair Lakes Ct. 
Fairfax VA 22033 
chinatsu_aone@sra.com 
 
 
  
 
Abstract 
We present two discriminative methods 
for name transliteration. The methods 
correspond to local and global modeling 
approaches in modeling structured output 
spaces. Both methods do not require 
alignment of names in different lan-
guages ? their features are computed di-
rectly from the names themselves. We 
perform an experimental evaluation of 
the methods for name transliteration from 
three languages (Arabic, Korean, and 
Russian) into English, and compare the 
methods experimentally to a state-of-the-
art joint probabilistic modeling approach. 
We find that the discriminative methods 
outperform probabilistic modeling, with 
the global discriminative modeling ap-
proach achieving the best performance in 
all languages.  
1 Introduction 
Name transliteration is an important task of tran-
scribing a name from alphabet to another. For 
example, an Arabic ???????, Korean ?????, and 
Russian ???????? all correspond to English 
?William?. We address the problem of translit-
eration in the general setting: it involves trying to 
recover original English names from their tran-
scription in a foreign language, as well as finding 
an acceptable spelling of a foreign name in Eng-
lish. 
We apply name transliteration in the context 
of cross-lingual information extraction. Name 
extractors are currently available in multiple lan-
guages. Our goal is to make the extracted names 
understandable to monolingual English speakers 
by transliterating the names into English. 
The extraction context of the transliteration 
application imposes additional complexity con-
straints on the task. In particular, we aim for the 
transliteration speed to be comparable to that of 
extraction speed. Since most current extraction 
systems are fairly fast (>1 Gb of text per hour), 
the complexity requirement reduces the range of 
techniques applicable to the transliteration. More 
precisely, we cannot use WWW and the web 
count information to hone in on the right translit-
eration candidate. Instead, all relevant translitera-
tion information has to be represented within a 
compact and self-contained transliteration model. 
We present two methods for creating and ap-
plying transliteration models. In contrast to most 
previous transliteration approaches, our models 
are discriminative. Using an existing translitera-
tion dictionary D (a set of name pairs {(f,e)}), we 
learn a function that directly maps a name f from 
one language into a name e in another language. 
We do not estimate either direct conditional 
p(e|f) or reverse conditional p(f|e) or joint p(e,f) 
probability models. Furthermore, we do away 
with the notion of alignment: our transliteration 
model does not require and is not defined of in 
terms of aligned e and f. Instead, all features 
used by the model are computed directly from 
the names f and e without any need for their 
alignment. 
The two discriminative methods that we pre-
sent correspond to local and global modeling 
paradigms for solving complex learning prob-
lems with structured output spaces. In the local 
setting, we learn linear classifiers that predict a 
letter ei from the previously predicted letters 
e1?ei-1 and the original name f. In the global set-
ting, we learn a function W mapping a pair (f,e) 
into a score W(f,e)? R. The function W is linear 
in features computed from the pair (f,e). We de-
scribe the pertinent feature spaces as well as pre-
612
sent both training and decoding algorithms for 
the local and global settings. 
We perform an experimental evaluation for 
three language pairs (transliteration from Arabic, 
Korean, and Russian into English) comparing 
our methods to a joint probabilistic modeling 
approach to transliteration, which was shown to 
deliver superior performance. We show experi-
mentally that both discriminative methods out-
perform the probabilistic approach, with global 
discriminative modeling achieving the best per-
formance in all languages. 
2 Preliminaries 
Let E and F be two finite alphabets. We will use 
lowercase latin letters e, f to denote letters e?E, 
f?F, and we use bold letters e?E*, f?F* to de-
note strings in the corresponding alphabets. The 
subscripted ei, fj denote ith and jth symbols of the 
strings e and f, respectively. We use e[i,j] to rep-
resent a substring ei?ej of e. If j<i, then e[i,j] is 
an empty string ?. 
 
A transliteration model is a function mapping a 
string f to a string e. We seek to learn a translit-
eration model from a transliteration dictionary 
D={(f,e)}.  We apply the model in conjunction 
with a decoding algorithm that produces a string 
e from a string f. 
 
3 Local Transliteration Modeling 
In local transliteration modeling, we represent a 
transliteration model as a sequence of local pre-
diction problems. For each local prediction, we 
use the history h representing the context of mak-
ing a single transliteration prediction. That is, we 
predict each letter ei based on the pair h=(e[1,i-
1], f) ? H.  
Formally, we map H?E into a d-dimensional 
feature space ?: H?E ? Rd, where each 
?k(h,e)(k?{1,..,d}) corresponds to a condition 
defined in terms of the history h and the cur-
rently predicted letter e. 
In order to model string termination, we aug-
ment E with a sentinel symbol $, and we append 
$ to each e from D.  
Given a transliteration dictionary D, we trans-
form the dictionary in a set of |E| binary learning 
problems. Each learning problem Le corresponds 
to predicting a letter e?E. More precisely, for a 
pair (f[1,m],e[1,n]) ? D and i ? {1,?,n}, we 
generate a positive example ?((e[1,i-1], f),ei) for 
the learning problem Le, where e=ei, and a nega-
tive example ?((e[1,i-1], f),e) for each Le, where 
e?ei. 
Each of the learning problems is a binary clas-
sification problem and we can use our favorite 
binary classifier learning algorithm to induce a 
collection of binary classifiers {ce : e?E}. From 
most classifiers we can also obtain an estimate of 
conditional probability p(e|h) of a letter e given a 
history h. 
For decoding, in our experiments we use the 
beam search to find the sequence of letters (ap-
proximately) maximizing p(e|h).  
3.1 Local Features 
The features used in local transliteration model-
ing correspond to pairs of substrings of e and f. 
We limit the length of substrings as well as their 
relative location with respect to each other. 
? For ?((e[1,i-1], f),e), generate a feature 
for every pair of substrings (e[i-w,i-1],f[j-
v,j]), where 1?w<W(E) and  0?v<W(F) 
and |i-j| ? d(E,F). Here, W(?) is the upper 
bound on the length of strings in the corre-
sponding alphabet, and d(E,F) is the upper 
bound on the relative distance between 
substrings. 
? For ?((e[1,i-1], f[1,m]),e), generate the 
length difference feature ?len=i-m. In ex-
periments, we discretize ?len to obtain 9 
binary features: ?len=l (l?[-3,3]), ?len ? -4, 
4 ? ?len. 
? For ?((e[1,i-1], f[1,m]),e), generate a 
language modeling feature p(e| e[1,i-1]). 
? For ?((e[1,i-1], f),e) and i=1, generate 
?start? features: (^f1,^e), (^f1f2,^e). 
? For ?((e[1,i-1], f),e) and i=2, generate 
?start? features: (^f1,^e1e2), (^f1f2,^e1e2).  
? For ?((e[1,i-1], f),e) and e=$, generate 
?end? features: (fm$,e$), (fm-1fm$,e$). 
The parameters W(E), W(F), and d(E,F) are, in 
general, language-specific, and we will show, in 
the experiments, that different values of the pa-
rameters are appropriate for different languages. 
4 Global Transliteration Modeling 
In global transliteration modeling, we directly 
model the agreement function between f and e. 
We follow (Collins 2002) and consider the 
global feature representation ?: F*?E*  ? Rd. 
613
Each global feature corresponds to a condition 
on the pair of strings. The value of a feature is 
the number of times the condition holds true for 
a given pair of strings. In particular, for every 
local feature ?k((e[1,i-1], f),ei) we can define the 
corresponding global feature: 
      )),],1,1[((),( ? ?=?
i
ikk ei feef ?         (1) 
We seek a transliteration model that is linear 
in the global features. Such a transliteration 
model is represented by d-dimensional weight 
vector W? Rd. Given a string f, model applica-
tion corresponds to finding a string e such that  
? ?=
k
kkW ),(maxarg
e'
e'fe             (2) 
As with the case of local modeling, due to 
computational constraints, we use beam search 
for decoding in global transliteration modeling. 
(Collins 2002) showed how to use the Voted 
Perceptron algorithm for learning W, and we use 
it for learning the global transliteration model. 
We use beam search for decoding within the 
Voted Perceptron training as well. 
4.1 Global Features 
The global features used in local transliteration 
modeling directly correspond to local features 
described in Section 3.1.  
? For e[1,n] and f[1,m], generate a feature 
for every pair of substrings (e[i-w,i],f[j-
v,j]), where 1?w<W(E) and  0?v<W(F) 
and |i-j| ? d(E,F).  
? For e[1,n] and f[1,m], generate the 
length difference feature ?len=n-m. In ex-
periments, we discretize ?len to obtain 9 
binary features: ?len=l (l?[-3,3]), ?len ? -4, 
4 ? ?len. 
? For e[1,n], generate a language model-
ing feature (p(e))1/n. 
? For e[1,n] and f[1,m],, generate ?start? 
features: (^f1,^e1), (^f1f2,^e1), (^f1,^e1e2), 
(^f1f2,^e1e2).  
? For e[1,n] and f[1,m], generate ?end? 
features: (fm$,en$), (fm-1fm$,en). 
5 Joint Probabilistic Modeling 
We compare the discriminative approaches to a 
joint probabilistic approach to transliteration in-
troduced in recent years. 
In the joint probabilistic modeling approach, 
we estimate a probability distribution p(e,f). We 
also postulate hidden random variables a repre-
senting the alignment of e and f. An alignment a 
of e and f is a sequence a1,a2,?aL, where al =  
(e[il-wl,il],f[jl-vl,jl]), il-1+1=il-wl, and jl-1+1=jl-vl. 
Note that we allow for at most one member of a 
pair al to be an empty string. 
Given an alignment a, we define the joint 
probability p(e,f|a): 
]),[],,[()|,( l
l
lllll jvjiwipp ? ??= feafe  
We learn the probabilities p(e[il-wl,il],f[jl-vl,jl]) 
using a version of EM algorithm. In our experi-
ments, we use the Viterbi version of the EM al-
gorithm: starting from random alignments of all 
string pairs in D, we use maximum likelihood 
estimates of the above probabilities, which are 
then employed to induce the most probable 
alignments in terms of the probability estimates. 
The process is repeated until the probability es-
timates converge. 
During the decoding process, given a string f, 
we seek both a string e and an alignment a such 
that p(e,f|a) is maximized. In our experiments, 
we used beam search for decoding. 
Note that with joint probabilistic modeling use 
of a language model p(e) is not strictly neces-
sary. Yet we found out experimentally that an 
adaptive combination of the language model with 
the joint probabilistic model improves the trans-
literation performance. We thus combine the 
joint log-likelihood log(p(e,f|a)) with log(p(e)): 
score(e|f) = log(p(e,f|a))+ ?log(p(e))          (3) 
We estimate the parameter ? on a held-out set 
by generating, for each f, the set of top K=10 
candidates with respect to log(p(e,f|a)), then us-
ing (3) for re-ranking the candidates, and picking 
? to minimize the number of transliteration er-
rors among re-ranked candidates.  
6 Experiments 
We present transliteration experiments for three 
language pairs. We consider transliteration from 
Arabic, Korean, and Russian into English. For all 
language pairs, we apply the same training and 
decoding algorithms.  
6.1 Data 
The training and testing transliteration dataset 
sizes are shown in Table 1. For Arabic and Rus-
sian, we created the dataset manually by keying 
in and translating Arabic, Russian, and English 
names. For Korean, we obtained a dataset of 
transliterated names from a Korean government 
website. The dataset contained mostly foreign 
614
names transliterated into Korean. All datasets 
were randomly split into training and (blind) test-
ing parts.  
 
 Training Testing 
Arabic 935 233 
Korean 11973 1363 
Russian 545 121 
Table 1. Transliteration Data. 
 
Prior to transliteration, the Korean words of 
the Korean transliteration data were converted 
from their Hangul (syllabic) representation to 
Jamo (letter-based) representation to effectively 
reduce the alphabet size for Korean. The conver-
sion process is completely automatic (see Uni-
code Standard 3.0 for details). 
6.2 Algorithm Details 
For language modeling, we used the list of 
100,000 most frequent names downloaded from 
the US Census website. Our language model is a  
5-gram model with interpolated Good-Turing 
smoothing (Gale and Sampson 1995). 
We used the learning-to-classify version of 
Voted Perceptron for training local models 
(Freund and Schapire 1999). We used Platt?s 
method for converting scores produced by 
learned linear classifiers into probabilities (Platt 
1999). We ran both local and global Voted Per-
ceptrons for 10 iterations during training.  
6.3 Transliteration Results 
 Our discriminative transliteration models 
have a number of parameters reflecting the 
length of strings chosen in either language as 
well as the relative distance between strings. 
While we found that choice of W(E)=W(F) = 2 
always produces the best results for all of our 
languages, the distance d(E,F) may have differ-
ent optimal values for different languages.  
Table 2 presents the transliteration results for 
all languages for different values of d. Note that 
the joint probabilistic model does not depend on 
d. The results reflect the accuracy of translitera-
tion, that is, the proportion of times when the top 
English candidate produced by a transliteration 
model agreed with the correct English translitera-
tion. We note that such an exact comparison may 
be too inflexible, for many foreign names may 
have more than one legitimate English spelling. 
In future experiments, we plan to relax the re-
quirement and consider alternative variants of 
transliteration scoring (e.g., edit distance, top-N 
candidate scoring). 
 
 Local Global Prob 
Arabic (d=1) 31.33 32.61 
Arabic (d=2) 30.04 30.04 
Arabic (d=3) 26.61 27.03 
 
25.75 
 
Korean (d=1) 26.93 30.44 
Korean (d=2) 28.84 34.26 
Korean (d=3) 30.96 35.28 
 
26.93 
 
Russian (d=1) 44.62 46.28 
Russian (d=2) 38.84 41.32 
Russian (d=3) 38.01 38.01 
 
39.67 
 
Table 2. Transliteration Results for Different  
              Values of Relative Distance (d). 
 
Table 2 shows that, for all three languages, the 
discriminative methods convincingly outperform 
the joint probabilistic approach. The global dis-
criminative approach achieves the best perform-
ance in all languages. It is interesting that differ-
ent values of relative distance are optimal for 
different languages. For example, in Korean, the 
Hangul-Jamo decomposition leads to fairly re-
dundant strings of Korean characters thereby 
making transliterated characters to be relatively 
far from each other. Therefore, Korean requires a 
larger relative distance bound. In Arabic and 
Russian, on the other hand, transliterated charac-
ters are relatively close to each other, so the dis-
tance d of 1 suffices. While for Russian such a 
small distance is to be expected, we are surprised 
by such a small relative distance for Arabic. Our 
intuition was that omitting short vowels in spell-
ing names in Arabic will increase d.  
We have the following explanation of the low 
value of d for Arabic from the machine learning 
perspective: incrementing d implies adding a lot 
of extraneous features to examples, that is, in-
creasing attribute noise. Increased attribute noise 
requires a corresponding increase in the number 
of training examples to achieve adequate per-
formance. While for Korean the number of train-
ing examples is sufficient to cope with the attrib-
ute noise, the relatively small Arabic training 
sample is not. We hypothesize that with increas-
ing the number of training examples for Arabic, 
the optimal value of d will also increase. 
7 Related Work 
Most work on name transliteration adopted a 
source-channel approach (Knight and Grael 
1998; Al-Onaizan and Knight 2002a; Virga and 
Khudanpur 2003; Oh and Choi 2000) incorporat-
615
ing phonetics as an intermediate representation. 
(Al-Onaizan and Knight 2002) showed that use 
of outside linguistic resources such as WWW 
counts of transliteration candidates can greatly 
boost transliteration accuracy. (Li et al 2004) 
introduced the joint transliteration model whose 
variant augmented with adaptive re-ranking we 
used in our experiments. 
Among direct (non-source-channel) models, 
we note the work of (Gao et al 2004) on apply-
ing Maximum Entropy to English-Chinese trans-
literation, and the English-Korean transliteration 
model of (Kang and Choi 2000) based on deci-
sion trees. 
All of the above models require alignment be-
tween names. We follow the recent work of 
(Klementiev and Roth 2006) who addressed the 
problem of discovery of transliterated named 
entities from comparable corpora and suggested 
that alignment may not be necessary for translit-
eration. 
Finally, our modeling approaches follow the 
recent  work on both local classifier-based mod-
eling of complex learning problems (McCallum 
et al 2000; Punyakanok and Roth 2001), as well 
as global discriminative approaches based on 
CRFs (Lafferty et al 2001), SVM (Taskar et al 
2005), and the Perceptron algorithm (Collins 
2002) that we used in our experiments. 
 
8 Conclusions 
We presented two novel discriminative ap-
proaches to name transliteration that do not em-
ploy the notion of alignment. We showed ex-
perimentally that the approaches lead to superior 
experimental results in all languages, with the 
global discriminative modeling approach achiev-
ing the best performance. 
The results are somewhat surprising, for the 
notion of alignment seems very intuitive and use-
ful for transliteration. We will investigate 
whether similar alignment-free methodology can 
be extended to full-text translation. It will also be 
interesting to study the relationship between our 
discriminative alignment-free methods and re-
cently proposed discriminative alignment-based 
methods for transliteration and translation 
(Taskar et al 2005a; Moore 2005). 
We also showed that for name transliteration, 
global discriminative modeling is superior to 
local classifier-based discriminative modeling. 
This may have resulted from poor calibration of 
scores and probabilities produced by individual 
classifiers. We plan to further investigate the re-
lationship between the local and global ap-
proaches to complex learning problems in natural 
language. 
References 
 
Y. Al-Onaizan and K. Knight. 2002. Translating 
Named Entities Using Monolingual and Bilingual 
Resources. Proceedings of ACL. 
Y. Al-Onaizan and K. Knight. 2002a. Machine Trans-
literation of Names in Arabic Text. Proceedings of 
ACL Workshop on Computational Approaches to 
Semitic Languages. 
M. Collins. 2002. Discriminative Training for Hidden 
Markov Models: Theory and Experiments with 
Perceptron Algorithms. In Proceedings of EMNLP. 
Y. Freund and R. Shapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine 
Learning, 37, 277?296. 
W. Gale and G. Sampson. 1995. Good-Turing fre-
quency estimation without tears. Journal of Quan-
titative Linguistics 2:217-235. 
Gao Wei, Kam-Fai Wong, and Wai Lam. 2004. Pho-
neme-based transliteration of foreign names for 
OOV problem. Proceedings of the First Interna-
tional Joint Conference on Natural Language 
Processing. 
B.J. Kang and Key-Sun Choi, 2000. Automatic Trans-
literation and Back-transliteration by Decision Tree 
Learning, Proceedings of the 2nd International 
Conference on Language Resources and Evalua-
tion. 
A. Klementiev and D. Roth. 2006. Named Entity 
Transliteration and Discovery from Multilingual 
Comparable Corpora. Proceedings of ACL. 
K. Knight and J. Graehl. 1998. Machine Translitera-
tion, Computational Linguistics, 24(4). 
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic Models for 
Segmenting and Labeling Sequence Data. Proceed-
ings of the Eighteenth International Conference on 
Machine Learning. 
Li Haizhou, Zhang Min, and Su Jian. 2004. A Joint 
Source-channel Model for Machine Transliteration. 
Proceedings of ACL 2004. 
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information ex-
traction and segmentation. Proceedings of ICML. 
R. Moore. 2005. A Discriminative Framework for 
Bilingual Word Alignment. Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing. 
616
Jong-Hoon Oh and Key-Sun Choi. 2000. An English-
Korean Transliteration Model Using Pronunciation 
and Contextual Rules. Proceedings of COLING. 
J. Platt. 1999. Probabilistic outputs for support vector 
machines and comparison to regularized likelihood 
methods. In Advances in Large Margin Classi?ers. 
V. Punyakanok and D. Roth. 2001. The Use of Classi-
fiers in Sequential Inference. Proceedings of the 
Conference on Advances in Neural Information 
Processing Systems. 
B. Taskar, V. Chatalbashev, D. Koller and C. Gues-
trin. 2005. Learning Structured Prediction Models: 
A Large Margin Approach. Proceedings of Twenty 
Second International Conference on Machine 
Learning. 
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005a. A 
Discriminative Matching Approach to Word Align-
ment. Proceedings of the Conference on Empirical 
Methods in Natural Language Processing. 
P. Virga and S. Khudanpur. 2003. Transliteration of 
Proper Names in Cross-lingual Information Re-
trieval. Proceedings of ACL 2003 workshop 
MLNER. 
 
 
617
