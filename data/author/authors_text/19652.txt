Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 827?836, Dublin, Ireland, August 23-29 2014.
Identifying Emotional and Informational Support in Online Health
Communities
Prakhar Biyani
1
Cornelia Caragea
2
Prasenjit Mitra
1
John Yen
1
(1) College of Information Sciences and Technology, The Pennsylvania State University, USA
(2) Department of Computer Science and Engineering, University of North Texas, USA
pxb5080@ist.psu.edu, ccaragea@unt.edu, {pmitra,jyen}@ist.psu.edu
Abstract
A large number of online health communities exist today, helping millions of people with social
support during difficult phases of their lives when they suffer from serious diseases. Interactions
between members in these communities contain discussions on practical problems faced by peo-
ple during their illness such as depression, side-effects of medications, etc and answers to those
problems provided by other members. Analyzing these interactions can be helpful in getting
crucial information about the community such as dominant health issues, identifying sentimental
effects of interactions on individual members and identifying influential members. In this paper,
we analyze user messages of an online cancer support community, Cancer Survivors Network
(CSN), to identity the two types of social support present in them: emotional support and infor-
mational support. We model the task as a binary classification problem. We use several generic
and novel domain-specific features. Experimental results show that we achieve high classifica-
tion performance. We, then, use the classifier to predict the type of support in CSN messages
and analyze the posting behaviors of regular members and influential members in CSN in terms
of the type of support they provide in their messages. We find that influential members generally
provide more emotional support as compared to regular members in CSN.
1 Introduction
Increasingly more people turn to online health communities (OHCs) to seek social support during their
illnesses (LaCoursiere, 2001; Beaudoin and Tao, 2007). When people suffering from a serious disease
such as cancer or AIDS interact with other people who have experienced similar medical conditions,
they feel emotionally supported. In addition, through these interactions, people can obtain important
information about the disease, e.g., about various medications, symptoms, and side-effects. Although
authoritative health-related web sites contain the information they search for, obtaining this information
directly from people in OHCs adds substantial value to it. Previous studies showed that obtaining social
support in OHCs can help people feel better (Dunkel-Schetter, 1984; Maloney-Krichmar and Preece,
2005; Beaudoin and Tao, 2007; Vilhauer, 2009; Qiu et al., 2011).
As a result of online interactions in OHCs, a huge volume of user-generated content exists today
on various issues/problems related to specific diseases. This content comprises of important information
such as people?s experiences with diseases, recommendations and feedbacks about certain medications or
medical procedures, and emotional support in the form of encouragement, sympathy, and success stories.
Mining this content can prove to be very useful in obtaining crucial insights into community dynamics
such as identifying dominant health issues or the effects of social support on community members,
identifying influential members, as well as designing smart information retrieval systems for users.
In this study, we focus on an online cancer support community, the Cancer Survivors Network
1
(CSN)
of the American Cancer Society. We analyze user messages of CSN to identify the two most important
types of social support present in them: informational and emotional support (Davison et al., 2000).
Emotional support comprises of seeking or providing caring/concern, understanding, empathy, sympathy,
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://www.csn.cancer.org
827
encouragement, affirmation and validation. In contrast, informational support comprises of seeking or
providing knowledge such as advice, referrals, and suggestions (Bambina, 2007). We further explore the
relation between the type of support present in messages and users? influence in the community.
Identifying the type of support in user messages in an OHC can potentially be used in many important
applications including the following:
1. Identify influential members in OHCs: Every community has a set of members who influence
(a much larger set of) other members in the community. These members are called leaders or
influential members. The attributes of a leader in a community depends upon the community?s
nature (QA, Twitter, OHC, forum, blogsite, etc.). For example, high activity may not be an indicator
of high influence in the blogosphere (Agarwal et al., 2008) and high popularity does not necessarily
imply influence in Twitter (Romero et al., 2011). In OHCs, bringing positivity in the community and
answering members? concerns effectively by posting messages that contain certain type of support
(informational or emotional) may be an indicator of influence.
2. Improve information search in OHCs: Interactions in OHCs contain valuable information in the
form of people?s experiences, advice, referrals, pertaining to diseases, medications, side-effects,
etc. Users embed this information often in messages containing other types of support, of which
emotional support constitutes a major part. To efficiently search OHCs for this information, it must
be separated from emotional support. Hence, identifying the type of support in user messages can
help improve search and retrieval in OHCs.
3. Understand social relationships in OHCs: Emotional support is one of the dimensions of social
tie strength between members in a social network (Gilbert and Karahalios, 2009). Previous stud-
ies have shown that members receiving emotional support in OHCs are more likely to remain in
the community for a longer period of time as compared to members receiving informational sup-
port (Wang et al., 2012). Identifying emotional and informational support can help understand the
social dynamics of an OHC. For example, it would be interesting to see if there is a correlation
between the social tie strength of members and the type of support present in their interactions.
Hi X, I had a bilateral with radical on the right and prophylactic on the left. I think all you can do is
gentle exercises to strengthen your back (yoga). There are also herbal painkillers that work well too.
I just tolerate the pain and consider it a signal of my new limit and go down to rest. You want to talk,
anytime! We are all there with you.
Table 1: A user message. Sentences in grey and black fonts are informational and emotional, respectively.
We model the task of identifying the two types of supports as a binary classification problem. Specifi-
cally, we classify each sentence in a user message as containing either emotional or informational sup-
port
2
. Table 1 shows a user message containing emotional and informational supports. We use several
features computed from sentences of messages such as unigrams, part-of-speech tags, lexicon-based fea-
tures and word patterns for the classification. After building the classification model, we predict the
amounts of the two supports in all CSN messages and explore the following research question:
RQ: Do influential members of CSN post one of the two types of supports significantly more
compared to regular members?
We analyze messages posted by regular members and messages posted by certain members, identified
as influential by the CSN community managers and two staff members who monitor the contents of the
CSN on a full time basis, for the type of support (informational and emotional) present in them. Using
the classification model, we calculate the amounts of the two supports posted by influential members and
regular members and compare them across the two populations (For details, see Section 3.1).
Previous works on analyzing social support in OHCs have mainly been in the field of social sci-
ence (Eriksson and Lauri, 2000; Rodgers and Chen, 2005; H?ybye et al., 2005; Pfeil and Zaphiris, 2007;
Beaudoin and Tao, 2007; Buis, 2008; Han et al., 2011). These works used manual techniques for iden-
tifying the type of support in user messages and hence, are limited to a small number of messages as
2
Although a sentence may belong to both the classes, we did not find such cases in our data.
828
compared to the real world data. In contrast, the current work builds machine learning classifiers that
can automatically predict the type of support in messages. Also, to the best of our knowledge, there have
been no reported works on analyzing the relationship between users? influence and the type of support
present in their messages in OHCs. Next, we review related works.
2 Related Work
Many studies in social science have focused on analyzing social support in user messages of OHCs (Cour-
saris and Liu, 2009; Han et al., 2011; Pfeil and Zaphiris, 2007), finding impacts of social support on
users (Eriksson and Lauri, 2000; Rodgers and Chen, 2005; Buis, 2008; H?ybye et al., 2005), identifying
information needs of users in OHCs (Rozmovits and Ziebland, 2004), etc. Among various types of social
supports, emotional support and informational support have received major attention. In this section, we
first review social science works on analyzing online social support, discuss works on identifying the
type of social support, and, finally, compare the current problem with subjectivity analysis.
LaCoursiere (2001) presented an integrated theory conceptualizing online social support. She defined
three channels through which online social support occurs: 1) perceptual: individual feeling the need of
social support arising due to emotional states such as stress, etc, 2) cognitive: individual seeking infor-
mation about certain medical entities such as procedures, medication, etc, 3) transactional: individual
evaluating the received social support. In our case, these channels correspond to emotional support and
informational support. H?ybye et al. (2005) conducted a qualitative study to analyze the effects of online
social support by interviewing women with breast cancer who used an online support group and found
that the women were empowered by the exchanges of knowledge and experience within the online sup-
port group. Rodgers et al. (2005) conducted a longitudinal content analysis of messages of participants
in a breast cancer discussion board to analyze changes in affect/sentiment of the participants towards
breast cancer and found that a positive shift in sentiment occurred over the period of time. Pfeil and
Zaphiris (2007) analyzed messages of SeniorNet forum to extract language patterns used to provide em-
pathic support. Budak and Agrawal (2013) interviewed participants of group chats in Twitter and found
that informational support is more important than emotional support in educational Twitter chats.
All the above works used manual methods of data preparation such as interviews with users of sup-
port groups, manual coding of messages to identify emotional and informational support and performed
further qualitative and/or quantitative analyses based on that data. Since, manual methods have seri-
ous limitations in terms of scalability, the number of messages used for analysis in these studies is too
small compared to the real world data which contains millions of messages. To address these limitations,
we develop automatic methods for identifying the type of support in user messages in an online cancer
support group using machine learning. We develop a classifier that learns on a smaller set of manually
labeled messages and makes predictions on a much larger set of messages with a very high accuracy.
A recent work by Wang et al. (2012) is close to our work. They used a linear regression model to
predict the amount of informational and emotional supports present in messages of a cancer forum. For
a test message, the trained model predicts the amount of the two supports on a scale of 1 ? 7. Since a
message may contain both types of support, it is generally difficult for human annotators to assess the
amount of each support in an entire message on a particular scale for model training. In contrast, we label
each sentence as belonging to either informational or emotional support class and identify the two types
of support at sentence level in messages (using binary classification). Note that it is much easier and less
ambiguous for a human annotator to identify the type of support present in a sentence (of a message)
compared to giving a score to an entire message based on the amount of the two supports present in it.
Relationship with Subjectivity Analysis: Subjectivity analysis is an active area of research in com-
putational linguistics. It essentially deals with separating subjective parts (e.g., expressing opinion, emo-
tion, speculation and other private states of mind) from objective parts (presenting facts, verifiable infor-
mation) of a text (Wiebe et al., 1999; Biyani et al., 2012a). It has been widely used in applications like
opinion mining from product reviews (Liu, 2010), community question-answering (Li et al., 2008a; Stoy-
anov et al., 2005a; Somasundaran et al., 2007), summarization (Carenini et al., 2006; Seki et al., 2005),
and finding opinionative threads in online forums (Biyani et al., 2014; Biyani et al., 2012b; Biyani et al.,
2013a). Though the current work has some relation with subjectivity analysis in the sense that both are
829
text classification, there are important differences between the two problems. The two classes in sub-
jectivity analysis (subjective and objective) are different from the two types of support that we identify.
While emotional support is subjective in nature, informational support is not necessarily objective as it
also contains opinions of users. Also, social support in OHCs encompasses several types of supports
such as understanding, caring, concern, sympathy, empathy, knowledge about medications, etc. which
are generally not provided by users in other sites such as product reviews, question-answering sites, etc.
These differences make the two problems different in both the nature and the approaches that can be used
to address them. For example, we use certain word patterns to identify sympathy and affirmation and use
the presence of terms related to cancer medications, procedures and side-effects for computing features
for classification. These features have not been used in subjectivity classification.
3 Problem Formulation
Online health communities provide social support to its members of which emotional and informational
supports constitute a major part and have received major attention as compared to other supports such
as companionship, community building, network support, etc. (Bambina, 2007; Meier et al., 2007;
Himle et al., 1991; Wang et al., 2012; Pfeil and Zaphiris, 2007). We focus on the two supports and
follow their definitions as given by Bambina (2007) in their study of social supports expressed in a can-
cer support group. They define emotional messages as the messages that have the following supports:
caring/concern, understanding, empathy, sympathy, encouragement, affirmation and validation. Infor-
mational support is defined as providing advice, knowledge and referrals. Since a user message often
contains a mixture of these supports, we identify the two supports at sentence level. Table 1 contains a
user message with sentences marked with the type of support in them. Specifically, given a sentence s,
in a user message, we want to classify it into one of the two classes: emotional support or informational
support. We use machine learning methods for classification. After training the classifier, we use it to
predict the type of support in the sentences of user messages in CSN and address our research question
outlined in Section 1. We present the details of the features used for classification in Section 3.2.
3.1 Research Question
To address the research question (RQ), we need to compute the amounts of the two supports in the
messages of regular and influential members and then compare the two amounts. Let u denote a user and
M be the set of messages posted by her such that M = {m
1
,m
2
, ....m
p
} where p is the total number
of messages in the set M . For a message m
k
? M , we compute its emotional index, e
uk
= n
ek
/(n
k
)
where n
ek
and n
k
are the number of sentences containing emotional support and the total number of
sentences in m
k
. Since a sentence can belong to either emotional support or informational support class,
informational index of m
k
, i
uk
= 1 ? e
uk
. The overall emotional index of u (e
u
) is the average of the
emotional indices of her messages: e
u
=
1
p
?
p
k=1
e
uk
. The informational index of u, i
u
= 1 ? e
u
.
Since, the informational index can be derived from emotional index, we compute only emotional indices
for all regular and influential members and compare them between the two user populations (regular
and influential). We compute the emotional indices of regular members, E
R
, and emotional indices of
influential members, E
I
. We compare the means of the two populations of emotional indices (?
Re
and
?
Ie
) and test the null hypothesis (H
0
) and the alternate hypothesis (H
1
) as follows:
H
0
: The two populations have equal means, i.e., ?
Re
? ?
Ie
= 0.
H
1
: The two populations have significantly different means , i.e., ?
Re
? ?
Ie
6= 0.
For one of the population indices to be significantly more than the other, we should have the null
hypothesis rejected. We use one-sided t-test to conduct hypothesis testing and report the results in Sec-
tion 4.5. Next, we discuss the features used in the classification.
3.2 Features for Classification
3.2.1 Words and POS tags
Words and their part-of-speech tags capture basic lexical properties of text and have been extensively
used in text classification problems such as subjectivity classification and sentiment classification (Li et
al., 2008b; Yu and Hatzivassiloglou, 2003; Biyani et al., 2013b). We use frequency of words and their
POS tags in a sentence as features in our classification model.
830
3.2.2 Lexicon-based Features
Emotional support expresses caring, concern, sympathy, and other kinds of sentimental support whereas
informational support provides knowledge about cancer medications, cancer reports, referrals, and other
kinds of information (Bambina, 2007). Due to this difference in the nature of these supports, a sentence
expressing emotional support is likely to contain emotional words which are subjective in nature and a
sentence containing informational support is likely to have cancer-related keywords such as drug names,
names of cancer procedures, etc. To capture this difference, we use frequencies of subjective words and
cancer-related keywords as features. Specifically, we design five features to code frequencies of weak
subjective words (numWeak), strong subjective words (numStrong), cancer drugs (numDrug), side-
effects of cancer medications (numSide), and cancer procedures (numProc) respectively in a sentence.
We use the subjectivity lexicon compiled from the MPQA corpus (Stoyanov et al., 2005b) to get weak
and strong subjective words. We compile lexicon of cancer drugs
3
, and CSN staff members helped get
a list of side-effects and cancer procedures. Some of the side-effects of cancer medications are hair loss,
neuropathy, fatigue, fibrosis, etc.
3.2.3 Linguistic Features
We analyzed user messages to find patterns that are expressive of emotional and informational support.
We found that members, generally, use certain word patterns to express similar feelings. For example,
to provide affirmation and sympathy, people use positive verbs such as know, feel, understand, sense,
support, etc. in patterns <I $posVerb> and <I $aux $posVerb>, where $posVerb is a positive verb and
$aux is an auxiliary verb from the set {can, could, do, would, will, may}. Some people use ?We? instead
of ?I? in their messages to provide support such as ?we understand what you are going through?.
To take into account such cases, we use the same patterns by replacing ?I ? with ?We?. Hence, we
get four patterns for emotional support. For providing informational support, people often use patterns
such as <You $advice>, <I $opinion>, <I $aux $opinion> to provide advice and opinions. $advice
is an auxilliary verb from the set {should, must, need, might}, $opinion is an opinion verb from the
set {recommend, advise, suggest, advocate, request}, and $aux is an auxilliary verb. People also give
information about their experiences using patterns such as <I too>, <I also> and <I $pastVerb> to
tell their own experiences related to similar problems as that of the support seeker where $pastVerb is a
past tense verb such as underwent, undergone, experienced, had, found, etc. So, we get six patterns for
informational support. We design two features (IsEmPattern and IsInPattern) to encode presence (1)
or absence (0) of the two types of patterns.
For a sentence, we also use its number of words (numWords) and its type, question sentence (IsQues)
and/or exclamatory sentence (isExclaim), as features. To identify question sentences, we see if a sen-
tence starts with any of the 5W1H words (what, why, who, when, where, how) or words in the set {do,
does, did} or ends with a question mark.
4 Experiments
We now describe our data and the experimental setting, and present our results.
4.1 Data Preparation
We use data from a popular online cancer support community, the Cancer Survivors? Network (CSN),
developed and maintained by the American Cancer Society. CSN is an online community for cancer
patients, cancer survivors, their families and friends. Its features are similar to many online forums
with dynamic interactive medium such as chat rooms, discussion boards, etc. Members of CSN post
in discussion boards for seeking and sharing information about cancer related issues and for seeking
and providing emotional support. To conduct our experiments, we used user messages in the discussion
threads of the Breast Cancer sub forum of CSN that were posted between June 2000 to June 2012. Breast
cancer is the largest among all the sub-forums of CSN. A dataset of 250, 868 messages posted by 5516
users in 22, 297 discussion threads is used in this study.
To prepare the evaluation dataset for classification experiments, we randomly sampled 240 messages
from 27 discussion threads. Since, our focus is on the messages that provide support, we do not consider
3
http://www.cancer.gov/cancertopics/druginfo/alphalist
831
messages posted by thread starters in discussion threads as they seek support. We took help of three
human annotators to tag all the sentences of all the messages in one of the two support classes. First,
two annotators tagged all the sentences. The percentage agreement between them was 89%. For the
remaining 11% sentences, majority vote was taken with the help of the third annotator. Following this
tagging scheme, we obtained a total of 1066 sentences with 390 sentences in the informational support
class and 676 sentences in the emotional support class. In many cases, members only write a few words,
e.g., see you, bye, or their names at the end of a message. To deal with these situations, we filter out
sentences that have less than four words.
4.2 Experimental Protocol
We experimented with various machine learning algorithms (Naive Bayes, Support Vector Machines,
Logistic Regression, Bagging, Boosting, etc.) to conduct our classification experiments. Naive Bayes
Multinomial gave the best performance with words & POS tags features, logistic regression with lexicon-
based features and AdaBoost (with Decision Stump as the weak learner) with linguistic features. For
combining the models built on the three types of features, we used the following three methods:
1. Feature combination: Classification model built on the feature set generated by combining the
three types of features. It is denoted by All. We use Multinomial Naive Bayes for this model.
2. Average confidence: Ensemble of the three classifiers built on the three types of features respec-
tively. The final confidence of the ensemble is calculated by taking average of the confidences
outputted by the three classifiers. It is denoted by AllAvgConf.
3. Highest confidence: Similar to the AllAvgConf model but the final prediction of the ensemble is
taken as the prediction of the most confident classifier of the three classifiers. More precisely, the
prediction for an instance is given by the classifier that returns the maximum prediction confidence
for one class or the other. It is denoted by AllMostConf.
Model Precision Recall F-1
Emotional support class
Words & POS tags 0.855 0.858 0.857
Lexicon-based features 0.722 0.836 0.775
Linguistic features 0.698 0.837 0.761
All 0.862 0.861 0.862
AllAvgConf 0.848 0.893 0.87
AllMostConf 0.851 0.911 0.88
Informational support class
Words & POS tags 0.753 0.749 0.751
Lexicon-based features 0.608 0.441 0.511
Linguistic features 0.569 0.372 0.45
All 0.76 0.762 0.761
AllAvgConf 0.797 0.723 0.758
AllMostConf 0.825 0.723 0.77
Overall
Words & POS tags 0.818 0.818 0.818
Lexicon-based features 0.68 0.691 0.678
Linguistic features 0.651 0.667 0.647
All 0.825 0.825 0.825
AllAvgConf 0.829 0.830 0.83
AllMostConf 0.841 0.842 0.84
Table 2: Classification results.
We used Weka data mining toolkit (Hall et al.,
2009) to conduct classification experiments. To
evaluate the performance of our classifiers, we
used macro-averaged precision, recall and F-1
score. We use F-1 score to compare perfor-
mances of two classifiers and used 10-fold cross
validation. A naive baseline that classifies all
the instances in the majority class will have a
macro-averaged precision, recall and F-1 score
of 0.402, 0.634 and 0.492, respectively.
4.3 Classification Results
Table 2 presents the results of the support clas-
sification experiments. The table reports preci-
sion, recall and F-1 score of different classifi-
cation models for the individual classes and the
overall result. Words & POS tags are the best
performing features followed by lexicon-based
features and linguistic features. Further, com-
bining all the features (model denoted as ?All?)
improves the performance over individual fea-
ture types for both classes. We see that All-
MostConf model is the best performing of all
the models, particularly outperforming All and
AllAvgConf models. This observation suggests that the three classifiers built on the three features types
have different knowledge. For some instances, a particular classifier is more confident than the rest while
for other instances, other classifiers are more confident. Hence, we see that taking prediction of the most
832
confident classifier gives the best performance. It is interesting to note that combining the three classi-
fiers? knowledge in this fashion is more effective than simply combining all the three types of features
and train a single classifier on the combined feature set. We also note that all the models have better
performance for the emotional support class than for the informational support class. This can be caused
by the fact that there are significantly more number of instances in the former class and, hence, more
patterns to learn for the class.
4.4 Informative Features
Next, we study the importance of individual features by measuring their chi-squared statistic with respect
to the class variable. We, first, study the word features and then present rankings of the other types of
features. Figure 1 shows a cloud of top 26 most informative words. The size of a word is proportional to
its chi-squared statistic, i.e., bigger a word, more informative it is. We see that cancer specific keywords
such as herceptin, tamoxifen, chemo, dose, stage, etc and words conveying emotions such as good, hope,
glad, pain, hugs, etc are highly informative for the support classification. Since, chi-square method
gives feature ranking for the class variable and not for individual classes, we compute word rankings for
individual classes using tf ? idf scores of words. Specifically, for a term t and a class c, we compute
the term frequency of t by counting its number of occurrences in the instances (sentences) belonging
to c and multiply the term frequency with the inverse document frequency of t (calculated from the
entire corpus) to get the tf ? idf score of t for c. Using this method, we calculated tf ? idf scores
for all the words and ranked them according to their scores for the two classes. Figure 2 shows top ten
tf? idf ranked keywords for the two classes. We see that cancer-related keywords and words expressing
emotions are among the top ten most informative words for the informational and the emotional support
classes respectively. We also note that most of the top ten words for the two classes in Figure 2 are in
the word cloud of the top 26 words computed using chi-squared method except ?keep? for the emotional
support class and ?after?, ?first?, ?because? and ?cancer? for the informational class. These words have
semantic relationships with the classes. For example, ?keep? is often used by support providers in phrases
such as ?keep you in prayers?, ?may god keep you in good health?, etc to provide emotional support and
?after? and ?first? are used in the context of providing one?s own experience related to cancer procedures,
medications, etc such as ?After my first chemo, I did not feel light?.
Figure 1: Top 26 words ranked by Chi-squared
test.
Emotional support Informational support
good chemo
know after
glad radiation
news first
hope herceptin
keep treatment
prayers tamoxifen
luck cancer
hugs because
better pain
Figure 2: Top ten words for the two classes
ranked using tf-idf scheme.
We, now, discuss the ranking of non-word features: POS tags, lexicon-based and linguistic features.
The chi-squared ranking for the lexicon-based and linguistic features is as follows: numStrong > num-
Words > isExclaim > numDrug > numSide > numProc > isInPattern > isEmPattern > numWeak >
isQues. The features on the right side of > have higher rank than those on the left side. We see that the
number of strong subjective words in a sentence is the most informative feature followed by number of
words in a sentence. Among cancer-related terms, drug names are more informative than side-effects
and cancer procedures. Also, informational support word patterns are more informative than word pat-
terns capturing emotional support. It is interesting to note that isQues is the least informative feature,
maybe due to the fact that, while providing support, people generally do not ask questions. The top 5
833
most informative POS tags are: cardinal number (CD) followed by singular noun (NN), participle verb
(VBN), past tense verb (VBD) and preposition (IN).
4.5 Influence versus Support type
Figure 3: Plot showing the change in mean emo-
tional indices of influential members (pink) and reg-
ular members (blue) with the threshold on the num-
ber of messages posted by them.
CSN managers provided a list of 62 influential
members (IMs) for the breast cancer forum. IMs
posted a total of 340, 147 sentences in 85, 244
messages and regular members posted 825, 651
sentences in 165, 624 messages in the breast
cancer forum. As described in Section 3, we
conduct statistical hypothesis testing on the two
populations of emotional indices (regular mem-
bers and IMs) to understand if there is a sig-
nificant difference in their posting behaviors in
terms of providing one of the two supports more
often than the other. To test our hypothesis, we
conducted one sided t-test on the two popula-
tions. We found that the mean of emotional in-
dices of IMs (0.713) is significantly larger than
that of the regular members (0.542). We also
note that the posting behavior of regular mem-
bers in CSN follows a power law distribution with most of the members posting very few messages
(mode = 1, median = 2, mean = 30) and only a few members posting very many messages. To verify
that this behavior does not have impacts on our hypothesis testing, we conducted three more t-tests be-
tween the two populations using a threshold on the number of messages that a member has posted. We
used three threshold values on the number of messages: 1, 2, and 30 (as mode, median and mean values).
For all the three t-tests, the null hypothesis was rejected at p-value< 0.001, suggesting that IMs posted
significantly more emotional support than regular members. The values of Mean Emotional Indices cor-
responding to the three thresholds are 0.715, 0.719 and 0.746 for influential members and 0.564, 0.581
and 0.646 for regular members respectively.
In our analysis, we observed an interesting behavior. As we increased the threshold, the mean of
emotional indices also increased. To further investigate this finding, we plotted the means of emotional
indices of regular members and IMs as the function of the threshold on the number of messages posted
by them. We increased the threshold from 10 to 1000 in steps of 10. Figure 3 reports the finding. We see
that the mean of emotional indices of regular members increase with the threshold suggesting that more
active members post more emotional support as compared to the less active members. We also see that
the mean of emotional indices of IMs is higher than that of regular members for all the thresholds. These
interesting observations can be helpful in analyzing behavior of influential members in OHCs.
5 Acknowledgments
We would like to thank Iulia Bivolaru for her help with data preparation. This material is based upon
work supported by the National Science Foundation under Grant No. 0845487.
6 Conclusion and Future Work
We identified two types of social support, emotional and informational, provided in user messages of an
online cancer support community using machine learning classification models. We used three types of
features and got the best results by using ensemble of the three classifiers built on the three individual
feature types. Our models achieved strong results with over 80% F-1 score. We also found that influ-
ential members provide significantly more emotional support to the community as compared to regular
members. The finding can be helpful in identifying properties of influential members in online health
communities. In future, we plan to analyze effects of the two types of supports on OHCs? dynamics and
use it to improve information search in OHCs.
834
References
Nitin Agarwal, Huan Liu, Lei Tang, and Philip S Yu. 2008. Identifying the influential bloggers in a community.
In Proceedings of the 2008 international conference on web search and data mining, pages 207?218. ACM.
Antonina Bambina. 2007. Online social support: The interplay of social networks and computer-mediated com-
munication. Cambria press.
Christopher E Beaudoin and Chen-Chao Tao. 2007. Benefiting from social capital in online support groups: An
empirical study of cancer patients. CyberPsychology & Behavior, 10(4):587?590.
Prakhar Biyani, Sumit Bhatia, Cornelia Caragea, and Prasenjit Mitra. 2012a. Thread specific features are help-
ful for identifying subjectivity orientation of online forum threads. In Proceedings of the 24th International
Conference on Computational Linguistics, pages 295?310.
Prakhar Biyani, Cornelia Caragea, Amit Singh, and Prasenjit Mitra. 2012b. I want what i need!: analyzing
subjectivity of online forum threads. In Proceedings of the 21st ACM International Conference on Information
and Knowledge Management, pages 2495?2498.
Prakhar Biyani, Cornelia Caragea, and Prasenjit Mitra. 2013a. Predicting subjectivity orientation of online forum
threads. In Proceedings of the 14th International Conference on Intelligent Text Processing and Computational
Linguistics, pages 109?120.
Prakhar Biyani, Cornelia Caragea, Prasenjit Mitra, Chong Zhou, John Yen, Greta E Greer, and Kenneth Portier.
2013b. Co-training over domain-independent and domain-dependent features for sentiment analysis of an online
cancer support community. In ASONAM, pages 413?417. ACM.
Prakhar Biyani, Sumit Bhatia, Cornelia Caragea, and Prasenjit Mitra. 2014. Using non-lexical features for identi-
fying factual and opinionative threads in online forums. Knowledge-Based Systems.
Ceren Budak and Rakesh Agrawal. 2013. On participation in group chats on twitter. In Proceedings of the
22nd international conference on World Wide Web, pages 165?176. International World Wide Web Conferences
Steering Committee.
Lorraine R Buis. 2008. Emotional and informational support messages in an online hospice support community.
Computers Informatics Nursing, 26(6):358?367.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-document summarization of evaluative text. In EACL, pages
305?312.
Constantinos K Coursaris and Ming Liu. 2009. An analysis of social support exchanges in online hiv/aids self-help
groups. Computers in Human Behavior, 25(4):911?918.
Kathryn P Davison, James W Pennebaker, and Sally S Dickerson. 2000. Who talks? the social psychology of
illness support groups. American Psychologist, 55(2):205.
Christine Dunkel-Schetter. 1984. Social support and cancer: Findings based on patient interviews and their
implications. Journal of Social Issues, 40(4):77?98.
Elina Eriksson and Sirkka Lauri. 2000. Informational and emotional support for cancer patients relatives. Euro-
pean Journal of Cancer Care, 9(1):8?15.
Eric Gilbert and Karrie Karahalios. 2009. Predicting tie strength with social media. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, pages 211?220. ACM.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10?18.
Jeong Yeob Han, Dhavan V Shah, Eunkyung Kim, Kang Namkoong, Sun-Young Lee, Tae Joon Moon, Rich
Cleland, Q Lisa Bu, Fiona M McTavish, and David H Gustafson. 2011. Empathic exchanges in online cancer
support groups: distinguishing message expression and reception effects. Health communication, 26(2):185?
197.
David P Himle, Srinika Jayaratne, and Paul Thyness. 1991. Buffering effects of four social support types on
burnout among social workers. In Social Work Research and Abstracts, volume 27, pages 22?27. Oxford
University Press.
835
Mette Terp H?ybye, Christoffer Johansen, and Tine Tj?rnh?j-Thomsen. 2005. Online interaction. effects of
storytelling in an internet breast cancer support group. Psycho-Oncology, 14(3):211?220.
Sheryl Perreault LaCoursiere. 2001. A theory of online social support. Advances in Nursing Science, 24(1):60?77.
B. Li, Y. Liu, A. Ram, E.V. Garcia, and E. Agichtein. 2008a. Exploring question subjectivity prediction in
community qa. In SIGIR, pages 735?736. ACM.
Baoli Li, Yandong Liu, and Eugene Agichtein. 2008b. Cocqa: co-training over questions and answers with an
application to predicting question subjectivity orientation. In EMNLP ?08, pages 937?946.
B. Liu. 2010. Sentiment analysis and subjectivity. Handbook of Natural Language Processing,, pages 978?
1420085921.
Diane Maloney-Krichmar and Jenny Preece. 2005. A multilevel analysis of sociability, usability, and community
dynamics in an online health community. TOCHI, 12(2):201?232.
Andrea Meier, Elizabeth J Lyons, Gilles Frydman, Michael Forlenza, and Barbara K Rimer. 2007. How cancer
survivors provide support on cancer-related internet mailing lists. Journal of Medical Internet Research, 9(2).
Ulrike Pfeil and Panayiotis Zaphiris. 2007. Patterns of empathy in online communication. In Proceedings of the
SIGCHI conference on Human factors in computing systems, pages 919?928. ACM.
Baojun Qiu, Kang Zhao, P. Mitra, Dinghao Wu, C. Caragea, J. Yen, G.E. Greer, and K. Portier. 2011. Get online
support, feel better ? sentiment analysis and dynamics in an online cancer survivor community. In SocialComm?
11, pages 274?281.
Shelly Rodgers and Qimei Chen. 2005. Internet community group participation: Psychosocial benefits for women
with breast cancer. Journal of Computer-Mediated Communication, 10(4):00?00.
Daniel M Romero, Wojciech Galuba, Sitaram Asur, and Bernardo A Huberman. 2011. Influence and passivity in
social media. In Machine learning and knowledge discovery in databases, pages 18?33. Springer.
Linda Rozmovits and Sue Ziebland. 2004. What do patients with prostate or breast cancer want from an internet
site? a qualitative study of information needs. Patient education and counseling, 53(1):57?64.
Y. Seki, K. Eguchi, N. Kando, and M. Aono. 2005. Multi-document summarization with subjectivity analysis at
duc 2005. In DUC. Citeseer.
S. Somasundaran, T. Wilson, J. Wiebe, and V. Stoyanov. 2007. Qa with attitude: Exploiting opinion type analysis
for improving question answering in on-line discussions and the news. In ICWSM.
V. Stoyanov, C. Cardie, and J. Wiebe. 2005a. Multi-perspective question answering using the opqa corpus. In
EMNLP 2005, pages 923?930. ACL.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe. 2005b. Multi-perspective question answering using the opqa
corpus. In HLT-EMNLP ?05, HLT ?05, pages 923?930, Stroudsburg, PA, USA. ACL.
Ruvanee P Vilhauer. 2009. Perceived benefits of online support groups for women with metastatic breast cancer.
Women & health, 49(5):381?404.
Yi-Chia Wang, Robert Kraut, and John M Levine. 2012. To stay or leave?: the relationship of emotional and infor-
mational support to commitment in online health support groups. In Proceedings of the ACM 2012 conference
on Computer Supported Cooperative Work, pages 833?842. ACM.
J.M. Wiebe, R.F. Bruce, and T.P. O?Hara. 1999. Development and use of a gold-standard data set for subjectivity
classifications. In ACL, pages 246?253. ACL.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sentences. In Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 129?136. Association for Computational Linguistics.
836
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1435?1446,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Citation-Enhanced Keyphrase Extraction from Research Papers:
A Supervised Approach
Cornelia Caragea
1
, Florin Bulgarov
1
, Andreea Godea
1
, Sujatha Das Gollapalli
2
1
Computer Science and Engineering, University of North Texas, TX, USA
2
Institute for Infocomm Research, A*STAR, Singapore
ccaragea@unt.edu, FlorinBulgarov@my.unt.edu,
AndreeaGodea@my.unt.edu, gsdas@cse.psu.edu
Abstract
Given the large amounts of online textual
documents available these days, e.g., news
articles, weblogs, and scientific papers, ef-
fective methods for extracting keyphrases,
which provide a high-level topic descrip-
tion of a document, are greatly needed. In
this paper, we propose a supervised model
for keyphrase extraction from research pa-
pers, which are embedded in citation net-
works. To this end, we design novel fea-
tures based on citation network informa-
tion and use them in conjunction with tra-
ditional features for keyphrase extraction
to obtain remarkable improvements in per-
formance over strong baselines.
1 Introduction
Keyphrase extraction is the problem of automat-
ically extracting important phrases or concepts
(i.e., the essence) of a document. Keyphrases
provide a high-level topic description of a docu-
ment and are shown to be rich sources of informa-
tion for many applications such as document clas-
sification, clustering, recommendation, indexing,
searching, and summarization (Jones and Stave-
ley, 1999; Zha, 2002; Hammouda et al., 2005;
Pudota et al., 2010; Turney, 2003). Despite the
fact that keyphrase extraction has been widely re-
searched in the natural language processing com-
munity, its performance is still far from being sat-
isfactory (Hasan and Ng, 2014).
Many previous approaches to keyphrase extrac-
tion generally used only the textual content of
a target document to extract keyphrases (Hulth,
2003; Mihalcea and Tarau, 2004; Liu et al., 2010).
Recently, Wan and Xiao (2008) proposed a model
that incorporates a local neighborhood of a doc-
ument. However, their neighborhood is limited
to textually-similar documents, where the cosine
similarity between the tf-idf vectors of documents
is used to compute their similarity. We posit
that, in addition to a document?s textual content
and textually-similar neighbors, other informative
neighborhoods exist that have the potential to im-
prove keyphrase extraction. For example, in a
scholarly domain, research papers are not isolated.
Rather, they are highly inter-connected in giant ci-
tation networks, in which papers cite or are cited
by other papers. In a citation network, information
flows from one paper to another via the citation re-
lation (Shi et al., 2010). This information flow and
the influence of one paper on another are specifi-
cally captured by means of citation contexts, i.e.,
short text segments surrounding a citation?s men-
tion. These contexts are not arbitrary, but they
serve as brief summaries of a cited paper. Figure
1 illustrates this idea using a small citation net-
work of a paper by Rendle et al. (2010) that cites
(Zimdars et al., 2001), (Hu et al., 2008), (Pan and
Scholz, 2009) and (Shani et al., 2005) and is cited
by (Cheng et al., 2013). The citation mentions
and citation contexts are shown with a dashed line.
Note the high overlap between the words in con-
texts and those in the title and abstract (shown in
bold) and the author-annotated keywords.
One question that can be raised is the following:
Can we effectively exploit information available
in large inter-linked document networks in order
to improve the performance of keyphrase extrac-
tion? The research that we describe in this paper
addresses specifically this question using citation
networks of research papers as a case study. Ex-
tracting keyphrases that can accurately ?represent?
research papers is crucial to dealing with the large
numbers of research papers published during these
?big data? times. The importance of keyphrase ex-
traction from research papers is also emphasized
by the recent SemEval 2010 Shared Task on this
topic (Kim et al., 2010; Kim et al., 2013).
Our contributions. We present a supervised
1435
Figure 1: A small citation network corresponding to a paper by Rendle et al. (2010).
approach to keyphrase extraction from research
papers that, in addition to the information con-
tained in a paper itself, effectively incorporates,
in the learned models, information from the pa-
per?s local neighborhood available in citation net-
works. To this end, we design novel features for
keyphrase extraction based on citation context in-
formation and use them in conjunction with tradi-
tional features in a supervised probabilistic frame-
work. We show empirically that the proposed
models substantially outperform strong baselines
on two datasets of research papers compiled from
two machine learning conferences: the World
Wide Web and Knowledge Discovery from Data.
The rest of the paper is organized as follows:
We summarize closely related work in Section 2.
The supervised classification for keyphrase extrac-
tion is discussed in Section 3. Experiments and re-
sults are presented in Section 4, followed by con-
clusions and future directions of our work.
2 Related Work
Many approaches to keyphrase extraction have
been proposed in the literature along two lines of
research: supervised and unsupervised, using dif-
ferent types of documents including scientific ab-
stracts, newswire documents, meeting transcripts,
and webpages (Frank et al., 1999; Hulth, 2003;
Nguyen and Kan, 2007; Liu et al., 2009; Marujo
et al., 2013; Mihalcea and Tarau, 2004).
In the supervised line of research, keyphrase
extraction is formulated as a binary classification
problem, where candidate phrases are classified as
either positive (i.e., keyphrases) or negative (i.e.,
non-keyphrases) (Frank et al., 1999; Turney, 2000;
Hulth, 2003). Different feature sets and classifica-
tion algorithms gave rise to different models. For
example, Hulth (2003) used four different features
in conjunction with a bagging technique. These
features are: term frequency, collection frequency,
the relative position of the first occurrence and the
part-of-speech tag of a term. Frank et al. (1999)
developed a system called KEA that used only
two features: tf-idf (term frequency-inverse doc-
ument frequency) of a phrase and the distance of
a phrase from the beginning of a document (i.e.,
its relative position) and used them as input to
Na??ve Bayes. Nguyen and Kan (2007) extended
KEA to include features such as the distribution
of keyphrases among different sections of a re-
search paper, and the acronym status of a term. In
contrast to these works, we propose novel features
extracted from the local neighborhoods of docu-
ments available in interlinked document networks.
Medelyan et al. (2009) extended KEA as well to
integrate information from Wikipedia. In contrast,
we used only information intrinsic to our data. En-
hancing our models with Wikipedia information
would be an interesting future direction to pursue.
In the unsupervised line of research, keyphrase
extraction is formulated as a ranking problem,
where keyphrases are ranked using their tf (Barker
and Cornacchia, 2000), tf-idf (Zhang et al., 2007;
Lee and Kim, 2008; Liu et al., 2009; Tonella et al.,
2003), and term informativeness (Wu and Giles,
2013; Rennie and Jaakkola, 2005; Kireyev, 2009)
(among others). The ranking based on tf-idf has
1436
been shown to work well in practice (Liu et al.,
2009; Hasan and Ng, 2010) despite its simplicity.
Frantzi et al. (1998) combined linguistics and sta-
tistical information to extract technical terms from
documents in digital libraries. Graph-based al-
gorithms and centrality measures are also widely
used in unsupervised models. A word graph is
built for each document such that nodes corre-
spond to words and edges correspond to word as-
sociation patterns. Nodes are then ranked using
graph centrality measures such as PageRank and
its variants (Mihalcea and Tarau, 2004; Wan and
Xiao, 2008; Liu et al., 2010; Zhao et al., 2011),
HITS scores (Litvak and Last, 2008), as well as
node degree and betweenness (Boudin, 2013; Xie,
2005). Wan and Xiao (2008) were the first to
consider modeling a local neighborhood of a tar-
get document in addition to the document itself,
and applied this approach to news articles on the
Web. Their local neighborhood consists of textu-
ally similar documents, and did not capture infor-
mation contained in document networks.
Using terms from citation contexts of scientific
papers is not a new idea. It was used before in
various applications. For example, Ritchie et al.
(2006) used a combination of terms from citation
contexts and existing index terms of a paper to
improve indexing of cited papers. Citation con-
texts were also used to improve the performance of
citation recommendation systems (Kataria et al.,
2010; He et al., 2010) and to study author influ-
ence (Kataria et al., 2011). This idea of using
terms from citation contexts resembles the anal-
ysis of hyperlinks and the graph structure of the
Web, which are instrumental in Web search (Man-
ning et al., 2008). Many current Web search en-
gines build on the intuition that the anchor text
pointing to a page is a good descriptor of its con-
tent, and thus use anchor text terms as additional
index terms for a target webpage. The use of links
and anchor text was thoroughly researched for IR
tasks (Koolen and Kamps, 2010), broadening a
user?s search (Chakrabarti et al., 1998), query re-
finement (Kraft and Zien, 2004), and enriching
document representations (Metzler et al., 2009).
Moreover, citation contexts were used for scien-
tific paper summarization (Abu-Jbara and Radev,
2011; Qazvinian et al., 2010; Qazvinian and
Radev, 2008; Mei and Zhai, 2008; Lehnert et al.,
1990; Nakov et al., 2004). Among these, proba-
bly the most similar to our work is the work by
Qazvinian et al. (2010), where a set of important
keyphrases is extracted first from the citation con-
texts in which the paper to be summarized is cited
by other papers and then the ?best? subset of sen-
tences that contain such keyphrases is returned as
the summary. However, keyphrases in (Qazvinian
et al., 2010) are extracted using frequent n-grams
in a language model framework, whereas in our
work, we propose a supervised approach to a dif-
ferent task: keyphrase extraction. Mei and Zhai
(2008) used information from citation contexts to
determine what sentences of a paper are of high
impact (as measured by the influence of a target
paper on further studies of similar or related top-
ics). These sentences constitute the impact-based
summary of the paper.
Despite the use of citation contexts and anchor
text in many IR and NLP tasks, to our knowl-
edge, we are the first to propose the incorporation
of information available in citation networks for
keyphrase extraction. In our recent work (Gol-
lapalli and Caragea, 2014), we designed a fully
unsupervised graph-based algorithm that incorpo-
rates evidence from multiple sources (citation con-
texts as well as document content) in a flexible
manner to score keywords. In the current work,
we present a supervised approach to keyphrase ex-
traction from research papers that are embedded in
large citation networks, and propose novel features
that show improvement over strong supervised and
unsupervised baselines. To our knowledge, fea-
tures extracted from citation contexts have not
been used before for keyphrase extraction in a su-
pervised learning framework.
3 Problem Characterization
In citation networks, in addition to the informa-
tion contained in a paper itself, citing and cited
papers capture different aspects (e.g., topicality,
domain of study, algorithms used) about the tar-
get paper (Teufel et al., 2006), with citation con-
texts playing an instrumental role. A citation con-
text is defined as a window of n words surround-
ing a citation mention. We conjecture that cita-
tion contexts, which act as brief summaries about a
cited paper, provide additional clues in extracting
keyphrases for a target paper. These clues give rise
to the unique design of our model, called citation-
enhanced keyphrase extraction (CeKE).
3.1 Citation-enhanced Keyphrase Extraction
Our proposed citation-enhanced keyphrase extrac-
tion (CeKE) model is a supervised binary classifi-
1437
Feature Name Description
Existing features for keyphrase extraction
tf-idf term frequency * inverse document
frequency, computed from a target
paper; used in KEA
relativePos the position of the first occurrence of a
phrase divided by the total number of
tokens; used in KEA and Hulth?s methods
POS the part-of-speech tag of the phrase;
used in Hulth?s methods
Novel features - Citation Network Based
inCited if the phrase occurs in cited contexts
inCiting if the phrase occurs in citing contexts
citation tf-idf the tf-idf value of the phrase, computed
from the aggregated citation contexts
Novel features - Extensions of Existing Features
first position the distance of the first occurrence of
a phrase from the beginning of a paper
tf-idf-Over tf-idf larger than a threshold ?
firstPosUnder the distance of the first occurrence of a
phrase from the beginning of a paper is
below some value ?
Table 1: The list of features used in our model.
cation model, built on a combination of novel fea-
tures that capture information from citation con-
texts and existing features from previous works.
The features are described in ?3.1.1. CeKE classi-
fies candidate phrases as keyphrases (i.e., positive)
or non-keyphrases (i.e., negative) using Na??ve
Bayes classifiers. Positive examples for train-
ing correspond to manually annotated keyphrases
from the training research papers, whereas nega-
tive examples correspond to the remaining candi-
date phrases from these papers. The generation of
candidate phrases is explained in ?3.2.
Note that Na??ve Bayes classifies a phrase as a
keyphrase if the probability of the phrase belong-
ing to the positive class is greater than 0.5. How-
ever, the default threshold of 0.5 can be varied to
allow only high-confidence (e.g., 0.9 confidence)
phrases to be classified as keyphrases.
3.1.1 Features
We consider the following features in our model,
which are shown in Table 1. They are divided
into three categories: (1) Existing features for
keyphrase extraction include: tf-idf, i.e., the term
frequency - inverse document frequency of a can-
didate phrase, computed for each target paper;
This feature was used in KEA (Frank et al., 1999);
relative position, i.e., the position of the first oc-
currence of a phrase normalized by the length (in
the number of tokens) of the target paper; POS,
i.e., a phrase?s part-of-speech tag. If a phrase is
composed by more than one term, then the POS
will contain the tags of all terms. The relative posi-
tion was used in both KEA and Hulth (2003), and
POS was used in Hulth; (2) Novel features - Cita-
tion Network Based include: inCited and inCiting,
i.e., boolean features that are true if the candidate
phrase occurs in cited and citing contexts, respec-
tively. We differentiate between cited and citing
contexts for a paper: let d be a target paper and C
be a citation network such that d ? C. A cited con-
text for d is a context in which d is cited by some
paper d
i
in C. A citing context for d is a context
in which d is citing some paper d
j
in C. If a paper
is cited in multiple contexts by another paper, the
contexts are aggregated into a single one; citation
tf-idf, i.e., the tf-idf score of each phrase computed
from the citation contexts; (3) Novel features - Ex-
tend Other Existing Features include: first position
of a candidate phrase, i.e., the distance of the first
occurrence of a phrase from the beginning of a pa-
per; this is similar to relative position except that
it does not consider the length of a paper; tf-idf-
Over, i.e., a boolean feature, which is true if the
tf-idf of a candidate phrase is greater than a thresh-
old ?, and firstPosUnder, also a boolean feature,
which is true if the distance of the first occurrence
of a phrase from the beginning of a target paper is
below some value ?. This feature is similar to the
feature is-in-title, used previously in the literature
(Litvak and Last, 2008; Jiang et al., 2009). Both
tf-idf and citation tf-idf features showed better re-
sults when each tf was divided by the maximum tf
values from the target paper or citation contexts.
The tf-idf features have high values for phrases
that are frequent in a paper or citation contexts,
but are less frequent in collection and have low
values for phrases with high collection frequency.
We computed the idf component from each col-
lection used in experiments. Phrases that occur in
cited and citing contexts as well as early in a paper
are likely to be keyphrases since: (1) they capture
some aspect about the target paper and (2) authors
start to describe their problem upfront.
3.2 Generating Candidate Phrases
We generate candidate phrases from the textual
content of a target paper by applying parts-of-
1438
Dataset Num. (#) Average Average Average #uni- #bi- #tri-
Papers Cited Ctx. Citing Ctx. Keyphrases grams grams grams
WWW 425 15.45 18.78 4.87 680 1036 247
KDD 365 12.69 19.74 4.03 363 853 189
Table 2: A summary of our datasets.
speech filters. Consistent with previous works
(Hulth, 2003; Mihalcea and Tarau, 2004; Liu
et al., 2010; Wan and Xiao, 2008), only nouns
and adjectives are retained to form candidate
phrases. The generation process consists of two
steps. First, using the NLP Stanford part of speech
tagger, we preprocess each document and keep
only the nouns and adjectives corresponding to
{NN,NNS,NNP,NNPS, JJ}. We apply the
Porter stemmer on every word. The position of
each word is kept consistent with the initial state
of the document before any word removal is made.
Second, words extracted in the first step that
have contiguous positions in a document are con-
catenated into n-grams. We used unigrams, bi-
grams, and trigrams (n = 1, 2, 3) as candidate
phrases for classification. Similar to Wan and Xiao
(2008), we eliminated phrases that end with an ad-
jective and the unigrams that are adjectives.
4 Experiments and Results
In this section, we first describe our datasets and
then present experimental design and results.
4.1 Datasets
In order to test the performance of our proposed
approach, we built our own datasets since citation-
enhanced evaluation benchmarks are not available
for keyphrase extraction tasks. In particular, we
compiled two datasets consisting of research pa-
pers from two top-tier machine learning confer-
ences: World Wide Web (WWW) and Knowledge
Discovery and Data Mining (KDD). Our choice
for WWW and KDD was motivated by the avail-
ability of author-input keywords for each paper,
which we used as gold-standard for evaluation.
Using the CiteSeer
x
digital library
1
, we re-
trieved the papers published in WWW and KDD
(available in CiteSeer
x
), and their citation network
information, i.e., their cited and citing contexts.
Since our goal is to study the impact of citation
network information on extracting keyphrases, a
paper was considered for analysis if it had at least
1
http://citeseerx.ist.psu.edu/
one cited and one citing context. For each paper,
we used: the title and abstract (referred to as the
target paper) and its citation contexts. The rea-
son for not considering the entire text of a paper
is that scientific papers contain details, e.g., dis-
cussion of results, experimental design, notation,
that do not provide additional benefits for extract-
ing keyphrases. Hence, similar to (Hulth, 2003;
Mihalcea and Tarau, 2004; Liu et al., 2009), we
did not use the entire text of a paper. However, ex-
tracting keyphrases from sections such as ?intro-
duction? or ?conclusion? needs further attention.
From the pdf of each paper, we extracted the
author-input keyphrases. An analysis of these
keyphrases revealed that generally authors de-
scribe their work using, almost half of the time,
bigrams, followed by unigrams and only rarely us-
ing trigrams (or higher n-grams). A summary of
our datasets that contains the number of papers,
the average number of cited and citing contexts
per paper, the average number of keyphrases per
paper, and the number of unigrams, bigrams and
trigrams, in each collection, is shown in Table 2.
Consistent with previous works (Frank et al.,
1999; Hulth, 2003), the positive and negative ex-
amples in our datasets correspond to candidate
phrases that consist of up to three tokens. The
positive examples are candidate phrases that have
a match in the author-input keyphrases, whereas
negative examples correspond to the remaining
candidate phrases.
Context lengths. In CiteSeer
x
, citation con-
texts have about 50 words on each side of a citation
mention. A previous study by Ritchie et al. (2008)
shows that a fixed window length of about 100
words around a citation mention is generally effec-
tive for information retrieval tasks. For this reason,
we used the contexts provided by CiteSeer
x
di-
rectly. However, in future, it would be interesting
to incorporate in our models more sophisticated
approaches to identifying the text that is relevant
to a target citation (Abu-Jbara and Radev, 2012;
Teufel, 1999) and study the influence of context
lengths on the quality of extracted keyphrase.
1439
WWW KDD
Method Precision Recall F1-score Precision Recall F1-score
Citation - Enhanced (CeKE) 0.227 0.386 0.284 0.213 0.413 0.280
Hulth - n-gram with tags 0.165 0.107 0.129 0.206 0.151 0.172
KEA 0.210 0.146 0.168 0.178 0.124 0.145
Table 3: The comparison of CeKE with supervised approaches on WWW and KDD collections.
4.2 Experimental Design
Our experiments are designed around the follow-
ing research questions:
1. How does the performance of citation-
enhanced keyphrase extraction (CeKE) com-
pare with the performance of existing super-
vised models that use only information intrin-
sic to the data and what are the most informa-
tive features for classification? We compared
CeKE?s performance with that of classifiers
trained on KEA features only and Hulth?s
features only and present a ranking of fea-
tures based on information gain.
2. How do supervised models that integrate ci-
tation network information compare with re-
cent unsupervised models? Since recent un-
supervised approaches are becoming compet-
itive with supervised approaches (Hasan and
Ng, 2014), we also compared CeKE with
unsupervised ranking of candidate phrases
by TF-IDF, TextRank (Mihalcea and Ta-
rau, 2004) and ExpandRank (Wan and Xiao,
2008). For unsupervised, we considered top
5 and top 10 ranked phrases when computing
?@5? and ?@10? measures.
3. How well does our proposed model perform
in the absence of either cited or citing con-
texts? Since newly published scientific pa-
pers are not cited by many other papers, e.g.,
due to their recency, no cited contexts are
available. We studied the quality of predicted
keyphrases when either cited or citing con-
texts are missing. For this, we compared
the performance of models trained using both
cited and citing contexts with that of models
that use either cited or citing contexts.
Evaluation metrics. To evaluate the perfor-
mance of CeKE, we used the following metrics:
precision, recall and F1-score for the positive class
since correct identification of keyphrases is of
most interest. These metrics were widely used in
previous works (Hulth, 2003; Mihalcea and Tarau,
2004; Wan and Xiao, 2008; Hasan and Ng, 2010).
The reported values are averaged in 10-fold cross-
validation experiments, where folds were created
at document level and candidate phrases were ex-
tracted from the documents in each fold to form
the training and test sets. In all experiments, we
used Na??ve Bayes and their Weka implementa-
tion
2
. However, any probabilistic classifier that re-
turns a posterior probability of the class given an
example, can be used with our features.
The ? parameter was set to the (title and ab-
stract) tf-idf averaged over the entire collection,
while ? was set to 20. These values were esti-
mated on a validation set sampled from training.
4.3 Results and Discussion
The impact of citation network information on the
keyphrase extraction task. Table 3 shows the re-
sults of the comparison of CeKE with two su-
pervised approaches, KEA and Hulth?s approach.
The features used in KEA are the tf-idf and the
relative position of a candidate phrase, whereas
those used in Hulth?s approach are tf, cf (i.e., col-
lection frequency), relative position and POS tags.
CeKE is trained using all features from Table 1.
Among the three methods for candidate phrase
formation proposed in Hulth (2003), i.e., n-grams,
NP-chunks, and POS Tag Patterns, our Hulth?s im-
plementation is based on n-grams since this gives
the best results among all methods (see (Hulth,
2003) for more details). In addition, the n-grams
method is the most similar to our candidate phrase
generation and that used in Frank et al. (1999).
As can be seen from Table 3, CeKE outperforms
KEA and Hulth?s approach in terms of all perfor-
mance measures on both WWW and KDD, with
a substantial improvement in recall over both ap-
proaches. For example, on WWW, CeKE achieves
a recall of 0.386 compared to 0.146 and 0.107 re-
call achieved by KEA and Hulth?s, respectively.
2
http://www.cs.waikato.ac.nz/ml/weka/
1440
WWW KDD
Method Precision Recall F1-score Precision Recall F1-score
Citation - Enhanced (CeKE) 0.227 0.386 0.284 0.213 0.413 0.280
TF-IDF - Top 5 0.089 0.100 0.094 0.083 0.102 0.092
TF-IDF - Top 10 0.075 0.169 0.104 0.080 0.203 0.115
TextRank - Top 5 0.058 0.071 0.062 0.051 0.065 0.056
TextRank - Top 10 0.062 0.133 0.081 0.053 0.127 0.072
ExpandRank - 1 neigh. - Top 5 0.088 0.109 0.095 0.077 0.103 0.086
ExpandRank - 1 neigh. - Top 10 0.078 0.165 0.101 0.071 0.177 0.098
ExpandRank - 5 neigh. - Top 5 0.093 0.113 0.100 0.080 0.108 0.090
ExpandRank - 5 neigh. - Top 10 0.080 0.172 0.104 0.068 0.172 0.095
ExpandRank - 10 neigh. - Top 5 0.094 0.113 0.100 0.077 0.103 0.086
ExpandRank - 10 neigh. - Top 10 0.076 0.162 0.099 0.065 0.164 0.091
Table 5: The comparison of CeKE with unsupervised approaches on WWW and KDD collections.
Rank Feature IG Score
1 abstract tf-idf 0.0234
2 first position 0.0188
3 citation tf-idf 0.0177
4 relativePos 0.0154
5 firstPosUnder 0.0148
6 inCiting 0.0129
7 inCited 0.0098
8 POS 0.0085
9 tf-idf-Over 0.0078
Table 4: Feature ranking by Info Gain on WWW.
Although there are only small variations from
KEA to Hulth?s approach, KEA performs better
on WWW, but worse on KDD compared with
Hulth?s approach. In contrast, CeKE shows con-
sistent improvement over the two approaches on
both datasets, hence, effectively making use of the
information available in the citation network.
In order to understand the importance of our
features, we ranked them based on Information
Gain (IG), which determines how informative a
feature is with respect to the class variable. Table
4 shows the features ranked in decreasing order of
their IG scores for WWW. As can be seen from
the table, tf-idf and citation tf-idf are both highly
ranked, first and third, respectively, illustrating
that they contain significant information in pre-
dicting keyphrases. The first position of a phrase
is also of great impact. This is consistent with the
fact that almost half of the identified keywords and
about 20% of the annotated keyphrases appear in
title. Similar ranking is obtained on KDD.
The comparison of CeKE with unsupervised
state-of-the-art models. Table 5 shows the re-
sults of the comparison of CeKE with three unsu-
pervised ranking approaches: TF-IDF (Tonella et
al., 2003), TextRank (Mihalcea and Tarau, 2004),
and ExpandRank (Wan and Xiao, 2008). TF-IDF
and TextRank use information only from the target
paper, whereas ExpandRank uses a small textual
neighborhood in addition to the target paper. Note
that, for all unsupervised methods, we used Porter
stemmer and the same candidate phrase generation
as in CeKE, as explained in ?3.2.
For TF-IDF, we first tokenized the target paper
and computed the score for each word, and then
formed phrases and summed up the score of every
word within a phrase. For TextRank, we built an
undirected graph for each paper, where the nodes
correspond to words in the target paper and edges
are drawn between two words that occur next to
each other in the text, i.e., the window size is 2.
For ExpandRank, we built an undirected graph
for each paper and its local textual neighborhood.
Again, nodes correspond to words in the target pa-
per and its textually similar papers and edges are
drawn between two words that occur within a win-
dow of 10 words from each other in the text, i.e.,
the window size is 10. We performed experiments
with 1, 5, and 10 textually-similar neighbors. For
TextRank and ExpandRank, we summed up the
scores of words within a phrase as in TF-IDF.
1441
WWW KDD
Method Precision Recall F1-score Precision Recall F1-score
CeKE - Both contexts 0.227 0.386 0.284 0.213 0.413 0.280
CeKE - Only cited contexts 0.222 0.286 0.247 0.192 0.300 0.233
CeKE - Only citing contexts 0.203 0.342 0.253 0.195 0.351 0.250
Table 6: Results of CeKE using both contexts and using with only cited or citing contexts.
For each unsupervised method, we computed
results for top 5 and top 10 ranked phrases. As
can be seen from Table 5, CeKE substantially out-
performs all the other methods for our domain of
study, i.e., papers from WWW and KDD, illustrat-
ing again that the citation network of a paper con-
tains important information that can show remark-
able benefits for keyphrase extraction. Among all
unsupervised methods, ExpandRank with fewer
textual similar neighbor (one or five) performs the
best. This is generally consistent with the results
shown in (Wan and Xiao, 2008) for news articles.
The effect of cited and citing contexts informa-
tion on models? performance. Table 6 shows the
precision, recall and F-score values for some vari-
ations of our method when: (1) all the citation con-
texts for a paper are used, (2) only cited contexts
are used, (3) only citing contexts are used. The
motivation behind this experiment was to deter-
mine how well the proposed model would perform
on newly published research papers that have not
accumulated citations yet. As shown in the table,
there is no substantial difference in terms of preci-
sion between CeKE models that use only cited or
only citing contexts, although the recall is substan-
tially higher for the case when only citing contexts
are used, for both WWW and KDD. The CeKE
that uses both citing and cited contexts achieves
a substantially higher recall and only a slightly
higher precision compared with the cases when
only one context type is available. The fact that
the citing context information provides a slight im-
provement in performance over cited contexts is
consistent with the intuition that when citing a pa-
per y, an author generally summarizes the main
ideas from y using important words from a target
paper x, making the citing contexts to have higher
overlap with words from x. In turn, a paper z that
cites x may use paraphrasing to summarize ideas
from x with words more similar to those from z.
Note that the results of all above experiments
are statistically significant at p-values ? 0.05, us-
ing a paired t-test on F1-scores.
4.4 Anecdotal Evidence
In order to check the transferability of our pro-
posed approach to other research fields, e.g., nat-
ural language processing, it would be interesting
to use our trained classifiers on WWW and KDD
collections and evaluate them on new collections
such as NLP related collections. Since NLP col-
lections annotated with keyphrases are not avail-
able, we show anecdotal evidence for only one pa-
per. We selected for this task an award winning pa-
per published in the EMNLP conference. The pa-
per?s title is ?Unsupervised semantic parsing? and
has won the Best Paper Award in the year 2009
(Poon and Domingos, 2009). In order for our al-
gorithm to work, we gathered from the Web (using
Google Scholar) all the cited and citing contexts
that were available (49 cited contexts and 30 cit-
ing contexts). We manually annotated the target
paper with keyphrases. The title, abstract and all
the contexts were POS tagged using the NLP Stan-
ford tool. We then trained a classifier on the fea-
tures shown in Table 1, on both WWW and KDD
datasets combined. The trained classifier was used
to make predictions, which were compared against
the manually annotated keyphrases. The results
are shown in Figure 2, which displays the title and
abstract of the paper and the predicted keyphrases.
Candidate phrases that are predicted as keyphrases
are marked in red bold, those predicted as non-
keyphrases are shown in black, while the filtered
out words are shown in light gray.
We tuned our classifier trained on WWW and
KDD to return as keyphrases only those that had
an extremely high probability to be keyphrases.
Specifically, we used a threshold of 0.985. The
probability of each returned keyphrase (which is
above 0.985) is shown in the upper right corner
of a keyphrase. Human annotated keyphrases are
marked in italic, under the figure. There is a clear
match between the predictions and the human an-
notations. It is also possible to extract more or
less keyphrases simply by adjusting the threshold
1442
Unsupervised Semantic Parsing
0.997
We present the first unsupervised approach to the problem of learning a semantic parser
1.000
, using
Markov logic
0.991
. Our USP system
0.985
transforms dependency trees into quasi-logical forms, recur-
sively induces lambda forms from these, and clusters them to abstract away syntactic variations of the
same meaning. The MAP semantic parse
1.000
of a sentence is obtained by recursively assigning its
parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a
knowledge base from biomedical abstracts and answer questions. USP
1.000
substantially outperforms
TextRunner, DIRT and an informed baseline on both precision and recall on this task.
Human annotated labels: unsupervised semantic parsing, Markov logic, USP system
Figure 2: The title and abstract of an EMNLP paper by Poon and Domingos (2009) and human annotated
keyphrases for the paper. Black words represent candidate phrases. Red bold words represent predicted
keyphrases. The numbers above predicted keyphrases are probabilities for the positive class assignment.
on the probability output by Na??ve Bayes. For ex-
ample, if we decrease the threshold to 0.920 the
following phrases would be added to the returned
set of keyphrases: dependency trees, quasi-logical
forms and unsupervised approach.
Another interesting aspect is the frequency of
occurrence of the predicted keyphrases in the cited
and citing contexts. Table 7 shows the term-
frequency of every predicted keyphrase within the
citation network. For example, the phrase seman-
tic parser appears in 29 cited contexts and 26 cit-
ing contexts. The reason for the higher cited con-
text frequency is not necessarily due to impor-
tance, but could be due to the larger number of
cited vs. citing contexts for this paper (49 vs. 30).
The high rate of keyphrases within the citation net-
work validates our assumption of the importance
of citation networks for keyphrase extraction.
Finally, we performed the same experiment
with Hulth?s and KEA methods. While the clas-
sifier trained on Hulth?s features did not identify
any keyphrases, KEA managed to identify several
good ones (e.g., USP, semantic parser), but left
out some important ones (e.g., Markov logic, un-
supervised). Moreover, the keyphrases predicted
by KEA have a lower confidence. For this reason,
lowering the probability threshold would result in
selecting other bad keyphrases.
4.5 Error analysis
We performed an error analysis and found that
candidate phrases are predicted as keyphrases
(FPs), although they do not appear in gold stan-
dard, i.e., the set of author-input keyphrases, in
cases when: 1) a more general terms is used to
describe an important concept of a document, e.g.,
Keyphrase #cited c. #citing c.
semantic parser 29 26
USP 31 10
Markov logic 15 10
unsupervised semantic parsing 12 1
USP system 3 2
Table 7: Frequency of the predicted keyphrases in
cited / citing contexts.
co-authorship prediction represented as link pre-
diction or Twitter platform represented as social
media; 2) an important concept is omitted (either
intentionally or forgetfully) from the set of author-
input keyphrases.
Hence, while we believe that authors are the
best keyphrase annotators for their own work,
there are cases when important keyphrases are
overlooked or expressed in different ways, possi-
bly due to the human subjective nature in choosing
important keyphrases that describe a document.
To this end, a limitation of our model is the use of
a single gold standard keyphrase annotation. In fu-
ture, we plan to acquire several human keyphrase
annotation sets for our datasets and test the perfor-
mance of the proposed approach on these annota-
tion sets, independently and in combination.
Keyphrases that appear in gold standard are
predicted as non-keyphrases (FNs) when: 1) a
keyphrase is infrequent in abstract; 2) its distance
from the beginning of a document is large; 3) does
not occur or occurs only rarely in a document?s
citation contexts, either citing or cited contexts.
Examples of FNs are model/algorithm/approach
names, e.g., random walks, that appear in sen-
tences such as: ?In this paper, we model the prob-
lem [? ? ?] by using random walks.? Although such
1443
a sentence may appear further away from the be-
ginning of an abstract, it contains significant in-
formation from the point of view of keyphrase
extraction. The design of patters such as <
by using $model > or < uses $model > could
lead to improved classification performance.
Further investigation of FPs and FNs will be
considered in future work. We believe that a bet-
ter understanding of errors has the potential to ad-
vance state-of-the-art for keyphrase extraction.
5 Conclusion and Future Directions
In this paper, we presented a supervised classifi-
cation model for keyphrase extraction from scien-
tific research papers that are embedded in citation
networks. More precisely, we designed novel fea-
tures that take into account citation network in-
formation for building supervised models for the
classification of candidate phrases as keyphrases
or non-keyphrases. The results of our experi-
ments show that the proposed supervised model
trained on a combination of citation-based features
and existing features for keyphrase extraction per-
forms substantially better compared with state-of-
the-art supervised and unsupervised models.
Although we illustrated the benefits of leverag-
ing inter-linked document networks for keyphrase
extraction from scientific documents, the proposed
model can be extended to other types of docu-
ments such as webpages, emails, and weblogs. For
example, the anchor text on hyperlinks in weblogs
can be seen as the ?citation context?.
Another aspect of future work would be the
use of external sources to better identify candi-
date phrases. For example, the use of Wikipedia
was studied before to check if the concept behind
a phrase has its own Wikipedia page (Medelyan
et al., 2009). Furthermore, since citations occur
in all sciences, extensions of the proposed model
to other domains, e.g., Biology and Chemistry,
and other applications, e.g., document summariza-
tion, similar to Mihalcea and Tarau (2004) and
Qazvinian et al. (2010), are of particular interest.
Acknowledgments
We are grateful to Dr. C. Lee Giles for the
CiteSeerX data, which allowed the generation of
citation graphs. We also thank Kishore Nep-
palli and Juan Fern?andez-Ram??zer for their help
with various dataset construction tasks. We very
much appreciate the constructive feedback from
our anonymous reviewers. This research was
supported in part by NSF awards #1353418 and
#1423337 to Cornelia Caragea. Any opinions,
findings, and conclusions expressed here are those
of the authors and do not necessarily reflect the
views of NSF.
References
Amjad Abu-Jbara and Dragomir Radev. 2011. Co-
herent citation-based summarization of scientific pa-
pers. In Proc. of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, HLT ?11, pages 500?509.
Amjad Abu-Jbara and Dragomir Radev. 2012. Ref-
erence scope identification in citing sentences. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 80?90.
Ken Barker and Nadia Cornacchia. 2000. Using Noun
Phrase Heads to Extract Document Keyphrases. In
Proceedings of the 13th Biennial Conference of the
Canadian Society on Computational Studies of Intel-
ligence: Advances in Artificial Intelligence, AI ?00,
pages 40?52, London, UK, UK. Springer-Verlag.
Florian Boudin. 2013. A comparison of centrality
measures for graph-based keyphrase extraction. In
Proc. of IJCNLP, pages 834?838, Nagoya, Japan.
Soumen Chakrabarti, Byron Dom, Prabhakar Ragha-
van, Sridhar Rajagopalan, David Gibson, and Jon
Kleinberg. 1998. Automatic resource compilation
by analyzing hyperlink structure and associated text.
Comput. Netw. ISDN Syst., 30(1-7):65?74, April.
Chen Cheng, Haiqin Yang, Michael R. Lyu, and Irwin
King. 2013. Where you like to go next: Succes-
sive point-of-interest recommendation. In Proc. of
IJCAI?13, pages 2605?2611, Beijing, China.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence - Volume 2, IJCAI?99, pages
668?673, Stockholm, Sweden.
Katerina T. Frantzi, Sophia Ananiadou, and Jun-ichi
Tsujii. 1998. The c-value/nc-value method of au-
tomatic recognition for multi-word terms. In Proc.
of ECDL ?98, pages 585?604.
Sujatha Das Gollapalli and Cornelia Caragea. 2014.
Extracting keyphrases from research papers using
citation networks. In Proceedings of the 28th
AAAI Conference on Artificial Intelligence (AAAI-
14), Qu?ebec City, Qu?ebec, Canada.
Khaled M. Hammouda, Diego N. Matute, and Mo-
hamed S. Kamel. 2005. Corephrase: Keyphrase ex-
traction for document clustering. In Proc. of the 4th
1444
International Conference on Machine Learning and
Data Mining in Pattern Recognition, MLDM?05,
pages 265?274, Leipzig, Germany.
Kazi Saidul Hasan and Vincent Ng. 2010. Conun-
drums in Unsupervised Keyphrase Extraction: Mak-
ing Sense of the State-of-the-Art. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 365?373.
Kazi Saidul Hasan and Vincent Ng. 2014. Automatic
keyphrase extraction: A survey of the state of the art.
In Proc. of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Qi He, Jian Pei, Daniel Kifer, Prasenjit Mitra, and Lee
Giles. 2010. Context-aware citation recommenda-
tion. In Proc. of WWW ?10, pages 421?430, Raleigh,
North Carolina, USA.
Yifan Hu, Yehuda Koren, and Chris Volinsky.
2008. Collaborative filtering for implicit feedback
datasets. In Proc. of the 8th IEEE Intl. Conference
on Data Mining, ICDM ?08, pages 263?272.
Anette Hulth. 2003. Improved Automatic Keyword
Extraction Given More Linguistic Knowledge. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, EMNLP
?03, pages 216?223.
Xin Jiang, Yunhua Hu, and Hang Li. 2009. A Ranking
Approach to Keyphrase Extraction. In Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 756?757. ACM.
Steve Jones and Mark S. Staveley. 1999. Phrasier:
A system for interactive document retrieval using
keyphrases. In Proceedings of SIGIR ?99, pages
160?167, Berkeley, California, USA.
Saurabh Kataria, Prasenjit Mitra, and Sumit Bhatia.
2010. Utilizing context in generative bayesian mod-
els for linked corpus. In In Proc. of AAAI ?10, pages
1340?1345, Atlanta, Georgia, USA.
Saurabh Kataria, Prasenjit Mitra, Cornelia Caragea,
and C. Lee Giles. 2011. Context sensitive topic
models for author influence in document networks.
In Proceedings of IJCAI?11, pages 2274?2280,
Barcelona, Catalonia, Spain.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. SemEval-2010 Task 5:
Automatic Keyphrase Extraction from Scientific Ar-
ticles. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ?10, pages
21?26, Los Angeles, California.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2013. Automatic keyphrase
extraction from scientific articles. Language Re-
sources and Evaluation, Springer, 47(3):723?742.
Kirill Kireyev. 2009. Semantic-based estimation of
term informativeness. In Proc. of NAACL ?09, pages
530?538, Boulder, Colorado.
Marijn Koolen and Jaap Kamps. 2010. The impor-
tance of anchor text for ad hoc search revisited. In
Proceedings of SIGIR ?10, pages 122?129, Geneva,
Switzerland.
Reiner Kraft and Jason Zien. 2004. Mining anchor text
for query refinement. In Proceedings of the 13th In-
ternational Conference on World Wide Web, WWW
?04, pages 666?674, New York, NY, USA. ACM.
Sungjick Lee and Han-joon Kim. 2008. News Key-
word Extraction for Topic Tracking. In Proceedings
of the 2008 Fourth International Conference on Net-
worked Computing and Advanced Information Man-
agement - Volume 02, NCM ?08, pages 554?559,
Washington, DC, USA. IEEE Computer Society.
Wendy Lehnert, Claire Cardie, and Ellen Rilofl. 1990.
Analyzing research papers using citation sentences.
In Proceedings of the 12th Annual Conference of the
Cognitive Science Society, pages 511?518.
Marina Litvak and Mark Last. 2008. Graph-
Based Keyword Extraction for Single-Document
Summarization. In Proceedings of the Workshop
on Multi-source Multilingual Information Extrac-
tion and Summarization, MMIES ?08, pages 17?24,
Manchester, United Kingdom.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009. Unsupervised Approaches for Automatic
Keyword Extraction Using Meeting Transcripts. In
Proceedings of NAACL ?09, pages 620?628, Boul-
der, Colorado.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic Keyphrase Ex-
traction via Topic Decomposition. In Proceedings
of EMNLP ?10, pages 366?376, Cambridge, Mas-
sachusetts.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Lu??s Marujo, Ricardo Ribeiro, David Martins
de Matos, Jo?ao Paulo Neto, Anatole Gershman, and
Jaime G. Carbonell. 2013. Key phrase extraction of
lightly filtered broadcast news. CoRR.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 3 - Volume 3, EMNLP
?09, pages 1318?1327, Singapore.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generat-
ing impact-based summaries for scientific literature.
In Proceedings of ACL-08: HLT, pages 816?824,
Columbus, Ohio.
1445
Donald Metzler, Jasmine Novak, Hang Cui, and Srihari
Reddy. 2009. Building enriched document repre-
sentations using aggregated anchor text. In Proc. of
SIGIR ?09, pages 219?226, Boston, MA, USA.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Proceedings of
EMNLP 2004, pages 404?411, Barcelona, Spain.
Preslav I. Nakov, Ariel S. Schwartz, and Marti A.
Hearst. 2004. Citances: Citation sentences for se-
mantic analysis of bioscience text. In SIGIR Work-
shop on Search and Discovery in Bioinformatics.
Thuy Dung Nguyen and Min-Yen Kan. 2007.
Keyphrase Extraction in Scientific Publications. In
Proc. of the Intl. Conf. on Asian digital libraries,
ICADL?07, pages 317?326, Hanoi, Vietnam.
Rong Pan and Martin Scholz. 2009. Mind the gaps:
Weighting the unknown in large-scale one-class col-
laborative filtering. In Proceedings of KDD ?09,
pages 667?676, Paris, France.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proc. of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?09, pages 1?10, Singapore.
Nirmala Pudota, Antonina Dattolo, Andrea Baruzzo,
Felice Ferrara, and Carlo Tasso. 2010. Auto-
matic keyphrase extraction and ontology mining for
content-based tag recommendation. International
Journal of Intelligent Systems.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proc. of the 22nd Intl. Conference
on Computational Linguistics, COLING ?08, pages
689?696, Manchester, United Kingdom.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
?
Ozg?ur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, COLING ?10, pages 895?903.
Steffen Rendle, Christoph Freudenthaler, and Lars
Schmidt-Thieme. 2010. Factorizing personalized
markov chains for next-basket recommendation. In
WWW ?10, pages 811?820, Raleigh, North Carolina.
Jason D. M. Rennie and Tommi Jaakkola. 2005. Using
Term Informativeness for Named Entity Detection.
In Proc. of SIGIR ?05, pages 353?360.
Anna Ritchie, Simone Teufel, and Stephen Robertson.
2006. How to find better index terms through cita-
tions. In Proc. of the Workshop on How Can Compu-
tational Linguistics Improve Information Retrieval?,
CLIIR ?06, pages 25?32, Sydney, Australia.
Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for information
retrieval. In Proc. of CIKM ?08, pages 213?222,
Napa Valley, California, USA.
Guy Shani, David Heckerman, and Ronen I. Brafman.
2005. An mdp-based recommender system. J.
Mach. Learn. Res., 6:1265?1295, December.
Xiaolin Shi, Jure Leskovec, and Daniel A. McFarland.
2010. Citing for high impact. In Proceedings of the
10th Annual Joint Conference on Digital Libraries,
JCDL ?10, pages 49?58, Gold Coast, Queensland,
Australia.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In Proceed-
ings of EMNLP-06.
S. Teufel. 1999. Argumentative Zoning: Information
Extraction from Scientific Text. Ph.D. thesis, Uni-
versity of Edinburgh.
Paolo Tonella, Filippo Ricca, Emanuele Pianta, and
Christian Girardi. 2003. Using Keyword Extrac-
tion for Web Site Clustering. In Web Site Evolution,
2003. Theme: Architecture. Proceedings. Fifth IEEE
International Workshop on, pages 41?48.
Peter D. Turney. 2000. Learning algorithms for
keyphrase extraction. Inf. Retr., 2.
Peter D. Turney. 2003. Coherent Keyphrase Extraction
via Web Mining. In Proceedings of the 18th inter-
national joint conference on Artificial intelligence,
IJCAI?03, pages 434?439, Acapulco, Mexico.
Xiaojun Wan and Jianguo Xiao. 2008. Single Doc-
ument Keyphrase Extraction Using Neighborhood
Knowledge. In Proceedings of AAAI ?08, pages
855?860, Chicago, Illinois.
Zhaohui Wu and Lee C. Giles. 2013. Measuring
term informativeness in context. In Proceedings of
NAACL ?13, pages 259?269, Atlanta, Georgia.
Zhuli Xie. 2005. Centrality Measures in Text Mining:
Prediction of Noun Phrases that Appear in Abstracts.
In Proceedings of the ACL Student Research Work-
shop, pages 103?108, Ann Arbor, Michigan.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In SIGIR.
Yongzheng Zhang, Evangelos Milios, and Nur Zincir-
Heywood. 2007. A Comparative Study on Key
Phrase Extraction Methods in Automatic Web Site
Summarization. Journal of Digital Information
Management, 5(5):323.
Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song,
Palakorn Achananuparp, Ee-Peng Lim, and Xiaom-
ing Li. 2011. Topical Keyphrase Extraction from
Twitter. In Proceedings of HLT ?11, pages 379?388,
Portland, Oregon.
Andrew Zimdars, David Maxwell Chickering, and
Christopher Meek. 2001. Using temporal data for
making recommendations. In Proceedings of the
17th Conference in Uncertainty in Artificial Intelli-
gence, UAI ?01, pages 580?588.
1446
