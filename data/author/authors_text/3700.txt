The Meeting Project at ICSI
Nelson Morgan1;4 Don Baron1;4 Jane Edwards1;4 Dan Ellis1;2 David Gelbart1;4
Adam Janin1;4 Thilo Pfau1 Elizabeth Shriberg1;3 Andreas Stolcke1;3
1International Computer Science Institute, Berkeley, CA
2Columbia University, New York, NY
3SRI International, Menlo Park, CA
4University of California at Berkeley, Berkeley, CA
fmorgan,dbaron,edwards,dpwe,gelbart,janin,tpfau,ees,stolckeg@icsi.berkeley.edu
ABSTRACT
In collaboration with colleagues at UW, OGI, IBM, and SRI, we are
developing technology to process spoken language from informal
meetings. The work includes a substantial data collection and tran-
scription effort, and has required a nontrivial degree of infrastruc-
ture development. We are undertaking this because the new task
area provides a significant challenge to current HLT capabilities,
while offering the promise of a wide range of potential applica-
tions. In this paper, we give our vision of the task, the challenges it
represents, and the current state of our development, with particular
attention to automatic transcription.
1. THE TASK
We are primarily interested in the processing (transcription,
query, search, and structural representation) of audio recorded from
informal, natural, and even impromptu meetings. By ?informal? we
mean conversations between friends and acquaintances that do not
have a strict protocol for the exchanges. By ?natural? we mean
meetings that would have taken place regardless of the recording
process, and in acoustic circumstances that are typical for such
meetings. By ?impromptu? we mean that the conversation may
take place without any preparation, so that we cannot require spe-
cial instrumentation to facilitate later speech processing (such as
close-talking or array microphones). A plausible image for such
situations is a handheld device (PDA, cell phone, digital recorder)
that is used when conversational partners agree that their discussion
should be recorded for later reference.
Given these interests, we have been recording and transcrib-
ing a series of meetings at ICSI. The recording room is one of
ICSI?s standard meeting rooms, and is instrumented with both
close-talking and distant microphones. Close-mic?d recordings
will support research on acoustic modeling, language modeling,
dialog modeling, etc., without having to immediately solve the
difficulties of far-field microphone speech recognition. The dis-
tant microphones are included to facilitate the study of these deep
acoustic problems, and to provide a closer match to the operating
conditions ultimately envisaged. These ambient signals are col-
.
lected by 4 omnidirectional PZM table-mount microphones, plus
a ?dummy? PDA that has two inexpensive microphone elements.
In addition to these 6 distant microphones, the audio setup permits
a maximum of 9 close-talking microphones to be simultaneously
recorded. A meeting recording infrastructure is also being put in
place at Columbia University, at SRI International, and by our col-
leagues at the University of Washington. Recordings from all sites
will be transcribed using standards evolved in discussions that also
involved IBM (who also have committed to assist in the transcrip-
tion task). Colleagues at NIST have been in contact with us to fur-
ther standardize these choices, since they intend to conduct related
collection efforts.
A segment from a typical discussion recorded at ICSI is included
below in order to give the reader a more concrete sense of the task.
Utterances on the same line separated by a slash indicate some de-
gree of overlapped speech.
A: Ok. So that means that for each utterance, .. we?ll need
the time marks.
E: Right. / A: the start and end of each utterance.
[a few turns omitted]
E: So we - maybe we should look at the um .. the tools that
Mississippi State has.
D: Yeah.
E: Because, I - I - I know that they published .. um .. annota-
tion tools.
A: Well, X-waves have some as well, .. but they?re pretty
low level .. They?re designed for uh - / D: phoneme / A: for
phoneme-level / D: transcriptions. Yeah.
J: I should -
A: Although, they also have a nice tool for - .. that could be
used for speaker change marking.
D: There?s a - there are - there?s a whole bunch of tools
J: Yes. / D: web page, where they have a listing. D: like 10
of them or something.
J: Are you speaking about Mississippi State per se? or
D: No no no, there?s some .. I mean, there just - there are -
there are a lot of / J: Yeah.
J: Actually, I wanted to mention - / D: (??)
J: There are two projects, which are .. international .. huge
projects focused on this kind of thing, actually .. one of
them?s MATE, one of them?s EAGLES .. and um.
D: Oh, EAGLES.
D: (??) / J: And both of them have
J: You know, I shou-, I know you know about the big book.
E: Yeah.
J: I think you got it as a prize or something.
E: Yeah. / D: Mhm.
J: Got a surprise. flaughg fJ. thought ?as a prize? sounded
like ?surprise?g
Note that interruptions are quite frequent; this is, in our expe-
rience, quite common in informal meetings, as is acoustic overlap
between speakers (see the section on error rates in overlap regions).
2. THE CHALLENGES
While having a searchable, annotatable record of impromptu
meetings would open a wide range of applications, there are sig-
nificant technical challenges to be met; it would not be far from the
truth to say that the problem of generating a full representation of a
meeting is ?AI complete?, as well as ?ASR complete?. We believe,
however, that our community can make useful progress on a range
of associated problems, including:
 ASR for very informal conversational speech, including the
common overlap problem.
 ASR from far-field microphones - handling the reverberation
and background noise that typically bedevil distant mics, as
well as the acoustic overlap that is more of a problem for
microphones that pick up several speakers at approximately
the same level.
 Segmentation and turn detection - recovering the different
speakers and turns, which also is more difficult with overlaps
and with distant microphones (although inter-microphone
timing cues can help here).
 Extracting nonlexical information such as speaker identifi-
cation and characterization, voice quality variation, prosody,
laughter, etc.
 Dialog abstraction - making high-level models of meet-
ing ?state?; identifying roles among participants, classifying
meeting types, etc. [2].
 Dialog analysis - identification and characterization of fine-
scale linguistic and discourse phenomena [3][10].
 Information retrieval from errorful meeting transcriptions -
topic change detection, topic classification, and query match-
ing.
 Summarization of meeting content [14] - representation of
the meeting structure from various perspectives and at vari-
ous scales, and issues of navigation in thes representations.
 Energy and memory resource limitation issues that arise in
the robust processing of speech using portable devices [7].
Clearly we and others working in this area (e.g., [15]) are at an
early stage in this research. However, the remainder of this pa-
per will show that even a preliminary effort in recording, manually
transcribing, and recognizing data from natural meetings has pro-
vided some insight into at least a few of these problems.
3. DATA COLLECTION AND HUMAN
TRANSCRIPTION
Using the data collection setup described previously, we have
been recording technical meetings at ICSI. As of this writing we
have recorded 38 meetings for a total of 39 hours. Note that there
are separate microphones for each participant in addition to the 6
far-field microphones, and there can be as many as 15 open chan-
nels. Consequently the sound files comprise hundreds of hours of
recorded audio. The total number of participants in all meetings is
237, and there were 49 unique speakers. The majority of the meet-
ings recorded so far have either had a focus on ?Meeting Recorder?
(that is, meetings by the group working on this technology) or ?Ro-
bustness? (primarily concerned with ASR robustness to acoustic
effects such as additive noise). A smaller number of other meeting
types at ICSI were also included.
In addition to the spontaneous recordings, we asked meeting par-
ticipants to read digit strings taken from a TI digits test set. This
was done to facilitate research in far-field microphone ASR, since
we expect this to be quite challenging for the more unconstrained
case. At the start or end of each meeting, each participant read 20
digit strings.
Once the data collection was in progress, we developed a set of
procedures for our initial transcription. The transcripts are word-
level transcripts, with speaker identifier, and some additional in-
formation: overlaps, interrupted words, restarts, vocalized pauses,
backchannels, and contextual comments, and nonverbal events
(which are further subdivided into vocal types such as cough and
laugh, and nonvocal types such as door slams and clicks). Each
event is tied to the time line through use of a modified version of the
?Transcriber? interface (described below). This Transcriber win-
dow provides an editing space at the top of the screen (for adding
utterances, etc), and the wave form at the bottom, with mechanisms
for flexibly navigating through the audio recording, and listening
and re-listening to chunks of virtually any size the user wishes.
The typical process involves listening to a stretch of speech until
a natural break is found (e.g., a long pause when no one is speak-
ing). The transcriber separates that chunk from what precedes and
follows it by pressing the Return key. Then he or she enters the
speaker identifier and utterance in the top section of the screen.
The interface is efficient and easy to use, and results in an XML
representation of utterances (and other events) tied to time tags for
further processing.
The ?Transcriber? interface [13] is a well-known tool for tran-
scription, which enables the user to link acoustic events to the wave
form. However, the official version is designed only for single-
channel audio. As noted previously, our application records up to
15 parallel sound tracks generated by as many as 9 speakers, and we
wanted to capture the start and end times of events on each channel
as precisely as possible and independently of one another across
channels. The need to switch between multiple audio channels to
clarify overlaps, and the need to display the time course of events
on independent channels required extending the ?Transcriber? in-
terface in two ways. First, we added a menu that allows the user to
switch the playback between a number of audio files (which are all
assumed to be time synchronized). Secondly, we split the time-
linked display band into as many independent display bands as
there are channels (and/or independent layers of time-synchronized
annotation). Speech and other events on each of the bands can now
be time-linked to the wave form with complete freedom and totally
independently of the other bands. This enables much more precise
start and end times for acoustic events.
See [8] for links to screenshots of these extensions to Transcriber
(as well as to other updates about our project).
In the interests of maximal speed, accuracy and consistency, the
transcription conventions were chosen so as to be: quick to type,
related to standard literary conventions where possible (e.g., - for
interrupted word or thought, .. for pause, using standard orthogra-
phy rather than IPA), and minimalist (requiring no more decisions
by transcribers than absolutely necessary).
After practice with the conventions and the interface, transcribers
achieved a 12:1 ratio of transcription time to speech time. The
amount of time required for transcription of spoken language is
known to vary widely as a function of properties of the discourse
(amount of overlap, etc.), and amount of detailed encoding (prosod-
ics, etc.), with estimates ranging from 10:1 for word-level with
minimal added information to 20:1, for highly detailed discourse
transcriptions (see [4] for details).
In our case, transcribers encoded minimal added detail, but had
two additional demands: marking boundaries of time bins, and
switching between audio channels to clarify the many instances of
overlapping speech in our data. We speeded the marking of time
bins by providing them with an automatically segmented version
(described below) in which the segmenter provided a preliminary
set of speech/nonspeech labels. Transcribers indicated that the pre-
segmentation was correct sufficiently often that it saved them time.
After the transcribers finished, their work was edited for consis-
tency and completeness by a senior researcher. Editing involved
checking exhaustive listings of forms in the data, spell check-
ing, and use of scripts to identify and automatically encode cer-
tain distinctions (e.g., the distinction between vocalized nonverbal
events, such as cough, and nonvocalized nonverbal events, like door
slams). This step requires on average about 1:1 - one minute of
editing for each minute of speech.
Using these methods and tools, we have currently transcribed
about 12 hours out of our 39 hours of data. Other data have
been sent to IBM for a rough transcription using commercial tran-
scribers, to be followed by a more detailed process at ICSI. Once
this becomes a routine component of our process, we expect it to
significantly reduce the time requirements for transcription at ICSI.
4. AUTOMATIC TRANSCRIPTION
As a preliminary report on automatic word transcription, we
present results for six example meetings, totalling nearly 7 hours
of speech, 36 total speakers, and 15 unique speakers (since many
speakers participated in multiple meetings). Note that these re-
sults are preliminary only; we have not yet had a chance to address
the many obvious approaches that could improve performance. In
particular, in order to facilitate efforts in alignment, pronuncia-
tion modeling, language modeling, etc., we worked only with the
close-mic?d data. In most common applications of meeting tran-
scription (including those that are our chief targets in this research)
such a microphone arrangement may not be practical. Nevertheless
we hope the results using the close microphone data will illustrate
some basic observations we have made about meeting data and its
automatic transcription.
4.1 Recognition system
The recognizer was a stripped-down version of the large-
vocabulary conversational speech recognition system fielded by
SRI in the March 2000 Hub-5 evaluation [11]. The system per-
forms vocal-tract length normalization, feature normalization, and
speaker adaptation using all the speech collected on each chan-
nel (i.e., from one speaker, modulo cross-talk). The acous-
tic model consisted of gender-dependent, bottom-up clustered
(genonic) Gaussian mixtures. The Gaussian means are adapted by
a linear transform so as to maximize the likelihood of a phone-loop
model, an approach that is fast and does not require recognition
prior to adaptation. The adapted models are combined with a bi-
gram language model for decoding. We omitted more elaborate
adaptation, cross-word triphone modeling, and higher-order lan-
guage and duration models from the full SRI recognition system
as an expedient in our initial recognition experiments (the omitted
steps yield about a 20% relative error rate reduction on Hub-5 data).
It should be noted that both the acoustic models and the lan-
guage model of the recognizer were identical to those used in the
Hub-5 domain. In particular, the acoustic front-end assumes a tele-
phone channel, requiring us to downsample the wide-band signals
of the meeting recordings. The language model contained about
30,000 words and was trained on a combination of Switchboard,
CallHome English and Broadcast News data, but was not tuned for
or augmented by meeting data.
4.2 Speech segmentation
As noted above, we are initially focusing on recognition of the
individual channel data. Such data provide an upper bound on
recognition accuracy if speaker segmentation were perfect, and
constitute a logical first step for obtaining high quality forced align-
ments against which to evaluate performance for both near- and far-
field microphones. Individual channel recordings were partitioned
into ?segments? of speech, based on a ?mixed? signal (addition
of the individual channel data, after an overall energy equalization
factor per channel). Segment boundary times were determined ei-
ther by an automatic segmentation of the mixed signal followed by
hand-correction, or by hand-correction alone. For the automatic
case, the data was segmented with a speech/nonspeech detector
consisting of an extension of an approach using an ergodic hidden
Markov model (HMM) [1]. In this approach, the HMM consists
of two main states, one representing ?speech? and one represent-
ing ?nonspeech? and a number of intermediate states that are used
to model the time constraints of the transitions between the two
main states. In our extension, we are incorporating mixture den-
sities rather than single Gaussians. This appears to be useful for
the separation of foreground from background speech, which is a
serious problem in these data.
The algorithm described above was trained on the
speech/nonspeech segmentation provided manually for the
first meeting that was transcribed. It was used to provide segments
of speech for the manual transcribers, and later for the recognition
experiments. Currently, for simplicity and to debug the various
processing steps, these segments are synchronous across chan-
nels. However, we plan to move to segments based on separate
speech/nonspeech detection in each individual channel. The latter
approach should provide better recognition performance, since it
will eliminate cross-talk in segments in which one speaker may
say only a backchannel (e.g. ?uhhuh?) while another speaker is
talking continuously.
Performance was scored for the spontaneous conversational por-
tions of the meetings only (i.e., the read digit strings referred to
earlier were excluded). Also, for this study we ran recognition only
on those segments during which a transcription was produced for
the particular speaker. This overestimates the accuracy of word
recognition, since any speech recognized in the ?empty? segments
would constitute an error not counted here. However, adding the
empty regions would increase data load by a factor of about ten?
which was impractical for us at this stage. Note that the current
NIST Hub-5 (Switchboard) task is similar in this respect: data are
recorded on separated channels and only the speech regions of a
speaker are run, not the regions in which they are essentially silent.
We plan to run all speech (including these ?empty? segments) in
future experiments, to better assess actual performance in a real
meeting task.
4.3 Recognition results and discussion
Overall error rates. Table 1 lists word error rates for the six
meetings, by speaker. The data are organized into two groups: na-
tive speakers and nonnative speakers. Since our recognition system
is not trained on nonnative speakers, we provide results only for the
native speakers; however the word counts are listed for all partici-
Table 1: Recognition performance by speaker and meeting (MRM = ?Meeting Recorder meeting?; ROB = ?Robustness meeting?).
Speaker gender is indicated by ?M? or ?F? in the speaker labels. ?* : : : *? marks speakers using a lapel microphone; all other cases
used close-talking head-mounted microphones. ??? indicates speakers with severely degraded or missing signals due to incorrect
microphone usage. Word error rates are in boldface, total number of words in Roman, and out-of-vocabulary (OOV) rates in italics.
OOV rate is by token, relative to a Hub-5 language model. WER is for conversational speech sections of meetings only, and are not
reported for nonnative speakers.
Meeting MRM002 MRM003 MRM004 MRM005 ROB005 ROB004
Duration (minutes) 45 78 60 68 81 70
Native speakers
M 004 42.4 48.1 44.3 48.4 45.1
4550 3087 3432 4912 5512
2.07 2.75 1.60 2.12 1.61
M 001 42.4 50.6 37.6 38.6
2311 2488 1904 3400
1.82 2.09 2.78 1.56
F 001 45.2 43.2 42.9 41.9
3008 3360 2714 2705
2.59 3.18 4.05 2.14
M 009 *100.1* *115.8* 38.2 *68.7*
1122 367 1066 696
1.59 2.45 1.88 2.01
F 002 45.2 43.7 *46.0*
1549 1481 2480
2.26 2.64 1.63
M 002 *55.6*
990
2.12
Speakers with low word counts
M 007 55.6 ?
198 69
2.97 2.90
M 008 72.7 59.5
55 121
5.45 5.79
M 015 ?
59
6.56
Non-native speakers (total words only)
M 003 (British) 2189
M 011 (Spanish) 2653 1239 663
F 003 (Spanish) 620 220
M 010 (German) 28
M 012 (German) 639
M 006 (French) 3524 2648
pants for completeness.1
The main result to note from Table 1 is that overall word error
rates are not dramatically worse than for Switchboard-style data.
This is particularly impressive since, as described earlier, no meet-
ing data were used in training, and no modifications of the acoustic
or language models were made. The overall WER for native speak-
ers was 46.5%, or only about a 7% relative increase over a compa-
rable recognition system on Hub-5 telephone conversations. This
suggests that from the point of view of pronunciation and language
(as opposed to acoustic robustness, e.g., for distant microphones),
Switchboard may also be ?ASR-complete?. That is, talkers may not
really speak in a more ?sloppy? manner in meetings than they do in
casual phone conversation. We further investigate this claim in the
next section, by breaking down results by overlap versus nonover-
lap regions, by microphone type and by speaker.
Note that in some cases there were very few contributions from
a speaker (e.g., speakers M 007, M 008, and M 015), and such
speakers also tended to have higher word error rates. We initially
suspected the problem was a lack of sufficient data for speaker
adaptation; indeed the improvement from adaptation was less than
for other speakers. Thus for such speakers it would make sense to
pool data across meetings for repeat participants. However, in look-
ing at their word transcripts we noted that their utterances, while
few, tended to be dense with information content. That is, these
were not the speakers uttering ?uhhuh? or short common phrases
(which are generally well modeled in the Switchboard recognizer)
but rather high-perplexity utterances that are generally harder to
recognize. Such speakers also tend to have a generally higher over-
all OOV rate than other speakers.
Error rates in overlap versus nonoverlap regions. As noted
in the previous section, the overall word error rate in our sam-
ple meetings was slightly higher than in Switchboard. An obvious
question to ask here is: what is the effect on recognition of over-
lapping speech? To address this question, we defined a crude mea-
sure of overlap. Since segments were channel-synchronous in these
meetings, a segment was either non-overlapping (only one speaker
was talking during that time segment), or overlapping (two or more
speakers were talking during the segment). Note that this does not
measure amount of overlap or number of overlapping speakers;
more sophisticated measures based on the phone backtrace from
forced alignment would provide a better measure for more detailed
analyses. Nevertheless, the crude measure provides a clear first
answer to our question. Since we were also interested in the inter-
action if any between overlap and microphone type, we computed
results separately for the head-mounted and lapel microphones. Re-
sults were also computed by speaker, since as shown earlier in Ta-
ble 1, speakers varied in word error rates, total words, and words by
microphone type. Note that speakers M 009 and F 002 have data
from both conditions.
As shown, our measure of overlap (albeit crude), clearly shows
that overlapping speech is a major problem for the recognition of
speech from meetings. If overlap regions are removed, the recog-
nition accuracy overall is actually better than that for Switchboard.
It is premature to make absolute comparisons here, but the fact that
the same pattern is observed for all speakers and across microphone
1Given the limitations of these pilot experiments (e.g., no on-task
training material and general pronunciation models), recognition
on nonnative speakers is essentially not working at present. In the
case of one nonnative speaker, we achieved a 200% word error rate,
surpassing a previous ICSI record. Word error results presented
here are based on meeting transcripts as of March 7, 2000, and are
subject to small changes as a result of ongoing transcription error
checking.
Table 2: Word error rates broken down by whether or not seg-
ment is in a region of overlapping speech.
Speaker No overlap With overlap
Headset Lapel Headset Lapel
M 004 41.0 - 50.3 -
M 001 34.2 - 47.6 -
F 001 40.5 - 45.8 -
M 009 30.7 41.0 40.7 117.8
F 002 37.7 29.8 50.5 56.3
M 002 - 48.6 - 71.3
M 007 52.2 - 81.3 -
M 008 50.9 - 69.9
Overall 39.9 38.5 48.7 85.2
conditions suggests that it is not the inherent speech properties of
participants that makes meetings difficult to recognize, but rather
the presence of overlapping speech.
Furthermore, one can note from Table 2 that there is a large inter-
action between microphone type and the effect of overlap. Overlap
is certainly a problem even for the close-talking head-mounted mi-
crophones. However, the degradation due to overlap is far greater
for the lapel microphone, which picks up a greater degree of back-
ground speech. As demonstrated by speaker F 002, it is possible
to have a comparatively good word error rate (29.8%) on the lapel
microphone in regions of no overlap (in this case 964/2480 words
were in nonoverlapping segments). Nevertheless, since the rate of
overlaps is so high in the data overall, we are avoiding the use
of the lapel microphone where possible in the future, preferring
head-mounted microphones for obtaining ground truth for research
purposes. We further note that for tests of acoustic robustness for
distant microphones, we tend to prefer microphones mounted on
the meeting table (or on a mock PDA frame), since they provide a
more realistic representation of the ultimate target application that
is a central interest to us - recognition via portable devices. In other
words, we are finding lapel mics to be too ?bad? for near-field mi-
crophone tests, and too ?good? for far-field tests.
Error rates by error type. The effect of overlapping speech on
error rates is due almost entirely to insertion errors, as shown in
Figure 1. Rates of other error types are nearly identical to those ob-
served for Switchboard (modulo a a slight increase in substitutions
associated with the lapel condition). This result is not surprising,
since background speech obviously adds false words in the hypoth-
esis. However, it is interesting that there is little increase in the
other error types, suggesting that a closer segmentation based on
individual channel data (as noted earlier) could greatly improve
recognition accuracy (by removing the surrounding background
speech).
Error rates by meeting type. Different types of meetings
should give rise to differences in speaking style and social interac-
tion, and we may be interested in whether such effects are realized
as differences in word error rates. The best way to measure such
effects is within speaker. The collection of regular, ongoing meet-
ings at ICSI offers the possibility of such within-speaker compar-
isons, since multiple speakers participate in more than one type of
regular meeting. Of the speakers shown in the data set used for this
study, speaker M 004 is a good case in point, since he has data from
three ?Meeting Recorder? meetings and two ?Robustness? meet-
ings. These two meeting types differ in social interaction; in the
first, there is a fairly open exchange between many of the partici-
Substitutions Deletions Insertions
Error Type
0
10
20
30
40
50
R
at
e 
 (%
)
Switchboard
Head?Mic, Overlap 
Head?Mic, Nonoverlap 
Lapel?Mic, Overlap 
Lapel?Mic, Nonoverlap 
Figure 1: Word error rates by error type and micro-
phone/overlap condition. Switchboard scores refer to an in-
ternal SRI development testset that is a representative subset
of the development data for the 2001 hub-5 evals. It contains
41 speakers (5-minute conversation sides), from Switchboard-
1, Switchboard-2 and Cellular Switchboard in roughly equal
proportions, and is also balanced for gender and ASR diffi-
culty. The other scores are evaluated for the data described
in the text.
pants, while in the second, speaker M 004 directs the flow of the
meeting. It can also be seen from the table that speaker M 004 con-
tributes a much higher rate of words relative to overall words in the
latter meeting type. Interestingly however, his recognition rate and
OOV rates are quite similar across the meeting types. Study of ad-
ditional speakers across meetings will allow us to further examine
this issue.
5. FUTURE WORK
The areas mentioned in the earlier section on ?Challenges? will
require much more work in the future. We and our colleagues at
collaborating institutions will be working in all of these. Here, we
briefly mention some of the work in our current plans for the study
of speech from meetings.
Far-field microphone ASR. Starting with the read digits and
proceeding to spontaneous speech, we will have a major focus on
improving recognition on the far-field channels. In earlier work we
have had some success in recognizing artificially degraded speech
[6][5], and will be adapting and more fully developing these ap-
proaches for the new data and task. Our current focus in these
methods is on the designing of multiple acoustic representations
and the combination of the resulting probability streams, but we
will also compare these to methods that are more standard (but im-
practical for the general case) such as echo cancellation using both
the close and distant microphones.
Overlap type modeling. One of the distinctive characteristics
of naturalistic conversation (in contrast to monolog situations) is
the presence of overlapping speech. Overlapping speech may be
of several types, and affects the flow of discourse in various ways.
An overlap may help to usurp the floor from another speaker (e.g.,
interruptions), or to encourage a speaker to continue (e.g., back
channels). Also, some overlaps may be accidental, or a part of joint
action (as when a group tries to help a speaker to recall a person?s
name when he is in mid-sentence). In addition, different speakers
may differ in the amount and kinds of overlap in which they engage
(speaker style). In future work we will explore types of overlaps
and their physical parameters, including prosodic aspects.
Language modeling. Meetings are also especially challenging
for the language model, since they tend to comprise a diverse range
of topics and styles, and matched training data is hard to come
by (at least in this initial phase of the project). Therefore, we ex-
pect meeting recognition to necessitate investigation into novel lan-
guage model adaptation and robustness techniques.
Prosodic modeling. Finally, we plan to study the potential con-
tribution of prosodic (temporal and intonational) features to auto-
matic processing of meeting data. A project just underway is con-
structing a database of prosodic features for meeting data, extend-
ing earlier work [10, 9]. Goals include using prosody combined
with language model information to help segment speech into co-
herent semantic units, to classify dialog acts [12], and to aid speaker
segmentation.
6. ACKNOWLEDGMENTS
The current work has been funded under the DARPA Communi-
cator project (in a subcontract from the University of Washington),
supplemented by an award from IBM. In addition to the authors of
this abstract, the project involves colleagues at a number of other
institutions, most notably: Mari Ostendorf, Jeff Bilmes, and Katrin
Kirchhoff from the University of Washington; and Hynek Herman-
sky from the Oregon Graduate Institute.
7. REFERENCES
[1] M. Beham and G. Ruske, Adaptiver stochastischer
Sprache/Pause-Detektor. Proc. DAGM Symposium
Mustererkennung, pp. 60?67, Bielefeld, May 1995, Springer.
[2] D. Biber, Variation across speech and writing. 1st pbk. ed.
Cambridge [England]; New York: Cambridge University
Press, 1991.
[3] W. Chafe, Cognitive constraints on information flow. In R. S.
Tomlin (ed.) Coherence and grounding in discourse.
Philadelphia: John Benjamins, pp. 21?51, 1987.
[4] J. Edwards, The transcription of Discourse. In D. Tannen, D.
Schiffrin, and H. Hamilton (eds). The Handbook of
Discourse Analysis. NY: Blackwell (in press).
[5] H. Hermansky, D. Ellis, and S. Sharma, Tandem
connectionist feature stream extraction for conventional
HMM systems, Proc. ICASSP, pp. III-1635?1638, Istanbul,
2000.
[6] H. Hermansky and N. Morgan, RASTA Processing of
Speech, IEEE Trans. Speech and Audio Processing 2(4),
578?589, 1994.
[7] A. Janin and N. Morgan, SpeechCorder, the Portable
Meeting Recorder, Workshop on hands-free speech
communication, Kyoto, April 9-11, 2001.
[8] http://www.icsi.berkeley.edu/speech/mtgrcdr.html
[9] E. Shriberg, R. Bates, A. Stolcke, P. Taylor, D. Jurafsky, K.
Ries, N. Coccaro, R. Martin, M. Meteer, and C. Van
Ess-Dykema. Can prosody aid the automatic classification of
dialog acts in conversational speech? Language and Speech,
41(3-4):439?487, 1998.
[10] E. Shriberg, A. Stolcke, D. Hakkani-Tu?r, and G. Tu?r.
Prosody-based automatic segmentation of speech into
sentences and topics. Speech Communication,
32(1-2):127?154, 2000.
[11] A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. R. Rao
Gadde, M. Plauche?, C. Richey, E. Shriberg, K. So?nmez, F.
Weng, and J. Zheng. The SRI March 2000 Hub-5
conversational speech transcription system. Proc. NIST
Speech Transcription Workshop, College Park, MD, May
2000.
[12] A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D.
Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M.
Meteer, Dialogue Act Modeling for Automatic Tagging and
Recognition of Conversational Speech, Computational
Linguistics 26(3), 339?373, 2000.
[13] http://www.etca.fr/CTA/gip/Projets/Transcriber/
[14] A. Waibel, M. Bett, M. Finke, and R. Stiefelhagen, Meeting
Browser: Tracking and Summarizing Meetings, Proc.
DARPA Broadcast News Transcription and Understanding
Workshop, Lansdowne, VA, 1998.
[15] H. Yu, C. Clark, R. Malkin, and A. Waibel, Experiments in
Automatic Meeting Transcription Using JRTK, Proc.
ICASSP, pp. 921?924, Seattle, 1998.
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 143?150,
Prague, June 2007. c?2007 Association for Computational Linguistics
Mutaphrase: Paraphrasing with FrameNet
Michael Ellsworth and Adam Janin
{infinity,janin}@icsi.berkeley.edu
International Computer Science Institute
1947 Center Street, Suite 600
Berkeley, CA 94704-1105 USA
Abstract
We describe a preliminary version of Mu-
taphrase, a system that generates para-
phrases of semantically labeled input sen-
tences using the semantics and syntax en-
coded in FrameNet, a freely available lexico-
semantic database. The algorithm generates
a large number of paraphrases with a wide
range of syntactic and semantic distances
from the input. For example, given the in-
put ?I like eating cheese?, the system out-
puts the syntactically distant ?Eating cheese
is liked by me?, the semantically distant ?I
fear sipping juice?, and thousands of other
sentences. The wide range of generated
paraphrases makes the algorithm ideal for a
range of statistical machine learning prob-
lems such as machine translation and lan-
guage modeling as well as other semantics-
dependent tasks such as query and language
generation.
1 Introduction
A central tenet of statistical natural language pro-
cessing (NLP) is ?there?s no data like more data?.
One method for generating more data is to restate
each phrase in a corpus, keeping similar seman-
tics while changing both the words and the word
sequence. The efficacy of this approach has been
well-established in many areas, including automated
evaluation of machine translation systems (Kauchak
and Barzilay, 2006), text summarization (Kittredge,
2002), question answering (Rinaldi et al, 2003),
document retrieval (Zukerman and Raskutti, 2002),
and many others.
Most of the reported work on paraphrase gener-
ation from arbitrary input sentences uses machine
learning techniques trained on sentences that are
known or can be inferred to be paraphrases of each
other (Bannard and Callison-Burch, 2005; Barzi-
lay and Lee, 2003; Barzilay and McKeown, 2001;
Callison-Burch et al, 2006; Dolan et al, 2004;
Ibrahim et al, 2003; Lin and Pantel, 2001; Pang et
al., 2003; Quirk et al, 2004; Shinyama et al, 2002).
Mutaphrase instead generates paraphrases algorith-
mically using an input sentence and FrameNet, a
freely available lexico-semantic resource (informa-
tion regarding FrameNet, including relevant termi-
nology, is presented in Section 2).
3YNTAX3IMILAR
$IFFERENT
3EM
ANTIC
S
) LIKE EATING CHEESE%ATING CHEESE IS LIKED BY ME
) LIKE TO SNACK ON BREAD
) FEAR SIPPING JUICE4O SIP ON JUICE DISTURBS ME
Figure 1: Syntactic and semantic similarity to I like
eating cheese.
Conceptually, the Mutaphrase algorithm takes a
semantic specification of a sentence, provided by an
automatic semantic parser such as Shalmaneser (Erk
143
and Pado?, 2006), and recursively replaces each se-
mantically parsed phrase with a semantically similar
phrase. To generate each new phrase, each of the se-
mantic parts of the original phrase is mapped, using
FrameNet data, onto a new word or phrase whose
position and syntactic marking may be quite differ-
ent.
The Mutaphrase algorithm outputs a large set of
paraphrases with a variety of distances from the in-
put in terms of both syntax and semantics; see Fig-
ure 1. Depending on the needs of the application, fil-
tering can be applied to limit the distance to a desired
range. For example, language modeling may bene-
fit from a wider variety of semantic outputs, since
if I like eating cheese is in-domain, then I like sip-
ping juice is also likely in-domain. Other applica-
tions, e.g. Question Answering, require more strin-
gent limits on semantic distance. See Section 4.
1.1 Current Limitations
The current implementation of Mutaphrase suffers
from several limitations. Perhaps the most signifi-
cant is that the input sentences must be semantically
labeled using FrameNet annotations. Since no au-
tomated systems for FrameNet-specific annotation
are currently incorporated into our algorithm, input
is limited to hand-annotated sentences. Also, cer-
tain types of semantic ill-formedness are permitted
(e.g. I like sipping meat), and some types of syntax
are not well supported (e.g. conjunctions, relative-
clauses). We believe all these factors can be ad-
dressed; they are covered briefly in Future Work
(Section 4). We confine ourselves in other sections
to describing the core Mutaphrase algorithm as cur-
rently implemented.
2 FrameNet
The primary resource used in Mutaphrase is
FrameNet (Fontenelle, 2003; FrameNet, 2007b),
a lexico-semantic database that describes con-
cepts and their interrelations, wordform and word-
sequence information, syntactic categories, and
mappings between conceptual and lexical/syntactic
information. All of these are grounded in hand-
annotated examples of real-world sentences. At a
slightly more abstract level, FrameNet can be de-
scribed as providing a two-way mapping between
meaning (semantics) and form (syntax, wordforms,
sequences).
2.1 Semantics
The conceptual information is represented using
frames, where a frame is a type of schema or sce-
nario (e.g. Motion, Commercial transaction), and
frame elements (FEs), which are the participants
and parameters of the frames (e.g. Motion.Path,
Commercial transaction.Buyer). Frames and their
frame elements are related and mapped with a lim-
ited type of conceptual ontology involving Inher-
itance (i.e. subtype), Subframe (i.e. temporal sub-
part), Using (i.e. presupposition) and a few other re-
lation types.
2.2 Syntax
On the form side, the representation is more min-
imal. Wordforms and word-sequences are repre-
sented so that words with multiple wordforms (e.g.
take/took) and word sequences with wordforms (e.g.
take/took off ) can be referred to as unitary objects.
We have a category Support (and the more specific
label ?Copula?) for pieces of multi-word expressions
that are optional for expressing the semantics of the
whole (e.g. take in take a bath). FrameNet alo rep-
resents a small but sufficiently rich set of syntactic
categories of English (i.e. phrase types or PTs, such
as ?Sfin?, i.e. finite sentence) and syntactic relations
(i.e. grammatical functions or GFs, e.g. ?Object?).
2.3 Syntax-Semantics Bindings
The most vital part of the FrameNet data for our Mu-
taphrase algorithm is the mappings between seman-
tics and syntax. There are several categories pertain-
ing to this in the data. Lexical units (LUs) are a pair-
ing of words/word sequences with the frame each
evokes. The valences for each LU are sequences
in which semantic and form information pertinent
to phrases are paired. They are not stored in the
database, so we have created a process that produces
them entirely automatically (see 3.2). For example,
for the LU hand in the Giving frame and possible in
the Likelihood frame, we have the following anno-
tated sentences:
1. [She]Donor/NP/Ext [handed]Target
[a bag]Theme/NP/Obj
[to Nob]Recipient/PP (to)/Dep
144
2. [It]Null [was]Copula [possible]Target [that he
had been hoping to frighten
Steve]Hypothetical event/Sfin(that)/Dep
Example 1 above shows a typical valence, in
which most of the positions are semantically labeled
with a frame element which is paired with syntac-
tic GF and PT information. The second annotation
(2) is more complex, exemplifying each of the major
categories that make up the positions of a valence.
The categories are:
1. a Null element, with syntax but no semantics
(usually there or it)
2. a Support or Copula with its wordforms
3. a Target (i.e. an LU or word that is part of an
LU) with its wordforms, conceptually repre-
senting a frame
4. a frame-element/phrase-type/grammatical-
function phrase description, which puts
together semantic (FE) information with
syntax (GF and PT); the PT also indicates
fixed words (e.g. the word that in the example
above)
We can abstract away from the individual sen-
tences, preserving only the sequences of positions
with their features, as in the following representa-
tion of sentence 2 above:
Null(it), Copula, Target(possible), Hypotheti-
cal event/Dep/Sfin(that)
These abstract valences are the basis for the al-
gorithm we present here. There are typically be-
tween two and ten basic patterns associated with
each annotated lexical unit, encompassing alterna-
tions in the realization of FEs such as Active/Passive
(I recommended her vs. She was recommended by
me), the Dative Alternation (He handed the paper to
Stephen vs. He handed Stephen the paper), optional
elements (I ate dinner vs. I ate) and many more.
Basing our algorithm on rearranging the fillers
of these FEs allows us to abstract away from syn-
tax, since the FEs of a frame express the same rela-
tions regardless of the LU or syntax they occur with.
Some meaning differences between LUs within the
same frame (e.g. drink vs. eat) are not overtly mod-
eled in FrameNet. Other resources, such as Word-
Net, could provide added information in cases re-
quiring finer granularity (see Section 4).
3 Mutaphrase Algorithm
At a very high level, the paraphrase algorithm that
we use is as follows: we begin with a sentence with
frame-semantic annotation, replace each lexical unit
and its associated frame Elements with an alternative
valence, then filter the output for its syntactic and
semantic fit with the original sentence. The valences
may be drawn from either the same LU, an LU of
the same frame, or an LU of a related frame.
Frame: Desiring
Frame: Opinion
NP/Ext
Event
"is desired"
Target
Poss/Gen "Your"
Cognizer
"opinion"
Target
+
=
NP/Ext "I" "want" Frame: OpinionNP/Obj
Poss/Gen "your"
Cognizer
"opinion"
Target
Frame: Desiring
Experiencer Event
Target
NP/Ext "is desired"
Frame: Desiring
Event Target
B: Attested ValenceA: Input Tree
C: Output Tree
Figure 2: Algorithm Sketch: A syntactic/semantic
tree of the original sentence (A) is rearranged to
match a different valence (B), producing a new tree
(C); thus I want your opinion yields the paraphrase
Your opinion is desired.
Figure 2 shows an example of one step of the al-
gorithm. An input tree for the sentence I want your
opinion is shown in Figure 2A. The particular va-
lence for the Desiring frame in Figure 2B describes
the relations between the word desire and its depen-
dents in sentences like A meeting was desired. Be-
cause the phrase types and grammatical functions of
the FEs between the input and the attested valence
are compatible, it is possible to replace the input
145
frame with the new valence. The output is shown
in Figure 2C.
The remainder of this section describes in more
detail how this algorithm is implemented.
3.1 Building a Syntax/Semantics Tree from
FrameNet Data
Because the FEs of the original sentence are often
filled by phrases with their own annotation, the ini-
tial syntactic/semantic annotation is (conceptually,
at least) in the form of a graph. Typically, the graph
is nearly a tree, with few or no non-tree edges1.
Hereafter, we will use the term ?tree? even for the
cases where there are non-tree edges.
Since the data are not organized in this format in
the FrameNet output, we have implemented a rou-
tine which can turn FrameNet data into a syntactico-
semantic tree; tree examples can be seen in Fig-
ure 2A and Figure 2C.
3.2 Building Ordered Valences from FrameNet
Data
As mentioned in Section 2.3, we have constructed
a routine to parse FrameNet data to produce the va-
lences for each LU of a frame. The basic output is
an ordered list of syntactico-semantic elements, op-
tional apositional features (e.g. passive +/-), and the
frequency of the pattern.2
One innovation of our algorithm is its ability to
handle multiword LUs. It simply identifies each
word of the LU as a separate element in the list,
marking each with the label ?Target?. Thus the or-
dered valences of take off.v in the Undressing frame
include, among others:
? Wearer/NP/Ext, take/Target, off/Target, Cloth-
ing/NP/Obj; Frequency: 57/68
(e.g. I TOOK OFF my watch)
? Wearer/NP/Ext, take/Target, Clothing/NP/Obj,
1These non-tree edges are introduced when a phrase is an
FE of more than one frame. In keeping with normal syntactic
analysis, we treat the node as non-local to all but one parent.
2Although frequency of a particular pattern in the FrameNet
data is not strictly representative of the frequency of that pattern
in the corpus, a close examination reveals that the rank order of
patterns is largely identical, i.e. the most common pattern in
FrameNet represents the most common pattern in the corpus.
How useful this inexact statistical data will be is the subject of
future research.
off/Target; Frequency: 7/68
(e.g. You TAKE your shoes OFF)
One way of thinking about the valence set is that it
represents possible orderings of subparts of a phrase
that is semantically a frame instance and syntacti-
cally a phrase headed by the Target (see, for exam-
ple, Figure 2B). This semantic/syntactic information
is detailed enough to build the syntax of a phrase,
given FrameNet-style semantics.
3.3 Core algorithm
Once the input has been turned into a tree and there
is a set of alternative ways of expressing each frame
that is in the input, the algorithm then recurses
downward and then, as it returns up, replaces each
phrase/frame node with a set of alternative phrases.
In the simplest case, these phrases are built from all
the valences that are attested for the frame that the
original phrase expressed 3. In other words, our al-
gorithm is a recursive tree-rewrite in which the cur-
rent valence of the current LU is replaced by many
alternate valences of many different LUs.
In the recursion, word and phrase nodes not
headed by an LU are kept the same (except for pro-
nouns, which are expanded to all their wordforms,
e.g. me to I/me/my/mine). The child phrases of such
an unparaphrased node, if they are headed by an
LU or pronoun, can be paraphrased as long as the
paraphrases match the phrase type and grammatical
function of the original child phrase.
In Figure 2, the original sentence (represented
in Figure 2A) has the phrase representing the De-
siring frame replaced with an alternative phrase
evoking the same frame (Figure 2B) to produce a
new, roughly semantically equivalent sentence (Fig-
ure 2C) by expressing the same set of frames in the
same FE relations to each other.
In practice, we have to throw away at the outset
many of the valences because they include FEs that
are not in the input sentence4 or because they have
syntactic requirements of their child phrases which
3Our algorithm will work just as well with related frames
as long as the relevant FEs are mapped in the FrameNet data.
Controlling the distance, direction, and relation-types of related
frames that are included for paraphrase (if any) is one way to
control the degree of semantic diversity of the paraphrase out-
put. See further Section 3.4.
4Thus attempting to use the valence Experiencer/NP/Ext,
Degree/AVP/Dep, want/Target, Event/NP/Obj (e.g. I really
146
cannot be filled by a paraphrase of the child phrases.
For example, for the input sentence I gave presents
to friends, the code can output 560 (unfiltered) para-
phrases. A random selection from the output in-
cludes Presents bequeathed to friends, I handed in
presents, and Presents donated by I. Of these, the
first and last are filtered out as not filling the original
sentential context and the last, in addition, is filtered
out because of the mismatch between the pronoun
wordform I and the non-subject grammatical func-
tion.
To further refine the paraphrases, we must elimi-
nate examples that are not compatible with the input
sentence. In our current implementation, our algo-
rithm filters out incorrect syntax during the recursion
over the tree. Ultimately, we will also filter out mal-
formed semantics. The rest of this section is devoted
to an explication of the details of this filtering.
3.4 Syntactic/Semantic Compatibility
For both syntax and semantics, the degree of via-
bility of a paraphrase can be divided up into two
components: well-formedness and similarity. Syn-
tactic and semantic well-formedness is always desir-
able and the algorithm seeks to maximize it in ways
that are outlined below. Similarity between the orig-
inal sentence and its paraphrases (or among the para-
phrases), however, may be more or less desirable de-
pending on the task. Figure 1 shows an example of
the various degrees of syntactic and semantic simi-
larity of the paraphrase output. To maintain flexibil-
ity, we will need several control parameters to allow
us to filter our output for syntactic/semantic similar-
ity.
3.4.1 Syntactic Compatibility
Syntactic incompatibilities most commonly result
from gross mismatches between the Phrase Type
called for in a new valence and the Phrase Type pos-
sibilities available for the child phrase.
For example, if the initial sentence for paraphrase
is I want your opinion as in 1 below (repeated from
Figure 2), Valence 2 below represents a PT mis-
match, since I, an NP filler of the Experiencer role
want another chance) when paraphrasing the initial sentence
in Figure 2 will not work, since there is nothing in the original
to fill the Degree FE mentioned here.
in the original sentence, is not modifiable into an ad-
jective phrase (AJP).
1. Experiencer/NP/Ext, want/Target,
Event/NP/Obj
2. There/Null, be/Copula, Experiencer/AJP/Dep,
desire/Target, Event/PP(for)/Dep
(e.g. There is a public desire for transparency)
3. There/Null, be/Copula, desire/Target,
Experiencer/PP(in)/Dep, Event/PP(for)/Dep
(e.g. There was a desire in America for home
rule)
This filtering is vital, as otherwise valence 2
would yield the awful There is me desire for your
opinion.
However, phrase types that are not exact matches
may nevertheless be compatible with each other. Va-
lence 3, for example, is compatible with the original
valence, since the original Experiencer and Event
FEs were filled by NPs, to which prepositions can
be added to match the PP realizations required by
Valence 3. This yields another paraphrase of the
sentence in Figure 2: There is a desire in me for
your opinion. Similarly, full sentential clauses can
be modified to match VPs by truncation of the Ex-
ternal (subject) argument, etc. A phrase from the
original sentence may also be omitted to match an
empty phrase in the paraphrase, as seen in the omis-
sion of the Experiencer in the paraphrase in Figure 2.
These alternations provide more variety in the po-
tential phrase types of the paraphrases. Which syn-
tactic modifications are allowed should be an ex-
ternally controllable parameter, but this has not yet
been implemented. In general, allowing fewer types
of modification should move the average output left-
ward in the syntax/semantic similarity graph in Fig-
ure 1 (toward more syntactic similarity).
Although every annotated valence represents a
grammatical structure, some of these structures will
more likely be judged as well-formed than others;
in particular, infrequent patterns are more likely ill-
formed than frequent ones. An additional control-
lable parameter, allowing a trade-off between re-
call and precision, is a frequency cut-off for accept-
ing a valence pattern based on the number of times
147
the pattern is found in the FrameNet data. Our al-
gorithm currently produces a ranked list of para-
phrases based on exactly this frequency parameter,
and downstream processing can choose a cut-off fre-
quency or n-best to reduce the total output.
3.4.2 Semantic Filtering
Lexical units of the same frame are not necessar-
ily synonyms; they may be antonyms or coordinate
terms (i.e. co-hyponyms). For example, cheese and
juice are both in the Food frame, but I like eating
cheese and I like eating juice are certainly not a se-
mantic match! In fact, the second is a semantically
ill-formed modification of the first. Similarly, like
and hate are both in the Experiencer subject frame.
While I hate eating cheese is similar to I like eat-
ing cheese in describing an attitude toward eating
cheese, they are not an exact semantic match either;
in this case, however, the lack of semantic similarity
does not lead to semantic ill-formedness.
For some tasks such as expanding a language
model, exact semantic match is not necessary, but
for tasks that require strict semantic match, there are
several simple ways to increase robustness.
Tighter filtering, of whatever kind, will move the
average output of the algorithm downward in the
syntax/semantic similarity graph in Figure 1 (toward
more semantic similarity).
3.5 Preliminary Results
We have implemented the above algorithm to the
point that it is capable of producing paraphrases of
arbitrary input sentences that have received proper
FrameNet annotation. A large number of para-
phrases with a variety of phrase types are produced,
but the lack of semantic filtering occasionally leads
to semantically ill-formed results. The output is
ranked purely according to the frequency in the
FrameNet data of the valences used to build the para-
phrase.
For the sentence I like eating cheese, the para-
phraser produced 8403 paraphrases, of which the
following was top-ranked: I resented drinking
cheese, which suffers from the semantic mismatch
problems discussed in Section 3.4.2. Some other
output at random:
? I am interested in cheese devouring.
? I was nervous that cheese?s ingested.
? I?m worried about gobbling down cheese.
? My regrets were that cheese was eaten by me.
Since most of the annotation in the Ingestion
frame (the frame for eat, etc.) concerns eating rather
than drinking, the majority of the output is semanti-
cally well-formed. The paraphrases generated from
the Experiencer subject frame (the frame for like, in-
terested, regret, etc.) are more uniformly felicitous,
even if semantically quite divergent from the mean-
ing of the original. Both the infelicity of drinking
cheese and the semantic divergence appear to be ad-
dressable by refining semantic tightness using Word-
Net. Averaging over senses, words like gobble and
ingest have lower WordNet-based semantic distance
from eat than drink.
For the sentence Nausea seems a commonplace
symptom, the paraphraser outputs 502 paraphrases,
of which the following was top-ranked: It seems a
commonplace sign. Other output at random:
? Tiredness looks indicative.
? Queasiness smelt of a commonplace sign.
? Sleepiness appears a commonplace sign.
? Queasiness smelt indicative queasiness.
? Somnolence appears to be indicative.
Longer sentences (e.g. Locally elected school
boards, especially in our larger cities, become the
prey of ambitious, generally corrupt, and invari-
ably demagogic local politicians or would-be politi-
cians) currently take excessive amounts of time and
memory to run, but typically produce 10,000+ para-
phrases. Pruning earlier during paraphrase genera-
tion should help address this issue.
4 Future Work
Currently, Mutaphrase requires the input sentences
to have been marked with FrameNet annotations
prior to processing. Although automatic semantic
parsing is a large and growing field (Moldovan et
al., 2004; Litkowski, 2004; Baldewein et al, 2004),
two problems present themselves. First, output from
148
an automated parser is not typically compatible with
FrameNet markup. Although this is mostly ?a sim-
ple matter of programming?, some linguistic tools
must be developed to convert between formats (e.g.
to infer FrameNet phrase types from part-of-speech
tags).5 Second, it is not yet clear how the inevitable
errors introduced by the parser will affect the Mu-
taphrase algorithm6. We plan to use application-
dependent measures to judge the effects of parsing
errors.
Certain types of semantic ill-formedness cannot
be detected by the current version of Mutaphrase. A
typical example is I like sipping beef as a paraphrase
of I like eating cheese. We can guarantee semantic
well-formedness by limiting paraphrases to morpho-
logically related words (e.g. consume, consumption)
and/or by choosing only the FrameNet LUs which
are in the same WordNet (Fellbaum, 1998; Word-
Net, 2006) synset or higher in the WN hierarchy
than the original LU (e.g. eat to consume). Clearly
this will exclude many well-formed paraphrases, so
for tasks in which breadth is more important than
accuracy of paraphrase, we anticipate experiment-
ing with WordNet hierarchy distances between the
original and paraphrase LUs as a quantitative mea-
sure of semantic similarity as a proxy for semantic
well-formedness.
Currently, paraphrase scores are computed sim-
ply from the frequency of a particular valence in
FrameNet data. We plan to significantly extend
scoring to simultaneously rate each paraphrase on
its WordNet similarity, syntactic edit distance7, and
language model scores. We also plan to measure the
correlation between these estimated scores and both
human-judged paraphrase accuracy and application
dependent metrics, e.g. extension of in-domain lan-
guage models by paraphrase.
WordNet can also be used to provide additional
paraphrases beyond the particular valences attested
in FrameNet. For example, we plan to use WordNet
5It is worth noting that the current SemEval competition
(FrameNet, 2007a) should lead to more complete automatic
FrameNet-style annotation.
6An anecdotal example from a semantic parse of I was pre-
pared for a hound, but not for such a creature as this. (Doyle,
1902) assigns prepared to the Cooking creation frame, leading
to the interesting paraphrase I was tenderized for a hound....
7We plan to base the syntactic distance on the edit distance
between the original and paraphrase syntactic valences.
to generate synonyms of target words so that, for ex-
ample, adore could be used anywhere like is used
even if adore never appears in the FrameNet data.
Finally, the structure of the Mutaphrase algorithm
makes multi-lingual paraphrase possible. This re-
quires FrameNet-like data in other languages, and
several projects are underway to provide just such
a resource (FrameNet, 2007d; FrameNet, 2007c;
SALSA, 2007). We plan to exploit these as they be-
come available.
5 Conclusions
We have presented the Mutaphrase algorithm, a sys-
tem for generating a large set of paraphrases of se-
mantically marked input sentences using FrameNet.
The generated sentences range widely in their sim-
ilarity to the input sentence both in terms of syntax
and semantics. Various methods of filtering the out-
put for well-formedness and semantic and syntactic
similarity were presented.
Although the current implementation suffers from
a number of limitations, we believe these can be
addressed, eventually providing a fully automated
paraphrase system suitable for use in a variety of sta-
tistical natural language processing systems.
Acknowledgments
This work was partly supported by the European
Union 6th FWP IST Integrated Project AMI (Aug-
mented Multi-party Interaction, FP6-506811), and
by the Swiss National Science Foundation through
NCCR?s IM2 project.
References
U. Baldewein, K. Erk, S. Pado?, and D. Prescher. 2004.
Semantic role labelling with similarity-based general-
ization using EM-based clustering. In R. Mihalcea and
P. Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Seman-
tic Analysis of Text, pages 64?68, Barcelona, Spain,
July. Association for Computational Linguistics.
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proceedings of the
43rd annual meeting of the Association for Computa-
tional Linguistics (ACL), pages 597?604, Ann Arbor,
June.
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
149
alignment. In Proceedings of the Human Language
Technology Conference (HLT), pages 16?23, Edmon-
ton, Canada, May.
R. Barzilay and K. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 50?57, Toulouse, July.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proceedings of the Human Language Tech-
nology Conference (HLT), pages 17?24, New York
City, June.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics (COLING), Geneva, Switzerland,
August.
A.C. Doyle. 1902. Hound of the Baskervilles. Project
Gutenburg web site.
http://www.gutenberg.org/dirs/etext02/bskrv11a.txt.
K. Erk and S. Pado?. 2006. Shalmaneser ? a flex-
ible toolbox for semantic role assignment. In Pro-
ceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation (LREC), pages 527?
532, Genoa, Italy, May.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. The MIT Press, May.
T. Fontenelle, editor. 2003. International Journal of Lex-
icography Special Issue on FrameNet and Frame Se-
mantics. Oxford University Press, September. volume
16(3).
FrameNet. 2007a. The FrameNet task on SemEval web
site. http://nlp.cs.swarthmore.edu/semeval/
tasks/task19/summary.shtml.
FrameNet. 2007b. FrameNet web site.
http://framenet.icsi.berkeley.edu.
Japanese FrameNet. 2007c. Japanese FrameNet web
site. http://jfn.st.hc.keio.ac.jp/.
Spanish FrameNet. 2007d. Spanish FrameNet web site.
http://gemini.uab.es:9080/SFNsite.
A. Ibrahim, B. Katz, and J. Lin. 2003. Extracting struc-
tural paraphrases from aligned monolingual corpora.
In Proceedings of the Second International Workshop
on Paraphrasing, pages 57?64, Sapporo, Japan, July.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proceedings of the Human
Language Technology Conference (HLT), pages 455?
462, New York City, June.
R. Kittredge. 2002. Paraphrasing for condensation in
journal abstracting. Journal of Biomedical Informat-
ics, 35(4):265?277.
D. Lin and P. Pantel. 2001. Discovery of inference rules
for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
K. Litkowski. 2004. Senseval-3 task: Automatic labeling
of semantic roles. In R. Mihalcea and P. Edmonds,
editors, Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text, pages 9?12, Barcelona, Spain, July. Association
for Computational Linguistics.
D. Moldovan, R. G??rju, M. Olteanu, and O. Fortu. 2004.
SVM classification of FrameNet semantic roles. In
Rada Mihalcea and Phil Edmonds, editors, Senseval-
3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 167?
170, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based
alignment of multiple translations: Extracting para-
phrases and generating new sentences. In Proceed-
ings of the Human Language Technology Conference
(HLT), pages 102?109, Edmonton, Canada, May.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolingual
machine translation for paraphrase generation. In Pro-
ceedings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing, pages 142?149,
Barcelona Spain, July.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla?. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the Second In-
ternational Workshop on Paraphrasing, pages 25?32,
July.
SALSA. 2007. SALSA Project web site.
http://www.coli.uni-saarland.de/projects/salsa/.
Y. Shinyama, S. Sekine, K. Sudo, and R. Grishman.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of Human Language Tech-
nology Conference (HLT), pages 40?46, San Diego,
March.
WordNet. 2006. WordNet web site.
http://wordnet.princeton.edu.
I. Zukerman and B. Raskutti. 2002. Lexical query para-
phrasing for document retrieval. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING), pages 1?7, Taipei, Taiwan, Au-
gust.
150
