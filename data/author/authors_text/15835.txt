Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 266?275,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Aspectual Type and Temporal Relation Classification
Francisco Costa
Universidade de Lisboa
fcosta@di.fc.ul.pt
Anto?nio Branco
Universidade de Lisboa
Antonio.Branco@di.fc.ul.pt
Abstract
In this paper we investigate the relevance of
aspectual type for the problem of temporal
information processing, i.e. the problems
of the recent TempEval challenges.
For a large list of verbs, we obtain sev-
eral indicators about their lexical aspect by
querying the web for expressions where
these verbs occur in contexts associated
with specific aspectual types.
We then proceed to extend existing solu-
tions for the problem of temporal informa-
tion processing with the information ex-
tracted this way. The improved perfor-
mance of the resulting models shows that
(i) aspectual type can be data-mined with
unsupervised methods with a level of noise
that does not prevent this information from
being useful and that (ii) temporal informa-
tion processing can profit from information
about aspectual type.
1 Introduction
Extracting the temporal information present in a
text is relevant to many natural language process-
ing applications, including question-answering,
information extraction, and even document sum-
marization, as summaries may be more readable
if they follow a chronological order.
Recent evaluation campaigns have focused on
the extraction of temporal information from writ-
ten text. TempEval (Verhagen et al 2007), in
2007, and more recently TempEval-2 (Verhagen
et al 2010), in 2010, were concerned with this
problem. Additionally, they provided data that
can be used to develop and evaluate systems that
can automatically temporally tag natural language
text. These data are annotated according to the
TimeML (Pustejovsky et al 2003) scheme.
Figure 1 shows a small and slightly simpli-
fied fragment of the data from TempEval, with
TimeML annotations. There, event terms, such
as the term referring to the event of releasing the
tapes, are annotated using EVENT tags. States
(such as the situations denoted by verbs like want
or love) are also considered events. Temporal ex-
pressions, such as today, are enclosed in TIMEX3
tags. The attribute value of time expressions
holds a normalized representation of the date or
time they refer to (e.g. the word today denotes the
date 1998-01-14 in this example). The TLINK
elements at the end describe temporal relations
between events and temporal expressions. For in-
stance, the event of the plane going down is anno-
tated as temporally preceding the date denoted by
the temporal expression today.
The major tasks of these two TempEval evalu-
ation challenges were about guessing the type of
temporal relations, i.e. the value of the relType
attribute of the TLINK elements in Figure 1, all
other annotations being given. Temporal relation
classification is also the most interesting problem
in temporal information processing. The other
relevant tasks (identifying and normalizing tem-
poral expressions and events) have a longer re-
search history and show better evaluation results.
TempEval was organized in three tasks
(TempEval-2 has four additional ones, that are not
relevant to this work): task A was concerned with
classifying temporal relations holding between an
event and a time mentioned in the same sentence
(although they could be syntactically unrelated, as
the temporal relation represented by the TLINK
with the lid with the value l1 in Figure 1); task
266
<s>In Washington <TIMEX3 tid="t53" type="DATE"
value="1998-01-14">today</TIMEX3>, the Federal
Aviation Administration <EVENT eid="e1"
class="OCCURRENCE" stem="release"
aspect="NONE" tense="PAST" polarity="POS"
pos="VERB">released</EVENT> air traffic control tapes from
<TIMEX3 tid="t54" type="TIME"
value="1998-XX-XXTNI">the night</TIMEX3> the TWA
Flight eight hundred <EVENT eid="e2"
class="OCCURRENCE" stem="go" aspect="NONE"
tense="PAST" polarity="POS"
pos="VERB">went</EVENT> down.</s>
<TLINK lid="l1" relType="BEFORE" eventID="e2"
relatedToTime="t53"/>
<TLINK lid="l2" relType="OVERLAP"
eventID="e2" relatedToTime="t54"/>
Figure 1: Sample of the data annotated for TempEval,
corresponding to the fragment: In Washington today,
the Federal Aviation Administration released air traf-
fic control tapes from the night the TWA Flight eight
hundred went down.
Task
A B C
Best system 0.62 0.80 0.55
Average of all participants 0.56 0.74 0.51
Majority class baseline 0.57 0.56 0.47
Table 1: Results for English in TempEval (F-measure),
from Verhagen et al(2009)
B focused on the temporal relation between events
and the document?s creation time, which is also
annotated in TimeML (not shown in that Figure);
and task C was about classifying the temporal re-
lation between the main events of two consecu-
tive sentences. The possible values for the type
of temporal relation are BEFORE, AFTER and
OVERLAP.1
Table 1 shows the results of the first TempEval
evaluation. The results of TempEval-2 are fairly
similar (Verhagen et al 2010), but the data used
are similar but not identical.
The best system in TempEval for tasks A and B
(Pus?cas?u, 2007) combined statistical and knowl-
edge based methods to propagate temporal con-
straints along parse trees coming from a syntac-
tic parser. The best system for task C (Min et
1There are the additional disjunctive values
BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER and
VAGUE, employed when the annotators could not make a
more specific decision, but these affect a small number of
instances.
al., 2007) also combined rule-based and machine
learning approaches. It employed sophisticated
NLP to compute some of the features used; more
specifically it used syntactic features.
Our goal with this work is to evaluate the im-
pact of information about aspectual type on these
tasks. The TimeML annotations include an at-
tribute class for EVENTs that encodes some as-
pectual information, distinguishing between sta-
tive (annotated with the value STATE) and non-
stative events (value OCCURRENCE). This at-
tribute is relevant to the classification problem at
hand, i.e. it is a useful feature for machine learned
classifiers for the TempEval tasks (although this
class attribute encodes other kinds of informa-
tion as well). However, aspectual distinctions can
be more fine-grained than a mere binary distinc-
tion, and so far no system has explored this sort of
information to help improve the solutions to tem-
poral relation classification.
In this paper we work with Portuguese, but in
principle there is no reason to believe that our
findings would not apply to other languages that
display similar aspectual phenomena, such as En-
glish. Some of the details, such as the material
in Section 4.2, are however language specific and
would need adaptation.
2 Aspectual Type
Distinctions of aspectual type (also referred to as
situation type, lexical aspect or Aktionsart) of the
sort of Vendler (1967) and Dowty (1979) are ex-
pected to improve the existing solutions to the
problem of temporal relation classification. The
major aspectual distinctions are between (i) states
(e.g. to hate beer, to know the answer, to own a
car, to stink), (ii) processes, also called activities
(to work, to eat ice cream, to grow, to play the
piano), (iii) culminated processes, also called ac-
complishments (to paint a picture, to burn down,
to deliver a sermon) and (iv) culminations, also
called achievements (to explode, to win the game,
to find the key). States and processes are atelic
situations in that they do not make salient a spe-
cific instant in time. Culminated processes and
culminations are telic situations: they have an in-
trinsic, instantaneous endpoint, called the culmi-
nation (e.g. in the case of to paint a picture, it is
the moment when the picture is ready; in the case
of to explode, it is the moment of the explosion).
There are several reasons to think aspectual
267
type is relevant to temporal information pro-
cessing. First, these distinctions are related to
how long events last: culminations are punctual,
whereas states can be very prolonged in time.
States are thus more likely to temporally overlap
other temporal entities than culminations, for in-
stance.
Second, there are grammatical consequences
on how events are anchored in time. Consider
the following examples, from Ritchie (1979) and
Moens and Steedman (1988):
(1) When they built the 59th Street bridge,
they used the best materials.
(2) When they built that bridge, I was still a
young lad.
The situation of building the bridge is a cul-
minated processed, composed by the process of
actively building a bridge followed by the culmi-
nation of the bridge being finished. In sentence
(1), the event described in the main clause (that of
using the best materials) is a process, but in sen-
tence (2) it is a state (the state of being a young
lad). Even though the two clauses in each sen-
tence are connected by when, the temporal rela-
tions holding between the events of each clause
are different. On the one hand, in sentence (1)
the event of using the best materials (a process)
overlaps with the process of actively building the
bridge and precedes the culmination of finishing
the bridge. On the other hand, in sentence (2)
the event of being a young lad (which is a state)
overlaps with both the process of actively build-
ing the bridge and the culmination of the bridge
being built. This difference is arguably caused by
the different aspectual types of the main events of
each sentence.
As another example, states overlap with tem-
poral location adverbials, as in (3), while culmi-
nations are included in them, as in (4).
(3) He was happy last Monday.
(4) He reached the top of Mount Everest last
Monday.
In other cases, differences in aspectual type can
disambiguate ambiguous linguistic material. For
instance, the preposition in is ambiguous as it can
be used to locate events in the future but also to
measure the duration of culminated processes; it
is thus ambiguous with culminated processes, as
in he will read the book in three days but not with
other aspectual types, as in he will be living there
in three days.
A factor related to aspectual class, that is not
trivial to account for, is the phenomenon of as-
pectual shift, or aspectual coercion (Moens and
Steedman, 1988; de Swart, 1998; de Swart, 2000).
Many linguistic contexts pose constraints on as-
pectual type. This does not mean, however, that
clashes of aspectual type cause ungrammatical-
ity. What often happens is that phrases associated
with an incompatible aspectual type get their type
changed in order to be of the required type, caus-
ing a change in meaning.
For instance, the progressive construction com-
bines with processes. When it combines with e.g.
a culminated process, the culmination is stripped
off from this culminated process, which is thus
converted into a process. The result is that a sen-
tence like (5) does not say that the bridge was fin-
ished (the event has no culmination), whereas one
such as (6) does say this (the event has a culmina-
tion).
(5) They were building that bridge.
(6) They built that bridge.
Aspectual type is not a property of just words,
but phrases as well. For example, while the
progressive construction just mentioned combines
with processes, the resulting phrase behaves as a
state (cf. the sentence When they built the 59th
Street bridge, they were using the best materi-
als and what was mentioned above about when
clauses).
3 Strategy
Aspectual type is hard to annotate. This is partly
because of what was just mentioned: it is not a
property of just words, but rather phrases, and
different phrases with the same head word can
have different aspectual types; however anno-
tation schemes like TimeML annotate the head
word as denoting events, not full phrases or
clauses.
For this reason, our strategy is to obtain aspec-
tual type information from unannotated data. Be-
cause these data are gradient?an event-denoting
word can be associated with different aspectual
types, depending on word sense?we do not aim
to extract categorical information, but rather nu-
268
meric values for each event term that reflect as-
sociations to aspectual types. These may be seen
as values that are indicative of the frequencies in
which an event term denotes a state, or a process,
etc.
In order to extract these indicators, we resort to
a methodology sometimes referred to as Google
Hits: large amounts of queries are sent to a web
search engine (not necessarily Google), and the
number of search results (the number of web
pages that match the query) is recorded and taken
as a measure of the frequency of the queried ex-
pression.
This methodology is not perfect, since multiple
occurrences of the queried expression in the same
web page are not reflected in the hit count, and
in many cases the hit counts reported by search
engines are just estimates and might not be very
accurate. Additionally, uncarefully formulated
queries can match expressions that are syntacti-
cally and semantically very different from what
was intended. In any case, it has the advantages
of being based on a very large amount of data and
not requiring any manual annotation, which can
introduce errors.
3.1 The Web as a Very Large Corpus
Hearst (1992) is one of the earliest studies where
specific textual patterns are used to extract lexico-
semantic information from very large corpora.
The author?s goal was to extract hyponymy rela-
tions. With the same goal, Kozareva et al(2008)
apply similar textual patterns to the web.
The web has been used as a corpus by many
other authors with the purpose of extracting syn-
tactic or semantic properties of words or re-
lations between them, e.g. Ravichandran and
Hovy (2002), Etzioni et al(2004), etc. Some
of this work is specially relevant to the problem
of temporal information processing. VerbOcean
(Chklovski and Pantel, 2004) is a database of
web mined relations between verbs. Among other
kinds of relations, it includes typical precedence
relations, e.g. sleeping happens before waking up.
This type of information has in fact been used by
some of the participating systems of TempEval-2
(Ha et al 2010), with good results.
More generally, there is a large body of work
focusing on lexical acquisition from corpora. Just
as an example, Mayol et al(2005) learn subcate-
gorization frames of verbs from large amounts of
data. Relevant to our work is that of Siegel and
McKeown (2000). The authors guess the aspec-
tual type of verbs by searching for specific pat-
terns in a one million word corpus that has been
syntactically parsed. They extract several linguis-
tic indicators and combine them with machine
learning algorithms. The indicators that they ex-
tract are naturally different from ours, since they
have access to syntactic structure and we do not,
but our data are based on a much larger corpus.
3.2 Textual Patterns as Indicators of
Aspectual Type
Because of aspectual shift phenomena (see Sec-
tion 2), full syntactic parsing is necessary in order
to determine the aspectual type of a natural lan-
guage expression. However, this can be approxi-
mated by frequencies: it is natural to expect that
e.g. stative verbs occur more frequently in stative
contexts than non-stative verbs, even if there may
be errors in determining these contexts if syntactic
parsing is not a possibility.
If one uses Google Hits, syntactic information
is not accessible. In return for its impreciseness,
Google Hits have the advantage of being based on
very large amounts of data.
4 Scope and Approach
In this study we focus exclusively on verbs, but
events can be denoted by words belonging to
other parts-of-speech. This limitation is linked to
the fact that the textual patterns that are used to
search for specific aspectual contexts are sensitive
to part-of-speech (i.e. what may work for a verb
may not work equally well for a noun).
In order to assess whether aspectual type in-
formation is relevant to the problem of temporal
relation classification, our approach is to check
whether incorporating that kind of information
into existing solutions for this problem can im-
prove their performance. TimeML annotated
data, such as those used for TempEval, can be
used to train machine learned classifiers. These
can then be augmented with attributes encoding
aspectual type information and their performance
compared to the original classifiers.
Additionally, we work with Portuguese data.
This is because our work is part of an effort to
implement a temporal processing system for Por-
tuguese. We briefly describe the data next.
269
<s>Em Washington, <TIMEX3 tid="t53" type="DATE"
value="1998-01-14">hoje</TIMEX3>, a Federal Aviation
Administration <EVENT eid="e1" class="OCCURRENCE"
stem="publicar" aspect="NONE" tense="PPI"
polarity="POS" pos="VERB">publicou</EVENT>
gravac?o?es do controlo de tra?fego ae?reo da <TIMEX3
tid="t54" type="TIME"
value="1998-XX-XXTNI">noite</TIMEX3> em que o voo
TWA800 <EVENT eid="e2" class="OCCURRENCE"
stem="cair" aspect="NONE" tense="PPI"
polarity="POS" pos="VERB">caiu</EVENT>.</s>
<TLINK lid="l1" relType="BEFORE" eventID="e2"
relatedToTime="t53"/>
<TLINK lid="l2" relType="OVERLAP"
eventID="e2" relatedToTime="t54"/>
Figure 2: Sample of the Portuguese data adapted from
the TempEval data, corresponding to the fragment: Em
Washington, hoje, a Federal Aviation Administration
publicou gravac?o?es do controlo de tra?fego ae?reo da
noite em que o voo TWA800 caiu.
4.1 Data
Our experiments used TimeBankPT (Costa and
Branco, 2010; Costa and Branco, 2012; Costa, to
appear). This corpus is an adaptation of the orig-
inal TempEval data to Portuguese, obtained by
translating it and then adapting the annotations.
Figure 2 shows the Portuguese equivalent to the
sample presented above in Figure 1. The two cor-
pora are quite similar, but there is of course the
language difference. TimeBankPT contains a few
corrections to the data (mostly the temporal rela-
tions), but these corrections only changed around
1.2% of the total number of annotated temporal
relations (Costa and Branco, 2012). Although we
did not test our results on English data, we specu-
late that our results carry over to other languages.
Just like the original English corpus for
TempEval, it is divided in a training part and a
testing part. The numbers (sentences, words, an-
notated events, time expressions and temporal re-
lations) are fairly similar for the two corpora (the
English one and the Portuguese one).
4.2 Extracting the Aspectual Indicators
We extracted the 4,000 most common verbs from
a 180 million word corpus of Portuguese news-
paper text, CETEMPu?blico. Because this corpus
is not annotated, we used a part-of-speech tag-
ger and morphological analyzer (Barreto et al
2006; Silva, 2007) to detect verbs and to obtain
their dictionary form. We then used an inflection
tool (Branco et al 2009) to generate the specific
verb forms that are used in the queries. They are
mostly third person singular forms of several dif-
ferent tenses.
The indicators that we used are ratios of Google
Hits. They compare two queries.
Several indicators were tested. We provide ex-
amples with the verb fazer ?do? for the queries
being compared by each indicator. The name of
each indicator reflects the aspectual type being
tested, i.e. states should present high values for
State Indicators 1 and 2, processes should show
high values for Process Indicators 1?4, etc.
? State Indicator 1 (Indicator S1) is about im-
perfective and perfective past forms of verbs.
It compares the number of hits a for an im-
perfective form fazia ?did? to the number of
hits b for a perfective form fez ?did?: aa+b .
Assuming the imperfective past constrains
the entire clause to be a state, and the perfec-
tive past constrains it to be telic, the higher
this value the more frequently the verb ap-
pears in stative clauses in a past tense.2
? State Indicator 2 (Indicator S2) is about the
co-occurrence with acaba de ?has just fin-
ished?. It compares the number of hits a
for acaba de fazer ?has just finished doing?
to the number of hits b for fazer ?to do?:
b
a+b . In Portuguese, this construction does
not seem to be felicitous with states.
? Process Indicator 1 (Indicator P1) is about
past progressive forms and simple past forms
(both imperfective). It compares the num-
ber of hits a for fazia ?did? to the number of
hits b for estava a fazer ?was doing?: ba+b .
Assuming the progressive construction is a
function from processes to states (see Sec-
tion 2), the higher this value, the more likely
the verb can occur with the interpretation of
a process.
2We expect this frequency to be indicative of states be-
cause states can appear in the imperfective past tense with
their interpretation unchanged, whereas non-stative events
have their interpretation shifted to a stative one in that con-
text (e.g. they get a habitual reading). In order to refer to an
event occurring in the past with an on-going interpretation,
non-stative verbs require the progressive construction to be
used in Portuguese, whereas states do not. Therefore, states
should occur more freely in the simple imperfective past.
270
? Process Indicator 2 (Indicator P2) is about
past progressive forms vs. simple past forms
(perfective). It compares the number of hits
a for fez ?did? to the number of hits b for
esteve a fazer ?was doing?: ba+b . Similarly
to the previous indicator, this one tests the
frequency of a verb appearing in a context
typical of processes.
? Process Indicator 3 (Indicator P3) is about
the occurrence of for Adverbials. It com-
pares the number of hits a for fez ?did? to
the number of hits b for fez durante muito
tempo ?did for a long time?: ba+b . This
number is also intended to be an indica-
tion of how frequent a verb can be used
with the interpretation of a process. Note
that Portuguese allows modifiers to occur
freely between a verb and its complements,
so this test should work for transitive verbs
(or any other subcategorization frame involv-
ing complements), not just intransitive ones.
? Process Indicator 4 (Indicator P4) is about
the co-occurrence of a verb with parar de ?to
stop?. It compares the number of hits a for
parou de fazer ?stopped doing? to the num-
ber of hits b for fazer ?to do?: aa+b . Just like
the English verbs stop and finish are sensitive
to the aspectual type of their complement, so
is the Portuguese verb parar, which selects
for processes.
? Atelicity Indicator 1 (Indicator A1) is about
comparing in and for adverbials. It compares
the number of hits a for fez num instante ?did
in an instant? to the number of hits b for fez
durante muito tempo ?did for a long time?:
b
a+b . Processes can be modified by for ad-
verbials, whereas culminated processes are
modified by in adverbials. This indicator
tests the occurrence of a verb in contexts that
require these aspectual types.
? Atelicity Indicator 2 (Indicator A2) is about
comparing for Adverbials with suddenly. It
compares the number of hits a for fez de re-
pente ?did suddenly? to the number of hits
b for fez durante muito tempo ?did for a
long time?: ba+b . De repente ?suddenly?
seems to modify culminations, so this indi-
cator compares process readings with culmi-
nation readings.
? Culmination Indicator1 (Indicator C1) is
about differentiating culminations and cul-
minated processes. It compares the number
of hits a for fez de repente ?did suddenly? to
the number of hits b for fez num instante ?did
in an instant?: aa+b .
For each of the 4,000 verbs, the necessary
queries required by these indicators were gener-
ated and then sent to a search engine. The queries
were enclosed in quotes, so as to guarantee ex-
act matches. The number of hits was recorded for
each query.
We had some problems with outliers for a few
rather infrequent verbs. These could show very
extreme values for some indicators. In order
to minimize their impact, for each indicator we
homogenized the 100 highest values that were
found. More specifically, for each indicator, each
one of the highest 100 values was replaced by the
100th highest value. The bottom 100 values were
similarly changed. This way the top 99 values and
the bottom 99 values are replaced by the 100th
highest value and the 100th lowest value respec-
tively.
Each indicator ranges between 0 and 1 in the-
ory. In practice, we seldom find values close to the
extremes, as this would imply that some queries
would have close to 0 hits, which does not occur
very often (after all, we intentionally used queries
for which we would expect large hit counts, as
these are more likely to be representative of true
language use). For this reason, each indicator is
scaled so that its minimum (actual) value is 0 and
its maximum (actual) value is 1.
5 Evaluation
As mentioned before, in order to assess the use-
fulness of these aspectual indicators for the tasks
of temporal relation classification, we checked
whether they can improve machine learned clas-
sifiers trained for this problem. We next describe
the classifiers that were used as the bases for com-
parison.
5.1 Experimental Setup
In order to obtain bases for comparison, we
trained machine learned classifiers on the Por-
tuguese corpus TimeBankPT, that is adapted from
the TempEval data (see Section 4.1). We took
inspiration in the work of Hepple et al(2007).
271
This was one of the participating systems of
TempEval. It used machine learning algorithms
implemented in Weka (Witten and Frank, 1999).
For our experiments, we used Weka?s implemen-
tation of the C4.5 algorithm, trees.J48 (Quin-
lan, 1993), the RIPPER algorithm as implemented
by Weka?s rules.JRip (Cohen, 1995), a near-
est neighbors classifier, lazy.KStar (Cleary
and Trigg, 1995), a Na??ve Bayes classifier, namely
Weka?s bayes.NaiveBayes (John and Lang-
ley, 1995), and a support vector classifier, Weka?s
functions.SMO (Platt, 1998) . We chose these
algorithms as they are representative of a wide
range of machine learning approaches.
Recall that the tasks of TempEval are to guess
the type of temporal relations. Each train or test
instance thus corresponds to a temporal relation,
i.e. a TLINK element in the TimeML annota-
tions (see Figures 1 and 2). The classification
problem is to determine the value of the attribute
relType of TimeML TLINK elements. These
temporal relations relate an event (referred by the
eventID attribute of TLINK elements) to an-
other temporal entity, that can be a time (pointed
to by the relatedToTime attribute), in the case
of tasks A and B, or, in the case of task C, an-
other event (given by the relatedToEvent at-
tribute).
As for the features that were employed, we also
took inspiration in the approach of Hepple et al
(2007). These authors used as classifier attributes
two types of features. The first group of features
corresponds to TimeML attributes: for instance
the value of the aspect attribute of EVENT el-
ements, for the events involved in the temporal
relation to be classified. The second group of fea-
tures corresponds to simple features that can be
computed with string manipulation and do not re-
quire any kind of natural language processing.
Table 2 shows the features that were tried and
employed.
The event features correspond to attributes
of EVENT elements, with the exception of
the event-string feature, which takes as
value the character data inside the correspond-
ing TimeML EVENT element. In a simi-
lar spirit, the timex3 features are taken from
the attributes of TIMEX3 elements with the
same name. The tlink-relType feature
is the class attribute and corresponds to the
relType attribute of the TimeML TLINK el-
Task
Attribute A B C
event-aspect ? X X
event-polarity X X X
event-POS ? ? X
event-stem ? X ?
event-string X ? ?
event-class X ? X
event-tense X X X
order-event-first X N/A N/A
order-event-between X N/A N/A
order-timex3-between ? N/A N/A
order-adjacent X N/A N/A
timex3-mod X ? N/A
timex3-type ? ? N/A
tlink-relType X X X
Table 2: Feature combinations used in the classifiers
used as comparison bases. Features inspired by the
ones used by Hepple et al(2007) in TempEval.
ement that represents the temporal relation to
be classified. The order features are the at-
tributes computed from the document?s textual
content. The feature order-event-first
encodes whether the event terms precedes in
the text the time expression it is related to by
the temporal relation to classify. The clas-
sifier attribute order-event-between de-
scribes whether any other event is mentioned
in the text between the two expressions for
the entities that are in the temporal relation,
and similarly order-timex3-between is
about whether there is an intervening tempo-
ral expression. Finally, order-adjacent is
true iff both order-timex3-between and
order-event-between are false (even if
other linguistic material occurs between the ex-
pressions denoting the two entities in the temporal
relation).
In order to arrive at the final set of features
(marked with a check mark in Table 2), we per-
formed exhaustive search on all possible combi-
nations of these features for each task, using the
Na??ve Bayes algorithm. They were compared us-
ing 10-fold cross-validation on the training data.
The feature combinations shown in Table 2 are
the optimal combinations arrived at in this way.
These are the classifiers that we used for the
272
comparison with the aspectual type indicators.
We chose this straightforward approach because it
forms a basis for comparison that is easily repro-
ducible: the algorithm implementations that were
used are part of freely available software, and the
features that were employed are easily computed
from the annotated data, with no need to run any
natural language processing tools whatsoever.
As mentioned before in Section 4.1, the data
used are organized in a training set and an evalu-
ation set. The training part is around 60K words
long, the test data containing around 9K words.
When tested on held-out data, these classifiers
present the scores shown in italics in Table 3.
These results are fairly similar to the scores that
the system of Hepple et al(2007) obtained in
TempEval with English data: 0.59 for task A, 0.73
for task B, and 0.54 for task C. They are also not
very far from the best results of TempEval. As
such they represent interesting bases for compar-
ison, as improving their performance is likely to
be relevant to the best systems that have been de-
veloped for temporal information processing.
5.2 Results and Discussion
After obtaining the bases for comparison de-
scribed above, we proceeded to check whether the
aspectual type indicators described in Section 4.2
can improve these results.
For each aspectual indicator, we implemented
a classifier feature that encodes its value for the
event term in the temporal relation (if it is not a
verb, this value is missing). In the case of task C,
two features are added for each indicator, one for
each event term.
We extended each of these classifiers with one
of these features at a time (two in the case of task
C), and checked whether it improved the results
on the test data. So for instance, in order to test
Indicator S1, we extended each of these classifiers
with a feature that encodes the value that this indi-
cator presents for the term that denotes the event
present in the temporal relation to be classified.
In the case of task C, two classifier features are
added, one for each event term, and both for the
same Indicator S1. For instance, for the (train-
ing) instance corresponding to the TLINK in Fig-
ure 2 with the lid attribute that has the value l1,
the classifier feature for Indicator S1 has the value
that was computed for the verb cair ?go down?,
since this is the stem of the word that denotes
Task
Classifier A B C
trees.J48 0.57 0.77 0.53
With best indicator 0.55
rules.JRip 0.60 0.76 0.51
With best indicator 0.61 0.54
lazy.KStar 0.54 0.70 0.52
With best indicator 0.73 0.53
bayes.NaiveBayes 0.50 0.76 0.53
With best indicator 0.53 0.54
functions.SMO 0.55 0.79 0.54
With best indicator 0.56 0.55
Table 3: Evaluation on held-out test data of classi-
fiers trained on full train data. Values for the classi-
fiers used as comparison bases are in italics. Boldface
highlights improvements resulting from incorporating
aspectual indicators as classifier features, and missing
values represent no improvement.
the event that is the first argument of this temporal
relation. After adding each of these features, we
retrained the classifiers on the training data and
tested them on the held-out test data. In order to
keep the evaluation manageable, we did not test
combinations of multiple indicators.
Table 3 shows the overall results. For task
A, the best indicators were P4 (with JRip), A1
(NaiveBayes) and S1 (SMO). For task B the
best one was P4 (KStar). For task C, the best
indicators were P3 (J48), A1 and P3 (JRip),
C1 (KStar), A1 (NaiveBayes) and P2 (SMO).
Each of the indicators S2, P1 and A2 either does
not improve the results or does so but not as much
as another, better indicator for the same task and
algorithm.
It seems clear from Table 3 that some tasks ben-
efit from these indicators more than others. In
particular, task C shows consistent improvements
whereas task B is hardly affected. Since task C
is about relations involving two events, the classi-
fiers may be picking up the sort of linguistic gen-
eralizations mentioned in Section 2 about when
clauses.
J48 and JRip produce human-readable mod-
els. We checked how these classifiers are taking
advantage of the aspectual indicators. For task C,
the induced models are generally associating high
273
values of the indicators A1 and P3 with overlap
relations and low values of these indicators with
other types of relations. This is expected. On the
one end, high values for these indicators are asso-
ciated with atelicity (i.e. the endpoint of the cor-
responding event is not presented). On the other
hand, both indicators are based on queries con-
taining the phrase durante muito tempo ?for a long
time?, which, in addition to picking up events that
can be modified by for adverbials, more specifi-
cally pick up events that happen for a long time
and are thus likely to overlap other events.
For task A, JRip also associates high values of
the indicator P4?which constitute evidence that
the corresponding events are processes (which are
atelic)?with overlap relations. This is a specially
interesting result, considering that the queries on
which this indicator is based reflect a purely as-
pectual constraint.
6 Concluding Remarks
In this paper, we evaluated the relevance of infor-
mation about aspectual type for temporal process-
ing tasks.
Temporal information processing has received
substantial attention recently with the two
TempEval challenges in 2007 and 2010. The most
interesting problem of temporal information pro-
cessing, that of temporal relation classification, is
still affected by high error rates.
Even though a very substantial part of the se-
mantics literature on tense and aspect focuses on
aspectual type, solutions to the problem of auto-
matic temporal relation classification have not in-
corporated this sort of semantic information. In
part this is expected, as aspectual type is very in-
terconnected with syntax (cf. the discussion about
aspectual coercion in Section 2), and the phe-
nomenon of aspect shift can make it hard to com-
pute even when syntactic information is available.
Our contribution with this paper is to incor-
porate this sort of information in existing ma-
chine learned classifiers that tackle this problem.
Even though these classifiers do not have access to
syntactic information, aspectual type information
seemed to be useful in improving the performance
of these models. We hypothesize that combin-
ing aspectual type information with information
about syntactic structure can further improve the
problems of temporal information processing, but
we leave that research to future work.
An interesting question that we hope will be ad-
dressed by future work is how these results extend
to other languages. We cannot provide an answer
to this question, as we do not have the data. How-
ever, this experiment can be replicated for any lan-
guage that has (i) TimeML annotated data, (ii) a
reasonable size of documents on the Web and a
search engine capable of separating them from the
documents in other languages and (iii) an aspec-
tual system similar enough that the question be-
ing addressed in this paper makes sense (and use-
ful patterns for queries can be constructed, even
if not entirely identical to the ones that we used).
The second criterion is met by many, many lan-
guages. The third one also seems to affect many
languages, as the existing literature on aspectual
phenomena indicates that these phenomena are
quite widespread. The second criterion is, at the
moment, the hardest to fulfill as not many lan-
guages have data with rich annotations about time
(i.e. including events and temporal relations). We
speculate that our results can extend to English,
although a different set of query patterns may
have to be used in order to extract the aspectual
indicators that are employed. We believe this be-
cause the two languages largely overlap when it
comes to aspectual phenomena.
References
Florbela Barreto, Anto?nio Branco, Eduardo Ferreira,
Ama?lia Mendes, Maria Fernanda Nascimento, Fil-
ipe Nunes, and Joa?o Silva. 2006. Open resources
and tools for the shallow processing of Portuguese:
the TagShare project. In Proceedings of LREC
2006.
Anto?nio Branco, Francisco Costa, Eduardo Ferreira,
Pedro Martins, Filipe Nunes, Joa?o Silva, and Sara
Silveira. 2009. LX-Center: a center of online lin-
guistic services. In Proceedings of the Demo Ses-
sion, ACL-IJCNLP2009, Singapore.
Timothy Chklovski and Patrick Pantel. 2004. Verb-
Ocean: Mining the Web for fine-grained semantic
verb relations. In In Proceedings of EMNLP-2004,
Barcelona, Spain.
John G. Cleary and Leonard E. Trigg. 1995. K*: An
instance-based learner using an entropic distance
measure. In 12th International Conference on Ma-
chine Learning, pages 108?114.
William W. Cohen. 1995. Fast effective rule induc-
tion. In Proceedings of the Twelfth International
Conference on Machine Learning, pages 115?123.
Francisco Costa and Anto?nio Branco. 2010. Tempo-
ral information processing of a new language: Fast
274
porting with minimal resources. In Proceedings of
ACL 2010.
Francisco Costa and Anto?nio Branco. 2012. Time-
BankPT: A TimeML annotated corpus of Por-
tuguese. In Proceedings of LREC2012.
Francisco Costa. to appear. Processing Temporal In-
formation in Unstructured Documents. Ph.D. the-
sis, Universidade de Lisboa, Lisbon.
Henrie?tte de Swart. 1998. Aspect shift and coercion.
Natural Language and Linguistic Theory, 16:347?
385.
Henrie?tte de Swart. 2000. Tense, aspect and coer-
cion in a cross-linguistic perspective. In Proceed-
ings of the Berkeley Formal Grammar conference,
Stanford. CSLI Publications.
David R. Dowty. 1979. Word Meaning and Montague
Grammar: the Semantics of Verbs and Times in
Generative Semantics and Montague?s PTQ. Rei-
del, Dordrecht.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, , Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in Know-
ItAll. In Proceedings of the 13th International Con-
ference on World Wide Web.
Eun Young Ha, Alok Baikadi, Carlyle Licata, and
James C. Lester. 2010. NCSU: Modeling temporal
relations with Markov logic and lexical ontology. In
Proceedings of SemEval 2010.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th Conference on Computational Linguistics,
volume 2, pages 539?545, Nantes, France.
Mark Hepple, Andrea Setzer, and Rob Gaizauskas.
2007. USFD: Preliminary exploration of fea-
tures and classifiers for the TempEval-2007 tasks.
In Proceedings of SemEval-2007, pages 484?487,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
George H. John and Pat Langley. 1995. Estimating
continuous distributions in Bayesian classifiers. In
Eleventh Conference on Uncertainty in Artificial In-
telligence, pages 338?345, San Mateo.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL-08: HLT, pages 1048?1056, Columbus, Ohio.
Association for Computational Linguistics.
Laia Mayol, Gemma Boleda, and Toni Badia. 2005.
Automatic acquisition of syntactic verb classes with
basic resources. Language Resources and Evalua-
tion, 39(4):295?312.
Congmin Min, Munirathnam Srikanth, and Abraham
Fowler. 2007. LCC-TE: A hybrid approach to
temporal relation identification in news text. pages
219?222.
Marc Moens and Mark Steedman. 1988. Temporal
ontology and temporal reference. Computational
Linguistics, 14(2):15?28.
John Platt. 1998. Fast training of support vec-
tor machines using sequential minimal optimiza-
tion. In Bernhard Scho?lkopf, Chris Burges, and
Alexander J. Smola, editors, Advances in Kernel
Methods?Support Vector Learning.
Georgiana Pus?cas?u. 2007. WVALI: Temporal rela-
tion identification by syntactico-semantic analysis.
In Proceedings of SemEval-2007, pages 484?487,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003. TimeML: Robust specification of
event and temporal expressions in text. In IWCS-
5, Fifth International Workshop on Computational
Semantics.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo,
CA.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a question an-
swering system. In Proceedings of ACL 2002.
Graeme D. Ritchie. 1979. Temporal clauses in En-
glish. Theoretical Linguistics, 6:87?115.
Eric V. Siegel and Kathleen McKeown. 2000.
Learning methods to combine linguistic indica-
tors: Improving aspectual classification and reveal-
ing linguistic insights. Computational Linguistics,
24(4):595?627.
Joa?o Ricardo Silva. 2007. Shallow processing
of Portuguese: From sentence chunking to nomi-
nal lemmatization. Master?s thesis, Faculdade de
Cie?ncias da Universidade de Lisboa, Lisbon, Portu-
gal.
Zeno Vendler. 1967. Verbs and times. Linguistics in
Philosophy, pages 97?121.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, and James Pustejovsky. 2007.
SemEval-2007 Task 15: TempEval temporal re-
lation identification. In Proceedings of SemEval-
2007.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The TempEval challenge: iden-
tifying temporal relations in text. Language Re-
sources and Evaluation.
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 task 13:
TempEval-2. In Proceedings of SemEval-2010.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann,
San Francisco.
275
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 62?71,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Assigning Deep Lexical Types
Using Structured Classifier Features for Grammatical Dependencies
Joa?o Silva
University of Lisbon
Dept. Informatics, Faculty of Sciences
Campo Grande, Lisboa, Portugal
jsilva@di.fc.ul.pt
Anto?nio Branco
University of Lisbon
Dept. Informatics, Faculty of Sciences
Campo Grande, Lisboa, Portugal
antonio.branco@di.fc.ul.pt
Abstract
Deep linguistic grammars are able to pro-
vide rich and highly complex grammatical
representations of sentences, capturing, for
instance, long-distance dependencies and re-
turning a semantic representation. These
grammars lack robustness in the sense that
they do not gracefully handle words miss-
ing from their lexicon. Several approaches
have been explored to handle this problem,
many of which consist in pre-annotating the
input to the grammar with shallow processing
machine-learning tools. Most of these tools,
however, use features based on a fixed win-
dow of context, such as n-grams. We investi-
gate whether the use of features that encode
discrete structures, namely grammatical de-
pendencies, can improve the performance of
a machine learning classifier that assigns deep
lexical types. In this paper we report on the
design and evaluation of this classifier.
1 Introduction
Parsing is one of the fundamental tasks in Nat-
ural Language Processing and a critical step in
many applications. Many of the most com-
monly used parsers rely on probabilistic approaches.
These parsers are obtained through data-driven
approaches, by inferring a probabilistic language
model over a dataset of annotated sentences.
Though these parsers always produce some analy-
sis of their input sentences, they do not go into deep
linguistic analysis.
Deep grammars, also referred to as precision
grammars, seek to make explicit information about
highly detailed linguistic phenomena and produce
complex grammatical representations for their in-
put sentences. For instance, they are able to cap-
ture long-distance dependencies and produce the se-
mantic representation of a sentence. Although there
is a great variety of parsing methods (see (Mitkov,
2004) for an overview), all CKY-based algorithms
require a lexical look-up initialization step that, for
each word in the input, returns all its possible cate-
gories.
From this it follows that if any of the words in
a sentence is not present in the lexicon?an out-
of-vocabulary (OOV) word?a full parse of that
sentence is impossible to obtain. Given that nov-
elty is one of the defining characteristics of natu-
ral languages, unknown words will eventually oc-
cur. Hence, being able to handle OOV words is of
paramount importance if one wishes to use a gram-
mar to analyze unrestricted texts.
Another important issue is that of lexical ambigu-
ity. That is, words that may bear more than one lexi-
cal category. The combinatorial explosion of lexical
and syntactic ambiguity may hinder parsing due to
increased requirements in terms of parsing time and
memory usage. Thus, even if there were no OOV
words in the input, being able to assign syntactic cat-
egories to words prior to parsing may be desirable
for efficiency reasons.
For the shallower parsing approaches, such as
plain constituency parsing, it suffices to determine
the part-of-speech of words, so pre-processing the
input with a POS tagger is a common and effective
way to tackle either of these problems. However, the
linguistic information contained in the lexicon of a
62
deep grammar is much more fine-grained, includ-
ing, in particular, the subcategorization frame (SCF)
of the word, which further constraints what can be
taken as a well-formed sentence by imposing sev-
eral restrictions on co-occurring expressions.
Thus, what for a plain POS tagger corresponds to
a single category is often expanded into hundreds of
different distinctions, and hence tags, when at the
level of detail required by a deep grammar. For in-
stance, the particular grammar we will be using for
the study reported in this paper?a grammar follow-
ing the HPSG framework?has in its current ver-
sion a lexicon with roughly 160 types for verbs and
nearly 200 types for common nouns.
While the deep grammar may proceed with the
analysis knowing only the base POS category of a
word, it does so at the cost of vastly increased am-
biguity1 which may even allow the grammar to ac-
cept ungrammatical sentences as valid. This has lead
to research that specifically targets annotating words
with a tagset suitable for deep grammars.
Current approaches tend to use shallow features
with limited context (e.g. n-grams). However, given
that the SCF is one of the most relevant pieces of
information that is associated with a word in the
lexicon of a deep grammar, one would expect that
features describing the inter-word dependencies in
a sentence would be highly discriminative and help
to accurately assign lexical types. Accordingly,
in this paper we investigate the use of structured
features that encode grammatical dependencies in
a machine-learning classifier and how it compares
with state-of-the-art approaches.
Our study targets Portuguese, a Romance lan-
guage with a rich morphology, in particular in what
concerns verb inflection (see for instance, (Mateus et
al., 2003) for a detailed account of Portuguese gram-
mar and (Branco et al, 2008) for an assessment of
the issues raised by verbal ambiguity).
Paper outline: Section 2 provides an overview of
related work, with a focus on supertagging, and in-
troduces tree kernels as a way of handling structured
classifier features. Section 3 introduces the particu-
lar deep grammar that is used in this work and how it
supports the creation of the corpus that provides the
1For instance, a common noun POS tag could be taken as
being any of the nearly 200 common noun types existing in the
lexicon of the grammar we use in this paper.
data for training and evaluation of the classifier. The
classifier itself, and the features it uses, are described
in Section 4. Section 5 covers empirical evaluation
and comparison with other approaches. Finally, Sec-
tion 6 concludes with some final remarks.
2 Background and Related Work
The construction of a hand-crafted lexicon for a deep
grammar is a time-consuming task requiring trained
linguists. More importantly, such lexica are invari-
ably incomplete since they often do not cover spe-
cialized domains and are slow to incorporate new
words.
Accordingly, much research in this area has been
focused on automatic lexical acquisition (Brent,
1991; Briscoe and Carroll, 1997; Baldwin, 2005).
That is, approaches that try to discover all the lex-
ical types a given unknown word may occur with,
thus effectively creating a new lexical entry. How-
ever, at run-time, it is still up to the grammar using
the newly acquired lexical entry to choose which of
those lexical types is the correct one for each par-
ticular occurrence of that word; and, ultimately, one
can only acquire the lexicon entries for those words
that are present in the corpus. Thus, any system that
is constantly exposed to new text?e.g. parsing text
from the Web?will eventually come across some
unknown word that has not yet been acquired. More-
over, such words must be dealt with on-the-fly, since
it is unlikely that the system can afford to wait until
it has accumulated enough occurrences of the un-
known word to be able to apply offline lexicon ac-
quisition methods.
In the work reported in the present paper we use
a different approach, closer to what is known as su-
pertagging, where we assign on-the-fly a single lex-
ical type to a word.
2.1 Supertagging
POS tagging is a task that relies only on local infor-
mation (e.g. the word and a small window of con-
text) to achieve a form of syntactic disambiguation.
As such, POS tags are commonly assigned prior
to parsing as a way of reducing parsing ambiguity
by restricting words to a certain syntactic category.
Less ambiguity leads to a greatly reduced search
space and, as a consequence, faster parsing.
63
Supertagging, first introduced by Bangalore and
Joshi (1994), can be seen as a natural extension of
this idea to a richer tagset, in particular to one that
includes information on subcategorization frames.
In (Bangalore and Joshi, 1994) supertagging was
applied to the Lexicalized Tree Adjoining Grammar
(LTAG) formalism. As the name indicates, this is a
lexicalized grammar, like HPSG, but in LTAG each
lexical item is associated with one or more trees,
the elementary structures, which localize informa-
tion on dependencies, even long-range ones, by re-
quiring that all and only the dependents be present
in the structure.
The supertagger in (Bangalore and Joshi, 1994)
assigns an elementary structure to each word us-
ing a simple trigram model. The data for training
was obtained by taking the sentences of length un-
der 15 words in the Wall Street Journal together with
some other minor corpora, and parsing them with
XTAG, a wide-coverage grammar for English based
on LTAG. In addition, and due to data-sparseness,
POS tags were used in training instead of words.
Evaluation was performed over 100 held-out sen-
tences from the Wall Street Journal. For a tagset of
365 elementary trees, this supertagger achieved 68%
accuracy, which is far too low to be useful for pars-
ing.
In a later experiment, the authors improved
the supertagger by smoothing model parameters
and adding additional training data (Bangalore and
Joshi, 1999). The larger dataset was obtained by
extending the corpus from the previous experiment
with Penn Treebank parses that were automatically
converted to LTAG. The conversion process relied
on several heuristics, and though it is not perfect,
the authors found that the issues concerning conver-
sion were far outweighed by the benefit of increased
training data.
The improved supertagger increased accuracy to
92% (Bangalore and Joshi, 1999). The supertagger
can also assign the n-best tags, which increases the
chances of it assigning the correct supertag at the
cost of leaving more unresolved ambiguity. With 3-
best tagging, it achieved 97% accuracy.
A supertagger was also used by Clark and Curran
(2007), in their case for a Combinatory Categorial
Grammar (CCG). This formalism uses a set of log-
ical combinators to manipulate linguistic construc-
tion tough, for our purposes here, it matters only
that lexical items receive complex tags that describe
the constituents they require to create a well-formed
construction.
The set of 409 lexical categories to be assigned
was selected by taking those categories that occur at
least 10 times in sections 02?21 of a CCG automatic
annotation of Penn Treebank (CCGBank).
Evaluation was performed over section 00 of
CCGBank, and achieved 92% per word accuracy.
As with the LTAG supertagger, assigning more
than one tag can greatly increase accuracy. How-
ever, instead of a fixed n-best number of tags?
which might be to low, or too high, depending on
the case at hand?the CCG supertagger assigns all
tags with a likelihood within a factor ? of the best
tag. A value for ? as small as 0.1, which results in
an average of 1.4 tags per word, is enough to boost
accuracy up to 97%.
Supertagging for HPSG: There has been some
work on using supertagging together with the HPSG
framework. As with other works on supertag-
ging, it is mostly concerned with restricting the
parser search space in order to increase parsing ef-
ficiency, and not specifically with the handling of
OOV words.
Prins and van Noord (2003) present an HMM-
based supertagger for the Dutch Alpino grammar.
An interesting feature of their approach is that the
supertagger is trained over the output of the parser
itself, thus avoiding the need for a hand-annotated
dataset.
The supertagger was trained over 2 million sen-
tences of newspaper text parsed by Alpino. A gold
standard was created by having Alpino choose the
best parse for a set of 600 sentences. The supertag-
ger, when assigning a single tag (from a tagset with
2,392 tags), achieves a token accuracy close to 95%.
It is not clear to what extent these results can be
affected by some sort of bias in the disambiguation
module of Alpino, given that both the sequence of
lexical types in the training dataset and in the gold
standard are taken from the best parse produced by
Alpino.
Matsuzaki et al (2007) use a supertagger with
the Enju grammar for English. The novelty in their
work comes from the use of a context-free gram-
mar (CFG) to filter the tag sequences produced by
64
the supertagger before running the HPSG parser. In
this approach, a CFG approximation of the HPSG
is created. The key property of this approxima-
tion is that the language it recognizes is a superset
of the parsable supertag sequences. Hence, if the
CFG is unable to parse a sequence, it can be safely
discarded, thus further reducing the amount of se-
quences the HPSG parser has to deal with.
The provided evaluation is mostly concerned with
showing the improvement in parsing speed. Nev-
ertheless, the quality of the supertagging process
can be inferred from the accuracy of the parse re-
sults, which achieved a labeled precision and recall
for predicate-argument relations of 90% and 86%,
respectively, over 2,300 sentences with up to 100
words in section 23 of the Penn Treebank.
Dridan (2009) tests two supertaggers, one induced
using the TnT tagger (Brants, 2000) and another us-
ing the C&C supertagger (Clark and Curran, 2007),
over different datasets. For simplicity, we will only
refer to the results of TnT over a dataset of 814 sen-
tences of tourism data.
The author experiments with various tag granu-
larities in order to find a balance between tag ex-
pressiveness and tag predictability. For instance, as-
signing only POS?a tagset with only 13 tags?is
the easiest task, with 97% accuracy, while a highly
granular supertag formed by the lexical type con-
catenated with any selectional restriction present in
the lexical entry increases the number of possible
tags to 803, with accuracy dropping to 91%.
2.2 Support-Vector Machines and Tree Kernels
Support-vector machines (SVM) are a well known
supervised machine-learning algorithm for linear
binary classification. They are part of the fam-
ily of kernel-based methods where a general pur-
pose learning algorithm is coupled with a problem-
specific kernel function (Cristianini and Shawe-
Taylor, 2000).
For the work presented in this paper we wish
to apply the learning algorithm over discrete tree-
like structures that encode grammatical dependen-
cies (see Figure 1 for an example). A suitable ker-
nel for such a task is the tree kernel introduced by
Collins and Duffy (2002), which uses a represen-
tation that implicitly tracks all subtrees seen in the
training data.
This representation starts by implicitly enumerat-
ing all subtrees that are found in the training data. A
given tree, T , is then represented by a (huge) vector
where the n-th position counts the number of occur-
rences of the n-th subtree in T .
Under this representation, the inner product of
two trees gives a measure of their similarity. How-
ever, explicitly calculating such an operation is pro-
hibitively expensive due to the high dimensions of
the feature space. Fortunately, the inner product can
be replaced by a rather simple kernel function that
sums over the subtrees that are common to both trees
(see (Collins and Duffy, 2002) for a proof).
3 Grammar and Base Dataset
The deep linguistic grammar used in this study
is LXGram, a hand-built HPSG grammar for Por-
tuguese (Branco and Costa, 2008; Branco and Costa,
2010).
We used this grammar to support the annota-
tion of a corpus. That is, the grammar is used
to provide the set of possible analyses for a sen-
tence (the parse forest). Human annotators then
perform manual disambiguation by picking the cor-
rect analysis from among all those that form the
parse forest.2 This grammar-supported approach to
corpus annotation ensures that the various linguis-
tic annotation layers?morphological, syntactic and
semantic?are consistent.
The corpus that was used is composed mostly by a
subset of the sentences in CETEMPu?blico, a corpus
of plain text excerpts from the Pu?blico newspaper.
After running LXGram and manually disam-
biguating the parse forests, we were left with a
dataset consisting of 5,422 sentences annotated with
all the linguistic information provided by LXGram.
4 Classifier and Feature Extraction
For training and classification we use SVM-light-TK
(Moschitti, 2006), an extension to the widely-used
SVM-light (Joachims, 1999) software for SVMs that
adds a function implementing the tree kernel intro-
duced in Section 2.2. With SVM-light-TK one can
2In our setup, two annotators work in a double-blind
scheme, where those cases where they disagree are adjudicated
by a third annotator. Inter-annotator agreement is 0.86.
65
directly provide one or more tree structures as fea-
tures (using the standard parenthesis representation
of trees) together with the numeric feature vectors
that are already accepted by SVM-light.
Given that the task at stake is a multi-class clas-
sification problem but an SVM is a binary classi-
fier, the problem must first be binarized (Galar et
al., 2011). For this work we have chosen a one-
vs-one binarization scheme, where multiple classi-
fiers are created, each responsible for discriminat-
ing between a pair of classes. This divides a prob-
lem with n classes into n(n ? 1)/2 separate binary
problems (i.e. one classifier for each possible class
pairing). Each classifier then performs a binary de-
cision, voting for one of the two classes it is tasked
with discriminating, and the class with the overall
largest number of votes is chosen.
The dataset, having been produced with the help
of a deep grammar, contains a great deal of linguistic
information. The first step is thus to extract from
each sentence the relevant features in a format that
can be used by SVM-light-TK.
Since we are aiming at discriminating between
deep lexical types, which, among other information,
encode the SCF of a word, the dependency structure
associated with a word is expected to be a piece of
highly relevant information. We start by extracting
the dependency representation of a sentence from
the output of LXGram.3 The dependency represen-
tation that is obtained through this process consists
of a list of tuples, each relating a pair of words in the
sentence through a grammatical relation.
The example in Figure 1 shows the dependency
representation of the sentence ?a o segundo dia de
viagem encontra?mos os primeiros golfinhos? (Eng.:
by the second day of travel we found the first dol-
phins).4 Note that each word is also annotated with
its lexical type, POS tag and lemma, though this is
not shown in the example for the sake of readability.
For a one-vs-one classifier tasked with discrim-
inating between types A and B we are concerned
with finding instances of type A to be taken as posi-
tive examples and instances of type B to be taken as
3The details of this process are outside the scope of the cur-
rent paper and will be reported elsewhere.
4Relations in the example: ADV (adverb), C (complement),
DO (direct object), PRED (predicate), SP (specifier) and TMP
(temporal modifier).
negative examples.
Take, for instance, the word ?encontra?mos? from
the example in Figure 1. Its lexical type in this par-
ticular occurrence is verb-dir trans-lex, the type as-
signed to transitive verbs by LXGram. A one-vs-one
classifier tasked with recognizing this type (against
some other type) will take this instance as a positive
example.
However, the full dependency representation of
the sentence has too many irrelevant features for
learning how to classify this word. Instead, we fo-
cus more closely on the information that is relevant
to determining the SCF of the word by looking only
at its immediate neighbors in the dependency graph:
its dependents and the word it depends on.
This information is encoded in two trees, shown
in Figure 2, which are the actual features given to
SVM-light-TK.
One tree, labeled with H as root, is used to repre-
sent the word and its dependents. The target word is
marked by being under an asterisk ?category? while
the dependents fall under a ?category? correspond-
ing to the relation between the target word and the
dependent. The words appears as the leafs of the
tree, with their POS tags as the pre-terminal nodes.5
The second feature tree, labeled with D as root,
encodes the target word?again marked with an
asterisk?and the word it is dependent on. In the
example shown in Figure 2, since the target word is
the main verb of the sentence, the feature tree has no
other nodes apart from that of the target word.
5 Evaluation
The following evaluation results were obtained fol-
lowing a standard 10-fold cross-validation approach,
where the folds were taken from a random shuffle of
the sentences in the corpus.
We compare the performance of our tree kernel
(TK) approach with two other automatic annotators,
TnT (Brants, 2000) and SVMTool (Gime?nez and
Ma`rquez, 2004).
TnT is a statistical POS tagger, well known for
its efficiency?in terms of training and tagging
speed?and for achieving state-of-the-art re-
sults despite having a quite simple underlying
5POS tags in the example: V (verb), PREP (preposition) and
CN (common noun).
66
C(de, viagem) SP(dia, o) C(a, dia)
ADV(dia, de) PRD(golfinhos, primeiros) TMP(encontra?mos, a)
PRD(dia, segundo) SP(golfinhos, os) DO(encontra?mos, golfinhos)
Figure 1: Dependency representation
H
TMP
PREP
a
by
DO
CN
golfinhos
dolphins
*
V
encontra?mos
we-found
D
*
V
encontra?mos
we-found
Figure 2: Features for SVM-light-TK
model. It is based on a second-order hidden
Markov model extended with linear smooth-
ing of parameters to address data-sparseness is-
sues and suffix analysis for handling unknown
words. TnT was used as a supertagger in (Dri-
dan, 2009), where it achieved the best results
for this task, and is thus a good representative
for this approach to supertagging. We run it
out-of-the-box using the default settings.
SVMTool is another statistical sequential tagger
which, as the name indicates, is based on
support-vector machines. It is extremely flexi-
ble in allowing to define which features should
be used in the model (e.g. size of word win-
dow, number of POS bigrams, etc.) and the tag-
ging strategy (left to right, bidirectional, num-
ber of passes, etc). In fact, due to this flexibil-
ity, it is described as being a tagger generator.
It beat TnT in a POS tagging task (Gime?nez
and Ma`rquez, 2004), so we use it in the current
paper to evaluate whether that lead is kept in
a supertagging task. We used the simplest set-
tings, ?M0 LR?, which uses Model 0 in a left
to right tagging direction.6
6See (Gime?nez and Ma`rquez, 2006) for an explanation of
these settings.
The type distribution in the dataset is highly
skewed. For instance, from the number of com-
mon noun types that occur in this corpus, the two
most frequent ones are enough to account for 57%
of all the common noun tokens. Such skewed cat-
egory distributions are usually a problematic issue
for machine-learning approaches since the number
of instances of the more rare categories is too small
to properly estimate the parameters of the model.
For many types there are not enough instances in
the dataset to train a classifier. Hence, the evalua-
tion that follows is done only for the most frequent
types. For instance, top-10 means picking the 10
most frequent types in the corpus, training one-vs-
one classifiers for those types, and evaluating only
over tokens with one of those types. In addition, we
show only the evaluation results of verb types, for
which SCF information is more varied and relevant.
Table 1 show the accuracy results for each tool
over the top-10, top-20 and top-30 most frequent
verb types.
Comparing both sequential supertaggers, one
finds that SVMTool is consistently better than TnT,
which is in accordance with the results for POS tag-
ging reported in (Gime?nez and Ma`rquez, 2004).
Our TK approach beats both supertaggers when
67
TnT SVMTool TK
top-10 92.98% 94.22% 94.71%
top-20 91.53% 92.39% 90.21%
top-30 91.42% 92.38% 88.70%
Table 1: Accuracy over frequent verb types
looking at the top-10 verb types, but falls behind as
soon as the number of types under consideration in-
creases. This seems to point towards data-sparseness
issues, an hypothesis we test by automatically ex-
tending the dataset, as discussed next.
5.1 Experiments with an Extended Dataset
The extended datasets were created by taking ad-
ditional sentences from the Pu?blico newspaper, as
well as sentences from the Portuguese Wikipedia
and from the Folha de Sa?o Paulo newspaper, pre-
processing them with a POS tagger, and running
them through LXGram.
Such an approach is only made possible because
LXGram, like many other modern HPSG gram-
mars, includes a stochastic disambiguation module
that automatically chooses the most likely analysis
among all those returned in the parse forest, instead
of requiring a manual choice by a human annota-
tor (Branco and Costa, 2010). The authors do not
provide a complete evaluation of this disambigua-
tion module. Instead, they perform a manual evalu-
ation of a sample of 50 sentences that indicates that
this module picks the correct reading in 40% of the
cases.
If this ratio is kept, 60% of the sentences in the ex-
tended datasets will have an analysis that is, in some
way, the wrong analysis, though it is not clear how
this translates into errors in the lexical types that end
up being assigned to the tokens. For instance, when
faced with the rather common case of PP-attachment
ambiguity, the disambiguation module may choose
the wrong attachment, which will count as being a
wrong analysis though most lexical types assigned
to the words in the sentence may be correct.
To evaluate this, we tested the disambiguation
module over the base dataset, where we know what
the correct parses are, and found that the grammar
picks the correct parse in 44% of the cases. If we
just look at whether the lexical types are correct, the
dataset sentences tokens unique oov
base 5,422 51,483 8,815 10.0%
+ Pu?blico 10,727 139,330 18,899 7.6%
+ Wiki 15,108 205,585 24,063 6.6%
+ Folha 21,217 288,875 30,204 6.0%
Table 2: Cumulative size of datasets
grammar picks a sentence with fully correct types in
68% of the cases.
LXGram displayed a coverage of roughly 30%,
and allowed us to build progressively larger datasets
as more data was added. The cumulative sizes of the
resulting datasets are shown in Table 2. The Table
also shows the ratio of OOV words, which was de-
termined by taking the average of the ratio for each
of the 10 folds (i.e. words that occur in a fold but not
in any of the other 9 folds).
We can now evaluate the tools over the four pro-
gressively larger datasets and plot their learning
curves. In the following Figures, the errors bars rep-
resent a 95% confidence interval.
All learning curves in the following Figures tell a
somewhat similar story.
The lead that SVMTool has over TnT when look-
ing only at the base corpus is kept in the extended
corpora. Both sequential supertaggers only start to
benefit from the increased dataset at the final stage,
when sentences from Folha de Sa?o Paulo are added.
Before that stage the added data seems to be slightly
detrimental to them, possibly due to them being sen-
sitive to noise in the automatically generated data.
The learning curves give credence to the hypoth-
esis put forward earlier that our TK approach was
being adversely affected by data-sparseness issues
when classifying a greater number of verb types, and
that it has much to gain by an increase in the amount
of training data.
For the top-10 verb types, for which there is
enough data in the base dataset, TK starts ahead
from the outset and significantly increases its mar-
gin over the two supertaggers.
For the top-20 and top-30 verb types, TK starts
behind but its accuracy raises quickly as more data
are added, ending slightly ahead of SVMTool when
running over the largest dataset.
68
5000 10000 15000 20000
0.90
0.92
0.94
0.96
Over top?10 verb types
Dataset size
Accu
racy
l
l l
l
l TnTSVMToolSVM?TK
Figure 3: Learning curves (over top-10 verb types)
5000 10000 15000 20000
0.90
0.92
0.94
0.96
Over top?20 verb types
Dataset size
Accu
racy
l l
l
l
l TnTSVMToolSVM?TK
Figure 4: Learning curves (over top-20 verb types)
5000 10000 15000 20000
0.90
0.92
0.94
0.96
Over top?30 verb types
Dataset size
Accu
racy
l l
l
l
l TnTSVMToolSVM?TK
Figure 5: Learning curves (over top-30 verb types)
dataset accuracy
base 87.24%
+ Pu?blico 82.67%
+ Wiki 82.30%
+ Folha 83.92%
Table 3: MaltParser labeled accuracy
5.2 Running over Predicted Dependencies
In the previous section, we were concerned with
evaluating the classifier itself. Accordingly, the fea-
tures used by the classifier were the gold dependen-
cies in the corpus. However, on a running system,
the features used by the classifier will be automati-
cally generated by a dependency parser. To evaluate
this setup, we used MaltParser (Nivre et al, 2007).
Like the other tools, the parser was run out-of-the-
box. The 10-fold average labeled accuracy scores
for each dataset shown in Table 3 can thus be seen
as a lower bound on the achievable accuracy. De-
spite this, the performance over the base dataset is
extremely good, on par with the best scores achieved
for other languages (cf. (Nivre et al, 2007)). How-
ever, performance drops sharply when automatically
annotated data is used, only beginning to pick up
again when running over the largest dataset.
As expected, the noisy features that result from
the automatic process have a detrimental effect on
the accuracy of the classifier. For the same set of
experiments reported previously, the accuracy of the
SVM-TK classifier when running over predicted de-
pendencies tends to trail 2.0?2.5% points behind
that of the classifier that uses gold dependencies, as
shown in Table 4.
6 Concluding Remarks
In this paper we reported on an novel approach to as-
signing deep lexical types. It uses an SVM classifier
with a tree kernel that allows it to seamlessly work
with features encoding discrete structures represent-
ing the grammatical dependencies between words.
Evaluation over the top-10 most frequent verb
types showed that the grammatical dependencies of
a word, which can be seen as information on its SCF,
are very helpful in allowing the classifier to accu-
rately assign lexical types. Our classifier clearly im-
69
top-10 top-20 top-30
dataset gold pred. gold pred. gold pred.
base 94.71% 93.14% 90.21% 88.66% 88.70% 87.01%
+ Pu?blico 96.02% 93.83% 92.34% 90.35% 91.32% 88.97%
+ Wiki 96.48% 93.95% 93.54% 91.29% 92.80% 90.21%
+ Folha 96.98% 94.55% 94.46% 92.26% 93.93% 91.50%
Table 4: SVM-TK classifier accuracy over gold and predicted features
proves over TnT, which had displayed the best su-
pertagging performance in other studies.
When running the classifier for a greater number
of verb types, data-sparseness issues led to a drop
in performance, which motivated additional experi-
ments where the dataset was extended with automat-
ically annotated data. This allowed us to plot learn-
ing curves that show that our approach can maintain
a lead in accuracy when given more training data.
Running the classifier over predicted features
shows an expected drop in performance. However,
we anticipate that using larger corpora will also
be effective in raising these scores since additional
training data not only improve the classifier, but also
the underlying parser that provides the dependencies
that are used as features.
References
Timothy Baldwin. 2005. Bootstrapping deep lexical re-
sources: Resources for courses. In Timothy Baldwin,
Anna Korhonen, and Aline Villavicencio, editors, Pro-
ceedings of the ACL-SIGLEX Workshop on Deep Lex-
ical Acquisition, pages 67?76.
Srinivas Bangalore and Aravind Joshi. 1994. Disam-
biguation of super parts of speech (or supertags): Al-
most parsing. In Proceedings of the 15th Conference
on Computational Linguistics (COLING), pages 154?
160.
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
Anto?nio Branco and Francisco Costa. 2008. A computa-
tional grammar for deep linguistic processing of Por-
tuguese: LX-Gram, version A.4.1. Technical Report
DI-FCUL-TR-08-17, University of Lisbon.
Anto?nio Branco and Francisco Costa. 2010. A deep lin-
guistic processing grammar for Portuguese. In Pro-
ceedings of the 9th Encontro para o Processamento
Computacional da L??ngua Portuguesa Escrita e Fal-
ada (PROPOR), LNAI, pages 86?89. Springer.
Anto?nio Branco, Francisco Costa, and Filipe Nunes.
2008. The processing of verbal inflection ambiguity:
Characterization of the problem space. In Proceedings
of the 21st Encontro Anual da Associac?a?o Portuguesa
de Lingu??stica (APL), pages 2577?2583.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In Proceedings of the 6th Applied Natu-
ral Language Processing Conference and the 1st North
American Chapter of the Association for Computa-
tional Linguistics, pages 224?231.
Michael Brent. 1991. Automatic acquisition of subcat-
egorization frames from untagged text. In Proceed-
ings of the 29th Annual Meeting of the Association for
Computational Linguistics, pages 209?214.
Ted Briscoe and John Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In Proceed-
ings of the 5th Applied Natural Language Processing
Conference, pages 356?363.
Stephen Clark and James Curran. 2007. Wide-coverage
efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33:493?552.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 263?270.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-Based Learning Methods. Cambridge Univer-
sity Press.
Rebecca Dridan. 2009. Using Lexical Statistics to Im-
prove HPSG Parsing. Ph.D. thesis, University of Saar-
land.
Mikel Galar, Alberto Fernande?z, Edurne Barrenechea,
Humberto Bustince, and Francisco Herrera. 2011. An
overview of ensemble methods for binary classifiers
in multi-class problems: Experimental study in one-
vs-one and one-vs-all schemes. Pattern Recognition,
44:1761?1776.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on support vector
70
machines. In Proceedings of the 4th Language Re-
sources and Evaluation Conference (LREC).
Jesu?s Gime?nez and Llu??s Ma`rquez, 2006. SVMTool:
Technical Manual v1.3. TALP Research Center, LSI
Department, Universitat Politecnica de Catalunya.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and A.
Smola, editors, Advances in Kernel Methods ? Sup-
port Vector Learning, chapter 11, pages 169?184. MIT
Press, Cambridge, MA.
Maria Helena Mira Mateus, Ana Maria Brito, Ine?s
Duarte, Isabel Hub Faria, So?nia Frota, Gabriela Matos,
Fa?tima Oliveira, Marina Viga?rio, and Alina Villalva.
2003. Grama?tica da L??ngua Portuguesa. Caminho,
5th edition.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient HPSG parsing with supertagging and
CFG-filtering. In Proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI), pages 1671?1676.
Ruslan Mitkov, editor. 2004. The Oxford Handbook of
Computational Linguistics. Oxford University Press.
Alessando Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proceedings
of the 11th European Chapter of the Association for
Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Robbert Prins and Gertjan van Noord. 2003. Reinforc-
ing parser preferences through tagging. Traitment Au-
tomatique des Langues, 44:121?139.
71
