Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254?263,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Cheap and Fast ? But is it Good?
Evaluating Non-Expert Annotations for Natural Language Tasks
Rion Snow? Brendan O?Connor? Daniel Jurafsky? Andrew Y. Ng?
?Computer Science Dept.
Stanford University
Stanford, CA 94305
{rion,ang}@cs.stanford.edu
?Dolores Labs, Inc.
832 Capp St.
San Francisco, CA 94110
brendano@doloreslabs.com
?Linguistics Dept.
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
Human linguistic annotation is crucial for
many natural language processing tasks but
can be expensive and time-consuming. We ex-
plore the use of Amazon?s Mechanical Turk
system, a significantly cheaper and faster
method for collecting annotations from a
broad base of paid non-expert contributors
over the Web. We investigate five tasks: af-
fect recognition, word similarity, recognizing
textual entailment, event temporal ordering,
and word sense disambiguation. For all five,
we show high agreement between Mechani-
cal Turk non-expert annotations and existing
gold standard labels provided by expert label-
ers. For the task of affect recognition, we also
show that using non-expert labels for training
machine learning algorithms can be as effec-
tive as using gold standard annotations from
experts. We propose a technique for bias
correction that significantly improves annota-
tion quality on two tasks. We conclude that
many large labeling tasks can be effectively
designed and carried out in this method at a
fraction of the usual expense.
1 Introduction
Large scale annotation projects such as TreeBank
(Marcus et al, 1993), PropBank (Palmer et
al., 2005), TimeBank (Pustejovsky et al, 2003),
FrameNet (Baker et al, 1998), SemCor (Miller et
al., 1993), and others play an important role in
natural language processing research, encouraging
the development of novel ideas, tasks, and algo-
rithms. The construction of these datasets, how-
ever, is extremely expensive in both annotator-hours
and financial cost. Since the performance of many
natural language processing tasks is limited by the
amount and quality of data available to them (Banko
and Brill, 2001), one promising alternative for some
tasks is the collection of non-expert annotations.
In this work we explore the use of Amazon Me-
chanical Turk1 (AMT) to determine whether non-
expert labelers can provide reliable natural language
annotations. We chose five natural language under-
standing tasks that we felt would be sufficiently nat-
ural and learnable for non-experts, and for which
we had gold standard labels from expert labelers,
as well as (in some cases) expert labeler agree-
ment information. The tasks are: affect recogni-
tion, word similarity, recognizing textual entailment,
event temporal ordering, and word sense disam-
biguation. For each task, we used AMT to annotate
data and measured the quality of the annotations by
comparing them with the gold standard (expert) la-
bels on the same data. Further, we compare machine
learning classifiers trained on expert annotations vs.
non-expert annotations.
In the next sections of the paper we introduce
the five tasks and the evaluation metrics, and offer
methodological insights, including a technique for
bias correction that improves annotation quality.2
1 http://mturk.com
2 Please see http://blog.doloreslabs.com/?p=109
for a condensed version of this paper, follow-ups, and on-
going public discussion. We encourage comments to be di-
rected here in addition to email when appropriate. Dolores
Labs Blog, ?AMT is fast, cheap, and good for machine learning
data,? Brendan O?Connor, Sept. 9, 2008. More related work at
http://blog.doloreslabs.com/topics/wisdom/.
254
2 Related Work
The idea of collecting annotations from volunteer
contributors has been used for a variety of tasks.
Luis von Ahn pioneered the collection of data via
online annotation tasks in the form of games, includ-
ing the ESPGame for labeling images (von Ahn and
Dabbish, 2004) and Verbosity for annotating word
relations (von Ahn et al, 2006). The Open Mind
Initiative (Stork, 1999) has taken a similar approach,
attempting to make such tasks as annotating word
sense (Chklovski and Mihalcea, 2002) and common-
sense word relations (Singh, 2002) sufficiently ?easy
and fun? to entice users into freely labeling data.
There have been an increasing number of experi-
ments using Mechanical Turk for annotation. In (Su
et al, 2007) workers provided annotations for the
tasks of hotel name entity resolution and attribute
extraction of age, product brand, and product model,
and were found to have high accuracy compared
to gold-standard labels. Kittur et al (2008) com-
pared AMT evaluations of Wikipedia article qual-
ity against experts, finding validation tests were im-
portant to ensure good results. Zaenen (Submitted)
studied the agreement of annotators on the problem
of recognizing textual entailment (a similar task and
dataset is explained in more detail in Section 4).
At least several studies have already used AMT
without external gold standard comparisons. In
(Nakov, 2008) workers generated paraphrases of
250 noun-noun compounds which were then used
as the gold standard dataset for evaluating an au-
tomatic method of noun compound paraphrasing.
Kaisser and Lowe (2008) use AMT to help build a
dataset for question answering, annotating the an-
swers to 8107 questions with the sentence contain-
ing the answer. Kaisser et al (2008) examines the
task of customizing the summary length of QA out-
put; non-experts from AMT chose a summary length
that suited their information needs for varying query
types. Dakka and Ipeirotis (2008) evaluate a docu-
ment facet generation system against AMT-supplied
facets, and also use workers for user studies of the
system. Sorokin and Forsyth (2008) collect data for
machine vision tasks and report speed and costs sim-
ilar to our findings; their summaries of worker be-
havior also corroborate with what we have found.
In general, volunteer-supplied or AMT-supplied
data is more plentiful but noisier than expert data.
It is powerful because independent annotations can
be aggregated to achieve high reliability. Sheng et
al. (2008) explore several methods for using many
noisy labels to create labeled data, how to choose
which examples should get more labels, and how to
include labels? uncertainty information when train-
ing classifiers. Since we focus on empirically val-
idating AMT as a data source, we tend to stick to
simple aggregation methods.
3 Task Design
In this section we describe Amazon Mechanical
Turk and the general design of our experiments.
3.1 Amazon Mechanical Turk
We employ the Amazon Mechanical Turk system
in order to elicit annotations from non-expert label-
ers. AMT is an online labor market where workers
are paid small amounts of money to complete small
tasks. The design of the system is as follows: one is
required to have an Amazon account to either sub-
mit tasks for annotations or to annotate submitted
tasks. These Amazon accounts are anonymous, but
are referenced by a unique Amazon ID. A Requester
can create a group of Human Intelligence Tasks (or
HITs), each of which is a form composed of an arbi-
trary number of questions. The user requesting an-
notations for the group of HITs can specify the num-
ber of unique annotations per HIT they are willing
to pay for, as well as the reward payment for each
individual HIT. While this does not guarantee that
unique people will annotate the task (since a single
person could conceivably annotate tasks using mul-
tiple accounts, in violation of the user agreement),
this does guarantee that annotations will be collected
from unique accounts. AMT also allows a requester
to restrict which workers are allowed to annotate a
task by requiring that all workers have a particular
set of qualifications, such as sufficient accuracy on
a small test set or a minimum percentage of previ-
ously accepted submissions. Annotators (variously
referred to as Workers or Turkers) may then annotate
the tasks of their choosing. Finally, after each HIT
has been annotated, the Requester has the option of
approving the work and optionally giving a bonus
to individual workers. There is a two-way commu-
255
nication channel between the task designer and the
workers mediated by Amazon, and Amazon handles
all financial transactions.
3.2 Task Design
In general we follow a few simple design principles:
we attempt to keep our task descriptions as succinct
as possible, and we attempt to give demonstrative
examples for each class wherever possible. We have
published the full experimental design and the data
we have collected for each task online3. We have
restricted our study to tasks where we require only
a multiple-choice response or numeric input within
a fixed range. For every task we collect ten inde-
pendent annotations for each unique item; this re-
dundancy allows us to perform an in-depth study of
how data quality improves with the number of inde-
pendent annotations.
4 Annotation Tasks
We analyze the quality of non-expert annotations on
five tasks: affect recognition, word similarity, rec-
ognizing textual entailment, temporal event recogni-
tion, and word sense disambiguation. In this section
we define each annotation task and the parameters
of the annotations we request using AMT. Addition-
ally we give an initial analysis of the task results,
and summarize the cost of the experiments.
4.1 Affective Text Analysis
This experiment is based on the affective text an-
notation task proposed in Strapparava and Mihalcea
(2007), wherein each annotator is presented with a
list of short headlines, and is asked to give numeric
judgments in the interval [0,100] rating the headline
for six emotions: anger, disgust, fear, joy, sadness,
and surprise, and a single numeric rating in the inter-
val [-100,100] to denote the overall positive or nega-
tive valence of the emotional content of the headline,
as in this sample headline-annotation pair:
Outcry at N Korea ?nuclear test?
(Anger, 30), (Disgust,30), (Fear,30), (Joy,0),
(Sadness,20), (Surprise,40), (Valence,-50).
3All tasks and collected data are available at
http://ai.stanford.edu/
?
rion/annotations/.
For our experiment we select a 100-headline sample
from the original SemEval test set, and collect 10
affect annotations for each of the seven label types,
for a total of 7000 affect labels.
We then performed two comparisons to evaluate
the quality of the AMT annotations. First, we asked
how well the non-experts agreed with the experts.
We did this by comparing the interannotator agree-
ment (ITA) of individual expert annotations to that
of single non-expert and averaged non-expert anno-
tations. In the original experiment ITA is measured
by calculating the Pearson correlation of one anno-
tator?s labels with the average of the labels of the
other five annotators. For each expert labeler, we
computed this ITA score of the expert against the
other five; we then average these ITA scores across
all expert annotators to compute the average expert
ITA (reported in Table 1 as ?E vs. E?. We then do the
same for individual non-expert annotations, averag-
ing Pearson correlation across all sets of the five ex-
pert labelers (?NE vs. E?). We then calculate the ITA
for each expert vs. the averaged labels from all other
experts and non-experts (marked as ?E vs. All?) and
for each non-expert vs. the pool of other non-experts
and all experts (?NE vs. All?). We compute these
ITA scores for each emotion task separately, aver-
aging the six emotion tasks as ?Avg. Emo? and the
average of all tasks as ?Avg. All?.
Emotion E vs. E E vs. All NE vs. E NE vs. All
Anger 0.459 0.503 0.444 0.573
Disgust 0.583 0.594 0.537 0.647
Fear 0.711 0.683 0.418 0.498
Joy 0.596 0.585 0.340 0.421
Sadness 0.645 0.650 0.563 0.651
Surprise 0.464 0.463 0.201 0.225
Valence 0.759 0.767 0.530 0.554
Avg. Emo 0.576 0.603 0.417 0.503
Avg. All 0.580 0.607 0.433 0.510
Table 1: Average expert and non-expert ITA on test-set
The results in Table 1 conform to the expectation
that experts are better labelers: experts agree with
experts more than non-experts agree with experts,
although the ITAs are in many cases quite close. But
we also found that adding non-experts to the gold
standard (?E vs. All?) improves agreement, suggest-
ing that non-expert annotations are good enough to
increase the overall quality of the gold labels. Our
256
first comparison showed that individual experts were
better than individual non-experts. In our next com-
parison we ask how many averaged non-experts it
would take to rival the performance of a single ex-
pert. We did this by averaging the labels of each pos-
sible subset of n non-expert annotations, for value
of n in {1, 2, . . . , 10}. We then treat this average as
though it is the output of a single ?meta-labeler?, and
compute the ITA with respect to each subset of five
of the six expert annotators. We then average the
results of these studies across each subset size; the
results of this experiment are given in Table 2 and in
Figure 1. In addition to the single meta-labeler, we
ask: what is the minimum number of non-expert an-
notations k from which we can create a meta-labeler
that has equal or better ITA than an expert annotator?
In Table 2 we give the minimum k for each emotion,
and the averaged ITA for that meta-labeler consist-
ing of k non-experts (marked ?k-NE?). In Figure 1
we plot the expert ITA correlation as the horizontal
dashed line.
Emotion 1-Expert 10-NE k k-NE
Anger 0.459 0.675 2 0.536
Disgust 0.583 0.746 2 0.627
Fear 0.711 0.689 ? ?
Joy 0.596 0.632 7 0.600
Sadness 0.645 0.776 2 0.656
Surprise 0.464 0.496 9 0.481
Valence 0.759 0.844 5 0.803
Avg. Emo. 0.576 0.669 4 0.589
Avg. All 0.603 0.694 4 0.613
Table 2: Average expert and averaged correlation over
10 non-experts on test-set. k is the minimum number of
non-experts needed to beat an average expert.
These results show that for all tasks except ?Fear?
we are able to achieve expert-level ITA with the
held-out set of experts within 9 labelers, and fre-
quently within only 2 labelers. Pooling judgments
across all 7 tasks we find that on average it re-
quires only 4 non-expert annotations per example to
achieve the equivalent ITA as a single expert anno-
tator. Given that we paid US$2.00 in order to collect
the 7000 non-expert annotations, we may interpret
our rate of 3500 non-expert labels per USD as at
least 875 expert-equivalent labels per USD.
4.2 Word Similarity
This task replicates the word similarity task used in
(Miller and Charles, 1991), following a previous
2 4 6 8 100
.4
5
0.
55
0.
65
co
rr
e
la
tio
n
anger
2 4 6 8 10
0.
55
0.
65
0.
75
co
rr
e
la
tio
n
disgust
2 4 6 8 100
.4
0
0.
50
0.
60
0.
70
co
rr
e
la
tio
n
fear
2 4 6 8 10
0.
35
0.
45
0.
55
0.
65
co
rr
e
la
tio
n
joy
2 4 6 8 100
.5
5
0.
65
0.
75
annotators
co
rr
e
la
tio
n
sadness
2 4 6 8 100
.2
0
0.
30
0.
40
0.
50
annotators
co
rr
e
la
tio
n
surprise
Figure 1: Non-expert correlation for affect recognition
task initially proposed by (Rubenstein and Good-
enough, 1965). Specifically, we ask for numeric
judgments of word similarity for 30 word pairs on
a scale of [0,10], allowing fractional responses4 .
These word pairs range from highly similar (e.g.,
{boy, lad}), to unrelated (e.g., {noon, string}). Nu-
merous expert and non-expert studies have shown
that this task typically yields very high interannota-
tor agreement as measured by Pearson correlation;
(Miller and Charles, 1991) found a 0.97 correla-
tion of the annotations of 38 subjects with the an-
notations given by 51 subjects in (Rubenstein and
Goodenough, 1965), and a following study (Resnik,
1999) with 10 subjects found a 0.958 correlation
with (Miller and Charles, 1991).
In our experiment we ask for 10 annotations each
of the full 30 word pairs, at an offered price of $0.02
for each set of 30 annotations (or, equivalently, at
the rate of 1500 annotations per USD). The most
surprising aspect of this study was the speed with
which it was completed; the task of 300 annotations
was completed by 10 annotators in less than 11 min-
4(Miller and Charles, 1991) and others originally used a
numerical score of [0,4].
257
utes from the time of submission of our task to AMT,
at the rate of 1724 annotations / hour.
As in the previous task we evaluate our non-
expert annotations by averaging the numeric re-
sponses from each possible subset of n annotators
and computing the interannotator agreement with
respect to the gold scores reported in (Miller and
Charles, 1991). Our results are displayed in Figure
2, with Resnik?s 0.958 correlation plotted as the hor-
izontal line; we find that at 10 annotators we achieve
a correlation of 0.952, well within the range of other
studies of expert and non-expert annotations.
2 4 6 8 10
0.
84
0.
90
0.
96
annotations
co
rr
e
la
tio
n
Word Similarity ITA
Figure 2: ITA for word similarity experiment
4.3 Recognizing Textual Entailment
This task replicates the recognizing textual entail-
ment task originally proposed in the PASCAL Rec-
ognizing Textual Entailment task (Dagan et al,
2006); here for each question the annotator is pre-
sented with two sentences and given a binary choice
of whether the second hypothesis sentence can be
inferred from the first. For example, the hypothesis
sentence ?Oil prices drop? would constitute a true
entailment from the text ?Crude Oil Prices Slump?,
but a false entailment from ?The government an-
nounced last week that it plans to raise oil prices?.
We gather 10 annotations each for all 800 sen-
tence pairs in the PASCAL RTE-1 dataset. For this
dataset expert interannotator agreement studies have
been reported as achieving 91% and 96% agreement
over various subsections of the corpus. When con-
sidering multiple non-expert annotations for a sen-
tence pair we use simple majority voting, breaking
ties randomly and averaging performance over all
possible ways to break ties. We collect 10 annota-
tions for each of 100 RTE sentence pairs; as dis-
played in Figure 3, we achieve a maximum accu-
racy of 89.7%, averaging over the annotations of 10
workers5.
2 4 6 8 100
.7
0
0.
80
0.
90
annotations
a
cc
u
ra
cy
RTE ITA
Figure 3: Inter-annotator agreement for RTE experiment
4.4 Event Annotation
This task is inspired by the TimeBank corpus (Puste-
jovsky et al, 2003), which includes among its anno-
tations a label for event-pairs that represents the tem-
poral relation between them, from a set of fourteen
relations (before, after, during, includes, etc.). We
implement temporal ordering as a simplified version
of the TimeBank event temporal annotation task:
rather than annotating all fourteen event types, we
restrict our consideration to the two simplest labels:
?strictly before? and ?strictly after?. Furthermore,
rather than marking both nouns and verbs in the text
as possible events, we only consider possible verb
events. We extract the 462 verb event pairs labeled
as ?strictly before? or ?strictly after? in the Time-
Bank corpus, and we present these pairs to annota-
tors with a forced binary choice on whether the event
described by the first verb occurs before or after the
second. For example, in a dialogue about a plane
explosion, we have the utterance: ?It just blew up in
the air, and then we saw two fireballs go down to the,
5It might seem pointless to consider an even number of an-
notations in this circumstance, since the majority voting mech-
anism and tie-breaking yields identical performance for 2n + 1
and 2n + 2 annotators; however, in Section 5 we will consider
methods that can make use of the even annotations.
258
to the water, and there was a big small, ah, smoke,
from ah, coming up from that?. Here for each anno-
tation we highlight the specific verb pair of interest
(e.g., go/coming, or blew/saw) and ask which event
occurs first (here, go and blew, respectively).
The results of this task are presented in Figure 4.
We achieve high agreement for this task, at a rate
of 0.94 with simple voting over 10 annotators (4620
total annotations). While an expert ITA of 0.77 was
reported for the more general task involving all four-
teen labels on both noun and verb events, no expert
ITA numbers have been reported for this simplified
temporal ordering task.
2 4 6 8 100
.7
0
0.
80
0.
90
annotators
a
cc
u
ra
cy
Temp. Ordering ITA
Figure 4: ITA for temporal ordering experiment
4.5 Word Sense Disambiguation
In this task we consider a simple problem on which
machine learning algorithms have been shown to
produce extremely good results; here we annotate
part of the SemEval Word Sense Disambiguation
Lexical Sample task (Pradhan et al, 2007); specif-
ically, we present the labeler with a paragraph of
text containing the word ?president? (e.g., a para-
graph containing ?Robert E. Lyons III...was ap-
pointed president and chief operating officer...?) and
ask the labeler which one of the following three
sense labels is most appropriate:
1) executive officer of a firm, corporation, or university
2) head of a country (other than the U.S.)
3) head of the U.S., President of the United States
We collect 10 annotations for each of 177 examples
of the noun ?president? for the three senses given in
SemEval. As shown in Figure 5, performing simple
majority voting (with random tie-breaking) over an-
notators results in a rapid accuracy plateau at a very
high rate of 0.994 accuracy. In fact, further analy-
sis reveals that there was only a single disagreement
between the averaged non-expert vote and the gold
standard; on inspection it was observed that the an-
notators voted strongly against the original gold la-
bel (9-to-1 against), and that it was in fact found to
be an error in the original gold standard annotation.6
After correcting this error, the non-expert accuracy
rate is 100% on the 177 examples in this task. This
is a specific example where non-expert annotations
can be used to correct expert annotations.
Since expert ITA was not reported per word on
this dataset, we compare instead to the performance
of the best automatic system performance for dis-
ambiguating ?president? in SemEval Task 17 (Cai et
al., 2007), with an accuracy of 0.98.
2 4 6 8 10
0.
98
0
0.
99
0
1.
00
0
annotators
a
cc
u
ra
cy
WSD ITA
Figure 5: Inter-annotator agreement for WSD experiment
4.6 Summary
Cost Time Labels Labels
Task Labels (USD) (hrs) per USD per hr
Affect 7000 $2.00 5.93 3500 1180.4
WSim 300 $0.20 0.174 1500 1724.1
RTE 8000 $8.00 89.3 1000 89.59
Event 4620 $13.86 39.9 333.3 115.85
WSD 1770 $1.76 8.59 1005.7 206.1
Total 21690 25.82 143.9 840.0 150.7
Table 3: Summary of costs for non-expert labels
6The example sentence began ?The Egyptian president said
he would visit Libya today...? and was mistakenly marked as
the ?head of a company? sense in the gold annotation (example
id 24:0@24@wsj/23/wsj 2381@wsj@en@on).
259
0 200 400 600 800
0.
4
0.
6
0.
8
1.
0
number of annotations
a
cc
u
ra
cy
Figure 6: Worker accuracies on the RTE task. Each point
is one worker. Vertical jitter has been added to points on
the left to show the large number of workers who did the
minimum amount of work (20 examples).
In Table 3 we give a summary of the costs asso-
ciated with obtaining the non-expert annotations for
each of our 5 tasks. Here Time is given as the to-
tal amount of time in hours elapsed from submitting
the group of HITs to AMT until the last assignment
is submitted by the last worker.
5 Bias correction for non-expert
annotators
The reliability of individual workers varies. Some
are very accurate, while others are more careless and
make mistakes; and a small few give very noisy re-
sponses. Furthermore, for most AMT data collec-
tion experiments, a relatively small number of work-
ers do a large portion of the task, since workers may
do as much or as little as they please. Figure 6 shows
accuracy rates for individual workers on one task.
Both the overall variability, as well as the prospect
of identifying high-volume but low-quality workers,
suggest that controlling for individual worker qual-
ity could yield higher quality overall judgments.
In general, there are at least three ways to enhance
quality in the face of worker error. More work-
ers can be used, as described in previous sections.
Another method is to use Amazon?s compensation
mechanisms to give monetary bonuses to highly-
performing workers and deny payments to unreli-
able ones; this is useful, but beyond the scope of
this paper. In this section we explore a third alterna-
tive, to model the reliability and biases of individual
workers and correct for them.
A wide number of methods have been explored to
correct for the bias of annotators. Dawid and Skene
(1979) are the first to consider the case of having
multiple annotators per example but unknown true
labels. They introduce an EM algorithm to simul-
taneously estimate annotator biases and latent label
classes. Wiebe et al (1999) analyze linguistic anno-
tator agreement statistics to find bias, and use a sim-
ilar model to correct labels. A large literature in bio-
statistics addresses this same problem for medical
diagnosis. Albert and Dodd (2004) review several
related models, but argue they have various short-
comings and emphasize instead the importance of
having a gold standard.
Here we take an approach based on gold standard
labels, using a small amount of expert-labeled train-
ing data in order to correct for the individual biases
of different non-expert annotators. The idea is to re-
calibrate worker?s responses to more closely match
expert behavior. We focus on categorical examples,
though a similar method can be used with numeric
data.
5.1 Bias correction in categorical data
Following Dawid and Skene, we model labels and
workers with a multinomial model similar to Naive
Bayes. Every example i has a true label xi. For sim-
plicity, assume two labels {Y,N}. Several differ-
ent workers give labels yi1, yi2, . . . yiW . A worker?s
conditional probability of response is modeled as
multinomial, and we model each worker?s judgment
as conditionally independent of other workers given
the true label xi, i.e.:
P (yi1, . . . , yiW , xi) =
(
?
w
P (yiw|xi)
)
p(xi)
To infer the posterior probability of the true label
for a new example, worker judgments are integrated
via Bayes rule, yielding the posterior log-odds:
log
P (xi = Y |yi1 . . . yiW )
P (xi = N |yi1 . . . yiW )
=
?
w
log
P (yiw|xi = Y )
P (yiw|xi = N)
+ log
P (xi = Y )
P (xi = N)
260
The worker response likelihoods P (yw|x = Y )
and P (yw|x = N) can be directly estimated from
frequencies of worker performance on gold standard
examples. (If we used maximum likelihood esti-
mation with no Laplace smoothing, then each yw|x
is just the worker?s empirical confusion matrix.)
For MAP label estimation, the above equation de-
scribes a weighted voting rule: each worker?s vote is
weighted by their log likelihood ratio for their given
response. Intuitively, workers who are more than
50% accurate have positive votes; workers whose
judgments are pure noise have zero votes; and an-
ticorrelated workers have negative votes. (A simpler
form of the model only considers accuracy rates,
thus weighting worker votes by log accw1?accw . But we
use the full unconstrained multinomial model here.)
5.1.1 Example tasks: RTE-1 and event
annotation
We used this model to improve accuracy on the
RTE-1 and event annotation tasks. (The other cate-
gorical task, word sense disambiguation, could not
be improved because it already had maximum accu-
racy.) First we took a sample of annotations giving
k responses per example. Within this sample, we
trained and tested via 20-fold cross-validation across
examples. Worker models were fit using Laplace
smoothing of 1 pseudocount; label priors were uni-
form, which was reasonably similar to the empirical
distribution for both tasks.
annotators
a
cc
u
ra
cy
0.
7
0.
8
0.
9
RTE
annotators
0.
7
0.
8
0.
9
before/after
Gold calibrated
Naive voting
Figure 7: Gold-calibrated labels versus raw labels
Figure 7 shows improved accuracy at different
numbers of annotators. The lowest line is for the
naive 50% majority voting rule. (This is equivalent
to the model under uniform priors and equal accu-
racies across workers and labels.) Each point is the
data set?s accuracy against the gold labels, averaged
across resamplings each of which obtains k annota-
tions per example. RTE has an average +4.0% ac-
curacy increase, averaged across 2 through 10 anno-
tators. We find a +3.4% gain on event annotation.
Finally, we experimented with a similar calibration
method for numeric data, using a Gaussian noise
model for each worker: yw|x ? N(x + ?w, ?w).
On the affect task, this yielded a small but consis-
tent increases in Pearson correlation at all numbers
of annotators, averaging a +0.6% gain.
6 Training a system with non-expert
annotations
In this section we train a supervised affect recogni-
tion system with expert vs. non-expert annotations.
6.1 Experimental Design
For the purpose of this experiment we create a sim-
ple bag-of-words unigram model for predicting af-
fect and valence, similar to the SWAT system (Katz
et al, 2007), one of the top-performing systems on
the SemEval Affective Text task.7 For each token
t in our training set, we assign t a weight for each
emotion e equal to the average emotion score ob-
served in each headline H that t participates in. i.e.,
if Ht is the set of headlines containing the token t,
then:
Score(e, t) =
?
H?Ht Score(e,H)
|Ht|
With these weights of the individual tokens we
may then compute the score for an emotion e of a
new headline H as the average score over the set of
tokens t ? H that we?ve observed in the training set
(ignoring those tokens not in the training set), i.e.:
Score(e,H) =
?
t?H
Score(e, t)
|H|
Where |H| is simply the number of tokens in
headline H , ignoring tokens not observed in the
training set.
7 Unlike the SWAT system we perform no lemmatization,
synonym expansion, or any other preprocessing of the tokens;
we simply use whitespace-separated tokens within each head-
line.
261
6.2 Experiments
We use 100 headlines as a training set (examples
500-599 from the test set of SemEval Task 14), and
we use the remaining 900 headlines as our test set.
Since we are fortunate to have the six separate ex-
pert annotations in this task, we can perform an ex-
tended systematic comparison of the performance of
the classifier trained with expert vs. non-expert data.
Emotion 1-Expert 10-NE k k-NE
Anger 0.084 0.233 1 0.172
Disgust 0.130 0.231 1 0.185
Fear 0.159 0.247 1 0.176
Joy 0.130 0.125 ? ?
Sadness 0.127 0.174 1 0.141
Surprise 0.060 0.101 1 0.061
Valence 0.159 0.229 2 0.146
Avg. Emo 0.116 0.185 1 0.135
Avg. All 0.122 0.191 1 0.137
Table 4: Performance of expert-trained and non-expert-
trained classifiers on test-set. k is the minimum number
of non-experts needed to beat an average expert.
For this evaluation we compare the performance
of systems trained on expert and non-expert annota-
tions. For each expert annotator we train a system
using only the judgments provided by that annota-
tor, and then create a gold standard test set using the
average of the responses of the remaining five label-
ers on that set. In this way we create six indepen-
dent expert-trained systems and compute the aver-
age across their performance, calculated as Pearson
correlation to the gold standard; this is reported in
the ?1-Expert? column of Table 4.
Next we train systems using non-expert labels;
for each possible subset of n annotators, for n ?
{1, 2, . . . , 10} we train a system, and evaluate by
calculating Pearson correlation with the same set of
gold standard datasets used in the expert-trained sys-
tem evaluation. Averaging the results of these stud-
ies yields the results in Table 4.
As in Table 2 we calculate the minimum number
of non-expert annotations per example k required on
average to achieve similar performance to the ex-
pert annotations; surprisingly we find that for five
of the seven tasks, the average system trained with a
single set of non-expert annotations outperforms the
average system trained with the labels from a sin-
gle expert. One possible hypothesis for the cause
of this non-intuitive result is that individual labelers
(including experts) tend to have a strong bias, and
since multiple non-expert labelers may contribute to
a single set of non-expert annotations, the annotator
diversity within the single set of labels may have the
effect of reducing annotator bias and thus increasing
system performance.
7 Conclusion
We demonstrate the effectiveness of using Amazon
Mechanical Turk for a variety of natural language
annotation tasks. Our evaluation of non-expert la-
beler data vs. expert annotations for five tasks found
that for many tasks only a small number of non-
expert annotations per item are necessary to equal
the performance of an expert annotator. In a detailed
study of expert and non-expert agreement for an af-
fect recognition task we find that we require an av-
erage of 4 non-expert labels per item in order to em-
ulate expert-level label quality. Finally, we demon-
strate significant improvement by controlling for la-
beler bias.
Acknowledgments
Thanks to Nathanael Chambers, Annie Zaenen,
Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Car-
penter, David Vickrey, William Morgan, and Lukas
Biewald for useful discussions, and for the gener-
ous support of Dolores Labs. This work was sup-
ported in part by the Disruptive Technology Office
(DTO)?s Advanced Question Answering for Intelli-
gence (AQUAINT) Phase III Program.
References
Paul S. Albert and Lori E. Dodd. 2004. A Cautionary
Note on the Robustness of Latent Class Models for
Estimating Diagnostic Error without a Gold Standard.
Biometrics, Vol. 60 (2004), pp. 427-435.
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc. of
COLING-ACL 1998.
Michele Banko and Eric Brill. 2001. Scaling to Very
Very Large Corpora for Natural Language Disam-
biguation. In Proc. of ACL-2001.
Junfu Cai, Wee Sun Lee and Yee Whye Teh. 2007. Im-
proving Word Sense Disambiguation Using Topic Fea-
tures. In Proc. of EMNLP-2007 .
262
Timothy Chklovski and Rada Mihalcea. 2002. Building
a sense tagged corpus with Open Mind Word Expert.
In Proc. of the Workshop on ?Word Sense Disam-
biguation: Recent Successes and Future Directions?,
ACL 2002.
Timothy Chklovski and Yolanda Gil. 2005. Towards
Managing Knowledge Collection from Volunteer Con-
tributors. Proceedings of AAAI Spring Symposium
on Knowledge Collection from Volunteer Contributors
(KCVC05).
Ido Dagan, Oren Glickman and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
Challenge. Machine Learning Challenges. Lecture
Notes in Computer Science, Vol. 3944, pp. 177-190,
Springer, 2006.
Wisam Dakka and Panagiotis G. Ipeirotis. 2008. Au-
tomatic Extraction of Useful Facet Terms from Text
Documents. In Proc. of ICDE-2008.
A. P. Dawid and A. M. Skene. 1979. Maximum Like-
lihood Estimation of Observer Error-Rates Using the
EM Algorithm. Applied Statistics, Vol. 28, No. 1
(1979), pp. 20-28.
Michael Kaisser and John B. Lowe. 2008. A Re-
search Collection of QuestionAnswer Sentence Pairs.
In Proc. of LREC-2008.
Michael Kaisser, Marti Hearst, and John B. Lowe.
2008. Evidence for Varying Search Results Summary
Lengths. In Proc. of ACL-2008.
Phil Katz, Matthew Singleton, Richard Wicentowski.
2007. SWAT-MP: The SemEval-2007 Systems for
Task 5 and Task 14. In Proc. of SemEval-2007.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk. In
Proc. of CHI-2008.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics 19:2, June 1993.
George A. Miller and William G. Charles. 1991. Con-
textual Correlates of Semantic Similarity. Language
and Cognitive Processes, vol. 6, no. 1, pp. 1-28, 1991.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunke. 1993. A semantic concordance. In
Proc. of HLT-1993.
Preslav Nakov. 2008. Paraphrasing Verbs for Noun
Compound Interpretation. In Proc. of the Workshop
on Multiword Expressions, LREC-2008.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: A Corpus Annotated with Se-
mantic Roles. Computational Linguistics, 31:1.
Sameer Pradhan, Edward Loper, Dmitriy Dligach and
Martha Palmer. 2007. SemEval-2007 Task-17: En-
glish Lexical Sample, SRL and All Words. In Proc.
of SemEval-2007 .
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proc. of Corpus Linguistics 2003, 647-656.
Philip Resnik. 1999. Semantic Similarity in a Taxon-
omy: An Information-Based Measure and its Applica-
tion to Problems of Ambiguity in Natural Language.
JAIR, Volume 11, pages 95-130.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communications
of the ACM, 8(10):627?633.
Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeiro-
tis. 2008. Get Another Label? Improving Data Qual-
ity and Data Mining Using Multiple, Noisy Labelers.
In Proc. of KDD-2008.
Push Singh. 2002. The public acquisition of common-
sense knowledge. In Proc. of AAAI Spring Sympo-
sium on Acquiring (and Using) Linguistic (and World)
Knowledge for Information Access, 2002.
Alexander Sorokin and David Forsyth. 2008. Util-
ity data annotation with Amazon Mechanical Turk.
To appear in Proc. of First IEEE Workshop on
Internet Vision at CVPR, 2008. See also:
http://vision.cs.uiuc.edu/annotation/
David G. Stork. 1999. The Open Mind Initiative.
IEEE Expert Systems and Their Applications pp. 16-
20, May/June 1999.
Carlo Strapparava and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text In Proc. of SemEval-
2007.
Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell C.
Baker. 2007. Internet-Scale Collection of Human-
Reviewed Data. In Proc. of WWW-2007.
Luis von Ahn and Laura Dabbish. 2004. Labeling Im-
ages with a Computer Game. In ACM Conference on
Human Factors in Computing Systems, CHI 2004.
Luis von Ahn, Mihir Kedia and Manuel Blum. 2006.
Verbosity: A Game for Collecting Common-Sense
Knowledge. In ACM Conference on Human Factors
in Computing Systems, CHI Notes 2006.
Ellen Voorhees and Hoa Trang Dang. 2006. Overview of
the TREC 2005 question answering track. In Proc. of
TREC-2005.
Janyce M. Wiebe, Rebecca F. Bruce and Thomas P.
O?Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proc. of ACL-1999.
Annie Zaenen. Submitted. Do give a penny for their
thoughts. International Journal of Natural Language
Engineering (submitted).
263
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1277?1287,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Latent Variable Model for Geographic Lexical Variation
Jacob Eisenstein Brendan O?Connor Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,brendano,nasmith,epxing}@cs.cmu.edu
Abstract
The rapid growth of geotagged social media
raises new computational possibilities for in-
vestigating geographic linguistic variation. In
this paper, we present a multi-level generative
model that reasons jointly about latent topics
and geographical regions. High-level topics
such as ?sports? or ?entertainment? are ren-
dered differently in each geographic region,
revealing topic-specific regional distinctions.
Applied to a new dataset of geotagged mi-
croblogs, our model recovers coherent top-
ics and their regional variants, while identi-
fying geographic areas of linguistic consis-
tency. The model also enables prediction of
an author?s geographic location from raw text,
outperforming both text regression and super-
vised topic models.
1 Introduction
Sociolinguistics and dialectology study how lan-
guage varies across social and regional contexts.
Quantitative research in these fields generally pro-
ceeds by counting the frequency of a handful of
previously-identified linguistic variables: pairs of
phonological, lexical, or morphosyntactic features
that are semantically equivalent, but whose fre-
quency depends on social, geographical, or other
factors (Paolillo, 2002; Chambers, 2009). It is left to
the experimenter to determine which variables will
be considered, and there is no obvious procedure for
drawing inferences from the distribution of multiple
variables. In this paper, we present a method for
identifying geographically-aligned lexical variation
directly from raw text. Our approach takes the form
of a probabilistic graphical model capable of iden-
tifying both geographically-salient terms and coher-
ent linguistic communities.
One challenge in the study of lexical variation is
that term frequencies are influenced by a variety of
factors, such as the topic of discourse. We address
this issue by adding latent variables that allow us to
model topical variation explicitly. We hypothesize
that geography and topic interact, as ?pure? topi-
cal lexical distributions are corrupted by geographi-
cal factors; for example, a sports-related topic will
be rendered differently in New York and Califor-
nia. Each author is imbued with a latent ?region?
indicator, which both selects the regional variant of
each topic, and generates the author?s observed ge-
ographical location. The regional corruption of top-
ics is modeled through a cascade of logistic normal
priors?a general modeling approach which we call
cascading topic models. The resulting system has
multiple capabilities, including: (i) analyzing lexi-
cal variation by both topic and geography; (ii) seg-
menting geographical space into coherent linguistic
communities; (iii) predicting author location based
on text alone.
This research is only possible due to the rapid
growth of social media. Our dataset is derived from
the microblogging website Twitter,1 which permits
users to post short messages to the public. Many
users of Twitter also supply exact geographical co-
ordinates from GPS-enabled devices (e.g., mobile
phones),2 yielding geotagged text data. Text in
computer-mediated communication is often more
vernacular (Tagliamonte and Denis, 2008), and as
such it is more likely to reveal the influence of ge-
ographic factors than text written in a more formal
genre, such as news text (Labov, 1966).
We evaluate our approach both qualitatively and
quantitatively. We investigate the topics and regions
1http://www.twitter.com
2User profiles also contain self-reported location names, but
we do not use that information in this work.
1277
that the model obtains, showing both common-sense
results (place names and sports teams are grouped
appropriately), as well as less-obvious insights about
slang. Quantitatively, we apply our model to predict
the location of unlabeled authors, using text alone.
On this task, our model outperforms several alterna-
tives, including both discriminative text regression
and related latent-variable approaches.
2 Data
The main dataset in this research is gathered from
the microblog website Twitter, via its official API.
We use an archive of messages collected over the
first week of March 2010 from the ?Gardenhose?
sample stream,3 which then consisted of 15% of
all public messages, totaling millions per day. We
aggressively filter this stream, using only messages
that are tagged with physical (latitude, longitude)
coordinate pairs from a mobile client, and whose au-
thors wrote at least 20 messages over this period. We
also filter to include only authors who follow fewer
than 1,000 other people, and have fewer than 1,000
followers. Kwak et al (2010) find dramatic shifts
in behavior among users with social graph connec-
tivity outside of that range; such users may be mar-
keters, celebrities with professional publicists, news
media sources, etc. We also remove messages con-
taining URLs to eliminate bots posting information
such as advertising or weather conditions. For inter-
pretability, we restrict our attention to authors inside
a bounding box around the contiguous U.S. states,
yielding a final sample of about 9,500 users and
380,000 messages, totaling 4.7 million word tokens.
We have made this dataset available online.4
Informal text from mobile phones is challeng-
ing to tokenize; we adapt a publicly available tok-
enizer5 originally developed for Twitter (O?Connor
et al, 2010), which preserves emoticons and blocks
of punctuation and other symbols as tokens. For
each user?s Twitter feed, we combine all messages
into a single ?document.? We remove word types
that appear in fewer than 40 feeds, yielding a vocab-
ulary of 5,216 words. Of these, 1,332 do not appear
in the English, French, or Spanish dictionaries of the
3http://dev.twitter.com/pages/streaming_api
4http://www.ark.cs.cmu.edu/GeoTwitter
5http://tweetmotif.com
spell-checking program aspell.
Every message is tagged with a location, but most
messages from a single individual tend to come from
nearby locations (as they go about their day); for
modeling purposes we use only a single geographic
location for each author, simply taking the location
of the first message in the sample.
The authors in our dataset are fairly heavy Twit-
ter users, posting an average of 40 messages per day
(although we see only 15% of this total). We have
little information about their demographics, though
from the text it seems likely that this user set skews
towards teens and young adults. The dataset cov-
ers each of the 48 contiguous United States and the
District of Columbia.
3 Model
We develop a model that incorporates two sources
of lexical variation: topic and geographical region.
We treat the text and geographic locations as out-
puts from a generative process that incorporates both
topics and regions as latent variables.6 During infer-
ence, we seek to recover the topics and regions that
best explain the observed data.
At the base level of model are ?pure? topics (such
as ?sports?, ?weather?, or ?slang?); these topics are
rendered differently in each region. We call this gen-
eral modeling approach a cascading topic model; we
describe it first in general terms before moving to the
specific application to geographical variation.
3.1 Cascading Topic Models
Cascading topic models generate text from a chain
of random variables. Each element in the chain de-
fines a distribution over words, and acts as the mean
of the distribution over the subsequent element in
the chain. Thus, each element in the chain can be
thought of as introducing some additional corrup-
tion. All words are drawn from the final distribution
in the chain.
At the beginning of the chain are the priors, fol-
lowed by unadulerated base topics, which may then
be corrupted by other factors (such as geography or
time). For example, consider a base ?food? topic
6The region could be observed by using a predefined geo-
graphical decomposition, e.g., political boundaries. However,
such regions may not correspond well to linguistic variation.
1278
that emphasizes words like dinner and delicious;
the corrupted ?food-California? topic would place
weight on these words, but might place extra em-
phasis on other words like sprouts.
The path through the cascade is determined by a
set of indexing variables, which may be hidden or
observed. As in standard latent Dirichlet alocation
(Blei et al, 2003), the base topics are selected by
a per-token hidden variable z. In the geographical
topic model, the next level corresponds to regions,
which are selected by a per-author latent variable r.
Formally, we draw each level of the cascade from
a normal distribution centered on the previous level;
the final multinomial distribution over words is ob-
tained by exponentiating and normalizing. To ensure
tractable inference, we assume that all covariance
matrices are uniform diagonal, i.e., aI with a > 0;
this means we do not model interactions between
words.
3.2 The Geographic Topic Model
The application of cascading topic models to ge-
ographical variation is straightforward. Each doc-
ument corresponds to the entire Twitter feed of a
given author during the time period covered by our
corpus. For each author, the latent variable r cor-
responds to the geographical region of the author,
which is not observed. As described above, r se-
lects a corrupted version of each topic: the kth basic
topic has mean ?k, with uniform diagonal covari-
ance ?2k; for region j, we can draw the regionally-
corrupted topic from the normal distribution, ?jk ?
N(?k, ?
2
kI).
Because ? is normally-distributed, it lies not in
the simplex but in RW . We deterministically com-
pute multinomial parameters ? by exponentiating
and normalizing: ?jk = exp(?jk)/
?
i exp(?
(i)
jk ).
This normalization could introduce identifiability
problems, as there are multiple settings for ? that
maximize P (w|?) (Blei and Lafferty, 2006a). How-
ever, this difficulty is obviated by the priors: given
? and ?2, there is only a single ? that maximizes
P (w|?)P (?|?, ?2); similarly, only a single ?max-
imizes P (?|?)P (?|a, b2).
The observed latitude and longitude, denoted y,
are normally distributed and conditioned on the re-
gion, with mean ?r and precision matrix ?r indexed
by the region r. The region index r is itself drawn
from a single shared multinomial ?. The model is
shown as a plate diagram in Figure 1.
Given a vocabulary size W , the generative story
is as follows:
? Generate base topics: for each topic k < K
? Draw the base topic from a normal distribu-
tion with uniform diagonal covariance: ?k ?
N(a, b2I),
? Draw the regional variance from a Gamma
distribution: ?2k ? G(c, d).
? Generate regional variants: for each region
j < J ,
? Draw the region-topic ?jk from a normal
distribution with uniform diagonal covari-
ance: ?jk ? N(?k, ?
2
kI).
? Convert ?jk into a multinomial
distribution over words by ex-
ponentiating and normalizing:
?jk = exp
(
?jk
)
/
?W
i exp(?
(i)
jk ),
where the denominator sums over the
vocabulary.
? Generate regions: for each region j < J ,
? Draw the spatial mean ?j from a normal dis-
tribution.
? Draw the precision matrix ?j from a Wishart
distribution.
? Draw the distribution over regions ? from a sym-
metric Dirichlet prior, ? ? Dir(??1).
? Generate text and locations: for each document d,
? Draw topic proportions from a symmetric
Dirichlet prior, ? ? Dir(?1).
? Draw the region r from the multinomial dis-
tribution ?.
? Draw the location y from the bivariate Gaus-
sian, y ? N(?r,?r).
? For each word token,
? Draw the topic indicator z ? ?.
? Draw the word token w ? ?rz .
4 Inference
We apply mean-field variational inference: a fully-
factored variational distribution Q is chosen to min-
imize the Kullback-Leibler divergence from the
true distribution. Mean-field variational inference
with conjugate priors is described in detail else-
where (Bishop, 2006; Wainwright and Jordan,
2008); we restrict our focus to the issues that are
unique to the geographic topic model.
1279
? ?
w
z
?
D
Nd
y
r K
?
J
?
?
?
??2
?
?k log of base topic k?s distribution over word types
?2k variance parameter for regional variants of topic k
?jk region j?s variant of base topic ?k
?d author d?s topic proportions
rd author d?s latent region
yd author d?s observed GPS location
?j region j?s spatial center
?j region j?s spatial precision
zn token n?s topic assignment
wn token n?s observed word type
? global prior over author-topic proportions
? global prior over region classes
Figure 1: Plate diagram for the geographic topic model, with a table of all random variables. Priors (besides ?) are
omitted for clarity, and the document indices on z and w are implicit.
We place variational distributions over all latent
variables of interest: ?, z, r,?,?,?, ?2,?, and ?,
updating each of these distributions in turn, until
convergence. The variational distributions over ?
and ? are Dirichlet, and have closed form updates:
each can be set to the sum of the expected counts,
plus a term from the prior (Blei et al, 2003). The
variational distributions q(z) and q(r) are categor-
ical, and can be set proportional to the expected
joint likelihood?to set q(z) we marginalize over r,
and vice versa.7 The updates for the multivariate
Gaussian spatial parameters ? and ? are described
by Penny (2001).
4.1 Regional Word Distributions
The variational region-topic distribution ?jk is nor-
mal, with uniform diagonal covariance for tractabil-
ity. Throughout we will write ?x? to indicate the ex-
pectation of x under the variational distribution Q.
Thus, the vector mean of the distribution q(?jk) is
written ??jk?, while the variance (uniform across i)
of q(?) is written V(?jk).
To update the mean parameter ??jk?, we max-
imize the contribution to the variational bound L
from the relevant terms:
L
[??(i)jk ?]
= ?log p(w|?, z, r)?+?log p(?(i)jk |?
(i)
k , ?
2
k)?,
(1)
7Thanks to the na??ve mean field assumption, we can
marginalize over z by first decomposing across all Nd words
and then summing over q(z).
with the first term representing the likelihood of the
observed words (recall that ? is computed determin-
istically from ?) and the second term corresponding
to the prior. The likelihood term requires the expec-
tation ?log??, but this is somewhat complicated by
the normalizer
?W
i exp(?
(i)), which sums over all
terms in the vocabulary. As in previous work on lo-
gistic normal topic models, we use a Taylor approx-
imation for this term (Blei and Lafferty, 2006a).
The prior on ? is normal, so the contribution from
the second term of the objective (Equation 1) is
? 1
2??2k?
?(?(i)jk ? ?
(i)
k )
2?. We introduce the following
notation for expected counts: N(i, j, k) indicates the
expected count of term i in region j and topic k, and
N(j, k) =
?
iN(i, j, k). After some calculus, we
can write the gradient ?L/???((i))jk ? as
N(i, j, k)?N(j, k)??(i)jk ? ? ??
?2
k ?(??
(i)
jk ? ? ??
(i)
k ?),
(2)
which has an intuitive interpretation. The first two
terms represent the difference in expected counts for
term i under the variational distributions q(z, r) and
q(z, r, ?): this difference goes to zero when ?(i)jk per-
fectly matches N(i, j, k)/N(j, k). The third term
penalizes ?(i)jk for deviating from its prior ?
(i)
k , but
this penalty is proportional to the expected inverse
variance ???2k ?. We apply gradient ascent to maxi-
mize the objective L. A similar set of calculations
gives the gradient for the variance of ?; these are
described in an forthcoming appendix.
1280
4.2 Base Topics
The base topic parameters are?k and ?
2
k; in the vari-
ational distribution, q(?k) is normally distributed
and q(?2k) is Gamma distributed. Note that ?k and
?2k affect only the regional word distributions ?jk.
An advantage of the logistic normal is that the vari-
ational parameters over ?k are available in closed
form,
??(i)k ? =
b2
?J
j ??
(i)
jk ?+ ??
2
k?a
(i)
b2J + ??2k?
V(?k) = (b
?2 + J???2k ?)
?1,
where J indicates the number of regions. The ex-
pectation of the base topic ? incorporates the prior
and the average of the generated region-topics?
these two components are weighted respectively by
the expected variance of the region-topics ??2k? and
the prior topical variance b2. The posterior variance
V(?) is a harmonic combination of the prior vari-
ance b2 and the expected variance of the region top-
ics.
The variational distribution over the region-topic
variance ?2k has Gamma parameters. These param-
eters cannot be updated in closed form, so gradi-
ent optimization is again required. The derivation
of these updates is more involved, and is left for a
forthcoming appendix.
5 Implementation
Variational scheduling and initialization are impor-
tant aspects of any hierarchical generative model,
and are often under-discussed. In our implementa-
tion, the variational updates are scheduled as fol-
lows: given expected counts, we iteratively update
the variational parameters on the region-topics ? and
the base topics?, until convergence. We then update
the geographical parameters ? and ?, as well as the
distribution over regions ?. Finally, for each doc-
ument we iteratively update the variational param-
eters over ?, z, and r until convergence, obtaining
expected counts that are used in the next iteration
of updates for the topics and their regional variants.
We iterate an outer loop over the entire set of updates
until convergence.
We initialize the model in a piecewise fashion.
First we train a Dirichlet process mixture model on
the locations y, using variational inference on the
truncated stick-breaking approximation (Blei and
Jordan, 2006). This automatically selects the num-
ber of regions J , and gives a distribution over each
region indicator rd from geographical information
alone. We then run standard latent Dirichlet aloca-
tion to obtain estimates of z for each token (ignoring
the locations). From this initialization we can com-
pute the first set of expected counts, which are used
to obtain initial estimates of all parameters needed
to begin variational inference in the full model.
The prior a is the expected mean of each topic
?; for each term i, we set a(i) = logN(i) ? logN ,
where N(i) is the total count of i in the corpus and
N =
?
iN(i). The variance prior b
2 is set to 1, and
the prior on ?2 is the Gamma distribution G(2, 200),
encouraging minimal deviation from the base topics.
The symmetric Dirichlet prior on ? is set to 12 , and
the symmetric Dirichlet parameter on ? is updated
from weak hyperpriors (Minka, 2003). Finally, the
geographical model takes priors that are linked to the
data: for each region, the mean is very weakly en-
couraged to be near the overall mean, and the covari-
ance prior is set by the average covariance of clusters
obtained by running K-means.
6 Evaluation
For a quantitative evaluation of the estimated rela-
tionship between text and geography, we assess our
model?s ability to predict the geographic location of
unlabeled authors based on their text alone.8 This
task may also be practically relevant as a step toward
applications for recommending local businesses or
social connections. A randomly-chosen 60% of au-
thors are used for training, 20% for development,
and the remaining 20% for final evaluation.
6.1 Systems
We compare several approaches for predicting au-
thor location; we divide these into latent variable
generative models and discriminative approaches.
8Alternatively, one might evaluate the attributed regional
memberships of the words themselves. While the Dictionary of
American Regional English (Cassidy and Hall, 1985) attempts
a comprehensive list of all regionally-affiliated terms, it is based
on interviews conducted from 1965-1970, and the final volume
(covering Si?Z) is not yet complete.
1281
6.1.1 Latent Variable Models
Geographic Topic Model This is the full version
of our system, as described in this paper. To pre-
dict the unseen location yd, we iterate until con-
vergence on the variational updates for the hidden
topics zd, the topic proportions ?d, and the region
rd. From rd, the location can be estimated as y?d =
arg maxy
?J
j p(y|?j ,?j)q(rd = j). The develop-
ment set is used to tune the number of topics and to
select the best of multiple random initializations.
Mixture of Unigrams A core premise of our ap-
proach is that modeling topical variation will im-
prove our ability to understand geographical varia-
tion. We test this idea by fixing K = 1, running our
system with only a single topic. This is equivalent
to a Bayesian mixture of unigrams in which each au-
thor is assigned a single, regional unigram language
model that generates all of his or her text. The de-
velopment set is used to select the best of multiple
random initializations.
Supervised Latent Dirichlet Allocation In a
more subtle version of the mixture-of-unigrams
model, we model each author as an admixture of re-
gions. Thus, the latent variable attached to each au-
thor is no longer an index, but rather a vector on the
simplex. This model is equivalent to supervised la-
tent Dirichlet alocation (Blei and McAuliffe, 2007):
each topic is associated with equivariant Gaussian
distributions over the latitude and longitude, and
these topics must explain both the text and the ob-
served geographical locations. For unlabeled au-
thors, we estimate latitude and longitude by esti-
mating the topic proportions and then applying the
learned geographical distributions. This is a linear
prediction
f(z?d;a) = (z?
T
da
lat, z?Tda
lon)
for an author?s topic proportions z?d and topic-
geography weights a ? R2K .
6.1.2 Baseline Approaches
Text Regression We perform linear regression
to discriminatively learn the relationship between
words and locations. Using term frequency features
xd for each author, we predict locations with word-
geography weights a ? R2W :
f(xd;a) = (x
T
da
lat, xTda
lon)
Weights are trained to minimize the sum of squared
Euclidean distances, subject to L1 regularization:
?
d
(xTda
lat ? ylatd )
2 + (xTda
lon ? ylond )
2
+ ?lat||a
lat||1 + ?lon||a
lon||1
The minimization problem decouples into two sep-
arate latitude and longitude models, which we fit
using the glmnet elastic net regularized regres-
sion package (Friedman et al, 2010), which ob-
tained good results on other text-based prediction
tasks (Joshi et al, 2010). Regularization parameters
were tuned on the development set. The L1 penalty
outperformed L2 and mixtures of L1 and L2.
Note that for both word-level linear regression
here, and the topic-level linear regression in SLDA,
the choice of squared Euclidean distance dovetails
with our use of spatial Gaussian likelihoods in the
geographic topic models, since optimizing a is
equivalent to maximum likelihood estimation un-
der the assumption that locations are drawn from
equivariant circular Gaussians centered around each
f(xd;a) linear prediction. We experimented with
decorrelating the location dimensions by projecting
yd into the principal component space, but this did
not help text regression.
K-Nearest Neighbors Linear regression is a poor
model for the multimodal density of human popula-
tions. As an alternative baseline, we applied super-
vised K-nearest neighbors to predict the location yd
as the average of the positions of the K most sim-
ilar authors in the training set. We computed term-
frequency inverse-document frequency features and
applied cosine similarity over their first 30 principal
components to find the neighbors. The choices of
principal components, IDF weighting, and neighbor-
hood size K = 20 were tuned on the development
set.
6.2 Metrics
Our principle error metrics are the mean and median
distance between the predicted and true location in
kilometers.9 Because the distance error may be dif-
ficult to interpret, we also report accuracy of classi-
9For convenience, model training and prediction use latitude
and longitude as an unprojected 2D Euclidean space. However,
properly measuring the physical distance between points on the
1282
Regression Classification accuracy (%)
System Mean Dist. (km) Median Dist. (km) Region (4-way) State (49-way)
Geographic topic model 900 494 58 24
Mixture of unigrams 947 644 53 19
Supervised LDA 1055 728 39 4
Text regression 948 712 41 4
K-nearest neighbors 1077 853 37 2
Mean location 1148 1018
Most common class 37 27
Table 1: Location prediction results; lower scores are better on the regression task, higher scores are better on the
classification task. Distances are in kilometers. Mean location and most common class are computed from the test set.
Both the geographic topic model and supervised LDA use the best number of topics from the development set (10 and
5, respectively).
fication by state and by region of the United States.
Our data includes the 48 contiguous states plus the
District of Columbia; the U.S. Census Bureau di-
vides these states into four regions: West, Midwest,
Northeast, and South.10 Note that while major pop-
ulation centers straddle several state lines, most re-
gion boundaries are far from the largest cities, re-
sulting in a clearer analysis.
6.3 Results
As shown in Table 1, the geographic topic model
achieves the strongest performance on all metrics.
All differences in performance between systems
are statistically significant (p < .01) using the
Wilcoxon-Mann-Whitney test for regression error
and the ?2 test for classification accuracy. Figure 2
shows how performance changes as the number of
topics varies.
Note that the geographic topic model and the mix-
ture of unigrams use identical code and parametriza-
tion ? the only difference is that the geographic topic
model accounts for topical variation, while the mix-
ture of unigrams sets K = 1. These results validate
our basic premise that it is important to model the
interaction between topical and geographical varia-
tion.
Text regression and supervised LDA perform es-
pecially poorly on the classification metric. Both
methods make predictions that are averaged across
Earth?s surface requires computing or approximating the great
circle distance ? we use the Haversine formula (Sinnott, 1984).
For the continental U.S., the relationship between degrees and
kilometers is nearly linear, but extending the model to a conti-
nental scale would require a more sophisticated approach.
10http://www.census.gov/geo/www/us_regdiv.pdf
0 5 10 15 20400
500
600
700
800
900
1000
1100
Number of topics
Me
dia
n e
rro
r (k
m)
 
 
Geographic Topic Model
Supervised LDA
Mean location
Figure 2: The effect of varying the number of topics on
the median regression error (lower is better).
each word in the document: in text regression, each
word is directly multiplied by a feature weight; in
supervised LDA the word is associated with a la-
tent topic first, and then multiplied by a weight. For
these models, all words exert an influence on the pre-
dicted location, so uninformative words will draw
the prediction towards the center of the map. This
yields reasonable distance errors but poor classifica-
tion accuracy. We had hoped that K-nearest neigh-
bors would be a better fit for this metric, but its per-
formance is poor at all values of K. Of course it is
always possible to optimize classification accuracy
directly, but such an approach would be incapable
of predicting the exact geographical location, which
is the focus of our evaluation (given that the desired
geographical partition is unknown). Note that the
geographic topic model is also not trained to opti-
mize classification accuracy.
1283
?basketball?
?popular
music?
?daily life? ?emoticons? ?chit chat?
PISTONS KOBE
LAKERS game
DUKE NBA
CAVS STUCKEY
JETS KNICKS
album music
beats artist video
#LAKERS
ITUNES tour
produced vol
tonight shop
weekend getting
going chilling
ready discount
waiting iam
:) haha :d :( ;) :p
xd :/ hahaha
hahah
lol smh jk yea
wyd coo ima
wassup
somethin jp
Boston
+ CELTICS victoryBOSTON
CHARLOTTE
playing daughter
PEARL alive war
comp
BOSTON ;p gna loveee
ese exam suttin
sippin
N. California+
THUNDER
KINGS GIANTS
pimp trees clap
SIMON dl
mountain seee 6am OAKLAND
pues hella koo
SAN fckn
hella flirt hut
iono OAKLAND
New York
+ NETS KNICKS BRONX iam cab oww wasssup nm
Los Angeles+
#KOBE
#LAKERS
AUSTIN
#LAKERS load
HOLLYWOOD
imm MICKEY
TUPAC
omw tacos hr
HOLLYWOOD
af papi raining
th bomb coo
HOLLYWOOD
wyd coo af nada
tacos messin
fasho bomb
Lake Erie
+
CAVS
CLEVELAND
OHIO BUCKS od
COLUMBUS
premiere prod
joint TORONTO
onto designer
CANADA village
burr
stink CHIPOTLE
tipsy
;d blvd BIEBER
hve OHIO
foul WIZ salty
excuses lames
officer lastnight
Table 2: Example base topics (top line) and regional variants. For the base topics, terms are ranked by log-odds
compared to the background distribution. The regional variants show words that are strong compared to both the base
topic and the background. Foreign-language words are shown in italics, while terms that are usually in proper nouns
are shown in SMALL CAPS. See Table 3 for definitions of slang terms; see Section 7 for more explanation and details
on the methodology.
Figure 3: Regional clustering of the training set obtained by one randomly-initialized run of the geographical topic
model. Each point represents one author, and each shape/color combination represents the most likely cluster as-
signment. Ellipses represent the regions? spatial means and covariances. The same model and coloring are shown in
Table 2.
1284
7 Analysis
Our model permits analysis of geographical vari-
ation in the context of topics that help to clarify
the significance of geographically-salient terms. Ta-
ble 2 shows a subset of the results of one randomly-
initialized run, including five hand-chosen topics (of
50 total) and five regions (of 13, as chosen automat-
ically during initialization). Terms were selected by
log-odds comparison. For the base topics we show
the ten strongest terms in each topic as compared to
the background word distribution. For the regional
variants, we show terms that are strong both region-
ally and topically: specifically, we select terms that
are in the top 100 compared to both the background
distribution and to the base topic. The names for the
topics and regions were chosen by the authors.
Nearly all of the terms in column 1 (?basketball?)
refer to sports teams, athletes, and place names?
encouragingly, terms tend to appear in the regions
where their referents reside. Column 2 contains sev-
eral proper nouns, mostly referring to popular mu-
sic figures (including PEARL from the band Pearl
Jam).11 Columns 3?5 are more conversational.
Spanish-language terms (papi, pues, nada, ese) tend
to appear in regions with large Spanish-speaking
populations?it is also telling that these terms ap-
pear in topics with emoticons and slang abbrevia-
tions, which may transcend linguistic barriers. Other
terms refer to people or subjects that may be espe-
cially relevant in certain regions: tacos appears in
the southern California region and cab in the New
York region; TUPAC refers to a rap musician from
Los Angeles, and WIZ refers to a rap musician from
Pittsburgh, not far from the center of the ?Lake Erie?
region.
A large number of slang terms are found to have
strong regional biases, suggesting that slang may
depend on geography more than standard English
does. The terms af and hella display especially
strong regional affinities, appearing in the regional
variants of multiple topics (see Table 3 for defini-
tions). Northern and Southern California use variant
spellings koo and coo to express the same meaning.
11This analysis is from an earlier version of our dataset that
contained some Twitterbots, including one from a Boston-area
radio station. The bots were purged for the evaluation in Sec-
tion 6, though the numerical results are nearly identical.
term definition
af as fuck (very)
coo cool
dl download
fasho for sure
gna going to
hella very
hr hour
iam I am
ima I?m going to
imm I?m
iono I don?t know
lames lame (not cool)
people
term definition
jk just kidding
jp just playing (kid-
ding)
koo cool
lol laugh out loud
nm nothing much
od overdone (very)
omw on my way
smh shake my head
suttin something
wassup what?s up
wyd what are you do-
ing?
Table 3: A glossary of non-standard terms from Ta-
ble 2. Definitions are obtained by manually inspecting
the context in which the terms appear, and by consulting
www.urbandictionary.com.
While research in perceptual dialectology does con-
firm the link of hella to Northern California (Bu-
choltz et al, 2007), we caution that our findings
are merely suggestive, and a more rigorous analysis
must be undertaken before making definitive state-
ments about the regional membership of individual
terms. We view the geographic topic model as an
exploratory tool that may be used to facilitate such
investigations.
Figure 3 shows the regional clustering on the
training set obtained by one run of the model. Each
point represents an author, and the ellipses represent
the bivariate Gaussians for each region. There are
nine compact regions for major metropolitan areas,
two slightly larger regions that encompass Florida
and the area around Lake Erie, and two large re-
gions that partition the country roughly into north
and south.
8 Related Work
The relationship between language and geography
has been a topic of interest to linguists since the
nineteenth century (Johnstone, 2010). An early
work of particular relevance is Kurath?s Word Geog-
raphy of the Eastern United States (1949), in which
he conducted interviews and then mapped the oc-
currence of equivalent word pairs such as stoop and
porch. The essence of this approach?identifying
variable pairs and measuring their frequencies?
remains a dominant methodology in both dialec-
1285
tology (Labov et al, 2006) and sociolinguis-
tics (Tagliamonte, 2006). Within this paradigm,
computational techniques are often applied to post
hoc analysis: logistic regression (Sankoff et al,
2005) and mixed-effects models (Johnson, 2009) are
used to measure the contribution of individual vari-
ables, while hierarchical clustering and multidimen-
sional scaling enable aggregated inference across
multiple variables (Nerbonne, 2009). However, in
all such work it is assumed that the relevant linguis-
tic variables have already been identified?a time-
consuming process involving considerable linguistic
expertise. We view our work as complementary to
this tradition: we work directly from raw text, iden-
tifying both the relevant features and coherent lin-
guistic communities.
An active recent literature concerns geotagged in-
formation on the web, such as search queries (Back-
strom et al, 2008) and tagged images (Crandall et
al., 2009). This research identifies the geographic
distribution of individual queries and tags, but does
not attempt to induce any structural organization of
either the text or geographical space, which is the
focus of our research. More relevant is the work
of Mei et al (2006), in which the distribution over
latent topics in blog posts is conditioned on the ge-
ographical location of the author. This is somewhat
similar to the supervised LDA model that we con-
sider, but their approach assumes that a partitioning
of geographical space into regions is already given.
Methodologically, our cascading topic model is
designed to capture multiple dimensions of variabil-
ity: topics and geography. Mei et al (2007) include
sentiment as a second dimension in a topic model,
using a switching variable so that individual word
tokens may be selected from either the topic or the
sentiment. However, our hypothesis is that individ-
ual word tokens reflect both the topic and the ge-
ographical aspect. Sharing this intuition, Paul and
Girju (2010) build topic-aspect models for the cross
product of topics and aspects. They do not impose
any regularity across multiple aspects of the same
topic, so this approach may not scale when the num-
ber of aspects is large (they consider only two as-
pects). We address this issue using cascading distri-
butions; when the observed data for a given region-
topic pair is low, the model falls back to the base
topic. The use of cascading logistic normal distri-
butions in topic models follows earlier work on dy-
namic topic models (Blei and Lafferty, 2006b; Xing,
2005).
9 Conclusion
This paper presents a model that jointly identifies
words with high regional affinity, geographically-
coherent linguistic regions, and the relationship be-
tween regional and topic variation. The key model-
ing assumption is that regions and topics interact to
shape observed lexical frequencies. We validate this
assumption on a prediction task in which our model
outperforms strong alternatives that do not distin-
guish regional and topical variation.
We see this work as a first step towards a unsuper-
vised methodology for modeling linguistic variation
using raw text. Indeed, in a study of morphosyn-
tactic variation, Szmrecsanyi (2010) finds that by
the most generous measure, geographical factors ac-
count for only 33% of the observed variation. Our
analysis might well improve if non-geographical
factors were considered, including age, race, gen-
der, income and whether a location is urban or ru-
ral. In some regions, estimates of many of these fac-
tors may be obtained by cross-referencing geogra-
phy with demographic data. We hope to explore this
possibility in future work.
Acknowledgments
We would like to thank Amr Ahmed, Jonathan Chang,
Shay Cohen, William Cohen, Ross Curtis, Miro Dud??k,
Scott Kiesling, Seyoung Kim, and the anonymous re-
viewers. This research was enabled by Google?s sup-
port of the Worldly Knowledge project at CMU, AFOSR
FA9550010247, ONR N0001140910758, NSF CAREER
DBI-0546594, NSF IIS-0713379, and an Alfred P. Sloan
Fellowship.
References
L. Backstrom, J. Kleinberg, R. Kumar, and J. Novak.
2008. Spatial variation in search engine queries. In
Proceedings of WWW.
C. M. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
D. M. Blei and M. I. Jordan. 2006. Variational infer-
ence for Dirichlet process mixtures. Bayesian Analy-
sis, 1:121?144.
1286
D. M. Blei and J. Lafferty. 2006a. Correlated topic mod-
els. In NIPS.
D. M. Blei and J. Lafferty. 2006b. Dynamic topic mod-
els. In Proceedings of ICML.
D. M. Blei and J. D. McAuliffe. 2007. Supervised topic
models. In NIPS.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. JMLR, 3:993?1022.
M. Bucholtz, N. Bermudez, V. Fung, L. Edwards, and
R. Vargas. 2007. Hella Nor Cal or totally So Cal?
the perceptual dialectology of California. Journal of
English Linguistics, 35(4):325?352.
F. G. Cassidy and J. H. Hall. 1985. Dictionary of Amer-
ican Regional English, volume 1. Harvard University
Press.
J. Chambers. 2009. Sociolinguistic Theory: Linguistic
Variation and its Social Significance. Blackwell.
D. J Crandall, L. Backstrom, D. Huttenlocher, and
J. Kleinberg. 2009. Mapping the world?s photos. In
Proceedings of WWW, page 761770.
J. Friedman, T. Hastie, and R. Tibshirani. 2010. Regular-
ization paths for generalized linear models via coordi-
nate descent. Journal of Statistical Software, 33(1).
D. E. Johnson. 2009. Getting off the GoldVarb standard:
Introducing Rbrul for mixed-effects variable rule anal-
ysis. Language and Linguistics Compass, 3(1):359?
383.
B. Johnstone. 2010. Language and place. In R. Mesthrie
and W. Wolfram, editors, Cambridge Handbook of So-
ciolinguistics. Cambridge University Press.
M. Joshi, D. Das, K. Gimpel, and N. A. Smith. 2010.
Movie reviews and revenues: An experiment in text
regression. In Proceedings of NAACL-HLT.
H. Kurath. 1949. A Word Geography of the Eastern
United States. University of Michigan Press.
H. Kwak, C. Lee, H. Park, and S. Moon. 2010. What
is Twitter, a social network or a news media? In Pro-
ceedings of WWW.
W. Labov, S. Ash, and C. Boberg. 2006. The Atlas of
North American English: Phonetics, Phonology, and
Sound Change. Walter de Gruyter.
W. Labov. 1966. The Social Stratification of English in
New York City. Center for Applied Linguistics.
Q. Mei, C. Liu, H. Su, and C. X Zhai. 2006. A proba-
bilistic approach to spatiotemporal theme pattern min-
ing on weblogs. In Proceedings of WWW, page 542.
Q. Mei, X. Ling, M. Wondra, H. Su, and C. X. Zhai.
2007. Topic sentiment mixture: modeling facets and
opinions in weblogs. In Proceedings of WWW.
T. P. Minka. 2003. Estimating a Dirichlet distribution.
Technical report, Massachusetts Institute of Technol-
ogy.
J. Nerbonne. 2009. Data-driven dialectology. Language
and Linguistics Compass, 3(1).
B. O?Connor, M. Krieger, and D. Ahn. 2010. TweetMo-
tif: Exploratory search and topic summarization for
twitter. In Proceedings of ICWSM.
J. C. Paolillo. 2002. Analyzing Linguistic Variation: Sta-
tistical Models and Methods. CSLI Publications.
M. Paul and R. Girju. 2010. A two-dimensional topic-
aspect model for discovering multi-faceted topics. In
Proceedings of AAAI.
W. D. Penny. 2001. Variational Bayes for d-dimensional
Gaussian mixture models. Technical report, Univer-
sity College London.
D. Sankoff, S. A. Tagliamonte, and E. Smith. 2005.
Goldvarb X: A variable rule application for Macintosh
and Windows. Technical report, Department of Lin-
guistics, University of Toronto.
R. W. Sinnott. 1984. Virtues of the Haversine. Sky and
Telescope, 68(2).
B. Szmrecsanyi. 2010. Geography is overrated. In
S. Hansen, C. Schwarz, P. Stoeckle, and T. Streck, ed-
itors, Dialectological and Folk Dialectological Con-
cepts of Space. Walter de Gruyter.
S. A. Tagliamonte and D. Denis. 2008. Linguistic ruin?
LOL! Instant messanging and teen language. Ameri-
can Speech, 83.
S. A. Tagliamonte. 2006. Analysing Sociolinguistic Vari-
ation. Cambridge University Press.
M. J. Wainwright and M. I. Jordan. 2008. Graphical
Models, Exponential Families, and Variational Infer-
ence. Now Publishers.
E. P. Xing. 2005. On topic evolution. Technical Report
05-115, Center for Automated Learning and Discov-
ery, Carnegie Mellon University.
1287
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594?604,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Predicting a Scientific Community?s Response to an Article
Dani Yogatama Michael Heilman Brendan O?Connor Chris Dyer
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dyogatama,mheilman,brenocon,cdyer}@cs.cmu.edu
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We consider the problem of predicting mea-
surable responses to scientific articles based
primarily on their text content. Specif-
ically, we consider papers in two fields
(economics and computational linguistics)
and make predictions about downloads and
within-community citations. Our approach is
based on generalized linear models, allowing
interpretability; a novel extension that cap-
tures first-order temporal effects is also pre-
sented. We demonstrate that text features
significantly improve accuracy of predictions
over metadata features like authors, topical
categories, and publication venues.
1 Introduction
Written communication is an essential component
of the complex social phenomenon of science. As
such, natural language processing is well-positioned
to provide tools for understanding the scientific pro-
cess, by analyzing the textual artifacts (papers, pro-
ceedings, etc.) that it produces. This paper is about
modeling collections of scientific documents to un-
derstand how their textual content relates to how a
scientific community responds to them. While past
work has often focused on citation structure (Borner
et al, 2003; Qazvinian and Radev, 2008), our em-
phasis is on the text content, following Ramage et
al. (2010) and Gerrish and Blei (2010).
Instead of task-independent exploratory data anal-
ysis (e.g., topic modeling) or multi-document sum-
marization, we consider supervised models of the
collective response of a scientific community to a
published article. There are many measures of im-
pact of a scientific paper; ours come from direct
measurements of the number of downloads (from
an established website where prominent economists
post papers before formal publication) and citations
(within a fixed scientific community). We adopt a
discriminative approach based on generalized lin-
ear models that can make use of any text or meta-
data features, and show that simple lexical fea-
tures offer substantial power in modeling out-of-
sample response and in forecasting response for fu-
ture articles. Realistic forecasting evaluations re-
quire methodological care beyond the usual best
practices of train/test separation, and we elucidate
these issues.
In addition, we introduce a new regularization
technique that leverages the intuition that the rela-
tionship between observable features and response
should evolve smoothly over time. This regularizer
allows the learner to rely more strongly on more re-
cent evidence, while taking into account a long his-
tory of training data. Our time series-inspired regu-
larizer is computationally efficient in learning and is
a significant advance over earlier text-driven fore-
casting models that ignore the time variable alto-
gether (Kogan et al, 2009; Joshi et al, 2010).
We evaluate our approaches in two novel experi-
mental settings: predicting downloads of economics
articles and predicting citation of papers at ACL
conferences. Our approaches substantially outper-
594
0
15
00
4 9
log(# downloads)
# d
oc
s.
0
25
00
0 18
# citations
# d
oc
s.
Figure 1: Left: the distribution of log download counts
for papers in the NBER dataset one year after post-
ing. Right: the distribution of within-dataset citations of
ACL papers within three years of publication (outliers ex-
cluded for readability).
form text-ignorant baselines on ground-truth predic-
tions. Our time series models permit flexibility in
features and offer a novel and perhaps more inter-
pretable view of the data than summary statistics.
2 Data
We make use of two collections of scientific litera-
ture, one from the economics domain, and the other
from computational linguistics and natural language
processing. Statistics are summarized in Table 1.
2.1 NBER
Our first dataset consists of research papers in eco-
nomics from the National Bureau of Economic
Research (NBER) from 1999 to 2009 (http://
www.nber.org). Approximately 1,000 research
economists are affiliated with the NBER. New
NBER working papers are posted to the website
weekly. The papers are not yet peer-reviewed, but
given the prominence of many economists affiliated
with the NBER, many of the papers are widely read.
Text from the abstracts of the papers and related
metadata are publicly available. Full text is available
to subscribers (universities typically have access).
The NBER provided us with download statistics
for these papers. For each paper, we computed
the total number of downloads in the first year af-
ter each paper?s posting.1 The download counts are
log-normally distributed, as shown in Figure 1, and
so our regression models (?3) minimize squared er-
rors in the log space. Our download logs begin in
1For the vast majority of papers, most of the downloads oc-
cur soon after the paper?s posting. We explored different mea-
sures with different download windows (two years, for exam-
ple) with broadly similar results. We leave a more detailed anal-
ysis of the time series patterns of downloads to future work.
Dataset # Docs. Avg. #
Words
Response
NBER 8,814 155 # downloads in first
year (mean 761)
ACL 4,026 3,966 at least 1 citation in
first 3 years? (54% no)
Table 1: Descriptive statistics about the datasets.
1999. We use the 8,814 papers from 1999?2009 pe-
riod (there are 16,334 papers in the full dataset dat-
ing back to 1985). We only use text from the ab-
stracts, since we were able to obtain full texts for
just a portion of the papers, and since the OCR of
the full texts we do have is very noisy.
2.2 ACL
Our second dataset consists of research papers
from the Association for Computational Linguis-
tics (ACL) from 1980 to 2006 (Radev et al, 2009a;
Radev et al, 2009b). We have the full texts for pa-
pers (OCR output) as well as structured citation data.
There are 15,689 papers in the whole dataset. For
the citation prediction task, we include conference
papers from ACL, EACL, HLT, and NAACL.2 We
remove journal papers, since they are characteristi-
cally different from conference papers, as well as
workshop papers. We do include short papers, in-
teractive demo session papers, and student research
papers that are included in the companion volumes
for these conferences (such papers are cited less than
full papers, but many are still cited). The resulting
dataset contains 4,026 papers. The number of pa-
pers in each year varies because not all conferences
are annual.
We look at citations in the three-year window fol-
lowing publication, excluding self-citations and only
considering citations from papers within these con-
ferences. Figure 1 shows a histogram; note that
many papers (54%) are not cited at all, and the dis-
tribution of citations per paper is neither normal nor
log-normal. We organize the papers into two classes:
those with zero citations and those with non-zero ci-
tations in the three-year window.
2EMNLP is a relatively recent conference, and, in this col-
lection, complete data for its papers postdate the end of the last
training period, so we chose to exclude it from our dataset.
595
3 Model
Our forecasting approach is based on generalized
linear models for regression and classification. The
models are trained with an `2-penalty, often called
a ?ridge? model (Hoerl and Kennard, 1970).3 For
the NBER data, where (log) number of downloads is
nearly a continuous measure, we use linear regres-
sion. For the ACL data, where response is the bi-
nary cited-or-not variable we use logistic regression,
often referred to as a ?maximum entropy? model
(Berger et al, 1996) or a log-linear model. We
briefly review the class of models. Then, we de-
scribe a time series model appropriate for time series
data.
3.1 Generalized Linear Models
Consider a model that predicts a response y given a
vector input x = ?x1, . . . , xd? ? Rd. Our models
are linear functions of x and parameterized by the
vector ?. Given a corpus of M document features,
X , and responses Y , we estimate:
?? = argmin? R(?) + L(?,X, Y ) (1)
where L is a model-dependent loss function and R
is a regularization penalty to encourage models with
small weight vectors. We describe models and loss
functions first and then turn to regularization.
For the NBER data, the (log) number of down-
loads is continuous, and so we use least-squares
linear regression model. The loss function is the
sum of the squared errors for the M documents in
our training data: L(?,X, Y ) = ?Mi=1(yi ? y?i)2,
where the prediction rule for new documents is:
y? =
?d
j=0 ?jxj . Probabilistically, this equates to an
assumption that ?>x is the mean of a normal (i.e.,
Gaussian) distribution from which random variable
y is drawn.
For the ACL data, we predict y from a discrete
set C (specifically, the binary set of zero citations or
more than zero citations), and we use logistic regres-
sion. This model assumes that for the ith training
input xi, the output yi is drawn according to:
p(yi | xi) =
(
exp?>c xi
) /(?
c??C exp?>c?xi
)
3Preliminary experiments found no consistent benefit from
`1 (?lasso?) models, though we note that `1-regularization leads
to sparse, compact models that may be more interpretable.
where there is a feature vector ?c for each class
c ? C. Under this interpretation, parameter esti-
mation is maximum a posteriori inference for ?,
and R(?) is a log-prior for the weights. The loss
function is the negative log likelihood for the M
documents: L(?,X, Y ) = ??Mi=1 log p(yi | xi).
The prediction rule for a new document is: y? =
argmaxc?C
?d
j=0 ?c,jxj . Generalized linear mod-
els and penalized regression are well-studied with
an extensive literature (Mccullagh and Nelder, 1989;
Hastie et al, 2009). We leave other types of mod-
els, such as Poisson (Cameron and Trivedi, 1998)
or ordinal (McCullagh, 1980) regression models, to
future work.
3.2 Ridge Regression
With large numbers of features, regularization is
crucial to avoid overfitting. In ridge regression (Ho-
erl and Kennard, 1970), a standard method to which
we compare the time series regularization discussed
in ?3.3, the penalty R(?) is proportional to the `2-
norm of ?:
R(?) = ????2 = ?
?
j ?2j
where ? is a regularization hyperparameter that is
tuned on development data or by cross-validation.4
This penalty pushes many ?j close (but not com-
pletely) to zero. In practice, we multiply the penalty
by the number of examples M to facilitate tuning of
?.
The ridge linear regression model can be inter-
preted probabilistically as each coefficient ?j is
drawn i.i.d. from a normal distribution with mean
0 and variance 2??1.
3.3 Time Series Regularization
A simple way to capture temporal variation is to con-
join traditional features with a time variable. Here,
we divide the dataset into T time steps (years). In the
new representation, the feature space expands from
Rd to RT?d. For a document published at year t, the
elements of x are non-zero only for those features
that correspond to year-t; that is xt?,j = 0 for all
t? 6= t.
4The linear regression has a bias ?0 that is always active.
The logistic regression also has an unpenalized bias ?c,0 for
each class c. This weight is not regularized.
596
Estimating this model with the new features using
the `2-penalty would be effectively estimating sepa-
rate models for each year under the assumption that
each ?t,j is independent; even for features that dif-
fered only temporally (e.g., ?t,j and ?t+1,j).
In this work, we apply time series regularization
to GLMs, enabling models that have coefficients that
change over time but prefer gradual changes across
time steps. Boyd and Vandenberghe (2004, ?6.3) de-
scribe a general version of this sort of regularizer.
To our knowledge, such regularizers have not previ-
ously been applied to temporal modeling of text.
The time series regularization penalty becomes:
R(?) = ?
T?
t=1
d?
j=1
?2t,j+??
T?
t=2
d?
j=0
(?t,j ? ?t?1,j)2
It includes a standard `2-penalty on the coefficients,
and a penalty for differences between coefficients
for adjacent time steps to induce smooth changes.5
Similar to the previous model, in practice, we mul-
tiply the regularization constant ? by MT to facili-tate tuning of ? for datasets with different numbers
of examples M and numbers of time steps T . The
new parameter, ?, controls the smoothness of the es-
timated coefficients. Setting ? to zero imposes no
penalty for time-variation in the coefficients and re-
sults in independent ridge regressions at each time
step. Also, when the number of examples is con-
stant across time steps, setting a large ? parameter
(? ? ?) results in a single ridge regression over all
years since it imposes ?t,j = ?t+1,j for all t ? T .
The partial derivative is:
?R/??t,j = 2??t,j
+ 1{t > 1}2??(?t,j ? ?t?1,j)
+ 1{t < T}2??(?t,j ? ?t+1,j)
This time series regularization can be applied more
generally, not just to linear and logistic regression.
With either ridge regularization or this time se-
ries regularization scheme, Eq. 1 is an unconstrained
convex optimization problem for the linear models
5Our implementation of the time series regularizer does not
penalize the magnitude of the weight for the bias feature (as in
ridge regression). It does, however, penalize the difference in
the bias weight between time steps (as with other features).
?
1
?
2
?
3
?
T
Y
1
Y
2
Y
3
Y
T
...
X
1
X
2
X
3
X
T
?,?
Figure 2: Time series regression as a graphical model;
the variables Xt and Yt are the sets of feature vectors
and response variables from documents dated t.
we describe here. There exist a number of optimiza-
tion procedures for it; we use the L-BFGS quasi-
Newton algorithm (Liu and Nocedal, 1989).
Probabilistic Interpretation
We can interpret the time series regularization prob-
abilistically as follows. Let the coefficient for the
jth feature over time be ?j = ??1,j , ?2,j , ..., ?T,j?.
?j are draws from a multivariate normal distribu-
tion with a tridiagonal precision matrix ??1 = ? ?
RT?T :
? = ?
?
??????
1 + ? ?? 0 0 . . .
?? 1 + 2? ?? 0 . . .
0 ?? 1 + 2? ?? . . .
0 0 ?? 1 + 2? . . .
... ... ... ... . . .
?
??????
The form of R(?) follows from noting:
?2 log p(?j ;?, ?) = ?>j ??j + constant
The squared difference between adjacent time steps
comes from the off-diagonal entries in the preci-
sion matrix.6 Figure 2 shows a graphical represen-
tation of the time series regularization in our model.
Its Markov chain structure corresponds to the off-
diagonals.
There is a rich literature on time series analysis
(Box et al, 2008; Hamilton, 1994). The prior dis-
tribution over the sequence ??1,j , . . . , ?T,j? that our
regularizer posits is closely linked to a first-order au-
toregressive process, AR(1).
6Consistent with the previous section, we assume that pa-
rameters for different features, ?j and ?k, are independent.
597
NBER ACL
Response log(#downloads+1) 1{#citations > 0}
GLM type normal / squared-loss logistic / log-loss
Metric 1 mean absolute error accuracy
Metric 2 Kendall?s ? Kendall?s ?
Table 2: Summary of the setup for the NBER download
and ACL citation prediction experiments.
4 Features
NBER metadata features
? Authors? last names. We treat each name as a bi-
nary feature. If a paper has multiple authors, all
authors are used and they have equal weights re-
gardless of their ordering.
? NBER program(s).7 There are 19 major re-
search programs at the NBER (e.g., Monetary
Economics, Health Economics, etc.).
ACL metadata features
? Authors? last names as binary features.
? Conference venues. We use first letter of the ACL
anthology paper ID, which correlates with its con-
ference venue (e.g., P for the ACL main confer-
ence, H for the HLT conference, etc.).8
Text features
? Binary indicator features for the presence of each
unigram, bigram, and trigram. For the NBER
data, we have separate features for titles and ab-
stracts. For the ACL data, we have separate fea-
tures for titles and full texts. We pruned text fea-
tures by document frequency (details in ?5).
? Log transformed word counts. We include fea-
tures for the numbers of words in the title and the
abstract (NBER) or the full text (ACL).
7Almost all NBER papers are tagged with one or more pro-
grams (we assign untagged papers a ?null? tag). The complete
list of NBER programs can be found at http://www.nber.
org/programs
8Papers in the ACL dataset have a tag which shows which
workshop, conference, or journal they appeared in. However,
sometimes a conference is jointly held with another confer-
ence, such that meta information in the dataset is different even
though the conference is the same. For this reason, we simply
use the first letter of the paper ID.
5 Experiments
For each of the datasets in ?2, we test our models
for two tasks: forecasting about future papers (i.e.,
making predictions about papers that appeared af-
ter a training dataset) and modeling held-out papers
from the past (i.e., making predictions within the
same time period as the training dataset, on held-out
examples).
For the NBER dataset, the task is to predict the
number of downloads a paper will receive in its first
year after publication. For the ACL dataset, the task
is to predict whether a paper will be cited at all (by
another ACL paper in our dataset) within the first
three years after its publication. To our knowledge,
clean, reliable citation counts are not available for
the NBER dataset; nor are download statistics avail-
able for the ACL dataset. Table 2 summarizes the
variables of interest, model types, and evaluation
metrics for the tasks.
5.1 Extrapolation
The lag between a paper?s publication and when its
outcome (download or citation count) can be ob-
served poses a unique methodological challenge.
Consider predicting the number of downloads over
g future time steps. If t is the time of forecasting,
we can observe the texts of all articles published be-
fore t. However, any article published in the interval
[t ? g, t] is too recent for the outcome measurement
of y to be taken. We refer to the interval [t? g, t] as
the ?forecast gap?. Since recent articles are some-
times the most relevant predictions at t, we do not
want to ignore them. Consider a paper at time step
t?, t?g < t? < t. To extrapolate its number of down-
loads, we consider the observed number in [t?, t], and
then estimate the ratio r of downloads that occur in
the first t?t? time steps, against the first g time steps,
using the fully observed portion of the training data.
We then scale the observed downloads during [t?, t]
by r?1 to extrapolate. The same method is used to
extrapolate citation counts.
In preliminary experiments, we observed that ex-
trapolating responses for papers in the forecast gap
led to better performance in general. For example,
for the ridge regressions trained on all past years
with the full feature set, the error dropped from 262
to 259 when using extrapolation compared to with-
598
out extrapolation. Also, the extrapolated download
counts were quite close to the true values (which we
have but do not use because of the forecast gap): for
example, the mean absolute error of the extrapolated
responses was 99 when extrapolated based on the
median of the fully observed portion of the training
data (measured monthly).
5.2 Forecasting NBER Downloads
In our first set of experiments, we predict the number
of downloads of an NBER paper within one year of
its publication.
We compare four approaches for predicting
downloads. The first is a baseline that simply uses
the median of the log of the training and develop-
ment data as the prediction. The second and third
use GLMs with ridge regression-style regularization
(?3.2), trained on all past years (?all years?) and on
the single most recent past year (?one year?), respec-
tively. The last model (?time series?) is a GLM with
time series regularization (?3.3).
We divided papers by year. Figure 3 illustrates the
experimental setup. We held out a random 20% of
papers for each year from 1999?2007 as a test set for
the task of modeling the past. To define the feature
set and tune hyperparameters, we used the remain-
ing 80% of papers from 1999?2005 as our training
data and the remaining papers in 2006 as our devel-
opment data. After pruning,9 we have 37,251 to-
tal features, of which 2,549 are metadata features.
When tuning hyperparameters, we simulated the ex-
istence of a forecast gap by using extrapolated re-
sponses for papers in the last year of the training
data instead of their true responses. We considered
? ? 5{2,1,...,?5,?6}, and ? ? 5{3,2,...,?1,?2} and se-
lected those that led to the best performance on the
development set.
We then used the selected feature set and hyperpa-
rameters to test the forecasting and modeling capa-
bilities of each model. For forecasting, we predicted
numbers of downloads of papers in 2008 and 2009.
We used the baseline median, ridge regression, and
time series regularization models trained on papers
in 1999?2007 and 1999?2008, respectively. We
treated the last year of the training data (2007 and
9For NBER, text features appearing in less than 0.1% or
more than 99.9% of the training documents were removed. For
ACL, the thresholds were 2% and 98%.
training
modeling test (unused)
gap dev.
trr taitan tag
training gap test
trr tai tam tao
ddd
ddd
modeling test (unused)
oae
lae
oae
lae
training gap test
trr tamddd
modeling test
oae
lae
tao tar
NBER Experiments
ACL Experiments
training
modeling test (unused)
dev.
toa tro ta ddd
oae
lae
s(u)p)v.(uu)p)vProceedings of NAACL-HLT 2013, pages 380?390,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improved Part-of-Speech Tagging for Online Conversational Text
with Word Clusters
Olutobi Owoputi? Brendan O?Connor? Chris Dyer?
Kevin Gimpel? Nathan Schneider? Noah A. Smith?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Toyota Technological Institute at Chicago, Chicago, IL 60637, USA
Corresponding author: brenocon@cs.cmu.edu
Abstract
We consider the problem of part-of-speech
tagging for informal, online conversational
text. We systematically evaluate the use of
large-scale unsupervised word clustering
and new lexical features to improve tagging
accuracy. With these features, our system
achieves state-of-the-art tagging results on
both Twitter and IRC POS tagging tasks;
Twitter tagging is improved from 90% to 93%
accuracy (more than 3% absolute). Quali-
tative analysis of these word clusters yields
insights about NLP and linguistic phenomena
in this genre. Additionally, we contribute the
first POS annotation guidelines for such text
and release a new dataset of English language
tweets annotated using these guidelines.
Tagging software, annotation guidelines, and
large-scale word clusters are available at:
http://www.ark.cs.cmu.edu/TweetNLP
This paper describes release 0.3 of the ?CMU
Twitter Part-of-Speech Tagger? and annotated
data.
1 Introduction
Online conversational text, typified by microblogs,
chat, and text messages,1 is a challenge for natu-
ral language processing. Unlike the highly edited
genres that conventional NLP tools have been de-
veloped for, conversational text contains many non-
standard lexical items and syntactic patterns. These
are the result of unintentional errors, dialectal varia-
tion, conversational ellipsis, topic diversity, and cre-
ative use of language and orthography (Eisenstein,
2013). An example is shown in Fig. 1. As a re-
sult of this widespread variation, standard model-
ing assumptions that depend on lexical, syntactic,
and orthographic regularity are inappropriate. There
1Also referred to as computer-mediated communication.
ikr
!
smh
G
he
O
asked
V
fir
P
yo
D
last
A
name
N
so
P
he
O
can
V
add
V
u
O
on
P
fb
?
lololol
!
Figure 1: Automatically tagged tweet showing nonstan-
dard orthography, capitalization, and abbreviation. Ignor-
ing the interjections and abbreviations, it glosses as He
asked for your last name so he can add you on Facebook.
The tagset is defined in Appendix A. Refer to Fig. 2 for
word clusters corresponding to some of these words.
is preliminary work on social media part-of-speech
(POS) tagging (Gimpel et al, 2011), named entity
recognition (Ritter et al, 2011; Liu et al, 2011), and
parsing (Foster et al, 2011), but accuracy rates are
still significantly lower than traditional well-edited
genres like newswire. Even web text parsing, which
is a comparatively easier genre than social media,
lags behind newspaper text (Petrov and McDonald,
2012), as does speech transcript parsing (McClosky
et al, 2010).
To tackle the challenge of novel words and con-
structions, we create a new Twitter part-of-speech
tagger?building on previous work by Gimpel et
al. (2011)?that includes new large-scale distribu-
tional features. This leads to state-of-the-art results
in POS tagging for both Twitter and Internet Relay
Chat (IRC) text. We also annotated a new dataset of
tweets with POS tags, improved the annotations in
the previous dataset from Gimpel et al, and devel-
oped annotation guidelines for manual POS tagging
of tweets. We release all of these resources to the
research community:
? an open-source part-of-speech tagger for online
conversational text (?2);
? unsupervised Twitter word clusters (?3);
? an improved emoticon detector for conversational
text (?4);
380
? POS annotation guidelines (?5.1); and
? a new dataset of 547 manually POS-annotated
tweets (?5).
2 MEMM Tagger
Our tagging model is a first-order maximum en-
tropy Markov model (MEMM), a discriminative se-
quence model for which training and decoding are
extremely efficient (Ratnaparkhi, 1996; McCallum
et al, 2000).2 The probability of a tag yt is condi-
tioned on the input sequence x and the tag to its left
yt?1, and is parameterized by a multiclass logistic
regression:
p(yt = k | yt?1,x, t;?) ?
exp
(
?(trans)yt?1,k +
?
j ?
(obs)
j,k fj(x, t)
)
We use transition features for every pair of labels,
and extract base observation features from token t
and neighboring tokens, and conjoin them against
all K = 25 possible outputs in our coarse tagset
(Appendix A). Our feature sets will be discussed
below in detail.
Decoding. For experiments reported in this paper,
we use the O(|x|K2) Viterbi algorithm for predic-
tion; K is the number of tags. This exactly max-
imizes p(y | x), but the MEMM also naturally al-
lows a fasterO(|x|K) left-to-right greedy decoding:
for t = 1 . . . |x|:
y?t ? argmaxk p(yt = k | y?t?1,x, t;?)
which we find is 3 times faster and yields similar ac-
curacy as Viterbi (an insignificant accuracy decrease
of less than 0.1% absolute on the DAILY547 test set
discussed below). Speed is paramount for social me-
dia analysis applications?which often require the
processing of millions to billions of messages?so
we make greedy decoding the default in the released
software.
2Although when compared to CRFs, MEMMs theoretically
suffer from the ?label bias? problem (Lafferty et al, 2001), our
system substantially outperforms the CRF-based taggers of pre-
vious work; and when comparing to Gimpel et al system with
similar feature sets, we observed little difference in accuracy.
This is consistent with conventional wisdom that the quality
of lexical features is much more important than the paramet-
ric form of the sequence model, at least in our setting: part-of-
speech tagging with a small labeled training set.
This greedy tagger runs at 800 tweets/sec. (10,000
tokens/sec.) on a single CPU core, about 40 times
faster than Gimpel et al?s system. The tokenizer by
itself (?4) runs at 3,500 tweets/sec.3
Training and regularization. During training,
the MEMM log-likelihood for a tagged tweet ?x,y?
is the sum over the observed token tags yt, each con-
ditional on the tweet being tagged and the observed
previous tag (with a start symbol before the first to-
ken in x),
`(x,y,?) =
?|x|
t=1 log p(yt | yt?1,x, t;?).
We optimize the parameters ? with OWL-QN, an
L1-capable variant of L-BFGS (Andrew and Gao,
2007; Liu and Nocedal, 1989) to minimize the regu-
larized objective
argmin
?
? 1N
?
?x,y? `(x,y,?) +R(?)
where N is the number of tokens in the corpus and
the sum ranges over all tagged tweets ?x,y? in the
training data. We use elastic net regularization (Zou
and Hastie, 2005), which is a linear combination of
L1 and L2 penalties; here j indexes over all features:
R(?) = ?1
?
j |?j |+
1
2?2
?
j ?
2
j
Using even a very small L1 penalty eliminates many
irrelevant or noisy features.
3 Unsupervised Word Clusters
Our POS tagger can make use of any number of pos-
sibly overlapping features. While we have only a
small amount of hand-labeled data for training, we
also have access to billions of tokens of unlabeled
conversational text from the web. Previous work has
shown that unlabeled text can be used to induce un-
supervised word clusters which can improve the per-
formance of many supervised NLP tasks (Koo et al,
2008; Turian et al, 2010; T?ckstr?m et al, 2012, in-
ter alia). We use a similar approach here to improve
tagging performance for online conversational text.
We also make our induced clusters publicly avail-
able in the hope that they will be useful for other
NLP tasks in this genre.
3Runtimes observed on an Intel Core i5 2.4 GHz laptop.
381
Binary path Top words (by frequency)
A1 111010100010 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol
A2 111010100011 haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah
A3 111010100100 yes yep yup nope yess yesss yessss ofcourse yeap likewise yepp yesh yw yuup yus
A4 111010100101 yeah yea nah naw yeahh nooo yeh noo noooo yeaa ikr nvm yeahhh nahh nooooo
A5 11101011011100 smh jk #fail #random #fact smfh #smh #winning #realtalk smdh #dead #justsaying
B 011101011 u yu yuh yhu uu yuu yew y0u yuhh youh yhuu iget yoy yooh yuo yue juu dya youz yyou
C 11100101111001 w fo fa fr fro ov fer fir whit abou aft serie fore fah fuh w/her w/that fron isn agains
D 111101011000 facebook fb itunes myspace skype ebay tumblr bbm flickr aim msn netflix pandora
E1 0011001 tryna gon finna bouta trynna boutta gne fina gonn tryina fenna qone trynaa qon
E2 0011000 gonna gunna gona gna guna gnna ganna qonna gonnna gana qunna gonne goona
F 0110110111 soo sooo soooo sooooo soooooo sooooooo soooooooo sooooooooo soooooooooo
G1 11101011001010 ;) :p :-) xd ;-) ;d (; :3 ;p =p :-p =)) ;] xdd #gno xddd >:) ;-p >:d 8-) ;-d
G2 11101011001011 :) (: =) :)) :] :?) =] ^_^ :))) ^.^ [: ;)) ((: ^__^ (= ^-^ :))))
G3 1110101100111 :( :/ -_- -.- :-( :?( d: :| :s -__- =( =/ >.< -___- :-/ </3 :\ -____- ;( /: :(( >_< =[ :[ #fml
G4 111010110001 <3 xoxo <33 xo <333 #love s2 <URL-twitition.com> #neversaynever <3333
Figure 2: Example word clusters (HMM classes): we list the most probable words, starting with the most probable, in
descending order. Boldfaced words appear in the example tweet (Figure 1). The binary strings are root-to-leaf paths
through the binary cluster tree. For example usage, see e.g. search.twitter.com, bing.com/social and
urbandictionary.com.
3.1 Clustering Method
We obtained hierarchical word clusters via Brown
clustering (Brown et al, 1992) on a large set of
unlabeled tweets.4 The algorithm partitions words
into a base set of 1,000 clusters, and induces a hi-
erarchy among those 1,000 clusters with a series of
greedy agglomerative merges that heuristically opti-
mize the likelihood of a hidden Markov model with a
one-class-per-lexical-type constraint. Not only does
Brown clustering produce effective features for dis-
criminative models, but its variants are better unsu-
pervised POS taggers than some models developed
nearly 20 years later; see comparisons in Blunsom
and Cohn (2011). The algorithm is attractive for our
purposes since it scales to large amounts of data.
When training on tweets drawn from a single
day, we observed time-specific biases (e.g., nu-
merical dates appearing in the same cluster as the
word tonight), so we assembled our unlabeled data
from a random sample of 100,000 tweets per day
from September 10, 2008 to August 14, 2012,
and filtered out non-English tweets (about 60% of
the sample) using langid.py (Lui and Baldwin,
2012).5 Each tweet was processed with our to-
4As implemented by Liang (2005), v. 1.3: https://
github.com/percyliang/brown-cluster
5https://github.com/saffsd/langid.py
kenizer and lowercased. We normalized all at-
mentions to ?@MENTION? and URLs/email ad-
dresses to their domains (e.g. http://bit.ly/
dP8rR8 ? ?URL-bit.ly?). In an effort to reduce
spam, we removed duplicated tweet texts (this also
removes retweets) before word clustering. This
normalization and cleaning resulted in 56 million
unique tweets (847 million tokens). We set the
clustering software?s count threshold to only cluster
words appearing 40 or more times, yielding 216,856
word types, which took 42 hours to cluster on a sin-
gle CPU.
3.2 Cluster Examples
Fig. 2 shows example clusters. Some of the chal-
lenging words in the example tweet (Fig. 1) are high-
lighted. The term lololol (an extension of lol for
?laughing out loud?) is grouped with a large number
of laughter acronyms (A1: ?laughing my (fucking)
ass off,? ?cracking the fuck up?). Since expressions
of laughter are so prevalent on Twitter, the algorithm
creates another laughter cluster (A1?s sibling A2),
that tends to have onomatopoeic, non-acronym vari-
ants (e.g., haha). The acronym ikr (?I know, right??)
is grouped with expressive variations of ?yes? and
?no? (A4). Note that A1?A4 are grouped in a fairly
specific subtree; and indeed, in this message ikr and
382
lololol are both tagged as interjections.
smh (?shaking my head,? indicating disapproval)
seems related, though is always tagged in the an-
notated data as a miscellaneous abbreviation (G);
the difference between acronyms that are interjec-
tions versus other acronyms may be complicated.
Here, smh is in a related but distinct subtree from the
above expressions (A5); its usage in this example
is slightly different from its more common usage,
which it shares with the other words in its cluster:
message-ending expressions of commentary or emo-
tional reaction, sometimes as a metacomment on the
author?s message; e.g., Maybe you could get a guy
to date you if you actually respected yourself #smh
or There is really NO reason why other girls should
send my boyfriend a goodmorning text #justsaying.
We observe many variants of categories tradition-
ally considered closed-class, including pronouns (B:
u = ?you?) and prepositions (C: fir = ?for?).
There is also evidence of grammatical categories
specific to conversational genres of English; clusters
E1?E2 demonstrate variations of single-word con-
tractions for ?going to? and ?trying to,? some of
which have more complicated semantics.6
Finally, the HMM learns about orthographic vari-
ants, even though it treats all words as opaque sym-
bols; cluster F consists almost entirely of variants
of ?so,? their frequencies monotonically decreasing
in the number of vowel repetitions?a phenomenon
called ?expressive lengthening? or ?affective length-
ening? (Brody and Diakopoulos, 2011; Schnoebe-
len, 2012). This suggests a future direction to jointly
model class sequence and orthographic informa-
tion (Clark, 2003; Smith and Eisner, 2005; Blunsom
and Cohn, 2011).
We have built an HTML viewer to browse these
and numerous other interesting examples.7
3.3 Emoticons and Emoji
We use the term emoticon to mean a face or icon
constructed with traditional alphabetic or punctua-
6One coauthor, a native speaker of the Texan English dialect,
notes ?finna? (short for ?fixing to?, cluster E1) may be an im-
mediate future auxiliary, indicating an immediate future tense
that is present in many languages (though not in standard En-
glish). To illustrate: ?She finna go? approximately means ?She
will go,? but sooner, in the sense of ?She is about to go.?
7http://www.ark.cs.cmu.edu/TweetNLP/
cluster_viewer.html
tion symbols, and emoji to mean symbols rendered
in software as small pictures, in line with the text.
Since our tokenizer is careful to preserve emoti-
cons and other symbols (see ?4), they are clustered
just like other words. Similar emoticons are clus-
tered together (G1?G4), including separate clusters
of happy [[ :) =) ?_? ]], sad/disappointed [[ :/ :(
-_- </3 ]], love [[ ?xoxo ?.? ]] and winking [[
;) (?_-) ]] emoticons. The clusters are not per-
fectly aligned with our POS annotation guidelines;
for example, the ?sad? emoticon cluster included
emotion-bearing terms that our guidelines define as
non-emoticons, such as #ugh, #tear, and #fml (?fuck
my life?), though these seem potentially useful for
sentiment analysis.
One difficult task is classifying different types
of symbols in tweets: our annotation guidelines
differentiate between emoticons, punctuation, and
garbage (apparently non-meaningful symbols or to-
kenization errors). Several Unicode character ranges
are reserved for emoji-style symbols (including the
three Unicode hearts in G4); however, depending
on the user?s software, characters in these ranges
might be rendered differently or not at all. We
have found instances where the clustering algo-
rithm groups proprietary iOS emoji symbols along
with normal emoticons; for example, the character
U+E056, which is interpreted on iOS as a smiling
face, is in the same G2 cluster as smiley face emoti-
cons. The symbol U+E12F, which represents a pic-
ture of a bag of money, is grouped with the words
cash and money.
3.4 Cluster-Based Features
Since Brown clusters are hierarchical in a binary
tree, each word is associated with a tree path rep-
resented as a bitstring with length ? 16; we use pre-
fixes of the bitstring as features (for all prefix lengths
? {2, 4, 6, . . . , 16}). This allows sharing of statisti-
cal strength between similar clusters. Using prefix
features of hierarchical clusters in this way was sim-
ilarly found to be effective for named-entity recog-
nition (Turian et al, 2010) and Twitter POS tag-
ging (Ritter et al, 2011).
When checking to see if a word is associated with
a cluster, the tagger first normalizes the word using
the same techniques as described in ?3.1, then cre-
ates a priority list of fuzzy match transformations
383
of the word by removing repeated punctuation and
repeated characters. If the normalized word is not
in a cluster, the tagger considers the fuzzy matches.
Although only about 3% of the tokens in the devel-
opment set (?6) did not appear in a clustering, this
method resulted in a relative error decrease of 18%
among such word tokens.
3.5 Other Lexical Features
Besides unsupervised word clusters, there are two
other sets of features that contain generalized lexi-
cal class information. We use the tag dictionary fea-
ture from Gimpel et al, which adds a feature for
a word?s most frequent part-of-speech tag.8 This
can be viewed as a feature-based domain adaptation
method, since it gives lexical type-level information
for standard English words, which the model learns
to map between PTB tags to the desired output tags.
Second, since the lack of consistent capitalization
conventions on Twitter makes it especially difficult
to recognize names?Gimpel et al and Foster et
al. (2011) found relatively low accuracy on proper
nouns?we added a token-level name list feature,
which fires on (non-function) words from names
from several sources: Freebase lists of celebrities
and video games (Google, 2012), the Moby Words
list of US Locations,9 and lists of male, female, fam-
ily, and proper names from Mark Kantrowitz?s name
corpus.10
4 Tokenization and Emoticon Detection
Word segmentation on Twitter is challenging due
to the lack of orthographic conventions; in partic-
ular, punctuation, emoticons, URLs, and other sym-
bols may have no whitespace separation from textual
8Frequencies came from the Wall Street Journal and Brown
corpus sections of the Penn Treebank. If a word has multiple
PTB tags, each tag is a feature with value for the frequency rank;
e.g. for three different tags in the PTB, this feature gives a value
of 1 for the most frequent tag, 2/3 for the second, etc. Coarse
versions of the PTB tags are used (Petrov et al, 2011). While
88% of words in the dictionary have only one tag, using rank
information seemed to give a small but consistent gain over only
using the most common tag, or using binary features conjoined
with rank as in Gimpel et al
9http://icon.shef.ac.uk/Moby/mwords.html
10http://www.cs.cmu.edu/afs/cs/project/
ai-repository/ai/areas/nlp/corpora/names/
0.html
words (e.g. no:-d,yes should parse as four tokens),
and internally may contain alphanumeric symbols
that could be mistaken for words: a naive split(/[^a-
zA-Z0-9]+/) tokenizer thinks the words ?p? and ?d?
are among the top 100 most common words on Twit-
ter, due to misanalysis of :p and :d. Traditional Penn
Treebank?style tokenizers are hardly better, often
breaking a string of punctuation characters into a
single token per character.
We rewrote twokenize (O?Connor et al,
2010), a rule-based tokenizer, emoticon, and URL
detector, for use in the tagger. Emoticons are es-
pecially challenging, since they are open-class and
productive. We revise O?Connor et al?s regular ex-
pression grammar that describes possible emoticons,
adding a grammar of horizontal emoticons (e.g. -_-),
known as ?Eastern-style,?11 though we observe high
usage in English-speaking Twitter (Fig. 2, G2?G3).
We also add a number of other improvements to the
patterns. Because this system was used as prepro-
cessing for the word clustering experiment in ?3, we
were able to infer the emoticon clusters in Fig. 2.
Furthermore, whether a token matches the emoticon
pattern is also used as a feature in the tagger (?2).
URL recognition is also difficult, since the http://
is often dropped, resulting in protocol-less URLs
like about.me. We add recognition patterns for these
by using a list of top-level and country domains.
5 Annotated Dataset
Gimpel et al (2011) provided a dataset of POS-
tagged tweets consisting almost entirely of tweets
sampled from one particular day (October 27,
2010). We were concerned about overfitting to time-
specific phenomena; for example, a substantial frac-
tion of the messages are about a basketball game
happening that day.
We created a new test set of 547 tweets for eval-
uation. The test set consists of one random English
tweet from every day between January 1, 2011 and
June 30, 2012. In order for a tweet to be considered
English, it had to contain at least one English word
other than a URL, emoticon, or at-mention. We no-
ticed biases in the outputs of langid.py, so we
instead selected these messages completely manu-
11http://en.wikipedia.org/wiki/List_of_
emoticons
384
ally (going through a random sample of one day?s
messages until an English message was found).
5.1 Annotation Methodology
Gimpel et al provided a tagset for Twitter (shown in
Appendix A), which we used unmodified. The orig-
inal annotation guidelines were not published, but in
this work we recorded the rules governing tagging
decisions and made further revisions while annotat-
ing the new data.12 Some of our guidelines reiter-
ate or modify rules made by Penn Treebank annota-
tors, while others treat specific phenomena found on
Twitter (refer to the next section).
Our tweets were annotated by two annotators who
attempted to match the choices made in Gimpel et
al.?s dataset. The annotators also consulted the POS
annotations in the Penn Treebank (Marcus et al,
1993) as an additional reference. Differences were
reconciled by a third annotator in discussion with all
annotators.13 During this process, an inconsistency
was found in Gimpel et al?s data, which we cor-
rected (concerning the tagging of this/that, a change
to 100 labels, 0.4%). The new version of Gimpel et
al.?s data (called OCT27), as well as the newer mes-
sages (called DAILY547), are both included in our
data release.
5.2 Compounds in Penn Treebank vs. Twitter
Ritter et al (2011) annotated tweets using an aug-
mented version of the PTB tagset and presumably
followed the PTB annotation guidelines. We wrote
new guidelines because the PTB conventions are in-
appropriate for Twitter in several ways, as shown in
the design of Gimpel et al?s tagset. Importantly,
?compound? tags (e.g., nominal+verbal and nomi-
nal+possessive) are used because tokenization is dif-
ficult or seemingly impossible for the nonstandard
word forms that are commonplace in conversational
text.
For example, the PTB tokenization splits contrac-
tions containing apostrophes: I?m? I/PRP ?m/VBP.
But conversational text often contains variants that
resist a single PTB tag (like im), or even chal-
lenge traditional English grammatical categories
12The annotation guidelines are available online at
http://www.ark.cs.cmu.edu/TweetNLP/
13Annotators are coauthors of this paper.
(like imma or umma, which both mean ?I am go-
ing to?). One strategy would be to analyze these
forms into a PTB-style tokenization, as discussed in
Forsyth (2007), who proposes to analyze doncha as
do/VBP ncha/PRP, but notes it would be difficult.
We think this is impossible to handle in the rule-
based framework used by English tokenizers, given
the huge (and possibly growing) number of large
compounds like imma, gonna, w/that, etc. These
are not rare: the word clustering algorithm discov-
ers hundreds of such words as statistically coherent
classes (e.g. clusters E1 and E2 in Fig. 2); and the
word imma is the 962nd most common word in our
unlabeled corpus, more frequent than cat or near.
We do not attempt to do Twitter ?normalization?
into traditional written English (Han and Baldwin,
2011), which we view as a lossy translation task. In
fact, many of Twitter?s unique linguistic phenomena
are due not only to its informal nature, but also a set
of authors that heavily skews towards younger ages
and minorities, with heavy usage of dialects that are
different than the standard American English most
often seen in NLP datasets (Eisenstein, 2013; Eisen-
stein et al, 2011). For example, we suspect that
imma may implicate tense and aspect markers from
African-American Vernacular English.14 Trying to
impose PTB-style tokenization on Twitter is linguis-
tically inappropriate: should the lexico-syntactic be-
havior of casual conversational chatter by young mi-
norities be straightjacketed into the stylistic conven-
tions of the 1980s Wall Street Journal? Instead, we
would like to directly analyze the syntax of online
conversational text on its own terms.
Thus, we choose to leave these word forms un-
tokenized and use compound tags, viewing com-
positional multiword analysis as challenging fu-
ture work.15 We believe that our strategy is suf-
ficient for many applications, such as chunking or
named entity recognition; many applications such
as sentiment analysis (Turney, 2002; Pang and Lee,
2008, ?4.2.3), open information extraction (Carl-
son et al, 2010; Fader et al, 2011), and informa-
tion retrieval (Allan and Raghavan, 2002) use POS
14See ?Tense and aspect? examples in http:
//en.wikipedia.org/wiki/African_American_
Vernacular_English
15For example, wtf has compositional behavior in ?Wtf just
happened???, but only debatably so in ?Huh wtf?.
385
#Msg. #Tok. Tagset Dates
OCT27 1,827 26,594 App. A Oct 27-28, 2010
DAILY547 547 7,707 App. A Jan 2011?Jun 2012
NPSCHAT 10,578 44,997 PTB-like Oct?Nov 2006
(w/o sys. msg.) 7,935 37,081
RITTERTW 789 15,185 PTB-like unknown
Table 1: Annotated datasets: number of messages, to-
kens, tagset, and date range. More information in ?5,
?6.3, and ?6.2.
patterns that seem quite compatible with our ap-
proach. More complex downstream processing like
parsing is an interesting challenge, since contraction
parsing on traditional text is probably a benefit to
current parsers. We believe that any PTB-trained
tool requires substantial retraining and adaptation
for Twitter due to the huge genre and stylistic differ-
ences (Foster et al, 2011); thus tokenization conven-
tions are a relatively minor concern. Our simple-to-
annotate conventions make it easier to produce new
training data.
6 Experiments
We are primarily concerned with performance on
our annotated datasets described in ?5 (OCT27,
DAILY547), though for comparison to previous
work we also test on other corpora (RITTERTW in
?6.2, NPSCHAT in ?6.3). The annotated datasets
are listed in Table 1.
6.1 Main Experiments
We use OCT27 to refer to the entire dataset de-
scribed in Gimpel et al; it is split into train-
ing, development, and test portions (OCT27TRAIN,
OCT27DEV, OCT27TEST). We use DAILY547 as
an additional test set. Neither OCT27TEST nor
DAILY547 were extensively evaluated against until
final ablation testing when writing this paper.
The total number of features is 3.7 million, all
of which are used under pure L2 regularization; but
only 60,000 are selected by elastic net regularization
with (?1, ?2) = (0.25, 2), which achieves nearly the
same (but no better) accuracy as pure L2,16 and we
use it for all experiments. We observed that it was
16We conducted a grid search for the regularizer values on
part of DAILY547, and many regularizer values give the best or
nearly the best results. We suspect a different setup would have
yielded similar results.
l
l
l
l l l l
1e+03 1e+05 1e+07
75
80
85
90
Number of Unlabeled Tweets
Ta
gg
ing
 Ac
cu
rac
y
l
l
l l
l l l
1e+03 1e+05 1e+07
0.6
0
0.6
5
0.7
0
Number of Unlabeled Tweets
To
ke
n 
Co
ve
ra
ge
Figure 3: OCT27 development set accuracy using only
clusters as features.
Model In dict. Out of dict.
Full 93.4 85.0
No clusters 92.0 (?1.4) 79.3 (?5.7)
Total tokens 4,808 1,394
Table 3: DAILY547 accuracies (%) for tokens in and out
of a traditional dictionary, for models reported in rows 1
and 3 of Table 2.
possible to get radically smaller models with only
a slight degradation in performance: (4, 0.06) has
0.5% worse accuracy but uses only 1,632 features, a
small enough number to browse through manually.
First, we evaluate on the new test set, training on
all of OCT27. Due to DAILY547?s statistical repre-
sentativeness, we believe this gives the best view of
the tagger?s accuracy on English Twitter text. The
full tagger attains 93.2% accuracy (final row of Ta-
ble 2).
To facilitate comparisons with previous work, we
ran a series of experiments training only on OCT27?s
training and development sets, then report test re-
sults on both OCT27TEST and all of DAILY547,
shown in Table 2. Our tagger achieves substantially
higher accuracy than Gimpel et al (2011).17
Feature ablation. A number of ablation tests in-
dicate the word clusters are a very strong source of
lexical knowledge. When dropping the tag dictio-
naries and name lists, the word clusters maintain
most of the accuracy (row 2). If we drop the clus-
ters and rely only on tag dictionaries and namelists,
accuracy decreases significantly (row 3). In fact,
we can remove all observation features except for
word clusters?no word features, orthographic fea-
17These numbers differ slightly from those reported by Gim-
pel et al, due to the corrections we made to the OCT27 data,
noted in Section 5.1. We retrained and evaluated their tagger
(version 0.2) on our corrected dataset.
386
Feature set OCT27TEST DAILY547 NPSCHATTEST
All features 91.60 92.80 91.19 1
with clusters; without tagdicts, namelists 91.15 92.38 90.66 2
without clusters; with tagdicts, namelists 89.81 90.81 90.00 3
only clusters (and transitions) 89.50 90.54 89.55 4
without clusters, tagdicts, namelists 86.86 88.30 88.26 5
Gimpel et al (2011) version 0.2 88.89 89.17 6
Inter-annotator agreement (Gimpel et al, 2011) 92.2 7
Model trained on all OCT27 93.2 8
Table 2: Tagging accuracies (%) in ablation experiments. OCT27TEST and DAILY547 95% confidence intervals are
roughly ?0.7%. Our final tagger uses all features and also trains on OCT27TEST, achieving 93.2% on DAILY547.
tures, affix n-grams, capitalization, emoticon pat-
terns, etc.?and the accuracy is in fact still better
than the previous work (row 4).18
We also wanted to know whether to keep the tag
dictionary and name list features, but the splits re-
ported in Fig. 2 did not show statistically signifi-
cant differences; so to better discriminate between
ablations, we created a lopsided train/test split of
all data with a much larger test portion (26,974 to-
kens), having greater statistical power (tighter con-
fidence intervals of ? 0.3%).19 The full system got
90.8% while the no?tag dictionary, no-namelists ab-
lation had 90.0%, a statistically significant differ-
ence. Therefore we retain these features.
Compared to the tagger in Gimpel et al, most of
our feature changes are in the new lexical features
described in ?3.5.20 We do not reuse the other lex-
ical features from the previous work, including a
phonetic normalizer (Metaphone), a name list con-
sisting of words that are frequently capitalized, and
distributional features trained on a much smaller un-
labeled corpus; they are all worse than our new
lexical features described here. (We did include,
however, a variant of the tag dictionary feature that
uses phonetic normalization for lookup; it seemed to
yield a small improvement.)
18Furthermore, when evaluating the clusters as unsupervised
(hard) POS tags, we obtain a many-to-one accuracy of 89.2%
on DAILY547. Before computing this, we lowercased the text
to match the clusters and removed tokens tagged as URLs and
at-mentions.
19Reported confidence intervals in this paper are 95% bino-
mial normal approximation intervals for the proportion of cor-
rectly tagged tokens: ?1.96
?
p(1? p)/ntokens . 1/
?
n.
20Details on the exact feature set are available in a technical
report (Owoputi et al, 2012), also available on the website.
Non-traditional words. The word clusters are es-
pecially helpful with words that do not appear in tra-
ditional dictionaries. We constructed a dictionary
by lowercasing the union of the ispell ?American?,
?British?, and ?English? dictionaries, plus the stan-
dard Unix words file from Webster?s Second Inter-
national dictionary, totalling 260,985 word types.
After excluding tokens defined by the gold stan-
dard as punctuation, URLs, at-mentions, or emoti-
cons,21 22% of DAILY547?s tokens do not appear in
this dictionary. Without clusters, they are very dif-
ficult to classify (only 79.2% accuracy), but adding
clusters generates a 5.7 point improvement?much
larger than the effect on in-dictionary tokens (Ta-
ble 3).
Varying the amount of unlabeled data. A tagger
that only uses word clusters achieves an accuracy of
88.6% on the OCT27 development set.22 We created
several clusterings with different numbers of unla-
beled tweets, keeping the number of clusters con-
stant at 800. As shown in Fig. 3, there was initially
a logarithmic relationship between number of tweets
and accuracy, but accuracy (and lexical coverage)
levels out after 750,000 tweets. We use the largest
clustering (56 million tweets and 1,000 clusters) as
the default for the released tagger.
6.2 Evaluation on RITTERTW
Ritter et al (2011) annotated a corpus of 787
tweets23 with a single annotator, using the PTB
21We retain hashtags since by our guidelines a #-prefixed to-
ken is ambiguous between a hashtag and a normal word, e.g. #1
or going #home.
22The only observation features are the word clusters of a
token and its immediate neighbors.
23https://github.com/aritter/twitter_nlp/
blob/master/data/annotated/pos.txt
387
Tagger Accuracy
This work 90.0 ? 0.5
Ritter et al (2011), basic CRF tagger 85.3
Ritter et al (2011), trained on more data 88.3
Table 4: Accuracy comparison on Ritter et al?s Twitter
POS corpus (?6.2).
Tagger Accuracy
This work 93.4 ? 0.3
Forsyth (2007) 90.8
Table 5: Accuracy comparison on Forsyth?s NPSCHAT
IRC POS corpus (?6.3).
tagset plus several Twitter-specific tags, referred
to in Table 1 as RITTERTW. Linguistic concerns
notwithstanding (?5.2), for a controlled comparison,
we train and test our system on this data with the
same 4-fold cross-validation setup they used, attain-
ing 90.0% (?0.5%) accuracy. Ritter et al?s CRF-
based tagger had 85.3% accuracy, and their best tag-
ger, trained on a concatenation of PTB, IRC, and
Twitter, achieved 88.3% (Table 4).
6.3 IRC: Evaluation on NPSCHAT
IRC is another medium of online conversational
text, with similar emoticons, misspellings, abbrevi-
ations and acronyms as Twitter data. We evaluate
our tagger on the NPS Chat Corpus (Forsyth and
Martell, 2007),24 a PTB-part-of-speech annotated
dataset of Internet Relay Chat (IRC) room messages
from 2006.
First, we compare to a tagger in the same setup as
experiments on this data in Forsyth (2007), training
on 90% of the data and testing on 10%; we average
results across 10-fold cross-validation.25 The full
tagger model achieved 93.4% (?0.3%) accuracy,
significantly improving over the best result they re-
port, 90.8% accuracy with a tagger trained on a mix
of several POS-annotated corpora.
We also perform the ablation experiments on this
corpus, with a slightly different experimental setup:
we first filter out system messages then split data
24Release 1.0: http://faculty.nps.edu/
cmartell/NPSChat.htm
25Forsyth actually used 30 different 90/10 random splits; we
prefer cross-validation because the same test data is never re-
peated, thus allowing straightforward confidence estimation of
accuracy from the number of tokens (via binomial sample vari-
ance, footnote 19). In all cases, the models are trained on the
same amount of data (90%).
into 5,067 training and 2,868 test messages. Results
show a similar pattern as the Twitter data (see final
column of Table 2). Thus the Twitter word clusters
are also useful for language in the medium of text
chat rooms; we suspect these clusters will be appli-
cable for deeper syntactic and semantic analysis in
other online conversational text mediums, such as
text messages and instant messages.
7 Conclusion
We have constructed a state-of-the-art part-of-
speech tagger for the online conversational text
genres of Twitter and IRC, and have publicly re-
leased our new evaluation data, annotation guide-
lines, open-source tagger, and word clusters at
http://www.ark.cs.cmu.edu/TweetNLP.
Acknowledgements
This research was supported in part by the National Sci-
ence Foundation (IIS-0915187 and IIS-1054319).
A Part-of-Speech Tagset
N common noun
O pronoun (personal/WH; not possessive)
^ proper noun
S nominal + possessive
Z proper noun + possessive
V verb including copula, auxiliaries
L nominal + verbal (e.g. i?m), verbal + nominal (let?s)
M proper noun + verbal
A adjective
R adverb
! interjection
D determiner
P pre- or postposition, or subordinating conjunction
& coordinating conjunction
T verb particle
X existential there, predeterminers
Y X + verbal
# hashtag (indicates topic/category for tweet)
@ at-mention (indicates a user as a recipient of a tweet)
~ discourse marker, indications of continuation across
multiple tweets
U URL or email address
E emoticon
$ numeral
, punctuation
G other abbreviations, foreign words, possessive endings,
symbols, garbage
Table 6: POS tagset from Gimpel et al (2011) used in this
paper, and described further in the released annotation
guidelines.
388
References
J. Allan and H. Raghavan. 2002. Using part-of-speech
patterns to reduce query ambiguity. In Proc. of SIGIR.
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In Proc. of ICML.
P. Blunsom and T. Cohn. 2011. A hierarchical Pitman-
Yor process HMM for unsupervised part of speech in-
duction. In Proc. of ACL.
S. Brody and N. Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proc. of EMNLP.
P. F. Brown, P. V. de Souza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Linguis-
tics, 18(4).
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hr-
uschka Jr, and T. M. Mitchell. 2010. Toward an archi-
tecture for never-ending language learning. In Proc. of
AAAI.
A. Clark. 2003. Combining distributional and morpho-
logical information for part of speech induction. In
Proc. of EACL.
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discov-
ering sociolinguistic associations with structured spar-
sity. In Proc. of ACL.
J. Eisenstein. 2013. What to do about bad language on
the internet. In Proc. of NAACL.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Proc. of
EMNLP.
E. N. Forsyth and C. H. Martell. 2007. Lexical and dis-
course analysis of online chat dialog. In Proc. of ICSC.
E. N. Forsyth. 2007. Improving automated lexical and
discourse analysis of online chat dialog. Master?s the-
sis, Naval Postgraduate School.
J. Foster, O. Cetinoglu, J. Wagner, J. L. Roux, S. Hogan,
J. Nivre, D. Hogan, and J. van Genabith. 2011. #hard-
toparse: POS tagging and parsing the Twitterverse. In
Proc. of AAAI-11 Workshop on Analysing Microtext.
K. Gimpel, N. Schneider, B. O?Connor, D. Das, D. Mills,
J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan,
and N. A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proc. of ACL.
Google. 2012. Freebase data dumps. http://
download.freebase.com/datadumps/.
B. Han and T. Baldwin. 2011. Lexical normalisation of
short text messages: Makn sens a #twitter. In Proc. of
ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical programming, 45(1).
X. Liu, S. Zhang, F. Wei, and M. Zhou. 2011. Recogniz-
ing named entities in tweets. In Proc. of ACL.
M. Lui and T. Baldwin. 2012. langid.py: An off-the-
shelf language identification tool. In Proc. of ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2).
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. of ICML.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic domain adaptation for parsing. In Proc. of
NAACL.
B. O?Connor, M. Krieger, and D. Ahn. 2010.
TweetMotif: exploratory search and topic summariza-
tion for Twitter. In Proc. of AAAI Conference on We-
blogs and Social Media.
O. Owoputi, B. O?Connor, C. Dyer, K. Gimpel, and
N. Schneider. 2012. Part-of-speech tagging for Twit-
ter: Word clusters and other advances. Technical Re-
port CMU-ML-12-107, Carnegie Mellon University.
B. Pang and L. Lee. 2008. Opinion mining and sentiment
analysis. Now Publishers.
S. Petrov and R. McDonald. 2012. Overview of the 2012
shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: An experimental
study. In Proc. of EMNLP.
T. Schnoebelen. 2012. Do you smile with your nose?
Stylistic variation in Twitter emoticons. University of
Pennsylvania Working Papers in Linguistics, 18(2):14.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL.
O. T?ckstr?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure. In Proc. of NAACL.
389
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In Proc. of ACL.
P. D. Turney. 2002. Thumbs up or thumbs down?: se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL.
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the elastic net. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
67(2):301?320.
390
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 42?47,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills,
Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith
School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA
{kgimpel,nschneid,brenocon,dipanjan,dpmills,
jacobeis,mheilman,dyogatama,jflanigan,nasmith}@cs.cmu.edu
Abstract
We address the problem of part-of-speech tag-
ging for English data from the popular micro-
blogging service Twitter. We develop a tagset,
annotate data, develop features, and report
tagging results nearing 90% accuracy. The
data and tools have been made available to the
research community with the goal of enabling
richer text analysis of Twitter and related so-
cial media data sets.
1 Introduction
The growing popularity of social media and user-
created web content is producing enormous quanti-
ties of text in electronic form. The popular micro-
blogging service Twitter (twitter.com) is one
particularly fruitful source of user-created content,
and a flurry of recent research has aimed to under-
stand and exploit these data (Ritter et al, 2010; Shar-
ifi et al, 2010; Barbosa and Feng, 2010; Asur and
Huberman, 2010; O?Connor et al, 2010a; Thelwall
et al, 2011). However, the bulk of this work eschews
the standard pipeline of tools which might enable
a richer linguistic analysis; such tools are typically
trained on newstext and have been shown to perform
poorly on Twitter (Finin et al, 2010).
One of the most fundamental parts of the linguis-
tic pipeline is part-of-speech (POS) tagging, a basic
form of syntactic analysis which has countless appli-
cations in NLP. Most POS taggers are trained from
treebanks in the newswire domain, such as the Wall
Street Journal corpus of the Penn Treebank (PTB;
Marcus et al, 1993). Tagging performance degrades
on out-of-domain data, and Twitter poses additional
challenges due to the conversational nature of the
text, the lack of conventional orthography, and 140-
character limit of each message (?tweet?). Figure 1
shows three tweets which illustrate these challenges.
(a) @Gunservatively@ obozo? willV goV nutsA
whenR PA? electsV aD RepublicanA GovernorN
nextP Tue? ., CanV youO sayV redistrictingV ?,
(b) SpendingV theD dayN withhhP mommmaN !,
(c) lmao! ..., s/oV toP theD coolA assN asianA
officerN 4P #1$ notR runninV myD licenseN and&
#2$ notR takinV druN booN toP jailN ., ThankV
uO God? ., #amen#
Figure 1: Example tweets with gold annotations. Under-
lined tokens show tagger improvements due to features
detailed in Section 3 (respectively: TAGDICT, METAPH,
and DISTSIM).
In this paper, we produce an English POS tagger
that is designed especially for Twitter data. Our con-
tributions are as follows:
? we developed a POS tagset for Twitter,
? we manually tagged 1,827 tweets,
? we developed features for Twitter POS tagging
and conducted experiments to evaluate them, and
? we provide our annotated corpus and trained POS
tagger to the research community.
Beyond these specific contributions, we see this
work as a case study in how to rapidly engi-
neer a core NLP system for a new and idiosyn-
cratic dataset. This project was accomplished in
200 person-hours spread across 17 people and two
months. This was made possible by two things:
(1) an annotation scheme that fits the unique char-
acteristics of our data and provides an appropriate
level of linguistic detail, and (2) a feature set that
captures Twitter-specific properties and exploits ex-
isting resources such as tag dictionaries and phonetic
normalization. The success of this approach demon-
strates that with careful design, supervised machine
learning can be applied to rapidly produce effective
language technology in new domains.
42
Tag Description Examples %
Nominal, Nominal + Verbal
N common noun (NN, NNS) books someone 13.7
O pronoun (personal/WH; not
possessive; PRP, WP)
it you u meeee 6.8
S nominal + possessive books? someone?s 0.1
? proper noun (NNP, NNPS) lebron usa iPad 6.4
Z proper noun + possessive America?s 0.2
L nominal + verbal he?s book?ll iono
(= I don?t know)
1.6
M proper noun + verbal Mark?ll 0.0
Other open-class words
V verb incl. copula,
auxiliaries (V*, MD)
might gonna
ought couldn?t is
eats
15.1
A adjective (J*) good fav lil 5.1
R adverb (R*, WRB) 2 (i.e., too) 4.6
! interjection (UH) lol haha FTW yea
right
2.6
Other closed-class words
D determiner (WDT, DT,
WP$, PRP$)
the teh its it?s 6.5
P pre- or postposition, or
subordinating conjunction
(IN, TO)
while to for 2 (i.e.,
to) 4 (i.e., for)
8.7
& coordinating conjunction
(CC)
and n & + BUT 1.7
T verb particle (RP) out off Up UP 0.6
X existential there,
predeterminers (EX, PDT)
both 0.1
Y X + verbal there?s all?s 0.0
Twitter/online-specific
# hashtag (indicates
topic/category for tweet)
#acl 1.0
@ at-mention (indicates
another user as a recipient
of a tweet)
@BarackObama 4.9
~ discourse marker,
indications of continuation
of a message across
multiple tweets
RT and : in retweet
construction RT
@user : hello
3.4
U URL or email address http://bit.ly/xyz 1.6
E emoticon :-) :b (: <3 o O 1.0
Miscellaneous
$ numeral (CD) 2010 four 9:30 1.5
, punctuation (#, $, '', (,
), ,, ., :, ``)
!!! .... ?!? 11.6
G other abbreviations, foreign
words, possessive endings,
symbols, garbage (FW,
POS, SYM, LS)
ily (I love you) wby
(what about you) ?s
 -->
awesome...I?m
1.1
Table 1: The set of tags used to annotate tweets. The
last column indicates each tag?s relative frequency in the
full annotated data (26,435 tokens). (The rates for M and
Y are both < 0.0005.)
2 Annotation
Annotation proceeded in three stages. For Stage 0,
we developed a set of 20 coarse-grained tags based
on several treebanks but with some additional cate-
gories specific to Twitter, including URLs and hash-
tags. Next, we obtained a random sample of mostly
American English1 tweets from October 27, 2010,
automatically tokenized them using a Twitter tok-
enizer (O?Connor et al, 2010b),2 and pre-tagged
them using the WSJ-trained Stanford POS Tagger
(Toutanova et al, 2003) in order to speed up man-
ual annotation. Heuristics were used to mark tokens
belonging to special Twitter categories, which took
precedence over the Stanford tags.
Stage 1 was a round of manual annotation: 17 re-
searchers corrected the automatic predictions from
Stage 0 via a custom Web interface. A total of
2,217 tweets were distributed to the annotators in
this stage; 390 were identified as non-English and
removed, leaving 1,827 annotated tweets (26,436 to-
kens).
The annotation process uncovered several situa-
tions for which our tagset, annotation guidelines,
and tokenization rules were deficient or ambiguous.
Based on these considerations we revised the tok-
enization and tagging guidelines, and for Stage 2,
two annotators reviewed and corrected all of the
English tweets tagged in Stage 1. A third anno-
tator read the annotation guidelines and annotated
72 tweets from scratch, for purposes of estimating
inter-annotator agreement. The 72 tweets comprised
1,021 tagged tokens, of which 80 differed from the
Stage 2 annotations, resulting in an agreement rate
of 92.2% and Cohen?s ? value of 0.914. A final
sweep was made by a single annotator to correct er-
rors and improve consistency of tagging decisions
across the corpus. The released data and tools use
the output of this final stage.
2.1 Tagset
We set out to develop a POS inventory for Twitter
that would be intuitive and informative?while at
the same time simple to learn and apply?so as to
maximize tagging consistency within and across an-
1We filtered to tweets sent via an English-localized user in-
terface set to a United States timezone.
2http://github.com/brendano/tweetmotif
43
notators. Thus, we sought to design a coarse tagset
that would capture standard parts of speech3 (noun,
verb, etc.) as well as categories for token varieties
seen mainly in social media: URLs and email ad-
dresses; emoticons; Twitter hashtags, of the form
#tagname, which the author may supply to catego-
rize a tweet; and Twitter at-mentions, of the form
@user, which link to other Twitter users from within
a tweet.
Hashtags and at-mentions can also serve as words
or phrases within a tweet; e.g. Is #qadaffi going down?.
When used in this way, we tag hashtags with their
appropriate part of speech, i.e., as if they did not start
with #. Of the 418 hashtags in our data, 148 (35%)
were given a tag other than #: 14% are proper nouns,
9% are common nouns, 5% are multi-word express-
sions (tagged as G), 3% are verbs, and 4% are some-
thing else. We do not apply this procedure to at-
mentions, as they are nearly always proper nouns.
Another tag, ~, is used for tokens marking spe-
cific Twitter discourse functions. The most popular
of these is the RT (?retweet?) construction to publish
a message with attribution. For example,
RT @USER1 : LMBO ! This man filed an
EMERGENCY Motion for Continuance on
account of the Rangers game tonight ! 
Wow lmao
indicates that the user @USER1 was originally the
source of the message following the colon. We ap-
ply ~ to the RT and : (which are standard), and
also, which separates the author?s comment from
the retweeted material.4 Another common discourse
marker is ellipsis dots (. . . ) at the end of a tweet,
indicating a message has been truncated to fit the
140-character limit, and will be continued in a sub-
sequent tweet or at a specified URL.
Our first round of annotation revealed that, due to
nonstandard spelling conventions, tokenizing under
a traditional scheme would be much more difficult
3Our starting point was the cross-lingual tagset presented by
Petrov et al (2011). Most of our tags are refinements of those
categories, which in turn are groupings of PTB WSJ tags (see
column 2 of Table 1). When faced with difficult tagging deci-
sions, we consulted the PTB and tried to emulate its conventions
as much as possible.
4These ?iconic deictics? have been studied in other online
communities as well (Collister, 2010).
than for Standard English text. For example, apos-
trophes are often omitted, and there are frequently
words like ima (short for I?m gonna) that cut across
traditional POS categories. Therefore, we opted not
to split contractions or possessives, as is common
in English corpus preprocessing; rather, we intro-
duced four new tags for combined forms: {nominal,
proper noun} ? {verb, possessive}.5
The final tagging scheme (Table 1) encompasses
25 tags. For simplicity, each tag is denoted with a
single ASCII character. The miscellaneous category
G includes multiword abbreviations that do not fit
in any of the other categories, like ily (I love you), as
well as partial words, artifacts of tokenization errors,
miscellaneous symbols, possessive endings,6 and ar-
rows that are not used as discourse markers.
Figure 2 shows where tags in our data tend to oc-
cur relative to the middle word of the tweet. We
see that Twitter-specific tags have strong positional
preferences: at-mentions (@) and Twitter discourse
markers (~) tend to occur towards the beginning of
messages, whereas URLs (U), emoticons (E), and
categorizing hashtags (#) tend to occur near the end.
3 System
Our tagger is a conditional random field (CRF; Laf-
ferty et al, 2001), enabling the incorporation of ar-
bitrary local features in a log-linear model. Our
base features include: a feature for each word type,
a set of features that check whether the word con-
tains digits or hyphens, suffix features up to length 3,
and features looking at capitalization patterns in the
word. We then added features that leverage domain-
specific properties of our data, unlabeled in-domain
data, and external linguistic resources.
TWORTH: Twitter orthography. We have features
for several regular expression-style rules that detect
at-mentions, hashtags, and URLs.
NAMES: Frequently-capitalized tokens. Micro-
bloggers are inconsistent in their use of capitaliza-
tion, so we compiled gazetteers of tokens which are
frequently capitalized. The likelihood of capital-
ization for a token is computed as Ncap+?CN+C , where
5The modified tokenizer is packaged with our tagger.
6Possessive endings only appear when a user or the tok-
enizer has separated the possessive ending from a possessor; the
tokenizer only does this when the possessor is an at-mention.
44
Figure 2: Average position, relative to the middle word in the tweet, of tokens labeled with each tag. Most tags fall
between ?1 and 1 on this scale; these are not shown.
N is the token count, Ncap is the capitalized to-
ken count, and ? and C are the prior probability
and its prior weight.7 We compute features for
membership in the top N items by this metric, for
N ? {1000, 2000, 3000, 5000, 10000, 20000}.
TAGDICT: Traditional tag dictionary. We add
features for all coarse-grained tags that each word
occurs with in the PTB8 (conjoined with their fre-
quency rank). Unlike previous work that uses tag
dictionaries as hard constraints, we use them as soft
constraints since we expect lexical coverage to be
poor and the Twitter dialect of English to vary sig-
nificantly from the PTB domains. This feature may
be seen as a form of type-level domain adaptation.
DISTSIM: Distributional similarity. When train-
ing data is limited, distributional features from un-
labeled text can improve performance (Schu?tze and
Pedersen, 1993). We used 1.9 million tokens from
134,000 unlabeled tweets to construct distributional
features from the successor and predecessor proba-
bilities for the 10,000 most common terms. The suc-
cessor and predecessor transition matrices are hori-
zontally concatenated into a sparse matrixM, which
we approximate using a truncated singular value de-
composition: M ? USVT, where U is limited to
50 columns. Each term?s feature vector is its row
in U; following Turian et al (2010), we standardize
and scale the standard deviation to 0.1.
METAPH: Phonetic normalization. Since Twitter
includes many alternate spellings of words, we used
the Metaphone algorithm (Philips, 1990)9 to create
a coarse phonetic normalization of words to simpler
keys. Metaphone consists of 19 rules that rewrite
consonants and delete vowels. For example, in our
7? = 1100 , C = 10; this score is equivalent to the posterior
probability of capitalization with a Beta(0.1, 9.9) prior.
8Both WSJ and Brown corpora, no case normalization. We
also tried adding the WordNet (Fellbaum, 1998) and Moby
(Ward, 1996) lexicons, which increased lexical coverage but did
not seem to help performance.
9Via the Apache Commons implementation: http://
commons.apache.org/codec/
data, {thangs thanks thanksss thanx thinks thnx}
are mapped to 0NKS, and {lmao lmaoo lmaooooo}
map to LM. But it is often too coarse; e.g. {war we?re
wear were where worry} map to WR.
We include two types of features. First, we use
the Metaphone key for the current token, comple-
menting the base model?s word features. Second,
we use a feature indicating whether a tag is the most
frequent tag for PTB words having the same Meta-
phone key as the current token. (The second feature
was disabled in both ?TAGDICT and ?METAPH ab-
lation experiments.)
4 Experiments
Our evaluation was designed to test the efficacy of
this feature set for part-of-speech tagging given lim-
ited training data. We randomly divided the set of
1,827 annotated tweets into a training set of 1,000
(14,542 tokens), a development set of 327 (4,770 to-
kens), and a test set of 500 (7,124 tokens). We com-
pare our system against the Stanford tagger. Due
to the different tagsets, we could not apply the pre-
trained Stanford tagger to our data. Instead, we re-
trained it on our labeled data, using a standard set
of features: words within a 5-word window, word
shapes in a 3-word window, and up to length-3
prefixes, length-3 suffixes, and prefix/suffix pairs.10
The Stanford system was regularized using a Gaus-
sian prior of ?2 = 0.5 and our system with a Gaus-
sian prior of ?2 = 5.0, tuned on development data.
The results are shown in Table 2. Our tagger with
the full feature set achieves a relative error reduction
of 25% compared to the Stanford tagger. We also
show feature ablation experiments, each of which
corresponds to removing one category of features
from the full set. In Figure 1, we show examples
that certain features help solve. Underlined tokens
10We used the following feature modules in the Stanford tag-
ger: bidirectional5words, naacl2003unknowns,
wordshapes(-3,3), prefix(3), suffix(3),
prefixsuffix(3).
45
Dev. Test
Our tagger, all features 88.67 89.37
independent ablations:
?DISTSIM 87.88 88.31 (?1.06)
?TAGDICT 88.28 88.31 (?1.06)
?TWORTH 87.51 88.37 (?1.00)
?METAPH 88.18 88.95 (?0.42)
?NAMES 88.66 89.39 (+0.02)
Our tagger, base features 82.72 83.38
Stanford tagger 85.56 85.85
Annotator agreement 92.2
Table 2: Tagging accuracies on development and test
data, including ablation experiments. Features are or-
dered by importance: test accuracy decrease due to ab-
lation (final column).
Tag Acc. Confused Tag Acc. Confused
V 91 N ! 82 N
N 85 ? L 93 V
, 98 ~ & 98 ?
P 95 R U 97 ,
? 71 N $ 89 P
D 95 ? # 89 ?
O 97 ? G 26 ,
A 79 N E 88 ,
R 83 A T 72 P
@ 99 V Z 45 ?
~ 91 ,
Table 3: Accuracy (recall) rates per class, in the test set
with the full model. (Omitting tags that occur less than
10 times in the test set.) For each gold category, the most
common confusion is shown.
are incorrect in a specific ablation, but are corrected
in the full system (i.e. when the feature is added).
The ?TAGDICT ablation gets elects, Governor,
and next wrong in tweet (a). These words appear
in the PTB tag dictionary with the correct tags, and
thus are fixed by that feature. In (b), withhh is ini-
tially misclassified an interjection (likely caused by
interjections with the same suffix, like ohhh), but is
corrected by METAPH, because it is normalized to the
same equivalence class as with. Finally, s/o in tweet
(c) means ?shoutout?, which appears only once in
the training data; adding DISTSIM causes it to be cor-
rectly identified as a verb.
Substantial challenges remain; for example, de-
spite the NAMES feature, the system struggles to
identify proper nouns with nonstandard capitaliza-
tion. This can be observed from Table 3, which
shows the recall of each tag type: the recall of proper
nouns (?) is only 71%. The system also struggles
with the miscellaneous category (G), which covers
many rare tokens, including obscure symbols and ar-
tifacts of tokenization errors. Nonetheless, we are
encouraged by the success of our system on the
whole, leveraging out-of-domain lexical resources
(TAGDICT), in-domain lexical resources (DISTSIM),
and sublexical analysis (METAPH).
Finally, we note that, even though 1,000 train-
ing examples may seem small, the test set accuracy
when training on only 500 tweets drops to 87.66%,
a decrease of only 1.7% absolute.
5 Conclusion
We have developed a part-of-speech tagger for Twit-
ter and have made our data and tools available to the
research community at http://www.ark.cs.
cmu.edu/TweetNLP. More generally, we be-
lieve that our approach can be applied to address
other linguistic analysis needs as they continue to
arise in the era of social media and its rapidly chang-
ing linguistic conventions. We also believe that the
annotated data can be useful for research into do-
main adaptation and semi-supervised learning.
Acknowledgments
We thank Desai Chen, Chris Dyer, Lori Levin, Behrang
Mohit, Bryan Routledge, Naomi Saphra, and Tae Yano
for assistance in annotating data. This research was sup-
ported in part by: the NSF through CAREER grant IIS-
1054319, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
ber W911NF-10-1-0533, Sandia National Laboratories
(fellowship to K. Gimpel), and the U. S. Department of
Education under IES grant R305B040063 (fellowship to
M. Heilman).
References
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proc. of WI-
IAT.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proc. of COLING.
Lauren Collister. 2010. Meaning variation of the iconic
deictics ? and <? in an online community. In New
Ways of Analyzing Variation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
46
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010. An-
notating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010b. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proc. of ICWSM (demo
track).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Lawrence Philips. 1990. Hanging on the Metaphone.
Computer Language, 7(12).
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In Proc.
of NAACL.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. of NAACL.
Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in Twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406?418.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
HLT-NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.
Grady Ward. 1996. Moby lexicon. http://icon.
shef.ac.uk/Moby.
47
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 352?361,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning Latent Personas of Film Characters
David Bamman Brendan O?Connor Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dbamman,brenocon,nasmith}@cs.cmu.edu
Abstract
We present two latent variable models for
learning character types, or personas, in
film, in which a persona is defined as a
set of mixtures over latent lexical classes.
These lexical classes capture the stereo-
typical actions of which a character is the
agent and patient, as well as attributes by
which they are described. As the first
attempt to solve this problem explicitly,
we also present a new dataset for the
text-driven analysis of film, along with
a benchmark testbed to help drive future
work in this area.
1 Introduction
Philosophers and dramatists have long argued
whether the most important element of narrative
is plot or character. Under a classical Aristotelian
perspective, plot is supreme;1 modern theoretical
dramatists and screenwriters disagree.2
Without addressing this debate directly, much
computational work on narrative has focused on
learning the sequence of events by which a story
is defined; in this tradition we might situate sem-
inal work on learning procedural scripts (Schank
and Abelson, 1977; Regneri et al, 2010), narrative
chains (Chambers and Jurafsky, 2008), and plot
structure (Finlayson, 2011; Elsner, 2012; McIn-
tyre and Lapata, 2010; Goyal et al, 2010).
We present a complementary perspective that
addresses the importance of character in defining
1?Dramatic action . . . is not with a view to the representa-
tion of character: character comes in as subsidiary to the ac-
tions . . . The Plot, then, is the first principle, and, as it were,
the soul of a tragedy: Character holds the second place.? Po-
etics I.VI (Aristotle, 335 BCE).
2?Aristotle was mistaken in his time, and our scholars are
mistaken today when they accept his rulings concerning char-
acter. Character was a great factor in Aristotle?s time, and no
fine play ever was or ever will be written without it? (Egri,
1946, p. 94); ?What the reader wants is fascinating, complex
characters? (McKee, 1997, 100).
a story. Our testbed is film. Under this perspec-
tive, a character?s latent internal nature drives the
action we observe. Articulating narrative in this
way leads to a natural generative story: we first de-
cide that we?re going to make a particular kind of
movie (e.g., a romantic comedy), then decide on a
set of character types, or personas, we want to see
involved (the PROTAGONIST, the LOVE INTER-
EST, the BEST FRIEND). After picking this set, we
fill out each of these roles with specific attributes
(female, 28 years old, klutzy); with this cast of
characters, we then sketch out the set of events by
which they interact with the world and with each
other (runs but just misses the train, spills coffee
on her boss) ? through which they reveal to the
viewer those inherent qualities about themselves.
This work is inspired by past approaches that in-
fer typed semantic arguments along with narra-
tive schemas (Chambers and Jurafsky, 2009; Reg-
neri et al, 2011), but seeks a more holistic view
of character, one that learns from stereotypical at-
tributes in addition to plot events. This work also
naturally draws on earlier work on the unsuper-
vised learning of verbal arguments and semantic
roles (Pereira et al, 1993; Grenager and Manning,
2006; Titov and Klementiev, 2012) and unsuper-
vised relation discovery (Yao et al, 2011).
This character-centric perspective leads to two
natural questions. First, can we learn what those
standard personas are by how individual charac-
ters (who instantiate those types) are portrayed?
Second, can we learn the set of attributes and ac-
tions by which we recognize those common types?
How do we, as viewers, recognize a VILLIAN?
At its most extreme, this perspective reduces
to learning the grand archetypes of Joseph Camp-
bell (1949) or Carl Jung (1981), such as the HERO
or TRICKSTER. We seek, however, a more fine-
grained set that includes not only archetypes, but
stereotypes as well ? characters defined by a fixed
set of actions widely known to be representative of
352
a class. This work offers a data-driven method for
answering these questions, presenting two proba-
blistic generative models for inferring latent char-
acter types.
This is the first work that attempts to learn ex-
plicit character personas in detail; as such, we
present a new dataset for character type induction
in film and a benchmark testbed for evaluating fu-
ture work.3
2 Data
2.1 Text
Our primary source of data comes from 42,306
movie plot summaries extracted from the
November 2, 2012 dump of English-language
Wikipedia.4 These summaries, which have a
median length of approximately 176 words,5
contain a concise synopsis of the movie?s events,
along with implicit descriptions of the characters
(e.g., ?rebel leader Princess Leia,? ?evil lord Darth
Vader?). To extract structure from this data, we
use the Stanford CoreNLP library6 to tag and
syntactically parse the text, extract entities, and
resolve coreference within the document. With
this structured representation, we extract linguistic
features for each character, looking at immediate
verb governors and attribute syntactic dependen-
cies to all of the entity?s mention headwords,
extracted from the typed dependency tuples pro-
duced by the parser; we refer to ?CCprocessed?
syntactic relations described in de Marneffe and
Manning (2008):
? Agent verbs. Verbs for which the entity is an
agent argument (nsubj or agent).
? Patient verbs. Verbs for which the entity is
the patient, theme or other argument (dobj,
nsubjpass, iobj, or any prepositional argu-
ment prep *).
? Attributes. Adjectives and common noun
words that relate to the mention as adjecti-
val modifiers, noun-noun compounds, appos-
itives, or copulas (nsubj or appos governors,
or nsubj, appos, amod, nn dependents of an
entity mention).
3All datasets and software for replication can be found at
http://www.ark.cs.cmu.edu/personas.
4http://dumps.wikimedia.org/enwiki/
5More popular movies naturally attract more attention on
Wikipedia and hence more detail: the top 1,000 movies by
box office revenue have a median length of 715 words.
6http://nlp.stanford.edu/software/
corenlp.shtml
These three roles capture three different ways in
which character personas are revealed: the actions
they take on others, the actions done to them, and
the attributes by which they are described. For ev-
ery character we thus extract a bag of (r, w) tu-
ples, where w is the word lemma and r is one
of {agent verb,patient verb, attribute} as iden-
tified by the above rules.
2.2 Metadata
Our second source of information consists of char-
acter and movie metadata drawn from the Novem-
ber 4, 2012 dump of Freebase.7 At the movie
level, this includes data on the language, country,
release date and detailed genre (365 non-mutually
exclusive categories, including ?Epic Western,?
?Revenge,? and ?Hip Hop Movies?). Many of the
characters in movies are also associated with the
actors who play them; since many actors also have
detailed biographical information, we can ground
the characters in what we know of those real peo-
ple ? including their gender and estimated age at
the time of the movie?s release (the difference be-
tween the release date of the movie and the actor?s
date of birth).
Across all 42,306 movies, entities average 3.4
agent events, 2.0 patient events, and 2.1 attributes.
For all experiments described below, we restrict
our dataset to only those events that are among the
1,000 most frequent overall, and only characters
with at least 3 events. 120,345 characters meet this
criterion; of these, 33,559 can be matched to Free-
base actors with a specified gender, and 29,802 can
be matched to actors with a given date of birth. Of
all actors in the Freebase data whose age is given,
the average age at the time of movie is 37.9 (stan-
dard deviation 14.1); of all actors whose gender
is known, 66.7% are male.8 The age distribution
is strongly bimodal when conditioning on gender:
the average age of a female actress at the time of a
movie?s release is 33.0 (s.d. 13.4), while that of a
male actor is 40.5 (s.d. 13.7).
3 Personas
One way we recognize a character?s latent type
is by observing the stereotypical actions they
7http://download.freebase.com/
datadumps/
8Whether this extreme 2:1 male/female ratio reflects an
inherent bias in film or a bias in attention on Freebase (or
Wikipedia, on which it draws) is an interesting research ques-
tion in itself.
353
perform (e.g., VILLAINS strangle), the actions
done to them (e.g., VILLAINS are foiled and ar-
rested) and the words by which they are described
(VILLAINS are evil). To capture this intuition, we
define a persona as a set of three typed distribu-
tions: one for the words for which the character is
the agent, one for which it is the patient, and one
for words by which the character is attributively
modified. Each distribution ranges over a fixed set
of latent word classes, or topics. Figure 1 illus-
trates this definition for a toy example: a ZOMBIE
persona may be characterized as being the agent
of primarily eating and killing actions, the patient
of killing actions, and the object of dead attributes.
The topic labeled eat may include words like eat,
drink, and devour.
eat kill lov
e
dea
d
hap
py
agent
0.0
0.2
0.4
0.6
0.8
1.0
eat kill lov
e
dea
d
hap
py
patient
0.0
0.2
0.4
0.6
0.8
1.0
eat kill lov
e
dea
d
hap
py
attribute
0.0
0.2
0.4
0.6
0.8
1.0
Figure 1: A persona is a set of three distributions
over latent topics. In this toy example, the ZOM-
BIE persona is primarily characterized by being
the agent of words from the eat and kill topics, the
patient of kill words, and the object of words from
the dead topic.
4 Models
Both models that we present here simultaneously
learn three things: 1.) a soft clustering over words
to topics (e.g., the verb ?strangle? is mostly a type
of Assault word); 2.) a soft clustering over top-
ics to personas (e.g., VILLIANS perform a lot of
Assault actions); and 3.) a hard clustering over
characters to personas (e.g., Darth Vader is a VIL-
LAIN.) They each use different evidence: since
our data includes not only textual features (in the
form of actions and attributes of the characters) but
also non-textual information (such as movie genre,
age and gender), we design a model that exploits
this additional source of information in discrimi-
nating between character types; since this extra-
linguistic information may not always be avail-
able, we also design a model that learns only from
the text itself. We present the text-only model first
?
?
p
z?
w
r
?
?
?
W
E
D
?
p me
md
?
?
?
2
z?
w
r
?
?
?
W
E
D
P Number of personas (hyperparameter)
K Number of word topics (hyperparameter)
D Number of movie plot summaries
E Number of characters in movie d
W Number of (role, word) tuples used by character e
?k Topic k?s distribution over V words.
r Tuple role: agent verb, patient verb, attribute
?p,r Distribution over topics for persona p in role r
?d Movie d?s distribution over personas
pe Character e?s persona (integer, p ? {1..P})
j A specific (r, w) tuple in the data
zj Word topic for tuple j
wj Word for tuple j
? Concentration parameter for Dirichlet model
? Feature weights for regression model
?, ?2 Gaussian mean and variance (for regularizing ?)
md Movie features (from movie metadata)
me Entity features (from movie actor metadata)
?r , ? Dirichlet concentration parameters
Figure 2: Above: Dirichlet persona model (left)
and persona regression model (right). Bottom:
Definition of variables.
for simplicity. Throughout, V is the word vocab-
ulary size, P is the number of personas, and K is
the number of topics.
4.1 Dirichlet Persona Model
In the most basic model, we only use informa-
tion from the structured text, which comes as a
bag of (r, w) tuples for each character in a movie,
where w is the word lemma and r is the rela-
tion of the word with respect to the character (one
of agent verb, patient verb or attribute, as out-
lined in ?2.1 above). The generative story runs as
follows. First, let there be K latent word topics;
as in LDA (Blei et al, 2003), these are words that
will be soft-clustered together by virtue of appear-
ing in similar contexts. Each latent word cluster
354
?k ? Dir(?) is a multinomial over the V words in
the vocabulary, drawn from a Dirichlet parameter-
ized by ?. Next, let a persona p be defined as a set
of three multinomials ?p over these K topics, one
for each typed role r, each drawn from a Dirichlet
with a role-specific hyperparameter (?r).
Every document (a movie plot summary) con-
tains a set of characters, each of which is associ-
ated with a single latent persona p; for every ob-
served (r, w) tuple associated with the character,
we sample a latent topic k from the role-specific
?p,r. Conditioned on this topic assignment, the
observed word is drawn from ?k. The distribu-
tion of these personas for a given document is de-
termined by a document-specific multinomial ?,
drawn from a Dirichlet parameterized by ?.
Figure 2 (above left) illustrates the form of the
model. To simplify inference, we collapse out the
persona-topic distributions ?, the topic-word dis-
tributions ? and the persona distribution ? for each
document. Inference on the remaining latent vari-
ables ? the persona p for each character type and
the topic z for each word associated with that char-
acter ? is conducted via collapsed Gibbs sampling
(Griffiths and Steyvers, 2004); at each iteration,
for each character e, we sample their persona pe:
P (pe = k | p?e, z, ?, ?) ?
(
c?ed,k + ?k
)
??j
(c?erj ,k,zj+?rj )
(c?erj ,k,?+K?rj )
(1)
Here, c?ed,k is the count of all characters in docu-ment d whose current persona sample is also k
(not counting the current character e under con-
sideration);9 j ranges over all (rj , wj) tuples asso-
ciated with character e. Each c?erj ,k,zj is the countof all tuples with role rj and current topic zj used
with persona k. c?erj ,k,? is the same count, summingover all topics z. In other words, the probabil-
ity that character e embodies persona k is propor-
tional to the number of other characters in the plot
summary who also embody that persona (plus the
Dirichlet hyperparameter ?k) times the contribu-
tion of each observed word wj for that character,
given its current topic assignment zj .
Once all personas have been sampled, we sam-
9The?e superscript denotes counts taken without consid-
ering the current sample for character e.
ple the latent topics for each tuple as the following.
P (zj = k | p, z?j , w, r, ?, ?) ?
(c?jrj ,p,k+?rj )
(c?jrj ,p,?+K?rj )
?
(c?jk,wj+?)
(c?jk,?+V ?)
(2)
Here, conditioned on the current sample p for
the character?s persona, the probability that tuple
j originates in topic k is proportional to the num-
ber of other tuples with that same role rj drawn
from the same topic for that persona (c?jrj ,p,k), nor-malized by the number of other rj tuples associ-
ated with that persona overall (c?jrj ,p,?), multiplied
by the number of times word wj is associated with
that topic (c?jk,wj ) normalized by the total numberof other words associated with that topic overall
(c?jk,?).We optimize the values of the Dirichlet hyper-
parameters ?, ? and ? using slice sampling with a
uniform prior every 20 iterations for the first 500
iterations, and every 100 iterations thereafter. Af-
ter a burn-in phase of 10,000 iterations, we collect
samples every 10 iterations (to lessen autocorrela-
tion) until a total of 100 have been collected.
4.2 Persona Regression
To incorporate observed metadata in the form of
movie genre, character age and character gen-
der, we adopt an ?upstream? modeling approach
(Mimno and McCallum, 2008), letting those ob-
served features influence the conditional probabil-
ity with which a given character is expected to as-
sume a particular persona, prior to observing any
of their actions. This captures the increased likeli-
hood, for example, that a 25-year-old male actor in
an action movie will play an ACTION HERO than
he will play a VALLEY GIRL.
To capture these effects, each character?s la-
tent persona is no longer drawn from a document-
specific Dirichlet; instead, the P -dimensional sim-
plex is the output of a multiclass logistic regres-
sion, where the document genre metadata md and
the character age and gender metadatame together
form a feature vector that combines with persona-
specific feature weights to form the following log-
linear distribution over personas, with the proba-
bility for persona k being:
P (p = k | md,me, ?) = exp([md;me]
>?k)
1+PP?1j=1 exp([md;me]>?j)(3)
The persona-specific ? coefficients are learned
through Monte Carlo Expectation Maximization
355
(Wei and Tanner, 1990), in which we alternate be-
tween the following:
1. Given current values for ?, for all characters
e in all plot summaries, sample values of pe
and zj for all associated tuples.
2. Given input metadata features m and the as-
sociated sampled values of p, find the values
of ? that maximize the standard multiclass lo-
gistic regression log likelihood, subject to `2
regularization.
Figure 2 (above right) illustrates this model. As
with the Dirichlet persona model, inference on p
for step 1 is conducted with collapsed Gibbs sam-
pling; the only difference in the sampling prob-
ability from equation 1 is the effect of the prior,
which here is deterministically fixed as the output
of the regression.
P (pe = k | p?e, z, ?,md,me, ?) ?
exp([md;me]>?k)?
?
j
(c?erj ,k,zj+?rj )
(c?erj ,k,?+K?rj )
(4)
The sampling equation for the topic assign-
ments z is identical to that in equation 2. In
practice we optimize ? every 1,000 iterations, un-
til a burn-in phase of 10,000 iterations has been
reached; at this point we following the same sam-
pling regime as for the Dirichlet persona model.
5 Evaluation
We evaluate our methods in two quantitative ways
by measuring the degree to which we recover two
different sets of gold-standard clusterings. This
evaluation also helps offer guidance for model se-
lection (in choosing the number of latent topics
and personas) by measuring performance on an
objective task.
5.1 Character Names
First, we consider all character names that occur in
at least two separate movies, generally as a conse-
quence of remakes or sequels; this includes proper
names such as ?Rocky Balboa,? ?Oliver Twist,?
and ?Indiana Jones,? as well as generic type names
such as ?Gang Member? and ?The Thief?; to mini-
mize ambiguity, we only consider character names
consisting of at least two tokens. Each of these
names is used by at least two different characters;
for example, a character named ?Jason Bourne?
is portrayed in The Bourne Identity, The Bourne
Supremacy, and The Bourne Ultimatum. While
these characters are certainly free to assume dif-
ferent roles in different movies, we believe that,
in the aggregate, they should tend to embody the
same character type and thus prove to be a natu-
ral clustering to recover. 970 character names oc-
cur at least twice in our data, and 2,666 individual
characters use one of those names. Let those 970
character names define 970 unique gold clusters
whose members include the individual characters
who use that name.
5.2 TV Tropes
As a second external measure of validation, we
consider a manually created clustering presented
at the website TV Tropes,10 a wiki that col-
lects user-submitted examples of common tropes
(narrative, character and plot devices) found in
television, film, and fiction, among other me-
dia. While TV Tropes contains a wide range of
such conventions, we manually identified a set of
72 tropes that could reasonably be labeled char-
acter types, including THE CORRUPT CORPO-
RATE EXECUTIVE, THE HARDBOILED DETEC-
TIVE, THE JERK JOCK, THE KLUTZ and THE
SURFER DUDE.
We manually aligned user-submitted examples
of characters embodying these 72 character types
with the canonical references in Freebase to cre-
ate a test set of 501 individual characters. While
the 72 character tropes represented here are a more
subjective measure, we expect to be able to at least
partially recover this clustering.
5.3 Variation of Information
To measure the similarity between the two clus-
terings of movie characters, gold clusters G and
induced latent persona clusters C, we calculate the
variation of information (Meila?, 2007):
V I(G, C) = H(G) +H(C)? 2I(G, C) (5)
= H(G|C) +H(C|G) (6)
VI measures the information-theoretic distance
between the two clusterings: a lower value means
greater similarity, and VI = 0 if they are iden-
tical. Low VI indicates that (induced) clusters
and (gold) clusters tend to overlap; i.e., knowing a
character?s (induced) cluster usually tells us their
(gold) cluster, and vice versa. Variation of infor-
mation is a metric (symmetric and obeys triangle
10http://tvtropes.org
356
Character Names ?5.1 TV Tropes ?5.2
K Model P = 25 P = 50 P = 100 P = 25 P = 50 P = 100
25 Persona regression 7.73 7.32 6.79 6.26 6.13 5.74Dirichlet persona 7.83 7.11 6.44 6.29 6.01 5.57
50 Persona regression 7.59 7.08 6.46 6.30 5.99 5.65Dirichlet persona 7.57 7.04 6.35 6.23 5.88 5.60
100 Persona regression 7.58 6.95 6.32 6.11 6.05 5.49Dirichlet persona 7.64 6.95 6.25 6.24 5.91 5.42
Table 1: Variation of information between learned personas and gold clusters for different numbers of
topics K and personas P . Lower values are better. All values are reported in bits.
Character Names ?5.1 TV Tropes ?5.2
K Model P = 25 P = 50 P = 100 P = 25 P = 50 P = 100
25 Persona regression 62.8 (?41%) 59.5 (?40%) 53.7 (?33%) 42.3 (?31%) 38.5 (?24%) 33.1 (?25%)Dirichlet persona 54.7 (?27%) 50.5 (?26%) 45.4 (?17%) 39.5 (?20%) 31.7 (?28%) 25.1 (?21%)
50 Persona regression 63.1 (?42%) 59.8 (?42%) 53.6 (?34%) 42.9 (?30%) 39.1 (?33%) 31.3 (?20%)Dirichlet persona 57.2 (?34%) 49.0 (?23%) 44.7 (?16%) 39.7 (?30%) 31.5 (?32%) 24.6 (?22%)
100 Persona regression 63.1 (?42%) 57.7 (?39%) 53.0 (?34%) 43.5 (?33%) 32.1 (?28%) 26.5 (?22%)Dirichlet persona 55.3 (?30%) 49.5 (?24%) 45.2 (?18%) 39.7 (?34%) 29.9 (?24%) 23.6 (?19%)
Table 2: Purity scores of recovering gold clusters. Higher values are better. Each absolute purity score
is paired with its improvement over a controlled baseline of permuting the learned labels while keeping
the cluster proportions the same.
inequality), and has a number of other desirable
properties.
Table 1 presents the VI between the learned per-
sona clusters and gold clusters, for varying num-
bers of personas (P = {25, 50, 100}) and top-
ics (K = {25, 50, 100}). To determine signifi-
cance with respect to a random baseline, we con-
duct a permutation test (Fisher, 1935; Pitman,
1937) in which we randomly shuffle the labels of
the learned persona clusters and count the num-
ber of times in 1,000 such trials that the VI of
the observed persona labels is lower than the VI
of the permuted labels; this defines a nonparamet-
ric p-value. All results presented are significant at
p < 0.001 (i.e. observed VI is never lower than
the simulation VI).
Over all tests in comparison to both gold clus-
terings, we see VI improve as both P and, to
a lesser extent, K increase. While this may be
expected as the number of personas increase to
match the number of distinct types in the gold
clusters (970 and 72, respectively), the fact that VI
improves as the number of latent topics increases
suggests that more fine-grained topics are helpful
for capturing nuanced character types.11
The difference between the persona regression
model and the Dirichlet persona model here is not
11This trend is robust to the choice of cluster metric: here
VI and F -score have a correlation of ?0.87; as more latent
topics and personas are added, clustering improves (causing
the F -score to go up and the VI distance to go down).
significant; while VI allows us to compare mod-
els with different numbers of latent clusters, its re-
quirement that clusterings be mutually informative
places a high overhead on models that are funda-
mentally unidirectional (in Table 1, for example,
the room for improvement between two models
of the same P and K is naturally smaller than
the bigger difference between different P or K).
While we would naturally prefer a text-only model
to be as expressive as a model that requires po-
tentially hard to acquire metadata, we tease apart
whether a distinction actually does exist by evalu-
ating the purity of the gold clusters with respect to
the labels assigned them.
5.4 Purity
For gold clusters G = {g1 . . . gk} and inferred
clusters C = {c1 . . . cj} we calculate purity as:
Purity = 1N
?
k
max
j
|gk ? cj | (7)
While purity cannot be used to compare models of
different persona size P , it can help us distinguish
between models of the same size. A model can
attain perfect purity, however, by placing all char-
acters into a single cluster; to control for this, we
present a controlled baseline in which each char-
acter is assigned a latent character type label pro-
portional to the size of the latent clusters we have
learned (so that, for example, if one latent per-
sona cluster contains 3.2% of the total characters,
357
Batman
Jim 
Gordon
dark, major, henchman
shoot, aim, overpower
sentence, arrest, assign
Tony 
Stark
Jason 
Bourne
The 
Joker
shoot, aim, overpower
testify, rebuff, confess
hatch, vow, undergo
Van Helsing
Colin 
Sullivan
Dracula
The Departed
The Dark 
Knight
Iron Man
The Bourne 
Identity
approve, die, suffer
relent, refuse, agree
inherit live imagine
Jack 
Dawson
Rachel
Titanic
Figure 3: Dramatis personae of The Dark Knight (2008), illustrating 3 of the 100 character types learned
by the persona regression model, along with links from other characters in those latent classes to other
movies. Each character type is listed with the top three latent topics with which it is associated.
the probability of selecting that persona at random
is 3.2%). Table 2 presents each model?s absolute
purity score paired with its improvement over its
controlled permutation (e.g., ?41%).
Within each fixed-size partition, the use of
metadata yields a substantial improvement over
the Dirichlet model, both in terms of absolute pu-
rity and in its relative improvement over its sized-
controlled baseline. In practice, we find that while
the Dirichlet model distinguishes between charac-
ter personas in different movies, the persona re-
gression model helps distinguish between differ-
ent personas within the same movie.
6 Exploratory Data Analysis
As with other generative approaches, latent per-
sona models enable exploratory data analysis. To
illustrate this, we present results from the persona
regression model learned above, with 50 latent
lexical classes and 100 latent personas. Figure 3
visualizes this data by focusing on a single movie,
The Dark Knight (2008); the movie?s protagonist,
Batman, belongs to the same latent persona as De-
tective Jim Gordon, as well as other action movie
protagonists Jason Bourne and Tony Stark (Iron
Man). The movie?s antagonist, The Joker, belongs
to the same latent persona as Dracula from Van
Helsing and Colin Sullivan from The Departed, il-
lustrating the ability of personas to be informed
by, but still cut across, different genres.
Table 3 presents an exhaustive list of all 50 top-
ics, along with an assigned label that consists of
the single word with the highest PMI for that class.
Of note are topics relating to romance (unite,
marry, woo, elope, court), commercial transac-
tions (purchase, sign, sell, owe, buy), and the clas-
sic criminal schema from Chambers (2011) (sen-
tence, arrest, assign, convict, promote).
Table 4 presents the most frequent 14 personas
in our dataset, illustrated with characters from
the 500 highest grossing movies. The personas
learned are each three separate mixtures of the
50 latent topics (one for agent relations, one for
patient relations, and one for attributes), as illus-
trated in figure 1 above. Rather than presenting
a 3 ? 50 histogram for each persona, we illus-
trate them by listing the most characteristic top-
ics, movie characters, and metadata features asso-
ciated with it. Characteristic actions and features
are defined as those having the highest smoothed
pointwise mutual information with that class; ex-
emplary characters are those with the highest pos-
terior probability of being drawn from that class.
Among the personas learned are canonical male
action heroes (exemplified by the protagonists of
The Bourne Supremacy, Speed, and Taken), super-
heroes (Hulk, Batman and Robin, Hector of Troy)
and several romantic comedy types, largely char-
acterized by words drawn from the FLIRT topic,
including flirt, reconcile, date, dance and forgive.
358
Label Most characteristic words Label Most characteristic words
UNITE unite marry woo elope court SWITCH switch confirm escort report instruct
PURCHASE purchase sign sell owe buy INFATUATE infatuate obsess acquaint revolve concern
SHOOT shoot aim overpower interrogate kill ALIEN alien child governor bandit priest
EXPLORE explore investigate uncover deduce CAPTURE capture corner transport imprison trap
WOMAN woman friend wife sister husband MAYA maya monster monk goon dragon
WITCH witch villager kid boy mom INHERIT inherit live imagine experience share
INVADE invade sail travel land explore TESTIFY testify rebuff confess admit deny
DEFEAT defeat destroy transform battle inject APPLY apply struggle earn graduate develop
CHASE chase scare hit punch eat EXPEL expel inspire humiliate bully grant
TALK talk tell reassure assure calm DIG dig take welcome sink revolve
POP pop lift crawl laugh shake COMMAND command abduct invade seize surrender
SING sing perform cast produce dance RELENT relent refuse agree insist hope
APPROVE approve die suffer forbid collapse EMBARK embark befriend enlist recall meet
WEREWOLF werewolf mother parent killer father MANIPULATE manipulate conclude investigate conduct
DINER diner grandfather brother terrorist ELOPE elope forget succumb pretend like
DECAPITATE decapitate bite impale strangle stalk FLEE flee escape swim hide manage
REPLY reply say mention answer shout BABY baby sheriff vampire knight spirit
DEMON demon narrator mayor duck crime BIND bind select belong refer represent
CONGRATULATE congratulate cheer thank recommend REJOIN rejoin fly recruit include disguise
INTRODUCE introduce bring mock read hatch DARK dark major henchman warrior sergeant
HATCH hatch don exist vow undergo SENTENCE sentence arrest assign convict promote
FLIRT flirt reconcile date dance forgive DISTURB disturb frighten confuse tease scare
ADOPT adopt raise bear punish feed RIP rip vanish crawl drive smash
FAIRY fairy kidnapper soul slave president INFILTRATE infiltrate deduce leap evade obtain
BUG bug zombie warden king princess SCREAM scream faint wake clean hear
Table 3: Latent topics learned for K = 50 and P = 100. The words shown for each class are those with
the highest smoothed PMI, with the label being the single word with the highest PMI.
Freq Actions Characters Features
0.109 DARKm, SHOOTa,
SHOOTp
Jason Bourne (The Bourne Supremacy), Jack Traven
(Speed), Jean-Claude (Taken)
Action, Male, War
film
0.079 CAPTUREp,
INFILTRATEa, FLEEa
Aang (The Last Airbender), Carly (Transformers: Dark of
the Moon), Susan Murphy/Ginormica (Monsters vs. Aliens)
Female, Action,
Adventure
0.067 DEFEATa, DEFEATp,
INFILTRATEa
Glenn Talbot (Hulk), Batman (Batman and Robin), Hector
(Troy)
Action, Animation,
Adventure
0.060 COMMANDa, DEFEATp,
CAPTUREp
Zoe Neville (I Am Legend), Ursula (The Little Mermaid),
Joker (Batman)
Action, Adventure,
Male
0.046 INFILTRATEa,
EXPLOREa, EMBARKa
Peter Parker (Spider-Man 3), Ethan Hunt (Mission:
Impossible), Jason Bourne (The Bourne Ultimatum)
Male, Action, Age
34-36
0.036 FLIRTa, FLIRTp,
TESTIFYa
Mark Darcy (Bridget Jones: The Edge of Reason), Jerry
Maguire (Jerry Maguire), Donna (Mamma Mia!)
Female, Romance
Film, Comedy
0.033 EMBARKa, INFILTRATEa,
INVADEa
Perseus (Wrath of the Titans), Maximus Decimus Meridius
(Gladiator), Julius (Twins)
Male, Chinese
Movies, Spy
0.027 CONGRATULATEa,
CONGRATULATEp,
SWITCHa
Professor Albus Dumbledore (Harry Potter and the
Philosopher?s Stone), Magic Mirror (Shrek), Josephine
Anwhistle (Lemony Snicket?s A Series of Unfortunate
Events)
Age 58+, Family
Film, Age 51-57
0.025 SWITCHa, SWITCHp,
MANIPULATEa
Clarice Starling (The Silence of the Lambs), Hannibal
Lecter (The Silence of the Lambs), Colonel Bagley (The
Last Samurai)
Age 58+, Male,
Age 45-50
0.022 REPLYa, TALKp, FLIRTp Graham (The Holiday), Abby Richter (The Ugly Truth),
Anna Scott (Notting Hill)
Female, Comedy,
Romance Film
0.020 EXPLOREa, EMBARKa,
CAPTUREp
Harry Potter (Harry Potter and the Philosopher?s Stone),
Harry Potter (Harry Potter and the Chamber of Secrets),
Captain Leo Davidson (Planet of the Apes)
Adventure, Family
Film, Horror
0.018 FAIRYm, COMMANDa,
CAPTUREp
Captain Jack Sparrow (Pirates of the Caribbean: At
World?s End), Shrek (Shrek), Shrek (Shrek Forever After)
Action, Family
Film, Animation
0.018 DECAPITATEa,
DECAPITATEp, RIPa
Jericho Cane (End of Days), Martin Riggs (Lethal Weapon
2), Gabriel Van Helsing (Van Helsing)
Horror, Slasher,
Teen
0.017 APPLYa, EXPELp,
PURCHASEp
Oscar (Shark Tale), Elizabeth Halsey (Bad Teacher), Dre
Parker (The Karate Kid)
Female, Teen,
Under Age 22
Table 4: Of 100 latent personas learned, we present the top 14 by frequency. Actions index the latent
topic classes presented in table 3; subscripts denote whether the character is predominantly the agent (a),
patient (p) or is modified by an attribute (m).
359
7 Conclusion
We present a method for automatically inferring
latent character personas from text (and metadata,
when available). While our testbed has been tex-
tual synopses of film, this approach is easily ex-
tended to other genres (such as novelistic fiction)
and to non-fictional domains as well, where the
choice of portraying a real-life person as embody-
ing a particular kind of persona may, for instance,
give insight into questions of media framing and
bias in newswire; self-presentation of individual
personas likewise has a long history in communi-
cation theory (Goffman, 1959) and may be use-
ful for inferring user types for personalization sys-
tems (El-Arini et al, 2012). While the goal of this
work has been to induce a set of latent character
classes and partition all characters among them,
one interesting question that remains is how a spe-
cific character?s actions may informatively be at
odds with their inferred persona, given the choice
of that persona as the single best fit to explain the
actions we observe. By examining how any indi-
vidual character deviates from the behavior indica-
tive of their type, we might be able to paint a more
nuanced picture of how a character can embody a
specific persona while resisting it at the same time.
Acknowledgments
We thank Megan Morrison at the CMU School of
Drama for early conversations guiding our work,
as well as the anonymous reviewers for helpful
comments. The research reported in this article
was supported by U.S. National Science Founda-
tion grant IIS-0915187 and by an ARCS scholar-
ship to D.B. This work was made possible through
the use of computing resources made available by
the Pittsburgh Supercomputing Center.
References
Aristotle. 335 BCE. Poetics, translated by Samuel H.
Butcher (1902). Macmillan, London.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. JMLR, 3:993?1022.
Joseph Campbell. 1949. The Hero with a Thousand
Faces. Pantheon Books.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the 47th Annual Meet-
ing of the ACL.
Nathanael Chambers. 2011. Inducing Event Schemas
and their Participants from Unlabeled Text. Ph.D.
thesis, Stanford University.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.
Lajos Egri. 1946. The Art of Dramatic Writing. Simon
and Schuster, New York.
Khalid El-Arini, Ulrich Paquet, Ralf Herbrich, Jurgen
Van Gael, and Blaise Agu?era y Arcas. 2012. Trans-
parent user models for personalization. In Proceed-
ings of the 18th ACM SIGKDD.
Micha Elsner. 2012. Character-based kernels for nov-
elistic plot structure. In Proceedings of the 13th
Conference of the EACL.
Mark Alan Finlayson. 2011. Learning Narrative
Structure from Annotated Folktales. Ph.D. thesis,
MIT.
R. A. Fisher. 1935. The Design of Experiments. Oliver
and Boyde, Edinburgh and London.
Erving Goffman. 1959. The Presentation of the Self in
Everyday Life. Anchor.
Amit Goyal, Ellen Riloff, and Hal Daume?, III. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of the 2010 Con-
ference on EMNLP.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on EMNLP.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228?5235.
Carl Jung. 1981. The Archetypes and The Collective
Unconscious, volume 9 of Collected Works. Bollin-
gen, Princeton, NJ, 2nd edition.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation. In
Proceedings of the 48th Annual Meeting of the ACL.
Association for Computational Linguistics.
Robert McKee. 1997. Story: Substance, Structure,
Style and the Principles of Screenwriting. Harper-
Colllins.
Marina Meila?. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
David Mimno and Andrew McCallum. 2008. Topic
models conditioned on arbitrary features with
dirichlet-multinomial regression. In Proceedings of
UAI.
360
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the 31st Annual Meeting of the ACL.
E. J. G. Pitman. 1937. Significance tests which may
be applied to samples from any population. Supple-
ment to the Journal of the Royal Statistical Society,
4(1):119?130.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the ACL.
Michaela Regneri, Alexander Koller, Josef Ruppen-
hofer, and Manfred Pinkal. 2011. Learning script
participants from unlabeled data. In Proceedings of
the Conference on Recent Advances in Natural Lan-
guage Processing.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals, and understanding: An inquiry into
human knowledge structures. Lawrence Erlbaum,
Hillsdale, NJ.
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
EACL.
Greg C. G. Wei and Martin A. Tanner. 1990. A Monte
Carlo implementation of the EM algorithm and the
poor man?s data augmentation algorithms. Journal
of the American Statistical Association, 85:699?704.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
Conference on EMNLP.
361
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1094?1104,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning to Extract International Relations from Political Context
Brendan O?Connor
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
brenocon@cs.cmu.edu
Brandon M. Stewart
Department of Government
Harvard University
Cambridge, MA 02139, USA
bstewart@fas.harvard.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We describe a new probabilistic model
for extracting events between major polit-
ical actors from news corpora. Our un-
supervised model brings together famil-
iar components in natural language pro-
cessing (like parsers and topic models)
with contextual political information?
temporal and dyad dependence?to in-
fer latent event classes. We quantita-
tively evaluate the model?s performance
on political science benchmarks: recover-
ing expert-assigned event class valences,
and detecting real-world conflict. We also
conduct a small case study based on our
model?s inferences.
A supplementary appendix, and replica-
tion software/data are available online, at:
http://brenocon.com/irevents
1 Introduction
The digitization of large news corpora has pro-
vided an unparalleled opportunity for the system-
atic study of international relations. Since the mid-
1960s political scientists have used political events
data, records of public micro-level interactions be-
tween major political actors of the form ?someone
does something to someone else? as reported in
the open press (Schrodt, 2012), to study the pat-
terns of interactions between political actors and
how they evolve over time. Scaling this data effort
to modern corpora presents an information extrac-
tion challenge: can a structured collection of ac-
curate, politically relevant events between major
political actors be extracted automatically and ef-
ficiently? And can they be grouped into meaning-
ful event types with a low-dimensional structure
useful for further analysis?
We present an unsupervised approach to event
extraction, in which political structure and linguis-
tic evidence are combined. A political context
model of the relationship between a pair of polit-
ical actors imposes a prior distribution over types
of linguistic events. Our probabilistic model in-
fers latent frames, each a distribution over textual
expressions of a kind of event, as well as a repre-
sentation of the relationship between each political
actor pair at each point in time. We use syntactic
preprocessing and a logistic normal topic model,
including latent temporal smoothing on the politi-
cal context prior.
We apply the model in a series of compar-
isons to benchmark datasets in political science.
First, we compare the automatically learned verb
classes to a pre-existing ontology and hand-crafted
verb patterns from TABARI,1 an open-source and
widely used rule-based event extraction system for
this domain. Second, we demonstrate correlation
to a database of real-world international conflict
events, the Militarized Interstate Dispute (MID)
dataset (Jones et al, 1996). Third, we qualitatively
examine a prominent case not included in the MID
dataset, Israeli-Palestinian relations, and compare
the recovered trends to the historical record.
We outline the data used for event discovery
(?2), describe our model (?3), inference (?4), eval-
uation (?5), and comment on related work (?6).
2 Data
The model we describe in ?3 is learned from a
corpus of 6.5 million newswire articles from the
English Gigaword 4th edition (1994?2008, Parker
et al, 2009). We also supplement it with a sam-
ple of data from the New York Times Annotated
Corpus (1987?2007, Sandhaus, 2008).2 The Stan-
1Available from the Penn State Event Data Project:
http://eventdata.psu.edu/
2For arbitrary reasons this portion of the data is much
smaller (we only parse the first five sentences of each arti-
cle, while Gigaword has all sentences parsed), resulting in
less than 2% as many tuples as from the Gigaword data.
1094
ford CoreNLP system,3 under default settings, was
used to POS-tag and parse the articles, to eventu-
ally produce event tuples of the form
?s, r, t, wpredpath?
where s and r denote ?source? and ?receiver? ar-
guments, which are political actor entities in a pre-
defined set E , t is a timestep (i.e., a 7-day pe-
riod) derived from the article?s published date, and
wpredpath is a textual predicate expressed as a de-
pendency path that typically includes a verb (we
use the terms ?predicate-path? and ?verb-path? in-
terchangeably). For example, on January 1, 2000,
the AP reported ?Pakistan promptly accused In-
dia,? from which our preprocessing extracts the tu-
ple ?PAK, IND, 678, accuse dobj??? ?. (The path ex-
cludes the first source-side arc.) Entities and verb
paths are identified through the following sets of
rules.
Named entity recognition and resolution is done
deterministically by finding instances of country
names from the CountryInfo.txt dictionary from
TABARI,4 which contains proper noun and adjec-
tival forms for countries and administrative units.
We supplement these with a few entries for in-
ternational organizations from another dictionary
provided by the same project, and clean up a few
ambiguous names, resulting in a final actor dictio-
nary of 235 entities and 2,500 names.
Whenever a name is found, we identify its en-
tity?s mention as the minimal noun phrase that
contains it; if the name is an adjectival or noun-
noun compound modifier, we traverse any such
amod and nn dependencies to the noun phrase
head. Thus NATO bombing, British view, and
Palestinian militant resolve to the entity codes IG-
ONAT, GBR, and PSE respectively.
We are interested in identifying actions initi-
ated by agents of one country targeted towards an-
other, and hence concentrate on verbs, analyzing
the ?CCprocessed? version of the Stanford Depen-
dencies (de Marneffe and Manning, 2008). Verb
paths are identified by looking at the shortest de-
pendency path between two mentions in a sen-
tence. If one of the mentions is immediately dom-
inated by a nsubj or agent relation, we consider
that the Source actor, and the other mention is the
Receiver. The most common cases are simple di-
rect objects and prepositional arguments like talk
3http://nlp.stanford.edu/software/
corenlp.shtml
4http://eventdata.psu.edu/software.
dir/dictionaries.html.
prep with????? and fight prep alongside??????? (?talk with R,? ?fight
alongside R?) but many interesting multiword con-
structions also result, such as reject dobj??? allegation
poss??? (?rejected R?s allegation?) or verb chains as
in offer xcomp??? help dobj??? (?offer to help R?).
We wish to focus on instances of directly re-
ported events, so attempt to remove factively com-
plicated cases such as indirect reporting and hy-
potheticals by discarding all predicate paths for
which any verb on the path has an off-path gov-
erning verb with a non-conj relation. (For exam-
ple, the verb at the root of a sentence always sur-
vives this filter.) Without this filter, the ?s, r, w?
tuple ?USA, CUB, want xcomp??? seize dobj??? ? is ex-
tracted from the sentence ?Parliament Speaker Ri-
cardo Alarcon said the United States wants to seize
Cuba and take over its lands?; the filter removes
it since wants is dominated by an off-path verb
through say ccomp??? wants. The filter was iteratively
developed by inspecting dozens of output exam-
ples and their labelings under successive changes
to the rules.
Finally, only paths length 4 or less are allowed,
the final dependency relation for the receiver may
not be nsubj or agent, and the path may not contain
any of the dependency relations conj, parataxis,
det, or dep. We use lemmatized word forms in
defining the paths.
Several document filters are applied before tu-
ple extraction. Deduplication removes 8.5% of ar-
ticles.5 For topic filtering, we apply a series of
keyword filters to remove sports and finance news,
and also apply a text classifier for diplomatic and
military news, trained on several hundred man-
ually labeled news articles (using `1-regularized
logistic regression with unigram and bigram fea-
tures). Other filters remove non-textual junk and
non-standard punctuation likely to cause parse er-
rors.
For experiments we remove tuples where the
source and receiver entities are the same, and re-
strict to tuples with dyads that occur at least 500
times, and predicate paths that occur at least 10
times. This yields 365,623 event tuples from
235,830 documents, for 421 dyads and 10,457
unique predicate paths. We define timesteps
to be 7-day periods, resulting in 1,149 discrete
5We use a simple form of shingling (ch. 3, Rajaraman and
Ullman, 2011): represent a document signature as its J = 5
lowercased bigrams with the lowest hash values, and reject a
document with a signature that has been seen before within
the same month. J was manually tuned, as it affects the pre-
cision/recall tradeoff.
1095
ik
?k,s,r,t
?s,r,t
z
 
wpredpath
r
L
a
n
g
u
a
g
e
 
M
o
d
e
l
P
(
T
e
x
t
 
 
|
 
 
E
v
e
n
t
 
T
y
p
e
)
C
o
n
t
e
x
t
 
M
o
d
e
l
P
(
E
v
e
n
t
 
T
y
p
e
 
 
|
 
 
C
o
n
t
e
x
t
)
b
 2k
ss "Source"
    entity
r "Receiver"
    entity
t  Timestep
i  Event tuple
k  Frame
kk
 k,s,r,t 1  k,s,r,t
?2
?k
Figure 1: Directed probabilistic diagram of the model for one
(s, r, t) dyad-time context, for the smoothed model.
timesteps (1987 through 2008, though the vast ma-
jority of data starts in 1994).
3 Model
We design two models to learn linguistic event
classes over predicate paths by conditioning on
real-world contextual information about interna-
tional politics, p(wpredpath | s, r, t), leveraging the
fact there tends to be dyadic and temporal coher-
ence in international relations: the types of actions
that are likely to occur between nations tend to be
similar within the same dyad, and usually their dis-
tribution changes smoothly over time.
Our model decomposes into two submodels:
a Context submodel, which encodes how politi-
cal context affects the probability distribution over
event types, and a Language submodel, for how
those events are manifested as textual predicate
paths (Figure 1). The overall generative process is
as follows. We color global parameters for a frame
blue, and local context parameters red, and use
the term ?frame? as a synonym for ?event type.?
The fixed hyperparameter K denotes the number
of frames.
? The context model generates a frame prior ?s,r,t
for every context (s, r, t).
? Language model:
? Draw lexical sparsity parameter b from a dif-
fuse prior (see ?4).
? For each frame k, draw a multinomial distri-
bution of dependency paths, ?k ? Dir(b/V )
(where V is the number of dependency path
types).
? For each (s, r, t), for every event tuple i in
that context,
? Sample its frame z(i) ? Mult(?s,r,t).
? Sample its predicate realization
w(i)predpath ? Mult(?z(i)).
Thus the language model is very similar to a topic
model?s generation of token topics and wordtypes.
We use structured logistic normal distributions
to represent contextual effects. The simplest is the
vanilla (V) context model,
? For each frame k, draw global parameters from
diffuse priors: prevalence ?k and variability ?2k.
? For each (s, r, t),
? Draw ?k,s,r,t ? N(?k, ?2k) for each frame k.
? Apply a softmax transform,
?k,s,r,t =
exp ?k,s,r,t?K
k?=1 exp ?k?,s,r,t
Thus the vector ??,s,r,t encodes the relative log-
odds of the different frames for events appearing
in the context (s, r, t). This simple logistic nor-
mal prior is, in terms of topic models, analogous
to the asymmetric Dirichlet prior version of LDA
in Wallach et al (2009), since the ?k parameter
can learn that some frames tend to be more likely
than others. The variance parameters ?2k controladmixture sparsity, and are analogous to a Dirich-
let?s concentration parameter.
Smoothing Frames Across Time
The vanilla model is capable of inducing frames
through dependency path co-occurences, when
multiple events occur in a given context. How-
ever, many dyad-time slices are very sparse; for
example, most dyads (all but 18) have events in
fewer than half the time slices in the dataset. One
solution is to increase the bucket size (e.g., to
months); however, previous work in political sci-
ence has demonstrated that answering questions
of interest about reciprocity dynamics requires re-
covering the events at weekly or even daily gran-
ularity (Shellman, 2004), and in any case wide
buckets help only so much for dyads with fewer
events or less media attention. Therefore we pro-
pose a smoothed frames (SF) model, in which the
1096
frame distribution for a given dyad comes from a
latent parameter ??,s,r,t that smoothly varies over
time. For each (s, r), draw the first timestep?s val-
ues as ?k,s,r,1 ? N(0, 100), and for each context
(s, r, t > 1),
? Draw ?k,s,r,t ? N(?k,s,r,t?1, ?2)
? Draw ?k,s,r,t ? N(?k + ?k,s,r,t, ?2k)
Other parameters (?k, ?2k) are same as the vanillamodel. This model assumes a random walk pro-
cess on ?, a variable which exists even for contexts
that contain no events. Thus inferences about ?
will be smoothed according to event data at nearby
timesteps. This is an instance of a linear Gaussian
state-space model (also known as a linear dynami-
cal system or dynamic linear model), and is a con-
venient formulation because it has well-known ex-
act inference algorithms. Dynamic linear models
have been used elsewhere in machine learning and
political science to allow latent topic frequencies
(Blei and Lafferty, 2006; Quinn et al, 2010) and
ideological positions (Martin and Quinn, 2002) to
smoothly change over time, and thus share statis-
tical strength between timesteps.
4 Inference
After randomly initializing all ?k,s,r,t, inference is
performed by a blocked Gibbs sampler, alternat-
ing resamplings for three major groups of vari-
ables: the language model (z,?), context model
(?, ?, ?, p), and the ?, ? variables, which bottle-
neck between the submodels.
The language model sampler sequentially up-
dates every z(i) (and implicitly ? via collapsing)
in the manner of Griffiths and Steyvers (2004):
p(z(i)|?, w(i), b) ? ?s,r,t,z(nw,z + b/V )/(nz + b),
where counts n are for all event tuples besides i.
For the context model, ? is conjugate resam-
pled as a normal mean. The random walk vari-
ables ? are sampled with the forward-filtering-
backward-sampling algorithm (FFBS; Harrison
and West, 1997; Carter and Kohn, 1994); there is
one slight modification of the standard dynamic
linear model that the zero-count weeks have no ?
observation; the Kalman filter implementation is
appropriately modified to handle this.
The ? update step is challenging since it is a
nonconjugate prior to the z counts. Logistic nor-
mal distributions were introduced to text mod-
eling by Blei and Lafferty (2007), who devel-
oped a variational approximation; however, we
find that experimenting with different models is
easier in the Gibbs sampling framework. While
Gibbs sampling for logistic normal priors is pos-
sible using auxiliary variable methods (Mimno
et al, 2008; Holmes and Held, 2006; Polson et al,
2012), it can be slow to converge. We opt for
the more computationally efficient approach of
Zeger and Karim (1991) and Hoff (2003), using
a Laplace approximation to p(? | ??,?, z), which
is a mode-centered Gaussian having inverse co-
variance equal to the unnormalized log-posterior?s
negative Hessian (?8.4 in Murphy, 2012). We find
the mode with the linear-time Newton algorithm
from Eisenstein et al (2011), and sample in linear
time by only using the Hessian?s diagonal as the
inverse covariance (i.e., an axis-aligned normal),
since a full multivariate normal sample requires
a cubic-time-to-compute Cholesky root of the co-
variance matrix. This ?? sample is a proposal for a
Metropolis-within-Gibbs step, which is moved to
according to the standard Metropolis-Hastings ac-
ceptance rule. Acceptance rates differ by K, rang-
ing approximately from 30% (K = 100) to nearly
100% (small K).
Finally, we use diffuse priors on all global pa-
rameters, conjugate resampling variances ?2, ?k
once per iteration, and slice sampling (Neal, 2003)
the Dirichlet concentration b every 100 iterations.
Automatically learning these was extremely con-
venient for model-fitting; the only hyperparameter
we set manually wasK. It also allowed us to mon-
itor the convergence of dispersion parameters to
help debug and assess MCMC mixing. For other
modeling and implementation details, see the on-
line appendix and software.
5 Experiments
We fit the two models on the dataset described in
?2, varying the number of frames K, with 8 or
more separate runs for each setting. Posteriors are
saved and averaged from 11 Gibbs samples (every
100 iterations from 9,000 to 10,000) for analysis.
We present intrinsic (?5.1) and extrinsic (?5.2)
quantitative evaluations, and a qualitative case
study (?5.4).
5.1 Lexical Scale Impurity
In the international relations literature, much of
the analysis of text-based events data makes use of
a unidimensional conflict to cooperation scale. A
popular event ontology in this domain, CAMEO,
consists of around 300 different event types, each
1097
given an expert-assigned scale in the range from
?10 to +10 (Gerner et al, 2002), derived from
a judgement collection experiment in Goldstein
(1992). The TABARI pattern-based event extrac-
tion program comes with a list of almost 16,000
manually engineered verb patterns, each assigned
to one CAMEO event type.
It is interesting to consider the extent to which
our unsupervised model is able to recover the
expert-designed ontology. Given that many of
the categories are very fine-grained (e.g. ?Express
intent to de-escalate military engagement?), we
elect to measure model quality as lexical scale pu-
rity: whether all the predicate paths within one
automatically learned frame tend to have similar
gold-standard scale scores. (This measures clus-
ter cohesiveness against a one-dimensional con-
tinuous scale, instead of measuring cluster cohe-
siveness against a gold-standard clustering as in
VI, Rand index, or purity.) To calculate this, we
construct a mapping between our corpus-derived
verb path vocabulary and the TABARI verb pat-
terns, many of which contain one to several word
stems that are intended to be matched in surface
order. Many of our dependency paths, when tra-
versed from the source to receiver direction, also
follow surface order, due to English?s SVO word
order.6 Therefore we convert each path to a
word sequence and match against the TABARI
lexicon?plus a few modifications for differences
in infinitives and stemming?and find 528 depen-
dency path matches. We assign each path w a
gold-standard scale g(w) by resolving through its
matching pattern?s CAMEO code.
We formalize lexical scale impurity as the av-
erage absolute difference of scale values between
two predicate paths under the same frame. Specif-
ically, we want a token-level posterior expectation
E(|g(wi)? g(wj)| | zi = zj , wi 6= wj) (1)
which is taken over pairs of path instances (i, j)
where both paths wi, wj are in M , the set of verb
paths that were matched between the lexicons.
This can be reformulated at the type level as:7
1
N
?
k
?
w,v?M
w 6=v
nw,k nv,k |g(w)? g(v)| (2)
6There are plenty of exceptions where a Source-to-
Receiver path traversal can have a right-to-left move, such
as dependency edges for posessives. This approach can not
match them.
7Derivation in supplementary appendix.
where n refers to the averaged Gibbs samples?
counts of event tuples having frame k and a par-
ticular verb path,8 and N is the number of to-
ken comparisons (i.e. the same sum, but with a
1 replacing the distance). The worst possible im-
purity is upper bounded at 20 (= max(g(w)) ?
min(g(w))) and the best possible is 0. We also
compute a randomized null hypothesis to see how
low impurity can be by chance: each of ?1000
simulations randomly assigns each path in M to
one of K frames (all its instances are exclusively
assigned to that frame), and computes the impu-
rity. On average the impurity is same at all K,
but variance increases with K (since small clus-
ters might by chance get a highly similar paths in
them), necessitating this null hypothesis analysis.
We report the 5th percentile over simulations.
5.2 Conflict Detection
Political events data has shown considerable
promise as a tool for crisis early warning systems
(O?Brien, 2010; Brandt et al, 2011). While con-
flict forecasting is a potential application of our
model, we conduct a simpler prediction task to
validate whether the model is learning something
useful: based on news text, tell whether or not an
armed conflict is currently happening. For a gold
standard, we use the Militarized Interstate Dispute
(MID) dataset (Jones et al, 1996; Ghosn et al,
2004), which documents historical international
disputes. While not without critics, the MID data
is the most prominent dataset in the field of in-
ternational relations. We use the Dyadic MIDs,
each of which ranks hostility levels between pairs
of actors on a five point scale over a date inter-
val; we define conflict to be the top two categories
?Use of Force? (4) and ?War? (5). We convert
the data into a variable ys,r,t, the highest hostility
level reached by actor s directed towards receiver
r in the dispute that overlaps with our 7-day in-
terval t, and want to predict the binary indicator
1{ys,r,t ? 4}. For the illustrative examples (USA
to Iraq, and the Israel-Palestine example below)
we use results from a smaller but more internally
comparable dataset consisting of the 2 million As-
sociated Press articles within the Gigaword cor-
pus.
For an example of the MID data, see Figure 2,
which depicts three disputes between the US and
8Results are nearly identical whether we use counts av-
eraged across samples (thus giving posterior marginals),
or simply use counts from a single sample (i.e., iteration
10,000).
1098
kill, fire at, 
seal, invade, 
enter
accuse, 
criticize, warn, 
reject, urge
accuse, 
reject, blame, 
kill, take
criticize, call, 
ask, condemn, 
denounce
USA to Iraq (Vanilla Model)
0.
0
0.
4
0.
8
1995 1996 1997 1998 1999 2000 2001 2002
USA to Iraq (Smoothed Frames)
0.
0
0.
4
0.
8
1995 1996 1997 1998 1999 2000 2001 2002
Figure 2: The USA?Iraq directed dyad, analyzed by
smoothed (above) and vanilla (below) models, showing (1)
gold-standard MID values (red intervals along top), (2) weeks
with non-zero event counts (vertical lines along x-axis), (3)
posterior E[?k,USA,IRQ,t] inferences for two frames chosen
from two different K = 5 models, and (4) most common
verb paths for each frame (right). Frames corresponding to
material and verbal conflict were chosen for display. Vertical
line indicates Operation Desert Fox (see ?5.2).
Iraq in this time period. The MID labels are
marked in red.
The first dispute is a ?display of force? (level
3), cataloguing the U.S. response to a series of
troop movements along the border with Kuwait.
The third dispute (10/7/1997 to 10/10/2001) be-
gins with increasing Iraqi violations of the no-
fly zone, resulting in U.S. and U.K. retaliation,
reaching a high intensity with Operation Desert
Fox, a four-day bombing campaign from Decem-
ber 16 to 19, 1998?which is not shown in MID.
These cases highlight MID?s limitations?while it
is well regarded in the political science literature,
its coarse level of aggregation can fail to capture
variation in conflict intensity.
Figure 2 also shows model inferences. Our
smoothed model captures some of these phenom-
ena here, showing clear trends for two relevant
frames, including a dramatic change in Decem-
ber 1998. The vanilla model has a harder time,
since it cannot combine evidence between differ-
ent timesteps.
The MID dataset overlaps with our data for 470
weeks, from 1993 through 2001. After excluding
dyads with actors that the MID data does not in-
tend to include?Kosovo, Tibet, Palestine, and in-
ternational organizations?we have 267 directed
dyads for evaluation, 117 of which have at least
one dispute in the MID data. (Dyads with no dis-
pute in the MID data, such as Germany-France,
are assumed to have y = 0 throughout the time
period.) About 7% of the dyad-time contexts have
a dispute under these definitions.
We split the dataset by time, training on the first
half of the data and testing on the second half, and
measure area under the receiver operating charac-
teristic curve (AUC).9 For each model, we train an
`1-regularized logistic regression10 with the K el-
ements of ??,s,r,t as input features, tuning the reg-
ularization parameter within the training set (by
splitting it in half again) to optimize held-out like-
lihood. We weight instances to balance positive
and negative examples. Training is on all individ-
ual ? samples at once (thus accounting for pos-
terior uncertainty in learning), and final predicted
probabilities are averaged from individual proba-
bilities from each ? test set sample, thus propa-
gating posterior uncertainty into the predictions.
We also create a baseline `1-regularized logistic
regression that uses normalized dependency path
counts as the features (10,457 features). For both
the baseline and vanilla model, contexts with no
events are given a feature vector of all zeros.11
(We also explored an alternative evaluation setup,
to hold out by dyad; however, the performance
variance is quite high between different random
dyad splits.)
5.3 Results
Results are shown in Figure 3.12
The verb-path logistic regression performs
strongly at AUC 0.62; it outperforms all of
the vanilla frame models. This is an exam-
ple of individual lexical features outperforming a
topic model for predictive task, because the topic
model?s dimension reduction obscures important
indicators from individual words. Similarly, Ger-
rish and Blei (2011) found that word-based regres-
sion outperformed a customized topic model when
predicting Congressional bill passage, and Eisen-
9AUC can be interpreted as follows: given a positive and
negative example, what is the probability that the classifier?s
confidences order them correctly? Random noise or predict-
ing all the same class both give AUC 0.5.
10Using the R glmnet package (Friedman et al, 2010).
11For the vanilla model, this performed better than linear
interpolation (about 0.03 AUC), and with less variance be-
tween runs.
12Due to an implementation bug, the model put the vast
majority of the probability mass only on K ? 1 frames,
so these settings might be better thought of as K =
1, 2, 3, 4, 9, . . .; see the appendix for details.
1099
l l
0.4
0.5
0.6
0.7
2 3 4 5 10 20 50 100Number of frames (K)
Co
nfli
ct p
red
icti
on 
AU
C
(hig
her 
is b
ette
r)
model
l Log. Reg
Vanilla
Smoothed
l l l l
l
l
l
l
1.5
2.5
3.5
4.5
5.5
2 3 4 5 10 20 50 100Number of frames (K)
Sca
le i
mp
urit
y
(low
er
 is 
bet
ter
)
model
l Null
Vanilla
Smoothed
Figure 3: Evaluation results. Each point indicates one model
run. Lines show the average per K, with vertical lines indi-
cating the 95% bootstrapped interval. Top: Conflict detection
AUC for different models (?5.2). Green line is the verb-path
logistic regression baseline. Bottom: Lexical scale impurity
(?5.1). Top green line indicates the simple random baseline
E(|g(wi) ? g(wj)|) = 5.33; the second green line is from
the random assignment baseline.
stein et al (2010) found word-based regression
outperformed Supervised LDA for geolocation,13
and we have noticed this phenomenon for other
text-based prediction problems.
However, adding smoothing to the model sub-
stantially increases performance, and in fact out-
performs the verb-path regression at K = 100.
It is unclear why the vanilla model fails to in-
crease performance in K. Note also, the vanilla
model exhibits very little variability in prediction
performance between model runs, in comparison
to the smoothed model which is much more vari-
able (presumably due to the higher number of pa-
rameters in the model); at small values of K, the
smoothed model can perform poorly. It would also
be interesting to analyze the smoothed model with
higher values of K and find where it peaks.
We view the conflict detection task only as one
of several validations, and thus turn to lexical eval-
uation of the induced frames. For lexical scale
purity (bottom of Figure 3), the models perform
about the same, with the smoothed model a lit-
tle bit worse at some values of K (though some-
times with better stability of the fits?opposite of
the conflict detection task). This suggests that se-
mantic coherence does not benefit from the longer-
13In the latter, a problem-specific topic model did best.
range temporal dependencies.
In general, performance improves with higher
K, but not beyond K = 50. This suggests the
model reaches a limit for how fine-grained of se-
mantics it can learn.
5.4 Case study
Here we qualitatively examine the narrative story
between the dyad with the highest frequency of
events in our dataset, the Israeli-Palestinian rela-
tionship, finding qualitative agreement with other
case studies of this conflict (Brandt et al, 2012;
Goldstein et al, 2001; Schrodt and Gerner, 2004).
(The MID dataset does not include this conflict be-
cause the Palestinians are not considered a state
actor.) Using the Associated Press subset, we plot
the highest incidence frames from one run of the
K = 20 smoothed frame models, for the two di-
rected dyads, and highlight some of the interesting
relationships.
Figure 4(a) shows that tradeoffs in the use of
military vs. police action by Israel towards the
Palestinians tracks with major historical events.
The first period in the data where police actions
(?impose, seal, capture, seize, arrest?) exceed mil-
itary actions (?kill, fire, enter, attack, raid?) is
with the signing of the ?Interim Agreement on the
West Bank and the Gaza Strip,? also known as the
Oslo II agreement. This balance persists until the
abrupt breakdown in relations that followed the
unsuccessful Camp David Summit in July of 2000,
which generally marks the starting point of the
wave of violence known as the Second Intifada.
In Figure 4(b) we show that our model produces
a frame which captures the legal aftermath of par-
ticular events (?accuse, criticize,? but also ?detain,
release, extradite, charge?). Each of the major
spikes in the data coincides with a particular event
which either involves the investigation of a par-
ticular attack or series of attacks (as in A,B,E) or
a discussion about prisoner swaps or mass arrests
(as in events D, F, J).
Our model also picks up positive diplomatic
events, as seen in Figure 4(c), a frame describ-
ing Israeli diplomatic actions towards Palestine
(?meet with, sign with, praise, say with, arrive
in?). Not only do the spikes coincide with major
peace treaties and negotiations, but the model cor-
rectly characterizes the relative lack of positively
valenced action from the beginning of the Second
Intifada until its end around 2005?2006.
In Figure 4(d) we show the relevant frames de-
1100
a.
0.0
0.4
0.8
Israeli Use of Force Tradeoff
1994 1997 2000 2002 2005 2007
Second Intifada BeginsOslo II Signed
b.
0.
0
0.
4
0.
8
1.
2
Police Actions and Crime Response
A
B
C
D
E
F
G
H I J
1994 1997 2000 2002 2005 2007
A: Series of Suicide Attacks 
in Jerusalem
B: Island of Peace Massacre
C: Arrests over Protests
D: Tensions over Treatment 
of Pal. Prisoners
E: Passover Massacre
F: 400-Person Prisoner Swap
G: Gaza Street Bus Bombing
H: Stage Club Bombing
I: House to House Sweep for 7 
militant leaders
J: Major Prisoner Release
c.
0.
0
0.
4
0.
8
Israeli?Palestinian Diplomacy
A B C D E F
1994 1997 2000 2002 2005 2007
C: U.S. Calls for West Bank 
Withdrawal
D: Deadlines for Wye River Peace 
Accord
E: Negotiations in Mecca
F: Annapolis Conference
A: Israel-Jordan Peace 
Treaty
B: Hebron Protocol
d.
0.0
0.4
0.8
Palestinian Use of Force
1994 1997 2000 2002 2005 2007
Figure 4: For Israel-Palestinian directed dyads, plots ofE[?] (proportion of weekly events in a frame) over time, annotated with
historical events. (a): Words are ?kill, fire at, enter, kill, attack, raid, strike, move, pound, bomb? and ?impose, seal, capture,
seize, arrest, ease, close, deport, close, release? (b): ?accuse, criticize, reject, tell, hand to, warn, ask, detain, release, order? (c):
?meet with, sign with, praise, say with, arrive in, host, tell, welcome, join, thank? (d): again the same ?kill, fire at? frame in (a),
plus the erroneous frame (see text) ?include, join, fly to, have relation with, protest to, call, include bomber appos???? informer
for?. Figures (b) and (c) use linear interpolation for zero-count weeks (thus relying exclusively on the model for smoothing);
(a) and (d) apply a lowess smoother. (a-c) are for the ISR?PSE direction; (d) is PSE?ISR.
picting use of force from the Palestinians towards
the Israelis (brown trend line). At first, the drop
in the use of force frame immediately following
the start of the Second Intifada seems inconsis-
tent with the historical record. However, there is a
concucrrent rise in a different frame driven by the
word ?include?, which actually appears here due
to an NLP error compounded with an artifact of
the data source. A casualties report article, con-
taining variants of the text ?The Palestinian fig-
ure includes... 13 Israeli Arabs...?, is repeated 27
times over two years. ?Palestinian figure? is er-
roneously identified as the PSE entity, and several
noun phrases in a list are identified as separate re-
ceivers. This issue causes 39 of all 86 PSE?ISR
events during this period to use the word ?in-
clude?, accounting for the rise in that frame. (This
highlights how better natural language processing
could help the model, and the dangers of false
positives for this type of data analysis, especially
in small-sample drilldowns.) Discounting this er-
roneous inference, the results are consistent with
heightened violence during this period.
We conclude the frame extractions for the
Israeli-Palestinian case are consistent with the his-
torical record over the period of study.
6 Related Work
6.1 Events Data in Political Science
Projects using hand-collected events data repre-
sent some of the earliest efforts in the statisti-
cal study of international relations, dating back to
the 1960s (Rummel, 1968; Azar and Sloan, 1975;
McClelland, 1970). Beginning in the mid-1980s,
political scientists began experimenting with au-
tomated rule-based extraction systems (Schrodt
and Gerner, 1994). These efforts culminated in
the open-source program, TABARI, which uses
pattern matching from extensive hand-developed
phrase dictionaries, combined with basic part of
speech tagging (Schrodt, 2001); a rough analogue
in the information extraction literature might be
the rule-based, finite-state FASTUS system for
MUC IE (Hobbs et al, 1997), though TABARI is
restricted to single sentence analysis. Later pro-
prietary work has apparently incorporated more
extensive NLP (e.g., sentence parsing) though
few details are available (King and Lowe, 2003).
The most recent published work we know of, by
Boschee et al (2013), uses a proprietary parsing
and coreference system (BBN SERIF, Ramshaw
et al, 2011), and directly compares to TABARI,
finding significantly higher accuracy. The origi-
1101
nal TABARI system is still actively being devel-
oped, including just-released work on a new 200
million event dataset, GDELT (Schrodt and Lee-
taru, 2013).14 All these systems crucially rely on
hand-built pattern dictionaries.
It is extremely labor intensive to develop these
dictionaries. Schrodt (2006) estimates 4,000
trained person-hours were required to create dic-
tionaries of political actors in the Middle East, and
the phrase dictionary took dramatically longer; the
comments in TABARI?s phrase dictionary indicate
some of its 15,789 entries were created as early as
1991. Ideally, any new events data solution would
incorporate the extensive work already completed
by political scientists in this area while minimiz-
ing the need for further dictionary development. In
this work we use the actor dictionaries, and hope
to incorporate the verb patterns in future work.
6.2 Events in Natural Language Processing
Political event extraction from news has also re-
ceived considerable attention within natural lan-
guage processing in part due to government-
funded challenges such as MUC-3 and MUC-4
(Lehnert, 1994), which focused on the extraction
of terrorist events, as well as the more recent
ACE program. The work in this paper is inspired
by unsupervised approaches that seek to discover
types of relations and events, instead of assuming
them to be pre-specified; this includes research un-
der various headings such as template/frame/event
learning (Cheung et al, 2013; Modi et al, 2012;
Chambers and Jurafsky, 2011; Li et al, 2010; Be-
jan, 2008), script learning (Regneri et al, 2010;
Chambers and Jurafsky, 2009), relation learning
(Yao et al, 2011), open information extraction
(Banko et al, 2007; Carlson et al, 2010), verb
caseframe learning (Rooth et al, 1999; Gildea,
2002; Grenager and Manning, 2006; Lang and La-
pata, 2010; O? Se?aghdha, 2010; Titov and Klemen-
tiev, 2012), and a version of frame learning called
?unsupervised semantic parsing? (Titov and Kle-
mentiev, 2011; Poon and Domingos, 2009). Un-
like much of the previous literature, we do not
learn latent roles/slots. Event extraction is also
a large literature, including supervised systems
targeting problems similar to MUC and political
events (Piskorski and Atkinson, 2011; Piskorski
et al, 2011; Sanfilippo et al, 2008).
One can also see this work as a relational ex-
14http://eventdata.psu.edu/data.dir/
GDELT.html
tension of co-occurence-based methods such as
Gerrish (2013; ch. 4), Diesner and Carley (2005),
Chang et al (2009), or Newman et al (2006),
which perform bag-of-words-style analysis of text
fragments containing co-occurring entities. (Ger-
rish also analyzed the international relations do-
main, using supervised bag-of-words regression
to assess the expressed valence between a pair
of actors in a news paragraph, using the predic-
tions as observations in a latent temporal model,
and compared to MID.) We instead use parsing to
get a much more focused and interpretable repre-
sentation of the relationship between textually co-
occurring entities; namely, that they are the source
and target of an action event. This is more in line
with work in relation extraction on biomedical sci-
entific articles (Friedman et al, 2001; Rzhetsky
et al, 2004) which uses parsing to extracting a net-
work of how different entities, like drugs or pro-
teins, interact.
7 Conclusion
Large-scale information extraction can dramati-
cally enhance the study of political behavior. Here
we present a novel unsupervised approach to an
important data collection effort in the social sci-
ences. We see international relations as a rich
and practically useful domain for the development
of text analysis methods that jointly infer events,
relations, and sociopolitical context. There are
numerous areas for future work, such as: using
verb dictionaries as semi-supervised seeds or pri-
ors; interactive learning between political science
researchers and unsupervised algorithms; build-
ing low-dimensional scaling, or hierarchical struc-
ture, into the model; and learning the actor lists
to handle changing real-world situations and new
domains. In particular, adding more supervision
to the model will be crucial to improve semantic
quality and make it useful for researchers.
Acknowledgments
Thanks to Justin Betteridge for providing the parsed Giga-
word corpus, Erin Baggott for help in developing the doc-
ument filter, and the anonymous reviewers for helpful com-
ments. This research was supported in part by NSF grant IIS-
1211277, and was made possible through the use of comput-
ing resources made available by the Pittsburgh Supercomput-
ing Center. Brandon Stewart gratefully acknowledges fund-
ing from an NSF Graduate Research Fellowship.
References
Azar, E. E. and Sloan, T. (1975). Dimensions of interactions.
Technical report, University Center of International Stud-
ies, University of Pittsburgh, Pittsburgh.
1102
Banko, M., Cafarella, M. J., Soderland, S., Broadhead, M.,
and Etzioni, O. (2007). Open Information Extraction from
the Web. IJCAI.
Bejan, C. A. (2008). Unsupervised discovery of event sce-
narios from texts. In Proceedings of the 21st Florida Arti-
ficial Intelligence Research Society International Confer-
ence (FLAIRS), Coconut Grove, FL, USA.
Blei, D. M. and Lafferty, J. D. (2006). Dynamic topic models.
In Proceedings of ICML.
Blei, D. M. and Lafferty, J. D. (2007). A correlated topic
model of science. Annals of Applied Statistics, 1(1), 17?
35.
Boschee, E., Natarajan, P., and Weischedel, R. (2013). Au-
tomatic extraction of events from open source text for
predictive forecasting. Handbook of Computational Ap-
proaches to Counterterrorism, page 51.
Brandt, P. T., Freeman, J. R., and Schrodt, P. A. (2011). Real
time, time series forecasting of inter-and intra-state po-
litical conflict. Conflict Management and Peace Science,
28(1), 41?64.
Brandt, P. T., Freeman, J. R., Lin, T.-m., and Schrodt, P. A.
(2012). A Bayesian time series approach to the compari-
son of conflict dynamics. In APSA 2012 Annual Meeting
Paper.
Carlson, A., Betteridge, J., Kisiel, B., Settles, B., Hruschka,
E. R., and Mitchell, T. M. (2010). Toward an architecture
for never-ending language learning. In Proceedings of the
Conference on Artificial Intelligence (AAAI), pages 1306?
1313.
Carter, C. K. and Kohn, R. (1994). On Gibbs sampling for
state space models. Biometrika, 81(3), 541?553.
Chambers, N. and Jurafsky, D. (2009). Unsupervised learn-
ing of narrative schemas and their participants. In Pro-
ceedings of ACL-IJCNLP. Association for Computational
Linguistics.
Chambers, N. and Jurafsky, D. (2011). Template-based infor-
mation extraction without the templates. In Proceedings
of ACL.
Chang, J., Boyd-Graber, J., and Blei, D. M. (2009). Con-
nections between the lines: augmenting social networks
with text. In Proceedings of the 15th ACM SIGKDD in-
ternational conference on Knowledge discovery and data
mining, pages 169?178. ACM.
Cheung, J. C. K., Poon, H., and Vanderwende, L. (2013).
Probabilistic frame induction. In Proceedings of NAACL.
arXiv preprint arXiv:1302.4813.
de Marneffe, M.-C. and Manning, C. D. (2008). Stanford
typed dependencies manual. Technical report, Stanford
University.
Diesner, J. and Carley, K. M. (2005). Revealing social
structure from texts: meta-matrix text analysis as a novel
method for network text analysis. In Causal mapping for
information systems and technology research, pages 81?
108. Harrisburg, PA: Idea Group Publishing.
Eisenstein, J., O?Connor, B., Smith, N. A., and Xing, E. P.
(2010). A latent variable model for geographic lexical
variation. In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing, pages
1277?1287.
Eisenstein, J., Ahmed, A., and Xing, E. (2011). Sparse ad-
ditive generative models of text. In Proceedings of ICML,
pages 1041?1048.
Friedman, C., Kra, P., Yu, H., Krauthammer, M., and Rzhet-
sky, A. (2001). GENIES: a natural-language process-
ing system for the extraction of molecular pathways from
journal articles. Bioinformatics, 17(suppl 1), S74?S82.
Friedman, J., Hastie, T., and Tibshirani, R. (2010). Regular-
ization paths for generalized linear models via coordinate
descent. Journal of Statistical Software, 33(1).
Gerner, D. J., Schrodt, P. A., Yilmaz, O., and Abu-Jabr, R.
(2002). The Creation of CAMEO (Conflict and Media-
tion Event Observations): An Event Data Framework for
a Post Cold War World. Annual Meeting of the American
Political Science Association.
Gerrish, S. M. (2013). Applications of Latent Variable Mod-
els in Modeling Influence and Decision Making. Ph.D.
thesis, Princeton University.
Gerrish, S. M. and Blei, D. M. (2011). Predicting legislative
roll calls from text. In Proceedings of ICML.
Ghosn, F., Palmer, G., and Bremer, S. A. (2004). The MID3
data set, 1993?2001: Procedures, coding rules, and de-
scription. Conflict Management and Peace Science, 21(2),
133?154.
Gildea, D. (2002). Probabilistic models of verb-argument
structure. In Proceedings of COLING.
Goldstein, J. S. (1992). A conflict-cooperation scale for
WEIS events data. Journal of Conflict Resolution, 36,
369?385.
Goldstein, J. S., Pevehouse, J. C., Gerner, D. J., and Telhami,
S. (2001). Reciprocity, triangularity, and cooperation in
the middle east, 1979-97. Journal of Conflict Resolution,
45(5), 594?620.
Grenager, T. and Manning, C. D. (2006). Unsupervised dis-
covery of a statistical verb lexicon. In Proceedings of the
2006 Conference on Empirical Methods in Natural Lan-
guage Processing, page 18.
Griffiths, T. L. and Steyvers, M. (2004). Finding scientific
topics. PNAS, 101(suppl. 1), 5228?5235.
Harrison, J. and West, M. (1997). Bayesian forecasting and
dynamic models. Springer Verlag, New York.
Hobbs, J. R., Appelt, D., Bear, J., Israel, D., Kameyama,
M., Stickel, M., and Tyson, M. (1997). FASTUS: A
cascaded finite-state transducer for extracting information
from natural-language text. Finite-State Language Pro-
cessing, page 383.
Hoff, P. D. (2003). Nonparametric modeling of hierarchi-
cally exchangeable data. University of Washington Statis-
tics Department, Technical Report, 421.
Holmes, C. C. and Held, L. (2006). Bayesian auxiliary
variable models for binary and multinomial regression.
Bayesian Analysis, 1(1), 145?168.
Jones, D., Bremer, S., and Singer, J. (1996). Militarized in-
terstate disputes, 1816?1992: Rationale, coding rules, and
empirical patterns. Conflict Management and Peace Sci-
ence, 15(2), 163?213.
King, G. and Lowe, W. (2003). An automated information
extraction tool for international conflict data with perfor-
mance as good as human coders: A rare events evaluation
design. International Organization, 57(3), 617?642.
Lang, J. and Lapata, M. (2010). Unsupervised induction of
semantic roles. In Human Language Technologies: The
2010 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages
939?947. Association for Computational Linguistics.
Lehnert, W. G. (1994). Cognition, computers, and car bombs:
How Yale prepared me for the 1990s. In Beliefs, Reason-
ing, and Decision-Making. Psycho-Logic in Honor of Bob
Abelson, pages 143?173, Hillsdale, NJ, Hove, UK. Erl-
baum. http://ciir.cs.umass.edu/pubfiles/
cognition3.pdf.
Li, H., Li, X., Ji, H., and Marton, Y. (2010). Domain-
independent novel event discovery and semi-automatic
1103
event annotation. In Proceedings of the 24th Pacific Asia
Conference on Language, Information and Computation,
Sendai, Japan, November.
Martin, A. D. and Quinn, K. M. (2002). Dynamic ideal point
estimation via Markov chain Monte Carlo for the U.S.
Supreme Court, 1953?1999. Political Analysis, 10(2),
134?153.
McClelland, C. (1970). Some effects on theory from the in-
ternational event analysis movement. Mimeo, University
of Southern California.
Mimno, D., Wallach, H., and McCallum, A. (2008). Gibbs
sampling for logistic normal topic models with graph-
based priors. In NIPS Workshop on Analyzing Graphs.
Modi, A., Titov, I., and Klementiev, A. (2012). Unsuper-
vised induction of frame-semantic representations. In Pro-
ceedings of the NAACL-HLT Workshop on the Induction of
Linguistic Structure, pages 1?7. Association for Computa-
tional Linguistics.
Murphy, K. P. (2012). Machine Learning: a Probabilistic
Perspective. MIT Press.
Neal, R. M. (2003). Slice sampling. Annals of Statistics,
pages 705?741.
Newman, D., Chemudugunta, C., and Smyth, P. (2006). Sta-
tistical entity-topic models. In Proceedings of the 12th
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 680?686. ACM.
O? Se?aghdha, D. (2010). Latent variable models of selectional
preference. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, pages
435?444. Association for Computational Linguistics.
O?Brien, S. P. (2010). Crisis early warning and decision sup-
port: Contemporary approaches and thoughts on future re-
search. International Studies Review, 12(1), 87?104.
Parker, R., Graff, D., Kong, J., Chen, K., and Maeda, K.
(2009). English Gigaword Fourth Edition. Linguistic Data
Consortium. LDC2009T13.
Piskorski, J. and Atkinson, M. (2011). Frontex real-time
news event extraction framework. In Proceedings of the
17th ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 749?752. ACM.
Piskorski, J., Tanev, H., Atkinson, M., van der Goot, E., and
Zavarella, V. (2011). Online news event extraction for
global crisis surveillance. Transactions on computational
collective intelligence V , pages 182?212.
Polson, N. G., Scott, J. G., and Windle, J. (2012). Bayesian
inference for logistic models using Polya-Gamma latent
variables. arXiv preprint arXiv:1205.0310.
Poon, H. and Domingos, P. (2009). Unsupervised semantic
parsing. In Proceedings of EMNLP, pages 1?10. Associa-
tion for Computational Linguistics.
Quinn, K. M., Monroe, B. L., Colaresi, M., Crespin, M. H.,
and Radev, D. R. (2010). How to analyze political atten-
tion with minimal assumptions and costs. American Jour-
nal of Political Science, 54(1), 209228.
Rajaraman, A. and Ullman, J. D. (2011). Mining of mas-
sive datasets. Cambridge University Press; http://
infolab.stanford.edu/?ullman/mmds.html.
Ramshaw, L., Boschee, E., Freedman, M., MacBride, J.,
Weischedel, R., , and Zamanian, A. (2011). SERIF lan-
guage processing effective trainable language understand-
ing. Handbook of Natural Language Processing and Ma-
chine Translation, pages 636?644.
Regneri, M., Koller, A., and Pinkal, M. (2010). Learning
script knowledge with web experiments. In Proceedings
of the 48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?10, pages 979?988.
Rooth, M., Riezler, S., Prescher, D., Carroll, G., and Beil,
F. (1999). Inducing a semantically annotated lexicon via
EM-based clustering. In Proceedings of the 37th annual
meeting of the Association for Computational Linguistics
on Computational Linguistics, page 104111.
Rummel, R. (1968). The Dimensionality of Nations project.
Rzhetsky, A., Iossifov, I., Koike, T., Krauthammer, M., Kra,
P., Morris, M., Yu, H., Duboue?, P. A., Weng, W., Wilbur,
W. J., Hatzivassiloglou, V., and Friedman, C. (2004).
GeneWays: a system for extracting, analyzing, visualiz-
ing, and integrating molecular pathway data. Journal of
Biomedical Informatics, 37(1), 43?53.
Sandhaus, E. (2008). The New York Times Annotated Cor-
pus. Linguistic Data Consortium. LDC2008T19.
Sanfilippo, A., Franklin, L., Tratz, S., Danielson, G., Mile-
son, N., Riensche, R., and McGrath, L. (2008). Automat-
ing frame analysis. Social computing, behavioral model-
ing, and prediction, pages 239?248.
Schrodt, P. (2012). Precedents, progress, and prospects in po-
litical event data. International Interactions, 38(4), 546?
569.
Schrodt, P. and Leetaru, K. (2013). GDELT: Global data
on events, location and tone, 1979-2012. In International
Studies Association Conference.
Schrodt, P. A. (2001). Automated coding of international
event data using sparse parsing techniques. International
Studies Association Conference.
Schrodt, P. A. (2006). Twenty Years of the Kansas Event
Data System Project. Political Methodologist.
Schrodt, P. A. and Gerner, D. J. (1994). Validity assessment
of a machine-coded event data set for the Middle East,
1982-1992. American Journal of Political Science.
Schrodt, P. A. and Gerner, D. J. (2004). An event data analy-
sis of third-party mediation in the middle east and balkans.
Journal of Conflict Resolution, 48(3), 310?330.
Shellman, S. M. (2004). Time series intervals and statistical
inference: The effects of temporal aggregation on event
data analysis. Political Analysis, 12(1), 97?104.
Titov, I. and Klementiev, A. (2011). A Bayesian model for
unsupervised semantic parsing. In Proceedings of ACL.
Titov, I. and Klementiev, A. (2012). A Bayesian approach
to unsupervised semantic role induction. Proceedings of
EACL.
Wallach, H., Mimno, D., and McCallum, A. (2009). Rethink-
ing lda: Why priors matter. Advances in Neural Informa-
tion Processing Systems, 22, 1973?1981.
Yao, L., Haghighi, A., Riedel, S., and McCallum, A. (2011).
Structured relation discovery using generative models. In
Proceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 1456?1466. Associ-
ation for Computational Linguistics.
Zeger, S. L. and Karim, M. R. (1991). Generalized linear
models with random effects; a Gibbs sampling approach.
Journal of the American Statistical Association, 86(413),
79?86.
1104
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 176?180,
Dublin, Ireland, August 23-24, 2014.
CMU: Arc-Factored, Discriminative Semantic Dependency Parsing
Sam Thomson Brendan O?Connor Jeffrey Flanigan David Bamman
Jesse Dodge Swabha Swayamdipta Nathan Schneider Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{sthomson,brenocon,jflanigan,dbamman,jessed,
swabha,nschneid,cdyer,nasmith}@cs.cmu.edu
Abstract
We present an arc-factored statistical model
for semantic dependency parsing, as de-
fined by the SemEval 2014 Shared Task 8
on Broad-Coverage Semantic Dependency
Parsing. Our entry in the open track placed
second in the competition.
1 Introduction
The task of broad coverage semantic dependency
parsing aims to provide a shallow semantic analysis
of text not limited to a specific domain. As distinct
from deeper semantic analysis (e.g., parsing to a
full lambda-calculus logical form), shallow seman-
tic parsing captures relationships between pairs
of words or concepts in a sentence, and has wide
application for information extraction, knowledge
base population, and question answering (among
others).
We present here two systems that produce seman-
tic dependency parses in the three formalisms of the
SemEval 2014 Shared Task 8 on Broad-Coverage
Semantic Dependency Parsing (Oepen et al., 2014).
These systems generate parses by extracting fea-
tures for each potential dependency arc and learn-
ing a statistical model to discriminate between good
arcs and bad; the first treats each labeled edge de-
cision as an independent multiclass logistic regres-
sion (?3.2.1), while the second predicts arcs as part
of a graph-based structured support vector machine
(?3.2.2). Common to both models is a rich set of
features on arcs, described in ?3.2.3. We include a
discussion of features found to have no discernable
effect, or negative effect, during development (?4).
Our system placed second in the open track of
the Broad-Coverage Semantic Dependency Parsing
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
Figure 1: Example annotations for DM (top), PAS (middle),
and PCEDT (bottom).
task (in which output from syntactic parsers and
other outside resources can be used). We present
our results in ?5.
2 Formalisms
The Shared Task 8 dataset consists of annota-
tions of the WSJ Corpus in three different se-
mantic dependency formalisms. DM is derived
from LinGO English Resource Grammar (ERG)
annotations in DeepBank (Flickinger et al., 2012).
PAS is derived from the Enju HPSG treebank us-
ing the conversion rules of Miyao et al. (2004).
PCEDT is derived from the tectogrammatical layer
of the Prague Czech-English Dependency Treebank
(Haji?c, 1998). See Figure 1 for an example.
The three formalisms come from very different
linguistic theories, but all are represented as labeled
directed graphs, with words as vertices, and all
have ?top? annotations, corresponding roughly to
the semantic focus of the sentence. (A ?top? need
not be a root of the graph.) This allows us to use
the same machinery (?3) for training and testing
statistical models for the three formalisms.
3 Models
We treat the problem as a three-stage pipeline. The
first stage prunes words by predicting whether they
have any incoming or outgoing edges at all (?3.1);
if a word does not, then it is not considered for
any attachments in later stages. The second stage
176
predicts where edges are present, and their labels
(?3.2). The third stage predicts whether a predicate
word is a top or not (?3.3). Formalisms sometimes
annotate more than one ?top? per sentence, but we
found that we achieve the best performance on all
formalisms by predicting only the one best-scoring
?top? under the model.
3.1 Singleton Classification
For each formalism, we train a classifier to rec-
ognize singletons, nodes that have no parents or
children. (For example, punctuation tokens are of-
ten singletons.) This makes the system faster with-
out affecting accuracy. For singleton prediction,
we use a token-level logistic regression classifier,
with features including the word, its lemma, and
its part-of-speech tag. If the classifier predicts a
probability of 99% or higher the token is pruned;
this removes around 10% of tokens. (The classi-
fier performs differently on different formalisms;
on PAS it has perfect accuracy, while on DM and
PCEDT accuracy is in the mid-90?s.)
3.2 Edge Prediction
In the second stage of the pipeline, we predict the
set of labeled directed edges in the graph. We use
the same set of edge-factored features (?3.2.3) in
two alternative models: an edge-independent mul-
ticlass logistic regression model (LOGISTICEDGE,
?3.2.1); and a structured SVM (Taskar et al., 2003;
Tsochantaridis et al., 2004) that enforces a deter-
minism constraint for certain labels, which allows
each word to have at most one outgoing edge with
that label (SVMEDGE, ?3.2.2). For each formalism,
we trained both models with varying features en-
abled and hyperparameter settings and submitted
the configuration that produced the best labeled F
1
on the development set. For DM and PCEDT, this
was LOGISTICEDGE; for PAS, this was SVMEDGE.
We report results only for the submitted configu-
rations, with different features enabled. Due to
time constraints, full hyperparameter sweeps and
comparable feature sweeps were not possible.
3.2.1 LOGISTICEDGE Parser
The LOGISTICEDGE model considers only token
index pairs (i, j) where |i ? j| ? 10, i 6= j,
and both t
i
and t
j
have been predicted to be non-
singletons by the first stage. Although this prunes
some gold edges, among the formalisms, 95%?97%
of all gold edges are between tokens of distance
10 or less. Both directions i ? j and j ? i are
considered between every pair.
Let L be the set of K + 1 possible output labels:
the formalism?s original K edge labels, plus the
additional label NOEDGE, which indicates that no
edge exists from i to j. The model treats every pair
of token indices (i, j) as an independent multiclass
logistic regression over output space L. Let x be
an input sentence. For candidate parent index i,
child index j, and edge label `, we extract a feature
vector f(x, i, j, `), where ` is conjoined with every
feature described in ?3.2.3. The multiclass logis-
tic regression model defines a distribution over L,
parametrized by weights ?:
P (` | ?, x, i, j) =
exp{? ? f(x, i, j, `)}
?
`
?
?L
exp{? ? f(x, i, j, `
?
)}
.
? is learned by minimizing total negative log-
likelihood of the above (with weighting; see be-
low), plus `
2
regularization. AdaGrad (Duchi et al.,
2011) is used for optimization. This seemed to opti-
mize faster than L-BFGS (Liu and Nocedal, 1989),
at least for earlier iterations, though we did no sys-
tematic comparison. Stochastic gradient steps are
applied one at a time from individual examples,
and a gradient step for the regularizer is applied
once per epoch.
The output labels have a class imbalance; in all
three formalisms, there are many more NOEDGE
examples than true edge examples. We improved
F
1
performance by downweighting NOEDGE
examples through a weighted log-likelihood
objective,
?
i,j
?
`
w
`
logP (` |?, x, i, j), with
w
NOEDGE
= 0.3 (selected on development set) and
w
`
= 1 otherwise.
Decoding: To predict a graph structure at test-time
for a new sentence, the most likely edge label is pre-
dicted for every candidate (i, j) pair of unpruned
tokens. If an edge is predicted for both directions
for a single (i, j) pair, only the edge with the higher
score is chosen. (There are no such bidirectional
edges in the training data.) This post-processing ac-
tually did not improve accuracy on DM or PCEDT;
it did improve PAS by ?0.2% absolute F
1
, but we
did not submit LOGISTICEDGE for PAS.
3.2.2 SVMEDGE Parser
In the SVMEDGE model, we use a structured SVM
with a determinism constraint. This constraint en-
sures that each word token has at most one outgoing
edge for each label in a set of deterministic labels
L
d
. For example, in DM a predicate never has more
177
than one child with edge label ?ARG1.? L
d
was
chosen to be the set of edges that were > 99.9%
deterministic in the training data.
1
Consider the fully dense graph of all edges be-
tween all words predicted as not singletons by the
singleton classifier ?3.1 (in all directions with all
possible labels). Unlike LOGISTICEDGE, the la-
bel set L does not include an explicit NOEDGE
label. If ? denotes the model weights, and f de-
notes the features, then an edge from i to j with
label ` in the dense graph has a weight c(i, j, `)
assigned to it using the linear scoring function
c(i, j, `) = ? ? f(x, i, j, `).
Decoding: For each node and each label `, if ` ?
L
d
, the decoder adds the highest scoring outgoing
edge, if its weight is positive. For ` 6? L
d
, every
outgoing edge with positive weight is added. This
procedure is guaranteed to find the highest scoring
subgraph (largest sum of edge weights) of the dense
graph subject to the determinism constraints. Its
runtime is O(n
2
).
The model weights are trained using the struc-
tured SVM loss. If x is a sentence and y is a
graph over that sentence, let the features be de-
noted f(x, y) =
?
(i,j,`)?y
f(x, i, j, `). The SVM
loss for each training example (x
i
, y
i
) is:
??
>
f(x
i
, y
i
)+max
y
?
>
f(x
i
, y)+cost(y, y
i
)
where cost(y, y
i
) = ?|y \ y
i
| + ?|y
i
\ y|. ? and
? trade off between precision and recall for the
edges (Gimpel and Smith, 2010). The loss is min-
imized with AdaGrad using early-stopping on a
development set.
3.2.3 Edge Features
Table 1 describes the features we used for predict-
ing edges. These features were computed over an
edge e with parent token s at index i and child
token t at index j. Unless otherwise stated, each
feature template listed has an indicator feature that
fires for each value it can take on. For the sub-
mitted results, LOGISTICEDGE uses all features
except Dependency Path v2, POS Path, and Dis-
tance Thresholds, and SVMEDGE uses all features
except Dependency Path v1. This was due to
SVMEDGE being faster to train than LOGISTIC-
EDGE when including POS Path features, and due
1
By this we mean that of the nodes that have at least
one outgoing ` edge, 99.9% of them have only one outgo-
ing ` edge. For DM, L
d
= L\{? and c,? ? or c,? ? then c,?
?loc,? ?mwe,? ?subord?}; for PAS, L
d
= L; and for PCEDT,
L
d
={?DPHR,? ?INTF,? ?VOCAT?}.
Tokens: The tokens s and t themselves.
Lemmas: Lemmas of s and t.
POS tags: Part of speech tags of s and t.
Linear Order: Fires if i < j.
Linear Distance: i? j.
Dependency Path v1 (LOGISTICEDGE only): The
concatenation of all POS tags, arc labels and up/down
directions on the path in the syntactic dependency tree
from s to t. Conjoined with s, with t, and without either.
Dependency Path v2 (SVMEDGE only): Same as De-
pendency Path v1, but with the lemma of s or t instead
of the word, and substituting the token for any ?IN? POS
tag.
Up/Down Dependency Path: The sequence of upward
and downward moves needed to get from s to t in the
syntactic dependency tree.
Up/Down/Left/Right Dependency Path: The unla-
beled path through the syntactic dependency tree from s
to t, annotated with whether each step through the tree
was up or down, and whether it was to the right or left in
the sentence.
Is Parent: Fires if s is the parent of t in the syntactic
dependency parse.
Dependency Path Length: Distance between s and t in
the syntactic dependency parse.
POS Context: Concatenated POS tags of tokens at i?1,
i, i+ 1, j ? 1, j, and j + 1. Concatenated POS tags of
tokens at i? 1, i, j ? 1, and j. Concatenated POS tags
of tokens at i, i+ 1, j, and j + 1.
Subcategorization Sequence: The sequence of depen-
dency arc labels out of s, ordered by the index of the
child. Distinguish left children from right children. If t
is a direct child of s, distinguish its arc label with a ?+?.
Conjoin this sequence with the POS tag of s.
Subcategorization Sequence with POS: As above, but
add the POS tag of each child to its arc label.
POS Path (SVMEDGE only): Concatenated POS tags
between and including i and j. Conjoined with head
lemma, with dependent lemma, and without either.
Distance Thresholds (SVMEDGE only): Fires for ev-
ery integer between 1 and blog(|i? j|+1)/ log(1.39)c
inclusive.
Table 1: Features used in edge prediction
to time constraints for the submission we were un-
able to retrain LOGISTICEDGE with these features.
3.2.4 Feature Hashing
The biggest memory usage was in the map from
feature names to integer indices during feature
extraction. For experimental expedience, we im-
plemented multitask feature hashing (Weinberger
et al., 2009), which hashes feature names to indices,
under the theory that errors due to collisions tend
to cancel. No drop in accuracy was observed.
3.3 Top Prediction
We trained a separate token-level binary logistic
regression model to classify whether a token?s node
had the ?top? attribute or not. At decoding time, all
predicted predicates (i.e., nodes where there is at
178
least one outbound edge) are possible candidates
to be ?top?; the classifier probabilities are evalu-
ated, and the highest-scoring node is chosen to be
?top.? This is suboptimal, since some graphs have
multiple tops (in PCEDT this is more common);
but selection rules based on probability thresholds
gave worse F
1
performance on the dev set. For a
given token t at index i, the top classifier?s features
included t?s POS tag, i, those two conjoined, and
the depth of t in the syntactic dependency tree.
4 Negative Results
We followed a forward-selection process during
feature engineering. For each potential feature,
we tested the current feature set versus the current
feature set plus the new potential feature. If the
new feature did not improve performance, we did
not add it. We list in table 2 some of the features
which we tested but did not improve performance.
In order to save time, we ran these feature se-
lection experiments on a subsample of the training
data, for a reduced number of iterations. These re-
sults thus have a strong caveat that the experiments
were not exhaustive. It may be that some of these
features could help under more careful study.
5 Experimental Setup
We participated in the Open Track, and used the
syntactic dependency parses supplied by the orga-
nizers. Feature engineering was performed on a
development set (?20), training on ??00?19. We
evaluate labeled precision (LP), labeled recall (LR),
labeled F
1
(LF), and labeled whole-sentence match
(LM) on the held-out test data using the evaluation
script provided by the organizers. LF was aver-
aged over the formalisms to determine the winning
system. Table 3 shows our scores.
6 Conclusion and Future Work
We found that feature-rich discriminative models
perform well at the task of mapping from sentences
to semantic dependency parses. While our final
approach is fairly standard for work in parsing,
we note here additional features and constraints
which did not appear to help (contrary to expecta-
tion). There are a number of clear extensions to
this work that could improve performance. While
an edge-factored model allows for efficient infer-
ence, there is much to be gained from higher-order
features (McDonald and Pereira, 2006; Martins
et al., 2013). The amount of information shared
Word vectors: Features derived from 64-dimensional
vectors from (Faruqui and Dyer, 2014), including the
concatenation, difference, inner product, and element-
wise multiplication of the two vectors associated with
a parent-child edge. We also trained a Random Forest
on the word vectors using Liaw and Wiener?s (2002) R
implementation. The predicted labels were then used as
features in LOGISTICEDGE.
Brown clusters Features derived from Brown clusters
(Brown et al., 1992) trained on a large corpus of web data.
Parent, child, and conjoined parent-child edge features
from cluster prefixes of length 2, 4, 6, 8, 10, and 12.
Conjunctions of those features with the POS tags of the
parent and child tokens.
Active/passive: Active/passive voice feature (as in Jo-
hansson and Nugues (2008)) conjoined with both the
Linear Distance features and the Subcategorization Se-
quence features. Voice information may already be cap-
tured by features from the Stanford dependency?style
parses, which include passivization information in arc
labels such as nsubjpass and auxpass (de Marneffe and
Manning, 2008).
Connectivity constraint: Enforcing that the graph is
connected (ignoring singletons), similar to Flanigan et al.
(2014). Almost all semantic dependency graphs in the
training data are connected (ignoring singletons), but
we found that enforcing this constraint significantly hurt
precision.
Tree constraint: Enforces that the graph is a tree. Un-
surprisingly, we found that enforcing a tree constraint
hurt performance.
Table 2: Features and constraints giving negative results.
LP LR LF LM
DM 0.8446 0.8348 0.8397 0.0875
PAS 0.9078 0.8851 0.8963 0.2604
PCEDT 0.7681 0.7072 0.7364 0.0712
Average 0.8402 0.8090 0.8241 0.1397
Table 3: Labeled precision (LP), recall (LR), F
1
(LF), and
whole-sentence match (LM) on the held-out test data.
between the three formalisms suggests that a multi-
task learning (Evgeniou and Pontil, 2004) frame-
work could lead to gains. And finally, there is
additional structure in the formalisms which could
be exploited (such as the deterministic processes
by which an original PCEDT tree annotation was
converted into a graph); formulating more subtle
graph constraints to capture this a priori knowl-
edge could lead to improved performance. We
leave such explorations to future work.
Acknowledgements
We are grateful to Manaal Faruqui for his help in word vector
experiments, and to reviewers for helpful comments. The re-
search reported in this paper was sponsored by the U.S. Army
Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533, DARPA
grant FA8750-12-2-0342 funded under the DEFT program,
U.S. NSF grants IIS-1251131 and IIS-1054319, and Google?s
support of the Reading is Believing project at CMU.
179
References
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Lin-
guistics, 18(4):467?479.
Marie-Catherine de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies representation. In
Coling 2008: Proc. of the Workshop on Cross-Framework
and Cross-Domain Parser Evaluation, pages 1?8. Manch-
ester, UK.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adap-
tive subgradient methods for online learning and stochas-
tic optimization. Journal of Machine Learning Research,
12:2121?2159.
Theodoros Evgeniou and Massimiliano Pontil. 2004. Regular-
ized multitask learning. In Proc. of KDD, pages 109?117.
Seattle, WA, USA.
Manaal Faruqui and Chris Dyer. 2014. Improving vector
space word representations using multilingual correlation.
In Proc. of EACL, pages 462?471. Gothenburg, Sweden.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer,
and Noah A. Smith. 2014. A discriminative graph-based
parser for the Abstract Meaning Representation. In Proc.
of ACL, pages 1426?1436. Baltimore, MD, USA.
Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012. Deep-
Bank: a dynamically annotated treebank of the Wall Street
Journal. In Proc. of the Eleventh International Workshop on
Treebanks and Linguistic Theories, pages 85?96. Lisbon,
Portugal.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-margin
training for structured log-linear models. Technical
Report CMU-LTI-10-008, Carnegie Mellon Univer-
sity. URL http://lti.cs.cmu.edu/sites/
default/files/research/reports/2010/
cmulti10008.pdf.
Jan Haji?c. 1998. Building a syntactically annotated corpus:
the Prague Dependency Treebank. In Eva Haji?cov?a, ed-
itor, Issues of Valency and Meaning. Studies in Honour
of Jarmila Panevov?a, pages 106?132. Prague Karolinum,
Charles University Press, Prague.
Richard Johansson and Pierre Nugues. 2008. Dependency-
based semantic role labeling of PropBank. In Proc. of
EMNLP, pages 69?78. Honolulu, HI, USA.
Andy Liaw and Matthew Wiener. 2002. Classification
and regression by randomForest. R News, 2(3):18?
22. URL http://cran.r-project.org/web/
packages/randomForest/.
Dong C. Liu and Jorge Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45(3):503?528.
Andr?e F. T. Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proc. of ACL, pages 617?622. Sofia,
Bulgaria.
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proc. of
EACL, pages 81?88. Trento, Italy.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii. 2004.
Corpus-oriented grammar development for acquiring a
head-driven phrase structure grammar from the Penn Tree-
bank. In Proc. of IJCNLP, pages 684?693. Hainan Island,
China.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel
Zeman, Dan Flickinger, Jan Haji?c, Angelina Ivanova, and
Yi Zhang. 2014. SemEval 2014 Task 8: Broad-coverage
semantic dependency parsing. In Proc. of SemEval. Dublin,
Ireland.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-
margin Markov networks. In Proc. of NIPS, pages 25?32.
Vancouver, British Columbia, Canada.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims,
and Yasemin Altun. 2004. Support vector machine learning
for interdependent and structured output spaces. In Proc.
of ICML, pages 104?111. Banff, Alberta, Canada.
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex
Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In Proc. of ICML, pages
1113?1120. Montreal, Quebec, Canada.
180
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 51?60,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
A Framework for (Under)specifying Dependency Syntax
without Overloading Annotators
Nathan Schneider?? Brendan O?Connor? Naomi Saphra? David Bamman?
Manaal Faruqui? Noah A. Smith? Chris Dyer? Jason Baldridge?
?School of Computer Science, Carnegie Mellon University
?Department of Linguistics, The University of Texas at Austin
Abstract
We introduce a framework for lightweight
dependency syntax annotation. Our for-
malism builds upon the typical represen-
tation for unlabeled dependencies, per-
mitting a simple notation and annotation
workflow. Moreover, the formalism en-
courages annotators to underspecify parts
of the syntax if doing so would streamline
the annotation process. We demonstrate
the efficacy of this annotation on three lan-
guages and develop algorithms to evaluate
and compare underspecified annotations.
1 Introduction
Computational representations for natural lan-
guage syntax are borne of competing design con-
siderations. When designing such representations,
there may be a tradeoff between parsimony and
expressiveness. A range of linguistic theories at-
tract support due to differing purposes and aes-
thetic principles (Chomsky, 1957; Tesni?re, 1959;
Hudson, 1984; Sgall et al, 1986; Mel?c?uk, 1988,
inter alia). Formalisms concerned with tractable
computation may care chiefly about learnabil-
ity or parsing efficiency (Shieber, 1992; Sleator
and Temperly, 1993; Kuhlmann and Nivre, 2006).
Further considerations may include psychologi-
cal and evolutionary plausibility (Croft, 2001;
Tomasello, 2003; Steels et al, 2011; Fossum and
Levy, 2012), integration with other representa-
tions such as semantics (Steedman, 2000; Bergen
and Chang, 2005), or suitability for particular ap-
plications (e.g., translation).
Here we elevate ease of annotation as a pri-
mary design concern for a syntactic annotation
formalism. Currently, a lack of annotated data
is a huge bottleneck for robust NLP, standing in
the way of parsers for social media text (Foster
et al, 2011) and many low-resourced languages
(to name two examples). Traditional syntactic an-
notation projects like the Penn Treebank (Marcus
?Corresponding author: nschneid@cs.cmu.edu
et al, 1993) or Prague Dependency Treebank (Ha-
jic?, 1998) require highly trained annotators and
huge amounts of effort. Lowering the cost of an-
notation, by making it easier and more accessi-
ble, could greatly facilitate robust NLP in new lan-
guages and genres.
To that end, we design and test new, lightweight
methodologies for syntactic annotation. We pro-
pose a formalism, Fragmentary Unlabeled De-
pendency Grammar (FUDG) for unlabeled de-
pendency syntax that addresses some of the most
glaring deficiencies of basic unlabeled dependen-
cies (?2), with little added burden on annotators.
FUDG requires minimal theoretical commitments,
and can be supplemented with a project-specific
style guide (we provide a brief one for English).
We contribute a simple ASCII markup language?
Graph Fragment Language (GFL; ?3)?that al-
lows annotations to be authored using any text ed-
itor, along with tools for validating, normalizing,
and visualizing GFL annotations.1
An important characteristic of our framework is
annotator flexibility. The formalism supports this
by allowing underspecification of structural por-
tions that are unclear or unnecessary for the pur-
poses of a project. Fully leveraging this power re-
quires new algorithms for evaluation, e.g., of inter-
annotator agreement, where annotations are par-
tial; such algorithms are presented in ?4.2
Finally, small-scale case studies (?5) apply our
framework (formalism, notation, and evaluations)
to syntactically annotate web text in English, news
in Malagasy, and dialogues in Kinyarwanda.
2 A Dependency Grammar for
Annotation
Although dependency-based approaches to syntax
play a major role in computational linguistics, the
nature of dependency representations is far from
uniform. Exemplifying one end of the spectrum
is the Prague Dependency Treebank, which articu-
lates an elaborate dependency-based syntactic the-
1https://github.com/brendano/gfl_syntax/
2Parsing algorithms are left for future work.
51
Found the scarriest mystery door in my school . I?M SO CURIOUS D:
Found** < (the scarriest mystery door*)
Found < in < (my > school)
I?M** < (SO > CURIOUS)
D:**
my = I?M
thers still like 1 1/2 hours till Biebs bday here :P
thers** < still
thers < ((1 1/2) > hours < till < (Biebs > bday))
(thers like 1 1/2 hours)
thers < here
:P**
Figure 1: Two tweets with example GFL annotations. (The formalism and notation are described in ?3.)
ory in a rich, multi-tiered formalism (Hajic?, 1998;
B?hmov? et al, 2003). On the opposite end of
the spectrum are the structures used in dependency
parsing research which organize all the tokens of
a sentence into a tree, sometimes with category la-
bels on the edges (K?bler et al, 2009). Insofar as
they reflect a theory of syntax, these vanilla de-
pendency grammars provide a highly reduction-
ist view of structure?indeed, parses used to train
and evaluate dependency parses are often simpli-
fications of Prague-style parses, or else converted
from constituent treebanks.
In addition to the binary dependency links of
vanilla dependency representations, we offer three
devices to capture certain linguistic phenomena
more straightforwardly:3
1. We make explicit the meaningful lexical units
over which syntactic structure is represented. Our
approach (a) allows punctuation and other extrane-
ous tokens to be excluded so as not to distract from
the essential structure; and (b) permits tokens to be
grouped into shallow multiword lexical units.4
2. Coordination is problematic to represent with
unlabeled dependencies due to its non-binary na-
ture. A coordinating conjunction typically joins
multiple expressions (conjuncts) with equal sta-
tus, and other expressions may relate to the com-
pound structure as a unit. There are several differ-
ent conventions for forcing coordinate structures
into a head-modifier straightjacket (Nivre, 2005;
de Marneffe and Manning, 2008; Marec?ek et al,
2013). Conjuncts, coordinators, and shared de-
pendents can be distinguished with edge labels;
we equivalently use a special notation, permitting
the coordinate structure to be automatically trans-
formed with any of the existing conventions.5
3Some of this is inspired by the conventions of Reed-
Kellogg sentence diagramming, a graphical dependency an-
notation system for English pedagogy (Reed and Kellogg,
1877; Kolln and Funk, 1994; Florey, 2006).
4The Stanford representation supports a limited notion of
multiword expressions (de Marneffe and Manning, 2008).
For simplicity, our formalism treats multiwords as unana-
lyzed (syntactically opaque) wholes, though some multiword
expressions may have syntactic descriptions (Baldwin and
Kim, 2010).
5Tesni?re (1959) and Hudson (1984) similarly use
special structures for coordination (Schneider, 1998;
3. Following Tesni?re (1959), our formalism
offers a simple facility to express anaphora-
antecedent relations (a subset of semantic relation-
ships) that are salient in particular syntactic phe-
nomena such as relative clauses, appositives, and
wh-expressions.
Underspecification. Our desire to facilitate
lightweight annotation scenarios requires us to
abandon the expectation that syntactic informants
provide a complete parse for every sentence. On
one hand, an annotator may be uncertain about the
appropriate parse due to lack of expertise, insuf-
ficiently mature annotation conventions, or actual
ambiguity in the sentence. On the other hand, an-
notators may be indifferent to certain phenomena.
This can happen for a variety of reasons:
? Some projects may only need annotations of
specific constructions. For example, building a
semantic resource for events may require anno-
tation of syntactic verb-argument relations, but
not internal noun phrase structure.
? As a project matures, it may be more useful to
annotate only infrequent lexical items.
? Semisupervised learning from partial annota-
tions may be sufficient to learn complete parsers
(Hwa, 1999; Clark and Curran, 2006).
? Beginning annotators may wish to focus on eas-
ily understood syntactic phenomena.
? Different members of a project may wish to spe-
cialize in different syntactic phenomena, reduc-
ing training cost and cognitive load.
Rather than treating annotations as invalid unless
and until they are complete trees, we formally rep-
resent and reason about partial parse structures.
Annotators produce annotations, which encode
constraints on the (inferred) analysis, the parse
structure, of a sentence. We say that a valid anno-
tation supports (is compatible with) one or more
analyses. Both annotations and analyses are rep-
resented as graphs (the graph representation is de-
scribed below in ?3.2). We require that the di-
rected edges in an analysis graph must form a tree
over all the lexical items in the sentence.6 Less
Sangati and Mazza, 2009).
6While some linguistic phenomena (e.g., relative clauses,
control constructions) can be represented using non-tree
52
stringent well-formedness constraints on the an-
notation graph leave room for underspecification.
Briefly, an annotation can be underspecified in
two ways: (a) an expression may not be attached to
any parent, indicating it might depend on any non-
descendant in a full analysis?this is useful for an-
notating sentences piece by piece; and (b) multiple
expressions may be grouped together in a fudge
expression (?3.3), a constraint that the elements
form a connected subgraph in the full analysis
while leaving the precise nature of that subgraph
indeterminate?this is useful for marking relation-
ships between chunks (possibly constituents).
A formalism, not a theory. Our framework for
dependency grammar annotation is a syntactic
formalism, but it is not sufficiently comprehen-
sive to constitute a theory of syntax. Though
it standardizes the basic treatment of a few ba-
sic phenomena, simplicity of the formalism re-
quires us to be conservative about making such
extensions. Therefore, just as with simpler for-
malisms, language- and project-specific conven-
tions will have to be developed for specific linguis-
tic phenomena. By embracing underspecified an-
notation, however, our formalism aims to encour-
age efficient corpus coverage in a nascent anno-
tation project, without forcing annotators to make
premature decisions.
3 Syntactic Formalism and GFL
In our framework, a syntactic annotation of a sen-
tence follows an extended dependency formalism
based on the desiderata enumerated in the previ-
ous section. We call our formalism Fragmentary
Unlabeled Dependency Grammar (FUDG).
To make it simple to create FUDG annotations
with a text editor, we provide a plain-text de-
pendency notation called Graph Fragment Lan-
guage (GFL). Fragments of the FUDG graph?
nodes and dependencies linking them?are en-
coded in this language; taken together, these frag-
ments describe the annotation in its entirety. The
ordering of GFL fragments, and of tokens within
each fragment, is of no formal consequence. Since
the underlying FUDG representation is transpar-
ently related to GFL constructions, GFL notation
will be introduced alongside the discussion of each
kind of FUDG node.7
structures, we find that being able to alert annotators when
they inadvertently violate the tree constraint is more useful
than the expressive flexibility.
7In principle, FUDG annotations could be created with
3.1 Tokens
We expect a tokenized string, such as a sentence
or short message. The provided tokenization is re-
spected in the annotation. For human readability,
GFL fragments refer to tokens as strings (rather
than offsets), so all tokens that participate in an
annotation must be unambiguous in the input.8 A
token may be referenced multiple times in the an-
notation.
3.2 Graph Encoding
Directed arcs. As in other dependency
formalisms, dependency arcs are directed
links indicating the syntactic headedness
relationship between pairs of nodes. In
GFL, directed arcs are indicated with an-
gle brackets pointing from the dependent to
its head, as in black > cat or (equivalently)
cat < black. Multiple arcs can be chained to-
gether: the > cat < black < jet describes three
arcs. Parentheses help group portions of a chain:
(the > cat < black < jet) > likes < fish (the
structure black < jet > likes, in which jet
appears to have two heads, is disallowed). Note
that another encoding for this structure would be
to place the contents of the parentheses and the
chain cat > likes < fish on separate lines. Curly
braces can be used to list multiple dependents of
the same head: {cat fish} > likes.
Anaphoric links. These undirected links join
coreferent anaphora to each other and to their an-
tecedent(s). In English this includes personal pro-
nouns, relative pronouns (who, which, that), and
anaphoric do and so (Leo loves Ulla and so does
Max). This introduces a bit of semantics into our
annotation, though at present we do not attempt to
mark non-anaphoric coreference. It also allows a
more satisfying treatment of appositives and rel-
ative clauses than would be possible from just the
directed tree (the third example in figures 2 and 3).
Lexical nodes. Whereas in vanilla dependency
grammar syntactic links are between pairs of to-
ken nodes, FUDG abstracts away from the indi-
vidual tokens in the input. The lowest level of a
FUDG annotation consists of lexical nodes, i.e.,
an alternative mechanism such as a GUI, as in Hajic? et al
(2001).
8If a word is repeated within the sentence, it must be in-
dexed in the input string in order to be referred to from a
fragment. In our notation, successive instances of the same
word are suffixed with ~1, ~2, ~3, etc. Punctuation and other
tokens omitted from an annotation do not need to be indexed.
53
'll
If
's
I wake_up
restin' it~1
it~2
weapons
Our three
are
$a
fear surprise efficiency
ruthless
and~1 and~2
are
We knights
the
who
say
Ni
Figure 2: FUDG graphs corresponding to the examples in figure 3. The two special kinds of directed edges are for attaching
conjuncts (bolded) and their coordinators (dotted) in a coordinate structure. Anaphoric links are undirected. The root node of
each sentence is omitted.
If it~1 's restin' I 'll wake it~2 up .
If < (it~1 > 's < restin')
I > 'll < [wake up] < it~2
If > 'll**
it~1 = it~2
Our three weapons are fear and~1 surprise and~2
ruthless efficiency ...
{Our three} > weapons > are < $a
$a :: {fear surprise efficiency} :: {and~1 and~2}
ruthless > efficiency
We are the knights who say ... Ni !
We > are < knights < the
knights < (who > say < Ni)
who = knights
Figure 3: GFL for the FUDG graphs in figure 2.
lexical item occurrences. Every token node maps
to 0 or 1 lexical nodes (punctuation, for instance,
can be ignored).
A multiword is a lexical node incorporating
more than one input token and is atomic (does
not contain internal structure). A multiword node
may group any subset of input tokens; this allows
for multiword expressions which are not neces-
sarily contiguous in the sentence (e.g., the verb-
particle construction make up in make the story
up). GFL notates multiwords with square brack-
ets, e.g., [break a leg].
Coordination nodes. Coordinate structures re-
quire at least two kinds of dependents: co-
ordinators (i.e., lexical nodes for coordinat-
ing conjunctions?at least one per coordina-
tion node) and conjuncts (heads of the con-
joined subgraphs?at least one per coordination
node). The GFL annotation has three parts:
a variable representing the node, a set of con-
juncts, and a set of coordinator nodes. For in-
stance, $a :: {[peanut butter] honey} :: {and}
(peanut butter and honey) can be embedded
within a phrase via the coordination node
variable $a; a [fresh [[peanut butter] and
honey] sandwich] snack would be formed with
{fresh $a} > sandwich > snack < a. A graphical
example of coordination can be seen in figure 2?
note the bolded conjunct edges and the dotted co-
ordinator edges. If the conjoined phrase as a whole
takes modifiers, these are attached to the coordina-
tion node with regular directed arcs. For example,
in Sam really adores kittens and abhors puppies.,
the shared subject Sam and adverb really attach to
the entire conjoined phrase. In GFL:
$a :: {adores abhors} :: {and}
Sam > $a < really
adores < kittens abhors < puppies
Root node. This is a special top-level node used
to indicate that a graph fragment constitutes a stan-
dalone utterance or a discourse connective. For an
input with multiple utterances, the head of each
should be designated with ** to indicate that it at-
taches to the root.
3.3 Means of Underspecification
As discussed in ?2, our framework distinguishes
annotations from full syntactic analyses. With re-
spect to dependency structure (directed edges), the
former may underspecify the latter, allowing the
annotator to commit only to a partial analysis.
For an annotationA, we define support(A) to be
the set of full analyses compatible with that anno-
tation. A full analysis is required to be a directed
rooted tree over all lexical nodes in the annotation.
An annotation is valid if its support is non-empty.
The 2 mechanisms for dependency underspeci-
fication are unattached nodes and fudge nodes.
Unattached nodes. For any node in an annota-
tion, the annotator is free to simply leave it not
attached to any head. This is interpreted as al-
lowing its head to be any other node (including
the root node), subject to the tree constraint. We
call a node?s possible heads its supported par-
ents. Formally, for an unattached node v in an-
notation A, suppParentsA(v) = nodes(A) \ ({v} ?
descendants(v)).
Fudge nodes. Sometimes, however, it is desir-
able to represent a sort of skeletal structure with-
out filling in all the details. A fudge expres-
sion (FE) asserts that a group of nodes (the ex-
pression?s members) belong together in a con-
nected subgraph, while leaving the internal struc-
ture of that subgraph unspecified.9 The notation
9This underspecification semantics is, to the best of our
knowledge, novel, though it has been proposed that con-
nected dependency subgraphs (known as catenae) are of the-
oretical importance in syntax (Osborne et al, 2012).
54
FN2
a
b
f
FN1
c d e
f
b
f
b
b c
b
b a
a
d
a
c a
d
a
d
c db e fe c e fe
a
d d
cf
e e f
c
Figure 4: Left: An annotation graph with 2 fudge nodes and 6 lexical nodes; it can be encoded with GFL fragments
((a b)* c d) < e and b < f. Right: All of its supported analyses: prom(A) = 6. com(A) = 1 ?
log 6
log 75
= .816.
for this is a list of two or more nodes within
parentheses: an annotation for Few if any witches
are friends with Maria. might contain the FE
(Few if any) so as to be compatible with the
structures Few < if < any, Few > if > any, etc.?
but not, for instance, Few > witches < any. In
the FUDG graph, this is represented with a fudge
node to which members are attached by special
member arcs. Fudge nodes may be linked to other
nodes: the GFL fragment (Few if any) > witches
is compatible with (Few < if < any) > witches,
(Few < (if > any)) > witches, and so forth.
Properties. Let f be a fudge expression. From
the connected subgraph definition and the tree
constraint on analyses, it follows that:
? Exactly 1 member of f must, in any compatible
analysis, have a parent that is not a member of f.
Call this node the top of the fudge expression,
denoted f ?. f ? dominates all other members of
f; it can be considered f?s ?internal head.?
? f does not necessarily form a full subtree. Any
of its members may have dependents that are
not themselves members of the fudge expres-
sion. (Such dependencies can be specified in
additional GFL fragments.)
Top designation. A single member of a fudge
expression may optionally be designated as its top
(internal head). This is specified with an asterisk:
(Few* if any) > witches indicates that Few must
attach to witches and also dominate both if and
any. In the FUDG graph, this is represented with
a special top arc as depicted in bold in figure 4.
Nesting. One fudge expression may nest
within another, e.g. (Few (if any)) > witches;
the word analyzed as attaching to witches might
be Few or whichever of (if any) heads the other.
A nested fudge expression can be designated as
top: (Vanishingly few (if any)*).
Modifiers. An arc attaching a node to a
fudge expression as a whole asserts that the
external node should modify the top of the fudge
expression (whether or not that top is designated
in the annotation). For instance, two of the
interpretations of British left waffles on Falklands
would be preserved by specifying British > left
and (left waffles) < on < Falklands. Analyses
British > left < waffles < on < Falklands and
(British > left < on < Falklands) > waffles
would be excluded because the preposition does
not attach to the head of (left waffles).10
Multiple membership. A node may be a mem-
ber of multiple fudge expressions, or a member
of an FE while attached to some other node via
an explicit arc. Each connected component of
the FUDG graph is therefore a polytree (not nec-
essarily a tree). The annotation graph minus all
member edges of fudge nodes and all (undirected)
anaphoric links must be a directed tree or forest.
Enumerating supported parents. Fudge ex-
pressions complicate the procedure for listing a
node?s supported parents (see above). Consider an
FE f having some member v. v might be the top
of f (unless some other node is so designated), in
which case anything the fudge node can attach to
is a potential parent of v. If some node other than
v might be the top of f, then v?s head could be any
member of f. Below (?4.1) we develop an algo-
rithm for enumerating supported parents for any
annotation graph node.
4 Annotation Evaluation Measures
For an annotation task which allows for a great
deal of latitude?as in our case, where a syntac-
tic annotation may be full or partial?quantitative
evaluation of data quality becomes a challenge. In
the context of our formalism, we propose mea-
sures that address:
? Annotation efficiency, quantified in terms of
annotator productivity (tokens per hour).
? The amount of information in an underspeci-
fied annotation. Intuitively, an annotation that
flirts with many full analyses conveys less syn-
tactic information than one which supports few
analyses. We define an annotation?s promiscu-
ity to be the number of full analyses it supports,
and develop an algorithm to compute it (?4.1).
10Not all attachment ambiguities can be precisely encoded
in FUDG. For instance, there is no way to forbid an attach-
ment to a word that lies along the path between the pos-
sible heads. The best that can be done given a sentence
like They conspired to defenestrate themselves on Tuesday. is
They > conspired < to < defenestrate < themselves and
(conspired* to defenestrate (on < Tuesday)).
55
? Inter-annotator agreement between two par-
tial annotations. Our measures for dependency
structure agreement (?4.2) incorporate the no-
tion of promiscuity.
We test these evaluations on our pilot annotation
data in the case studies (?5).
4.1 Promiscuity vs. Commitment
Given a FUDG annotation of a sentence, we quan-
tify the extent to which it underspecifies the full
structure by counting the number of analyses that
are compatible with the constraints in the annota-
tion. We call this number the promiscuity of the
annotation. Each analysis tree is rooted with the
root node and must span all lexical nodes.11
A na?ve algorithm for computing promiscuity
would be to enumerate all directed spanning trees
over the lexical nodes, and then check each of
them for compatibility with the annotation. But
this quickly becomes intractable: for n nodes,
one of which is designated as the root, there are
nn?2 spanning trees. However, we can filter out
edges that are known to be incompatible with
the annotation before searching for spanning
trees. Our ?upward-downward? method for
constructing a graph of supported edges first
enumerates a set of candidate top nodes for every
fudge expression, then uses that information
to infer a set of supported parents for every
node.12 The supported edge graph then consists
of vertices lexnodes(A) ? {root} and edges
?
v?lexnodes(A) {(v? v?) ? v? ? suppParentsA(v)}.
From this graph we can count all directed span-
ning trees in cubic time using Kirchhoff?s matrix
tree theorem (Chaiken and Kleitman, 1978; Smith
and Smith, 2007; Margoliash, 2010).13 If some
lexical node has no supported parents, this reflects
conflicting constraints in the annotation, and no
spanning tree will be found.
Promiscuity will tend to be higher for longer
sentences. To control for this, we define a second
quantity, the annotation?s commitment quotient
(commitment being the opposite of promiscuity),
11This measure assumes a fixed lexical analysis (set of lex-
ical nodes) and does not consider anaphoric links. Coordinate
structures are simplified into ordinary dependencies, with co-
ordinate phrases headed by the coordinator?s lexical node. If
a coordination node has multiple coordinators, one is arbi-
trarily chosen as the head and the others as its dependents.
12Python code for these algorithms appears in Schneider
et al (2013) and the accompanying software release.
13Due to a technicality with non-member attachments to
fudge nodes, for some annotations this is only an upper bound
on promiscuity; see Schneider et al (2013).
which normalizes for the number of possible span-
ning trees given the sentence length. The commit-
ment quotient for an annotation of a sentence with
n?1 lexical nodes and one root node is given by:
com(A) = 1 ?
log prom(A)
log nn?2
(the logs are to attenuate the dominance of the ex-
ponential term). This will be 1 if only a single
tree is supported by the annotation, and 0 if the
annotation does not constrain the structure at all.
(If the constraints in the annotation are internally
inconsistent, then promiscuity will be 0 and com-
mitment undefined.) In practice, there is a trade-
off between efficiency and commitment: more de-
tailed annotations require more time. The value of
minimizing promiscuity will therefore depend on
the resources and goals of the annotation project.
4.2 Inter-Annotator Agreement
FUDG can encode flat groupings and coreference
at the lexical level, as well as syntactic structure
over lexical items. Inter-annotator agreement can
be measured separately for each of these facets.
Pilot annotator feedback indicated that our initial
lexical-level guidelines were inadequate, so we fo-
cus here on measuring structural agreement pend-
ing further clarification of the lexical conventions.
Attachment accuracy, a standard measure for
evaluating dependency parsers, cannot be com-
puted between two FUDG annotations if either of
them underspecifies any part of the dependency
structure. One solution is to consider the inter-
section of supported full trees, in the spirit of
our promiscuity measure. For annotations A1 and
A2 of sentence s, one annotation?s supported an-
alyses can be enumerated and then filtered sub-
ject to the constraints of the other annotation.
The tradeoff between inter-annotator compatibil-
ity and commitment can be accounted for by tak-
ing their product, i.e. comPrec(A1 | A2) =
com(A1)
|supp(A1)?supp(A2)|
|supp(A1)|
.
A limitation of this support-intersection ap-
proach is that if the two annotations are not
compatible, the intersection will be empty. A
more fine-grained approach is to decompose
the comparison by lexical node: we general-
ize attachment accuracy with softComPrec(A1 |
A2) = com(A1)
?
`?s
?
i?{1,2} suppParentsAi (`)?
`?s suppParentsA1 (`)
, comput-
ing com(?) and suppParents(?) as in the previous
section. As lexical nodes may differ between the
two annotations, a reconciliation step is required
56
Language Tokens Rate (tokens/hr)
English Tweets (partial) 667 430
English Tweets (full) 388 250
Malagasy 4,184 47
Kinyarwanda 8,036 80
Table 1: Productivity estimates from pilot annotation project.
All annotators were native speakers of English.
to compare the structures: multiwords proposed in
only one of the two annotations are converted to
fudge expressions. Tokens annotated by neither
annotator are ignored. Like with the promiscuity
measure, we simplify coordinate structures to or-
dinary dependencies (see footnote 11).
5 Case Studies
5.1 Annotation Time
To estimate annotation efficiency, we performed
a pilot annotation project consisting of annotating
several hundred English tweets, about 1,000 sen-
tences in Malagasy, and a further 1,000 sentences
in Kinyarwanda.14 Table 1 summarizes the num-
ber of tokens annotated and the effort required. For
the two Twitter cases, the same annotator was first
permitted to do partial annotation of 100 tweets,
and then spend the same amount of time doing a
complete annotation of all tokens. Although this is
a very small study, the results clearly suggest she
was able to make much more rapid progress when
partial annotation was an option.15
This pilot study helped us to identify linguistic
phenomena warranting specific conventions: these
include wh-expressions, comparatives, vocatives,
discourse connectives, null copula constructions,
and many others. We documented these cases in a
20-page style guide for English,16 which informed
the subsequent pilot studies discussed below.
5.2 Underspecification and Agreement
We annotated 2 small English data samples in
order to study annotators? use of underspecifica-
tion. The first is drawn from Owoputi et al?s 2013
Twitter part-of-speech corpus; the second is from
the Reviews portion of the English Web Treebank
14Malagasy is a VOS Austronesian language spoken by 15
million people, mostly in Madagascar. Kinyarwanda is an
SVO Bantu language spoken by 12 million people mostly in
Rwanda. All annotations were done by native speakers of En-
glish. The Kinyarwanda and Malagasy annotators had basic
proficiency in these languages.
15As a point of comparison, during the Penn Treebank
project, annotators corrected the syntactic bracketings pro-
duced by a high-quality hand-written parser (Fidditch) and
achieved a rate of only 375 tokens/hour using a specialized
GUI interface (Marcus et al, 1993).
16Included with the data and software release (footnote 1).
Omit. prom Hist. Mean
1Ws MWs Tkns FEs 1 >1 ?10 ?102 com
Tweets 60 messages, 957 tokens
A 597 56 304 23 43 17 11 5 .96
B 644 47 266 28 37 23 12 6 .95
Reviews 55 sentences, 778 tokens
A 609 33 136 2 53 2 2 1 1.00
C ? D 643 19 116 114 11 44 38 21 .82
T 704 ? 74 ? 55 0 0 0 1
Table 2: Measures of our annotation samples. Note that
annotator ?D? specialized in noun phrase?internal structure,
while annotator ?C? specialized in verb phrase/clausal phe-
nomena; C ? D denotes the combination of their annotation
fragments. ?T? denotes our dependency conversion of the
English Web Treebank parses. (The value 1.00 was rounded
up from .9994.)
(EWTB) (Bies et al, 2012). (Our annotators only
saw the tokenized text.) Both datasets are infor-
mal and conversational in nature, and are dom-
inated by short messages/sentences. In spite of
their brevity, many of the items were deemed to
contain multiple ?utterances,? which we define to
include discourse connectives and emoticons (at
best marginal parts of the syntax); utterance heads
are marked with ** in figure 1.
Table 2 indicates the sizes of the two data sam-
ples, and gives statistics over the output of each
annotator: total counts of single-word and mul-
tiword lexical nodes, tokens not represented by
any lexical node, and fudge nodes; as well as
a histogram of promiscuity counts and the aver-
age of commitment quotients (see ?4.1). For in-
stance, the two sets of annotations obtained for the
Tweets sample used underspecification in 17/60
and 23/60 tweets, respectively, though the promis-
cuity rarely exceeded 100 compatible trees per an-
notation. Examples can be seen in figure 1, where
annotator ?A? marked only the noun phrase head
for the scarriest mystery door, opted not to choose
a head within the quantity 1 1/2, and left ambigu-
ous the attachment of the hedge like. The strong
but not utter commitment to the dependency struc-
ture is reflected in the mean commitment quotients
for this dataset, both of which exceed 0.95.
Inter-annotator agreement (IAA) is quantified in
table 3. The row marked A ? B, for instance,
considers the agreement between annotator ?A?
and annotator ?B?. Measuring IAA on the depen-
dency structure requires a common set of lexical
nodes, so a lexical reconciliation step ensures that
(a) any token used by either annotation is present
in both, and (b) no multiword node is present
in only one annotation?solved by relaxing in-
compatible multiwords to FEs (which increases
promiscuity). For Tweets, lexical reconciliation
57
thus reduces the commitment averages for each
annotation?to a greater extent for annotator ?A?
(.96 in table 2 vs. .82 in table 3) because ?A?
marked more multiwords. An analysis fully com-
patible with both annotations exists for only 27/60
sentences; the finer-grained softComPrec measure
(?4.2), however, offers insight into the balance be-
tween commitment and agreement.
Qualitatively, we observe three leading causes
of incompatibilities (disagreements): obvious an-
notator mistakes (such as the marked as a head);
inconsistent handling of verbal auxiliaries; and un-
certainty whether to attach expressions to a verb
or the root node, as with here in figure 1.17 An-
notators noticed occasional ambiguous cases and
attempted to encode the ambiguity with fudge ex-
pressions: again in the tweet maybe put it off un-
til you feel like ~ talking again ? is one example.
More often, fudge expressions proved useful for
syntactically difficult constructions, such as those
shown in figure 1 as well as: 2 shy of breaking it,
asked what tribe I was from, a $ 13 / day charge,
you two, and the most awkward thing ever.
5.3 Annotator Specialization
As an experiment in using underspecification for
labor division, two of the annotators of Reviews
data were assigned specific linguistic phenomena
to focus on. Annotator ?D? was tasked with the in-
ternal structure of base noun phrases, including re-
solving the antecedents of personal pronouns. ?C?
was asked to mark the remaining phenomena?
i.e., utterance/clause/verb phrase structure?but to
mark base noun phrases as fudge expressions,
leaving their internal structure unspecified. Both
annotators provided a full lexical analysis. For
comparison, a third individual, ?A,? annotated the
same data in full. The three annotators worked
completely independently.
Of the results in tables 2 and 3, the most notable
difference between full and specialized annotation
is that the combination of independent specialized
annotations (C ? D) produces somewhat higher
promiscuity/lower commitment. This is unsurpris-
ing because annotators sometimes overlook rela-
tionships that fall under their specialty.18 Still, an-
notators reported that specialization made the task
17Another example: Some uses of conjunctions like and
and so can be interpreted as either phrasal coordinators or dis-
course connectives (cf. The PDTB Research Group, 2007).
18A more practical and less error-prone approach might be
for specialists to work sequentially or collaboratively (rather
than independently) on each sentence.
com softComPrec
IAA 1 2 N|?|>0 1|2 2|1 F1
Tweets (N=60)
A ? B .82 .91 27 .57 .72 .63
Reviews (N=55)
A ? (C ? D) .95 .76 30 .64 .40 .50
A ? T .92 1 26 .48 .91 .63
(C ? D) ? T .73 1.00 28 .33 .93 .49
Table 3: Measures of inter-annotator agreement. Annotator
labels are as in table 2. Per-annotator com (with lexical rec-
onciliation) and inter-annotator softComPrec are aggregated
over sentences by arithmetic mean.
less burdensome, and the specialized annotations
did prove complementary to each other.19
5.4 Treebank Comparison
Though the annotators in our study were native
speakers well acquainted with representations of
English syntax, we sought to quantify their agree-
ment with the expert treebankers who created the
EWTB (the source of the Reviews sentences). We
converted the EWTB?s constituent parses to de-
pendencies via the PennConverter tool (Johansson
and Nugues, 2007),20 then removed punctuation.
Agreement with the converted treebank parses
appears in the bottom two rows of table 3. Be-
cause the EWTB commits to a single analysis,
precision scores are quite lopsided. Most of its
attachments are consistent with our annotations
(softComPrec > 0.9), but these allow many ad-
ditional analyses (hence the scores below 0.5).
6 Conclusion
We have presented a framework for simple depen-
dency annotation that overcomes some of the rep-
resentational limitations of unlabeled dependency
grammar and embraces the practical realities of
resource-building efforts. Pilot studies (in multiple
languages and domains, supported by a human-
readable notation and a suite of open-source tools)
showed this approach lends itself to rapid annota-
tion with minimal training.
The next step will be to develop algorithms ex-
ploiting these representations for learning parsers.
Other future extensions might include additional
expressive mechanisms (e.g., multi-headedness,
labels), crowdsourcing of FUDG annotations
(Snow et al, 2008), or even a semantic counter-
part to the syntactic representation.
19In fact, for only 2 sentences did ?C? and ?D? have in-
compatible annotations, and both were due to simple mis-
takes that were then fixed in the combination.
20We ran PennConverter with options chosen to emulate
our annotation conventions; see Schneider et al (2013).
58
Acknowledgments
We thank Lukas Biewald, Yoav Goldberg, Kyle Jerro, Vi-
jay John, Lori Levin, Andr? Martins, and several anony-
mous reviewers for their insights. This research was sup-
ported in part by the U. S. Army Research Laboratory and
the U. S. Army Research Office under contract/grant number
W911NF-10-1-0533 and by NSF grant IIS-1054319.
References
Timothy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural Language
Processing, Second Edition. CRC Press, Taylor and
Francis Group, Boca Raton, FL.
Benjamin K. Bergen and Nancy Chang. 2005. Embod-
ied Construction Grammar in simulation-based lan-
guage understanding. In Jan-Ola ?stman and Mir-
jam Fried, editors, Construction grammars: cog-
nitive grounding and theoretical extensions, pages
147?190. John Benjamins, Amsterdam.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. Technical Re-
port LDC2012T13, Linguistic Data Consortium,
Philadelphia, PA.
Alena B?hmov?, Jan Hajic?, Eva Hajic?ov?, Barbora
Hladk?, and Anne Abeill?. 2003. The Prague De-
pendency Treebank: a three-level annotation sce-
nario. In Treebanks: building and using parsed cor-
pora, pages 103?127. Springer.
Seth Chaiken and Daniel J. Kleitman. 1978. Matrix
Tree Theorems. Journal of Combinatorial Theory,
Series A, 24(3):377?381.
Noam Chomsky. 1957. Syntactic Structures. Mouton,
La Haye.
Stephen Clark and James Curran. 2006. Partial training
for a lexicalized-grammar parser. In Proceedings of
the Human Language Technology Conference of the
NAACL (HLT-NAACL 2006), pages 144?151. As-
sociation for Computational Linguistics, New York
City, USA.
William Croft. 2001. Radical Construction Grammar:
Syntactic Theory in Typological Perspective. Oxford
University Press, Oxford.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies man-
ual. http://nlp.stanford.edu/downloads/
dependencies_manual.pdf.
Kitty Burns Florey. 2006. Sister Bernadette?s Barking
Dog: The quirky history and lost art of diagramming
sentences. Melville House, New York.
Victoria Fossum and Roger Levy. 2012. Sequential
vs. hierarchical syntactic models of human incre-
mental sentence processing. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics (CMCL 2012), pages 61?69. As-
sociation for Computational Linguistics, Montr?al,
Canada.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS Tagging and Parsing the Twitter-
verse. In Proceedings of the 2011 AAAI Workshop
on Analyzing Microtext, pages 20?25. AAAI Press,
San Francisco, CA.
The PDTB Research Group. 2007. The Penn Discourse
Treebank 2.0 annotation manual. Technical Report
IRCS-08-01, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania, Philadelphia, PA.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: the Prague Dependency Treebank. In Eva
Hajic?ov?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov?, pages 12?
19. Prague Karolinum, Charles University Press,
Prague.
Jan Hajic?, Barbora Vidov? Hladk?, and Petr Pajas.
2001. The Prague Dependency Treebank: anno-
tation structure and support. In Proceedings of
the IRCS Workshop on Linguistic Databases, pages
105?114. University of Pennsylvania, Philadelphia,
USA.
Richard A. Hudson. 1984. Word Grammar. Blackwell,
Oxford.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-99), pages 73?79. Association for Computa-
tional Linguistics, College Park, Maryland, USA.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English.
In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-
chnek, and Mare Koit, editors, Proceedings of the
16th Nordic Conference of Computational Linguis-
tics (NODALIDA-2007), pages 105?112. Tartu, Es-
tonia.
Martha Kolln and Robert Funk. 1994. Understanding
English Grammar. Macmillan, New York.
Sandra K?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool, San Rafael, CA.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 507?514. Association for Computa-
tional Linguistics, Sydney, Australia.
59
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David Marec?ek, Martin Popel, Loganathan Ramasamy,
Jan ?te?p?nek, Daniel Zeman, Zdene?k ?abokrtsk?,
and Jan Hajic?. 2013. Cross-language study on in-
fluence of coordination style on dependency parsing
performance. Technial Report 49, ?FAL MFF UK.
Jonathan Margoliash. 2010. Matrix-Tree Theorem for
directed graphs. http://www.math.uchicago.
edu/~may/VIGRE/VIGRE2010/REUPapers/
Margoliash.pdf.
Igor Aleksandrovic? Mel?c?uk. 1988. Dependency Syn-
tax: Theory and Practice. SUNY Press, Albany,
NY.
Joakim Nivre. 2005. Dependency grammar and depen-
dency parsing. Technical Report MSI report 05133,
V?xj? University School of Mathematics and Sys-
tems Engineering, V?xj?, Sweden.
Timothy Osborne, Michael Putnam, and Thomas Gro?.
2012. Catenae: introducing a novel unit of syntactic
analysis. Syntax, 15(4):354?396.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390. Association for Computational Lin-
guistics, Atlanta, Georgia, USA.
Alonzo Reed and Brainerd Kellogg. 1877. Work on
English grammar & composition. Clark & Maynard.
Federico Sangati and Chiara Mazza. 2009. An English
dependency treebank ? la Tesni?re. In Marco Pas-
sarotti, Adam Przepi?rkowski, Savina Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eigth
International Workshop on Treebanks and Linguistic
Theories, pages 173?184. EDUCatt, Milan, Italy.
Gerold Schneider. 1998. A linguistic comparison of
constituency, dependency and link grammar. Mas-
ter?s thesis, University of Zurich.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. arXiv:1306.2091
[cs.CL]. arxiv.org/pdf/1306.2091.
Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?.
1986. The Meaning of the Sentence in its Seman-
tic and Pragmatic Aspects. Reidel, Dordrecht and
Academia, Prague.
Stuart M. Shieber. 1992. Constraint-Based Grammar
Formalisms. MIT Press, Cambridge, MA.
Daniel Sleator and Davy Temperly. 1993. Parsing En-
glish with a link grammar. In Proceedings of the
Third International Workshop on Parsing Technol-
ogy (IWPT?93), pages 277?292. Tilburg, Nether-
lands.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic models of nonprojective dependency trees.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 132?140. Associa-
tion for Computational Linguistics, Prague, Czech
Republic.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP 2008), pages 254?263. As-
sociation for Computational Linguistics, Honolulu,
Hawaii.
Mark Steedman. 2000. The Syntatic Process. MIT
Press, Cambridge, MA.
Luc Steels, Jan-Ola ?stman, and Kyoko Ohara, editors.
2011. Design patterns in Fluid Construction Gram-
mar. Number 11 in Constructional Approaches to
Language. John Benjamins, Amsterdam.
Lucien Tesni?re. 1959. El?ments de Syntaxe Struc-
turale. Klincksieck, Paris.
Michael Tomasello. 2003. Constructing a Language: A
Usage-Based Theory of Language Acquisition. Har-
vard University Press, Cambridge, MA.
60
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 1?13,
Baltimore, Maryland, USA, June 27, 2014.
c
?2014 Association for Computational Linguistics
MITEXTEXPLORER: Linked brushing and mutual information for
exploratory text data analysis
Figure 1: Screenshot of MITEXTEXPLORER, analyzing geolocated tweets.
Brendan O?Connor
Machine Learning Department
Carnegie Mellon University
brenocon@cs.cmu.edu
http://brenocon.com
Abstract
In this paper I describe a preliminary ex-
perimental system, MITEXTEXPLORER,
for textual linked brushing, which allows
an analyst to interactively explore statis-
tical relationships between (1) terms, and
(2) document metadata (covariates). An
analyst can graphically select documents
embedded in a temporal, spatial, or other
continuous space, and the tool reports
terms with strong statistical associations
for the region. The user can then drill
down to specific term and term groupings,
viewing further associations, and see how
terms are used in context. The goal is to
rapidly compare language usage across in-
teresting document covariates.
I illustrate examples of using the tool on
several datasets: geo-located Twitter mes-
sages, presidential State of the Union ad-
dresses, the ACL Anthology, and the King
James Bible.
1 Introduction: Can we ?just look? at
statistical text data?
Exploratory data analysis (EDA) is an approach
to extract meaning from data, which emphasizes
learning about a dataset through an iterative pro-
cess of many analyses which suggest and refine
possible hypotheses. It is vital in early stages of a
data analysis for data cleaning and sanity checks,
which are crucial to help ensure a dataset will be
useful. Exploratory techniques can also suggest
possible hypotheses or issues for further investi-
gation.
1
3/18/14 Anscombe's_quartet_3.svg
file:///Users/brendano/projects/textexplore/writing/Anscombe's_quartet_3.svg 1/1
Figure 2: Anscombe Quartet. (Source: Wikipedia)
The classical approach to EDA, as pioneered in
works such as Tukey (1977) and Cleveland (1993)
(and other work from the Bell Labs statistics group
during that period) emphasizes visual analysis un-
der nonparametric, model-free assumptions, in
which visual attributes are a fairly direct reflec-
tion of numerical or categorical aspects of data.
As a simple example, consider the well-known
Anscombe Quartet (1973), a set of four bivari-
ate example datasets. The Pearson correlation, a
very widely used measure of dependence that as-
sumes a linear Gaussian model of the data, finds
that each dataset has an identical amount of de-
pendence (r = 0.82). However, a scatterplot in-
stantly reveals that very different dependence re-
lationships hold in each dataset (Figure 2). The
scatterplot is possibly the simplest visual analysis
tool for investigating the relationship between two
variables, in which the variables? numerical values
are mapped to horizontal and vertical space. While
the correlation coefficient is a model-based analy-
sis tool, the scatterplot is model-free (or at least, it
is effective under an arguably wider range of data
generating assumptions), which is crucial for this
example.
This nonparametric, visual approach to EDA
has been encoded into many data analysis pack-
ages, including the now-ubiquitous R language (R
Core Team, 2013), which descends from earlier
software by the Bell Labs statistics group (Becker
and Chambers, 1984). In R, tools such as his-
tograms, boxplots, barplots, dotplots, mosaicplots,
etc. are built-in, basic operators in the language.
(Wilkinson (2006)?s grammar of graphics more
extensively systematizes this approach; see also
(Wickham, 2010; Bostock et al., 2011).)
In the meantime, textual data has emerged as
a resource of increasing interest for many scien-
Figure 3: Linked brushing with the anal-
ysis software GGobi. More references at
source: http://www.infovis-wiki.net/index.
php?title=Linking_and_Brushing
tific, business, and government data analysis ap-
plications. Consider the use case of automated
content analysis (a.k.a. text mining) as a tool for
investigating social scientific and humanistic ques-
tions (Grimmer and Stewart, 2013; Jockers, 2013;
Shaw, 2012; O?Connor et al., 2011). The content
of the data is under question: analysts are inter-
ested in what/when/how/by-whom different con-
cepts, ideas, or attitudes are expressed in a cor-
pus, and the trends in these factors across time,
space, author communities, or other document-
level covariates (often called metadata). Compar-
isons of word statistics across covariates are ab-
solutely essential to many interesting questions or
social measurement problems, such as
? What topics tend to get censored by the Chi-
nese government online, and why (Bamman
et al., 2012; King et al., 2013)? Covari-
ates: whether a message is deleted by cen-
sors, time/location of message.
? What drives media bias? Do newspapers
slant their coverage in response to what read-
ers want (Gentzkow and Shapiro, 2010)? Co-
variates: political preferences of readers,
competitiveness of media markets.
There exist dozens, if not more, of other examples
in social scientific and humanities research; see
references in O?Connor et al. (2011); O?Connor
(2014).
In this work, I focus on the question: What
2
should be the baseline exploratory tools for textual
data, to discover important statistical associations
between text and document covariates? Ideally,
we?d like to ?just look? at the data, in the spirit of
scatterplotting the Anscombe Quartet. An analy-
sis tool to support this should not require any sta-
tistical model assumptions, and should display the
data in as direct a form as possible.
For low-dimensional, non-textual data, the base
functionality of R prescribes a broad array of use-
ful defaults: one-dimensional continuous data can
be histogrammed (hist(x)), or kernel density plot-
ted (plot(density(x))), while the relationship be-
tween two dimensions of continuous variables can
be viewed as a scatterplot (plot(x,y)); or perhaps
a boxplot for discrete x and continous y (box-
plot(x,y)); and so on. Commercial data analysis
systems such as Excel, Stata, Tableau, JMP, etc.,
have similar functionality.
These visual tools can be useful for analyz-
ing derived content statistics from text?for exam-
ple, showing a high-level topic or sentiment fre-
quency trending over time?but they cannot visu-
alize the text itself. Text data consists of a linear
sequence of high-dimensional discrete variables
(words). The most aggressive and common anal-
ysis approach, bag-of-words, eliminates the prob-
lematic sequential structure, by reducing a docu-
ment to a high-dimensional discrete counts over
words. But still, none of the above visual tools
makes sense for visualizing a word distribution;
many popular tools simply crash or become very
slow when given word count data. And besides
the issues of discrete high-dimensionality, text is
unique in that it has to be manually read in order
to more reliably understand its meaning. Natural
language processing tools can sometimes extract
partial views of text meaning, but full understand-
ing is a long ways off; and the quality of available
NLP tools varies greatly across corpora and lan-
guages. A useful exploratory tool should be able
to work with a variety of levels of sophistication
in NLP tooling, and allow the user to fall back to
manual reading when necessary.
2 MITEXTEXPLORER: linked brushing
for text and covariate correlations
The analysis tool presented here, MITEXTEX-
PLORER, is designed for exploratory analysis of
relationships between document covariates?such
as time, space, or author community?against tex-
tual variables?words, or other units of meaning,
that can be counted per document. Unlike topic
model approaches to analyzing covariate-text re-
lationships (Mimno, 2012; Roberts et al., 2013),
there is no dimension reduction of the terms. In-
stead, interactivity allows a user to explore more of
the high-dimensional space, by specifying a doc-
ument selection (Q) and/or a term selection (T ).
We are inspired by the linking and brushing family
of techniques in interactive data visualization, in
which an analyst can select a group of data points
under a query in one covariate space, and see the
same data selection in a different covariate space
(Figure 3; see Buja et al. (1996), and e.g. Becker
and Cleveland (1987); Buja et al. (1991); Martin
and Ward (1995); Cook and Swayne (2007)). In
our case, one of the variables is text.
The interface consists of several linked views,
which contain:
(A) a view of the documents in a two-dimensional
covariate space (e.g. scatterplot),
(B) an optional list of pinned terms,
(C) document-associated terms: a view of the rel-
atively most frequent terms for the current
document selection,
(D) term-associated terms: a view of terms that
relatively frequently co-occur with the current
term selection; and
(E) a keyword-in-context (KWIC) display of tex-
tual passages for the current term selection.
Figure 1 shows the interface viewing a corpus of
201,647 geo-located Twitter messages from 2,000
users during 2009-2012, which have been tagged
with their author?s spatial coordinates through a
mobile phone client and posted publicly; for data
analysis, their texts have been lowercased and
tokenized appropriately (Owoputi et al., 2013;
O?Connor et al., 2010). Since this type of corpus
contains casual, everyday language, it is a dataset
that may illuminate geographic patterns of slang
and lexical variation in local dialects (Eisenstein
et al., 2012, 2010).
The document covariate display (A) uses (longi-
tude, latitude) positions as the 2D space. The cor-
pus has been preprocessed to define a document as
the concatenation of messages from a single au-
thor, with its position the average location of the
author?s messages. When the interface loads, all
3
points in (A) are initially gray, and all other panels
are blank.
2.1 Covariate-driven queries
A core interaction, brushing, consists of using the
mouse to select a rectangle in the (x,y) covariate
space. Figure 1 shows a selection around the Bay
Area metropolitan area (blue rectangle). Upon
selection, the document-driven term display (C)
is updated to show the relatively most frequent
terms in the document selection. Let Q denote
the set of documents that are selected by the cur-
rent covariate query. The tool ranks terms w by
their (exponentiated) pointwise mutual informa-
tion, a.k.a. lift, for Q:
lift(w;Q) =
p(w|Q)
p(w)
(
=
p(w,Q)
p(w)p(Q)
)
(1)
This quantity measures how much more frequent
the term is in the queryset, compared to the base-
line global probability in the corpus (p(w)). Prob-
abilities are calculated with simple MLE relative
frequencies, i.e.
p(w|Q)
p(w)
=
?
d?Q
n
dw
?
d?Q
n
d
N
n
w
(2)
where d denotes a document ID, n
dw
the count
of word w in document d, and N the number
of tokens in the corpus. PMI gives results that
are much more interesting than results from rank-
ing w on raw probability within the query set
(p(w|Q)), since that simply shows grammatical
function words or other terms that are common
both in the queryset and across the corpus, and not
distinctive for the queryset.
1
A well-known weakness of PMI is over-
emphasis on rare terms; terms that appear
only in the queryset, even if they appear only
once, will attain the highest PMI value. One
way to address this is through a smoothing
prior/pseudocounts/regularization, or through sta-
tistical significance ranking (see ?3). For simplic-
ity, we use a minimum frequency threshold filter.
The user interface allows minimums for either lo-
cal or global term frequencies, and to easily ad-
just them, which naturally shifts the emphasis be-
tween specific and generic language. All methods
1
The term ?lift? is used in business applications (Provost
and Fawcett, 2013), while PMI has been used in many NLP
applications to measure word associations.
to protect against rare probabilistic events neces-
sarily involve such a tradeoff parameter that the
user ought to experiment with; given this situation,
we might prefer a transparent mechanism instead
of mathematical priors (though see also ?3).
Figure 1 shows that hella is the highest ranked
term for this spatial selection (and freqency thresh-
old), occurring 7.8 times more frequently com-
pared to the overall corpus; this comports with
surveyed intuitions of Californian English speak-
ers (Bucholtz et al., 2007). For full transparency
to the user, the local and global term counts are
shown in the table. (Since hella occurred 18 times
in the queryset and 90 times globally, this im-
plies the simple conditional probability p(Q|w) =
18/90; and indeed, ranking on p(Q|w) is equiva-
lent to ranking on PMI, since exponentiated PMI
is p(Q|w)/p(Q).) The user can also sort by local
count to see the raw most-frequent term report for
the document selection. As the user reshapes the
query box, or drags it around the space, the terms
in panel (C) are updated.
Not shown are options to change the term fre-
quency representation. For exposition here, proba-
bilities are formulated as counts of tokens, but this
can be problematic for social media data, since a
single user might use a term a very large number
of times. The above analysis is conducted with
an indicator representation of terms per user, so
all frequencies refer to the probability that a user
uses the term at least once. However, the other ex-
amples in this paper use token-level frequencies,
which seem to work fine. It is an interesting statis-
tical analysis question how to derive a single range
of methods to work across these situations.
2.2 Term selection and KWIC views
Terms in the table (C) can be clicked and selected,
forming a term selection as a set of terms T . This
action drives several additional views:
(A) documents containing the term are high-
lighted in the document covariate display
(here, in red),
(E) examples of the term?s usage, in Keyword-in-
Context style with vertical alignment for the
query term; and
(D) other terms that frequently co-occur with T
(?2.3).
The KWIC report in (E) shows examples of term?s
usage. For example, why is the term ?la? in
4
Figure 4: KWIC examples of ?la? usage in tweets
selected in Figure 1.
the PMI list? My initial thought was that this
was an example of ?LA?, short for ?Los Ange-
les?. But clicking on ?la? instantly disproves this
hypothesis?Figure 4, showing the Los Angeles
sense, but also the ?la la la? sense, as well as the
Spanish function word.
The KWIC alignment makes it easier to rapidly
browse examples, and think about a rough as-
sessment of their word sense or how they are
used. Figure 5 compares how the term ?God?
is used by U.S. presidents Ronald Reagan and
Barack Obama, in a corpus of State of the Union
speeches, from two different displays of the tool.
The predominant usage is the invocation of ?God
bless America? or similar, nearly ornamental, ex-
pressions, but Reagan also has substantive us-
ages, such as references to the role of religion
in schools. The vertical alignments of the right-
side context words makes it easy to see the ?God
bless? word sense. I initially found this exam-
ple simply by browsing the covariate space, and
noticing ?god? as a frequent term for Reagan,
though still occurring for other presidents; the
KWIC drilldown better illuminated these distinc-
tions, and suggests differences in political ideolo-
gies between the presidents.
In lots of exploratory text analysis work, espe-
cially in the topic modeling literature, it is com-
mon to look at word lists produced by a statistical
analysis method and think about what they might
mean. At least in my experience doing this, I?ve
often found that seeing examples of words in con-
text has disproved my initial intuitions. Hopefully,
supporting this activity in an interactive user inter-
face might make exploratory analysis more effec-
tive. Currently, the interface simply shows a sam-
ple of in-context usages from the document query-
set; it would be interesting to perform grouping
and stratified sampling based on local contextual
statistics. Summarizing local context by frequen-
cies could be done as a trie visualization (Watten-
berg and Vi?egas, 2008); see ?5.
2.3 Term-association queries
When a term is selected, its interaction with co-
variates is shown by highlighting documents in (B)
that contain the term. This can be thought of as
another document query: instead of being spec-
ified as a region in the covariate space, is spec-
ified as a fragment of the discrete lexical space.
As illustrated in much previous work (e.g. Church
and Hanks (1990); Turney (2001, 2002)), word-to-
word PMI scores can find other terms with similar
meanings, or having interesting semantic relation-
ships, to the target term.
2
This panel ranks terms u by their association
with the query term v. The simplest method is to
analyze the relative frequencies of terms in docu-
ments that contain v,
bool-tt-epmi(u, v) =
p(w
i
= u|v ? supp(d
i
))
p(w
i
= u)
Here, the subscript i denotes a token position in
the entire corpus, for which there is a wordtype
w
i
and a document ID d
i
. In this notation, the
covariate PMI in 2.1 would be p(w
i
= u|d
i
?
Q)/p(w
i
= u). supp(d
i
) denotes the set of terms
that occur at least once in document d
i
.
This measure is a very simple extension of
the document covariate selection mechanism, and
easy to understand. However, it is less satisfy-
ing for longer documents, since a larger number
of occurrences of v do not lead to a stronger asso-
ciation score. A possible extension is to consider
the joint random event of selecting two tokens i
and j in the corpus, and consider if the two to-
kens being in the same document is informative
for whether the tokens are the words (u, v), i.e.
2
For finding terms with similar semantic meaning, dis-
tributional similarity may be more appropriate (Turney and
Pantel, 2010); this could be interesting to incorporate into the
software.
5
Figure 5: KWIC examples of ?God? in speeches by Reagan versus Obama.
PMI[(w
i
, w
j
) = (u, v); d
i
= d
j
],
freq-tt-epmi(u, v) =
p(w
i
= u,w
j
= v|d
i
= d
j
)
p(w
i
= u,w
j
= v)
In terms of word counts, this expression has the
form
freq-tt-epmi(u, v) =
?
d
n
du
n
dv
n
u
n
v
N
2
?
d
n
2
d
The right-side term is a normalizing constant in-
variant to u and v. The left-side term is interesting:
it can be viewed as a similarity measure, where
the numerator is the inner product of the inverted
term-document vectors n
.,u
and n
.,v
, and the de-
nominator is the product of their `
1
norms. This
is a very similar form as cosine similarity, which
is another normalized inner product, except its de-
nominator is the product of the vectors? `
2
norms.
Term-to-term associations allow a navigation of
the term space, complementing the views of terms
driven by document covariates. This part of the
tool is still at a more preliminary stage of develop-
ment. One important enhancement would be ad-
justment of the context window size allowed for
co-occurrences; the formulations above assume a
context window the size of the document. Medium
sized context windows might capture more fo-
cused topical content, especially in very long dis-
courses such as speeches; and the smallest context
windows, of size 1, should be more like colloca-
tion detection (though see ?3; this is arguably bet-
ter done with significance tests, not PMI).
2.4 Pinned terms
The term PMI views of (C) and (D) are very dy-
namic, which can cause interesting terms to disap-
pear when their supporting query is changed. It is
often useful to select terms to be constantly viewed
when the document covariate queries change.
Any term can be double-clicked to be moved to
the the table of pinned terms (B). The set of terms
here does not change as the covariate query is
changed; a user can fix a set of terms and see how
their PMI scores change while looking at differ-
ent parts of the covariate space. One possible use
of term pinning is to manually build up clusters of
terms?for example, topical or synonymous term
sets?whose aggregate statistical behavior (i.e. as
a disjunctive query) may be interesting to observe.
Manually built sets of keywords are a very useful
form of text analysis; in fact, the WordSeer cor-
pus analysis tool has explicit support to help users
create them (Shrikumar, 2013).
3 Statistical term association measures
There exist many measures to measure the sta-
tistical strength of an association between a term
and a document covariate, or between two terms.
A number of methods are based on significance
testing, looking for violations of a null hypothesis
that term frequencies are independent. For collo-
cation detection, which aims to find meaningful
non-compositional lexical items through frequen-
cies of neighboring words, likelihood ratio (Dun-
ning, 1993) and chi-square tests have been used
(see review in Manning and Sch?utze (1999)). For
term-covariate associations, chi-square tests were
6
used by Gentzkow and Shapiro (2010) to find po-
litically loaded phrases often used by members of
one political party; this same method is often used
as a feature selection method for supervised learn-
ing (Guyon and Elisseeff, 2003).
The approach we take here is somewhat differ-
ent, being a point estimate approach, analyzing
the estimated difference (and giving poor results
when counts are small). Some related work for
topic model analysis, looking at statistical associa-
tions between words and latent topics (as opposed
to between words and observed covariates in this
work) includes Chuang et al. (2012b), whose term
saliency function measures one word?s associa-
tions against all topics; a salient term tends to have
most of its probability mass in a small set of top-
ics. The measure is a form of mutual information,
3
and may be useful for our purposes here if the user
wishes to see a report of distinctive terms for a
group of several different observed covariate val-
ues at once. Blei and Lafferty (2009) ranks words
per topic by a measure inspired by TFIDF, which
like PMI downweights words that are generically
common across all topics.
Finally, hierarchical priors and regularizers can
also be used; for example, by penalizing the
log-odds parameterization of term probabilities
(Eisenstein et al., 2011; Taddy, 2013). These
methods are better in that they incorporate both
protection against small count situations, while
paying attention to effect size, as well as allow-
ing overlapping covariates and regression control
variables; but unfortunately, they are more compu-
tationally intensive, as opposed to the above mea-
sures which all work directly from sufficient count
statistics. An association measure that fulfilled all
these desiderata would be very useful. For term-
covariate analysis, Monroe et al. (2008) contains a
review of many different methods, from both po-
litical science as well as computer science; they
also propose a hierarchical prior method, and to
rank by statistical significance via the asymptotic
3
This is apparent as follows, using notation from their sec-
tion 3.1:
saliency(w) = p(w)
?
T
p(T |w) log[p(T |w)/p(T )]
=
?
T
p(w, T ) log[p(w, T )/[p(w)p(T )]]
This might be called a ?half-pointwise? mutual information:
between a specific word w and the topic random variable T .
Mutual information is
?
w
saliency(w).
standard error of the terms? odds ratios.
Given the large amount of previous work using
the significance approach, it merits further explo-
ration for this system.
4 Phrase selection
The simplest approach to defining the terms is to
use all words (unigrams). This can be insightful,
but single words are both too coarse and too nar-
row a unit of analysis. They can be too narrow
when there are multiple ways of saying the same
thing, such as synonyms?for example, while we
have evidence about differing usages of the term
?god? in presidential rhetoric, in order to make a
claim about religious themes, we might need to
find other terms such as ?creator?, ?higher power?,
etc. Another problematic case is alternate names
or anaphoric references to an entity. In general,
any NLP tool that extracts interesting discrete vari-
able indicators of word meaning could be used
for mutual information and covariate exploratory
analysis?for example, a coreference system?s en-
tity ID predictions could be browsed by the system
as the term variables. (More complex concepts, of
course, would also require more UI support.)
At the same time, words can be too coarse com-
pared to the longer phrases they are contained
within, which often contain more interesting and
distinctive concepts: for example, ?death tax?
and ?social security? are important concepts in
U.S. politics that get missed under a unigram anal-
ysis. In fact, Sim et al. (2013)?s analysis of U.S.
politicians? speeches found that domain experts
had a hard time understanding unigrams out-of-
context, but bigrams and trigrams worked much
better; Gentzkow and Shapiro (2010) similarly fo-
cus on partisan political phrases.
It sometimes works to simply add overlap-
ping n-grams as more terms, but sometimes odd
phrases get selected that cross constituent bound-
aries from their source sentences, and are thus not
totally meaningful. I?ve experimented with a very
strong filtering approach to phrase selection: be-
sides using all unigrams, take all n-grams up to
length 5 that have nominal part-of-speech patterns:
either the sequence consists of zero or more ad-
jectives followed by one or more noun tokens, or
all tokens were classified as names by a named
entity recognition system.
4
This tends to yield
4
For traditional text, the tool currently uses Stanford
CoreNLP; for Twitter, CMU ARK TweetNLP.
7
Figure 6: MITEXTEXPLORER for paper titles in the ACL Anthology (Radev et al., 2009). Y-axis is venue
(conference or journal name), X-axis is year of publication. Unlike the other figures, docvar-associated
terms are sorted alphabetically.
Figure 7: MITEXTEXPLORER for the King James Bible. Y-axis is book, X-axis is chapter (truncated to
39).
8
(partial) constituents, and nouns tend to be more
interesting than other content words (perhaps be-
cause they are relatively less reliant on predicate-
argument structure to express their semantics?as
opposed to adjectives or verbs, say?and a bag-of-
terms analysis does not allow expression of argu-
ment structure.) However, for many corpora, POS
or NER taggers work poorly?for example, I?ve
seen paper titles from the ACL Anthology have
capitalized prepositions tagged as names?so sim-
pler stopword heuristics are necessary.
The phrase selection approach could be im-
proved in many ways; for example, a real noun
phrase recognizer could get important (NP PP)
constructs like ?war on terror.? Furthermore,
Chuang et al. (2012a) find that while these sorts
of syntactic features are helpful in choosing useful
keyphrases to summarize of scientific abstracts,
it is also very useful to add in collocation de-
tection scores. Similarly to the PMI calculations
used here, likelihood ratio or chi-square collo-
cation detection statistics are also very rapid to
compute and may benefit from interactive adjust-
ment of decision thresholds. More generally, any
type of lexicalized linguistic structures could po-
tentially be used, such as dependency paths or
constituents from a syntactic parser, or predicate-
argument structures from a semantic parser. Lin-
guistic structures extracted from more sophisti-
cated NLP tools may indeed be better-generalized
units of linguistic meaning compared to words and
phrases, but they will still bear the same high-
dimensionality issues for data analysis purposes.
5 Related work: Exploratory text
analysis
Many systems and techniques have been devel-
oped for interactive text analysis. Two such sys-
tems, WordSeer and Jigsaw, have been under de-
velopment for several years, each having had a se-
ries of user experiments and feedback. Recent and
interesting review papers and theses are available
for both of them.
The WordSeer system (Shrikumar, 2013)
5
con-
tains many different interactive text visualization
tools, including syntax-based search, and was ini-
tially designed for the needs of text analysis in
the humanities; the WordSeer 3.0 system includes
a word frequency analysis component that can
compare word frequencies along document covari-
5
http://wordseer.berkeley.edu/
ates. Interestingly, Shrikumar found in user stud-
ies with literary experts that data comparisons and
annotation/note-taking support were very impor-
tant capabilities to add to the system. Unique to
the work in this paper is the emphasis on condi-
tioning on document covariates to analyze rela-
tive word frequencies, and encouraging the user to
change the statistical parameters that govern text
correlation measurements. (The term pinning and
term-to-term association techniques are certainly
less developed than previous work.)
Another text analysis system is Jigsaw (G?org
et al., 2013),
6
originally developed for investiga-
tive analysis (as in law enforcement or intelli-
gence), which again has many features. It empha-
sizes visualizations based on entity extractions,
such as for names, places, and dates. G?org et al.
note that errors in entity extraction were a major
problem for users; this might be a worthwhile ar-
gument to focus on getting something to first work
with simple words/phrases before tackling more
complex units of meaning. A section of the review
paper is entitled ?Reading the documents still mat-
ters?, pointing out that analysts did not want just to
visualize high-level relationships, but also wanted
to read documents in context; this capability was
added to later versions of Jigsaw, and supports the
emphasis here on the KWIC display.
Both these systems also use variants of Watten-
berg and Vi?egas (2008)?s word tree visualization,
which gives a sequential word frequencies as a
tree (i.e., what computational linguists might call a
trie representation of a high-order Markov model).
The ?God bless? word sense example from ?2 in-
dicates that such statistical summarization of local
contextual information may be useful to integrate;
it is worth thinking how to integrate this against
the important need of document covariate analy-
sis, while being efficient with the use of space.
Many other systems, especially ones designed
for literary content analysis, emphasize concor-
dances and keyword searches within a text; for
example, Voyeur/Voyant (Rockwell et al., 2010),
7
which also features some document covariate
analysis through temporal trend analyses for indi-
vidual terms. Another class of approaches empha-
sizes the use of document clustering or topic mod-
els (Gardner et al., 2010; Newman et al., 2010;
6
http://www.cc.gatech.edu/gvu/ii/
jigsaw/
7
http://voyant-tools.org/,
http://hermeneuti.ca/voyeur
9
Grimmer and King, 2011; Chaney and Blei, 2013),
while Overview
8
emphasizes hierarchical docu-
ment clustering paired with manual tagging.
Finally, considerable research has examined
exploratory visual interfaces for information re-
trieval, in which a user specifies an information
need in order to find relevant documents or pas-
sages from a corpus (Hearst (2009), Ch. 10). In-
formation retrieval problems have some similari-
ties to text-as-data analysis in the need for an ex-
ploratory process of iterative refinement, but the
text-as-data perspective differs in that it requires
an analyst to understand content and contextual
factors across multiple or many documents.
6 Future work
The current MITEXTEXPLORER system is an ex-
tremely simple prototype to explore what sorts of
?bare words? text-and-covariates analyses are pos-
sible. Several major changes will be necessary for
more serious use.
First, essential basic capabilities must be added,
such as a search box the user can use to search and
filter the term list.
Second, the document covariate display needs
to support more than just scatterplots. When there
are hundreds or more documents, summarization
is necessary in the form of histograms, kernel den-
sity plots, or other tools. For example, for a large
corpus of documents over time, a lineplot or tem-
poral histogram is more appropriate, where each
timestep has a document count. The ACL An-
thology scatterplot (Figure 6, Radev et al. (2009)),
which has hundreds of overplotted points at each
(year,venue) position, makes clear the limitations
of the current approach.
Better visual feedback for term selections here
could be useful?for example, sizing document
points monotonically with the term?s frequency
(rather than just presence/absence), or using
stacked line plots?though certain visual depic-
tions of frequency may be difficult given the Zip-
fian distribution of word frequencies.
Furthermore, document structures may be
thought of as document covariates. A single book
has interesting internal variation that could be an-
alyzed itself. Figure 7 shows the King James
Bible, which has a hierarchical structure of book,
chapter, and verse. Here, the (y,x) coordinates
8
https://www.overviewproject.org/ http:
//overview.ap.org/
represent books and chapters. A more special-
ized display for book-level structures, or other dis-
course structures, may be appropriate for book-
length texts.
Finally, a major goal of this work is to use anal-
ysis methods that can be computed on the fly,
but the current prototype only works with small
datasets. Hierarchical spatial indexing techniques
(e.g. r-trees), may make it possible to interactively
compute sums for covariate PMI scoring over very
large numbers of documents. Text indexing is
also important for term-driven queries and KWIC
views. Techniques from ad-hoc data querying sys-
tems may be necessary for further scale (e.g. Mel-
nik et al. (2010)).
Many other directions are possible. The proto-
type tool, as described in ?2, will be available as
open-source software at: http://brenocon.
com/MiTextExplorer. It is a desktop appli-
cation written in Java.
Acknowledgments
Thanks to Michael Heilman and Bryan Routledge,
for many discussions and creative text analysis
scripts that inspired this work. Thanks also to the
anonymous reviewers for very helpful feedback.
This research was supported in part by NSF grant
IIS-1211277 and CAREER grant IIS-1054319.
References
Francis J Anscombe. Graphs in statistical analysis.
The American Statistician, 27(1):17?21, 1973.
David Bamman, Brendan O?Connor, and Noah A.
Smith. Censorship and deletion practices in
Chinese social media. First Monday, 17(3),
2012.
Richard A Becker and John M Chambers. S: an
interactive environment for data analysis and
graphics. CRC Press, 1984.
Richard A. Becker and William S. Cleveland.
Brushing scatterplots. Technometrics, 29(2):
127?142, 1987.
David M. Blei and John D. Lafferty. Topic mod-
els. Text mining: classification, clustering, and
applications, 10:71, 2009.
Michael Bostock, Vadim Ogievetsky, and Jeffrey
Heer. D3: Data-driven documents. IEEE Trans.
Visualization & Comp. Graphics (Proc. Info-
Vis), 2011. URL http://vis.stanford.
edu/papers/d3.
10
Mary Bucholtz, Nancy Bermudez, Victor Fung,
Lisa Edwards, and Rosalva Vargas. Hella
Nor Cal or totally So Cal? the per-
ceptual dialectology of California. Jour-
nal of English Linguistics, 35(4):325?352,
2007. URL http://people.duke.edu/
?
eec10/hellanorcal.pdf.
Andreas Buja, John Alan McDonald, John Micha-
lak, and Werner Stuetzle. Interactive data visu-
alization using focusing and linking. In Visu-
alization, 1991. Visualization?91, Proceedings.,
IEEE Conference on, pages 156?163. IEEE,
1991.
Andreas Buja, Dianne Cook, and Deborah F
Swayne. Interactive high-dimensional data vi-
sualization. Journal of Computational and
Graphical Statistics, 5(1):78?99, 1996.
Allison J.B. Chaney and David M. Blei. Visual-
izing topic models. In Proceedings of ICWSM,
2013.
Jason Chuang, Christopher D. Manning, and
Jeffrey Heer. ?without the clutter of unim-
portant words?: Descriptive keyphrases for
text visualization. ACM Trans. on Computer-
Human Interaction, 19:1?29, 2012a. URL
http://vis.stanford.edu/papers/
keyphrases.
Jason Chuang, Christopher D. Manning, and Jef-
frey Heer. Termite: Visualization techniques for
assessing textual topic models. In Advanced Vi-
sual Interfaces, 2012b. URL http://vis.
stanford.edu/papers/termite.
K. W Church and P. Hanks. Word association
norms, mutual information, and lexicography.
Computational linguistics, 16(1):2229, 1990.
William S. Cleveland. Visualizing data. Hobart
Press, 1993.
Dianne Cook and Deborah F. Swayne. Interactive
and dynamic graphics for data analysis: with R
and GGobi. Springer, 2007.
Ted Dunning. Accurate methods for the statistics
of surprise and coincidence. Computa-
tional Linguistics, 19:61?74, 1993. doi:
10.1.1.14.5962. URL http://citeseerx.
ist.psu.edu/viewdoc/summary?
doi=10.1.1.14.5962.
J. Eisenstein, A. Ahmed, and E.P. Xing. Sparse ad-
ditive generative models of text. In Proceedings
of ICML, pages 1041?1048, 2011.
Jacob Eisenstein, Brendan O?Connor, Noah A.
Smith, and Eric P. Xing. A latent variable model
for geographic lexical variation. In Proceedings
of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1277?
1287, 2010.
Jacob Eisenstein, Brendan O?Connor, Noah A.
Smith, and Eric P. Xing. Mapping the geograph-
ical diffusion of new words. In NIPS Workshop
on Social Network and Social Media Analy-
sis, 2012. URL http://arxiv.org/abs/
1210.5268.
M.J. Gardner, J. Lutes, J. Lund, J. Hansen,
D. Walker, E. Ringger, and K. Seppi. The topic
browser: An interactive tool for browsing topic
models. In NIPS Workshop on Challenges of
Data Visualization. MIT Press, 2010.
Matthew Gentzkow and Jesse M Shapiro. What
drives media slant? evidence from us daily
newspapers. Econometrica, 78(1):35?71, 2010.
Carsten G?org, Zhicheng Liu, and John Stasko.
Reflections on the evolution of the jigsaw vi-
sual analytics system. Information Visualiza-
tion, 2013.
Justin Grimmer and Gary King. General purpose
computer-assisted clustering and conceptualiza-
tion. Proceedings of the National Academy of
Sciences, 108(7):2643?2650, 2011.
Justin Grimmer and Brandon M Stewart. Text
as Data: The promise and pitfalls of au-
tomatic content analysis methods for polit-
ical texts. Political Analysis, 21(3):267?
297, 2013. URL http://www.stanford.
edu/
?
jgrimmer/tad2.pdf.
Isabelle Guyon and Andr?e Elisseeff. An introduc-
tion to variable and feature selection. The Jour-
nal of Machine Learning Research, 3:1157?
1182, 2003.
Marti Hearst. Search user interfaces. Cambridge
University Press, 2009.
Matthew L Jockers. Macroanalysis: Digital meth-
ods and literary history. University of Illinois
Press, 2013.
Gary King, Jennifer Pan, and Margaret E. Roberts.
How censorship in china allows government
criticism but silences collective expression.
American Political Science Review, 107:1?18,
2013.
11
Christopher D Manning and Hinrich Sch?utze.
Foundations of statistical natural language pro-
cessing. MIT press, 1999.
Allen R. Martin and Matthew O. Ward. High di-
mensional brushing for interactive exploration
of multivariate data. In Proceedings of the
6th Conference on Visualization?95, page 271.
IEEE Computer Society, 1995.
Sergey Melnik, Andrey Gubarev, Jing Jing Long,
Geoffrey Romer, Shiva Shivakumar, Matt
Tolton, and Theo Vassilakis. Dremel: interac-
tive analysis of web-scale datasets. Proceed-
ings of the VLDB Endowment, 3(1-2):330?339,
2010.
David Mimno. Topic regression. PhD thesis, Uni-
versity of Massachusetts Amherst, 2012.
B. L. Monroe, M. P. Colaresi, and K. M. Quinn.
Fightin?Words: lexical feature selection and
evaluation for identifying the content of politi-
cal conflict. Political Analysis, 16(4):372, 2008.
D. Newman, T. Baldwin, L. Cavedon, E. Huang,
S. Karimi, D. Martinez, F. Scholer, and J. Zo-
bel. Visualizing search results and document
collections using topic maps. Web Semantics:
Science, Services and Agents on the World Wide
Web, 8(2):169?175, 2010.
Brendan O?Connor. Statistical Text Analysis for
Social Science. PhD thesis, Carnegie Mellon
University, 2014.
Brendan O?Connor, Michel Krieger, and David
Ahn. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proceedings of
the International AAAI Conference on Weblogs
and Social Media, 2010.
Brendan O?Connor, David Bamman, and Noah A.
Smith. Computational text analysis for social
science: Model assumptions and complexity. In
Second Workshop on Comptuational Social Sci-
ence and the Wisdom of Crowds (NIPS 2011),
2011.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. Improved part-of-speech tagging for on-
line conversational text with word clusters. In
Proceedings of NAACL, 2013.
Foster Provost and Tom Fawcett. Data Science for
Business. O?Reilly Media, 2013.
R Core Team. R: A Language and Environment for
Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria, 2013. URL
http://www.R-project.org/. ISBN 3-
900051-07-0.
Dragomir R. Radev, Pradeep Muthukrishnan, and
Vahed Qazvinian. The ACL anthology network
corpus. In Proc. of ACL Workshop on Natu-
ral Language Processing and Information Re-
trieval for Digital Libraries, 2009.
Margaret E. Roberts, Brandon M. Stewart, and
Edoardo M. Airoldi. Structural topic models.
2013. URL http://scholar.harvard.
edu/bstewart/publications/
structural-topic-models. Work-
ing paper.
Geoffrey Rockwell, St?efan G Sinclair, Stan
Ruecker, and Peter Organisciak. Ubiquitous
text analysis. paj: The Journal of the Initiative
for Digital Humanities, Media, and Culture, 2
(1), 2010.
Ryan Shaw. Text-mining as a research
tool, 2012. URL http://aeshin.org/
textmining/.
Aditi Shrikumar. Designing an Exploratory Text
Analysis Tool for Humanities and Social Sci-
ences Research. PhD thesis, University of Cali-
fornia at Berkeley, 2013.
Yanchuan Sim, Brice Acree, Justin H Gross, and
Noah A Smith. Measuring ideological propor-
tions in political speeches. In Proceedings of
EMNLP, 2013.
Matt Taddy. Multinomial inverse regression for
text analysis. Journal of the American Statisti-
cal Association, 108(503):755?770, 2013.
John W. Tukey. Exploratory data analysis. 1977.
P. D Turney. Thumbs up or thumbs down?: seman-
tic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th
Annual Meeting on Association for Computa-
tional Linguistics, page 417424, 2002.
P. D Turney and P. Pantel. From frequency to
meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37
(1):141188, 2010. ISSN 1076-9757.
Peter Turney. Mining the web for syn-
onyms: Pmi-ir versus lsa on toefl. In
Proceedings of the Twelth European Con-
ference on Machine Learning, 2001.
URL http://nparc.cisti-icist.
12
nrc-cnrc.gc.ca/npsi/ctrl?
action=rtdoc&an=5765594.
Martin Wattenberg and Fernanda B Vi?egas. The
word tree, an interactive visual concordance.
Visualization and Computer Graphics, IEEE
Transactions on, 14(6):1221?1228, 2008.
Hadley Wickham. A layered grammar of graph-
ics. Journal of Computational and Graphical
Statistics, 19(1):328, 2010. doi: 10.1198/jcgs.
2009.07098.
Leland Wilkinson. The grammar of graphics.
Springer, 2006.
13
