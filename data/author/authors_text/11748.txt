Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Coupling Semi-Supervised Learning of Categories and Relations
Andrew Carlson1, Justin Betteridge1, Estevam R. Hruschka Jr.1,2 and Tom M. Mitchell1
1School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
{acarlson,jbetter,tom.mitchell}@cs.cmu.edu
2Federal University of Sao Carlos
Sao Carlos, SP - Brazil
estevam@dc.ufscar.br
Abstract
We consider semi-supervised learning of
information extraction methods, especially
for extracting instances of noun categories
(e.g., ?athlete,? ?team?) and relations (e.g.,
?playsForTeam(athlete,team)?). Semi-
supervised approaches using a small number
of labeled examples together with many un-
labeled examples are often unreliable as they
frequently produce an internally consistent,
but nevertheless incorrect set of extractions.
We propose that this problem can be over-
come by simultaneously learning classifiers
for many different categories and relations
in the presence of an ontology defining
constraints that couple the training of these
classifiers. Experimental results show that
simultaneously learning a coupled collection
of classifiers for 30 categories and relations
results in much more accurate extractions
than training classifiers individually.
1 Introduction
A great wealth of knowledge is expressed on the web
in natural language. Translating this into a struc-
tured knowledge base containing facts about enti-
ties (e.g., ?Disney?) and relations between those en-
tities (e.g. CompanyIndustry(?Disney?, ?entertain-
ment?)) would be of great use to many applications.
Although fully supervised methods for learning to
extract such facts from text work well, the cost
of collecting many labeled examples of each type
of knowledge to be extracted is impractical. Re-
searchers have also explored semi-supervised learn-
ing methods that rely primarily on unlabeled data,
Figure 1: We show that significant improvements in ac-
curacy result from coupling the training of information
extractors for many inter-related categories and relations
(B), compared with the simpler but much more difficult
task of learning a single information extractor (A).
but these approaches tend to suffer from the fact that
they face an under-constrained learning task, result-
ing in extractions that are often inaccurate.
We present an approach to semi-supervised learn-
ing that yields more accurate results by coupling the
training of many information extractors. The intu-
ition behind our approach (summarized in Figure 1)
is that semi-supervised training of a single type of
extractor such as ?coach? is much more difficult than
simultaneously training many extractors that cover
a variety of inter-related entity and relation types.
In particular, prior knowledge about the relation-
ships between these different entities and relations
(e.g., that ?coach(x)? implies ?person(x)? and ?not
sport(x)?) allows unlabeled data to become a much
more useful constraint during training.
Although previous work has coupled the learning
of multiple categories, or used static category rec-
ognizers to check arguments for learned relation ex-
1
tractors, our work is the first we know of to couple
the simultaneous semi-supervised training of multi-
ple categories and relations. Our experiments show
that this coupling results in more accurate extrac-
tions. Based on our results reported here, we hy-
pothesize that significant accuracy improvements in
information extraction will be possible by coupling
the training of hundreds or thousands of extractors.
2 Problem Statement
It will be helpful to first explain our use of common
terms. An ontology is a collection of unary and bi-
nary predicates, also called categories and relations,
respectively.1 An instance of a category, or a cate-
gory instance, is a noun phrase; an instance of a rela-
tion, or a relation instance, is a pair of noun phrases.
Instances can be positive or negative with respect
to a specific predicate, meaning that the predicate
holds or does not hold for that particular instance.
A promoted instance is an instance which our algo-
rithm believes to be a positive instance of some pred-
icate. Also associated with both categories and rela-
tions are patterns: strings of tokens with placehold-
ers (e.g., ?game against arg1? and ?arg1 , head coach
of arg2?). A promoted pattern is a pattern believed
to be a high-probability indicator for some predicate.
The challenge addressed by this work is to learn
extractors to automatically populate the categories
and relations of a specified ontology with high-
confidence instances, starting from a few seed pos-
itive instances and patterns for each predicate and
a large corpus of sentences annotated with part-of-
speech (POS) tags. We focus on extracting facts that
are stated multiple times in the corpus, which we
can assess probabilistically using corpus statistics.
We do not resolve strings to real-world entities? the
problems of synonym resolution and disambiguation
of strings that can refer to multiple entities are left
for future work.
3 Related Work
Work on multitask learning has demonstrated that
supervised learning of multiple ?related? functions
together can yield higher accuracy than learning the
functions separately (Thrun, 1996; Caruana, 1997).
Semi-supervised multitask learning has been shown
1We do not consider predicates of higher arity in this work.
to increase accuracy when tasks are related, allow-
ing one to use a prior that encourages similar pa-
rameters (Liu et al, 2008). Our work also involves
semi-supervised training of multiple coupled func-
tions, but differs in that we assume explicit prior
knowledge of the precise way in which our multi-
ple functions are related (e.g., that the values of the
functions applied to the same input are mutually ex-
clusive, or that one implies the other).
In this paper, we focus on a ?bootstrapping?
method for semi-supervised learning. Bootstrap-
ping approaches start with a small number of la-
beled ?seed? examples, use those seed examples to
train an initial model, then use this model to la-
bel some of the unlabeled data. The model is
then retrained, using the original seed examples plus
the self-labeled examples. This process iterates,
gradually expanding the amount of labeled data.
Such approaches have shown promise in applica-
tions such as web page classification (Blum and
Mitchell, 1998), named entity classification (Collins
and Singer, 1999), parsing (McClosky et al, 2006),
and machine translation (Ueffing, 2006).
Bootstrapping approaches to information extrac-
tion can yield impressive results with little initial
human effort (Brin, 1998; Agichtein and Gravano,
2000; Ravichandran and Hovy, 2002; Pasca et al,
2006). However, after many iterations, they usu-
ally suffer from semantic drift, where errors in label-
ing accumulate and the learned concept ?drifts? from
what was intended (Curran et al, 2007). Coupling
the learning of predicates by using positive exam-
ples of one predicate as negative examples for oth-
ers has been shown to help limit this drift (Riloff and
Jones, 1999; Yangarber, 2003). Additionally, ensur-
ing that relation arguments are of certain, expected
types can help mitigate the promotion of incorrect
instances (Pas?ca et al, 2006; Rosenfeld and Feld-
man, 2007). Our work builds on these ideas to cou-
ple the simultaneous bootstrapped training of multi-
ple categories and multiple relations.
Our approach to information extraction is based
on using high precision contextual patterns (e.g., ?is
mayor of arg1? suggests that arg1 is a city). An early
pattern-based approach to information extraction ac-
quired ?is a? relations from text using generic con-
textual patterns (Hearst, 1992). This approach was
later scaled up to the web by Etzioni et al (2005).
2
Other research explores the task of ?open informa-
tion extraction?, where the predicates to be learned
are not specified in advance (Shinyama and Sekine,
2006; Banko et al, 2007), but emerge instead from
analysis of the data. In contrast, our approach re-
lies strongly on knowledge in the ontology about the
predicates to be learned, and relationships among
them, in order to achieve high accuracy.
Chang et al (2007) present a framework for
learning that optimizes the data likelihood plus
constraint-based penalty terms than capture prior
knowledge, and demonstrate it with semi-supervised
learning of segmentation models. Constraints that
capture domain knowledge guide bootstrap learn-
ing of a structured model by penalizing or disallow-
ing violations of those constraints. While similar in
spirit, our work differs in that we consider learning
many models, rather than one structured model, and
that we are consider a much larger scale application
in a different domain.
4 Approach
4.1 Coupling of Predicates
As mentioned above, our approach hinges on the no-
tion of coupling the learning of multiple functions
in order to constrain the semi-supervised learning
problem we face. Our system learns four different
types of functions. For each category c:
1. fc,inst : NP (C)? [0, 1]
2. fc,patt : PattC(C)? [0, 1]
and for each relation r:
1. fr,inst : NP (C)?NP (C)? [0, 1]
2. fr,patt : PattR(C)? [0, 1]
where C is the input corpus, NP (C) is the set of
valid noun phrases in C, PattC(C) is the set of valid
category patterns in C, and PattR(C) is the set of
valid relation patterns in C. ?Valid? noun phrases,
category patterns, and relation patterns are defined
in Section 4.2.2.
The learning of these functions is coupled in two
ways:
1. Sharing among same-arity predicates according
to logical relations
2. Relation argument type-checking
These methods of coupling are made possible by
prior knowledge in the input ontology, beyond the
lists of categories and relations mentioned above.
We provide general descriptions of these methods
of coupling in the next sections, while the details are
given in section 4.2.
4.1.1 Sharing among same-arity predicates
Each predicate P in the ontology has a list of other
same-arity predicates with which P is mutually
exclusive, where mutuallyExclusive(P, P ?) ?
(P (arg1) ? ?P ?(arg1)) ? (P ?(arg1) ?
?P (arg1)), and similarly for relations. These mu-
tually exclusive relationships are used to carry out
the following simple but crucial coupling: if predi-
cate A is mutually exclusive with predicate B, A?s
positive instances and patterns become negative in-
stances and negative patterns for B. For example,
if ?city?, having an instance ?Boston? and a pattern
?mayor of arg1?, is mutually exclusive with ?scien-
tist?, then ?Boston? and ?mayor of arg1? will become
a negative instance and a negative pattern respec-
tively for ?scientist.? Such negative instances and
patterns provide negative evidence to constrain the
bootstrapping process and forestall divergence.
Some categories are declared to be a subset of
one of the other categories being populated, where
subset(P, P ?) ? P (arg1) ? P ?(arg1), (e.g., ?ath-
lete? is a subset of ?person?). This prior knowledge
is used to share instances and patterns of the subcat-
egory (e.g., ?athlete?) as positive instances and pat-
terns for the super-category (e.g., ?person?).
4.1.2 Relation argument type-checking
The last type of prior knowledge we use to couple
the learning of functions is type checking informa-
tion which couples the learning of relations with cat-
egories. For example, the arguments of the ?ceoOf?
relation are declared to be of the categories ?person?
and ?company?. Our approach does not promote a
pair of noun phrases as an instance of a relation un-
less the two noun phrases are classified as belonging
to the correct argument types. Additionally, when a
relation instance is promoted, the arguments become
promoted instances of their respective categories.
4.2 Algorithm Description
In this section, we describe our algorithm, CBL
(Coupled Bootstrap Learner), in detail.
The inputs to CBL are a large corpus of POS-
tagged sentences and an initial ontology with pre-
3
Algorithm 1: CBL Algorithm
Input: An ontology O, and text corpus C
Output: Trusted instances/patterns for each
predicate
SHARE initial instances/patterns among
predicates;
for i = 1, 2, . . . ,? do
foreach predicate p ? O do
EXTRACT candidate instances/patterns;
FILTER candidates;
TRAIN instance/pattern classifiers;
ASSESS candidates using classifiers;
PROMOTE highest-confidence candidates;
end
SHARE promoted items among predicates;
end
defined categories, relations, mutually exclusive re-
lationships between same-arity predicates, subset re-
lationships between some categories, seed instances
for all predicates, and seed patterns for the cate-
gories. Categories in the input ontology also have
a flag indicating whether instances must be proper
nouns, common nouns, or whether they can be ei-
ther (e.g., instances of ?city? are proper nouns).
Algorithm 1 gives a summary of the CBL algo-
rithm. First, seed instances and patterns are shared
among predicates using the available mutual exclu-
sion, subset, and type-checking relations. Then,
for an indefinite number of iterations, CBL expands
the sets of promoted instances and patterns for each
predicate, as detailed below.
CBL was designed to allow learning many pred-
icates simultaneously from a large sample of text
from the web. In each iteration of the algorithm, the
information needed from the text corpus is gathered
in two passes through the corpus using the MapRe-
duce framework (Dean and Ghemawat, 2008). This
allows us to complete an iteration of the system in
1 hour using a corpus containing millions of web
pages (see Section 5.3 for details on the corpus).
4.2.1 Sharing
At the start of execution, seed instances and pat-
terns are shared among predicates according to the
mutual exclusion, subset, and type-checking con-
straints. Newly promoted instances and patterns are
shared at the end of each iteration.
4.2.2 Candidate Extraction
CBL finds new candidate instances by using
newly promoted patterns to extract the noun phrases
that co-occur with those patterns in the text corpus.
To keep the size of this set manageable, CBL lim-
its the number of new candidate instances for each
predicate to 1000 by selecting the ones that occur
with the most newly promoted patterns. An analo-
gous procedure is used to extract candidate patterns.
Candidate extraction is performed for all predicates
in a single pass through the corpus using the MapRe-
duce framework.
The candidate extraction procedure has defini-
tions for valid instances and patterns that limit ex-
traction to instances that look like noun phrases and
patterns that are likely to be informative. Here we
provide brief descriptions of those definitions.
Category Instances In the placeholder of a cate-
gory pattern, CBL looks for a noun phrase. It uses
part-of-speech tags to segment noun phrases, ignor-
ing determiners. Proper nouns containing prepo-
sitions are segmented using a reimplementation of
the Lex algorithm (Downey et al, 2007). Cate-
gory instances are only extracted if they obey the
proper/common noun specification of the category.
Category Patterns If a promoted category in-
stance is found in a sentence, CBL extracts the pre-
ceding words as a candidate pattern if they are verbs
followed by a sequence of adjectives, prepositions,
or determiners (e.g., ?being acquired by arg1?) or
nouns and adjectives followed by a sequence of ad-
jectives, prepositions, or determiners (e.g., ?former
CEO of arg1?).
CBL extracts the words following the instance as
a candidate pattern if they are verbs followed option-
ally by a noun phrase (e.g., ?arg1 broke the home run
record?), or verbs followed by a preposition (e.g.,
?arg1 said that?).
Relation Instances If a promoted relation pattern
(e.g., ?arg1 is mayor of arg2?) is found, a candi-
date relation instance is extracted if both placehold-
ers are valid noun phrases, and if they obey the
proper/common specifications for their categories.
Relation Patterns If both arguments from a pro-
moted relation instance are found in a sentence then
4
the intervening sequence of words is extracted as a
candidate relation pattern if it contains no more than
5 tokens, has a content word, has an uncapitalized
word, and has at least one non-noun.
4.2.3 Candidate Filtering
Candidate instances and patterns are filtered to
maintain high precision, and to avoid extremely spe-
cific patterns. An instance is only considered for as-
sessment if it co-occurs with at least two promoted
patterns in the text corpus, and if its co-occurrence
count with all promoted patterns is at least three
times greater than its co-occurrence count with neg-
ative patterns. Candidate patterns are filtered in the
same manner using instances.
All co-occurrence counts needed by the filtering
step are obtained with an additional pass through
the corpus using MapReduce. This implementa-
tion is much more efficient than one that relies on
web search queries. CBL typically requires co-
occurrence counts of at least 10,000 instances with
any of at least 10,000 patterns, which would require
100 million hit count queries.
4.2.4 Candidate Assessment
Next, for each predicate CBL trains a discretized
Na??ve Bayes classifier to classify the candidate in-
stances. Its features include pointwise mutual infor-
mation (PMI) scores (Turney, 2001) of the candidate
instance with each of the positive and negative pat-
terns associated with the class. The current sets of
promoted and negative instances are used as training
examples for the classifier. Attributes are discretized
based on information gain (Fayyad and Irani, 1993).
Patterns are assessed using an estimate of the pre-
cision of each pattern p:
Precision(p) =
?
i?I count(i, p)
count(p)
where I is the set of promoted instances for the
predicate currently being considered, count(i, p) is
the co-occurrence count of instance i with pattern p,
and count(p) is the hit count of the pattern p. This
is a pessimistic estimate because it assumes that the
rest of the occurrences of pattern p are not with pos-
itive examples of the predicate. We also penalize
extremely rare patterns by thresholding the denomi-
nator using the 25th percentile candidate pattern hit
count (McDowell and Cafarella, 2006).
All of the co-occurrence counts needed for the as-
sessment step are collected in the same MapReduce
pass as those required for filtering candidates.
4.2.5 Candidate Promotion
CBL then ranks the candidates according to their
assessment scores and promotes at most 100 in-
stances and 5 patterns for each predicate.
5 Experimental Evaluation
We designed our experimental evaluation to try to
answer the following questions: Can CBL iterate
many times and still achieve high precision? How
helpful are the types of coupling that we employ?
Can we extend existing semantic resources?
5.1 Configurations of the Algorithm
We ran our algorithm in three configurations:
? Full: The algorithm as described in Section 4.2.
? No Sharing Among Same-Arity Predicates (NS):
This configuration couples predicates only us-
ing type-checking constraints. It uses the full
algorithm, except that predicates of the same ar-
ity do not share promoted instances and patterns
with each other. Seed instances and patterns are
shared, though, so each predicate has a small,
fixed pool of negative evidence.
? No Category/Relation coupling (NCR): This
configuration couples predicates using mutual
exclusion and subset constraints, but not type-
checking. It uses the full algorithm, except
that relation instance arguments are not fil-
tered or assessed using their specified categories,
and arguments of promoted relations are not
shared as promoted instances of categories. The
only type-checking information used is the com-
mon/proper noun specifications of arguments for
filtering out implausible instances.
5.2 Initial ontology
Our ontology contained categories and relations re-
lated to two domains: companies and sports. Ex-
tra categories were added to provide negative evi-
dence to the domain-related categories: ?hobby? for
?economic sector?; ?actor,? ?politician,? and ?scien-
tist? for ?athlete? and ?coach?; and ?board game? for
?sport?. Table 1 lists each predicate in the leftmost
column. Categories were started with 10?20 seed
5
5 iterations 10 iterations 15 iterations
Predicate Full NS NCR Full NS NCR Full NS NCR
Actor 93 100 100 93 97 100 100 97 100
Athlete 100 100 100 100 93 100 100 73 100
Board Game 93 76 93 89 27 93 89 30 93
City 100 100 100 100 97 100 100 100 100
Coach 100 63 73 97 53 43 97 47 47
Company 100 100 100 97 90 97 100 90 100
Country 60 40 60 30 43 27 40 23 40
Economic Sector 77 63 73 57 67 67 50 63 40
Hobby 67 63 67 40 40 57 20 23 30
Person 97 97 90 97 93 97 93 97 93
Politician 93 93 97 73 53 90 90 53 87
Product 97 87 90 90 87 100 97 90 77
Product Type 93 93 90 70 73 97 77 80 67
Scientist 100 90 97 97 63 97 93 60 100
Sport 100 90 100 93 67 83 97 27 90
Sports Team 100 97 100 97 70 100 90 50 100
Category Average 92 84 89 82 70 84 83 63 79
Acquired(Company, Company) 77 77 80 67 80 47 70 63 47
CeoOf(Person, Company) 97 87 100 90 87 97 90 80 83
CoachesTeam(Coach, Sports Team) 100 100 100 100 100 97 100 100 90
CompetesIn(Company, Econ. Sector) 97 97 80 100 93 67 97 63 60
CompetesWith(Company, Company) 93 80 60 77 70 37 70 60 43
HasOfficesIn(Company, City) 97 93 40 93 90 27 93 57 30
HasOperationsIn(Company, Country) 100 95 50 100 97 40 90 83 13
HeadquarteredIn(Company, City) 77 90 20 70 77 27 70 60 7
LocatedIn(City, Country) 90 67 57 63 50 43 73 50 30
PlaysFor(Athlete, Sports Team) 100 100 0 100 97 7 100 43 0
PlaysSport(Athlete, Sport) 100 100 27 93 80 10 100 40 30
TeamPlaysSport(Sports Team, Sport) 100 100 77 100 97 80 93 83 67
Produces(Company, Product) 91 83 90 83 93 67 93 80 57
HasType(Product, Product Type) 73 63 17 33 67 33 40 57 27
Relation Average 92 88 57 84 84 48 84 66 42
All 92 86 74 83 76 68 84 64 62
Table 1: Precision (%) for each predicate. Results are presented after 5, 10, and 15 iterations, for the Full, No Sharing
(NS), and No Category/Relation Coupling (NCR) configurations of CBL . Note that we expect Full and NCR to
perform similarly for categories, but for Full to outperform NCR on relations and for Full to outperform NS on both
categories and relations.
6
instances and 5 seed patterns. The seed instances
were specified by a human, and the seed patterns
were derived from the generic patterns of Hearst
for each predicate (Hearst, 1992). Relations were
started with similar numbers of seed instances, and
no seed patterns (it is less obvious how to gener-
ate good seed patterns from relation names). Most
predicates were declared as mutually exclusive with
most others, except for special cases (e.g., ?hobby?
and ?sport?; ?university? and ?sports team?; and ?has
offices in? and ?headquartered in?).
5.3 Corpus
Our text corpus was from a 200-million page web
crawl. We parsed the HTML, filtered out non-
English pages using a stop word ratio threshold, then
filtered out web spam and adult content using a ?bad
word? list. The pages were then segmented into sen-
tences, tokenized, and tagged with parts-of-speech
using the OpenNLP package. Finally, we filtered
the sentences to eliminate those that were likely to
be noisy and not useful for learning (e.g., sentences
without a verb, without any lowercase words, with
too many words that were all capital letters). This
yielded a corpus of roughly 514-million sentences.
5.4 Experimental Procedure
We ran each configuration for 15 iterations. To eval-
uate the precision of promoted instances, we sam-
pled 30 instances from the promoted set for each
predicate in each configuration after 5, 10, and 15 it-
erations, pooled together the samples for each pred-
icate, and then judged their correctness. The judge
did not know which run an instance was sampled
from. We estimated the precision of the promoted
instances from each run after 5, 10, and 15 itera-
tions as the number of correct promoted instances
divided by the number sampled. While samples of
30 instances do not produce tight confidence inter-
vals around individual estimates, they are sufficient
for testing for the effects in which we are interested.
5.5 Results
Table 1 shows the precision of each of the three al-
gorithm configurations for each category and rela-
tion after 5, 10, and 15 iterations. As is apparent
in this table, fully coupled training (Full) outper-
forms training when coupling is removed between
categories and relations (NCR), and also when cou-
pling is removed among predicates of the same ar-
ity (NS). The net effect is substantial, as is appar-
ent from the bottom row of Table 1, which shows
that the precision of Full outperforms NS by 6% and
NCR by 18% after the first 5 iterations, and by an
even larger 20% and 22% after 15 iterations. This
increasing gap in precision as iterations increase re-
flects the ability of coupled learning to constrain the
system to reduce the otherwise common drift asso-
ciated with self-trained classifiers.
Using Student?s paired t-test, we found that for
categories, the difference in performance between
Full and NS is statistically significant after 5, 10,
and 15 iterations (p-value < 0.05).2 No significant
difference was found between Full and NCR for cat-
egories, but this is not a surprise, because NCR still
uses mutually exclusive and subset constraints. The
same test finds that the differences between Full and
NS are significant for relations after 15 iterations,
and the differences between Full and NCR are sig-
nificant after 5, 10, and 15 iterations for relations.
The worst-performing categories after 15 itera-
tions of Full are ?country,? ?economic sector,? and
?hobby.? The Full configuration of CBL promoted
1637 instances for ?country,? far more than the num-
ber of correct answers. Many of these are general
geographic regions like ?Bayfield Peninsula? and
?Baltic Republics.? In the ?hobby? case, promoting
patterns like ?the types of arg1? led to the category
drifting into a general list of plural common nouns.
?Economic sector? drifted into academic fields like
?Behavioral Science? and ?Political Sciences.? We
expect that the learning of these categories would
be significantly better if there were even more cat-
egories being learned to provide additional negative
evidence during the filtering and assessment steps of
the algorithm.
At this stage of development, obtaining high re-
call is not a priority because our intent is to create
a continuously running and continuously improving
system; it is our hope that high recall will come with
time. However, to very roughly convey the com-
pleteness of the current results we show in Table 2
the average number of instances promoted for cate-
2Our selection of the paired t-test was motivated by the work
of Smucker et al (2007), but the Wilcoxon signed rank test
gives the same results.
7
Categories Relations
Configuration Instances Prec. Instances Prec.
Full 970 83 191 84
NS 1337 63 307 66
NCR 916 79 458 42
Table 2: Average numbers of promoted category and re-
lation instances and estimates of their precision for each
configuration of CBL after 15 iterations.
Figure 2: Extracted facts for two companies discovered
by CBL Full. These two companies were extracted by
the learned ?company? extractor, and the relations shown
were extracted by learned relation extractors.
gories and relations for each of the three configura-
tions of CBL after 15 iterations. For categories, not
sharing examples results in fewer negative examples
during the filtering and assessment steps. This yields
more promoted instances on average. For relations,
not using type checking yields higher relative recall,
but at a much lower level of precision.
Figure 2 gives one view of the type of information
extracted by the collection of learned category and
relation classifiers. Note the initial seed examples
provided to CBL did not include information about
either company or any of these relation instances.3
5.6 Comparison to an Existing Database
To estimate the capacity of our algorithm to con-
tribute additional facts to publicly available seman-
tic resources, we compared the complete lists of in-
stances promoted during the Full 15 iteration run
for certain categories to corresponding lists in the
Freebase database (Metaweb Technologies, 2009).
Excluding the categories that did not have a di-
rectly corresponding Freebase list, we computed for
each category: Precision ? |CBLInstances| ?
|Matches|, where Precision is the estimated pre-
cision from our random sample of 30 instances,
|CBLInstances| is the total number of instances
promoted for that category, and |Matches| is the
3See http://rtw.ml.cmu.edu/sslnlp09 for re-
sults from a full run of the system.
Est. CBL Freebase Est. New
Category Prec. Instances Matches Instances
Actor 100 522 465 57
Athlete 100 117 54 63
Board Game 89 18 6 10
City 100 1799 1665 134
Company 100 1937 995 942
Econ. Sector 50 1541 137 634
Politician 90 962 74 792
Product 97 1259 0 1221
Sports Team 90 414 139 234
Sport 97 613 134 461
Table 3: Estimated numbers of ?new instances? (correct
instances promoted by CBL in the Full 15 iteration run
which do not have a match in Freebase) and the values
used in calculating them.
number of promoted instances that had an exact
match in Freebase. While exact matches may under-
estimate the number of matches, it should be noted
that rather than make definitive claims, our intent
here is simply to give rough estimates, which are
shown in Table 3. These approximate numbers in-
dicate a potential to use CBL to extend existing se-
mantic resources like Freebase.
6 Conclusion
We have presented a method of coupling the semi-
supervised learning of categories and relations and
demonstrated empirically that the coupling forestalls
the problem of semantic drift associated with boot-
strap learning methods. We suspect that learning
additional predicates simultaneously will yield even
more accurate learning. An approximate compari-
son with an existing repository of semantic knowl-
edge, Freebase, suggests that our methods can con-
tribute new facts to existing resources.
Acknowledgments
This work is supported in part by DARPA, Google,
a Yahoo! Fellowship to Andrew Carlson, and the
Brazilian research agency CNPq. We also gratefully
acknowledge Jamie Callan for making available his
collection of web pages, Yahoo! for use of their M45
computing cluster, and the anonymous reviewers for
their comments.
8
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In JCDL.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IJCAI.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In WebDB Workshop at
6th International Conference on Extending Database
Technology.
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28:41?75.
Ming-Wei Chang, Lev-Arie Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In ACL.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In EMNLP.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In PACLING.
Jeffrey Dean and Sanjay Ghemawat. 2008. Mapreduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107?113.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating complex named entities in web text.
In IJCAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91?134.
Usama M. Fayyad and Keki B. Irani. 1993. Multi-
interval discretization of continuous-valued attributes
for classification learning. In UAI.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING.
Qiuhua Liu, Xuejun Liao, and Lawrence Carin. 2008.
Semi-supervised multitask learning. In NIPS.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In NAACL.
Luke K. McDowell and Michael Cafarella. 2006.
Ontology-driven information extraction with on-
tosyphon. In ISWC.
Metaweb Technologies. 2009. Freebase data dumps.
http://download.freebase.com/datadumps/.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and similarities on
the web: fact extraction in the fast lane. In ACL.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and search-
ing the world wide web of facts - step one: The one-
million fact extraction challenge. In AAAI.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In ACL.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In AAAI.
Benjamin Rosenfeld and Ronen Feldman. 2007. Us-
ing corpus statistics on entities to improve semi-
supervised relation extraction from the web. In ACL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In HLT-NAACL.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests for
information retrieval evaluation. In CIKM.
Sebastian Thrun. 1996. Is learning the n-th thing any
easier than learning the first? In NIPS.
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In EMCL.
Nicola Ueffing. 2006. Self-training for machine trans-
lation. In NIPS workshop on Machine Learning for
Multilingual Information Access.
Roman Yangarber. 2003. Counter-training in discovery
of semantic patterns. In ACL.
9
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1447?1455,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Discovering Relations between Noun Categories 
 
Thahir P Mohamed * Estevam R Hruschka Jr. Tom M Mitchell 
University Of Pittsburgh Federal University of Sao Carlos Carnegie Mellon University 
pmthahir@gmail.com estevam@cs.cmu.edu tom.mitchell@cs.cmu.edu 
   
   
Abstract 
Traditional approaches to Relation Extraction 
from text require manually defining the rela-
tions to be extracted.  We propose here an ap-
proach to automatically discovering relevant 
relations, given a large text corpus plus an ini-
tial ontology defining hundreds of noun cate-
gories (e.g., Athlete, Musician, Instrument).  
Our approach discovers frequently stated rela-
tions between pairs of these categories, using a 
two step process. For each pair of categories 
(e.g., Musician and Instrument) it first co-
clusters the text contexts that connect known 
instances of the two categories, generating a 
candidate relation for each resulting cluster.  It 
then applies a trained classifier to determine 
which of these candidate relations is semanti-
cally valid. Our experiments apply this to a text 
corpus containing approximately 200 million 
web pages and an ontology containing 122 cat-
egories from the NELL system [Carlson et al, 
2010b], producing a set of 781 proposed can-
didate relations, approximately half of which 
are semantically valid.  We conclude this is a 
useful approach to semi-automatic extension of 
the ontology for large-scale information extrac-
tion systems such as NELL. 
1 Introduction 
The Never-Ending Language Learner (NELL) 
(Carlson et al, 2010b)) is a computer system that 
learns continuously to extract facts from the web.  
NELL is given as input an initial ontology that 
specifies the semantic categories (e.g. city, compa-
ny, sportsTeam) and semantic relations (e.g. hasOf-
ficesIn(company,city), teamPlay-
sInCity(sportsTeam,city)) it must extract from the 
web.  In addition, it is provided 10-20 seed positive 
training examples for each of these categories and 
relations, along with hundreds of millions of unla-
beled web page.  Given this input, NELL applies a 
large-scale multitask, semisupervised learning 
method to learn to extract new instances of these 
categories (e.g., city(?London?)) and relations 
(e.g., teamPlaysInCity(?Steelers?,?Pittsburgh?)) 
from the web.  During the past 17 months NELL 
has been running nearly continuously, learning to 
extract over 600 categories and relations, and pop-
ulating a knowledge base containing over 700,000 
instances of these categories and relations with a 
precision of approximately 0.851. 
This paper considers the problem of automati-
cally discovering new relations to extend the on-
tology of systems such as NELL, enabling them to 
increase over time their learning and extraction 
capabilities.  More precisely, we consider the fol-
lowing problem: 
 
Input: 
? An ontology specifying a set of categories 
? A knowledge base containing instances of these 
categories (perhaps including errors) 
? A large text corpus 
 
Output: 
? A set of two-argument relations that are fre-
quently mentioned in the text corpus, and 
whose argument types correspond to categories 
in the input ontology (e.g., RiverFlows-
ThroughCity(<River>,<City>). 
? For each proposed relation, a set of instances 
(i.e. RiverFlowsThroughCity(?Nile?,?Cairo?)). 
? For each proposed relation, a set of text extrac-
tion patterns that can be used to extract addi-
tional instances of the relation (e.g., the text ?X 
in the heart of Y?, where X is a known river, 
and Y a known City, suggests extracting 
RiverFlowsThroughCity(X,Y)). 
 
Note the above inputs are easily available from 
NELL in the form of its existing ontology and ex-
tracted knowledge base.  Note also that the outputs 
                                                          
*  Thahir P. Mohamed is currently at Amazon Inc. 
1  NELL?s extracted knowledge can be viewed and 
downloaded at http://rtw.ml.cmu.edu. 
1447
  
of our system are sufficient to initiate NELL?s 
learning of additional extraction methods to further 
populate each proposed relation.  One goal of this 
research is to create a system that can provide 
NELL with an ongoing set of new learning and 
extraction tasks. The system is called OntExt (On-
tology Extension System) 
 
Table 1 shows a sample of successful relations 
and corresponding relation contexts and sample 
seed instances generated by OntExt. 
 
Table 1. Examples of valid relations (generated 
by OntExt), their text extraction patterns and 
extracted instances. 
name(category1- 
main context- 
category2) 
Extraction pat-
terns 
Seed 
Instances 
 
River 
-in heart of- 
City 
 
?in heart of? 
?in the center 
of? 
?which flows 
through? 
?Seine, Paris? 
?Nile, Cairo? 
?Tiber river, Rome? 
?River arno, Florence? 
Food 
-to produce-
Chemical 
?to produce? 
?to make? 
?to form? 
?Salt, Chlorine? 
?Sugar, Carbon diox-
ide? 
?Protein, Serotonin? 
 
StadiumOrVenue 
-in downtown- 
City 
 
?in downtown? 
 
?Ford field, Detroit? 
?Superdome, New Or-
leans? 
?Turner field, Atlanta? 
 
Disease 
-caused by- 
Bacteria 
 
?caused by? 
?is the causa-
tive agent of? 
?is the cause 
of? 
?pneumonia, legionel-
la? 
?mastitis, staphylococ-
cus aureus? 
?gonorrhea, neisseria 
gonorrhoeae? 
Disease 
-destroys-
CellType 
?destroys? 
?attacks? 
"alzheimer, brain cells" 
?vitiligo", melano-
cytes" 
"aids, lymphocytes" 
County 
-county-
StateOrProvince 
?county? 
?county of? 
?county in? 
"sufolk, massachusetts" 
"marin, california" 
"sussex, delaware" 
"osceola, michigan" 
2 Background 
Traditional Relation Extraction 
We define Traditional RE systems as those that 
require the user to specify information about the 
relations to be learned. For instance, SnowBall 
(Agichtein and Gravano 2000) & CPL (Carlson et 
al. 2009) are bootstrapped learning systems that 
require manual input of relation predicates. In the-
se systems, for each relation predicate, the relation 
name (e.g. City ?Capital of? Country), the seed in-
stances and the category type (e.g. City, Country, 
Celebrity etc) are provided (for domain and range). 
In CPL (Carlson et al 2009), learning of rela-
tion/category instances is coupled by using con-
straints such as mutual exclusion relationships 
among the predicates. The authors show that this 
coupling reduces semantic drift, which commonly 
occurs with bootstrapping systems, thus leading to 
improved precision. CPL achieved 89% precision 
for the relation instances extracted (Carlson, Bet-
teridge et al 2009).  KNOWITALL (Etzioni, Ca-
farella et al 2005) is a web-scale relation extrac-
tion system, which requires as input the relation 
names. Hence, in these ?traditional relation extrac-
tion? methods, the need to manually define the re-
lations to be extracted makes it difficult to work in 
applications having thousands of possible relation 
predicates.  
2.1 Open Relation Extraction 
Open RE methods do not require a user to manual-
ly specify the information about the relations to be 
learned, such as their names, seed examples, etc. 
TextRunner (Banko, Cararella et al 2007) is such 
an Open Information Extraction system that re-
trieves from the web millions of relational tuples 
between noun phrase entities. TextRunner uses a 
deep linguistic parser to perform self-supervised 
learning and extracts a positive set (i.e. valid rela-
tion between entities) and a negative set (i.e. inva-
lid relationships) of relational tuples based on cer-
tain heuristics. Then, a Naive Bayes classifier is 
built having features such as part-of-speech tags of 
the words in the relation tuples, number of tokens, 
stopwords etc., and uses the labeled instances as 
the training set. This classifier runs on sentences 
from a web corpus to extract millions of relational 
tuples. However, of the 11 million high confident 
relational tuples extracted by this system only 1 
million were concrete facts (Banko, Cararella et al 
2007). Of these concrete facts 88% were estimated 
to be correct. For instance, (Mountain View, head-
quarters of, Google) is a tuple representing a valid 
concrete fact. The remaining 90% of the tuples are 
abstract or do not have well-formed arguments or 
well-formed relations. For instance, (Einstein, de-
rived, theory) is an abstract tuple as it does not 
1448
  
have enough information to indicate a concrete fact 
(Banko, Cararella et al 2007) because the specific 
theory which Einstein derived is missing in that 
tuple. In the tuple (45, ?went to?, ?Boston?), one of 
the arguments (i.e. 45) is not well formed.  
In (Banko and Etzioni, 2008) a Conditional 
Random Field (CRF) classifier is used to perform 
Open Relation Extraction which improves by more 
than 60% the F-score achieved by the Naive Bayes 
model in the TextRunner system. However the 
CRF approach does not solve the problem associ-
ated with extraction of abstract/non-well formed 
tuples. Further, in the same work, it is shown that 
Open RE has a much lower recall in comparison to 
Traditional RE systems. On four common relations 
(Acquisition, Birthplace, InvetorOf, WonAward), 
Open RE attained a recall of 18.4% in comparison 
to 58.4% achieved by Traditional RE (Banko and 
Etzioni 2008). Both Open RE systems discussed 
(Banko, Cararella et al 2007; Banko and Etzioni 
2008) do not perform learning of the category type 
of the entities involved in the relations. They are 
single-pass and do not perform continuous learning 
to improve/extend on what has been learnt. 
2.2 Unsupervised Methods to Extract Rela-
tions between Named Entities 
In general, traditional RE methods extract concrete 
facts and have much higher recall for a given rela-
tion, than Open RE methods. This is due to the 
knowledge fed into Traditional RE methods such 
as the category type of the entities in the relation 
and seed instances for the relation. Traditional RE 
methods require the relations to be manually de-
fined and extract instances only for them. Open RE 
methods, on the other hand, do not require any 
such domain specific knowledge to be manually 
input.  They extract instances for a wide spectrum 
of relations that are not manually pre-defined.  
To overcome the drawbacks of using Traditional 
and Open RE methods, some researchers have used 
unsupervised learning methods to automatically 
generate new relations (with seeds and contexts) 
between specific categories. These automatically 
generated relations can then be used as input to 
Traditional RE systems.  
Hasegawa et.al (Hasegawa, Sekine et al 2004), 
propose an unsupervised clustering based ap-
proach. One feature vector for each co-occurring 
NE pair is formed based on the context words in 
which the NE pair co-occurs. Then, a cosine-
similarity metric is applied to each pair of feature 
vectors to generate a ?NE-pair x NE-pair? matrix. 
Clustering is done on this matrix and each cluster 
of NE-pairs corresponds to a relation predicate.  
The work by Zhang et.al (Zhang, Su et al 2005) 
generates a shallow parse tree for each sentence 
containing a NE pair to generate relation instances. 
A tree similarity metric is used to cluster the rela-
tion instances. This method gives improved F-
score over Hasegawa et.al (Hasegawa, Sekine et al 
2004). Further they use a specialized NE tagger 
built to recognize entities that belong to specific 
predefined categories. The aforementioned meth-
ods (Hasegawa, Sekine et al 2004) (Zhang, Su et 
al. 2005) were tested on a news corpus to identify 
relations between only a couple of pairs of entity 
types (Person-GeoPoliticalEntity and Company-
Company).  
Both of these methods cluster NE-pairs primari-
ly based on lexical similarity of the context words 
connecting the entities. Hence NE-pairs connected 
by lexically different but semantically similar con-
text patterns (e.g. river ?in heart of? city and river 
?flows through? city) would probably not get clus-
tered together. The web data is, however, much 
noisier and has a larger number of entity types (i.e. 
category predicates), thus, another issue is that for 
web scale data NE pairs X NE pairs similarity ma-
trix would not be scalable for many thousands of 
NE-pairs. 
3 Ontology Extension System - OntExt 
The OntExt system for ontology extension, pro-
posed in this paper, combines characteristics from 
both ?Traditional RE? and ?Open RE,? to discover 
new relations among categories that are already 
present in the ontology, and for which many in-
stances have already been extracted.  
Our proposed method for automatic relation ex-
traction offers the following advantages over the 
methods discussed above.  
? The key idea in our approach is to make use 
of redundancy of information in web data - the 
same relational fact is often stated multiple 
times in large text corpora, using different con-
text patterns.  We use this redundancy to clus-
ter together context patterns which are seman-
tically similar although they may be lexically 
dissimilar. 
1449
  
? Instead of clustering on the ?NE-pairs X 
NE-pairs? matrix, clustering is done on a ?Con-
text-pattern X Context-pattern? matrix. This is 
much more scalable as the context patterns are 
fewer in number and since our method applies 
several criteria to prune out irrelevant patterns. 
? To accommodate errors in the input catego-
ry instances and ambiguity in web data, we 
build a classifier which learns to distinguish 
valid relations from semantically invalid rela-
tions. 
 
OntExt has 3 components. 1) It starts exploring 
a large web corpus and 2) category instances ex-
tracted by CPL to generate new relations. After the 
relations are generated, 3) a classifier is developed 
to classify semantically valid relations. 
3.1 Pre-processing 
Following along the same strategy used in [Carlson 
et al, 2010], OntExt uses as input a corpus of 2 
billion sentences, which was generated by using 
the OpenNLP2 package to extract, tokenize, and 
POS-tag sentences from the 500 million web page 
English portion of the ClueWeb09 data [Callan and 
Hoy, 2009]. Before performing relation extraction, 
this corpus is preprocessed. First, sentences which 
contain a pair of known category instances are re-
trieved (e.g. the sentence ?Ottawa is the capital of 
Canada.?, where ?Ottawa? is a known instance of 
the ?City? category and ?Canada? is a known in-
stance of ?Country?).  For every category pair (e.g. 
<City, Country>) the sentences containing known 
instances of both categories are grouped into a set 
S. The text between the two instances is called the 
?context pattern? (e.g. ?is the capital of? is a context 
pattern). Three types of pruning are done on this 
set S. 
1. If the context pattern is a rare one (i.e. if the 
context pattern occurs in less than a threshold 
number of sentences), all sentences with that 
context pattern are removed. Thus we retain 
only frequently occurring contexts.  We use a 
threshold requiring at least 5 sentences in the 
experiments presented in Section 4. 
2. Context patterns which co-occur with very 
few instances of either category type are re-
moved. For example, the category pair <Vehi-
cle,SportsTeam> has several sentences such as 
                                                          
2  http://opennlp.sourceforge.net. 
?Car was engulfed in flames?,  ?Truck was en-
gulfed in flames? etc. Note that Flames (Calga-
ry Flames) is a SportsTeam. But here flames 
clearly does not refer to a Sportsteam. This 
context ?was engulfed in? connects several in-
stance of a ?Vehicle? category to a single in-
stance of SportsTeam instance. Hence all sen-
tences with this context are removed. Note this 
context would not have been removed in step 1 
as that is just a threshold on the number of sen-
tences in which any pair occurs. We use a 
threshold requiring at least 3 distinct instances 
of both the domain and the range, for each 
context. 
3. Banko et.al, 2008 show that most binary re-
lational contexts fall under certain types of lex-
ico-synctatic patterns. They include context 
patterns like ?C1 Verb C2?, ?C1 NP Prep C2?, 
?C1 Verb Prep C2? and ?C1 to Verb C2? (C1 
and C2 are category instances). Hence context 
patterns which do not fall under the above 
types are removed from the set S as they are 
not likely to produce relation instances.  
3.2 Relation Generation 
From the previous pre-processing step OntExt re-
trieves for each category pair a pruned set S? of 
sentences. Each sentence has a pair of category 
instances and the context connecting them. 
 
Algorithm 1: Relation Generator 
 
Input: One pair of Categories (C1, C2) and set of 
sentences, each containing a pair of instances 
known to belong to C1 and C2. The phrase con-
necting the instances in the sentence is the context. 
Output: Relations and their seed instances  
 
Steps: 
1. From the input sentences, build a Context by 
Context co-occurrence matrix (Shown in figure 
1). The matrix is then normalized. 
2.  Apply K-means clustering on the matrix to 
cluster the related contexts together. Each clus-
ter corresponds to a possible new relation be-
tween the two input categories. (Weka Ma-
chine Learning package [Hall et al, 2009] was 
used to perform K-means clustering. The value 
of K was set to 5 based on trial and error ex-
periments.) 
1450
  
3. Rank the known instance pairs (belonging to 
C1,C2) for each cluster and take the top 50 as 
seed instances for the relation 
 
The key data structure used by OntExt is a co-
occurrence matrix of the contexts for each category 
pair, as shown in Figure 1. In this matrix, each cell 
corresponds to the number of pairs of category in-
stances that both contexts co-occur with (e.g. the 
sentences ?Vioxx can cure Arthritis? and ?Vioxx is 
a treatment for Arthritis? provide a case where the 
2 contexts ?can cure? and ?is a treatment for? co-
occur with an instance pair [Vioxx, Arthritis]). Ini-
tially, the value of Matrix(I,j) is the number of cat-
egory instance pairs that occur with both context i 
and context j.  We then normalize each cell in the 
matrix, dividing it by by the total count for its row. 
?
?
? N
j
jiMatrix
jiMatrixjiMatrix
0
),(
),(),(
 
We also give higher weight to contexts which co-
occur with only a few contexts over ones which are 
generic and co-occur with most contexts. 
|}0),(:)({|*),(),( ?? jiMatrixjContext
NjiMatrixjiMatrix
 
 
Where N is the total number of contexts, and 
|{Context(j) : Matrix(i,j) > 0}| refers to the number 
of cells in the row Matrix(i) which are greater than 
zero. 
For example, for the <drug, disease> category 
pair after 122 contexts were obtained after prepro-
cessing. Contexts such as ?to treat?, ?for treatment 
of?, ?medication? which all indicate the same rela-
tion (drug-to treat-disease) have high co-
occurrence values (see Figure 1). Similarly con-
texts such as ?can cause?, ?may cause?, ?can lead to? 
(indicating the relation drug-can cause-disease) 
have high co-occurrence values (see Figure 1). 
When OntExt performs clustering on this co-
occurrence matrix the contexts with large co-
occurrences get clustered together. Each cluster is 
then used to propose a possible new relation. The 
centroid of each cluster is used to build the relation 
name. If the centroid of a cluster is the context ?for 
treatment of?, then the relation name is ?drug-for-
treatment-of-disease?. 
OntExt next generates seed instances for the 
proposed relation. The seed instances which co-
occur with contexts corresponding to the cluster 
centroid or close to centroid will be best repre-
sentative of the relation. So the strength of the seed 
instance is inversely proportional to the standard 
deviation of the context from the centroid of the 
relation contexts cluster.  Also the strength of the 
seed instance is directly proportional to the number 
of times it co-occurs with the context. 
 
 
 
Figure 1: This figure shows the Context by Context 
sub-matrix (with 6 contexts) for the category pair 
(Drug, Disease) and the seed instances for each 
relation.  As described in the text, each entry gives 
the normalized count of the number of known 
<drug, disease> pairs that occur with both the row 
context and the column context. 
 
To summarize, each seed instance s (pair of cat-
egory instances) is weighted as follows  
   
Where, 
Pattern_cluster is the cluster of pattern contexts for 
this given relation 
Occ(c,s) is the number of times instance ?s? co-
occurs with the pattern context ?c? 
sd(c) is the standard deviation of the context from 
the centroid of the pattern cluster.   
Using this metric the instances are ranked and the 
top 50 are output as initial seed instances for the 
proposed relation.   
1451
  
3.3 Classifying semantically valid relations 
More than half of the relations generated in the 
previous step are invalid due to the following rea-
sons 
1. Error in category instances: The category 
instances input to OntExt come from NELL. In 
the version of the knowledge base used in the-
se experiments, the accuracy of these instances 
was 78%. Due to the erroneous category in-
stances some invalid relations are generated by 
OntExt. For instance the generated relation, 
?condiment-wearing-clothing? with seeds 
(pig,dress), (rabbit,pants) etc. Here ?pig?  and 
?rabbit? were incorrectly identified by NELL as 
instances of ?condiment?. 
2. Semantic Ambiguity: Consider the generated 
relation ?bakedgood-baking-magazine? with in-
stances (cookies,time), (cupcakes, people), etc. 
Here the instances ?time? and ?people? do not 
refer to magazines, although they can in gen-
eral.  Due to the semantic ambiguity of these 
instances this invalid relation got generated 
3. Semantically Incomplete relations: Some of 
the generated relations require a third entity or 
some more contextual information, in order to 
be considered semantically valid. For instance, 
?personUs-said-company? or ?newspaper-is-
reporting-that-company?. These don?t stand by 
themselves as two-argument relational facts 
and need more information to be complete 
4. Illogical relations: Some generated relations 
simply have no real semantic meaning. These 
relations are generated due to the category in-
stances appearing together in some unrelated 
contexts. E.g. the generated relation ?date-
starting-date? with seeds such as (Wednesday, 
June), (friday, July) and the relation ?country-
minister-of- economicsector? with seeds (ja-
pan,agriculture), (india, industry).  
The introduction of these invalid relations can 
adversely affect the performance of NELL. How-
ever, it is a challenging problem to develop auto-
mated ways to distinguish between valid and inva-
lid relations without any domain specific 
knowledge. To approach this problem, we identi-
fied a set of features which can help characterizing 
valid and invalid relations, and which can be gen-
erated automatically. Below is a description of the 
features and the intuition behind their use for this 
classification task. 
Each generated relation has a pair of category 
types (C1, C2), a corresponding set of seed in-
stances (which are pairs of instances belonging to 
C1 and C2) and pattern contexts connecting C1 
and C2.  Let N be the number of seed instance 
pairs and N1 and N2 be number of unique instanc-
es (out of these N instance pairs) belonging to cat-
egories C1 and C2 respectively. 
1. Normalized frequency count: The frequency 
count of each category instance is obtained 
from the corpus and normalized by the catego-
ry instance with maximum count. For a given 
relation, a feature is generated by averaging 
the normalized frequency counts of the in-
stances belonging to C1. Another similar fea-
ture is generated for C2 following the same 
strategy. For example the relation <Profession 
?believe that? Movie> was generated due to 
common words like ?predator?, ?earthquake? 
being identified as movie names out of con-
text. These features can help identify such in-
valid relations.   
2. Distribution of extraction patterns: NELL 
learns instances as well as extraction patterns 
for each category (e.g. the category Actor has 
extraction patterns such as ?_ got an Oscar 
award?, ?_ is the movie?s lead actor?). If a cat-
egory instance co-occurs in the web corpus 
with several extraction patterns belonging to 
other categories, then that instance has large 
ambiguity. We measure ambiguity of an in-
stance (i) belonging to category ?C? with re-
spect to another category ?M? (where M is not 
a sub type or super type of ?C?) as   
Ambiguity(i,M) =  
 withoccurs-co i''  that C''in  patterns extraction of #
 ithoccurs-co i'' that M''in  patterns extraction of # 
We measure the average ambiguity for the set 
of instances (of size N) belonging to category 
C in the generated seeds as follows, 
  
??CiM NMiAmbiguityMax /),((
) 
   
Two features are generated for categories C1  
and C2 in the relation. 
3. Relationship characteristics: We identified a 
few characteristics of the relation which help 
in identifying valid relations.  If in the generat-
ed relation, most instances of C1 co-occur only 
1452
  
with very few instances of C2 (or vice versa) 
then the relation could be weak. For example, 
<Organization ?Provides? EconomicSector> - 
the instance ?Information? (of category Eco-
nomicSector) connects to a large percentage of 
items in the category ?Organization? but does 
not express a meaningful relation. So we con-
sider the instance (in this example ?Infor-
mation?, let us call it ?maxconnect_instance?) 
co-occurring with maximum number of in-
stances of the other category. The percentage 
of instances it co-occurs with from among the 
total number of instances of the other category 
which are part of the seed instances is taken as 
a feature. Also if that instance is a very com-
mon word (like ?information? which in several 
contexts does not refer to ?EconomicSector?) 
then this could indicate the presence of an in-
valid relation. So the normalized frequency 
count of this instance (maxconnect_instance) is 
taken as another feature.  
4. Pattern Contexts: The number of pattern con-
texts attained through pattern clustering for the 
relation is taken as another feature. The pres-
ence of several pattern contexts connecting the 
instances between the two categories could in-
dicate that the relation is a valid one.  The 
presence of Hearst patterns (Hearst M, 1992) 
referring to a hyponym (?is-a?) relation in pat-
tern contexts indicates the possibility of a valid 
relation, and is taken as another feature 
Another feature is regarding how specific is 
the context pattern to this relation. If the same 
context connects say C1 instances to instances 
of several other categories apart from C2, then 
this context is not unique to this relation and 
might not indicate a meaningful valid relation-
ship. So the ratio of the number of instances in 
C2 connected to C1 versus the number of in-
stances from all categories connected to C1 by 
the most significant pattern context (i.e. cen-
troid in pattern cluster) is taken as a feature. A 
similar feature is generated for C2 as well. 
4 Experimental Setup and Results 
4.1 CPL System 
CPL (Carlson et al, 2010) is a semi-supervised 
learning system which takes in an input ontology 
(containing category and relation predicates and 
corresponding seed instances) and constraints 
(such as Mutual exclusion rules between predi-
cates). The system iteratively extracts patterns and 
instances for the category/relation predicates from 
a web corpus of around 500 million web pages.  
CPL is one learning component in NELL (the Nev-
er Ending Language Learner) (Carlson et al, 
2010b). 
4.2 Relation Generation: 
We use approximately 22,000 category instances 
belonging to 122 categories extracted by CPL at 
the end of its 20th iteration and the web corpus as 
input to perform the co-clustering described in 
Section 3.2 and generate the new relations. The 
process generated 781 relations. For each relation, 
the relation name, types of the categories involved 
in the relation and the seed instances and patterns 
for each relation were generated. Table 1 in section 
1 shows a sample of valid relations generated by 
this method. 
Tables 2, 3, 4 and 5 show invalid relations for 
each type of invalidity, ?Error in the Category In-
stances?, ?Semantic Ambiguity?, ?Semantically 
Incomplete Relations? and ?Illogical Relations? 
respectively. More specifically, Table 2 shows a 
sample of relations generated due to an entity be-
ing labeled incorrectly as to belong to a category. 
The incorrect category instances are in italics. 
Table 3 presents a sample of relations which 
were generated because of semantic ambiguity. 
Instances with ambiguity are in italics. 
Table 4 shows some of the generated relations 
which are semantically incomplete. 
Table 5 presents samples of illogical relations 
which do not establish any concrete fact. 
Table 2. Examples of Incorrect category in-
stances. 
name(category1 
-main context- 
category2) 
Relation 
Contexts 
Seed 
Instances 
SportsGame 
-Beating- 
Country 
?beating? 
 
"tournament,Sri Lanka" 
"champions, France" 
"match, canada" 
Animal 
-will eat-
Condiment 
?will eat? 
?eating? 
 
"wolf, sheep" 
"fox, rabbit" 
"lion, lamb" 
 
 
 
 
 
1453
  
 
Table 3. Examples of Semantically Ambiguous 
relations.  
Name Relation 
Contexts 
Seed 
Instances 
Bird 
-play- 
City 
?play? 
 
"Cardinals, Atlanta" 
"Ravens, Miami" 
"Eagles, Chicago" 
BakedGood 
-baking- 
Magazine 
?baking? 
"time, cakes" 
"people, cookies" 
 
Table 4. Examples of semantically incomplete 
relations. 
Name Relation 
Contexts 
Seed 
Instances 
Personus 
acknowledged 
Date 
 
?acknowledged? 
?warned? 
?met? 
 
"mr obama, tues-
day" 
"george w . bush, 
tuesday" 
"al gore, thursday" 
NewsPaper 
-is reporting 
that- 
Company 
?is reporting 
that? 
?writes that? 
?reported that? 
"financial times, 
apple" 
"wall street jour-
nal, gm" 
"wall street jour-
nal, yahoo" 
 
Table 5. Examples of relations representing 
facts that are not concrete.  
Name Relation 
Contexts 
Seed 
Instances 
Emotion 
-of living in-
StateOrPro 
vince 
?of living in? 
"joy, california" 
"excitement, colora-
do" 
"fear, iowa" 
BodyPart 
-to keep- 
BodyPart 
 
?to keep? 
?guard? 
?hand, eye? 
?nose, throat? 
?eye, brain? 
?elbow, hand? 
4.3 Relation Classification: 
To determine the feasibility of automatically classi-
fying OntExt?s proposed relations as valid or inva-
lid, we trained and tested a classifier using the fea-
tures described above, using manually assigned 
class label for some of the generated relations (252 
relations) as valid or invalid (the criteria for which 
was explained before). 115 of these 252 relations 
were found to be valid by manual evaluation. This 
shows the need for a machine learning classifier to 
identify valid/invalid relations. The various fea-
tures described earlier (such as normalized fre-
quency count, relationship characteristics, pattern 
context features, distribution of extraction patterns) 
were generated for each relation. Ten-fold cross 
validation experiments were carried out with vari-
ous classifiers.  A Random Forest classifier per-
formed the best.  Precision, recall and ROC-area is 
shown in the table below (ROC area is the area 
under the ROC curve which plots the classifier 
performance by having the True Positive Rate on 
the Y-axis and False Positive Rate on the X-axis). 
 
Table 6. Classifier performance. 
RelationType Precision Recall ROC Area 
Valid 71.6 72.2 0.804 
Invalid 76.5 75.9 0.804 
Weighted 
Avg. 
74.2 74.2 0.804 
 
These results indicate that the system is able to 
learn to identify semantically valid relations with-
out using any manually input information. The val-
id relations generated can be input to NELL, al-
lowing it to iteratively learn additional instances 
for each proposed relation. 
5 Conclusion and Future work: 
Open Relation Extraction and Traditional Relation 
Extraction have their respective strengths and 
weaknesses. The OntExt system proposed in this 
work combines the strengths of both of those 
methods. The relation predicates automatically 
generated by our approach are typed, have a mean-
ingful name identifying the relation, and are ac-
companied by suggested context patterns and seed 
instances.  These relations can be input to NELL to 
learn more instances for the relation. We propose 
in the future to integrate this relation generation 
system into NELL, to iteratively extend NELL?s 
initial ontology, providing an ongoing stream of 
new learning tasks.  After every fixed set of 
NELL?s iterations, its growing knowledge base 
would be input to the relation generation system 
which will in turn feed NELL with new relation 
predicates.  One additional area for future research 
is to extend OntExt to discover new categories in 
addition to new relations. 
1454
  
Acknowledgements  
We gratefully acknowledge support for this re-
search from Darpa, Google, Yahoo! and the Brazil-
ian research agency CNPq. We also gratefully 
acknowledge Dr. Madhavi Ganapathiraju (at Uni-
versity of Pittsburgh) for her support and encour-
agement. 
References  
Agichtein, E. and L. Gravano (2000). "Snowball: Ex-
tracting relations from large plain-text collections." 
Procs. of the Fifth ACM International Conference on 
Digital Libraries. 
Banko, M., M. Cararella, et al (2007). "Open infor-
mation extraction from the web." In Procs. of IJCAI. 
Banko, M. and O. Etzioni (2008). "The Tradeoffs Be-
tween Open and Traditional Relation Extraction." In 
Proceedings of ACL-08. 
Callan, J., and Hoy, M. (2009). Clueweb09 data set. 
http://boston.lti.cs.cmu.edu/Data/clueweb09/. 
Carlson, A., J. Betteridge, et al (2009). "Coupling 
Semi-Supervised Learning of Categories and Rela-
tions." Proceedings of the NAACL HLT 2009 Work-
shop on Semi-supervised Learning for Natural Lan-
guage Processing. 
A. Carlson, J. Betteridge, et al (2010). ?Coupled Semi-
Supervised Learning for Information Extraction,? 
Proceedings of the ACM International Conference on 
Web Search and Data Mining (WSDM), 2010. 
A. Carlson, J. Betteridge, et al, (2010b). ?Toward an 
Architecture for Never-Ending Language Learning,? 
Proceedings of the Conference on Artificial Intelli-
gence (AAAI), 2010. 
Etzioni, O., M. Cafarella, et al (2005). "Unsupervised 
named-entity extraction from the web: An experi-
mental study." Artificial Intelligence. 
Hasegawa, T., S. Sekine, et al (2004). "Discovering 
relations among named entities from large corpora." 
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics  
Zhang, M., J. Su, et al (2005). "Discovering Relations 
between Named Entities from a Large Raw Corpus 
Using Tree Similarity-based Clustering." IJCNLP 05. 
Hasegawa, T., S. Sekine, et al (2004). "Discovering 
relations among named entities from large corpora." 
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics  
Zhang, M., J. Su, et al (2005). "Discovering Relations 
between Named Entities from a Large Raw Corpus 
Using Tree Similarity-based Clustering." IJCNL 
Hearst, M. (1992) Automatic Acquisition of Hyponyms 
from Large Text Corpora. Proc. of the Fourteenth In-
ternational Conference on Computational Linguistics, 
Nantes, F 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann, Ian H. Witten (2009); 
The WEKA Data Mining Software: An Update; 
SIGKDD Explorations, Volume 11, Issue 1.
 
1455
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 123?128,
Dublin, Ireland, August 23-24, 2014.
Biocom Usp: Tweet Sentiment Analysis with Adaptive Boosting
Ensemble
Na?dia F. F. Silva, Eduardo R. Hruschka
University of S?ao Paulo, USP
S?ao Carlos, SP, Brazil
nadia, erh@icmc.usp.br
Estevam Rafael Hruschka Jr.
Department of Computer Science
Federal University of Sao Carlos.
S?ao Carlos, SP, Brazil
estevam@dc.ufscar.br
Abstract
We describe our approach for the
SemEval-2014 task 9: Sentiment Analy-
sis in Twitter. We make use of an en-
semble learning method for sentiment
classification of tweets that relies on
varied features such as feature hash-
ing, part-of-speech, and lexical fea-
tures. Our system was evaluated in
the Twitter message-level task.
1 Introduction
The sentiment analysis is a field of study that
investigates feelings present in texts. This
field of study has become important, espe-
cially due to the internet growth, the content
generated by its users, and the emergence of
the social networks. In the social networks
such as Twitter people post their opinions in a
colloquial and compact language, and it is be-
coming a large dataset, which can be used as
a source of information for various automatic
tools of sentiment inference. There is an enor-
mous interest in sentiment analysis of Twit-
ter messages, known as tweets, with applica-
tions in several segments, such as (i) directing
marketing campaigns, extracting consumer re-
views of services and products (Jansen et al.,
2009); (ii) identifying manifestations of bully-
ing (Xu et al., 2012); (iii) predicting to fore-
cast box-office revenues for movies (Asur and
Huberman, 2010); and (iv) predicting accep-
tance or rejection of presidential candidates
(Diakopoulos and Shamma, 2010; O?Connor
et al., 2010).
This work is licensed under a Creative
Commons Attribution 4.0 International Li-
cence. Page numbers and proceedings footer
are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
One of the problems encountered by re-
searchers in tweet sentiment analysis is the
scarcity of public datasets. Although Twit-
ter sentiment datasets have already been cre-
ated, they are either small ? such as Obama-
McCain Debate corpus (Shamma et al., 2009)
and Health Care Reform corpus (Speriosu et
al., 2011) or big and proprietary such as in
(Lin and Kolcz, 2012). Others rely on noisy
labels obtained from emoticons and hashtags
(Go et al., 2009). The SemEval-2014 task 9: Sen-
timent Analysis in Twitter (Nakov et al., 2013)
provides a public dataset to be used to com-
pare the accuracy of different approaches.
In this paper, we propose to analyse tweet
sentiment with the use of Adaptive Boost-
ing (Freund and Schapire, 1997), making
use of the well-known Multinomial Classi-
fier. Boosting is an approach to machine
learning that is based on the idea of creat-
ing a highly accurate prediction rule by com-
bining many relatively weak and inaccurate
rules. The AdaBoost algorithm (Freund and
Schapire, 1997) was the first practical boost-
ing algorithm, and remains one of the most
widely used and studied, with applications in
numerous fields. Therefore, it has potential to
be very useful for tweet sentiment analysis, as
we address in this paper.
2 Related Work
Classifier ensembles for tweet sentiment anal-
ysis have been underexplored in the literature
? a few exceptions are (Lin and Kolcz, 2012;
Clark and Wicentwoski, 2013; Rodriguez et
al., 2013; Hassan et al., 2013).
Lin and Kolcz (2012) used logistic regres-
sion classifiers learned from hashed byte 4-
grams as features ? The feature extractor con-
siders the tweet as a raw byte array. It moves
a four-byte sliding window along the array,
123
and hashes the contents of the bytes, the value
of which was taken as the feature id. Here the
4-grams refers to four characters (and not to
four words). They made no attempt to per-
form any linguistic processing, not even word
tokenization. For each of the (proprietary)
datasets, they experimented with ensembles
of different sizes. The ensembles were formed
by different models, obtained from different
training sets, but with the same learning algo-
rithm (logistic regression). Their results show
that the ensembles lead to more accurate clas-
sifiers.
Rodr??gues et al. (2013) and Clark et al.
(2013) proposed the use of classifier ensem-
bles at the expression-level, which is related
to Contextual Polarity Disambiguation. In this
perspective, the sentiment label (positive,
negative, or neutral) is applied to a specific
phrase or word within the tweet and does not
necessarily match the sentiment of the entire
tweet.
Finally, another type of ensemble frame-
work has been recently proposed by Hassan
et al. (2013), who deal with class imbalance,
sparsity, and representational issues. The au-
thors propose to enrich the corpus using mul-
tiple additional datasets related to the task of
sentiment classification. Differently from pre-
vious works, the authors use a combination of
unigrams and bigrams of simple words, part-
of-speech, and semantic features.
None of the previous works used AdaBoost
(Freund and Schapire, 1996). Also, lexicons
and/or part-of-speech in combination with
feature hashing, like in (Lin and Kolcz, 2012)
have not been addressed in the literature.
3 AdaBoost Ensemble
Boosting is a relatively young, yet extremely
powerful, machine learning technique. The
main idea behind boosting algorithms is to
combine multiple weak learners ? classifi-
cation algorithms that perform only slightly
better than random guessing ? into a power-
ful composite classifier. Our focus is on the
well known AdaBoost algorithm (Freund and
Schapire, 1997) based on Multinomial Naive
Bayes as base classifiers (Figure 1).
AdaBoost and its variants have been ap-
plied to diverse domains with great success,
owing to their solid theoretical foundation,
accurate prediction, and great simplicity (Fre-
und and Schapire, 1997). For example, Viola
and Jones (2001) used AdaBoost to face de-
tection, Hao and Luo (2006) dealt with im-
age segmentation, recognition of handwritten
digits, and outdoor scene classification prob-
lems. In (Bloehdorn and Hotho, 2004) text
classification is explored.
Figure 1: AdaBoost Approach
4 Feature Engineering
The most commonly used text representation
method adopted in the literature is known as
Bag of Words (BOW) technique, where a doc-
ument is considered as a BOW, and is repre-
sented by a feature vector containing all the
words appearing in the corpus. In spite of
BOW being simple and very effective in text
classification, a large amount of information
from the original document is not considered,
word order is ruptured, and syntactic struc-
tures are broken. Therefore, sophisticated fea-
ture extraction methods with a deeper under-
standing of the documents are required for
sentiment classification tasks. Instead of us-
ing only BOW, alternative ways to represent
text, including Part of Speech (PoS) based fea-
tures, feature hashing, and lexicons have been
addressed in the literature.
We implemented an ensemble of classifiers
that receive as input data a combination of
three features sets: i) lexicon features that cap-
tures the semantic aspect of a tweet; ii) fea-
ture hashing that captures the surface-form as
abbreviations, slang terms from this type of
social network, elongated words (for exam-
ple, loveeeee), sentences with words without
a space between them (for instance, Ilovveap-
ple!), and so on; iii) and a specific syntactic fea-
tures for tweets. Technical details of each fea-
ture set are provided in the sequel.
Lexicon Features
We use the sentimental lexicon provided by
(Thelwall et al., 2010) and (Hu and Liu, 2004).
The former is known as SentiStrength and
124
provides: an emotion vocabulary, an emoti-
cons list (with positive, negative, and neutral
icons), a negation list, and a booster word list.
We use the negative list in cases where the
next term in a sentence is an opinion word
(either positive or negative). In such cases
we have polarity inversion. For example, in
the sentence ?The house is not beautiful?, the
negative word ?not? invert the polarity of the
opinion word beautiful. The booster word list
is composed by adverbs that suggest more or
less emphasis in the sentiment. For exam-
ple, in the sentence ?He was incredibly rude.?
the term ?incredibly? is an adverb that lay em-
phasis on the opinion word ?rude?. Besides
using SentiStrength, we use the lexicon ap-
proach proposed by (Hu and Liu, 2004). In
their approach, a list of words and associa-
tions with positive and negative sentiments
has been provided that are very useful for
sentiment analysis.
These two lexicons were used to build the
first feature set according to Table 1, where it
is presented an example of tweet representa-
tion for the tweet
1
: ?The soccer team didn?t
play extremely bad last Wednesday.? The
word ?bad? exists in the lexicon list of (Hu
and Liu, 2004), and it is a negative word.
The word ?bad? also exists in the negation
list provided by (Thelwall et al., 2010). The
term ?didn?t? is a negative word according to
SentiStrength (Thelwall et al., 2010) and there
is a polarity inversion of the opinion words
ahead. Finally, the term ?extremely? belongs
the booster word list and this word suggests
more emphasis to the opinion words existing
ahead.
positive negative neutral class
tweet
1
3 0 0 positive
Table 1: Representing Twitter messages with
lexicons.
Feature hashing
Feature hashing has been introduced for text
classification in (Shi et al., 2009), (Wein-
berger et al., 2009), (Forman and Kirshen-
baum, 2008), (Langford et al., 2007), (Caragea
et al., 2011). In the context of tweet classi-
fication, feature hashing offers an approach
to reducing the number of features provided
as input to a learning algorithm. The origi-
nal high-dimensional space is ?reduced? by
hashing the features into a lower-dimensional
space, i.e., mapping features to hash keys.
Thus, multiple features can be mapped to the
same hash key, thereby ?aggregating? their
counts.
We used the MurmurHash3 function
(SMHasher, 2010), that is a non-cryptographic
hash function suitable for general hash-based
lookup tables. It has been used for many
purposes, and a recent approach that has
emerged is its use for feature hashing or
hashing trick. Instead of building and storing
an explicit traditional bag-of-words with
n-grams, the feature hashing uses a hash
function to reduce the dimensionality of the
output space and the length of this space
(features) is explicitly fixed in advance. For
this paper, we used this code (in Python):
Code Listing 1: Murmurhash:
from sklearn.utils.murmurhash
import murmurhash3_bytes_u32
for w in "i loveee apple".split():
print("{0} => {1}".format(
w,murmurhash3_bytes_u32(w,0)%2
**
10))
The dimensionality is 2 ? ?10, i.e 2
10
fea-
tures. In this code the output is a hash code
for each word ?w? in the phrase ?i loveee
apple?, i.e. i => 43, loveee => 381 and
apple => 144. Table 2 shows an example of
feature hashing representation.
1 2 3 4 ? ? ? 1024 class
tweet
1
0 0 1 1 ? ? ? 0 positive
tweet
2
0 1 0 3 ? ? ? 0 negative
tweet
3
2 0 0 0 ? ? ? 0 positive
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. ? ? ?
.
.
.
.
.
.
tweet
n
0 0 2 1 ? ? ? 0 neutral
Table 2: Representing Twitter messages with
feature hashing.
Specific syntactic (PoS) features
We used the Part of Speech (PoS) tagged for
tweets with the Twitter NLP tool (Gimpel et
al., 2011). It encompasses 25 tags including
Nominal, Nominal plus Verbal, Other open-
class words like adjectives, adverbs and in-
terjection, Twitter specific tags such as hash-
tags, mention, discourse marker, just to name
125
a few. Table 3 shows an example of syntactic
features representation.
tag
1
tag
2
tag
3
tag
4
? ? ? tag
25
class
tweet
1
0 0 3 1 ? ? ? 0 positive
tweet
2
0 2 0 1 ? ? ? 0 negative
tweet
3
1 0 0 0 ? ? ? 0 positive
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. ? ? ?
.
.
.
.
.
.
tweet
n
0 0 1 1 ? ? ? 0 neutral
Table 3: Representing Twitter messages with
syntactic features.
A combination of lexicons, feature hashing,
and part-of-speech is used to train the ensem-
ble classifiers, thereby resulting in 1024 fea-
tures from feature hashing, 3 features from
lexicons, and 25 features from PoS.
5 Experimental Setup and Results
We conducted experiments by using the
WEKA platform
1
. Table 4 shows the class dis-
tributions in training, development, and test-
ing sets. Table 5 presents the results for posi-
tive and negative classes with the classifiers
used in training set, and Table 6 shows the
computed results by SemEval organizers in
the test sets.
Training Set
Set Positive Negative Neutral Total
Train 3,640 (37%) 1,458 (15%) 4,586 (48%) 9,684
Development Set
Set Positive Negative Neutral Total
Dev 575 (35%) 340(20%) 739 (45%) 1,654
Testing Sets
Set Positive Negative Neutral Total
LiveJournal 427 (37%) 304 (27%) 411 (36%) 1,142
SMS2013 492 (23%) 394(19%) 1,207 (58%) 2,093
Twitter2013 1,572 (41%) 601 (16%) 1,640 (43%) 3,813
Twitter2014 982 (53%) 202 (11%) 669 (36%) 1,853
Twitter2014Sar 33 (38%) 40 (47%) 13 (15%) 86
Table 4: Class distributions in the training set
(Train), development set (Dev) and testing set
(Test).
6 Concluding Remarks
From our results, we conclude that the use of
AdaBoost provides good performance in the
sentiment analysis (message-level subtask).
In the cross-validation process, Multinomial
Naive Bayes (MNB) has shown better results
than Support Vector Machines (SVM) as a
component for AdaBoost. However, we feel
1
http://www.cs.waikato.ac.nz/ml/weka/
Set Algorithm F-Measure
Positive
F-Measure
Negative
Average
Train MNB 63.40 49.40 56.40
Train SVM 64.00 44.50 54.20
Train AdaBoost w/ SVM 62.50 44.50 53.50
Train AdaBoost w/ MNB 65.10 49.60 57.35
Table 5: Results from 10-fold cross validation
in the training set with default parameters of
Weka. MNB and SVM stand for Multinomial
Naive Bayes and Support Vector Machine, re-
spectively.
Scoring LiveJournal2014
class precision recall F-measure
positive 69.79 64.92 67.27
negative 76.64 61.64 68.33
neutral 51.82 69.84 59.50
overall score : 67.80
Scoring SMS2013
positive 61.99 46.78 53.32
negative 72.34 42.86 53.82
neutral 53.85 83.76 65.56
overall score : 53.57
Scoring Twitter2013
positive 68.07 66.13 67.08
negative 48.09 50.00 49.02
neutral 67.20 68.15 67.67
overall score : 58.05
Scoring Twitter2014
positive 65.17 70.48 67.72
negative 53.47 48.21 50.70
neutral 59.94 55.62 57.70
overall score : 59.21
Scoring Twitter2014Sarcasm
positive 63.64 44.68 52.50
negative 22.50 75.00, 34.62
neutral 76.92 37.04 50.00
overall score : 43.56
Table 6: Results in the test sets ? AdaBoost
plus Multinomial Naive Bayes, which was the
best algorithm in cross validation.
that further investigations are necessary be-
fore making strong claims about this result.
Overall, the SemEval Tasks have make evi-
dent the usual challenges when mining opin-
ions from Social Media channels: noisy text,
irregular grammar and orthography, highly
specific lingo, and others. Moreover, tempo-
ral dependencies can affect the performance if
the training and test data have been gathered
at different.
Acknowledgements
The authors would like to thank the Re-
search Agencies CAPES, FAPESP, and CNPq
for their financial support.
References
Sitaram Asur and Bernardo A. Huberman. 2010.
Predicting the future with social media. In Pro-
ceedings of the 2010 International Conference on
126
Web Intelligence and Intelligent Agent Technology
- Volume 01, WI-IAT ?10, pages 492?499, Wash-
ington, DC, USA. IEEE Computer Society.
Stephan Bloehdorn and Andreas Hotho. 2004.
Text classification by boosting weak learners
based on terms and concepts. In Proceedings of
the Fourth IEEE International Conference on Data
Mining, pages 331?334. IEEE Computer Society
Press, November.
Cornelia Caragea, Adrian Silvescu, and Prasen-
jit Mitra. 2011. Protein sequence classifica-
tion using feature hashing. In Fang-Xiang Wu,
Mohammed Javeed Zaki, Shinichi Morishita,
Yi Pan, Stephen Wong, Anastasia Christianson,
and Xiaohua Hu, editors, BIBM, pages 538?543.
IEEE.
Sam Clark and Rich Wicentwoski. 2013. Swatcs:
Combining simple classifiers with estimated
accuracy. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages
425?429, Atlanta, Georgia, USA, June.
Nicholas A. Diakopoulos and David A. Shamma.
2010. Characterizing debate performance via
aggregated twitter sentiment. In Proceedings of
the SIGCHI Conference on Human Factors in Com-
puting Systems, CHI ?10, pages 1195?1198, New
York, NY, USA. ACM.
George Forman and Evan Kirshenbaum. 2008.
Extremely fast text feature extraction for clas-
sification and indexing. In CIKM ?08: Proceed-
ing of the 17th ACM conference on Information and
knowledge management, pages 1221?1230, New
York, NY, USA. ACM.
Yoav Freund and Robert E. Schapire. 1996. Ex-
periments with a new boosting algorithm. In
Thirteenth International Conference on Machine
Learning, pages 148?156, San Francisco. Morgan
Kaufmann.
Yoav Freund and Robert E Schapire. 1997.
A decision-theoretic generalization of on-line
learning and an application to boosting. Jour-
nal of Computer and System Sciences, 55(1):119 ?
139.
Kevin Gimpel, Nathan Schneider, Brendan
O?Connor, Dipanjan Das, Daniel Mills, Jacob
Eisenstein, Michael Heilman, Dani Yogatama,
Jeffrey Flanigan, and Noah A. Smith. 2011.
Part-of-speech tagging for twitter: Annotation,
features, and experiments. In Proceedings of
the 49th Annual Meeting of the Association for
Computational Linguistics ? Short Papers - Volume
2, HLT ?11, pages 42?47, Stroudsburg, PA, USA.
Alec Go, Richa Bhayani, and Lei Huang. 2009.
Twitter sentiment classification using distant
supervision. Processing, pages 1?6.
Wei Hao and Jiebo Luo. 2006. Generalized
Multiclass AdaBoost and Its Applications to
Multimedia Classification. In Computer Vision
and Pattern Recognition Workshop, 2006. CVPRW
&#039;06. Conference on, page 113, Washington,
DC, USA, June. IEEE.
Ammar Hassan, Ahmed Abbasi, and Daniel
Zeng. 2013. Twitter sentiment analysis: A
bootstrap ensemble framework. In SocialCom,
pages 357?364. IEEE.
Minqing Hu and Bing Liu. 2004. Mining and
summarizing customer reviews. In Proceed-
ings of the tenth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
KDD ?04, pages 168?177, New York, NY, USA.
ACM.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and
Abdur Chowdury. 2009. Twitter power:
Tweets as electronic word of mouth. J. Am. Soc.
Inf. Sci. Technol., 60(11):2169?2188, nov.
John Langford, Alex Strehl, and Lihong Li. 2007.
Vowpal wabbit online learning project. http:
//mloss.org/software/view/53/.
Jimmy Lin and Alek Kolcz. 2012. Large-scale ma-
chine learning at twitter. In Proceedings of the
2012 ACM SIGMOD International Conference on
Management of Data, SIGMOD ?12, pages 793?
804, New York, NY, USA. ACM.
Preslav Nakov, Sara Rosenthal, Zornitsa
Kozareva, Veselin Stoyanov, Alan Ritter,
and Theresa Wilson. 2013. Semeval-2013 task
2: Sentiment analysis in twitter. In Second
Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the
Seventh International Workshop on Semantic Eval-
uation (SemEval 2013), pages 312?320, Atlanta,
Georgia, USA, June.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In ICWSM?10, pages
1?1.
Penagos Carlos Rodriguez, Jordi Atserias, Joan
Codina-Filba, David Garc?a-Narbona, Jens
Grivolla, Patrik Lambert, and Roser Saur?.
2013. Fbm: Combining lexicon-based ml and
heuristics for social media polarities. In Pro-
ceedings of SemEval-2013 ? International Work-
shop on Semantic Evaluation Co-located with *Sem
and NAACL, Atlanta, Georgia. Url date at 2013-
10-10.
David A. Shamma, Lyndon Kennedy, and Eliz-
abeth F. Churchill. 2009. Tweet the debates:
Understanding community annotation of un-
collected sources. In In WSM ?09: Proceedings
of the international workshop on Workshop on So-
cial.
127
Qinfeng Shi, James Petterson, Gideon Dror,
John Langford, Alex Smola, and S.V.N. Vish-
wanathan. 2009. Hash kernels for structured
data. J. Mach. Learn. Res., 10:2615?2637.
SMHasher. 2010. The murmurhash family of
hash functions.
Michael Speriosu, Nikita Sudan, Sid Upadhyay,
and Jason Baldridge. 2011. Twitter polarity
classification with label propagation over lexi-
cal links and the follower graph. In Proceedings
of the First Workshop on Unsupervised Learning in
NLP, pages 53?63, Stroudsburg, PA, USA.
Mike Thelwall, Kevan Buckley, Georgios Pal-
toglou, Di Cai, and Arvid Kappas. 2010. Senti-
ment in short strength detection informal text.
J. Am. Soc. Inf. Sci. Technol., 61(12):2544?2558,
December.
Paul Viola and Michael Jones. 2001. Robust real-
time object detection. In International Journal of
Computer Vision.
Kilian Q. Weinberger, Anirban Dasgupta, John
Langford, Alexander J. Smola, and Josh Atten-
berg. 2009. Feature hashing for large scale mul-
titask learning. In Andrea Pohoreckyj Dany-
luk, L Bottou, and Michael L. Littman, editors,
ICML, volume 382 of ACM International Confer-
ence Proceeding Series, page 140. ACM.
Jun-Ming Xu, Kwang-Sung Jun, Xiaojin Zhu, and
Amy Bellmore. 2012. Learning from bullying
traces in social media. In HLT-NAACL, pages
656?666.
128
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 129?134,
Dublin, Ireland, August 23-24, 2014.
Biocom Usp: Tweet Sentiment Analysis with Adaptive Boosting
Ensemble
Na?dia F. F. Silva, Eduardo R. Hruschka
University of S?ao Paulo, USP
S?ao Carlos, SP, Brazil
nadia, erh@icmc.usp.br
Estevam Rafael Hruschka Jr.
Department of Computer Science
Federal University of Sao Carlos.
S?ao Carlos, SP, Brazil
estevam@dc.ufscar.br
Abstract
We describe our approach for the
SemEval-2014 task 9: Sentiment Analy-
sis in Twitter. We make use of an en-
semble learning method for sentiment
classification of tweets that relies on
varied features such as feature hash-
ing, part-of-speech, and lexical fea-
tures. Our system was evaluated in
the Twitter message-level task.
1 Introduction
The sentiment analysis is a field of study that
investigates feelings present in texts. This
field of study has become important, espe-
cially due to the internet growth, the content
generated by its users, and the emergence of
the social networks. In the social networks
such as Twitter people post their opinions in a
colloquial and compact language, and it is be-
coming a large dataset, which can be used as
a source of information for various automatic
tools of sentiment inference. There is an enor-
mous interest in sentiment analysis of Twit-
ter messages, known as tweets, with applica-
tions in several segments, such as (i) directing
marketing campaigns, extracting consumer re-
views of services and products (Jansen et al.,
2009); (ii) identifying manifestations of bully-
ing (Xu et al., 2012); (iii) predicting to fore-
cast box-office revenues for movies (Asur and
Huberman, 2010); and (iv) predicting accep-
tance or rejection of presidential candidates
(Diakopoulos and Shamma, 2010; O?Connor
et al., 2010).
This work is licensed under a Creative
Commons Attribution 4.0 International Li-
cence. Page numbers and proceedings footer
are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
One of the problems encountered by re-
searchers in tweet sentiment analysis is the
scarcity of public datasets. Although Twit-
ter sentiment datasets have already been cre-
ated, they are either small ? such as Obama-
McCain Debate corpus (Shamma et al., 2009)
and Health Care Reform corpus (Speriosu et
al., 2011) or big and proprietary such as in
(Lin and Kolcz, 2012). Others rely on noisy
labels obtained from emoticons and hashtags
(Go et al., 2009). The SemEval-2014 task 9: Sen-
timent Analysis in Twitter (Nakov et al., 2013)
provides a public dataset to be used to com-
pare the accuracy of different approaches.
In this paper, we propose to analyse tweet
sentiment with the use of Adaptive Boost-
ing (Freund and Schapire, 1997), making
use of the well-known Multinomial Classi-
fier. Boosting is an approach to machine
learning that is based on the idea of creat-
ing a highly accurate prediction rule by com-
bining many relatively weak and inaccurate
rules. The AdaBoost algorithm (Freund and
Schapire, 1997) was the first practical boost-
ing algorithm, and remains one of the most
widely used and studied, with applications in
numerous fields. Therefore, it has potential to
be very useful for tweet sentiment analysis, as
we address in this paper.
2 Related Work
Classifier ensembles for tweet sentiment anal-
ysis have been underexplored in the literature
? a few exceptions are (Lin and Kolcz, 2012;
Clark and Wicentwoski, 2013; Rodriguez et
al., 2013; Hassan et al., 2013).
Lin and Kolcz (2012) used logistic regres-
sion classifiers learned from hashed byte 4-
grams as features ? The feature extractor con-
siders the tweet as a raw byte array. It moves
a four-byte sliding window along the array,
129
and hashes the contents of the bytes, the value
of which was taken as the feature id. Here the
4-grams refers to four characters (and not to
four words). They made no attempt to per-
form any linguistic processing, not even word
tokenization. For each of the (proprietary)
datasets, they experimented with ensembles
of different sizes. The ensembles were formed
by different models, obtained from different
training sets, but with the same learning algo-
rithm (logistic regression). Their results show
that the ensembles lead to more accurate clas-
sifiers.
Rodr??gues et al. (2013) and Clark et al.
(2013) proposed the use of classifier ensem-
bles at the expression-level, which is related
to Contextual Polarity Disambiguation. In this
perspective, the sentiment label (positive,
negative, or neutral) is applied to a specific
phrase or word within the tweet and does not
necessarily match the sentiment of the entire
tweet.
Finally, another type of ensemble frame-
work has been recently proposed by Hassan
et al. (2013), who deal with class imbalance,
sparsity, and representational issues. The au-
thors propose to enrich the corpus using mul-
tiple additional datasets related to the task of
sentiment classification. Differently from pre-
vious works, the authors use a combination of
unigrams and bigrams of simple words, part-
of-speech, and semantic features.
None of the previous works used AdaBoost
(Freund and Schapire, 1996). Also, lexicons
and/or part-of-speech in combination with
feature hashing, like in (Lin and Kolcz, 2012)
have not been addressed in the literature.
3 AdaBoost Ensemble
Boosting is a relatively young, yet extremely
powerful, machine learning technique. The
main idea behind boosting algorithms is to
combine multiple weak learners ? classifi-
cation algorithms that perform only slightly
better than random guessing ? into a power-
ful composite classifier. Our focus is on the
well known AdaBoost algorithm (Freund and
Schapire, 1997) based on Multinomial Naive
Bayes as base classifiers (Figure 1).
AdaBoost and its variants have been ap-
plied to diverse domains with great success,
owing to their solid theoretical foundation,
accurate prediction, and great simplicity (Fre-
und and Schapire, 1997). For example, Viola
and Jones (2001) used AdaBoost to face de-
tection, Hao and Luo (2006) dealt with im-
age segmentation, recognition of handwritten
digits, and outdoor scene classification prob-
lems. In (Bloehdorn and Hotho, 2004) text
classification is explored.
Figure 1: AdaBoost Approach
4 Feature Engineering
The most commonly used text representation
method adopted in the literature is known as
Bag of Words (BOW) technique, where a doc-
ument is considered as a BOW, and is repre-
sented by a feature vector containing all the
words appearing in the corpus. In spite of
BOW being simple and very effective in text
classification, a large amount of information
from the original document is not considered,
word order is ruptured, and syntactic struc-
tures are broken. Therefore, sophisticated fea-
ture extraction methods with a deeper under-
standing of the documents are required for
sentiment classification tasks. Instead of us-
ing only BOW, alternative ways to represent
text, including Part of Speech (PoS) based fea-
tures, feature hashing, and lexicons have been
addressed in the literature.
We implemented an ensemble of classifiers
that receive as input data a combination of
three features sets: i) lexicon features that cap-
tures the semantic aspect of a tweet; ii) fea-
ture hashing that captures the surface-form as
abbreviations, slang terms from this type of
social network, elongated words (for exam-
ple, loveeeee), sentences with words without
a space between them (for instance, Ilovveap-
ple!), and so on; iii) and a specific syntactic fea-
tures for tweets. Technical details of each fea-
ture set are provided in the sequel.
Lexicon Features
We use the sentimental lexicon provided by
(Thelwall et al., 2010) and (Hu and Liu, 2004).
The former is known as SentiStrength and
130
provides: an emotion vocabulary, an emoti-
cons list (with positive, negative, and neutral
icons), a negation list, and a booster word list.
We use the negative list in cases where the
next term in a sentence is an opinion word
(either positive or negative). In such cases
we have polarity inversion. For example, in
the sentence ?The house is not beautiful?, the
negative word ?not? invert the polarity of the
opinion word beautiful. The booster word list
is composed by adverbs that suggest more or
less emphasis in the sentiment. For exam-
ple, in the sentence ?He was incredibly rude.?
the term ?incredibly? is an adverb that lay em-
phasis on the opinion word ?rude?. Besides
using SentiStrength, we use the lexicon ap-
proach proposed by (Hu and Liu, 2004). In
their approach, a list of words and associa-
tions with positive and negative sentiments
has been provided that are very useful for
sentiment analysis.
These two lexicons were used to build the
first feature set according to Table 1, where it
is presented an example of tweet representa-
tion for the tweet
1
: ?The soccer team didn?t
play extremely bad last Wednesday.? The
word ?bad? exists in the lexicon list of (Hu
and Liu, 2004), and it is a negative word.
The word ?bad? also exists in the negation
list provided by (Thelwall et al., 2010). The
term ?didn?t? is a negative word according to
SentiStrength (Thelwall et al., 2010) and there
is a polarity inversion of the opinion words
ahead. Finally, the term ?extremely? belongs
the booster word list and this word suggests
more emphasis to the opinion words existing
ahead.
positive negative neutral class
tweet
1
3 0 0 positive
Table 1: Representing Twitter messages with
lexicons.
Feature hashing
Feature hashing has been introduced for text
classification in (Shi et al., 2009), (Wein-
berger et al., 2009), (Forman and Kirshen-
baum, 2008), (Langford et al., 2007), (Caragea
et al., 2011). In the context of tweet classi-
fication, feature hashing offers an approach
to reducing the number of features provided
as input to a learning algorithm. The origi-
nal high-dimensional space is ?reduced? by
hashing the features into a lower-dimensional
space, i.e., mapping features to hash keys.
Thus, multiple features can be mapped to the
same hash key, thereby ?aggregating? their
counts.
We used the MurmurHash3 function
(SMHasher, 2010), that is a non-cryptographic
hash function suitable for general hash-based
lookup tables. It has been used for many
purposes, and a recent approach that has
emerged is its use for feature hashing or
hashing trick. Instead of building and storing
an explicit traditional bag-of-words with
n-grams, the feature hashing uses a hash
function to reduce the dimensionality of the
output space and the length of this space
(features) is explicitly fixed in advance. For
this paper, we used this code (in Python):
Code Listing 1: Murmurhash:
from sklearn.utils.murmurhash
import murmurhash3_bytes_u32
for w in "i loveee apple".split():
print("{0} => {1}".format(
w,murmurhash3_bytes_u32(w,0)%2
**
10))
The dimensionality is 2 ? ?10, i.e 2
10
fea-
tures. In this code the output is a hash code
for each word ?w? in the phrase ?i loveee
apple?, i.e. i => 43, loveee => 381 and
apple => 144. Table 2 shows an example of
feature hashing representation.
1 2 3 4 ? ? ? 1024 class
tweet
1
0 0 1 1 ? ? ? 0 positive
tweet
2
0 1 0 3 ? ? ? 0 negative
tweet
3
2 0 0 0 ? ? ? 0 positive
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. ? ? ?
.
.
.
.
.
.
tweet
n
0 0 2 1 ? ? ? 0 neutral
Table 2: Representing Twitter messages with
feature hashing.
Specific syntactic (PoS) features
We used the Part of Speech (PoS) tagged for
tweets with the Twitter NLP tool (Gimpel et
al., 2011). It encompasses 25 tags including
Nominal, Nominal plus Verbal, Other open-
class words like adjectives, adverbs and in-
terjection, Twitter specific tags such as hash-
tags, mention, discourse marker, just to name
131
a few. Table 3 shows an example of syntactic
features representation.
tag
1
tag
2
tag
3
tag
4
? ? ? tag
25
class
tweet
1
0 0 3 1 ? ? ? 0 positive
tweet
2
0 2 0 1 ? ? ? 0 negative
tweet
3
1 0 0 0 ? ? ? 0 positive
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. ? ? ?
.
.
.
.
.
.
tweet
n
0 0 1 1 ? ? ? 0 neutral
Table 3: Representing Twitter messages with
syntactic features.
A combination of lexicons, feature hashing,
and part-of-speech is used to train the ensem-
ble classifiers, thereby resulting in 1024 fea-
tures from feature hashing, 3 features from
lexicons, and 25 features from PoS.
5 Experimental Setup and Results
We conducted experiments by using the
WEKA platform
1
. Table 4 shows the class dis-
tributions in training, development, and test-
ing sets. Table 5 presents the results for posi-
tive and negative classes with the classifiers
used in training set, and Table 6 shows the
computed results by SemEval organizers in
the test sets.
Training Set
Set Positive Negative Neutral Total
Train 3,640 (37%) 1,458 (15%) 4,586 (48%) 9,684
Development Set
Set Positive Negative Neutral Total
Dev 575 (35%) 340(20%) 739 (45%) 1,654
Testing Sets
Set Positive Negative Neutral Total
LiveJournal 427 (37%) 304 (27%) 411 (36%) 1,142
SMS2013 492 (23%) 394(19%) 1,207 (58%) 2,093
Twitter2013 1,572 (41%) 601 (16%) 1,640 (43%) 3,813
Twitter2014 982 (53%) 202 (11%) 669 (36%) 1,853
Twitter2014Sar 33 (38%) 40 (47%) 13 (15%) 86
Table 4: Class distributions in the training set
(Train), development set (Dev) and testing set
(Test).
6 Concluding Remarks
From our results, we conclude that the use of
AdaBoost provides good performance in the
sentiment analysis (message-level subtask).
In the cross-validation process, Multinomial
Naive Bayes (MNB) has shown better results
than Support Vector Machines (SVM) as a
component for AdaBoost. However, we feel
1
http://www.cs.waikato.ac.nz/ml/weka/
Set Algorithm F-Measure
Positive
F-Measure
Negative
Average
Train MNB 63.40 49.40 56.40
Train SVM 64.00 44.50 54.20
Train AdaBoost w/ SVM 62.50 44.50 53.50
Train AdaBoost w/ MNB 65.10 49.60 57.35
Table 5: Results from 10-fold cross validation
in the training set with default parameters of
Weka. MNB and SVM stand for Multinomial
Naive Bayes and Support Vector Machine, re-
spectively.
Scoring LiveJournal2014
class precision recall F-measure
positive 69.79 64.92 67.27
negative 76.64 61.64 68.33
neutral 51.82 69.84 59.50
overall score : 67.80
Scoring SMS2013
positive 61.99 46.78 53.32
negative 72.34 42.86 53.82
neutral 53.85 83.76 65.56
overall score : 53.57
Scoring Twitter2013
positive 68.07 66.13 67.08
negative 48.09 50.00 49.02
neutral 67.20 68.15 67.67
overall score : 58.05
Scoring Twitter2014
positive 65.17 70.48 67.72
negative 53.47 48.21 50.70
neutral 59.94 55.62 57.70
overall score : 59.21
Scoring Twitter2014Sarcasm
positive 63.64 44.68 52.50
negative 22.50 75.00, 34.62
neutral 76.92 37.04 50.00
overall score : 43.56
Table 6: Results in the test sets ? AdaBoost
plus Multinomial Naive Bayes, which was the
best algorithm in cross validation.
that further investigations are necessary be-
fore making strong claims about this result.
Overall, the SemEval Tasks have make evi-
dent the usual challenges when mining opin-
ions from Social Media channels: noisy text,
irregular grammar and orthography, highly
specific lingo, and others. Moreover, tempo-
ral dependencies can affect the performance if
the training and test data have been gathered
at different.
Acknowledgements
The authors would like to thank the Re-
search Agencies CAPES, FAPESP, and CNPq
for their financial support.
References
Sitaram Asur and Bernardo A. Huberman. 2010.
Predicting the future with social media. In Pro-
ceedings of the 2010 International Conference on
132
Web Intelligence and Intelligent Agent Technology
- Volume 01, WI-IAT ?10, pages 492?499, Wash-
ington, DC, USA. IEEE Computer Society.
Stephan Bloehdorn and Andreas Hotho. 2004.
Text classification by boosting weak learners
based on terms and concepts. In Proceedings of
the Fourth IEEE International Conference on Data
Mining, pages 331?334. IEEE Computer Society
Press, November.
Cornelia Caragea, Adrian Silvescu, and Prasen-
jit Mitra. 2011. Protein sequence classifica-
tion using feature hashing. In Fang-Xiang Wu,
Mohammed Javeed Zaki, Shinichi Morishita,
Yi Pan, Stephen Wong, Anastasia Christianson,
and Xiaohua Hu, editors, BIBM, pages 538?543.
IEEE.
Sam Clark and Rich Wicentwoski. 2013. Swatcs:
Combining simple classifiers with estimated
accuracy. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages
425?429, Atlanta, Georgia, USA, June.
Nicholas A. Diakopoulos and David A. Shamma.
2010. Characterizing debate performance via
aggregated twitter sentiment. In Proceedings of
the SIGCHI Conference on Human Factors in Com-
puting Systems, CHI ?10, pages 1195?1198, New
York, NY, USA. ACM.
George Forman and Evan Kirshenbaum. 2008.
Extremely fast text feature extraction for clas-
sification and indexing. In CIKM ?08: Proceed-
ing of the 17th ACM conference on Information and
knowledge management, pages 1221?1230, New
York, NY, USA. ACM.
Yoav Freund and Robert E. Schapire. 1996. Ex-
periments with a new boosting algorithm. In
Thirteenth International Conference on Machine
Learning, pages 148?156, San Francisco. Morgan
Kaufmann.
Yoav Freund and Robert E Schapire. 1997.
A decision-theoretic generalization of on-line
learning and an application to boosting. Jour-
nal of Computer and System Sciences, 55(1):119 ?
139.
Kevin Gimpel, Nathan Schneider, Brendan
O?Connor, Dipanjan Das, Daniel Mills, Jacob
Eisenstein, Michael Heilman, Dani Yogatama,
Jeffrey Flanigan, and Noah A. Smith. 2011.
Part-of-speech tagging for twitter: Annotation,
features, and experiments. In Proceedings of
the 49th Annual Meeting of the Association for
Computational Linguistics ? Short Papers - Volume
2, HLT ?11, pages 42?47, Stroudsburg, PA, USA.
Alec Go, Richa Bhayani, and Lei Huang. 2009.
Twitter sentiment classification using distant
supervision. Processing, pages 1?6.
Wei Hao and Jiebo Luo. 2006. Generalized
Multiclass AdaBoost and Its Applications to
Multimedia Classification. In Computer Vision
and Pattern Recognition Workshop, 2006. CVPRW
&#039;06. Conference on, page 113, Washington,
DC, USA, June. IEEE.
Ammar Hassan, Ahmed Abbasi, and Daniel
Zeng. 2013. Twitter sentiment analysis: A
bootstrap ensemble framework. In SocialCom,
pages 357?364. IEEE.
Minqing Hu and Bing Liu. 2004. Mining and
summarizing customer reviews. In Proceed-
ings of the tenth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
KDD ?04, pages 168?177, New York, NY, USA.
ACM.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and
Abdur Chowdury. 2009. Twitter power:
Tweets as electronic word of mouth. J. Am. Soc.
Inf. Sci. Technol., 60(11):2169?2188, nov.
John Langford, Alex Strehl, and Lihong Li. 2007.
Vowpal wabbit online learning project. http:
//mloss.org/software/view/53/.
Jimmy Lin and Alek Kolcz. 2012. Large-scale ma-
chine learning at twitter. In Proceedings of the
2012 ACM SIGMOD International Conference on
Management of Data, SIGMOD ?12, pages 793?
804, New York, NY, USA. ACM.
Preslav Nakov, Sara Rosenthal, Zornitsa
Kozareva, Veselin Stoyanov, Alan Ritter,
and Theresa Wilson. 2013. Semeval-2013 task
2: Sentiment analysis in twitter. In Second
Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the
Seventh International Workshop on Semantic Eval-
uation (SemEval 2013), pages 312?320, Atlanta,
Georgia, USA, June.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In ICWSM?10, pages
1?1.
Penagos Carlos Rodriguez, Jordi Atserias, Joan
Codina-Filba, David Garc?a-Narbona, Jens
Grivolla, Patrik Lambert, and Roser Saur?.
2013. Fbm: Combining lexicon-based ml and
heuristics for social media polarities. In Pro-
ceedings of SemEval-2013 ? International Work-
shop on Semantic Evaluation Co-located with *Sem
and NAACL, Atlanta, Georgia. Url date at 2013-
10-10.
David A. Shamma, Lyndon Kennedy, and Eliz-
abeth F. Churchill. 2009. Tweet the debates:
Understanding community annotation of un-
collected sources. In In WSM ?09: Proceedings
of the international workshop on Workshop on So-
cial.
133
Qinfeng Shi, James Petterson, Gideon Dror,
John Langford, Alex Smola, and S.V.N. Vish-
wanathan. 2009. Hash kernels for structured
data. J. Mach. Learn. Res., 10:2615?2637.
SMHasher. 2010. The murmurhash family of
hash functions.
Michael Speriosu, Nikita Sudan, Sid Upadhyay,
and Jason Baldridge. 2011. Twitter polarity
classification with label propagation over lexi-
cal links and the follower graph. In Proceedings
of the First Workshop on Unsupervised Learning in
NLP, pages 53?63, Stroudsburg, PA, USA.
Mike Thelwall, Kevan Buckley, Georgios Pal-
toglou, Di Cai, and Arvid Kappas. 2010. Senti-
ment in short strength detection informal text.
J. Am. Soc. Inf. Sci. Technol., 61(12):2544?2558,
December.
Paul Viola and Michael Jones. 2001. Robust real-
time object detection. In International Journal of
Computer Vision.
Kilian Q. Weinberger, Anirban Dasgupta, John
Langford, Alexander J. Smola, and Josh Atten-
berg. 2009. Feature hashing for large scale mul-
titask learning. In Andrea Pohoreckyj Dany-
luk, L Bottou, and Michael L. Littman, editors,
ICML, volume 382 of ACM International Confer-
ence Proceeding Series, page 140. ACM.
Jun-Ming Xu, Kwang-Sung Jun, Xiaojin Zhu, and
Amy Bellmore. 2012. Learning from bullying
traces in social media. In HLT-NAACL, pages
656?666.
134
