Clustering Polysemic Subcategorization Frame Distributions Semantically
Anna Korhonen?
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
alk23@cl.cam.ac.uk
Yuval Krymolowski
Division of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
Scotland, UK
ykrymolo@inf.ed.ac.uk
Zvika Marx
Interdisciplinary Center
for Neural Computation,
The Hebrew University
Jerusalem, Israel
zvim@cs.huji.ac.il
Abstract
Previous research has demonstrated the
utility of clustering in inducing semantic
verb classes from undisambiguated cor-
pus data. We describe a new approach
which involves clustering subcategoriza-
tion frame (SCF) distributions using the
Information Bottleneck and nearest neigh-
bour methods. In contrast to previous
work, we particularly focus on cluster-
ing polysemic verbs. A novel evaluation
scheme is proposed which accounts for
the effect of polysemy on the clusters, of-
fering us a good insight into the potential
and limitations of semantically classifying
undisambiguated SCF data.
1 Introduction
Classifications which aim to capture the close rela-
tion between the syntax and semantics of verbs have
attracted a considerable research interest in both lin-
guistics and computational linguistics (e.g. (Jack-
endoff, 1990; Levin, 1993; Pinker, 1989; Dang et
al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).
While such classifications may not provide a means
for full semantic inferencing, they can capture gen-
eralizations over a range of linguistic properties, and
can therefore be used as a means of reducing redun-
dancy in the lexicon and for filling gaps in lexical
knowledge.
?This work was partly supported by UK EPSRC project
GR/N36462/93: ?Robust Accurate Statistical Parsing (RASP)?.
Verb classifications have, in fact, been used to
support many natural language processing (NLP)
tasks, such as language generation, machine transla-
tion (Dorr, 1997), document classification (Klavans
and Kan, 1998), word sense disambiguation (Dorr
and Jones, 1996) and subcategorization acquisition
(Korhonen, 2002).
One attractive property of these classifications is
that they make it possible, to a certain extent, to in-
fer the semantics of a verb on the basis of its syn-
tactic behaviour. In recent years several attempts
have been made to automatically induce semantic
verb classes from (mainly) syntactic information
in corpus data (Joanis, 2002; Merlo et al, 2002;
Schulte im Walde and Brew, 2002).
In this paper, we focus on the particular task
of classifying subcategorization frame (SCF) distri-
butions in a semantically motivated manner. Pre-
vious research has demonstrated that clustering
can be useful in inferring Levin-style semantic
classes (Levin, 1993) from both English and Ger-
man verb subcategorization information (Brew and
Schulte im Walde, 2002; Schulte im Walde, 2000;
Schulte im Walde and Brew, 2002).
We propose a novel approach, which involves: (i)
obtaining SCF frequency information from a lexi-
con extracted automatically using the comprehen-
sive system of Briscoe and Carroll (1997) and (ii)
applying a clustering mechanism to this informa-
tion. We use clustering methods that process raw
distributional data directly, avoiding complex pre-
processing steps required by many advanced meth-
ods (e.g. Brew and Schulte im Walde (2002)).
In contrast to earlier work, we give special empha-
sis to polysemy. Earlier work has largely ignored
this issue by assuming a single gold standard class
for each verb (whether polysemic or not). The rel-
atively good clustering results obtained suggest that
many polysemic verbs do have some predominating
sense in corpus data. However, this sense can vary
across corpora (Roland et al, 2000), and assuming a
single sense is inadequate for an important group of
medium and high frequency verbs whose distribu-
tion of senses in balanced corpus data is flat rather
than zipfian (Preiss and Korhonen, 2002).
To allow for sense variation, we introduce a new
evaluation scheme against a polysemic gold stan-
dard. This helps to explain the results and offers
a better insight into the potential and limitations of
clustering undisambiguated SCF data semantically.
We discuss our gold standards and the choice of
test verbs in section 2. Section 3 describes the
method for subcategorization acquisition and sec-
tion 4 presents the approach to clustering. Details
of the experimental evaluation are supplied in sec-
tion 5. Section 6 concludes with directions for future
work.
2 Semantic Verb Classes and Test Verbs
Levin?s taxonomy of verbs and their classes (Levin,
1993) is the largest syntactic-semantic verb classifi-
cation in English, employed widely in evaluation of
automatic classifications. It provides a classification
of 3,024 verbs (4,186 senses) into 48 broad / 192
fine grained classes. Although it is quite extensive,
it is not exhaustive. As it primarily concentrates on
verbs taking NP and PP complements and does not
provide a comprehensive set of senses for verbs, it
is not suitable for evaluation of polysemic classifi-
cations.
We employed as a gold standard a substan-
tially extended version of Levin?s classification
constructed by Korhonen (2003). This incorpo-
rates Levin?s classes, 26 additional classes by
Dorr (1997)1, and 57 new classes for verb types not
covered comprehensively by Levin or Dorr.
110 test verbs were chosen from this gold stan-
dard, 78 polysemic and 32 monosemous ones. Some
low frequency verbs were included to investigate the
1These classes are incorporated in the ?LCS database?
(http://www.umiacs.umd.edu/?bonnie/verbs-English.lcs).
effect of sparse data on clustering performance. To
ensure that our gold standard covers all (or most)
senses of these verbs, we looked into WordNet
(Miller, 1990) and assigned all the WordNet senses
of the verbs to gold standard classes.2
Two versions of the gold standard were created:
monosemous and polysemic. The monosemous one
lists only a single sense for each test verb, that cor-
responding to its predominant (most frequent) sense
in WordNet. The polysemic one provides a compre-
hensive list of senses for each verb. The test verbs
and their classes are shown in table 1. The classes
are indicated by number codes from the classifica-
tions of Levin, Dorr (the classes starting with 0) and
Korhonen (the classes starting with A).3 The pre-
dominant sense is indicated by bold font.
3 Subcategorization Information
We obtain our SCF data using the subcategorization
acquisition system of Briscoe and Carroll (1997).
We expect the use of this system to be benefi-
cial: it employs a robust statistical parser (Briscoe
and Carroll, 2002) which yields complete though
shallow parses, and a comprehensive SCF classifier,
which incorporates 163 SCF distinctions, a super-
set of those found in the ANLT (Boguraev et al,
1987) and COMLEX (Grishman et al, 1994) dictio-
naries. The SCFs abstract over specific lexically-
governed particles and prepositions and specific
predicate selectional preferences but include some
derived semi-predictable bounded dependency con-
structions, such as particle and dative movement.
78 of these ?coarse-grained? SCFs appeared in our
data. In addition, a set of 160 fine grained frames
were employed. These were obtained by parameter-
izing two high frequency SCFs for prepositions: the
simple PP and NP + PP frames. The scope was re-
stricted to these two frames to prevent sparse data
problems in clustering.
A SCF lexicon was acquired using this system
from the British National Corpus (Leech, 1992,
BNC) so that the maximum of 7000 citations were
2As WordNet incorporates particularly fine grained sense
distinctions, some senses were found which did not appear in
our gold standard. As many of them appeared marginal and/or
low in frequency, we did not consider these additional senses in
our experiment.
3The gold standard assumes Levin?s broad classes (e.g. class
10) instead of possible fine-grained ones (e.g. class 10.1).
TEST GOLD STANDARD TEST GOLD STANDARD TEST GOLD STANDARD TEST GOLD STANDARD
VERB CLASSES VERB CLASSES VERB CLASSES VERB CLASSES
place 9 dye 24, 21, 41 focus 31, 45 stare 30
lay 9 build 26, 45 force 002, 11 glow 43
drop 9, 45, 004, 47, bake 26, 45 persuade 002 sparkle 43
51, A54, A30
pour 9, 43, 26, 57, 13, 31 invent 26, 27 urge 002, 37 dry 45
load 9 publish 26, 25 want 002, 005, 29, 32 shut 45
settle 9, 46, A16, 36, 55 cause 27, 002 need 002, 005, 29, 32 hang 47, 9, 42, 40
fill 9, 45, 47 generate 27, 13, 26 grasp 30, 15 sit 47, 9
remove 10, 11, 42 induce 27, 002, 26 understand 30 disappear 48
withdraw 10, A30 acknowledge 29, A25, A35 conceive 30, 29, A56 vanish 48
wipe 10, 9 proclaim 29, 37, A25 consider 30, 29 march 51
brush 10, 9, 41, 18 remember 29, 30 perceive 30 walk 51
filter 10 imagine 29, 30 analyse 34, 35 travel 51
send 11, A55 specify 29 evaluate 34, 35 hurry 53, 51
ship 11, A58 establish 29, A56 explore 35, 34 rush 53, 51
transport 11, 31 suppose 29, 37 investigate 35, 34 begin 55
carry 11, 54 assume 29, A35, A57 agree 36, 22, A42 continue 55, 47, 51
drag 11, 35, 51, 002 think 29, 005 communicate 36, 11 snow 57, 002
push 11, 12, 23, 9, 002 confirm 29 shout 37 rain 57
pull 11, 12, 13, 23, 40, 016 believe 29, 31, 33 whisper 37 sin 003
give 13 admit 29, 024, 045, 37 talk 37 rebel 003
lend 13 allow 29, 024, 13, 002 speak 37 risk 008, A7
study 14, 30, 34, 35 act 29 say 37, 002 gamble 008, 009
hit 18, 17, 47, A56, 31, 42 behave 29 mention 37 beg 015, 32
bang 18, 43, 9, 47, 36 feel 30, 31, 35, 29 eat 39 pray 015, 32
carve 21, 25, 26 see 30, 29 drink 39 seem 020
add 22, 37, A56 hear 30, A32 laugh 40, 37 appear 020, 48, 29
mix 22, 26, 36 notice 30, A32 smile 40, 37
colour 24, 31, 45 concentrate 31, 45 look 30, 35
Table 1: Test verbs and their monosemous/polysemic gold standard senses
used per test verb. The lexicon was evaluated against
manually analysed corpus data after an empirically
defined threshold of 0.025 was set on relative fre-
quencies of SCFs to remove noisy SCFs. The method
yielded 71.8% precision and 34.5% recall. When we
removed the filtering threshold, and evaluated the
noisy distribution, F-measure4 dropped from 44.9 to
38.51.5
4 Clustering Method
Data clustering is a process which aims to partition a
given set into subsets (clusters) of elements that are
similar to one another, while ensuring that elements
that are not similar are assigned to different clusters.
We use clustering for partitioning a set of verbs. Our
hypothesis is that information about SCFs and their
associated frequencies is relevant for identifying se-
mantically related verbs. Hence, we use SCFs as rel-
evance features to guide the clustering process.6
4F = 2?precision?recallprecision+recall
5These figures are not particularly impressive because our
evaluation is exceptionally hard. We use 1) highly polysemic
test verbs, 2) a high number of SCFs and 3) evaluate against
manually analysed data rather than dictionaries (the latter have
high precision but low recall).
6The relevance of the features to the task is evident when
comparing the probability of a randomly chosen pair of verbs
verbi and verbj to share the same predominant sense (4.5%)
with the probability obtained when verbj is the JS-divergence
We chose two clustering methods which do not in-
volve task-oriented tuning (such as pre-fixed thresh-
olds or restricted cluster sizes) and which approach
data straightforwardly, in its distributional form: (i)
a simple hard method that collects the nearest neigh-
bours (NN) of each verb (figure 1), and (ii) the In-
formation Bottleneck (IB), an iterative soft method
(Tishby et al, 1999) based on information-theoretic
grounds.
The NN method is very simple, but it has some
disadvantages. It outputs only one clustering config-
uration, and therefore does not allow examination
of different cluster granularities. It is also highly
sensitive to noise. Few exceptional neighbourhood
relations contradicting the typical trends in the data
are enough to cause the formation of a single cluster
which encompasses all elements.
Therefore we employed the more sophisticated
IB method as well. The IB quantifies the rele-
vance information of a SCF distribution with re-
spect to output clusters, through their mutual infor-
mation I(Clusters; SCFs). The relevance informa-
tion is maximized, while the compression informa-
tion I(Clusters;V erbs) is minimized. This en-
sures optimal compression of data through clusters.
The tradeoff between the two constraints is realized
nearest neighbour of verbi (36%).
NN Clustering:
1. For each verb v:
2. Calculate the JS divergence between the SCF
distributions of v and all other verbs:
JS(p, q) = 12
[
D
(
p
?
?
?
p+q
2
)
+ D
(
q
?
?
?
p+q
2
)]
3. Connect v with the most similar verb;
4. Find all the connected components
Figure 1: Connected components nearest neighbour (NN)
clustering. D is the Kullback-Leibler distance.
through minimizing the cost term:
L = I(Clusters;V erbs)? ?I(Clusters; SCFs) ,
where ? is a parameter that balances the constraints.
The IB iterative algorithm finds a local minimum
of the above cost term. It takes three inputs: (i) SCF-
verb distributions, (ii) the desired number of clusters
K, and (iii) the value of ?.
Starting from a random configuration, the algo-
rithm repeatedly calculates, for each cluster K, verb
V and SCF S, the following probabilities: (i) the
marginal proportion of the cluster p(K); (ii) the
probability p(S|K) for a SCF to occur with mem-
bers of the cluster; and (iii) the probability p(K|V )
for a verb to be assigned to the cluster. These prob-
abilities are used, each in its turn, for calculating the
other probabilities (figure 2). The collection of all
p(S|K)?s for a fixed cluster K can be regarded as a
probabilistic center (centroid) of that cluster in the
SCF space.
The IB method gives an indication of the
most informative values of K.7 Intensifying the
weight ? attached to the relevance information
I(Clusters; SCFs) allows us to increase the num-
ber K of distinct clusters being produced (while too
small ? would cause some of the output clusters to
be identical to one another). Hence, the relevance in-
formation grows with K. Accordingly, we consider
as the most informative output configurations those
for which the relevance information increases more
sharply between K? 1 and K clusters than between
K and K + 1.
7Most works on clustering ignore this issue and refer to an
arbitrarily chosen number of clusters, or to the number of gold
standard classes, which cannot be assumed in realistic applica-
tions.
IB Clustering (fixed ?):
Perform till convergence, for each time step
t = 1, 2, . . . :
1. zt(K,V ) = pt?1(K) e??D[p(S|V )?pt?1(S|K)]
(When t = 1, initialize zt(K,V ) arbitrarily)
2. pt(K|V ) = zt(K,V )?
K? zt(K
?,V )
3. pt(K) =
?
V p(V )pt(K|V )
4. pt(S|K) =
?
V p(S|V )pt(V |K)
Figure 2: Information Bottleneck (IB) iterative clustering. D
is the Kullback-Leibler distance.
When the weight of relevance grows, the assign-
ment to clusters is more constrained and p(K|V ) be-
comes more similar to hard clustering. Let
K(V ) = argmax
K
p(K|V )
denote the most probable cluster of a verb V .
For K ? 30, more than 85% of the verbs have
p(K(V )|V ) > 90% which makes the output cluster-
ing approximately hard. For this reason, we decided
to use only K(V ) as output and defer a further ex-
ploration of the soft output to future work.
5 Experimental Evaluation
5.1 Data
The input data to clustering was obtained from the
automatically acquired SCF lexicon for our 110 test
verbs (section 2). The counts were extracted from
unfiltered (noisy) SCF distributions in this lexicon.8
The NN algorithm produced 24 clusters on this in-
put. From the IB algorithm, we requested K = 2
to 60 clusters. The upper limit was chosen so as
to slightly exceed the case when the average clus-
ter size 110/K = 2. We chose for evaluation the
IB results for K = 25, 35 and 42. For these val-
ues, the SCF relevance satisfies our criterion for a
notable improvement in cluster quality (section 4).
The value K=35 is very close to the actual number
(34) of predominant senses in the gold standard. In
this way, the IB yields structural information beyond
clustering.
8This yielded better results, which might indicate that the
unfiltered ?noisy? SCFs contain information which is valuable
for the task.
5.2 Method
A number of different strategies have been proposed
for evaluation of clustering. We concentrate here on
those which deliver a numerical value which is easy
to interpret, and do not introduce biases towards spe-
cific numbers of classes or class sizes. As we cur-
rently assign a single sense to each polysemic verb
(sec. 5.4) the measures we use are also applicable
for evaluation against a polysemous gold standard.
Our first measure, the adjusted pairwise preci-
sion (APP), evaluates clusters in terms of verb pairs
(Schulte im Walde and Brew, 2002) 9:
APP = 1K
K?
i=1
num. of correct pairs in ki
num. of pairs in ki ?
|ki|?1
|ki|+1
.
APP is the average proportion of all within-cluster
pairs that are correctly co-assigned. It is multiplied
by a factor that increases with cluster size. This fac-
tor compensates for a bias towards small clusters.
Our second measure is derived from purity, a
global measure which evaluates the mean precision
of the clusters, weighted according to the cluster size
(Stevenson and Joanis, 2003). We associate with
each cluster its most prevalent semantic class, and
denote the number of verbs in a cluster K that take
its prevalent class by nprevalent(K). Verbs that do
not take this class are considered as errors. Given
our task, we are only interested in classes which con-
tain two or more verbs. We therefore disregard those
clusters where nprevalent(K) = 1. This leads us to
define modified purity:
mPUR =
?
nprevalent(ki)?2
nprevalent(ki)
number of verbs .
The modification we introduce to purity removes the
bias towards the trivial configuration comprised of
only singletons.
5.3 Evaluation Against the Predominant Sense
We first evaluated the clusters against the predom-
inant sense, i.e. using the monosemous gold stan-
dard. The results, shown in Table 2, demonstrate
that both clustering methods perform significantly
9Our definition differs by a factor of 2 from that of
Schulte im Walde and Brew (2002).
Alg. K +PP ?PP +PP ?PP
APP: mPUR:
NN (24) 21% 19% 48% 45%
25 12% 9% 39% 32%
IB 35 14% 9% 48% 38%
42 15% 9% 50% 39%
RAND 25 3% 15%
Table 2: Clustering performance on the predominant senses,
with and without prepositions. The last entry presents the per-
formance of random clustering with K = 25, which yielded the
best results among the three values K=25, 35 and 42.
better on the task than our random clustering base-
line. Both methods show clearly better performance
with fine-grained SCFs (with prepositions, +PP) than
with coarse-grained ones (-PP).
Surprisingly, the simple NN method performs
very similarly to the more sophisticated IB. Being
based on pairwise similarities, it shows better per-
formance than IB on the pairwise measure. The IB
is, however, slightly better according to the global
measure (2% with K = 42). The fact that the NN
method performs better than the IB with similar K
values (NN K = 24 vs. IB K = 25) seems to suggest
that the JS divergence provides a better model for
the predominant class than the compression model
of the IB. However, it is likely that the IB perfor-
mance suffered due to our choice of test data. As the
method is global, it performs better when the target
classes are represented by a high number of verbs.
In our experiment, many semantic classes were rep-
resented by two verbs only (section 2).
Nevertheless, the IB method has the clear advan-
tage that it allows for more clusters to be produced.
At best it classified half of the verbs correctly ac-
cording to their predominant sense (mPUR = 50%).
Although this leaves room for improvement, the re-
sult compares favourably to previously published re-
sults10. We argue, however, that evaluation against a
monosemous gold standard reveals only part of the
picture.
10Due to differences in task definition and experimental
setup, a direct comparison with earlier results is impossible.
For example, Stevenson and Joanis (2003) report an accuracy
of 29% (which implies mPUR ? 29%), but their task involves
classifying 841 verbs to 14 classes based on differences in the
predicate-argument structure.
K Pred. Multiple Pred. Multiple
sense senses sense senses
APP: mPUR:
NN:
(24) 21% 29% (23% + 5?) 48% 60% (46%+ 2?)
IB:
25 12% 18% (14% + 5?) 39% 48% (43%+ 3?)
35 14% 20% (16% + 6?) 47% 59% (50%+ 4?)
42 15% 19% (16% + 3?) 50% 59% (54%+ 2?)
Table 3: Evaluation against the monosemous (Pred.) and pol-
ysemous (Multiple) gold standards. The figures in parentheses
are results of evaluation on randomly polysemous data + sig-
nificance of the actual figure. Results were obtained with fine-
grained SCFs (including prepositions).
5.4 Evaluation Against Multiple Senses
In evaluation against the polysemic gold standard,
we assume that a verb which is polysemous in our
corpus data may appear in a cluster with verbs that
share any of its senses. In order to evaluate the clus-
ters against polysemous data, we assigned each pol-
ysemic verb V a single sense: the one it shares with
the highest number of verbs in the cluster K(V ).
Table 3 shows the results against polysemic and
monosemous gold standards. The former are notice-
ably better than the latter (e.g. IB with K = 42 is 9%
better). Clearly, allowing for multiple gold standard
classes makes it easier to obtain better results with
evaluation.
In order to show that polysemy makes a non-
trivial contribution in shaping the clusters, we mea-
sured the improvement that can be due to pure
chance by creating randomly polysemous gold stan-
dards. We constructed 100 sets of random gold stan-
dards. In each iteration, the verbs kept their original
predominant senses, but the set of additional senses
was taken entirely from another verb - chosen at ran-
dom. By doing so, we preserved the dominant sense
of each verb, the total frequency of all senses and the
correlations between the additional senses.
The results included in table 3 indicate, with
99.5% confidence (3? and above), that the improve-
ment obtained with the polysemous gold standard is
not artificial (except in two cases with 95% confi-
dence).
5.5 Qualitative Analysis of Polysemy
We performed qualitative analysis to further inves-
tigate the effect of polysemy on clustering perfor-
Different Pairs Fraction
Senses in cluster
0 39 51%
1 85 10%
2 625 7%
3 1284 3%
4 1437 3%
Table 4: The fraction of verb pairs clustered together, as a
function of the number of different senses between pair mem-
bers (results of the NN algorithm)
Common one irregular no irregular
Senses Pairs in cluster Pairs in cluster
0 2180 3% 3018 3%
1 388 9% 331 12%
2 44 20% 31 35%
Table 5: The fraction of verb pairs clustered together, as a
function of the number of shared senses (results of the NN algo-
rithm)
mance. The results in table 4 demonstrate that the
more two verbs differ in their senses, the lower their
chance of ending up in the same cluster. From the
figures in table 5 we see that the probability of two
verbs to appear in the same cluster increases with
the number of senses they share. Interestingly, it is
not only the degree of polysemy which influences
the results, but also the type. For verb pairs where at
least one of the members displays ?irregular? poly-
semy (i.e. it does not share its full set of senses with
any other verb), the probability of co-occurrence in
the same cluster is far lower than for verbs which are
polysemic in a ?regular? manner (Table 5).
Manual cluster analysis against the polysemic
gold standard revealed a yet more comprehensive
picture. Consider the following clusters (the IB out-
put with K = 42):
A1: talk (37), speak (37)
A2: look (30, 35), stare (30)
A3: focus (31, 45), concentrate (31, 45)
A4: add (22, 37, A56)
We identified a close relation between the clus-
tering performance and the following patterns of se-
mantic behaviour:
1) Monosemy: We had 32 monosemous test
verbs. 10 gold standard classes included 2 or more
or these. 7 classes were correctly acquired us-
ing clustering (e.g. A1), indicating that clustering
monosemous verbs is fairly ?easy?.
2) Predominant sense: 10 clusters were exam-
ined by hand whose members got correctly classi-
fied together, despite one of them being polysemous
(e.g. A2). In 8 cases there was a clear indication in
the data (when examining SCFs and the selectional
preferences on argument heads) that the polysemous
verb indeed had its predominant sense in the rele-
vant class and that the co-occurrence was not due to
noise.
3) Regular Polysemy: Several clusters were pro-
duced which represent linguistically plausible inter-
sective classes (e.g. A3) (Dang et al, 1998) rather
than single classes.
4) Irregular Polysemy: Verbs with irregular pol-
ysemy11 were frequently assigned to singleton clus-
ters. For example, add (A4) has a ?combining and
attaching? sense in class 22 which involves NP and
PP SCFs and another ?communication? sense in 37
which takes sentential SCFs. Irregular polysemy was
not a marginal phenomenon: it explains 5 of the 10
singletons in our data.
These observations confirm that evaluation
against a polysemic gold standard is necessary in
order to fully explain the results from clustering.
5.6 Qualitative Analysis of Errors
Finally, to provide feedback for further development
of our verb classification approach, we performed a
qualitative analysis of errors not resulting from poly-
semy. Consider the following clusters (the IB output
for K = 42):
B1: place (9), build (26, 45),
publish (26, 25), carve (21, 25, 26)
B2: sin (003), rain (57), snow (57, 002)
B3: agree (36, 22, A42), appear (020, 48, 29),
begin (55), continue (55, 47, 51)
B4: beg (015, 32)
Three main error types were identified:
1) Syntactic idiosyncracy: This was the most fre-
quent error type, exemplified in B1, where place is
incorrectly clustered with build, publish and carve
merely because it takes similar prepositions to these
verbs (e.g. in, on, into).
2) Sparse data: Many of the low frequency verbs
(we had 12 with frequency less than 300) performed
11Recall our definition of irregular polysemy, section 5.4.
poorly. In B2, sin (which had 53 occurrences) is
classified with rain and snow because it does not
occur in our data with the preposition against -
the ?hallmark? of its gold standard class (?Conspire
Verbs?).
3) Problems in SCF acquisition: These were not
numerous but occurred e.g. when the system could
not distinguish between different control (e.g. sub-
ject/object equi/raising) constructions (B3).
6 Discussion and Conclusions
This paper has presented a novel approach to auto-
matic semantic classification of verbs. This involved
applying the NN and IB methods to cluster polysemic
SCF distributions extracted from corpus data using
Briscoe and Carroll?s (1997) system. A principled
evaluation scheme was introduced which enabled us
to investigate the effect of polysemy on the resulting
classification.
Our investigation revealed that polysemy has a
considerable impact on the clusters formed: pol-
ysemic verbs with a clear predominant sense and
those with similar regular polysemy are frequently
classified together. Homonymic verbs or verbs with
strong irregular polysemy tend to resist any classifi-
cation.
While it is clear that evaluation should account
for these cases rather than ignore them, the issue of
polysemy is related to another, bigger issue: the po-
tential and limitations of clustering in inducing se-
mantic information from polysemic SCF data. Our
results show that it is unrealistic to expect that the
?important? (high frequency) verbs in language fall
into classes corresponding to single senses. How-
ever, they also suggest that clustering can be used
for novel, previously unexplored purposes: to de-
tect from corpus data general patterns of seman-
tic behaviour (monosemy, predominant sense, reg-
ular/irregular polysemy).
In the future, we plan to investigate the use of soft
clustering (without hardening the output) and de-
velop methods for evaluating the soft output against
polysemous gold standards. We also plan to work
on improving the accuracy of subcategorization ac-
quisition, investigating the role of noise (irregular /
regular) in clustering, examining whether different
syntactic/semantic verb types require different ap-
proaches in clustering, developing our gold standard
classification further, and extending our experiments
to a larger number of verbs and verb classes.
References
B. Boguraev, E. J. Briscoe, J. Carroll, D. Carter, and
C. Grover. 1987. The derivation of a grammatically-
indexed lexicon from the longman dictionary of con-
temporary english. In Proc. of the 25th ACL, pages
193?200, Stanford, CA.
C. Brew and S. Schulte im Walde. 2002. Spectral clus-
tering for german verbs. In Conference on Empirical
Methods in Natural Language Processing, Philadel-
phia, USA.
E. J. Briscoe and J. Carroll. 1997. Automatic extraction
of subcategorization from corpora. In 5th ACL Confer-
ence on Applied Natural Language Processing, pages
356?363, Washington DC.
E. J. Briscoe and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In 3rd International
Conference on Language Resources and Evaluation,
pages 1499?1504, Las Palmas, Gran Canaria.
H. T. Dang, K. Kipper, M. Palmer, and J. Rosenzweig.
1998. Investigating regular sense extensions based on
intersective Levin classes. In Proc. of COLING/ACL,
pages 293?299, Montreal, Canada.
B. Dorr and D. Jones. 1996. Role of word sense disam-
biguation in lexical acquisition: predicting semantics
from syntactic cues. In 16th International Conference
on Computational Linguistics, pages 322?333, Copen-
hagen, Denmark.
B. Dorr. 1997. Large-scale dictionary construction
for foreign language tutoring and interlingual machine
translation. Machine Translation, 12(4):271?325.
R. Grishman, C. Macleod, and A. Meyers. 1994. Com-
lex syntax: building a computational lexicon. In In-
ternational Conference on Computational Linguistics,
pages 268?272, Kyoto, Japan.
R. Jackendoff. 1990. Semantic Structures. MIT Press,
Cambridge, Massachusetts.
E. Joanis. 2002. Automatic verb classification using a
general feature space. Master?s thesis, University of
Toronto.
J. L. Klavans and M. Kan. 1998. Role of verbs in docu-
ment analysis. In Proc. of COLING/ACL, pages 680?
686, Montreal, Canada.
A. Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, UK.
A. Korhonen. 2003. Extending Levin?s Classification
with New Verb Classes. Unpublished manuscript, Uni-
versity of Cambridge Computer Laboratory.
G. Leech. 1992. 100 million words of english: the british
national corpus. Language Research, 28(1):1?13.
B. Levin. 1993. English Verb Classes and Alternations.
Chicago University Press, Chicago.
P. Merlo and S. Stevenson. 2001. Automatic verb clas-
sification based on statistical distributions of argument
structure. Computational Linguistics, 27(3):373?408.
P. Merlo, S. Stevenson, V. Tsang, and G. Allaria. 2002.
A multilingual paradigm for automatic verb classifica-
tion. In Proc. of the 40th ACL, Pennsylvania, USA.
G. A. Miller. 1990. WordNet: An on-line lexi-
cal database. International Journal of Lexicography,
3(4):235?312.
S. Pinker. 1989. Learnability and Cognition: The Acqui-
sition of Argument Structure. MIT Press, Cambridge,
Massachusetts.
J. Preiss and A. Korhonen. 2002. Improving subcate-
gorization acquisition with WSD. In ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, Philadelphia, USA.
D. Roland, D. Jurafsky, L. Menn, S. Gahl, E. Elder, and
C. Riddoch. 2000. Verb subcatecorization frequency
differences between business-news and balanced cor-
pora. In ACL Workshop on Comparing Corpora, pages
28?34.
S. Schulte im Walde and C. Brew. 2002. Inducing ger-
man semantic verb classes from purely syntactic sub-
categorisation information. In Proc. of the 40th ACL,
Philadephia, USA.
S. Schulte im Walde. 2000. Clustering verbs seman-
tically according to their alternation behaviour. In
Proc. of COLING-2000, pages 747?753, Saarbru?cken,
Germany.
S. Stevenson and E. Joanis. 2003. Semi-supervised
verb-class discovery using noisy features. In Proc. of
CoNLL-2003, Edmonton, Canada.
N. Tishby, F. C. Pereira, and W. Bialek. 1999. The infor-
mation bottleneck method. In Proc. of the 37th Annual
Allerton Conference on Communication, Control and
Computing, pages 368?377.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 345?352,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Classification of Verbs in Biomedical Texts
Anna Korhonen
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge CB3 0GD, UK
alk23@cl.cam.ac.uk
Yuval Krymolowski
Dept. of Computer Science
Technion
Haifa 32000
Israel
yuvalkr@cs.technion.ac.il
Nigel Collier
National Institute of Informatics
Hitotsubashi 2-1-2
Chiyoda-ku, Tokyo 101-8430
Japan
collier@nii.ac.jp
Abstract
Lexical classes, when tailored to the appli-
cation and domain in question, can provide
an effective means to deal with a num-
ber of natural language processing (NLP)
tasks. While manual construction of such
classes is difficult, recent research shows
that it is possible to automatically induce
verb classes from cross-domain corpora
with promising accuracy. We report a
novel experiment where similar technol-
ogy is applied to the important, challeng-
ing domain of biomedicine. We show that
the resulting classification, acquired from
a corpus of biomedical journal articles,
is highly accurate and strongly domain-
specific. It can be used to aid BIO-NLP
directly or as useful material for investi-
gating the syntax and semantics of verbs
in biomedical texts.
1 Introduction
Lexical classes which capture the close relation
between the syntax and semantics of verbs have
attracted considerable interest in NLP (Jackendoff,
1990; Levin, 1993; Dorr, 1997; Prescher et al,
2000). Such classes are useful for their ability to
capture generalizations about a range of linguis-
tic properties. For example, verbs which share the
meaning of ?manner of motion? (such as travel,
run, walk), behave similarly also in terms of
subcategorization (I traveled/ran/walked, I trav-
eled/ran/walked to London, I traveled/ran/walked
five miles). Although the correspondence between
the syntax and semantics of words is not perfect
and the classes do not provide means for full se-
mantic inferencing, their predictive power is nev-
ertheless considerable.
NLP systems can benefit from lexical classes
in many ways. Such classes define the mapping
from surface realization of arguments to predicate-
argument structure, and are therefore an impor-
tant component of any system which needs the
latter. As the classes can capture higher level
abstractions they can be used as a means to ab-
stract away from individual words when required.
They are also helpful in many operational contexts
where lexical information must be acquired from
small application-specific corpora. Their predic-
tive power can help compensate for lack of data
fully exemplifying the behavior of relevant words.
Lexical verb classes have been used to sup-
port various (multilingual) tasks, such as compu-
tational lexicography, language generation, ma-
chine translation, word sense disambiguation, se-
mantic role labeling, and subcategorization acqui-
sition (Dorr, 1997; Prescher et al, 2000; Korho-
nen, 2002). However, large-scale exploitation of
the classes in real-world or domain-sensitive tasks
has not been possible because the existing classi-
fications, e.g. (Levin, 1993), are incomprehensive
and unsuitable for specific domains.
While manual classification of large numbers of
words has proved difficult and time-consuming,
recent research shows that it is possible to auto-
matically induce lexical classes from corpus data
with promising accuracy (Merlo and Stevenson,
2001; Brew and Schulte im Walde, 2002; Ko-
rhonen et al, 2003). A number of ML methods
have been applied to classify words using features
pertaining to mainly syntactic structure (e.g. sta-
tistical distributions of subcategorization frames
(SCFs) or general patterns of syntactic behaviour,
e.g. transitivity, passivisability) which have been
extracted from corpora using e.g. part-of-speech
tagging or robust statistical parsing techniques.
345
This research has been encouraging but it has
so far concentrated on general language. Domain-
specific lexical classification remains unexplored,
although it is arguably important: existing clas-
sifications are unsuitable for domain-specific ap-
plications and these often challenging applications
might benefit from improved performance by uti-
lizing lexical classes the most.
In this paper, we extend an existing approach
to lexical classification (Korhonen et al, 2003)
and apply it (without any domain specific tun-
ing) to the domain of biomedicine. We focus on
biomedicine for several reasons: (i) NLP is criti-
cally needed to assist the processing, mining and
extraction of knowledge from the rapidly growing
literature in this area, (ii) the domain lexical re-
sources (e.g. UMLS metathesaurus and lexicon1)
do not provide sufficient information about verbs
and (iii) being linguistically challenging, the do-
main provides a good test case for examining the
potential of automatic classification.
We report an experiment where a classifica-
tion is induced for 192 relatively frequent verbs
from a corpus of 2230 biomedical journal articles.
The results, evaluated with domain experts, show
that the approach is capable of acquiring classes
with accuracy higher than that reported in previous
work on general language. We discuss reasons for
this and show that the resulting classes differ sub-
stantially from those in extant lexical resources.
They constitute the first syntactic-semantic verb
classification for the biomedical domain and could
be readily applied to support BIO-NLP.
We discuss the domain-specific issues related to
our task in section 2. The approach to automatic
classification is presented in section 3. Details of
the experimental evaluation are supplied in sec-
tion 4. Section 5 provides discussion and section
6 concludes with directions for future work.
2 The Biomedical Domain and Our Task
Recent years have seen a massive growth in the
scientific literature in the domain of biomedicine.
For example, the MEDLINE database2 which cur-
rently contains around 16M references to journal
articles, expands with 0.5M new references each
year. Because future research in the biomedical
sciences depends on making use of all this existing
knowledge, there is a strong need for the develop-
1http://www.nlm.nih.gov/research/umls
2http://www.ncbi.nlm.nih.gov/PubMed/
ment of NLP tools which can be used to automat-
ically locate, organize and manage facts related to
published experimental results.
In recent years, major progress has been made
on information retrieval and on the extraction of
specific relations e.g. between proteins and cell
types from biomedical texts (Hirschman et al,
2002). Other tasks, such as the extraction of fac-
tual information, remain a bigger challenge. This
is partly due to the challenging nature of biomedi-
cal texts. They are complex both in terms of syn-
tax and semantics, containing complex nominals,
modal subordination, anaphoric links, etc.
Researchers have recently began to use deeper
NLP techniques (e.g. statistical parsing) in the do-
main because they are not challenged by the com-
plex structures to the same extent than shallow
techniques (e.g. regular expression patterns) are
(Lease and Charniak, 2005). However, deeper
techniques require richer domain-specific lexical
information for optimal performance than is pro-
vided by existing lexicons (e.g. UMLS). This is
particularly important for verbs, which are central
to the structure and meaning of sentences.
Where the lexical information is absent, lexical
classes can compensate for it or aid in obtaining
it in the ways described in section 1. Consider
e.g. the INDICATE and ACTIVATE verb classes in
Figure 1. They capture the fact that their members
are similar in terms of syntax and semantics: they
have similar SCFs and selectional preferences, and
they can be used to make similar statements which
describe similar events. Such information can be
used to build a richer lexicon capable of support-
ing key tasks such as parsing, predicate-argument
identification, event extraction and the identifica-
tion of biomedical (e.g. interaction) relations.
While an abundance of work has been con-
ducted on semantic classification of biomedical
terms and nouns, less work has been done on the
(manual or automatic) semantic classification of
verbs in the biomedical domain (Friedman et al,
2002; Hatzivassiloglou and Weng, 2002; Spasic et
al., 2005). No previous work exists in this domain
on the type of lexical (i.e. syntactic-semantic) verb
classification this paper focuses on.
To get an initial idea about the differences be-
tween our target classification and a general lan-
guage classification, we examined the extent to
which individual verbs and their frequencies dif-
fer in biomedical and general language texts. We
346
PROTEINS: p53
p53Tp53Dmp53...
ACTIVATE
suggestsdemonstratesindicatesimplies...
GENES: WAF1
WAF1CIP1p21...
It
INDICATE
that
activatesup-regulatesinducesstimulates...
...
Figure 1: Sample lexical classes
BIO BNC
show do
suggest say
use make
indicate go
contain see
describe take
express get
bind know
require come
observe give
find think
determine use
demonstrate find
perform look
induce want
Table 1: The 15 most frequent verbs in the
biomedical data and in the BNC
created a corpus of 2230 biomedical journal arti-
cles (see section 4.1 for details) and compared the
distribution of verbs in this corpus with that in the
British National Corpus (BNC) (Leech, 1992). We
calculated the Spearman rank correlation between
the 1165 verbs which occurred in both corpora.
The result was only a weak correlation: 0.37 ?
0.03. When the scope was restricted to the 100
most frequent verbs in the biomedical data, the
correlation was 0.12 ? 0.10 which is only 1.2?
away from zero. The dissimilarity between the
distributions is further indicated by the Kullback-
Leibler distance of 0.97. Table 1 illustrates some
of these big differences by showing the list of 15
most frequent verbs in the two corpora.
3 Approach
We extended the system of Korhonen et al (2003)
with additional clustering techniques (introduced
in sections 3.2.2 and 3.2.4) and used it to ob-
tain the classification for the biomedical domain.
The system (i) extracts features from corpus data
and (ii) clusters them using five different methods.
These steps are described in the following two sec-
tions, respectively.
3.1 Feature Extraction
We employ as features distributions of SCFs spe-
cific to given verbs. We extract them from cor-
pus data using the comprehensive subcategoriza-
tion acquisition system of Briscoe and Carroll
(1997) (Korhonen, 2002). The system incorpo-
rates RASP, a domain-independent robust statis-
tical parser (Briscoe and Carroll, 2002), which
tags, lemmatizes and parses data yielding com-
plete though shallow parses and a SCF classifier
which incorporates an extensive inventory of 163
verbal SCFs3. The SCFs abstract over specific
lexically-governed particles and prepositions and
specific predicate selectional preferences. In our
work, we parameterized two high frequency SCFs
for prepositions (PP and NP + PP SCFs). No filter-
ing of potentially noisy SCFs was done to provide
clustering with as much information as possible.
3.2 Classification
The SCF frequency distributions constitute the in-
put data to automatic classification. We experi-
ment with five clustering methods: the simple hard
nearest neighbours method and four probabilis-
tic methods ? two variants of Probabilistic Latent
Semantic Analysis and two information theoretic
methods (the Information Bottleneck and the In-
formation Distortion).
3.2.1 Nearest Neighbours
The first method collects the nearest neighbours
(NN) of each verb. It (i) calculates the Jensen-
Shannon divergence (JS) between the SCF distri-
butions of each pair of verbs, (ii) connects each
verb with the most similar other verb, and finally
(iii) finds all the connected components. The NN
method is very simple. It outputs only one clus-
tering configuration and therefore does not allow
examining different cluster granularities.
3.2.2 Probabilistic Latent Semantic Analysis
The Probabilistic Latent Semantic Analysis
(PLSA, Hoffman (2001)) assumes a generative
model for the data, defined by selecting (i) a verb
verbi, (ii) a semantic class classk from the dis-
tribution p(Classes | verbi), and (iii) a SCF scfj
from the distribution p(SCFs | classk). PLSA uses
Expectation Maximization (EM) to find the dis-
tribution p?(SCFs |Clusters, V erbs) which max-
imises the likelihood of the observed counts. It
does this by minimising the cost function
F = ?? log Likelihood(p? | data) +H(p?) .
3See http://www.cl.cam.ac.uk/users/alk23/subcat/subcat.html
for further detail.
347
For ? = 1 minimising F is equivalent to the stan-
dard EM procedure while for ? < 1 the distri-
bution p? tends to be more evenly spread. We use
? = 1 (PLSA/EM) and ? = 0.75 (PLSA?=0.75).
We currently ?harden? the output and assign each
verb to the most probable cluster only4.
3.2.3 Information Bottleneck
The Information Bottleneck (Tishby et al,
1999) (IB) is an information-theoretic method
which controls the balance between: (i) the
loss of information by representing verbs as
clusters (I(Clusters;V erbs)), which has to be
minimal, and (ii) the relevance of the output
clusters for representing the SCF distribution
(I(Clusters; SCFs)) which has to be maximal.
The balance between these two quantities ensures
optimal compression of data through clusters. The
trade-off between the two constraints is realized
through minimising the cost function:
LIB = I(Clusters;V erbs)
? ?I(Clusters; SCFs) ,
where ? is a parameter that balances the con-
straints. IB takes three inputs: (i) SCF-verb dis-
tributions, (ii) the desired number of clusters K,
and (iii) the initial value of ?. It then looks for
the minimal ? that decreases LIB compared to its
value with the initial ?, using the given K. IB de-
livers as output the probabilities p(K|V ). It gives
an indication for the most informative number of
output configurations: the ones for which the rele-
vance information increases more sharply between
K ? 1 and K clusters than between K and K + 1.
3.2.4 Information Distortion
The Information Distortion method (Dimitrov
and Miller, 2001) (ID) is otherwise similar to IB
but LID differs from LIB by an additional term that
adds a bias towards clusters of similar size:
LID = ?H(Clusters |V erbs)
? ?I(Clusters; SCFs)
= LIB ?H(Clusters) .
ID yields more evenly divided clusters than IB.
4 Experimental Evaluation
4.1 Data
We downloaded the data for our experiment from
the MEDLINE database, from three of the 10 lead-
4The same approach was used with the information theo-
retic methods. It made sense in this initial work on biomedi-
cal classification. In the future we could use soft clustering a
means to investigate polysemy.
ing journals in biomedicine: 1) Genes & Devel-
opment (molecular biology, molecular genetics),
2) Journal of Biological Chemistry (biochemistry
and molecular biology) and 3) Journal of Cell Bi-
ology (cellular structure and function). 2230 full-
text articles from years 2003-2004 were used. The
data included 11.5M words and 323,307 sentences
in total. 192 medium to high frequency verbs (with
the minimum of 300 occurrences in the data) were
selected for experimentation5. This test set was
big enough to produce a useful classification but
small enough to enable thorough evaluation in this
first attempt to classify verbs in the biomedical do-
main.
4.2 Processing the Data
The data was first processed using the feature ex-
traction module. 233 (preposition-specific) SCF
types appeared in the resulting lexicon, 36 per verb
on average.6 The classification module was then
applied. NN produced Knn = 42 clusters. From
the other methods we requested K = 2 to 60 clus-
ters. We chose for evaluation the outputs corre-
sponding to the most informative values of K: 20,
33, 53 for IB, and 17, 33, 53 for ID.
4.3 Gold Standard
Because no target lexical classification was avail-
able for the biomedical domain, human experts (4
domain experts and 2 linguists) were used to cre-
ate the gold standard. They were asked to examine
whether the test verbs similar in terms of their syn-
tactic properties (i.e. verbs with similar SCF distri-
butions) are similar also in terms of semantics (i.e.
they share a common meaning). Where this was
the case, a verb class was identified and named.
The domain experts examined the 116 verbs
whose analysis required domain knowledge
(e.g. activate, solubilize, harvest), while the lin-
guists analysed the remaining 76 general or scien-
tific text verbs (e.g. demonstrate, hypothesize, ap-
pear). The linguists used Levin (1993) classes as
gold standard classes whenever possible and cre-
ated novel ones when needed. The domain ex-
perts used two purely semantic classifications of
biomedical verbs (Friedman et al, 2002; Spasic et
al., 2005)7 as a starting point where this was pos-
5230 verbs were employed initially but 38 were dropped
later so that each (coarse-grained) class would have the min-
imum of 2 members in the gold standard.
6This number is high because no filtering of potentially
noisy SCFs was done.
7See http://www.cbr-masterclass.org.
348
1 Have an effect on activity (BIO/29) 8 Physical Relation
1.1 Activate / Inactivate Between Molecules (BIO/20)
1.1.1 Change activity: activate, inhibit 8.1 Binding: bind, attach
1.1.2 Suppress: suppress, repress 8.2 Translocate and Segregate
1.1.3 Stimulate: stimulate 8.2.1 Translocate: shift, switch
1.1.4 Inactivate: delay, diminish 8.2.2 Segregate: segregate, export
1.2 Affect 8.3 Transmit
1.2.1 Modulate: stabilize, modulate 8.3.1 Transport: deliver, transmit
1.2.2 Regulate: control, support 8.3.2 Link: connect, map
1.3 Increase / decrease: increase, decrease 9 Report (GEN/30)
1.4 Modify: modify, catalyze 9.1 Investigate
2 Biochemical events (BIO/12) 9.1.1 Examine: evaluate, analyze
2.1 Express: express, overexpress 9.1.2 Establish: test, investigate
2.2 Modification 9.1.3 Confirm: verify, determine
2.2.1 Biochemical modification: 9.2 Suggest
dephosphorylate, phosphorylate 9.2.1 Presentational:
2.2.2 Cleave: cleave hypothesize, conclude
2.3 Interact: react, interfere 9.2.2 Cognitive:
3 Removal (BIO/6) consider, believe
3.1 Omit: displace, deplete 9.3 Indicate: demonstrate, imply
3.2 Subtract: draw, dissect 10 Perform (GEN/10)
4 Experimental Procedures (BIO/30) 10.1 Quantify
4.1 Prepare 10.1.1 Quantitate: quantify, measure
4.1.1 Wash: wash, rinse 10.1.2 Calculate: calculate, record
4.1.2 Mix: mix 10.1.3 Conduct: perform, conduct
4.1.3 Label: stain, immunoblot 10.2 Score: score, count
4.1.4 Incubate: preincubate, incubate 11 Release (BIO/4): detach, dissociate
4.1.5 Elute: elute 12 Use (GEN/4): utilize, employ
4.2 Precipitate: coprecipitate 13 Include (GEN/11)
coimmunoprecipitate 13.1 Encompass: encompass, span
4.3 Solubilize: solubilize,lyse 13.2 Include: contain, carry
4.4 Dissolve: homogenize, dissolve 14 Call (GEN/3): name, designate
4.5 Place: load, mount 15 Move (GEN/12)
5 Process (BIO/5): linearize, overlap 15.1 Proceed:
6 Transfect (BIO/4): inject, microinject progress, proceed
7 Collect (BIO/6) 15.2 Emerge:
7.1 Collect: harvest, select arise, emerge
7.2 Process: centrifuge, recover 16 Appear (GEN/6): appear, occur
Table 2: The gold standard classification with a
few example verbs per class
sible (i.e. where they included our test verbs and
also captured their relevant senses)8.
The experts created a 3-level gold standard
which includes both broad and finer-grained
classes. Only those classes / memberships were
included which all the experts (in the two teams)
agreed on.9 The resulting gold standard includ-
ing 16, 34 and 50 classes is illustrated in table 2
with 1-2 example verbs per class. The table in-
dicates which classes were created by domain ex-
perts (BIO) and which by linguists (GEN). Each
class was associated with 1-30 member verbs10.
The total number of verbs is indicated in the table
(e.g. 10 for PERFORM class).
4.4 Measures
The clusters were evaluated against the gold stan-
dard using measures which are applicable to all the
8Purely semantic classes tend to be finer-grained than lex-
ical classes and not necessarily syntactic in nature. Only these
two classifications were found to be similar enough to our tar-
get classification to provide a useful starting point. Section 5
includes a summary of the similarities/differences between
our gold standard and these other classifications.
9Experts were allowed to discuss the problematic cases
to obtain maximal accuracy - hence no inter-annotator agree-
ment is reported.
10The minimum of 2 member verbs were required at the
coarser-grained levels of 16 and 34 classes.
classification methods and which deliver a numer-
ical value easy to interpret.
The first measure, the adjusted pairwise preci-
sion, evaluates clusters in terms of verb pairs:
APP = 1K
K?
i=1
num. of correct pairs in ki
num. of pairs in ki ?
|ki|?1
|ki|+1
APP is the average proportion of all within-
cluster pairs that are correctly co-assigned. Multi-
plied by a factor that increases with cluster size it
compensates for a bias towards small clusters.
The second measure is modified purity, a global
measure which evaluates the mean precision of
clusters. Each cluster is associated with its preva-
lent class. The number of verbs in a cluster K that
take this class is denoted by nprevalent(K). Verbs
that do not take it are considered as errors. Clus-
ters where nprevalent(K) = 1 are disregarded as
not to introduce a bias towards singletons:
mPUR =
?
nprevalent(ki)?2
nprevalent(ki)
number of verbs
The third measure is the weighted class accu-
racy, the proportion of members of dominant clus-
ters DOM-CLUSTi within all classes ci.
ACC =
C?
i=1
verbs in DOM-CLUSTi
number of verbs
mPUR can be seen to measure the precision of
clusters and ACC the recall. We define an F mea-
sure as the harmonic mean of mPUR and ACC:
F = 2 ?mPUR ? ACCmPUR + ACC
The statistical significance of the results is mea-
sured by randomisation tests where verbs are
swapped between the clusters and the resulting
clusters are evaluated. The swapping is repeated
100 times for each output and the average avswaps
and the standard deviation ?swaps is measured.
The significance is the scaled difference signif =
(result? avswaps)/?swaps .
4.5 Results from Quantitative Evaluation
Table 3 shows the performance of the five clus-
tering methods for K = 42 clusters (as produced
by the NN method) at the 3 levels of gold stan-
dard classification. Although the two PLSA vari-
ants (particularly PLSA?=0.75) produce a fairly ac-
curate coarse grained classification, they perform
worse than all the other methods at the finer-
grained levels of gold standard, particularly ac-
cording to the global measures. Being based on
349
16 Classes 34 Classes 50 Classes
APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
NN 81 86 39 53 64 74 62 67 54 67 73 69
IB 74 88 47 61 61 76 74 75 55 69 87 76
ID 79 89 37 52 63 78 65 70 53 70 77 73
PLSA/EM 55 72 49 58 43 53 61 57 35 47 66 55
PLSA?=0.75 65 71 68 70 53 48 76 58 41 34 77 47
Table 3: The performance of the NN, PLSA, IB and ID methods with Knn = 42 clusters
16 Classes 34 Classes 50 Classes
K APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
20 IB 74 77 66 71 60 56 86 67 54 48 93 63
17 ID 67 76 60 67 43 56 81 66 34 46 91 61
33 IB 78 87 52 65 69 75 81 77 61 67 93 77
ID 81 88 43 57 65 75 70 72 54 67 82 73
53 IB 71 87 41 55 61 78 66 71 54 72 79 75
ID 79 89 33 48 66 79 55 64 53 72 68 69
Table 4: The performance of IB and ID for the 3 levels of class hierarchy for informative values of K
pairwise similarities, NN shows mostly better per-
formance than IB and ID on the pairwise measure
APP but the global measures are better for IB and
ID. The differences are smaller in mPUR (yet sig-
nificant: 2? between NN and IB and 3? between
NN and ID) but more notable in ACC (which is
e.g. 8 ? 12% better for IB than for NN). Also
the F results suggest that the two information the-
oretic methods are better overall than the simple
NN method.
IB and ID also have the advantage (over NN) that
they can be used to produce a hierarchical verb
classification. Table 4 shows the results for IB and
ID for the informative values of K. The bold font
indicates the results when the match between the
values of K and the number of classes at the par-
ticular level of the gold standard is the closest.
IB is clearly better than ID at all levels of gold
standard. It yields its best results at the medium
level (34 classes) with K = 33: F = 77 and APP
= 69 (the results for ID are F = 72 and APP =
65). At the most fine-grained level (50 classes),
IB is equally good according to F with K = 33,
but APP is 8% lower. Although ID is occasion-
ally better than IB according to APP and mPUR
(see e.g. the results for 16 classes with K = 53)
this never happens in the case where the corre-
spondence between the number of gold standard
classes and the values of K is the closest. In other
words, the informative values of K prove really
informative for IB. The lower performance of ID
seems to be due to its tendency to create evenly
sized clusters.
All the methods perform significantly better
than our random baseline. The significance of the
results with respect to two swaps was at the 2?
level, corresponding to a 97% confidence that the
results are above random.
4.6 Qualitative Evaluation
We performed further, qualitative analysis of clus-
ters produced by the best performing method IB.
Consider the following clusters:
A: inject, transfect, microinfect, contransfect (6)
B: harvest, select, collect (7.1)
centrifuge, process, recover (7.2)
C: wash, rinse (4.1.1)
immunoblot (4.1.3)
overlap (5)
D: activate (1.1.1)
When looking at coarse-grained outputs, in-
terestingly, K as low as 8 learned the broad
distinction between biomedical and general lan-
guage verbs (the two verb types appeared only
rarely in the same clusters) and produced large se-
mantically meaningful groups of classes (e.g. the
coarse-grained classes EXPERIMENTAL PROCE-
DURES, TRANSFECT and COLLECT were mapped
together). K = 12 was sufficient to iden-
tify several classes with very particular syntax
One of them was TRANSFECT (see A above)
whose members were distinguished easily be-
cause of their typical SCFs (e.g. inject /trans-
fect/microinfect/contransfect X with/into Y).
On the other hand, even K = 53 could not iden-
tify classes with very similar (yet un-identical)
syntax. These included many semantically similar
sub-classes (e.g. the two sub-classes of COLLECT
350
shown in B whose members take similar NP and
PP SCFs). However, also a few semantically dif-
ferent verbs clustered wrongly because of this rea-
son, such as the ones exemplified in C. In C, im-
munoblot (from the LABEL class) is still somewhat
related to wash and rinse (the WASH class) because
they all belong to the larger EXPERIMENTAL PRO-
CEDURES class, but overlap (from the PROCESS
class) shows up in the cluster merely because of
syntactic idiosyncracy.
While parser errors caused by the challeng-
ing biomedical texts were visible in some SCFs
(e.g. looking at a sample of SCFs, some adjunct
instances were listed in the argument slots of the
frames), the cases where this resulted in incorrect
classification were not numerous11.
One representative singleton resulting from
these errors is exemplified in D. Activate ap-
pears in relatively complicated sentence struc-
tures, which gives rise to incorrect SCFs. For ex-
ample, MECs cultured on 2D planar substrates
transiently activate MAP kinase in response to
EGF, whereas... gets incorrectly analysed as SCF
NP-NP, while The effect of the constitutively ac-
tivated ARF6-Q67L mutant was investigated... re-
ceives the incorrect SCF analysis NP-SCOMP. Most
parser errors are caused by unknown domain-
specific words and phrases.
5 Discussion
Due to differences in the task and experimental
setup, direct comparison of our results with pre-
viously published ones is impossible. The clos-
est possible comparison point is (Korhonen et al,
2003) which reported 50-59% mPUR and 15-19%
APP on using IB to assign 110 polysemous (gen-
eral language) verbs into 34 classes. Our results
are substantially better, although we made no ef-
fort to restrict our scope to monosemous verbs12
and although we focussed on a linguistically chal-
lenging domain.
It seems that our better result is largely due
to the higher uniformity of verb senses in the
biomedical domain. We could not investigate this
effect systematically because no manually sense
11This is partly because the mistakes of the parser are
somewhat consistent (similar for similar verbs) and partly be-
cause the SCFs gather data from hundreds of corpus instances,
many of which are analysed correctly.
12Most of our test verbs are polysemous according to
WordNet (WN) (Miller, 1990), but this is not a fully reliable
indication because WN is not specific to this domain.
annotated data (or a comprehensive list of verb
senses) exists for the domain. However, exami-
nation of a number of corpus instances suggests
that the use of verbs is fairly conventionalized in
our data13. Where verbs show less sense varia-
tion, they show less SCF variation, which aids the
discovery of verb classes. Korhonen et al (2003)
observed the opposite with general language data.
We examined, class by class, to what extent our
domain-specific gold standard differs from the re-
lated general (Levin, 1993) and domain classifica-
tions (Spasic et al, 2005; Friedman et al, 2002)
(recall that the latter were purely semantic clas-
sifications as no lexical ones were available for
biomedicine):
33 (of the 50) classes in the gold standard are
biomedical. Only 6 of these correspond (fully or
mostly) to the semantic classes in the domain clas-
sifications. 17 are unrelated to any of the classes in
Levin (1993) while 16 bear vague resemblance to
them (e.g. our TRANSPORT verbs are also listed
under Levin?s SEND verbs) but are too different
(semantically and syntactically) to be combined.
17 (of the 50) classes are general (scientific)
classes. 4 of these are absent in Levin (e.g. QUAN-
TITATE). 13 are included in Levin, but 8 of them
have a more restricted sense (and fewer members)
than the corresponding Levin class. Only the re-
maining 5 classes are identical (in terms of mem-
bers and their properties) to Levin classes.
These results highlight the importance of build-
ing or tuning lexical resources specific to different
domains, and demonstrate the usefulness of auto-
matic lexical acquisition for this work.
6 Conclusion
This paper has shown that current domain-
independent NLP and ML technology can be used
to automatically induce a relatively high accu-
racy verb classification from a linguistically chal-
lenging corpus of biomedical texts. The lexical
classification resulting from our work is strongly
domain-specific (it differs substantially from pre-
vious ones) and it can be readily used to aid BIO-
NLP. It can provide useful material for investigat-
ing the syntax and semantics of verbs in biomed-
ical data or for supplementing existing domain
lexical resources with additional information (e.g.
13The different sub-domains of the biomedical domain
may, of course, be even more conventionalized (Friedman et
al., 2002).
351
semantic classifications with additional member
verbs). Lexical resources enriched with verb class
information can, in turn, better benefit practical
tasks such as parsing, predicate-argument identifi-
cation, event extraction, identification of biomedi-
cal relation patterns, among others.
In the future, we plan to improve the accu-
racy of automatic classification by seeding it with
domain-specific information (e.g. using named en-
tity recognition and anaphoric linking techniques
similar to those of Vlachos et al (2006)). We also
plan to conduct a bigger experiment with a larger
number of verbs and demonstrate the usefulness of
the bigger classification for practical BIO-NLP ap-
plication tasks. In addition, we plan to apply sim-
ilar technology to other interesting domains (e.g.
tourism, law, astronomy). This will not only en-
able us to experiment with cross-domain lexical
class variation but also help to determine whether
automatic acquisition techniques benefit, in gen-
eral, from domain-specific tuning.
Acknowledgement
We would like to thank Yoko Mizuta, Shoko
Kawamato, Sven Demiya, and Parantu Shah for
their help in creating the gold standard.
References
C. Brew and S. Schulte im Walde. 2002. Spectral
clustering for German verbs. In Conference on Em-
pirical Methods in Natural Language Processing,
Philadelphia, USA.
E. J. Briscoe and J. Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In 5th ACL
Conference on Applied Natural Language Process-
ing, pages 356?363, Washington DC.
E. J. Briscoe and J. Carroll. 2002. Robust accurate
statistical annotation of general text. In 3rd Interna-
tional Conference on Language Resources and Eval-
uation, pages 1499?1504, Las Palmas, Gran Ca-
naria.
A. G. Dimitrov and J. P. Miller. 2001. Neural coding
and decoding: communication channels and quanti-
zation. Network: Computation in Neural Systems,
12(4):441?472.
B. Dorr. 1997. Large-scale dictionary construction for
foreign language tutoring and interlingual machine
translation. Machine Translation, 12(4):271?325.
C. Friedman, P. Kra, and A. Rzhetsky. 2002. Two
biomedical sublanguages: a description based on the
theories of Zellig Harris. Journal of Biomedical In-
formatics, 35(4):222?235.
V. Hatzivassiloglou and W. Weng. 2002. Learning an-
chor verbs for biological interaction patterns from
published text articles. International Journal of
Medical Inf., 67:19?32.
L. Hirschman, J. C. Park, J. Tsujii, L. Wong, and C. H.
Wu. 2002. Accomplishments and challenges in lit-
erature data mining for biology. Journal of Bioin-
formatics, 18(12):1553?1561.
T. Hoffman. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning,
42(1):177?196.
R. Jackendoff. 1990. Semantic Structures. MIT Press,
Cambridge, Massachusetts.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003.
Clustering polysemic subcategorization frame distri-
butions semantically. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 64?71, Sapporo, Japan.
A. Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, UK.
M. Lease and E. Charniak. 2005. Parsing biomedical
literature. In Second International Joint Conference
on Natural Language Processing, pages 58?69.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
B. Levin. 1993. English Verb Classes and Alterna-
tions. Chicago University Press, Chicago.
P. Merlo and S. Stevenson. 2001. Automatic verb
classification based on statistical distributions of
argument structure. Computational Linguistics,
27(3):373?408.
G. A. Miller. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography,
3(4):235?312.
D. Prescher, S. Riezler, and M. Rooth. 2000. Using
a probabilistic class-based lexicon for lexical am-
biguity resolution. In 18th International Confer-
ence on Computational Linguistics, pages 649?655,
Saarbru?cken, Germany.
I. Spasic, S. Ananiadou, and J. Tsujii. 2005. Master-
class: A case-based reasoning system for the clas-
sification of biomedical terms. Journal of Bioinfor-
matics, 21(11):2748?2758.
N. Tishby, F. C. Pereira, and W. Bialek. 1999. The
information bottleneck method. In Proc. of the
37th Annual Allerton Conference on Communica-
tion, Control and Computing, pages 368?377.
A. Vlachos, C. Gasperin, I. Lewin, and E. J. Briscoe.
2006. Bootstrapping the recognition and anaphoric
linking of named entitites in drosophila articles. In
Pacific Symposium in Biocomputing.
352
Applying System Combinat ion to Base Noun Phrase Identif ication 
Er ik  F. T jong  K im Sang",  Wal ter  Dae lemans  '~, Herv6  D6 jean  ~, 
Rob  Koel ingT,  Yuva l  Krymolowski /~,  Vas in  Punyakanok  '~, Dan Roth" 
~University of Antwert) 
Uifiversiteitsplohl 1 
13.-261.0 Wilri jk, Belgium 
{erikt,daelem}@uia.ua.ac.be 
r Unive.rsitiil; Tii l) ingen 
Kleine Wilhehnstrat./e 113 
I)-72074 T/il)ingen, Germany 
(lejean((~sl:q, ni)hil.ulfi-l;uebingen.de, 
7S1{,I Cambridge 
23 Millers Yard,Mil l  Lane 
Cambridge, CB2 ll{Q, UK 
koeling@caln.sri.coIn 
;~Bal'-Ilan University 
lbunat  Gan, 52900, Israel 
yuwdk(c~)macs. 1)iu.ac.il 
"University of Illinois 
1304: W. Sl)ringfield Ave. 
Url)ana, IL 61801, USA 
{lmnyakan,(lanr} ((~cs.uiuc.edu 
A1)s t rac t  
We us('. seven machine h;arning algorithms tbr 
one task: idenl;it~ying l)ase holm phrases. The 
results have 1)een t)rocessed by ditt'erent system 
combination methods and all of these (mtt)er- 
formed the t)est individual result. We have ap- 
t)lied the seven learners with the best (:omt)ina- 
tot, a majority vote of the top tive systenls, to a 
standard (lata set and lllallage(1 I;O ilnl)rov(', 1;11(' 
t)est pul)lished result %r this (lata set. 
1 In t roduct ion  
Van Haltor(m eta\ ] .  (1998) and Brill and Wu 
(1998) show that part-ofst)ee(:h tagger l)erfor- 
mance can 1)e iml)roved 1)y (:oml)ining ditl'erent 
tatters. By using te(:hni(tues su(:h as majority 
voting, errors made l)y 1;11(; minority of the tag- 
gers can 1)e r(;moved. Van Ilaltere, n et al (1998) 
rel)ort that the results of such a ('oml)ined al)- 
proach can improve ll\])Oll the aCcllracy error of 
the best individual system with as much as 19%. 
Tim positive (;tl'e(:t of system combination tbr 
non-language t)ro(:essing tasks has t)een shown 
in a large l)o(ly of mac\]fine l arning work. 
In this 1)aper we will use system (:omt)ination 
for identifying base noun 1)hrases (1)aseNt)s). 
W(; will at)l)ly seven machine learning algo- 
rithms to the same 1)aseNP task. At two l)oints 
we will al)ply confl)ination methods. We will 
start with making the systems process five out- 
trot representations and combine the l'esults t)y 
(:hoosing the majority of the outtmt tL'atures. 
Three of the seven systems use this al)l)roaeh. 
Afl, er this w(; will make an overall eoml)ination 
of the results of the seven systems. There we 
will evaluate several system combination meth- 
()(Is. The 1)est l)erforming method will 1)e at)- 
t)lied to a standard ata set tbr baseNP identi- 
tication. 
2 Methods  and exper iments  
in this se(:tion we will describe our lem:ning task: 
recognizing 1)ase noun phrases. After this we 
will (tes(:ril)e the data representations we used 
and the ma('hine learning algorithms that we 
will at)l)ly to the task. We will con(:ludc with 
an overview of the (:ombination metllo(ls that 
we will test. 
2.1 Task descript ion 
Base noun \])hrases (1)aseNPs) are n(mn phrases 
whi(:h do not (:ontain another noun l)hrase. \]?or 
cxamt)le , the sentence 
In \[early trading\] in \[ IIong Kong\] 
\[ Mo,l,tay \], \[ g,,la \] was q, loted at 
\[ $ 366. 0 \] \ [a .  o1,,,.(; \] .  
contains six baseN1)s (marked as phrases be- 
tween square 1)rackets). The phrase $ 266.50  
an  ounce  ix a holm phrase as well. However, it 
is not a baseNP since it contains two other noun 
phrases. Two baseNP data sets haw.' been put 
forward by Ramshaw and Marcus (1995). The 
main data set consist of tbur sections of the Wall 
Street Journal (WSJ) part of the Penn Tree- 
bank (Marcus et al, 1.993) as training mate- 
rial (sections 15-18, 211727 tokens) and one sec- 
tion aS test material (section 20, 47377 tokens)5. 
The data contains words, their part-of-speech 
1This Ramshaw and Marcus (1995) bascNP data set 
is availal)le via ffp://fti).cis.upe,m.edu/pub/chunker/ 
857 
(POS) tags as computed by the Brill tagger and 
their baseNP segmentation asderived from the 
%'eebank (with some modifications). 
In the baseNP identitication task, perfor- 
mance is measured with three rates. First, 
with the percentage of detected noun phrases 
that are correct (precision). Second, with the 
1)ercentage of noun phrases in the data that 
were found by the classifier (recall). And third, 
with the F#=~ rate which is equal to (2*preci- 
sion*recall)/(precision+recall). The latter rate 
has been used as the target for optimization. 
2.2 Data representat ion 
In our example sentence in section 2.1, noun 
phrases are represented by bracket structures. 
It has been shown by Mufioz et al (1999) 
that for baseNP recognition, the representa- 
tion with brackets outperforms other data rep- 
resentations. One classifier can be trained to 
recognize open brackets (O) and another can 
handle close brackets (C). Their results can be 
combined by making pairs of open and close 
brackets with large probability scores. We have 
used this bracket representation (O+C) as well. 
However, we have not used the combination 
strategy from Mufioz et al (1999) trot in- 
stead used the strategy outlined in Tjong Kim 
Sang (2000): regard only the shortest possi- 
ble phrases between candidate open and close 
brackets as base noun phrases. 
An alternative representation for baseNPs 
has been put tbrward by Ramshaw and Mar- 
cus (1995). They have defined baseNP recog- 
nition as a tagging task: words can be inside a 
baseNP (I) or outside a baseNP (O). In the case 
that one baseNP immediately follows another 
baseNP, the first word in the second baseNP 
receives tag B. Example: 
Ino early1 trading1 ino Hongi Kongi 
MondayB ,o gold1 waso quotedo ato 
$I 366.501 anu ounce1 .o 
This set of three tags is sufficient for encod- 
ing baseNP structures since these structures are 
nonrecursive and nonoverlapping. 
Tjong Kiln Sang (2000) outlines alternative 
versions of this tagging representation. First, 
the B tag can be used for tile first word of ev- 
ery baseNP (IOB2 representation). Second, in- 
stead of the B tag an E tag can be used to 
nlark the last word of a baseNP immediately 
before another baseNP (IOE1). And third, the 
E tag call be used for every noun phrase final 
word (IOE2). He used the Ramshaw and Mar- 
cus (1995) representation as well (IOB1). We 
will use these tbur tagging representations and 
the O+C representation for the system-internal 
combination experiments. 
2.a Machine learning algorithms 
This section contains a brief description of tile 
seven machine learning algorithms that we will 
apply to the baseNP identification task: AL- 
LiS, c5.0, IO~?ee, MaxEnt, MBL, MBSL and 
SNOW. 
ALLiS 2 (Architecture for Learning Linguistic 
Structures) is a learning system which uses the- 
ory refinement in order to learn non-recursive 
NP and VP structures (Ddjean, 2000). ALLiS 
generates a regular expression grammar which 
describes the phrase structure (NP or VP). This 
grammar is then used by the CASS parser (Ab- 
hey, 1996). Following the principle of theory re- 
finement, tile learning task is composed of two 
steps. The first step is the generation of an 
initial wa, mmar. The generation of this grmn- 
mar uses the notion of default values and some 
background knowledge which provides general 
expectations concerning the immr structure of 
NPs and VPs. This initial grammar provides 
an incomplete and/or incorrect analysis of tile 
data. The second step is the refinement of this 
grammar. During this step, the validity of the 
rules of the initial grammar is checked and the 
rules are improved (refined) if necessary. This 
refinement relies on the use of two operations: 
the contextualization (i which contexts uch a 
tag always belongs to the phrase) and lexical- 
ization (use of information about the words and 
not only about POS). 
05.0 a, a commercial version of 04.5 (Quin- 
lan, 1993), performs top-do,vn induction of de- 
cision trees (TDIDT). O,1 the basis of an in- 
stance base of examples, 05.0 constructs a deci- 
sion tree which compresses the classification i - 
formation in the instance base by exploiting dif- 
tbrences in relative importance of different fea- 
tures. Instances are stored in the tree as paths 
2A demo f the NP and VP ctmnker is available at 
ht;t:p: / /www.sfb441.unituebingen.de/~ dej an/chunker.h 
tml 
aAvailable fl'om http://www.rulequest.com 
858 
of commcted nodes ending in leaves which con- 
tain classification information. Nodes are con- 
nected via arcs denoting feature wflues. Feature 
inff)rmation gain (nmt;ual inforniation 1)etween 
features and class) is used to determine the or- 
der in which features are mnt)loyed as tests at all 
levels of the tree (Quinlan, 1993), With the full 
inlmt representation (words and POS tags)~ we 
were not able to run comt)lete xperiments. We 
therefore xperimented only with the POS tags 
(with a context of two left; and right). We have 
used the default parameter setting with decision 
trees coml)ined with wflue groul)ing. 
We have used a nearest neighbor algoritlml 
(IBI.-1G, here listed as MBL) and a decision tree 
algoritlmi (llG\[lh:ee) from the TiMBL learning 
package (Da(flmnans et al, 19991)). Both algo- 
rithms store the training data and ('lassi(y new 
it;eros by choosing the most frequent (:lassiti(:a- 
lion among training items which are closest to 
this new item. l)ata it(uns rare rel)resented as 
sets of thature-vahu; 1)airs. Each ti;ature recc'ives 
a weight which is t)ased on the amount of in- 
formation whi(:h it t/rovides fi)r comtmting the 
classification of t;t1(; items in the training data. 
IBI-IG uses these weights tbr comt)uting the dis- 
lance l)etween a t)air of data items and IGTree 
uses them fi)r deciding which feature-value de- 
cisions shouM t)e made in the top nod(;s of the 
decision tree (l)a(;lenJans et al, 19991)). We 
will use their det, mlt pm:amet('a:s excel)t for the 
IBI-IG t)arameter for the numl)er of exmnine(t 
m',arest n(,ighl)ors (k) whi('h we h~ve s(,t to 3 
(Daelemans et al, 1999a). The classifiers use a 
left and right context of four words and part- 
ofsl)eech tags. t~i)r |;lie four IO representations 
we have used a second i)rocessing stage which 
used a smaller context lint which included in- 
formation at)out the IO tags 1)redicted by the 
first processing phase (Tjong Kim Sang, 2000). 
When /)uilding a classifier, one must gather 
evidence ti)r predicting the correct class of an 
item from its context. The Maxinmm Entropy 
(MaxEnt) fl:mnework is especially suited tbr 
integrating evidence tiom various inti)rmal;ion 
sources. Frequencies of evidence/class combi~ 
nations (called features) are extracted fl'om a 
sample corlms and considere(t to be t)roperties 
of the classification process. Attention is con- 
strained to models with these l)roperties. The 
MaxEnt t)rinciph; now demands that among all 
1;11(; 1)robability distributions that obey these 
constraints, the most mfiform is chosen, l)ur- 
ing training, features are assigned weights in 
such a way that, given the MaxEnt principle, 
the training data is matched as well as possible. 
During evaluation it is tested which features are 
active (i.e. a feature is active when the context 
meets the requirements given by t;11(', feature). 
For every class the weights of the active fea- 
tures are combined and the best scoring class 
is chosen (Berger et al, 1996). D)r the classi- 
tier built here the surromlding words, their POS 
tags and lmseNP tags predicted for the previous 
words are used its evidence. A mixture of simple 
features (consisting of one of the mentioned in- 
formation sources) and complex features (com- 
binations thereof) were used. The left context 
never exceeded 3 words, the right context was 
maximally 2 words. The model wits (:ah:ulated 
using existing software (l)ehaspe, 1997). 
MBSL (Argalnon et al, 1999) uses POS data 
in order to identit~y t/aseNPs, hfferenee re- 
lies on a memory which contains all the o(:- 
cm:rences of P()S sequences which apt)ear in 
the t)egimfing, or the end, of a 1)aseNl? (in- 
(:hiding complete t)hrases). These sequences 
may include a thw context tags, up to a 1)re- 
st)ecifi('d max_(:ont<~:t. \])uring inti',rence, MBSL 
tries to 'tile' each POS string with parts of 
noun-l)hrases from l;he memory. If the string 
coul(1 l)e fully covered t)y the tiles, il; becomes 
l)art of a (:andidate list, anfl)iguities 1)etween 
candidates are resolved by a constraint )ropa- 
gation algorithm. Adding a (:ontext extends the 
possil)ilities for tiling, thereby giving more op- 
portunities to 1)etter candidates. The at)t)roaeh 
of MBSL to the i)rot)lem of identifying 1)aseNPs 
is sequence-1)ased rather than word-based, that 
is, decisions are taken per POS sequence, or per 
candidate, trot not for a single word. In addi- 
tion, the tiling l)rocess gives no preference to 
any (tirection in the sentence. The tiles may 1)e 
of any length, up to the maximal ength of a 
1)hrase in the training (ILl;L, which gives MBSL 
a generalization power that compensates for the 
setup of using only POS tags. The results t)re- 
seated here were obtained by optimizing MBSL 
parameters based on 5-fold CV on the training 
data. 
SNoW uses the Open/Close model, described 
in Mufioz et al (1999). As is shown there, this 
859 
section 21 
IOB1 
IOB2 
IOE1 
IOE2 
O+C 
0 
97.81% 
97.63% 
97.80% 
97.72% 
97.72% 
MBL 
Majority 98.04% 98.20% 
C Ffl=l 
97.97% 91.68 
97.96% 91.79 
97.92% 91.54 
97.94% 92.06 
98.04% 92.03 
92.82 
MaxEnt 
O C 
97.90% 98.11% 
97.81% 98.14% 
97.88% 98.12% 
97.84% 98.12% 
97.82% 98.15% 
97.94% 98.24% 
Ffl=l 
92.43 
92.14 
92.37 
92.13 
92.26 
92.60 
IGTree 
O C 
96.62% 96.89% 
97.27% 97.30% 
95.88% 96.01% 
97.19% 97.62% 
96.89% 97.49% 
97.70% 97.99% 
F\[~=1 
87.88 
90.03 
82.80 
89.98 
89.37 
91.92 
Table 1: The effects of system-internal combination by using different output representations. A 
straight-forward majority vote of the output yields better bracket accuracies and Ffl=l rates than 
any included individual classifier. The bracket accuracies in the cohmms O and C show what 
percentage of words was correctly classified as baseNP start, baseNP end or neither. 
model produced better results than the other 
paradigm evaluated there, the Inside/Outside 
paradigm. The Open/Close model consists of 
two SNoW predictors, one of which predicts the 
beginning of baseNPs (Open predictor), and the 
other predicts the end of the ptlrase (Close pre- 
dictor). The Open predictor is learned using 
SNoW (Carlson el; al., 1999; Roth, 1998) as a 
flmction of features that utilize words and POS 
tags in the sentence and, given a new sentence, 
will predict for each word whether it is the first 
word in the phrase or not. For each Open, the 
Close predictor is learned using SNoW as a func- 
tion of features that utilize the words ill the sen- 
tence, the POS tags and the open prediction. It 
will predict, tbr each word, whether it Call be 
the end of" the I)hrase, given the previously pre- 
dicted Open. Each pair of predicted Open mid 
Close forms a candidate of a baseNP. These can- 
didates may conflict due to overlapping; at this 
stage, a graph-based constraint satisfaction al- 
gorithm that uses the confidence values SNoW 
associates with its predictions i elnployed. This 
algorithln ("the combinator') produces tile list 
of" the final baseNPs fbr each sentence. Details 
of SNOW, its application in shallow parsing and 
the combinator% Mgorithm are in Mufioz et al 
(1999). 
2.4 Combinat ion techniques 
At two points in our noun phrase recognition 
process we will use system combination. We will 
start with system-internal combination: apply 
the same learning algorithm to variants of the 
task and combine the results. The approach 
we have chosen here is the same as in Tjong 
Kim Sang (2000): generate different variants 
of the task by using different representations 
of the output (IOB1, IOB2, IOE1, IOE2 and 
O+C). The five outputs will converted to the 
open bracket representation (O) and the close 
bracket; representation (C) and M'ter this, tile 
most frequent of the five analyses of each word 
will chosen (inajority voting, see below). We 
expect the systems which use this combination 
phase to perform better than their individuM 
members (Tjong Kim Sang, 2000). 
Our seven learners will generate different clas- 
sifications of tile training data and we need to 
find out which combination techniques are most 
appropriate. For the system-external combi- 
nation experiment, we have evaluated itfi;rent 
voting lllechanisms~ effectively the voting meth- 
ods as described in Van Halteren et al (1998). 
In the first method each classification receives 
the same weight and the most frequent classifi- 
cation is chosen (Majority). The second nmthod 
regards as tile weight of each individual clas- 
sification algorithm its accuracy on solne part 
of the data, tile tuning data (TotPrecision). 
The third voting method computes the preci- 
sion of each assigned tag per classifer and uses 
this value as a weight for tile classifier in those 
cases that it chooses the tag (TagPrecision). 
The fourth method uses both the precision of 
each assigned tag and tile recall of the com- 
peting tags (Precision-Recall). Finally, tile fifth 
lnethod uses not only a weight for tile current 
classification but it also computes weights tbr 
other possible classifications. The other classi- 
fications are deternfined by exalnining the tun- 
860 
ing data and registering the correct wflues for 
(;very pair of classitier esults (pair-wise voting, 
see Van Halteren et al (1998) tbr an elaborate 
explanation). 
Apart from these five voting methods we have 
also processed the output streams with two clas- 
sifters: MBL and IG%'ee. This approach is 
called classifier stacking. Like Van Halteren et 
al. (1998), we have used diff'erent intmt ver- 
sions: olle containing only the classitier Otltl)ut 
and another containing both classifier outlmt 
and a compressed representation of the data 
item tamer consideration. \]?or the latter lmr- 
pose we have used the part-of-speech tag of the 
carrent word. 
3 Resul ts  4 
We want to find out whether system combi- 
nation could improve performmlce of baseNP 
recognition and, if this is the fact, we want to 
seJect the best confl)ination technique. For this 
lmrpose we have pertbrmed an experiment with 
sections 15-18 of the WSJ part of the Prom %'ee- 
bank as training data (211727 tokens) and sec- 
tion 21 as test data (40039 tokens). Like the 
data used by Ramshaw and Marcus (1995), this 
data was retagged by the Brill tagger in order 
to obtain realistic part-of  speech (POS) tags 5. 
The data was seglnente.d into baseNP parts and 
non-lmseNP t)arts ill a similar fitshion as the 
data used 1)y Ramshaw and Marcus (1995). Of 
the training data, only 90% was used for train- 
ing. The remaining 10% was used as laming 
data for determining the weights of the combi- 
nation techniques. 
D)r three classifiers (MBL, MaxEnt and 
IGTree) we haw; used system-internal coral)i- 
nation. These learning algorithms have pro- 
cessed five dittbrent representations of the out- 
put (IOB1, IOB2, IOE1, IOE2 and O-t-C) and 
the results have been combined with majority 
voting. The test data results can 1)e fimnd in 
Table 1. In all cases, the combined results were 
better than that of the best included system. 
Tile results of ALLiS, 05.0, MB SL and SNoW 
have tmen converted to the O and the C repre- 
4Detailed results of our experiments me available on 
http: / /lcg-www.uia.ae.be/-erikt /np('oml,i / 
SThe retagging was necessary to assure that the per- 
formance rates obtained here would be similar to rates 
obtained for texts for which no Treebank POS tags are 
available. 
section 21 
Classifier 
ALLiS 
05.0 
IGTree 
MaxEnt 
MBL 
MBSL 
SNoW 
Simple Voting 
Majority 
TotPrecision 
TagPrecision 
Precision-Recall 
0 
97.87% 
97.05% 
97.70% 
97.94% 
98.04% 
97.27% 
97.78% 
98.08% 
98.08% 
98.08% 
98.08% 
C FS=j 
98.08% 92.15 
97.76% 89.97 
97.99% 91.92 
98.24% 92.60 
98.20% 92.82 
97.66% 90.71 
97.68% 91.87 
98.21% 92.95 
98.21% 92.95 
98.21% 92.95 
98.21% 92.95 
Pairwise Voting 
TagPair 98.13% 98.23% 
Memory-Based 
Tags 98.24% 98.35% 
Tags 4- P()S 98.14% 98.33% 
Deeision Trees 
Tags 98.24% 98.35% 
Tags + POS 98.13% 98.32% 
93.07 
93.39 
93.24 
93.39 
93.21 
Table 2: Bracket accuracies and Ff~=l scores 
for section WSJ 21 of the Penn ~15'eebank with 
seve, n individual classifiers and combinations of 
them. Each combination t)erforms t)etter than 
its best individual me, tuber. The stacked classi- 
tiers without COllte, xt intbrmation perform best. 
sentation. Together with the bracket; ret)resen- 
tations of the other three techniques, this gave 
us a total of seven O results and seven C results. 
These two data streams have been combined 
with the combination techniques described in 
section 2.4. After this, we built baseNPs from 
the, O and C results of each combinatkm tech- 
nique, like, described in section 2.2. The bracket 
accuracies and tile F~=I scores tbr test data can 
be found in Table 2. 
All combinations iml)rove the results of the 
best individual classifier. The best results were 
obtained with a memory-based stacked classi- 
ter. This is different from the combination re- 
sults presented in Van Ilalteren et al (1998), 
in which pairwise voting pertbrmed best. How- 
eves, in their later work stacked classifiers out- 
perIbrm voting methods as well (Van Halteren 
et al, to appear). 
861 
section 20 accuracy precision recall 
Best-five combination 0:98.32% C:98.41% 94.18% 93.55% 
Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89% 
Mufioz et al (1999) O:98.1% C:98.2% 92.4% 93.1% 
Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27% 
Argamon et al (1999) - 91.6% 91.6% 
F/3=1 
93.86 
93.26 
92.8 
92.03 
91.6 
Table 3: The overall pertbrmance of the majority voting combination of our best five systems 
(selected on tinting data perfbrnmnce) applied to the standard data set pnt tbrward by Ramshaw 
and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how 
often a word was classified correctly with the representation used (O, C or IOB1). The combined 
system outperforms all earlier reported results tbr this data set. 
Based on an earlier combination study 
(Tjong Kim Sang, 2000) we had expected the 
voting methods to do better. We suspect hat 
their pertbrmance is below that of the stacked 
classifiers because the diflhrence between tile 
best and the worst individual system is larger 
than in our earlier study. We assume that the 
voting methods might perform better if they 
were only applied to the classifiers that per- 
form well on this task. In order to test this 
hypothesis, we have repeated the combination 
experiments with the best n classitiers, where 
n took vahms from 3 to 6 and the classifiers 
were ranked based on their performance on the 
tnning data. The t)est pertbrmances were ob- 
tained with five classifiers: F/~=1=93.44 for all 
five voting methods with tile best stacked classi- 
tier reaching 93.24. With the top five classifiers, 
tile voting methods outpertbrm the best; combi- 
nation with seven systems G. Adding extra clas- 
sification results to a good combination system 
should not make overall performance worse so 
it is clear that there is some room left for im- 
provement of our combination algorithms. 
We conclude that the best results ill this 
task can be obtained with tile simplest voting 
method, majority voting, applied to the best 
five of our classifiers. Our next task was to 
apply the combination apt)roach to a standard 
data set so that we could compare our results 
with other work. For this purpose we have used 
6V~re are unaware of a good method for determining 
the significance of F~=I differences but we assume that 
this F~=I difference is not significant. However, we be- 
lieve that the fact that more colnbination methods per- 
tbrm well, shows that it easier to get a good pertbrmmlce 
out of the best; five systems than with all seven. 
tile data put tbrward by ll,amshaw and Marcus 
(1995). Again, only 90% of the training data 
was used tbr training while the remaining 11)% 
was reserved tbr ranking the classifiers. The 
seven learners were trained with the same pa- 
rameters as in the previous experiment. Three 
of the classifiers (MBL, MaxEnt and iG%'ee) 
used system-internal combination by processing 
different output representations. 
The classifier output was converted to the 
O and the C representation. Based on the 
tuning data performance, the classifiers ALLiS, 
IGTREE, MaxEnt, MBL and SNoW were se- 
lected for being combined with majority vot- 
ing. After this, the resulting O and C repre- 
sentations were combined to baseNPs by using 
the method described in section 2.2. The re- 
sults can be found in Table 3. Our combined 
system obtains an F/~=I score of 93.86 which 
corresponds to an 8% error reduction compared 
with tile best published result tbr this data set 
(93.26). 
4 Conc lud ing  remarks  
In this paper we have examined two methods for 
combining the results of machine learuing algo- 
rithms tbr identii}cing base noun phrases. Ill the 
first Inethod, the learner processed ifferent out- 
put data representations and tile results were 
combined by majority voting. This approach 
yielded better results than the best included 
classifier. Ill the second combination approach 
we have combined the results of seven learning 
systems (ALLiS, c5.0, IGTree, MaxEnt, MBL, 
MBSL and SNOW). Here we have tested d i f  
ferent confl)ination methods. Each coilfl)ination 
862 
nmthod outt)erformed the best individual learn- 
ing algorithm and a majority vote of the tol) 
five systems peribrmed best. We, have at}i}lie, d 
this approach of system-internal nd system- 
external coral}|nation to a standard ata set for 
base noun phrase identification and the 1}ertbr- 
mance of our system was 1)etter than any other 
tmblished result tbr this data set. 
Our study shows that the c, omt)ination meth- 
(}{Is that we have tested are sensitive for the in- 
clusion of classifier esults of poor quality. This 
leaves room for imt)rovement of our results t}y 
evaluating other coml}inators. Another interest- 
ing apl)roach which might lead to a l}etter t)er- 
f{}rmance is taking into a{-com~t more context 
inibrmation, for example by coral)in|rig com- 
plete 1}hrases instead of indet}endent t}ra{:kets. 
It would also be worthwhile to evaluate using 
more elaborate me, thods lbr building baseNPs 
out of ot}en and close t}ra{:ket (:an{ti{tates. 
Acknowledgements  
l)djean, Koeling and 'l?jong Kim Sang are 
funded by the TMII. 1\]etwork Learning (Jompu- 
tational Grammars r. 1}unyakanok and Roth are 
SUl)t}orted by NFS grants IIS-98{}1638 an{t SBR- 
9873450. 
Re ferences  
Steven Alm{',y. 1996. Partial t)a\]'sing via finite- 
state cascades. In l'n, l}~wce, di'ngs of the /~,gS- 
LLI '95 l?,obust 1)arsi'n9 Worlcsh, op. 
SMomo Argam(m, Ido l)agan, an(l YllV~t\] Kry- 
molowsld. 1999. A memory-1}ased at}proach 
to learning shalh}w natural anguage patterns. 
Journal of E:rperimental and Th, eovetical AL 
11(3). 
Adam L. Berge, r, SteI}hen A. l)ellaPietra, and 
Vincent J. DellaPietra. 1996. A inaximum 
entrol)y apI)roach to natural language pro- 
cessing. Computational Linguistics, 22(1). 
Eric Bri\]l and ,lun Wu. 1998. Classifier com- 
bination tbr improved lexical disaml)iguation. 
In P~vccedings o.f COLING-A 6'15 '98. Associ- 
ation for Computational Linguistics. 
A. Carlson, C. Cunfl)y, J. Rosen, and 
D. l/,oth. 1.999. The SNoW learning archi- 
tecture. Technical Report UIUCDCS-11,-99- 
2101, UIUC Computer Science Department, 
May. 
r httl): / /lcg-www.ui',,.ac.be~/ 
Walter Daelemans, A.ntal van den Bosch, and 
Jakub Zavrel. 1999a. \])brgetting exceptions 
is harmflll in language learning. Machine 
Learning, 34(1). 
Walter Daelemans, Jakub Zavrel, Ko wmder  
Sloot, and Antal van den Bosch. 1999b. 
TiMBL: Tilb'arg Memory Bused Learner, ver- 
sion 2.G Rqfi;rence Guide. ILK Te(:hnical 
th',port 99-01. http:// i lk.kub.nl/.  
Luc Dehaspe. 1997. Maximum entropy model- 
ing with clausal constraints, in PTvcecdings oJ' 
th, c 7th, 1}l, ternational Workshop on ind'uctivc 
Logic Programming. 
Hervd Ddjean. 200(I. Theory refinement and 
natural language processing. In Proceedings 
of the ColingEO00. Association for Computa- 
tional Linguistics. 
Mitchell 17 }. Marcus, Beatrice Santorini, and 
Mary Aim Marcinkiewicz. 1993. Building a 
large mmotated corpus of english: the penn 
treebank. Computational Linguistics, 19(2). 
Marcia Munoz,  Vasin Punyakanok, l)an l l,oth, 
and Day Zimak. 1999. A learning ap- 
t}roa(:h to shallow t)arsing. In P~vceedings of 
EMNLP-WVLC'99.  Asso('iation for Coml)u- 
tational Linguisti(:s. 
J. Ross Quinlan. 1993. c/t.5: Programs for Ma- 
th,|he Learning. Morgan Kauflnann. 
Lance A. Ramshaw and Mitchell P. Marcus. 
1995. Text chunking using transformation- 
l)ase{t learn|Jig. In 1}roceeding s o\[ the Th, i'rd 
A CL Worksh, op on Ve, r~.l LacTic Corpora. As- 
sociation for Comlmtational Linguistics. 
D. Roth. 1.9!t8. Learning to resolve natural an- 
guage aml}iguities: A unified approach. In 
AAAL98.  
Erik F. Tjong Kim Sang. 2000. N{mn phrase 
recognition by system {:ombination. In Pro- 
ceedings of th, e ANLP-NAA CL-2000. Seattle, 
Washington, USA. Morgan Kauflnan Pub- 
lishers. 
Hans van Halteren, Jakub Zavrel, and Wal- 
ter Daelemans. 1998. Iml)roving data driven 
wordclass tagging by system corot}|nation. In
P~veeedings of COLING-ACL '98. Associa- 
tion tbr Computational Linguistics. 
Hans van Halteren, Jakub Zavrel, and Walter 
Daelemans. to appear, hnproving accuracy 
ill nlp through coati)|nation ofmachine learn- 
ing systems. 
863 
 	
	
Using the Distribution of Performance for Studying Statistical
NLP Systems and Corpora
Yuval Krymolowski
Department of Mathematics and Computer Science
Bar-Ilan University
52900 Ramat Gan, Israel
Abstract
Statistical NLP systems are fre-
quently evaluated and compared on
the basis of their performances on
a single split of training and test
data. Results obtained using a single
split are, however, subject to sam-
pling noise. In this paper we ar-
gue in favour of reporting a distri-
bution of performance figures, ob-
tained by resampling the training
data, rather than a single num-
ber. The additional information
from distributions can be used to
make statistically quantified state-
ments about differences across pa-
rameter settings, systems, and cor-
pora.
1 Introduction
The common practice in evaluating statistical
NLP systems is using a standard corpus (e.g.,
Penn TreeBank for parsing, Reuters for text
categorization) along with a standard split be-
tween training and test data. As systems im-
prove, it becomes harder to achieve additional
improvements, and the performance of vari-
ous state-of-the-art systems is approximately
identical. This makes performance compar-
isons difficult.
In this paper, we argue in favour of studying
the distribution of performance, and present
conclusions drawn from studying the recall
distribution. This distribution provides mea-
sures for answering the following questions:
Q1: Comparing systems on given data: Is
classifier A better than classifier B for
given training and test data?
Q2: Adequacy of training data to test data:
Is a system trained on dataset X ade-
quate for analysing dataset Y ? Are fea-
tures from X indicative in Y ?
Q3: Comparing data sets with a given sys-
tem: If a different training set improves
the result of system A on dataset Y1, will
this be the case on dataset Y2 as well?
The answers to these questions can provide
useful insight into statistical NLP systems.
In particular, about sensitivity to features in
the training data, and transferability. These
properties can be different even when similar
performance is reported.
A statistical treatment of Question 1 is
presented by Yeh (2000). He tests for the
significance of performance differences on
fixed training and test data sets. In other
related works, Martin and Hirschberg (1996)
provides an overview of significance tests
of error differences in small samples, and
Dietterich (1998) discusses results of a num-
ber of tests.
Questions 2 and 3 have been frequently
raised in NLP, but not explicitly addressed,
since the prevailing evaluation methods pro-
vide no means of addressing them. In this
paper we propose addressing all three ques-
tions with a single experimental methodology,
which uses the distribution of recall.
2 Motivation
Words, parts-of-speech (POS), words, or any
feature in text may be regarded as outcomes
of a statistical process. Therefore, word
counts, count ratios, and other data used in
creating statistical NLP models are statisti-
cal quantities as well, and as such prone to
sampling noise. Sampling noise results from
the finiteness of the data, and the particular
choice of training and test data.
A model is an approximation or a more ab-
stract representation of training data. One
may look at a model as a collection of es-
timators analogous, e.g., to the slope calcu-
lated by linear regression. These estimators
are statistics with a distribution related to the
way they were obtained, which may be very
complicated. The performance figures, being
dependent on these estimators, have a distri-
bution function which may be difficult to find
theoretically. This distribution gives rise to
intrinsic noise.
Performance comparisons based on a single
run or a few runs do not take these noises into
account. Because we cannot assign the result-
ing statements a confidence measure, they are
more qualitative than quantitative. The de-
gree to which we can accept such statements
depends on the noise level and more generally,
on the distribution of performance.
In this paper, we use recall as a perfor-
mance measure (cf. Section 4.4 and Section
3.2 in (Yeh, 2000)). Recall samples are ob-
tained by resampling from training data and
training classifiers on these samples.
The resampling methods used here are
cross-validation and bootstrap (Efron and
Gong, 1983; Efron and Tibshirani, 1993,
cf. Section 3). Section 4 presents the experi-
mental goals and setup. Results are presented
and discussed in Section 5, and a summary is
provided in Section 6.
3 The Bootstrap Method
The bootstrap is a re-sampling technique
designed for obtaining empirical distribu-
tions of estimators. It can be thought
of as a smoothed version of k-fold cross-
validation (CV). The method has been ap-
plied to decision tree and bayesian classifiers
by Kohavi (1995) and to neural networks by,
e.g., LeBaron and Weigend (1998).
In this paper, we use the bootstrap method
to obtain the distribution of performance of a
system which learns to identify non-recursive
noun-phrases (base-NPs). While there are a
few refinements of the method, the intention
of this paper is to present the benefits of ob-
taining distributions, rather than optimising
bias or variance. We do not aim to study the
properties of bootstrap estimation.
Let a statistic S = S(x1, . . . , xn) be a func-
tion of the independent observations {xi}ni=1
of a statistical variable X. The bootstrap
method constructs the distribution function
of S by successively re-sampling x with re-
placements.
After B samples, we have a set of bootstrap
samples {xb1, . . . , x
b
n}
B
b=1, each of which yields
an estimate S?b for S. The distribution of S? is
the bootstrap estimate for the distribution of
S. That distribution is mostly used for esti-
mating the standard deviation, bias, or confi-
dence interval of S.
In the present work, xi are the base-NP in-
stances in a given corpus, and the statistic S
is the recall on a test set.
4 Experimental Setup
The aim of our experiments is to test whether
the recall distribution can be helpful in an-
swering the questions Q1?Q3 mentioned in
the introduction of this paper.
The data and learning algorithms are pre-
sented in Sections 4.1 and 4.2. Section 4.3
describes the sampling method in detail. Sec-
tion 4.4 motivates the use of recall and de-
scribes the experiments.
4.1 Data
We used Penn-Treebank (Marcus et al, 1993)
data, presented in Table 1. Wall-Street Jour-
nal (WSJ) Sections 15-18 and 20 were used
by Ramshaw and Marcus (1995) as training
and test data respectively for evaluating their
base-NP chunker. These data have since be-
come a standard for evaluating base-NP sys-
tems.
The WSJ texts are economic newspaper
reports, which often include elaborated sen-
tences containing about six base-NPs on the
Source Sentences Words Base
NPs
WSJ 15-18 8936 229598 54760
WSJ 20 2012 51401 12335
ATIS 190 2046 613
WSJ 20a 100 2479 614
WSJ 20b 93 2661 619
Table 1: Data sources
average.
The ATIS data, on the other hand, are
a collection of customer requests related to
flight schedules. These typically include short
sentences which contain only three base-NPs
on the average. For example:
I have a friend living in Denver
that would like to visit me
here in Washington DC .
The structure of sentences in the ATIS data
differs significantly from that in the WSJ
data. We expect this difference to be reflected
in the recall of systems tested on both data
sets.
The small size of the ATIS data can influ-
ence the results as well. To distinguish the
size effect from the structural differences, we
drew two equally small samples from WSJ
Section 20. These samples, WSJ20a and
WSJ20b, consist of the first 100 and the fol-
lowing 93 sentences respectively. There is a
slight difference in size because sentences were
kept complete, as explained Section 4.3.
4.2 Learning Algorithms
We evaluated base-NP learning systems based
on two algorithms: MBSL (Argamon et al,
1999) and SNoW (Mun?oz et al, 1999).
MBSL is a memory-based system which
records, for each POS sequence containing a
border (left, right, or both) of a base-NP, the
number of times it appears with that border
vs. the number of times it appears without
it. It is possible to set an upper limit on the
length of the POS sequences.
Given a sentence, represented by a sequence
of POS tags, the system examines each sub-
sequence for being a base-NP. This is done
by attempting to tile it using POS sequences
that appeared in the training data with the
base-NP borders at the same locations.
For the purpose of the present work, suffice
it to mention that one of the parameters is
the context size (c). It denotes the maximal
number of words considered before or after a
base-NP when recording sub-sequences con-
taining a border.
SNoW (Roth, 1998, ?Sparse Network of
Winnow?) is a network architecture of Win-
now classifiers (Littlestone, 1988). Winnow
is a mistake-driven algorithm for learning a
linear separator, in which feature weights are
updated by multiplication. The Winnow al-
gorithm is known for being able to learn well
even in the presence of many noisy features.
The features consist of one to four consec-
utive POSs in a 3-word window around each
POS. Each word is classified as a beginning of
a base-NP, as an end, or neither.
4.3 Sampling Method
In generating the training samples we sampled
complete sentences. In MBSL, an un-marked
boundary may be counted as a negative ex-
ample for the POS-subsequences which con-
tains it. Therefore, sampling only part of the
base-NPs in a sentence will generate negative
examples.
For SNoW, each word is an example, but
most of the words are neither a beginning nor
an end of a base-NP. Random sampling of
words might generate a sample with an im-
proper balance between the three classes.
To avoid these problems, we sampled
full sentences instead of words or instances.
Within a good approximation, it can be as-
sumed that base-NP patterns in a sentence do
not correlate. The base-NP instances drawn
from the sampled sentences can therefore be
regarded as independent.
As described at the end of Sec. 4.1, the
WSJ20a and WSJ20b data were created so
that they contain 613 instances, like the ATIS
data. In practice, the number of instances
exceeds 613 slightly due to the full-sentence
constraint. For the purpose of this work, it is
enough that their size is very close to the size
of ATIS.
Dataset Sentences Base-NPs
Training 8938 ? 48 54763 ? 2
Unique: 5648 ? 34
Table 2: Sentence and instant counts for the
bootstrap samples. The second line refers to
unique sentences in the training data.
We used the WSJ15-18 dataset for train-
ing. This dataset contains n0 = 54760 base-
NP instances. The number of instances in a
bootstrap sample depends on the number of
instances in the last sampled sentence. As
Table 2 shows, it is slightly more than n0.
For k-CV sampling, the data were divided
into k random distinct parts, each containing
n0
k ?2 instances. Table 3 shows the number of
recall samples in each experiment (MBSL and
SNoW experiments were carried out seper-
ately).
Method MBSL SNoW
Bootstrap 2200 1000
CV (total folds) 1500 1000
Table 3: Number of bootstrap samples and
total CV folds.
4.4 Experiments
We trained SNoW and MBSL; the latter us-
ing context sizes of c=1 and c=3. Data sets
WSJ20, ATIS, WSJ20a, and WSJ20b were
used for testing. MBSL runs with the two
c values were conducted on the same training
samples, therefore it is possible to compare
their results directly.
Each run yielded recall and precision. Re-
call may be viewed as the expected 0-1 loss-
function on the given test sample of instances.
Precision, on the other hand, may be viewed
as the expected 0-1 loss on the sample of in-
stances detected by the learning system. Care
should be taken when discussing the distribu-
tion of precision values because this sample
varies from run to run. We will therefore only
analyse the distribution of recall in this work.
In the following, r1 and r3 denote recall
samples of MBSL with c = 1 and c = 3,
with standard deviations ?1 and ?3. ?13 de-
notes the cross-correlation between r1 and r3.
SNoW recall and standard deviation will be
denoted by rSN and ?SN.
To approach the questions raised in the in-
troduction we made the following measure-
ments:
Q1: System comparison was addressed by
comparing r1 and r3 on the same test data.
With samples at hand, we obtained an esti-
mate of P (r3 > r1).
Q2: We studied training and test adequacy
through the effect of more specific features on
recall, and on its standard deviation.
Setting c = 3 takes into account sequences
with context of two and three words in ad-
dition to those with c = 1. Sequences with
larger context are more specific, and an im-
provement in recall implies that they are in-
formative in the test data as well.
For particular choices of parameters and
test data, the recall spread yields an estimate
of the training sampling noise. On inade-
quate data, where the statistics differ signif-
icantly from those in the training data, even
small changes in the model can lead to a no-
ticeable difference in recall. This is because
the model relies on statistics which appear
relatively rarely in the test data. Not only
do these statistics provide little information
about the problem, but even small differences
in weighting them are relatively influential.
Therefore, the more training and test data
differ from each other, the more spread we can
expect in results.
Q3: For comparing test data sets with a
system, we used cross-correlations between r1,
r3, or rSN samples obtained on these data sets.
We know that WSJ data are different from
ATIS data, and so expect the results on WSJ
to correlate with ATIS results less than with
other WSJ results.
5 Results and Discussion
For each of the five test datasets, Table 4 re-
ports averages and standard deviations of r1,
r3, and rSN obtained by 3, 5, 10, and 20-fold
cross-validation, and by bootstrap. ?13 and
P (r3 > r1) are reported as well.
We discuss our results by considering to
what extent they provide information for an-
swering the three questions:
Q1 ? Comparing systems on given data:
For the WSJ data sets, the difference between
r3 and r1 was well above their standard de-
viations, and r3 > r1 nearly always. For
ATIS, the standard deviation of the differ-
ence (?2r3?r1 = (?
1)2 + (?3)2 ? 2?1?3 ? ?13)
was small due to the high ?13, and r1 > r3
nearly always.
Q2 ? The adequacy of training and test
sets: It is clear that adding more specific
features, by increasing the context, improved
recall on the WSJ test data and degraded it
on the ATIS data. This is likely to be an indi-
cation of the difference in syntactic structure
between ATIS and WSJ texts.
Another evidence of structural difference
comes from standard deviations. The spread
of the ATIS results always exceeded that of
the WSJ results, with all three experiments.
That difference cannot be solely attributed
to the small size of ATIS, since WSJ20a
and WSJ20b results displayed a much smaller
spread. Indeed, these results had a wider
standard deviation than WSJ20, probably
due to the smaller size, but not as wide as
ATIS. This indicates that base-NPs in ATIS
text have different characteristics than those
in WSJ texts.
Q3 ? Comparing datasets by a system:
Table 5 reports, for each pair of datasets, the
correlation between the 5-fold CV recall sam-
ples of each experiment on these datasets.
The correlations change with CV fold num-
ber, 5-fold results were chosen as they repre-
sent intermediary values.
Both MBSL experiments yielded negligible
correlations of ATIS results with any WSJ
data set, whether large or small. These corre-
lations were always weaker than with WSJ20a
and WSJ20b, which are about the same size.
This is due to ATIS being a different kind
of text. The correlation between WSJ20a and
WSJ20b results was also weak. This may be
due to their small sizes; these texts might not
share enough features to make a significant
correlation.
SNoW results were highly correlated for all
pairs. That behaviour is markedly different
from the MBSL results, and indicates a high
level of noise in the SNoW features. Indeed,
Winnow is able to learn well in the presence
of noise, but that noise causes the high corre-
lations observed here.
5.1 Further Observations
The decrease of ?13 with CV fold number is
related to stabilization of the system. As the
folds become larger, training samples become
more similar to each other, and the spread of
results decreases. This effect was not visible
in the SNoW data, most likely due to the high
level of noise in the features. This noise also
contributes to the higher standard deviation
of SNoW results.
6 Summary and Further Research
In this work, we used the distribution of re-
call to address questions concerning base-NP
learning systems and corpora. Two of these
questions, of training and test adequacy, and
of comparing data sets using NLP systems,
were not addressed before.
The recall distributions were obtained using
CV and bootstrap resampling.
We found differences between algorithms
with similar recall, related to the features they
use.
We demonstrated that using an inadequate
test set may lead to noisy performance results.
This effect was observed with two different
learning algorithms. We also reported a case
when changing a parameter of a learning al-
gorithm improved results on one dataset but
degraded results on another.
We used classifiers as ?similarity rulers?,
for producing a similarity measure between
datasets. Classifiers may have various prop-
erties as similarity rulers, even when their re-
calls are similar. Each classifier should be
scaled differently according to its noise level.
This demonstrates the way we can use clas-
sifiers to study data, as well as use data to
study classifiers.
Test Method MBSL SNoW
data E(r1)? ?1 E(r3)? ?3 ?13 P (r3 > r1) E(rSN)? ?SN
3-CV 89.64 ? 0.16 91.26 ? 0.12 0.36 100% 90.18 ? 1.01
5-CV 89.75 ? 0.14 91.43 ? 0.10 0.30 100% 90.37 ? 1.03
WSJ 20 10-CV 89.80 ? 0.12 91.53 ? 0.08 0.25 100% 90.47 ? 1.11
20-CV 89.81 ? 0.11 91.56 ? 0.07 0.28 100% 90.51 ? 1.19
Bootstrap 89.58 ? 0.17 91.16 ? 0.14 0.42 100% 89.83 ? 0.93
E(?) 89.74 91.58 91.23
3-CV 85.70 ? 2.03 83.99 ? 1.87 0.82 3% 83.70 ? 4.11
5-CV 85.76 ? 1.87 83.69 ? 1.57 0.79 1% 83.53 ? 4.52
ATIS 10-CV 85.90 ? 1.31 84.78 ? 0.92 0.78 4% 83.38 ? 5.14
20-CV 85.78 ? 1.16 83.28 ? 0.85 0.77 0% 83.23 ? 5.36
Bootstrap 85.72 ? 1.95 84.69 ? 1.95 0.81 16% 83.50 ? 3.35
E(?) 85.81 83.20 85.48
3-CV 89.45 ? 0.42 91.25 ? 0.56 0.33 100% 90.84 ? 1.04
5-CV 89.66 ? 0.36 91.64 ? 0.54 0.32 100% 91.07 ? 1.15
WSJ 20a 10-CV 89.79 ? 0.28 91.85 ? 0.49 0.20 100% 91.14 ? 1.26
20-CV 89.82 ? 0.23 91.89 ? 0.44 0.18 100% 91.11 ? 1.39
Bootstrap 89.42 ? 0.47 91.55 ? 0.57 0.33 99% 90.76 ? 1.00
E(?) 89.73 92.18 90.07
3-CV 88.95 ? 0.41 90.12 ? 0.39 0.37 99% 89.79 ? 0.81
5-CV 89.03 ? 0.36 90.15 ? 0.31 0.31 99% 89.81 ? 0.84
WSJ 20b 10-CV 89.06 ? 0.33 90.14 ? 0.22 0.28 99% 89.83 ? 0.86
20-CV 89.07 ? 0.27 90.13 ? 0.18 0.22 100% 89.87 ? 0.88
Bootstrap 89.00 ? 0.44 90.17 ? 0.44 0.38 98% 89.93 ? 0.80
E(?) 89.01 91.55 90.79
Table 4: Recall statistic summary for MBSL with contexts c = 1 and c = 3, and SNoW. The
E(?) figures were obtained using the full training set. Note the monotonic change of standard
deviation with fold number. The s.d. of the bootstrap samples are closest to those of low-fold
CV samples.
5-CV WSJ 20b WSJ 20a ATIS
r1 r3 rSN r1 r3 rSN r1 r3 rSN
WSJ 20 0.33 0.19 0.72 0.26 0.29 0.78 0.08 0.02 0.76
ATIS -0.01 0.00 0.59 0.02 -0.01 0.63
WSJ 20a 0.07 0.04 0.59
Table 5: Cross-correlations between recalls of the three experiments on the test data for five-fold
CV. Correlations of r1 capture dataset similarity in the best way.
By using MBSL with different context sizes,
our results provide insights into the relation
between training and test data sets, in terms
of general and specific features. That issue be-
comes important when one plans to use a sys-
tem trained on certain data set for analysing
an arbitrary text. Another approach to this
topic, examining the effect of using lexical
bigram information, which is very corpus-
specific, appears in (Gildea, 2001).
In our experiments with systems trained on
WSJ data, there was a clear difference be-
tween their behaviour on other WSJ data and
on the ATIS data set, in which the structure
of base-NPs is different. That difference was
observed with correlations and standard devi-
ations. This shows that resampling the train-
ing data is essential for noticing these struc-
ture differences.
To control the effect of small size of the
ATIS dataset, we provided two equally-small
WSJ data sets. The effect of different genres
was stronger than that of the small-size.
In future study, it would be helpful to study
the distribution of recall using training and
test data from a few genres, across genres,
and on combinations (e.g. ?known-similarity
corpora? (Kilgarriff and Rose, 1998)). This
will provide a measure of the transferability
of a model.
We would like to study whether there is a
relation between bootstrap and 2 or 3-CV re-
sults. The average number of unique base-
NPs in a random bootstrap training sample
is about 63% of the total training instances
(Table 2). That corresponds roughly to the
size of a 3-CV training sample. More work is
required to see whether this relation between
bootstrap and low-fold CV is meaningful.
We also plan to study the distribution of
precision. As mentioned in Sec. 4.4, the pre-
cisions of different runs are now taken from
different sample spaces. This makes the boot-
strap estimator unsuitable, and more study is
required to overcome this problem.
References
S. Argamon, I. Dagan, and Y. Krymolowski. 1999.
A memory-based approach to learning shallow
natural language patterns. Journal of Experi-
mental and Theoretical AI, 11:369?390. CMP-
LG/9806011.
T. G. Dietterich. 1998. Approximate statisti-
cal tests for comparing supervised classifica-
tion learning algorithms. Neural Computation,
10(7).
Bradley Efron and Gail Gong. 1983. A leisurely
look at the bootstrap, the jackknife, and cross-
validation. Am. Stat., 37(1):36?48.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and
Hall, New York.
Daniel Gildea. 2001. Corpus variation and parser
performance. In Proc. 2001 Conf. on Empir-
ical Methods in Natural Language Processing
(EMNLP?2001), Carnegie Mellon University,
Pittsburgh, June. ACL-SIGDAT.
Adam Kilgarriff and Tony Rose. 1998. Mea-
sures for corpus similarity and homogeneity. In
Proc. 3rd Conf. on Empirical Methods in Nat-
ural Language Processing (EMNLP?3), pages
46?52, Granada, Spain, June. ACL-SIGDAT.
Ron Kohavi. 1995. A study of cross-validation
and bootstrap for accuracy estimation and
model selection. In proceedings of the Inter-
national Joint Conference on Artificial Intelli-
gence, pages 1137?1145.
B. LeBaron and A. S. Weigend. 1998. A boot-
strap evaluation of the effect of data splitting
on financial time series. IEEE Transactions on
Neural Networks, 9(1):213?220, January.
N. Littlestone. 1988. Learning quickly when
irrelevant attributes abound: A new linear-
threshold algorithm. Machine Learning, 2:285?
318.
M. P. Marcus, B. Santorini, and
M. Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313?330,
June.
J. Martin and D. Hirschberg. 1996. Small sample
statistics for classification error rates II: Confi-
dence intervals and significance tests. Technical
report, Dept. of Information and Computer Sci-
ence, University of California, Irvine. Technical
Report 96-22.
M. Mun?oz, V. Punyakanok, D. Roth, and D. Zi-
mak. 1999. A learning approach to shallow
parsing. In EMNLP-VLC?99, the Joint SIG-
DAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Cor-
pora, pages 168?178, June.
L. A. Ramshaw and M. P. Marcus. 1995. Text
chunking using transformation-based learning.
In Proceedings of the Third Workshop on Very
Large Corpora.
D. Roth. 1998. Learning to resolve natural lan-
guage ambiguities: A unified approach. In
proc. of the Fifteenth National Conference on
Artificial Intelligence, pages 806?813, Menlo
Park, CA, USA, July. AAAI Press.
Alexander Yeh. 2000. More accurate tests for
the statistical significance of result differences.
In 18th International Conference on Computa-
tional Linguistics (COLING), pages 947?953,
July.
 
	 	
	
ffProceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 449?456
Manchester, August 2008
The Choice of Features for Classification of Verbs in Biomedical Texts
Anna Korhonen
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
alk23@cl.cam.ac.uk
Yuval Krymolowski
Dept. of Computer Science
Haifa University
Israel
yuvalkry@gmail.com
Nigel Collier
National Institute of Informatics
Hitotsubashi 2-1-2
Chiyoda-ku, Tokyo 101-8430
Japan
collier@nii.ac.jp
Abstract
We conduct large-scale experiments to in-
vestigate optimal features for classification
of verbs in biomedical texts. We intro-
duce a range of feature sets and associated
extraction techniques, and evaluate them
thoroughly using a robust method new to
the task: cost-based framework for pair-
wise clustering. Our best results compare
favourably with earlier ones. Interestingly,
they are obtained with sophisticated fea-
ture sets which include lexical and seman-
tic information about selectional prefer-
ences of verbs. The latter are acquired au-
tomatically from corpus data using a fully
unsupervised method.
1 Introduction
Recent years have seen a massive growth in the
scientific literature in the domain of biomedicine.
Because future research in the biomedical sciences
depends on making use of all this existing knowl-
edge, there is a strong need for the development of
natural language processing (NLP) tools which can
be used to automatically locate, organize and man-
age facts related to published experimental results.
Major progress has been made on information
retrieval and on the extraction of specific rela-
tions (e.g. between proteins and cell types) from
biomedical texts (Ananiadou et al, 2006). Other
tasks, such as the extraction of factual information,
remain a bigger challenge.
Researchers have recently begun to use deeper
NLP techniques (e.g. statistical parsing) for im-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
PROTEINS: p53
p53
Tp53
Dmp53
...
ACTIVATE
suggestsdemonstrates
indicatesimplies...
GENES: WAF1
WAF1CIP1p21
...
It
INDICATE
that
activates
up-regulates
induces
stimulates...
...
Figure 1: Sample lexical classes
proved processing of the challenging linguistic
structures (e.g. complex nominals, modal subordi-
nation, anaphoric links) in biomedical texts. For
optimal performance, many of these techniques
require richer syntactic and semantic informa-
tion than is provided by existing domain lexicons
(e.g. UMLS metathesaurus and lexicon
1
). This par-
ticularly applies to verbs, which are central to the
structure and meaning of sentences.
Where the information is absent, lexical classi-
fication can compensate for it, or aid in obtaining
it. Lexical classes which capture the close rela-
tion between the syntax and semantics of verbs
provide generalizations about a range of linguis-
tic properties (Levin, 1993). For example, con-
sider the INDICATE and ACTIVATE verb classes in
Figure 1. Their members have similar subcatego-
rization frames SCFs (e.g. activate / up-regulate /
induce / stimulate NP) and selectional preferences
(e.g. activate / up-regulate / induce / stimulate
GENES:WAF1), and they can be used to make sim-
ilar statements describing similar events (e.g. PRO-
TEINS:P53 ACTIVATE GENES:WAF1).
Lexical classes can be used to abstract away
from individual words, or to build a lexical or-
ganization which predicts much of the behaviour
of a new word by associating it with an appro-
priate class. They have proved useful for various
NLP application tasks, e.g. parsing, word sense dis-
1
http://www.nlm.nih.gov/research/umls
449
ambiguation, semantic role labeling, information
extraction, question-answering, machine transla-
tion (Dorr, 1997; Prescher et al, 2000; Swier
and Stevenson, 2004; Dang, 2004; Shi and Mi-
halcea, 2005). A large-scale classification spe-
cific to the biomedical data could support key BIO-
NLP tasks such as anaphora resolution, predicate-
argument identification, event extraction and the
identification of biomedical (e.g. interaction) rela-
tions. However, no such classification is available.
Recent research shows that it is possible to auto-
matically induce lexical classes from corpora with
promising accuracy (Schulte im Walde, 2006; Joa-
nis et al, 2007; Sun et al, 2008). A number of
machine learning (ML) methods have been applied
to classify mainly syntactic features (e.g. subcat-
egorization frames (SCFs)) extracted from cross-
domain corpora using e.g. part-of-speech tagging
or robust statistical parsing techniques. Korho-
nen et al (2006) have recently applied such an
approach to biomedical texts. Their preliminary
experiment shows encouraging results but further
work is required before such an approach can be
used to benefit practical BIO-NLP.
We conduct a large-scale investigation to find
optimal features for biomedical verb classification.
We introduce a range of theoretically-motivated
feature sets and evaluate them thoroughly using
a robust method new to the task: a cost-based
framework for pairwise clustering. Our best re-
sults compare favourably with earlier ones. Inter-
estingly, they are obtained using feature sets which
have proved challenging in general language verb
classification: ones which incorporate information
about selectional preferences of verbs. Unlike in
earlier work, we acquire the latter from corpus data
using a fully unsupervised method.
We present our lexical classification approach in
section 2 and data in section 3. Experimental eval-
uation is reported in section 4. Section 5 provides
discussion and section 6 concludes.
2 Approach
Our lexical classification approach involves (i) ex-
tracting features from corpus data and (ii) cluster-
ing them. These steps are described in the follow-
ing two sections, respectively.
2.1 Features
Lexical classifications are based on diathesis alter-
nations which manifest in alternating sets of syn-
tactic frames (Levin, 1993). Most verb classifi-
cation approaches have therefore employed shal-
low syntactic slots or SCFs as basic features. Some
have supplemented them with further information
about verb tense, voice, and/or semantic selec-
tional preferences on argument heads.
2
The preliminary experiment on biomedical verb
classification (Korhonen et al, 2006) employed
basic syntactic features only: SCFs extracted
from corpus data using the system of Briscoe
and Carroll (1997) which operates on the output
of a domain-independent robust statistical parser
(RASP) (Briscoe and Carroll, 2002). Because such
deep syntactic features seem ideally suited for
challenging biomedical data, we adopted the same
basic approach, but we designed and extracted a
range of novel feature sets which include addi-
tional syntactic and semantic information.
The SCF extraction system assigns each occur-
rence of a verb in the parsed data as a member of
one of the 163 verbal SCFs, builds a lexical entry
for each verb (type) and SCF combination, and fil-
ters noisy entries out of the lexicon. We do not
employ the filter in our work because its primary
aim is to filter out SCFs containing adjuncts (as op-
posed to arguments). Adjuncts have been shown
to be beneficial for general language verb classifi-
cation (Sun et al, 2008; Joanis et al, 2007) and
particularly meaningful in biomedical texts (Co-
hen and Hunter, 2006).
The lexical entries provide various information
useful for verb classification, including e.g. the fre-
quency of the entry in the data, the part-of-speech
(POS) tags of verb tokens, the argument heads in
argument positions, the prepositions in PP frames,
and the number of verbal occurrences in active and
passive. Making use of this information we de-
signed ten feature sets for experimentation.
The first three feature sets F1-F3 include basic
SCF frequency information for each verb:
F1: SCFs and their relative frequencies. The SCFs
abstract over lexically governed particles and
prepositions.
F2: F1 with two high frequency PP frames pa-
rameterized for prepositions: the simple PP
and NP-PP frames refined according to the
prepositions provided in the lexical entries
(e.g. PP at, PP on, PP in).
2
See section 5 for discussion on previous work.
450
F3: F2 with 13 additional high frequency PP
frames parameterized for prepositions.
Although prepositions are an important part of
the syntactic description of lexical classes and
therefore F3 should be the most informative fea-
ture set, we controlled the number of PP frames
parameterized for prepositions to examine the ef-
fect of sparse data in automatic classification.
F4-F7 build on the most refined SCF-based fea-
ture set F3, supplementing it with information
about verb tense (F4-F5) and voice (F6-F7):
F4: The frequencies of POS tags (e.g. VVD for
activated) calculated over all the SCFs of the
verb.
F5: The frequencies of POS tags calculated spe-
cific to each SCF of the verb.
F6: The frequency of the active and passive oc-
currences of the verb (calculated over all the
SCFs of the verb).
F7: The frequency of the active and passive occur-
rences of the verb (calculated specific to each
SCF of the verb).
Also F8-F10 build on feature set F3. They sup-
plement it with information about lexical or se-
mantic selectional preferences (SPs) of the verbs
in the following slots: subject, direct object, sec-
ond object, and the NP within the PP complement.
The SPs are acquired using argument head data in
the ten most frequent SCFs. We use two baseline
methods (F8 and F9) which employ raw data and
one method based on clustering (F10):
F8: The raw argument head types are considered
as SP classes.
F9: Only those raw argument head types which
occur with four or more verbs with frequency
of ? 3 are considered as SP classes.
F10: SPs are acquired by clustering those argu-
ment heads which occur with ten or more
verbs with frequency of ? 3. We used the PC
clustering method described below in section
2. The number of clusters K
np
was set to 10,
20, and 50 to produce SP classes. We call the
feature sets corresponding to these different
values of K
np
F10A, F10B and F10C, respec-
tively. Since the clustering algorithms have
an element of randomness, clustering was ran
100 times. The output is a result of voting
among the outputs of the runs.
F3-F10 are entirely novel feature sets in biomed-
ical verb classification. Variations of some of them
have been used in earlier work on general language
classification (see section 5 for details).
2.2 Classification
The clustering method which proved the best in the
preliminary experiment on biomedical verb classi-
fication was Information Bottleneck (IB) (Tishby
et al, 1999). We compare this method against a
probabilistic method: a cost-based framework for
pairwise clustering (PC) (Puzicha et al, 2000).
2.2.1 Information Bottleneck
IB is an information-theoretic method which
controls the balance between: (i) the loss of
information by representing verbs as clusters
(I(Clusters;V erbs)), which has to be min-
imal, and (ii) the relevance of the output
clusters for representing the SCF distribution
(I(Clusters; SCFs)) which has to be maximal.
The balance between these two quantities ensures
optimal compression of data through clusters. The
trade-off between the two constraints is realized
through minimising the cost function:
L
IB
= I(Clusters;V erbs)
? ?I(Clusters; SCFs) ,
where ? is a parameter that balances the con-
straints. IB takes three inputs: (i) SCF-verb -based
distributions, (ii) the desired number of clusters K,
and (iii) the initial value of ?. It then looks for the
minimal ? that decreases L
IB
compared to its value
with the initial ?, using the given K. IB delivers as
output the probabilities p(K|V ).
2.2.2 Pairwise Clustering
PC is a method where a cost criterion guides
the search for a suitable clustering configuration.
This criterion is realized through a cost function
H(S,M) where
(i) S = {sim(a, b)}, a, b ? A : a collection of pairwise
similarity values, each of which pertains to a pair of
data elements a, b ? A.
(ii) M = (A
1
, . . . , A
k
) : a candidate clustering configu-
ration, specifying assignments of all elements into the
disjoint clusters (that is ?A
j
= A and A
j
? A
j
?
= ?
for every 1 ? j < j
?
? k).
451
1 Have an effect on activity (BIO/29) 9 Report (GEN/30)
1.1 Activate / Inactivate 9.1 Investigate
1.1.1 Change activity: activate, inhibit 9.1.1 Examine: evaluate, analyze
1.1.2 Suppress: suppress, repres s 9.1.2 Establish: test, investigate
1.1.3 Stimulate: stimulate 9.1.3 Confirm: verify, determine
1.1.4 Inactivate: delay, diminish 9.2 Suggest
1.2 Affect 9.2.1 Presentational:
1.2.1 Modulate: stabilize, modulate hypothesize, conclude
1.2.2 Regulate: control, support 9.2.2 Cognitive:
1.3 Increase / decrease: increase, decrease consider, believe
1.4 Modify: modify, catalyze 9.3 Indicate: demonstrate, imply
Table 1: Sample classes from the gold standard
Journal Years Words
Genes & Development 2003-5 4.7M
Journal of Biological Chemistry 2004 5.2M
(Vol.1-9)
The Journal of Cell Biology 2003-5 5.6M
Cancer Research 2005 6.5M
Carcinogenesis 2003-5 3.4M
Nature Immunology 2003-5 2.3M
Drug Metabolism and Disposition 2003-5 2.3M
Toxicological Sciences 2003-5 3.1M
Total: 33.1M
Table 2: Data from MEDLINE
The cost function is defined as follows:
H = ?
P
n
j
?Avgsim
j
,
Avgsim
j
=
1
n
j
?(n
j
?1)
P
{a,b?A
j
}
sim(a, b)
where n
j
is the size of the j
th
cluster and Avgsim
j
is the average similarity between cluster members.
We used the Jensen-Shannon divergence (JS) as the
similarity measure.
3 Data
3.1 Test Verbs and Gold Standard
We employed in our experiments the same gold
standard as earlier employed by Korhonen et al
(2006). This three level gold standard was created
by a team of human experts: 4 domain experts and
2 linguists. It includes 192 test verbs (typically fre-
quent verbs in biomedical journal articles) classi-
fied into 16, 34 and 50 classes, respectively. The
classes created by domain experts are labeled as
BIO and those created by linguists as GEN. BIO
classes include 116 verbs whose analysis required
domain knowledge (e.g. activate, solubilize, har-
vest). GEN classes include 76 general or scientific
text verbs (e.g. demonstrate, hypothesize, appear).
Each class is associated with 1-30 member verbs.
Table 1 illustrates two of the gold standard classes
with 1-2 example verbs per (sub-)class.
3.2 Test Data
We downloaded the data from the MEDLINE
database, from eight journals covering various ar-
SCF F1 98 39
F2 247 64
F3 486 75
F3 + tense F4 490 79
F5 920 176
F3 + voice F6 488 77
F7 682 153
F3 + SP F8 150407 2112
F9 13352 344
F10A 110280 2091
F10B 115208 2091
F10C 114793 2091
Table 3: (i) The total number of features and (ii)
the average per verb for all the feature sets
eas of biomedicine. The first column in table 2
lists each journal, the second shows the years from
which the articles were downloaded, and the third
indicates the size of the data. We experimented
with two test sets: 1) The 15.5M word sub-set
shown in the first three rows of the table (this was
used for creating the gold standard). 2) All the
data: this new larger data was necessary for exper-
iments with new feature sets as the most refined
ones do not appear in 1) with sufficient frequency.
4 Experimental Evaluation
4.1 Processing the Data
The data was first processed using the feature ex-
traction module. Table 3 shows (i) the total num-
ber of features in each feature set and (ii) the av-
erage per verb in the resulting lexicon. The clas-
sification module was then applied. We requested
K = 2 to 60 clusters from both clustering meth-
ods. We did not want to enforce the actual num-
ber of classes but preferred to let the class hierar-
chy emerge from the clustering results. In order
to find the values of K where the clustering output
might correspond to a level in the class hierarchy
we used the relevance criterion. For each method
(clustering method and feature set combination)
we choose as informative K?s the values for which
the relevance information I(Clusters; SCFs)) in-
creases more sharply between K?1 and K clusters
than between K and K+1. We then chose for eval-
uation the outputs corresponding only to informa-
tive values of K. The clustering was run 50 times
for each method. The output is a result of voting
among the outputs of the runs.
4.2 Measures
The clusters were evaluated against the gold stan-
dard using four methods. The first measure, the
452
adjusted pairwise precision, evaluates clusters in
terms of verb pairs:
APP =
1
K
K
P
i=1
num. of correct pairs in k
i
num. of pairs in k
i
?
|k
i
|?1
|k
i
|+1
APP is the average proportion of all within-
cluster pairs that are correctly co-assigned. Mul-
tiplied by a factor that increases with cluster size it
compensates for a bias towards small clusters.
The second measure is modified purity, a global
measure which evaluates the mean precision of
clusters. Each cluster is associated with its preva-
lent class. The number of verbs in a cluster K that
take this class is denoted by n
prevalent
(K). Verbs
that do not take it are considered as errors. Clusters
where n
prevalent
(K) = 1 are disregarded as not to
introduce a bias towards singletons:
mPUR =
P
n
prevalent
(k
i
)?2
n
prevalent
(k
i
)
number of verbs
The third measure is the weighted class accu-
racy, the proportion of members of dominant clus-
ters DOM-CLUST
i
within all classes c
i
.
ACC =
C
P
i=1
verbs in DOM-CLUST
i
number of verbs
mPUR can be seen to measure the precision of
clusters and ACC the recall. We define an F mea-
sure as the harmonic mean of mPUR and ACC:
F =
2 ?mPUR ? ACC
mPUR + ACC
The experiments were run 50 times on each in-
put to get the distribution of performance due to
the randomness in the initial clustering. We calcu-
lated the average performance and standard devia-
tion from the results of these runs.
4.3 Results for Test Set 1
We first compared IB and PC on the smaller test set
1 using feature set F2. We chose for evaluation the
outputs corresponding to the most informative val-
ues of K: 20, 33, 53 for IB, and 19, 26, 51 for PC.
In the results included in table 4 IB shows slightly
better performance than PC, but the difference is
not significant for K=34 and 50. We decided to use
PC for larger experiments because it has two ad-
vantages over IB: 1) It can cluster the large test set
2 with K = 10 ? 60 in minutes, while IB requires
a day for this. 2) It can deal with (and combine)
different feature sets, while IB runs into numeri-
cal problems. Due to its speed and flexibility PC
is thus more suitable for larger-scale experiments
involving comparison of complex feature sets.
4.4 Results for Test Set 2
Tables 5 and 6 include the PC results on the larger
test set 2. Table 5 shows the results for each in-
dividual feature set (indicated in the second col-
umn). It shows also the standard deviations (?
avg
)
of the four performance measures averaged across
all the runs. These are very similar for 16, 34, and
50 classes and hence only included in one of the
columns. In addition, ?
diff
is indicated. This is
?
2 ? ?
avg
and used for calculating the significance
of the performance differences. In the following
discussion we consider a difference of more than
2?
diff
(p > 97.7%) as significant.
The first feature sets F1-F3 include basic SCF
(frequency) information for each verb, F2-F3 re-
fined with prepositions. F2 shows clearly better
results than F1 (over 10 F-measure) at all the levels
of gold standard. This demonstrates the usefulness
of prepositions for the task. When moving to F3
the performance decreases for 34 and 50 classes,
while improving for 16 classes, but these differ-
ences are not statistically significant.
Feature sets F4-F10 build on F3. F4-F5 include
information about verb tense. This information
proves quite useful for verb classification, partic-
ularly when specific to individual SCFs. When
compared against the baseline featureset F3, F5
is clearly better - particularly at 50 classes where
the difference is 3.9 in F-measure (2?
diff
). Verb
voice information is not equally helpful: F6-F7 are
not better than F3. In some comparisons they are
worse, e.g. F7 vs. F3 at 16 classes.
F8-F10 supplement F3 with information about
SPs. Surprisingly, these lexical and semantic fea-
tures prove the most useful for our task. At the
level of 34 and 50 classes, the best SP features are
even better than the best tense features (the dif-
ference is statistically significant), and they yield
notable improvement over the baseline features
(e.g. 6.8 difference in F-measure between F9 and
F3). The performance is not equally good at 16
classes. This makes perfect sense because class
members are unlikely to have similar SPs at such a
coarse level of semantic classification.
When comparing the five sets of SPs features
against each other, F9 and F10C produce the best
results at 34 and 50 classes. F9 uses raw (filtered)
argument head data for SP acquisition while F10C
uses clustering. It is interesting that the differ-
ence between these two very different methods is
not statistically significant. Whether one employs
453
16 Classes 34 Classes 50 Classes
APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
IB 74 77 66 71 69 75 81 77 54 72 79 75
PC 71 78 58 67 64 71 81 75 63 71 73 72
? 1.1 1.0 1.0 0.8 1.8 1.6 1.3 1.4 2.1 1.5 1.6 1.1
Table 4: Performance on test set 1
16 Classes 34 Classes 50 Classes
APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
SCF F1 62.7 68.2 54.6 60.6 50.4 58.4 53.4 55.8 41.5 50.3 55.7 52.9
F2 68.7 76.4 66.4 71.1 61.9 65.5 65.8 65.6 53.9 61.2 65.4 63.2
F3 69.3 77.7 67.6 72.3 61.6 66.0 64.0 65.0 53.7 60.2 65.9 62.9
F3 + tense F4 70.1 77.5 65.5 71.0 62.0 70.3 69.4 69.8 53.3 60.6 68.0 64.1
F5 68.5 75.4 71.7 73.5 61.9 67.8 68.2 68.0 58.2 62.7 71.7 66.8
F3 + voice F6 70.6 78.1 64.0 70.4 61.2 66.0 65.8 65.9 54.3 59.6 70.1 64.4
F7 74.0 79.5 59.7 68.2 62.6 65.4 65.1 65.2 55.1 60.9 69.2 64.7
F3 + SP F8 77.1 78.2 61.6 68.9 69.6 69.3 71.2 70.2 61.3 62.7 71.1 66.6
F9 72.4 77.1 64.0 69.9 72.2 72.0 71.6 71.8 62.3 65.6 72.4 68.8
F10A 75.6 80.0 63.2 70.6 66.1 69.2 70.6 69.9 59.4 63.5 69.0 66.2
F10B 68.8 77.1 69.2 72.9 65.3 67.2 69.8 68.5 59.9 61.9 70.5 65.9
F10C 74.1 78.9 65.7 71.7 68.8 71.7 69.7 70.7 59.8 63.4 71.1 67.0
?
avg
2.2 1.5 1.8 1.4
?
diff
3.1 2.1 2.5 2.0
Table 5: Performance on test set 2: PC clustering results for individual feature sets at the three levels of
gold standard. ?
avg
and ?
diff
were calculated across all the three classification levels.
16 CL. F5+F9 F4+ F10C F5 F5+ F8
APP 72.3 68.2 68.5 72.2
mPUR 76.4 77.0 75.4 76.5
ACC 73.6 70.9 71.7 69.9
F 75.0 73.8 73.5 73.0
34 CL. F5+ F9 F5+ F8 F9 F4+ F10A
APP 68.7 71.0 72.2 62.9
mPUR 70.1 71.0 72.0 68.4
ACC 74.8 73.4 71.6 75.0
F 72.4 72.2 71.8 71.5
50 CL. F9 F5+ F9 F5+ F8 F4+ F9
APP 62.3 59.8 62.8 59.7
mPUR 65.6 63.8 64.1 63.1
ACC 72.4 72.7 71.0 71.8
F 68.8 68.0 67.4 67.1
Table 6: Results for the top four feature set combi-
nations. All the feature sets build on F3.
fine grained clusters (F10C) or coarse-grained ones
(F10A) as SPs does not make much difference.
We next combined various feature sets. Table 6
shows the performance for the top four combina-
tions. Comparing these results against the ones in
Table 5, (see the ?
diff
values in Table 5) we can see
that combining feature sets does not result in better
performance
3
. The only exception is the difference
in APP and mPUR between F9 and F4 + F10A at
N=34. However, these results show similar ten-
dencies as the earlier ones: at 16 classes the most
3
Recall that all F4-F10 are actually already ?combined?
with F3 - we do not refer to this combination here.
useful features are based on verb tense, while at 34
and 50 classes they are based on SPs.
5 Discussion
The results presented in the previous section are
in interesting contrast with those reported in ear-
lier work. In previous work on general lan-
guage verb classification, syntactic features (slots
or SCFs) have proved generally the most help-
ful features, e.g. (Schulte im Walde, 2006; Joa-
nis et al, 2007). The preliminary experiment on
biomedical verb classification (Korhonen et al,
2006) experimented only with them. In our ex-
periments, SCFs proved useful baseline features.
When we refined them further, we faced sparse
data problems: considerable improvement was ob-
tained when moving from F1 to F2, but not when
moving to F3. Although many verb classes are
sensitive to preposition types, many of the types
are low in frequency. Future work could address
this problem by employing smoothing techniques,
or backing off to preposition classes.
Joanis et al (2007) experimented with tense
and voice -based features in general English verb
classification. They offered no significant im-
provement over basic syntactic features. Also in
our experiments, we obtained little improvement
with voice features. This could be due to the
454
un-distinctiveness of passive in biomedical texts
where it is used typically with high frequency.
However, tense-based features clearly improved
the baseline performance in our experiments. This
could be partly because we ?parameterize? POS in-
formation for SCFs, and partly because semanti-
cally similar verbs in biomedical language tend to
behave similarly also in terms of tense (Friedman
et al, 2002).
Joanis (2002) and Schulte im Walde (2006) used
SP-based features in general English and German
verb classifications, respectively. The former ac-
quired them from WordNet (Miller, 1990) and
the latter from GermaNet (Kunze, 2000). Joa-
nis (2002) obtained no improvement over syntactic
features while Schulte im Walde (2006) obtained,
but the improvement was not significant. In our
experiments, SP features gave the best results and
the clearest improvement over the baseline features
at the finer-grained levels of classification where
class members are indeed likely to be the most uni-
form in terms of their SPs.
We obtained this improvement despite using
a fully unsupervised approach to SP acquisition.
We did not exploit lexical resources like Joa-
nis (2002) and Schulte im Walde (2006) because
it would have required combining general re-
sources (e.g. WordNet) with domain specific ones
(e.g. UMLS). We opted for a simpler approach in
this initial work ? using raw argument heads and
clustering ? and obtained surprisingly good results.
In our experiments filtering of raw argument heads
and clustering with N=50 produced equivalent re-
sults, suggesting that relatively fine-grained clus-
ters are optimal. Future work will require quali-
tative analysis of noun clusters and comparison of
these against classes in lexical resources to deter-
mine an optimal method for SP acquisition.
Does the fact that we obtain good results with
features which have not proved helpful in general
language classification indicate a need for domain-
specific feature engineering? We do not believe
so. The feature sets we experimented with are the-
oretically well-motivated and should, in principle,
also aid general language verb classification. We
believe they proved helpful in our experiments be-
cause being domain-specific, biomedical language
is conventionalised and therefore less varied in
terms of verb sense and usage than general lan-
guage. For example, verbs have stronger SPs for
their argument heads when many of their corpus
occurrences are of similar sense. This renders SP-
based features more useful for classification.
Due to differences in the data, methods, and ex-
perimental setup, direct comparison of our perfor-
mance figures with previously published ones is
difficult. The closest comparison point with gen-
eral language is (Korhonen et al, 2003) which re-
ported 59% mPUR using IB to assign 110 polyse-
mous English verbs into 34 classes. Our best re-
sults are substantially better (72-80% mPUR). It
is encouraging that we obtained such good results
despite focusing on a linguistically challenging do-
main.
In addition to the points mentioned earlier, our
future plans include seeding automatic classifica-
tion with more sophisticated information acquired
automatically from domain-specific texts (e.g. us-
ing named entity recognition and anaphoric link-
ing (Vlachos et al, 2006)). We will also explore
semi-automatic ML technology and active learn-
ing in aiding the classification. Finally, we plan to
conduct a bigger experiment with a larger number
of verbs, make the resulting classification publicly
available, and demonstrate its usefulness for prac-
tical BIO-NLP application tasks.
6 Conclusion
We reported large-scale experiments to investigate
the optimal characteristics of features required for
biomedical verb classification. A range of feature
sets and associated extraction methods were intro-
duced for this work, along with a robust cluster-
ing method capable of dealing with large data and
complex feature sets. A number of experiments
were reported. The best performing feature sets
proved to be the ones which include information
about SCFs supplemented with information about
verb tense and SPs in particular. The latter were
acquired automatically from corpus data using an
unsupervised method. Similar feature sets have
not proved equally useful in earlier work in gen-
eral language verb classification. We discussed
reasons for this and highlighted several areas for
future work.
Acknowledgement
Work on this paper was funded by the Royal So-
ciety, EPSRC (?ACLEX? project, GR/T19919/01)
and MRC (?CRAB? project, G0601766), UK.
455
References
Ananiadou, S., B. D. Kell, and J. Tsujii. 2006. Text
mining and its potential applications in systems biol-
ogy. Trends in Biotechnology, 24(12):571?579.
Briscoe, E. J. and J. Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In 5
th
ACL
Conference on Applied Natural Language Process-
ing, pages 356?363, Washington DC.
Briscoe, E. J. and J. Carroll. 2002. Robust accurate
statistical annotation of general text. In 3
rd
Interna-
tional Conference on Language Resources and Eval-
uation, pages 1499?1504, Las Palmas, Gran Canaria.
Cohen, K. B. and L. Hunter. 2006. A critical review of
PASBio?s argument structures for biomedical verbs.
BMC Bioinformatics, 7(3).
Dang, H. T. 2004. Investigations into the Role of Lexi-
cal Semantics in Word Sense Disambiguation. Ph.D.
thesis, CIS, University of Pennsylvania.
Dorr, B. J. 1997. Large-scale dictionary construction
for foreign language tutoring and interlingual ma-
chine translation. Machine Translation, 12(4):271?
322.
Friedman, C., P. Kra, and A. Rzhetsky. 2002. Two
biomedical sublanguages: a description based on the
theories of zellig harris. Journal of Biomedical In-
formatics, 35(4):222?235.
Joanis, E., S. Stevenson, and D. James. 2007. A gen-
eral feature space for automatic verb classification.
Natural Language Engineering.
Joanis, E. 2002. Automatic verb classification using a
general feature space. Master?s thesis, University of
Toronto.
Korhonen, A., Y. Krymolowski, and N. Collier. 2006.
Automatic classification of verbs in biomedical texts.
In ACL-COLING, Sydney, Australia.
Kunze, C. 2000. Extension and use of germanet,
a lexical-semantic database. In 2nd International
Conference on Language Resources and Evaluation,
Athens, Greece.
Levin, B. 1993. English Verb Classes and Alterna-
tions. Chicago University Press, Chicago.
Miller, G. A. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography,
3(4):235?312.
Prescher, D., S. Riezler, and M. Rooth. 2000. Using
a probabilistic class-based lexicon for lexical am-
biguity resolution. In 18th International Confer-
ence on Computational Linguistics, pages 649?655,
Saarbr?ucken, Germany.
Puzicha, J., T. Hofmann, and J. M. Buhmann. 2000.
A theory of proximity-based clustering: structure
detection by optimization. Pattern Recognition,
33(4):617?634.
Schulte im Walde, S. 2006. Experiments on the au-
tomatic induction of german semantic verb classes.
Computational Linguistics, 32(2):159?194.
Shi, L. and R. Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Proceedings of
the Sixth International Conference on Intelligent Text
Processing and Computational Linguistics, Mexico
City, Mexico.
Sun, L., A. Korhonen, and Y. Krymolowski. 2008.
Verb class discovery from rich syntactic data. In
9th International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, Haifa, Is-
rael.
Swier, R. and S. Stevenson. 2004. Unsupervised se-
mantic role labelling. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 95?102, Barcelona, Spain,
August.
Tishby, N., F. C. Pereira, and W. Bialek. 1999. The
information bottleneck method. In Proc. of the
37
th
Annual Allerton Conference on Communica-
tion, Control and Computing, pages 368?377.
Vlachos, A., C. Gasperin, I. Lewin, and E. J. Briscoe.
2006. Bootstrapping the recognition and anaphoric
linking of named entitites in drosophila articles. In
Pacific Symposium in Biocomputing, Maui, Hawaii.
456
Automatic Classification of English Verbs Using Rich Syntactic Features
Lin Sun and Anna Korhonen
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
ls418,alk23@cl.cam.ac.uk
Yuval Krymolowski
Department of Computer Science
University of Haifa
31905, Haifa
Israel
yuvalkry@gmail.com
Abstract
Previous research has shown that syntactic
features are the most informative features
in automatic verb classification. We exper-
iment with a new, rich feature set, extracted
from a large automatically acquired subcate-
gorisation lexicon for English, which incor-
porates information about arguments as well
as adjuncts. We evaluate this feature set us-
ing a set of supervised classifiers, most of
which are new to the task. The best classi-
fier (based on Maximum Entropy) yields the
promising accuracy of 60.1% in classifying
204 verbs to 17 Levin (1993) classes. We
discuss the impact of this result on the state-
of-art, and propose avenues for future work.
1 Introduction
Recent research shows that it is possible, using cur-
rent natural language processing (NLP) and machine
learning technology, to automatically induce lex-
ical classes from corpus data with promising ac-
curacy (Merlo and Stevenson, 2001; Korhonen et
al., 2003; Schulte im Walde, 2006; Joanis et al,
2007). This research is interesting, since lexi-
cal classifications, when tailored to the application
and domain in question, can provide an effective
means to deal with a number of important NLP
tasks (e.g. parsing, word sense disambiguation, se-
mantic role labeling), as well as enhance perfor-
mance in many applications (e.g. information ex-
traction, question-answering, machine translation)
(Dorr, 1997; Prescher et al, 2000; Swier and Steven-
son, 2004; Dang, 2004; Shi and Mihalcea, 2005).
Lexical classes are useful because they capture
generalizations over a range of (cross-)linguistic
properties. Being defined in terms of similar mean-
ing components and (morpho-)syntactic behaviour
of words (Jackendoff, 1990; Levin, 1993) they
generally incorporate a wider range of properties
than e.g. classes defined solely on semantic grounds
(Miller, 1990). They can be used to build a lexical
organization which effectively captures generaliza-
tions and predicts much of the syntax and semantics
of a new word by associating it with an appropriate
class. This can help compensate for lack of data for
individual words in NLP.
Large-scale exploitation of lexical classes in real-
world or domain-sensitive tasks has not been pos-
sible because existing manually built classifications
are incomprehensive. They are expensive to extend
and do not incorporate important statistical infor-
mation about the likelihood of different classes for
words. Automatic classification is a better alterna-
tive. It is cost-effective and gathers statistical infor-
mation as a side-effect of the acquisition process.
Most work on automatic classification has fo-
cussed on verbs which are typically the main pred-
icates in sentences. Syntactic features have proved
the most informative in verb classification. Exper-
iments have been reported using both (i) deep syn-
tactic features (e.g. subcategorization frames (SCFs))
extracted using parsers and subcategorisation acqui-
sition systems (Schulte im Walde, 2000; Korhonen
et al, 2003; Schulte im Walde, 2006) and (ii) shal-
low ones (e.g. NPs/PPs preceding/following verbs)
extracted using taggers and chunkers (Merlo and
Stevenson, 2001; Joanis et al, 2007).
769
(i) correspond closely with features used for
manual classification (Levin, 1993). They have
proved successful in the classification of German
(Schulte im Walde, 2006) and English verbs (Ko-
rhonen et al, 2003). Yet promising results have also
been reported when using (ii) for English verb clas-
sification (Merlo and Stevenson, 2001; Joanis et al,
2007). This may indicate that (i) are optimal for the
task when combined with additional syntactic infor-
mation from (ii).
We investigate this matter by experimenting with
a new, rich feature set which incorporates informa-
tion about SCFs (arguments) as well as adjuncts. It
was extracted from VALEX, a large automatically
acquired SCF lexicon for English (Korhonen et al,
2006). We evaluate the feature set thoroughly us-
ing set of supervised classifiers, most of which are
new in verb classification. The best performing clas-
sifier (Maximum Entropy) yields the accuracy of
60.1% on classifying 204 verbs into 17 Levin (1993)
classes. This result is good, considering that we per-
formed no sophisticated feature engineering or se-
lection based on the properties of the target classi-
fication (Joanis et al, 2007). We propose various
avenues for future work.
We introduce our target classification in section 2
and syntactic features in section 3. The classifica-
tion techniques are presented in section 4. Details
of the experimental evaluation are supplied in sec-
tion 5. Section 6 provides discussion and concludes
with directions for future work.
2 Test Verbs and Classes
We adopt as a target classification Levin?s (1993)
well-known taxonomy where verbs taking similar
diathesis alternations are assumed to share meaning
components and are organized into a semantically
coherent class. For instance, the class of ?Break
Verbs? (class 45.1) is partially characterized by its
participation in the following alternations:
1. Causative/inchoative alternation:
Tony broke the window ? The window broke
2. Middle alternation:
Tony broke the window ? The window broke easily
3. Instrument subject alternation:
Tony broke the window with the hammer ? The hammer
broke the window
LEVIN CLASS EXAMPLE VERBS
9.1 PUT bury, place, install, mount, put
10.1 REMOVE remove, abolish, eject, extract, deduct
11.1 SEND ship, post, send, mail, transmit
13.5.1 GET win, gain, earn, buy, get
18.1 HIT beat, slap, bang, knock, pound
22.2 AMALGAMATE contrast, match, overlap, unite, unify
29.2 CHARACTERIZE envisage, portray, regard, treat, enlist
30.3 PEER listen, stare, look, glance, gaze
31.1 AMUSE delight, scare, shock, confuse, upset
36.1 CORRESPOND cooperate, collide, concur, mate, flirt
37.3 MANNER OF shout, yell, moan, mutter, murmur
SPEAKING
37.7 SAY say, reply, mention, state, report
40.2 NONVERBAL smile, laugh, grin, sigh, gas
EXPRESSION
43.1 LIGHT EMISSION shine, flash, flare, glow, blaze
45.4 CHANGE OF STATE soften, weaken, melt, narrow, deepen
47.3 MODES OF BEING quake, falter, sway, swirl, teeter
WITH MOTION
51.3.2 RUN swim, fly, walk, slide, run
Table 1: Test classes and example verbs
Alternations are expressed as pairs of SCFs. Addi-
tional properties related to syntax, morphology and
extended meanings of member verbs are specified
with some classes. The taxonomy provides a classi-
fication of 4,186 verb senses into 48 broad and 192
fine-grained classes according to their participation
in 79 alternations involving NP and PP complements.
We selected 17 fine-grained classes and 12 mem-
ber verbs per class (table 2) for experimentation.
The small test set enabled us to evaluate our results
thoroughly. The classes were selected to (i) include
both syntactically and semantically similar and dif-
ferent classes (to vary the difficulty of the classifi-
cation task), and to (ii) have enough member verbs
whose predominant sense belongs to the class in
question (we verified this according to the method
described in (Korhonen et al, 2006)). As VALEX
was designed to maximise coverage most test verbs
had 1000-9000 occurrences in the lexicon.
3 Syntactic Features
We employed as features distributions of SCFs spe-
cific to given verbs. We extracted them from the re-
cent VALEX (Korhonen et al, 2006) lexicon which
provides SCF frequency information for 6,397 En-
glish verbs. VALEX was acquired automatically
from five large corpora and the Web (using up to
10,000 occurrences per verb) using the subcatego-
rization acquisition system of Briscoe and Carroll
(1997). The system incorporates RASP, a domain-
independent robust statistical parser (Briscoe and
770
Carroll, 2002), and a SCF classifier which iden-
tifies 163 verbal SCFs. The basic SCFs abstract
over lexically-governed particles and prepositions
and predicate selectional preferences.
We used the noisy unfiltered version of VALEX
which includes 33 SCFs per verb on average1. Some
are genuine SCFs but some express adjuncts (e.g.
I sang in the party could be SCF PP). A lexical
entry for each verb and SCF combination provides
e.g. the frequency of the entry (in active and passive)
in corpora, the POS tags of verb tokens, the argument
heads in argument positions, and the prepositions in
PP slots. We experimented with three feature sets:
1. Feature set 1: SCFs and their frequencies
2. Feature set 2: Feature set 1 with two high frequency
PP frames parameterized for prepositions: the simple PP
(e.g. they apologized to him) and NP-PP (e.g. he removed
the shoes from the bag) frames.
3. Feature set 3: Feature set 2 with three additional high
frequency PP frames parameterized for prepositions: the
NP-FOR-NP (e.g. he bought a book for him), NP-TO-NP
(e.g. he gave a kiss to her), and OC-AP, EQUI, AS (e.g. he
condemned him as stupid) frames.
In feature sets 2 and 3, 2-5 PP SCFs were refined ac-
cording to the prepositions provided in the VALEX
SCF entries (e.g. PP at, PP on, PP in) because Levin
specifies prepositions with some SCFs / classes. The
scope was restricted to the 3-5 highest ranked PP
SCFs to reduce the effects of sparse data.
4 Classification
4.1 Preparing the Data
A feature vector was constructed for each verb.
VALEX includes 107, 287 and 305 SCF types for fea-
ture sets 1, 2, and 3, respectively. Each feature corre-
sponds to a SCF type, and its value is the relative fre-
quency of the SCF with the verb in question. Some
of the feature values are zero, because most verbs
take only a subset of the possible SCFs.
4.2 Machine Learning Methods
We implemented three methods for classification:
the K nearest neighbours (KNN), support vector ma-
chines (SVM), and maximum entropy (ME). To our
knowledge, only SVM has been previously used for
1The SCF accuracy of this lexicon is 23.7 F-measure, see
(Korhonen et al, 2006) for details.
verb classification. The free parameters were opti-
mised for each feature set by (i) defining the value
range (as explained below), and by (ii) searching for
the optimal value on the training data using 10 fold
cross validation (section 5.2).
4.2.1 K Nearest Neighbours
KNN is a memory-based classification method
based on the distances between verbs in the feature
space. For each verb in the test data, we measure
its distance to each verb in the training data. The
verb class label is the most frequent label in the
top K closest training verbs. We use the entropy-
based Jensen-Shannon (JS) divergence as the dis-
tance measure:
JS(P,Q) = 12
?D(P?P+Q2 ) +D(Q?P+Q2 )
?
The range of the parameter K is 2-20.
4.2.2 Support Vector Machines
SVM (Vapnik, 1995) tries to find a maximal mar-
gin hyperplane to separate between two groups of
verb feature vectors. In practice, a linear hyperplane
does not always exist. SVM uses a kernel function
to map the original feature vectors to higher dimen-
sion space. The ?maximal margin? optimizes our
choice of dimensionality to avoid over-fitting. We
use Chang and Lin (2001) ?s LIBSVM library to im-
plement the SVM. Following Hsu et al (2003), we
use the radial basis function as the kernel function:
K(xi, xj) = exp (??||xi ? xj ||2), ? > 0
? and the cost of the error term C (the penalty for
margin errors) are optimized. The search ranges of
Hsu et al (2003) are used:
C = 2?5, 2?3, . . . , 215, 217 ; ? = 2?17, 2?15, . . . , 21, 23
4.2.3 Maximum Entropy
ME constructs a probabilistic model that maxi-
mizes entropy on test data subject to a set of feature
constraints. If verb x is in class 10.1 and takes the
SCF 49 (NP-PP) with the relative frequency of 0.6 in
feature function f , we have
f(x, y) = 0.6 if y = 10.1 and x = 49
The expected value of a feature f with respect to the
empirical distribution (training data) is
E?(f) ?Px,y p?(x, y)f(x, y)
The expected value of the feature f (on test data)
with respect to the model p(y|x) is
771
E(f) ?Px,y p?(x)p(y|x)f(x, y)
p?(x) is the empirical distribution of x in the train-
ing data. We constrain E(f) to be the same as E?(f)
E(f) = E?(f)
The model must maximize the entropy H(Y |X)
H(Y |X) ? ?Px,y p?(x)p(y|x) log p(y|x)
The constraint-optimization problem is solved by
the Lagrange multiplier (Pietra et al, 1997). We
used Zhang (2004)?s maximum entropy toolkit for
implementation. The number of iterations i (5-50)
of the parameter estimation algorithm is optimised.
5 Experiments
5.1 Methodology
We split the data into training and test sets using two
methods. The first is ?leave one out? cross-validation
where one verb in each class is held out as test data,
and the remaining N-1 (i.e. 11) verbs are used as
training data. The overall accuracy is the average
accuracy of N rounds. The second method is re-
sampling. For each class, 3 verbs are selected ran-
domly as test data, and 9 are used as training data.
The process is repeated 30 times, and the average
result is recorded.
5.2 Measures
The methods are evaluated using first accuracy ? the
percentage of correct classifications out of all the
classifications:
Accuracy = truePositivestruePositives+falseNegatives
When evaluating the performance at class level, pre-
cision and recall are calculated as follows:
Precision = truePositivestruePositives+falsePositives
Recall = truePositivestruePositives+falseNegatives
F-score is the balance over recall and precision. We
report the average F-score over the 17 classes. Given
there are 17 classes in the data, the accuracy of ran-
domly assigning a verb into one of the 17 classes is
1/17 ? 5.8%.
5.3 Results from Quantitative Evaluation
Table 2 shows the average performance of each clas-
sifier and feature set according to ?leave one out?
cross-validation2. Each classifier performs consid-
erably better than the random baseline. The simple
2Recall is not shown as it is identical here with accuracy.
KNN method produces the lowest accuracy (44.1-
54.9) and SVM and ME the best (47.1-57.9 and 47.5-
59.3, respectively).
The performance of all methods improves sharply
when moving from the feature set 1 to the refined
feature set 2: both accuracy and F-measure improve
by over 10%. When moving from feature set 2 to
the sparser feature set 3 (which includes a higher
number of low frequency PP features) KNN worsens
clearly (c. 5% in accuracy and F-measure) while the
improvement in other methods is very small. This
suggests that KNN deals worse than other methods
with sparse data.
The resampling results in table 3 reveal that some
classifiers perform worse than others when less
training data is available3. KNN produces consid-
erably lower results, particularly with the sparse
feature set 3: 28.2 F-measure vs. 48.2 with cross-
validation. Also SVM performs worse with fea-
ture set 3: 54.6 F-measure vs. 58.2 with cross-
validation. ME thus appears the most robust method
with smaller training data, producing results compa-
rable with those in cross-validation.
Figure 1 shows the F-measure for 17 individual
classes when the methods are used with feature set
3. Levin classes 40.2, 29.2, and 37.3 (see table 2)
(the ones taking fewer prepositions with higher fre-
quency) have the best average performance (65% or
more), and classes 47.3, 45.4 and 18.1 the worst
(40% or less). ME outperforms SVM with 9 of the
17 classes.
5.4 Qualitative Evaluation
We did some qualitative analysis to trace the ori-
gin of error types produced by ME with feature set
3. Examination of the worst performing class 47.3
(MODES OF BEING INVOLVING MOTION verbs) il-
lustrates well the various error types. 10 of the 12
verbs in this class are classified incorrectly:
? 3 in class 43.1 (LIGHT EMISSION verbs): Verbs in 47.3
and 43.1 describe intrinsic properties of their subjects
(e.g. a jewel sparkles, a flag flutters). Their similar al-
ternations and PP SCFs make it difficult to separate them
on syntactic grounds.
? 2 in class 51.3.2 (RUN verbs): 47.3 and 51.3.2 share the
meaning component of motion. Their members take sim-
ilar alternations and SCFs, which causes the confusion.
3Recall that the amount of training data is smaller with re-
sampling evaluation, see section 5.2.
772
Feature set 1 Feature set 2 Feature set 3
ACC P F ACC P F ACC P F
RAND 5.8 5.8 5.8
KNN 44.1 48.4 44.0 54.9 56.9 53.9 49.5 47.0 48.2
ME 47.5 49.4 47.6 59.3 61.4 59.9 59.3 61.9 60.0
SVM 47.1 50.4 47.8 57.8 59.4 57.9 57.8 60.1 58.2
Table 2: ?Leave one out? cross-validation results for KNN, ME, and SVM
Feature set 1 Feature set 2 Feature set 3
ACC P F ACC P F ACC P F
RAND 5.8 5.8 5.8
KNN 37.3 39.9 36.5 42.7 47.2 42.6 27.1 34.2 28.2
ME 47.1 47.3 47.0 58.1 59.1 58.1 60.1 60.5 59.8
SVM 47.3 50.2 47.7 56.8 59.5 57.1 54.4 56.5 54.6
Table 3: Re-sampling results for KNN, ME, and SVM
? 2 in class 37.7 (SAY verbs) and 1 in class 37.3 (MANNER
OF SPEAKING verbs): 47.3 differs in semantics and syn-
tax from 37.7 and 37.3. The confusion is due to idiosyn-
cratic properties of individual verbs (e.g. quake, wiggle).
? 1 in class 36.1 (CORRESPOND verbs): 47.3 and 36.1 are
semantically very different, but their members take simi-
lar intransitive and PP SCFs with high frequency.
? 1 in class 45.4 (OTHER CHANGE OF STATE verbs):
Classes 47.3 and 45.3 are semantically different. Their
similar PP SCFs explains the misclassification.
Most errors concern classes which are in fact se-
mantically related. Unfortunately there is no gold
standard which would comprehensively capture the
semantic relatedness of Levin classes. Other er-
rors concern semantically unrelated but syntactically
similar classes ? cases which we may be able to ad-
dress in the future with careful feature engineering.
Some errors relate to syntactic idiosyncracy. These
show the true limits of lexical classification - the fact
that the correspondence between the syntax and se-
mantics of verbs is not always perfect.
6 Discussion and Conclusion
Our best results (e.g. 60.1 accuracy and 59.8 F-
measure of ME) are good, considering that no so-
phisticated feature engineering / selection based on
the properties of the target classification was per-
formed in these experiments. The closest compari-
son point is the recent experiment reported by Joanis
et al (2007) which involved classifying 835 English
verbs to 14 Levin classes using SVM. Features were
specifically selected via analysis of alternations that
are used to characterize Levin classes. Both shal-
low syntactic features (syntactic slots obtained us-
ing a chunker) and deep ones (SCFs extracted using
Briscoe and Carroll?s system) were used. The accu-
racy was 58% with the former and only 38% with
the latter. This experiment is not directly compa-
rable with ours as we classified a smaller number
of verbs (204) to a higher number of Levin classes
(17) (i.e. we had less training data) and did not se-
lect the optimal set of features using Levin?s alter-
nations. We nevertheless obtained better accuracy
with our best performing method, and better accu-
racy (47%) with the same method (SVM) when the
comparable feature set 1 was acquired using the very
same subcategorization acquisition system.
It is likely that using larger and noisier SCF data
explains the better result, suggesting that rich syn-
tactic features incorporating information about both
arguments and adjuncts are ideal for verb classifica-
tion. Further experiments are required to determine
the optimal set of features. In the future, we plan
to experiment with different (noisy and filtered) ver-
sions of VALEX and add to the comparison a shal-
lower set of features (e.g. NP and PP slots in VALEX
regardless of the specific SCFs). We will also im-
prove the features e.g. by enriching them with addi-
tional syntactic information available in VALEX lex-
ical entries.
Acknowledgement
This work was partially supported by the Royal So-
ciety, UK.
773
Figure 1: Class level F-score for feature set 3 (cross-validation)
References
E. J. Briscoe and J. Carroll. 1997. Automatic extraction
of subcategorization from corpora. In Proceedings of
the 5th ACL Conference on Applied Natural Language
Processing, pages 356?363, Washington DC.
E. J. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the
3rd LREC, pages 1499?1504, Las Palmas, Gran Ca-
naria.
C. Chang and J. Lin, 2001. LIBSVM: a library for sup-
port vector machines.
H. T. Dang. 2004. Investigations into the Role of Lexi-
cal Semantics in Word Sense Disambiguation. Ph.D.
thesis, CIS, University of Pennsylvania.
B. J. Dorr. 1997. Large-scale dictionary construction
for foreign language tutoring and interlingual machine
translation. Machine Translation, 12(4):271?322.
W. Hsu, C. Chang, and J. Lin. 2003. A practical guide to
support vector classification.
R. Jackendoff. 1990. Semantic Structures. MIT Press,
Cambridge, Massachusetts.
E. Joanis, S. Stevenson, and D. James. 2007. A general
feature space for automatic verb classification. Natu-
ral Language Engineering, Forthcoming.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003.
Clustering polysemic subcategorization frame distri-
butions semantically. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 64?71.
A. Korhonen, Y. Krymolowski, and T. Briscoe. 2006.
A large subcategorization lexicon for natural language
processing applications. In Proceedings of LREC.
B. Levin. 1993. English Verb Classes and Alternations.
Chicago University Press, Chicago.
P. Merlo and S. Stevenson. 2001. Automatic verb clas-
sification based on statistical distributions of argument
structure. Computational Linguistics, 27(3):373?408.
G. A. Miller. 1990. WordNet: An on-line lexi-
cal database. International Journal of Lexicography,
3(4):235?312.
S. D. Pietra, J. D. Pietra, and J. D. Lafferty. 1997. Induc-
ing features of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 19(4):380?
393.
D. Prescher, S. Riezler, and M. Rooth. 2000. Using a
probabilistic class-based lexicon for lexical ambiguity
resolution. In 18th International Conference on Com-
putational Linguistics, pages 649?655, Saarbru?cken,
Germany.
S. Schulte im Walde. 2000. Clustering verbs semanti-
cally according to their alternation behaviour. In Pro-
ceedings of COLING, pages 747?753, Saarbru?cken,
Germany.
S. Schulte im Walde. 2006. Experiments on the au-
tomatic induction of german semantic verb classes.
Computational Linguistics, 32(2):159?194.
L. Shi and R. Mihalcea. 2005. Putting pieces together:
Combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Proceedings of the Sixth In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics, Mexico City, Mexico.
R. Swier and S. Stevenson. 2004. Unsupervised seman-
tic role labelling. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 95?102, Barcelona, Spain, August.
V. N. Vapnik. 1995. The nature of statistical learning
theory. Springer-Verlag New York, Inc., New York,
NY, USA.
L. Zhang, 2004. Maximum Entropy Modeling Toolkit for
Python and C++, December.
774
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
page 165, New York City, June 2006. c?2006 Association for Computational Linguistics
The Treebanks Used in the Shared Task
This page contains references to all the treebanks
used in the CoNLL-X shared task. This page should
be consulted whenever a shared task paper refers to
a treebank without including the actual reference.
References
A. Abeille?, editor. 2003. Treebanks: Building and Us-
ing Parsed Corpora, volume 20 of Text, Speech and
Language Technology. Kluwer Academic Publishers,
Dordrecht.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Flo-
resta sinta?(c)tica?: a treebank for Portuguese. In Proc.
of the Third Intern. Conf. on Language Resources and
Evaluation (LREC), pages 1698?1703.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annota-
tion process in the Turkish treebank. In Proc. of the 4th
Intern. Workshop on Linguistically Interpreteted Cor-
pora (LINC).
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
M. Civit, Ma A. Mart??, B. Navarro, N. Bufi, B. Ferna?ndez,
and R. Marcos. 2003. Issues in the syntactic annota-
tion of Cast3LB. In Proc. of the 4th Intern. Workshop
on Linguistically Interpreteted Corpora (LINC).
M. Civit Torruella and Ma A. Mart?? Anton??n. 2002. De-
sign principles for a Spanish treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. ?Zabokrtsky, and A. ?Zele. 2006. Towards a Slovene
dependency treebank. In Proc. of the Fifth Intern.
Conf. on Language Resources and Evaluation (LREC).
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese treebank in VERBMOBIL. Verbmobil-
Report 240, Seminar fu?r Sprachwissenschaft, Univer-
sita?t Tu?bingen.
M. T. Kromann. 2003. The Danish dependency treebank
and the underlying linguistic theory. In Proc. of the
Second Workshop on Treebanks and Linguistic Theo-
ries (TLT).
B. Navarro, M. Civit, Ma A. Mart??, R. Marcos, and
B. Ferna?ndez. 2003. Syntactic, semantic and prag-
matic annotation in Cast3LB. In Proc. of the Work-
shop on Shallow Processing of Large Corpora (SPro-
LaC).
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from an-
tiquity. In Proc. of the NODALIDA Special Session on
Treebanks.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15.
P. Osenova and K. Simov. 2004. BTB-TR05: Bul-
TreeBank stylebook. BulTreeBank version 1.0. Bul-
treebank project technical report. Available at:
http://www.bultreebank.org/TechRep/BTB-TR05.pdf.
K. Simov and P. Osenova. 2003. Practical annotation
scheme for an HPSG treebank of Bulgarian. In Proc.
of the 4th Intern. Workshop on Linguistically Inter-
preteted Corpora (LINC), pages 17?24.
K. Simov, G. Popova, and P. Osenova. 2002. HPSG-
based syntactic treebank of Bulgarian (BulTreeBank).
In A. Wilson, P. Rayson, and T. McEnery, editors, A
Rainbow of Corpora: Corpus Linguistics and the Lan-
guages of the World, pages 135?142. Lincom-Europa,
Munich.
K. Simov, P. Osenova, and M. Slavcheva. 2004.
BTB-TR03: BulTreeBank morphosyntac-
tic tagset. BTB-TS version 2.0. Bultree-
bank project technical report. Available at:
http://www.bultreebank.org/TechRep/BTB-TR03.pdf.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov.
2005. Design and implementation of the Bulgarian
HPSG-based treebank. In Journal of Research on Lan-
guage and Computation ? Special Issue, pages 495?
522. Kluwer Academic Publishers.
L. van der Beek, G. Bouma, R. Malouf, and G. van No-
ord. 2002. The Alpino dependency treebank. In Com-
putational Linguistics in the Netherlands (CLIN).
165
