Proceedings of the ACL 2010 Conference Short Papers, pages 109?114,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Collocation Extraction beyond the Independence Assumption
Gerlof Bouma
Universita?t Potsdam, Department Linguistik
Campus Golm, Haus 24/35
Karl-Liebknecht-Stra?e 24?25
14476 Potsdam, Germany
gerlof.bouma@uni-potsdam.de
Abstract
In this paper we start to explore two-part
collocation extraction association measures
that do not estimate expected probabili-
ties on the basis of the independence as-
sumption. We propose two new measures
based upon the well-known measures of
mutual information and pointwise mutual
information. Expected probabilities are de-
rived from automatically trained Aggregate
Markov Models. On three collocation gold
standards, we find the new association mea-
sures vary in their effectiveness.
1 Introduction
Collocation extraction typically proceeds by scor-
ing collocation candidates with an association mea-
sure, where high scores are taken to indicate likely
collocationhood. Two well-known such measures
are pointwise mutual information (PMI) and mu-
tual information (MI). In terms of observing a com-
bination of words w1, w2, these are:
i(w1, w2) = log
p(w1, w2)
p(w1) p(w2)
, (1)
I (w1, w2) =
?
x?{w1,?w1}
y?{w2,?w2}
p(x, y) i(x, y). (2)
PMI (1) is the logged ratio of the observed bi-
gramme probability and the expected bigramme
probability under independence of the two words
in the combination. MI (2) is the expected outcome
of PMI, and measures how much information of the
distribution of one word is contained in the distribu-
tion of the other. PMI was introduced into the collo-
cation extraction field by Church and Hanks (1990).
Dunning (1993) proposed the use of the likelihood-
ratio test statistic, which is equivalent to MI up to
a constant factor.
Two aspects of (P)MI are worth highlighting.
First, the observed occurrence probability pobs is
compared to the expected occurrence probability
pexp. Secondly, the independence assumption un-
derlies the estimation of pexp.
The first aspect is motivated by the observa-
tion that interesting combinations are often those
that are unexpectedly frequent. For instance, the
bigramme of the is uninteresting from a colloca-
tion extraction perspective, although it probably is
amongst the most frequent bigrammes for any En-
glish corpus. However, we can expect to frequently
observe the combination by mere chance, simply
because its parts are so frequent. Looking at pobs
and pexp together allows us to recognize these cases
(Manning and Schu?tze (1999) and Evert (2007) for
more discussion).
The second aspect, the independence assump-
tion in the estimation of pexp, is more problem-
atic, however, even in the context of collocation
extraction. As Evert (2007, p42) notes, the assump-
tion of ?independence is extremely unrealistic,? be-
cause it ignores ?a variety of syntactic, semantic
and lexical restrictions.? Consider an estimate for
pexp(the the). Under independence, this estimate
will be high, as the itself is very frequent. However,
with our knowledge of English syntax, we would
say pexp(the the) is low. The independence assump-
tion leads to overestimated expectation and the the
will need to be very frequent for it to show up as a
likely collocation. A less contrived example of how
the independence assumption might mislead collo-
cation extraction is when bigramme distribution is
influenced by compositional, non-collocational, se-
mantic dependencies. Investigating adjective-noun
combinations in a corpus, we might find that beige
cloth gets a high PMI, whereas beige thought does
not. This does not make the former a collocation or
multiword unit. Rather, what we would measure is
the tendency to use colours with visible things and
not with abstract objects. Syntactic and semantic
109
associations between words are real dependencies,
but they need not be collocational in nature. Be-
cause of the independence assumption, PMI and
MI measure these syntactic and semantic associa-
tions just as much as they measure collocational
association. In this paper, we therefore experimen-
tally investigate the use of a more informed pexp in
the context of collocation extraction.
2 Aggregate Markov Models
To replace pexp under independence, one might
consider models with explicit linguistic infor-
mation, such as a POS-tag bigramme model.
This would for instance give us a more realistic
pexp(the the). However, lexical semantic informa-
tion is harder to incorporate. We might not know
exactly what factors are needed to estimate pexp
and even if we do, we might lack the resources
to train the resulting models. The only thing we
know about estimating pexp is that we need more
information than a unigramme model but less than
a bigramme model (as this would make pobs/pexp
uninformative). Therefore, we propose to use Ag-
gregate Markov Models (Saul and Pereira, 1997;
Hofmann and Puzicha, 1998; Rooth et al, 1999;
Blitzer et al, 2005)1 for the task of estimating pexp.
In an AMM, bigramme probability is not directly
modeled, but mediated by a hidden class variable c:
pamm(w2|w1) =
?
c
p(c|w1)p(w2|c). (3)
The number of classes in an AMM determines the
amount of dependency that can be captured. In the
case of just one class, AMM is equivalent to a uni-
gramme model. AMMs become equivalent to the
full bigramme model when the number of classes
equals the size of the smallest of the vocabular-
ies of the parts of the combination. Between these
two extremes, AMMs can capture syntactic, lexical,
semantic and even pragmatic dependencies.
AMMs can be trained with EM, using no more
information than one would need for ML bigramme
probability estimates. Specifications of the E- and
M-steps can be found in any of the four papers cited
above ? here we follow Saul and Pereira (1997). At
each iteration, the model components are updated
1These authors use very similar models, but with differing
terminology and with different goals. The term AMM is used
in the first and fourth paper. In the second paper, the models
are referred to as Separable Mixture Models. Their use in
collocation extraction is to our knowledge novel.
according to:
p(c|w1)?
?
w n(w1, w)p(c|w1, w)?
w,c? n(w1, w)p(c
?|w1, w)
, (4)
p(w2|c)?
?
w n(w,w2)p(c|w,w2)?
w,w? n(w,w
?)p(c|w,w?)
, (5)
where n(w1, w2) are bigramme counts and the pos-
terior probability of a hidden category c is esti-
mated by:
p(c|w1, w2) =
p(c|w1)p(w2|c)
?
c? p(c
?|w1)p(w2|c?)
. (6)
Successive updates converge to a local maximum
of the AMM?s log-likelihood.
The definition of the counterparts to (P)MI with-
out the independence assumption, the AMM-ratio
and AMM-divergence, is now straightforward:
ramm(w1, w2) = log
p(w1, w2)
p(w1) pamm(w2|w1)
, (7)
damm(w1, w2) =
?
x?{w1,?w1}
y?{w2,?w2}
p(x, y) ramm(x, y). (8)
The free parameter in these association measures is
the number of hidden classes in the AMM, that is,
the amount of dependency between the bigramme
parts used to estimate pexp. Note that AMM-ratio
and AMM-divergence with one hidden class are
equivalent to PMI and MI, respectively. It can be
expected that in different corpora and for differ-
ent types of collocation, different settings of this
parameter are suitable.
3 Evaluation
3.1 Data and procedure
We apply AMM-ratio and AMM-divergence to
three collocation gold standards. The effectiveness
of association measures in collocation extraction is
measured by ranking collocation candidates after
the scores defined by the measures, and calculat-
ing average precision of these lists against the gold
standard annotation. We consider the newly pro-
posed AMM-based measures for a varying number
of hidden categories. The new measures are com-
pared against two baselines: ranking by frequency
(pobs) and random ordering. Because AMM-ratio
and -divergence with one hidden class boil down
to PMI and MI (and thus log-likelihood ratio), the
evaluation contains an implicit comparison with
110
these canonical measures, too. However, the re-
sults will not be state-of-the-art: for the datasets
investigated below, there are more effective extrac-
tion methods based on supervised machine learning
(Pecina, 2008).
The first gold standard used is the German
adjective-noun dataset (Evert, 2008). It contains
1212 A-N pairs taken from a German newspaper
corpus. We consider three subtasks, depending on
how strict we define true positives. We used the
bigramme frequency data included in the resource.
We assigned all types with a token count ?5 to one
type, resulting in AMM training data of 10k As,
20k Ns and 446k A-N pair types.
The second gold standard consists of 5102 Ger-
man PP-verb combinations, also sampled from
newspaper texts (Krenn, 2008). The data con-
tains annotation for support verb constructions
(FVGs) and figurative expressions. This resource
also comes with its own frequency data. After fre-
quency thresholding, AMMs are trained on 46k
PPs, 7.6k Vs, and 890k PP-V pair types.
Third and last is the English verb-particle con-
struction (VPC) gold standard (Baldwin, 2008),
consisting of 3078 verb-particle pairs and annota-
tion for transitive and intransitive idiomatic VPCs.
We extract frequency data from the BNC, follow-
ing the methods described in Baldwin (2005). This
results in two slightly different datasets for the two
types of VPC. For the intransitive VPCs, we train
AMMs on 4.5k Vs, 35 particles, and 43k pair types.
For the transitive VPCs, we have 5k Vs, 35 parti-
cles and 54k pair types.
All our EM runs start with randomly initialized
model vectors. In Section 3.3 we discuss the impact
of model variation due to this random factor.
3.2 Results
German A-N collocations The top slice in Ta-
ble 1 shows results for the three subtasks of the
A-N dataset. We see that using AMM-based pexp
initially improves average precision, for each task
and for both the ratio and the divergence measure.
At their maxima, the informed measures outper-
form both baselines as well as PMI and MI/log-
likelihood ratio (# classes=1). The AMM-ratio per-
forms best for 16-class AMMs, the optimum for
AMM-divergence varies slightly.
It is likely that the drop in performance for the
larger AMM-based measures is due to the AMMs
learning the collocations themselves. That is, the
AMMs become rich enough to not only capture
the broadly applicative distributional influences of
syntax and semantics, but also provide accurate
pexps for individual, distributionally deviant combi-
nations ? like collocations. An accurate pexp results
in a low association score.
One way of inspecting what kind of dependen-
cies the AMMs pick up is to cluster the data with
them. Following Blitzer et al (2005), we take the
200 most frequent adjectives and assign them to
the category that maximizes p(c|w1); likewise for
nouns and p(w2|c). Four selected clusters (out of
16) are given in Table 2.2 The esoteric class 1 con-
tains ordinal numbers and nouns that one typically
uses those with, including references to temporal
concepts. Class 2 and 3 appear more semantically
motivated, roughly containing human and collec-
tive denoting nouns, respectively. Class 4 shows
a group of adjectives denoting colours and/or po-
litical affiliations and a less coherent set of nouns,
although the noun cluster can be understood if we
consider individual adjectives that are associated
with this class. Our informal impression from look-
ing at clusters is that this is a common situation: as
a whole, a cluster cannot be easily characterized,
although for subsets or individual pairs, one can
get an intuition for why they are in the same class.
Unfortunately, we also see that some actual collo-
cations are clustered in class 4, such as gelbe Karte
?warning? (lit.: ?yellow card?) and dickes Auto ?big
(lit.: fat) car?.
German PP-Verb collocations The second slice
in Table 1 shows that, for both subtypes of PP-V
collocation, better pexp-estimates lead to decreased
average precision. The most effective AMM-ratio
and -distance measures are those equivalent to
(P)MI. Apparently, the better pexps are unfortunate
for the extraction of the type of collocations in this
dataset.
The poor performance of PMI on these data ?
clearly below frequency ? has been noticed before
by Krenn and Evert (2001). A possible explanation
for the lack of improvement in the AMMs lies in
the relatively high performing frequency baselines.
The frequency baseline for FVGs is five times the
2An anonymous reviewer rightly warns against sketching
an overly positive picture of the knowledge captured in the
AMMs by only presenting a few clusters. However, the clus-
tering performed here is only secondary to our main goal
of improving collocation extraction. The model inspection
should thus not be taken as an evaluation of the quality of the
models as clustering models.
111
# classes
1 2 4 8 16 32 64 128 256 512 Rnd Frq
A-N
category 1 ramm 45.6 46.4 47.6 47.3 48.3 48.0 47.0 46.1 44.7 41.9 30.1 32.2
damm 42.3 42.9 44.4 45.2 46.1 46.5 45.0 46.3 45.5 45.5
category 1?2 ramm 55.7 56.3 57.4 57.5 58.1 58.1 57.7 56.9 55.7 52.8 43.1 47.0
damm 56.3 57.0 58.1 58.4 59.8 60.1 59.3 60.6 59.2 59.3
category 1?3 ramm 62.3 62.8 63.9 64.0 64.4 62.2 62.2 62.7 62.4 60.0 52.7 56.4
damm 64.3 64.7 65.9 66.6 66.7 66.3 66.3 65.4 66.0 64.7
PP-V
figurative ramm 7.5 6.1 6.4 6.0 5.6 5.4 4.5 4.2 3.8 3.5 3.3 10.5
damm 14.4 13.0 13.3 13.1 12.2 11.2 9.0 7.7 6.9 5.7
FVG ramm 4.1 3.4 3.4 3.0 2.9 2.7 2.2 2.1 2.0 2.0 3.0 14.7
damm 15.3 12.7 12.6 10.7 9.0 7.7 3.4 3.2 2.5 2.3
VPC
intransitive ramm 9.3 9.2 9.0 8.3 5.5 5.3 4.8 14.7
damm 12.2 12.2 14.0 16.3 6.9 5.8
transitive ramm 16.4 14.8 15.2 14.5 11.3 10.0 10.1 20.1
damm 19.6 17.3 20.7 23.8 12.8 10.1
Table 1: Average precision for AMM-based association measures and baselines on three datasets.
Cl Adjective Noun
1 dritt ?third?, erst ?first?, fu?nft ?fifth?, halb ?half?, kommend
?next?, laufend ?current?, letzt ?last?, nah ?near?, paar ?pair?,
vergangen ?last?, viert ?fourth?, wenig ?few?, zweit ?sec-
ond?
Jahr ?year?, Klasse ?class?, Linie ?line?, Mal ?time?, Monat
?month?, Platz ?place?, Rang ?grade?, Runde ?round?, Saison
?season?, Satz ?sentence?, Schritt ?step?, Sitzung ?session?, Son-
ntag ?Sunday?, Spiel ?game?, Stunde ?hour?, Tag ?day?, Woche
?week?, Wochenende ?weekend?
2 aktiv ?active?, alt ?old?, ausla?ndisch ?foreign?, betroffen
?concerned?, jung ?young?, lebend ?alive?, meist ?most?,
unbekannt ?unknown?, viel ?many?
Besucher ?visitor?, Bu?rger ?citizens?, Deutsche ?German?, Frau
?woman?, Gast ?guest?, Jugendliche ?youth?, Kind ?child?, Leute
?people?, Ma?dchen ?girl?, Mann ?man?, Mensch ?human?, Mit-
glied ?member?
3 deutsch ?German?, europa?isch ?European?, ganz ?whole?,
gesamt ?whole?, international ?international?, national ?na-
tional?, o?rtlich ?local?, ostdeutsch ?East-German?, privat
?private?, rein ?pure?, sogenannt ?so-called?, sonstig ?other?,
westlich ?western?
Betrieb ?company?, Familie ?family?, Firma ?firm?, Gebiet
?area?, Gesellschaft ?society?, Land ?country?, Mannschaft
?team?, Markt ?market?, Organisation ?organisation?, Staat
?state?, Stadtteil ?city district?, System ?system?, Team ?team?,
Unternehmen ?enterprise?, Verein ?club?, Welt ?world?
4 blau ?blue?, dick ?fat?, gelb ?yellow?, gru?n ?green?, linke
?left?, recht ?right?, rot ?red?, schwarz ?black?, white ?wei??
Auge ?eye?, Auto ?car?, Haar ?hair?, Hand ?hand?, Karte ?card?,
Stimme ?voice/vote?
Table 2: Selected adjective-noun clusters from a 16-class AMM.
random baseline, and MI does not outperform it by
much. Since the AMMs provide a better fit for the
more frequent pairs in the training data, they might
end up providing too good pexp-estimates for the
true collocations from the beginning.
Further investigation is needed to find out
whether this situation can be ameliorated and, if
not, whether we can systematically identify for
what kind of collocation extraction tasks using bet-
ter pexps is simply not a good idea.
English Verb-Particle constructions The last
gold standard is the English VPC dataset, shown
in the bottom slice of Table 1. We have only used
class-sizes up to 32, as there are only 35 particle
types. We can clearly see the effect of the largest
AMMs approaching the full bigramme model as
average precision here approaches the random base-
line. The VPC extraction task shows a difference
between the two AMM-based measures: AMM-
ratio does not improve at all, remaining below the
frequency baseline. AMM-divergence, however,
shows a slight decrease in precision first, but ends
up performing above the frequency baseline for the
8-class AMMs in both subtasks.
Table 3 shows four clusters of verbs and par-
ticles. The large first cluster contains verbs that
involve motion/displacement of the subject or ob-
ject and associated particles, for instance walk
about or push away. Interestingly, the description
of the gold standard gives exactly such cases as
negatives, since they constitute compositional verb-
particle constructions (Baldwin, 2008). Classes 2
and 3 show syntactic dependencies, which helps
112
Cl Verb Particle
1 break, bring, come, cut, drive, fall, get, go, lay, look, move, pass, push,
put, run, sit, throw, turn, voice, walk
across, ahead, along, around, away, back, back-
ward, down, forward, into, over, through, together
2 accord, add, apply, give, happen, lead, listen, offer, pay, present, refer,
relate, return, rise, say, sell, send, speak, write
astray, to
3 know, talk, tell, think about
4 accompany, achieve, affect, cause, create, follow, hit, increase, issue,
mean, produce, replace, require, sign, support
by
Table 3: Selected verb-particle clusters from an 8-class AMM on transitive data.
collocation extraction by decreasing the impact of
verb-preposition associations that are due to PP-
selecting verbs. Class 4 shows a third type of distri-
butional generalization: the verbs in this class are
all frequently used in the passive.
3.3 Variation due to local optima
We start each EM run with a random initializa-
tion of the model parameters. Since EM finds local
rather than global optima, each run may lead to
different AMMs, which in turn will affect AMM-
based collocation extraction. To gain insight into
this variation, we have trained 40 16-class AMMs
on the A-N dataset. Table 4 gives five point sum-
maries of the average precision of the resulting
40 ?association measures?. Performance varies con-
siderably, spanning 2?3 percentage points in each
case. The models consistently outperform (P)MI in
Table 1, though.
Several techniques might help to address this
variation. One might try to find a good fixed way of
initializing EM or to use EM variants that reduce
the impact of the initial state (Smith and Eisner,
2004, a.o.), so that a run with the same data and
the same number of classes will always learn (al-
most) the same model. On the assumption that an
average over several runs will vary less than indi-
vidual runs, we have also constructed a combined
pexp by averaging over 40 pexps. The last column
Variation in avg precision
min q1 med q3 max Comb
A-N
cat 1 ramm 46.5 47.3 47.9 48.4 49.1 48.4
damm 44.4 45.4 45.8 46.1 47.1 46.4
cat 1?2 ramm 56.7 57.2 57.9 58.2 59.0 58.2
damm 58.1 58.8 59.2 59.4 60.4 60.0
cat 1?3 ramm 63.0 63.7 64.2 64.6 65.3 64.6
damm 65.2 66.0 66.4 66.6 67.6 66.9
Table 4: Variation on A-N data over 40 EM runs
and result of combining pexps.
in Table 4 shows this combined estimator leads to
good extraction results.
4 Conclusions
In this paper, we have started to explore collocation
extraction beyond the assumption of independence.
We have introduced two new association measures
that do away with this assumption in the estima-
tion of expected probabilities. The success of using
these association measures varies. It remains to be
investigated whether they can be improved more.
A possible obstacle in the adoption of AMMs in
collocation extraction is that we have not provided
any heuristic for setting the number of classes for
the AMMs. We hope to be able to look into this
question in future research. Luckily, for the AN and
VPC data, the best models are not that large (in the
order of 8?32 classes), which means that model fit-
ting is fast enough to experiment with different set-
tings. In general, considering these smaller models
might suffice for tasks that have a fairly restricted
definition of collocation candidate, like the tasks
in our evaluation do. Because AMM fitting is un-
supervised, selecting a class size is in this respect
no different from selecting a suitable association
measure from the canon of existing measures.
Future research into association measures that
are not based on the independence assumption will
also include considering different EM variants and
other automatically learnable models besides the
AMMs used in this paper. Finally, the idea of us-
ing an informed estimate of expected probability
in an association measure need not be confined
to (P)MI, as there are many other measures that
employ expected probabilities.
Acknowledgements
This research was carried out in the context of
the SFB 632 Information Structure, subproject D4:
Methoden zur interaktiven linguistischen Korpus-
analyse von Informationsstruktur.
113
References
Timothy Baldwin. 2005. The deep lexical acquisition
of english verb-particle constructions. Computer
Speech and Language, Special Issue on Multiword
Expressions, 19(4):398?414.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of English verb-particle con-
structions. In Proceedings of the LREC 2008 Work-
shop Towards a Shared Task for Multiword Expres-
sions (MWE 2008), pages 1?2, Marrakech.
John Blitzer, Amir Globerson, and Fernando Pereira.
2005. Distributed latent variable models of lexical
co-occurrences. In Tenth International Workshop on
Artificial Intelligence and Statistics.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
Stefan Evert. 2007. Corpora and collocations. Ex-
tended Manuscript of Chapter 58 of A. Lu?deling and
M. Kyto?, 2008, Corpus Linguistics. An International
Handbook, Mouton de Gruyter, Berlin.
Stefan Evert. 2008. A lexicographic evaluation of Ger-
man adjective-noun collocations. In Proceedings of
the LREC 2008 Workshop Towards a Shared Task
for Multiword Expressions (MWE 2008), pages 3?6,
Marrakech.
Thomas Hofmann and Jan Puzicha. 1998. Statisti-
cal models for co-occurrence data. Technical report,
MIT. AI Memo 1625, CBCL Memo 159.
Brigitte Krenn and Stefan Evert. 2001. Can we do
better than frequency? a case study on extracting PP-
verb collocations. In Proceedings of the ACL Work-
shop on Collocations, Toulouse.
Brigitte Krenn. 2008. Description of evaluation re-
source ? German PP-verb data. In Proceedings of
the LREC 2008 Workshop Towards a Shared Task
for Multiword Expressions (MWE 2008), pages 7?
10, Marrakech.
Chris Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press, Cambridge, MA.
Pavel Pecina. 2008. A machine learning approach to
multiword expression extraction. In Proceedings of
the LREC 2008 Workshop Towards a Shared Task
for Multiword Expressions (MWE 2008), pages 54?
57, Marrakech.
Mats Rooth, Stefan Riester, Detlef Prescher, Glenn Car-
rol, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via em-based clustering. In Pro-
ceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics, College Park,
MD.
Lawrence Saul and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language processing. In Proceedings of the Second
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81?89.
Noah A. Smith and Jason Eisner. 2004. Anneal-
ing techniques for unsupervised statistical language
learning. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguis-
tics.
114
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 212?216,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Syntactic tree queries in Prolog
Gerlof Bouma
Universit?t Potsdam, Department Linguistik
Campus Golm, Haus 24/35
Karl-Liebknecht-Stra?e 24?25
14476 Potsdam, Germany
gerlof.bouma@uni-potsdam.de
Abstract
In this paper, we argue for and demonstrate
the use of Prolog as a tool to query an-
notated corpora. We present a case study
based on the German T?Ba-D/Z Treebank
to show that flexible and efficient corpus
querying can be started with a minimal
amount of effort. We end this paper with a
brief discussion of performance, that sug-
gests that the approach is both fast enough
and scalable.
1 Introduction
Corpus investigations that go beyond formulating
queries and studying (graphical renderings of) the
retrieved annotation very quickly begin to require
a general purpose programming language to do
things like manipulating and transforming annota-
tion, categorizing results, performing non-trivial
counting and even statistical analysis, as query
tools only offer a fixed, restricted set of operations.
The use of a general purpose programming lan-
guage has drawbacks, too, however: one has to deal
with interfacing with a database, non-deterministic
search, definition of linguistically relevant relations
and properties in terms of the lower level database
relations, etcetera.
As a solution for this dilemma of trading flex-
ibility and power against the ease with which
one can query corpora, we propose to use Pro-
log. Prolog is well suited to query databases (Nils-
son and Maluszynski, 1998). Unlike in other gen-
eral purpose languages, the programmer is re-
lieved of the burden of writing functions to non-
deterministically search through the corpus or
database.
In comparison to dedicated query languages and
their processors, the fact that one can always extend
the Prolog predicates that constitute the query lan-
guage lifts many restrictions on the kinds of queries
one can pose. A more specific point is that we can
have fine grained control over the scope of nega-
tion and quantification in queries in Prolog, some-
thing that is sometimes lacking from dedicated lan-
guages (for discussion, see Lai and Bird (2004);
for a prominent example, K?nig et al (2003); for
an exception, Kepser (2003))
Lai and Bird (2004) formulated a number of
queries to compare query languages for syntacti-
cally annotated corpora. In this paper, we demon-
strate the ease with which a flexible and fast query
environment can be constructed by implementing
these queries and using them as a rudimentary
benchmark for performance.
2 Representing the T?Ba-D/Z corpus
The T?Ba-D/Z treebank of German newspaper arti-
cles (Telljohann et al, 2006, v5) comprises about
800k tokens in 45k sentences. We store the corpus
as collection of directed acyclic graphs, with edges
directed towards the roots of the syntactic trees
(Brants, 1997).
% node/7 SentId NodeId MotherId
% Form Edge Cat Other
node(153, 4, 503, die, -, art, [morph=asf]).
node(153, 503, 508, ?$phrase?, hd, nx, []).
By using the sentence number as the first argument
of node/7 facts, we leverage first argument index-
ing to gain fast access to any node in the treebank.
Provided we know the sentence number, we never
need to consider more nodes than the largest tree
in the corpus. Since all nodes that stand in a syntac-
tic relation are within the same sentence, querying
syntactic structure is generally fast. An example
tree and its full representation is given in Figure 1.
Note that in this paper, we only consider the pri-
mary nodes and edges, even though we are in no
fundamental way restricted to querying only this
annotation level.
A set of interface relations provide a first level
of abstraction over this representation. Direct dom-
212
SIMPX
VF
NX
PDS
Dieser
LK
VXFIN
VAFIN
hat
MF
NX
NX
NN
Auswirkungen
PX
APPR
auf
NX
ART
die
NN
Bereitschaft ,
NF
SIMPX
MF
NX
NN
Therapieangebote
VC
VXINF
VVIZU
anzunehmen .
?This has effects on the willingness to accept therapy.?
node(153, 0, 500, ?Dieser?, hd, pds, [morph=nsm]). node(153, 515, 0, ?$phrase?, --, simpx, []).
node(153, 1, 501, hat, hd, vafin, [morph=?3sis?]). node(153, 506, 515, ?$phrase?, -, vf, []).
node(153, 2, 502, ?Auswirkungen?, hd, nn, [morph=apf]). node(153, 500, 506, ?$phrase?, on, nx, []).
node(153, 3, 508, auf, -, appr, [morph=a]). node(153, 507, 515, ?$phrase?, -, lk, []).
node(153, 4, 503, die, -, art, [morph=asf]). node(153, 501, 507, ?$phrase?, hd, vxfin, []).
node(153, 5, 503, ?Bereitschaft?, hd, nn, [morph=asf]). node(153, 513, 515, ?$phrase?, -, mf, []).
node(153, 6, 0, (?,?), --, ?$,?, [morph= --]). node(153, 511, 513, ?$phrase?, oa, nx, []).
node(153, 7, 504, ?Therapieangebote?, hd, nn, [morph=apn]). node(153, 502, 511, ?$phrase?, hd, nx, []).
node(153, 8, 505, anzunehmen, hd, vvizu, [morph= --]). node(153, 508, 511, ?$phrase?, -, px, []).
node(153, 9, 0, ?.?, --, $., [morph= --]). node(153, 503, 508, ?$phrase?, hd, nx, []).
node(153, 514, 515, ?$phrase?, -, nf, []).
node(153, 512, 514, ?$phrase?, mod, simpx, []).
node(153, 509, 512, ?$phrase?, -, mf, []).
secondary(153,503,512,refint). node(153, 504, 509, ?$phrase?, oa, nx, []).
node(153, 510, 512, ?$phrase?, -, vc, []).
node(153, 505, 510, ?$phrase?, hd, vxinf, []).
Figure 1: A tree from T?ba-D/Z and its Prolog representation.
inance and other simple relations are defined di-
rectly in terms of this interface.
has_sentid(node(A_s,_,_,_,_,_,_),A_s).
has_nodeid(node(_,A_n,_,_,_,_,_),A_n).
has_mother(node(_,_,A_m,_,_,_,_),A_m).
has_form(node(_,_,_,A_f,_,_,_),A_f).
has_poscat(node(_,_,_,_,_,A_p,_),A_p).
is_under(A,B):-
has_mother(A,A_m,A_s),
is_phrasal(B),
has_nodeid(B,A_m,A_s).
are_sentmates(A,B):-
has_sentid(A,A_s),
has_sentid(B,A_s).
is_phrasal(A):-
has_form(A,?$phrase?).
None of these predicates consult the database. Ac-
tually looking up a graph involves calling the nodes
describing it. So, is_phrasal(A), A, will return
once for each phrasal node in the corpus. Transitive
closures over the relations above define familiar
tree navigation predicates like dominance (closure
of is_under/2). In contrast with the simple relations,
these closures do look up their arguments.
has_ancestor(A,B):-
has_ancestor(A,B,_).
has_ancestor(A,B,AB_path):-
are_sentmates(A,B),
A, is_under(A,A1), A1,
has_ancestor_rfl(A1,B,AB_path).
has_ancestor_rfl(A,A,[]).
has_ancestor_rfl(A,B,[A|AB_path]):-
is_under(A,A1), A1,
has_ancestor_rfl(A1,B,AB_path).
At this point, linear precedence is still undefined
for phrases. We define string position of a phrase
as its span over the string, which we get by taking
indices of the first and last words in its yield.
yields_dl(A,Bs):-
is_phrasal(A)
-> ( is_above(A,A1),
findall(A1, A1, A1s),
map(yields_dl,A1s,Bss),
fold(append_dl,Bss,Bs)
)
; % is_lexical(A)
Bs = [A|Cs]\Cs.
spans(A,A_beg,A_end):-
yields_dl(A,Bs\[]),
map(has_nodeid,Bs,B_ns),
fold(min,B_ns,A_beg),
fold(max,B_ns,B_n_mx),
A_end is B_n_mx+1
Thus, the span of the word Auswirkungen in the tree
in Figure 1 is 2?3, and the span of the MF-phrase is
2?6. It makes sense to precalculate spans/3, as this
is an expensive way of calculating linear order and
we are likely to need this information frequently,
for instance in predicates like:
213
precedes(A,B):-
are_sentmates(A,B),
spans(A,_,A_end),
spans(B,B_beg,_),
A_end =< B_beg.
directly_precedes(A,B):-
are_sentmates(A,B),
spans(A,_,A_end),
spans(B,A_end,_).
are_right_aligned(A,B):-
are_sentmates(A,B),
spans(A,_,A_end),
spans(B,_,A_end).
TIGERSearch implements an alternative definition
of linear precedence, where two left-corners are
compared (K?nig et al, 2003). It would be straight-
forward to implement this alternative.
3 Application & Comparison
Lai and Bird (2004) compare the expressiveness
of query languages by formulating queries that
test different aspects of a query language, such
as the ability to constrain linear order and dom-
inance, to use negation and/or universal quantifi-
cation, and to separate context from the returned
subgraphs. The queries have thus been designed
to highlight strengths and weaknesses of different
query languages in querying linguistic structure.
Six of these queries ? with categories changed to
match the T?ba-D/Z corpus ? are given in Table 1
and expressed in TIGERSearch query syntax in
Table 2. Since TIGERSearch does not allow for
negation to outscope existential quantification of
nodes, queries Q2 and Q5 are not expressible (also
see Marek et al (2008) for more discussion). In
addition, Q7 has two interpretations, depending on
whether one wants to return NPs once for each PP
in the context or just once altogether. TIGERSearch
does not allow us to differentiate between these two
interpretations.
Q1 & Q2 The implementation of domination,
has_ancestor/2, performs database lookup. We
therefore call it last in q1/1. To ensure the correct
scope of the negation, lookup of A in q2/1 is explicit
and outside the scope of negation-as-Prolog-failure
\+/1, whereas B is looked up inside its scope.
q1(A):-
has_cat(A,simpx),
has_surf(B,?sah?),
has_ancestor(B,A).
q2(A):-
has_cat(A,simpx),
has_surf(B,sah),
A, \+ has_ancestor(B,A).
Q1 Find sentences that include the word sah.
Q2 Find sentences that do not include sah.
Q3 Find NPs whose rightmost child is an N.
Q4 Find NPs that contain an AdjP immediately
followed by a noun that is immediately fol-
lowed by a prepositional phrase.
Q5 Find the first common ancestor of sequences
of an NP followed by a PP.
Q7 Find an NP dominated by a PP. Return the
subtree dominated by that NP only.
Table 1: Query descriptions
Q1 [cat="SIMPX"] >* [word="sah"]
Q2 (not expressible)
Q3 #n1:[cat="NX"] > #n2:[pos="NN"]
& #n1 >@r #n2
Q4 #nx:[cat="NX"] >* #ax:[cat="ADJX"]
& #nx >* #n:[pos="NN"]
& #nx >* #px:[cat="PX"]
& #px >@l #pxl
& #ax >@r #axr
& #axr . #n
& #n . #pxl
Q5 (not expressible)
Q7 [cat="PX"] >* #nx:[cat="NX"]
Table 2: TIGERSearch queries
Q3, Q4 The implementation of spans/3 relies on
given nodes, which means that database lookup is
performed before checking linear order constraints,
explicitly in q3/1 and implicitly in q4_a/1. In addi-
tion, these constraints are expensive to check, so
we make sure we postpone their evaluation as much
as possible.
q3(A):-
has_cat(A,nx),
has_pos(B,nn),
is_under(B,A),
A, B, are_right_aligned(A,B).
q4_a(A):-
has_cat(A,nx),
has_cat(B,adjx),
has_pos(C,nn),
has_cat(D,px),
has_ancestor(B,A),
has_ancestor(C,A),
has_ancestor(D,A),
directly_precedes(B,C),
directly_precedes(C,D).
If we precalculate spans/3, the alternative order of
checking dominance and linear precedence con-
straints becomes viable, as in q4_b/1.
q4_b(A):-
has_cat(A,nx,A_s),
has_cat(B,adjx,A_s),
has_pos(C,nn,A_s),
has_cat(D,px,A_s),
B,C,D, % (cont. on next page)
214
directly_precedes(B,C),
directly_precedes(C,D),
has_ancestor(B,A),
has_ancestor(C,A),
has_ancestor(D,A).
The procedural sides of Prolog make that these two
alternatives are processed with considerable speed
differences.
Q5 The lowest common ancestor part of Q5 can
be implemented by constraining the paths between
two nodes and their common ancestor:
q5(A):-
has_cat(B,nx,A_s),
has_cat(C,px,A_s),
B, C,
precedes(B,C),
has_ancestor(B,A,BA_path),
has_ancestor(C,A,CA_path),
\+ ( last(BA_path,D), last(CA_path,D) ).
Q7 Precise control over the quantification of the
two nodes in Q7 is achieved by using the built-in
once/1 predicate (?existential quantification) and
by choosing different moments of database lookup
for the two nodes.
q7_a(A):- % once for each np-pp pair
has_cat(A,nx),
has_cat(B,px),
has_ancestor(A,B).
q7_b(A):- % just once per np
has_cat(A,nx),
has_cat(B,px),
A, once(has_ancestor(A,B)).
4 Performance
In Table 3, we list wall-clock times for execution of
each of the queries. These serve to demonstrate the
fact that our straightforward use of Prolog results
in a system that is not only flexible and with short
development times, but that is also fast enough to
be usable. We have also included TIGERSearch
execution times for the same queries to give an
idea of the speed of querying with Prolog.1
Table 3 shows Prolog execution times fall well
within useable ranges, provided we precalculate
span/3 facts for queries that rely heavily on linear
order. The non-declarative side of Prolog is most
clearly seen in the difference between Q4-a and
Q4-b ? the latter constraint ordering is more than
twice as fast. Even with precalculated span/3 facts,
the whole corpus and query code uses less than
0.5Gbytes of RAM to run.
1Machine specifications: 1.6Ghz Intel Core 2 Duo,
2GBytes RAM. SWI-prolog (v5.6) on a 32-bit Linux. The
TIGERSearch times were taken on the same machine. The
TIGERSearch corpus was compiled with ?extended indexing?.
Precalc. spans
# hits T.Search no yes
Loading from source 30 50
Loading precompiled 3 4
Precalculating spans/3 90
Q1 73 3 1
Q2 65727 1
Q3 152669 33 10 4
Q4-a 8185 200 60 50
Q4-b 21
Q5 312753 196 70
Q7-a 145737 6 8
Q7-b 119649 6
Table 3: Rounded up wall-clock times in seconds.
To give an impression of scalability, we can re-
port Prolog queries on a 40M tokens, dependency
parsed corpus (Bouma et al, 2010). The setup re-
quires about 13Gbyte of RAM on a 64-bit machine.
Loading a corpus takes under a minute when pre-
compiled. Due to first-argument indexing, time per
answer does not increase much. Handling of larger
corpora remains a topic for future work.
5 Conclusions
On the basis of six queries designed to highlight
strengths and weaknesses of query languages, we
have demonstrated that querying syntactically an-
notated corpora using Prolog is straightforward,
flexible and efficient. Due to space constraints, the
example queries have been rather simple, and many
of the more interesting aspects of using a general
purpose programming language like Prolog for cor-
pus querying have not been dealt with, such as
querying structures between and above the sen-
tence, result categorization, on-the-fly annotation
transformation, and the combination of annotation
layers. For examples of these and other use cases,
we refer the reader to Witt (2005), Bouma (2008),
Bouma et al (2010), and Bouma (Ms). This paper?s
Prolog code and further conversion scripts will be
available from the author?s website.
Acknowledgements
This research was carried out in the context of
the SFB 632 Information Structure, subproject D4:
Methoden zur interaktiven linguistischen Korpus-
analyse von Informationsstruktur.
215
References
Gerlof Bouma, Lilja ?vrelid, and Jonas Kuhn. 2010.
Towards a large parallel corpus of clefts. In Proceed-
ings of LREC 2010, Malta.
Gerlof Bouma. 2008. Starting a Sentence in Dutch: A
corpus study of subject- and object-fronting. Ph.D.
thesis, University of Groningen.
Gerlof Bouma. Ms. Querying linguistic corpora with
Prolog. Manuscript, May 2010, University of Pots-
dam.
Thorsten Brants. 1997. The negra export format. Tech-
nical report, Saarland University, SFB378.
Stephan Kepser. 2003. Finite structure query - a tool
for querying syntactically annotated corpora. In Pro-
ceedings of EACL 2003, pages 179?186.
Esther K?nig, Wolfgang Lezius, and Holger Voormann.
2003. Tigersearch 2.1 user?s manual. Technical re-
port, IMS Stuttgart.
Catherine Lai and Steven Bird. 2004. Querying and up-
dating treebanks: A critical survey and requirements
analysis. In Proceedings of the Australasion Lan-
guage Technology Workshop, Sydney.
Torsten Marek, Joakim Lundborg, and Martin Volk.
2008. Extending the tiger query language with uni-
versal quantification. In KONVENS 2008: 9. Kon-
ferenz zur Verarbeitung nat?rlicher Sprache, pages
5?17, Berlin.
Ulf Nilsson and Jan Maluszynski. 1998. Logic, pro-
gramming and Prolog. John Wiley & Sons, 2nd edi-
tion.
Heike Telljohann, Erhard Hinrichs, Sandra K?bler, and
Heike Zinsmeister. 2006. Stylebook for the t?bin-
gen treebank of written german (t?ba-d/z). revised
version. Technical report, Seminar f?r Sprachwis-
senschaft, Universit?t T?bingen.
Andreas Witt. 2005. Multiple hierarchies: New as-
pects of an old solution. In Stefani Dipper, Michael
G?tze, and Manfred Stede, editors, Heterogeneity
in Focus: Creating and Using Linguistic Databases,
Interdisciplinary Studies on Information Structure
(ISIS) 2, pages 55?86. Universit?tsverlag Potsdam,
Potsdam.
216
