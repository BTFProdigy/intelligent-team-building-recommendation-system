Poisson Naive Bayes for Text Classification with Feature Weighting
Sang-Bum Kim, Hee-Cheol Seo and Hae-Chang Rim
Dept. of CSE., Korea University
5-ka Anamdong, SungPuk-ku, SEOUL 136-701, KOREA
sbkim,hcseo,rim@nlp.korea.ac.kr
Abstract
In this paper, we investigate the use of
multivariate Poisson model and feature
weighting to learn naive Bayes text clas-
sifier. Our new naive Bayes text classifi-
cation model assumes that a document is
generated by a multivariate Poisson model
while the previous works consider a doc-
ument as a vector of binary term features
based on the presence or absence of each
term. We also explore the use of feature
weighting for the naive Bayes text classifi-
cation rather than feature selection, which
is a quite costly process when a small
number of the new training documents are
continuously provided.
Experimental results on the two test col-
lections indicate that our new model with
the proposed parameter estimation and the
feature weighting technique leads to sub-
stantial improvements compared to the
unigram language model classifiers that
are known to outperform the original pure
naive Bayes text classifiers.
1 Introduction
The naive Bayes classifier has been one of the core
frameworks in the information retrieval research for
many years. Recently, naive Bayes is emerged as a
research topic itself because it sometimes achieves
good performances on various tasks, compared to
more complex learning algorithms, in spite of the
wrong independence assumptions on naive Bayes.
Similarly, naive Bayes is also an attractive ap-
proach in the text classification task because it is
simple enough to be practically implemented even
with a great number of features. This simplicity en-
ables us to integrate the text classification and filter-
ing modules with the existing information retrieval
systems easily. It is because that the frequency re-
lated information stored in the general text retrieval
systems is all the required information in naive
Bayes learning. No further complex generaliza-
tion processes are required unlike the other machine
learning methods such as SVM or boosting. More-
over, incremental adaptation using a small number
of new training documents can be performed by just
adding or updating frequencies.
Several earlier works have extensively studied the
naive Bayes text classification (Lewis, 1992; Lewis,
1998; McCallum and Nigam, 1998). However,
their pure naive Bayes classifiers considered a doc-
ument as a binary feature vector, and so they can?t
utilize the term frequencies in a document, result-
ing the poor performances. For that reason, the
unigram language model classifier (or multinomial
naive Bayes text classifier) has been referred as an
alternative and promising naive Bayes by a num-
ber of researchers(McCallum and Nigam, 1998; Du-
mais et al, 1998; Yang and Liu, 1999; Nigam et
al., 2000). Although the unigram language model
classifiers usually outperform the pure naive Bayes,
they also have given the disappointing results com-
pared to many other statistical learning methods
such as nearest neighbor classifiers(Yang and Chute,
1994), support vector machines(Joachims, 1998),
and boosting(Schapire and Singer, 2000), etc.
In the real world, an operational text classifica-
tion system is usually placed in the environment
where the amount of human-annotated training doc-
uments is small in spite of the hundreds of thousands
classes. Moreover, re-training of the text classifiers
is required since a small number of new training
documents are continuously provided. In this envi-
ronment, naive Bayes is probably the most appropri-
ate model for the practical systems rather than other
complex learning models. Therefore, more inten-
sive studies about the naive Bayes text classification
model are required.
In this paper, we revisit the naive Bayes frame-
work, and propose a Poisson naive Bayes model for
text classification with a statistical feature weight-
ing method. Feature weighting has many advan-
tages compared to the previous feature selection ap-
proaches, especially when the new training exam-
ples are continuously provided. Our new model as-
sumes that a document is generated by a multivari-
ate Poisson model, and their parameters are esti-
mated by weighted averaging of the normalized and
smoothed term frequencies over all the training doc-
uments. Under the assumption, we have tested the
feature weighting approach with three measures: in-
formation gain, -statistic, and newly introduced
probability ratio. With the proposed model and fea-
ture weighting techniques, we can get much better
performance without losing the simplicity and effi-
ciency of the naive Bayes model.
The remainder of this paper is organized as fol-
lows. The next section presents a naive Bayes frame-
work for the text classification briefly. Section 3
describes our new naive Bayes model and the pro-
posed technique, and the experimental results are
presented in Section 4. Finally, we conclude the pa-
per by suggesting possible directions for future work
in Section 5.
2 Naive Bayes Text Classification
A naive Bayes classifier is a well-known and highly
practical probabilistic classifier, and has been em-
ployed in many applications. It assumes that all
attributes of the examples are independent of each
other given the context of the class, that is, an in-
dependent assumption. Several studies show that
naive Bayes performs surprisingly well in many do-
mains(Domingos and Pazzani, 1997) in spite of its
wrong independent assumption.
In the context of text classification, the probabil-
ity of class  given a document 

is calculated by
Bayes? theorem as follows:


 












  









 






   
(1)
Now, if we define a new function 

,


 






(2)
then, Equation (1) can be rewritten as


 



 



   
(3)
Using Equation (3), we can get the posterior prob-
ability 

 by obtaining 

, which is a form of
log ratio similar to the BIM retrieval model(Jones et
al., 2000). It means that the linked independence as-
sumption(Cooper et al, 1992), which explains that
the strong independent assumption can be relaxed
in the BIM model, is sufficient for the use of naive
Bayes text classification model.
With this framework, two representative naive
Bayes text classification approaches are well intro-
duced in (McCallum and Nigam, 1998). They desig-
nated the pure naive Bayes as multivariate Bernoulli
model, and the unigram language model classifier as
multinomial model. Instead, we introduce multivari-
ate Poisson model to improve the pure naive Bayes
text classification in the next section.
3 Poisson Naive Bayes Text Classification
3.1 Overview
The Poisson distribution is most commonly used to
model the number of random occurrences of some
phenomenon in a specified unit of space or time,
for example, the number of phone calls received by
a telephone operator in a 10-minute period. If we
think that the occurrence of each term is a random
occurrence in a fixed unit of space (i.e. a length
of document) the Poisson distribution is intuitively
suitable to model the term frequencies in a given
document. For that reason, the use of Poisson model
is widely investigated in the IR literature, but it is
rarely used for the text classification task(Lewis,
1998). It motivates us to adopt the Poisson model
for learning the naive Bayes text classification.
Our model assumes that 

is generated by multi-
variate Poisson model. In other words, a document


is a random vector which consists of the Poisson
random variables 

, and 

has the value of within-
term-frequency 

for the 	-th term 


. Thus, if we
assume the independence among the terms in 

, a
probability of 

is represented by,


 
 

 

 

 (4)
where,   is a vocabulary size, and each  




 is given by,
 

 

 



	



	
(5)
where,  is the Poisson mean.
As a result, the 

function of Equation (2) is
rewritten using Equations (4) and (5) as follows:



 



 

 


 

 



 








	







	

(6)
where, 

and 

is the Poisson mean for 


in class
 and class , respectively.
The most important issues of this work are as fol-
lows:
 How to decide the frequency 

representing
the characteristic of document 

?
 How to estimate the model parameter 

and 

representing the characteristic of each class?
We propose the possible answers in the next subsec-
tion.
3.2 Parameter Estimation
Since 

is a frequency of a term 	 in a document


with a fixed length according to the definition of
Poisson distribution, we should normalize the actual
term frequencies in the documents with the different
length. In addition, many earlier works in NLP and
IR fields recommend that smoothing term frequen-
cies is necessary in order to build a more accurate
model.
Thus, we estimate 

as the normalized and
smoothed frequency of actual term frequency 

,
represented by,







 


    
  (7)
where  is a laplace smoothing parameter,  is any
huge value which makes all the 


in our model an
integer value1, and 

is the length of 

.
Conceptually, 


can be regarded as the value es-
timated by the following steps : 1) Add  of all  
terms to the document 

, 2) Scale 

up to 

whose
total length is  without changing the proportion of
frequency for each term 


, 3) Count 


in 

.
Then, Poisson mean 

, which represents an aver-
age number of occurrence of 


in the documents be-
longing to class , is estimated using the normalized
and smoothed 


values over the training documents
as follows:












 




(8)
where 

is the set of training documents belonging
to class , and 


2 is the interpolation of the
uniform probability and the probability proportional
to the length of the document, and the interpolation
is calculated as follows:


  




   









(9)
Simple averaging of 


, the case of =1, seems to
be correct to estimate 

. However, the statistics
1Since 

is a value of random variable 

representing
the frequency in our Poisson distribution, we multiply the nor-
malized frequency with some unnatural constant  to make 

integer value. However,  is dropped in the final induced func-
tion.
2We use the notation 

 for the distribution defined
only in the training documents, to distinguish it from the no-
tation 

 used in the Section 2.
from the long documents can be more reliable than
those in the short documents, hence we try to inter-
polate between the two different probabilities with
the parameter  ranging from 0 to 1. Consequently,


is a weighted average over all training documents
belonging to the class , and 

for the class  can be
estimated in the same manner.
3.3 Feature Weighting
Feature selection is often performed as a preprocess-
ing step for the purpose of both reducing the fea-
ture space and improving the classification perfor-
mance. Text classifiers are then trained with various
machine learning algorithms in the resulting feature
space. (Yang and Pedersen, 1997) investigated some
measures to select useful term features including
mutual information(MI), information gain(IG), and


-statistics(CHI), etc. On the contrary, (Joachims,
1998) claimed that there is no useless term features,
and it is preferable to use all term features. It is
clear that learning and classification become very
efficient when the feature space is considerably re-
duced. However, there is no definite conclusion
about the contribution of feature selection to im-
prove overall performances of the text classification
systems. It may considerably depend on the em-
ployed learning algorithm. We believe that proper
external feature selection or weighting is required to
improve the performances of naive Bayes since the
naive Bayes has no framework of the discriminative
optimizing process in itself. Of the two possible ap-
proaches, feature selection is very inefficient in case
that the additional training documents are provided
continuously. It is because the feature set should
be redefined according to the modified term statis-
tics in the new training document set, and classifiers
should be trained again with this new feature set. For
that reason, we prefer to use feature weighting to
improve naive Bayes rather than feature selection.
With the feature weighting method, our 

is rede-
fined as follows:



 




W

 










	


 






	

(10)
where, 

is the weight of feature for the class ,
and W

is the normalization factor, that is,





.
In our work, three measures are used to weight
Table 1: Two-way contingency table
presence of 


absence of 


labeled as  a b
not labeled as  c d
each term feature: information gain, -statistics
and probability ratio. Information gain (or aver-
age mutual information) is an information-theoretic
measure defined by the amount of reduced uncer-
tainty given a piece of information. We use the in-
formation gain value as the weight of each term for
the class , and the value is calculated using a docu-
ment event model as follows:


   Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 410?418,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Bridging Lexical Gaps between Queries and Questions on
Large Online Q&A Collections with Compact Translation Models
Jung-Tae Lee? and Sang-Bum Kim? and Young-In Song? and Hae-Chang Rim?
?Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
?Search Business Team, SK Telecom, Seoul, Korea
?Dept. of Computer Science & Engineering, Korea University, Seoul, Korea
{jtlee,sbkim,song,rim}@nlp.korea.ac.kr
Abstract
Lexical gaps between queries and questions
(documents) have been a major issue in ques-
tion retrieval on large online question and
answer (Q&A) collections. Previous stud-
ies address the issue by implicitly expanding
queries with the help of translation models
pre-constructed using statistical techniques.
However, since it is possible for unimpor-
tant words (e.g., non-topical words, common
words) to be included in the translation mod-
els, a lack of noise control on the models can
cause degradation of retrieval performance.
This paper investigates a number of empirical
methods for eliminating unimportant words in
order to construct compact translation mod-
els for retrieval purposes. Experiments con-
ducted on a real world Q&A collection show
that substantial improvements in retrieval per-
formance can be achieved by using compact
translation models.
1 Introduction
Community-driven question answering services,
such as Yahoo! Answers1 and Live Search QnA2,
have been rapidly gaining popularity among Web
users interested in sharing information online. By
inducing users to collaboratively submit questions
and answer questions posed by other users, large
amounts of information have been collected in the
form of question and answer (Q&A) pairs in recent
years. This user-generated information is a valu-
able resource for many information seekers, because
1http://answers.yahoo.com/
2http://qna.live.com/
users can acquire information straightforwardly by
searching through answered questions that satisfy
their information need.
Retrieval models for such Q&A collections
should manage to handle the lexical gaps or word
mismatches between user questions (queries) and
answered questions in the collection. Consider the
two following examples of questions that are seman-
tically similar to each other:
? ?Where can I get cheap airplane tickets??
? ?Any travel website for low airfares??
Conventional word-based retrieval models would
fail to capture the similarity between the two, be-
cause they have no words in common. To bridge the
query-question gap, prior work on Q&A retrieval by
Jeon et al (2005) implicitly expands queries with the
use of pre-constructed translation models, which lets
you generate query words not in a question by trans-
lation to alternate words that are related. In prac-
tice, these translation models are often constructed
using statistical machine translation techniques that
primarily rely on word co-occurrence statistics ob-
tained from parallel strings (e.g., question-answer
pairs).
A critical issue of the translation-based ap-
proaches is the quality of translation models con-
structed in advance. If no noise control is conducted
during the construction, it is possible for translation
models to contain ?unnecessary? translations (i.e.,
translating a word into an unimportant word, such as
a non-topical or common word). In the query expan-
sion viewpoint, an attempt to identify and decrease
410
the proportion of unnecessary translations in a trans-
lation model may produce an effect of ?selective?
implicit query expansion and result in improved re-
trieval. However, prior work on translation-based
Q&A retrieval does not recognize this issue and uses
the translation model as it is; essentially no attention
seems to have been paid to improving the perfor-
mance of the translation-based approach by enhanc-
ing the quality of translation models.
In this paper, we explore a number of empiri-
cal methods for selecting and eliminating unimpor-
tant words from parallel strings to avoid unnecessary
translations from being learned in translation models
built for retrieval purposes. We use the term compact
translation models to refer to the resulting models,
since the total number of parameters for modeling
translations would be minimized naturally. We also
present experiments in which compact translation
models are used in Q&A retrieval. The main goal of
our study is to investigate if and how compact trans-
lation models can improve the performance of Q&A
retrieval.
The rest of this paper is organized as follows.
The next section introduces a translation-based re-
trieval model and accompanying techniques used to
retrieve query-relevant questions. Section 3 presents
a number of empirical ways to select and eliminate
unimportant words from parallel strings for training
compact translation models. Section 4 summarizes
the compact translation models we built for retrieval
experiments. Section 5 presents and discusses the
results of retrieval experiments. Section 6 presents
related works. Finally, the last section concludes the
paper and discusses future directions.
2 Translation-based Retrieval Model
This section introduces the translation-based lan-
guage modeling approach to retrieval that has been
used to bridge the lexical gap between queries and
already-answered questions in this paper.
In the basic language modeling framework for re-
trieval (Ponte and Croft, 1998), the similarity be-
tween a query Q and a document D for ranking may
be modeled as the probability of the document lan-
guage model MD built from D generating Q:
sim(Q,D) ? P (Q|MD) (1)
Assuming that query words occur independently
given a particular document language model, the
query-likelihood P (Q|MD) is calculated as:
P (Q|MD) =
?
q?Q
P (q|MD) (2)
where q represents a query word.
To avoid zero probabilities in document language
models, a mixture between a document-specific
multinomial distribution and a multinomial distribu-
tion estimated from the entire document collection
is widely used in practice:
P (Q|MD) =
?
q?Q
[
(1? ?) ? P (q|MD)
+? ? P (q|MC)
]
(3)
where 0 < ? < 1 and MC represents a language
model built from the entire collection. The probabil-
ities P (w|MD) and P (w|MC) are calculated using
maximum likelihood estimation.
The basic language modeling framework does not
address the issue of lexical gaps between queries
and question. Berger and Lafferty (1999) viewed
information retrieval as statistical document-query
translation and introduced translation models to map
query words to document words. Assuming that
a translation model can be represented by a condi-
tional probability distribution of translation T (?|?)
between words, we can model P (q|MD) in Equa-
tion 3 as:
P (q|MD) =
?
w?D
T (q|w)P (w|MD) (4)
where w represents a document word.3
The translation probability T (q|w) virtually rep-
resents the degree of relationship between query
word q and document word w captured in a differ-
ent, machine translation setting. Then, in the tra-
ditional information retrieval viewpoint, the use of
translation models produce an implicit query expan-
sion effect, since query words not in a document are
mapped to related words in the document. This im-
plies that translation-based retrieval models would
make positive contributions to retrieval performance
only when the pre-constructed translation models
have reliable translation probability distributions.
3The formulation of our retrieval model is basically equiva-
lent to the approach of Jeon et al (2005).
411
2.1 IBM Translation Model 1
Obviously, we need to build a translation model in
advance. Usually the IBM Model 1, developed in
the statistical machine translation field (Brown et al,
1993), is used to construct translation models for
retrieval purposes in practice. Specifically, given a
number of parallel strings, the IBM Model 1 learns
the translation probability from a source word s to a
target word t as:
T (t|s) = ??1s
N?
i
c(t|s;Ji) (5)
where ?s is a normalization factor to make the sum
of translation probabilities for the word s equal to 1,
N is the number of parallel string pairs, and Ji is the
ith parallel string pair. c(t|s; Ji) is calculated as:
c(t|s; Ji) =
( P (t|s)
P (t|s1) + ? ? ?+ P (t|sn)
)
?freqt,Ji ? freqs,Ji (6)
where {s1, . . . , sn} are words in the source text in
J i. freqt,Ji and freqs,Ji are the number of times
that t and s occur in Ji, respectively.
Given the initial values of T (t|s), Equations (5)
and (6) are used to update T (t|s) repeatedly until
the probabilities converge, in an EM-based manner.
Note that the IBM Model 1 solely relies on
word co-occurrence statistics obtained from paral-
lel strings in order to learn translation probabilities.
This implies that if parallel strings have unimportant
words, a resulted translation model based on IBM
Model 1 may contain unimportant words with non-
zero translation probabilities.
We alleviate this drawback by eliminating unim-
portant words from parallel strings, avoiding them
from being included in the conditional translation
probability distribution. This naturally induces the
construction of compact translation models.
2.2 Gathering Parallel Strings from Q&A
Collections
The construction of statistical translation models
previously discussed requires a corpus consisting of
parallel strings. Since monolingual parallel texts are
generally not available in real world, one must arti-
ficially generate a ?synthetic? parallel corpus.
Question and answer as parallel pairs: The
simplest approach is to directly employ questions
and their answers in the collections by setting ei-
ther as source strings and the other as target strings,
with the assumption that a question and its cor-
responding answer are naturally parallel to each
other. Formally, if we have a Q&A collection as
C = {D1, D2, . . . , Dn}, where Di refers to an ith
Q&A data consisting of a question qi and its an-
swer ai, we can construct a parallel corpus C ? as
{(q1, a1), . . . , (qn, an)}?{(a1, q1), . . . , (an, qn)} =
C ? where each element (s, t) refers to a parallel pair
consisting of source string s and target string t. The
number of parallel string samples would eventually
be twice the size of the collections.
Similar questions as parallel pairs: Jeon et
al. (2005) proposed an alternative way of auto-
matically collecting a relatively larger set of par-
allel strings from Q&A collections. Motivated
by the observation that many semantically identi-
cal questions can be found in typical Q&A collec-
tions, they used similarities between answers cal-
culated by conventional word-based retrieval mod-
els to automatically group questions in a Q&A col-
lection as pairs. Formally, two question strings qi
and qj would be included in a parallel corpus C ?
as {(qi, qj), (qj , qi)} ? C ? only if their answer
strings ai and aj have a similarity higher than a
pre-defined threshold value. The similarity is cal-
culated as the reverse of the harmonic mean of ranks
as sim(ai, aj) = 12( 1rj + 1ri ), where rj and ri refer to
the rank of the aj and ai when ai and aj are given as
queries, respectively. This approach may artificially
produce much more parallel string pairs for training
the IBM Model 1 than the former approach, depend-
ing on the threshold value.4
To our knowledge, there has not been any study
comparing the effectiveness of the two approaches
yet. In this paper, we try both approaches and com-
pare the effectiveness in retrieval performance.
3 Eliminating Unimportant Words
We adopt a term weight ranking approach to iden-
tify and eliminate unimportant words from parallel
strings, assuming that a word in a string is unim-
4We have empirically set the threshold (0.05) for our exper-
iments.
412
Figure 1: Term weighting results of tf-idf and TextRank (window=3). Weighting is done on underlined words only.
portant if it holds a relatively low significance in the
document (Q&A pair) of which the string is origi-
nally taken from. Some issues may arise:
? How to assign a weight to each word in a doc-
ument for term ranking?
? How much to remove as unimportant words
from the ranked list?
The following subsections discuss strategies we use
to handle each of the issues above.
3.1 Assigning Term Weights
In this section, the two different term weighting
strategies are introduced.
tf-idf: The use of tf-idf weighting on evaluating
how unimportant a word is to a document seems to
be a good idea to begin with. We have used the fol-
lowing formulas to calculate the weight of word w
in document D:
tf -idfw,D = tfw,D ? idfw (7)
tfw,D = freqw,D|D| , idfw = log
|C|
dfw
where freqw,D refers to the number of times w oc-
curs in D, |D| refers to the size of D (in words), |C|
refers to the size of the document collection, and dfw
refers to the number of documents where w appears.
Eventually, words with low tf-idf weights may be
considered as unimportant.
TextRank: The task of term weighting, in fact,
has been often applied to the keyword extraction
task in natural language processing studies. As
an alternative term weighting approach, we have
used a variant of Mihalcea and Tarau (2004)?s Tex-
tRank, a graph-based ranking model for keyword
extraction which achieves state-of-the-art accuracy
without the need of deep linguistic knowledge or
domain-specific corpora.
Specifically, the ranking algorithm proceeds as
follows. First, words in a given document are added
as vertices in a graph G. Then, edges are added be-
tween words (vertices) if the words co-occur in a
fixed-sized window. The number of co-occurrences
becomes the weight of an edge. When the graph is
constructed, the score of each vertex is initialized
as 1, and the PageRank-based ranking algorithm is
run on the graph iteratively until convergence. The
TextRank score of a word w in document D at kth
iteration is defined as follows:
Rkw,D = (1? d)+ d ?
?
?j:(i,j)?G
ei,j?
?l:(j,l)?G ej,l
Rk?1w,D
(8)
where d is a damping factor usually set to 0.85, and
ei,j is an edge weight between i and j.
The assumption behind the use of the variant of
TextRank is that a word is likely to be an important
word in a document if it co-occurs frequently with
other important words in the document. Eventually,
words with low TextRank scores may be considered
as unimportant. The main differences of TextRank
compared to tf-idf is that it utilizes the context infor-
mation of words to assign term weights.
Figure 1 demonstrates that term weighting results
of TextRank and tf-idf are greatly different. Notice
that TextRank assigns low scores to words that co-
413
Corpus: (Q?A) Vocabulary Size (%chg) Average Translations (%chg)
tf-idf TextRank tf-idf TextRank
Initial 90,441 73
25%Removal 90,326 (?0.1%) 73,021 (?19.3%) 73 (?0.0%) 44 (?39.7%)
50%Removal 90,230 (?0.2%) 72,225 (?20.1%) 72 (?1.4%) 43 (?41.1%)
75%Removal 88,763 (?1.9%) 65,268 (?27.8%) 53 (?27.4%) 38 (?47.9%)
Avg.Score 66,412 (?26.6%) 31,849 (?64.8%) 14 (?80.8%) 18 (?75.3%)
Table 1: Impact of various word elimination strategies on translation model construction using (Q?A) corpus.
Corpus: (Q?Q) Vocabulary Size (%chg) Average Translations (%chg)
tf-idf TextRank tf-idf TextRank
Initial 34,485 442
25%Removal 34,374 (?0.3%) 26,900 (?22.0%) 437 (?1.1%) 282 (?36.2%)
50%Removal 34,262 (?0.6%) 26,421 (?23.4%) 423 (?4.3%) 274 (?38.0%)
75%Removal 32,813 (?4.8%) 23,354 (?32.3%) 288 (?34.8%) 213 (?51.8%)
Avg.Score 28,613 (?17.0%) 16,492 (?52.2%) 163 (?63.1%) 164 (?62.9%)
Table 2: Impact of various word elimination strategies on translation model construction using (Q?Q) corpus.
occur only with stopwords. This implies that Tex-
tRank weighs terms more ?strictly? than the tf-idf
approach, with use of contexts of words.
3.2 Deciding the Quantity to be Removed from
Ranked List
Once a final score (either tf-idf or TextRank score)
is obtained for each word, we create a list of words
ranked in decreasing order of their scores and elim-
inate the ones at lower ranks as unimportant words.
The question here is how to decide the proportion or
quantity to be removed from the ranked list.
Removing a fixed proportion: The first ap-
proach we have used is to decide the number of
unimportant words based on the size of the original
string. For our experiments, we manually vary the
proportion to be removed as 25%, 50%, and 75%.
For instance, if the proportion is set to 50% and an
original string consists of ten words, at most five
words would be remained as important words.
Using average score as threshold: We also have
used an alternate approach to deciding the quantity.
Instead of eliminating a fixed proportion, words are
removed if their score is lower than the average score
of all words in a document. This approach decides
the proportion to be removed more flexibly than the
former approach.
4 Building Compact Translation Models
We have initially built two parallel corpora from
a Q&A collection5, denoted as (Q?A) corpus and
(Q?Q) corpus henceforth, by varying the methods
in which parallel strings are gathered (described in
Section 2.2). The (Q?A) corpus consists of 85,938
parallel string pairs, and the (Q?Q) corpus contains
575,649 parallel string pairs.
In order to build compact translation models, we
have preprocessed the parallel corpus using differ-
ent word elimination strategies so that unimpor-
tant words would be removed from parallel strings.
We have also used a stoplist6 consisting of 429
words to remove stopwords. The out-of-the-box
GIZA++7 (Och and Ney, 2004) has been used to
learn translation models using the pre-processed par-
allel corpus for our retrieval experiments. We have
also trained initial translation models, using a par-
allel corpus from which only the stopwords are re-
moved, to compare with the compact translation
models.
Eventually, the number of parameters needed
for modeling translations would be minimized if
unimportant words are eliminated with different ap-
5Details on this data will be introduced in the next section.
6http://truereader.com/manuals/onix/stopwords1.html
7http://www.fjoch.com/GIZA++.html
414
proaches. Table 1 and 2 shows the impact of various
word elimination strategies on the construction of
compact translation models using the (Q?A) corpus
and the (Q?Q) corpus, respectively. The two tables
report the size of the vocabulary contained and the
average number of translations per word in the re-
sulting compact translation models, along with per-
centage decreases with respect to the initial transla-
tion models in which only stopwords are removed.
We make these observations:
? The translation models learned from the (Q?Q)
corpus have less vocabularies but more aver-
age translations per word than the ones learned
from the (Q?A) corpus. This result implies that
a large amount of noise may have been cre-
ated inevitably when a large number of parallel
strings (pairs of similar questions) were artifi-
cially gathered from the Q&A collection.
? The TextRank strategy tends to eliminate larger
sets of words as unimportant words than the
tf-idf strategy when a fixed proportion is re-
moved, regardless of the corpus type. Recall
that the TextRank approach assigns weights to
words more strictly by using contexts of words.
? The approach to remove words according to
the average weight of a document (denoted as
Avg.Score) tends to eliminate relatively larger
portions of words as unimportant words than
any of the fixed-proportion strategies, regard-
less of either the corpus type or the ranking
strategy.
5 Retrieval Experiments
Experiments have been conducted on a real world
Q&A collection to demonstrate the effectiveness of
compact translation models on Q&A retrieval.
5.1 Experimental Settings
In this section, four experimental settings for the
Q&A retrieval experiments are described in detail.
Data: For the experiments, Q&A data have been
collected from the Science domain of Yahoo! An-
swers, one of the most popular community-based
question answering service on the Web. We have
obtained a total of 43,001 questions with a best an-
swer (selected either by the questioner or by votes of
other users) by recursively traversing subcategories
of the Science domain, with up to 1,000 question
pages retrieved.8
Among the obtained Q&A pairs, 32 Q&A pairs
have been randomly selected as the test set, and the
remaining 42,969 questions have been the reference
set to be retrieved. Each Q&A pair has three text
fields: question title, question content, and answer.9
The fields of each Q&A pair in the test set are con-
sidered as various test queries; the question title,
the question content, and the answer are regarded
as a short query, a long query, and a supplementary
query, respectively. We have used long queries and
supplementary queries only in the relevance judg-
ment procedure. All retrieval experiments have been
conducted using short queries only.
Relevance judgments: To find relevant Q&A
pairs given a short query, we have employed a pool-
ing technique used in the TREC conference series.
We have pooled the top 40 Q&A pairs from each
retrieval results generated by varying the retrieval
algorithms, the search field, and the query type.
Popular word-based models, including the Okapi
BM25, query-likelihood language model, and pre-
vious translation-based models (Jeon et al, 2005),
have been used.10
Relevance judgments have been done by two stu-
dent volunteers (both fluent in English). Since
many community-based question answering ser-
vices present their search results in a hierarchical
fashion (i.e. a list of relevant questions is shown
first, and then the user chooses a specific question
from the list to see its answers), a Q&A pair has been
judged as relevant if its question is semantically sim-
ilar to the query; neither quality nor rightness of the
answer has not been considered. When a disagree-
ment has been made between two volunteers, one of
the authors has made the final judgment. As a result,
177 relevant Q&A pairs have been found in total for
the 32 short queries.
Baseline retrieval models: The proposed ap-
8Yahoo! Answers did not expose additional question pages
to external requests at the time of collecting the data.
9When collecting parallel strings from the Q&A collection,
we have put together the question title and the question content
as one question string.
10The retrieval model using compact translation models has
not been used in the pooling procedure.
415
proach to Q&A retrieval using compact translation
models (denoted as CTLM henceforth) is compared
to three baselines:
QLM: Query-likelihood language model for re-
trieval (equivalent to Equation 3, without use of
translation models). This model represents word-
based retrieval models widely used in practice.
TLM(Q?Q): Translation-based language model
for question retrieval (Jeon et al, 2005). This model
uses IBM Model 1 learned from the (Q?Q) corpus
of which stopwords are removed.
TLM(Q?A): A variant of the translation-based ap-
proach. This model uses IBM model 1 learned from
the (Q?A) corpus.
Evaluation metrics: We have reported the re-
trieval performance in terms of Mean Average Pre-
cision (MAP) and Mean R-Precision (R-Prec).
Average Precision can be computed based on the
precision at each relevant document in the ranking.
Mean Average Precision is defined as the mean of
the Average Precision values across the set of all
queries:
MAP (Q) = 1|Q|
?
q?Q
1
mq
mq?
k=1
Precision(Rk) (9)
where Q is the set of test queries, mq is the number
of relevant documents for a query q, Rk is the set of
ranked retrieval results from the top until rank posi-
tion k, and Precision(Rk) is the fraction of relevant
documents in Rk (Manning et al, 2008).
R-Precision is defined as the precision after
R documents have been retrieved where R is
the number of relevant documents for the current
query (Buckley and Voorhees, 2000). Mean R-
Precision is the mean of the R-Precisions across the
set of all queries.
We take MAP as our primary evaluation metric.
5.2 Experimental Results
Preliminary retrieval experiments have been con-
ducted using the baseline QLM and different fields
of Q&A data as retrieval unit. Table 3 shows the
effectiveness of each field.
The results imply that the question title field is the
most important field in our Yahoo! Answers collec-
tion; this also supports the observation presented by
Retrieval unit MAP R-Prec
Question title 0.1031 0.2396
Question content 0.0422 0.0999
Answer 0.0566 0.1062
Table 3: Preliminary retrieval results.
Model MAP R-Prec
(%chg) (%chg)
QLM 0.1031 0.2396
TLM(Q?Q)* 0.1121 0.2251
(49%) (?6%)
CTLM(Q?Q) 0.1415 0.2425
(437%) (41%)
TLM(Q?A) 0.1935 0.3135
(488%) (431%)
CTLM(Q?A) 0.2095 0.3585
(4103%) (450%)
Table 4: Comparisons with three baseline retrieval mod-
els. * indicates that it is equivalent to Jeon et al (2005)?s
approach. MAP improvements of CTLMs have been
tested to be statistically significant using paired t-test.
Jeon et al (2005). Based on the preliminary obser-
vations, all retrieval models tested in this paper have
ranked Q&A pairs according to the similarity scores
between queries and question titles.
Table 4 presents the comparison results of three
baseline retrieval models and the proposed CTLMs.
For each method, the best performance after empir-
ical ? parameter tuning according to MAP is pre-
sented.
Notice that both the TLMs and CTLMs have out-
performed the word-based QLM. This implies that
word-based models that do not address the issue of
lexical gaps between queries and questions often fail
to retrieve relevant Q&A data that have little word
overlap with queries, as noted by Jeon et al (2005).
Moreover, notice that the proposed CTLMs have
achieved significantly better performances in all
evaluation metrics than both QLM and TLMs, regard-
less of the parallel corpus in which the incorporated
translation models are trained from. This is a clear
indication that the use of compact translation models
built with appropriate word elimination strategies is
effective in closing the query-question lexical gaps
416
(Q?Q) MAP (%chg)
tf-idf TextRank
Initial 0.1121
25%Rmv 0.1141 (41.8) 0.1308 (416.7)
50%Rmv 0.1261 (412.5) 0.1334 (419.00)
75%Rmv 0.1115 (?0.5) 0.1160 (43.5)
Avg.Score 0.1056 (?5.8) 0.1415 (426.2)
Table 5: Contributions of various word elimination strate-
gies on MAP performance of CTLM(Q?Q).
(Q?A) MAP (%chg)
tf-idf TextRank
Initial 0.1935
25%Rmv 0.2095 (48.3) 0.1733 (?10.4)
50%Rmv 0.2085 (47.8) 0.1623 (?16.1)
75%Rmv 0.1449 (?25.1) 0.1515 (?21.7)
Avg.Score 0.1168 (?39.6) 0.1124 (?41.9)
Table 6: Contributions of various word elimination strate-
gies on MAP performance of CTLM(Q?A).
for improving the performance of question retrieval
in the context of language modeling framework.
Note that the retrieval performance varies by the
type of training corpus; CTLM(Q?A) has outper-
formed CTLM(Q?Q) significantly. This proves the
statement we made earlier that the (Q?Q) corpus
would contain much noise since the translation mod-
els learned from the (Q?Q) corpus tend to have
smaller vocabulary sizes but significantly more aver-
age translations per word than the ones learned from
the (Q?A) corpus.
Table 5 and 6 show the effect of various word
elimination strategies on the retrieval performance
of CTLMs in which the incorporated compact trans-
lation models are trained from the (Q?Q) corpus and
the (Q?A) corpus, respectively. It is interesting to
note that the importance of modifications in word
elimination strategies also varies by the type of train-
ing corpus.
The retrieval results indicate that when the trans-
lation model is trained from the ?less noisy? (Q?A)
corpus, eliminating a relatively large proportions of
words may hurt the retrieval performance of CTLM.
In the case when the translation model is trained
from the ?noisy? (Q?Q) corpus, a better retrieval
performance may be achieved if words are elimi-
nated appropriately to a certain extent.
In terms of weighting scheme, the TextRank ap-
proach, which is more ?strict? than tf-idf in elim-
inating unimportant words, has led comparatively
higher retrieval performances on all levels of re-
moval quantity when the translation model has been
trained from the ?noisy? (Q?Q) corpus. On the con-
trary, the ?less strict? tf-idf approach has led better
performances when the translation model has been
trained from the ?less noisy? (Q?A) corpus.
In summary, the results imply that the perfor-
mance of translation-based retrieval models can be
significantly improved when strategies for building
of compact translation models are chosen properly,
regarding the expected noise level of the parallel cor-
pus for training the translation models. In a case
where a noisy parallel corpus is given for training
of translation models, it is better to get rid of noise
as much as possible by using ?strict? term weight-
ing algorithms; when a less noisy parallel corpus is
given for building the translation models, a tolerant
approach would yield better retrieval performance.
6 Related Works
Our work is most closely related to Jeon et
al. (2005)?s work, which addresses the issue of
word mismatch between queries and questions in
large online Q&A collections by using translation-
based methods. Apart from their work, there have
been some related works on applying translation-
based methods for retrieving FAQ data. Berger et
al. (2000) report some of the earliest work on FAQ
retrieval using statistical retrieval models, includ-
ing translation-based approaches, with a small set
of FAQ data. Soricut and Brill (2004) present an an-
swer passage retrieval system that is trained from 1
million FAQs collected from the Web using trans-
lation methods. Riezler et al (2007) demonstrate
the advantages of translation-based approach to an-
swer retrieval by utilizing a more complex trans-
lation model also trained from a large amount of
data extracted from FAQs on the Web. Although all
of these translation-based approaches are based on
the statistical translation models, including the IBM
Model 1, none of them focus on addressing the noise
issues in translation models.
417
7 Conclusion and Future Work
Bridging the query-question gap has been a major is-
sue in retrieval models for large online Q&A collec-
tions. In this paper, we have shown that the perfor-
mance of translation-based retrieval on real online
Q&A collections can be significantly improved by
using compact translation models of which the noise
(unimportant word translations) is properly reduced.
We have also observed that the performance en-
hancement may be achieved by choosing the appro-
priate strategies regarding the strictness of various
term weighting algorithms and the expected noise
level of the parallel data for learning such transla-
tion models.
Future work will focus on testing the effective-
ness of the proposed method on a larger set of Q&A
collections with broader domains. Since the pro-
posed approach cannot handle many-to-one or one-
to-many word transformations, we also plan to in-
vestigate the effectiveness of phrase-based transla-
tion models in closing gaps between queries and
questions for further enhancement of Q&A retrieval.
Acknowledgments
This work was supported by Microsoft Research
Asia. Any opinions, findings, and conclusions or
recommendations expressed above are those of the
authors and do not necessarily reflect the views of
the sponsor.
References
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the Lexical
Chasm: Statistical Approaches to Answer-Finding. In
Proceedings of the 23rd Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval, pages 192?199.
Adam Berger and John Lafferty. 1999. Information Re-
trieval as Statistical Translation. In Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 222?229.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Buckley and Ellen M. Voorhees. 2000. Evaluating
Evaluation Measure Stability. In Proceedings of the
23rd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 33?40.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding Similar Questions in Large Question and An-
swer Archives. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, pages 84?90.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Text. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 404?411.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Jay M. Ponte and W. Bruce Croft. 1998. A Language
Modeling Approach to Information Retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 275?281.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
Machine Translation for Query Expansion in Answer
Retrieval. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
pages 464?471.
Radu Soricut and Eric Brill. 2004. Automatic Question
Answering: Beyond the Factoid. In Proceedings of
the 2004 Human Language Technology and Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 57?64.
418
Coling 2010: Poster Volume, pages 623?629,
Beijing, August 2010
A Post-processing Approach to Statistical Word Alignment  
Reflecting Alignment Tendency between Part-of-speeches 
Jae-Hee Lee1, Seung-Wook Lee1, Gumwon Hong1, 
Young-Sook Hwang2, Sang-Bum Kim2,  Hae-Chang Rim1 
1Dept. of Computer and Radio Communications Engineering, Korea University 
2Institute of Future Technology, SK Telecom 
1{jlee,swlee,gwhong,rim}@nlp.korea.ac.kr, 
2{yshwang,sangbum.kim}@sktelecom.com 
 
Abstract 
Statistical word alignment often suffers 
from data sparseness. Part-of-speeches 
are often incorporated in NLP tasks to 
reduce data sparseness. In this paper, 
we attempt to mitigate such problem by 
reflecting alignment tendency between 
part-of-speeches to statistical word 
alignment. Because our approach does 
not rely on any language-dependent 
knowledge, it is very simple and purely 
statistic to be applied to any language 
pairs. End-to-end evaluation shows that 
the proposed method can improve not 
only the quality of statistical word 
alignment but the performance of sta-
tistical machine translation. 
1 Introduction 
Word alignment is defined as mapping corre-
sponding words in parallel text. A word 
aligned parallel corpora are very valuable re-
sources in NLP. They can be used in various 
applications such as word sense disambigua-
tion, automatic construction of bilingual lexi-
con, and statistical machine translation (SMT). 
In particular, the initial quality of statistical 
word alignment dominates the quality of SMT 
(Och and Ney 2000; Ganchev et al, 2008); 
almost all current SMT systems basically refer 
to the information inferred from word align-
ment result. 
One of the widely used approaches to statis-
tical word alignment is based on the IBM 
models (Brown et al, 1993). IBM models are 
constructed based on words? co-occurrence 
and positional information. If sufficient train-
ing data are given, IBM models can be suc-
cessfully applied to any language pairs. How-
ever, for minority language pairs such as Eng-
lish-Korean and Swedish-Japanese, it is very 
difficult to obtain large amounts of parallel 
corpora. Without sufficient amount of parallel 
corpus, it is very difficult to learn the correct 
correspondences between words that infre-
quently occur in the training data. 
Part-of-speeches (POS), which represent 
morphological classes of words, can give valu-
able information about individual words and 
their neighbors. Identifying whether a word is 
a noun or a verb can let us predict which words 
are likely to be mapped in word alignment and 
which words are likely to occur in its vicinity 
in target sentence generation. 
Many studies incorporate POS information 
in SMT. Some researchers perform POS tag-
ging on their bilingual training data (Lee et al, 
2006; Sanchis and S?nchez, 2008). Some of 
them replace individual words as new words, 
such as in ?word/POS? form, producing new, 
extended vocabulary. The advantage of this 
approach is that POS information can help to 
resolve lexical ambiguity and thus improve 
translation quality. 
On the other hand, Koehn et al (2007) pro-
pose a factored translation model that can in-
corporate any linguistic factors including POS 
information in phrase-based SMT. The model 
provides a generalized representation of a 
translation model, because it can map multiple 
source and target factors. 
Although all of these approaches are shown 
to improve SMT performance by utilizing POS 
information, we observe that the influence is 
virtually marginal in two ways: 
623
 
 
 
 
1) The POS information tagged to each word 
may help to disambiguate in selecting 
word correspondences, but the increased 
vocabulary can also make the training data 
more sparse. 
2) The factored translation model may help to 
effectively handle out-of-vocabulary 
(OOV) by incorporating many linguistic 
factors, but it still crucially relies on the in-
itial quality of word alignment that will 
dominate the translation probabilities. 
This paper focuses on devising a better 
method for incorporating POS information in 
word alignment. It attempts to answer the fol-
lowing questions: 
1) Can the information regarding POS align-
ment tendency affect the post-processing 
of word alignment? 
2) Can the result of word alignment affected 
by such information help improving the 
quality of SMT? 
2 POS Alignment Tendency 
Despite the language pairs, words with similar 
POSs often correspond to each other in statisti-
cal word alignment. Similarly, words with dif-
ferent POSs are seldom aligned. For example, 
Korean proper nouns very often align with 
English proper nouns very often but seldom 
align with English adverbs. We believe that 
this phenomenon occurs not only on English-
Korean pairs but also on most of other lan-
guage pairs.  
Thus, in this study we hypothesize that all 
source language (SL) POSs have some rela-
tionship with target language (TL) POSs. Fig-
ure 1 exemplifies some results of using the 
IBM Models in English-Korean word align-
ment. As can be seen in the figure, the English 
word ?visiting? is incorrectly and excessively 
aligned to four Korean morphemes ?maejang?, 
?chat?, ?yeoseong?, and ?gogaek?. One reason 
for this is the sparseness of the training data; 
the only correct Korean morpheme ?chat? does 
not sufficiently co-occur with ?visiting? in the 
training data. However, it is generally believed 
that an English verb is more likely aligned to a 
Korean verb rather than a Korean noun. Like-
wise, we suppose that among many POSs, 
there are strong relationships between similar 
POSs and relatively weak relationships be-
tween different POSs. We hypothesize that the 
discovery of such relationships in advance can 
lead to better word alignment results. 
 In this paper, we propose a new method to 
obtain the relationship from word alignment 
results. The relationships among POSs, hence-
forth the POS alignment tendency, can be 
identified by the probability of the given POS 
pairs? alignment result where the source lan-
guage POS and the target language POS co-
occur in bilingual sentences. We formulate this 
idea using the maximum likelihood estimation 
as follows: 
     (          |   ( )    ( ))   
 
    
     (              ( )    ( ))
?      (           ( )    ( ))  *          +
 
 
where f and e denote source word and target 
word respectively. count() is a function that 
returns the number of co-occurrence of f and e 
when they are aligned (or not aligned). Then, 
we adjust the formula with the existing align-
ment score between f and e. 
     (   )        (   )   
             (   ) (          |   ( )    ( )) 
 
where )|( efPIBM  indicates the alignment prob-
ability estimated by the IBM models.   is a 
weighting parameter to interpolate the reliabili-
ties of both alignment factors. In the expe-
 
Figure 1. An example of inaccurate word alignment 
624
 
 
 
 
riment,   is empirically set to improve the 
word alignment performance ( =0.5).  
3 Modifying Alignment 
Based on the new scoring scheme as intro-
duced in the previous section, we modify the 
result of the initial word alignment. The modi-
fication is performed in the following proce-
dure: 
1. For each source word f that has out-bound 
alignment link other than null, 
2. Find the target word e that has the maxi-
mum alignment score according to the 
proposed alignment adjustment measure, 
and change the alignment result by map-
ping f to e. 
This modification guarantees that the number 
of alignment does not change; the algorithm is 
designed to minimize the risk by maintaining 
the fertility of a word estimated by the IBM 
Model. Figure 2 illustrates the result before 
and after the alignment modification. Incor-
rectly links from e1 and e3 are deleted and 
missing links from e2 and e4 are generated dur-
ing this alignment modification. 
The alignment modification through the re-
flection of POS alignment tendency is per-
formed on both e-to-f and f-to-e bidirectional 
word alignments. The bidirectional word 
alignment results are then symmetrized. 
4 Experiments 
In this paper, we attempt to reflect the POS 
alignment tendency in improving the word 
alignment performance. This section provides 
the experimental setup and the results that 
demonstrate whether the proposed approach 
can improve the statistical word alignment per-
formance. 
We collected bilingual texts from major bi-
lingual news broadcasting sites. 500K sentence 
pairs are collected and refined manually to 
construct correct parallel sentences pairs. The 
same number of monolingual sentences is also 
used from the same sites to train Korean lan-
guage. We also prepared a subset of the bilin-
gual text with the size of 50K to show that the 
proposed model is very effective when the 
training set is small. 
In order to evaluate the performance of 
word alignment, we additionally constructed a 
reference set with 400 sentence pairs. The 
evaluation is performed using precision, recall, 
and F-score. We use the GIZA++ toolkit for 
word alignment as well as four heuristic sym-
metrizations: intersection, union, grow-diag-
final, and grow-diag (Och, 2000).  
4.1 Word Alignment 
We now evaluate the effectiveness of the pro-
posed word alignment method. Table 1 and 2 
report the experimental results by adding POS 
information to the parallel corpus. ?Lexical? 
denotes the result of conventional word align-
ment produced by GIZA++. No pre-processing 
or post-processing is applied in this result. 
?Lemma/POS? is the result of word alignment 
with the pre-processing introduced Lee et al 
(2006). Compared to the result, lemmatized 
lexical and POS tags are proven to be useful 
information for word alignment. ?Lemma/POS? 
consistently outperforms ?Lexical? despite the 
symmetrization heuristics in terms of precision, 
recall and F-score. We expect this improve-
ment is benefited from the alleviated data 
sparseness by using lemmatized lexical and 
POS tags rather than using the lexical itself. 
 
 
Figure 2. An example of word alignment modification 
625
 
 
 
 
  
 Alignment heuristic Precision Recall F-score 
Lexical 
Intersection 94.0% 50.8% 66.0% 
Union 53.2% 81.2% 64.3% 
Grow-diag-final 54.6% 80.9% 65.2% 
Grow-diag 60.9% 67.2% 63.9% 
Lemma/POS 
Intersection 95.8% 55.3% 70.1% 
Union 58.1% 83.3% 68.4% 
Grow-diag-final 59.7% 83.0% 69.5% 
Grow-diag 67.0% 71.6% 69.2% 
Lemma/POS 
+ POS alignment 
tendency 
Intersection 96.1% 63.5% 76.5% 
Union 67.4% 85.1% 75.2% 
Grow-diag-final 69.8% 84.9% 76.6% 
Grow-diag 80.0% 77.0% 78.5% 
Table 1. The performance of word alignment using small training set (50k pairs) 
 
Experimental Setup Alignment heuristic Precision Recall F-score 
Lexical 
Intersection 96.8% 64.9% 77.7% 
Union 66.6% 87.4% 75.6% 
Grow-diag-final 67.8% 87.1% 76.2% 
Grow-diag 74.4% 79.2% 76.7% 
Lemma/POS 
Intersection 97.3% 66.2% 78.8% 
Union 70.7% 89.0% 78.8% 
Grow-diag-final 72.1% 88.8% 79.6% 
Grow-diag 78.8% 80.5% 79.7% 
Lemma/POS 
+ POS alignment 
tendency 
Intersection 97.2% 69.3% 80.9% 
Union 73.9% 86.7% 79.8% 
Grow-diag-final 75.6% 86.4% 80.7% 
Grow-diag 85.2% 81.5% 83.4% 
Table 2. The performance of word alignment using a large training set (500k pairs) 
 
Experimental Setup Symmetrization Heuristic BLEU(50k) BLEU (500k) 
Lexical 
Intersection 20.1% 29.2% 
Union 18.6% 27.2% 
Grow-diag-final 19.9% 27.7% 
Grow-diag 20.2% 29.4% 
Lemma/POS 
Intersection 20.3% 26.4% 
Union 18.5% 27.8% 
Grow-diag-final 20.1% 29.2% 
Grow-diag 20.4% 30.8% 
Factored Model 
(Lemma, POS) 
Intersection 20.5% 30.0% 
Union 18.1% 27.5% 
Grow-diag-final 20.3% 28.2% 
Grow-diag 20.9% 31.1% 
Lemma/POS 
+ POS alignment 
tendency 
Intersection 21.8% 29.3% 
Union 19.5% 27.2% 
Grow-diag-final 21.3% 28.4% 
Grow-diag 20.8% 29.1% 
Table  3. The performance of translation 
626
 
 
 
 
 
Since lemmatized lexical and POS tags are 
shown to be useful, our post-processing meth-
od is applied to ?Lemma/POS?. 
The experimental results show that the pro-
posed method consistently improves word 
alignment in terms of F-score. It is interesting 
that the proposed method improves the recall 
of the intersection result and the precision of 
the union result. Thus, the proposed method 
achieves the best alignment performance. 
As can be seen in Table 1 and 2, our method 
consistently improves the performance of word 
alignment despite the size of training data. In a 
small data set, the improvement of our method 
is much higher than that in a large set. This 
implies that our method is more helpful when 
the training data set is insufficient.  
We investigate whether the proposed meth-
od actually alleviates the data sparseness prob-
lem by analyzing the aligned word pairs of low 
co-occurrence frequency. There are multiple 
word pairs that share the same number of co-
occurrence in the corpus. For example, let us 
assume that ?report-bogoha?, ?newspaper-
sinmun? and ?China-jungguk? pairs are co-
occurred 1,000 times. We can calculate the 
mean of their individual recalls. We refer to 
this new measurement as average recall. The 
average recalls of these pairs are relatively 
higher than those of pairs with low co-
occurrence frequency such as ?food-jinji? and 
?center-chojeom? pairs. These pairs are diffi-
cult to be linked, because the word alignment 
model suffers from data sparseness when esti-
mating their translation probability.  
Figure 3 shows the average recall according 
to the number of co-occurrence. We can ob-
serve that the word alignment model tends to 
link word pairs more correctly if they are more 
frequently co-occurred. Both ?Lemma/POS? 
and our method consistently show higher aver-
age recall throughout all frequencies, and the 
proposed method shows the best performance. 
It is also notable that the both ?Lemma/POS? 
and our method achieve much more improve-
ment for low co-occurrence frequencies (e.g., 
11~40). This implies that the proposed method 
incorporates POS information more effectively 
than the previous method, since the proposed 
method achieves much higher average recall. 
4.2 Statistical Machine Translation 
Next, we examine the effect of the improve-
ment of the word alignment on the translation 
quality. For this, we built some SMT systems 
with the word alignment results. We use the 
Moses toolkit for translation (Koehn et al, 
2007). Moses is an implementation of phrase-
based statistical machine translation model that 
has shown a state-of-the-art performance in 
various evaluation sets. We also perform the 
evaluation of the Factored model (Koehn et al, 
2007) using Moses.  
To investigate how the improved word 
alignment affect the quality of machine trans-
lation, we calculate the BLEU score for trans-
lation results with different word alignment 
settings as shown in Table 3. First of all, we 
can easily conclude that the quality of the 
translation is strongly dominated by the size of 
the training data. We can also find that the 
quality of the translation is correlated to the 
performance of the word alignment. 
For a small test set, the proposed method 
achieved the best performance in terms of 
BLEU (21.8%). For a larger test set, however, 
the proposed method could not improve the 
performance of the translation with better word 
alignment. It is not feasible to investigate the 
factors that affect this deterioration, since Mo-
ses is a black box module to our system. The 
training of the phrase-based SMT model in-
volves the extraction of phrases, and the result 
of word alignment is reflected within this pro-
cess. When the training data is small, the num-
ber of extracted phrases is also apparently 
small. However, abundant phrases are extract-
ed from a large amount of training data. In this 
case, we hypothesize that the most plausible 
 
Figure 3. Average recall of word alignment pairs 
according to the number of their co-occurrence 
627
 
 
 
 
phrases are already obtained, and the effect of 
more accurate word alignment seems insignifi-
cant. More thorough analysis of this is re-
mained as future work. 
4.3 Acquisition of Bilingual Dictionary 
One of the most applications of word align-
ment is the construction of bilingual dictionar-
ies. By using word alignment, we can collect a 
(ranked) list of bilingual word pairs. Table 4 
reports the top 10 translations (the most ac-
ceptable target words to align) for Korean 
word ?bap? (food). The table contains the 
probabilities estimated by the IBM Models, the 
adjusted scores, and the number of co-
occurrence, respectively. Italicized translations 
are in fact incorrect translations. Highlighted 
ones are new translation candidates that are 
correct. As can be seen in the table, the pro-
posed approach shows a positive effect of rais-
ing new and better candidates for translation. 
For example, ?bread? and ?breakfast? have 
come up to the top 10 translations. This 
demonstrates that the low co-occurrences of 
?bap? with ?bread? and ?breakfast? are not 
suitably handled by alignments solely based on 
lexicals. However, the proposed approach 
ranks them at higher positions by reflecting the 
alignment tendency of POSs. 
5 Conclusion 
In this paper, we propose a new method for 
incorporating the POS alignment tendency to 
improve traditional word alignment model in 
post processing step. Experimental results 
show that the proposed method helps to allevi-
ate the data sparseness problem especially 
when the training data is insufficient. 
It is still difficult to conclude that better 
word alignment always leads to better transla-
tion. We plan on investigating the effective-
ness of the proposed method using other trans-
lation system, such as Hiero (Chiang et al, 
2005). We also plan to incorporate our method 
into other effective models, such as Factored 
translation model. 
References 
David Chiang et al, 2005. The Hiero machine 
translation system: Extensions, evaluation, and 
analysis. In Proc. of HLT-EMLP:779?786, Oct. 
 
Franz Josef Och. 2000. Giza++: Training of statis-
tical translation models. Available at http://www-
i6.informatik.rwthaachen.de/?och/software/GIZA+
+.html. 
 
Franz Josef Och & Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models. Computational Linguistics 29 (1):19-51. 
 
G. Sanchis and J.A. S?nchez. Vocabulary exten- 
sion via POS information for SMT. In Mixing 
Approaches to Machine Translation, 2008. 
 
Jonghoon Lee, Donghyeon Lee and Gary Geunbae 
Lee. Improving Phrase-based Korean-English Sta-
tistical Machine Translation. INTERSPEECH 2006. 
 
Kuzman Ganchev, Joao V. Graca and Ben Taskar. 
2008. Better Alignments = Better Translations? 
Proceedings of ACL-08: HLT: 986?993. 
 
Peter F. Brown et al,1993. The Mathematics of 
Statistical Machine Translation: Parameter Esti-
mation. Computational Linguistics 9(2): 263-311 
 
Rank 
IBM Model POS Alignment Tendency 
translation     (   ) #co-occur translation      (   ) #co-occur 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
bob/NNP 
rice/NN 
eat/VB 
meal/NN 
food/NN 
bob/NN 
feed/VB 
cook/VB 
living/NN 
dinner/NN 
0.348 
0.192 
0.107 
0.075 
0.043 
0.038 
0.010 
0.010 
0.008 
0.008 
83 
73 
57 
43 
29 
10 
7 
9 
4 
10 
bob/NNP 
rice/NN 
meal/NN 
food/NN 
eat/VB 
bob/NN 
living/NN 
dinner/NN 
bread/NN 
breakfast/NN 
0.214 
0.136 
0.078 
0.062 
0.061 
0.059 
0.045 
0.044 
0.044 
0.043 
83 
73 
43 
29 
57 
10 
4 
10 
9 
6 
Table 4. Top 10 translations for Korean word ?bap? (food). 
628
 
 
 
 
Philipp Koehn and Hieu Hoang. Factored Transla-
tion Models. EMNLP 2007. 
 
Phillipp Koehn et al, 2007. Moses: Open source 
toolkit for statistical machine translation.In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, demonstation ses-
sion. 
629
