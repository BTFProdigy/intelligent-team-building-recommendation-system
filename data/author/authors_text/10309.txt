Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 410?418,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Bridging Lexical Gaps between Queries and Questions on
Large Online Q&A Collections with Compact Translation Models
Jung-Tae Lee? and Sang-Bum Kim? and Young-In Song? and Hae-Chang Rim?
?Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
?Search Business Team, SK Telecom, Seoul, Korea
?Dept. of Computer Science & Engineering, Korea University, Seoul, Korea
{jtlee,sbkim,song,rim}@nlp.korea.ac.kr
Abstract
Lexical gaps between queries and questions
(documents) have been a major issue in ques-
tion retrieval on large online question and
answer (Q&A) collections. Previous stud-
ies address the issue by implicitly expanding
queries with the help of translation models
pre-constructed using statistical techniques.
However, since it is possible for unimpor-
tant words (e.g., non-topical words, common
words) to be included in the translation mod-
els, a lack of noise control on the models can
cause degradation of retrieval performance.
This paper investigates a number of empirical
methods for eliminating unimportant words in
order to construct compact translation mod-
els for retrieval purposes. Experiments con-
ducted on a real world Q&A collection show
that substantial improvements in retrieval per-
formance can be achieved by using compact
translation models.
1 Introduction
Community-driven question answering services,
such as Yahoo! Answers1 and Live Search QnA2,
have been rapidly gaining popularity among Web
users interested in sharing information online. By
inducing users to collaboratively submit questions
and answer questions posed by other users, large
amounts of information have been collected in the
form of question and answer (Q&A) pairs in recent
years. This user-generated information is a valu-
able resource for many information seekers, because
1http://answers.yahoo.com/
2http://qna.live.com/
users can acquire information straightforwardly by
searching through answered questions that satisfy
their information need.
Retrieval models for such Q&A collections
should manage to handle the lexical gaps or word
mismatches between user questions (queries) and
answered questions in the collection. Consider the
two following examples of questions that are seman-
tically similar to each other:
? ?Where can I get cheap airplane tickets??
? ?Any travel website for low airfares??
Conventional word-based retrieval models would
fail to capture the similarity between the two, be-
cause they have no words in common. To bridge the
query-question gap, prior work on Q&A retrieval by
Jeon et al (2005) implicitly expands queries with the
use of pre-constructed translation models, which lets
you generate query words not in a question by trans-
lation to alternate words that are related. In prac-
tice, these translation models are often constructed
using statistical machine translation techniques that
primarily rely on word co-occurrence statistics ob-
tained from parallel strings (e.g., question-answer
pairs).
A critical issue of the translation-based ap-
proaches is the quality of translation models con-
structed in advance. If no noise control is conducted
during the construction, it is possible for translation
models to contain ?unnecessary? translations (i.e.,
translating a word into an unimportant word, such as
a non-topical or common word). In the query expan-
sion viewpoint, an attempt to identify and decrease
410
the proportion of unnecessary translations in a trans-
lation model may produce an effect of ?selective?
implicit query expansion and result in improved re-
trieval. However, prior work on translation-based
Q&A retrieval does not recognize this issue and uses
the translation model as it is; essentially no attention
seems to have been paid to improving the perfor-
mance of the translation-based approach by enhanc-
ing the quality of translation models.
In this paper, we explore a number of empiri-
cal methods for selecting and eliminating unimpor-
tant words from parallel strings to avoid unnecessary
translations from being learned in translation models
built for retrieval purposes. We use the term compact
translation models to refer to the resulting models,
since the total number of parameters for modeling
translations would be minimized naturally. We also
present experiments in which compact translation
models are used in Q&A retrieval. The main goal of
our study is to investigate if and how compact trans-
lation models can improve the performance of Q&A
retrieval.
The rest of this paper is organized as follows.
The next section introduces a translation-based re-
trieval model and accompanying techniques used to
retrieve query-relevant questions. Section 3 presents
a number of empirical ways to select and eliminate
unimportant words from parallel strings for training
compact translation models. Section 4 summarizes
the compact translation models we built for retrieval
experiments. Section 5 presents and discusses the
results of retrieval experiments. Section 6 presents
related works. Finally, the last section concludes the
paper and discusses future directions.
2 Translation-based Retrieval Model
This section introduces the translation-based lan-
guage modeling approach to retrieval that has been
used to bridge the lexical gap between queries and
already-answered questions in this paper.
In the basic language modeling framework for re-
trieval (Ponte and Croft, 1998), the similarity be-
tween a query Q and a document D for ranking may
be modeled as the probability of the document lan-
guage model MD built from D generating Q:
sim(Q,D) ? P (Q|MD) (1)
Assuming that query words occur independently
given a particular document language model, the
query-likelihood P (Q|MD) is calculated as:
P (Q|MD) =
?
q?Q
P (q|MD) (2)
where q represents a query word.
To avoid zero probabilities in document language
models, a mixture between a document-specific
multinomial distribution and a multinomial distribu-
tion estimated from the entire document collection
is widely used in practice:
P (Q|MD) =
?
q?Q
[
(1? ?) ? P (q|MD)
+? ? P (q|MC)
]
(3)
where 0 < ? < 1 and MC represents a language
model built from the entire collection. The probabil-
ities P (w|MD) and P (w|MC) are calculated using
maximum likelihood estimation.
The basic language modeling framework does not
address the issue of lexical gaps between queries
and question. Berger and Lafferty (1999) viewed
information retrieval as statistical document-query
translation and introduced translation models to map
query words to document words. Assuming that
a translation model can be represented by a condi-
tional probability distribution of translation T (?|?)
between words, we can model P (q|MD) in Equa-
tion 3 as:
P (q|MD) =
?
w?D
T (q|w)P (w|MD) (4)
where w represents a document word.3
The translation probability T (q|w) virtually rep-
resents the degree of relationship between query
word q and document word w captured in a differ-
ent, machine translation setting. Then, in the tra-
ditional information retrieval viewpoint, the use of
translation models produce an implicit query expan-
sion effect, since query words not in a document are
mapped to related words in the document. This im-
plies that translation-based retrieval models would
make positive contributions to retrieval performance
only when the pre-constructed translation models
have reliable translation probability distributions.
3The formulation of our retrieval model is basically equiva-
lent to the approach of Jeon et al (2005).
411
2.1 IBM Translation Model 1
Obviously, we need to build a translation model in
advance. Usually the IBM Model 1, developed in
the statistical machine translation field (Brown et al,
1993), is used to construct translation models for
retrieval purposes in practice. Specifically, given a
number of parallel strings, the IBM Model 1 learns
the translation probability from a source word s to a
target word t as:
T (t|s) = ??1s
N?
i
c(t|s;Ji) (5)
where ?s is a normalization factor to make the sum
of translation probabilities for the word s equal to 1,
N is the number of parallel string pairs, and Ji is the
ith parallel string pair. c(t|s; Ji) is calculated as:
c(t|s; Ji) =
( P (t|s)
P (t|s1) + ? ? ?+ P (t|sn)
)
?freqt,Ji ? freqs,Ji (6)
where {s1, . . . , sn} are words in the source text in
J i. freqt,Ji and freqs,Ji are the number of times
that t and s occur in Ji, respectively.
Given the initial values of T (t|s), Equations (5)
and (6) are used to update T (t|s) repeatedly until
the probabilities converge, in an EM-based manner.
Note that the IBM Model 1 solely relies on
word co-occurrence statistics obtained from paral-
lel strings in order to learn translation probabilities.
This implies that if parallel strings have unimportant
words, a resulted translation model based on IBM
Model 1 may contain unimportant words with non-
zero translation probabilities.
We alleviate this drawback by eliminating unim-
portant words from parallel strings, avoiding them
from being included in the conditional translation
probability distribution. This naturally induces the
construction of compact translation models.
2.2 Gathering Parallel Strings from Q&A
Collections
The construction of statistical translation models
previously discussed requires a corpus consisting of
parallel strings. Since monolingual parallel texts are
generally not available in real world, one must arti-
ficially generate a ?synthetic? parallel corpus.
Question and answer as parallel pairs: The
simplest approach is to directly employ questions
and their answers in the collections by setting ei-
ther as source strings and the other as target strings,
with the assumption that a question and its cor-
responding answer are naturally parallel to each
other. Formally, if we have a Q&A collection as
C = {D1, D2, . . . , Dn}, where Di refers to an ith
Q&A data consisting of a question qi and its an-
swer ai, we can construct a parallel corpus C ? as
{(q1, a1), . . . , (qn, an)}?{(a1, q1), . . . , (an, qn)} =
C ? where each element (s, t) refers to a parallel pair
consisting of source string s and target string t. The
number of parallel string samples would eventually
be twice the size of the collections.
Similar questions as parallel pairs: Jeon et
al. (2005) proposed an alternative way of auto-
matically collecting a relatively larger set of par-
allel strings from Q&A collections. Motivated
by the observation that many semantically identi-
cal questions can be found in typical Q&A collec-
tions, they used similarities between answers cal-
culated by conventional word-based retrieval mod-
els to automatically group questions in a Q&A col-
lection as pairs. Formally, two question strings qi
and qj would be included in a parallel corpus C ?
as {(qi, qj), (qj , qi)} ? C ? only if their answer
strings ai and aj have a similarity higher than a
pre-defined threshold value. The similarity is cal-
culated as the reverse of the harmonic mean of ranks
as sim(ai, aj) = 12( 1rj + 1ri ), where rj and ri refer to
the rank of the aj and ai when ai and aj are given as
queries, respectively. This approach may artificially
produce much more parallel string pairs for training
the IBM Model 1 than the former approach, depend-
ing on the threshold value.4
To our knowledge, there has not been any study
comparing the effectiveness of the two approaches
yet. In this paper, we try both approaches and com-
pare the effectiveness in retrieval performance.
3 Eliminating Unimportant Words
We adopt a term weight ranking approach to iden-
tify and eliminate unimportant words from parallel
strings, assuming that a word in a string is unim-
4We have empirically set the threshold (0.05) for our exper-
iments.
412
Figure 1: Term weighting results of tf-idf and TextRank (window=3). Weighting is done on underlined words only.
portant if it holds a relatively low significance in the
document (Q&A pair) of which the string is origi-
nally taken from. Some issues may arise:
? How to assign a weight to each word in a doc-
ument for term ranking?
? How much to remove as unimportant words
from the ranked list?
The following subsections discuss strategies we use
to handle each of the issues above.
3.1 Assigning Term Weights
In this section, the two different term weighting
strategies are introduced.
tf-idf: The use of tf-idf weighting on evaluating
how unimportant a word is to a document seems to
be a good idea to begin with. We have used the fol-
lowing formulas to calculate the weight of word w
in document D:
tf -idfw,D = tfw,D ? idfw (7)
tfw,D = freqw,D|D| , idfw = log
|C|
dfw
where freqw,D refers to the number of times w oc-
curs in D, |D| refers to the size of D (in words), |C|
refers to the size of the document collection, and dfw
refers to the number of documents where w appears.
Eventually, words with low tf-idf weights may be
considered as unimportant.
TextRank: The task of term weighting, in fact,
has been often applied to the keyword extraction
task in natural language processing studies. As
an alternative term weighting approach, we have
used a variant of Mihalcea and Tarau (2004)?s Tex-
tRank, a graph-based ranking model for keyword
extraction which achieves state-of-the-art accuracy
without the need of deep linguistic knowledge or
domain-specific corpora.
Specifically, the ranking algorithm proceeds as
follows. First, words in a given document are added
as vertices in a graph G. Then, edges are added be-
tween words (vertices) if the words co-occur in a
fixed-sized window. The number of co-occurrences
becomes the weight of an edge. When the graph is
constructed, the score of each vertex is initialized
as 1, and the PageRank-based ranking algorithm is
run on the graph iteratively until convergence. The
TextRank score of a word w in document D at kth
iteration is defined as follows:
Rkw,D = (1? d)+ d ?
?
?j:(i,j)?G
ei,j?
?l:(j,l)?G ej,l
Rk?1w,D
(8)
where d is a damping factor usually set to 0.85, and
ei,j is an edge weight between i and j.
The assumption behind the use of the variant of
TextRank is that a word is likely to be an important
word in a document if it co-occurs frequently with
other important words in the document. Eventually,
words with low TextRank scores may be considered
as unimportant. The main differences of TextRank
compared to tf-idf is that it utilizes the context infor-
mation of words to assign term weights.
Figure 1 demonstrates that term weighting results
of TextRank and tf-idf are greatly different. Notice
that TextRank assigns low scores to words that co-
413
Corpus: (Q?A) Vocabulary Size (%chg) Average Translations (%chg)
tf-idf TextRank tf-idf TextRank
Initial 90,441 73
25%Removal 90,326 (?0.1%) 73,021 (?19.3%) 73 (?0.0%) 44 (?39.7%)
50%Removal 90,230 (?0.2%) 72,225 (?20.1%) 72 (?1.4%) 43 (?41.1%)
75%Removal 88,763 (?1.9%) 65,268 (?27.8%) 53 (?27.4%) 38 (?47.9%)
Avg.Score 66,412 (?26.6%) 31,849 (?64.8%) 14 (?80.8%) 18 (?75.3%)
Table 1: Impact of various word elimination strategies on translation model construction using (Q?A) corpus.
Corpus: (Q?Q) Vocabulary Size (%chg) Average Translations (%chg)
tf-idf TextRank tf-idf TextRank
Initial 34,485 442
25%Removal 34,374 (?0.3%) 26,900 (?22.0%) 437 (?1.1%) 282 (?36.2%)
50%Removal 34,262 (?0.6%) 26,421 (?23.4%) 423 (?4.3%) 274 (?38.0%)
75%Removal 32,813 (?4.8%) 23,354 (?32.3%) 288 (?34.8%) 213 (?51.8%)
Avg.Score 28,613 (?17.0%) 16,492 (?52.2%) 163 (?63.1%) 164 (?62.9%)
Table 2: Impact of various word elimination strategies on translation model construction using (Q?Q) corpus.
occur only with stopwords. This implies that Tex-
tRank weighs terms more ?strictly? than the tf-idf
approach, with use of contexts of words.
3.2 Deciding the Quantity to be Removed from
Ranked List
Once a final score (either tf-idf or TextRank score)
is obtained for each word, we create a list of words
ranked in decreasing order of their scores and elim-
inate the ones at lower ranks as unimportant words.
The question here is how to decide the proportion or
quantity to be removed from the ranked list.
Removing a fixed proportion: The first ap-
proach we have used is to decide the number of
unimportant words based on the size of the original
string. For our experiments, we manually vary the
proportion to be removed as 25%, 50%, and 75%.
For instance, if the proportion is set to 50% and an
original string consists of ten words, at most five
words would be remained as important words.
Using average score as threshold: We also have
used an alternate approach to deciding the quantity.
Instead of eliminating a fixed proportion, words are
removed if their score is lower than the average score
of all words in a document. This approach decides
the proportion to be removed more flexibly than the
former approach.
4 Building Compact Translation Models
We have initially built two parallel corpora from
a Q&A collection5, denoted as (Q?A) corpus and
(Q?Q) corpus henceforth, by varying the methods
in which parallel strings are gathered (described in
Section 2.2). The (Q?A) corpus consists of 85,938
parallel string pairs, and the (Q?Q) corpus contains
575,649 parallel string pairs.
In order to build compact translation models, we
have preprocessed the parallel corpus using differ-
ent word elimination strategies so that unimpor-
tant words would be removed from parallel strings.
We have also used a stoplist6 consisting of 429
words to remove stopwords. The out-of-the-box
GIZA++7 (Och and Ney, 2004) has been used to
learn translation models using the pre-processed par-
allel corpus for our retrieval experiments. We have
also trained initial translation models, using a par-
allel corpus from which only the stopwords are re-
moved, to compare with the compact translation
models.
Eventually, the number of parameters needed
for modeling translations would be minimized if
unimportant words are eliminated with different ap-
5Details on this data will be introduced in the next section.
6http://truereader.com/manuals/onix/stopwords1.html
7http://www.fjoch.com/GIZA++.html
414
proaches. Table 1 and 2 shows the impact of various
word elimination strategies on the construction of
compact translation models using the (Q?A) corpus
and the (Q?Q) corpus, respectively. The two tables
report the size of the vocabulary contained and the
average number of translations per word in the re-
sulting compact translation models, along with per-
centage decreases with respect to the initial transla-
tion models in which only stopwords are removed.
We make these observations:
? The translation models learned from the (Q?Q)
corpus have less vocabularies but more aver-
age translations per word than the ones learned
from the (Q?A) corpus. This result implies that
a large amount of noise may have been cre-
ated inevitably when a large number of parallel
strings (pairs of similar questions) were artifi-
cially gathered from the Q&A collection.
? The TextRank strategy tends to eliminate larger
sets of words as unimportant words than the
tf-idf strategy when a fixed proportion is re-
moved, regardless of the corpus type. Recall
that the TextRank approach assigns weights to
words more strictly by using contexts of words.
? The approach to remove words according to
the average weight of a document (denoted as
Avg.Score) tends to eliminate relatively larger
portions of words as unimportant words than
any of the fixed-proportion strategies, regard-
less of either the corpus type or the ranking
strategy.
5 Retrieval Experiments
Experiments have been conducted on a real world
Q&A collection to demonstrate the effectiveness of
compact translation models on Q&A retrieval.
5.1 Experimental Settings
In this section, four experimental settings for the
Q&A retrieval experiments are described in detail.
Data: For the experiments, Q&A data have been
collected from the Science domain of Yahoo! An-
swers, one of the most popular community-based
question answering service on the Web. We have
obtained a total of 43,001 questions with a best an-
swer (selected either by the questioner or by votes of
other users) by recursively traversing subcategories
of the Science domain, with up to 1,000 question
pages retrieved.8
Among the obtained Q&A pairs, 32 Q&A pairs
have been randomly selected as the test set, and the
remaining 42,969 questions have been the reference
set to be retrieved. Each Q&A pair has three text
fields: question title, question content, and answer.9
The fields of each Q&A pair in the test set are con-
sidered as various test queries; the question title,
the question content, and the answer are regarded
as a short query, a long query, and a supplementary
query, respectively. We have used long queries and
supplementary queries only in the relevance judg-
ment procedure. All retrieval experiments have been
conducted using short queries only.
Relevance judgments: To find relevant Q&A
pairs given a short query, we have employed a pool-
ing technique used in the TREC conference series.
We have pooled the top 40 Q&A pairs from each
retrieval results generated by varying the retrieval
algorithms, the search field, and the query type.
Popular word-based models, including the Okapi
BM25, query-likelihood language model, and pre-
vious translation-based models (Jeon et al, 2005),
have been used.10
Relevance judgments have been done by two stu-
dent volunteers (both fluent in English). Since
many community-based question answering ser-
vices present their search results in a hierarchical
fashion (i.e. a list of relevant questions is shown
first, and then the user chooses a specific question
from the list to see its answers), a Q&A pair has been
judged as relevant if its question is semantically sim-
ilar to the query; neither quality nor rightness of the
answer has not been considered. When a disagree-
ment has been made between two volunteers, one of
the authors has made the final judgment. As a result,
177 relevant Q&A pairs have been found in total for
the 32 short queries.
Baseline retrieval models: The proposed ap-
8Yahoo! Answers did not expose additional question pages
to external requests at the time of collecting the data.
9When collecting parallel strings from the Q&A collection,
we have put together the question title and the question content
as one question string.
10The retrieval model using compact translation models has
not been used in the pooling procedure.
415
proach to Q&A retrieval using compact translation
models (denoted as CTLM henceforth) is compared
to three baselines:
QLM: Query-likelihood language model for re-
trieval (equivalent to Equation 3, without use of
translation models). This model represents word-
based retrieval models widely used in practice.
TLM(Q?Q): Translation-based language model
for question retrieval (Jeon et al, 2005). This model
uses IBM Model 1 learned from the (Q?Q) corpus
of which stopwords are removed.
TLM(Q?A): A variant of the translation-based ap-
proach. This model uses IBM model 1 learned from
the (Q?A) corpus.
Evaluation metrics: We have reported the re-
trieval performance in terms of Mean Average Pre-
cision (MAP) and Mean R-Precision (R-Prec).
Average Precision can be computed based on the
precision at each relevant document in the ranking.
Mean Average Precision is defined as the mean of
the Average Precision values across the set of all
queries:
MAP (Q) = 1|Q|
?
q?Q
1
mq
mq?
k=1
Precision(Rk) (9)
where Q is the set of test queries, mq is the number
of relevant documents for a query q, Rk is the set of
ranked retrieval results from the top until rank posi-
tion k, and Precision(Rk) is the fraction of relevant
documents in Rk (Manning et al, 2008).
R-Precision is defined as the precision after
R documents have been retrieved where R is
the number of relevant documents for the current
query (Buckley and Voorhees, 2000). Mean R-
Precision is the mean of the R-Precisions across the
set of all queries.
We take MAP as our primary evaluation metric.
5.2 Experimental Results
Preliminary retrieval experiments have been con-
ducted using the baseline QLM and different fields
of Q&A data as retrieval unit. Table 3 shows the
effectiveness of each field.
The results imply that the question title field is the
most important field in our Yahoo! Answers collec-
tion; this also supports the observation presented by
Retrieval unit MAP R-Prec
Question title 0.1031 0.2396
Question content 0.0422 0.0999
Answer 0.0566 0.1062
Table 3: Preliminary retrieval results.
Model MAP R-Prec
(%chg) (%chg)
QLM 0.1031 0.2396
TLM(Q?Q)* 0.1121 0.2251
(49%) (?6%)
CTLM(Q?Q) 0.1415 0.2425
(437%) (41%)
TLM(Q?A) 0.1935 0.3135
(488%) (431%)
CTLM(Q?A) 0.2095 0.3585
(4103%) (450%)
Table 4: Comparisons with three baseline retrieval mod-
els. * indicates that it is equivalent to Jeon et al (2005)?s
approach. MAP improvements of CTLMs have been
tested to be statistically significant using paired t-test.
Jeon et al (2005). Based on the preliminary obser-
vations, all retrieval models tested in this paper have
ranked Q&A pairs according to the similarity scores
between queries and question titles.
Table 4 presents the comparison results of three
baseline retrieval models and the proposed CTLMs.
For each method, the best performance after empir-
ical ? parameter tuning according to MAP is pre-
sented.
Notice that both the TLMs and CTLMs have out-
performed the word-based QLM. This implies that
word-based models that do not address the issue of
lexical gaps between queries and questions often fail
to retrieve relevant Q&A data that have little word
overlap with queries, as noted by Jeon et al (2005).
Moreover, notice that the proposed CTLMs have
achieved significantly better performances in all
evaluation metrics than both QLM and TLMs, regard-
less of the parallel corpus in which the incorporated
translation models are trained from. This is a clear
indication that the use of compact translation models
built with appropriate word elimination strategies is
effective in closing the query-question lexical gaps
416
(Q?Q) MAP (%chg)
tf-idf TextRank
Initial 0.1121
25%Rmv 0.1141 (41.8) 0.1308 (416.7)
50%Rmv 0.1261 (412.5) 0.1334 (419.00)
75%Rmv 0.1115 (?0.5) 0.1160 (43.5)
Avg.Score 0.1056 (?5.8) 0.1415 (426.2)
Table 5: Contributions of various word elimination strate-
gies on MAP performance of CTLM(Q?Q).
(Q?A) MAP (%chg)
tf-idf TextRank
Initial 0.1935
25%Rmv 0.2095 (48.3) 0.1733 (?10.4)
50%Rmv 0.2085 (47.8) 0.1623 (?16.1)
75%Rmv 0.1449 (?25.1) 0.1515 (?21.7)
Avg.Score 0.1168 (?39.6) 0.1124 (?41.9)
Table 6: Contributions of various word elimination strate-
gies on MAP performance of CTLM(Q?A).
for improving the performance of question retrieval
in the context of language modeling framework.
Note that the retrieval performance varies by the
type of training corpus; CTLM(Q?A) has outper-
formed CTLM(Q?Q) significantly. This proves the
statement we made earlier that the (Q?Q) corpus
would contain much noise since the translation mod-
els learned from the (Q?Q) corpus tend to have
smaller vocabulary sizes but significantly more aver-
age translations per word than the ones learned from
the (Q?A) corpus.
Table 5 and 6 show the effect of various word
elimination strategies on the retrieval performance
of CTLMs in which the incorporated compact trans-
lation models are trained from the (Q?Q) corpus and
the (Q?A) corpus, respectively. It is interesting to
note that the importance of modifications in word
elimination strategies also varies by the type of train-
ing corpus.
The retrieval results indicate that when the trans-
lation model is trained from the ?less noisy? (Q?A)
corpus, eliminating a relatively large proportions of
words may hurt the retrieval performance of CTLM.
In the case when the translation model is trained
from the ?noisy? (Q?Q) corpus, a better retrieval
performance may be achieved if words are elimi-
nated appropriately to a certain extent.
In terms of weighting scheme, the TextRank ap-
proach, which is more ?strict? than tf-idf in elim-
inating unimportant words, has led comparatively
higher retrieval performances on all levels of re-
moval quantity when the translation model has been
trained from the ?noisy? (Q?Q) corpus. On the con-
trary, the ?less strict? tf-idf approach has led better
performances when the translation model has been
trained from the ?less noisy? (Q?A) corpus.
In summary, the results imply that the perfor-
mance of translation-based retrieval models can be
significantly improved when strategies for building
of compact translation models are chosen properly,
regarding the expected noise level of the parallel cor-
pus for training the translation models. In a case
where a noisy parallel corpus is given for training
of translation models, it is better to get rid of noise
as much as possible by using ?strict? term weight-
ing algorithms; when a less noisy parallel corpus is
given for building the translation models, a tolerant
approach would yield better retrieval performance.
6 Related Works
Our work is most closely related to Jeon et
al. (2005)?s work, which addresses the issue of
word mismatch between queries and questions in
large online Q&A collections by using translation-
based methods. Apart from their work, there have
been some related works on applying translation-
based methods for retrieving FAQ data. Berger et
al. (2000) report some of the earliest work on FAQ
retrieval using statistical retrieval models, includ-
ing translation-based approaches, with a small set
of FAQ data. Soricut and Brill (2004) present an an-
swer passage retrieval system that is trained from 1
million FAQs collected from the Web using trans-
lation methods. Riezler et al (2007) demonstrate
the advantages of translation-based approach to an-
swer retrieval by utilizing a more complex trans-
lation model also trained from a large amount of
data extracted from FAQs on the Web. Although all
of these translation-based approaches are based on
the statistical translation models, including the IBM
Model 1, none of them focus on addressing the noise
issues in translation models.
417
7 Conclusion and Future Work
Bridging the query-question gap has been a major is-
sue in retrieval models for large online Q&A collec-
tions. In this paper, we have shown that the perfor-
mance of translation-based retrieval on real online
Q&A collections can be significantly improved by
using compact translation models of which the noise
(unimportant word translations) is properly reduced.
We have also observed that the performance en-
hancement may be achieved by choosing the appro-
priate strategies regarding the strictness of various
term weighting algorithms and the expected noise
level of the parallel data for learning such transla-
tion models.
Future work will focus on testing the effective-
ness of the proposed method on a larger set of Q&A
collections with broader domains. Since the pro-
posed approach cannot handle many-to-one or one-
to-many word transformations, we also plan to in-
vestigate the effectiveness of phrase-based transla-
tion models in closing gaps between queries and
questions for further enhancement of Q&A retrieval.
Acknowledgments
This work was supported by Microsoft Research
Asia. Any opinions, findings, and conclusions or
recommendations expressed above are those of the
authors and do not necessarily reflect the views of
the sponsor.
References
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the Lexical
Chasm: Statistical Approaches to Answer-Finding. In
Proceedings of the 23rd Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval, pages 192?199.
Adam Berger and John Lafferty. 1999. Information Re-
trieval as Statistical Translation. In Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 222?229.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Buckley and Ellen M. Voorhees. 2000. Evaluating
Evaluation Measure Stability. In Proceedings of the
23rd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 33?40.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding Similar Questions in Large Question and An-
swer Archives. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, pages 84?90.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Text. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 404?411.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Jay M. Ponte and W. Bruce Croft. 1998. A Language
Modeling Approach to Information Retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 275?281.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
Machine Translation for Query Expansion in Answer
Retrieval. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
pages 464?471.
Radu Soricut and Eric Brill. 2004. Automatic Question
Answering: Beyond the Factoid. In Proceedings of
the 2004 Human Language Technology and Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 57?64.
418
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1048?1056,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Word or Phrase?
Learning Which Unit to Stress for Information Retrieval?
Young-In Song? and Jung-Tae Lee? and Hae-Chang Rim?
?Microsoft Research Asia, Beijing, China
?Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
yosong@microsoft.com?, {jtlee,rim}@nlp.korea.ac.kr?
Abstract
The use of phrases in retrieval models has
been proven to be helpful in the literature,
but no particular research addresses the
problem of discriminating phrases that are
likely to degrade the retrieval performance
from the ones that do not. In this paper, we
present a retrieval framework that utilizes
both words and phrases flexibly, followed
by a general learning-to-rank method for
learning the potential contribution of a
phrase in retrieval. We also present use-
ful features that reflect the compositional-
ity and discriminative power of a phrase
and its constituent words for optimizing
the weights of phrase use in phrase-based
retrieval models. Experimental results on
the TREC collections show that our pro-
posed method is effective.
1 Introduction
Various researches have improved the quality
of information retrieval by relaxing the tradi-
tional ?bag-of-words? assumption with the use of
phrases. (Miller et al, 1999; Song and Croft,
1999) explore the use n-grams in retrieval mod-
els. (Fagan, 1987; Gao et al, 2004; Met-
zler and Croft, 2005; Tao and Zhai, 2007) use
statistically-captured term dependencies within a
query. (Strzalkowski et al, 1994; Kraaij and
Pohlmann, 1998; Arampatzis et al, 2000) study
the utility of various kinds of syntactic phrases.
Although use of phrases clearly helps, there still
exists a fundamental but unsolved question: Do all
phrases contribute an equal amount of increase in
the performance of information retrieval models?
Let us consider a search query ?World Bank Crit-
icism?, which has the following phrases: ?world
?This work was done while Young-In Song was with the
Dept. of Computer & Radio Communications Engineering,
Korea University.
bank? and ?bank criticism?. Intuitively, the for-
mer should be given more importance than its con-
stituents ?world? and ?bank?, since the meaning
of the original phrase cannot be predicted from
the meaning of either constituent. In contrast, a
relatively less attention could be paid to the lat-
ter ?bank criticism?, because there may be alter-
nate expressions, of which the meaning is still pre-
served, that could possibly occur in relevant docu-
ments. However, virtually all the researches ig-
nore the relation between a phrase and its con-
stituent words when combining both words and
phrases in a retrieval model.
Our approach to phrase-based retrieval is moti-
vated from the following linguistic intuitions: a)
phrases have relatively different degrees of signif-
icance, and b) the influence of a phrase should be
differentiated based on the phrase?s constituents in
retrieval models. In this paper, we start out by
presenting a simple language modeling-based re-
trieval model that utilizes both words and phrases
in ranking with use of parameters that differenti-
ate the relative contributions of phrases and words.
Moreover, we propose a general learning-to-rank
based framework to optimize the parameters of
phrases against their constituent words for re-
trieval models that utilize both words and phrases.
In order to estimate such parameters, we adapt the
use of a cost function together with a gradient de-
scent method that has been proven to be effective
for optimizing information retrieval models with
multiple parameters (Taylor et al, 2006; Metzler,
2007). We also propose a number of potentially
useful features that reflect not only the characteris-
tics of a phrase but also the information of its con-
stituent words for minimizing the cost function.
Our experimental results demonstrate that 1) dif-
ferentiating the weights of each phrase over words
yields statistically significant improvement in re-
trieval performance, 2) the gradient descent-based
parameter optimization is reasonably appropriate
1048
to our task, and 3) the proposed features can dis-
tinguish good phrases that make contributions to
the retrieval performance.
The rest of this paper is organized as follows.
The next section discusses previous work. Section
3 presents our learning-based retrieval framework
and features. Section 4 reports the evaluations of
our techniques. Section 5 finally concludes the pa-
per and discusses future work.
2 Previous Work
To date, there have been numerous researches to
utilize phrases in retrieval models. One of the
most earliest work on phrase-based retrieval was
done by (Fagan, 1987). In (Fagan, 1987), the ef-
fectiveness of proximity-based phrases (i.e. words
occurring within a certain distance) in retrieval
was investigated with varying criteria to extract
phrases from text. Subsequently, various types
of phrases, such as sequential n-grams (Mitra et
al., 1997), head-modifier pairs extracted from syn-
tactic structures (Lewis and Croft, 1990; Zhai,
1997; Dillon and Gray, 1983; Strzalkowski et al,
1994), proximity-based phrases (Turpin and Mof-
fat, 1999), were examined with conventional re-
trieval models (e.g. vector space model). The ben-
efit of using phrases for improving the retrieval
performance over simple ?bag-of-words? models
was far less than expected; the overall perfor-
mance improvement was only marginal and some-
times even inconsistent, specifically when a rea-
sonably good weighting scheme was used (Mitra
et al, 1997). Many researchers argued that this
was due to the use of improper retrieval models
in the experiments. In many cases, the early re-
searches on phrase-based retrieval have only fo-
cused on extracting phrases, not concerning about
how to devise a retrieval model that effectively
considers both words and phrases in ranking. For
example, the direct use of traditional vector space
model combining a phrase weight and a word
weight virtually yields the result assuming inde-
pendence between a phrase and its constituent
words (Srikanth and Srihari, 2003).
In order to complement the weakness, a number
of research efforts were devoted to the modeling
of dependencies between words directly within re-
trieval models instead of using phrases over the
years (van Rijsbergen, 1977; Wong et al, 1985;
Croft et al, 1991; Losee, 1994). Most stud-
ies were conducted on the probabilistic retrieval
framework, such as the BIM model, and aimed on
producing a better retrieval model by relaxing the
word independence assumption based on the co-
occurrence information of words in text. Although
those approaches theoretically explain the relation
between words and phrases in the retrieval con-
text, they also showed little or no improvements
in retrieval effectiveness, mainly because of their
statistical nature. While a phrase-based approach
selectively incorporated potentially-useful relation
between words, the probabilistic approaches force
to estimate parameters for all possible combina-
tions of words in text. This not only brings
parameter estimation problems but causes a re-
trieval system to fail by considering semantically-
meaningless dependency of words in matching.
Recently, a number of retrieval approaches have
been attempted to utilize a phrase in retrieval mod-
els. These approaches have focused to model sta-
tistical or syntactic phrasal relations under the lan-
guage modeling method for information retrieval.
(Srikanth and Srihari, 2003; Maisonnasse et al,
2005) examined the effectiveness of syntactic re-
lations in a query by using language modeling
framework. (Song and Croft, 1999; Miller et al,
1999; Gao et al, 2004; Metzler and Croft, 2005)
investigated the effectiveness of language model-
ing approach in modeling statistical phrases such
as n-grams or proximity-based phrases. Some of
them showed promising results in their experi-
ments by taking advantages of phrases soundly in
a retrieval model.
Although such approaches have made clear dis-
tinctions by integrating phrases and their con-
stituents effectively in retrieval models, they did
not concern the different contributions of phrases
over their constituents in retrieval performances.
Usually a phrase score (or probability) is simply
combined with scores of its constituent words by
using a uniform interpolation parameter, which
implies that a uniform contribution of phrases
over constituent words is assumed. Our study is
clearly distinguished from previous phrase-based
approaches; we differentiate the influence of each
phrase according to its constituent words, instead
of allowing equal influence for all phrases.
3 Proposed Method
In this section, we present a phrase-based retrieval
framework that utilizes both words and phrases ef-
fectively in ranking.
1049
3.1 Basic Phrase-based Retrieval Model
We start out by presenting a simple phrase-based
language modeling retrieval model that assumes
uniform contribution of words and phrases. For-
mally, the model ranks a document D according to
the probability of D generating phrases in a given
query Q, assuming that the phrases occur indepen-
dently:
s(Q;D) = P (Q|D) ?
|Q|?
i=1
P (qi|qhi , D) (1)
where qi is the ith query word, qhi is the head word
of qi, and |Q| is the query size. To simplify the
mathematical derivations, we modify Eq. 1 using
logarithm as follows:
s(Q;D) ?
|Q|?
i=1
log[P (qi|qhi , D)] (2)
In practice, the phrase probability is mixed with
the word probability (i.e. deleted interpolation) as:
P (qi|qhi ,D)??P (qi|qhi ,D)+(1??)P (qi|D) (3)
where ? is a parameter that controls the impact of
the phrase probability against the word probability
in the retrieval model.
3.2 Adding Multiple Parameters
Given a phrase-based retrieval model that uti-
lizes both words and phrases, one would definitely
raise a fundamental question on how much weight
should be given to the phrase information com-
pared to the word information. In this paper, we
propose to differentiate the value of ? in Eq. 3
according to the importance of each phrase by
adding multiple free parameters to the retrieval
model. Specifically, we replace ? with well-
known logistic function, which allows both nu-
merical and categorical variables as input, whereas
the output is bounded to values between 0 and 1.
Formally, the input of a logistic function is a
set of evidences (i.e. feature vector) X generated
from a given phrase and its constituents, whereas
the output is the probability predicted by fitting X
to a logistic curve. Therefore, ? is replaced as fol-
lows:
?(X) = 11 + e?f(X) ? ? (4)
where ? is a scaling factor to confine the output to
values between 0 and ?.
f(X) = ?0 +
|X|?
i=1
?ixi (5)
where xi is the ith feature, ?i is the coefficient pa-
rameter of xi, and ?0 is the ?intercept?, which is
the value of f(X) when all feature values are zero.
3.3 RankNet-based Parameter Optimization
The ? parameters in Eq. 5 are the ones we wish
to learn for resulting retrieval performance via pa-
rameter optimization methods. In many cases, pa-
rameters in a retrieval model are empirically de-
termined through a series of experiments or auto-
matically tuned via machine learning to maximize
a retrieval metric of choice (e.g. mean average
precision). The most simple but guaranteed way
would be to directly perform brute force search
for the global optimum over the entire parame-
ter space. However, not only the computational
cost of this so-called direct search would become
undoubtfully expensive as the number of parame-
ters increase, but most retrieval metrics are non-
smooth with respect to model parameters (Met-
zler, 2007). For these reasons, we propose to adapt
a learning-to-rank framework that optimizes mul-
tiple parameters of phrase-based retrieval models
effectively with less computation cost and without
any specific retrieval metric.
Specifically, we use a gradient descent method
with the RankNet cost function (Burges et al,
2005) to perform effective parameter optimiza-
tions, as in (Taylor et al, 2006; Metzler, 2007).
The basic idea is to find a local minimum of a cost
function defined over pairwise document prefer-
ence. Assume that, given a query Q, there is
a set of document pairs RQ based on relevance
judgements, such that (D1, D2) ? RQ implies
document D1 should be ranked higher than D2.
Given a defined set of pairwise preferences R, the
RankNet cost function is computed as:
C(Q,R) =
?
?Q?Q
?
?(D1,D2)?RQ
log(1 + eY ) (6)
whereQ is the set of queries, and Y = s(Q;D2)?
s(Q;D1) using the current parameter setting.
In order to minimize the cost function, we com-
pute gradients of Eq. 6 with respect to each pa-
rameter ?i by applying the chain rule:
?C
??i =
?
?Q?Q
?
?(D1,D2)?RQ
?C
?Y
?Y
??i (7)
where ?C?Y and ?Y??i are computed as:
?C
?Y =
exp[s(Q;D2)? s(Q;D1)]
1 + exp[s(Q;D2)? s(Q;D1)] (8)
1050
?Y
??i =
?s(Q;D2)
??i ?
?s(Q;D1)
??i (9)
With the retrieval model in Eq. 2 and ?(X),
f(X) in Eq. 4 and 5, the partial derivate of
s(Q;D) with respect to ?i is computed as follows:
?s(Q;D)
??i
=
|Q|?
i=1
xi?(X)(1? ?(X)? )?(P (qi|qhi,D)?P (qi|D))
?(X)P (qi|qhi , D) + (1? ?(X))P (qi|D)
(10)
3.4 Features
We experimented with various features that are
potentially useful for not only discriminating a
phrase itself but characterizing its constituents. In
this section, we report only the ones that have
made positive contributions to the overall retrieval
performance. The two main criteria considered
in the selection of the features are the followings:
compositionality and discriminative power.
Compositionality Features
Features on phrase compositionality are designed
to measure how likely a phrase can be represented
as its constituent words without forming a phrase;
if a phrase in a query has very high composition-
ality, there is a high probability that its relevant
documents do not contain the phrase. In this case,
emphasizing the phrase unit could be very risky in
retrieval. In the opposite case that a phrase is un-
compositional, it is obvious that occurrence of a
phrase in a document can be a stronger evidence
of relevance than its constituent words.
Compositionality of a phrase can be roughly
measured by using corpus statistics or its linguis-
tic characteristics; we have observed that, in many
times, an extremely-uncompositional phrase ap-
pears as a noun phrase, and the distance between
its constituent words is generally fixed within a
short distance. In addition, it has a tendency to be
used repeatedly in a document because its seman-
tics cannot be represented with individual con-
stituent words. Based on these intuitions, we de-
vise the following features:
Ratio of multiple occurrences (RMO): This is a
real-valued feature that measures the ratio of the
phrase repeatedly used in a document. The value
of this feature is calculated as follows:
x =
?
?D;count(wi?whi ,D)>1
count(wi?whi , D)
count(wi ? whi , C) + ?
(11)
where wi ? whi is a phrase in a given query,
count(x, y) is the count of x in y, and ? is a small-
valued constant to prevent unreliable estimation
by very rarely-occurred phrases.
Ratio of single-occurrences (RSO): This is a bi-
nary feature that indicates whether or not a phrase
occurs once in most documents containing it. This
can be regarded as a supplementary feature of
RMO.
Preferred phrasal type (PPT): This feature indi-
cates the phrasal type that the phrase prefers in a
collection. We consider only two cases (whether
the phrase prefers verb phrase or adjective-noun
phrase types) as features in the experiments1.
Preferred distance (PD): This is a binary feature
indicating whether or not the phrase prefers long
distance (> 1) between constituents in the docu-
ment collection.
Uncertainty of preferred distance (UPD): We also
use the entropy (H) of the modification distance
(d) of the given phrase in the collection to measure
the compositionality; if the distance is not fixed
and is highly uncertain, the phrase may be very
compositional. The entropy is computed as:
x = H(p(d = x|wi ? whi)) (12)
where d ? 1, 2, 3, long and all probabilities are
estimated with discount smoothing. We simply
use two binary features regarding the uncertainty
of distance; one indicates whether the uncertainty
of a phrase is very high (> 0.85), and the other
indicates whether the uncertainty is very low (<
0.05)2.
Uncertainty of preferred phrasal type (UPPT): As
similar to the uncertainty of preferred distance, the
uncertainty of the preferred phrasal type of the
phrase can be also used as a feature. We consider
this factor as a form of a binary feature indicating
whether the uncertainty is very high or not.
Discriminative Power Features
In some cases, the occurrence of a phrase can be a
valuable evidence even if the phrase is very likely
to be compositional. For example, it is well known
that the use of a phrase can be effective in retrieval
when its constituent words appear very frequently
in the collection, because each word would have a
very low discriminative power for relevance. On
the contrary, if a constituent word occurs very
1For other phrasal types, significant differences were not
observed in the experiments.
2Although it may be more natural to use a real-valued fea-
ture, we use these binary features because of the two practical
reasons; firstly, it could be very difficult to find an adequate
transformation function with real values, and secondly, the
two intervals at tails were observed to be more important than
the rest.
1051
rarely in the collection, it could not be effective
to use the phrase even if the phrase is highly un-
compositional. Similarly, if the probability that a
phrase occurs in a document where its constituent
words co-occur is very high, we might not need to
place more emphasis on the phrase than on words,
because co-occurrence information naturally in-
corporated in retrieval models may have enough
power to distinguish relevant documents. Based
on these intuitions, we define the following fea-
tures:
Document frequency of constituents (DF): We
use the document frequency of a constituent as
two binary features: one indicating whether the
word has very high document frequency (>10%
of documents in a collection) and the other one
indicating whether it has very low document fre-
quency (<0.2% of documents, which is approxi-
mately 1,000 in our experiments).
Probability of constituents as phrase (CPP): This
feature is computed as a relative frequency of doc-
uments containing a phrase over documents where
two constituent words appear together.
One interesting fact that we observe is that doc-
ument frequency of the modifier is generally a
stronger evidence on the utility of a phrase in re-
trieval than of the headword. In the case of the
headword, we could not find an evidence that it
has to be considered in phrase weighting. It seems
to be a natural conclusion, because the importance
of the modifier word in retrieval is subordinate to
the relation to its headword, but the headword is
not in many phrases. For example, in the case of
the query ?tropical storms?, retrieving a document
only containing tropical can be meaningless, but a
document about storm can be meaningful. Based
on this observation, we only incorporate document
frequency features of syntactic modifiers in the ex-
periments.
4 Experiments
In this section, we report the retrieval perfor-
mances of the proposed method with appropriate
baselines over a range of training sets.
4.1 Experimental Setup
Retrieval models: We have set two retrieval mod-
els, namely the word model and the (phrase-based)
one-parameter model, as baselines. The ranking
function of the word model is equivalent to Eq. 2,
with ? in Eq. 3 being set to zero (i.e. the phrase
probability makes no effect on the ranking). The
ranking function of the one-parameter model is
also equivalent to Eq. 2, with ? in Eq. 3 used ?as
is? (i.e. as a constant parameter value optimized
using gradient descent method, without being re-
placed to a logistic function). Both baseline mod-
els cannot differentiate the importance of phrases
in a query. To make a distinction from the base-
line models, we will name our proposed method
as a multi-parameter model.
In our experiments, all the probabilities in all
retrieval models are smoothed with the collection
statistics by using dirichlet priors (Zhai and Laf-
ferty, 2001).
Corpus (Training/Test): We have conducted
large-scale experiments on three sets of TREC?s
Ad Hoc Test Collections, namely TREC-6, TREC-
7, and TREC-8. Three query sets, TREC-6 top-
ics 301-350, TREC-7 topics 351-400, and TREC-
8 topics 401-450, along with their relevance judg-
ments have been used. We only used the title field
as query.
When performing experiments on each query
set with the one-parameter and the multi-
parameter models, the other two query sets have
been used for learning the optimal parameters. For
each query in the training set, we have generated
document pairs for training by the following strat-
egy: first, we have gathered top m ranked doc-
uments from retrieval results by using the word
model and the one-parameter model (by manually
setting ? in Eq. 3 to the fixed constants, 0 and 0.1
respectively). Then, we have sampled at most r
relevant documents and n non-relevant documents
from each one and generated document pairs from
them. In our experiments, m, r, and n is set to
100, 10, and 40, respectively.
Phrase extraction and indexing: We evaluate
our proposed method on two different types of
phrases: syntactic head-modifier pairs (syntac-
tic phrases) and simple bigram phrases (statisti-
cal phrases). To index the syntactic phrases, we
use the method proposed in (Strzalkowski et al,
1994) with Connexor FDG parser3, the syntactic
parser based on the functional dependency gram-
mar (Tapanainen and Jarvinen, 1997). All neces-
sary information for feature values were indexed
together for both syntactic and statistical phrases.
To maintain indexes in a manageable size, phrases
3Connexor FDG parser is a commercial parser; the demo
is available at: http://www.connexor.com/demo
1052
Test set ? Training set
6 ? 7+8 7 ? 6+8 8 ? 6+7
Model Metric \ Query all partial all partial all partial
Word MAP 0.2135 0.1433 0.1883 0.1876 0.2380 0.2576
(Baseline 1) R-Prec 0.2575 0.1894 0.2351 0.2319 0.2828 0.2990
P@10 0.3660 0.3333 0.4100 0.4324 0.4520 0.4517
One-parameter MAP 0.2254 0.1633? 0.1988 0.2031 0.2352 0.2528
(Baseline 2) R-Prec 0.2738 0.2165 0.2503 0.2543 0.2833 0.2998
P@10 0.3820 0.3600 0.4540 0.4971 0.4580 0.4621
Multi-parameter MAP 0.2293? 0.1697? 0.2038? 0.2105? 0.2452 0.2701
(Proposed) R-Prec 0.2773 0.2225 0.2534 0.2589 0.2891 0.3099
P@10 0.4020 0.3933 0.4540 0.4971 0.4700 0.4828
Table 1: Retrieval performance of different models on syntactic phrases. Italicized MAP values with
symbols ? and ? indicate statistically significant improvements over the word model according to Stu-
dent?s t-test at p < 0.05 level and p < 0.01 level, respectively. Bold figures indicate the best performed
case for each metric.
that occurred less than 10 times in the document
collections were not indexed.
4.2 Experimental Results
Table 1 shows the experimental results of the three
retrieval models on the syntactic phrase (head-
modifier pair). In the table, partial denotes the
performance evaluated on queries containing more
than one phrase that appeared in the document col-
lection4; this shows the actual performance differ-
ence between models. Note that the ranking re-
sults of all retrieval models would be the same as
the result of the word model if a query does not
contain any phrases in the document collection,
because P (qi|qhi , D) would be calculated as zero
eventually. As evaluation measures, we used the
mean average precision (MAP), R-precision (R-
Prec), and precisions at top 10 ranks (P@10).
As shown in Table 1, when a syntactic phrase is
used for retrieval, one-parameter model trained by
gradient-descent method generally performs bet-
ter than the word model, but the benefits are in-
consistent; it achieves approximately 15% and 8%
improvements on the partial query set of TREC-
6 and 7 over the word model, but it fails to show
any improvement on TREC-8 queries. This may
be a natural result since the one-parameter model
is very sensitive to the averaged contribution of
phrases used for training. Compared to the queries
in TREC-6 and 7, the TREC-8 queries contain
more phrases that are not effective for retrieval
4The number of queries containing a phrase in TREC-6,
7, and 8 query set is 31, 34, and 29, respectively.
(i.e. ones that hurt the retrieval performance when
used). This indicates that without distinguishing
effective phrases from ineffective phrases for re-
trieval, the model trained from one training set for
phrase would not work consistently on other un-
seen query sets.
Note that the proposed model outperforms all
the baselines over all query sets; this shows that
differentiating relative contributions of phrases
can improve the retrieval performance of the one-
parameter model considerably and consistently.
As shown in the table, the multi-parameter model
improves by approximately 18% and 12% on the
TREC-6 and 7 partial query sets, and it also
significantly outperforms both the word model
and the one-parameter model on the TREC-8
query set. Specifically, the improvement on the
TREC-8 query set shows one advantage of using
our proposed method; by separating potentially-
ineffective phrases and effective phrases based on
the features, it not only improves the retrieval
performance for each query but makes parameter
learning less sensitive to the training set.
Figure 1 shows some examples demonstrating
the different behaviors of the one-parameter model
and the multi-parameters model. On the figure, the
un-dotted lines indicate the variation of average
precision scores when ? value in Eq. 3 is manu-
ally set. As ? gets closer to 0, the ranking formula
becomes equivalent to the word model.
As shown in the figure, the optimal point of ? is
quiet different from query to query. For example,
in cases of the query ?ferry sinking? and industrial
1053
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0 0.1 0.2 0.3 0.4 0.5
A
v
g
P
r
lambda
Performance variation for the query ?ferry sinking?
varing lambdaone-parametermultiple-parameter
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0 0.1 0.2 0.3 0.4 0.5
A
v
g
P
r
lambda
Performance variation for the query ?industrial espionage?
varing lambdaone-parametermultiple-parameter
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
A
v
g
P
r
lambda
Performance variation for the query ? declining birth rates?
varing lambdaone-parametermultiple-parameter 0.2
0.25
0.3
0.35
0.4
0.45
0 0.1 0.2 0.3 0.4 0.5
A
v
g
P
r
lambda
Performance variation for the query ?amazon rain forest?
varing lambdaone-parametermultiple-parameter
Figure 1: Performance variations for the queries ?ferry sinking?, ?industrial espionage?, ?declining birth
rate? and ?Amazon rain forest? according to ? in Eq. 3.
espionage? on the upper side, the optimal point is
the value close to 0 and 1 respectively. This means
that the occurrences of the phrase ?ferry sinking?
in a document is better to be less-weighted in
retrieval while ?industrial espionage? should be
treated as a much more important evidence than its
constituent words. Obviously, such differences are
not good for one-parameter model assuming rela-
tive contributions of phrases uniformly. For both
opposite cases, the multi-parameter model signifi-
cantly outperforms one-parameter model.
The two examples at the bottom of Figure 1
show the difficulty of optimizing phrase-based re-
trieval using one uniform parameter. For example,
the query ?declining birth rate? contains two dif-
ferent phrases, ?declining rate? and ?birth rate?,
which have potentially-different effectiveness in
retrieval; the phrase ?declining rate? would not
be helpful for retrieval because it is highly com-
positional, but the phrase ?birth rate? could be a
very strong evidence for relevance since it is con-
ventionally used as a phrase. In this case, we
can get only small benefit from the one-parameter
model even if we find optimal ? from gradient
descent, because it will be just a compromised
value between two different, optimized ?s. For
such query, the multi-parameter model could be
more effective than the one-parameter model by
enabling to set different ?s on phrases accord-
ing to their predicted contributions. Note that the
multi-parameter model significantly outperforms
the one-parameter model and all manually-set ?s
for the queries ?declining birth rate? and ?Amazon
rain forest?, which also has one effective phrase,
?rain forest?, and one non-effective phrase, ?Ama-
zon forest?.
Since our method is not limited to a particular
type of phrases, we have also conducted experi-
ments on statistical phrases (bigrams) with a re-
duced set of features directed applicable; RMO,
RSO, PD5, DF, and CPP; the features requiring
linguistic preprocessing (e.g. PPT) are not used,
because it is unrealistic to use them under bigram-
based retrieval setting. Moreover, the feature UPD
is not used in the experiments because the uncer-
5In most cases, the distance between words in a bigram
is 1, but sometimes, it could be more than 1 because of the
effect of stopword removal.
1054
Test ? Training
Model Metric 6 ? 7+8 7 ? 6+8 8 ? 6+7
Word MAP 0.2135 0.1883 0.2380
(Baseline 1) R-Prec 0.2575 0.2351 0.2828
P@10 0.3660 0.4100 0.4520
One-parameter MAP 0.2229 0.1979 0.2492?
(Baseline 2) R-Prec 0.2716 0.2456 0.2959
P@10 0.3720 0.4500 0.4620
Multi-parameter MAP 0.2224 0.2025? 0.2499?
(Proposed) R-Prec 0.2707 0.2457 0.2952
P@10 0.3780 0.4520 0.4600
Table 2: Retrieval performance of different models, using statistical phrases.
tainty of preferred distance does not vary much for
bigram phrases. The results are shown in Table 2.
The results of experiments using statistical
phrases show that multi-parameter model yields
additional performance improvement against
baselines in many cases, but the benefit is in-
significant and inconsistent. As shown in Table 2,
according to the MAP score, the multi-parameter
model outperforms the one-parameter model on
the TREC-7 and 8 query sets, but it performs
slightly worse on the TREC-6 query set.
We suspect that this is because of the lack
of features to distinguish an effective statistical
phrases from ineffective statistical phrase. In our
observation, the bigram phrases also show a very
similar behavior in retrieval; some of them are
very effective while others can deteriorate the per-
formance of retrieval models. However, in case
of using statistical phrases, the ? computed by our
multi-parameter model would be often similar to
the one computed by the one-parameter model,
when there is no sufficient evidence to differen-
tiate a phrase. Moreover, the insufficient amount
of features may have caused the multi-parameter
model to overfit to the training set easily.
The small size of training corpus could be an an-
other reason. The number of queries we used for
training is less than 80 when removing a query not
containing a phrase, which is definitely not a suf-
ficient amount to learn optimal parameters. How-
ever, if we recall that the multi-parameter model
worked reasonably in the experiments using syn-
tactic phrases with the same training sets, the lack
of features would be a more important reason.
Although we have not mainly focused on fea-
tures in this paper, it would be strongly necessary
to find other useful features, not only for statistical
phrases, but also for syntactic phrases. For exam-
ple, statistics from query logs and the probability
of snippet containing a same phrase in a query is
clicked by user could be considered as useful fea-
tures. Also, the size of the training data (queries)
and the document collection may not be sufficient
enough to conclude the effectiveness of our pro-
posed method; our method should be examined in
a larger collection with more queries. Those will
be one of our future works.
5 Conclusion
In this paper, we present a novel method to differ-
entiate impacts of phrases in retrieval according
to their relative contribution over the constituent
words. The contributions of this paper can be sum-
marized in three-fold: a) we proposed a general
framework to learn the potential contribution of
phrases in retrieval by ?parameterizing? the fac-
tor interpolating the phrase weight and the word
weight on features and optimizing the parameters
using RankNet-based gradient descent algorithm,
b) we devised a set of potentially useful features
to distinguish effective and non-effective phrases,
and c) we showed that the proposed method can be
effective in terms of retrieval by conducting a se-
ries of experiments on the TREC test collections.
As mentioned earlier, the finding of additional
features, specifically for statistical phrases, would
be necessary. Moreover, for a thorough analysis
on the effect of our framework, additional experi-
ments on larger and more realistic collections (e.g.
the Web environment) would be required. These
will be our future work.
1055
References
Avi Arampatzis, Theo P. van der Weide, Cornelis H. A.
Koster, and P. van Bommel. 2000. Linguistically-
motivated information retrieval. In Encyclopedia of
Library and Information Science.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
Proceedings of ICML ?05, pages 89?96.
W. Bruce Croft, Howard R. Turtle, and David D. Lewis.
1991. The use of phrases and structured queries in
information retrieval. In Proceedings of SIGIR ?91,
pages 32?45.
Martin Dillon and Ann S. Gray. 1983. Fasit: A
fully automatic syntactically based indexing system.
Journal of the American Society for Information Sci-
ence, 34(2):99?108.
Joel L. Fagan. 1987. Automatic phrase indexing for
document retrieval. In Proceedings of SIGIR ?87,
pages 91?101.
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and Gui-
hong Cao. 2004. Dependence language model for
information retrieval. In Proceedings of SIGIR ?04,
pages 170?177.
Wessel Kraaij and Rene?e Pohlmann. 1998. Comparing
the effect of syntactic vs. statistical phrase indexing
strategies for dutch. In Proceedings of ECDL ?98,
pages 605?617.
David D. Lewis and W. Bruce Croft. 1990. Term clus-
tering of syntactic phrases. In Proceedings of SIGIR
?90, pages 385?404.
Robert M. Losee, Jr. 1994. Term dependence: truncat-
ing the bahadur lazarsfeld expansion. Information
Processing and Management, 30(2):293?303.
Loic Maisonnasse, Gilles Serasset, and Jean-Pierre
Chevallet. 2005. Using syntactic dependency and
language model x-iota ir system for clips mono and
bilingual experiments in clef 2005. In Working
Notes for the CLEF 2005 Workshop.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In Pro-
ceedings of SIGIR ?05, pages 472?479.
Donald Metzler. 2007. Using gradient descent to opti-
mize language modeling smoothing parameters. In
Proceedings of SIGIR ?07, pages 687?688.
David R. H. Miller, Tim Leek, and Richard M.
Schwartz. 1999. A hidden markov model informa-
tion retrieval system. In Proceedings of SIGIR ?99,
pages 214?221.
Mandar Mitra, Chris Buckley, Amit Singhal, and Claire
Cardie. 1997. An analysis of statistical and syn-
tactic phrases. In Proceedings of RIAO ?97, pages
200?214.
Fei Song and W. Bruce Croft. 1999. A general lan-
guage model for information retrieval. In Proceed-
ings of CIKM ?99, pages 316?321.
Munirathnam Srikanth and Rohini Srihari. 2003. Ex-
ploiting syntactic structure of queries in a language
modeling approach to ir. In Proceedings of CIKM
?03, pages 476?483.
Tomek Strzalkowski, Jose Perez-Carballo, and Mihnea
Marinescu. 1994. Natural language information re-
trieval: Trec-3 report. In Proceedings of TREC-3,
pages 39?54.
Tao Tao and ChengXiang Zhai. 2007. An exploration
of proximity measures in information retrieval. In
Proceedings of SIGIR ?07, pages 295?302.
Pasi Tapanainen and Timo Jarvinen. 1997. A non-
projective dependency parser. In Proceedings of
ANLP ?97, pages 64?71.
Michael Taylor, Hugo Zaragoza, Nick Craswell,
Stephen Robertson, and Chris Burges. 2006. Opti-
misation methods for ranking functions with multi-
ple parameters. In Proceedings of CIKM ?06, pages
585?593.
Andrew Turpin and Alistair Moffat. 1999. Statisti-
cal phrases for vector-space information retrieval. In
Proceedings of SIGIR ?99, pages 309?310.
C. J. van Rijsbergen. 1977. A theoretical basis for the
use of co-occurrence data in information retrieval.
Journal of Documentation, 33(2):106?119.
S. K. M. Wong, Wojciech Ziarko, and Patrick C. N.
Wong. 1985. Generalized vector spaces model in
information retrieval. In Proceedings of SIGIR ?85,
pages 18?25.
Chengxiang Zhai and John Lafferty. 2001. A study
of smoothing methods for language models applied
to ad hoc information retrieval. In Proceedings of
SIGIR ?01, pages 334?342.
Chengxiang Zhai. 1997. Fast statistical parsing of
noun phrases for document indexing. In Proceed-
ings of ANLP ?97, pages 312?319.
1056
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 29?32,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Novel Word Segmentation Approach for
Written Languages with Word Boundary Markers
Han-Cheol Cho
?
, Do-Gil Lee
?
, Jung-Tae Lee
?
, Pontus Stenetorp
?
, Jun?ichi Tsujii
?
and Hae-Chang Rim
?
?
Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan
?
Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
{hccho,pontus,tsujii}@is.s.u-tokyo.ac.jp, {dglee,jtlee,rim}@nlp.korea.ac.kr
Abstract
Most NLP applications work under the as-
sumption that a user input is error-free;
thus, word segmentation (WS) for written
languages that use word boundary mark-
ers (WBMs), such as spaces, has been re-
garded as a trivial issue. However, noisy
real-world texts, such as blogs, e-mails,
and SMS, may contain spacing errors that
require correction before further process-
ing may take place. For the Korean lan-
guage, many researchers have adopted a
traditional WS approach, which eliminates
all spaces in the user input and re-inserts
proper word boundaries. Unfortunately,
such an approach often exacerbates the
word spacing quality for user input, which
has few or no spacing errors; such is the
case, because a perfect WS model does
not exist. In this paper, we propose a
novel WS method that takes into consider-
ation the initial word spacing information
of the user input. Our method generates
a better output than the original user in-
put, even if the user input has few spacing
errors. Moreover, the proposed method
significantly outperforms a state-of-the-art
Korean WS model when the user input ini-
tially contains less than 10% spacing er-
rors, and performs comparably for cases
containing more spacing errors. We be-
lieve that the proposed method will be a
very practical pre-processing module.
1 Introduction
Word segmentation (WS) has been a fundamen-
tal research issue for languages that do not have
word boundary markers (WBMs); on the con-
trary, other languages that do have WBMs have re-
garded the issue as a trivial task. Texts segmented
with such WBMs, however, could contain a hu-
man writer?s intentional or un-intentional spacing
errors; and even a few spacing errors can cause
error-propagation for further NLP stages.
For written languages that have WBMs, such as
for the Korean language, the majority of recent
research has been based on a traditional WS ap-
proach (Nakagawa, 2004). The first step of the
traditional approach is to eliminate all spaces in
the user input, and then re-locate the proper places
to insert WBMs. One state-of-the-art Korean WS
model (Lee et al, 2007) is known to achieve a per-
formance of 90.31% word-unit precision, which is
comparable with other WS models for the Chinese
or Japanese language.
Still, there is a downside to the evaluation
method. If the user input has a few or no spac-
ing errors, traditional WS models may cause more
spacing errors than it correct because they produce
the same output regardless the word spacing states
of the user input.
In this paper, we propose a new WS method that
takes into account the word spacing information
from the user input. Our proposed method first
generates the best word spacing states for the user
input by using a traditional WS model; however
the method does not immediately apply the out-
put. Secondly, the method estimates a threshold
based on the word spacing quality of the user in-
put. Finally, the method uses the new word spac-
ing states that have probabilities that are higher
than the threshold.
The most important contribution of the pro-
posed method is that, for most cases, the method
generates an output that is better than the user in-
put. The experimental results show that the pro-
posed method produces a better output than the
user input even if the user input has less than 1%
spacing errors in terms of the character-unit pre-
cision. Moreover, the proposed method outper-
forms (Lee et al, 2007) significantly, when the
29
user input initially contains less than 10% spacing
errors, and even performs comparably, when the
input contains more than 10% errors. Based on
these results, we believe that the proposed method
would be a very practical pre-processing module
for other NLP applications.
The paper is organized as follows: Section 2 ex-
plains the proposed method. Section 3 shows the
experimental results. Finally, the last section de-
scribes the contributions of the proposed method.
2 The Proposed Method
The proposed method consists of three steps: a
baseline WS model, confidence and threshold es-
timation, and output optimization. The following
sections will explain the steps in detail.
2.1 Baseline Word Segmentation Model
We use the tri-gram Hidden Markov Model
(HMM) of (Lee et al, 2007) as the baseline WS
model; however, we adopt the Maximum Like-
lihood (ML) decoding strategy to independently
find the best word spacing states. ML-decoding
allows us to directly compare each output to the
threshold. There is little discrepancy in accuracy
when using ML-decoding, as compared to Viterbi-
decoding, as mentioned in (Merialdo, 1994).
1
Let o
1,n
be a sequence of n-character user input
without WBMs, x
t
be the best word spacing state
for o
t
where 1 ? t ? n. Assume that x
t
is either 1
(space after o
t
) or 0 (no space after o
t
). Then each
best word spacing state x?
t
for all t can be found by
using Equation 1.
x?
t
= argmax
i?(0,1)
P (x
t
= i|o
1,n
) (1)
= argmax
i?(0,1)
P (o
1,n
, x
t
= i) (2)
= argmax
i?(0,1)
?
x
t?2
,x
t?1
P (x
t
= i|x
t?2
, o
t?1
, x
t?1
, o
t
)
?
?
x
t?1
P (o
t+1
|o
t?1
, x
t?1
, o
t
, x
t
= i)
?
?
x
t+1
P (o
t+2
|o
t
, x
t
= i, o
t+1
, x
t+1
) (3)
Equation 2 is derived by applying the Bayes?
rule and by eliminating the constant denominator.
Moreover, the equation is simplified, as is Equa-
tion 3, by using the Markov assumption, and by
1
In the preliminary experiment, Viterbi-decoding showed
a 0.5% higher word-unit precision.
eliminating the constant parts. Every part of Equa-
tion 3 can be calculated by adding the probabilities
of all possible combinations of x
t?2
, x
t?1
, x
t+1
and x
t+2
values.
The model is trained by using the relative fre-
quency information of the training data, and a
smoothing technique is applied to relieve the data-
sparseness problem which is the linear interpola-
tion of n-grams that are used in (Lee et al, 2007).
2.2 Confidence and Threshold Estimation
We set a variable threshold that is proportional to
the word spacing quality of the user input, Confi-
dence. Formally, we can define the threshold T as
a function of a confidence C, as in Equation 4.
T = f(C) (4)
Then, we define the confidence as is done in
Equation 5. Because calculating such a variable
is impossible, we estimate the value by substi-
tuting the word spacing states produced by the
baseline WS model, x
WS
1,n
, with the correct word
spacing states, x
correct
1,n
, as is done in Equation 6.
This estimation is based on the assumption that
the word spacing states of the WS model is suf-
ficiently similar to the correct word spacing states
in the character-unit precision.
2
C =
# of x
input
t
same to x
correct
t
# of x
input
t
(5)
?
# of x
input
t
same to x
WS
t
# of x
input
t
(6)
?
n
?
?
?
?
n
?
k=1
P (x
input
k
|o
1,n
) (7)
To handle the estimation error for short sen-
tences, we use the probability generating word
spacing states of the user input with the length nor-
malization as shown in Equation 7.
Figure 1 shows that the estimated confidence of
Equation 7 is almost linearly proportional to the
true confidence of Equation 5, thus suggesting that
the threshold T can be defined as a function of the
estimated confidence of Equation 7.
3
2
In the experiment with the development data, the base-
line WS model shows about 97% character-unit precision.
3
The development data is generated by randomly intro-
ducing spacing errors into correctly spaced sentences. We
think that this reflects various intentional and un-intentional
error patterns of individuals.
30
20%30%
40%50%
60%70%
80%90%
100%
100% 96% 92% 88% 84% 80%
Estim
ated C
onfid
ence
True Confidence
Figure 1: The relationship between estimated con-
fidence and true confidence
To keep the focus on the research subject of this
paper, we simply assume f(x) = x as in Equation
8, for the threshold function f .
T ? f(C) = C (8)
In the experimental results, we confirm that
even this simple threshold function can be help-
ful in improving the performance of the proposed
method against traditional WS models.
2.3 Output Optimization
After completing the two steps described in Sec-
tion 2.1 and 2.2, we have acquired the new spacing
states for the user input generated by the baseline
WS model, and the threshold measuring the word
spacing quality of the user input.
The proposed method only applies a part of the
new word spacing states to the user input, which
have probabilities that are higher than the thresh-
old; further the method discards the other new
word spacing states that have probabilities that are
lower than the threshold. By rejecting the unreli-
able output of the baseline WS model in this way,
the proposed method can effectively improve the
performance when the user input contains a rela-
tively small number of spacing errors.
3 Experimental Results
Two types of experiments have been performed.
In the first experiment, we investigate the level of
performance improvement based on different set-
tings of the user input?s word spacing error rate.
Because it is nearly impossible to obtain enough
test data for any error rate, we generate pseudo test
data in the same way that we generate develop-
ment data.
4
In the second experiment, we attempt
4
See Footnote 3.
figuring out whether the proposed method really
improves the word spacing quality of the user in-
put in a real-world setting.
3.1 Performance Improvement according to
the Word Spacing Error Rate of User
Input
For the first experiment, we use the Sejong corpus
5
from 1998-1999 (1,000,000 Korean sentences) for
the training data, and ETRI corpus (30,000 sen-
tences) for the test data (ETRI, 1999). To gener-
ate the test data that have spacing errors, we make
twenty one copies of the test data and randomly
insert spacing errors from 0% to 20% in the same
way in which we made the development data. We
feel that this strategy can model both the inten-
tional and un-intentional human error patterns.
In Figure 2, the x-axis indicates the word spac-
ing error rate of the user input in terms of the
character-unit precision, and the y-axis shows the
word-unit precision of the output. Each graph de-
picts the word-unit precision of the test corpus,
a state-of-the-art Korean WS model (Lee et al,
2007), the baseline WS model, and the proposed
method.
Although Lee?s model is known to perform
comparably with state-of-the-art Chinese and
Japanese WS models, it does not necessarily sug-
gest that the word spacing quality of the model?s
output is better than the user input. In Figure 2,
Lee?s model exacerbates the user input when it has
spacing errors that are lower than 3%.
The proposed method, however, produces a bet-
ter output, even if the user input has 1% spacing er-
rors. Moreover, the proposed method shows a con-
siderably better performance within the 10% spac-
ing error range, as compared to Lee?s model, al-
though the baseline WS model itself does not out-
performs Lee?s model. The performance improve-
ment in this error range is fairly significant be-
cause we found that the spacing error rate of texts
collected for the second experiment was about
9.1%.
3.2 Performance Comparison with Web Text
having Usual Error Rate
In the second experiment, we attempt finding out
whether the proposed method can be beneficial un-
der real-world circumstances. Web texts, which
consist of 1,000 erroneous sentences from famous
5
Details available at: http://www.sejong.or.kr/eindex.php
31
84%
86%
88%
90%
92%
94%
96%
98%
100%
0% 2% 4% 6% 8% 10% 12% 14% 16% 18% 20%
w
or
d-u
nit
 
 
pr
ec
isio
n
word spacing error rate of user input (in character-unit precision)
Test corpus Lee's model Baseline WS model Proposed method
Figure 2: Performance improvement according to the word spacing error rate of user input
Method Web Text
Test Corpus 70.89%
Lee?s Model 70.45%
Baseline WS Model 69.13%
Proposed Method 73.74%
Table 1: Performance comparison with Web text
Web portals and personal blogs, were collected
and used as the test data. Since the test data tend
to have a similar error rate to the narrow standard
deviation, we computed the overall performance
over the average word spacing error rate, which is
9.1%. The baseline WS model is trained on the
Sejong corpus, described in Section 3.1.
The test result is shown in Table 1. The
overall performance of Lee?s model, the baseline
WS model and the proposed method decreased
by roughly 18%. We hypothesize that the per-
formance degradation probably results from the
spelling errors of the test data, and the inconsis-
tencies that exist between the training data and the
test data. However, the proposed method still im-
proves the word spacing quality of the user input
by 3%, while the two traditional WS models de-
grades the quality. Such a result indicates that
the proposed method is effective for real-world
environments, as we had intended. Furthermore,
we also believe that the performance can be im-
proved if a proper training corpus is provided, or
if a spelling correction method is integrated.
4 Conclusion
In this paper, we proposed a new WS method that
uses the word spacing information of the user in-
put, for languages with WBMs. By utilizing the
user input, the proposed method effectively refines
the output of the baseline WS model and improves
the overall performance.
The most important contribution of this work is
that it produces an output that is better than the
user input even if it contains few spacing errors.
Therefore, the proposed method can be applied as
a pre-processing module for practical NLP appli-
cations without introducing a risk that would gen-
erate a worse output than the user input. Moreover,
the performance is notably better than a state-of-
the-art Korean WS model (Lee et al, 2007) within
the 10% spacing error range, which human writers
seldom exceed. It also performs comparably, even
if the user input contains more than 10% spacing
errors.
5 Acknowledgment
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
References
ETRI. 1999. Pos-tag guidelines. Technical report.
Electronics and Telecomminications Research Insti-
tute.
Do-Gil Lee, Hae-Chang Rim, and Dongsuk Yook.
2007. Automatic Word Spacing Using Probabilistic
Models Based on Character n-grams. IEEE Intelli-
gent Systems, 22(1):28?35.
Bernard Merialdo. 1994. Tagging English text with a
probabilistic model. Comput. Linguist., 20(2):155?
171.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level
information. In COLING ?04, page 466, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
32
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 321?324,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
The Contribution of Stylistic Information to
Content-based Mobile Spam Filtering
Dae-Neung Sohn and Jung-Tae Lee and Hae-Chang Rim
Department of Computer and Radio Communications Engineering
Korea University
Seoul, 136-713, South Korea
{danny,jtlee,rim}@nlp.korea.ac.kr
Abstract
Content-based approaches to detecting
mobile spam to date have focused mainly
on analyzing the topical aspect of a SMS
message (what it is about) but not on the
stylistic aspect (how it is written). In this
paper, as a preliminary step, we investigate
the utility of commonly used stylistic fea-
tures based on shallow linguistic analysis
for learning mobile spam filters. Experi-
mental results show that the use of stylis-
tic information is potentially effective for
enhancing the performance of the mobile
spam filters.
1 Introduction
Mobile spam, also known as SMS spam, is a sub-
set of spam that involves unsolicited advertising
text messages sent to mobile phones through the
Short Message Service (SMS) and has increas-
ingly become a major issue from the early 2000s
with the popularity of mobile phones. Govern-
ments and many service providers have taken var-
ious countermeasures in order to reduce the num-
ber of mobile spam (e.g. by imposing substantial
fines on spammers, blocking specific phone num-
bers, creating an alias address, etc.). Nevertheless,
the rate of mobile spam continues to rise.
Recently, a more technical approach to mobile
spam filtering based on the content of a SMS mes-
sage has started gaining attention in the spam re-
search community. G?omez Hidalgo et al (2006)
previously explored the use of statistical learning-
based classifiers trained with lexical features, such
as character and word n-grams, for mobile spam
filtering. However, content-based spam filtering
directed at SMS messages are very challenging,
due to the fact that such messages consist of only
a few words. More recent studies focused on ex-
panding the feature set for learning-based mobile
spam classifiers with additional features, such as
orthogonal sparse word bi-grams (Cormack et al,
2007a; Cormack et al, 2007b).
Collectively, the features exploited in earlier
content-based approach to mobile spam filtering
are topical terms or phrases that statistically in-
dicate the spamness of a SMS message, such as
?loan? or ?70% off sale?. However, there is
no guarantee that legitimate (non-spam) messages
would not contain such expressions. Any of us
may send a SMS message such as ?need ur ad-
vise on private loans, plz call me? or ?mary,
abc.com is having 70% off sale today?. For cur-
rent content-based mobile spam filters, there is a
chance that they would classify such legitimate
messages as spam. This motivated us to not only
rely on the message content itself but incorporate
new features that reflect its ?style,? the manner in
which the content is expressed, in mobile spam fil-
tering.
The main goal of this paper is to investigate the
potential of stylistic features in improving the per-
formance of learning-based mobile spam filters. In
particular, we adopt stylistic features previously
suggested in authorship attribution studies based
on stylometry, the statistical analysis of linguistic
style.
1
Our assumption behind adopting the fea-
tures from authorship attribution are as follows:
? There are two types of SMS message senders,
namely spammers and non-spammers.
? Spammers have distinctive linguistic styles
and writing behaviors (as opposed to non-
spammers) and use them consistently.
? The SMS message as an end product carries
the author?s ?fingerprints?.
1
Authorship attribution involves identifying the author of
a text given some stylistic characteristics of authors? writing.
See Holmes (1998) for overview.
321
Although there are many types of stylistic fea-
tures suggested in the literature, we make use of
the ones that are readily computable and countable
from SMS message texts without any complex lin-
guistic analysis as a preliminary step, including
word and sentence lengths (Mendenhall, 1887),
frequencies of function words (Mosteller and Wal-
lace, 1964), and part-of-speech tags and tag n-
grams (Argamon-Engelson et al, 1998; Koppel et
al., 2003; Santini, 2004).
Our experimental result on a large-scale, real
world SMS dataset demonstrates that the newly
added stylistic features effectively contributes to
statistically significant improvement on the perfor-
mance of learning-based mobile spam filters.
2 Stylistic Feature Set
All stylistic features listed below have been auto-
matically extracted using shallow linguistic analy-
sis. Note that most of them have been motivated
from previous stylometry studies.
2.1 Length features: LEN
Mendenhall (1887) first created the idea of count-
ing word lengths to judge the authorship of texts,
followed by Yule (1939) and Morton (1965) with
the use of sentence lengths. In this paper, we mea-
sure the overall byte length of SMS messages and
the average byte length of words in the message as
features.
2.2 Function word frequencies: FW
Motivated from a number of stylometry studies
based on function words including Mosteller and
Wallace (1964), Tweedie et al (1996) and Arg-
amon and Levitan (2005), we measure the fre-
quencies of function words in SMS messages as
features. The intuition behind function words is
that due to their high frequency in languages and
highly grammaticalized roles, such words are un-
likely to be subject to conscious control by the au-
thor and that the frequencies of different function
words would vary greatly across different authors
(Argamon and Levitan, 2005).
2.3 Part-of-speech n-grams: POS
Following the work of Argamon-Engelson et al
(1998), Koppel et al (2003), Santini (2004) and
Gamon (2004), we extract part-of-speech n-grams
(up to trigrams) from the SMS messages and use
their frequencies as features. The idea behind their
utility is that spammers would favor certain syn-
tactic constructions in their messages.
2.4 Special characters: SC
We have observed that many SMS messages con-
tain special characters and that their usage varies
between spam and non-spam messages. For in-
stance, non-spammers often use special characters
to create emoticons to express their mood, such as
?:-)? (smiling) or ?T T? (crying), whereas spam-
mers tend to use special character or patterns re-
lated to monetary matters, such as ?$$$? or ?%?.
Therefore, we also measured the ratio of special
characters, the number of emoticons, and the num-
ber of special character patterns in SMS messages
as features.
2
3 Learning a Mobile Spam Filter
In this paper, we use maximum entropy model,
which have shown robust performance in various
text classification tasks in the literature, for learn-
ing the mobile spam filter. Simply put, given a
number of training samples (in our case, SMS
messages), each with a label Y (where Y = 1 if
spam and 0 otherwise) and a feature vector x, the
filter learns a vector of feature weight parameters
w. Given a test sample X with its feature vector x,
the filer outputs the conditional probability of pre-
dicting the data as spam, P (Y = 1|X = x). We
use the L-BFGS algorithm (Malouf, 2002) and the
Information Gain (IG) measure for parameter esti-
mation and feature selection, respectively.
4 Experiments
4.1 SMS test collections
We use a collection of mobile SMS messages in
Korean, with 18,000 (60%) legitimate messages
and 12,000 (40%) spam messages. This collec-
tion is based on one used in our previous work
(Sohn et al, 2008) augmented with 10,000 new
messages. Note that the size is approximately 30
times larger than the most previous work by Cor-
mack et al (2007a) on mobile spam filtering.
4.2 Feature setting
We compare three types of feature sets, as follows:
2
For emoticon and special pattern counts, we used man-
ually constructed lexicons consisting of 439 emoticons and
229 special patterns.
322
? Baseline: This set consists of lexical features
in SMS messages, including words, charac-
ter n-grams, and orthogonal sparse word bi-
grams (OSB)
3
. This feature set represents
the content-based approaches previously pro-
posed by G?omez Hidalgo et al (2006), Cor-
mack et al (2007a) and Cormack et al
(2007b).
? Proposed: This feature set consists of all the
stylistic features mentioned in Section 2.
? Combined: This set is a combination of both
the baseline and proposed feature sets.
For all three sets, we make use of 100 features with
the highest IG values.
4.3 Evaluation measures
Since spam filtering task is very sensitive to false-
positives (i.e. legitimate classified as spam) and
false-negatives (i.e. spam classified as legitimate),
special care must be taken when choosing an ap-
propriate evaluation criterion.
Following the TREC Spam Track, we evalu-
ate the filters using ROC curves that plot false-
positive rate against false-negative rate. As a sum-
mary measure, we report one minus area under
the ROC curve (1?AUC) as a percentage with
confidence intervals, which is the TREC?s official
evaluation measure.
4
Note that lower 1?AUC(%)
value means better performance. We used the
TREC Spam Filter Evaluation Toolkit
5
in order to
perform the ROC analysis.
4.4 Results
All experiments were performed using 10-fold
cross validation. Statistical significance of differ-
ences between results were computed with a two-
tailed paired t-test. The symbol ? indicates statis-
tical significance over an appropriate baseline at
p < 0.01 level.
Table 1 reports the 1?AUC(%) summary for
each feature settings listed in Section 4.2. Notice
that Proposed achieves significantly better perfor-
mance than Baseline. (Recall that the smaller, the
3
OSB refers to words separated by 3 or fewer words,
along with an indicator of the difference in word positions;
for example, the expression ?the quick brown fox? would
induce following OSB features: ?the (0) quick?, ?the (1)
brown?, ?the (2) fox?, ?quick (0) brown?, ?quick (1) fox?,
and ?brown (0) fox? (Cormack et al, 2007a).
4
For detail on ROC analysis, see Cormack et al (2007a).
5
Available at http://plg.uwaterloo.ca/.trlynam/spamjig/
Feature set 1?AUC (%)
Baseline 10.7227 [9.4476 - 12.1176]
Proposed 4.8644
?
[4.2726 - 5.5886]
Combined 3.7538
?
[3.1186 - 4.4802]
Table 1: Performance of different feature settings.
50.00
10.00
1.00
50.0010.001.000.100.01
Fal
se 
Ne
gat
ive
 Ra
te(lo
git s
cale
)
False Positve Rate (logit scale)
CombinedProposedBaseline
Figure 1: ROC curves of different feature settings.
better.) An even greater performance gain is ob-
tained by combining both Proposed and Baseline.
This clearly indicates that stylistic aspects of SMS
messages are potentially effective for mobile spam
filtering.
Figure 1 shows the ROC curves of each fea-
ture settings. Notice the tradeoff when Proposed
is used solely with comparison to Baseline; false-
positive rate is worsened in return for gaining bet-
ter false-negative rate. Fortunately, when both fea-
ture sets are combined, false-positive rate is re-
mained unchanged while the lowest false-negative
rate is achieved. This suggests that the addition of
stylistic features contributes to the enhancement of
false-negative rate while not hurting false-positive
rate (i.e. the cases where spam is classified as le-
gitimate are significantly lessened).
In order to evaluate the contribution of different
types of stylistic features, we conducted a series
of experiments by removing features of a specific
type at a time from Combined. Table 2 shows the
detailed result. Notice that LEN and SC features
are the most helpful, since the performance drops
significantly after removing either of them. Inter-
estingly, FW and POS features show similar con-
tributions; we suggest that these two feature types
have similar effects in this filtering task.
We also conducted another series of experi-
ments, by adding one feature type at a time to
Baseline. Table 3 reports the results. Notice that
LEN features are consistently the most helpful.
The most interesting result is that POS features
continuously contributes the least. We carefully
323
Feature set 1?AUC (%)
Combined 3.7538 [3.1186 - 4.4802]
? LEN 4.7351
?
[4.0457 - 5.6405]
? FW 3.9823
?
[3.3048 - 4.5930]
? POS 4.0712
?
[3.4057 - 4.8630]
? SC 4.7644
?
[4.1012 - 5.4350]
Table 2: Performance by removing one stylistic
feature set from the Combined set.
Feature set 1?AUC (%)
Baseline 10.7227 [9.4476 - 12.1176]
+ LEN 5.5275
?
[4.0457 - 6.6281]
+ FW 6.0828
?
[5.1783 - 6.9249]
+ POS 9.6103
?
[8.7190 - 11.0579]
+ SC 7.5288
?
[6.6049 - 8.4466]
Table 3: Performance by adding one stylistic fea-
ture set to the Baseline set.
hypothesize that the result is due to high depen-
dencies between POS and lexical features.
5 Discussion
In this paper, we have introduced new features that
indicate the written style of texts for content-based
mobile spam filtering. We have also shown that the
stylistic features are potentially useful in improv-
ing the performance of mobile spam filters.
This is definitely a work in progress, and much
more experimentation is required. Deep linguis-
tic analysis-based stylistic features, such as con-
text free grammar production frequencies (Ga-
mon, 2004) and syntactic rewrite rules in an au-
tomatic parse (Baayen et al, 1996), that have al-
ready been successfully used in the stylometry lit-
erature may be considered. Perhaps most impor-
tantly, the method must be tested on various mo-
bile spam data sets written in languages other than
Korean. These would be our future work.
References
Shlomo Argamon and Shlomo Levitan. 2005. Measur-
ing the usefulness of function words for authorship
attribution. In Proceedings of ACH/ALLC ?05.
Shlomo Argamon-Engelson, Moshe Koppel, and Galit
Avneri. 1998. Style-based text categorization:
What newspaper am i reading? In Proceedings of
AAAI ?98 Workshop on Text Categorization, pages
1?4.
H. Baayen, H. van Halteren, and F. Tweedie. 1996.
Outside the cave of shadows: using syntactic annota-
tion to enhance authorship attribution. Literary and
Linguistic Computing, 11(3):121?132.
Gordon V. Cormack, Jos?e Mar??a G?omez Hidalgo, and
Enrique Puertas S?anz. 2007a. Spam filtering for
short messages. In Proceedings of CIKM ?07, pages
313?320.
Gordon V. Cormack, Jos?e Mar??a G?omez Hidalgo, and
Enrique Puertas S?anz. 2007b. Feature engineering
for mobile (sms) spam filtering. In Proceedings of
SIGIR ?07, pages 871?872.
Michael Gamon. 2004. Linguistic correlates of style:
Authorship classification with deep linguistic analy-
sis features. In Proceedings of COLING ?04, page
611.
Jos?e Mar??a G?omez Hidalgo, Guillermo Cajigas
Bringas, Enrique Puertas S?anz, and Francisco Car-
rero Garc??a. 2006. Content based sms spam filter-
ing. In Proceedings of DocEng ?06, pages 107?114.
David I. Holmes. 1998. The evolution of stylometry
in humanities scholarship. Literary and Linguistic
Computing, 13(3):111?117.
Moshe Koppel, Shlomo Argamon, and Anat R. Shi-
moni. 2003. Automatically categorizing written
texts by author gender. Literary and Linguistic
Computing, 17(4):401?412.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of COLING ?02, pages 1?7.
T. C. Mendenhall. 1887. The characteristic curves of
composition. Science, 9(214):237?246.
A. Q. Morton. 1965. The authorship of greek prose.
Journal of the Royal Statistical Society Series A
(General), 128(2):169?233.
Frederick Mosteller and David L. Wallace. 1964. In-
ference and Disputed Authorship: The Federalist.
Addison-Wesley.
Marina Santini. 2004. A shallow approach to syntactic
feature extraction for genre classification. In Pro-
ceedings of CLUK Colloquium ?04.
Dae-Neung Sohn, Joong-Hwi Shin, Jung-Tae Lee,
Seung-Wook Lee, and Hae-Chang Rim. 2008.
Contents-based korean sms spam filtering using
morpheme unit features. In Proceedings of HCLT
?08, pages 194?199.
E. J. Tweedie, S. Singh, and D. I. Holmes. 1996. Neu-
ral network applications in stylometry: The federal-
ist papers. Computers and the Humanities, 30:1?10.
G. Udny Yule. 1939. On sentence-length as a statisti-
cal characteristic of style in prose, with application
to two cases of disputed authorship. Biometrika,
30(3-4):363?390.
324
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 251?256,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Korea University System in the HOO 2012 Shared Task
Jieun Lee? and Jung-Tae Lee? and Hae-Chang Rim?
?Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
?Research Institute of Computer Information & Communication, Korea University, Seoul, Korea
{jelee,jtlee,rim}@nlp.korea.ac.kr
Abstract
In this paper, we describe the Korea Univer-
sity system that participated in the HOO 2012
Shared Task on the correction of preposition
and determiner errors in non-native speaker
texts. We focus our work on training the sys-
tem on a large collection of error-tagged texts
provided by the HOO 2012 Shared Task or-
ganizers and incrementally applying several
methods to achieve better performance.
1 Introduction
In the literature, there have been efforts aimed at de-
veloping grammar correction systems designed es-
pecially for non-native English speakers. A typi-
cal approach is to train statistical models on well-
formed texts written by native English speakers and
apply the learned models to non-native speaker texts
to correct textual errors based on given context. This
approach, however, fails to model the types of errors
that non-native speakers usually make. Recent stud-
ies demonstrate that it is possible to improve the per-
formance of error correction systems by training the
models on error-annotated non-native speaker texts
(Han et al, 2010; Dahlmeier and Ng, 2011; Gamon,
2010). Most recently, a large collection of training
data consisting of preposition and determiner errors
made by non-native English speakers has been re-
leased in the HOO (Helping Our Own) 2012 Shared
Task, which aims at promoting the research and de-
velopment of automated tools for assisting authors
in writing (Dale et al, 2012).
In this paper, we introduce our error correction
system that participated in the HOO 2012 Shared
Task, where the goal is to correct errors in the use of
prepositions and determiners by non-native speakers
of English. We mainly focus our efforts on training
the system using the non-native speaker texts pro-
vided in the HOO 2012 Shared Task. We also share
our experience in handling some issues that emerged
while exclusively using the non-native speaker texts
for training our system. In the following sections,
we will describe the system in detail.
2 System Architecture
The goal of our system is to detect and correct prepo-
sition and determiner errors in a given text. Our sys-
tem consists of two types of classifiers, namely edit
and insertion classifiers. Inputs for the two types
of classifiers are noun phrases (NP), verb phrases
(VP), and prepositional phrases (PP); we initially
pre-process the text given for training/testing by us-
ing the Illinois Chunker1 and the Stanford Part-of-
Speech Tagger (Toutanova et al, 2003). For learn-
ing the classifiers, we use maximum entropy models,
which have been successfully applied to many tasks
in natural language processing. We particularly use
Le Zhang?s Maximum Entropy Modeling Toolkit2
for implementation.
2.1 Edit Classifiers
The role of an edit classifier is to check the source
preposition/determiner word originally chosen by
the author in a given text. If the source word
is incorrect, the classifier replaces it with a bet-
ter choice. For every preposition/determiner word,
1Available at http://cogcomp.cs.illinois.edu
2Available at http://homepages.inf.ed.ac.uk/lzhang10/
251
we train a classifier using examples that are ob-
served in training data. The choice for preposi-
tions is limited to eleven prepositions (about, at,
as, by, for, from, in, of, on , to, with) that most
frequently occur in the training data, and the can-
didates for determiner choice are the and a/an. In
summary, we train a total of thirteen edit classifiers,
one for each source preposition or determiner. For
each edit classifier, the set of candidate outputs con-
sists of the source preposition/determiner word it-
self, other confusable preposition/determiner words,
and no preposition/determiner in case the source
word should be deleted. Note that the number of
confusable words for each source preposition is de-
cided flexibly, depending on examples observed in
the training data; a similar approach has been pro-
posed earlier by Rozovskaya and Roth (2010a). For
a particular source preposition/determiner word in
the test data, the system decides whether to correct
it or not based on the output of the classifier for that
source word.
2.2 Insertion Classifier
Although the edit classifiers described above are
capable of deciding whether a source preposi-
tion/determiner word that appears in the test data
should be replaced or removed, a large proportion
of common mistakes for non-native English writers
consists of missing prepositions/determiners (i.e.,
leaving them out by mistake). To deal with those
types of errors, we train a special classifier for inser-
tions. A training or testing event for this particular
classifier is any whitespace before or after a word
in a noun or verb phrase that is a potential loca-
tion for a preposition or determiner. Table 1 shows
the five simple heuristic patterns based on part-of-
speech tags that the system uses in order to locate
potential sites for prepositions/determiners. Note
that s is a whitespace to be examined, an asterisk (*)
means wildcard, and NN includes the tags that start
with NN, such as NNS, NNP, and NNPS. VB is also
treated in the same manner as NN. The set of can-
didate outputs consists of the eleven prepositions,
the two determiners, and no preposition/determiner
class. Once a candidate position for insertion is de-
tected in the test data, the system decides whether to
make an insertion or not based on the output of the
insertion classifier.
Pattern Example
s+NN I?ll give you all information
s+*+NN I need few days
s+VB It may seem relaxing at beginning
s+*+VB Buy new colored clothes
VB+s I?m looking forward your reply
Table 1: Patterns of candidates for insertion
2.3 Features
Both edit and insertion classifiers can be trained us-
ing three types of features described below.
? LEX/POS/HEAD This feature set refers to the
contextual features from a window of n tokens
to the right and left that are practically used in
error correction studies (Rozovskaya and Roth,
2010b; Han et al, 2010; Gamon, 2010). Such
features include lexical features, part-of-speech
tags, and head words of the preceding and the
following chunks of the source word. In this
work, we set n to be 3.
? HAN This represents the set of features specifi-
cally used in the work of Han et al (2010); they
demonstrate that a model trained on non-native
speaker texts can outperform one trained solely
on well-formed texts.
? L13 L1 refers to the first language of the au-
thor. There have been some efforts to leverage
L1 information for improving error correction
performance. For example, Rozovskaya and
Roth (2011) propose an algorithm for adapting
a learned model to the L1 of the author. There
have been many studies leveraging writers? L1.
In this work, we propose to directly utilize L1
information of the authors as features. We also
leverage additional features by combining L1
and individual head words that govern or are
governed by VP or NP.
3 Additional Methods for Improvement
The training data provided in the HOO 2012 Shared
Task consists of exam scripts drawn from the pub-
licly available FCE dataset (Yannakoudakis et al,
3L1 information was provided in the training data but not in
the test data. Therefore, the benefits of using L1 remain incon-
clusive in this paper.
252
a/an the NULL
6028 114 203
Table 2: Training data distribution for a/an classifier
about as at by for from
0 3 2510 1 2 3
in of on to with NULL
75 7 20 30 3 41
Table 3: Training data distribution for at classifier
2011) with textual errors annotated in HOO data
format. From this data, we extract examples for
training our classifiers. For example, let w be a
source word that we specifically want our classifier
to learn. Every use of w that appears in the train-
ing data may be an example that the classifier can
learn from. However, it is revealed that for all w,
there are always many more examples where w is
used correctly than examples where w is replaced or
removed. Table 2 and Table 3 respectively show the
class distributions of all examples for source words
a/an and at that are observable from the whole train-
ing data for training a/an- and at-specific classifiers.
We can see that various classes among the training
data are unevenly represented. When training data is
highly skewed as shown in the two tables, construct-
ing a useful classifier becomes a challenging task.
We observed from our preliminary experiments that
classifiers learned on highly unbalanced data hardly
tend to correct the incorrect choices made by non-
native speakers. Therefore, we investigate two sim-
ple ways to alleviate this problem.
3.1 Filtering Examples Less Likely to be
Incorrect
As mentioned above, there are many more exam-
ples where the source preposition/determiner is used
without any error. One straightforward way to ad-
just the training data distribution is to reduce the
number of examples where the source word is less
likely to be replaced or removed by using language
model probabilities. If a language model learned on
a very large collection of well-formed texts returns
a very high language model probability for a source
word surrounded by its context, it may be reason-
Class Initial After After
Distribution Filtering Adding
about 0 0 528
as 3 3 275
at 2510 2367 2367
by 1 1 207
for 2 2 1159
from 3 3 550
in 75 75 1521
of 7 7 1454
on 20 20 541
to 30 30 2309
with 3 3 727
NULL 41 41 41
Table 4: Refined data distribution for at classifier
able to assume that the source word is used correctly.
Therefore we build a language model trained on the
English Gigaword corpus by utilizing trigrams. Be-
fore providing examples to the classifiers for train-
ing or testing, we filter out those that have very high
language model probabilities above a pre-defined
threshold value.
3.2 Adding Artificial Errors
Our second approach is to introduce more artificial
examples to the training data, so that the class dis-
tribution of all training examples becomes more bal-
anced. For example, if we aim at adding more train-
ing examples for a/an classifier, we would extract
correct phrases such as ?the different actor? from the
training data and artificially convert it into ?a differ-
ent actor? so that an example of a/an being corrected
to the is also provided to a/an classifier for training.
When adding aritificial examples into the training
data, we avoid the number of examples belonging
to each class exceeding the number of cases where
the source word is not replaced or removed. Table
4 demonstrates the results of both the filtering and
adding approaches for training the a/an classifier.
4 Experiments
4.1 Runs
This section describes individual runs that we sub-
mitted to the HOO 2012 Shared Task organizers. Ta-
ble 5 represents the setting of each run.
253
Runs Models Features Filtering Adding
Threshold
Run0 LM n/a n/a
Run1 ME LEX/POS/HEAD X X
Run2 ME HAN X X
Run3 ME LEX/POS/HEAD -2 X
Run4 ME LEX/POS/HEAD -2 O
Run5 ME LEX/POS/HEAD, L1 -2 O
Run6 ME LEX/POS/HEAD, L1, age -2 O
Run7 ME Insertion: POS/HEAD X X
Other: LEX/POS/HEAD
Run8 ME LEX/POS/HEAD -3 X
Table 5: The explanation of each runs
? Run0 This is a baseline run that represents the
language model approach proposed by Gamon
(2010). We train our language model on Giga-
word corpus, utilizing trigrams with interpola-
tion and Kneser-Ney discount smoothing.
? Run1, 2 Run1 and 2 represent our system us-
ing the LEX/POS/HEAD feature sets and HAN
feature sets respectively. Neither additional
method described in Section 3 is applied.
? Run3, 8 These runs represent our system us-
ing LEX/POS/HEAD features (Run1), where
examples that are less likely to be incorrect are
filtered out by consulting our language model.
The threshold value is set to ?2 and ?3 for
Runs 3 and 8 respectively.
? Run4 This particular run is one where we intro-
duce additional errors in order to make the class
distribution of the training data for the classi-
fiers more balanced. This step is incrementally
applied in the setting of Run3.
? Run5, 6 Run5 and 6 are when we consider L1
information and age respectively as additional
features for training the classifiers. The basic
setup is same as Run4.
? Run7 This run represents our system with
its insertion classifier trained using POS and
HEAD features only. No LEX features are
used.
Runs Precision Recall F-score
Run0 1.45 15.45 2.65
Run1 1.35 10.82 2.39
Run2 1.23 11.48 2.22
Run3 1.33 10.6 2.36
Run4 1.19 11.26 2.15
Run5 1.02 10.38 1.87
Run6 0.99 9.93 1.79
Run7 1.16 11.26 2.1
Run8 1.46 11.04 2.58
Table 6: Correction before test data revisions
5 Results
Table 6 shows the correction scores of the individual
runs that we originally submitted. Unfortunately, we
should confess that we made a vital mistake while
generating the runs from 1-8; the modules imple-
mented for learning the insertion classifier had some
bugs that we could not notice during the submission
time. Because of this, our system was unable to han-
dle MD and MT type errors properly. This is the
reason why the performance figures of our runs are
very low. For reference, we include Tables 7-10 that
illustrate the performance of our individual runs that
we calculated by ourselves using the test data and
the evaluation tool provided by the organizers.
We can observe that Run3 outperforms Run1 and
Run4 performs better than Run3, which demon-
strates that our attempts to improve the system per-
formance by adjusting training data for classifiers
254
Runs Precision Recall F-score
Run1 42.67 7.06 12.12
Run2 49.28 7.51 13.03
Run3 47.62 6.62 11.63
Run4 45.45 7.73 13.21
Run5 33.82 10.15 15.62
Run6 8.68 18.54 11.82
Run7 33.33 10.82 16.33
Run8 50.0 7.28 12.72
Table 7: Recognition before test data revisions (system
revised)
Runs Precision Recall F-score
Run1 32.0 5.3 9.09
Run2 42.03 6.4 11.11
Run3 34.92 4.86 8.53
Run4 37.66 6.4 10.94
Run5 26.47 7.94 12.22
Run6 5.68 12.14 7.74
Run7 24.49 7.95 12.0
Run8 42.42 7.28 10.79
Table 8: Correction before test data revisions (system re-
vised)
help. Moreover, we can also see that L1 informa-
tion helps when directly used for training features.
6 Conclusion
This was our first attempt to participate in a shared
task that involves the automatic correction of gram-
matical errors made by non-native speakers of En-
glish. In this work, we tried to focus on investigating
simple ways to improve the error correction system
learned on non-native speaker texts. While we had
made some critical mistakes on the submitted runs,
we were able to observe that our method can poten-
tially improve error correction systems.
Acknowledgments
We would like to thank Hyoung-Gyu Lee for his
technical assistance.
References
Daniel Dahlmeier and Hwee Tou Ng. 2011. Gram-
matical error correction with alternating structure op-
Runs Precision Recall F-score
Run1 49.33 7.82 13.50
Run2 52.17 7.61 13.28
Run3 52.38 6.98 12.31
Run4 51.95 8.46 14.55
Run5 37.5 10.78 16.75
Run6 9.29 19.03 12.5
Run7 36.73 11.42 17.42
Run8 51.52 7.18 12.62
Table 9: Recognition after test data revisions (system re-
vised)
Runs Precision Recall F-score
Run1 34.67 5.5 9.49
Run2 42.02 6.13 10.7
Run3 36.51 4.86 8.58
Run4 38.96 6.34 10.91
Run5 29.42 8.45 13.13
Run6 6.40 13.11 8.61
Run7 25.85 8.03 12.26
Run8 42.42 5.92 10.39
Table 10: Correction after test data revisions (system re-
vised)
timization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, ACL-HLT
?11, pages 915?923, Portland, Oregon.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. Hoo 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the 7th Workshop on Innovative Use of NLP for
Building Educational Applications, HOO ?12, Mon-
treal, Canada.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing: a meta-classifier
approach. In Human Language Technologies: Pro-
ceedings of the 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, NAACL-HLT ?10, pages 163?171,
Los Angeles, California.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an error-annotated learner
corpus to develop an esl/efl error correction system.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation, LREC ?10,
pages 763?770, Malta.
Alla Rozovskaya and Dan Roth. 2010a. Generating
confusion sets for context-sensitive error correction.
255
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 961?970, Cambridge, Massachusetts.
Alla Rozovskaya and Dan Roth. 2010b. Training
paradigms for correcting errors in grammar and usage.
In Human Language Technologies: Proceedings of the
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL-HLT ?10, pages 154?162, Los Angeles, Cali-
fornia.
Alla Rozovskaya and Dan Roth. 2011. Algorithm se-
lection and model adaptation for esl correction tasks.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, ACL-HLT ?11, pages
924?933, Portland, Oregon.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Edmonton, Canada.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, ACL-
HLT ?11, pages 180?189, Portland, Oregon.
256
Proceedings of the TextGraphs-7 Workshop at ACL, pages 15?19,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Using Link Analysis to Discover Interesting Messages Spread Across Twitter
Min-Chul Yang? and Jung-Tae Lee? and Hae-Chang Rim?
?Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
?Research Institute of Computer Information & Communication, Korea University, Seoul, Korea
{mcyang,jtlee,rim}@nlp.korea.ac.kr
Abstract
Twitter, a popular social networking service,
enables its users to not only send messages
but re-broadcast or retweet a message from an-
other Twitter user to their own followers. Con-
sidering the number of times that a message is
retweeted across Twitter is a straightforward
way to estimate how interesting it is. How-
ever, a considerable number of messages in
Twitter with high retweet counts are actually
mundane posts by celebrities that are of inter-
est to themselves and possibly their followers.
In this paper, we leverage retweets as implicit
relationships between Twitter users and mes-
sages and address the problem of automati-
cally finding messages in Twitter that may be
of potential interest to a wide audience by us-
ing link analysis methods that look at more
than just the sheer number of retweets. Exper-
imental results on real world data demonstrate
that the proposed method can achieve better
performance than several baseline methods.
1 Introduction
Twitter (http://twitter.com) is a popular so-
cial networking and microblogging service that en-
ables its users to share their status updates, news,
observations, and findings in real-time by posting
text-based messages of up to 140 characters, called
tweets. The service rapidly gained worldwide pop-
ularity as a communication tool, with millions of
users generating millions of tweets per day. Al-
though many of those tweets contain valuable in-
formation that is of interest to many people, many
others are mundane tweets, such as ?Thanks guys
for the birthday wishes!!? that are of interest only to
the authors and users who subscribed to their tweets,
known as followers. Finding tweets that are of po-
tential interest to a wide audience from large volume
of tweets being accumulated in real-time is a crucial
but challenging task. One straightforward way is to
rely on the numbers of times each tweet has been
propagated or retweeted by readers of the tweet.
Hong et al (2011) propose to regard retweet count
as a measure of popularity and present classifiers for
predicting whether and how often new tweets will be
retweeted in the future. However, mundane tweets
by highly popular users, such as celebrities with
huge numbers of followers, can record high retweet
counts. Alonso et al (2010) use crowdsourcing to
categorize a set of tweets as ?only interesting to au-
thor and friends? and ?possibly interesting to others?
and report that the presence of a URL link is a single,
highly effective feature for distinguishing interest-
ing tweets with more than 80% accuracy. This sim-
ple rule, however, may incorrectly recognize many
interesting tweets as not interesting, simply because
they do not contain links. Lauw et al (2010) suggest
several features for identifying interesting tweets but
do not experimentally validate them.
In this study, we follow the definition of inter-
esting tweets provided by Alonso et al (2010) and
focus on automatic methods for finding tweets that
may be of potential interest to not only the authors
and their followers but a wider audience. Since
retweets are intended to spread tweets to new audi-
ences, they are often a recommendation or, accord-
ing to Boyd et al (2010), productive communica-
tion tool. Thus, we model Twitter as a graph con-
15
sisting of user and tweet nodes implicitly connected
by retweet links, each of which is formed when one
user retweets what another user tweeted. We present
a variant of the popular HITS algorithm (Kleinberg,
1999) that exploits the retweet link structure as an
indicator of how interesting an individual tweet is.
Specifically, we draw attention on the fact that not all
retweets are meaningful. Some users retweet a mes-
sage, not because of its content, but only because
they were asked to, or because they regard retweet-
ing as an act of friendship, loyalty, or homage to-
wards the person who originally tweeted (Boyd et
al., 2010). The algorithm proposed in this paper is
designed upon the premise that not all retweet links
are created equal, assuming that some retweets may
carry more importance or weight than others. Welch
et al (2011) and Romero et al (2011) similarly ex-
tend link analysis to Twitter, but address essentially
different problems. We conduct experiments on real
world tweet data and demonstrate that our method
achieves better performance than the simple retweet
count approach and a similar recent work on Twitter
messages (Castillo et al, 2011) that uses supervised
learning with a broad spectrum of features.
2 Proposed Method
We treat the problem of finding interesting tweets as
a ranking problem where the goal is to derive a scor-
ing function which gives higher scores to interesting
tweets than to uninteresting ones in a given set of
tweets. To derive the scoring function, we adopt a
variant of HITS, a popular link analysis method that
emphasizes mutual reinforcement between authority
and hub nodes (Kleinberg, 1999).
Formally, we model the Twitter structure as di-
rected graph G = (N,E) with nodes N and di-
rectional edges E. We consider both users U =
{u1, . . . , unu} and tweets T = {t1, . . . , tnt} as
nodes and the retweet relations between these nodes
as directional edges. For instance, if tweet ta, cre-
ated by user ua, retweets tb, written by user ub, we
create a retweet edge eta,tb from ta to tb and another
retweet edge eua,ub from ua to ub.
1 Strictly speak-
ing,G has two subgraphs, one based only on the user
nodes and another based on the tweet nodes. Instead
of running HITS on the tweet subgraph right away,
1Note that two user nodes can have multiple edges.
we first run it on the user subgraph and let tweets in-
herit the scores from their publishers. Our premise
is that the scores of a user is an important prior in-
formation to infer the scores of the tweets that the
user published.
User-level procedure: We first run the algorithm
on the user subgraph. ?ui, we update the authority
scores A(ui) as:
?
?j:euj,ui?E
|{uk ? U : euj ,uk ? E}|
|{k : euj ,uk ? E}|
?H(uj) (1)
Then, ?ui, we update the hub scores H(ui) to be:
?
?j:eui,uj?E
|{uk ? U : euk,uj ? E}|
|{k : euk,uj ? E}|
?A(uj) (2)
A series of iterations is performed until the scores
are converged. After each iteration, the author-
ity/hub scores are normalized by dividing each of
them by the square root of the sum of the squares
of all authority/hub values. When this user-level
stage ends, the algorithm outputs a function SUA :
U ? [0, 1], which represents the user?s final au-
thority score, and another function SUH : U ?
[0, 1], which outputs the user?s final hub score. Note
that, unlike the standard HITS, the authority/hub
scores are influenced by edge weights that reflect the
retweet behaviors of individual users. The idea here
is to dampen the influence of users who devote most
of their retweet activities toward a very few other
users, such as celebrities, and increase the weight
of users who retweet many different users? tweets.
To demonstrate the effectiveness of the parameter,
we have done some preliminary experiments. The
column Userfrom in Table 1 shows the retweet be-
havior of users who retweeted tweets belonging to
?uninteresting? and ?interesting? classes observed
in our Twitter dataset. The values are calculated
by the ratio of all other users that a user retweeted
to all retweet outlinks from the user; a value closer
to 1 means that outlinks are pointed to many dif-
ferent users.2 We observe that the value for users
who retweeted interesting tweets is shown to be
higher, which means that they tend to retweet mes-
sages from many different users, more than users
who retweeted uninteresting ones.
2For calculating the ratios, we limit the target to users who
retweeted two or more times in our dataset.
16
Class Userfrom F = ? #
Not Interesting 0.591 0.252 1985
Possibly Interesting 0.711 0.515 1115
Both 0.634 0.346 3100
Table 1: Dataset analysis.
Tweet-level procedure: After the user-level
stage, we start computing the scores of the tweet
nodes. In each iteration, we start out with each tweet
node initially inheriting the scores of its publisher.
Let P : T ? U be a function that returns the pub-
lisher of a given tweet. ?ti, we update A(ti) to be:
SUA(P (ti)) +
?
?j:etj ,ti?E
F (etj ,ti)?H(tj) (3)
Then, ?ti, we update H(ti) to be:
SUH (P (ti)) +
?
?j:eti,tj?E
F (eti,tj )?A(tj) (4)
where F (eta,tb) is a parameter function that returns
? > 1 if P (ta) is not a follower of P (tb) and 1 other-
wise. It is intuitive that if users retweet other users?
tweets even if they are not friends, then it is more
likely that those tweets are interesting. The column
F = ? in Table 1 shows the ratio of all unfollow-
ers who retweeted messages in a particular class to
all users who retweeted messages in that class, ob-
served in our dataset. We observe that users retweet
interesting messages more, even when they do not
follow the publishers. Similar observation has also
been made by Recuero et al (2011). After each it-
eration, the authority/hub scores are normalized as
done in the user-level. After performing several it-
erations until convergence, the algorithm finally out-
puts a scoring function STA : T ? [0, 1], which rep-
resents the tweet node?s final authority score. We use
this function to produce the final ranking of tweets.
Text pattern rules: We observe that in some
cases users retweet messages from their friends, not
because of the contents, but via retweet requests to
simply evoke attention. To prevent useless tweets
containing such requests from receiving high author-
ity scores, we collect 20 simple text pattern match-
ing rules that frequently appear in those tweets.
Specifically, we let the rules make influence while
updating the scores of tweets by modifying the sum-
mations in Eq. (3) and (4) respectively as:
?
?j:etj ,ti?E
F (etj ,ti)?R(ti)?H(tj) (5)
?
?j:eti,tj?E
F (eti,tj )?R(tj)?A(tj) (6)
where R(t) is a rule-based function that returns 0 if
tweet t contains one of the pre-defined text patterns
and 1 otherwise. Such patterns include ?RT this if?
and ?If this tweet gets RT * times I will?.
3 Experiment and Discussion
Our Twitter dataset is collected during 31 days of
October 2011, containing 64,107,169 tweets and
2,824,365 users. For evaluation, we generated 31
immediate Twitter graphs composed of 1.5 million
retweet links in average and 31 initially ranked lists
of tweets, each consisting of top 100 tweets created
on a specific date of the month with highest retweet
counts accumulated during the next 7 days. Two an-
notators were instructed to categorize each tweet as
interesting or not, by inspecting its content as done
in the work of Alonso et al (2010). In case of dis-
agreement (about 15% of all cases), a final judgment
was made by consensus between the two annotators.
We observe that the ratio of tweets judged to be in-
teresting is about 36%; the column ?#? in Table 1
shows the actual counts of each class. The goal of
this evaluation is to demonstrate that our method is
able to produce better ranked lists of tweets by re-
ranking interesting tweets highly.
Table 2 reports the ranking performance of vari-
ous methods in terms of Precisions @10 and @20,
R-Precision, and MAP. We compare our approach
to four baselines. The first baseline, #RT, is obvi-
ously based on retweet counts; tweets with higher
retweet counts are ranked higher. The second base-
line, #URL+#RT, favors tweets that contain URL
links (Alonso et al, 2010). Since it is less likely for
a tweet to contain more than one link, we addition-
ally use #RT to break ties in tweet ranking. Thirdly,
HITSoriginal, is the standard HITS algorithm run on
both user and tweet subgraphs that calculates author-
ity/hub scores of a node purely by the sum of hub
values that point to it and the sum of authority val-
ues that it points to, respectively, during iterations;
17
Method P@10 P@20 R-Prec MAP
#RT 0.294 0.313 0.311 0.355
#URL+#RT 0.245 0.334 0.362 0.361
HITSoriginal 0.203 0.387 0.478 0.465
MLmessage 0.671 0.645 0.610 0.642
MLall 0.819 0.795 0.698 0.763
HITSproposed 0.881 0.829 0.744 0.807
Table 2: Performance of individual methods
no other influential factors are considered in the cal-
culations. Lastly, we choose one recent work by
Castillo et al (2011) that addresses a related prob-
lem to ours, which aims at learning to classify tweets
as credible or not credible. Although interestingness
and credibility are two distinct concepts, the work
presents a wide range of features that may be ap-
plied for assessing interestingness of tweets using
machine learning. For re-implementation, we train
a binary SVM classifier using features proposed by
Castillo et al (2011), which include features from
users? tweet and retweet behavior, the text of the
tweets, and citations to external sources; we use
the probability estimates of the learned classifier for
re-ranking.3 We use leave-one-out cross validation
in order to evaluate this last approach, denoted as
MLall. MLmessage is a variant that relies only on
message-based features of tweets. Our method, with
? empirically set to 7, is denoted as HITSproposed.
We observe that #RT alone is not sufficient mea-
sure for discovering interesting tweets. Additionally
leveraging #URL helps, but the improvements are
only marginal. By manually inspecting tweets with
both high retweet counts and links, it is revealed
that many of them were tweets from celebrities with
links to their self-portraits photographed in their
daily lives, which may be of interest to their own
followers only. HITSoriginal performs better than
both #RT and #URL across most evaluation met-
rics but generally does not demonstrate good per-
formance. MLmessage always outperform the first
three significantly; we observe that tweet lengths
in characters and in words are the two most effec-
tive message-based features for finding interesting
tweets. The results of MLall demonstrates that more
3We do not use some topic-based features in (Castillo et al,
2011) since such information is not available in our case.
Method P@10 P@20 R-Prec MAP
HITSproposed 0.881 0.829 0.744 0.807
w/o User 0.677 0.677 0.559 0.591
w/o Tweet 0.861 0.779 0.702 0.772
w/o Rule 0.858 0.81 0.733 0.781
Table 3: Contributions of individual stages.
reasonable performance can be achieved when user-
and propagation-based features are combined with
message-based features. The proposed method sig-
nificantly outperforms all the baselines. This is a
significant result in that our method is an unsuper-
vised approach that relies on a few number of tweet
features and does not require complex training.
We lastly report the contribution of individual
procedures in our algorithm in Table 3 by ablat-
ing each of the stages at a time. ?w/o User? is
when tweet nodes do not initially inherit the scores
of their publishers. ?w/o Tweet? is when tweets
are re-ranked according to the authority scores of
their publishers. ?w/o Rule? is when we use Eq.
(3) and (4) instead of Eq. (5) and (6) for updating
tweet scores. We observe that the user-level proce-
dure plays the most crucial role. We believe this is
because of the ability of HITS to distinguish good
?hub-users?. Since authoritative users can post ordi-
nary status updates occasionally in Twitter, we can-
not always expect them to create interesting content
every time they tweet. However, good hub-users4
tend to continuously spot and retweet interesting
messages; thus, we can expect the tweets they share
to be interesting steadily. The role of hubs is not as
revealed on the tweet side of the Twitter graph, since
each tweet node can only have at most one retweet
outlink. The exclusion of text pattern rules does not
harm the overall performance much. We suspect this
is because of the small number of rules and expect
more improvement if we add more effective rules.
Acknowledgments
This work was supported by the Ministry of Knowl-
edge Economy of Korea, under the title ?Develop-
ment of social web issue detection-monitoring &
prediction technology for big data analytic listening
platform of web intelligence (10039158)?.
4Often referred to as content curators (Bhargava, 2009).
18
References
Omar Alonso, Chad Carson, David Gerster, Xiang Ji, and
Shubha U. Nabar. 2010. Detecting uninteresting con-
tent in text streams. In Proceedings of the SIGIR 2010
Workshop on Crowdsourcing for Search Evaluation,
CSE ?10, pages 39?42.
Rohit Bhargava. 2009. Manifesto for the content curator:
The next big social media job of the future? http:
//rohitbhargava.typepad.com/.
Danah Boyd, Scott Golder, and Gilad Lotan. 2010.
Tweet, tweet, retweet: Conversational aspects of
retweeting on twitter. In Proceedings of the 2010 43rd
Hawaii International Conference on System Sciences,
HICSS ?10, pages 1?10.
Carlos Castillo, Marcelo Mendoza, and Barbara Poblete.
2011. Information credibility on twitter. In Proceed-
ings of the 20th international conference on World
wide web, WWW ?11, pages 675?684.
Liangjie Hong, Ovidiu Dan, and Brian D. Davison. 2011.
Predicting popular messages in twitter. In Proceed-
ings of the 20th international conference companion
on World wide web, WWW ?11, pages 57?58.
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. J. ACM, 46(5):604?632.
Hady W. Lauw, Alexandros Ntoulas, and Krishnaram
Kenthapadi. 2010. Estimating the quality of postings
in the real-time web. In Proceedings of the WSDM
2010 Workshop on Search in Social Media, SSM ?10.
Raquel Recuero, Ricardo Araujo, and Gabriela Zago.
2011. How does social capital affect retweets? In Pro-
ceedings of the 5th International AAAI Conference on
Weblogs and Social Media, ICWSM ?11, pages 305?
312.
Daniel M. Romero, Wojciech Galuba, Sitaram Asur, and
Bernardo A. Huberman. 2011. Influence and passiv-
ity in social media. In Proceedings of the 2011 Euro-
pean Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases,
ECML-PKDD ?11, pages 18?33.
Michael J. Welch, Uri Schonfeld, Dan He, and Junghoo
Cho. 2011. Topical semantics of twitter links. In Pro-
ceedings of the 4th International Conference on Web
Search and Web Data Mining, WSDM ?11, pages 327?
336.
19
