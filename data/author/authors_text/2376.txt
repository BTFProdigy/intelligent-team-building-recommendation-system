The LinGO Redwoods Treebank
Motivation and Preliminary Applications
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten Brants
{oe |kristina |manning |dan}@csli.stanford.edu,
shieber@deas.harvard.edu, brants@parc.xerox.com
Abstract
The LinGO Redwoods initiative is a seed activity in the de-
sign and development of a new type of treebank. While sev-
eral medium- to large-scale treebanks exist for English (and
for other major languages), pre-existing publicly available re-
sources exhibit the following limitations: (i) annotation is
mono-stratal, either encoding topological (phrase structure) or
tectogrammatical (dependency) information, (ii) the depth of
linguistic information recorded is comparatively shallow, (iii)
the design and format of linguistic representation in the tree-
bank hard-wires a small, predefined range of ways in which
information can be extracted from the treebank, and (iv) rep-
resentations in existing treebanks are static and over the (often
year- or decade-long) evolution of a large-scale treebank tend
to fall behind the development of the field. LinGO Redwoods
aims at the development of a novel treebanking methodology,
rich in nature and dynamic both in the ways linguistic data can
be retrieved from the treebank in varying granularity and in the
constant evolution and regular updating of the treebank itself.
Since October 2001, the project is working to build the foun-
dations for this new type of treebank, to develop a basic set of
tools for treebank construction and maintenance, and to con-
struct an initial set of 10,000 annotated trees to be distributed
together with the tools under an open-source license.
1 Why Another (Type of) Treebank?
For the past decade or more, symbolic, linguistically ori-
ented methods and statistical or machine learning ap-
proaches to NLP have often been perceived as incompat-
ible or even competing paradigms. While shallow and
probabilistic processing techniques have produced use-
ful results in many classes of applications, they have not
met the full range of needs for NLP, particularly where
precise interpretation is important, or where the variety
of linguistic expression is large relative to the amount
of training data available. On the other hand, deep
approaches to NLP have only recently achieved broad
enough grammatical coverage and sufficient processing
efficiency to allow the use of precise linguistic grammars
in certain types of real-world applications.
In particular, applications of broad-coverage analyti-
cal grammars for parsing or generation require the use of
sophisticated statistical techniques for resolving ambigu-
ities; the transfer of Head-Driven Phrase Structure Gram-
mar (HPSG) systems into industry, for example, has am-
plified the need for general parse ranking, disambigua-
tion, and robust recovery techniques. We observe general
consensus on the necessity for bridging activities, com-
bining symbolic and stochastic approaches to NLP. But
although we find promising research in stochastic pars-
ing in a number of frameworks, there is a lack of appro-
priately rich and dynamic language corpora for HPSG.
Likewise, stochastic parsing has so far been focussed on
information-extraction-type applications and lacks any
depth of semantic interpretation. The Redwoods initia-
tive is designed to fill in this gap.
In the next section, we present some of the motivation
for the LinGO Redwoods project as a treebank develop-
ment process. Although construction of the treebank is
in its early stages, we present in Section 3 some prelim-
inary results of using the treebank data already acquired
on concrete applications. We show, for instance, that
even simple statistical models of parse ranking trained
on the Redwoods corpus built so far can disambiguate
parses with close to 80% accuracy.
2 A Rich and Dynamic Treebank
The Redwoods treebank is based on open-source HPSG
resources developed by a broad consortium of re-
search groups including researchers at Stanford (USA),
Saarbru?cken (Germany), Cambridge, Edinburgh, and
Sussex (UK), and Tokyo (Japan). Their wide distribution
and common acceptance make the HPSG framework and
resources an excellent anchor point for the Redwoods
treebanking initiative.
The key innovative aspect of the Redwoods ap-
proach to treebanking is the anchoring of all linguis-
tic data captured in the treebank to the HPSG frame-
work and a generally-available broad-coverage gram-
mar of English, the LinGO English Resource Grammar
(Flickinger, 2000) as implemented with the LKB gram-
mar development environment (Copestake, 2002). Un-
like existing treebanks, there is no need to define a (new)
form of grammatical representation specific to the tree-
bank. Instead, the treebank records complete syntacto-
semantic analyses as defined by the LinGO ERG and pro-
vide tools to extract different types of linguistic informa-
tion at varying granularity.
The treebanking environment, building on the [incr
tsdb()] profiling environment (Oepen & Callmeier,
2000), presents annotators, one sentence at a time, with
the full set of analyses produced by the grammar. Using
a pre-existing tree comparison tool in the LKB (similar
in kind to the SRI Cambridge TreeBanker; Carter, 1997),
annotators can quickly navigate through the parse for-
est and identify the correct or preferred analysis in the
current context (or, in rare cases, reject all analyses pro-
posed by the grammar). The tree selection tool presents
users, who need little expert knowledge of the underly-
ing grammar, with a range of basic properties that distin-
guish competing analyses and that are relatively easy to
judge. All disambiguating decisions made by annotators
are recorded in the [incr tsdb()] database and thus become
available for (i) later dynamic extraction from the anno-
tated profile or (ii) dynamic propagation into a more re-
cent profile obtained from re-running a newer version of
the grammar on the same corpus.
Important innovative research aspects in this approach
to treebanking are (i) enabling users of the treebank to
extract information of the type they need and to trans-
form the available representation into a form suited to
their needs and (ii) the ability to update the treebank with
an enhanced version of the grammar in an automated
fashion, viz. by re-applying the disambiguating decisions
on the corpus with an updated version of the grammar.
Depth of Representation and Transformation of In-
formation Internally, the [incr tsdb()] database records
analyses in three different formats, viz. (i) as a deriva-
tion tree composed of identifiers of lexical items and con-
structions used to build the analysis, (ii) as a traditional
phrase structure tree labeled with an inventory of some
fifty atomic labels (of the type ?S?, ?NP?, ?VP? et al), and
(iii) as an underspecified MRS (Copestake, Lascarides,
& Flickinger, 2001) meaning representation. While rep-
resentation (ii) will in many cases be similar to the rep-
resentation found in the Penn Treebank, representation
(iii) subsumes the functor ? argument (or tectogrammati-
cal) structure advocated in the Prague Dependency Tree-
bank or the German TiGer corpus. Most importantly,
however, representation (i) provides all the information
required to replay the full HPSG analysis (using the orig-
inal grammar and one of the open-source HPSG process-
ing environments, e.g., the LKB or PET, which already
have been interfaced to [incr tsdb()]). Using the latter ap-
proach, users of the treebank are enabled to extract infor-
mation in whatever representation they require, simply
by reconstructing full analyses and adapting the exist-
ing mappings (e.g., the inventory of node labels used for
phrase structure trees) to their needs. Likewise, the ex-
isting [incr tsdb()] facilities for comparing across compe-
tence and performance profiles can be deployed to evalu-
ate results of a (stochastic) parse disambiguation system,
essentially using the preferences recorded in the treebank
as a ?gold standard? target for comparison.
Automating Treebank Construction Although a pre-
cise HPSG grammar like the LinGO ERG will typically
assign a small number of analyses to a given sentence,
choosing among a few or sometimes a few dozen read-
ings is time-consuming and error-prone. The project is
exploring two approaches to automating the disambigua-
tion task, (i) seeding lexical selection from a part-of-
speech (POS) tagger and (ii) automated inter-annotator
comparison and assisted resolution of conflicts.
Treebank Maintenance and Evolution One of the
challenging research aspects of the Redwoods initiative
is about developing a methodology for automated up-
dates of the treebank to reflect the continuous evolution
of the underlying linguistic framework and of the LinGO
grammar. Again building on the notion of elementary
linguistic discriminators, we expect to explore the semi-
automatic propagation of recorded disambiguating deci-
sions into newer versions of the parsed corpus. While
it can be assumed that the basic phrase structure inven-
tory and granularity of lexical distinctions have stabilized
to a certain degree, it is not guaranteed that one set of
discriminators will always fully disambiguate a more re-
cent set of analyses for the same utterance (as the gram-
mar may introduce new ambiguity), nor that re-playing
a history of disambiguating decisions will necessarily
identify the correct, preferred analysis for all sentences.
A better understanding of the nature of discriminators
and relations holding among them is expected to provide
the foundations for an update procedure that, ultimately,
should be mostly automated, with minimal manual in-
spection, and which can become part of the regular re-
gression test cycle for the grammar.
Scope and Current State of Seeding Initiative The
first 10,000 trees to be hand-annotated as part of the
kick-off initiative are taken from a domain for which the
English Resource Grammar is known to exhibit broad
and accurate coverage, viz. transcribed face-to-face dia-
logues in an appointment scheduling and travel arrange-
ment domain.1 For the follow-up phase of the project, it
is expected to move into a second domain and text genre,
presumably more formal, edited text taken from newspa-
per text or another widely available on-line source. As
of June 2002, the seeding initiative is well underway.
The integrated treebanking environment, combining [incr
tsdb()] and the LKB tree selection tool, has been estab-
lished and has been deployed in a first iteration of anno-
tating the VerbMobil utterances. The approach to parse
selection through minimal discriminators turned out to
be not hard to learn for a second-year Stanford under-
graduate in linguistics, and allowed completion of the
first iteration in less than ten weeks. Table 1 summarizes
the current Redwoods status.
1Corpora of some 50,000 such utterances are readily available from
the VerbMobil project (Wahlster, 2000) and have already been studied
extensively among researchers world-wide.
2Of the four data sets only VM32 has been double-checked by
an expert grammarian and (almost) completely disambiguated to date;
therefore it exhibits an interestingly higher degree of phrasal ambiguity
in the ?active = 1? subset.
total active = 0 active = 1 active > 1 unannotated
corpus ] ?  ? ] ?  ? ] ?  ? ] ?  ? ] ?  ?
VM6 2422 7?7 4?2 32?9 218 8?0 4?4 9?7 1910 7?0 4?0 7?5 80 10?0 4?8 23?8 214 14?9 4?3 287?5
VM13 1984 8?5 4?0 37?9 175 8?5 4?1 9?9 1491 7?2 3?9 7?5 85 9?9 4?5 22?1 233 14?1 4?2 22?1
VM31 1726 6?2 4?5 22?4 164 7?9 4?6 8?0 1360 6?6 4?5 5?9 61 10?1 4?2 14?5 141 13?5 4?7 201?5
VM32 608 7?4 4?3 25?6 51 10?7 4?3 54?4 551 7?9 4?4 19?0 5 12?2 3?9 27?2 1 21?0 6?1 2220?0
Table 1: Redwoods development status as of June 2002: four sets of transcribed and hand-segmented VerbMobil dialogues have
been annotated. The columns are, from left to right, the total number of sentences (excluding fragments) for which the LinGO
grammar has at least one analysis (?]?), average length (???), lexical and structural ambiguity (?? and ???, respectively), followed
by the last four metrics broken down for the following subsets: sentences (i) for which the annotator rejected all analyses (no active
trees), (ii) where annotation resulted in exactly one preferred analysis (one active tree), (iii) those where full disambiguation was
not accomplished through the first round of annotation (more than one active tree), and (iv) massively ambiguous sentences that
have yet to be annotated.2
3 Early Experimental Results
Development of the treebank has just started. Nonethe-
less, we have performed some preliminary experiments
on concrete applications to motivate the utility of the re-
source being developed. In this section, we describe ex-
periments using the Redwoods treebank to build and test
systems for parse disambiguation. As a component, we
build a tagger for the HPSG lexical tags in the treebank,
and report results on this application as well.
Any linguistic system that allows multiple parses
of strings must address the problem of selecting from
among the admitted parses the preferred one. A variety
of approaches for building statistical models of parse se-
lection are possible. At the simplest end, we might look
only at the lexical type sequence assigned to the words
by each parse and rank the parse based on the likelihood
of that sequence. These lexical types ? the preterminals
in the derivation ? are essentially part-of-speech tags, but
encode considerably finer-grained information about the
words. Well-understood statistical part-of-speech tag-
ging technology is sufficient for this approach.
In order to use more information about the parse,
we might examine the entire derivation of the string.
Most probabilistic parsing research ? including, for ex-
ample, work by by Collins (1997), and Charniak (1997)
? is based on branching process models (Harris, 1963).
The HPSG derivations that the treebank makes available
can be viewed as just such a branching process, and
a stochastic model of the trees can be built as a prob-
abilistic context-free grammar (PCFG) model. Abney
(1997) notes important problems with the soundness of
the approach when a unification-based grammar is ac-
tually determining the derivations, motivating the use
of log-linear models (Agresti, 1990) for parse ranking
that Johnson and colleagues further developed (Johnson,
Geman, Canon, Chi, & Riezler, 1999). These models
can deal with the many interacting dependencies and
the structural complexity found in constraint-based or
unification-based theories of syntax.
Nevertheless, the naive PCFG approach has the advan-
tage of simplicity, so we pursue it and the tagging ap-
proach to parse ranking in these proof-of-concept exper-
iments (more recently, we have begun work on building
log-linear models over HPSG signs (Toutanova & Man-
ning, 2002)). The learned models were used to rank
possible parses of unseen test sentences according to the
probabilities they assign to them. We report parse se-
lection performance as percentage of test sentences for
which the correct parse was highest ranked by the model.
(We restrict attention in the test corpus to sentences that
are ambiguous according to the grammar, that is, for
which the parse selection task is nontrivial.) We examine
four models: an HMM tagging model, a simple PCFG, a
PCFG with grandparent annotation, and a hybrid model
that combines predictions from the PCFG and the tagger.
These models will be described in more detail presently.
The tagger that we have implemented is a standard tri-
gram HMM tagger, defining a joint probability distribu-
tion over the preterminal sequences and yields of these
trees. Trigram probabilities are smoothed by linear in-
terpolation with lower-order models. For comparison,
we present the performance of a unigram tagger and an
upper-bound oracle tagger that knows the true tag se-
quence and scores highest the parses that have the correct
preterminal sequence.
The PCFG models define probability distributions
over the trees of derivational types corresponding to the
HPSG analyses of sentences. A PCFG model has parame-
ters ?i, j for each rule Ai ? ? j in the corresponding con-
text free grammar.3 In our application, the nonterminals
in the PCFG Ai are rules of the HPSG grammar used to
build the parses (such as HEAD-COMPL or HEAD-ADJ).
We set the parameters to maximize the likelihood of the
set of derivation trees for the preferred parses of the sen-
tences in a training set. As noted above, estimating prob-
abilities from local tree counts in the treebank does not
provide a maximum likelihood estimate of the observed
data, as the grammar rules further constrain the possible
derivations. Essentially, we are making an assumption of
context-freeness of rule application that does not hold in
the case of the HPSG grammar. Nonetheless, we can still
build the model and use it to rank parses.
3For an introduction to PCFG grammars see, for example, Manning
& Schu?tze (1999).
As previously noted by other researchers (Charniak &
Caroll, 1994), extending a PCFG with grandparent an-
notation improves the accuracy of the model. We imple-
mented an extended PCFG that conditions each node?s
expansion on its parent in the phrase structure tree. The
extended PCFG (henceforth PCFG-GP) has parameters
P(Ak Ai ? ? j |Ak, Ai) . The resulting grammar can be
viewed as a PCFG whose nonterminals are pairs of the
nonterminals of the original PCFG.
The combined model scores possible parses using
probabilities from the PCFG-GP model together with the
probability of the preterminal sequence of the parse tree
according to a trigram tag sequence model. More specif-
ically, for a tree T ,
Score(t) = log(PPCFG-GP(T )) + ? log(PTRIG(tags(T ))
where PTRIG(tags(T )) is the probability of the sequence
of preterminals t1 ? ? ? tn in T according to a trigram tag
model:
PTRIG(t1 ? ? ? tn) =
?n
i=1
P(ti |ti?1, ti?2)
with appropriate treatment of boundaries. The trigram
probabilities are smoothed as for the HMM tagger. The
combined model is relatively insensitive to the relative
weights of the two component models, as specified by ?;
in any case, exact optimization of this parameter was not
performed. We refer to this model as Combined. The
Combined model is not a sound probabilistic model as it
does not define a probability distribution over parse trees.
It does however provide a crude way to combine ancestor
and left context information.
The second column in Table 2 shows the accuracy
of parse selection using the models described above.
For comparison, a baseline showing the expected perfor-
mance of choosing parses randomly according to a uni-
form distribution is included as the first row. The accu-
racy results are averaged over a ten-fold cross-validation
on the data set summarized in Table 1. The data we used
for this experiment was the set of disambiguated sen-
tences that have exactly one preferred parse (comprising
a total of 5312 sentences). Often the stochastic models
we are considering give the same score to several differ-
ent parses. When a model ranks a set of m parses highest
with equal scores and one of those parses is the preferred
parse in the treebank, we compute the accuracy on this
sentence as 1/m.
Since our approach of defining the probability of anal-
yses using derivation trees is different from the tradi-
tional approach of learning PCFG grammars from phrase
structure trees, a comparison of the two is probably in
order. We tested the model PCFG-GP defined over the
corresponding phrase structure trees and its average ac-
curacy was 65.65% which is much lower than the accu-
racy of the same model over derivation trees (71.73%).
This result suggests that the information about grammar
constructions is very helpful for parse disambiguation.
Method Task
tag sel. parse sel.
Random 90.13% 25.81%
Tagger unigram 96.75% 44.15%
trigram 97.87% 47.74%
oracle 100.00% 54.59%
PCFG simple 97.40% 66.26%
grandparent 97.43% 71.73%
combined 98.08% 74.03%
Table 2: Performance of the HMM and PCFG models for the
tag and parse selection tasks (accuracy).
The results in Table 2 indicate that high disambigua-
tion accuracy can be achieved using very simple statisti-
cal models. The performance of the perfect tagger shows
that, informally speaking, roughly half of the information
necessary to disambiguate parses is available in the lexi-
cal types alone. About half of the remaining information
is recovered by our best method, Combined.
An alternative (more primitive) task is the tagging task
itself. It is interesting to know how much the tagging
task can be improved by perfecting parse disambigua-
tion. With the availability of a parser, we can examine the
accuracy of the tag sequence of the highest scoring parse,
rather than trying to tag the word sequence directly. We
refer to this problem as the tag selection problem, by
analogy with the relation between the parsing problem
and the parse selection problem. The first column of Ta-
ble 2 presents the performance of the models on the tag
selection problem. The results are averaged accuracies
over 10 cross-validation splits of the same corpus as the
previous experiment, and show that parse disambigua-
tion using information beyond the lexical type sequence
slightly improves tag selection performance. Note that
in these experiments, the models are used to rank the tag
sequences of the possible parses and not to find the most
probable tag sequence. Therefore tagging accuracy re-
sults are higher than they would be in the latter case.
Since our corpus has relatively short sentences and low
ambiguity it is interesting to see how much the perfor-
mance degrades as we move to longer and more highly
ambiguous sentences. For this purpose, we report in Ta-
ble 3 the parse ranking accuracy of the Combined model
as a function of the number of possible analyses for sen-
tences. Each row corresponds to a set of sentences with
number of possible analyses greater or equal to the bound
shown in the first column. For example, the first row con-
tains information for the sentences with ambiguity ? 2,
which is all ambiguous sentences. The columns show the
total number of sentences in the set, the expected accu-
racy of guessing at random, and the accuracy of the Com-
bined model. We can see that the parse ranking accuracy
is decreasing quickly and more powerful models will be
needed to achieve good accuracy for highly ambiguous
sentences.
Despite several differences in corpus size and compo-
Analyses Sentences Random Combined
? 2 3824 25.81% 74.03%
? 5 1789 9.66% 59.64%
? 10 1027 5.33% 51.61%
? 20 525 3.03% 45.33%
Table 3: Parse ranking accuracy by number of possible parses.
sition, it is perhaps nevertheless useful to compare this
work with other work on parse selection for unification-
based grammars. Johnson et al (1999) estimate a
Stochastic Unification Based Grammar (SUBG) using a
log-linear model. The features they include in the model
are not limited to production rule features but also ad-
junct and argument and other linguistically motivated
features. On a dataset of 540 sentences (total training
and test set) from a Verbmobil corpus they report parse
disambiguation accuracy of 58.7% given a baseline accu-
racy for choosing at random of 9.7%. The random base-
line is much lower than ours for the full data set, but it is
comparable for the random baseline for sentences with
more than 5 analyses. The accuracy of our Combined
model for these sentences is 59.64%, so the accuracies
of the two models seem fairly similar.
4 Related Work
To the best of our knowledge, no prior research has
been conducted exploring the linguistic depth, flexibil-
ity in available information, and dynamic nature of tree-
banks that we have proposed. Earlier work on building
corpora of hand-selected analyses relative to an exist-
ing broad-coverage grammar was carried out at Xerox
PARC, SRI Cambridge, and Microsoft Research. As all
these resources are tuned to proprietary grammars and
analysis engines, the resulting treebanks are not publicly
available, nor have reported research results been repro-
ducible. Yet, especially in light of the successful LinGO
open-source repository, it seems vital that both the tree-
bank and associated processing schemes and stochastic
models be available to the general (academic) public. An
on-going initiative at Rijksuniversiteit Groningen (NL) is
developing a treebank of dependency structures (Mullen,
Malouf, & Noord, 2001), derived from an HPSG-like
grammar of Dutch (Bouma, Noord, & Malouf, 2001).
The general approach resembles the Redwoods initiative
(specifically the discriminator-based method of tree se-
lection; the LKB tree comparison tool was originally de-
veloped by Malouf, after all), but it provides only a sin-
gle stratum of representation, and has no provision for
evolving analyses in tandem with the grammar. Dipper
(2000) presents the application of a broad-coverage LFG
grammar for German to constructing tectogrammatical
structures for the TiGer corpus. The approach is similar
to the Groningen framework, and shares its limitations.
References
Abney, S. P. (1997). Stochastic attribute-value grammars.
Computational Linguistics, 23, 597 ? 618.
Agresti, A. (1990). Categorical data analysis. John Wiley &
Sons.
Bouma, G., Noord, G. van, & Malouf, R. (2001).
Alpino. Wide-coverage computational analysis of Dutch. In
W. Daelemans, K. Sima-an, J. Veenstra, & J. Zavrel (Eds.),
Computational linguistics in the Netherlands (pp. 45 ? 59).
Amsterdam, The Netherlands: Rodopi.
Carter, D. (1997). The TreeBanker. A tool for supervised
training of parsed corpora. In Proceedings of the Workshop
on Computational Environments for Grammar Development
and Linguistic Engineering. Madrid, Spain.
Charniak, E. (1997). Statistical parsing with a context-free
grammar and word statistics. In Proceedings of the Four-
teenth National Conference on Artificial Intelligence (pp.
598 ? 603). Providence, RI.
Charniak, E., & Caroll, G. (1994). Context-sensitive statistics
for improved grammatical language models. In Proceedings
of the Twelth National Conference on Artificial Intelligence
(pp. 742 ? 747). Seattle, WA.
Collins, M. J. (1997). Three generative, lexicalised models for
statistical parsing. In Proceedings of the 35th Meeting of
the Association for Computational Linguistics and the 7th
Conference of the European Chapter of the ACL (pp. 16 ?
23). Madrid, Spain.
Copestake, A. (2002). Implementing typed feature structure
grammars. Stanford, CA: CSLI Publications.
Copestake, A., Lascarides, A., & Flickinger, D. (2001). An
algebra for semantic construction in constraint-based gram-
mars. In Proceedings of the 39th Meeting of the Association
for Computational Linguistics. Toulouse, France.
Dipper, S. (2000). Grammar-based corpus annotation. In
Workshop on linguistically interpreted corpora LINC-2000
(pp. 56 ? 64). Luxembourg.
Flickinger, D. (2000). On building a more efficient grammar
by exploiting types. Natural Language Engineering, 6 (1)
(Special Issue on Efficient Processing with HPSG), 15 ? 28.
Harris, T. E. (1963). The theory of branching processes.
Berlin, Germany: Springer.
Johnson, M., Geman, S., Canon, S., Chi, Z., & Riezler, S.
(1999). Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Meeting of the Associa-
tion for Computational Linguistics (pp. 535 ? 541). College
Park, MD.
Manning, C. D., & Schu?tze, H. (1999). Foundations of statis-
tical Natural Language Processing. Cambridge, MA: MIT
Press.
Mullen, T., Malouf, R., & Noord, G. van. (2001). Statistical
parsing of Dutch using Maximum Entropy models with fea-
ture merging. In Proceedings of the Natural Language Pro-
cessing Pacific Rim Symposium. Tokyo, Japan.
Oepen, S., & Callmeier, U. (2000). Measure for mea-
sure: Parser cross-fertilization. Towards increased compo-
nent comparability and exchange. In Proceedings of the 6th
International Workshop on Parsing Technologies (pp. 183 ?
194). Trento, Italy.
Toutanova, K., & Manning, C. D. (2002). Feature selection
for a rich HPSG grammar using decision trees. In Proceed-
ings of the sixth conference on natural language learning
(CoNLL-2002). Taipei.
Wahlster, W. (Ed.). (2000). Verbmobil. Foundations of speech-
to-speech translation. Berlin, Germany: Springer.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 153?160
Manchester, August 2008
Hybrid processing for grammar and style checking
Berthold Crysmann?, Nuria Bertomeu?, Peter Adolphs?, Dan Flickinger?, Tina Klu?wer??
? Universita?t Bonn, Poppeldorfer Allee 47, D-53115 Bonn, {bcr,tkl}@ifk.uni-bonn.de
? Zentrum fu?r Allgemeine Sprachwissenschaft, Berlin, nuria.bertomeu@dfki.de
? DFKI GmbH, Berlin, {peter.adolphs,kluewer}@dfki.de
? CSLI, Stanford University, danf@csli.stanford.edu
Abstract
This paper presents an implemented hy-
brid approach to grammar and style
checking, combining an industrial pattern-
based grammar and style checker with bi-
directional, large-scale HPSG grammars
for German and English. Under this ap-
proach, deep processing is applied selec-
tively based on the error hypotheses of a
shallow system. We have conducted a com-
parative evaluation of the two components,
supporting an integration scenario where
the shallow system is best used for error de-
tection, whereas the HPSG grammars add
error correction for both grammar and con-
trolled language style errors.
1 Introduction
With the enormous amount of multilingual techni-
cal documentation produced by companies nowa-
days grammar and controlled language checking
(henceforth: style checking) is becoming an appli-
cation highly in demand. It is not only a helpful
tool for authors, but also facilitates the translation
of documents into foreign languages. Through the
use of controlled language by the authors, docu-
ments can be automatically translated more suc-
cessfully than with the use of free language. Style
checking should make authors aware of the con-
structions which should not be used, as well as
aiding in reformulating them. This can save a lot
of translation costs for companies producing large
amounts of mulitilingual documentation. Another
application of grammar and style checking is the
development of tutorial systems for learning a for-
eign language, as well as any kind of authoring sys-
tem for non-native speakers.
Previous approaches to grammar and style
checking can be divided into those based on fi-
nite state methods and those based on linguisti-
cally motivated grammars. To the former group be-
long e.g. the systems FLAG (Bredenkamp et al,
2000a; Bredenkamp et al, 2000b) and MultiLint
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
(Haller, 1996; Schmidt-Wigger, 1998). The basic
approach taken by such systems is the description
of error patterns through finite state automata. The
automata access the textual input enriched with
annotations from shallow linguistic analysis com-
ponents, such as part-of-speech tagging, morphol-
ogy and chunking. In FLAG, for instance, the an-
notation delivered by the shallow components is
integrated into a complex feature structure. Rules
are defined as finite state automata over feature
structures. The great advantages of such systems
are their robustness and efficient processing, which
make them highly suitable for real-life grammar
and style checking applications. However, since
shallow modules usually cannot provide a full syn-
tactic analysis, the coverage of these systems is
limited to error types not requiring a broader (non-
local) syntactic context for their detection. There-
fore their precision in the recognition of non-local
errors is not satisfactory.
Another short-coming of most shallow ap-
proaches to grammar checking is that they typi-
cally do not provide error correction: owing to the
absence of an integrated target grammar, genera-
tion of repairs cannot take the syntactic context
into account: as a result, some of the repairs sug-
gested by shallow systems are not globally well-
formed.
Grammar-based error checking constitutes the
other main strand in language checking technol-
ogy. These systems are typically equipped with a
model of target well-formedness. The main prob-
lem, when applied to the task of error checking
is that the sentences that are the focus of a gram-
mar checker are ideally outside the scope of the
grammar. To address this problem, grammar-based
checkers typically employ robustness techniques
(Ravin, 1988; Jensen et al, 1993; Douglas, 1995;
Menzel, 1998; Heinecke et al, 1998). The addi-
tion of robustness features, while inevitable for a
grammar-based approach, has the disadvantage of
considerably slowing down runtime performance.
Another issue with purely grammar-based check-
ing is related to the scarce distribution of actual
errors: thus, most effort is spent on the processing
of perfectly impeccable utterances. Finally, since
coverage of real-world grammars is never perfect,
these system also have difficulty to distinguish be-
153
tween extragrammatical and truly ungrammatical
sentences. Conversely, since grammars often over-
generate, a successful parse does not guarantee
wellformedness either.
One of the two major robustness techniques
used in the context of grammar-based language
checking are constraint relaxation (see e.g. (Dou-
glas, 1995; Menzel, 1998; Heinecke et al, 1998)),
which is typically realised by means of modifica-
tions to the parser (e.g. relaxation levels, robust uni-
fication). An alternative approach is error anticipa-
tion where errors are explicitly modelled by means
of grammar rules, so-called MAL-rules (McCoy et
al., 1996). This approach has already been inves-
tigated with an HPSG grammar, the ERG (Copes-
take and Flickinger, 2000), in the scenario of a tu-
torial system for language learning by (Bender et
al., 2004). We will follow this approach in the part
of our hybrid system based on deep processing.
Finite state methods and linguistically motivated
grammars are not only compatible, but also com-
plementary. Shallow methods are robust and effi-
cient, while deep processing based on grammars
provides high precision and detail. With the fo-
cussed application of deep analysis in finite state
based grammar and style checking systems, both
coverage and precision can be improved, while
the performance remains acceptable for real-world
applications. The combination of shallow and
deep components, hybrid processing, has already
been investigated in several modular architectures,
such as GATE (Gaizauskas et al, 1996), White-
board (Crysmann et al, 2002) and Heart-of-Gold
(Callmeier et al, 2004). Moreover, the improve-
ment in efficiency and robustness in deep process-
ing together with methods for its efficient applica-
tion makes the employment of deep processing in
real-world applications quite feasible. Hybrid pro-
cessing has been used for applications such as in-
formation extraction and question answering. But
to the best of our knowledge, the application of hy-
brid processing to grammar and style checking has
not been previously investigated.
In this paper, we present an implemented proto-
type of a hybrid grammar and style checking sys-
tem for German and English, called Checkpoint.
As the baseline shallow system we have taken
an industrial strength grammar and controlled lan-
guage style checker, which is based on the FLAG
technology. The deep processing platform used in
the project is the PET parser (Callmeier, 2000)
operating on wide-coverage English and German
HPSG grammars, the English Resource Grammar
(ERG) (Copestake and Flickinger, 2000) and the
German Grammar (GG) (Mu?ller and Kasper, 2000;
Crysmann, 2005; Crysmann, 2007), respectively.
The ERG and the GG have been developed for over
15 years and have already been used as deep pro-
cessing engines in the Heart-of-Gold hybrid pro-
cessing platform. We have developed an approach
for the selective application of deep processing
based on the error hypotheses of the shallow sys-
tem. Error detection in the deep system follows a
MAL-rule approach. In order to compare the ben-
efits of the selective application of deep process-
ing with its nonselective application, we have de-
veloped two scenarios: one parallel and one inte-
grated. While the parallel (nonselective) scenario
enables improvement in both recall and precision,
the integrated (selective) scenario only enables im-
provement in precision. However, the performance
of the integrated approach is much better. We have
also investigated several possibilities of integrating
deep processing in the selective scenario. Since the
HPSG grammars are suitable both for parsing and
generation, the system can successfully provide
both error corrections and paraphrases of stylistic
errors. For the purpose of investigation, evaluation
and statistical parse ranking, we have collected and
annotated several corpora of texts from technical
manuals. Finally, the approach has been evaluated
regarding error detection and performance.
2 The approach
Checkpoint has two main goals: (a) improving the
precision and recall of existing pattern-based gram-
mar and style checking systems for error types
whose detection requires considering more than
the strictly local syntactic context; and (b) gener-
ating error corrections for both grammar and style
errors. Accordingly, we have chosen to focus on
certain error types based on the difficulties of the
pattern-based system.
2.1 Anticipation of grammar errors
Grammar errors are detected by means of error
anticipation rules, or MAL-rules. MAL-rules ex-
actly model errors, so that erroneous sentences can
be parsed by the grammar. For this purpose we
enlarged two HPSG grammars for German, the
GG, and English, the ERG, with MAL-rules for
error types that were problematic for the pattern-
based shallow system. For German the following
phenomena have been handled: subject verb agree-
ment (subject verb agreement), NP internal agree-
ment (NP internal agreement), confusion of the
complementiser ?dass? with the homophonous pro-
noun or determiner ?das? (dass das), as well as
editing errors, such as local and non local repeti-
tion of words (repetitions). Here follow some ex-
amples (taken from the FLAG error corpus (Becker
et al, 2000), and die tageszeitung ?taz?, a German
newspaper):
(1) Auch in AOL gibt es Newsgroups, die
dieses Thema diskutiert [=diskutieren]. (FLAG)
Also in AOL are there newgroups, which (Pl)
this topic discuss (Sg).
?There are also newsgroups in AOL which dis-
cuss this topic.?
154
(2) Ich habe dem ganze [=ganzen] Geschehen
von meinem Sofa aus zugesehen. (FLAG)
I have the whole (wrong adj. form) events
from my couch out watched.
?I have watched the whole events from my
couch.?
(3) Vor allem im Su?den . . . fu?hrten [=haben] die
Liberalen der MR einen heftigen Wahlkampf
gegen die PS gefu?hrt. (taz, June 2007)
Above all in the south . . . led (past tense) the
liberals of the MR a hard election campaign
against the PS led (past participle).
?Particularly in the south, the liberals of the
MR led a hard election campaign against the
PS.?
For English, MAL-rules for errors concerning
subject verb agreement and missing determiners
were implemented.
2.2 Detection of stylistic errors
Stylistic errors are grammatical constructions that
are dispreferred in a particular register or type
of document. Sometimes certain constructions are
not desirable because machine translation systems
have problems dealing with them or because they
prevent easy understanding. In such cases a con-
trolled language approach is taken, where the prob-
lematic constructions are paraphrased into equiv-
alent less problematic constructions. Since these
constructions are grammatical they can be parsed
and, thus, detected. A generation of a paraphrase
is possible based on the semantic representation
obtained through parsing. For German the follow-
ing phenomena were handled: passive, future and
implicit conditional sentences, as in the following
example:
(4) Wartet man zulange, kriegt man keine Karten.
Waits one too long, gets one no tickets.
?If one waits too long one gets no tickets.?
Correct: Wenn man zulange wartet, kriegt
man keine Karten.
For English we focussed on the following phenom-
ena: passive (avoid passive), future (avoid future),
modal verbs (avoid modal verbs), subjunctive
(avoid subjunctive), stand-alone deictic pro-
nouns (use this that these those with noun) and
clause order in conditional sentences (condi-
tion must precede action).
2.3 Integrated vs. parallel scenarios
We have developed two integration scenarios: an
integrated one and a parallel one. In the parallel
scenario the pattern-based shallow system and the
deep processing parser run independently of each
other, that is, all sentences are parsed independent
of whether the shallow system has found an error
in them. In the integrated scenario the deep parser
is only called for those sentences where the shal-
low system has detected some error of the type
of those which Checkpoint is able to process (enu-
merated in subsection 2.1). The parallel scenario
allows improvement in the recall of the shallow
system, since Checkpoint can find errors that the
shallow system has not found. In the integrated
scenario, on the contrary, only the precision of the
shallow system can be improved, since Checkpoint
departs from the hypotheses of the shallow system.
The integrated scenario, however, promises to per-
form better in time than the parallel scenario, since
only a fraction of the whole text has to be scanned
for errors. Moreover, the performance of the inte-
grated system can also be improved with the se-
lective activation of the MAL-rules that model the
specific errors found by the shallow system. This
greatly reduces the enormous search space of the
parsing algorithms and the processing time result-
ing from the simultaneous processing of several
MAL-rules.
The integration of the shallow system and the
deep parser has been achieved through an exten-
sion of the PET parser that allows it to receive any
kind of input information and integrate this into
the chart. This preprocessing information can be,
for example, part-of-speech tagging, morphology
and lemmatisation, and already guides the parsing
process. It allows, for instance, recognition of un-
known words or identification of the correct lexi-
cal entry in cases where there is ambiguity. An in-
put format in terms of feature structures, the ?Fea-
ture Structure Chart? (FSC) format, has been devel-
oped for this purpose (Adolphs et al, 2008). The
shallow system, thus, produces a feature structure
chart, based on the information delivered by the
various shallow modules, and this information is
given as input to the PET deep parser, which reads
it and integrates it into the chart.
Error hypotheses from the shallow system are
passed to the deep parser by means of specific fea-
tures in the input feature structure (MAL-features)
of every input token in the FSC, permitting selec-
tive activation of MAL-rules. To this end, the origi-
nal FSC generated by the shallow system, which
contains information on the part-of-speech, the
lemma and morphological features such as num-
ber, gender and case, will be extended with MAL-
features. These MAL-features correspond to the
class of some MAL-rule in the grammar and have
boolean values. Signs in the grammar are speci-
fied for these MAL-features. MAL-rules are de-
fined such that they can only take as their daughters
edges with a positive value for the corresponding
MAL-feature. All information in the FSC input to-
kens is passed to the tokens in the chart through
a feature called TOKEN in lexical items. Thus, er-
ror hypotheses are passed from the input tokens to
the lexical items in the chart by stating that the val-
ues of the MAL-features in the lexical items are
155
equal to the values of the MAL-features in the cor-
responding input tokens in the FSC.
The values of the MAL-features are obtained
by checking the error report delivered by the shal-
low system. For certain errors detected by the shal-
low system there is a mapping to MAL-features.
The value of a MAL-feature will be set to ?+? if
the shallow system has found the corresponding
error. The rest of the MAL-features can be set to
?bool? if we want to allow other MAL-rules to
fire (which can improve recall, but increases am-
biguity and, consequently, has a negative effect on
performance). The values of the rest of the MAL-
features can also be set to ?-?, if we want to prevent
other MAL-rules from firing (which allows im-
provement only in precision, but limits ambiguity
and, consequently, results in better performance).
There is also the possibility of activating the rel-
evant MAL-features only for those tokens which
are, according to the shallow system, within the er-
ror span, instead of activating the MAL-features
for all tokens in the erroneous sentence.
2.4 Generation of corrections and
paraphrases
One of the advantages of using deep processing
in grammar and style checking is the possibility
of generating corrections and paraphrases which
obey the constraints imposed by the syntactic con-
text. Since the HPSG grammars that we are using
are suitable both for parsing and generation, this
is straightforward. Robust parsing delivers as out-
put a semantic representation in the Minimal Re-
cursion Semantics formalism (MRS) (Copestake
et al, 2006) of the sentence which can be used for
generation with the LKB (Carroll et al, 1999).
The MAL-rules directly assign well-formed se-
mantic representations from which a correct sur-
face string can be generated. In the case of stylis-
tic errors, transfer rules are used to generate the
desired paraphrase, using MRS-to-MRS mapping
rules modelled on the semantic transfer-based ma-
chine translation approach of (L?nning et al,
2004).
We identified two areas where generation of re-
pairs will actually provide a considerable added
value to a grammar checking system: first, for non-
native speakers, simple highlighting of the error
location is often insufficient, since the user may
not be familiar with the rules of the language. Sec-
ond, some areas, in particular stylistic ones may
involve considerable rearrangement of the entire
sentence. In these cases, generation of repairs and
paraphrases can reduce editing cost and also min-
imise the issue of editing errors associated with
non-local phenomena.
The generator and HPSG grammars we use are
able to provide a range of realisations for a given
semantic input. As a result, realisation ranking is
of utmost importance. In order to select repairs
which are both smooth and maximally faithful to
the input, modulo the error site, of course, we com-
bined two methods: a discriminative PCFG-model
trained on a generation treebank, enhanced by an
n-gram language model, cf. (Velldal and Oepen,
2005), and an alignment approach that chooses the
most conservative edit from a set of input realisa-
tions. As our similarity measure, we employed a
variant of BLEU score (NEVA), suggested in (Fors-
bom, 2003). The probabilistic ranking models we
trained achieve an exact match accuracy of 73%
for both English (Velldal and Oepen, 2005) and
German (as evaluated on the subset of TiGer the
error corpus was based on).
3 Error corpora
In order to learn more about the frequencies of the
different error types, to induce statistical models
that allow us to obtain the best parse in the do-
main of technical manuals and to evaluate our im-
plemented approach to grammar and style check-
ing, we collected and manually annotated corpora
from the domain of technical documentation.
Since errors in pre-edited text tend to be very
scarcely distributed, manual annotation is quite
costly. As a result, instance of certain well-known
error types cannot be tested in a greater variety of
linguistic environments. To overcome this problem,
we semi-automatically derived an additional error
corpus from a treebank of German.
English For purposes of evaluation in a real
world scenario, we constructed a corpus for En-
glish, consisting of 12241 sentences (169459
words) from technical manuals. The corpus was
semi-automatically annotated with several types of
grammar and style errors. For this purpose annota-
tion guidelines were developed, which contained
the description of the errors together with exam-
ples of each and their possible corrections. The an-
notation took place in two phases. First, we wanted
to find out about the precision of the shallow sys-
tem, so we ran the shallow system over the data.
This resulted in an annotation for each error found
consisting of the erroneous sentence, the error span
and the type of error. The annotators, who were na-
tive speakers, then decided whether the errors had
been correctly detected. In the second phase, we
aimed to create a gold standard, so as to be able to
evaluate both the shallow system and Checkpoint
regarding recall and precision. For this purpose, we
extracted the errors that had been annotated as cor-
rectly detected in the previous phase and the an-
notators only had to find the non-detected errors
in the rest of the corpus. For the latter, they also
marked the span and identified the error type.
Subsets of these two datasets were treebanked
with the corresponding HPSG grammars. We em-
ployed the treebanking methodology developed for
Redwoods (Oepen et al, 2002), which involved
156
first parsing a corpus and recording for each item
the alternative analyses (the parse forest) assigned
by the grammar, then manually identifying the cor-
rect analysis (if available) within that parse forest.
This approach provides both a gold standard syn-
tactic/semantic analysis for each parsed item, and
positive and negative training data for building an
accurate statistical model for automatic parse selec-
tion.
German For German, we pursued a complemen-
tary approach towards corpus construction. Here
the focus lay on creating a test and evaluation cor-
pus that provided instances of common error types
in a variety of linguistic contexts. Since manual
error annotation is highly costly, owing to scarce
error distributions in pre-edited text, we chose to
automatically derive an error corpus from an ex-
isting treebank resource. As for the error types, we
focussed on those errors which are arguably perfor-
mance errors, as e.g. missing final consonants in in-
flectional endings, the confusion of homophonous
complementiser and relative pronoun, or else, edit-
ing errors, such as local and non-local duplicates.
We introduced instances of errors in a sub-
corpus of the German TiGer treebank (Brants
et al, 2002), nicknamed TiG-ERR, consisting of
77275 words (5652 sentences) from newspaper
texts. All the sentences in this subcorpus were
parsable, so that an evaluation of Checkpoint in
the ideal situation of 100% coverage could be car-
ried out. The artificially introduced errors were
of the following types: subject verb agreement,
NP internal agreement, dass/das, and repetitions,
all of them already illustrated with examples in sec-
tion 2.1.
Additionally, we annotated a corpus of technical
documents for these error types to estimate the dis-
tribution of these error types in pre-edited text.
4 Error models
In order to construct a statistical parse-ranking
model which could determine the intended use of
a MAL-rule in the analysis of a sentence where the
grammar produced analyses both with and without
MAL-rules, the English treebank was constructed
using the version of the ERG which included the
MAL-rules. 4000 sentences from the English cor-
pus were presented to the parser, of which 86.8%
could be parsed with the ERG, and of these, the an-
notators found an intended analysis for 2500 sen-
tences, including some which correctly used MAL-
rules. From these annotations, a customised parse
selection model was computed and then used in
parsing all of the corpus, this time recording only
the one analysis determined to be most likely ac-
cording to this model. We also compared accu-
racy of error detection based on this new model
with the accuracy of a pre-existing parse-selection
model trained on tourism data for LOGON, and
confirmed that the new model indeed improved
over the old one.
For German, we have not created a specific sta-
tistical model yet, but, instead, we have used an ex-
isting parse selection model (Crysmann, 2008) and
combined it with some heuristics which enable us
to select the best error hypothesis. The heuristics
check for each parsed sentence whether there is an
analysis containing no MAL-rule. If there is one
and this is not ranked as the best parse, it is moved
to the first position in the parse list. As a result, we
can eliminate a high percentage of false alarms.
5 Evaluation results
We have evaluated the English and the German ver-
sions of Checkpoint against the corpora described
in section 3.
German For German we have taken as a test
corpus standard the TiG-ERR subcorpus contain-
ing the automatically introduced errors, and have
parsed all its sentences. The following table shows
the frequencies of the different types of handled er-
rors in the corpus of technical manuals, the FLAG
error corpus (Becker et al, 2000), and in the TiG-
ERR corpus. The electronic version of the FLAG
corpus consists of 14,492 sentences, containing
1,547 grammar or style errors.
ERROR TYPE MANUALS FLAG TiG-ERR
NP internal agr 119 180 2258
subject verb agr 17 63 748
dass/das 1 152 75
repetitions 19 n/a 2571
Table 1: Frequencies of the error types for German
The following charts show the values for recall
and precision for the shallow system and Check-
point. As you can see, Checkpoint improves the
recall for the error types subject verb agreement
and NP internal agreement, whereas the precision
remains more or less the same. For the error type
dass/das Checkpoint improves both recall and pre-
cision. For the error type repetitions, which is only
partially handled by the spell checker in the shal-
low system, Checkpoint reaches considerable re-
call and precision values.
Deep processing on average improves the recall
of the shallow system by 21% and the precision
remains equal at 0.83. According to the error fre-
quencies in the corpus of technical manuals, deep
processing would improve the recall of the shallow
system by only 1.7%, since the error types sub-
ject verb agreement, NP internal agreement and
dass/das only make up 6.57% of the total amount
of annotated errors. However, as we found out later,
the corpora of technical manuals consist of texts
that have already undergone correction, so the er-
rors are very sparse.
157
Figure 1: Checkpoint values for recall and preci-
sion for German
Figure 2: Values for recall and precision for the
shallow system for German
Through the MAL-rules the coverage of the GG
on the TiG-ERR corpus increased to 85% - 95%,
whereas without the MAL-rules the coverage was
10%. This 10% coverage included overgeneration
by the grammar, as well as sentences that, after the
automatic insertion of errors, still remained gram-
matical, although they didn?t express the intended
meaning any more.
The performance of the parallel and integrated
scenarios was compared. The ambiguity of the
MAL-rules, that is, the possibility of applying sev-
eral MAL-rules to a unique error, considerably de-
teriorates the performance when processing sen-
tences containing several errors. In a subcorpus
containing NP internal agreement errors, the aver-
age processing time per sentence increases from
8.3 seconds with the selective activation of MAL-
rules to 31.4 seconds with the activation of all
MAL-rules. Particularly the MAL-rules modeling
the error subject verb agreement are a source of
ambiguity. If these MAL-rules are only selectively
activated the average processing time per sentence
decreases to 14.9 seconds.
Finally, we have evaluated the performance of
the German grammar in the task of error correction,
using non-local duplicates and adjectival agree-
ment errors as a test bed. For these error types,
the German HPSG grammar generated repairs for
85.4% of the detected non-local duplicates and
90% of the detected agreement errors.
English For English we have only implemented
and evaluated the parallel scenario. The focus for
English evaluation was the recognition of those
stylistic errors whose correction requires a re-
structuring of the sentence, and the generation of
the corresponding paraphrases. The recognition of
such error types is not based on MAL-rules, but
on certain already existing rules in the grammar.
The approach was evaluated taking the manually
annotated English corpus of technical manuals as
a gold standard. The following table shows the fre-
quencies of the error types handled by Checkpoint.
ERROR TYPE OCCURRENCES
avoid future 404
avoid modal verbs 657
avoid passive 213
Table 2: Frequencies of the error types for English
The PET parser with the ERG reached 86.1%
coverage on the full corpus. The following charts
show the values for recall and precision for Check-
point and the shallow system.
Figure 3: Checkpoint values for recall and preci-
sion for English
Figure 4: Values for recall and precision for the
shallow system for English
As one can see, for the stylistic errors
avoid future and avoid modal verbs, Checkpoint
reaches values which, although relatively high, are
lower than the shallow system. In most cases a
paraphrase for these errors can be constructed,
so the improvement Checkpoint provides here is
the generation of corrections. For the error type
avoid passive the precision is not so high, which
is due in part to mistakes in the manual annotation.
The passive sentences found by Checkpoint are
actually passive sentences. However, these were
158
not annotated as passives, because the annotators
were told to annotate only those stylistic errors
for which a paraphrase was possible. The same
happens for stylistic errors like avoid subjunctive,
use this that these those with noun and condi-
tion must precede action. In principle, Check-
point is very good at finding these types of errors,
but we cannot yet present a reliable evaluation here,
since only those errors were annotated for which
a paraphrase was possible. This approach is rea-
sonable, since no error alarm should be produced
when there is no other possibility of expressing the
same. However, since we have not yet developed
a method which allows us to automatically distin-
guish those cases for which a paraphrase is possi-
ble from those for which none is, we would need
to annotate all occurrences of a phenomenon in the
corpus, and introduce a further annotation tag for
the paraphrase potential of the sentence.
Nevertheless, even if the grammar-based re-
search prototype cannot beat the industrial pattern-
based system in terms of f-measures, we still be-
lieve that the results are highly valuable in the con-
text of our integrated hybrid scenario: Since the
full reversibility of the ERG has already been estab-
lished independently by (Velldal and Oepen, 2005),
the combined system is able to generate error cor-
rection for a great proportion of the errors detected
by the shallow component. This includes 80% and
above for avoid future and avoid modal verbs.
6 Summary and conclusions
In this paper we have presented an implemented
approach to grammar and style checking based on
hybrid processing. The hybrid system has two com-
ponents: a shallow grammar and style checking
system based on the FLAG technology, and the
PET deep parser operating on linguistically moti-
vated grammars for German and English. The Ger-
man version of the hybrid system improves the re-
call and in certain cases the precision of the shal-
low system and generates error corrections. For
English, the hybrid system in most cases success-
fully generates paraphrases of sentences contain-
ing stylistic errors. Although we only have ex-
plored some of the possibilities of integrating deep
and shallow processing for the grammar and style
checking application, these results speak for the
feasibility of using hybrid processing in this task.
We have developed an integrated strategy which
forwards the output of the shallow system, includ-
ing both the output from several pre-processing
linguistic modules and the error hypotheses, as in-
put to the deep parser. This procedure not only im-
proves the robustness of the deep parser with the
recognition of unknown words and reduces ambi-
guity by instantiating only those lexical items con-
sistent with the hypotheses of the POS tagger or
the morphology; but it also allows the selective
application of grammar rules, which considerably
reduces the search space for parsing and, conse-
quently, improves performance. Based on the error
hypotheses of the shallow system, the selective ap-
plication of grammar rules is achieved by positing
features in the Feature Structure Chart whose par-
ticular values are a pre-condition for MAL-rules
to apply. The improvement in performance sug-
gests that this strategy can be extensible to parsing
in general based on pre-processing components.
Given the output of a chunker, for example, certain
syntactic configurations can already be excluded.
Having features whose values allow one to switch
off certain rules not compatible with these con-
figurations would considerably reduce the search
space.
On the other hand, we have run the two mod-
ules independently from each other to find out
how the recall of the shallow system can be im-
proved by deep processing. The fact that for sev-
eral error types, such as subject verb agreement
and NP internal agreement, recall can be consider-
ably improved suggests that, in order not to parse
all sentences, the shallow system should send an
error hypothesis to the deep system when finding
particular syntactic configurations which may indi-
cate the occurrence of such errors. In this way, such
error hypotheses, although not reliably detectable
by the shallow system alone, could be confirmed
or discarded with a focussed application of deep
processing, which would not be as resource con-
suming as parsing every sentence.
One of the results of the experiment has been
an on-line demonstration system. The running sys-
tem shows that the different modules can be eas-
ily combined with each other. Our hybrid approach,
however, is generic and portable. Although imple-
mented for our specific baseline system, it can in
principle be used with other shallow systems.
Acknowledgements
The research reported in this paper has been car-
ried out as part of the DFKI project Checkpoint,
running from February until November 2007. The
project was funded by the ProFIT program of the
Federal State of Berlin and the EFRE program of
the European Union.
References
Adolphs P., S. Oepen, U. Callmeier, B. Crysmann, and
B. Kiefer. 2008. Some Fine Points of Hybrid Natural
Language Parsing. Proceedings LREC-2008, Mar-
rakech, Morocco.
Becker M., A. Bredenkamp, B. Crysmann, and J. Klein.
2003. Annotation of error types for a German news-
group corpus. In A. Abeille?, editor, Treebanks.
Building and Using Parsed Corpora, number 20 in
Text, Speech And Language Technology. Kluwer,
Dordrecht.
159
Bender, E. M., D. Flickinger, S. Oepen, A. Walsh,
and T. Baldwin. 2004. Arboretum: Using a preci-
sion grammar for grammar checking in call. In In-
STIL/ICALL symposium 2004. NLP and speech tech-
nologies in advanced language learning systems.
Venice, Italy.
Brants, T., S. Dipper, S. Hansen, W. Lezius, and G.
Smith. 2002. The TIGER Treebank. In Proceedings
of the Workshop on Treebanks and Linguistic Theo-
ries. Sozopol.
Bredenkamp, A., B. Crysmann and M. Petrea. 2000.
Looking for errors: A declarative formalism for
resource-adaptive language checking. In Proceed-
ings LREC-2000. Athens, Greece.
Bredenkamp, A., B. Crysmann and M. Petrea. 2000.
Building multilingual controlled language perfor-
mance checkers. In Proceedings of the CLAW 2000.
Seattle, WA.
Callmeier, U., A. Eisele, U. Scha?fer, and M. Siegel.
2004. The Deepthought core architecture frame-
work. In Proceedings of LREC-2004, 1205?1208,
Lisbon, Portugal.
Callmeier, Ulrich. 2000. PET ? a platform for ex-
perimentation with efficient HPSG processing tech-
niques. Natural Language Engineering 6(1):99?
108.
Carroll, John and Ann Copestake and Dan Flickinger
and Victor Poznanski. 1999. An efficient chart gen-
erator for semi-lexicalist grammars. Proceedings of
ENLG, pp. 86?95.
Copestake, A., and D. Flickinger. 2000. An open-
source grammar development environment and
broad-coverage english grammar using HPSG. In
Proceedings LREC-2000. Athens, Greece.
Copestake, A., D. Flickinger, C. Pollard, and I. Sag.
2006. Minimal recursion semantics: an introduction.
Research on Language and Computation 3(4):281?
332.
Crysmann, B., A. Frank, B. Kiefer, S. Mu?ller, G. Neu-
mann, J. Piskorski, U. Scha?fer, M. Siegel, H. Uszko-
reit, F. Xu, M. Becker, and H.-U. Krieger. An inte-
grated architecture for shallow and deep processing.
In Proceedings of ACL 2002, University of Pennsyl-
vania, Philadelphia, 2002.
Crysmann B. 2005. Relative clause extraposition in
German: An efficient and portable implementation.
Research on Language and Computation, 3(1):61?
82.
Crysmann B. 2007 Local ambiguity packing and dis-
continuity in German. In T. Baldwin, M. Dras,
J. Hockenmaier, T. H. King, and G. van Noord, ed-
itors, Proceedings of the ACL 2007 Workshop on
Deep Linguistic Processing, pages 144?151, Prague,
Czech Republic.
Crysmann B. 2008. Parse Selection with a German
HPSG Grammar. In S. Ku?bler and G. Penn, editors,
Proceedings of the ACL 2008 Workshop on Parsing
German (PaGe), pages 9?15, Columbus, Ohio, USA.
Douglas, S. 1995. Robust PATR for error detec-
tion and correction. In A. Schoeter and C. Vogel
(Eds.)Nonclassical feature systems, Vol. 10, pp. 139-
156. Centre for Cognitive Science, University of Ed-
inburgh.
Forsbom, E. 2003. Training a Super Model Look-
Alike. Proceedings of the MT Summit IX Workshop
?Towards Systemizing MT Evaluation?, pp. 29-36.
Gaizauskas, R., H. Cunningham, Y. Wilks, P. Rodgers
and K. Humphreys. 1996. GATE: An environment
to support research and development in natural lan-
guage engineering. In Proceedings of the 8th IEEE
international conference on tools with artificial in-
telligence. Toulouse, France.
Jensen, K., G. E., Heidorn and S. D. Richardson (Eds.).
1993. Natural language processing: The PLNLP ap-
proach. Boston - Dordrecht - London.
Haller, J. 1996. MULTILINT: A technical documenta-
tion system with multilingual intelligence. In Trans-
lating and the computer 18. London.
Heinecke, J., J. Kunze, W. Menzel, and I. Schroeder.
1998. Eliminative parsing with graded constraints.
In Proceedings ACL/Coling 1998, Vol. I, pp. 526-
530. Universite de Montreal, Montreal, Quebec,
Canada.
L?nning J. T. , S. Oepen, D. Beermann, L. Hellan, J.
Carroll, H. Dyvik, D. Flickinger, J. B. Johannessen,
P. Meurer, T. Nordga?rd, V. Rose?n and E. Velldal.
2004. LOGON. A Norwegian MT effort. In Pro-
ceedings of the Workshop in Recent Advances in
Scandinavian Machine Translation. Uppsala, Swe-
den.
McCoy, K. F., C. A. Pennington, and L. Z. Suri. 1996.
English error correction: A syntactic user model
based on principled ?mal-rule? scoring. In Proceed-
ings of UM-96, the Fifth International Conference on
User Modeling, pp. 59-66. Kona, Hawaii.
Menzel, W. 1998. Constraint satisfaction for robust
parsing of natural language. In Theoretical and Ex-
perimental Artificial Intelligence, 10 (1), 77-89.
Mu?ller, S., and W. Kasper. 2000. HPSG analysis of
German. In W. Walster (Ed.), Verbmobil: Foun-
dations of Speech-to-Speech Translation, 238?253
Springer, Berlin.
Oepen, S., K. Toutanova, S. Shieber, C. Manning, D.
Flickinger and T Brants. 2002. The LinGO Red-
woods Treebank. Motivation and Preliminary Appli-
cations. In Proceedings of COLING 2002. Taipei,
Taiwan.
Ravin, Y. 1998. Grammar errors and style weaknesses
in a text-critiquing system. In IEEE Transactions on
Communication, 31 (3)
Schmidt-Wigger, A. 1998. Grammar and style check-
ing for German. In Proceedings of CLAW 98. Pitts-
burgh, PA.
Velldal, E. and S. Oepen. 2005. Maximum entropy
models for realization ranking. In Proceedings of
the 10th MT-Summit (X), Phuket, Thailand.
160
Rapid Prototyping of Scalable Grammars: Towards Modularity in
Extensions to a Language-Independent Core
Emily M. Bender
Department of Linguistics
University of Washington
Box 354340
Seattle WA 98195-4340 USA
ebender@u.washington.edu
Dan Flickinger
Center for the Study of Language and Information
Stanford University
Stanford CA 94305-2150 USA
danf@csli.stanford.edu
Abstract
We present a new way to simplify the
construction of precise broad-coverage
grammars, employing typologically-
motivated, customizable extensions to
a language-independent core grammar.
Each ?module? represents a salient di-
mension of cross-linguistic variation,
and presents the grammar developer
with simple choices that result in auto-
matically generated language-specific
software. We illustrate the approach for
several phenomena and explore the in-
terdependence of the modules.
1 Introduction
Manual development of precise broad-coverage
grammar implementations, useful in a range of
natural language processing/understanding tasks,
is a labor-intensive undertaking, requiring many
years of work by highly trained linguists. Many
recent efforts toward reducing the time and level
of expertise needed to produce a new grammar
have focused on adapting an existing grammar of
another language (Butt et al, 2002; Kim et al,
2003; Bateman et al, ip). Our work on the ?Gram-
mar Matrix? has pursued an alternative approach,
identifying a set of language-independent gram-
mar constraints to which language-specific con-
straints can be added (Bender et al, 2002). This
approach has the hitherto unexploited potential
to benefit from the substantial theoretical work
on language typology. In this paper, we present
a prototype Grammar Matrix customization sys-
tem. This system draws on phenomenon-specific
modules encoding dimensions of linguistic varia-
tion, presents the grammar developer with simple
choices for each phenomenon, and then automati-
cally generates a working starter-grammar, incor-
porating both the cross-linguistic Matrix core and
language-specific constraints. The prototype ad-
dresses basic word order, sentential negation, yes-
no questions, and a small range of lexical entries.
2 The Grammar Matrix
Wide-coverage grammars representing deep lin-
guistic analysis exist in several frameworks, in-
cluding Head-Driven Phrase Structure Grammar
(HPSG), Lexical-Functional Grammar, and Lex-
icalized Tree Adjoining Grammar. In HPSG (P.
and Sag, 1994), the most extensive grammars
are those of English (Flickinger, 2000), German
(Hinrichs et al, 1997; Mu?ller and Kasper, 2000;
Crysmann, ip), and Japanese (Siegel, 2000; Siegel
and Bender, 2002). The Grammar Matrix is an at-
tempt to distill the wisdom of existing grammars
and document it in a form that can be used as the
basis for new grammars. The main goals of the
project are: (i) to develop in detail semantic rep-
resentations and the syntax-semantics interface,
consistent with other work in HPSG; (ii) to repre-
sent generalizations across linguistic objects and
across languages; and (iii) to allow for very quick
start-up as the Matrix is applied to new languages.
The original Grammar Matrix consisted of
types defining the basic feature geometry, types
associated with Minimal Recursion Semantics
(e.g., (Copestake et al, 2001)), types for lex-
203
ical and syntactic rules, and configuration files
for the LKB grammar development environment
(Copestake, 2002) and the PET system (Callmeier,
2000). Subsequent releases have refined the orig-
inal types and developed a lexical hierarchy. The
constraints in this ?core? Matrix are intended to be
language-independent and monotonically exten-
sible in any given grammar. With the typology-
based modules presented here, we extend the con-
straint definitions which can be supplied to gram-
mar developers to those that capture generaliza-
tions holding only for subsets of languages.
3 Typology-based modules
In general, we find two kinds of typological vari-
ation across languages. On the one hand, there
are systems (formal or functional) which must be
represented in every language. For example, ev-
ery language has some set of permissible word or-
ders (formal) and a means of expressing sentential
negation (functional). On the other hand, there
are linguistic phenomena which appear in only
some languages, and are not typically conceptual-
ized as alternative realizations of some universal
function, phenomena such as noun incorporation,
numeral classifiers, and auxiliary verbs. Each of
these phenomena are found in recurring varieties
that can be subjected to typological analysis (see,
e.g., (Mithun, 1984)). Our approach is designed
to handle both kinds of typological variation.
As with earlier versions of the Matrix, we aim
to support rapid prototyping of precision gram-
mars that can scale up to broad-coverage (as have
the NorSource (Hellan and Haugereid, 2003) and
Modern Greek (Kordoni and Neu, 2003) gram-
mars, based on early versions of the Matrix). This
sets a high bar for the modules themselves, requir-
ing them to be good early approximations which
may need to be refined but not thrown out. It also
requires that the automatically generated gram-
mar files maintain a high degree of readability so
that they may be effectively modified. In future
work, we intend to extend the system to allow the
linguist to revise decisions in the face of new in-
formation or improved linguistic analyses.
The core Matrix and modular extensions to it
may appear analogous to the Principles and Pa-
rameters proposed by Chomsky (1981) and oth-
ers. However, whereas Parameters are meant to
be abstract ?switches? which simultaneously con-
trol multiple different, apparently unrelated phe-
nomena, the modules in the Matrix each encode
the constraints necessary to handle one particu-
lar phenomenon. Nonetheless, this does not make
the modules trivial: they need to be carefully de-
signed in order to be mutually consistent, ide-
ally across all possible combinations. Our strat-
egy is thus consistent with a bottom-up, data-
driven investigation of linguistic universals and
constraints on cross-linguistic variation. As the
number and breadth of implemented grammars
grows, we expect linguistic predictions to emerge
and become part of improved modules, particu-
larly with respect to interactions among the dis-
tinct phenomena covered. Our approach should in
time be instrumental in assisting large-scale typo-
logical investigations (covering hundreds of lan-
guages), making use of the linguistically precise
constraints encoded in these modules to uncover
deeper and more subtle facts about languages.
4 Implementations of prototype system
We have implemented a prototype system with
a small set of modules targeting basic word or-
der, main-clause yes-no questions, and senten-
tial negation.1 The corresponding choices and
a questionnaire for creating a small lexicon are
presented to the user through an html form inter-
face. A perl/cgi back-end produces a starter gram-
mar from the user input and an internal knowl-
edge base. The resulting grammars can be used
immediately to parse and generate a fragment of
the target language. The system can be accessed
at http://www.delph-in.net/matrix/modules.html.
This section describes its linguistic coverage.
4.1 Word order
The word order module addresses the so-called
basic word order in a language: the relative or-
der of subjects, verbs, and verbal complements.
Languages vary in their rigidity in this respect,
and the question of how to determine the basic
word-order of a language is notoriously complex.
Nonetheless, we believe that most linguists work-
ing on linguistic description analyze some orders
as primary and others as derived. Thus the word
1Drellishak and Bender (ta) present a module for coordi-
nation which is integrated with those described here.
204
order module is meant to capture the relative or-
dering of major constituents in clauses without
word-order changing phenomena such as topical-
ization, extraposition, subject-verb inversion, etc.
Modules for such phenomena will need to interact
appropriately with the basic word-order module.
The Matrix core grammar provides defini-
tions of basic head-complement and head-subject
schemata which are consistent with our imple-
mentation of compositional semantics (Flickinger
and Bender, 2003), as well as definitions of head-
initial and head-final phrase types. The word
order module creates subtypes joining the head-
complement and head-subject schemata with the
types specifying head/dependent order, creates in-
stances of those types as required by the LKB
parser, and constrains the rules to eliminate spu-
rious ambiguity in the case of free word order. It
currently handles SOV, SVO, VSO, VOS, OVS,
OSV, V-final, V-initial, and free word order. We
leave to future work variations such as V2 or-
der, differing word order in main v. subordinate
clauses, and flexible ordering among comple-
ments in otherwise strict word order languages.
4.2 Yes-no questions
For yes-no questions, we implement four alterna-
tives: inversion of the subject and a main or aux-
iliary verb relative to declarative word order and
sentence-initial or final question particles.
Inversion of the subject and the main verb is
implemented with a lexical rule which relocates
the subject (the value of SUBJ in the valence spec-
ifications) to be the first on the COMPS list, and
further assigns a positive value for an additional
feature INV (inverted) on verbs. This feature may
well have independent syntactic motivation in the
language, but is in any case used here so the
declarative/interrogative distinction can be made
in the semantics once the clause is constructed.
Subject-aux inversion is a minor extension of the
basic inversion type, constraining the lexical rule
to only apply to auxiliary verbs. This module han-
dles ?support? verbs like do in English in not li-
censing inversion with main verbs, while licens-
ing similar strings with a semantically empty sup-
port verb (if it is in the lexicon). The third type of
mechanism employs a distinct question particle,
here treated as a pre- or post-modifying sentence
adverb. The grammar developer is prompted for
this positional distinction, and for the spelling of
the particle; the code for the relevant lexical en-
try is then autogenerated, instantiating a question
particle type which supplies the remaining syn-
tactic and semantic constraints needed.
Future work on this module includes support
for ?intonation questions?, where the same string
can be associated with either proposition or ques-
tion semantics, as well as the integration of
declarative/interrogative punctuation contrasts.
4.3 Sentential negation
The sentential negation module handles two gen-
eral negation strategies, several variants on each,
and allows for both to coexist in a single grammar.
The first strategy is negation via verbal inflec-
tion. For this strategy, the grammar developer
specifies whether the inflection attaches to main
verbs, auxiliaries, or either; whether it is a prefix
or a suffix; and the form of the affix. We cur-
rently only allow for strictly concatenative mor-
phology. In a more fully developed system, the
syntax-semantics modules here would be inter-
faced with a separate means of specifying mor-
phophonology (cf. (Bender and Good, ip)).
The second strategy is negation via a negative
adverb, with two sub-cases: The negative adverb
may be an independent modifier (of V, VP, or S
and pre- or post-head) or it may be a selected
complement of the verb (main verbs only, aux-
iliaries only, or both) (Kim, 2000). The grammar
developer specifies the form of the adverb.
Neither, either or both of these strategies may
be selected. If neither, the grammar produced will
not contain an analysis of negation. If both, the
grammar developer must specify how the strate-
gies interact, from among five choices: (i) the two
strategies are in complementary distribution, (ii)
the two strategies can appear independently or to-
gether, (iii) both inflection and an adverb are re-
quired to express sentential negation, (iv) the ad-
verb is obligatory, but it may appear with or with-
out the inflection, and (v) the inflection is obliga-
tory, but it may appear with or without the adverb.
In the generated grammars, independent ad-
verbs are implemented by adding appropriate lex-
ical types and lexical entries. Selected adverbs
and inflection are handled via lexical rules similar
205
to those presented in (Sag et al, 2003). For exam-
ple, in a language where sentential negation can
be expressed by inflection alone or inflection in
combination with a (selected) adverb, we gener-
ate two lexical rules. One changes the form of the
verb and adds the negative semantics. The other
changes the form of the verb and adds the nega-
tive adverb to its complements list.
4.4 Lexicon
As HPSG is a strongly lexicalist theory, words
tend to carry quite a bit of information. This
information is encoded in lexical types; lexical
entries merely specify the type they instantiate,
their orthographic form, and their semantic predi-
cate. Many of the constraints required (e.g., for
the linking of syntactic to semantic arguments)
are already provided by the core Matrix. How-
ever, there is also cross-linguistic variation.
We ask the grammar developer to specify two
nouns and two verbs (one transitive and one in-
transitive), as well as an auxiliary, two deter-
miners, two case-marking adpositions, a nega-
tive adverb and a question particle, if appropriate.
Nouns are specified as to whether they require,
allow, or disallow determiners. Verbs are speci-
fied as to whether each argument is expressed as
an NP or a PP, and optionally for an additional
(non-finite) form. Auxiliaries are specified as to
whether they introduce independent predicates or
only carry tense/aspect; take S, VP or V com-
plements; appear to the left, right or either side
of their complements; and take NP or PP sub-
jects. Case-marking adpositions must be specified
as either prepositions or postpositions. Finally,
the questionnaire requires orthographic forms and
predicate names. Note that the forms are assumed
to be fully inflected (modulo negation), support
morphological processes awaiting future work.
We use this information and the knowledge
base to produce a set of lexical types inherit-
ing from the types defined in the core Matrix
and specifying appropriate language-specific con-
straints, and a set of lexical entries.
5 Limits of modularity
Recent computational work in HPSG has asked
whether different parts of a single grammar can
be abstracted into separate, independent mod-
ules, either for processing (Kasper and Krieger,
1996; Theofilidis et al, 1997) or grammar devel-
opment (Kes?elj, 2001). Our work is most simi-
lar to Kes?elj?s though we are pursuing different
goals: Kes?elj is looking to support a division of
labor among multiple individuals working on the
same grammar and to support variants of a single
grammar for different domains. His modules each
have private and public features and types, and he
illustrates the approach with a small-scale ques-
tion answering system. In contrast, we are ap-
proaching this issue from the perspective of reuse
of grammar code in the context of multilingual
grammar engineering (a possibility suggested, but
not developed, by Theofilidis et al.
Our notion of modularity is influenced by the
following constraints: (i) The questions in the
customization interface must be sensible to the
working linguist; (ii) The resulting starter gram-
mars must be highly readable so that they can
be extended by the grammar developer (typically
only one per grammar); and (iii) HPSG prac-
tice values capturing linguistic generalizations by
having single types encode many different con-
straints and, ideally, single constraints contribute
to the analysis of many different phenomena.
Even with the modest linguistic coverage of
the existing system, we have found many cases
of non-trivial interaction between the modules:
Our phrase structure rules, following HPSG prac-
tice, capture cross-categorial generalizations: if
both verbs and adpositions follow their comple-
ments, then a single complement-head rule serves
for both. However, few languages (if any) are
completely consistent in their ordering of heads
and dependents. Thus, before defining the types
and instances for these rules, we must determine
whether the fragment requires auxiliaries (for
negation or yes-no questions) or case-marking ad-
positions, and whether their order with respect to
their complements is consistent with that of main
verbs. A second example is the lexical type for
main verbs, whose definition depends on whether
the language has auxiliaries (requiring a feature
AUX distinguishing the two kinds of verbs and
a feature FORM governing the distribution of fi-
nite and non-finite verbs). As a third example, the
negation and question modules each have options
requiring auxiliaries, but we must posit the asso-
206
ciated types and constraints at most once.
Thus we find that, for our purposes, the relevant
notion of modularity is modularity from the point
of view of the linguist who uses the system to cre-
ate a starter grammar. To support this, we strive to
make the questions we ask of the linguist be as in-
dependent of each other as possible, and to make
it clear when one particular choice (e.g., negation
as inflection) requires further information (suffix
v. prefix). The fact that the questions we present
to the linguist don?t correspond to neatly isolated
parts of the underlying knowledge base is not a
failure of the approach, but rather a reflection of
the complexity of language. The very intercon-
nectedness of grammatical phenomena is at the
heart of research in theoretical syntax. We in-
tend our system to provide a data-driven cross-
linguistic exploration of that interconnection.
6 Validation of prototype system
To verify the mutual consistency of the mod-
ules developed so far and to illustrate their ap-
plicability to a interesting range of languages,
we developed abstract test suites for seven lan-
guages. This convenience sample of languages is
not representative, either typologically or geneti-
cally. The grammatical and ungrammatical exam-
ples in each test suite use a small, artificial lexi-
con, and reflect the typological properties of each
language along the dimensions of basic word or-
der, sentential negation, and yes-no questions (Ta-
ble 1). Table 2 presents the performance of each
grammar (as generated by our prototype system
with appropriate input) on its associated test suite.
Language2 Order Negation Yes-no Q3
English SVO aux-selected adv aux inv
Hindi SOV pre-V adv S-init part.
Japanese V-final verbal suffix S-final part
Mandarin SVO pre-V adv S-final part,
A-not-A
Polish free pre-V adv S-init part
Slave SOV post-V adv S-init part
Spanish SVO pre-V adv main V inv
Table 1: Languages used in testing
While these test suites are quite modest, we be-
lieve they show that the prototype system is able
2Sources: Hindi: Snell and Weightman, 2000, Mandarin:
Li and Thompson, 1981, Polish: Adam Przepio?rkowski, p.c.,
Slave (Athabaskan): Rice, 1989
3In addition to intonation questions, if any.
Language Pos. Coverage Neg. Overgen.
English 5 100% 10 10%
Hindi 5 100% 10 0%
Japanese 6 100% 10 0%
Mandarin 4 75% 9 0%
Polish 14 100% 8 0%
Slave 3 100% 6 0%
Spanish 5 100% 7 0%
Table 2: Parsing evaluation results
to produce good first-pass grammar fragments for
an interesting variety of languages. More study is
needed to develop a means of testing the cross-
compatibility of all choices on all modules, to
evaluate the coverage against a typologically jus-
tified sample, and to gauge the success of this
strategy in producing grammars which are com-
prehensible to beginning grammar developers.
7 Conclusion and outlook
We have described a method for extending
a language-independent core grammar like the
Grammar Matrix with modules handling cross-
linguistically variable but still recurring patterns.
This method allows for extremely rapid prototyp-
ing of deep precision grammars in such a way
that the prototypes themselves can serve as the
basis for sustained development. We envision at
least four potential uses for this kind of grammar
prototyping: (i) in pedagogical contexts, where
it would allow grammar engineering students to
more quickly work on cutting-edge problems, (ii)
in language documentation, where a documen-
tary linguist in the field might be collaborating
remotely with a grammar engineer to propose and
test hypotheses, (iii) in leveraging the results from
economically powerful languages to reduce the
cost of creating resources for minority languages,
and (iv) in supporting typological or comparative
studies of linguistic phenomena or interactions
between phenomena across languages.
Acknowledgments
We thank Scott Drellishak, Stephan Oepen, Lau-
rie Poulson, and the 2004 and 2005 multilingual
grammar engineering classes at the University of
Washington for valuable input and NTT Com-
munication Science Laboratories for their support
through a grant to CSLI (Stanford). All remaining
errors are our own.
207
References
J.A. Bateman, I. Kruijff-Korbayova?, and G.-J. Krui-
jff. ip. Multilingual resource sharing across both
related and unrelated languages: an implemented,
open-source framework for practical natural lan-
guage generation. Res. on Lang. and Computation.
E.M. Bender and J. Good. ip. Implementation for
discovery: A bipartite lexicon to support morpho-
logical and syntactic analysis. In CLS 41.
E.M. Bender, D. Flickinger, and S. Oepen. 2002.
The grammar matrix. COLING 2002 Workshop on
Grammar Engineering and Evaluation.
M. Butt, H. Dyvik, T.H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project. In
COLING 2002 Workshop on Grammar Engineering
and Evaluation.
U. Callmeier. 2000. PET ? A platform for ex-
perimentation with efficient HPSG processing tech-
niques. Natural Lang. Engineering, 6 (1):99 ? 108.
N. Chomsky. 1981. Lectures on Government and
Binding. Foris, Dordrecht.
A. Copestake, A. Lascarides, and D. Flickinger. 2001.
An algebra for semantic construction in constraint-
based grammars. In ACL 2001.
A. Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI, Stanford, CA.
B. Crysmann. ip. Relative clause extraposition in ger-
man: An efficient and portable implementation. Re-
search on Lang. and Computation.
S. Drellishak and E.M. Bender. ta. Coordination
modules for a crosslinguistic grammar resource. In
Proc. of HPSG 2005.
D. Flickinger and E.M. Bender. 2003. Compositional
semantics in a multilingual grammar resource. In
Proc. of the Workshop on Ideas and Strategies for
Multilingual Grammar Development, ESSLLI 2003,
pages 33?42.
D. Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Lang. En-
gineering, 6 (1):15 ? 28.
L. Hellan and P. Haugereid. 2003. Norsource: An
exercise in matrix grammar-building design. In
Proc. of the Workshop on Ideas and Strategies for
Multilingual Grammar Development, ESSLLI 2003,
pages 41?48.
W.D. Hinrichs, E.and Meurers, F. Richter, M. Sailer,
and H. Winhart. 1997. Ein HPSG-Fragement des
Deutschen. Arbeitspapiere des Sonderforschungs-
bereichs 340, Bericht Nr. 95.
W. Kasper and H.-U. Krieger. 1996. Modularizing
codescriptive grammars for efficient parsing. In
COLING 1996, pages 628?633.
V. Kes?elj. 2001. Modular HPSG. Technical Report
CS-2001-05, Department of Computer Science,
University of Waterloo, Waterloo, Ont., Canada.
R. Kim, M. Dalrymple, R.M. Kaplan, T.H. King,
H. Masuichi, and T. Ohkuma. 2003. Multlingual
grammar development via grammar porting. In
Proc. of the Workshop on Ideas and Strategies for
Multilingual Grammar Development, ESSLLI 2003,
pages 49?56.
J. Kim. 2000. The Grammar of Negation: A
Constraint-Based Approach. CSLI, Stanford, CA.
V. Kordoni and J. Neu. 2003. Deep gramamr develop-
ment for Modern Greek. In Proc. of the Workshop
on Ideas and Strategies for Multilingual Grammar
Development, ESSLLI 2003, pages 65?72.
C.N. Li and S.A. Thompson. 1981. Mandarin Chi-
nese: A Functional Reference Grammar. Univer-
sity of California Press, Berkeley, CA.
M. Mithun. 1984. The evolution of noun incorpora-
tion. Language, 60(4):847?894.
S. Mu?ller and W. Kasper. 2000. HPSG analy-
sis of German. In W. Wahlster, editor, Verbmo-
bil. Foundations of Speech-to-Speech Translation,
pages 238 ? 253. Springer, Berlin, Germany.
Carl P. and I.A. Sag. 1994. Head-Driven Phrase
Structure Grammar. The Univeristy of Chicago
Press, Chicago, IL.
K. Rice. 1989. A Grammar of Slave. Mouton de
Gruyter, Berlin.
I.A. Sag, T. Wasow, and E.M. Bender. 2003. Synactic
Theory: A Formal Introduction. CSLI, Stanford,
CA, 2nd edition.
M. Siegel and E.M. Bender. 2002. Efficient deep
processing of Japanese. In Proc. of the 3rd Work-
shop on Asian Language Resources and Interna-
tional Standardization at COLING 2002.
M. Siegel. 2000. HPSG analysis of Japanese.
In W. Wahlster, editor, Verbmobil. Foundations
of Speech-to-Speech Translation, pages 265 ? 280.
Springer, Berlin, Germany.
R. Snell and S. Weightman. 2000. Hindi. Teach Your-
self Books.
A. Theofilidis, P. Schmidt, and T. Declerck. 1997.
Grammar modularization for efficient processing:
Language engineering devices and their instantia-
tions. In Proc. of the DGFS/CL.
208
The Grammar Matrix: An Open-Source Starter-Kit for the Rapid
Development of Cross-Linguistically Consistent Broad-Coverage Precision
Grammars
Emily M. Bender and Dan Flickinger and Stephan Oepen
Center for the Study of Language and Information
Stanford University
fbender jdan joeg@csli.stanford.edu
Abstract
The grammar matrix is an open-source
starter-kit for the development of broad-
coverage HPSGs. By using a type hierar-
chy to represent cross-linguistic generaliza-
tions and providing compatibility with other
open-source tools for grammar engineering,
evaluation, parsing and generation, it facil-
itates not only quick start-up but also rapid
growth towards the wide coverage necessary
for robust natural language processing and
the precision parses and semantic represen-
tations necessary for natural language under-
standing.
1 Introduction
The past decade has seen the development of
wide-coverage implemented grammars represent-
ing deep linguistic analysis of several languages
in several frameworks, including Head-Driven
Phrase Structure Grammar (HPSG), Lexical-
Functional Grammar (LFG), and Lexicalized Tree
Adjoining Grammar (LTAG). In HPSG, the most ex-
tensive grammars are those of English (Flickinger,
2000), German (Mu?ller & Kasper, 2000), and
Japanese (Siegel, 2000; Siegel & Bender, 2002).
Despite being couched in the same general frame-
work and in some cases being written in the
same formalism and consequently being compati-
ble with the same parsing and generation software,
these grammars were developed more or less inde-
pendently of each other. They each represent be-
tween 5 and 15 person years of research efforts,
and comprise 35?70,000 lines of code. Unfor-
tunately, most of that research is undocumented
and the accumulated analyses, best practices for
grammar engineering, and tricks of the trade are
only available through painstaking inspection of
the grammars and/or consultation with their au-
thors. This lack of documentation holds across
frameworks, with certain notable exceptions, in-
cluding Alshawi (1992), Mu?ller (1999), and Butt,
King, Nin?o, & Segond (1999).
Grammars which have been under development
for many years tend to be very difficult to mine for
information, as they contain layers upon layers of
interacting analyses and decisions made in light of
various intermediate stages of the grammar. As a
result, when embarking on the creation of a new
grammar for another language, it seems almost
easier to start from scratch than to try to model it on
an existing grammar. This is unfortunate?being
able to leverage the knowledge and infrastructure
embedded in existing grammars would greatly ac-
celerate the process of developing new ones. At the
same time, these grammars represent an untapped
resource for the bottom-up exploration of language
universals.
As part of the LinGO consortium?s multi-lingual
grammar engineering effort, we are developing a
?grammar matrix? or starter-kit, distilling the wis-
dom of existing grammars and codifying and doc-
umenting it in a form that can be used as the basis
for new grammars.
In the following sections, we outline the inven-
tory of a first, preliminary version of the grammar
matrix, discuss the interaction of basic construc-
tion types and semantic composition in unification
grammars by means of a detailed example, and
consider extensions to the core inventory that we
foresee and an evaluation methodology for the ma-
trix proper.
2 Preliminary Development of Matrix
We have produced a preliminary version of the
grammar matrix relying heavily on the LinGO
project?s English Resource Grammar, and to a
lesser extent on the Japanese grammar developed
jointly between DFKI Saarbru?cken (Germany) and
YY Technologies (Mountain View, CA). This early
version of the matrix comprises the following com-
ponents:
 Types defining the basic feature geometry and
technical devices (e.g., for list manipulation).
 Types associated with Minimal Recursion Se-
mantics (see, e.g., Copestake, Lascarides, &
Flickinger, 2001), a meaning representation
language which has been shown to be well-
suited for semantic composition in typed fea-
ture structure grammars. This portion of the
grammar matrix includes a hierarchy of rela-
tion types, types and constraints for the prop-
agation of semantic information through the
phrase structure tree, a representation of illo-
cutionary force, and provisions for grammar
rules which make semantic contributions.
 General classes of rules, including deriva-
tional and inflectional (lexical) rules, unary
and binary phrase structure rules, headed and
non-headed rules, and head-initial and head-
final rules. These rule classes include im-
plementations of general principles of HPSG,
like, for example, the Head Feature and Non-
Local Feature Principles.
 Types for basic constructions such as head-
complement, head-specifier, head-subject,
head-filler, and head-modifier rules, coordi-
nation, as well as more specialized classes
of constructions, such as relative clauses and
noun-noun compounding. Unlike in specific
grammars, these types do not impose any or-
dering on their daughters in the grammar ma-
trix.
Included with the matrix are configuration and
parameter files for the LKB grammar engineering
environment (Copestake, 2002).
Although small, this preliminary version of
the matrix already reflects the main goals of
the project: (i) Consistent with other work in
HPSG, semantic representations and in particular
the syntax-semantics interface are developed in de-
tail; (ii) the types of the matrix are each represen-
tations of generalizations across linguistic objects
and across languages; and (iii) the richness of the
matrix and the incorporation of files which connect
it with the LKB allow for extremely quick start-up
as the matrix is applied to new languages.
Since February 2002, this preliminary version of
the matrix has been in use at two Norwegian uni-
versities, one working towards a broad-coverage
reference implementation of Norwegian (NTNU),
the other?for the time being?focused on specific
aspects of clause structure and lexical description
(Oslo University). In the first experiment with
the matrix, at NTNU, basic Norwegian sentences
were parsing and producing reasonable semantics
within two hours of downloading the matrix files.
Linguistic coverage should scale up quickly, since
the foundation supplied by the matrix is designed
not only to provide a quick start, but also to support
long-term development of broad-coverage gram-
mars. Both initiatives have confirmed the utility of
the matrix starter kit and already have contributed
to a series of discussions on cross-lingual HPSG
design aspects, specifically in the areas of argu-
ment structure representations in the lexicon and
basic assumptions about constituent structure (in
one view, Norwegian exhibits a VSO topology in
the main clause). The user groups have suggested
refinements and extensions of the basic inventory,
and it is expected that general solutions, as they are
identified jointly, will propagate into the existing
grammars too.
3 A Detailed Example
As an example of the level of detail involved in
the grammar matrix, in this section we consider
the analysis of intersective and scopal modifica-
tion. The matrix is built to give Minimal Recursion
Semantics (MRS; Copestake et al, 2001; Copes-
take, Flickinger, Sag, & Pollard, 1999; Copestake,
Flickinger, Malouf, Riehemann, & Sag, 1995) rep-
resentations. The two English examples in (1)
exemplify the difference between intersective and
scopal modification:1
(1) a. Keanu studied Kung Fu on a spaceship.
b. Keanu probably studied Kung Fu.
The MRSs for (1a-b) (abstracting away from
agreement information) are given in (2) and (3).
The MRSs are ordered tuples consisting of a top
handle (h1 in both cases), an instance or event vari-
able (e in both cases), a bag of elementary predica-
tions (eps), and a bag of scope constraints (in these
cases, QEQ constraints or ?equal modulo quanti-
fiers?). In a well-formed MRS, the handles can be
1These examples also differ in that probably is a pre-
head modifier while on a spaceship is a post-head modifier.
This word-order distinction cross-cuts the semantic distinc-
tion, and our focus is on the latter, so we won?t consider the
word-order aspects of these examples here.
identified in one or more ways respecting the scope
constraints such that the dependencies between the
eps form a tree. For a detailed description of MRS,
see the works cited above. Here, we will focus on
the difference between the intersective modifier on
(a spaceship) and the scopal modifier probably.
In (2), the ep contributed by on (?on-rel?) shares
its handle (h7) with the ep contributed by the verb
it is modifying (?study-rel?). As such, the two will
always have the same scope; no quantifier can in-
tervene. Further, the second argument of the on-rel
(e) is the event variable of the study-rel. The first
argument, e0, is the event variable of the on-rel and
the third argument, z, is the instance variable of the
spaceship-rel.
(2) h h1, e,
f h1:prpstn-rel(h2), h3:def-np-rel(x, h4, h5),
h6:named-rel(x, ?Keanu?), h7:study-rel(e, x, y),
h8:def-np-rel(y, h9, h10),
h11:named-rel(y, ?Kung Fu?), h7:on-rel(e0, e, z),
h12:a-quant-rel(z, h13, h14),
h15:spaceship-rel(z) g,
f h2 QEQ h7, h4 QEQ h6, h19 QEQ h11,
h13 QEQ h15 g i
In (3), the ep contributed by the scopal modifier
probably (?probably-rel?) has its own handle (h7)
which is not shared by anything. Furthermore, it
takes a handle (h8) rather than the event variable
of the study-rel as its argument. h8 is equal mod-
ulo quantifiers (QEQ) to the handle of the study-rel
(h9), and h7 is equal modulo quantifiers to the ar-
gument of the prpstn-rel (h2). The prpstn-rel is the
ep representing the illocutionary force of the whole
expression. This means that quantifiers associated
with the NPs Keanu and Kung Fu can scope inside
or outside probably.
(3) h h1, e,
f h1:prpstn-rel(h2), h3:def-np-rel(x, h4, h5),
h6:named-rel(x, ?Keanu?),
h7:probably-rel(h8), h9:study-rel(e, x, y),
h10:def-np-rel(y, h11, h12),
h13:named-rel(y, ?Kung Fu?) g,
f h2 QEQ h7, h4 QEQ h6, h8 QEQ h9,
h11 QEQ h13 g i
While the details of modifier placement, which
parts of speech can modify which kinds of phrases,
etc., differ across languages, we believe that all
languages display a distinction between scopal and
intersective modification. Accordingly, the types
isect-mod-phrase := head-mod-phr-simple &
[ HEAD-DTR.SYNSEM.LOCAL
[ CONT [ TOP #hand,
INDEX #index ],
KEYS.MESSAGE 0-dlist ],
NON-HEAD-DTR.SYNSEM.LOCAL
[ CAT.HEAD.MOD <[ LOCAL isect-mod ]>,
CONT.TOP #hand ],
C-CONT.INDEX #index ].
Figure 1: TDL description of isect-mod-phrase
scopal-mod-phrase := head-mod-phr-simple &
[ NON-HEAD-DTR.SYNSEM.LOCAL
[ CAT.HEAD.MOD <[ LOCAL scopal-mod ]>,
CONT.INDEX #index ],
C-CONT.INDEX #index ].
Figure 2: TDL description of scopal-mod-phrase
necessary for describing these two kinds of modi-
fication are included in the matrix.
The types isect-mod-phrase and scopal-mod-
phrase (shown in Figures 1 and 2) encode the in-
formation necessary to build up in a compositional
manner the modifier portions of the MRSs in (2)
and (3).
These types are embedded in the type hierar-
chy of the matrix. Through their supertype head-
mod-phr-simple they inherit information common
to many types of phrases, including the basic fea-
ture geometry, head feature and non-local feature
passing, and semantic compositionality. These
types also have subtypes in the matrix specifying
the two word-order possibilities (pre- or post-head
modifiers), giving a total of four subtypes.2
The most important difference between these
types is in the treatment of the handle of the head
daughter?s semantics, to distinguish intersective
and scopal modification. In isect-mod-phrase, the
top handles (TOP) of the head and non-head (i.e.,
modifier) daughters are identified (#hand). This
allows for MRSs like (2) where the eps contributed
by the head (?study-rel?) and the modifier (?on-rel?)
take the same scope. The type scopal-mod-phrase
bears no such constraint. This allows for MRSs
like (3) where the modifier?s semantic contribution
(?probably-rel?) takes the handle of the head?s se-
mantics (?study-rel?) as its argument, so that the
modifier outscopes the head. In both types of mod-
2All four subtypes are provided on the theory that most
languages will make use of all or most of them.
ifier phrase, a constraint inherited from the super-
type ensures that the handle of the modifier is also
the handle of the whole phrase.
The constraints on the LOCAL value inside
the modifier?s MOD value regulate which lexi-
cal items can appear in which kind of phrase.
Intersective modifiers specify lexically that they
are [ MOD h [ LOCAL isect-mod ] i] and sco-
pal modifiers specify lexically that they are
[ MOD h [ LOCAL scopal-mod ] i].3 These con-
straints exemplify the kind of information that will
be developed in the lexical hierarchy of the matrix.
It is characteristic of broad-coverage grammars
that every particular analysis interacts with many
other analyses. Modularization is an on-going con-
cern, both for maintainability of individual gram-
mars, and for providing the right level of abstrac-
tion in the matrix. For the same reasons, we have
only been able to touch on the highlights of the se-
mantic analysis of modification here, but hope that
this quick tour will suffice to illustrate the extent
of the jump-start the matrix can give in the devel-
opment of new grammars.
4 Future Extensions
The initial version of the matrix, while sufficient to
support some useful grammar work, will require
substantial further development on several fronts,
including lexical representation, syntactic gener-
alization, sociolinguistic variation, processing is-
sues, and evaluation. This first version drew most
heavily from the implementation of the English
grammar, with some further insights drawn from
the grammar of Japanese. Extensions to the ma-
trix will be based on careful study of existing im-
plemented grammars for other languages, notably
German, Spanish and Japanese, as well as feed-
back from those using the first version of the ma-
trix.
For lexical representation, one of the most ur-
gent needs is to provide a language-independent
type hierarchy for the lexicon, at least for major
parts of speech, establishing the mechanisms used
for linking syntactic subcategorization to seman-
tic predicate-argument structure. Lexical rules pro-
vide a second mechanism for expressing general-
3Note that there are no further subtypes of LOCAL values
beyond isect-mod and scopal-mod. Since these grammars do
not make extensive use of subtypes of LOCAL values, they
were available for encoding this distinction. Alternative solu-
tions include positing a new feature.
izations within the lexicon, and offer ready oppor-
tunities for cross-linguistic abstractions for both
inflectional and derivational regularities. Work is
also progressing on establishing a standard rela-
tional database (using PostgreSQL) for storing in-
formation for the lexical entries themselves, im-
proving both scalability and clarity compared to
the current simple text file representation. Form-
based tools will be provided both for constructing
lexical entries and for viewing the contents of the
lexicon.
The primary focus of work on syntactic general-
ization in the matrix is to support more freedom
in word order, for both complements and modi-
fiers. The first step will be a relatively conserva-
tive extension along the lines of Netter (1996), al-
lowing the grammar writer more control over how
a head combines with complements of different
types, and their interleaving with modifier phrases.
Other areas of immediate cross-linguistic interest
include the hierarchy of head types, control phe-
nomena, clitics, auxiliary verbs, noun-noun com-
pounds, and more generally, phenomena that in-
volve the word/phrase distinction, such as noun in-
corporation. A study of the existing grammars for
English, German, Japanese, and Spanish reveals
a high degree of language-specificity for several
of these phenomena, but also suggests promise of
reusable abstractions.
Several kinds of sociolinguistic variation require
extensions to the matrix, including grammaticized
aspects of pragmatics such as politeness and em-
pathy, as well as dialect and register alternations.
The grammar of Japanese provides a starting point
for representations of both empathy and politeness.
Implementations of familiar vs. formal verb forms
in German and Spanish provide further instances
of politeness to help build the cross-linguistic ab-
stractions. Extensions for dialect variation will
build on some exploratory work in adapting the
English grammar to support American, British,
and Australian regionalisms, both lexical and syn-
tactic, while restricting dialect mixture in genera-
tion and associated spurious ambiguity in parsing.
While the development of the matrix will be
built largely on the LKB platform, support will also
be needed for using the emerging grammars on
other processing platforms, and for linking to other
packages for pre-processing the linguistic input.
Several other platforms exist which can efficiently
parse text using the existing grammars, includ-
ing the PET system developed in C++ at Saarland
University (Germany) and the DFKI (Callmeier,
2000); the PAGE system developed in Lisp at the
DFKI (Uszkoreit et al, 1994); the LiLFeS system
developed at Tokyo University (Makino, Yoshida,
Torisawa, & Tsujii, 1998), and a parallel process-
ing system developed in Objective C at Delft Uni-
versity (The Netherlands; van Lohuizen, 2002).
As part of the matrix package, sample configura-
tion files and documentation will be provided for
at least some of these additional platforms.
Existing pre-processing packages can also sig-
nificantly reduce the effort required to develop
a new grammar, particularly for coping with the
morphology/syntax interface. For example, the
ChaSen package for segmenting Japanese input
into words and morphemes (Asahara & Mat-
sumoto, 2000) has been linked to at least the LKB
and PET systems. Support for connecting im-
plementations of language-specific pre-processing
packages of this kind will be preserved and ex-
tended as the matrix develops. Likewise, config-
uration files are included to support generation, at
least within the LKB, provided that the grammar
conforms to certain assumptions about semantic
representation using the Minimal Recursion Se-
mantics framework.
Finally, a methodology is under development for
constructing and using test suites organized around
a typology of linguistic phenomena, using the im-
plementation platform of the [incr tsdb()] profil-
ing package (Oepen & Flickinger, 1998; Oepen
& Callmeier, 2000). These test suites will enable
better communication about current coverage of a
given grammar built using the matrix, and serve as
the basis for identifying additional phenomena that
need to be addressed cross-linguistically within the
matrix. Of course, the development of the typol-
ogy of phenomena is itself a major undertaking
for which a systematic cross-linguistic approach
will be needed, a discussion of which is outside
the scope of this report. But the intent is to seed
this classification scheme with a set of relatively
coarse-grained phenomenon classes drawn from
the existing grammars, then refine the typology as
it is applied to these and new grammars built using
the matrix.
5 Case Studies
One important part of the matrix package will be a
library of phenomenon-based analyses drawn from
the existing grammars and over time from users of
the matrix, to provide working examples of how
the matrix can be applied and extended. Each case
study will be a set of grammar files, simplified for
relevance, along with documentation of the anal-
ysis, and a test suite of sample sentences which
define the range of data covered by the analysis.
This library, too, will be organized around the ty-
pology of phenomena introduced above, but will
also make explicit reference to language families,
since both similarities and differences among re-
lated languages will be of interest in these case
studies. Examples to be included in the first re-
lease of this library include numeral classifiers in
Japanese, subject pro drop in Spanish, partial-VP
fronting in German, and verb diathesis in Norwe-
gian.
6 Evaluation and Evolution
The matrix itself is not a grammar but a collec-
tion of generalizations across grammars. As such,
it cannot be tested directly on corpora from partic-
ular languages, and we must find other means of
evaluation. We envision overall evaluation of the
matrix based on case studies of its performance
in helping grammar engineers quickly start new
grammars and in helping them scale those gram-
mars up. Evaluation in detail will based on au-
tomatable deletion/substitution metrics, i.e., tools
that determine which types from the matrix get
used as is, which get used with modifications, and
which get ignored in various matrix-derived gram-
mars. Furthermore, if the matrix evolves to include
defeasible constraints, these tools will check which
constraints get overridden and whether the value
chosen is indeed common enough to be motivated
as a default value. This evaluation in detail should
be paired with feedback from the grammar engi-
neers to determine why changes were made.
The main goal of evaluation is, of course, to im-
prove the matrix over time. This raises the ques-
tion of how to propagate changes in the matrix to
grammars based on earlier versions. The following
three strategies (meant to be used in combination)
seem promising: (i) segregate changes that are im-
portant to sync to (e.g., changes that affect MRS
outputs, fundamental changes to important anal-
yses), (ii) develop a methodology for communi-
cating changes in the matrix, their motivation and
their implementation to the user community, and
(iii) develop tools for semi-automating resynching
of existing grammars to upgrades of the matrix.
These tools could use the type hierarchy to predict
where conflicts are likely to arise and bring these
to the engineer?s attention, possibly inspired by the
approach under development at CSLI for the dy-
namic maintenance of the LinGO Redwoods tree-
bank (Oepen et al, 2002).
Finally, while initial development of the ma-
trix has been and will continue to be highly cen-
tralized, we hope to provide support for proposed
matrix improvements from the user community.
User feedback will already come in the form of
case studies for the library as discussed in Sec-
tion 5 above, but also potentially in proposals for
modification of the matrix drawing on experiences
in grammar development. In order to provide
users with some cross-linguistic context in which
to develop and evaluate such proposals themselves,
we intend to provide some sample matrix-derived
grammars and corresponding testsuites with the
matrix. A user could thus make a proposed change
to the matrix, run the testsuites for several lan-
guages using the supplied grammars which draw
from that changed matrix, and use [incr tsdb()]
to determine which phenomena have been affected
by the change. It is clear that full automation of
this evaluation process will be difficult, but at least
some classes of changes to the matrix will per-
mit this kind of quick cross-linguistic feedback to
users with only a modest amount of additional in-
frastructure.
7 Conclusion
This project carries linguistic, computational, and
practical interest. The linguistic interest lies in the
HPSG community?s general bottom-up approach
to language universals, which involves aiming for
good coverage of a variety of languages first, and
leaving the task of what they have in common for
later. (Of course, theory building is never purely
data-driven, and there are substantive hypotheses
within HPSG about language universals.) Now
that we have implementations with fairly extensive
coverage for a somewhat typologically diverse set
of languages, it is a good time to take the next step
in this program, working to extract and generalize
what is similar across these existing wide-coverage
grammars. Moreover, the central role of types in
the representation of linguistic generalizations en-
ables the kind of underspecification which is useful
for expressing what is common among related lan-
guages while allowing for the further specializa-
tion which necessarily distinguishes one language
from another.
The computational interest is threefold. First
there is the question of what formal devices the
grammar matrix will require. Should it include
defaults? What about domain union (linearization
theory)? The selection and deployment of formal
devices should be informed by on-going research
on processing schemes, and here the crosslinguis-
tic perspective can be particularly helpful. Where
there are several equivalent analyses of the same
linguistic phenomena (e.g., morphosyntactic am-
biguity or optionality), the choice of analysis can
have processing implications that aren?t necessar-
ily apparent in a single grammar. Second, having
a set of wide-coverage HPSGs with fairly standard-
ized fundamentals could prove interesting for re-
search on stochastic processing and disambigua-
tion, especially if the languages differ in gross ty-
pological features such as word order. Finally,
there are also computational issues involved in
how the grammar matrix would evolve over time
as it is used in new grammars. The matrix en-
ables the developer of a grammar for a new lan-
guage to get a quick start on producing a system
that parses and generates with non-trivial seman-
tics, while also building the foundation for a wide-
coverage grammar of the language. But the matrix
itself may well change in parallel with the devel-
opment of the grammar for a particular language,
so appropriate mechanisms must be developed to
support the merging of enhancements to both.
There is also practical industrial benefit to this
project. Companies that are consumers of these
grammars benefit when grammars of multiple lan-
guages work with the same parsing and generation
algorithms and produce standardized semantic rep-
resentations derived from a rich, linguistically mo-
tivated syntax-semantics interface. More impor-
tantly, the grammar matrix will help to remove one
of the primary remaining obstacles to commercial
deployment of grammars of this type and indeed of
the commercial use of deep linguistic analysis: the
immense cost of developing the resource.
Acknowledgements
Since the grammar matrix draws on prior re-
search and existing grammars, it necessarily re-
flects contributions from many people. Rob
Malouf, Jeff Smith, John Beavers, and Kathryn
Campbell-Kibler have contributed to the LinGO
ERG; Melanie Siegel is the original developer for
the Japanese grammar. Tim Baldwin, Ann Copes-
take, Ivan Sag, Tom Wasow, and other members
of the LinGO Laboratory at CSLI have had a great
deal of influence on the design of the grammatical
analyses and corresponding MRS representations.
Warmest thanks to Lars Hellan and his colleagues
at NTNU and Jan Tore L?nning and his students
at Oslo University for their cooperation, patience,
and tolerance.
References
Alshawi, H. (Ed.). (1992). The Core Language Engine.
Cambridge, MA: MIT Press.
Asahara, M., & Matsumoto, Y. (2000). Extended mod-
els and tools for high-performance part-of-speech
tagger. In Proceedings of the 18th International
Conference on Computational Linguistics (pp. 21 ?
27). Saarbru?cken, Germany.
Butt, M., King, T. H., Nin?o, M.-E., & Segond, F.
(1999). A grammar writer?s cookbook. Stanford,
CA: CSLI Publications.
Callmeier, U. (2000). PET ? A platform for ex-
perimentation with efficient HPSG processing tech-
niques. Natural Language Engineering, 6 (1) (Spe-
cial Issue on Efficient Processing with HPSG), 99 ?
108.
Copestake, A. (2002). Implementing typed feature
structure grammars. Stanford, CA: CSLI Publica-
tions.
Copestake, A., Flickinger, D., Malouf, R., Riehemann,
S., & Sag, I. (1995). Translation using minimal re-
cursion semantics. In Proceedings of the Sixth In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation. Leuven,
Belgium.
Copestake, A., Flickinger, D. P., Sag, I. A., & Pol-
lard, C. (1999). Minimal Recursion Semantics. An
introduction. in preparation, CSLI Stanford, Stan-
ford, CA.
Copestake, A., Lascarides, A., & Flickinger, D. (2001).
An algebra for semantic construction in constraint-
based grammars. In Proceedings of the 39th Meet-
ing of the Association for Computational Linguistics.
Toulouse, France.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1) (Special Issue on Efficient Process-
ing with HPSG), 15 ? 28.
van Lohuizen, M. (2002). Efficient and thread-safe
unification with LinGO. In S. Oepen, D. Flickinger,
J. Tsujii, & H. Uszkoreit (Eds.), Collaborative
language engineering. A case study in efficient
grammar-based processing. Stanford, CA: CSLI
Publications. (forthcoming)
Makino, T., Yoshida, M., Torisawa, K., & Tsujii, J.
(1998). LiLFeS ? towards a practical HPSG parser.
In Proceedings of the 17th International Conference
on Computational Linguistics and the 36th Annual
Meeting of the Association for Computational Lin-
guistics (pp. 807 ? 11). Montreal, Canada.
Mu?ller, S. (1999). Deutsche syntax deklarativ. Head-
Driven Phrase Structure Grammar fu?r das Deutsche.
Tu?bingen, Germany: Max Niemeyer Verlag.
Mu?ller, S., & Kasper, W. (2000). HPSG analysis of
German. In W. Wahlster (Ed.), Verbmobil. Foun-
dations of speech-to-speech translation (Artificial
Intelligence ed., pp. 238 ? 253). Berlin, Germany:
Springer.
Netter, K. (1996). Functional categories in an HPSG
for German. Unpublished doctoral dissertation,
Saarland University, Saarbru?cken, Germany.
Oepen, S., & Callmeier, U. (2000). Measure for mea-
sure: Parser cross-fertilization. Towards increased
component comparability and exchange. In Pro-
ceedings of the 6th International Workshop on Pars-
ing Technologies (pp. 183 ? 194). Trento, Italy.
Oepen, S., & Flickinger, D. P. (1998). Towards sys-
tematic grammar profiling. Test suite technology ten
years after. Journal of Computer Speech and Lan-
guage, 12 (4) (Special Issue on Evaluation), 411 ?
436.
Oepen, S., Toutanova, K., Shieber, S., Manning, C.,
Flickinger, D., & Brants, T. (2002). The LinGO
Redwoods treebank. Motivation and preliminary ap-
plications. In Proceedings of the 19th International
Conference on Computational Linguistics. Taipei,
Taiwan.
Siegel, M. (2000). HPSG analysis of Japanese.
In W. Wahlster (Ed.), Verbmobil. Foundations of
speech-to-speech translation (Artificial Intelligence
ed., pp. 265 ? 280). Berlin, Germany: Springer.
Siegel, M., & Bender, E. M. (2002). Efficient deep
processing of japanese. In Proceedings of the 19th
International Conference on Computational Linguis-
tics. Taipei, Taiwan.
Uszkoreit, H., Backofen, R., Busemann, S., Diagne,
A. K., Hinkelman, E. A., Kasper, W., Kiefer, B.,
Krieger, H.-U., Netter, K., Neumann, G., Oepen, S.,
& Spackman, S. P. (1994). DISCO ? an HPSG-
based NLP system and its application for appoint-
ment scheduling. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics.
Kyoto, Japan.
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 10?17,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multilingual Ontology Acquisition from Multiple MRDs
Eric Nichols?, Francis Bond?, Takaaki Tanaka?, Sanae Fujita?, Dan Flickinger ?
? Nara Inst. of Science and Technology ? NTT Communication Science Labs ? Stanford University
Grad. School of Information Science Natural Language Research Group CSLI
Nara, Japan Keihanna, Japan Stanford, CA
eric-n@is.naist.jp {bond,takaaki,sanae}@cslab.kecl.ntt.co.jp danf@csli.stanford.edu
Abstract
In this paper, we outline the develop-
ment of a system that automatically con-
structs ontologies by extracting knowledge
from dictionary definition sentences us-
ing Robust Minimal Recursion Semantics
(RMRS). Combining deep and shallow
parsing resource through the common for-
malism of RMRS allows us to extract on-
tological relations in greater quantity and
quality than possible with any of the meth-
ods independently. Using this method,
we construct ontologies from two differ-
ent Japanese lexicons and one English lex-
icon. We then link them to existing, hand-
crafted ontologies, aligning them at the
word-sense level. This alignment provides
a representative evaluation of the qual-
ity of the relations being extracted. We
present the results of this ontology con-
struction and discuss how our system was
designed to handle multiple lexicons and
languages.
1 Introduction
Automatic methods of ontology acquisition have a
long history in the field of natural language pro-
cessing. The information contained in ontolo-
gies is important for a number of tasks, for ex-
ample word sense disambiguation, question an-
swering and machine translation. In this paper,
we present the results of experiments conducted
in automatic ontological acquisition over two lan-
guages, English and Japanese, and from three dif-
ferent machine-readable dictionaries.
Useful semantic relations can be extracted from
large corpora using relatively simple patterns (e.g.,
(Pantel et al, 2004)). While large corpora often
contain information not found in lexicons, even a
very large corpus may not include all the familiar
words of a language, let alne those words occur-
ring in useful patterns (Amano and Kondo, 1999).
Therefore it makes sense to also extract data from
machine readable dictionaries (MRDs).
There is a great deal of work on the creation
of ontologies from machine readable dictionaries
(a good summary is (Wilkes et al, 1996)), mainly
for English. Recently, there has also been inter-
est in Japanese (Tokunaga et al, 2001; Nichols
et al, 2005). Most approaches use either a special-
ized parser or a set of regular expressions tuned
to a particular dictionary, often with hundreds of
rules. Agirre et al (2000) extracted taxonomic
relations from a Basque dictionary with high ac-
curacy using Constraint Grammar together with
hand-crafted rules. However, such a system is lim-
ited to one language, and it has yet to be seen
how the rules will scale when deeper semantic re-
lations are extracted. In comparison, as we will
demonstrate, our system produces comparable re-
sults while the framework is immediately applica-
ble to any language with the resources to produce
RMRS. Advances in the state-of-the-art in pars-
ing have made it practical to use deep processing
systems that produce rich syntactic and semantic
analyses to parse lexicons. This high level of se-
mantic information makes it easy to identify the
relations between words that make up an ontol-
ogy. Such an approach was taken by the MindNet
project (Richardson et al, 1998). However, deep
parsing systems often suffer from small lexicons
and large amounts of parse ambiguity, making it
difficult to apply this knowledge broadly.
Our ontology extraction system uses Robust
Minimal Recursion Semantics (RMRS), a formal-
ism that provides a high level of detail while, at
the same time, allowing for the flexibility of un-
derspecification. RMRS encodes syntactic infor-
mation in a general enough manner to make pro-
cessing of and extraction from syntactic phenom-
ena including coordination, relative clause analy-
10
sis and the treatment of argument structure from
verbs and verbal nouns. It provides a common for-
mat for naming semantic relations, allowing them
to be generalized over languages. Because of this,
we are able to extend our system to cover new lan-
guages that have RMRS resourses available with
a minimal amount of effort. The underspecifica-
tion mechanism in RMRS makes it possible for us
to produce input that is compatible with our sys-
tem from a variety of different parsers. By select-
ing parsers of various different levels of robustness
and informativeness, we avoid the coverage prob-
lem that is classically associated with approaches
using deep-processing; using heterogeneous pars-
ing resources maximizes the quality and quantity
of ontological relations extracted. Currently, our
system uses input from parsers from three lev-
els: with morphological analyzers the shallowest,
parsers using Head-driven Phrase Structure Gram-
mars (HPSG) the deepest and dependency parsers
providing a middle ground.
Our system was initially developed for one
Japanese dictionary (Lexeed). The use of the ab-
stract formalism, RMRS, made it easy to extend to
a different Japanese lexicon (Iwanami) and even a
lexicon in a different language (GCIDE).
Section 2 provides a description of RMRS and
the tools used by our system. The ontological ac-
quisition system is presented in Section 3. The re-
sults of evaluating our ontologies by comparison
with existing resources are given in Section 4. We
discuss our findings in Section 5.
2 Resources
2.1 The Lexeed Semantic Database of
Japanese
The Lexeed Semantic Database of Japanese is a
machine readable dictionary that covers the most
familiar open class words in Japanese as measured
by a series of psycholinguistic experiments (Kasa-
hara et al, 2004). Lexeed consists of all open class
words with a familiarity greater than or equal to
five on a scale of one to seven. This gives 28,000
words divided into 46,000 senses and defined with
75,000 definition sentences. All definition sen-
tences and example sentences have been rewritten
to use only the 28,000 familiar open class words.
The definition and example sentences have been
treebanked with the JACY grammar (? 2.4.2).
2.2 The Iwanami Dictionary of Japanese
The Iwanami Kokugo Jiten (Iwanami) (Nishio
et al, 1994) is a concise Japanese dictionary.
A machine tractable version was made avail-
able by the Real World Computing Project for
the SENSEVAL-2 Japanese lexical task (Shirai,
2003). Iwanami has 60,321 headwords and 85,870
word senses. Each sense in the dictionary con-
sists of a sense ID and morphological information
(word segmentation, POS tag, base form and read-
ing, all manually post-edited).
2.3 The Gnu Contemporary International
Dictionary of English
The GNU Collaborative International Dictionary
of English (GCIDE) is a freely available dic-
tionary of English based on Webster?s Revised
Unabridged Dictionary (published in 1913), and
supplemented with entries from WordNet and ad-
ditional submissions from users. It currently
contains over 148,000 definitions. The version
used in this research is formatted in XML and is
available for download from www.ibiblio.org/
webster/.
We arranged the headwords by frequency and
segmented their definition sentences into sub-
sentences by tokenizing on semicolons (;). This
produced a total of 397,460 pairs of headwords
and sub-sentences, for an average of slightly less
than four sub-sentences per definition sentence.
For corpus data, we selected the first 100,000 def-
inition sub-sentences of the headwords with the
highest frequency. This subset of definition sen-
tences contains 12,440 headwords with 36,313
senses, covering approximately 25% of the defi-
nition sentences in the GCIDE. The GCIDE has
the most polysemy of the lexicons used in this re-
search. It averages over 3 senses per word defined
in comparison to Lexeed and Iwanami which both
have less than 2.
2.4 Parsing Resources
We used Robust Minimal Recursion Semantics
(RMRS) designed as part of the Deep Thought
project (Callmeier et al, 2004) as the formal-
ism for our ontological relation extraction en-
gine. We used deep-processing tools from the
Deep Linguistic Processing with HPSG Initiative
(DELPH-IN: http://www.delph-in.net/) as
well as medium- and shallow-processing tools for
Japanese processing (the morphological analyzer
11
ChaSen and the dependency parser CaboCha)
from the Matsumoto Laboratory.
2.4.1 Robust Minimal Recursion Semantics
Robust Minimal Recursion Semantics is a form
of flat semantics which is designed to allow deep
and shallow processing to use a compatible se-
mantic representation, with fine-grained atomic
components of semantic content so shallow meth-
ods can contribute just what they know, yet with
enough expressive power for rich semantic content
including generalized quantifiers (Frank, 2004).
The architecture of the representation is based on
Minimal Recursion Semantics (Copestake et al,
2005), including a bag of labeled elementary pred-
icates (EPs) and their arguments, a list of scoping
constraints which enable scope underspecification,
and a handle that provides a hook into the repre-
sentation.
The representation can be underspecified in
three ways: relationships can be omitted (such
as quantifiers, messages, conjunctions and so on);
predicate-argument relations can be omitted; and
predicate names can be simplified. Predicate
names are defined in such a way as to be as
compatible (predictable) as possible among differ-
ent analysis engines, using a lemma pos subsense
naming convention, where the subsense is optional
and the part-of-speech (pos) for coarse-grained
sense distinctions is drawn from a small set of gen-
eral types (noun, verb, sahen (verbal noun), . . . ).
The predicate unten s (?U unten ?drive?), for
example, is less specific than unten s 2 and thus
subsumes it. In order to simplify the combination
of different analyses, the EPs are indexed to the
corresponding character positions in the original
input sentence.
Examples of deep and shallow results for the
same sentence ?k?U2d0 jido?sha wo
unten suru hito ?a person who drives a car (lit:
car-ACC drive do person)? are given in Figures 1
and 2 (omitting the indexing). Real predicates are
prefixed by an under-bar ( ). The deep parse gives
information about the scope, message types and
argument structure, while the shallow parse gives
little more than a list of real and grammatical pred-
icates with a hook.
2.4.2 Deep Parsers (JACY, ERG and PET)
For both Japanese and English, we used the PET
System for the high-efficiency processing of typed
feature structures (Callmeier, 2000). For Japanese,
we used JACY (Siegel, 2000), for English we used
the English Resource Grammar (ERG: Flickinger
2000).1
JACY The JACY grammar is an HPSG-based
grammar of Japanese which originates from work
done in the Verbmobil project (Siegel, 2000) on
machine translation of spoken dialogues in the do-
main of travel planning. It has since been ex-
tended to accommodate written Japanese and new
domains (such as electronic commerce customer
email and machine readable dictionaries).
The grammar implementation is based on a sys-
tem of types. There are around 900 lexical types
that define the syntactic, semantic and pragmatic
properties of the Japanese words, and 188 types
that define the properties of phrases and lexical
rules. The grammar includes 50 lexical rules
for inflectional and derivational morphology and
47 phrase structure rules. The lexicon contains
around 36,000 lexemes.
The English Resource Grammar (ERG) The
English Resource Grammar (ERG: (Flickinger,
2000)) is a broad-coverage, linguistically precise
grammar of English, developed within the Head-
driven Phrase Structure Grammar (HPSG) frame-
work, and designed for both parsing and gen-
eration. It was also originally launched within
the Verbmobil (Wahlster, 2000) spoken language
machine translation project for the particular do-
mains of meeting scheduling and travel planning.
The ERG has since been substantially extended in
both grammatical and lexical coverage, reaching
80-90% coverage of sizeable corpora in two ad-
ditional domains: electronic commerce customer
email and tourism brochures.
The grammar includes a hand-built lexicon of
23,000 lemmas instantiating 850 lexical types, a
highly schematic set of 150 grammar rules, and a
set of 40 lexical rules, all organized in a rich multi-
ple inheritance hierarchy of some 3000 typed fea-
ture structures. Like other DELPH-IN grammars,
the ERG can be processed by several parsers and
generators, including the LKB (Copestake, 2002)
and PET (Callmeier, 2000). Each successful ERG
analysis of a sentence or fragment includes a fine-
grained semantic representation in MRS.
For the task of parsing the dictionary defini-
tions in GCIDE (the GNU Collaborative Interna-
1Both grammars, the LKB and PET are available at
<http://www.delph-in.net/>.
12
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TEXT ?k?U2d0
TOP h1
RELS
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
proposition m rel
LBL h1
ARG0 e2 tense=present
MARG h3
?
?
?
?
?
jidousha n rel
LBL h4
ARG0 x5
?
?
?
?
?
?
?
udef rel
LBL h6
ARG0 x5
RSTR h7
BODY h8
?
?
?
?
?
?
?
?
?
?
unten s rel
LBL h9
ARG0 e11 tense=present
ARG1 x10
ARG2 x5
?
?
?
?
?
?
?
hito n rel
LBL h12
ARG0 x10
?
?
?
?
?
?
?
udef rel
LBL h13
ARG0 x10
RSTR h14
BODY h15
?
?
?
?
?
?
?
?
proposition m rel
LBL h10001
ARG0 e11 tense=present
MARG h16
?
?
?
?
?
?
unknown rel
LBL h17
ARG0 e2 tense=present
ARG x10
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
HCONS {h3 qeq h17,h7 qeq h4,h14 qeq h12,h16 qeq h9}
ING {h12 ing h10001}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: RMRS for the Sense 2 of doraiba- ?driver? (Cabocha/JACY)
?
?
?
?
?
TEXT ?k?U2d0
TOP h9
RELS
?
?
?
?
?
jidousha n rel
LBL h1
ARG0 x2
?
?
?
?
o p rel
LBL h3
ARG0 u4
?
?
?
?
unten s rel
LBL h5
ARG0 e6
?
?
?
?
suru v rel
LBL h7
ARG0 x8
?
?
?
?
hito n rel
LBL h9
ARG0 x10
?
?
?
?
?
?
?
?
?
?
Figure 2: RMRS for the Sense 2 of doraiba- ?driver? (ChaSen)
tional Dictionary of English; see below), the ERG
was minimally extended to include two additional
fragment rules, for gap-containing VPs and PPs
(idiosyncratic to this domain), and additional lex-
ical entries were manually added for all missing
words in the alphabetically first 10,000 definition
sentences.
These first 10,000 sentences were parsed and
then manually tree-banked to provide the train-
ing material for constructing the stochastic model
used for best-only parsing of the rest of the defini-
tion sentences. Using POS-based unknown-word
guessing for missing lexical entries, MRSes were
obtained for about 75% of the first 100,000 defini-
tion sentences.
2.4.3 Medium Parser (CaboCha-RMRS)
For Japanese, we produce RMRS from the de-
pendency parser Cabocha (Kudo and Matsumoto,
2002). The method is similar to that of Spreyer
and Frank (2005), who produce RMRS from de-
tailed German dependencies. CaboCha provides
fairly minimal dependencies: there are three links
(dependent, parallel, apposition) and they link
base phrases (Japanese bunsetsu), marked with
the syntactic and semantic head. The CaboCha-
RMRS parser uses this information, along with
heuristics based on the parts-of-speech, to produce
underspecified RMRSs. CaboCha-RMRS is ca-
pable of making use of HPSG resources, includ-
ing verbal case frames, to further enrich its out-
put. This allows it to produce RMRS that ap-
proaches the granularity of the analyses given by
HPSG parsers. Indeed, CaboCha-RMRS and JACY
give identical parses for the example sentence in
Figure 1. One of our motivations in including a
medium parser in our system is to extract more re-
lations that require special processing; the flexibil-
ity of CaboCha-RMRS and the RMRS formalism
make this possible.
2.4.4 Shallow Parser (ChaSen-RMRS)
The part-of-speech tagger, ChaSen (Matsumoto
et al, 2000) was used for shallow processing of
Japanese. Predicate names were produced by
transliterating the pronunciation field and map-
ping the part-of-speech codes to the RMRS super
types. The part-of-speech codes were also used
to judge whether predicates were real or gram-
matical. Since Japanese is a head-final language,
the hook value was set to be the handle of the
right-most real predicate. This is easy to do for
Japanese, but difficult for English.
3 Ontology Construction
We adopt the ontological relation extraction algo-
rithm used by Nichols et al (2005). Its goal is to
identify the semantic head(s) of a dictionary def-
inition sentence ? the relation(s) that best sum-
marize it. The algorithm does this by traversing
the RMRS structure of a given definition sentence
starting at the HOOK (the highest-scoping seman-
tic relationship) and following its argument struc-
ture. When the algorithm can proceed no fur-
ther, it returns the a tuple consisting of the def-
inition word and the word identified by the se-
13
mantic relation where the algorithm halted. Our
extended algorithm has the following characteris-
tics: sentences with only one content-bearing re-
lation are assumed to identify a synonym; spe-
cial relation processing (? 3.1) is used to gather
meta-information and identify ontological rela-
tions; processing of coordination allows for ex-
traction of multiple ontological relations; filtering
by part-of-speech screens out unlikely relations
(? 3.2).
3.1 Special Relations
Occasionally, relations which provide ontological
meta-information, such as the specification of do-
main or temporal expressions, or which help iden-
tify the type of ontological relation present are en-
countered. Nichols et al (2005) identified these
as special relations. We use a small number of
rules to determine where the semantic head is and
what ontological relation should be extracted. A
sample of the special relations are listed in Ta-
ble 1. This technique follows in a long tradition of
special treatment of certain words that have been
shown to be particularly relevant to the task of
ontology construction or which are semantically
content-free. These words or relations have also
be referred to as ?empty heads?, ?function nouns?,
or ?relators? in the literature (Wilkes et al, 1996).
Our approach generalizes the treatment of these
special relations to rules that are portable for any
RMRS (modulo the language specific predicate
names) giving it portability that cannot be found
in approaches that use regular expressions or spe-
cialized parsers.
Special Predicate (s) Ontological
Japanese English Relation
isshu, hitotsu form, kind, one hypernym
ryaku(shou) abbreviation abbreviation
bubun, ichibu part, peice meronym
meishou name name
keishou ?polite name for? name:honorific
zokushou ?slang for? name:slang
Table 1: Special predicates and their associated
ontological relations
Augmenting the system to work on English def-
inition sentence simply entailed writing rules to
handle special relations that occur in English. Our
system currently has 26 rules for Japanese and 50
rules for English. These rules provide process-
ing of relations like those found in Table 1, and
they also handle processing of coordinate struc-
tures, such as noun phrases joined together with
conjunctions such as and, or, and punctuation.
3.2 Filtering by Part-of-Speech
One of the problems encountered in expanding the
approach in Nichols et al (2005) to handle En-
glish dictionaries is that many of the definition
sentences have a semantic head with a part-of-
speech different than that of the definition word.
We found that differing parts-of-speech often indi-
cated an undesirable ontological relation. One rea-
son such relations can be extracted is when a sen-
tence with a non-defining role, for example indi-
cating usage, is encountered. Definition sentence
for non-content-bearing words such as of or the
also pose problems for extraction.
We avoid these problems by filtering by parts-
of-speech twice in the extraction process. First, we
select candidate sentences for extraction by veri-
fying that the definition word has a content word
POS (i.e. adjective, adverb, noun, or verb). Fi-
nally, before we extract any ontological relation,
we make sure that the definition word and the se-
mantic head are in compatible POS classes.
While adopting this strategy does reduce the
number of total ontological relations that we ac-
quire, it increases their reliability. The addition of
a medium parser gives us more RMRS structures
to extract from, which helps compensate for any
loss in number.
4 Results and Evaluation
We summarize the relationships acquired in Ta-
ble 2. The columns specify source dictionary
and parsing method while the rows show the rela-
tion type. These counts represent the total num-
ber of relations extracted for each source and
method combination. The majority of relations
extracted are synonyms and hypernyms; however,
some higher-level relations such as meronym and
abbreviation are also acquired. It should also
be noted that both the medium and deep meth-
ods were able to extract a fair number of spe-
cial relations. In many cases, the medium method
even extracted more special relations than the deep
method. This is yet another indication of the
flexibility of dependency parsing. Altogether, we
extracted 105,613 unique relations from Lexeed
(for 46,000 senses), 183,927 unique relations from
Iwanami (for 85,870 senses), and 65,593 unique
relations from GCIDE (for 36,313 senses). As can
be expected, a general pattern in our results is that
the shallow method extracts the most relations in
total followed by the medium method, and finally
14
Relation Lexeed Iwanami GCIDE
Shallow Medium Deep Shallow Medium Deep Deep
hypernym 47,549 43,006 41,553 113,120 113,433 66,713 40,583
synonym 12,692 13,126 9,114 31,682 32,261 18,080 21,643
abbreviation 340 429 1,533 739
meronym 235 189 395 202 472
name 100 89 271 140
Table 2: Results of Ontology Extraction
the deep method.
4.1 Verification with Hand-crafted
Ontologies
Because we are interested in comparing lexical se-
mantics across languages, we compared the ex-
tracted ontology with resources in both the same
and different languages.
For Japanese we verified our results by com-
paring the hypernym links to the manually con-
structed Japanese ontology Goi-Taikei (GT). It is
a hierarchy of 2,710 semantic classes, defined for
over 264,312 nouns Ikehara et al (1997). The se-
mantic classes are mostly defined for nouns (and
verbal nouns), although there is some information
for verbs and adjectives. For English, we com-
pared relations to WordNet 2.0 (Fellbaum, 1998).
Comparison for hypernyms is done as follows:
look up the semantic class or synset C for both the
headword (wi) and genus term(s) (wg). If at least
one of the index word?s classes is subsumed by at
least one of the genus? classes, then we consider
the relationship confirmed (1).
?(ch,cg) : {ch ? cg;ch ?C(wh);cg ?C(wg)} (1)
To test cross-linguistically, we looked up the
headwords in a translation lexicon (ALT-J/E (Ike-
hara et al, 1991) and EDICT (Breen, 2004)) and
then did the confirmation on the set of translations
ci ? C(T (wi)). Although looking up the transla-
tion adds noise, the additional filter of the relation-
ship triple effectively filters it out again.
The total figures given in Table 3 do not match
the totals given in Table 2. These totals represent
the number of relations where both the definition
word and semantic head were found in at least one
of the ontologies being used in this comparison.
By comparing these numbers to the totals given
in Section 4, we can get an idea of the coverage
of the ontologies being used in comparison. Lex-
eed has a coverage of approx. 55.74% ( 58,867105,613 ),
with Iwanami the lowest at 48.20% ( 88,662183,927 ), and
GCIDE the highest at 69.85% (45,81465,593 ). It is clear
that there are a lot of relations in each lexicon that
are not covered by the hand-crafted ontologies.
This demonstrates that machine-readable dictio-
naries are still a valuable resource for constructing
ontologies.
4.1.1 Lexeed
Our results using JACY achieve a confirmation
rate of 66.84% for nouns only and 60.67% over-
all (Table 3). This is an improvement over both
Tokunaga et al (2001), who reported 61.4% for
nouns only, and Nichols et al (2005) who reported
63.31% for nouns and 57.74% overall. We also
achieve an impressive 33,333 confirmed relations
for a rate of 56.62% overall. It is important to
note that our total counts include all unique re-
lations regardless of source, unlike Nichols et al
(2005) who take only the relation from the deepest
source whenever multiple relations are extracted.
It is interesting to note that shallow processing out
performs medium with 22,540 verified relations
(59.40%) compared to 21,806 (57.76%). This
would seem to suggest that for the simplest task of
retrieving hyperynms and synonyms, more infor-
mation than that is not necessary. However, since
medium and deep parsing obtain relations not cov-
ered by shallow parsing and can extract special re-
lations, a task that cannot be performed without
syntactic information, it is beneficial to use them
as well.
Agirre et al (2000) reported an error rate of
2.8% in a hand-evaluation of the semantic rela-
tions they automatically extracted from a machine-
readable Basque dictionary. In a similar hand-
evaluation of a stratified sampling of relations ex-
tracted from Lexeed, we achieved an error rate
of 9.2%, demonstrating that our method is also
highly accurate (Nichols et al, 2005).
4.2 Iwanami
Iwanami?s verification results are similar to Lex-
eed?s (Table 3). There are on average around 3%
more verifications and a total of almost 20,000
more verified relations extracted. It is particu-
larly interesting to note that deep processing per-
15
Confirmed Relations in Lexeed
Method / Relation hypernym synonym Total
Shallow 58.55 % ( 16585 / 28328 ) 61.93 % ( 5955 / 9615 ) 59.40 % ( 22540 / 37943 )
Medium 55.97 % ( 15431 / 27570 ) 62.61 % ( 6375 / 10182 ) 57.76 % ( 21806 / 37752 )
Deep 54.78 % ( 4954 / 9043 ) 67.76 % ( 5098 / 7524 ) 60.67 % ( 10052 / 16567 )
All 55.22 % ( 23802 / 43102 ) 60.46 % ( 9531 / 15765 ) 56.62 % ( 33333 / 58867 )
Confirmed Relations in Iwanami
Method / Relation hypernym synonym Total
Shallow 61.20 % ( 35208 / 57533 ) 63.57 % ( 11362 / 17872 ) 61.76 % ( 46570 / 75405 )
Medium 60.69 % ( 35621 / 58698 ) 62.86 % ( 11037 / 17557 ) 61.19 % ( 46658 / 76255 )
Deep 63.59 % ( 22936 / 36068 ) 64.44 % ( 8395 / 13027 ) 63.82 % ( 31331 / 49095 )
All 59.36 % ( 40179 / 67689 ) 61.66 % ( 12931 / 20973 ) 59.90 % ( 53110 / 88662 )
Confirmed Relations in GCIDE
POS / Relation hypernym synonym Total
Adjective 2.88 % ( 37 / 1283 ) 16.77 % ( 705 / 4203 ) 13.53 % ( 742 / 5486 )
Noun 57.60 % ( 7518 / 13053 ) 50.71 % ( 3522 / 6945 ) 55.21 % ( 11040 / 19998 )
Verb 24.22 % ( 3006 / 12411 ) 21.40 % ( 1695 / 7919 ) 23.12 % ( 4701 / 20330 )
Total 39.48 % ( 10561 / 26747 ) 31.06 % ( 5922 / 19067 ) 35.98 % ( 16483 / 45814 )
Table 3: Confirmed Relations, measured against GT and WordNet
forms better here than on Lexeed (63.82% vs
60.67%), even though the grammar was developed
and tested on Lexeed. There are two reasons for
this: The first is that the process of rewriting Lex-
eed to use only familiar words actually makes the
sentences harder to parse. The second is that the
less familiar words in Iwanami have fewer senses,
and easier to parse definition sentences. In any
case, the results support our claims that our onto-
logical relation extraction system is easily adapt-
able to new lexicons.
4.3 GCIDE
At first glance, it would seem that GCIDE has
the most disappointing of the verification results
with overall verification of not even 36% and only
16,483 relations confirmed. However, on closer
inspection one can see that noun hypernyms are a
respectable 57.60% with over 55% for all nouns.
These figures are comparable with the results we
are obtaining with the other lexicons. One should
also bear in mind that the definitions found in
GCIDE can be archaic; after all this dictionary
was first published in 1913. This could be one
cause of parsing errors for ERG. Despite these ob-
stacles, we feel that GCIDE has a lot of poten-
tial for ontological acquisition. A dictionary of
its size and coverage will most likely contain rela-
tions that may not be represented in other sources.
One only has to look at the definition of ??
{? ?driver?/driver to confirm this; GT has
two senses (?screwdriver? and ?vehicle operator?)
Lexeed and Iwanami have 3 senses each (adding
?golf club?), and WordNet has 5 (including ?soft-
ware driver?), but GCIDE has 6, not including
?software driver? but including spanker ?a kind of
sail?. It should be beneficial to propagate these
different senses across ontologies.
5 Discussion and Future Work
We were able to successfully combine deep pro-
cessing of various levels of depth in order to
extract ontological information from lexical re-
sources. We showed that, by using a well defined
semantic representation, the extraction can be gen-
eralized so much that it can be used on very differ-
ent dictionaries from different languages. This is
an improvement on the common approach to using
more and more detailed regular expressions (e.g.
Tokunaga et al (2001)). Although this provides a
quick start, the results are not generally reusable.
In comparison, the shallower RMRS engines are
immediately useful for a variety of other tasks.
However, because the hook is the only syntactic
information returned by the shallow parser, onto-
logical relation extraction is essentially performed
by this hook-identifying heuristic. While this is
sufficient for a large number of sentences, it is not
possible to process special relations with the shal-
low parser since none of the arguments are linked
with the predicates to which they belong. Thus, as
Table 2 shows, our shallow parser is only capable
of retrieving hypernyms and synonyms. It is im-
portant to extract a variety of semantic relations in
order to form a useful ontology. This is one of the
reasons why we use a combination of parsers of
16
different analytic levels rather than depending on
a single resource.
The other innovation of our approach is the
cross-lingual evaluation. As a by-product of
the evaluation we enhance the existing resources
(such as GT or WordNet) by linking them, so
that information can be shared between them. In
this way we can use the cross-lingual links to fill
gaps in the monolingual resources. GT and Word-
Net both lack complete cover - over half the rela-
tions were confirmed with only one resource. This
shows that the machine readable dictionary is a
useful source of these relations.
6 Conclusion
In this paper, we presented the results of experi-
ments conducted in automatic ontological acqui-
sition over two languages, English and Japanese,
and from three different machine-readable dictio-
naries. Our system is unique in combining parsers
of various levels of analysis to generate its input
semantic structures. The system is language ag-
nostic and we give results for both Japanese and
English MRDs. Finally, we presented evaluation
of the ontologies constructed by comparing them
with existing hand-crafted English and Japanese
ontologies.
References
Eneko Agirre, Olatz Ansa, Xabier Arregi, Xabier Artola,
Arantza Diaz de Ilarraza, Mikel Lersundi, David Martinez,
Kepa Sarasola, and Ruben Urizar. 2000. Extraction of
semantic relations from a Basque monolingual dictionary
using Constraint Grammar. In EURALEX 2000.
Shigeaki Amano and Tadahisa Kondo. 1999. Nihongo-no
Goi-Tokusei (Lexical properties of Japanese). Sanseido.
J. W. Breen. 2004. JMDict: a Japanese-multilingual dictio-
nary. In Coling 2004 Workshop on Multilingual Linguistic
Resources, pages 71?78. Geneva.
Ulrich Callmeier. 2000. PET - a platform for experimenta-
tion with efficient HPSG processing techniques. Natural
Language Engineering, 6(1):99?108.
Ulrich Callmeier, Andreas Eisele, Ulrich Scha?fer, and
Melanie Siegel. 2004. The DeepThought core architecture
framework. In Proceedings of LREC-2004, volume IV.
Lisbon.
Ann Copestake. 2002. Implementing Typed Feature Structure
Grammars. CSLI Publications.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A.
Sag. 2005. Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4):281?
332.
Christine Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Dan Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15?28. (Special Issue on Efficient Processing with
HPSG).
Anette Frank. 2004. Constraint-based RMRS construction
from shallow grammars. In 20th International Con-
ference on Computational Linguistics: COLING-2004,
pages 1269?1272. Geneva.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
? effects of new methods in ALT-J/E ?. In Third Ma-
chine Translation Summit: MT Summit III, pages 101?
106. Washington DC. (http://xxx.lanl.gov/abs/
cmp-lg/9510008).
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lex-
icon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in Japanese).
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In CoNLL 2002:
Proceedings of the 6th Conference on Natural Language
Learning 2002 (COLING 2002 Post-Conference Work-
shops), pages 63?69. Taipei.
Yuji Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda,
and Asahara. 2000. Nihongo Keitaiso Kaiseki System:
Chasen. http://chasen.naist.jp/hiki/ChaSen/.
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictio-
naries. In Proceedings of the International Joint Confer-
ence on Artificial Intelligence IJCAI-2005, pages 1111?
1116. Edinburgh.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.
1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami
Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo.
(in Japanese).
Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.
2004. Towards terascale knowledge acquisition. In 20th
International Conference on Computational Linguistics:
COLING-2004, pages 771?777. Geneva.
Stephen D. Richardson, William B. Dolan, and Lucy Van-
derwende. 1998. MindNet: acquiring and structuring se-
mantic information from text. In 36th Annual Meeting
of the Association for Computational Linguistics and 17th
International Conference on Computational Linguistics:
COLING/ACL-98, pages 1098?1102. Montreal.
Kiyoaki Shirai. 2003. SENSEVAL-2 Japanese dictionary
task. Journal of Natural Language Processing, 10(3):3?
24. (in Japanese).
Melanie Siegel. 2000. HPSG analysis of Japanese. In
Wahlster (2000), pages 265?280.
Kathrin Spreyer and Anette Frank. 2005. The TIGER RMRS
700 bank: RMRS construction from dependencies. In Pro-
ceedings of the 6th International Workshop on Linguisti-
cally Interpreted Corpora (LINC 2005), pages 1?10. Jeju
Island, Korea.
Takenobu Tokunaga, Yasuhiro Syotu, Hozumi Tanaka, and
Kiyoaki Shirai. 2001. Integration of heterogeneous lan-
guage resources: A monolingual dictionary and a the-
saurus. In Proceedings of the 6th Natural Language Pro-
cessing Pacific Rim Symposium, NLPRS2001, pages 135?
142. Tokyo.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer, Berlin, Germany.
Yorick A. Wilkes, Brian M. Slator, and Louise M. Guthrie.
1996. Electric Words. MIT Press.
17
An Algebra for Semantic Construction in Constraint-based Grammars
Ann Copestake
Computer Laboratory
University of Cambridge
New Museums Site
Pembroke St, Cambridge, UK
aac@cl.cam.ac.uk
Alex Lascarides
Division of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, Scotland, UK
alex@cogsci.ed.ac.uk
Dan Flickinger
CSLI, Stanford University and
YY Software
Ventura Hall, 220 Panama St
Stanford, CA 94305, USA
danf@csli.stanford.edu
Abstract
We develop a framework for formaliz-
ing semantic construction within gram-
mars expressed in typed feature struc-
ture logics, including HPSG. The ap-
proach provides an alternative to the
lambda calculus; it maintains much of
the desirable flexibility of unification-
based approaches to composition, while
constraining the allowable operations in
order to capture basic generalizations
and improve maintainability.
1 Introduction
Some constraint-based grammar formalisms in-
corporate both syntactic and semantic representa-
tions within the same structure. For instance, Fig-
ure 1 shows representations of typed feature struc-
tures (TFSs) for Kim, sleeps and the phrase Kim
sleeps, in an HPSG-like representation, loosely
based on Sag and Wasow (1999). The semantic
representation expressed is intended to be equiv-
alent to r name(x,Kim) ? sleep(e, x).1 Note:
1. Variable equivalence is represented by coin-
dexation within a TFS.
2. The coindexation in Kim sleeps is achieved
as an effect of instantiating the SUBJ slot in
the sign for sleeps.
3. Structures representing individual predicate
applications (henceforth, elementary predi-
cations, or EPs) are accumulated by an ap-
pend operation. Conjunction of EPs is im-
plicit.
1The variables are free, we will discuss scopal relation-
ships and quantifiers below.
4. All signs have an index functioning some-
what like a ?-variable.
A similar approach has been used in a large
number of implemented grammars (see Shieber
(1986) for a fairly early example). It is in many
ways easier to work with than ?-calculus based
approaches (which we discuss further below) and
has the great advantage of allowing generaliza-
tions about the syntax-semantics interface to be
easily expressed. But there are problems. The
operations are only specified in terms of the TFS
logic: the interpretation relies on an intuitive cor-
respondence with a conventional logical represen-
tation, but this is not spelled out. Furthermore
the operations on the semantics are not tightly
specified or constrained. For instance, although
HPSG has the Semantics Principle (Pollard and
Sag, 1994) this does not stop the composition pro-
cess accessing arbitrary pieces of structure, so it
is often not easy to conceptually disentangle the
syntax and semantics in an HPSG. Nothing guar-
antees that the grammar is monotonic, by which
we mean that in each rule application the seman-
tic content of each daughter subsumes some por-
tion of the semantic content of the mother (i.e.,
no semantic information is dropped during com-
position): this makes it impossible to guarantee
that certain generation algorithms will work ef-
fectively. Finally, from a theoretical perspective,
it seems clear that substantive generalizations are
being missed.
Minimal Recursion Semantics (MRS: Copes-
take et al(1999), see also Egg (1998)) tight-
ens up the specification of composition a little.
It enforces monotonic accumulation of EPs by
making all rules append the EPs of their daugh-
ters (an approach which was followed by Sag
and Wasow (1999)) but it does not fully spec-
Kim
?
?
?
?
?
?
?
?
?
SYN
?
?
np
HEAD noun
SUBJ< >
COMPS< >
?
?
SEM
?
?
?
INDEX 5 ref-ind
RESTR<
[
RELN R NAME
INSTANCE 5
NAME KIM
]
>
?
?
?
?
?
?
?
?
?
?
?
?
sleeps
?
?
?
?
?
?
?
?
?
?
?
?
SYN
?
?
?
?
HEAD verb
SUBJ<
[
SYN np
SEM
[
INDEX 6
RESTR 7
]
]
>
COMPS< >
?
?
?
?
SEM
?
?
?
INDEX 15 event
RESTR<
[
RELN SLEEP
SIT 15
ACT 6
]
>
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Kim sleeps
?
?
?
?
?
?
?
?
?
?
SYN
[
HEAD 0 verb
]
SEM
?
?
?
INDEX 2 event
RESTR 10 <
[
RELN R NAME
INSTANCE 4
NAME KIM
]
> ? 11 <
[
RELN SLEEP
SIT 2 event
ACT 4
]
>
?
?
?
HEAD-DTR.SEM
[
INDEX 2
RESTR 10
]
NON-HD-DTR.SEM.RESTR 11
?
?
?
?
?
?
?
?
?
?
Figure 1: Expressing semantics in TFSs
ify compositional principles and does not for-
malize composition. We attempt to rectify these
problems, by developing an algebra which gives
a general way of expressing composition. The
semantic algebra lets us specify the allowable
operations in a less cumbersome notation than
TFSs and abstracts away from the specific fea-
ture architecture used in individual grammars, but
the essential features of the algebra can be en-
coded in the hierarchy of lexical and construc-
tional type constraints. Our work actually started
as an attempt at rational reconstruction of se-
mantic composition in the large grammar imple-
mented by the LinGO project at CSLI (available
via http://lingo.stanford.edu). Se-
mantics and the syntax/semantics interface have
accounted for approximately nine-tenths of the
development time of the English Resource Gram-
mar (ERG), largely because the account of seman-
tics within HPSG is so underdetermined.
In this paper, we begin by giving a formal ac-
count of a very simplified form of the algebra and
in ?3, we consider its interpretation. In ?4 to ?6,
we generalize to the full algebra needed to capture
the use of MRS in the LinGO English Resource
Grammar (ERG). Finally we conclude with some
comparisons to the ?-calculus and to other work
on unification based grammar.
2 A simple semantic algebra
The following shows the equivalents of the struc-
tures in Figure 1 in our algebra:
Kim: [x2]{[]subj , []comp}[r name(x2,Kim)]{}
sleeps: [e1]{[x1]subj , []comp}[sleep(e1, x1)]{}
Kim sleeps: [e1]{[]subj , []comp}[sleep(e1, x1),
r name(x2,Kim)]{x1 = x2}
The last structure is semantically equivalent to:
[sleep(e1, x1), r name(x1,Kim)].
In the structure for sleeps, the first part, [e1], is
a hook and the second part ([x1]subj and []comp)
is the holes. The third element (the lzt) is a bag
of elementary predications (EPs).2 Intuitively, the
hook is a record of the value in the semantic en-
tity that can be used to fill a hole in another entity
during composition. The holes record gaps in the
semantic form which occur because it represents
a syntactically unsaturated structure. Some struc-
tures have no holes, such as that for Kim. When
structures are composed, a hole in one structure
(the semantic head) is filled with the hook of the
other (by equating the variables) and their lzts are
appended. It should be intuitively obvious that
there is a straightforward relationship between
this algebra and the TFSs shown in Figure 1, al-
though there are other TFS architectures which
would share the same encoding.
We now give a formal description of the alge-
bra. In this section, we simplify by assuming that
each entity has only one hole, which is unlabelled,
and only consider two sorts of variables: events
and individuals. The set of semantic entities is
built from the following vocabulary:
2As usual in MRS, this is a bag rather than a set because
we do not want to have to check for/disallow repeated EPs;
e.g., big big car.
1. The absurdity symbol ?.
2. indices i1, i2, . . ., consisting of two subtypes
of indices: events e1, e2, . . . and individuals
x1, x2, . . ..
3. n-place predicates, which take indices as ar-
guments
4. =.
Equality can only be used to identify variables of
compatible sorts: e.g., x1 = x2 is well formed,
but e = x is not. Sort compatibility corresponds
to unifiability in the TFS logic.
Definition 1 Simple Elementary Predications
(SEP)
An SEP contains two components:
1. A relation symbol
2. A list of zero or more ordinary variable ar-
guments of the relation (i.e., indices)
This is written relation(arg1, . . . ,argn). For in-
stance, like(e, x, y) is a well-formed SEP.
Equality Conditions: Where i1 and i2 are in-
dices, i1 = i2 is an equality condition.
Definition 2 The Set ? of Simple semantic Enti-
ties (SSEMENT)
s ? ? if and only if s = ? or s = ?s1, s2, s3, s4?
such that:
? s1 = {[i]} is a hook;
? s2 = ? or {[i?]} is a hole;
? s3 is a bag of SEPs(the lzt)
? s4 is a set of equalities between variables
(the eqs).
We write a SSEMENT as: [i1][i2][SEPs]{EQs}.
Note for convenience we omit the set markers {}
from the hook and hole when there is no possible
confusion. The SEPs, and EQs are (partial) de-
scriptions of the fully specified formulae of first
order logic.
Definition 3 The Semantic Algebra
A Semantic Algebra defined on vocabulary V is
the algebra ??, op? where:
? ? is the set of SSEMENTs defined on the vo-
cabulary V , as given above;
? op : ? ? ? ?? ? is the operation of se-
mantic composition. It satisfies the follow-
ing conditions. If a1 = ? or a2 = ? or
hole(a2) = ?, then op(a1, a2) = ?. Other-
wise:
1. hook(op(a1, a2)) = hook(a2)
2. hole(op(a1, a2)) = hole(a1)
3. lzt(op(a1, a2)) = lzt(a1) ? lzt(a2)
4. eq(op(a1, a2)) = Tr(eq(a1)?eq(a2)?
hook(a1) = hole(a2)})
where Tr stands for transitive closure
(i.e., if S = {x = y, y = z}, then
Tr(S) = {x = y, y = z, x = z}).
This definition makes a2 the equivalent of a se-
mantic functor and a1 its argument.
Theorem 1 op is a function
If a1 = a3 and a2 = a4, then a5 = op(a1, a2) =
op(a3, a4) = a6. Thus op is a function. Further-
more, the range of op is within ?. So ??, op? is
an algebra.
We can assume that semantic composition al-
ways involves two arguments, since we can de-
fine composition in ternary rules etc as a sequence
of binary operations. Grammar rules (i.e., con-
structions) may contribute semantic information,
but we assume that this information obeys all the
same constraints as the semantics for a sign, so
in effect such a rule is semantically equivalent to
having null elements in the grammar. The corre-
spondence between the order of the arguments to
op and linear order is specified by syntax.
We use variables and equality statements to
achieve the same effect as coindexation in TFSs.
This raises one problem, which is the need to
avoid accidental variable equivalences (e.g., acci-
dentally using x in both the signs for cat and dog
when building the logical form of A dog chased
a cat). We avoid this by adopting a convention
that each instance of a lexical sign comes from
a set of basic sements that have pairwise distinct
variables. The equivalent of coindexation within
a lexical sign is represented by repeating the same
variable but the equivalent of coindexation that
occurs during semantic composition is an equality
condition which identifies two different variables.
Stating this formally is straightforward but a little
long-winded, so we omit it here.
3 Interpretation
The SEPs and EQs can be interpreted with respect
to a first order model ?E,A, F ? where:
1. E is a set of events
2. A is a set of individuals
3. F is an interpretation function, which as-
signs tuples of appropriate kinds to the pred-
icates of the language.
The truth definition of the SEPs and EQs
(which we group together under the term SMRS,
for simple MRS) is as follows:
1. For all events and individuals v, [v]?M,g? =
g(v).
2. For all n-predicates Pn,
[Pn]?M,g? = {?t1, . . . , tn? : ?t1, . . . , tn? ?
F (Pn)}.
3. [Pn(v1, . . . , vn)]?M,g? = 1 iff
?[v1]?M,g?, . . . , [vn]?M,g?? ? [Pn]?M,g?.
4. [? ? ?]?M,g? = 1 iff
[?]?M,g? = 1 and [?]?M,g? = 1.
Thus, with respect to a modelM , an SMRS can be
viewed as denoting an element of P(G), where
G is the set of variable assignment functions (i.e.,
elements ofG assign the variables e, . . . and x, . . .
their denotations):
[smrs]M = {g : g is a variable assignment
function and M |=g smrs}
We now consider the semantics of the algebra.
This must define the semantics of the operation op
in terms of a function f which is defined entirely
in terms of the denotations of op?s arguments. In
other words, [op(a1, a2)] = f([a1], [a2]) for
some function f . Intuitively, where the SMRS
of the SEMENT a1 denotes G1 and the SMRS of
the SEMENT a2 denotes G2, we want the seman-
tic value of the SMRS of op(a1, a2) to denote the
following:
G1 ?G2 ? [hook(a1) = hole(a2)]
But this cannot be constructed purely as a func-
tion of G1 and G2.
The solution is to add hooks and holes to the
denotations of SEMENTS (cf. Zeevat, 1989). We
define the denotation of a SEMENT to be an ele-
ment of I ? I ? P(G), where I = E ? A, as
follows:
Definition 4 Denotations of SEMENTs
If a 6= ? is a SEMENT, [[a]]M = ?[i], [i?], G?
where:
1. [i] = hook(a)
2. [i?] = hole(a)
3. G = {g : M |=g smrs(a)}
[[?]]M = ??, ?, ??
So, the meanings of SEMENTs are ordered three-
tuples, consisting of the hook and hole elements
(from I) and a set of variable assignment func-
tions that satisfy the SMRS.
We can now define the following operation f
over these denotations to create an algebra:
Definition 5 Semantics of the Semantic Con-
struction Algebra
?I ? I ? P(G), f? is an algebra, where:
f(??, ?, ??, ?[i2], [i?2], G2?) = ??, ?, ??
f(?[i1], [i?1], G1?, ??, ?, ??) = ??, ?, ??
f(?[i1], [i?1], G1?, ?[i2], ?, G2? = ??, ?, ??
f(?[i1], [i?1], G1?, ?[i2], [i
?
2], G2?) =
?[i2], [i?1], G1 ?G2 ?G
??
where G? = {g : g(i1) = g(i?2)}
And this operation demonstrates that semantic
construction is compositional:
Theorem 2 Semantics of Semantic Construction
is Compositional
The mapping [[]] : ??, op? ?? ??I, I,G?, f?
is a homomorphism (so [[op(a1, a2)]] =
f([[a1]], [[a2]])).
This follows from the definitions of [], op and f .
4 Labelling holes
We now start considering the elaborations neces-
sary for real grammars. As we suggested earlier,
it is necessary to have multiple labelled holes.
There will be a fixed inventory of labels for any
grammar framework, although there may be some
differences between variants.3 In HPSG, comple-
ments are represented using a list, but in general
there will be a fixed upper limit for the number
of complements so we can label holes COMP1,
COMP2, etc. The full inventory of labels for
3For instance, Sag and Wasow (1999) omit the distinction
between SPR and SUBJ that is often made in other HPSGs.
the ERG is: SUBJ, SPR, SPEC, COMP1, COMP2,
COMP3 and MOD (see Pollard and Sag, 1994).
To illustrate the way the formalization goes
with multiple slots, consider opsubj :
Definition 6 The definition of opsubj
opsubj(a1, a2) is the following: If a1 = ? or a2 =
? or holesubj(a2) = ?, then opsubj(a1, a2) = ?.
And if ?l 6= subj such that:
|holel(a1) ? holel(a2)| > 1
then opsubj(a1, a2) = ?. Otherwise:
1. hook(opsubj(a1, a2)) = hook(a2)
2. For all labels l 6= subj:
holel(opsubj(a1, a2)) = holel(a1) ?
holel(a2)
3. lzt(opsubj(a1, a2)) = lzt(a1) ? lzt(a2)
4. eq(opsubj(a1, a2)) = Tr(eq(a1) ? eq(a2)?
{hook(a1) = holesubj(a2)})
where Tr stands for transitive closure.
There will be similar operations opcomp1,
opcomp2 etc for each labelled hole. These
operations can be proved to form an algebra
??, opsubj , opcomp1, . . .? in a similar way to the
unlabelled case shown in Theorem 1. A lit-
tle more work is needed to prove that opl is
closed on ?. In particular, with respect to
clause 2 of the above definition, it is necessary
to prove that opl(a1, a2) = ? or for all labels l?,
|holel?(opl(a1, a2))| ? 1, but it is straightforward
to see this is the case.
These operations can be extended in a straight-
forward way to handle simple constituent coor-
dination of the kind that is currently dealt with
in the ERG (e.g., Kim sleeps and talks and Kim
and Sandy sleep); such cases involve daughters
with non-empty holes of the same label, and
the semantic operation equates these holes in the
mother SEMENT.
5 Scopal relationships
The algebra with labelled holes is sufficient to
deal with simple grammars, such as that in Sag
and Wasow (1999), but to deal with scope, more is
needed. It is now usual in constraint based gram-
mars to allow for underspecification of quantifier
scope by giving labels to pieces of semantic in-
formation and stating constraints between the la-
bels. In MRS, labels called handles are associ-
ated with each EP. Scopal relationships are rep-
resented by EPs with handle-taking arguments.
If all handle arguments are filled by handles la-
belling EPs, the structure is fully scoped, but in
general the relationship is not directly specified
in a logical form but is constrained by the gram-
mar via additional conditions (handle constraints
or hcons).4 A variety of different types of condi-
tion are possible, and the algebra developed here
is neutral between them, so we will simply use
relh to stand for such a constraint, intending it to
be neutral between, for instance, =q (qeq: equal-
ity modulo quantifiers) relationships used in MRS
and the more usual ? relationships from UDRT
(Reyle, 1993). The conditions in hcons are accu-
mulated by append.
To accommodate scoping in the algebra, we
will make hooks and holes pairs of indices and
handles. The handle in the hook corresponds to
the LTOP feature in MRS. The new vocabulary is:
1. The absurdity symbol ?.
2. handles h1, h2, . . .
3. indices i1, i2, . . ., as before
4. n-predicates which take handles and indices
as arguments
5. relh and =.
The revised definition of an EP is as in MRS:
Definition 7 Elementary Predications (EPs)
An EP contains exactly four components:
1. a handle, which is the label of the EP
2. a relation
3. a list of zero or more ordinary variable ar-
guments of the relation (i.e., indices)
4. a list of zero or more handles corresponding
to scopal arguments of the relation.
4The underspecified scoped forms which correspond to
sentences can be related to first order models of the fully
scoped forms (i.e., to models of WFFs without labels) via
supervaluation (e.g., Reyle, 1993). This corresponds to stip-
ulating that an underspecified logical form u entails a base,
fully specified form ? only if all possible ways of resolving
the underspecification in u entails ?. For reasons of space,
we do not give details here, but note that this is entirely con-
sistent with treating semantics in terms of a description of
a logical formula. The relationship between the SEMENTS
of non-sentential constituents and a more ?standard? formal
language such as ?-calculus will be explored in future work.
This is written h:r(a1, . . . ,an,sa1, . . . ,sam). For
instance, h:every(x, h1, h2) is an EP.5
We revise the definition of semantic entities to
add the hcons conditions and to make hooks and
holes pairs of handles and indices.
H-Cons Conditions: Where h1 and h2 are
handles, h1relhh2 is an H-Cons condition.
Definition 8 The Set ? of Semantic Entities
s ? ? if and only if s = ? or s =
?s1, s2, s3, s4, s5? such that:
? s1 = {[h, i]} is a hook;
? s2 = ? or {[h?, i?]} is a hole;
? s3 is a bag of EP conditions
? s4 is a bag of HCONS conditions
? s5 is a set of equalities between variables.
SEMENTs are: [h1, i1]{holes}[eps][hcons]{eqs}.
We will not repeat the full composition def-
inition, since it is unchanged from that in ?2
apart from the addition of the append operation
on hcons and a slight complication of eq to deal
with the handle/index pairs:
eq(op(a1, a2)) = Tr(eq(a1) ? eq(a2)?
{hdle(hook(a1)) = hdle(hole(a2)),
ind(hook(a1)) = ind(hole(a2))})
where Tr stands for transitive closure as before
and hdle and ind access the handle and index of
a pair. We can extend this to include (several) la-
belled holes and operations, as before. And these
revised operations still form an algebra.
The truth definition for SEMENTS is analogous
to before. We add to the model a set of la-
bels L (handles denote these via g) and a well-
founded partial order ? on L (this helps interpret
the hcons; cf. Fernando (1997)). A SEMENT then
denotes an element of H? . . .H?P(G), where
the Hs (= L? I) are the new hook and holes.
Note that the language ? is first order, and
we do not use ?-abstraction over higher or-
der elements.6 For example, in the standard
Montagovian view, a quantifier such as every
5Note every is a predicate rather than a quantifier in
this language, since MRSs are partial descriptions of logical
forms in a base language.
6Even though we do not use ?-calculus for composition,
we could make use of ?-abstraction as a representation de-
vice, for instance for dealing with adjectives such as former,
cf., Moore (1989).
is represented by the higher-order expression
?P?Q?x(P (x), Q(x)). In our framework, how-
ever, every is the following (using qeq conditions,
as in the LinGO ERG):
[hf , x]{[]subj , []comp1, [h?, x]spec, . . .}
[he : every(x, hr, hs)][hr =q h?]{}
and dog is:
[hd, y]{[]subj , []comp1, []spec, . . .}[hd : dog(y)][]{}
So these composes via opspec to yield every dog:
[hf , x]{[]subj , []comp1, []spec, . . .}
[he : every(x, hr, hs), hd : dog(y)]
[hr =q h?]{h? = hd, x = y}
This SEMENT is semantically equivalent to:
[hf , x]{[]subj , []comp1, []spec, . . .}
[he : every(x, hr, hs), hd : dog(x)][hr =q hd]{}
A slight complication is that the determiner is
also syntactically selected by the N? via the SPR
slot (following Pollard and Sag (1994)). How-
ever, from the standpoint of the compositional
semantics, the determiner is the semantic head,
and it is only its SPEC hole which is involved: the
N? must be treated as having an empty SPR hole.
In the ERG, the distinction between intersective
and scopal modification arises because of distinc-
tions in representation at the lexical level. The
repetition of variables in the SEMENT of a lexical
sign (corresponding to TFS coindexation) and the
choice of type on those variables determines the
type of modification.
Intersective modification: white dog:
dog: [hd, y]{[]subj , []comp1, . . . , []mod}
[hd : dog(y)][]{}
white: [hw, x]{[]subj , []comp1, .., [hw, x]mod}
[hw : white(x)][]{}
white dog: [hw, x]{[]subj , []comp1, . . . , []mod}
(opmod) [hd : dog(y), hw : white(x)][]
{hw = hd, x = y}
Scopal Modification: probably walks:
walks: [hw, e?]{[h?, x]subj , []comp1, . . . , []mod}
[hw : walks(e?, x)][]{}
probably: [hp, e]{[]subj , []comp1, . . . , [h, e]mod}
[hp : probably(hs)][hs =q h]{}
probably [hp, e]{[h?, x]subj , []comp1, . . . , []mod}
walks: [hp:probably(hs), hw:walks(e?, x)]
(opmod) [hs =q h]{hw = h, e = e?}
6 Control and external arguments
We need to make one further extension to allow
for control, which we do by adding an extra slot to
the hooks and holes corresponding to the external
argument (e.g., the external argument of a verb
always corresponds to its subject position). We
illustrate this by showing two uses of expect; note
the third slot in the hooks and holes for the exter-
nal argument of each entity. In both cases, x?e is
both the external argument of expect and its sub-
ject?s index, but in the first structure x?e is also the
external argument of the complement, thus giving
the control effect.
expect 1 (as in Kim expected to sleep)
[he, ee, x?e]{[hs, x
?
e, x
?
s]subj , [hc, ec, x
?
e]comp1, . . .}
[he : expect(ee, x?e, h?e)][h?e =q hc]{}
expect 2 (Kim expected that Sandy would sleep)
[he, ee, x?e]{[hs, x
?
e, x
?
s]subj , [hc, ec, x
?
c]comp1, . . .}
[h : expect(ee, x?e, h?e)][h?e =q hc]{}
Although these uses require different lexical en-
tries, the semantic predicate expect used in the
two examples is the same, in contrast to Montago-
vian approaches, which either relate two distinct
predicates via meaning postulates, or require an
additional semantic combinator. The HPSG ac-
count does not involve such additional machinery,
but its formal underpinnings have been unclear:
in this algebra, it can be seen that the desired re-
sult arises as a consequence of the restrictions on
variable assignments imposed by the equalities.
This completes our sketch of the algebra neces-
sary to encode semantic composition in the ERG.
We have constrained accessibility by enumerating
the possible labels for holes and by stipulating the
contents of the hooks. We believe that the han-
dle, index, external argument triple constitutes all
the semantic information that a sign should make
accessible to a functor. The fact that only these
pieces of information are visible means, for in-
stance, that it is impossible to define a verb that
controls the object of its complement.7 Although
obviously changes to the syntactic valence fea-
tures would necessitate modification of the hole
labels, we think it unlikely that we will need to in-
crease the inventory further. In combination with
7Readers familiar with MRS will notice that the KEY fea-
ture used for semantic selection violates these accessibility
conditions, but in the current framework, KEY can be re-
placed by KEYPRED which points to the predicate alone.
the principles defined in Copestake et al(1999)
for qeq conditions, the algebra presented here re-
sults in a much more tightly specified approach
to semantic composition than that in Pollard and
Sag (1994).
7 Comparison
Compared with ?-calculus, the approach to com-
position adopted in constraint-based grammars
and formalized here has considerable advantages
in terms of simplicity. The standard Montague
grammar approach requires that arguments be
presented in a fixed order, and that they be strictly
typed, which leads to unnecessary multiplication
of predicates which then have to be interrelated
by meaning postulates (e.g., the two uses of ex-
pect mentioned earlier). Type raising also adds
to the complexity. As standardly presented, ?-
calculus does not constrain grammars to be mono-
tonic, and does not control accessibility, since the
variable of the functor that is ?-abstracted over
may be arbitrarily deeply embedded inside a ?-
expression.
None of the previous work on unification-
based approaches to semantics has considered
constraints on composition in the way we have
presented. In fact, Nerbonne (1995) explicitly
advocates nonmonotonicity. Moore (1989) is
also concerned with formalizing existing prac-
tice in unification grammars (see also Alshawi,
1992), though he assumes Prolog-style unifica-
tion, rather than TFSs. Moore attempts to for-
malize his approach in the logic of unification,
but it is not clear this is entirely successful. He
has to divorce the interpretation of the expres-
sions from the notion of truth with respect to the
model, which is much like treating the semantics
as a description of a logic formula. Our strategy
for formalization is closest to that adopted in Uni-
fication Categorial Grammar (Zeevat et al 1987),
but rather than composing actual logical forms we
compose partial descriptions to handle semantic
underspecification.
8 Conclusions and future work
We have developed a framework for formally
specifying semantics within constraint-based rep-
resentations which allows semantic operations in
a grammar to be tightly specified and which al-
lows a representation of semantic content which
is largely independent of the feature structure ar-
chitecture of the syntactic representation. HPSGs
can be written which encode much of the algebra
described here as constraints on types in the gram-
mar, thus ensuring that the grammar is consistent
with the rules on composition. There are some as-
pects which cannot be encoded within currently
implemented TFS formalisms because they in-
volve negative conditions: for instance, we could
not write TFS constraints that absolutely prevent
a grammar writer sneaking in a disallowed coin-
dexation by specifying a path into the lzt. There is
the option of moving to a more general TFS logic
but this would require very considerable research
to develop reasonable tractability. Since the con-
straints need not be checked at runtime, it seems
better to regard them as metalevel conditions on
the description of the grammar, which can any-
way easily be checked by code which converts the
TFS into the algebraic representation.
Because the ERG is large and complex, we have
not yet fully completed the exercise of retrospec-
tively implementing the constraints throughout.
However, much of the work has been done and
the process revealed many bugs in the grammar,
which demonstrates the potential for enhanced
maintainability. We have modified the grammar
to be monotonic, which is important for the chart
generator described in Carroll et al(1999). A
chart generator must determine lexical entries di-
rectly from an input logical form: hence it will
only work if all instances of nonmonotonicity can
be identified in a grammar-specific preparatory
step. We have increased the generator?s reliability
by making the ERG monotonic and we expect fur-
ther improvements in practical performance once
we take full advantage of the restrictions in the
grammar to cut down the search space.
Acknowledgements
This research was partially supported by the Na-
tional Science Foundation, grant number IRI-
9612682. Alex Lascarides was supported by an
ESRC (UK) research fellowship. We are grateful
to Ted Briscoe, Alistair Knott and the anonymous
reviewers for their comments on this paper.
References
Alshawi, Hiyan [1992] (ed.) The Core Language
Engine, MIT Press.
Carroll, John, Ann Copestake, Dan Flickinger
and Victor Poznanski [1999] An Efficient Chart
Generator for Lexicalist Grammars, The 7th In-
ternational Workshop on Natural Language Gen-
eration, 86?95.
Copestake, Ann, Dan Flickinger, Ivan Sag
and Carl Pollard [1999] Minimal Recursion Se-
mantics: An Introduction, manuscript at www-
csli.stanford.edu/?aac/newmrs.ps
Egg, Marcus [1998] Wh-Questions in Under-
specified Minimal Recursion Semantics, Journal
of Semantics, 15.1:37?82.
Fernando, Tim [1997] Ambiguity in Changing
Contexts, Linguistics and Philosophy, 20.6: 575?
606.
Moore, Robert C. [1989] Unification-based Se-
mantic Interpretation, The 27th Annual Meeting
for the Association for Computational Linguistics
(ACL-89), 33?41.
Nerbonne, John [1995] Computational
Semantics?Linguistics and Processing, Shalom
Lappin (ed.) Handbook of Contemporary
Semantic Theory, 461?484, Blackwells.
Pollard, Carl and Ivan Sag [1994] Head-
Driven Phrase Structure Grammar, University of
Chicago Press.
Reyle, Uwe [1993] Dealing with Ambiguities
by Underspecification: Construction, Represen-
tation and Deduction, Journal of Semantics, 10.1:
123?179.
Sag, Ivan, and Tom Wasow [1999] Syntactic
Theory: An Introduction, CSLI Publications.
Shieber, Stuart [1986] An Introduction to
Unification-based Approaches to Grammar,
CSLI Publications.
Zeevat, Henk [1989] A Compositional Ap-
proach to Discourse Representation Theory, Lin-
guistics and Philosophy, 12.1: 95?131.
Zeevat, Henk, Ewan Klein and Jo Calder
[1987] An introduction to unification categorial
grammar, Nick Haddock, Ewan Klein and Glyn
Morrill (eds), Categorial grammar, unification
grammar, and parsing: working papers in cogni-
tive science, Volume 1, 195?222, Centre for Cog-
nitive Science, University of Edinburgh.
Using an open-source unification-based system for CL/NLP teaching
Ann Copestake
Computer Laboratory
University of Cambridge
Cambridge, UK
aac@cl.cam.ac.uk
John Carroll
Cognitive and Computing Sciences
University of Sussex
Falmer, Brighton, UK
johnca@cogs.susx.ac.uk
Dan Flickinger
CSLI, Stanford University and
YY Software
Ventura Hall,
Stanford, USA
danf@csli.stanford.edu
Robert Malouf
Alfa Informatica,
University of Groningen,
Postbus 716, 9700 AS Groningen,
The Netherlands
malouf@let.rug.nl
Stephan Oepen
YY Software and
CSLI, Stanford University
110 Pioneer Way
Mountain View, USA
oe@yy.com
Abstract
We demonstrate the open-source LKB
system which has been used to teach the
fundamentals of constraint-based gram-
mar development to several groups of
students.
1 Overview of the LKB system
The LKB system is a grammar development
environment that is distributed as part of the
open source LinGO tools (http://www-
csli.stanford.edu/?aac/lkb.html
and http://lingo.stanford.edu, see
also Copestake and Flickinger, 2000). It is an
open-source grammar development environment
implemented in Common Lisp, distributed not
only as source but also as a standalone application
that can be run on Linux, Solaris and Windows
(see the website for specific requirements). It
will also run under Macintosh Common Lisp,
but for this a license is required. The LKB in-
cludes a parser, generator, support for large-scale
inheritance hierarchies (including the use of
defaults), various tools for manipulating semantic
representations, a rich set of graphical tools
for analyzing and debugging grammars, and
extensive on-line documentation. Grammars of
all sizes have been written using the LKB, for
several languages, mostly within the linguistic
frameworks of Categorial Grammar and Head-
Driven Phrase Structure Grammar. The LKB
system was initially developed in 1991, but has
gone through multiple versions since then. It
is in active use by a considerable number of
researchers worldwide. An introductory book
on implementing grammars in typed feature
structure formalisms using the LKB is near
completion (Copestake, in preparation).
2 Demo outline
Although the LKB has been successfully used for
large-scale grammar development, this demon-
stration will concentrate on its use with relatively
small scale teaching grammars, of a type which
can be developed by students in practical exer-
cises. We will show an English grammar frag-
ment which is linked to a textbook on formal syn-
tax (Sag and Wasow, 1999) to illustrate how the
system may be used in conjunction with more tra-
ditional materials in a relatively linguistically ori-
ented course. We will demonstrate the tools for
analyzing parses and for debugging and also dis-
cuss the way that parse selection mechanisms can
be incorporated in the system. If time permits, we
will show how semantic analyses produced with a
somewhat more complex grammar can be linked
up to a theorem prover and also exploited in se-
mantic transfer for Machine Translation. Exer-
cises where the grammar is part of a larger system
are generally appropriate for advanced courses or
for NLP application courses.
The screen dump in the figure is from a session
working with a grammar fragment for Esperanto.
This shares its basic types and rules with the
English textbook grammar fragment mentioned
above. The windows shown are:
1. The LKB Top interaction window: main
  
 
 
 
Figure 1: Screen dump of the LKB system
menus plus feedback and error messages
2. Type hierarchy window (fragment): the
more general types are on the left. Nodes in
the hierarchy have menus that provide more
information about the types, such as their as-
sociated constraints.
3. Type constraint for the type intrans-verb:
again nodes are clickable for further infor-
mation.
4. Parse tree for La knabo dormas (the boy
sleeps): a larger display for parse trees is
also available, but this scale is useful for
summary information. Menus associated
with trees allow for display of associated se-
mantic information if any is included in the
grammar and for generation. Here the dis-
play shows inflectional rules as well as nor-
mal syntactic rules: hence the VP node un-
der dormas, which corresponds to the stem.
5. In the middle is an emacs window displaying
the source file for the lexicon associated with
this grammar.1 It shows the entry for the lex-
1(We generally use emacs as an editor when teaching,
eme dorm, which, like most lexical entries in
this grammar, just specifies a spelling and a
type (here intrans-verb).
6. Part of the parse chart corresponding to the
tree is shown in the bottom window: nodes
which have knabo as a descendant are high-
lighted. Again, these nodes are active: one
very useful facility associated with them is a
unification checker which allows the gram-
mar writer to establish why a rule did not
apply to a phrase or phrases.
3 Use of the LKB in teaching
Teaching uses of the LKB have included under-
graduate and graduate courses on formal syntax
and on computational linguistics at several sites,
grammar engineering courses at two ESSLLI
summer schools, and numerous student projects
at undergraduate, masters and doctoral levels. An
advantage of the LKB is that students learn to use
a system which is sufficiently heavy duty for more
advanced work, up to the scale at least of research
although this causes some overhead, especially for students
who are only used to word processing programs.
prototypes. This provides them with a good plat-
form on which to build for further research. Feed-
back from the courses we have taught has mostly
been very positive, but we have found a ratio of
six students to one instructor (or teaching assis-
tant) to be the maximum that is workable. One
major reason is that debugging students? gram-
mars and teaching debugging techniques is time-
consuming.
When teaching an introductory course with the
LKB, we start the students off with a very sim-
ple grammar, which they are asked to expand
in specific ways. We introduce various addi-
tional techniques and formal devices (such as in-
flectional and lexical rules, defaults, difference
lists and gaps) gradually during a course. Mate-
rial from our ESSLLI courses, including starting
grammars, exercises and solutions is distributed
via the website. Several other small grammars
developed by students are also distributed as part
of the LKB system and we would welcome fur-
ther contributions. We are hoping to facilitate this
by making it easier for people outside the LinGO
group to add and modify grammars.
Several graduate students have used versions
of the LKB system as part of their thesis work,
for diverse projects including machine transla-
tion and grammar learning. It has been used
in the development of several large grammars,
especially the LinGO English Resource Gram-
mar (ERG), which is itself open-source. Re-
search applications for the ERG include spoken
language machine translation in Verbmobil, gen-
eration for a speech prosthesis, and automated
email response, under development for commer-
cial use. The LKB/ERG combination can be used
by researchers who require a grammar which pro-
vides a detailed semantic analysis and reason-
ably broad coverage, for instance for experiments
on dialogue. The LKB has also been used as
a grammar preprocessor to facilitate experiments
on efficiency using the ERG with other systems
(Flickinger et al 2000).
4 Comparison with other work
There is a long history of the use of fea-
ture structure based systems in teaching, dat-
ing back at least to PATR (Shieber, 1986:
see http://www.ling.gu.se/?li/). The
Alvey Natural Language Tools (Briscoe et al
1987) have been used for teaching at several uni-
versities: Briscoe and Grover developed an ex-
tensive set of teaching examples and exercises,
which is however unpublished. Versions of the
SRI Core Language Engine (Alshawi, 1992) and
of the XTAG grammar (XTAG group, 1995) and
parser have also been used for teaching. Besides
the LKB, typed feature structure environments
have been used at many universities, though un-
like the systems cited above, most have only been
used with small grammars and may not scale
up. Hands on courses using various systems have
been run at many recent summer schools includ-
ing ESSLLI 99 (using the Xerox XLE, see Butt
et al 1999) and ESSLLI 97 and the 1999 LSA
summer school (both using ConTroll, see Hin-
richs and Meurers, 1999). Very little seems to
have been formally published describing expe-
riences in teaching with grammar development
environments, though Bouma (1999) describes
material for teaching a computational linguistics
course that includes exercises using the Hdrug
unification-based enviroment to extend a gram-
mar.
Despite this rich variety of tools, we believe
that the LKB system has a combination of fea-
tures which make it distinctive and give it a useful
niche in teaching. The most important points are
that its availability as open source, combined with
scale and efficiency, allow advanced projects to be
supported as well as introductory courses. As far
as we are aware, it is the only system freely avail-
able with a broad coverage grammar that sup-
ports semantic interpretation and generation. Es-
pecially for more linguistically oriented courses,
the link to the Sag and Wasow textbook is also
important. Similar grammars could be developed
for other systems, but would be less directly com-
parable to the textbook since this assumes a de-
fault formalism which so far is only implemented
in the LKB.
On the other hand, the LKB is not a suitable ba-
sis for a course that involves the students learning
to implement a unifier, parser and so on. The sys-
tem is quite complex (about 120 files and 40,000
lines of Lisp code) and though the vast majority
of this is concerned with non-core functionality,
such as the graphical interfaces, it is still some-
what daunting. This seems an inevitable trade-
off of having a system powerful enough for real
applications (see Bouma (1999) for related dis-
cussion). It is questionable whether the LKB is
entirely satisfactory as a student?s first computa-
tional grammar system, although we have used it
with students who have no prior experience of this
sort: ideally we would suggest starting off with
brief exercises with a pure context-free grammar
to explain the concepts of well-formedness, re-
cursion and so on. We also wouldn?t necessar-
ily advocate using the LKB as a core component
of a first course on formal syntax for linguistic
students, since the specifics of dealing with an
implementation may interfere with understanding
of basic concepts, though it is suitable as a sup-
plement to an initial course or as the basis for a
slightly more advanced course.
We think there is considerable potential for
building materials for courses that allow students
to work with realistic but transparent applications
using the LKB and a large grammar as a compo-
nent. Developing such materials is clearly nec-
essary in order to give students useful practical
experience. It is however very time-consuming,
and most probably will have to be undertaken as
part of a cooperative, open-source development
involving people from several different institu-
tions.
Acknowledgements
This research was partially supported by the Na-
tional Science Foundation, grant number IRI-
9612682. The current versions of the English
grammars associated with the Sag and Wasow
textbook were largely developed by Christopher
Callison-Burch while he was an undergraduate at
Stanford.
References
Alshawi, Hiyan (ed). [1992] The Core Language
Engine, MIT Press, Cambridge, MA.
Bouma, Gosse. [1999] ?A modern computa-
tional linguistics course using Dutch.? In Frank
van Eynde and Ineke Schuurman, editors, CLIN
1998, Papers from the Ninth CLIN Meeting, Am-
sterdam. Rodopi Press.
Briscoe, Ted, Claire Grover, Bran Boguraev
and John Carroll. [1987] ?A formalism and en-
vironment for the development of a large gram-
mar of English?, Proceedings of the 10th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI-87), Milan, Italy, 703?708.
Butt, Miriam, Anette Frank and Jonas Kuhn.
[1999] ?Development of large scale LFG gram-
mars ? Linguistics, Engineering and Resources?,
http://www.xrce.xerox.com/people/
frank/esslli99-hp/index.html
Copestake, Ann. [in preparation] Implementing
typed feature structure grammars, CSLI Publica-
tions, Stanford.
Copestake, Ann and Dan Flickinger. [2000]
?An open-source grammar development environ-
ment and broad-coverage English grammar us-
ing HPSG?, Second conference on Language Re-
sources and Evaluation (LREC-2000), Athens,
Greece.
Flickinger, Daniel, Stephan Oepen, Hans
Uszkoreit and Jun?ichi Tsujii. [2000] Journal of
Natural Language Engineering. Special Issue on
Efficient Processing with HPSG: Methods, Sys-
tems, Evaluation, 6(1).
Hinrichs, Erhard and Detmar Meurers [1999]
?Grammar Development in Constraint-Based
Formalisms?,
http://www.ling.ohio-state.edu/
?dm/lehre/lsa99/material.html,
see also http://www.sfs.nphil.uni-
tuebingen.de/controll/
Sag, Ivan, and Tom Wasow [1999] Syntactic
Theory: An Introduction, CSLI Publications.
Shieber, Stuart [1986] An Introduction to
Unification-based Approaches to Grammar,
CSLI Publications.
The XTAG Research Group [1995]. ?A Lex-
icalized Tree Adjoining Grammar for English?
IRCS Report 95-03, University of Pennsylvania?
Parallel Distributed Grammar Engineering for Practical Applications
Stephan Oepen?, Emily M. Bender?, Uli Callmeier?, Dan Flickinger??, Melanie Siegel?
?CSLI Stanford ?YY Technologies ?DFKI GmbH
Stanford (CA) Mountain View (CA) Saarbru?cken (Germany)
?
?
?
oe
bender
dan
?
?
?
@csli.stanford.edu
{
uc
dan
}
@yy.com siegel@dfki.de
Abstract
Based on a detailed case study of paral-
lel grammar development distributed across
two sites, we review some of the require-
ments for regression testing in grammar en-
gineering, summarize our approach to sys-
tematic competence and performance profil-
ing, and discuss our experience with gram-
mar development for a commercial applica-
tion. If possible, the workshop presentation
will be organized around a software demon-
stration.
1 Background
The production of large-scale constraint-based
grammars and suitable processing environments is
a labour- and time-intensive process that, maybe,
has become somewhat of a growth industry over
the past few years, as companies explore products
that incorporate grammar-based language process-
ing. Many broad-coverage grammars have been
developed over several years, sometimes decades,
typically coordinated by a single grammarian who
would often draw on additional contributors (e.g.
the three HPSG implementations developed as part
of the VerbMobil effort, see Flickinger, Copes-
take, & Sag, 2000, Mu?ller & Kasper, 2000, and
Siegel, 2000; or the LFG implementations devel-
oped within the ParGram consortium, Butt, King,
Nin?o, & Segond, 1999).
More recently, we also find genuinely shared
and distributed development of broad-coverage
grammars, and we will use one such initiative as an
example?viz. an open-source HPSG implementa-
tion for Japanese jointly developed between DFKI
Saarbru?cken (Germany) and YY Technologies
(Mountain View, CA)?to demonstrate the techno-
logical and methodological challenges present in
distributed grammar and system engineering.
2 Parallel Distributed Grammar
Development?A Case Study
The Japanese grammar builds on earlier work per-
formed jointly between DFKI and the Computa-
tional Linguistics Department at Saarland Univer-
sity (Germany) within VerbMobil; much like for
the German VerbMobil grammar, two people were
contributing to the grammar in parallel, one build-
ing out syntactic analyses, the other charged with
integrating semantic composition into the syntax.
This relatively strict separation of responsibilities
mostly enabled grammarians to serialize incre-
mental development of the resource: the syntacti-
cian would supply a grammar with extended cov-
erage to the semanticist and, at the onset of the fol-
lowing iteration, start subsequent work on syntax
from the revised grammar.
In the DFKI ? YY cooperation the situation was
quite different. Over a period of eight months,
both partners had a grammarian working on syn-
tax and semantics simultaneously on a day-to-
day basis; both grammarians were submitting
changes to a joint, version-controlled source repos-
itory and usually would start the work day by re-
trieving the most recent revisions. At the same
time, product building and the development of
so-called ?domain libraries? (structured collections
of knowledge about a specific domain that is in-
stantiated from semantic representations delivered
from grammatical analysis) at YY already incorpo-
rated the grammar and depended on it for actual,
customer-specific contracts. Due to a continuous
demand for improvements in coverage and analy-
sis accuracy, the grammar used in the main product
line would be updated from the current develop-
ment version about once or twice a week. Parallel
to work on the Japanese grammar (and simultane-
ous work on grammars for English and Spanish),
both the grammar development environment (the
open-source LKB system; Copestake, 2002) and
the HPSG run-time component powering the YY
linguistic analysis engine (the open-source PET
parser; Callmeier, 2002) continued to evolve, as
did the YY-proprietary mapping of meaning repre-
sentations extracted from the HPSG grammars into
domain knowledge?all central parts of a complex
system of interacting components and constraints.
As has been argued before (see, for exam-
ple, Oepen & Flickinger, 1998), the nature of a
large-scale constraint-based grammar and the sub-
tle interactions of lexical and constructional con-
straints make it virtually impossible to predict how
a change in one part of the grammar affects over-
all system behaviour. A relatively minor repair in
one lexical class, numeral adjectives as in ?three
books were ordered? for instance, will have the po-
tential of breaking the interaction of that class with
the construction deriving named (numeric) entities
from a numeral (e.g. as in ?three is my favourite
number?) or the partitive construction (e.g. as in
?three have arrived already?). A ripple effect of
a single change can thus corrupt the semantics
produced for any of these cases and in the con-
sequence cause failure or incorrect behaviour in
the back-end system. In addition to these qual-
ity assurance requirements on grammatical cover-
age and correctness, the YY application (like most
applications for grammar-based linguistic analy-
sis) utilizes a set of hand-constructed parse rank-
ing heuristics that enables the parser to operate
in best-first search mode and to return only one
reading, i.e. the analysis that is ranked best by the
heuristic component. The parse ranking machin-
ery builds on preferences that are associated with
individual or classes of lexical items and construc-
tions. The set of preferences is maintained in par-
allel to the grammar, in a sense providing a layer
of performance-oriented annotations over the basic
building blocks of the core competence grammar.
Without discussing the details of the parse ranking
approach, it creates an additional element of un-
certainty in assessing grammar changes: since the
preference for a specific analysis results implic-
itly from a series of local preferences (of lexical
items and constructions contributing to the com-
plete derivation), introducing additional elements
(i.e. new local or global ambiguity) into the search
space and subjecting them to the partial ordering
can quickly skew the overall result.
Summing up, the grammar and application engi-
neering example presented here illustrates a num-
ber of highly typical requirements on the engi-
neering environment. First, all grammarians and
system engineers participating in the development
process need to keep frequent, detailed, and accu-
rate records of a large number of relevant parame-
ters, including but not limited to grammatical cov-
erage, correctness of syntactic analyses and cor-
responding semantic forms, parse selection accu-
racy, and overall system performance. Second, as
modifications to the system as a whole are made
daily?and sometimes several times each day?all
developers must be able to assess the impact of
recent changes and track their effects on all rele-
vant parameters; gathering the data and analyzing
it must be simple, fast, and automated as much as
possible. Third, not all modifications (to the gram-
mar or underlying software) will result in ?mono-
tonic? or backwards-compatible effects. A change
in the treatment of optional nominal complements,
for example, may affect virtually all derivation
trees and render a comparison of results at this
level uninformative. At the same time, a primarily
syntactic change of this nature will not cause an ef-
fect in associated meaning representations, so that
a semantic equivalence test over analyses should
be expected to yield an exact match to earlier re-
sults. Hence, the machinery for representation and
comparison of relevant parameters needs to facil-
itate user-level specification of informative tests
and evolution criteria. Finally, the metrics used in
tracking grammar development cannot be isolated
from measurements of system resource consump-
tion and overall performance (specific properties
of a grammar may trigger idiosyncrasies or soft-
ware bugs in a particular version of the process-
ing system); therefore, and to enable exchange of
reference points and comparability of experiments,
grammarians and system developers alike should
use the same, homogenuous set of relevant param-
eters.
3 Integrated Competence and
Performance Profiling
The integrated competence and performance pro-
filing methodology and associated engineering
platform, dubbed [incr tsdb()] (Oepen & Callmeier,
2000)1 and reviewed in the remainder of this sec-
1See ?http://www.coli.uni-sb.de/itsdb/?
for the (draft) [incr tsdb()] user manual, pronunciation rules,
and instructions on obtaining and installing the package.
tion, was designed to meet al of the requirements
identified in the DFKI ? YY case study. Generally
speaking, the [incr tsdb()] environment is an in-
tegrated package for diagnostics, evaluation, and
benchmarking in practical grammar and system
engineering. The toolkit implements an approach
to grammar development and system optimization
that builds on precise empirical data and system-
atic experimentation, as it has been advocated by,
among others, Erbach & Uszkoreit (1990), Erbach
(1991), and Carroll (1994). [incr tsdb()] has been
integrated with, as of June 2002, nine different
constraint-based grammar development and pars-
ing systems (including both environments in use at
YY, i.e. the LKB and PET), thus providing a pre-
standard reference point for a relatively large (and
growing) community of NLP developers. The [incr
tsdb()] environment builds on the following com-
ponents and modules:
? test and reference data stored with annota-
tions in a structured database; annotations
can range from minimal information (unique
test item identifier, item origin, length et al)
to fine-grained linguistic classifications (e.g.
regarding grammaticality and linguistic phe-
nomena presented in an item), as they are rep-
resented in the TSNLP test suites, for example
(Oepen, Netter, & Klein, 1997);
? tools to browse the available data, identify
suitable subsets and feed them through the
analysis component of processing systems
like the LKB and PET, LiLFeS (Makino,
Yoshida, Torisawa, & Tsujii, 1998), TRALE
(Penn, 2000), PAGE (Uszkoreit et al, 1994),
and others;
? the ability to gather a multitude of precise and
fine-grained (grammar) competence and (sys-
tem) performance measures?like the num-
ber of readings obtained per test item, various
time and memory usage statistics, ambigu-
ity and non-determinism metrics, and salient
properties of the result structures?and store
them in a uniform, platform-independent data
format as a competence and performance pro-
file; and
? graphical facilities to inspect the resulting
profiles, analyze system competence (i.e.
grammatical coverage and overgeneration)
and performance (e.g. cpu time and memory
usage, parser search space, constraint solver
'
&
$
%
Parser 3Parser 2Parser 1Grammar 3Grammar 2Grammar 1
TestSet 3TestSet 2TestSet 1
ParallelVirtualMachine
C and Lisp APIRelationalDBMS Batch ControlStatistics UserInterface
ANSI C Common-Lisp Tcl/Tk
Figure 1: Rough sketch of [incr tsdb()] architec-
ture: the core engine comprises the database man-
agement, batch control and statistics component,
and the user interface.
workload, and others) at variable granulari-
ties, aggregate, correlate, and visualize the
data, and compare among profiles obtained
from previous grammar or system versions or
other processing environments.
As it is depicted in Figure 1, the [incr tsdb()]
architecture can be broken down into three major
parts: (i) the underlying database management sys-
tem (DBMS), (ii) the batch control and statistics
kernel (providing a C and Lisp application pro-
gram interface to client systems that can be dis-
tributed across the network), and (iii) the graphi-
cal user interface (GUI). Although, historically, the
DBMS was developed independently and the ker-
nel can be operated without the GUI, the full func-
tionality of the integrated competence and perfor-
mance laboratory?as demonstrated below?only
emerges from the combination of all three com-
ponents. Likewise, the flexibility of a clearly de-
fined API to client systems and its ability to par-
allelize batch processing and distribute test runs
across the network have greatly contributed to the
success of the package. The following paragraphs
review some of the fundamental aspects in more
detail, sketch essential functionality, and comment
on how they have been exploited in the DFKI ? YY
cooperation.
Abstraction over Processors The [incr tsdb()]
environment, by virtue of its generalized pro-
file format, abstracts over specific processing en-
vironments. While grammar engineers in the
DFKI ? YY collaboration regularly use both the
LKB (primarily for interactive development) and
PET (mostly for batch testing and the assessment
of results obtained in the YY production envi-
ronment), usage of the [incr tsdb()] profile anal-
ysis routines in most aspects hides the specifics
of the token processor used in obtaining a profile.
Both platforms interprete the same typed feature
structure formalism, load the same set of gram-
mar source files, and (unless malfunctioning) pro-
duce equivalent results. Using [incr tsdb()], gram-
marians can obtain summary views of grammati-
cal coverage and overgeneration, inspect relevant
subsets of the available data, break down analysis
views according to various aggregation schemes,
and zoom in on specific aggregates or individual
test items as appropriate. Moreover, processing
results obtained from the (far more efficient) PET
parser (that has no visualization or debugging sup-
port built in), once recorded as an [incr tsdb()] pro-
file, can be used in conjunction with the LKB (con-
tingent on the use of identical grammars), thereby
facilitating graphical inspection of parse trees and
semantic formulae.
Parallelization of Test Runs The [incr tsdb()] ar-
chitecture (see Figure 1) separates the batch con-
trol and statistics kernel from what is referred to
as client processors (i.e. parsing systems like the
LKB or PET) through an application program inter-
face (API) and the Parallel Virtual Machine (PVM;
Geist, Bequelin, Dongarra, Manchek, & Sun-
deram, 1994) message-passing protocol layer. The
use of PVM?in connection with task scheduling,
error recovery, and roll-over facilities in the [incr
tsdb()] kernel?enables developers to transparently
parallelize and distribute execution of batch pro-
cessing. At YY, grammarians had a cluster of net-
worked Linux compute servers configured as a sin-
gle PVM instance, so that execution of a test run?
using the efficient PET run-time engine?could be
completed as a matter of a few seconds. The com-
bination of near-instantaneous profile creation and
[incr tsdb()] facilities for quick, semi-automated as-
sessment of relevant changes (see below) enabled
developers to pursue a strongly empiricist style of
grammar engineering, assessing changes and their
effects on actual system behavior in small incre-
ments (often many times per hour).
Structured Comparison One of the facilities
that has proven particularly useful in the dis-
tributed grammar engineering setup outlined in
Section 2 above is the flexible comparison of com-
petence and performance profiles. The [incr tsdb()]
package eases comparison of results on a per-
item basis, using an approach similar to Un?x
diff(1), but generalized for structured data sets.
By selection of a set of parameters for intersec-
tion (and optionally a comparison predicate), the
user interface allows browsing the subset of test
items (and associated results) that fail to match
in the selected properties. One dimension that
grammarians found especially useful in intersect-
ing profiles is on the number of readings assigned
per item?detecting where coverage was lost or
added?and on derivation trees (bracketed struc-
tures labeled with rule names and identifiers of lex-
ical items) associated with each parser analysis?
assessing where analyses have changed. Addition-
ally, using a user-supplied equivalence predicate,
the same technique was regularly used at YY to
track the evolution of meaning representations (as
they form the interface from linguistic analysis into
the back-end knowledge processing engine), both
for all readings and the analysis ranked best by the
parse selection heuristics.
Zooming and Interactive Debugging In
analysing a new competence and performance
profile, grammarians typically start from summary
views (overall grammatical coverage, say), then
single out relevant (or suspicious) subsets of
profile data, and often end up zooming in to
the level of individual test items. For most [incr
tsdb()] analysis views the ?success? criteria can be
varied according to user decisions: in assessing
grammatical coverage, for example, the scoring
function can refer to virtually arbitrary profile
elements?ranging from the most basic coverage
measure (assigning at least one reading) to more
refined or application-specific metrics, the produc-
tion of a well-formed meaning representation, say.
Although the general approach allows output an-
notations on the test data (full or partial constituent
structure descriptions, for example), developers so
far have found the incremental, semi-automated
comparison against earlier results a more adequate
means of regression testing. It would appear
that, especially in an application-driven and
tightly scheduled engineering situation like the
DFKI ? YY partnership, the pace of evolution
and general lack of locality in changes (see the
examples discussed in Section 2) precludes the
construction of a static, ?gold-standard? target for
comparison. Instead, the structured comparison
facilities of [incr tsdb()] enable developers to
incrementally approximate target results and, even
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
40
45
50
55
60
65
70
75
80
85
90
95
Grammatical Coverage (Per Cent)
(generated by [incr tsdb()] at 29-jun-2002 (20:49 h))
?
? ???
? ??
?
???
?
?
?
?
?
?
?
?
?
???
??
?
??
?
?
? ?? ??
?
?
?
??
?? ? ?
?
?
?
?
?
?
?
? ???
?
?
????
??
?? ????
?? ?banking?
?? ?trading?
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
0
10
20
30
40
50
60
70
80
90
Ambiguity (Average Number of Analyses)
(generated by [incr tsdb()] at 29-jun-2002 (20:59 h))
?
?
? ? ?
?
?? ?? ? ???? ?? ? ?
?
?
?
?
?
? ?
?
??
??? ?
????
?? ?banking?
?? ?trading?
Figure 2: Evolution of grammatical coverage and average ambiguity (number of readings per test item) over
a five-month period; ?banking? and ?trading? are two data sets (of some 700 and 400 sentences, respectively)
of domain data.
in a highly dynamic environment where grammar
and processing environment evolve in parallel,
track changes and identify regression with great
confidence.
4 Looking Back?Quantifying Evolution
Over time, the [incr tsdb()] profile storage accu-
mulates precise data on the grammar development
process. Figure 2 summarizes two aspects of
grammatical evolution compiled over a five-month
period (and representing some 130 profiles that
grammarians put aside for future reference): gram-
matical coverage over two representative samples
of customer data?one for an on-line banking ap-
plication, the other from an electronic stock trad-
ing domain?is contrasted with the development
of global ambiguity (i.e. the average number of
analyses assinged to each test item). As should
be expected, grammatical coverage on both data
sets increases significantly as grammar develop-
ment focuses on these domains (?banking? for the
first three months, ?trading? from there on). While
the collection of available profiles, apparently, in-
cludes a number of data points corresponding to
?failed? experiments (fairly dramatic losses in cov-
erage), the larger picture shows mostly monotonic
improvement in coverage. As a control experi-
ment, the coverage graph includes another data
point for the ?banking? data towards the end of the
reporting period. Two months of focussed devel-
opment on the ?trading? domain have not nega-
tively affected grammatical coverage on the data
set used earlier. Corresponding to the (desirable)
increase in coverage, the graph on the right of Fig-
ure 2 depicts the evolution of grammatical ambi-
guity. As hand-built linguistic grammars put great
emphasis on the precision of grammatical analy-
sis and the exclusion of ungrammatical input, the
overall average of readings assigned to each sen-
tence varies around relatively small numbers. For
the moderately complex email data2 the grammar
often assigns less than ten analyses, rarely more
than a few dozens. However, not surprisingly
the addition of grammatical coverage comes with
a sharp increase in ambiguity (which may indi-
cate overgeneration): the graphs in Figure 2 clearly
show that, once coverage on the ?trading? data was
above eighty per cent, grammarians shifted their
engineering focus on ?tightening? the grammar, i.e.
the elimination of spurious ambiguity and overgen-
eration (see Siegel & Bender, 2002, for details on
the grammar).
Another view on grammar evolution is pre-
sented in Figure 3, depicting the ?size? of the
Japanese grammar over the same five-month de-
velopment cycle. Although measuring the size of
2Quantifying input complexity for Japanese is a non-
trivial task, as the count of the number of input words would
depend on the approach to string segmentation used in a spe-
cific system (the fairly aggressive tokenizer of ChaSen, Asa-
hara & Matsumoto, 2000, in our case); to avoid potential for
confusion, we report input complexity in the (overtly system-
specific) number of lexical items stipulated by the grammar
instead: around 50 and 80, on average, for the ?banking? and
?trading? data sets, respectively (as of February 2002).
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
8800
9000
9200
9400
9600
9800
10000
10200
88
90
92
94
96
98
100
102
104
106
Grammar Size
(generated by [incr tsdb()] at 30-jun-2002 (16:09 h))????
???
???
????
??
?????
?
?
?
?
??
?
?
? ???????
?????? ?
??























 





 

?? types
? rules
Figure 3: Evolution of grammar size (in the num-
bers of types, plotted against the left axis, and
grammar rules, plotted against the right axis) over
a five-month period.
computational grammars is a difficult challenge,
for the HPSG framework two metrics suggest them-
selves: the number of types (i.e. the size of the
grammatical ontology) and the number of gram-
mar rules (i.e. the inventory of construction types).
As would be expected, both numbers increase
more or less monotonically over the reporting pe-
riod, where the shift of focus from the ?banking?
into the ?trading? domain is marked with a sharp
increase in (primarily lexical) types. Contrasted
to the significant gains in grammatical coverage
(a relative improvement of more than seventy per
cent on the ?banking? data), the increase in gram-
mar size is moderate, though: around fifteen and
twenty per cent in the number of types and rules,
respectively.
5 Conclusions
At YY and cooperating partners (primarily DFKI
Saarbru?cken and CSLI Stanford), grammarians
(for all languages) as well as developers of both the
grammar development tools and of the production
system all used the competence and performance
profiling environment as part of their daily engi-
neering toolbox. The combination of [incr tsdb()]
facilities to parallelize test run processing and a
break-through in client system efficiency (using
the PET parser; Callmeier, 2002) has created an ex-
perimental development environment where gram-
marians can obtain near-instantaneous feedback on
the effects of changes they explore.
For the Japanese grammar specifically, the
grammar developers at both ends would typically
spend the first ten to twenty minutes of the day ob-
taining fresh profiles for a number of shared test
sets and diagnostic corpora, thereby assessing the
most recent set of changes through empirical anal-
ysis of their effects. In conjunction with a certain
rigor in documentation and communication, it was
the ability of both partners to regularly, quickly,
and semi-automatically monitor the evolution of
the joint resource with great confidence that has
enabled truly parallel development of a single,
shared HPSG grammar across continents. Within
a relatively short time, the partners succeeded
in adapting an existing grammar to a new genre
(email rather than spoken language) and domain
(customer service requests rather than appointment
scheduling), greatly extending grammatical cov-
erage (from initially around forty to above ninety
per cent on representative customer corpora), and
incorporating the grammar-based analysis engine
into a commercial product. And even though in
February 2002, for business reasons, YY decided
to reorganize grammar development for Japanese,
the distributed, parallel grammar development ef-
fort positively demonstrates that methodological
and technological advances in constraint-based
grammar engineering have enabled commercial
development and deployment of broad-coverage
HPSG implementations, a paradigm that until re-
cently was often believed to still lack the maturity
for real-world applications.
Acknowledgements
The DFKI ? YY partnership involved a large group
of people at both sites. We would like to thank
Kirk Oatman, co-founder of YY and first CEO,
and Hans Uszkoreit, Scientific Director at DFKI,
for their initiative and whole-hearted support to
the project; it takes vision for both corporate and
academic types to jointly develop an open-source
resource. Atsuko Shimada (from Saarland Uni-
versity), as part of a two-month internship at YY,
has greatly contributed to the preparation of repre-
sentative data samples, development of robust pre-
processing rules, and extensions to lexical cover-
age. Our colleague and friend Asahara-san (of the
Nara Advanced Institute of Technology, Japan),
co-developer of the open-source ChaSen tokenizer
and morphological analyzer for Japanese, was in-
strumental in the integration of ChaSen into the
YY product and also helped a lot in adapting and
(sometimes) fixing tokenization and morphology.
References
Asahara, M., & Matsumoto, Y. (2000). Extended
models and tools for high-performance part-of-
speech tagger. In Proceedings of the 18th In-
ternational Conference on Computational Lin-
guistics (pp. 21 ? 27). Saarbru?cken, Germany.
Butt, M., King, T. H., Nin?o, M.-E., & Segond, F.
(1999). A grammar writer?s cookbook. Stan-
ford, CA: CSLI Publications.
Callmeier, U. (2002). Preprocessing and encoding
techniques in PET. In S. Oepen, D. Flickinger,
J. Tsujii, & H. Uszkoreit (Eds.), Collabora-
tive language engineering. A case study in ef-
ficient grammar-based processing. Stanford,
CA: CSLI Publications. (forthcoming)
Carroll, J. (1994). Relating complexity to practi-
cal performance in parsing with wide-coverage
unification grammars. In Proceedings of the
32nd Meeting of the Association for Computa-
tional Linguistics (pp. 287 ? 294). Las Cruces,
NM.
Copestake, A. (2002). Implementing typed fea-
ture structure grammars. Stanford, CA: CSLI
Publications.
Erbach, G. (1991). An environment for exper-
imenting with parsing strategies. In J. My-
lopoulos & R. Reiter (Eds.), Proceedings of
IJCAI 1991 (pp. 931 ? 937). San Mateo, CA:
Morgan Kaufmann Publishers.
Erbach, G., & Uszkoreit, H. (1990). Grammar
engineering. Problems and prospects (CLAUS
Report # 1). Saarbru?cken, Germany: Compu-
tational Linguistics, Saarland University.
Flickinger, D., Copestake, A., & Sag, I. A. (2000).
HPSG analysis of English. In W. Wahlster
(Ed.), Verbmobil. Foundations of speech-to-
speech translation (Artificial Intelligence ed.,
pp. 321 ? 330). Berlin, Germany: Springer.
Geist, A., Bequelin, A., Dongarra, J., Manchek, W.
J. R., & Sunderam, V. (Eds.). (1994). PVM ?
parallel virtual machine. A users? guide and tu-
torial for networked parallel computing. Cam-
bridge, MA: The MIT Press.
Makino, T., Yoshida, M., Torisawa, K., & Tsu-
jii, J. (1998). LiLFeS ? towards a practical
HPSG parser. In Proceedings of the 17th In-
ternational Conference on Computational Lin-
guistics and the 36th Annual Meeting of the
Association for Computational Linguistics (pp.
807 ? 11). Montreal, Canada.
Mu?ller, S., & Kasper, W. (2000). HPSG analy-
sis of German. In W. Wahlster (Ed.), Verbmo-
bil. Foundations of speech-to-speech transla-
tion (Artificial Intelligence ed., pp. 238 ? 253).
Berlin, Germany: Springer.
Oepen, S., & Callmeier, U. (2000). Measure for
measure: Parser cross-fertilization. Towards
increased component comparability and ex-
change. In Proceedings of the 6th International
Workshop on Parsing Technologies (pp. 183 ?
194). Trento, Italy.
Oepen, S., & Flickinger, D. P. (1998). Towards
systematic grammar profiling. Test suite tech-
nology ten years after. Journal of Computer
Speech and Language, 12 (4) (Special Issue on
Evaluation), 411 ? 436.
Oepen, S., Netter, K., & Klein, J. (1997). TSNLP
? Test Suites for Natural Language Process-
ing. In J. Nerbonne (Ed.), Linguistic Databases
(pp. 13 ? 36). Stanford, CA: CSLI Publica-
tions.
Penn, G. (2000). Applying constraint handling
rules to HPSG. In Proceedings of the first in-
ternational conference on computational logic
(pp. 51 ? 68). London, UK.
Siegel, M. (2000). HPSG analysis of Japanese. In
W. Wahlster (Ed.), Verbmobil. Foundations of
speech-to-speech translation (Artificial Intelli-
gence ed., pp. 265 ? 280). Berlin, Germany:
Springer.
Siegel, M., & Bender, E. M. (2002). Efficient
deep processing of japanese. In Proceedings
of the 19th International Conference on Com-
putational Linguistics. Taipei, Taiwan.
Uszkoreit, H., Backofen, R., Busemann, S., Di-
agne, A. K., Hinkelman, E. A., Kasper, W.,
Kiefer, B., Krieger, H.-U., Netter, K., Neu-
mann, G., Oepen, S., & Spackman, S. P.
(1994). DISCO ? an HPSG-based NLP
system and its application for appointment
scheduling. In Proceedings of the 15th Interna-
tional Conference on Computational Linguis-
tics. Kyoto, Japan.
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 397?408,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Parser Evaluation over
Local and Non-Local Deep Dependencies in a Large Corpus
Emily M. Bender?, Dan Flickinger?, Stephan Oepen?, Yi Zhang?
?Dept of Linguistics, University of Washington, ?CSLI, Stanford University
?Dept of Informatics, Universitetet i Oslo, ?Dept of Computational Linguistics, Saarland University
ebender@uw.edu, danf@stanford.edu, oe@ifi.uio.no, yzhang@coli.uni-sb.de
Abstract
In order to obtain a fine-grained evaluation of
parser accuracy over naturally occurring text,
we study 100 examples each of ten reason-
ably frequent linguistic phenomena, randomly
selected from a parsed version of the En-
glish Wikipedia. We construct a correspond-
ing set of gold-standard target dependencies
for these 1000 sentences, operationalize map-
pings to these targets from seven state-of-the-
art parsers, and evaluate the parsers against
this data to measure their level of success in
identifying these dependencies.
1 Introduction
The terms ?deep? and ?shallow? are frequently used
to characterize or contrast different approaches to
parsing. Inevitably, such informal notions lack a
clear definition, and there is little evidence of com-
munity consensus on the relevant dimension(s) of
depth, let alne agreement on applicable metrics. At
its core, the implied dichotomy of approaches al-
ludes to differences in the interpretation of the pars-
ing task. Its abstract goal, on the one hand, could
be pre-processing of the linguistic signal, to enable
subsequent stages of analysis. On the other hand, it
could be making explicit the (complete) contribution
that the grammatical form of the linguistic signal
makes to interpretation, working out who did what
to whom. Stereotypically, one expects correspond-
ing differences in the choice of interface representa-
tions, ranging from various levels of syntactic anal-
ysis to logical-form representations of semantics.
In this paper, we seek to probe aspects of variation
in automated linguistic analysis. We make the as-
sumption that an integral part of many (albeit not all)
applications of parsing technology is the recovery of
structural relations, i.e. dependencies at the level of
interpretation. We suggest a selection of ten linguis-
tic phenomena that we believe (a) occur with reason-
ably high frequency in running text and (b) have the
potential to shed some light on the depths of linguis-
tic analysis. We quantify the frequency of these con-
structions in the English Wikipedia, then annotate
100 example sentences for each phenomenon with
gold-standard dependencies reflecting core proper-
ties of the phenomena of interest. This gold standard
is then used to estimate the recall of these dependen-
cies by seven commonly used parsers, providing the
basis for a qualitative discussion of the state of the
art in parsing for English.
In this work, we answer the call by Rimell et
al. (2009) for ?construction-focused parser evalua-
tion?, extending and complementing their work in
several respects: (i) we investigate both local and
non-local dependencies which prove to be challeng-
ing for many existing state-of-the-art parsers; (ii) we
investigate a wider range of linguistic phenomena,
each accompanied with an in-depth discussion of
relevant properties; and (iii) we draw our data from
the 50-million sentence English Wikipedia, which
is more varied and a thousand times larger than the
venerable WSJ corpus, to explore a more level and
ambitious playing field for parser comparison.
2 Background
All parsing systems embody knowledge about possi-
ble and probable pairings of strings and correspond-
ing linguistic structure. Such linguistic and proba-
bilistic knowledge can be hand-coded (e.g., as gram-
mar rules) or automatically acquired from labeled or
397
unlabeled training data. A related dimension of vari-
ation is the type of representations manipulated by
the parser. We briefly review some representative
examples along these dimensions, as these help to
position the parsers we subsequently evaluate.1
2.1 Approaches to parsing
Source of linguistic knowledge At one end of this
dimension, we find systems whose linguistic knowl-
edge is encoded in hand-crafted rules and lexical en-
tries; for English, the ParGram XLE system (Rie-
zler et al, 2002) and DELPH-IN English Resource
Grammar (ERG; Flickinger (2000))?each reflect-
ing decades of continuous development?achieve
broad coverage of open-domain running text, for ex-
ample. At the other end of this dimension, we find
fully unsupervised approaches (Clark, 2001; Klein
and Manning, 2004), where the primary source of
linguistic knowledge is co-occurrence patterns of
words in unannotated text. As Haghighi and Klein
(2006) show, augmenting this knowledge with hand-
crafted prototype ?seeds? can bring strong improve-
ments. Somewhere between these poles, a broad
class of parsers take some or all of their linguistic
knowledge from annotated treebanks, e.g. the Penn
Treebank (PTB), which encodes ?surface grammati-
cal analysis? (Marcus et al, 1993). Such approaches
include those that directly (and exclusively) use the
information in the treebank (e.g. Charniak (1997),
Collins (1999), Petrov et al (2006), inter alios) as
well as those that complement treebank structures
with some amount of hand-coded linguistic knowl-
edge (e.g. O?Donovan et al (2004), Miyao et al
(2004), Hockenmaier and Steedman (2007), inter
alios). Another hybrid in terms of its acquisition of
linguistic knowledge is the RASP system of Briscoe
et al (2006), combining a hand-coded grammar over
PoS tag sequences with a probabilistic tagger and
statistical syntactic disambiguation.
Design of representations Approaches to parsing
also differ fundamentally in the style of represen-
tation assigned to strings. These vary both in their
1Additional sources of variation among extant parsing tech-
nologies include (a) the behavior with respect to ungrammatical
inputs and (b) the relationship between probabilistic and sym-
bolic knowledge in the parser, where parsers with a hand-coded
grammar at their core typically also incorporate an automati-
cally trained probabilistic disambiguation component.
formal nature and the ?granularity? of linguistic in-
formation (i.e. the number of distinctions assumed),
encompassing variants of constituent structure, syn-
tactic dependencies, or logical-form representations
of semantics. Parser interface representations range
between the relatively simple (e.g. phrase structure
trees with a limited vocabulary of node labels as in
the PTB, or syntactic dependency structures with a
limited vocabulary of relation labels as in Johansson
and Nugues (2007)) and the relatively complex, as
for example elaborate syntactico-semantic analyses
produced by the ParGram or DELPH-IN grammars.
There tends to be a correlation between the
methodology used in the acquisition of linguistic
knowledge and the complexity of representations: in
the creation of a mostly hand-crafted treebank like
the PTB, representations have to be simple enough
for human annotators to reliably manipulate. Deriv-
ing more complex representations typically presup-
poses further computational support, often involv-
ing some hand-crafted linguistic knowledge?which
can take the form of mappings from PTB-like repre-
sentations to ?richer? grammatical frameworks (as
in the line of work by O?Donovan et al (2004), and
others; see above), or can be rules for creating the
parse structures in the first place (i.e. a computa-
tional grammar), as for example in the treebanks of
van der Beek et al (2002) or Oepen et al (2004).2
In principle, one might expect that richer repre-
sentations allow parsers to capture complex syntac-
tic or semantic dependencies more explicitly. At the
same time, such ?deeper? relations may still be re-
coverable (to some degree) from comparatively sim-
ple parser outputs, as demonstrated for unbounded
dependency extraction from strictly local syntactic
dependency trees by Nivre et al (2010).
2.2 An armada of parsers
Stanford Parser (Klein and Manning, 2003) is a
probabilistic parser which can produce both phrase
structure trees and grammatical relations (syntactic
dependencies). The parsing model we evaluate is the
2A noteworthy exception to this correlation is the annotated
corpus of Zettlemoyer and Collins (2005), which pairs sur-
face strings from the realm of natural language database inter-
faces directly with semantic representations in lambda calculus.
These were hand-written on the basis of database query state-
ments distributed with the original datasets.
398
English factored model which combines the prefer-
ences of unlexicalized PCFG phrase structures and
of lexical dependencies, trained on sections 02?21
of the WSJ portion of the PTB. We chose Stanford
Parser from among the state-of-the-art PTB-derived
parsers for its support for grammatical relations as
an alternate interface representation.
Charniak&Johnson Reranking Parser (Char-
niak and Johnson, 2005) is a two-stage PCFG parser
with a lexicalized generative model for the first-
stage, and a discriminative MaxEnt reranker for the
second-stage. The models we evaluate are also
trained on sections 02?21 of the WSJ. Top-50 read-
ings were used for the reranking stage. The output
constituent trees were then converted into Stanford
Dependencies. According to Cer et al (2010), this
combination gives the best parsing accuracy in terms
of Stanford dependencies on the PTB.
Enju (Miyao et al, 2004) is a probabilistic HPSG
parser, combining a hand-crafted core grammar with
automatically acquired lexical types from the PTB.3
The model we evaluate is trained on the same ma-
terial from the WSJ sections of the PTB, but the
treebank is first semi-automatically converted into
HPSG derivations, and the annotation is enriched
with typed feature structures for each constituent.
In addition to HPSG derivation trees, Enju also pro-
duces predicate argument structures.
C&C (Clark and Curran, 2007) is a statistical
CCG parser. Abstractly similar to the approach of
Enju, the grammar and lexicon are automatically
induced from CCGBank (Hockenmaier and Steed-
man, 2007), a largely automatic projection of (the
WSJ portion of) PTB trees into the CCG framework.
In addition to CCG derivations, the C&C parser can
directly output a variant of grammatical relations.
RASP (Briscoe et al, 2006) is an unlexicalized
robust parsing system, with a hand-crafted ?tag se-
quence? grammar at its core. The parser thus anal-
yses a lattice of PoS tags, building a parse forest
from which the most probable syntactic trees and
sets of corresponding grammatical relations can be
extracted. Unlike other parsers in our mix, RASP
did not build on PTB data in either its PoS tagging
3This hand-crafted grammar is distinct from the ERG, de-
spite sharing the general framework of HPSG. The ERG is not
included in our evaluation, since it was used in the extraction of
the original examples and thus cannot be fairly evaluated.
or syntactic disambiguation components.
MSTParser (McDonald et al, 2005) is a data-
driven dependency parser. The parser uses an edge-
factored model and searches for a maximal span-
ning tree that connects all the words in a sentence
into a dependency tree. The model we evaluate
is the second-order projective model trained on the
same WSJ corpus, where the original PTB phrase
structure annotations were first converted into de-
pendencies, as established in the CoNLL shared task
2009 (Johansson and Nugues, 2007).
XLE/ParGram (Riezler et al, 2002, see also
Cahill et al, 2008) applies a hand-built Lexical
Functional Grammar for English and a stochastic
parse selection model. For our evaluation, we used
the Nov 4, 2010 release of XLE and the Nov 25,
2009 release of the ParGram English grammar, with
c-structure pruning turned off and resource limita-
tions set to the maximum possible to allow for ex-
haustive search. In particular, we are evaluating the
f-structures output by the system.
Each parser, of course, has its own requirements
regarding preprocessing of text, especially tokeniza-
tion. We customized the tokenization to each parser,
by using the parser?s own internal tokenization or
pre-tokenizing to match the parser?s desired input.
The evaluation script is robust to variations in tok-
enization across parsers.
3 Phenomena
In this section we summarize the ten phenomena we
explore and our motivations for choosing them. Our
goal was to find phenomena where the relevant de-
pendencies are relatively subtle, such that more lin-
guistic knowledge is beneficial in order to retrieve
them. Though this set is of course only a sampling,
these phenomena illustrate the richness of structure,
both local and non-local, involved in the mapping
from English strings to their meanings. We discuss
the phenomena in four sets and then briefly review
their representation in the Penn Treebank.
3.1 Long distance dependencies
Three of our phenomena can be classified as involv-
ing long-distance dependencies: finite that-less rel-
atives clauses (?barerel?), tough adjectives (?tough?)
and right node raising (?rnr?). These are illustrated
399
in the following examples:4
(1) barerel: This is the second time in a row Aus-
tralia has lost their home tri-nations? series.
(2) tough: Original copies are very hard to find.
(3) rnr: Ilu?vatar, as his names imply, exists before
and independently of all else.
While the majority of our phenomena involve lo-
cal dependencies, we include these long-distance
dependency types because they are challenging for
parsers and enable more direct comparison with the
work of Rimell et al (2009), who also address right
node raising and bare relatives. Our barerel category
corresponds to their ?object reduced relative? cate-
gory with the difference that we also include adverb
relatives, where the head noun functions as a modi-
fier within the relative clause, as does time in (1). In
contrast, our rnr category is somewhat narrower than
Rimell et al (2009)?s ?right node raising? category:
where they include raised modifiers, we restrict our
category to raised complements.
Part of the difficulty in retrieving long-distance
dependencies is that the so-called extraction site is
not overtly marked in the string. In addition to this
baseline level of complication, these three construc-
tion types present further difficulties: Bare relatives,
unlike other relative clauses, do not carry any lexi-
cal cues to their presence (i.e., no relative pronouns).
Tough adjective constructions require the presence
of specific lexical items which form a subset of a
larger open class. They are rendered more difficult
by two sources of ambiguity: alternative subcatego-
rization frames for the adjectives and the purposive
adjunct analysis (akin to in order to) for the infiniti-
val VP. Finally, right node raising often involves co-
ordination where one of the conjuncts is in fact not
a well-formed phrase (e.g., independently of in (3)),
making it potentially difficult to construct the correct
coordination structure, let alne associate the raised
element with the correct position in each conjunct.
3.2 Non-dependencies
Two of our phenomena crucially look for the lack of
dependencies. These are it expletives (?itexpl?) and
verb-particle constructions (?vpart?):
4All examples are from our data. Words involved in the rel-
evant dependencies are highlighted in italics (dependents) and
boldface (heads).
(4) itexpl: Crew negligence is blamed, and it is sug-
gested that the flight crew were drunk.
(5) vpart: He once threw out two baserunners at
home in the same inning.
The English pronoun it can be used as an ordi-
nary personal pronoun or as an expletive: a place-
holder for when the language demands a subject (or
occasionally object) NP but there is no semantic role
for that NP. The expletive it only appears when it
is licensed by a specific construction (such as ex-
traposition, (4)) or selecting head. If the goal of
parsing is to recover from the surface string the de-
pendencies capturing who did what to whom, exple-
tive it should not feature in any of those dependen-
cies. Likewise, instances of expletive it should be
detected and discarded in reference resolution. We
hypothesize that detecting expletive it requires en-
coding linguistic knowledge about its licensers.
The other non-dependency we explore is between
the particle in verb-particle constructions and the
direct object. Since English particles are almost
always homophonous with prepositions, when the
object of the verb-particle pair follows the par-
ticle, there will always be a competing analysis
which analyses the sequence as V+PP rather than
V+particle+NP. Furthermore, since verb-particle
pairs often have non-compositional semantics (Sag
et al, 2002), misanalyzing these constructions could
be costly to downstream components.
3.3 Phrasal modifiers
Our next category concerns modifier phrases:
(6) ned: Light colored glazes also have softening
effects when painted over dark or bright images.
(7) absol: The format consisted of 12 games, each
team facing the other teams twice.
The first, (?ned?), is a pattern which to our knowl-
edge has not been named in the literature, where a
noun takes the typically verbal -ed ending, is modi-
fied by another noun or adjective, and functions as a
modifier or a predicate. We believe this phenomenon
to be interesting because its unusual morphology is
likely to lead PoS-taggers astray, and because the
often-hyphenated Adj+N-ed constituent has produc-
tive internal structure constraining its interpretation.
The second phrasal modifier we investigate is the
absolutive construction. An absolutive consists of an
400
NP followed by a non-finite predicate (such as could
appear after the copula be). The whole phrase mod-
ifies a verbal projection that it attaches to. Absolu-
tives may be marked with with or unmarked. Here,
we focus on the unmarked type as this lack of lexical
cue can make the construction harder to detect.
3.4 Subtle arguments
Our final three phenomena involve ways in which
verbal arguments can be more difficult to identify
than in ordinary finite clauses. These include de-
tecting the arguments of verbal gerunds (?vger?), the
interleaving of arguments and adjuncts (?argadj?) and
raising/control (?control?) constructions.
(8) vger: Accessing the website without the ?www?
subdomain returned a copy of the main site for
?EP.net?.
(9) argadj: The story shows, through flashbacks, the
different histories of the characters.
(10) control: Alfred ?retired? in 1957 at age 60 but
continued to paint full time.
In a verbal gerund, the -ing form a verb retains
verbal properties (e.g., being able to take NP com-
plements, rather than only PP complements) but
heads a phrase that fills an NP position in the syn-
tax (Malouf, 2000). Since gerunds have the same
morphology as present participle VPs, their role in
the larger clause is susceptible to misanalysis.
The argadj examples are of interest because En-
glish typically prefers to have direct objects directly
adjacent to the selecting verb. Nonetheless, phe-
nomena such as parentheticals and heavy-NP shift
(Arnold et al, 2000), in which ?heavy? constituents
appear further to the right in the string, allow for
adjunct-argument order in a minority of cases. We
hypothesize that the relative infrequency of this con-
struction will lead parsers to prefer incorrect analy-
ses (wherein the adjunct is picked up as a comple-
ment, the complement as an adjunct or the structure
differs entirely) unless they have access to linguis-
tic knowledge providing constraints on possible and
probable complementation patterns for the head.
Finally, we turn to raising and control verbs (?con-
trol?) (e.g., Huddleston and Pullum (2002, ch. 14)).
These verbs select for an infinitival VP complement
and stipulate that another of their arguments (sub-
ject or direct object in the examples we explore) is
identified with the unrealized subject position of the
infinitival VP. Here it is the dependency between
the infinitval VP and the NP argument of the ?up-
stairs? verb which we expect to be particularly sub-
tle. Getting this right requires specific lexical knowl-
edge about which verbs take these complementation
patterns. This lexical knowledge needs to be repre-
sented in such a way that it can be used robustly even
in the case of passives, relative clauses, etc.5
3.5 Penn Treebank representations
We investigated the representation of these 10 phe-
nomena in the PTB (Marcus et al, 1993) in two
steps: First we explored the PTB?s annotation guide-
lines (Bies et al, 1995) to determine how the rele-
vant dependencies were intended to be represented.
We then used Ghodke and Bird?s (2010) Treebank
Search to find examples of the intended annotations
as well as potential examples of the phenomena an-
notated differently, to get a sense of the consistency
of the annotation from both precision and recall per-
spectives. In this study, we take the phrase structure
trees of the PTB to represent dependencies based on
reasonable identification of heads.
The barerel, vpart, and absol phenomena are com-
pletely unproblematic, with their relevant dependen-
cies explicitly and reliably represented. In addition,
the tough construction is reliably annotated, though
one of the dependencies we take to be central is not
directly represented: The missing argument is linked
to a null wh head at the left edge of the comple-
ment of the tough predicate, rather than to its sub-
ject. Two further phenomena (rnr and vger) are es-
sentially correctly represented: the representations
of the dependencies are explicit and mostly but not
entirely consistently applied. Two out of a sample of
20 examples annotated as containing rnr did not, and
two out of a sample of 35 non-rnr-annotated coordi-
nations actually contained rnr. For vger the primary
problem is with the PoS tagging, where the gerund
is sometimes given a nominal tag, contrary to PTB
guidelines, though the structure above it conforms.
The remaining four constructions are more prob-
lematic. In the case of object control, while the guide-
5Distinguishing between raising and control requires fur-
ther lexical knowledge and is another example of a ?non-
dependency? (in the raising examples). We do not draw that
distinction in our annotations.
401
lines specify an analysis in which the shared NP is
attached as the object of the higher verb, the PTB
includes not only structures conforming to that anal-
ysis but also ?small clause? structures, with the latter
obscuring the relationship of the shared argument to
the higher verb. In the case of itexpl, the adjoined
(S(-NONE- *EXP*)) indicating an expletive use of
it is applied consistently for extraposition (as pre-
scribed in the guidelines). However, the set of lex-
ical licensers of the expletive is incomplete. For ar-
gadj we run into the problem that the PTB does not
explicitly distinguish between post-verbal modifiers
and verbal complements in the way that they are at-
tached. The guidelines suggest that the function tags
(e.g., PP-LOC, etc.) should allow one to distinguish
these, but examination of the PTB itself suggests
that they are not consistently applied. Finally, the
ned construction is not mentioned in the PTB guide-
lines nor is its internal structure represented in the
treebank. Rather, strings like gritty-eyed are left un-
segmented and tagged as JJ.
We note that the PTB representations of many of
these phenomena (barerel, tough, rnr, argadj, control,
itexpl) involve empty elements and/or function tags.
Systems that strip these out before training, as is
common practice, will not benefit from the informa-
tion that is in the PTB.
Our purpose here is not to criticize the PTB,
which has been a tremendously important resource
to the field. Rather, we have two aims: The first is
to provide context for the evaluation of PTB-derived
parsers on these phenomena. The second is to high-
light the difficulty of producing consistent annota-
tions of any complexity as well as the hurdles faced
by a hand-annotation approach when attempting to
scale a resource to more complex representations
and/or additional phenomena (though cf. Vadas and
Curran (2008) on improving PTB representations).
4 Methodology
4.1 Data extraction
We processed 900 million tokens of Wikipedia text
using the October 2010 release of the ERG, follow-
ing the work of the WikiWoods project (Flickinger
et al, 2010). Using the top-ranked ERG deriva-
tion trees as annotations over this corpus and sim-
ple patterns using names of ERG-specific construc-
Phenomenon Frequency Candidates
barerel 2.12% 546
tough 0.07% 175
rnr 0.69% 1263
itexpl 0.13% 402
vpart 4.07% 765
ned 1.18% 349
absol 0.51% 963
vger 5.16% 679
argadj 3.60% 1346
control 3.78% 124
Table 1: Relative frequencies of phenomena matches in
Wikipedia, and number of candidate strings vetted.
tions or lexical types, we randomly selected a set
of candidate sentences for each of our ten phenom-
ena. These candidates were then hand-vetted in se-
quence by two annotators to identify, for each phe-
nomenon, 100 examples that do in fact involve the
phenomenon in question and which are both gram-
matical and free of typos. Examples that were ei-
ther deemed overly basic (e.g. plain V+V coordi-
nation, which the ERG treats as rnr) or inappropri-
ately complex (e.g. non-constituent coordination ob-
scuring the interleaving of arguments and adjuncts)
were also discarded at this step. Table 1 summarizes
relative frequencies of each phenomenon in about
47 million parsed Wikipedia sentences, as well as
the total size of the candidate sets inspected. For
the control and tough phenomena hardly any filtering
for complexity was applied, hence these can serve
as indicators of the rate of genuine false positives.
For phenomena that partially overlap with those of
Rimell et al (2009), it appears our frequency es-
timates are comparable to what they report for the
Brown Corpus (but not the WSJ portion of the PTB).
4.2 Annotation format
We annotated up to two dependency triples per phe-
nomenon instance, identifying the heads and depen-
dents by the surface form of the head words in the
sentence suffixed with a number indicating word po-
sition (see Table 2).6 Some strings contain more
than one instance of the phenomenon they illustrate;
in these cases, multiple sets of dependencies are
6As the parsers differ in tokenization strategies, our evalua-
tion script treats these position IDs as approximate indicators.
402
Item ID Phenomenon Polarity Dependency
1011079100200 absol 1 having-2|been-3|passed-4 ARG act-1
1011079100200 absol 1 withdrew-9 MOD having-2|been-3|passed-4
1011079100200 absol 1 carried+on-12 MOD having-2|been-3|passed-4
Table 2: Sample annotations for sentence # 1011079100200: The-0 act-1 having-2 been-3 passed-4 in-5 that-6 year-7
Jessop-8 withdrew-9 and-10 Whitworth-11 carried-12 on-13 with-14 the-15 assistance-16 of-17 his-18 son-19.
Phenomenon Head Type Dependent Distance
Bare relatives gapped predicate in relative ARG2/MOD modified noun 3.0 (8)
(barerel) modified noun MOD top predicate of relative 3.3 (8)
Tough adjectives tough adjective ARG2 to-VP complement 1.7 (5)
(tough) gapped predicate in to-VP ARG2 subject/modifiee of adjective 6.4 (21)
Right Node Raising verb/prep2 ARG2 shared noun 2.8 (9)
(rnr) verb/prep1 ARG2 shared noun 6.1 (12)
Expletive It it-subject taking verb !ARG1 it 1.2 (3)
(itexpl) raising-to-object verb !ARG2 it ?
Verb+particle constructions particle !ARG2 complement 2.7 (9)
(vpart) verb+particle ARG2 complement 3.7 (10)
Adj/Noun2 + Noun1-ed head noun MOD Noun1-ed 2.4 (17)
(ned) Noun1-ed ARG1/MOD Adj/Noun2 1.0 (1.5)
Absolutives absolutive predicate ARG1 subject of absolutive 1.7 (12)
(absol) main clause predicate MOD absolutive predicate 9.8 (26)
Verbal gerunds selecting head ARG[1,2] gerund 1.9 (13)
(vger) gerund ARG2/MOD first complement/modifier of gerund 2.3 (8)
Interleaved arg/adj selecting verb MOD interleaved adjunct 1.2 (7)
(argadj) selecting verb ARG[2,3] displaced complement 5.9 (26)
Control ?upstairs? verb ARG[2,3] ?downstairs? verb 2.4 (23)
(control) ?downstairs? verb ARG1 shared argument 4.8 (17)
Table 3: Dependencies labeled for each phenomenon type, including average and maximum surface distances.
recorded. In addition, some strings evince more than
one of the phenomena we are studying. However,
we only annotate the dependencies associated with
the phenomenon the string was selected to repre-
sent. Finally, in examples with coordinated heads or
dependents, we recorded separate dependencies for
each conjunct. In total, we annotated 2127 depen-
dency triples for the 1000 sentences, including 253
negative dependencies (see below). Table 3 outlines
the dependencies annotated for each phenomenon.
To allow for multiple plausible attachment sites,
we give disjunctive values for heads or dependents
in several cases: (i) with auxiliaries, (ii) with com-
plementizers (that or to, as in Table 2), (iii) in cases
of measure or classifier nouns or partitives, (iv) with
multi-word proper names and (v) where there is
genuine attachment ambiguity for modifiers. As
these sets of targets are disjunctive, these conven-
tions should have the effect of increasing measured
parser performance. 580 (27%) of the annotated de-
pendencies had at least one disjunction.
4.3 Annotation and reconciliation process
The entire data set was annotated independently by
two annotators. Both annotators were familiar with
the ERG, used to identify these sentences in the
WikiWoods corpus, but the annotation was done
without reference to the ERG parses. Before begin-
ning annotation on each phenomenon, we agreed on
which dependencies to annotate. We also communi-
cated with each other about annotation conventions
as the need for each convention became clear. The
annotation conventions address how to handle co-
ordination, semantically empty auxiliaries, passives
and similar orthogonal phenomena.
Once the entire data set was dual-annotated, we
compared annotations, identifying the following
sources of mismatch: typographical errors, incom-
pletely specified annotation conventions, inconsis-
tent application of conventions (101 items, dropping
in frequency as the annotation proceeded), and gen-
uine disagreement about what to annotate, either dif-
ferent numbers of dependencies of interest identified
403
in an item (59 items) or conflicting elements in a de-
pendency (54 items).7 Overall, our initial annotation
pass led to agreement on 79% of the items, and a
higher per-dependency level of agreement. Agree-
ment could be expected to approach 90% with more
experience in applying annotation conventions.
We then reconciled the annotations, using the
comparison to address all sources of difference. In
most cases, we readily agreed which annotation was
correct and which was in error. In a few cases, we
decided that both annotations were plausible alter-
natives (e.g., in terms of alternative attachment sites
for modifiers) and so created a single merged anno-
tation expressing the disjunction of both (cf. ? 4.2).
5 Evaluation
With the test data consisting of 100 items for each of
our ten selected phenomena, we ran all seven pars-
ing systems and recorded their dependency-style
outputs for each sentence. While these outputs
are not directly comparable with each other, we
were able to associate our manually-annotated tar-
get dependencies with parser-specific dependencies,
by defining sets of phenomenon-specific regular ex-
pressions for each parser. In principle, we allow this
mapping to be somewhat complex (and forgiving to
non-contentful variation), though we require that it
work deterministically and not involve specific lexi-
cal information. An example set is given in Fig. 2.
"absol" =>
{?ARG1? => [
?\(ncsubj \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+) _\)?,
?\(ncmod _ \W*{W2}\W*_(\d+) \W*{W1}\W*_(\d+)\)?],
?ARG? => [
?\(ncsubj \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+) _\)?,
?\(ncmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?],
?MOD? => [
?\(xmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?,
?\(ncmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?,
?\(cmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?]}
Figure 2: Regexp set to evaluate C&C for absol.
These expressions fit the output that we got from the
C&C parser, illustrated in Fig. 3 with a relevant por-
tion of the dependencies produced for the example
in Table 2. Here the C&C dependency (ncsubj
passed 4 Act 1 ) matches the first target in the
7We do not count typographical errors or incompletely spec-
ified conventions as failures of inter-annotator agreement.
gold-standard (Table 2), but no matching C&C de-
pendency is found for the other two targets.
(xmod _ Act_1 passed_4)
(ncsubj passed_4 Act_1 _)
(ncmod _ withdrew,_9 Jessop_8)
(dobj year,_7 withdrew,_9)
Figure 3: Excerpts of C&C output for item in Table 2.
The regular expressions operate solely on the de-
pendency labels and are not lexically-specific. They
are specific to each phenomenon, as we did not at-
tempt to write a general dependency converter, but
rather to discover what patterns of dependency rela-
tions describe the phenomenon when it is correctly
identified by each parser. Thus, though we did not
hold out a test set, we believe that they would gener-
alize to additional gold standard material annotated
in the same way for the same phenomena.8
In total, we wrote 364 regular expressions to han-
dle the output of the seven parsers, allowing some
leeway in the role labels used by a parser for any
given target dependency. The supplementary mate-
rials for this paper include the test data, parser out-
puts, target annotations, and evaluation script.
Fig. 1 provides a visualization of the results of our
evaluation. Each column of points represents one
dependency type. Dependency types for the same
phenomenon are represented by adjacent columns.
The order of the columns within a phenomenon fol-
lows the order of the dependency descriptions in
Table 3: For each pair, the dependency type with
the higher score for the majority of the parsers is
shown first (to the left). The phenomena them-
selves are also arranged according to increasing (av-
erage) difficulty. itexpl only has one column, as we
annotated just one dependency per instance here.
(The two descriptions in Table 3 reflect different,
mutually-incompatible instance types.) Since ex-
pletive it should not be the semantic dependent of
any head, the targets are generalized for this phe-
nomenon and the evaluation script counts as incor-
8In the case of the XLE, our simplistic regular-expression
approach to the interpretation of parser outputs calls for much
more complex patterns than for the other parsers. This is owed
to the rich internal structure of LFG f-structures and higher
granularity of linguistic analysis, where feature annotations on
nodes as well as reentrancies need to be taken into account.
Therefore, our current results for the XLE admit small amounts
of both over- and under-counting.
404
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
vger
vpart
control
argadj
barerel
rnr tough
ned itexpl
absol
enju
xle
c&j
c&c
stanford
mst
rasp
Figure 1: Individual dependency recall for seven parsers over ten phenomena.
rect any dependency involving referential it.
We observe fairly high recall of the dependencies
for vpart and vger (with the exception of RASP), and
high recall for both dependencies representing con-
trol for five systems. While Enju, Stanford, MST,
and RASP all found between 70 and 85% of the de-
pendency between the adjective and its complement
in the tough construction, only Enju and XLE rep-
resented the dependency between the subject of the
adjective and the gap inside the adjective?s comple-
ment. For the remaining phenomena, each parser
performed markedly worse on one dependency type,
compared to the other. The only exceptions here
are XLE and C&C?s (and to a lesser extent, C&J?s)
scores for barerel. No system scored higher than
33% on the harder of the two dependencies in rnror
absol, and Stanford, MST, and RASP all scored be-
low 25% on the harder dependency in barerel. Only
XLE scored higher than 10% on the second depen-
dency for ned and higher than 50% for itexpl.
6 Discussion
From the results in Fig. 1, it is clear that even the best
of these parsers fail to correctly identify a large num-
ber of relevant dependencies associated with linguis-
tic phenomena that occur with reasonable frequency
in the Wikipedia. Each of the parsers attempts
with some success to analyze each of these phe-
nomena, reinforcing the claim of relevance, but they
vary widely across phenomena. For the two long-
distance phenomena that overlap with those studied
in Rimell et al (2009), our results are comparable.9
Our evaluation over Wikipedia examples thus shows
the same relative lack of success in recovering long-
distance dependencies that they found for WSJ sen-
tences. The systems did better on relatively well-
studied phenomena including control, vger, and vpart,
but had less success with the rest, even though all but
two of those remaining phenomena involve syntac-
tically local dependencies (as indicated in Table 3).
Successful identification of the dependencies in
these phenomena would, we hypothesize, benefit
from richer (or deeper) linguistic information when
parsing, whether it is lexical (tough, control, itexpl,
and vpart), or structural (rnr, absol, vger, argadj, and
barerel), or somewhere in between, as with ned. In
the case of treebank-trained parsers, for the informa-
tion to be available, it must be consistently encoded
in the treebank and attended to during training. As
9Other than Enju, which scores 16 points higher in the eval-
uation of Rimell et al, our average scores for each parser across
the dependencies for these phenomena are within 12 points of
those reported by Rimell et al (2009) and Nivre et al (2010).
405
noted in Sections 2.1 and 3.5, there is tension be-
tween developing sufficiently complex representa-
tions to capture linguistic phenomena and keeping
an annotation scheme simple enough that it can be
reliably produced by humans, in the case of hand-
annotation.
7 Related Work
This paper builds on a growing body of work which
goes beyond (un)labeled bracketing in parser evalua-
tion, including Lin (1995), Carroll et al (1998), Ka-
plan et al (2004), Rimell et al (2009), and Nivre et
al. (2010). Most closely related are the latter two of
the above, as we adopt their ?construction-focused
parser evaluation methodology?.
There are several methodological differences be-
tween our work and that of Rimell et al First, we
draw our evaluation data from a much larger and
more varied corpus. Second, we automate the com-
parison of parser output to the gold standard, and we
distribute the evaluation scripts along with the anno-
tated corpus, enhancing replicability. Third, where
Rimell et al extract evaluation targets on the basis
of PTB annotations, we make use of a linguistically
precise broad-coverage grammar to identify candi-
date examples. This allows us to include both local
and non-local dependencies not represented or not
reliably encoded in the PTB, enabling us to evalu-
ate parser performance with more precision over a
wider range of linguistic phenomena.
These methodological innovations bring two em-
pirical results. The first is qualitative: Where previ-
ous work showed that overall Parseval numbers hide
difficulties with long-distance dependencies, our re-
sults show that there are multiple kinds of reason-
ably frequent local dependencies which are also dif-
ficult for the current standard approaches to pars-
ing. The second is quantitative: Where Rimell et
al. found two phenomena which were virtually un-
analyzed (recall below 10%) for one or two parsers
each, we found eight phenomena which were vir-
tually unanalyzed by at least one system, includ-
ing two unanalyzed by five and one by six. Every
system had at least one virtually unanalyzed phe-
nomenon. Thus we have shown that the dependen-
cies being missed by typical modern approaches to
parsing are more varied and more numerous than
previously thought.
8 Conclusion
We have presented a detailed construction-focused
evaluation of seven parsers over 10 phenomena,
with 1000 examples drawn from English Wikipedia.
Gauging recall of such ?deep? dependencies, in our
view, can serve as a proxy for downstream pro-
cessing involving semantic interpretation of parser
outputs. Our annotations and automated evaluation
script are provided in the supplementary materials,
for full replicability. Our results demonstrate that
significant opportunities remain for parser improve-
ment, and highlight specific challenges that remain
invisible in aggregate parser evaluation (e.g. Parse-
val or overall dependency accuracy). These results
suggest that further progress will depend on train-
ing data that is both more extensive and more richly
annotated than what is typically used today (seeing,
for example, that a large part of more detailed PTB
annotation remains ignored in much parsing work).
There are obvious reasons calling for diversity in
approaches to parsing and for different trade-offs
in, for example, the granularity of linguistic analy-
sis, average accuracy, cost of computation, or ease
of adaptation. Our proposal is not to substitute
construction-focused evaluation on Wikipedia data
for widely used aggregate metrics and reference cor-
pora, but rather to augment such best practices in
the spirit of Rimell et al (2009) and expand the
range of phenomena considered in such evaluations.
Across frameworks and traditions (and in principle
languages), it is of vital importance to be able to
evaluate the quality of parsing (and grammar induc-
tion) algorithms in a maximally informative manner.
Acknowledgments
We are grateful to Tracy King for her assistance in
setting up the XLE system and to three anonymous
reviewers for helpful comments. The fourth author
thanks DFKI and the DFG funded Excellence Clus-
ter on MMCI for their support of the work. Data
preparation on the scale of Wikipedia was made pos-
sible through access to large-scale HPC facilities,
and we are grateful to the Scientific Computing staff
at UiO and the Norwegian Metacenter for Computa-
tional Science.
406
References
Jennifer E. Arnold, Thomas Wasow, Anthony Losongco,
and Ryan Ginstrom. 2000. Heaviness vs. newness:
The effects of structural complexity and discourse sta-
tus on constituent ordering. Language, 76(1):28?55.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank II
style Penn treebank project. Technical report, Univer-
sity of Pennsylvania.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions, pages 77?80, Sydney, Australia.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2008. Speeding
up LFG parsing using c-structure pruning. In Coling
2008: Proceedings of the workshop on Grammar En-
gineering Across Frameworks, pages 33?40, Manch-
ester, England, August. Coling 2008 Organizing Com-
mittee.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: A survey and a new proposal.
In Proceedings of the International Conference on
Language Resources and Evaluation, pages 447?454,
Granada.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
Stanford dependencies: Trade-offs between speed and
accuracy. In 7th International Conference on Lan-
guage Resources and Evaluation (LREC 2010), Malta.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 173?180, Ann Arbor, Michigan.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Artifi-
cial Intelligence, pages 598 ? 603, Providence, RI.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Alexander Clark. 2001. Unsupervised induction
of stochastic context-free grammars using distribu-
tional clustering. In Proceedings of the 5th Confer-
ence on Natural Language Learning, pages 105?112.
Toulouse, France.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.
2010. WikiWoods. Syntacto-semantic annotation for
English Wikipedia. In Proceedings of the 6th Interna-
tional Conference on Language Resources and Evalu-
ation, Valletta, Malta.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1) (Special Issue on Efficient Processing
with HPSG):15 ? 28.
Sumukh Ghodke and Steven Bird. 2010. Fast query for
large treebanks. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 267?275, Los Angeles, California, June.
Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
grammar induction. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 881?888, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
In Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia.
Ron Kaplan, Stefan Riezler, Tracy H King, John T
Maxwell III, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Susan Dumais, Daniel Marcu,
and Salim Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 97?104, Boston, Massachusetts,
USA, May. Association for Computational Linguis-
tics.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10, Cambridge, MA. MIT
Press.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics, pages 478?485, Barcelona, Spain.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings of
IJCAI-95, pages 1420?1425, Montreal, Canada.
Robert Malouf. 2000. Verbal gerunds as mixed cate-
gories in HPSG. In Robert Borsley, editor, The Nature
407
and Function of Syntactic Categories, pages 133?166.
Academic Press, New York.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceedings
of the 2005 Conference on Empirical Methods in Natu-
ral Language Processing, pages 523?530, Vancouver,
Canada.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2004. Corpus-oriented grammar development for ac-
quiring a Head-driven Phrase Structure Grammar from
the Penn Treebank. In Proceedings of the 1st Interna-
tional Joint Conference on Natural Language Process-
ing, pages 684?693, Hainan Island, China.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos
Go?mez Rodr??guez. 2010. Evaluation of dependency
parsers on unbounded dependencies. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 833?841, Beijing, China.
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef
Van Genabith, and Andy Way. 2004. Large-scale in-
duction and evaluation of lexical resources from the
penn-ii treebank. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics,
pages 367?374, Barcelona, Spain.
Stephan Oepen, Daniel Flickinger, Kristina Toutanova,
and Christopher D. Manning. 2004. LinGO Red-
woods. A rich and dynamic treebank for HPSG.
Journal of Research on Language and Computation,
2(4):575 ? 596.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Meet-
ing of the Association for Computational Linguistics,
Philadelphia, PA.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009.
Unbounded dependency recovery for parser evalua-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 813?821, Singapore. Association for Computa-
tional Linguistics.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions. A pain in the neck for NLP. In Alexander Gel-
bukh, editor, Computational Linguistics and Intelli-
gent Text Processing, volume 2276 of Lecture Notes in
Computer Science, pages 189?206. Springer, Berlin,
Germany.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proceedings of ACL-
08: HLT, pages 335?343, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
L. van der Beek, Gosse Bouma, Robert Malouf, and Gert-
jan van Noord. 2002. The Alpino Dependency Tree-
bank. In Mariet Theune, Anton Nijholt, and Hen-
dri Hondorp, editors, Computational Linguistics in the
Netherlands, Amsterdam, The Netherlands. Rodopi.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of the Twenty-First Annual Conference on
Uncertainty in Artificial Intelligence, pages 658?666,
Arlington, Virginia. AUAI Press.
408
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 582?586,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Multimodal Grammar Implementation
Katya Alahverdzhieva
University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB
K.Alahverdzhieva@sms.ed.ac.uk
Dan Flickinger
Stanford University
Stanford, CA 94305-2150
danf@stanford.edu
Alex Lascarides
University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB
alex@inf.ed.ac.uk
Abstract
This paper reports on an implementation of a
multimodal grammar of speech and co-speech
gesture within the LKB/PET grammar engi-
neering environment. The implementation ex-
tends the English Resource Grammar (ERG,
Flickinger (2000)) with HPSG types and rules
that capture the form of the linguistic signal,
the form of the gestural signal and their rel-
ative timing to constrain the meaning of the
multimodal action. The grammar yields a sin-
gle parse tree that integrates the spoken and
gestural modality thereby drawing on stan-
dard semantic composition techniques to de-
rive the multimodal meaning representation.
Using the current machinery, the main chal-
lenge for the grammar engineer is the non-
linear input: the modalities can overlap tem-
porally. We capture this by identical speech
and gesture token edges. Further, the semantic
contribution of gestures is encoded by lexical
rules transforming a speech phrase into a mul-
timodal entity of conjoined spoken and gestu-
ral semantics.
1 Introduction
Our aim is to regiment the form-meaning mapping
of multimodal actions consisting of speech and co-
speech gestures. The language of study is English,
and the gestures of interest are depicting?the hand
depicts the referent?and deictic?the hand points
at the referent?s spatial coordinates.
Motivation for encoding the form-meaning map-
ping in the grammar stems from the fact that form
effects judgments of multimodal grammaticality:
e.g., in (1)1 the gesture performance along with
1The speech item where the gesture is performed is marked
by underlining, and the accented item is given in uppercase.
the unaccented ?called? in a single prosodic phrase
seems ill-formed despite the gesture depicting an as-
pect of the referent?the act of calling.
(1) * Your MOTHER called . . .
Hand lifts to the ear to imitate holding a receiver.
This intuitive judgment is in line with the em-
pirical findings of Giorgolo and Verstraten (2008)
who observed that prosody influences the perception
of temporally misaligned speech-and-gesture sig-
nals as ill-formed. Further, Alahverdzhieva and Las-
carides (2010) established empirically that the ges-
ture performance can be predicted from the prosodic
prominence in speech and that gestures not overlap-
ping subject NPs cannot be semantically related with
that subject NP. The fact that speech-and-gesture in-
tegration is informed by the form of the linguistic
signal suggests formalising the integration within
the grammar. Alternatively, integrating the gestu-
ral contribution by discourse update would involve
pragmatic reasoning accessing information about
linguistic form, disrupting the transition between
syntax/semantics and pragmatics.
The work is set within HPSG ? a constraint-based
grammar framework with the different types and
rules organised in a hierarchy. The semantic infor-
mation, derived in parallel with syntax, is expressed
in Minimal Recursion Semantics (MRS) which sup-
ports a high level of underspecifiability (Copestake
et al, 2005). This is useful for computing gesture
meaning since even through discourse processing
not all semantic information resolves to a specific
interpretation.
The rest of the paper is structured as follows: ?2
provides theoretical background, ?3 details the im-
plementation and ?4 discusses the evaluation.
582
2 Background
2.1 Attachment Ambiguity
We view the integration of gesture and the syn-
chronous, semantically related speech phrase as an
attachment in a single parse tree constrained by the
form of the speech signal?its prosodic prominence.
With standard methods for semantic composition,
we map this multimodal tree to an Underspecified
Logical Form (ULF) which supports the possible in-
terpretations of the speech and gesture in their con-
text. The choices of attachment are not unique. Sim-
ilarly to ?John saw the man with the telescope?,
there is ambiguity as to which linguistic phrase a
gesture is semantically related to, and hence likewise
ambiguity as to which linguistic phrase it attaches to
in syntax; e.g., in (2) the open vertical hand shape
can denote a container containing books or a con-
tainee of books. This interpretation is supported by
a gesture attachment to the N ?books?. A higher
attachment to the root node of the tree supports an-
other, metaphoric interpretation where the forward
movement is the conduit metaphor of giving.
(2) I can give you other BOOKS . . .
Hands are parallel with palms open vertical. They
perform a short forward move to the frontal centre.
We address this ambiguity by grammar rules
that allow for multiple attachments in the syntactic
tree constrained by the prosodic prominence of the
speech signal. The two basic rules are as follows:
1. Prosodic Word Constraint. Gesture can at-
tach to a prosodically prominent spoken word
if there is an overlap between the timing of the
gesture and the timing of the speech word.
2. Head-Argument Constraint. Gesture can at-
tach to a syntactic head partially or fully sat-
urated with its arguments and/or modifiers if
there is a temporal overlap between the syntac-
tic constituent and the gesture.
Applied to (2), these rules would attach the ges-
ture to ?books? (a prosodically prominent item),
also to ?other books?, ?give you other books?, ?can
give you other books? and even to ?I can give you
other books? (heads saturated with their arguments).
However, nothing licenses attachments to ?I? or
?give?. These distinct attachments would support
the interpretations proposed above.
2.2 Representing Gesture Form and Meaning
It is now commonplace to represent gesture form
with Typed Feature Structures (TFS) where each fea-
ture captures an aspect of the gesture?s meaning;
e.g., the gesture in (2) maps to the TFS in (3). Note
that the TFS is typed as depicting so as to differen-
tiate between, say, a hand shape of depicting ges-
ture and a hand shape of deixis. This distinction ef-
fects the gestural interpretation: a depicting gesture
provides non-spatial aspects of the referent?s deno-
tation, and so form bears resemblance to meaning.
Conversely, deixis identifies the spatial coordinates
of the referent in the physical space.
(3)
?
?
?
?
?
?
?
depicting
HAND-SHAPE open-flat
PALM-ORIENT towards-centre
FINGER-ORIENT away-body
HAND-LOCATION centre-low
HAND-MOVEMENT away-body-straight
?
?
?
?
?
?
?
Each feature introduces an underspecified ele-
mentary predication (EP) into LF; e.g., the hand
shape introduces l1 : hand shape open flat(i1)
where l1 is a unique label that underspecifies the
scope of the EP relative to other EPs in the ges-
ture?s LF, i1 is a unique metavariable that under-
specifies the main argument? sort (e.g., in (2) it can
resolve to an individual if the gesture denotes the
books or an event if it denotes the giving act) and
hand shape open flat underspecifies reference to
a property that the entity i1 has and that can be de-
picted through the gesture?s open flat hand shape.
In the grammar, we introduce underspecified se-
mantic relations vis rel(s,g) between speech s and
depicting gesture g, and deictic rel(s,d) between
speech s and deixis d. The resolution of these un-
derspecified predicates is a matter of commonsense
reasoning (Lascarides and Stone, 2009) and it there-
fore lies outside the scope of the grammar.
3 Implementation
The grammar was implemented in the LKB grammar
engineering platform (Copestake, 2002) which was
designed for TFS grammars such as HPSG. Since
the LKB parser accepts as input linearly ordered
strings and we represent gesture form with TFSs,
we used the PET engine (Callmeier, 2000) which al-
lows for injecting an arbitrary XML-based FS into
583
the input tokens. The input to our grammar is a lat-
tice of FSs where the spoken tokens are augmented
with prosodic information and the gesture tokens are
feature-value pairs such as (3).
The main challenge for the multimodal grammar
implementation stems from the non-linear multi-
modal input. The HPSG-based parsing platforms?
LKB, PET and TRALE?can parse linearly ordered
strings, and so they do not handle multimodal sig-
nals whose input comes from separate channels con-
nected through temporal relations. Also, these pars-
ing platforms do not support quantitative compari-
son operations over the time stamps of the input to-
kens. This is essential for our grammar since the
multimodal integration is constrained by temporal
overlap between speech and gesture (recall ?2.1).
To solve this, we pre-processed the XML-based
FS input so that overlapping TIME START and
TIME END values were ?translated? into identical
start and end edges of the speech token and the ges-
ture token as follows:
<edge source="v0" target="v1">
<fs type="speech_token">
<edge source="v0" target="v1">
<fs type="gesture_token">
This robust pre-processing step is sufficient since
the only temporal relation required by the grammar
is overlap, an abstraction over more fined-grained
relations between speech (S) and gesture (G) such
as (precedence(start(S), start(G)) ? identity (end(S),
end(G))).
The linking of gesture to its temporally over-
lapping speech segment happens prior to parsing
via chart-mapping rules (Adolphs et al, 2008)
which involve re-writing chart items into FSs. The
gesture-unary-rule (see Fig.1) rewrites an in-
put (I) speech token in the context (C) of a gesture
token into a combined speech+gesture token where
the +GEST and +PROS values of the speech and ges-
ture tokens are copied onto the output (O).
gesture-unary-rule := cm_rule &
[+CONTEXT <gesture_token & [+GEST #gest]>,
+INPUT <speech_token & [+PROS #pros]>,
+OUTPUT <speech+gesture_token &
[+GEST #gest, +PROS #pros]>,
+POSITION "O1@I1, I1@C1" ].
Figure 1: Definition of gesture-unary-rule
The +PROS attribute contains prosodic informa-
tion and the +GEST attribute is a feature-structure
representation as shown in (3). The +POSITION con-
straint restricts the position of the I, O and C items to
an overlap (@), i.e., the edge markers of the gesture
token should be identical to those of the speech to-
ken, and also identical to the speech+gesture token.
This chart-mapping rule recognises the gesture to-
ken overlapping the speech token and it records this
by ?augmenting? the speech token with the gesture
feature-values.
In the grammar, we extended the ERG word and
phrase rules with prosodic and gestural information
where the +PROS and +GEST features of the input
token are identified with the PROS and GEST of the
word and/or lexical phrase in the grammar. We then
added a lexical rule (see Fig. 2) which projects a ges-
ture daughter to a complex gesture-marked entity of
a single argument for which both the PROS and GEST
features are appropriate.
gesture_lexrule := phrase_or_lexrule &
[ ORTH [ PROS #pros ],
ARGS <[ ORTH [ GEST gesture-form,
PROS p-word & #pros ]]>].
Figure 2: Definition of gesture lexrule
This rule constrains PROS to a prosodically promi-
nent word of type p-word thereby preventing a ges-
ture from plugging into a prosodically unmarked
word. The gesture-form value is a supertype over the
distinct gesture types?depicting and deictic. The
gesture lexrule is inherited by a lexical rule
specific to depicting gestures, and by a lexical rule
specific to deictic gestures. In this way, we can en-
code the semantic contribution of depicting gestures
which is different from the semantic contribution of
deixis. For the sake of space, Fig. 3 presents only the
depicting lexrule. The semantic information
contributed by the rule is encoded within C-CONT.
Following ?2.2, the rule introduces an underspec-
ified vis rel between the main label #dltop of the
spoken sign (via the HCONS constraints) and the
main label #glbl of the gesture semantics (via the
HCONS constraints). Note that these two arguments
are in a geq (greater or equal) constraint. This means
that vis rel can operate over any projection of the
speech word; e.g., attaching the gesture to ?book? in
(2) means that the relation is not restricted to the EPs
contributed by ?books? but it can be also over the
EPs of a higher projection. The gesture?s semantics
is a bag of EPs (see ?2.2), all of which are outscoped
584
?gesture/12-04-02/pet? Coverage Profile
total positive word lexical distinct total overall
Aggregate items items string items analyses results coverage
] ] ? ? ? ] %
90 ? i-length < 95 126 92 93.00 26.46 1.67 92 100.0
70 ? i-length < 75 78 54 71.00 12.00 1.00 54 100.0
60 ? i-length < 65 249 179 60.00 9.42 1.00 179 100.0
45 ? i-length < 50 18 14 49.00 7.00 1.00 14 100.0
Total 471 339 70.25 14.35 1.18 339 100.0
Table 1: Coverage Profile of Test Items generated by [incr tsdb()]
depicting_lexrule := gesture_lexrule &
[ARGS <[ SYNSEM.LOCAL.CONT.HOOK.LTOP
#dltop,
ORTH [ GEST depicting] >,
C-CONT [ RELS <![ PRED vis_rel,
S-ARG #arg1,
G-ARG #arg2 ],
[ PRED G_mod,
LBL #glbl,
ARG1 #harg ],
[ LBL #larg1 ],...!>,
HCONS <!geq&[ HARG #arg1,
LARG #dltop ],
qeq&[ HARG #arg2,
LARG #glbl ],
qeq&[ HARG #harg,
LARG #larg1 ],
...!>]].
Figure 3: Definition of depicting lexrule
by the gestural modality [G]. The rule therefore in-
troduces in RELS a label (here #larg1) for an EP
which is in qeq constraints with [G]. The instanti-
ation of the particular EPs comes from the gestural
lexical entry. In the real implementation, the num-
ber of these labels corresponds to the number of fea-
tures. They are designed in the same way and we
thus forego any details about the rest.
4 Evaluation
The evaluation was performed against a test suite
designed in analogy to the traditional phenomenon-
based test-suites (Lehmann et al, 1996): manually-
crafted to ensure coverage of well-formed and ill-
formed data, but inspired by an examination of natu-
ral data. We systematically tested syntactic phenom-
ena (intransitivity, transitivity, complex NPs, coordi-
nation, negation and modification) over well-formed
and ill-formed examples where the ill-formed items
were derived by means of the following operations:
prosodic permutation (varying the prosodic marked-
ness, e.g., from (4a) we derive (4b) to reflect in-
tuitions of native speakers); gesture variation (test-
ing distinct gesture types) and temporal permutation
(moving the gestural performance over the distinct
speech items).
(4) a. ANNA ate . . .
Depicting gesture along with ?Anna?.
b. *anna ATE . . .
Depicting gesture along with ?Anna?.
The test set contained 471 multimodal items (72%
well-formed) covering the full range of prosodic
(prosodic markedness and unmarkedness) and ges-
ture (the span of depicting/deictic gesture and its
temporal relation to the prosodically marked ele-
ments) permutations. The gestural vocabulary was
limited since a larger gesture lexicon has no effects
on the performance. To test the grammar, we used
the [incr tsdb()]2 competence and performance tool
which enables batch processing of test items and
which creates a coverage profile of the test set (see
Table 1). The values are as follows: the left col-
umn separates the items per aggregation criterion
(the length of test items); the next column shows the
number of test items per aggregate; then we have
the number of grammatical items; average length of
test item; average number of lexical items; average
number of distinct analyses and total coverage.
5 Conclusions and Future Work
This paper reported on an implementation of a mul-
timodal grammar combining spoken and gestural in-
put. The main challenge for the current parsing
platforms was the non-linear input which we solved
by extending the spoken sign with the synchronous
gestural sign semantics where synchrony was estab-
lished by means of identical token edges. In the fu-
ture, we shall extend the lexical coverage so that the
grammar can handle various gestures and we also
intend to evaluate the grammar with naturally occur-
ring examples in XML format.
2http://www.delph-in.net/itsdb/
585
References
Peter Adolphs, Stephan Oepen, Ulrich Callmeier,
Berthold Crysmann, Daniel Flickinger, and Bernd
Kiefer. 2008. Some fine points of hybrid natural lan-
guage parsing. In Proceedings of the Sixth Interna-
tional Language Resources and Evaluation. ELRA.
Katya Alahverdzhieva and Alex Lascarides. 2010.
Analysing speech and co-speech gesture in constraint-
based grammars. In Stefan Mu?ller, editor, The Pro-
ceedings of the 17th International Conference on
Head-Driven Phrase Structure Grammar, pages 6?26,
Stanford. CSLI Publications.
Ulrich Callmeier. 2000. PET ? A platform for experi-
mentation with efficient HPSG processing techniques.
Natural Language Engineering, 6 (1) (Special Issue on
Efficient Processing with HPSG):99 ? 108.
Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pol-
lard. 2005. Minimal recursion semantics: An intro-
duction. Journal of Research on Language and Com-
putation, 3(2?3):281?332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stanford,
CA.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering.
Gianluca Giorgolo and Frans Verstraten. 2008. Per-
ception of speech-and-gesture integration. In Pro-
ceedings of the International Conference on Auditory-
Visual Speech Processing 2008, pages 31?36.
Alex Lascarides and Matthew Stone. 2009. A formal
semantic analysis of gesture. Journal of Semantics.
Sabine Lehmann, Stephan Oepen, Sylvie Regnier-Prost,
Klaus Netter, Veronika Lux, Judith Klein, Kirsten
Falkedal, Frederik Fouvry, Dominique Estival, Eva
Dauphin, Herve Compagnion, Judith Baur, Lorna
Balkan, and Doug Arnold. 1996. Tsnlp - test suites
for natural language processing. In COLING, pages
711?716.
586
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63?72,
Dublin, Ireland, August 23-24, 2014.
SemEval 2014 Task 8:
Broad-Coverage Semantic Dependency Parsing
Stephan Oepen
??
, Marco Kuhlmann
?
, Yusuke Miyao
?
, Daniel Zeman
?
,
Dan Flickinger
?
, Jan Haji
?
c
?
, Angelina Ivanova
?
, and Yi Zhang
?
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
?
Link?ping University, Department of Computer and Information Science
?
National Institute of Informatics, Tokyo
?
Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
?
Stanford University, Center for the Study of Language and Information
?
Nuance Communications Aachen GmbH
sdp-organizers@emmtee.net
Abstract
Task 8 at SemEval 2014 defines Broad-
Coverage Semantic Dependency Pars-
ing (SDP) as the problem of recovering
sentence-internal predicate?argument rela-
tionships for all content words, i.e. the se-
mantic structure constituting the relational
core of sentence meaning. In this task
description, we position the problem in
comparison to other sub-tasks in compu-
tational language analysis, introduce the se-
mantic dependency target representations
used, reflect on high-level commonalities
and differences between these representa-
tions, and summarize the task setup, partic-
ipating systems, and main results.
1 Background and Motivation
Syntactic dependency parsing has seen great ad-
vances in the past decade, in part owing to rela-
tively broad consensus on target representations,
and in part reflecting the successful execution of a
series of shared tasks at the annual Conference for
Natural Language Learning (CoNLL; Buchholz &
Marsi, 2006; Nivre et al., 2007; inter alios). From
this very active research area accurate and efficient
syntactic parsers have developed for a wide range
of natural languages. However, the predominant
data structure in dependency parsing to date are
trees, in the formal sense that every node in the de-
pendency graph is reachable from a distinguished
root node by exactly one directed path.
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and the
proceedings footer are added by the organizers: http://
creativecommons.org/licenses/by/4.0/.
Unfortunately, tree-oriented parsers are ill-suited
for producing meaning representations, i.e. mov-
ing from the analysis of grammatical structure to
sentence semantics. Even if syntactic parsing ar-
guably can be limited to tree structures, this is not
the case in semantic analysis, where a node will
often be the argument of multiple predicates (i.e.
have more than one incoming arc), and it will often
be desirable to leave nodes corresponding to se-
mantically vacuous word classes unattached (with
no incoming arcs).
Thus, Task 8 at SemEval 2014, Broad-Coverage
Semantic Dependency Parsing (SDP 2014),
1
seeks
to stimulate the dependency parsing community
to move towards more general graph processing,
to thus enable a more direct analysis of Who did
What to Whom? For English, there exist several
independent annotations of sentence meaning over
the venerable Wall Street Journal (WSJ) text of the
Penn Treebank (PTB; Marcus et al., 1993). These
resources constitute parallel semantic annotations
over the same common text, but to date they have
not been related to each other and, in fact, have
hardly been applied for training and testing of data-
driven parsers. In this task, we have used three
different such target representations for bi-lexical
semantic dependencies, as demonstrated in Figure 1
below for the WSJ sentence:
(1) A similar technique is almost impossible to apply to
other crops, such as cotton, soybeans, and rice.
Semantically, technique arguably is dependent on
the determiner (the quantificational locus), the mod-
ifier similar, and the predicate apply. Conversely,
the predicative copula, infinitival to, and the vac-
1
See http://alt.qcri.org/semeval2014/
task8/ for further technical details, information on how to
obtain the data, and official results.
63
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
A1 A2
(a) Partial semantic dependencies in PropBank and NomBank.
A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.
top
ARG2 ARG3 ARG1
ARG2mwe _and_cARG1ARG1
BV
ARG1 implicit_conjARG1
(b) DELPH-IN Minimal Recursion Semantics?derived bi-lexical dependencies (DM).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice
top
ARG1
ARG2
ARG1
ARG2
ARG2
ARG1
ARG1 ARG1 ARG1ARG1
ARG1
ARG2
ARG1
ARG2
ARG1
ARG2
ARG1 ARG1 ARG1 ARG2
(c) Enju Predicate?Argument Structures (PAS).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
RSTR
PAT
EXT
PAT
ACT
RSTR
ADDR
ADDR
ADDR
ADDR
APPS.m
APPS.m
CONJ.m
CONJ.m CONJ.m
top
(d) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT).
Figure 1: Sample semantic dependency graphs for Example (1).
uous preposition marking the deep object of ap-
ply can be argued to not have a semantic contri-
bution of their own. Besides calling for node re-
entrancies and partial connectivity, semantic depen-
dency graphs may also exhibit higher degrees of
non-projectivity than is typical of syntactic depen-
dency trees.
In addition to its relation to syntactic dependency
parsing, the task also has some overlap with Se-
mantic Role Labeling (SRL; Gildea & Jurafsky,
2002). In much previous work, however, target
representations typically draw on resources like
PropBank and NomBank (Palmer et al., 2005; Mey-
ers et al., 2004), which are limited to argument
identification and labeling for verbal and nominal
predicates. A plethora of semantic phenomena?
for example negation and other scopal embedding,
comparatives, possessives, various types of modi-
fication, and even conjunction?typically remain
unanalyzed in SRL. Thus, its target representations
are partial to a degree that can prohibit seman-
tic downstream processing, for example inference-
based techniques. In contrast, we require parsers
to identify all semantic dependencies, i.e. compute
a representation that integrates all content words in
one structure. Another difference to common inter-
pretations of SRL is that the SDP 2014 task defini-
tion does not encompass predicate disambiguation,
a design decision in part owed to our goal to focus
on parsing-oriented, i.e. structural, analysis, and in
part to lacking consensus on sense inventories for
all content words.
Finally, a third closely related area of much cur-
rent interest is often dubbed ?semantic parsing?,
which Kate and Wong (2010) define as ?the task of
mapping natural language sentences into complete
formal meaning representations which a computer
can execute for some domain-specific application.?
In contrast to most work in this tradition, our SDP
target representations aim to be task- and domain-
independent, though at least part of this general-
ity comes at the expense of ?completeness? in the
above sense; i.e. there are aspects of sentence mean-
ing that arguably remain implicit.
2 Target Representations
We use three distinct target representations for se-
mantic dependencies. As is evident in our run-
ning example (Figure 1), showing what are called
the DM, PAS, and PCEDT semantic dependencies,
there are contentful differences among these anno-
tations, and there is of course not one obvious (or
even objective) truth. In the following paragraphs,
64
we provide some background on the ?pedigree? and
linguistic characterization of these representations.
DM: DELPH-IN MRS-Derived Bi-Lexical De-
pendencies These semantic dependency graphs
originate in a manual re-annotation of Sections 00?
21 of the WSJ Corpus with syntactico-semantic
analyses derived from the LinGO English Re-
source Grammar (ERG; Flickinger, 2000). Among
other layers of linguistic annotation, this resource?
dubbed DeepBank by Flickinger et al. (2012)?
includes underspecified logical-form meaning rep-
resentations in the framework of Minimal Recur-
sion Semantics (MRS; Copestake et al., 2005).
Our DM target representations are derived through
a two-step ?lossy? conversion of MRSs, first to
variable-free Elementary Dependency Structures
(EDS; Oepen & L?nning, 2006), then to ?pure?
bi-lexical form?projecting some construction se-
mantics onto word-to-word dependencies (Ivanova
et al., 2012). In preparing our gold-standard
DM graphs from DeepBank, the same conversion
pipeline was used as in the system submission of
Miyao et al. (2014). For this target representa-
tion, top nodes designate the highest-scoping (non-
quantifier) predicate in the graph, e.g. the (scopal)
degree adverb almost in Figure 1.
2
PAS: Enju Predicate-Argument Structures
The Enju parsing system is an HPSG-based parser
for English.
3
The grammar and the disambigua-
tion model of this parser are derived from the Enju
HPSG treebank, which is automatically converted
from the phrase structure and predicate?argument
structure annotations of the PTB. The PAS data
set is extracted from the WSJ portion of the Enju
HPSG treebank. While the Enju treebank is an-
notated with full HPSG-style structures, only its
predicate?argument structures are converted into
the SDP data format for use in this task. Top
nodes in this representation denote semantic heads.
Again, the system description of Miyao et al. (2014)
provides more technical detail on the conversion.
PCEDT: Prague Tectogrammatical Bi-Lexical
Dependencies The Prague Czech-English De-
pendency Treebank (PCEDT; Haji
?
c et al., 2012)
4
is a set of parallel dependency trees over the WSJ
2
Note, however, that non-scopal adverbs act as mere in-
tersective modifiers, e.g. loudly is a predicate in DM, but the
main verb provides the top node in structures like Abrams
sang loudly.
3
See http://kmcs.nii.ac.jp/enju/.
4
See http://ufal.mff.cuni.cz/pcedt2.0/.
id form lemma pos top pred arg1 arg2
#20200002
1 Ms. Ms. NNP ? + _ _
2 Haag Haag NNP ? ? compound ARG1
3 plays play VBZ + + _ _
4 Elianti Elianti NNP ? ? _ ARG2
5 . . . ? ? _ _
Table 1: Tabular SDP data format (showing DM).
texts from the PTB, and their Czech translations.
Similarly to other treebanks in the Prague family,
there are two layers of syntactic annotation: an-
alytical (a-trees) and tectogrammatical (t-trees).
PCEDT bi-lexical dependencies in this task have
been extracted from the t-trees. The specifics of
the PCEDT representations are best observed in the
procedure that converts the original PCEDT data to
the SDP data format; see Miyao et al. (2014). Top
nodes are derived from t-tree roots; i.e. they mostly
correspond to main verbs. In case of coordinate
clauses, there are multiple top nodes per sentence.
3 Graph Representation
The SDP target representations can be character-
ized as labeled, directed graphs. Formally, a se-
mantic dependency graph for a sentence x =
x
1
, . . . , x
n
is a structure G = (V,E, `
V
, `
E
) where
V = {1, . . . , n} is a set of nodes (which are in
one-to-one correspondence with the tokens of the
sentence); E ? V ? V is a set of edges; and `
V
and `
E
are mappings that assign labels (from some
finite alphabet) to nodes and edges, respectively.
More specifically for this task, the label `
V
(i) of a
node i is a tuple consisting of four components: its
word form, lemma, part of speech, and a Boolean
flag indicating whether the corresponding token
represents a top predicate for the specific sentence.
The label `
E
(i? j) of an edge i? j is a seman-
tic relation that holds between i and j. The exact
definition of what constitutes a top node and what
semantic relations are available differs among our
three target representations, but note that top nodes
can have incoming edges.
All data provided for the task uses a column-
based file format (dubbed the SDP data format)
similar to the one of the 2009 CoNLL Shared Task
(Haji
?
c et al., 2009). As in that task, we assume gold-
standard sentence and token segmentation. For
ease of reference, each sentence is prefixed by a
line with just a unique identifier, using the scheme
2SSDDIII, with a constant leading 2, two-digit sec-
tion code, two-digit document code (within each
65
section), and three-digit item number (within each
document). For example, identifier 20200002 de-
notes the second sentence in the first file of PTB
Section 02, the classic Ms. Haag plays Elianti. The
annotation of this sentence is shown in Table 1.
With one exception, our fields (i.e. columns in
the tab-separated matrix) are a subset of the CoNLL
2009 inventory: (1) id, (2) form, (3) lemma, and
(4) pos characterize the current token, with token
identifiers starting from 1 within each sentence. Be-
sides the lemma and part-of-speech information, in
the closed track of our task, there is no explicit
analysis of syntax. Across the three target represen-
tations in the task, fields (1) and (2) are aligned and
uniform, i.e. all representations annotate exactly
the same text. On the other hand, fields (3) and (4)
are representation-specific, i.e. there are different
conventions for lemmatization, and part-of-speech
assignments can vary (but all representations use
the same PTB inventory of PoS tags).
The bi-lexical semantic dependency graph over
tokens is represented by two or more columns start-
ing with the obligatory, binary-valued fields (5)
top and (6) pred. A positive value in the top
column indicates that the node corresponding to
this token is a top node (see Section 2 below). The
pred column is a simplification of the correspond-
ing field in earlier tasks, indicating whether or not
this token represents a predicate, i.e. a node with
outgoing dependency edges. With these minor dif-
ferences to the CoNLL tradition, our file format can
represent general, directed graphs, with designated
top nodes. For example, there can be singleton
nodes not connected to other parts of the graph,
and in principle there can be multiple tops, or a
non-predicate top node.
To designate predicate?argument relations, there
are as many additional columns as there are pred-
icates in the graph (i.e. tokens marked + in the
pred column); these additional columns are called
(7) arg1, (8) arg2, etc. These colums contain
argument roles relative to the i-th predicate, i.e. a
non-empty value in column arg1 indicates that
the current token is an argument of the (linearly)
first predicate in the sentence. In this format, graph
reentrancies will lead to a token receiving argument
roles for multiple predicates (i.e. non-empty arg
i
values in the same row). All tokens of the same sen-
tence must always have all argument columns filled
in, even on non-predicate words; in other words,
all lines making up one block of tokens will have
the same number n of fields, but n can differ across
DM PAS PCEDT
(1) # labels 51 42 68
(2) % singletons 22.62 4.49 35.79
(3) # edge density 0.96 1.02 0.99
(4) %
g
trees 2.35 1.30 56.58
(5) %
g
projective 3.05 1.71 53.29
(6) %
g
fragmented 6.71 0.23 0.56
(7) %
n
reentrancies 27.35 29.40 9.27
(8) %
g
topless 0.28 0.02 0.00
(9) # top nodes 0.9972 0.9998 1.1237
(10) %
n
non-top roots 44.71 55.92 4.36
Table 2: Contrastive high-level graph statistics.
sentences, depending on the count of graph nodes.
4 Data Sets
All three target representations are annotations of
the same text, Sections 00?21 of the WSJ Cor-
pus. For this task, we have synchronized these
resources at the sentence and tokenization levels
and excluded from the SDP 2014 training and test-
ing data any sentences for which (a) one or more of
the treebanks lacked a gold-standard analysis; (b) a
one-to-one alignment of tokens could not be estab-
lished across all three representations; or (c) at least
one of the graphs was cyclic. Of the 43,746 sen-
tences in these 22 first sections of WSJ text, Deep-
Bank lacks analyses for close to 15%, and the Enju
Treebank has gaps for a little more than four per-
cent. Some 500 sentences show tokenization mis-
matches, most owing to DeepBank correcting PTB
idiosyncrasies like ?G.m.b, H.?, ?S.p, A.?, and
?U.S., .?, and introducing a few new ones (Fares
et al., 2013). Finally, 232 of the graphs obtained
through the above conversions were cyclic. In total,
we were left with 34,004 sentences (or 745,543
tokens) as training data (Sections 00?20), and 1348
testing sentences (29,808 tokens), from Section 21.
Quantitative Comparison As a first attempt at
contrasting our three target representations, Table 2
shows some high-level statistics of the graphs com-
prising the training data.
5
In terms of distinctions
5
These statistics are obtained using the ?official? SDP
toolkit. We refer to nodes that have neither incoming nor
outgoing edges and are not marked as top nodes as singletons;
these nodes are ignored in subsequent statistics, e.g. when
determining the proportion of edges per node (3) or the per-
centages of rooted trees (4) and fragmented graphs (6). The
notation ?%
n
? denotes (non-singleton) node percentages, and
?%
g
? percentages over all graphs. We consider a root node any
(non-singleton) node that has no incoming edges; reentrant
nodes have at least two incoming edges. Following Sagae and
Tsujii (2008), we consider a graph projective when there are
no crossing edges (in a left-to-right rendering of nodes) and no
roots are ?covered?, i.e. for any root j there is no edge i? k
66
Directed Undirected
DM PAS PCEDT DM PAS PCEDT
DM ? .6425 .2612 ? .6719 .5675
PAS .6688 ? .2963 .6993 ? .5490
PCEDT .2636 .2963 ? .5743 .5630 ?
Table 3: Pairwise F
1
similarities, including punctu-
ation (upper right diagonals) or not (lower left).
drawn in dependency labels (1), there are clear dif-
ferences between the representations, with PCEDT
appearing linguistically most fine-grained, and PAS
showing the smallest label inventory. Unattached
singleton nodes (2) in our setup correspond to
tokens analyzed as semantically vacuous, which
(as seen in Figure 1) include most punctuation
marks in PCEDT and DM, but not PAS. Further-
more, PCEDT (unlike the other two) analyzes some
high-frequency determiners as semantically vacu-
ous. Conversely, PAS on average has more edges
per (non-singleton) nodes than the other two (3),
which likely reflects its approach to the analysis of
functional words (see below).
Judging from both the percentage of actual trees
(4), the proportions of projective graphs (5), and the
proportions of reentrant nodes (7), PCEDT is much
more ?tree-oriented? than the other two, which at
least in part reflects its approach to the analysis
of modifiers and determiners (again, see below).
We view the small percentages of graphs without
at least one top node (8) and of graphs with at
least two non-singleton components that are not
interconnected (6) as tentative indicators of general
well-formedness. Intuitively, there should always
be a ?top? predicate, and the whole graph should
?hang together?. Only DM exhibits non-trivial (if
small) degrees of topless and fragmented graphs,
and these may indicate imperfections in the Deep-
Bank annotations or room for improvement in the
conversion from full MRSs to bi-lexical dependen-
cies, but possibly also exceptions to our intuitions
about semantic dependency graphs.
Finally, in Table 3 we seek to quantify pairwise
structural similarity between the three representa-
tions in terms of unlabeled dependency F
1
(dubbed
UF in Section 5 below). We provide four variants
of this metric, (a) taking into account the direc-
tionality of edges or not and (b) including edges
involving punctuation marks or not. On this view,
DM and PAS are structurally much closer to each
other than either of the two is to PCEDT, even more
such that i < j < k.
so when discarding punctuation. While relaxing
the comparison to ignore edge directionality also
increases similarity scores for this pair, the effect
is much more pronounced when comparing either
to PCEDT. This suggests that directionality of se-
mantic dependencies is a major source of diversion
between DM and PAS on the one hand, and PCEDT
on the other hand.
Linguistic Comparison Among other aspects,
Ivanova et al. (2012) categorize a range of syntac-
tic and semantic dependency annotation schemes
according to the role that functional elements take.
In Figure 1 and the discussion of Table 2 above, we
already observed that PAS differs from the other
representations in integrating into the graph aux-
iliaries, the infinitival marker, the case-marking
preposition introducing the argument of apply (to),
and most punctuation marks;
6
while these (and
other functional elements, e.g. complementizers)
are analyzed as semantically vacuous in DM and
PCEDT, they function as predicates in PAS, though
do not always serve as ?local? top nodes (i.e. the se-
mantic head of the corresponding sub-graph): For
example, the infinitival marker in Figure 1 takes the
verb as its argument, but the ?upstairs? predicate
impossible links directly to the verb, rather than to
the infinitival marker as an intermediate.
At the same time, DM and PAS pattern alike
in their approach to modifiers, e.g. attributive ad-
jectives, adverbs, and prepositional phrases. Un-
like in PCEDT (or common syntactic dependency
schemes), these are analyzed as semantic predi-
cates and, thus, contribute to higher degrees of
node reentrancy and non-top (structural) roots.
Roughly the same holds for determiners, but here
our PCEDT projection of Prague tectogrammatical
trees onto bi-lexical dependencies leaves ?vanilla?
articles (like a and the) as singleton nodes.
The analysis of coordination is distinct in the
three representations, as also evident in Figure 1.
By design, DM opts for what is often called
the Mel?
?
cukian analysis of coordinate structures
(Mel?
?
cuk, 1988), with a chain of dependencies
rooted at the first conjunct (which is thus consid-
ered the head, ?standing in? for the structure at
large); in the DM approach, coordinating conjunc-
tions are not integrated with the graph but rather
contribute different types of dependencies. In PAS,
the final coordinating conjunction is the head of the
6
In all formats, punctuation marks like dashes, colons, and
sometimes commas can be contentful, i.e. at times occur as
both predicates, arguments, and top nodes.
67
employee stock investment plans
compound compound compound
employee stock investment plans
ARG1
ARG1
ARG1
employee stock investment plans
ACT
PAT REG
Figure 2: Analysis of nominal compounding in DM, PAS, and PCEDT, respectively .
structure and each coordinating conjunction (or in-
tervening punctuation mark that acts like one) is a
two-place predicate, taking left and right conjuncts
as its arguments. Conversely, in PCEDT the last
coordinating conjunction takes all conjuncts as its
arguments (in case there is no overt conjunction, a
punctuation mark is used instead); additional con-
junctions or punctuation marks are not connected
to the graph.
7
A linguistic difference between our representa-
tions that highlights variable granularities of anal-
ysis and, relatedly, diverging views on the scope
of the problem can be observed in Figure 2. Much
noun phrase?internal structure is not made explicit
in the PTB, and the Enju Treebank from which
our PAS representation derives predates the brack-
eting work of Vadas and Curran (2007). In the
four-way nominal compounding example of Fig-
ure 2, thus, PAS arrives at a strictly left-branching
tree, and there is no attempt at interpreting seman-
tic roles among the members of the compound ei-
ther; PCEDT, on the other hand, annotates both the
actual compound-internal bracketing and the as-
signment of roles, e.g. making stock the PAT(ient)
of investment. In this spirit, the PCEDT annota-
tions could be directly paraphrased along the lines
of plans by employees for investment in stocks. In
a middle position between the other two, DM dis-
ambiguates the bracketing but, by design, merely
assigns an underspecified, construction-specific de-
pendency type; its compound dependency, then,
is to be interpreted as the most general type of de-
pendency that can hold between the elements of
this construction (i.e. to a first approximation either
an argument role or a relation parallel to a prepo-
sition, as in the above paraphrase). The DM and
PCEDT annotations of this specific example hap-
pen to diverge in their bracketing decisions, where
the DM analysis corresponds to [...] investments
in stock for employees, i.e. grouping the concept
7
As detailed by Miyao et al. (2014), individual con-
juncts can be (and usually are) arguments of other predicates,
whereas the topmost conjunction only has incoming edges in
nested coordinate structures. Similarly, a ?shared? modifier of
the coordinate structure as a whole would take as its argument
the local top node of the coordination in DM or PAS (i.e. the
first conjunct or final conjunction, respectively), whereas it
would depend as an argument on all conjuncts in PCEDT.
employee stock (in contrast to ?common stock?).
Without context and expert knowledge, these de-
cisions are hard to call, and indeed there has been
much previous work seeking to identify and anno-
tate the relations that hold between members of a
nominal compound (see Nakov, 2013, for a recent
overview). To what degree the bracketing and role
disambiguation in this example are determined by
the linguistic signal (rather than by context and
world knowledge, say) can be debated, and thus the
observed differences among our representations in
this example relate to the classic contrast between
?sentence? (or ?conventional?) meaning, on the one
hand, and ?speaker? (or ?occasion?) meaning, on
the other hand (Quine, 1960; Grice, 1968). In
turn, we acknowledge different plausible points of
view about which level of semantic representation
should be the target representation for data-driven
parsing (i.e. structural analysis guided by the gram-
matical system), and which refinements like the
above could be construed as part of a subsequent
task of interpretation.
5 Task Setup
Training data for the task, providing all columns in
the file format sketched in Section 3 above, together
with a first version of the SDP toolkit?including
graph input, basic statistics, and scoring?were
released to candidate participants in early Decem-
ber 2013. In mid-January, a minor update to the
training data and optional syntactic ?companion?
analyses (see below) were provided, and in early
February the description and evaluation of a sim-
ple baseline system (using tree approximations and
the parser of Bohnet, 2010). Towards the end of
March, an input-only version of the test data was
released, with just columns (1) to (4) pre-filled; par-
ticipants then had one week to run their systems on
these inputs, fill in columns (5), (6), and upwards,
and submit their results (from up to two different
runs) for scoring. Upon completion of the testing
phase, we have shared the gold-standard test data,
official scores, and system results for all submis-
sions with participants and are currently preparing
all data for general release through the Linguistic
Data Consortium.
68
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Peking 85.91 90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05
Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42
Copenhagen-
80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01
Malm?
Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19
Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82
Link?ping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Priberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68
CMU 82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12
Turku 80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82
Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42
Alpage 78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60
In-House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30
Table 4: Results of the closed (top) and open tracks (bottom). For each system, the second column (LF)
indicates the averaged LF score across all target representations), which was used to rank the systems.
Evaluation Systems participating in the task
were evaluated based on the accuracy with which
they can produce semantic dependency graphs for
previously unseen text, measured relative to the
gold-standard testing data. The key measures for
this evaluation were labeled and unlabeled preci-
sion and recall with respect to predicted dependen-
cies (predicate?role?argument triples) and labeled
and unlabeled exact match with respect to complete
graphs. In both contexts, identification of the top
node(s) of a graph was considered as the identifi-
cation of additional, ?virtual? dependencies from
an artificial root node (at position 0). Below we
abbreviate these metrics as (a) labeled precision,
recall, and F
1
: LP, LR, LF; (b) unlabeled precision,
recall, and F
1
: UP, UR, UF; and (c) labeled and
unlabeled exact match: LM, UM.
The ?official? ranking of participating systems, in
both the closed and the open tracks, is determined
based on the arithmetic mean of the labeled depen-
dency F
1
scores (i.e. the geometric mean of labeled
precision and labeled recall) on the three target rep-
resentations (DM, PAS, and PCEDT). Thus, to be
considered for the final ranking, a system had to
submit semantic dependencies for all three target
representations.
Closed vs. Open Tracks The task was sub-
divided into a closed track and an open track, where
systems in the closed track could only be trained
on the gold-standard semantic dependencies dis-
tributed for the task. Systems in the open track, on
the other hand, could use additional resources, such
as a syntactic parser, for example?provided that
they make sure to not use any tools or resources
that encompass knowledge of the gold-standard
syntactic or semantic analyses of the SDP 2014
test data, i.e. were directly or indirectly trained or
otherwise derived from WSJ Section 21.
This restriction implies that typical off-the-shelf
syntactic parsers had to be re-trained, as many data-
driven parsers for English include this section of
the PTB in their default training data. To simplify
participation in the open track, the organizers pre-
pared ready-to-use ?companion? syntactic analyses,
sentence- and token-aligned to the SDP data, in
two formats, viz. PTB-style phrase structure trees
obtained from the parser of Petrov et al. (2006) and
Stanford Basic syntactic dependencies (de Marn-
effe et al., 2006) produced by the parser of Bohnet
and Nivre (2012).
6 Submissions and Results
From 36 teams who had registered for the task,
test runs were submitted for nine systems. Each
team submitted one or two test runs per track. In
total, there were ten runs submitted to the closed
track and nine runs to the open track. Three teams
submitted to both the closed and the open track.
The main results are summarized and ranked in
Table 4. The ranking is based on the average LF
score across all three target representations, which
is given in the LF column. In cases where a team
submitted two runs to a track, only the highest-
ranked score is included in the table.
69
Team Track Approach Resources
Link?ping C extension of Eisner?s algorithm for DAGs, edge-factored
structured perceptron
?
Potsdam C & O graph-to-tree transformation, Mate companion
Priberam C & O model with second-order features, decoding with dual decom-
position, MIRA
companion
Turku O cascade of SVM classifiers (dependency recognition, label
classification, top recognition)
companion,
syntactic n-grams,
word2vec
Alpage C & O transition-based parsing for DAGs, logistic regression, struc-
tured perceptron
companion,
Brown clusters
Peking C transition-based parsing for DAGs, graph-to-tree transforma-
tion, parser ensemble
?
CMU O edge classification by logistic regression, edge-factored struc-
tured SVM
companion
Copenhagen-Malm? C graph-to-tree transformation, Mate ?
In-House O existing parsers developed by the organizers grammars
Table 5: Overview of submitted systems, high-level approaches, and additional resources used (if any).
In the closed track, the average LF scores across
target representations range from 85.91 to 72.20.
Comparing the results for different target represen-
tations, the average LF scores across systems are
85.96 for PAS, 82.97 for DM, and 70.17 for PCEDT.
The scores for labeled exact match show a much
larger variation across both target representations
and systems.
8
In the open track, we see very similar trends.
The average LF scores across target representations
range from 86.27 to 75.89 and the corresponding
scores across systems are 88.64 for PAS, 84.95
for DM, and 67.52 for PCEDT. While these scores
are consistently higher than in the closed track,
the differences are small. In fact, for each of the
three teams that submitted to both tracks (Alpage,
Potsdam, and Priberam) improvements due to the
use of additional resources in the open track do not
exceed two points LF.
7 Overview of Approaches
Table 5 shows a summary of the systems that sub-
mitted final results. Most of the systems took
a strategy to use some algorithm to process (re-
stricted types of) graph structures, and apply ma-
chine learning like structured perceptrons. The
methods for processing graph structures are clas-
sified into three types. One is to transform graphs
into trees in the preprocessing stage, and apply con-
ventional dependency parsing systems (e.g. Mate;
Bohnet, 2010) to the converted trees. Some sys-
tems simply output the result of dependency pars-
ing (which means they inherently lose some depen-
8
Please see the task web page at the address indicated
above for full labeled and unlabeled scores.
dencies), while the others apply post-processing
to recover non-tree structures. The second strat-
egy is to use a parsing algorithm that can directly
generate graph structures (in the spirit of Sagae &
Tsujii, 2008; Titov et al., 2009). In many cases
such algorithms generate restricted types of graph
structures, but these restrictions appear feasible for
our target representations. The last approach is
more machine learning?oriented; they apply classi-
fiers or scoring methods (e.g. edge-factored scores),
and find the highest-scoring structures by some de-
coding method.
It is difficult to tell which approach is the best;
actually, the top three systems in the closed and
open tracks selected very different approaches. A
possible conclusion is that exploiting existing sys-
tems or techniques for dependency parsing was
successful; for example, Peking built an ensemble
of existing transition-based and graph-based depen-
dency parsers, and Priberam extended an existing
dependency parser. As we indicated in the task de-
scription, a novel feature of this task is that we have
to compute graph structures, and cannot assume
well-known properties like projectivity and lack of
reentrancies. However, many of the participants
found that our representations are mostly tree-like,
and this fact motivated them to apply methods that
have been well studied in the field of syntactic de-
pendency parsing.
Finally, we observe that three teams participated
in both the closed and open tracks, and all of them
reported that adding external resources improved
accuracy by a little more than one point. Systems
with (only) open submissions extensively use syn-
tactic features (e.g. dependency paths) from exter-
nal resources, and they are shown effective even
70
with simple machine learning models. Pre-existing,
tree-oriented dependency parsers are relatively ef-
fective, especially when combined with graph-to-
tree transformation. Comparing across our three
target representations, system scores show a ten-
dency PAS> DM> PCEDT, which can be taken as
a tentative indicator of relative levels of ?parsabil-
ity?. As suggested in Section 4, this variation most
likely correlates at least in part with diverging de-
sign decisions, e.g. the inclusion of relatively local
and deterministic dependencies involving function
words in PAS, or the decision to annotate contex-
tually determined speaker meaning (rather than
?mere? sentence meaning) in at least some construc-
tions in PCEDT.
8 Conclusions and Outlook
We have described the motivation, design, and out-
comes of the SDP 2014 task on semantic depen-
dency parsing, i.e. retrieving bi-lexical predicate?
argument relations between all content words
within an English sentence. We have converted to
a common format three existing annotations (DM,
PAS, and PCEDT) over the same text and have put
this to use for the first time in training and testing
data-driven semantic dependency parsers. Building
on strong community interest already to date and
our belief that graph-oriented dependency parsing
will further gain importance in the years to come,
we are preparing a similar (slightly modified) task
for SemEval 2015. Candidate modifications and
extensions will include cross-domain testing and
evaluation at the level of ?complete? predications
(in contrast to more lenient per-dependency F
1
used
this year). As optional new sub-tasks, we plan on
offering cross-linguistic variation and predicate (i.e.
semantic frame) disambiguation for at least some of
the target representations. To further probe the role
of syntax in the recovery of semantic dependency
relations, we will make available to participants
a wider selection of syntactic analyses, as well as
add a third (idealized) ?gold? track, where syntactic
dependencies are provided directly from available
syntactic annotations of the underlying treebanks.
Acknowledgements
We are grateful to ?eljko Agi
?
c and Bernd Bohnet
for consultation and assistance in preparing our
baseline and companion parses, to the Linguistic
Data Consortium (LDC) for support in distributing
the SDP data to participants, as well as to Emily M.
Bender and two anonymous reviewers for feedback
on this manuscript. Data preparation was supported
through access to the ABEL high-performance com-
puting facilities at the University of Oslo, and we
acknowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. Part of this
work has been supported by the infrastructural fund-
ing by the Ministry of Education, Youth and Sports
of the Czech Republic (CEP ID LM2010013).
References
Bohnet, B. (2010). Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (p. 89 ? 97). Beijing, China.
Bohnet, B., & Nivre, J. (2012). A transition-based
system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 1455 ? 1465). Jeju
Island, Korea.
Buchholz, S., & Marsi, E. (2006). CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of the 10th Conference on Natural Lan-
guage Learning (p. 149 ? 164). New York, NY,
USA.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4),
281 ? 332.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 449 ? 454). Genoa, Italy.
Fares, M., Oepen, S., & Zhang, Y. (2013). Machine
learning for high-quality tokenization. Replicating
variable tokenization schemes. In Computational lin-
guistics and intelligent text processing (p. 231 ? 244).
Springer.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1), 15 ? 28.
Flickinger, D., Zhang, Y., & Kordoni, V. (2012). Deep-
Bank. A dynamically annotated treebank of the Wall
Street Journal. In Proceedings of the 11th Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries (p. 85 ? 96). Lisbon, Portugal: Edi??es Colibri.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling
of semantic roles. Computational Linguistics, 28,
71
245 ? 288.
Grice, H. P. (1968). Utterer?s meaning, sentence-
meaning, and word-meaning. Foundations of Lan-
guage, 4(3), 225 ? 242.
Haji?c, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart?, M. A., M?rquez, L., . . . Zhang, Y. (2009).
The CoNLL-2009 Shared Task. syntactic and seman-
tic dependencies in multiple languages. In Proceed-
ings of the 13th Conference on Natural Language
Learning (p. 1 ? 18). Boulder, CO, USA.
Haji?c, J., Haji?cov?, E., Panevov?, J., Sgall, P., Bojar,
O., Cinkov?, S., . . . ?abokrtsk?, Z. (2012). An-
nouncing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(p. 3153 ? 3160). Istanbul, Turkey.
Ivanova, A., Oepen, S., ?vrelid, L., & Flickinger, D.
(2012). Who did what to whom? A contrastive study
of syntacto-semantic dependencies. In Proceedings
of the Sixth Linguistic Annotation Workshop (p. 2 ?
11). Jeju, Republic of Korea.
Kate, R. J., & Wong, Y. W. (2010). Semantic pars-
ing. The task, the state of the art and the future. In
Tutorial abstracts of the 20th Meeting of the Associ-
ation for Computational Linguistics (p. 6). Uppsala,
Sweden.
Marcus, M., Santorini, B., & Marcinkiewicz, M. A.
(1993). Building a large annotated corpora of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19, 313 ? 330.
Mel?
?
cuk, I. (1988). Dependency syntax. Theory and
practice. Albany, NY, USA: SUNY Press.
Meyers, A., Reeves, R., Macleod, C., Szekely, R.,
Zielinska, V., Young, B., & Grishman, R. (2004).
Annotating noun argument structure for NomBank.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (p. 803 ?
806). Lisbon, Portugal.
Miyao, Y., Oepen, S., & Zeman, D. (2014). In-house:
An ensemble of pre-existing off-the-shelf parsers. In
Proceedings of the 8th International Workshop on
Semantic Evaluation. Dublin, Ireland.
Nakov, P. (2013). On the interpretation of noun com-
pounds: Syntax, semantics, and entailment. Natural
Language Engineering, 19(3), 291 ? 330.
Nivre, J., Hall, J., K?bler, S., McDonald, R., Nilsson,
J., Riedel, S., & Yuret, D. (2007). The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 915 ? 932). Prague,
Czech Republic.
Oepen, S., & L?nning, J. T. (2006). Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The
Proposition Bank. A corpus annotated with semantic
roles. Computational Linguistics, 31(1), 71 ? 106.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Meeting of the Association for Computational
Linguistics (p. 433 ? 440). Sydney, Australia.
Quine, W. V. O. (1960). Word and object. Cambridge,
MA, USA: MIT press.
Sagae, K., & Tsujii, J. (2008). Shift-reduce depen-
dency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (p. 753 ? 760). Manchester, UK.
Titov, I., Henderson, J., Merlo, P., & Musillo, G.
(2009). Online graph planarisation for synchronous
parsing of semantic and syntactic dependencies. In
Proceedings of the 21st International Joint Confer-
ence on Artifical Intelligence (p. 1562 ? 1567).
Vadas, D., & Curran, J. (2007). Adding Noun Phrase
Structure to the Penn Treebank. In Proceedings of
the 45th Meeting of the Association for Computa-
tional Linguistics (p. 240 ? 247). Prague, Czech Re-
public.
72
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 24?28
Manchester, August 2008
Toward a cross-framework parser annotation standard
Dan Flickinger
CSLI, Stanford University
danf@stanford.edu
Abstract
Efficient and precise comparison of parser
results across frameworks will require a
negotiated agreement on a target represen-
tation which embodies a good balance of
three competing dimensions: consistency,
clarity, and flexibility. The various annota-
tions provided in the COLING-08 shared
task for the ten ?required? Wall Street Jour-
nal sentences can serve as a useful ba-
sis for these negotations. While there is
of course substantial overlap in the con-
tent of the various schemes for these sen-
tences, no one of the schemes is ideal.
This paper presents some desiderata for
a negotiated target annotation scheme for
which straightforward mappings can be
constructed from each of the supplied an-
notation schemes.
1 Introduction
Efficient and precise comparison of parser results
across frameworks will require a negotiated agree-
ment on a target representation which embodies a
good balance of three competing dimensions: con-
sistency, clarity, and flexibility. The various anno-
tations provided in the COLING-08 shared task for
the ten ?required? Wall Street Journal sentences can
serve as a useful basis for these negotations. While
there is of course substantial overlap in the content
of the various schemes for these sentences, no one
of the schemes is ideal, containing either too much
or too little detail, or sometimes both.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
2 Predicate-argument structures, not
labelled bracketings
Competing linguistic frameworks can vary dramat-
ically in the syntactic structures they assign to sen-
tences, and this variation makes cross-framework
comparison of labelled bracketings difficult and
in the limit uninteresting. The syntactic struc-
tures of Combinatory Categorial Grammar (CCG:
Steedman (2000), Hockenmaier (2003), Clark and
Curran (2003)), for example, contrast sharply
with those of the Penn Treebank Marcus et al
(1993), and the PTB structures differ in many less
dramatic though equally important details from
those assigned in Lexical Functional Grammar
(LFG: Bresnan and Kaplan (1982)) or Head-driven
Phrase Structure Grammar (HPSG: Pollard and
Sag (1994)). We even find variation in the assign-
ments of part-of-speech tags for individual tokens,
for example with words like ?missionary? or ?clas-
sical? treated as adjectives in some of the annota-
tions and as nouns in others. Furthermore, a sim-
ple labelled bracketing of surface tokens obscures
the fact that a single syntactic constituent can fill
multiple roles in the logical structure expressed
by a sentence, as with controlled subjects, relative
clauses, appositives, coordination, etc. More de-
tailed discussions of the obstacles to directly com-
paring syntactic structures include Preiss (2003),
Clark and Curran (2007), and most recently Sagae
et al (2008).
Since it is this underlying logical content that
we seek when parsing a sentence, the target anno-
tation for cross-framework comparison should not
include marking of syntactic constituents, but fo-
cus instead on the predicate argument structures
determined by the syntactic analysis, as proposed
ten years ago by Carroll et al (1998). Several of
24
the annotations provided in the shared task already
do this, providing a good set of starting points for
negotiating a common target.
3 General annotation characteristics
Some of the issues in need of negotiation are quite
general in nature, while many others involve spe-
cific phenomena. First, the general ones:
3.1 Unique identifiers
Since a given word can appear multiple times
within a single sentence, each token-derived el-
ement of the annotation needs a unique identi-
fier. Some of the supplied annotations use the to-
ken position in the sentence for this purpose, but
this is not general enough to support competing
hypotheses about the number of tokens in a sen-
tence. A sharp example of this is the word pixie-
like in sentence 56, which one of the annotations
(CONLL08) analyzes as two tokens, quite reason-
ably, since -like is a fully productive compound-
ing element. So a better candidate for the unique
identifier for each annotation element would be the
initial character position of the source token in the
original sentence, including spaces and punctua-
tion marks as characters. Thus in the sentence the
dog slept the annotation elements would be the-
1, dog-5, and slept-9. The original sentences in
this shared task were presented with spaces added
around punctuation, and before ?n?t?. so the char-
acter positions for this task would be computed
taking this input as given. Using character posi-
tions rather than token positions would also better
accommodate differing treatments of multi-word
expressions, as for example with Los Angeles in
sentence 9, which most of the supplied schemes
annotate as two tokens with Los modifying Ange-
les, but which PARC treats as a single entity.
3.2 One token in multiple roles
Most of the supplied annotations include some no-
tational convention to record the fact that (a phrase
headed by) a single token can fill more than one
logical role at the predicate-argument level of rep-
resentation. This is clear for controlled subjects
as in the one for play in sentence 53: ?doesn?t
have to play...concertos?, and equally clear for the
missing objects in tough-type adjective phrases,
like the object of apply in sentence 133: ?impos-
sible to apply?. This multiple filling of roles by
a single syntactic constituent can be readily ex-
pressed in a target annotation of the predicate argu-
ment structure if the token heading that constituent
bears the unique positional identifier which has al-
ready been motivated above. Supplied annotation
schemes that already directly employ this approach
include PARC and Stanford, and the necessary
positional information is also readily available in
the CCG-PA, HPSG-PA, and CONLL08 schemes,
though not in the RASP-GR or PTB notations. It
will be desirable to employ this same convention
for the logical dependencies in other constructions
with missing arguments, including relative clauses,
other unbounded dependencies like questions, and
comparative constructions like sentence 608?s than
President Bush has allowed .
3.3 Stem vs surface form
Some of the supplied annotations (CCG-PA,
RASP-GR, and Stanford) simply use the surface
forms of the tokens as the elements of relations,
while most of the others identify the stem forms
for each token. While stemming might introduce
an additional source of inconsistency in the anno-
tations, the resulting annotations will be better nor-
malized if the stems rather than the surface forms
of words are used. This normalization would also
open the door to making such annotations more
suitable for validation by reasoning engines, or for
later word-sense annotation, or for applications.
3.4 Identification of root
Most but not all of the supplied annotation
schemes identify which token supplies the outer-
most predication for the sentence, either directly
or indirectly. An explicit marking of this outer-
most element, typically the finite verb of the main
clause of a sentence, should be included in the tar-
get annotation, since it avoids the spurious ambi-
guity found for example in the HPSG-PA annota-
tion for sentence 22, which looks like it would be
identical for both of the following two sentences:
? Not all those who wrote oppose the changes .
? Not all those who oppose the changes wrote .
3.5 Properties of entities and events
Some of the supplied annotation schemes include
information about morphosyntactically marked
properties of nouns and verbs, including person,
number, gender, tense, and aspect. Providing for
explicit marking of these properties in a common
25
target annotation is desirable, at least to the level of
detail adopted by several of the supplied schemes.
While several of the supplied annotation
schemes marked some morphosyntactic properties
some of the time, the PARC annotation of positive
degree for all adjectives reminds us that it would be
useful to adopt a notion of default values for these
properties in the target annotation. These defaults
would be explicitly defined once, and then only
non-default values would need to be marked ex-
plicitly in the annotation for a given sentence. For
example, the PARC annotation marks the ?perf?
(perfect) attribute for a verb only when it has a
positive value, implicitly using the negative value
as the default. This use of defaults would improve
the readability of the target annotation without any
loss of information.
Marking of the contrast between declarative, in-
terrogative, and imperative clauses is included in
some but not all of the annotation schemes. Since
this contrast is highly salient and (almost always)
easily determined, it should be marked explicitly in
the target annotation, at least for the main clause.
3.6 Named entities
The supplied annotations represent a variety of ap-
proaches to the treatment of named entities where
multiple tokens comprise the relevant noun phrase,
as in sentence 53?s ?The oboist Heinz Holliger?.
Several schemes treat both oboist and Heinz sim-
ply as modifiers of Holliger, drawing no distinc-
tion between the two. The PARC and PTB anno-
tations identify Heinz Holliger as a named entity,
with oboist as a modifier, and only the CONLL08
scheme analyses this expression as an apposition,
with oboist as the head predicate of the whole PN.
Since complex proper names appear frequently
with modifiers and in apposition constructions, and
since competing syntactic and semantic analyses
can be argued for many such constituents, the tar-
get annotation should contain enough detail to il-
luminate the substantive differences without exag-
gerating them. Interestingly, this suggests that the
evaluation of a given analysis in comparison with a
gold standard in the target annotation may require
some computation of near-equivalence at least for
entities in complex noun phrases. If scheme A
treats Holliger as the head token for use in exter-
nal dependencies involving the above noun phrase,
while scheme B treats oboist as the head token, it
will be important in evaluation to exploit the fact
that both schemes each establish some relation be-
tween oboist and Holliger which can be interpreted
as substitutional equivalence with respect to those
external dependencies. This means that even when
a target annotation scheme has been agreed upon,
and a mapping defined to convert a native anno-
tated analyis into a target annotation, it will still be
necessary to create non-trivial software which can
evaluate the mapped analysis against a gold stan-
dard analysis.
4 Notational conventions to be negotiated
A number of notational conventions will have to be
negotiated for a common target annotation scheme,
ranging from quite general design decisions to de-
tails about very specific linguistic phenomena.
4.1 Naming of arguments and relations
It seems plausible that agreement could be reached
quickly on the names for at least the core gram-
matical functions of subject, direct object, indirect
object, and verbal complement, and perhaps also
on the names for adjectival and adverbial modi-
fiers. Prepositions are more challenging, since they
are very often two-place relations, and often live
on the blurry border between arguments and ad-
juncts. For example, most of the supplied anno-
tation schemes treated the by-PP following moved
in sentence 608 as a marker for the logical subject
of the passive verb, but this was at least not clear
in the CCG-PA annotation. In sentence 56, there
was variation in how the from and to PPs were an-
notated, with CONLL08 making the two to PPs
dependents of the from PP rather than of the verb
range.
Some of the supplied annotation schemes in-
troduced reasonable but idiosyncratic names for
other frequently occurring relations or dependen-
cies such as relative clauses, appositives, noun-
noun compounds, and subordinate clauses. An in-
ventory of these frequently occurring phenomena
should be constructed, and a target name negoti-
ated for each, recognizing that there will always be
a long tail of less frequently occurring phenomena
where names will not (yet) have been negotiated.
4.2 Coordination
Perhaps the single most frequent source of appar-
ent incompatibility in the supplied annotations for
the ten required sentences in this task involves co-
ordination. Some schemes, like HPSG-PA and
26
Stanford, treat the first conjunct as the primary en-
tity which participates in other predications, with
the other conjunct(s) dependent on the first, though
even here they usually (but not always) distribute
conjoined verbal arguments with separate predica-
tions for each conjunct. Some schemes, like the
PTB, PARC, and RASP-GR, represent the group-
ing of three or more conjuncts as flat, while others
like the Stanford scheme represent them as pairs.
Most schemes make each conjunction word itself
explicit, but for example the PARC annotation of
866 marks only one occurrence of and even though
this three-part coordinate structure includes two
explicit conjunctions.
While the distribution of conjoined elements in
coordinate structures may be the most practical tar-
get annotation, it should at least be noted that this
approach will not accommodate collective read-
ings of coordinate NPs as in well-known examples
like ?Tom and Mary carried the piano upstairs.?
But the alternative, to introduce a new conjoined
entity for every coordinate structure, may be too
abstract to find common support among develop-
ers of current annotation schemes, and perhaps not
worth the effort at present.
However, it should be possible to come to agree-
ment on how to annotate the distribution of con-
joined elements consistently, such that it is clear
both which elements are included in a coordinate
structure, and what role each plays in the relevant
predicate argument structures.
4.3 Verb-particle expressions
Another phenomenon exhibited several times in
these ten sentences involves verb-particle expres-
sions, as with thrash out and perhaps also stop by.
Most of the supplied schemes distinguished this
dependency, but some simply treated the particle
as a modifier of the verb. It would be desirable
to explicitly distinguish in a target annotation the
contrast between stopped a session and stopped by
a session without having to hunt around in the an-
notation to see if there happens to be a modifier of
stop that would dramaticaly change its meaning.
The example with stop by a session also high-
lights the need for an annotation scheme which lo-
calizes the differences between competing analy-
ses where possible. Though all of the supplied an-
notations treat by as a particle just like up in ?look
up the answer?, in fact by fails the clearest test for
being a particle, namely the ability to appear after
the NP argument: ?*He stopped the session by.?
An analysis treating ?by the session? as a selected-
for PP with a semantically empty by might bet-
ter fit the linguistic facts, but the target annotation
could remain neutral about this syntactic debate if
it simply recorded the predicate as stop by, taking
an NP argument just as is usually done for the com-
plement of rely in ?rely on us?.
4.4 Less frequent phenomena
Since each new phenomenon encountered may
well require negotiation in order to arrive at a
common target annotation, it will be important to
include some provisional annotation for relations
that have not yet been negotiated. Even these
ten example sentences include a few expressions
where there was little or no agreement among the
schemes about the annotations, such as ?if not
more so? in sentence 30, or ?to be autographed?
in sentence 216. It would be convenient if the
target annotation scheme included a noncommittal
representation for some parts of a given sentence
explicitly noting the lack of clarity about what the
structure should be.
4.5 Productive derivational morphology
It was surprising that only one of the annota-
tion schemes (CONLL08) explicitly annotated the
nominal gerund conducting in sentence 53 as pro-
ductively related to the verb conduct.. While the
issue of derivational morphology is of course a
slippery slope, the completely productive gerund-
forming process in English should be accommo-
dated in any target annotation scheme, as should a
small number of other highly productive and mor-
phologically marked derivational regularities, in-
cluding participial verbs used as prenominal mod-
ifiers, and comparative and superlative adjectives.
Including this stemming would provide an infor-
mative level of detail in the target annotation, and
one which can almost always be readily deter-
mined from the syntactic context.
5 Next steps
The existing annotation schemes supplied for this
task exhibit substantial common ground in the
nature and level of detail of information being
recorded, making plausible the idea of investing a
modest amount of joint effort to negotiate a com-
mon target representation which addresses at least
some of the issues identified here. The initial com-
27
mon target annotation scheme should be one which
has the following properties:
? Each existing scheme?s annotations can be
readily mapped to those of the target scheme
via an automatic procedure.
? The annotations appear in compact, humanly
readable form as sets of tuples recording
either predicate-argument dependencies or
properties of entities and events, such as num-
ber and tense.
? The inventory of recorded distinctions is rich
enough to accommodate most of what any
one scheme records, though it may not be
a superset of all such distinctions. For ex-
ample, some scheme might record quantifier
scope information, yet the target annotation
scheme might not, either because it is not of
high priority for most participants, or because
it would be difficult to produce consistently in
a gold standard.
The primary purposes of such a target annotation
scheme should be to facilitate the automatic com-
parison of results across frameworks, and to sup-
port evaluation of results against gold standard
analyses expressed in this target scheme. It might
also be possible to define the scheme such that the
target annotations contain enough information to
serve as the basis for some application-level tasks
such as reasoning, but the primary design criteria
should be to enable detailed comparison of analy-
ses.
References
Bresnan, Joan and Ronald M. Kaplan, 1982. Lexical-
Functional Grammar. A Formal System for Gram-
matical Representation. The Mental Representation
of Grammatical Relations, ed. Joan Bresnan. MIT
Press, Cambridge, MA.
Carroll, John, Edward Briscoe and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. Proceedings of the 1st International Confer-
ence on Language Resources and Evaluation.
Clark, Stephen and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. Proceed-
ings of the 2003 conference on Empirical methods in
natural language processing, pp.97?104.
Clark, Stephen and James R. Curran. 2007.
Formalism-Independent Parser Evaluation with
CCG and DepBank. Proceedings of the Association
for Computational Linguistics 2007.
Harrison, P., S. Abney, D. Flickinger, C. Gdaniec,
R. Grishman, D. Hindle, B. Ingria, M. Marcus, B.
Santorini, , and T. Strzalkowski. 1991. Evaluat-
ing syntax performance of parser/grammars of En-
glish. Natural Language Processing Systems Eval-
uation Workshop, Technical Report RL- TR-91-6, J.
G. Neal and S. M. Walter, eds.
Hockenmaier, Julia. 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Marcus, Mitchell P., Beatrice Santorini and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English. The Penn Treebank. Computa-
tional Linguistics 19:313?330.
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. The University of
Chicago Press and CSLI Publications, Chicago, IL
and Stanford, CA.
Preiss, Judita. 2003. Using Grammatical Relations to
Compare Parsers. Proceedings of the European As-
sociation for Computational Linguistics 2003.
Sagae, Kenji, Yusuke Miyao, Takuya Matsuzaki and
Jun?ichi Tsujii. 2008. Challenges in Map-
ping of Syntactic Representations for Framework-
Independent Parser Evaluation. Proceedings of the
Workshop on Automated Syntatic Annotations for In-
teroperable Language Resources at the 1st Inter-
national Conference on Global Interoperability for
Language Resources, pp.61?68.
Steedman, Mark. 2000. The syntactic process. MIT
Press, Cambridge, MA.
28
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 68?73,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Toward More Precision in Correction of Grammatical Errors
Dan Flickinger
Center for the Study of
Language and Information
Stanford University
danf@stanford.edu
Jiye Yu
Center for the Study of
Language and Information
Stanford University
jyu2009@stanford.edu
Abstract
We describe a system for detecting and
correcting instances of a small class of fre-
quently occurring grammatical error types
in a corpus of essays which have been
manually annotated for these errors. Our
system employs a precise broad-coverage
grammar of English which has been aug-
mented with a set of mal-rules and mal-
entries to explicitly license certain types of
erroneous expressions. The derivation tree
produced by a parser using this grammar
identifies the location and type of an error
in an ill-formed sentence, enabling a post-
processing script to use the tree and the in-
ventory of error types to delete and/or in-
sert tokens in order to produce a corrected
version of the original sentence.
1 Overview
As a participating group in the 2013 CoNLL
Shared Task on Grammatical Error Correction,
we adapted an existing system for error detec-
tion in a simpler closed-vocabulary domain to
meet the additional demands of accommodating
an open vocabulary and producing corrections for
the errors identified. The training and test data
for this shared task are from the NUCLE cor-
pus (Dahlmeier et al, 2013), which consists of
about one million words of short essays written
by relatively competent English language learn-
ers. Each sentence has been manually annotated
to identify and correct a wide range of grammat-
ical and stylistic error types, though the shared
task focused only on correcting instances of five
of these types. Following standard procedure for
such shared tasks, the organizers supplied most of
the annotated data as a development corpus, and
held out a 1381-sentence test corpus which was
used for the evaluation of system output.
2 Resources and Method
The system developed for this task is an extension
of an existing language-processing engine used to
identify grammatical errors in short sentences and
paragraphs written by elementary school students
as part of the automated Language Arts and Writ-
ing course included in the EPGY (Education Pro-
gram for Gifted Youth) course offerings (Suppes et
al., 2012). This error detection engine consists of
a grammar, a parser, and a post-processing script
that interprets the error codes in the derivation
tree for each parsed sentence. Both the grammar
and the parser are open-source resources devel-
oped and distributed as part of the DELPH-IN con-
sortium (www.delph-in.net). We use the English
Resource Grammar, described below, which we
have augmented with both rules and lexical entries
that license instances of certain error types, using
the mal-rule approach of (Schneider and McCoy,
1998), adapted and extended for the ERG as de-
scribed in (Bender et al, 2004). For parsing each
sentence with this grammar, we use the relatively
efficient PET parser (Callmeier, 2002), along with
a parse-ranking method based on a model trained
on a manually disambiguated treebank, so far con-
sisting only of parses of well-formed sentences. In
addition to using the manually constructed 37,000-
word lexicon included in the ERG, we accommo-
date unknown words by mapping POS tags pro-
duced by TnT (Brants, 2000) to generic lexical en-
try types on the fly. The bottom-up chart parser
then exhaustively applies the rules of the grammar
to the lexical entries introduced by the tokens in
the input sentence, producing a packed forest of
analyses (derivations) ranked by likelihood, and
then presents the most likely derivation for post-
processing. The post-processor is a script which
uses the derivation tree to identify the type and lo-
cation of each error, and then takes appropriate ac-
tion, which in the course is an instructional mes-
68
sage to the student, and in this shared task is a cor-
rected version of the original sentence.
2.1 English Resource Grammar
The English Resource Grammar used for this task
(ERG: (Flickinger, 2000), (Flickinger, 2011)) is
a broad-coverage grammar implementation which
has been under continuous development since the
mid-1990s at Stanford. As an implementation
within the theoretical framework of Head-driven
Phrase Structure Grammar (HPSG: (Pollard and
Sag, 1994)), the ERG has since its inception en-
coded both morphosyntactic and semantic prop-
erties of English, in a declarative representation
that enables both parsing and generation. While
development has always taken place in the con-
text of specific applications, primary emphasis in
the ERG has consistently been on the linguistic
accuracy of the resulting analyses, at some ex-
pense to robustness. Its initial use was for gener-
ation within the German-English machine transla-
tion prototype developed in the Verbmobil project
(Wahlster, 2000), so constraining the grammar
to avoid overgeneration was a necessary design
requirement that fit well with the broader aims
of its developers. Applications using the gram-
mar since then have included automatic processing
of e-commerce customer support email messages,
a second machine translation system (LOGON:
(Lnning et al, 2004)), and information extraction
over the full English Wikipedia (Flickinger et al,
2010).
At present, the ERG consists of a rich hier-
archy of types encoding regularities both in the
lexicon and in the syntactic constructions of En-
glish. The lexicon contains 40,000 manually con-
structed lexeme entries, each assigned to one of
975 lexical types at the leaves of this hierarchy,
where the types encode idiosyncracies of subcat-
egorization, modification targets, exceptional be-
havior with respect to lexical rules, etc. The gram-
mar also includes 70 derivational and inflectional
rules which apply to these lexemes (or to each
other?s outputs) to produce the words as they ap-
pear in text. The grammar provides 225 syntactic
rules which admit either unary or binary phrases;
these include a relatively small number of highly
schematic rules which license ordinary combina-
tions of heads with their arguments and their mod-
ifiers, and a rather larger number of construction-
specific rules both for frequently occurring phrase
types such as coordinate structures or appositives,
and for phrase types that occur with markedly
differing frequencies in verious corpus domains,
such as questions or vocatives. Statistical models
trained on manually annotated treebanks are used
both in parsing (Toutanova et al, 2005) and in gen-
eration (Velldal, 2008) to rank the relative likeli-
hoods of the outputs, in order to address the issue
of disambiguation which is central to the use of
any broad-coverage grammar for almost any task.
2.2 Mal-rule example
Each of the hand-coded mal-rules added to the
standard ERG is a variant of a rule needed to anal-
yse well-formed English input. A simple exam-
ple of a mal-rule is given below, expressed in the
attribute-value representation for an HPSG rule;
this unary rule licenses a noun phrase headed by a
singular count noun but lacking its normally oblig-
atory article, as for the NP black cet in That dog
chased black cat. Here the single daughter in this
noun phrase (the HD-DTR) is a nominal phrase still
seeking an obligatory specifier (the article or de-
terminer in a well formed noun phrase), where the
head noun is a singular count noun (non-divisible).
The SYNSEM value in the rule discharges that
obligatory specifier requirement just as the normal
unary rule for bare plural noun phrases does, and
supplies the necessary implicit quantifier in the se-
mantics of the phrase.
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
SYNSEM
?
?
?
?
?
LOCAL
?
?
?
?
CAT
?
?
HEAD
1
noun
VAL
[
SPR <>
COMPS <>
]
?
?
CONT
[
RELS < quant rel >
]
?
?
?
?
?
?
?
?
?
HD-DTR
?
?
?
?
?
?
?
SYNSEM
?
?
?
?
?
?
?
LOCAL
?
?
?
?
?
?
CAT
?
?
?
HEAD
1
VAL
[
SPR <
[
OPT ?
]
>
COMPS <>
]
?
?
?
AGR
[
PN 3sing
DIV ?
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Mal-rule for bare singular NP
2.3 Error types in the task
Of the five error types used in the shared task,
four were already included in the grammar as used
in the EPGY course, involving errors with arti-
cles/determiners, number on nouns, subject-verb
agreement, and verb form. For the task, we added
mal-rules and mal-entries to analyze a subset of er-
rors of the fifth type, which involve incorrect use
of prepositions. Within the ERG, each of the five
error types is associated with multiple mal-rules or
69
mal-entries, each licensing one specific error con-
figuration, such as a mal-rule to accommodate the
omission of an obligatory determiner for a noun
phrase headed by a singular count noun, or a mal-
entry for the unwanted use of the with a proper
name.
Most of these grammar-internal error identifiers
correspond to a simple adjustment for correction
in the original sentence, such as the insertion or
deletion of a particular token, or a change to the in-
flection of a particular noun or verb. However, for
some errors, several candidate corrections are trig-
gered by the error identifier, so the post-processing
script must select the most suitable of these correc-
tion candidates. The most frequent correction il-
lustrating this ambiguity is for singular count noun
phrases missing the determiner, such as black cat
in we admired black cat., where the correction
might be the black cat, a black cat, or black cats.
Lacking a rich discourse representation of the con-
text surrounding the error, we employ an N-gram
based ranking approach to choose among the three
alternatives, where the post-processor currently
calls the Microsoft N-gram online resource (Wang
et al, 2011).
Since the development and test data is presented
as pre-tokenized input with one token per line in
familiar CoNLL format, we also employ an offline
script which converts a file of this format into one
which has a single sentence per line, preserving
the tokenization of the CoNLL file, and it is this
one-sentence-per-line file which is processed by
the correction script, which in turn calls the parser
and applies the post-processing steps to its output.
3 An example
We illustrate our method with a simple example
sentence, to show each step of the process. Con-
sider the analysis in Figure 1 of the following sen-
tence taken from the test corpus:
In supermarkets monitors is needed because we
have to track thieves .
The parser is called with this sentence as in-
put, constructs a packed forest of all candidate
analyses licensed by the grammar, and identifies
the most likely analysis as determined by a
general-purpose statistical model trained only
on analyses of well-formed sentences. A more
detailed view of the parse tree in Figure 1 is the
bracketed derivation tree given in (2). Each line of
the derivation identifies the syntactic construction,
lexical rule, or lexical entry used to build each
constituent, and shows its token span, and for the
leaf nodes, the lexical entry, its type (after the
slash), and the surface form of that word in the
input sentence. The boldface identifier on the first
line of the derivation tree shows that this analysis
contains at least one erroneous constituent, which
a perusal of the tree locates as the other boldface
identifier, be c is rbst, for the mal-entry for is that
licenses a mismatch in subject-verb agreement.
(2) Derivation tree view of Fig. 1:
hd-aj scp c 0 11 [ root robust s ]
flr-hd nwh-nc-pp c 0 5
hd-cmp u c 0 2
in/p np i-reg 0 1 "in"
hdn bnp c 1 2
n pl olr 1 2
supermarket n1/n - c 1 2
"supermarkets"
sb-hd nmc c 2 5
hdn bnp c 2 3
n pl olr 2 3
monitor n1/n - c 2 3 "monitors"
hd-cmp u c 3 5
be c is rbst 3 4 "is"
hd xaj-int-vp c 4 5
hd optcmp c 4 5
v pas odlr 4 5
need v1/v np 4 5 "needed"
hd-cmp u c 5 11
because/p cp s 5 6 "because"
sb-hd nmc c 6 11
hdn bnp-qnt c 6 7
we/n - pr-we 6 7 "we"
hd-cmp u c 7 11
v n3s-bse ilr 7 8
have to1/v vp ssr 7 8 "have"
hd-cmp u c 8 11
to c prop/cm vp to 8 9 "to"
hd-cmp u c 9 11
v n3s-bse ilr 9 10
track v1/v np* 9 10 "track"
hdn bnp c 10 11
period plr 10 11
n pl olr 10 11
thief n1/n - c 10 11 "thieves."
The correction script finds this mal-entry identi-
fier in the derivation tree, notes its token position,
and determines from the identifier that the required
correction consists of a simple token substitution,
replacing the surface token is with are. Since no
other errors are present in the derivation tree, the
script then records in the corpus output file the cor-
rected sentence with only the one alteration from
its original form.
Of course, a derivation tree will often identify
multiple errors, and for some error types may re-
quire that multiple tokens be modified for a sin-
70
S




HH
HH
H
HH
H
PP
 HH
P
in
NP
N
N
supermarkets
S/PP




H
HH
H
HH
S/PP


H
HH
NP
N
N
monitors
VP/PP
 HH
V/PP
is
VP/PP
VP
V
V
needed
PP


H
HH
P
because
S


HH
H
NP
NP
we
VP


HH
H
V
V
have
VP


HH
H
COMP
to
VP
 HH
V
V
track
NP
N
N
N
thieves.
Figure 1: Sample parse tree produced with ERG
gle error, such as in the correction of the equip-
ments have arrived to the equipment has arrived.
Each mal-rule or mal-entry identifier is associated
with a specific correction procedure defined in the
correction script, and the script carries out these
changes in a pre-determined order, for the rela-
tively infrequent instances where the order of ap-
plication matters. For simple alterations such as
a change of number on nouns or verbs, we could
have used the grammar-internal inflectional rule
machinery, but found it more convenient to use ex-
isting Perl and Python modules for English word
inflection.
4 Results and Discussion
During the development phase of the shared task,
we adapted and refined our method using the first
5000 NUCLE sentences from the roughly 50,000-
sentence development corpus. Since our focus in
this task is on precision more than on recall, we
carried out repeated detailed examinations of the
correction procedure?s outputs on the first 500
sentences. In comparing our system?s proposed
corrections with the ?gold? human annotations
of errors for these 500, we found the following
frequencies of mismatches between system and
gold:
(3) Comparison of System and Gold on Dev-500
Alteration # of Sentences
Both match 34
Missing gold 26
Differing correction 25
Wrong alteration 28
Examples of the missing gold annotations include
(a) ?ArtOrDet? errors such as the missing article
for habitable environment in sentence 829-4-0 and
for password in sentence 830-1-1; (b) ?SVA? er-
rors such as for the verb increase in sentence 831-
3-8, and the verb are in sentence 840-4-2; and (c)
?Nn? errors such as for the noun equipments ap-
pearing in sentence 836-1-0, or evidences in sen-
tence 837-2-11.
These varying sources of mismatches made the
automated scoring script used in the evaluation
phase of the shared task (Dahlmeier and Ng, 2012)
not so helpful during development, since it re-
ported our system?s precision as 28%, whereas the
system is actually correct in more than 50% of the
alterations it makes for these first 500 sentences of
the development corpus.
This inconsistency in the gold annotations was
less of an issue, but still present, in our system?s
71
precision measure in the evaluation phase of the
shared task, as we found in studying the gold
annotations distributed for the test data after the
evaluation phase ended. The official scored results
for the system output that we submitted are given
in the table in (4).
(4) Official scoring of system output on test data
Precision 29.93%
Recall 5.86 %
F1 9.81 %
In examining the gold annotations for the 1381
sentences comprising the test corpus, we found
47 instances of genuine errors that were miss-
ing gold annotation, but that our system correctly
identified and repaired. While this led to a some-
what lower precision measure, we acknowledge
that compared with the total number of more than
1600 annotated corrections, this level of imperfec-
tion in the annotations was not seriously problem-
atic for evaluation, and we view the official results
in (4) as a reasonable measure of the system output
we submitted for scoring.
While comparing our system results with the
gold test annotations after the evaluation phase
ended, we have found and repaired several sources
of undesirable behavior in the grammar and in our
correction script, with the most significant being
the revision of lexical entries for two compound
nouns appearing with high frequency in the test
corpus: life expectancy (91 occurrences) and pop-
ulation aging/ageing (40 occurrences). Our lexi-
con had erroneously identified life expectancy as
countable, and the parser had wrongly analyzed
population aging as a noun modified by a partici-
ple, analogous to the person speaking. A third
frequently occurring error in the corpus was not
so simple to correct in our grammar, namely the
word society (95 occurrences), which is used con-
sistently in the test corpus as an abstract noun of-
ten wrongly appearing with the. Since this noun
can be used in a different sense (as an organiza-
tion) where the article is appropriate, as in the so-
ciety of wealthy patrons, we would need to find
some other knowledge source to determine that
in the domain of the test corpus, this sense is not
used. Hence our system still fails to identify and
correct the frequent and spurious the in the society.
With the small number of corrections made to
our system?s lexicon, and some minor improve-
ments to the post-processing script, our system
now produces output on the test corpus with an
improved precision measure of 47.5%, and a more
modest improvement in recall to 13.2%, for an F1
of 20.7%. Given the inconsistency of annotation
in the development corpus, it is as yet difficult to
evaluate whether these changes to our correction
script will result in corresponding improvements
in precision on unseen data.
5 Next steps
We see prospects for significant improvement us-
ing the method we are developing for the kind of
automatic correction studied in this shared task.
Many of the missteps that our correction proce-
dure makes can be traced to imperfect parse selec-
tion from among the candidate analyses produced
by the parser, and this could well be improved by
creating a Redwoods-style treebank that includes
both well-formed and ill-formed sentences for an-
notation, so the mal-rules and mal-entries get in-
cluded in the ranking model trained on such a tree-
bank. While our primary focus will continue to be
on increased precision in the corrections the sys-
tem proposes, we welcome the attention to recall
that this task brings, and expect to work with hy-
brid systems that do more with large-scale corpora
such as the English Wikipedia.
References
Emily M. Bender, Dan Flickinger, Stephan Oepen, An-
nemarie Walsh, and Timothy Baldwin. 2004. Ar-
boretum. Using a precision grammar for grammar
checking in CALL. In Proceedings of the InSTIL
Symposium on NLP and Speech Technologies in Ad-
vanced Language Learning Systems, Venice, Italy,
June.
Thorsten Brants. 2000. TnT - A statistical part-of-
speech tagger. In Proceedings of the 6th ACL Con-
ference on Applied Natural Language Processing,
Seattle, WA.
Ulrich Callmeier. 2002. Preprocessing and encod-
ing techniques in PET. In Stephan Oepen, Daniel
Flickinger, J. Tsujii, and Hans Uszkoreit, editors,
Collaborative Language Engineering. A Case Study
in Efficient Grammar-based Processing. CSLI Pub-
lications, Stanford, CA.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
568 ? 572, Montreal, Canada.
72
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The NUS Corpus of Learner English. In To
appear in Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations, Atlanta, Georgia.
Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.
2010. WikiWoods. Syntacto-semantic annotation
for English Wikipedia. In Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation, Valletta, Malta.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6 (1) (Special Issue on Efficient Pro-
cessing with HPSG):15 ? 28.
Dan Flickinger. 2011. Accuracy vs. robustness in
grammar engineering. In Emily M. Bender and Jen-
nifer E. Arnold, editors, Language from a Cogni-
tive Perspective: Grammar, Usage, and Processing,
pages 31?50. Stanford: CSLI Publications.
Jan Tore Lnning, Stephan Oepen, Dorothee Beer-
mann, Lars Hellan, John Carroll, Helge Dyvik, Dan
Flickinger, Janne Bondi Johannessen, Paul Meurer,
Torbjrn Nordgrd, Victoria Rosn, and Erik Velldal.
2004. LOGON. A Norwegian MT effort. In Pro-
ceedings of the Workshop in Recent Advances in
Scandinavian Machine Translation, Uppsala, Swe-
den.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Studies in Contempo-
rary Linguistics. The University of Chicago Press
and CSLI Publications, Chicago, IL, and Stanford,
CA.
David Schneider and Kathleen McCoy. 1998. Recog-
nizing syntactic errors in the writing of second lan-
guage learners. In Proceedings of Coling-ACL 1998,
pages 1198 ? 1204, Montreal.
Patrick Suppes, Dan Flickinger, Elizabeth Macken,
Jeanette Cook, and L. Liang. 2012. Description
of the EPGY Stanford University online courses for
Mathematics and the Language Arts. In Proceed-
ings of the International Society for Technology in
Education, San Diego, California.
Kristina Toutanova, Christoper D. Manning, Dan
Flickinger, and Stephan Oepen. 2005. Stochastic
HPSG parse selection using the Redwoods corpus.
Journal of Research on Language and Computation,
3(1):83 ? 105.
Erik Velldal. 2008. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo, Department of In-
formatics.
Wolfgang Wahlster, editor. 2000. Verbmobil. Foun-
dations of Speech-to-Speech Translation. Springer,
Berlin, Germany.
Kuansan Wang, Christopher Thrasher, Evelyne Viegas,
Xiaolong Li, , and Paul Hsu. 2011. An overview
of Microsoft Web N-gram corpus and applications.
In Proceedings of the 2011 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Portland, Oregon.
73
