Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 161?164,
Prague, June 2007. c?2007 Association for Computational Linguistics
GYDER: maxent metonymy resolution
Richa?rd Farkas
University of Szeged
Department of Informatics
H-6720 Szeged, A?rpa?d te?r 2.
rfarkas@inf.u-szeged.hu
Eszter Simon
Budapest U. of Technology
Dept. of Cognitive Science
H-1111 Budapest, Stoczek u 2.
esimon@cogsci.bme.hu
Gyo?rgy Szarvas
University of Szeged
Department of Informatics
H-6720 Szeged, A?rpa?d te?r 2.
szarvas@inf.u-szeged.hu
Da?niel Varga
Budapest U. of Technology
MOKK Media Research
H-1111 Budapest, Stoczek u 2.
daniel@mokk.bme.hu
Abstract
Though the GYDER system has achieved
the highest accuracy scores for the
metonymy resolution shared task at
SemEval-2007 in all six subtasks, we don?t
consider the results (72.80% accuracy for
org, 84.36% for loc) particularly impres-
sive, and argue that metonymy resolution
needs more features.
1 Introduction
In linguistics metonymy means using one term, or
one specific sense of a term, to refer to another,
related term or sense. For example, in ?the pen
is mightier than the sword? pen refers to writing,
the force of ideas, while sword refers to military
force. Named Entity Recognition (NER) is of
key importance in numerous natural language pro-
cessing applications ranging from information ex-
traction to machine translation. Metonymic usage
of named entities is frequent in natural language.
On the basic NER categories person, place,
organisation state-of-the-art systems generally
perform in the mid to the high nineties. These sys-
tems typically do not distinguish between literal or
metonymic usage of entity names, even though this
would be helpful for most applications. Resolving
metonymic usage of proper names would therefore
directly benefit NER and indirectly all NLP tasks
(such as anaphor resolution) that require NER.
Markert and Nissim (2002) outlined a corpus-
based approach to proper name metonymy as a se-
mantic classification problem that forms the basis
of the 2007 SemEval metonymy resolution task.
Instances like ?He was shocked by Vietnam? or
?Schengen boosted tourism? were assigned to broad
categories like place-for-event, sometimes
ignoring narrower distinctions, such as the fact that
it wasn?t the signing of the treaty at Schengen but
rather its actual implementation (which didn?t take
place at Schengen) that boosted tourism. But the
corpus makes clear that even with these (sometimes
coarse) class distinctions, several metonymy types
seem to appear extremely rarely in actual texts.
The shared task focused on two broad named en-
tity classes as metonymic sources, location and
org, each having several target classes. For more
details on the data sets, see the task description pa-
per Markert and Nissim (2007).
Several categories (e.g. place-for-event,
organisation-for-index) did not contain a
sufficient number of examples for machine learn-
ing, and we decided early on to accept the fact that
these categories will not be learned and to concen-
trate on those classes where learning seemed feasi-
ble. The shared task itself consisted of 3 subtasks
of different granularity for both organisation and lo-
cation names. The fine-grained evaluation aimed
at distinguishing between all categories, while the
medium-grained evaluation grouped different types
of metonymic usage together and addressed literal /
mixed / metonymic usage. The coarse-grained sub-
task was in fact a literal / nonliteral two-class classi-
fication task.
Though GYDER has obtained the highest accu-
racy for the metonymy shared task at SemEval-2007
in all six subtasks, we don?t consider the results
161
(72.80% accuracy for org, 84.36% for loc) par-
ticularly impressive. In Section 3 we describe the
feature engineering lessons learned fromworking on
the task. In Section 5 we offer some speculative re-
marks on what it would take to improve the results.
2 Learning
GYDER (the acronym was formed from the initials
of the author? first names) is a maximum entropy
learner. It uses Zhang Le?s 1 maximum entropy
toolkit, setting the Gaussian prior to 1. We used ran-
dom 5-fold cross-validation to determine the useful-
ness of a particular feature. Due to the small num-
ber of instances and features, the learning algorithm
always converged before 30 iterations, so the cross-
validation process took only seconds.
We also tested the classic C4.5 decision tree learn-
ing algorithm Quinlan (1993), but our early exper-
iments showed that the maximum entropy learner
was consistently superior to the decision tree clas-
sifier for this task, yielding about 2-5% higher accu-
racy scores on average on both tasks (on the training
set, using cross-validation).
3 Feature Engineering
We tested several features describing orthographic,
syntactic, or semantic characteristics of the Possibly
Metonymic Words (PMWs). Here we follow Nissim
and Markert (2005), who reported three classes of
features to be the most relevant for metonymy res-
olution: the grammatical annotations provided for
the corpus examples by the task organizers, the de-
terminer, and the grammatical number of the PMW.
We also report on some features that didn?t work.
3.1 Grammatical annotations
We used the grammatical annotations provided for
each PMW in several ways. First, we used as a
feature the type of the grammatical relation and the
word form of the related word. (If there was more
than one related word, each became a feature.) To
overcome data sparseness, it is useful to general-
ize from individual headwords Markert and Nissim
(2003). We used three different methods to achieve
this:
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
First, we used Levin?s (1993) verb classification
index to generalize the headwords of the most rele-
vant grammatical relations (subject and object). The
added feature was simply the class assigned to the
verb by Levin.
We also used WordNet (Fellbaum 1998) to gen-
eralize headwords. First we gathered the hypernym
path from WordNet for each headword?s sense#1 in
the train corpus. Based on these paths we collected
synsets whose tree frequently indicated metonymic
sense. We indicated with a feature if the headword
in question was in one of such collected subtrees.
Third, we have manually built a very small verb
classification ?Trigger? table for specific cases. E.g.
announce, say, declare all trigger the same feature.
This table is the only resource in our final system
that was manually built by us, so we note that on the
test corpus, disabling this ?Trigger? feature does not
alter org accuracy, and decreases loc accuracy by
0.44%.
3.2 Determiners
Following Nissim and Markert (2005), we distin-
guished between definite, indefinite, demonstrative,
possessive, wh and other determiners. We also
marked if the PMW was sentence-initial, and thus
necessarily determinerless. This feature was useful
for the resolution of organisation PMWs so we used
it only for the org tasks. It was not straightforward,
however, to assign determiners to the PMWswithout
proper syntactic analysis. After some experiments,
we linked the nearest determiner and the PMW to-
gether if we found only adjectives (or nothing) be-
tween them.
3.3 Number
This feature was particularly useful to separate
metonymies of the org-for-product class. We
assumed that only PMWs ending with letter s might
be in plural form, and for themwe compared the web
search result numbers obtained by the Google API.
We ran two queries for each PMWs, one for the full
name, and one for the name without its last charac-
ter. If we observed a significant increase in the num-
ber of hits returned by Google for the shorter phrase,
we set this feature for plural.
162
3.4 PMW word form
We included the surface form of the PMW as a fea-
ture, but only for the org domain. Cross-validation
on the training corpus showed that the use of this
feature causes an 1.5% accuracy improvement for
organisations, and a slight degradation for locations.
The improvement perfectly generalized to the test
corpora. Some company names are indeed more
likely to be used in a metonymic way, so we be-
lieve that this feature does more than just exploit-
ing some specificity of the shared task corpora. We
note that the ranking of our system would have been
unaffected even if we didn?t use this feature.
3.5 Unsuccessful features
Here we discuss those features where cross-
validation didn?t show improvements (and thus were
not included in the submitted system).
Trigger words were automatically collected lists of
word forms and phrases that more frequently
appeared near metonymic PMWs.
Expert triggers were similar trigger words or
phrases, but suggested by a linguist expert to
be potentially indicative for metonymic usage.
We experimented with sample-level, sentence-
level and vicinity trigger phrases.
Named entity labels given by a state-of-the-art
named entity recognizer (Szarvas et al 2006).
POS tags around PMWs.
Ortographical features such as capitalisation and
and other surface characteristics for the PMW
and nearby words.
Individual tokens of the potentially metonymic
phrase.
Main category of Levin?s hierarchical classification.
Inflectional category of the verb nearest to the PMW
in the sentence.
4 Results
Table 1. shows the accuracy scores of our submitted
system on fine classification granularity. As a base-
line, we also evalute the system without the Word-
Net, Levin, Trigger and PMW word form features.
This baseline system is quite similar to the one de-
scribed by Nissim and Markert (2005). We also pub-
lish the majority baseline scores.
run majority baseline submitted
org train 5-fold 63.30 77.51 80.92
org test 61.76 70.55 72.80
loc train 5-fold 79.68 85.58 88.36
loc test 79.41 83.59 84.36
Table 1: Accuracy of the submitted system
We could not exploit the hierarchical structure of
the fine-grained tag set, and ended up treating it as
totally unstructured even for the mixed class, unlike
Nissim and Markert, who apply complicated heuris-
tics to exploit the special semantics of this class.
For the coarse and medium subtasks of the loc
domain, we simply coarsened the fine-grained re-
sults. For the coarse and medium subtasks of
the org domain, we coarsened the train corpus to
medium coarseness before training. This idea was
based on observations on training data, but was
proven to be unjustified: it slightly decreased the
system?s accuracy on the medium subtask.
coarse medium fine
location 85.24 84.80 84.36
organisation 76.72 73.28 72.80
Table 2: Accuracy of the GYDER system for each
domain / granularity
In general, the coarser grained evaluation did not
show a significantly higher accuracy (see Table 2.),
proving that the main difficulty is to distinguish be-
tween literal and metonymic usage, rather than sepa-
rating metonymy classes from each other (since dif-
ferent classes represent significantly different usage
/ context). Because of this, data sparseness remained
a problem for coarse-grained classification as well.
Per-class results of the submitted system for
both domains are shown on Table 3. Note
that our system never predicted loc values from
the four small classes place-for-event and
product, object-for-name and other as
these had only 26 instances altogether. Since
we never had significant results for the mixed
category, in effect the loc task ended up a bi-
nary classification task between literal and
place-for-people.
163
loc class # prec rec f
literal 721 86.83 95.98 91.17
place-for-people 141 68.22 51.77 58.87
mixed 20 25.00 5.00 8.33
othermet 11 - 0.0 -
place-for-event 10 - 0.0 -
object-for-name 4 - 0.0 -
place-for-product 1 - 0.0 -
org class # prec rec f
literal 520 75.76 90.77 82.59
org-for-members 161 65.99 60.25 62.99
org-for-product 67 82.76 35.82 50.00
mixed 60 43.59 28.33 34.34
org-for-facility 16 100.0 12.50 22.22
othermet 8 - 0.0 -
object-for-name 6 50.00 16.67 25.00
org-for-index 3 - 0.0 -
org-for-event 1 - 0.0 -
Table 3: Per-class accuracies for both domains
While in the org set the system also ig-
nores the smallest categories othermet,
org-for-index and event (a total of 11
instances), the six major categories literal,
org-for-members, org-for-product,
org-for-facility, object-for-name,
mixed all receive meaningful hypotheses.
5 Conclusions, Further Directions
The features we eventually selected performed well
enough to actually achieve the best scores in all six
subtasks of the shared task, and we think they are
useful in general. But it is worth emphasizing that
many of these features are based on the grammatical
annotation provided by the task organizers, and as
such, would require a better dependency parser than
we currently have at our disposal to create a fully
automatic system.
That said, there is clearly a great deal of merit to
provide this level of annotation, and we would like
to speculate what would happen if even more de-
tailed annotation, not just grammatical, but also se-
mantical, were provided manually. We hypothesize
that the metonymy task would break down into the
task of identifying several journalistic cliches such
as ?location for sports team?, ?capital city for gov-
ernment?, and so on, which are not yet alays dis-
tinguished by the depth of the annotation.
It would be a true challenge to create a data set
of non-cliche metonymy cases, or a corpus large
enough to represent rare metonymy types and chal-
lenging non-cliche metonymies better.
We feel that at least regarding the corpus used for
the shared task, the potential of the grammatical an-
notation for PMWs was more or less well exploited.
Future systems should exploit more semantic knowl-
edge, or the power of a larger data set, or preferably
both.
Acknowledgement
We wish to thank Andra?s Kornai for help and
encouragement, and the anonymous reviewers for
valuable comments.
References
Christiane Fellbaum ed. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. A Preliminary Investigation. The University of
Chicago Press.
Katja Markert and Malvina Nissim. 2002. Metonymy
resolution as a classification task. Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002). Philadelphia, USA.
Katja Markert and Malvina Nissim. 2003. Syntactic Fea-
tures and Word Similarity for Supervised Metonymy
Resolution. Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL2003). Sapporo, Japan.
Malvina Nissim and Katja Markert. 2005. Learning
to buy a Renault and talk to BMW: A supervised
approach to conventional metonymy. International
Workshop on Computational Semantics (IWCS2005).
Tilburg, Netherlands.
Katja Markert and Malvina Nissim. 2007. SemEval-
2007 Task 08: Metonymy Resolution at SemEval-
2007. In Proceedings of SemEval-2007.
Ross Quinlan. 1993. C4.5: Programs for machine learn-
ing. Morgan Kaufmann.
Gyo?rgy Szarvas, Richa?rd Farkas and Andra?s Kocsor.
2006. Multilingual Named Entity Recognition Sys-
tem Using Boosting and C4.5 Decision Tree Learning
Algorithms. Proceedings of Discovery Science 2006,
DS2006, LNAI 4265 pp. 267-278. Springer-Verlag.
164
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 38?45,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The BioScope corpus: annotation for negation, uncertainty and their 
scope in biomedical texts 
 
 
Gy?rgy Szarvas1, Veronika Vincze1, Rich?rd Farkas2 and J?nos Csirik2 
1Department of Informatics 2Research Group on Artificial Intelligence 
University of Szeged Hungarian Academy of Science 
H-6720, Szeged, ?rp?d t?r 2. H-6720, Szeged, Aradi v?rtan?k tere 1. 
{szarvas, vinczev, rfarkas, csirik}@inf.u-szeged.hu 
 
Abstract 
This article reports on a corpus annotation 
project that has produced a freely available re-
source for research on handling negation and 
uncertainty in biomedical texts (we call this 
corpus the BioScope corpus). The corpus con-
sists of three parts, namely medical free texts, 
biological full papers and biological scientific 
abstracts. The dataset contains annotations at 
the token level for negative and speculative 
keywords and at the sentence level for their 
linguistic scope. The annotation process was 
carried out by two independent linguist anno-
tators and a chief annotator ? also responsible 
for setting up the annotation guidelines ? who 
resolved cases where the annotators disagreed. 
We will report our statistics on corpus size, 
ambiguity levels and the consistency of anno-
tations. 
1 Introduction 
Detecting uncertain and negative assertions is es-
sential in most Text Mining tasks where in general, 
the aim is to derive factual knowledge from textual 
data. This is especially so for many tasks in the 
biomedical (medical and biological) domain, 
where these language forms are used extensively in 
textual documents and are intended to express im-
pressions, hypothesised explanations of experi-
mental results or negative findings. Take, for 
example, the clinical coding of medical reports, 
where the coding of a negative or uncertain disease 
diagnosis may result in an over-coding financial 
penalty. Another example from the biological do-
main is interaction extraction, where the aim is to 
mine text evidence for biological entities with cer-
tain relations between them. Here, while an uncer-
tain relation or the non-existence of a relation 
might be of some interest for an end-user as well, 
such information must not be confused with real 
textual evidence (reliable information). A general 
conclusion is that for text mining, extracted infor-
mation that is within the scope of some negative / 
speculative (hedge or soft negation) keyword 
should either be discarded or presented separately 
from factual information.  
Even though many successful text processing 
systems (Friedman et al, 1994, Chapman et al 
2001, Elkin et al 2005) handle the above-
mentioned phenomena, most of them exploit hand-
crafted rule-based negation/uncertainty detection 
modules. To the best of our knowledge, there are 
no publicly available standard corpora of reason-
able size that are usable for evaluating the auto-
matic detection and scope resolution of these 
language phenomena. The availability of such a 
resource would undoubtedly facilitate the devel-
opment of corpus-based statistical systems for ne-
gation/hedge detection and resolution.  
Our study seeks to fill this gap by presenting the 
BioScope corpus, which consists of medical and 
biological texts annotated for negation, speculation 
and their linguistic scope. This was done to permit 
a comparison between and to facilitate the devel-
opment of systems for negation/hedge detection 
and scope resolution. The corpus described in this 
paper has been made publicly available for re-
search purposes and it is freely downloadable1. 
                                                          
1 www.inf.u-szeged.hu/rgai/bioscope  
38
1.1 Related work 
Chapman et al (2001) created a simple regular 
expression algorithm called NegEx that can detect 
phrases indicating negation and identify medical 
terms falling within the negative scope. With this 
process, a large part of negatives can be identified 
in discharge summaries. 
Mutalik et al (2001) earlier developed 
Negfinder in order to recognise negated patterns in 
medical texts. Their lexer uses regular expressions 
to identify words indicating negation and then it 
passes them as special tokens to the parser, which 
makes use of the single-token look-ahead strategy. 
Thus, without appealing to the syntactic structure 
of the sentence, Negfinder can reliably identify 
negated concepts in medical narrative when they 
are located near the negation markers. 
Huang and Lowe (2007) implemented a hybrid 
approach to automated negation detection. They 
combined regular expression matching with 
grammatical parsing: negations are classified on 
the basis of syntactic categories and they are 
located in parse trees. Their hybrid approach is 
able to identify negated concepts in radiology 
reports even when they are located at some 
distance from the negative term. 
The Medical Language Extraction and Encoding 
(MedLEE) system was developed as a general 
natural language processor in order to encode 
clinical documents in a structured form (Friedman 
et al, 1994). Negated concepts and certainty 
modifiers are also encoded within the system, thus 
it enables them to make a distinction between 
negated/uncertain concepts and factual information 
which is crucial in information retrieval. 
Elkin et al (2005) use a list of negation words 
and a list of negation scope-ending words in order 
to identify negated statements and their scope. 
Although a fair amount of literature on 
uncertainty (or hedging) in scientific texts has been 
produced since the 1990s (e.g. Hyland, 1994), 
speculative language from a Natural Language 
Processing perspective has only been studied in the 
past few years. Previous studies (Light et al, 2004) 
showed that the detection of hedging can be solved 
effectively by looking for specific keywords which 
imply speculative content. 
Another possibility is to treat the problem as a 
classification task and train a statistical  model to 
discriminate speculative and non-speculative 
assertions. This approach requires the availability 
of labeled instances to train the models on. 
Medlock and Briscoe (2007) proposed a weakly 
supervised setting for hedge classification in 
scientific texts where the aim is to minimise human 
supervision needed to obtain an adequate amount 
of training data. Their system focuses on locating 
hedge cues in text and thus they do not determine 
the scopes (in other words in a text they define the 
scope to be a whole sentence). 
1.2 Related resources 
Even though the problems of negation (mainly in 
the medical domain) and hedging (mainly in the 
scientific domain) have received much interest in 
the past few years, open access annotated resources 
for training, testing and comparison are rare and 
relatively small in size. Our corpus is the first one 
with an annotation of negative/speculative 
keywords and their scope. The authors are only 
aware of the following related corpora: 
 
? The Hedge classification corpus (Medlock 
and Briscoe, 2007), which has been 
annotated for hedge cues (at the sentence 
level) and consists of five full biological 
research papers (1537 sentences). No scope 
annotation is given in the original corpus. 
We included this publicly available corpus 
in ours, enriching the data with annotation 
for negation cues and linguistic scope for 
both hedging and negation. 
? The Genia Event corpus (Kim et al, 2008), 
which annotates biological events with 
negation and three levels of uncertainty 
(1000 abstracts). 
? The BioInfer corpus (Pyysalo et al, 2007), 
where biological relations are annotated for 
negation (1100 sentences in size).  
In the two latter corpora biological terms 
(relations and events) have been annotated for both 
negation and hedging, but linguistic cues (i.e. 
which keyword modifies the semantics of the 
statement) have not been annotated. We annotated 
keywords and their linguistic scope, which is very 
useful for machine learning or rule-based negation 
and hedge detection systems. 
39
2 Annotation guidelines 
This section describes the basic principles on the 
annotation of speculative and negative scopes in 
biomedical texts. Some basic definitions and tech-
nical details are given in Section 2.1, then the gen-
eral guidelines are discussed in Section 2.2 and the 
most typical keywords and their scopes are illus-
trated with examples in Section 2.3. Some special 
cases and exceptions are listed in Section 2.4, then 
the annotation process of the corpus is described 
and discussed in Section 2.5. The complete annota-
tion guidelines document is available from the cor-
pus homepage. 
2.1 Basic issues 
In a text, just sentences with some instance of 
speculative or negative language are considered for 
annotation. The annotation is based on linguistic 
principles, i.e. parts of sentences which do not con-
tain any biomedical term are also annotated if they 
assert the non-existence/uncertainty of something.  
As for speculative annotation, if a sentence is a 
statement, that is, it does not include any specula-
tive element that suggests uncertainty, it is disre-
garded. Questions inherently suggest uncertainty ? 
which is why they are asked ?, but they will be 
neglected and not annotated unless they contain 
speculative language. 
Sentences containing any kind of negation are 
examined for negative annotation. Negation is un-
derstood as the implication of the non-existence of 
something. However, the presence of a word with 
negative content does not imply that the sentence 
should be annotated as negative, since there are 
sentences that include grammatically negative 
words but have a speculative meaning or are actu-
ally regular assertions (see the examples below). 
In the corpus, instances of speculative and nega-
tive language ? that is, keywords and their scope ? 
are annotated. Speculative elements are marked by 
angled brackets: <or>, <suggests> etc., while 
negative keywords are marked by square brackets: 
[no], [without] etc. The scope of both negative and 
speculative keywords is denoted by parentheses. 
Also, the speculative or negative cue is always in-
cluded within its scope: 
This result (<suggests> that the valency of Bi in 
the material is smaller than + 3). 
Stable appearance the right kidney ([without] hy-
dronephrosis). 
In the following, the general guidelines for specu-
lative and negative annotation are presented. 
2.2 General guidelines 
During the annotation process, we followed a min-
max strategy for the marking of keywords and their 
scope. When marking the keywords, a minimalist 
strategy was followed: the minimal unit that ex-
pressed hedging or negation was marked as a key-
word. However, there are some cases when hedge 
or negation can be expressed via a phrase rather 
than a single word. Complex keywords are phrases 
that express uncertainty or negation together, but 
they cannot do this on their own (the meaning or 
the semantics of its subcomponents are signifi-
cantly different from the semantics of the whole 
phrase). An instance of a complex keyword can be 
seen in the following sentence: 
Mild bladder wall thickening (<raises the question 
of> cystitis). 
On the other hand, a sequence of words cannot be 
marked as a complex keyword if it is only one of 
those words that express speculative or negative 
content (even without the other word). Thus prepo-
sitions, determiners, adverbs and so on are not an-
notated as parts of the complex keyword if the 
keyword can have a speculative or negative con-
tent on its own: 
The picture most (<likely> reflects airways dis-
ease). 
Complex keywords are not to be confused with the 
sequence of two or more keywords because they 
can express hedge or negation on their own, that is, 
without the other keyword as well. In this case, 
each keyword is annotated separately, as is shown 
in the following example: 
Slightly increased perihilar lung markings (<may> 
(<indicate> early reactive airways disease)). 
2.3 Scope marking 
When marking the scopes of negative and specula-
tive keywords, we extended the scope to the big-
gest syntactic unit possible (in contrast to other 
corpora like the one described in (Mutalik et al, 
2001)). Thus, annotated scopes always have the 
40
maximal length ? as opposed to the strategy for 
annotating keywords, where we marked the mini-
mal unit possible. Our decision was supported by 
two facts. First, since scopes must contain their 
keywords, it seemed better to include every ele-
ment in between the keyword and the target word 
in order to avoid ?empty? scopes, that is, scopes 
without a keyword. In the next example, however 
is not affected by the hedge cue but it should be 
included within the scope, otherwise the keyword 
and its target phrase would be separated: 
(Atelectasis in the right mid zone is, however, 
<possible>). 
Second, the status of modifiers is occasionally 
vague: it is sometimes not clear whether the modi-
fier of the target word belongs to its scope as well. 
The following sentence can describe two different 
situations: 
There is [no] primary impairment of glucocorti-
coid metabolism in the asthmatics. 
First, the glucocorticoid metabolism is impaired in 
the asthmatics but not primarily, that is, the scope 
of no extends to primary. Second, the scope of no 
extends to impairment (and its modifiers and com-
plements as well), thus there is no impairment of 
the glucocorticoid metabolism at all. Another ex-
ample is shown here: 
Mild viral <or> reactive airways disease is de-
tected. 
The syntactic structure of the above sentence is 
ambiguous. First, the airways disease is surely 
mild, but it is not known whether it is viral or reac-
tive; or second, the airways disease is either mild 
and viral or reactive and not mild. Most of the sen-
tences with similar problems cannot be disambigu-
ated on the basis of contextual information, hence 
the proper treatment of such sentences remains 
problematic. However, we chose to mark the wid-
est scope available: in other words, we preferred to 
include every possible element within the scope 
rather than exclude elements that should probably 
be included. 
 The scope of a keyword can be determined on 
the basis of syntax. The scope of verbs, auxiliaries, 
adjectives and adverbs usually extends to the right 
of the keyword. In the case of verbal elements, i.e. 
verbs and auxiliaries, it ends at the end of the 
clause (if the verbal element is within a relative 
clause or a coordinated clause) or the sentence, 
hence all complements and adjuncts are included, 
in accordance with the principle of maximal scope 
size. Take the following examples: 
The presence of urothelial thickening and mild 
dilatation of the left ureter (<suggest> that the 
patient may have continued vesicoureteral reflux). 
These findings that (<may> be from an acute 
pneumonia) include minimal bronchiectasis as 
well. 
These findings (<might> be chronic) and (<may> 
represent reactive airways disease). 
The scope of attributive adjectives generally ex-
tends to the following noun phrase, whereas the 
scope of predicative adjectives includes the whole 
sentence. For example, in the following two state-
ments: 
This is a 3 month old patient who had (<possible> 
pyelonephritis) with elevated fever. 
(The demonstration of hormone receptor proteins 
in cells from malignant effusions is <possible>). 
Sentential adverbs have a scope over the entire 
sentence, while the scope of other adverbs usually 
ends at the end of the clause or sentence. For in-
stance, 
(The chimaeric oncoprotein <probably> affects 
cell survival rather than cell growth). 
Right upper lobe volume loss and (<probably> 
pneumonia). 
The scope of conjunctions extends to all members 
of the coordination. That is, it usually extends to 
the both left and right: 
Symptoms may include (fever, cough <or> itches). 
Complex keywords such as either ? or have one 
scope: 
Mild perihilar bronchial wall thickening may rep-
resent (<either> viral infection <or> reactive 
airways disease). 
Prepositions have a scope over the following 
(noun) phrase: 
Mildly hyperinflated lungs ([without] focal opac-
ity). 
41
When the subject of the sentence contains the 
negative determiners no or neither, its scope ex-
tends to the entire sentence: 
Surprisingly, however, ([neither] of these proteins 
bound in vitro to EBS1 or EBS2). 
The main exception that changes the original scope 
of the keyword is the passive voice. The subject of 
the passive sentence was originally the object of 
the verb, that is, it should be within its scope. This 
is why the subject must also be marked within the 
scope of the verb or auxiliary. For instance, 
(A small amount of adenopathy <cannot be> com-
pletely <excluded>). 
Another example of scope change is the case of 
raising verbs (seem, appear, be expected, be likely 
etc.). These can have two different syntactic pat-
terns, as the following examples suggest:  
It seems that the treatment is successful. 
The treatment seems to be successful. 
In the first case, the scope of seems starts right 
with the verb. If this was the case in the second 
pattern, the treatment would not be included in the 
scope, but it should be like that shown in the first 
pattern. Hence in the second sentence, the scope 
must be extended to the subject as well: 
It (<seems> that the treatment is successful). 
(The treatment <seems> to be successful). 
Sometimes a negative keyword is present in the 
text apparently without a scope: negative obviously 
expresses negation, but the negated fact ? what 
medical problem the radiograph is negative for ? is 
not part of the sentence. In such cases, the keyword 
is marked and the scope contains just the keyword: 
([Negative]) chest radiograph. 
In the case of elliptic sentences, the same strategy 
is followed: the keyword is marked and its scope 
includes only the keyword since the verbal phrase, 
that is, the scope of not, is not repeated in the sen-
tence. 
This decrease was seen in patients who responded 
to the therapy as well as in those who did ([not]). 
Generally, punctuation marks or conjunctions 
function as scope boundary markers in the corpus, 
in contrast to the corpus described in (Mutalik et 
al., 2001) where certain lexical items are treated as 
negation-termination tokens. Since in our corpus 
the scope of negation or speculation is mostly ex-
tended to the entire clause in the case of verbal 
elements, it is clear that markers of a sentence or 
clause boundary determine the end of their scope. 
2.4 Special cases 
It seems unequivocal that whenever there is a 
speculative or negative cue in the sentence, the 
sentence expresses hedge or negation. However, 
we have come across several cases where the pres-
ence of a speculative/negative keyword does not 
imply a hedge/negation. That is, some of the cues 
do not denote speculation or negation in all their 
occurrences, in other words, they are ambiguous. 
For instance, the following sentence is a state-
ment and it is the degree of probability that is pre-
cisely determined, but it is not an instance of 
hedging although it contains the cue probable: 
The planar amide groups in which is still digging 
nylon splay around 30 less probable event. 
As for negative cues, sentences including a nega-
tive keyword are not necessarily to be annotated 
for negation. They can, however, have a specula-
tive content as well. The following sentence con-
tains cannot, which is a negative keyword on its 
own, but not in this case: 
(A small amount of adenopathy <cannot be> com-
pletely <excluded>). 
Some other sentences containing a negative key-
word are not to be annotated either for speculation 
or for negation. In the following example, the 
negative keyword is accompanied by an adverb 
and their meaning is neither speculative nor nega-
tive. The sequence of the negative keyword and the 
adverb can be easily substituted by another adverb 
or adjective having the same (or a similar) mean-
ing, which is by no means negative ? as shown in 
the example. In this way, the sentence below can 
be viewed as a positive assertion (not a statement 
of the non-existence of something). 
Thus, signaling in NK3.3 cells is not always 
(=sometimes) identical with that in primary NK 
cells. 
As can be seen from the above examples, hedging 
or negation is determined not just by the presence 
42
of an apparent cue: it is rather an issue of the key-
word, the context and the syntactic structure of the 
sentence taken together. 
2.5 Annotation process 
Our BioScope corpus was annotated by two inde-
pendent linguists following the guidelines written 
by our linguist expert before the annotation of the 
corpus was initiated. These guidelines were devel-
oped throughout the annotation process as annota-
tors were often confronted with problematic issues. 
The annotators were not allowed to communicate 
with each other as far as the annotation process 
was concerned, but they could turn to the expert 
when needed and regular meetings were also held 
between the annotators and the linguist expert in 
order to discuss recurring and/or frequent problem-
atic issues. When the two annotations for one sub-
corpus were finalised, differences between the two 
were resolved by the linguist expert, yielding the 
gold standard labeling of the subcorpus. 
3 Corpus details 
In this section we will discuss in detail the overall 
characteristics of the corpus we developed, includ-
ing a brief description of the texts that constitute 
the BioScope corpus and some general statistics 
concerning the size of each part, distribution of 
negation/hedge cues, ambiguity levels and finally 
we will present statistics on the final results of the 
annotation work. 
3.1 Corpus texts 
The corpus consists of texts taken from 4 different 
sources and 3 different types in order to ensure that 
it captures the heterogenity of language use in the 
biomedical domain. We decided to add clinical 
free-texts (radiology reports), biological full papers 
and biological paper abstracts (texts from Genia). 
Table 1 summarises the chief characteristics of 
the three subcorpora. The 3rd and 5th rows of the 
table show the ratio of sentences which contain 
negated or uncertain statements. The 4rd and 6th 
rows show the number of negation and hedge cue 
occurrences in the given corpus.  
A major part of the corpus consists of clinical 
free-texts. We chose to add medical texts to the 
corpus in order to facilitate research on nega-
tion/hedge detection in the clinical domain. The 
radiology report corpus that was used for the clini-
cal coding challenge (Pestian et al, 2007) organ-
ised by the Computational Medicine Center in 
Cincinatti, Ohio in 2007 was annotated for nega-
tions and uncertainty along with the scopes of each 
phenomenon. This part contains 1954 documents, 
each having a clinical history and an impression 
part, the latter being denser in negated and specula-
tive parts. 
Another part of the corpus consists of full sci-
entific articles. 5 articles from FlyBase (the same 
data were used by Medlock and Briscoe (2007) for 
evaluating sentence-level hedge classifiers) and 4 
articles from the open access BMC Bioinformatics 
website were downloaded and annotated for nega-
tions, uncertainty and their scopes. Full papers are 
particularly useful for evaluating negation/hedge 
classifiers as different parts of an article display 
different properties in the use of speculative or ne-
gated phrases. Take, for instance, the Conclusions 
section of scientific papers that tends to contain 
significantly more uncertain or negative findings 
than the description of Experimental settings and 
methods. 
Scientific abstracts are the main targets for 
various Text Mining applications like protein-
protein interaction mining due to their public ac-
cessibility (e.g. through PubMed). We therefore 
decided to include quite a lot of texts from the ab-
stracts of scientific papers. This is why we in-
cluded the abstracts of the Genia corpus (Collier et 
al., 1999). This decision was straightforward for 
two reasons. First, the Genia corpus contains syn-
tax tree annotation, which allows a comparison 
between scope annotation and syntactic structure. 
Being syntactic in nature, scopes should align with 
the bracket structure of syntax trees, while scope 
resolution algorithms that exploit treebank data can 
be used as a theoretical upper bound for the 
evaluation of parsers for resolving negative/hedge 
scopes. The other reason was that scope annotation 
can mutually benefit from the rich annotations of 
the Genia corpus, such as term annotation (evalua-
tion) and event annotation (comparison with the 
biologist uncertainty labeling of events). 
The corpus consists of more than 20.000 anno-
tated sentences altogether. We consider this size to 
be sufficiently large to serve as a standard evalua-
tion corpus for negation/hedge detection in the 
biomedical domain. 
 
43
 Clinical Full Paper Abstract 
#Documents 1954 9 1273 
#Sentences 6383 2624 11872 
Negation  
sentences 6.6% 13.76% 13.45% 
#Negation cues 871 404 1757 
Hedge sentences 13.4% 22.29% 17.69% 
#Hedge cues 1137 783 2691 
Table 1: Statistics of the three subcorpora. 
3.2 Agreement analysis 
We measured the consistency level of the annota-
tion using inter-annotator agreement analysis. The 
inter-annotator agreement rate is defined as the 
F?=1 measure of one annotation, treating the second 
one as the gold standard. We calculated agreement 
rates for all three subcorpora between the two in-
dependent annotators and between the two annota-
tors and the gold standard labeling. The gold 
standard labeling was prepared by the creator of 
the annotation guide, who resolved all cases where 
the two annotators disagreed on a keyword or its 
scope annotation. 
We measured the agreement rate of annotating 
negative and hedge keywords, and the agreement 
rate of annotating the linguistic scope for each 
phenomenon. We distinguished left-scope, right-
scope and full scope agreement that required both 
left and right scope boundaries to match exactly to 
be considered as coinciding annotations. A detailed 
analysis of the consistency levels for the three sub-
corpora and the ambiguity levels for each negative 
and hedge keyword (that is, the ratio of a keyword 
being annotated as a negative/speculative cue and 
the number of all the occurrences of the same 
keyword in the corpus) can be found at the corpus 
homepage. 
 
3.3 BioScope corpus availability 
The corpus is available free of charge for research 
purposes and can be obtained for a modest price 
for business use. For more details, see the Bio-
Scope homepage: 
www.inf.u-szeged.hu/rgai/bioscope. 
4 Conclusions 
In this paper we reported on the construction of a 
corpus annotated for negations, speculations and 
their linguistic scopes. The corpus is accessible for 
academic purposes and is free of charge. Apart 
from the intended goal of serving as a common 
resource for the training, testing and comparison of 
biomedical Natural Language Processing systems, 
the corpus is also a good resource for the linguistic 
analysis of scientific and clinical texts. 
The most obvious conclusions here are that the 
usual language of clinical documents makes it 
much easier to detect negation and uncertainty 
cues than in scientific texts because of the very 
high ratio of the actual cue words (i.e. low ambigu-
ity level), which explains the high accuracy scores 
reported in the literature. In scientific texts ? which 
are nowadays becoming a popular target for Text 
Mining (for literature-based knowledge discovery) 
? the detection and scope resolution of negation 
and uncertainty is, on the other hand, a problem of 
great complexity, with the percentage of non-
hedge occurrences being as high as 90% for some 
hedge cue candidates in biological paper abstracts. 
Take for example the keyword or which is labeled 
as a speculative keyword in only 11.32% of the 
cases in scientific abstracts, while it was labeled as 
speculative in 97.86% of the cases in clinical texts. 
Identifying the scope is also more difficult in sci-
entific texts where the average sentence length is 
much longer than in clinical data, and the style of 
the texts is also more literary in the former case. 
In our study we found that hedge detection is a 
more difficult problem than identifying negations 
because the number of possible cue words is higher 
and the ratio of real cues is significantly lower in 
the case of speculation (higher keyword/non-
keyword ambiguity). The annotator-agreement ta-
ble also confirms this opinion: the detection of 
hedging is more complicated than negation even 
for humans. 
Our corpus statistics also prove the importance 
of negation and hedge detection. The ratio of ne-
gated and hedge sentences in the corpus varies in 
the subcorpora, but we can say that over 20% of 
the sentences contains a modifier that radically 
influences the semantic content of the sentence. 
One of the chief construction principles of the 
BioScope corpus was to facilitate the train-
ing/development of automatic negation and hedge 
detection systems. Such systems have to solve two 
sub-problems: they have to identify real cue words 
(note that the probability of any word being a key-
word can be different for various domains) and 
44
then they have to determine the linguistic scope of 
actual keywords. 
These automatic hedge and negation detection 
methods can be utilised in a variety of ways in a 
(biomedical) Text Mining system. They can be 
used as a preprocessing tool, i.e. each word in a 
detected scope can be removed from the docu-
ments if we seek to extract true assertions. This can 
significantly reduce the level of noise for process-
ing in such cases where only a document-level la-
beling is provided (like that for the ICD-9 coding 
dataset) and just clear textual evidence for certain 
things should be extracted. On the other hand, 
similar systems can classify previously extracted 
statements according to their certainty or uncer-
tainty, which is generally an important issue in the 
automatic processing of scientific texts. 
Acknowledgments 
This work was supported in part by the NKTH 
grant of the Jedlik ?nyos R&D Programme 2007 
(project codename TUDORKA7) of the Hungarian 
government. The authors wish to thank the anony-
mous reviewers for their useful suggestions and 
comments. The authors also wish to thank the crea-
tors of the ICD-9 coding dataset and the Genia 
corpus for making the texts that were used here 
publicly available. The authors thank Jin-Dong 
Kim as well for the useful comments and sugges-
tions on the annotation guide and Orsolya Vincze 
and Mih?ly Mink? (the two annotators) for their 
work. 
References  
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper and Bruce G. Buchanan. 2001. A 
Simple Algorithm for Identifying Negated Findings 
and Diseases in Discharge Summaries. Journal of 
Biomedical Informatics, 34(5):301?310. 
N. Collier, H. S. Park, N. Ogata, Y. Tateishi, C. Nobata, 
T. Ohta, T. Sekimizu, H. Imai, K. Ibushi, and J. Tsu-
jii. 1999. The GENIA project: corpus-based knowl-
edge acquisition and information extraction from 
genome research papers. Proceedings of EACL-99. 
Peter L. Elkin, Steven H. Brown, Brent A. Bauer, Casey 
S. Husser, William Carruth, Larry R. Bergstrom and 
Dietlind L. Wahner-Roedler. 2005. A controlled trial 
of automated classification of negation from clinical 
notes. BMC Medical Informatics and Decision Mak-
ing 5:13 doi:10.1186/1472-6947-5-13. 
C. Friedman, P.O. Alderson, J.H. Austin, J.J. Cimino, 
and S.B. Johnson. 1994. A general natural-language 
text processor for clinical radiology. Journal of the 
American Medical Informatics Association, 
1(2):161?174. 
Yang Huang and Henry J. Lowe. 2007. A Novel Hybrid 
Approach to Automated Negation Detection in Clini-
cal Radiology Reports. Journal of the American 
Medical Informatics Association, 14(3):304?311. 
Ken Hyland. 1994. Hedging in academic writing and 
EAP textbooks. English for Specific Purposes, 
13(3):239?256. 
Jin-Dong Kim, Tomoko Ohta, and Jun'ichi Tsujii. 2008. 
Corpus annotation for mining biomedical events 
from literature. BMC Bioinformatics 2008, 9:10. 
Marc Light, Xin Ting Qui, and Padmini Srinivasan. 
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings of 
BioLink 2004 Workshop on Linking Biological Lit-
erature, Ontologies and Databases: Tools for Users. 
Boston, Massachusetts, Association for Computa-
tional Linguistics, 17?24. 
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific 
literature. In Proceedings of the 45th Annual Meeting 
of the Association of Computational Linguistics. Pra-
gue, Association for Computational Linguistics, 992?
999. 
Pradeep G. Mutalik, Aniruddha Deshpande, and 
Prakash M. Nadkarni. 2001. Use of General-purpose 
Negation Detection to Augment Concept Indexing of 
Medical Documents: A Quantitative Study Using the 
UMLS. Journal of the American Medical Informatics 
Association, 8(6):598?609. 
John P. Pestian, Chris Brew, Pawel Matykiewicz, DJ 
Hovermale, Neil Johnson, K. Bretonnel Cohen, and 
Wlodzislaw Duch. 2007. A shared task involving 
multi-label classification of clinical free text. In Bio-
logical, translational, and clinical language process-
ing. Prague, Association for Computational 
Linguistics, 97?104. 
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari 
Bj?rne, Jorma Boberg, Jouni J?rvinen, and Tapio 
Salakoski. 2007. BioInfer: a corpus for information 
extraction in the biomedical domain. BMC Bioinfor-
matics 2007, 8:50 doi:10.1186/1471-2105-8-50. 
45
Proceedings of the Workshop on BioNLP: Shared Task, pages 137?140,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring ways beyond the simple supervised learning approach for
biological event extraction
Gyo?rgy Mo?ra1, Richa?rd Farkas1, Gyo?rgy Szarvas2?, Zsolt Molna?r3
gymora@gmail.com, rfarkas@inf.u-szeged.hu,
szarvas@tk.informatik.tu-darmstadt.de, zsolt@acheuron.hu
1 Hungarian Academy of Sciences, Research Group on Artificial Intelligence
Aradi ve?rtanuk tere 1., H-6720 Szeged, Hungary
2 Ubiquitous Knowledge Processing Lab, Technische Universita?t Darmstadt
Hochschulstra?e 10., D-64289 Darmstadt, Germany
3 Acheuron Hungary Ltd., Chemo-, and Bioinformatics group,
Tiszavira?g u. 11., H-6726 Szeged, Hungary
Abstract
Our paper presents the comparison of a
machine-learnt and a manually constructed
expert-rule-based biological event extraction
system and some preliminary experiments to
apply a negation and speculation detection
system to further classify the extracted events.
We report results on the BioNLP?09 Shared
Task on Event Extraction evaluation datasets,
and also on an external dataset for negation
and speculation detection.
1 Introduction
When we consider the sizes of publicly available
biomedical scientific literature databases for re-
searchers, valuable biological knowledge is acces-
sible today in enormous amounts. The efficient pro-
cessing of these large text collections is becoming
an increasingly important issue in Natural Language
Processing. For a survey on techniques used in bio-
logical Information Extraction, see (Tuncbag et al,
2009).
The BioNLP?09 Shared Task (Kim et al, 2009)
involved the recognition of bio-molecular events in
scientific abstracts. In this paper we describe our
systems submitted to the event detection and charac-
terization (Task1) and the recognition of negations
and speculations (Task3) subtasks. Our experiments
can be regarded as case studies on i) how to define
a framework for a hybrid human-machine biological
information extraction system, ii) how the linguis-
tic scopes of negation/speculation keywords relate
to biological event annotations.
?On leave from RGAI of Hungarian Acad. Sci.
2 Event detection
We formulated the event extraction task as a classifi-
cation problem for each event-trigger-word/protein
pair. A domain expert collected 140 keywords
which he found meaningful and reliable by manual
inspection of the corpus. This set of high-precision
keywords covered 69.8% of the event annotations in
the training data.
We analysed each occurrence of these keywords
in two different approaches. We used C4.5 deci-
sion tree classifier to predict one of the event types
considered in the shared task or the keyword/protein
pair being unrelated; and we also developed a hand-
crafted expert system with a biological expert. We
observed that the two systems extract markedly dif-
ferent sets of true positive events. Our final submis-
sion was thus the union of the events extracted by
the expert-rule-based and the statistical systems (we
call this hybrid system later on).
2.1 The statistical event classifier
The preprocessing of the data was performed us-
ing the UltraCompare (Kano et al, 2008) repository
provided by the organizers of the challenge: Genia
sentence splitter, Genia tagger for POS coding and
NER.
The statistical system classified each key-
word/protein pair into 9 event and 2 non-event
classes. A pair was either labeled according to
the predicted event type (the keyword as an event
trigger and the protein name as the theme of the
event), non-event (keyword not an event trigger)
or wrong-protein (the theme of the event is a
different protein). We chose to use two non-event
137
classes to make the decision tree more human read-
able (the negative cases being separated). This made
the comparison of the statistical model and the rule-
based system easier.
The features we used were the following: 1) the
words and POS codes in a window (? 3 tokens)
around the keyword, preserving position informa-
tion relative to the keyword; 2) the distances be-
tween the keyword and the two nearest annotated
proteins (left and right) and the theme candidate as
numeric features1. The protein annotations were re-
placed by the term $protein, Genia tagger anno-
tations by $genia-protein (mainly complexes),
to enable the classifier to learn the difference be-
tween events involved in the shared task, and events
out of the scope of the task. Events with protein
complexes and families often had the same linguistic
structure as events with annotated proteins. As com-
plexes did not form events in the shared task, they
sometimes misled our local-context-based classifier.
For example ?the binding of ISGF3? was not anno-
tated as an event because the theme is not a ?protein?
(as defined by the shared task guidelines), while ?the
binding of TRAF2? was (TRAF2 being a protein,
and not a complex as in the former example).
We trained a C4.5 decision tree classifier using
Weka (Witten and Frank, 2005). The human read-
able models and fast training time motivated our
selection of a learning algorithm which allowed a
straightforward comparison with the expert system.
2.2 Expert-rule-based system
The expert system was constructed by a biologist
who had over 4 years of experience in similar tasks.
The main idea was to define rules ? which have a
very high precision ? in order to compare them with
the learnt decision trees and to increase the cover-
age of the final system by adding these annotations
to the output of the statistical system. We only man-
aged to prepare expert rules for the Phosphorylation
and Gene expression classes due to time constraints
(a total of 46 patterns). The expert was asked to
construct high-precision rules (they were tested on
the train set to keep the false positive rate near zero)
in order to gain insight into the structure of reliable
1More information on the features
and parameters used can be found at
www.inf.u-szeged.hu/rgai/BioEventExtraction
rules.
Here each rule is bound to a specific keyword. Ev-
ery rule is a sequence of ?word patterns? (with or
without a suffix). A word pattern can match a pro-
tein, an arbitrary word, an exact word or the key-
word. Every pattern can have a Regular Expression
style suffix:
Table 1: Word pattern types and suffixes
<keyword> matching the keyword of the event
"word" matching regular words
matching any token
$protein matching any annotated protein
? zero or one of the word pattern
* zero or more of the word pattern
+ one or more of the word pattern
{a,b} definite number of word patterns
For example the ?<expression> ? "of"
? $protein? pattern recognizes an event with
the keyword expression, followed by an arbitrary
word and then the word of, or immediately by of and
then a protein (or immediately by the protein name).
An obvious drawback of this system is that nega-
tion is not allowed, so the expert was unable to de-
fine a word pattern like !"of" to match any to-
ken besides of. This extension would have been a
straightforward way of improving the system.
2.3 Experimental results
We expected the recall of the hybrid system to be
near the sum of the recalls of the individual systems,
meaning that they had recognized different events,
as the pattern matching was mainly based on the
order of the tokens, while the statistical classifier
learned position-oriented contextual clues. Thanks
to the high precision of the rule-based system, the
overall precision also increased. The two event
classes which were included in the expert system
had a significantly better precision score. The cov-
erage of the Phosphorylation class was lower than
that for the Gene expression class because its pat-
terns were still incomplete2.
2A discussion on comparing the contribution of the
two approaches and individual rules can be found at
www.inf.u-szeged.hu/rgai/BioEventExtraction
138
Table 2: Results of rule based-system compared to the
statistical and combined systems (R/P/fscore)
All Event Gene exp. Phosph.
stat. 16 / 31 / 21 36 / 41 / 38 73 / 37 / 49
rule 5 / 80 / 10 20 / 85 / 33 17 / 58 / 26
hybrid 22 / 37 / 27 56 / 51 / 54 81 / 40 / 53
3 Recognition of negations and
speculations
For negation and speculation detection, we applied
a model trained on a different dataset (Vincze et al,
2008) of scientific abstracts, which had been spe-
cially annotated for negative and uncertain keywords
and their linguistic scope. Due to time constraints
we used our model to produce annotations for Task3
without any sort of fine tuning to the shared task gold
standard annotations.
The only exception here was a subclass of specu-
lative annotations that were not triggered by a word
used to express uncertainty, but were judged to be
speculative because the sentence itself reported on
some experiments performed, the focus of the in-
vestigations described in the article, etc. That is,
it was not the meaning of the text that was uncer-
tain, but ? as saying that something has been exam-
ined does not mean it actually exists ? the sentence
implicitly contained uncertain information. Since
such sentences were not covered by our corpus, for
these cases we collected the most reliable text cues
from the shared task training data and applied a
dictionary-lookup-based approach. We did this so
as to get a comprehensive model for the Genia nega-
tion and speculation task.
As for the explicit uncertain and negative state-
ments, we applied a more sophisticated approach
that exploited the annotations of the BioScope cor-
pus (Vincze et al, 2008). For each frequent and am-
biguous keyword found in the approximately 1200
abstracts annotated in BioScope, we trained a sepa-
rate classifier to discriminate keyword/non-keyword
uses of each term, using local contextual patterns
(neighbouring lemmas, their POS codes, etc.) as
features. In others words, for the most common
uncertain and negative keywords, we attempted a
context-based disambiguation, instead of a simple
keyword lookup. Having the keywords, we pre-
dicted their scope using simple heuristics (?to the
end of the sentence?, ?to the next punctation mark
in both directions?, etc.). In the shared task we ex-
amined each extracted event and they were said to
be negated or hedged when some of their arguments
(trigger word, theme or clause) were within a lin-
guistic scope.
3.1 Experimental results
First we evaluated our negation and speculation
keyword/non-keyword classification models on the
BioScope corpus by 5-fold cross-validation. We
trained models for 15 negation and 41 speculative
keywords. We considered different word forms of
the same lemma to be different keywords because
they may be used in a different meaning/context.
For instance, different keyword/non-keyword deci-
sion rules must be used for appear, appears and ap-
peared. We trained a C4.5 decision tree using word
uni- and bigram features and POS codes to discrim-
inate keyword/non-keyword uses and compared the
results with the most frequent class (MFC) baseline.
Overall, our context-based classification method
outperformed the baseline algorithm by 3.7% (giv-
ing an error reduction of 46%) and 3.1% (giving an
error reduction of 27%) on the negation and specula-
tion keywords, respectively. The learnt models were
typically very small decision trees i.e. they repre-
sented very simple rules indicating collocations (like
?hypothesis is a keyword if and only if followed by
that, etc.). More complex rules (e.g. ?clear is a key-
word if and only if not is in ?3 environment?) were
learnt just in a few cases.
Our second set of experiments focused on Task3
of the shared task (Kim et al, 2009). As the offi-
cial evaluation process of Task3 was built upon the
detected events of Task1, it did not provide any use-
ful feedback about our negation and speculation de-
tection approach. Thus instead of our Task1 out-
put, we evaluated our model on the gold standard
Task1 annotation of the training and the develop-
ment datasets. The statistical parts of the system
were learnt on the BioScope corpus, thus the train
set was kept blind as well. Table 3 summarises the
results obtained by the explicit negation, speculation
and by the full speculation (both explicit and implicit
keywords) detection methods.
Analysing the errors of the system, we found that
139
Table 3: Negation and speculation detection results
Train (R/P/F) Dev. (R/P/F)
negation 46.9 / 61.3 / 52.8 42.8 / 57.9 / 49.2
exp. spec. 15.4 / 39.5 / 23.6 15.4 / 32.6 / 20.1
full spec. 25.5 / 71.1 / 37.5 27.9 / 65.3 / 39.1
most of the false positives came from the different
approaches of the BioScope and the Genia annota-
tions (see below for a detailed discussion). Most of
the false negative predictions were a consequence of
the incompleteness of our keyword list.
3.2 Discussion
We applied this negation and speculation detection
model more as a case study to assess the usability
of the BioScope corpus. This means that we did not
fine-tune the system to the Genia annotations. Our
experiments revealed some fundamental and inter-
esting differences between the Genia-interpretation
of negation and speculation, and the corpus used by
us. The chief difference is that the BioScope corpus
was constructed following more linguistic-oriented
principles than the Genia negation and speculation
annotation did, which sought to extract biological
information. These differences taken together ex-
plain the relatively poor results we got for the shared
task.
There are significant differences in the interpreta-
tion of both at the keyword level (i.e. what triggers
negation/uncertainty and what does not) and in the
definition of the scope of keywords. For example,
in a sentence like ?have NO effect on the inducibil-
ity of the IL-2 promoter?, Genia annotation just con-
siders the effect to be negated. This means that the
inducibility of IL-2 is regarded as an assertive event
here. In BioScope, the complements of effect are
also placed within the scope of no, thus it would also
be annotated as a negative one. We argue here that
the above example is not a regular sentence to ex-
press the fact: IL-2 is inducible. We rather think
that if the paper has some result (evidence) regard-
ing this event, it should be stated elsewhere in the
text, and we should not retrieve this information as a
fact just based on the above sentence. Thus we argue
that more sophisticated guidelines are needed for the
consistent annotation and efficient handling of nega-
tion and uncertainty in biomedical text mining.
4 Conclusions
We described preliminary experiments on two dif-
ferent approaches which take us beyond the ?take-
goldstandard-data, extract-some-features, train-a-
classifier? approach for biomedical event extraction
from scientific texts (incorporating rule-based sys-
tems and linguistic negation/uncertainty detection).
The systems introduced here participated in the Ge-
nia Event annotation shared task. They achieved rel-
atively poor results on this dataset, mainly due to
1) the special annotation guidelines of the shared
task (like disregarding events with protein complex
or family arguments, and treating subevents as as-
sertive information) and 2) the limited resources we
had to allocate for the task during the challenge
timeline. We consider that the lessons learnt here
are still useful and we also plan to improve our sys-
tem in the near future.
5 Acknowledgements
The authors would like to thank the organizers of
the shared task for their efforts. This work was sup-
ported in part by the NKTH grant of the Hungarian
government (codename BELAMI).
References
Y. Kano, N. Nguyen, R. Saetre, K. Yoshida, Y. Miyao,
Y. Tsuruoka, Y. Matsubayashi, S. Ananiadou, and
J. Tsujii. 2008. Filling the gaps between tools and
users: a tool comparator, using protein-protein inter-
action as an example. Pac Symp Biocomput.
J-D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of bionlp?09 shared task on event
extraction. In Proceedings of Natural Language Pro-
cessing in Biomedicine (BioNLP) NAACL 2009 Work-
shop. To appear.
N. Tuncbag, G. Kar, O. Keskin, A. Gursoy, and R. Nussi-
nov. 2009. A survey of available tools and web servers
for analysis of protein-protein interactions and inter-
faces. Briefings in Bioinformatics.
V. Vincze, Gy. Szarvas, R. Farkas, Gy. Mo?ra, and
J. Csirik. 2008. The bioscope corpus: biomedi-
cal texts annotated for uncertainty, negation and their
scopes. BMC Bioinformatics, 9(Suppl 11):S9.
I. H. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann.
140
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 1?9,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Researcher affiliation extraction from homepages
Istva?n Nagy1, Richa?rd Farkas1,2, Ma?rk Jelasity2
Nagy.Istvan@gmail.com, {rfarkas,jelasity}@inf.u-szeged.hu
1 University of Szeged, Department of Informatics
A?rpad te?r 2., H-6720 Szeged, Hungary
2 Hungarian Academy of Sciences, Research Group on Artificial Intelligence
Aradi ve?rtanuk tere 1., H-6720 Szeged, Hungary
Abstract
Our paper discusses the potential use of
Web Content Mining techniques for gath-
ering scientific social information from the
homepages of researchers. We will intro-
duce our system which seeks [affiliation,
position, start year, end year] information
tuples on these homepages along with pre-
liminary experimental results. We believe
that the lessons learnt from these experi-
ments may be useful for further scientific
social web mining.
1 Introduction
Scientific social network analysis (Yang et al,
2009; Said et al, 2008) seeks to discover global
patterns in the network of researchers working in
a particular field. Common approaches uses bibli-
ographic/scholarly data as the basis for this anal-
ysis. In this paper, we will discuss the poten-
tial of exploiting other resources as an informa-
tion source, such as the homepages of researchers.
The homepage of a researcher contains several
useful pieces of scientific social information like
the name of their supervisor, affiliations, academic
ranking and so on.
The information on homepages may be present
in a structured or natural text form. Here we
shall focus on the detection and analysis of full
text regions of the homepages as they may con-
tain a huge amount of information while requires
more sophisticated analysis than that for struc-
tured ones. We will show that this kind of Web-
based Relation Extraction requires different tech-
niques than the state-of-the-art seed-based ap-
proaches as it has to acquire information from the
long-tail of the World Wide Web.
As a case study, we chose one particular sci-
entific social information type and sought to ex-
tract information tuples concerning the previous
and current affiliations of the researcher in ques-
tion. We defined ?affiliation? as the current and
previous physical workplaces and higher edu-
cational institutes of the researcher in question.
Our aim is to use this kind of information to
discover collegial relationships and workplace-
changing behaviour which may be complementary
to the items of information originating from bibli-
ographic databases.
Based on a manually annotated corpus we car-
ried out several information extraction experi-
ments. The architecture of the complex system
and the recognised problems will be discussed in
Section 3, while our empirical results will be pre-
sented in Section 4. In the last two sections we
will briefly discuss our results and then draw our
main conclusions.
2 Related work
The relationship to previous studies will be dis-
cussed from a scientific social network analysis as
an application point of view and from a Web Con-
tent Mining point of view as well.
2.1 Researcher affiliation extraction
Scientific social network analysis has become a
growing area in recent years ((Yang et al, 2009;
Robardet and Fleury, 2009; Said et al, 2008)
just to name a few in recent studies). Its goal is
to provide a deeper insight into a research field
or into the personal connections among fields by
analysing relationships among researchers. The
existing studies use the co-authorship (e.g. (New-
man, 2001; Baraba?si et al, 2002)) or/and the cita-
tion (Goodrum et al, 2001; Teufel et al, 2006) in-
formation ? generally by constructing a graph with
nodes representing researchers ? as the basis for
their investigations.
Apart from publication-related relationships
? which are presented in structured scholarly
datasets ?, useful scientific social information can
1
be gathered from theWWW. Take, for instance the
homepage of a researchers where they summarise
their topic of interest, list supervisors and students,
nationality, age, memberships and so on. Our goal
is to develop an automatic Web Content Mining
systemwhich crawls the homepages of researchers
and extracts useful social information from them.
A case study will be outlined here, where the
previous and current affiliations of the researcher
in question were gathered automatically. Having
a list of normalised affiliations for each researcher
of a field (i) we ought to be able to discover col-
legial relationships (whether they worked with the
same group at the same time) which may differ
from the co-authorship relation and (ii) we hope
to be able to answer questions like ?Do American
or European researchers change their workplace
more often??.
2.2 Information extraction from homepages
From a technology point of view our procedure
is a Web Content Mining tool, but it differs from
the popular techniques used nowadays. The aim
of Web Content Mining (Liu and Chen-Chuan-
Chang, 2004) is to extract useful information from
the natural language-written parts of websites.
The first attempts on Web Content Mining be-
gan with the Internet around 1998-?99 (Adelberg,
1998; Califf and Mooney, 1999; Freitag, 1998;
Kosala and Blockeel, 2000). They were expert
systems with hand-crafted rules or induced rules
used in a supervised manner and based on labeled
corpora.
The next generation of approaches on the other
hand work in weakly-supervised settings (Etzioni
et al, 2005; Sekine, 2006; Bellare et al, 2007).
Here, the input is a seed list of target information
pairs and the goal is to gather a set of pairs which
are related to each other in the same manner as the
seed pairs. These pairs may contain related enti-
ties (for example, country - capital city in (Etzioni
et al, 2005) and celebrity partnerships in (Cheng
et al, 2009)) or form an entity-attribute pair (like
Nobel Prize recipient - year in (Feiyu Xu, 2007))
or may be concerned with retrieving all available
attributes for entities (Bellare et al, 2007; Pas?ca,
2009). These systems generally download web
pages which contain the seed pairs then learn syn-
tactical/semantical rules from the sentences of the
pairs (they generally use the positive instances for
one case as negative instances for another case).
According to these patterns, they can download a
new set of web pages and parse them to acquire
new pairs.
These seed-based systems exploit the redun-
dancy of the WWW. They are based on the hy-
pothesis that important information can be found
at several places and in several forms on the Web,
hence a few accurate rules can be used to collect
the required lists. Their goal is to find and recog-
nise (at least) one occurrence of the target infor-
mation and not to find their every occurrence on
the Web. But this is not the case in our scenario.
Several pieces of social information for the re-
searchers are available just on their homepages (or
nowhere). Thus here we must capture each men-
tion of the information. The weakly-supervised
(redundancy-based) systems can build on high-
precision and lower recall information extraction,
while we have to have target a perfect recall. For
the evaluation of such a system we constructed a
manually annotated corpus of researchers? home-
pages. This corpus was also used as a training cor-
pus for the preliminary information extraction ex-
periments described in this paper.
3 The architecture of the system
The general task of our system is to gather sci-
entific social information from the homepages of
researchers. In the use case presented in this pa-
per, the input is a set of researchers? names who
work in a particular research field (later on, this
list can be automatically gathered, for example,
from a call for papers) and the output is a list of
affiliations for each researcher. Here the affiliation
is a tuple of affiliation, position type and start/end
dates. We think that the lessons learnt from affili-
ation extraction will be useful for the development
of a general social information extraction system.
The system has to solve several subproblems
which will be described in the following subsec-
tions.
3.1 Locating the homepage of the researcher
Homepage candidates can be efficiently found by
using web search engine queries for the given
name. In our case study the homepage of the
researcher (when it existed) were among the top
10 responses of the Google API1 in each case.
However, selecting the correct homepage from
the top 10 responses is a harder task. Among
1http://code.google.com/apis/
2
these sites there are (i) publication-related ones
(books/articles written by the researchers, call for
papers), sites of the institute/group associated with
the researcher and (ii) homepages of people shar-
ing the same name.
In our preliminary experiments, we ignored
these two basic problems and automatically parsed
each website. However in the future we plan to
develop a two-stage approach to solve them. In
the first stage a general homepage detection model
? a binary classification problem with classes
homepage/non-homepage ? will be applied.
In the second stage we will attempt to automati-
cally extract textual clues for the relations among
the researchers (e.g. the particular field they work
in) from the homepage candidates and utilise these
cues for name disambiguation along with other bi-
ographical cues. For a survey of state-of-the-art
name disambiguation, see (Artiles et al, 2009).
3.2 Locating the relevant parts of the site
The URL got from the search engine usually
points to the main page of the homepage site. An
ideal system should automatically find every page
which might contain scientific social information
likeCurriculum Vitae, Research interests, Projects
etc. This can be done by analysing the text of the
links or even the linked page. In our case study we
simply parsed the pages to a depth of 1 (i.e. the
main page and each page which was linked from
it).
The located web pages usually have their con-
tent arranged in sections. The first step of infor-
mation extraction may be a relevant section se-
lection module. For example, in the affiliation
extraction task the Positions Held and Education
type sections are relevant while Selected Papers
is not. Having several relevant sections with their
textual positions, an automatic classification sys-
tem can filter out a huge number of probably irrel-
evant sections. In our experiments, we statistically
collected a few ?relevant keywords? and filtered
out sections and paragraphs which did not contain
any of these keywords.
3.3 Extracting information tuples
Pieces of scientific social information are usually
present on the homepages and in the CVs even in
an itemised (structured) form or in a natural lan-
guage full text form. Information extraction is
performed from the structured parts of the docu-
ments by automatically constructed rules based on
the HTML tags and keywords. This field is called
Wrapper Induction (Kushmerick, 2000).
We shall focus on the information extraction
from raw texts here because we found that more
pages express content in textual form than in a
structured one in the researchers? homepages of
our case study and this task still has several un-
solved problems. We mentioned above that sci-
entific social information extraction has to cap-
ture each occurrence of the target information.
We manually labeled homepages for the evalua-
tion of these systems. We think that the DOM
structure of the homepages (e.g. formatting tags,
section headers) could provide useful information,
hence the labeling was carried out in their origi-
nal HTML form (Farkas et al, 2008). In our pre-
liminary experiments we also used this corpus to
train classification models (they were evaluated in
a one-researcher-leave-out scheme). The purpose
of these supervised experiments was to gain an in-
sight into the nature of the problem, but we suggest
that a real-world system for this task should work
in a weakly-supervised setting.
3.4 Normalisation
The output of the extraction phase outlined above
is a list of affiliations for each researcher in the
form that occurred in the documents. However, for
scientific social network analysis, several normal-
isation steps should be performed. For example,
for collegial relationship extraction, along with
the matching of various transliteration of research
groups (like Massachusetts Institute of Technology
and MIT AI Lab), we have to identify the appropri-
ate institutional level where two researchers prob-
ably still have a personal contact as well.
4 Experiments
Now we will present the affiliation corpus which
was constructed manually for evaluation purposes
along with several preliminary experiments on af-
filiation extraction.
4.1 The affiliation corpus
We manually constructed a web page corpus
containing HTML documents annotated for pub-
licly available information about researchers. We
downloaded 455 sites, 5282 pages for 89 re-
searchers (who form the Programme Committee
of the SASO07 conference2), and two indepen-
2http://projects.csail.mit.edu/saso2007/tmc.html
3
dent annotators carried out their manual labeling
in the original (HTML) format of the web pages,
following an annotation guideline (Farkas et al,
2008). All the labels that were judged inconsis-
tent were collected together from the corpus for a
review by the two annotators and the chief annota-
tor. We defined a three-level deep annotation hier-
archy with 44 classes (labels). The wide range of
the labels and the inter-annotator agreement both
suggest that the automatic reproduction of this full
labelling is a hard task.
We selected one particular information class,
namely affiliation from our class hierarchy for our
case study. We defined ?affiliation? as the current
and previous physical workplaces and higher ed-
ucational institutes of the researcher in question
as we would like to use this kind of information
to discover collegial relationships and workplace-
changing behaviour. Here institutes related to re-
view activities, awards, or memberships are not re-
garded as affiliations. We call position the tu-
ple of <affiliation, position types,
years>, as for example in <National Depart-
ment of Computer Science and Operational Re-
search at the University of Montreal, adjunct Pro-
fessor, {1995, 2002}>3. Among the four slots
just the affiliation slot is mandatory (it is
the head) as the others are usually missing in real
homepages.
The problem of finding the relevant pages of a
homepage site originating from a seed URL was
not addressed in this study. We found that pages
holding affiliation information was the one re-
trieved by Google in 135 cases and directly linked
to the main page in 50 cases. We found affilia-
tion information for all of the 89 researchers of our
case study in the depth of 1, but we did not check
whether deeper crawling could have yielded new
information.
The affiliation information (like every piece of
scientific social information) can be present on
web pages in an itemised or natural text format.
We manually investigated our corpus and found
that the 47% of the pages contained affiliation in-
formation exclusively in a textual form, 24% ex-
clusively in an itemised form and 29% were hy-
brid. Information extraction from these two for-
mats requires different methods. We decided to
address the problem of affiliation extraction just
3the example is extracted from
http://bcr2.uwaterloo.ca/?rboutaba/biography2.htm
by using the raw text parts of the homepages.
We partitioned each downloaded page at HTML
breaking tags and kept the parts (paragraphs)
which were regarded as ?raw text?. Here we used
the following rule: a textual paragraph has to be
longer than 40 characters and contain at least one
verb. Certainly this rule is far from perfect (para-
graphs describing publication and longer items of
lists are still present), but it seems to be a reason-
able one as it extracts paragraphs even from ?hy-
brid? pages. We found 86,735 paragraphs in the
5282 downloaded pages and used them in experi-
ments in a raw txt format (HTML tags were re-
moved).
Table 4.1 summarises the size-related figures
for the part of this textual corpus which contains
affiliation information (these paragraphs contain
manually labeled information). The corpus is
freely available for non-commercial use4.
# researchers 59
# pages 103
# paragraph 151
# sentences 181
# affiliation 374
# position type 326
# year 212
Table 1: The size of the textual corpus which con-
tains affiliation information.
4.2 The multi-stage model of relation
extraction
Our relation extraction system follows the archi-
tecture described in the previous section. We fo-
cus on the relevant part location and information
extraction steps in this study. We applied simple
rules to recognise the relevant parts of the home-
pages. We extract textual paragraphs as described
above and then filter out probably irrelevant ones
(Section 4.3).
Preliminary supervised information extraction
experiments were carried out in our case study in
order to get an insight into the special nature of
the problem. We used a one-researcher-leave-out
evaluation setting (i.e. the train sets consisted of
the paragraphs of 88 researchers and the test sets
concerned 1 researcher), thus we avoided the situ-
ations where a training set contained possibly re-
4www.inf.u-szeged.hu/rgai/homepagecorpus
4
dundant information about the subject of the test
texts.
A two-stage information extraction system was
applied here. In the first phase, a model should
recognise each possible slot/entities of the target
information tuples (Section 4.4). Then the tuples
have to be filled, i.e. the roles have to be assigned
and irrelevant entities should be ignored (Section
4.5).
4.3 Paragraph filtering
Because just a small portion of extracted textual
paragraphs contained affiliation information, we
carried out experiments on filtering out probably
irrelevant paragraphs.
Our filtering method exploited the paragraphs
containing position (positive paragraphs).
We calculated the P (word|positive) conditional
probabilities and the best words based on this mea-
sure (e.g. university, institute and professor) then
formed the so-called positive wordset. The para-
graphs which did not contain any word from the
positive wordset were removed. Note that stan-
dard positive and negative sample-based classifi-
cation is not applicable here as the non-positive
paragraphs may contain these indicative words,
but in an irrelevant context or with a connection
to people outside of our scope of interest. Our 1-
DNF hypothesis described above uses just positive
examples and it was inspired by (Yu et al, 2002).
After performing this procedure we kept 14,686
paragraphs (from the full set of 86,735), but we
did not leave out any annotated text. Hence the in-
formation extraction module could then work with
a smaller and less noisy dataset.
4.4 Detecting possible slots
We investigated a Named Entity Recognition
(NER) tool for detecting possible actors of a
position tuple. But note that this task is not a
classical NER problem because our goal here is to
recognise just those entities which may play a role
in a position event. For example there were
many year tokens in the text ? having the same
orthographic properties ? but only a few were re-
lated to affiliation information. The contexts of the
tokens should play an important role in this kind
of an NER targeting of very narrow semantic NE
classes.
For training and evaluating the NER systems,
we used each 151 paragraphs containing at least
one manually labeled position along with 200
other manually selected paragraphs which do not
contain any labeled position. We decided to
use just this 151+200 paragraphs instead of the
full set of 86,735 paragraphs for CPU time rea-
sons. Manual selection ? instead of random sam-
pling ? was required as there were several para-
graphs which contained affiliation information un-
related to the researcher in question, thus introduc-
ing noise. In our multi-stage architecture, the NER
model trained on this reduced document set was
than predicated for the full set of paragraphs and
false positives (note that the paragraphs outside the
NER-train do not contain any gold-standard anno-
tation) has to be eliminated.
We employed the Condition Random Fields
(Lafferty et al, 2001) (implementation MALLET
(McCallum, 2002)) for our NER experiments.
The feature set employed was developed for gen-
eral NER and includes the following categories
(Szarvas et al, 2006):
orthographical features: capitalisation, word
length, bit information about the word form
(contains a digit or not, has uppercase char-
acter inside the word, and so on), character
level bi/trigrams,
dictionaries of first names, company types, de-
nominators of locations,
frequency information: frequency of the token,
the ratio of the token?s capitalised and low-
ercase occurrences, the ratio of capitalised
and sentence beginning frequencies of the to-
ken which was derived from the Gigaword
dataset5,
contextual information: sentence position, trig-
ger words (the most frequent and unambigu-
ous tokens in a window around the NEs) from
the train text, the word between quotes, and
so on.
This basic set was extended by two domain-
specific gazetteers, namely a list of university
names and position types. We should add that
a domain-specific exception list (containing e.g.
Dr., Ph.D.) for augmenting a general sentence
splitter was employed here.
Table 2 lists the phrase-level F?=1 results ob-
tained by CRF in the one-researcher-leave-out
5Linguistic Data Consortium (LDC),
catalogId: LDC2003T05
5
evaluation scheme, while Table 3 lists the results
of a baseline method which labels each member
of the university and position type gazetteers and
identifies years using regular expressions. This
comparison highlights the fact that labeling each
occurrences of this easily recognisable classes
cannot be applied. It gives an extremely low pre-
cision thus contextual information has to be lever-
aged.
Precision Recall F?=1
affiliation 66.78 53.28 59.27
position type 87.50 70.22 77.91
year 86.42 69.31 76.92
TOTAL 78.73 62.88 69.92
Table 2: The results achieved by CRF.
Precision Recall F?=1
affiliation 21.43 9.68 13.33
position type 23.27 66.77 34.51
year 65.77 98.99 79.03
TOTAL 32.16 44.08 37.19
Table 3: NER baseline results.
4.5 The assignment of roles
When we apply the NER module to unknown doc-
uments we have to decide whether the recognised
entities have any connection with the particular
person as downloaded pages often contain infor-
mation about other researchers (supervisors, stu-
dents, etc.) as well. The subject of the informa-
tion is generally expressed by a proper noun at
the beginning of the page or paragraph and then
anaphoric references are used. We assumed here
that each position tuple in a paragraph was re-
lated to exactly one person and when the subject of
the first sentence of the paragraph was a personal
pronoun I, she, he then the paragraph belonged to
the author of the page.
To automatically find the subject of the para-
graphs we tried out two procedures and evaluated
them on the predictions of the NER model intro-
duced in the previous subsection. First, we applied
a NER trained on the person names of the CoNLL-
2003 corpus (Tjong Kim Sang and De Meulder,
2003). The names predicted by this method were
then compared to the owner of the homepage us-
ing name normalisation techniques. If no name
was found by the tagger we regarded the para-
graph as belonging to the author. Its errors had two
sources; the NER trained on an out-domain corpus
made a lot of false negatives and the normalisation
method had to deal with incorrect ?names? (like
Paul Hunter Curator as a name phrase) as well.
The second method was simpler. We kept
the position tuples whose paragraph contained
any part of the researcher name or any of the ?I?,
?she?, ?he? personal pronouns. Its errors came, for
instance, from finding the ?Paul? string for ?Paul
Robertson? in the text snippet ?Paul Berger?.
We applied these two subject detection meth-
ods to the predictions of our slot detection NER
modul. Table 4 summarises the accuracies of the
systems, i.e. whether they made the correct deci-
sion on ?is this forecasted affiliation corresponds
to the researcher in question?. The columns of
this table shows how many affiliation pre-
diction was carried out by the slot detection sys-
tem, i.e. how many times has to made a de-
cision. ?name. det? and ?p. pronouns? refer
to the two methods, to the name detection-based
and to the personal pronoun-matcher ones. We
investigated their performance on the paragraphs
which contained manually labeled information,
on the paragraphs which did contained any but
the slot detection module forecasted at least one
affiliation here and on the union of these
sets of paragraphs. The figures of the table shows
that the personal pronoun detection approach per-
forms significantly better on the paragraphs which
really contains affiliation information. This is due
to the fact that this method removes less predic-
tion compared to the name based one and there are
just a few forecast which has to be removed on the
paragraphs which contain information.
#pred name det. p. pronouns
annotated 165 66.9 87.8
non-ann. 214 71.5 61.2
full set 379 69.4 73.4
Table 4: Accuracies of subject detection methods.
To find relationships among the other types of
predicated entities (affiliation, position type, start
year, end year) we used a very simple heuristic.
As the affiliation slot is the head of the tuple
we simply assigned every other detected entity to
the nearest affiliation and regarded the ear-
lier preidcated year token as the start year.
6
This method made the correct decision in the
91.3% and 71.8% of the cases applied on the gold-
standard annotation and the predicated entities, re-
spectively. We should add that using the predicted
labels during the evaluation, the false positives of
the NER counts automatically an error in relation
detection as well.
5 Discussion
The first step of the information extraction sys-
tem of this case study was the localisation of rele-
vant information. We found that Web search en-
gines are efficient tools for finding homepages.
We empirically showed that a very simple crawl-
ing (downloading everything to a depth of 1) can
be applied, because the irrelevant contents can be
removed later. The advantage of focused crawl-
ing (i.e. making a decision before download-
ing a linked page) is that it can avoid the time-
consuming analysis of pages. However making
the decision of whether the linked document might
contain relevant information is a hard task. On the
other hand we showed that the requested informa-
tion is reachable in depth 1 and that a fast string-
matching based filtering method can significantly
reduce the amount of texts which have to be anal-
ysed without losing any information. Moreover,
the positive example-based filtering approach can
be employed in a seed-driven setting as well.
For the information extraction phase we think
that a high-recall system has to be developed. We
constructed a corpus with contextual occurrences
for evaluation issues. The extraction can be re-
lationship detection-based (e.g. the state-of-the-
art seed-driven approaches seek to acquire syntac-
tic/semantic patterns which are typical of the re-
lationship itself) or entity-based (like our method,
these approaches first identify possible actors then
look for relationships among them). We expect
that the latter one is more suitable for high-recall
tasks.
The NER system of this case study achieved
significantly better results than those for the base-
line method. We experimentally showed that
it could exploit the contextual information and
that the labeled entities were those which were
affiliation-related. However, the overall system
has to be improved in the future. We manually
analysed the errors on a part of the corpus and
found a few typical errors were present. Our
annotation guide said that the geographical loca-
tion of the affiliation was a part of the affilia-
tion as it sometimes identifies the department (e.g.
?Hewlett-Packard Labs in Palo Alto?). This ex-
tension of the phrase proved to be difficult because
there were several cases with the same ortho-
graphic features (e.g. Ph.D. from MIT in Physics).
The acronyms immediately after the affiliation are
a similar case, which we regard as part of the name
and it is difficult for the NER to handle (e.g. Cen-
tre for Policy Modelling (CPM)). As there is no
partial credit; an incorrect entity boundary is pe-
nalised both as a false positive and as a false neg-
ative.
These points also explain the surprisingly low
precision of the baseline system as it labeled uni-
versity names without more detailed identifica-
tion of the unit (e.g. Department of Computer
Science, [Waterloo University]BASELINE). We
should add that these two annotation guidelines
are questionable, but we expect that information
might get lost without them. Moreover, there is
an another reason for the low recall, it is that our
human annotators found textual clues for position
types on verbs as well (e.g. I leadTY PE the Dis-
tributed Systems Group). The context of these la-
beled examples are clearly different from that of
the usual position type.
Comparing the two subject detection methods,
we see that the name detection model which learnt
on an out-domain corpus made a lot of mistakes,
thus the method based on it judged more para-
graphs as irrelevant ones. The name detection
could be improved by a domain corpus (for exam-
ple the training corpus did not contain any Prof.
NAME example) and by applying more sophisti-
cated name normalisation techniques. When we
manually analysed the errors of these procedures
we found that each false negative of the sim-
pler subject detection method was due to the er-
rors of the textual paragraph identification defini-
tion used. There were several itemisations whose
header was type of ?Previously I worked for:? and
the textual items themselves did not contain the
subject of the affiliation information. The false
positives often originated from pages which did
not belong to the researcher in question but con-
tained him name (e.g. I am a Ph.D. Student work-
ing under the supervision of Prof. NAME).
Lastly, an error analysis of the affiliation head
seeking heuristic revealed that the 44% of the
predicted position type and year entities?s
7
sentences did not contain any affiliation
prediction. With the gold-standard labeling, there
were 6 sentences without affiliation labels
and only one of them used an anaphoric refer-
ence, the others were a consequence of the erro-
neous automatic sentence splitting of the HTML
documents. The prediction of the NER sys-
tem contained many more sentences without any
affiliation label. These could be fixed
by forcing a second forecast phase to predict
affiliation in these sentences or by remov-
ing these labels in a post-processing step.
The remaining errors of the affiliation head as-
signment could be avoided just by employing a
proper syntactic analyser. The most important lin-
guistic phenomena which should be automatically
identify for this problem is enumeration. For in-
stance, we should distinguish between the enumer-
ation and clause splitting roles of ?and? (e.g. ?I?m
a senior researcher and leader of the GROUP?
and ?He got his PhD fromUNIVERSITY1 in YEAR
and has a Masters from UNIVERSITY2?). This
requires a deep syntactic analysis, i.e. the use of
a dependency parser which has to make accurate
predictions on several certain types of dependen-
cies is probably needed.
6 Conclusions
In this paper we introduced a Web Content Mining
system for gathering affiliation information from
the homepages of researchers. The affiliation in-
formation collected from this source might be of
great value for scientific social network analysis.
We discussed the special nature of this task
compared to common Web-based relation extrac-
tion approaches and identified several subtasks of
the system during our preliminary experiments.
We argued that the evaluation of this kind of sys-
tem should be carried out on a manually labeled
reference corpus. We introduced simple but ef-
fective solutions for the subproblems along with
empirical results on a corpus. We achieved rea-
sonable results with an overall phrase-level F?=1
score of 70% on the possible slot detection and
an accuracy of 61% on relation extraction (as an
aggregation of the subject detection and the affil-
iation head selection procedures). However each
subproblem requires more sophisticated solutions,
which we plan to address in the near future.
Acknowledgments
This work was supported in part by the NKTH
grant of the Jedlik A?nyos R&D Programme
(project codename TEXTREND) of the Hungar-
ian government. The authors would like to thank
the annotators of the corpus for their devoted ef-
forts.
References
Brad Adelberg. 1998. Nodose - a tool for semi-
automatically extracting structured and semistruc-
tured data from text documents. ACM SIGMOD,
27(2):283?294.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2009. Weps 2 evaluation campaign: overview of the
web people search clustering task. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th
WWW Conference.
A. L. Baraba?si, H. Jeong, Z. Ne?da, E. Ravasz, A. Schu-
bert, and T. Vicsek. 2002. Evolution of the so-
cial network of scientific collaborations. Physica A:
Statistical Mechanics and its Applications, 311(3-
4):590 ? 614.
Kedar Bellare, Partha Talukdar, Giridhar Kumaran,
Fernando Pereira, Mark Liberman, Andrew McCal-
lum, and Mark Dredze. 2007. Lightly-supervised
attribute extraction for web search. In Proceedings
of NIPS 2007 Workshop on Machine Learning for
Web Search.
Mary Elaine Califf and Raymond J. Mooney. 1999.
Relational learning of pattern-match rules for in-
formation extraction. In Proceedings of the Six-
teenth National Conference on Artificial Intelli-
gence, pages 328?334.
Xiwen Cheng, Peter Adolphs, Feiyu Xu, Hans Uszko-
reit, and Hong Li. 2009. Gossip galore ? a self-
learning agent for exchanging pop trivia. In Pro-
ceedings of the Demonstrations Session at EACL
2009, pages 13?16, Athens, Greece, April. Associa-
tion for Computational Linguistics.
Oren Etzioni, Michael Cafarella, Doug Downey,
Ana maria Popescu, Tal Shaked, Stephen Soderl,
Daniel S. Weld, and Er Yates. 2005. Unsupervised
named-entity extraction from the web: An experi-
mental study. Artificial Intelligence, 165:91?134.
Richa?rd Farkas, Ro?bert Orma?ndi, Ma?rk Jelasity, and
Ja?nos Csirik. 2008. A manually annotated html cor-
pus for a novel scientific trend analysis. In Proc. of
The Eighth IAPR Workshop on Document Analysis
Systems.
Hong Li Feiyu Xu, Hans Uszkoreit. 2007. A seed-
driven bottom-up machine learning framework for
8
extracting relations of various complexity. In Pro-
ceedings of ACL 2007, 45th Annual Meeting of the
Association for Computational Linguistics, Prague,
Czech Republic, 6.
Dayne Freitag. 1998. Information extraction from
html: Application of a general machine learning ap-
proach. In Proceedings of the Fifteenth National
Conference on Artificial Intelligence, pages 517?
523.
A. A Goodrum, K. W McCain, S. Lawrence, and C. L
Giles. 2001. Scholarly publishing in the internet
age: a citation analysis of computer science liter-
ature. Information Processing and Management,
37:661?675, September.
Raymond Kosala and Hendrik Blockeel. 2000. Web
mining research: A survey. SIGKDD Explorations,
2:1?15.
Nicholas Kushmerick. 2000. Wrapper induction: Ef-
ficiency and expressiveness. Artificial Intelligence,
118:2000.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282?289. Morgan Kauf-
mann, San Francisco, CA.
Bing Liu and Kevin Chen-Chuan-Chang. 2004. Edito-
rial: special issue on web content mining. SIGKDD
Explor. Newsl., 6(2):1?4.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
M. E. J. Newman. 2001. The structure of scientific
collaboration networks. In Proceedings National
Academy of Sciences USA, pages 404?418.
Marius Pas?ca. 2009. Outclassing Wikipedia in open-
domain information extraction: Weakly-supervised
acquisition of attributes over conceptual hierarchies.
In Proceedings of the 12th Conference of the Eu-
ropean Chapter of the ACL (EACL 2009), Athens,
Greece, March.
Celine Robardet and Eric Fleury. 2009. Communi-
ties detection and the analysis of their dynamics in
collaborative networks. Int. J. Web Based Commu-
nities, 5(2):195?211.
Yasmin H. Said, Edward J. Wegman, Walid K. Shara-
bati, and John T. Rigsby. 2008. Social networks
of author-coauthor relationships. Computational
Statistics & Data Analysis, 52(4):2177?2184.
Satoshi Sekine. 2006. On-demand information ex-
traction. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 731?738,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Gyo?rgy Szarvas, Richa?rd Farkas, and Andra?s Kocsor.
2006. A multilingual named entity recognition sys-
tem using boosting and c4.5 decision tree learning
algorithms. DS2006, LNAI, 4265:267?278.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. An annotation scheme for citation function.
In Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue, pages 80?87, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142?147. Edmon-
ton, Canada.
Y. Yang, C. M. Au Yeung, M. J. Weal, and H. Davis.
2009. The researcher social network: A social net-
work based on metadata of scientific publications.
Hwanjo Yu, Jiawei Han, and Kevin Chen-Chuan
Chang. 2002. Pebl: positive example based learn-
ing for web page classification using svm. In KDD
?02: Proceedings of the eighth ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 239?248, New York, NY, USA.
ACM.
9
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 759?770,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning Local Content Shift Detectors from Document-level Information
Richa?rd Farkas
Institute for Natural Language Processing
University of Stuttgart
farkas@ims.uni-stuttgart.de
Abstract
Information-oriented document labeling is a
special document multi-labeling task where
the target labels refer to a specific information
instead of the topic of the whole document.
These kind of tasks are usually solved by look-
ing up indicator phrases and analyzing their
local context to filter false positive matches.
Here, we introduce an approach for machine
learning local content shifters which detects
irrelevant local contexts using just the origi-
nal document-level training labels. We handle
content shifters in general, instead of learn-
ing a particular language phenomenon detec-
tor (e.g. negation or hedging) and form a sin-
gle system for document labeling and content
shift detection. Our empirical results achieved
24% error reduction ? compared to supervised
baseline methods ? on three document label-
ing tasks.
1 Introduction
There are special document multi-labeling tasks
where the target labels refer to a specific piece of
information extractable from the document instead
of the overall topic of the document. In these kinds
of tasks the target information is usually an attribute
or relation related to the target entity (usually a per-
son or an organisation) of the document in question,
but the task is to assign class labels at the document
(entity) level. For example, the smoking habits of
the patients are frequently discussed in the textual
parts of clinical notes (Uzuner et al, 2008). In this
case the task is to find specific information in the
text ? i.e. the patient in question is a smoker, past
smoker, non-smoker ? but at the end an applica-
tion has to assign labels to the documents(patients).
Similarly, the soccer club names where a sportsman
played for are document(sportman)-level labels in
Wikipedia articles expressed by the Wikipedia cat-
egories. The target information in these tasks is
usually just mentioned in the document and much
of the document is irrelevant for this information
request in contrast to standard document classifi-
cation tasks where the goal is to identify the top-
ics of the whole document. On the other hand,
they are not a standard information extraction task
as the task is to assign class labels to documents,
and the training dataset contains labels just at this
level. These special tasks lie somewhere between
information extraction and document classification
and require special approaches to solve them. We
will call them Information-oriented document label-
ing throughout this paper. There are several appli-
cation areas where information-oriented document
labels are naturally present in an enormous amount
like clinical records, Wikipedia categories and user-
generated tags of news.
Previous evaluation campaigns (Uzuner et al,
2008; Pestian et al, 2007; Uzuner, 2009) demon-
strated that information-oriented document labeling
can be effectively performed by looking up indicator
phrases which can be gathered by hand, by corpus
statistics or in a hybrid way. However these cam-
paigns also highlighted that the analysis of the local
context of the indicator phrases is crucial. For in-
stance, in the smoking habit detection task there are
a few indicator words (e.g. smokes, cigarette) and
the local context of their occurrences in texts should
759
be analysed to see whether their semantic was rad-
ically changed (e.g. they are negated or in a past
tense), for instance:
The patient has a 20 pack-year smoking
history.
The patient denies any smoking history.
He has a greater than 100 pack year
smoking history and quit 9 to 10 years
ago.
We propose a simple but efficient approach for
information-oriented document labeling tasks by ad-
dressing the automatic detection of language phe-
nomena for a particular task which alters the sense
or information content of the indicator phrase?s oc-
currences. For example, they may be logical modi-
fiers (e.g. negation) or modal modifiers (e.g. auxil-
iaries like might and can); they may refer to a subject
which differs from the target entity of the task (e.g.
clinical notes usually contain information about the
family history of the patient); or the semantic con-
tent of the shifter may change the role of the tar-
get span of a text (e.g. a sportsman can play for or
against a particular team). We call these phenom-
ena content shifters and the task of identifying them
content shift detection (CSD).
Existing CSD approaches focus on a particular
class of language phenomena (especially negation
or hedging) and use hand-crafted rules (Chapman et
al., 2007) or a supervised learning approach that ex-
ploits corpora manually annotated at the token-level
for a particular type of content shifter (Morante et
al., 2009). Moreover higher level applications (like
document labeling and information extraction) use a
separate CSD module which is developed indepen-
dently from the target task. We argue that the nature
of content shifters is domain and task dependent, so
training corpora (at the token-level) are required for
content shifters which are important for a particular
task but the construction of such training corpora is
expensive. Here, we propose an alternative approach
which uses only document-level labels.
The input of our system is a training corpus la-
beled on the document level (e.g. a clinical dataset
consisting clinical notes and meta-data about pa-
tients). Our approach extracts indicator phrases and
trains a CSD jointly. We focus on local content
shifters and we analyse just the sentences of indi-
cator phrase occurrences. Our chief assumption is
that CSD can be learnt by exploiting the false pos-
itive occurrences of indicator phrases in the train-
ing dataset. We show that our method performs sig-
nificantly better than standard document classifiers
(which were designed for a slightly different task).
The chief contributions of our work are that (i)
we handle the CSD problem in general, so we de-
tect all content shifters instead of focusing on one
particular language phenomenon, (ii) we form a sin-
gle framework for joint CSD and document labeling,
(iii) moreover our approach does not require a dedi-
cated annotated training dataset for content shifters.
2 Related Work
Information-oriented document classification tasks
were first highlighted in the clinical domain where
medical reports contain useful information about the
patient in question, but labels are only available at
the document (patient) level. The field of clinical
NLP has been studied extensively since the 1990s
(Larkey and Croft, 1995), but the most recent results
are related to the shared task challenges organized
relatively recently (Pestian et al, 2007; Uzuner et
al., 2008; Uzuner, 2009). For example the first
I2B2 challenge in 2006 (Uzuner et al, 2008) fo-
cused on the smoking habits of the patient, the CMC
challenge in 2007 (Pestian et al, 2007) dealt with
the problem of automatically constructing ICD cod-
ing systems and the second I2B2 challenge (Uzuner,
2009) addressed the classification of discharge sum-
maries according to the question ?Who?s obese and
what co-morbidities do they have??. These chal-
lenges were dominated by entirely or partly rule-
based systems that solved the tasks using indicator
phrase lookup and incorporated explicit mechanisms
for detecting speculation and negation.
Another domain for information-oriented docu-
ment classification might be Wikipedia, which con-
tains rich information about entities like persons,
places or organisations. Some items of information
are available about these entities in the form of cate-
gories and infoboxes assigned to articles. Automatic
document labeling methods can be trained based on
these assignments (Scho?nhofen, 2006), but these la-
bels do not refer to the main theme of the article but
760
to a certain type of information.
Existing content shift detection approaches focus
on a particular class of language phenomena, espe-
cially on negation and hedge recognitions. Avail-
able tools work mainly on clinical and biological
domains. The first systems were fully hand-crafted
(Light et al, 2004; Friedman et al, 1994; Chapman
et al, 2007) without any empirical evaluation on a
dedicated corpus. Recently, there have been several
corpora published with manual sentence-, event- or
token-level annotation for negation, certainty and
factuality in the biological (Medlock and Briscoe,
2007; Vincze et al, 2008), newswire (Strassel et al,
2008; Sauri and Pustejovsky, 2009) and encyclope-
dical (Farkas et al, 2010) domains.
Exploiting these corpora, machine learning mod-
els were also developed. Solving the sentence-
level task, Medlock and Briscoe (2007) used sin-
gle words as input features in order to classify sen-
tences from biological articles as speculative or non-
speculative. Szarvas (2008) extended their method-
ology to use n-gram features and a semi-supervised
selection of the keyword features. Ganter and Strube
(2009) proposed an approach for the automatic de-
tection of sentences containing uncertainty based
on Wikipedia weasel tags and syntactic patterns.
For in-sentence negation and speculation detection,
Morante et al (2009) developed scope ? i.e. con-
tent shifted text spans ? detectors for negation and
speculation following a supervised sequence label-
ing approach, while ?Ozgu?r and Radev (2009) devel-
oped a rule-based system that exploits syntactic pat-
terns. The goal of the CoNLL 2010 Shared Task
(Farkas et al, 2010) was to develop linguistic scope
detectors as well. The participants usually followed
a supervised sequence labeling approach or used a
rule-based system that exploits syntactic patterns.
The approach of classifying identified events into
whether they fall under negation or speculation was
followed by Sauri and Pustejovsky (2009) and the
participants of the BioNLP?09 Shared Task (Kim et
al., 2009). Here the systems investigated the syn-
tax path between the event trigger and a cue word
(which came from a small lexicon) (Kilicoglu and
Bergler, 2009; Aramaki et al, 2009).
Our approach differs from the previous works
fundamentally. We deal with the two tasks
(information-oriented document classification and
content shift detection) together and introduce a co-
learning approach for them. Our approach han-
dles content shifters in a data-driven and general-
ized way i.e. it is not specialized for a certain class
of language phenomena. Instead it tries to recog-
nize task-specific syntactic and semantic patterns
which are responsible for semantic changes or irrel-
evance. In addition, we have no access to a gold-
standard sentence-level or in-sentence-level annota-
tion but exploit document-level ones.
3 Tasks and Datasets
Before introducing our approach in detail we de-
scribe three tasks and datasets which were used in
our experiments in order to give an insight into the
challenges of the information-oriented document la-
beling tasks. Table 1 summarizes the key statistical
figures (the number of documents in the corpora, the
size of the label sets along with the average number
of tokens and label assignments per document) of
the datasets used for the experimental evaluations.
Table 1: The datasets used in our experiments.
CMC Obes Soccer
domain clinical clinical encycl.
|train| 978 730 4850
|eval| 976 507 1736
#token/d 25 1387 389
#labels 45 16 12
#label/d 1.24 4.37 1.23
The CMC ICD Coding Dataset was originally
prepared for a shared task challenge organized by
the Computational Medicine Center (CMC) in Cin-
cinatti, Ohio in 2007 (Pestian et al, 2007). It con-
tains radiology reports along with document-level
International Classification of Diseases (ICD) codes
given by three human experts. ICD is a coding of
diseases, signs, symptoms and abnormal findings. In
our experiments we used the train/evaluation split of
the shared task. The ICD coding guide states that
negative or uncertain diagnosis should not be coded
in any case.
The corpus contains very short documents. For
instance, the document
HISTORY: Left lower chest pain. Rule-out
761
pneumonia. IMPRESSION: Normal chest.
has one label 786.50 (cough) as 486 (pneumonia) is
ruled out.
The main conclusion of the shared task in 2007
was that simple rule-based systems generally out-
perform bag-of-words-based machine learning mod-
els. The rules were extracted from ICD guidelines
and/or from the training corpus using simple sta-
tistical measures, then they were checked or ex-
tended manually. Several systems of the challenge
employed a negation and speculation detection sub-
module. The (manually highly fine-tuned) top sys-
tems of the CMC shared task achieved an F-measure
of 88-89 (Pestian et al, 2007; Farkas and Szarvas,
2008).
The I2B2 Obesity Dataset was also the subject
of a clinical natural language processing shared
task. The challenge in 2008 focused on analyzing
clinical discharge summary texts and addressed the
following question: ?Who is obese and what co-
morbidities do they have?? (Uzuner, 2009). Tar-
get diseases (document labels) included obesity and
its 15 most frequent co-morbidities exhibited by
patients. In our experiments, we used the same
train/evaluation split as that of the shared task. Here
a special aspect of the corpus is that the docu-
ments are semi-structured, i.e. they contain head-
ings like discharge medications and admit diagno-
sis. By pasting the given heading to the beginning
of each sentence, we incorporated it into the local
context. The top performing systems of the shared
task employed mostly hand-crafted rules for indica-
tor selection and for negation and uncertainty detec-
tion as well. They achieved an F-measure1 of 96-97
(Uzuner, 2009; Solt et al, 2009).
Wikipedia Soccer Dataset. We constructed a cor-
pus based on Wikipedia articles and categories2.
The categories assigned to Wikipedia articles can be
regarded as labels (for example, the labels of David
Beckham in the Wikipedia are English people, ex-
patriate soccer player, male model and A.C. Milan
player, Manchester United player). Based on the
1Using the definitions of the challenge, the evaluation metric
applied here is the micro F-measure of the textual task on the
YES versus every other class.
2The dataset is available as the supplementary material.
categories of Wikipedia, classifiers can be trained to
tag unlabeled texts or even add missing category as-
signments to Wikipedia (Scho?nhofen, 2006).
For a case study we focused on learning En-
glish soccer clubs that a given sportsman played
for. Note that this task is an information-oriented
document labeling task as the clubs for which a
sportsman played are usually just mentioned (espe-
cially for smaller clubs) in the article of a player.
The Wikipedia category Footballers in England by
club contains 408 subcategories (for the present and
past). We selected the best known clubs (where the
category label for the club is assigned to more than
500 player pages). Each article referring to a player
having a category assignment to these clubs was
downloaded and the textual parts were extracted.
Then a random 3:1 train:evaluation split of the doc-
ument set was used.
4 Document-labeling with CSD
We introduce here an iterative solution which selects
indicator phrases and trains a content shift detec-
tor at the same time. Our focus will be on multi-
label document classification tasks where multiple
class labels can be assigned to a single document.
In this study we will not deal with the modeling of
inter-label dependencies, so binary (positive versus
negative) and multi-class document classifications
(where exactly one label has to be assigned to a sin-
gle document) can be regarded as special cases of
this multi-label classification problem. Our result-
ing multi-label model is then a set of binary classi-
fiers ? ?assign a label? classifiers for each class label
? and the final prediction on a document is simply
the union of the labels forecasted by the individual
classifiers.
Our key assumption in the multi-label environ-
ment is that while indicator phrases have to be se-
lected on a per class basis, the content shifters can be
learnt in a class-independent (aggregated) way i.e.
we can assume that within one task, each class label
belongs to a given semantic domain (determined by
the task), thus the content shifters for their indicator
phrases are the same. This approach provides an ad-
equate amount of training samples for content shift
detector learning.
762
Table 2: Example feature representation of local contexts of Arsenal. The prefix NP stands for the lemma features
from the deepest noun phrase; D,DR and DEP marks the lemmas, roles and their combination in the dependency path,
respectively; SUBJ and SUBJD denote the lemmas and dependency roles on the ?subject path?, respectively.
His brother, Paul had a long career at Newcastle. (sentenceId=1, indicator=Newcastle)
bag-of-word features syntax-based features
he, brother, Paul, have, NP#a, NP#long, NP#career, NP#at
a, long, career, at D#career, D#have, DR#prepat, DR#dobj, DEP#career#prepat, DEP#have#dobj
SUBJ#brother, SUBJ#Paul, SUBJ#he, SUBJD#he#poss
He was born in Gosforth, Newcastle and played for Arsenal. (sentenceId=2, indicator=Arsenal)
bag-of-word features syntax-based features
he, be, bear, in, Gosforth, D#play, DR#prepfor, DEP#play#prepfor
Newcastle, and, play, for SUBJ#he
4.1 Learning Content Shift Detectors
The key idea behind our approach is that a training
corpus for task-specific content shifter learning can
be automatically generated by exploiting the occur-
rences of indicators in various contexts. The local
context of an indicator is assumed to have altered if
it yields a false positive document-level prediction.
More precisely, a training dataset can be constructed
for learning a content shift detector in a way that the
instances are the local contexts of each occurrence of
indicator phrases in the training document set. The
instances of this content shifter training dataset are
then labeled as non-altered when the indicated
label is among the gold-standard labels of the doc-
ument in question or is labeled as altered other-
wise. On this dataset, arbitrary binary classification
models (S) can be trained.
As a feature representation of a local context of an
indicator phrase, the bag-of-words of the sentence
instance (excluding the indicator phrase itself) was
used at the beginning. Our preliminary experiments
showed that the tokens of the sentence after the indi-
cator played a negligible role, hence we represented
contexts just by tokens before the indicator.
Features concerning the syntactic context of the
given indicator were also investigated. For this, we
extended the feature set with features derived from
the constituent and dependency parses of the sen-
tence3. First, the deepest noun phrase which in-
cludes the indicator phrase was identified, then all
3We parse only the sentences which contain indicator phrase
which makes these features computable in reasonable time even
on bigger document sets.
lemmas from its subtree were gathered. From the
dependency parse, the lemmas and dependency la-
bels on the directed path from the indicator to the
root node (main path) were extracted. The directed
paths branching from this main path starting with
subject dependency were also used for feature
extraction (note that these walk in opposite direction
to that of the main path). The intuition of the latter
was that the subject of the given information ? as it
can differ from the target entity of extraction ? is of
great importance. We note that we recognize the in-
sentence subject and employing a co-reference mod-
ule would probably increase the value of these fea-
tures.
Table 2 exemplifies the feature representation of
local contexts of the Newcastle and Arsenal indi-
cators for the Wikipedia soccer task. In both sen-
tences, a naive system would extract Newcastle as
false positives. We want to learn content shifters
from them along with the true positive match of
Arsenal in sentence 2. From the first example the
CSD could learn even that the bag-of-word con-
tains brother or the SUBJ=brother. However, in
the second example, the bag-of-word representa-
tion is not sufficient to learn that the local context
of Newcastle is altered because it is the sub-
set of the bag-of-word representation of Arsenal?s
non-altered local context. In this case the syn-
tactic context representation can help and in our
CSD DEP=play#prepfor gets high weight for the
non-altered class.
763
4.2 Co-learning of Indicator Selection and CSD
If document labels are available at training time,
an iterative approach can be used to learn the local
content shift detector and the indicator phrases as
well. The training phase of this procedure (see Al-
gorithm 1) has two outputs, namely the set of indica-
tor phrases for each label I and the content shift de-
tector S which is a binary function for determining
whether the sense of an indicator in a particular local
context is being altered. Good indicator phrases are
those that identify the class label in question when
they are present. In each step of the iteration we se-
lect indicator phrases I[l] for each label l based on
the actual state of the document set D?. Using these
I[l]s we train a CSD S. Then we apply it to the orig-
inal dataset D and we delete each local context from
the documents which was predicted to be altered by
S.
Algorithm 1 Co-learning of labels and CSD
Input: L class labels, D labeled training documents
D? ? D
repeat
for all l ? L do
I[l]? indicatorSelection(D? , l)
end for
S ? learnCSD(D?, I)
D? ? removeAlteredParts(D,S)
until convergence
return I, S
The indicator selection and content shifter learn-
ing phases can form an iterative process. The bet-
ter the selected indicators are, the better the content
shift detectors can be learnt. By applying the content
shift detector to each token of the documents, each
part of the texts lying within the scope of a content
shifter can be removed4. By using such a cleaned
training document set (D?), better indicators can be
selected. These steps can be repeated until some
convergence criterion is reached. In our experiments
we simply used a fixed iteration number to gain an
insight into the behavior of the approach.
4In our first experiments introduced here, we removed the
parts of the documents classified as altered. Instead of removing
these parts they may be marked and then different features may
be extracted from them.
Algorithm 2 Document labeling with CSD
Input: d document, I indicator sets, S CSD
pred? ?
for all l ? L do
for all o ? occurrences(d, I[l]) do
if not altered(o, S) then
pred? pred ? l
end if
end for
end for
return pred
The prediction procedure of the approach (see Al-
gorithm 2) then looks for occurrences of the indica-
tor phrases in the text and checks whether they are
altered in a certain local context. A non-altered indi-
cator directly assigns a class label without any global
consistency check on assigned labels.
We note here, that the local relationship among
tokens (i.e. the local context) may be taken into
account by incorporating this information directly
into the feature space of a document classifier (as
an alternative of our co-learning procedure), but
the number of features would exponentially increase
and submodels for each indicator phrases should be
learnt which would made such a classification task
intractable.
4.3 Indicator Phrase Selection
Indicator phrases are sequences of tokens whose
presence implies the positive class. We aimed to
extract phrases with the length of 1,2 or 3 (and we
used exact matching after lemmatisation). There are
several possible ways of developing indicator selec-
tion algorithms. One way is to treat it as a special
feature selection procedure where the goal is to se-
lect a set of features (uni-, bi-, trigrams of a bag-
of-word model) which achieves high recall along
with moderate precision as false positives are ex-
pected to be eliminated by the local CSD in our two-
step approach. Indicator selectors can be even de-
rived from most classifiers which are based on fea-
ture weighting (like MaxEnt and AvgPerceptron) or
feature ranking (like rule-based classifiers)5 as well.
However indicator selection is not the focus of this
5A derivation is more complicated or unfeasible for
example-based classifiers like SVMs.
764
Table 3: Results obtained for local content shift detection in a precision/recall/F-measure format.
CMC Obesity Soccer
Trained
BoW 90.7 / 60.7 / 72.7 82.1 / 35.4 / 49.4 75.0 / 70.6 / 72.7
BoW+syntactic 88.3 / 60.2 / 71.6 84.4 / 33.3 / 47.8 81.0 / 78.9 / 79.9
Hand-
crafted
CSSDB 94.7 / 53.3 / 68.2 42.0 / 57.9 / 48.7 36.8 / 9.8 / 15.5
in-sentence 80.7 / 65.2 / 72.2 70.5 / 40.5 / 51.5 N/A
work.
For our experiments, a feature evaluation-based
greedy algorithm was employed to select the set of
indicators from the pool of token uni- and bigrams.
The aim of the the indicator selection here is to cover
each positive documents while introducing a rela-
tively small amount of false positives. The greedy
algorithm iteratively selects the 1-best phrase ac-
cording to a feature evaluation metric based on the
actual state of covered documents and adds it to the
indicator phrase set. The process is iterated while
the score ? in terms of the applied feature evaluation
metric ? of the 1-best phrase is above a threshold
t. The quality of the selected indicator set is highly
dependent on the stopping threshold t, but as indi-
vidual feature evaluation functions are very fast and
the number of good indicators is usually low (4-5),
the whole greedy indicator selection is fast, hence t
can be fine-tuned without overfitting on the training
sets employing a cross-validation procedure. As a
feature evaluation metric we employed p(+|f) the
probability of the positive class ?+? conditioned on
the presence of a feature f because preliminary ex-
periments did not show any significant advances for
more complex metrics.
5 Experiments
Experiments were carried out on the three datasets
introduced in Section 3 with local content shift de-
tection as an individual task and also to investigate
its added value to information-oriented document la-
beling.
In our experiments, we applied the sentence split-
ter and lemmatizer implementation of the Mor-
phAdorner package6 and the Stanford tokenizer
and lexicalized PCFG parser (Klein and Manning,
2003)7.
6morphadorner.northwestern.edu/
7The JAVA implementation of the entire framework and
5.1 Content Shifter Learning Results
In order to evaluate content shift detection as an indi-
vidual task, a set of indicator phrases have to be fixed
as an input to the CSD. We used manually collected
indicator phrases for each label for each dataset. We
utilized the terms of Farkas and Szarvas (2008) and
Farkas et al (2009) collected for the CMC and Obe-
sity datasets, respectively and club names for the
Soccer dataset in our first branch of experiments.
Note that the clinical term sets here have been man-
ually fine-tuned as they were developed for partici-
pating systems of the shared tasks of the corpora.
Based on the occurrences of these fixed indicator
phrases, CSD training datasets were built from the
local contexts of the three datasets and binary clas-
sification was carried out by using MaxEnt. Table
3 shows the results achieved by the learnt CSDs us-
ing the bag-of-word feature representation (row 1)
along with the ones obtained by the feature set that
was extended with syntactic patterns (raw 2). Here,
the precision/recall/F-measure values measure how
many false positive matches of the indicator phrases
can be recognized (the F-measure of the altered
class), i.e. here, the true positives are local contexts
of an indicator phrase which do not indicate a docu-
ment label in the evaluation set and the local content
shift detector predicted it to be altered.
For comparison purposes, we employed manu-
ally developed CSDs which were fine-tuned for the
medical shared task datasets. Row 3 of Table 3 (we
refer to it as content shifted sentence detection base-
line (CSSDB) later on) shows the results archived
by the method which predicts every sentence to be
altered which contain any cue phrases for nega-
tion, modality and different experiencer. Note that
off-the-shelf tools are available just for these types
of content shifters. We collected cue phrases for
such a content shifted sentence detection from the
dataset adapters can be found as the supplementary material
765
works of Chapman et al (2007), Light et al (2004)
and Vincze et al (2008) and from the experiments of
Farkas and Szarvas (2008) and Farkas et al (2009).
For the CMC and Obesity tasks, hand-crafted
in-sentence CSDs were also available (Farkas and
Szarvas, 2008; Farkas et al, 2009), i.e. they apply
heuristics ? which usually tries to recognise clause
boundaries ? for determining the scope of a nega-
ton/modality cue. This CSD is more fine-grained
than the sentence-level one as here a part of a sen-
tence can be detected as alteredwhile other parts
as non-altered. The results of these detectors ?
two different CSDs, both highly fine-tuned for the
corresponding shared task ? are listed in the last row
of Table 3.
On the CMC dataset, our machine learning ap-
proach identified mostly negation and speculation
expressions as content shifters; the top weighted
features for the positive class of the MaxEnt model
were no, without, may and vs. They can filter out
false positive matches like
Hyperinflated without focal pneumonia..
On the Obesity dataset, similar content shifters
were learnt along with references to family mem-
bers (like the terms mother and uncle, and the fam-
ily history header). The significance of these types
of content shifters may be illustrated by the follow-
ing sentence:
History of hypertension in mother and sis-
ter.
The soccer task highlighted totally different con-
tent shifters which is also the reason for the poor per-
formance of CSSDB. The mention of a club name
which the person in question did not play for (false
positives) is usually a rival club, club of an unsuc-
cessful negotiation or club which was managed by
the footballer after his retirement. For example:
His last game was against Chelsea at
Stamford Bridge.
He was a coach at United during his son?s
playing career.
Summing up, the machine learnt CSDs proved to
be competitive with the manually fine-tuned CSD
on the three datasets. Table 3 shows that learnt
CSDs were able to eliminate a significant amount
of false positive indicator phrase matches on each
of the three datasets. The hand-crafted CSDs de-
veloped for the medical texts certainly work poorly
(an F-score of 15.5) on the Soccer dataset as content
shifters different from negation, hedge and experi-
encer are useful there. On the other hand, the content
shifters could be learnt on this dataset by our CSD
approach (achieving F-score of 79.9). In the clinical
corpora, the features from the syntactic parses just
confused the system, but they proved to be useful
on the Soccer corpus. Here, the dependency parse
achieved improvements in terms of both precision
and recall (the number of true positives increased by
137) which can be mainly attributed to the preposi-
tions against and over. The reason why it did not
advance on the clinical corpora is probably the do-
main difference between the training corpus of the
parsers and the target texts, i.e. the parsers trained
on the Wall Street Journal could not build adequate
dependency parses on clinical notes.
As a final comparison we investigated the manu-
ally annotated BioScope corpus (Vincze et al, 2008)
as a CSD. The CMC corpus is included in the Bio-
Scope corpus where text spans in the in-sentence
scope of speculation and negation were annotated.
We used this manual annotation as an oracle CSD
and got an F-measure of 75.2 (which is significantly
higher than the scores 72.2 and 72.7 archived by the
hand-crafted and trained CSD respectively). This
score can be regarded as an upper bound for the
amount of false positive indicator matches that can
be fixed by local speculation and negation detec-
tors. The remaining false positives are not covered
by the linguistically motivated annotations of Bio-
Scope, i.e. false positives recognizable by domain
knowledge (e.g. coding symptoms should be omit-
ted when a certain diagnosis that is connected with
the symptom in question is present in the document)
are not marked.
Our error analysis revealed that most of the er-
rors of the learnt CSDs is due to the lack of seman-
tic link between lexical units. For instance, on the
Soccer dataset it could learn that the token coach
occuring in the sentence in question indicates an
altered content, but it was not able to recognise
this for trainer. The reason for that is simple, the
ratio of occurrences of trainer:coach is 5:95 in the
766
Table 4: Results obtained by document multi-labeling algorithms in a precision/recall/F-measure format.
rowID CMC Obesity Soccer
1 SVM 87.7 / 76.7 / 81.8 90.0 / 81.3 / 85.4 92.2 / 75.1 / 82.8
2 Baseline MaxEnt with CSSDB 92.2 / 72.2 / 81.0 91.4 / 87.6 / 89.4 92.2 / 77.4 / 84.2
3 PART 83.9 / 80.6 / 82.2 87.3 / 86.4 / 86.8 81.2 / 77.0 / 79.0
4 without CSD 78.0 / 85.1 / 81.4 89.2 / 93.6 / 91.3 84.4 / 83.7 / 84.1
5 Indicator with CSSDB 79.0 / 84.1 / 81.4 94.8 / 86.6 / 91.1 85.2 / 85.5 / 85.3
6 Selection with learnt CSD 83.1 / 83.2 / 83.2 91.7 / 92.9 / 92.3 91.7 / 85.2 / 88.3
7 after 3 iterations 82.4 / 86.8 / 84.6 92.6 / 95.4 / 94.0 92.5 / 84.0 / 88.0
8 after 10 iterations 82.4 / 86.8 / 84.6 92.7 / 95.4 / 94.0 92.5 / 84.0 / 88.0
9 Baseline MaxEnt with learnt CSD 89.9 / 77.0 / 83.0 91.9 / 90.4 / 91.1 95.0 / 78.7 / 86.1
training corpus. Increasing the training size may be
a simple way to overcome this shortcoming. Note
that increasing the number of labels (e.g. introduc-
ing more soccer clubs in the Soccer task) would also
directly increase the size of training dataset as we
use the occurrences of the indicator phrases belong-
ing to each of the labels for training a CSD. The so-
lution for the rare cases would require the explicit
handling of semantic relatedness (by utilising ex-
isting semantic resources or trying to automatically
identify task-specific relations).
5.2 Document Labeling Results
The second branch of experiments investigated the
added value of CSDs in information-oriented doc-
ument labeling tasks. Table 4 summarizes the re-
sults we got on the three datasets using the micro-
averaged F?=1 of assigned labels (positive class).
As baseline systems we trained binary SVMs
with a linear kernel, MaxEnts and PARTs ? a rule-
learner classification algorithm (Frank and Witten,
1998) ? for each label using the bag-of-word rep-
resentation of the documents (implementations of
SVMligth (Joachims, 1999), MALLET (McCallum,
2002) and WEKA (Witten and Frank, 1999) were
used). The first two learners are popular choices for
document classification, while the third is similar to
our simple indicator selection procedure. We did not
tuned the parameters of the classifiers, we used the
default ones everywhere.
To have a fair comparison, we applied to pre-
processing steps on dataset of these document clas-
sifiers. First, we removed from the training and
evaluation raw documents which were predicted to
be altered by CSSDB. Second, as our indicator
selection phrase can be regarded as a special fea-
ture selection method, we carried out an Information
Gain-based feature selection (keeping the 500 best-
rated features proved to be the best solution) on the
bag-of-word representation of the documents. The
effect of these two preprocessing steps varied among
datasets. It improved the F-score of the MaxEnt
baseline document classifier by 20%, 2% and 3% on
the Obesity, CMC and Soccer datasets, respectively
(the F-measures of Table 4 are the values we got by
employing pre-processing).
The indicator selection results presented in the
rows 4-8 of Table 4 made use of the p(+|f)-based
indicator selector with a five-fold-cross-validated
stopping threshold t (introduced in Section 4.3).
Row 4 contains the results of using the selected in-
dicators without any CSD. Indicator selection with
the CSSDB was applied for the 5th row. Rows 6-
8 of Table 4 show the results obtained after one,
three and ten iterations of the full learning algo-
rithm (see Algorithm 1). For training the CSD,
we employed MaxEnt as a binary classifier for de-
tecting altered local contexts and we used the
basic BoW feature representation for the clinical
tasks while the extended (BoW+syntactic) one for
the Soccer dataset.
In the final experiment (the last row of Table
4)) we investigated whether the learnt content shift
detector can be applied as a general ?document
cleaner? tool. For this, we trained the baseline Max-
Ent document classifier with feature selection on
documents from which the text spans predicted to
be altered by the learnt CSD in the tenth iter-
ation were removed. This means that the systems
767
used in row 2 and row 9 differ only in the applied
document cleaner pre-processing steps (the first one
applied the CSSDB while the latter one employed
the the learnt CSD).
The difference between the best baseline and the
indicator selector with learnt CSD and between the
best baseline and the document classifier with learnt
CSD were statistically significant8 on each dataset.
The difference between the predictions after the 1st
and 3rd iterations were statistically significant on
the CMC and the Obesity corpora but not signifi-
cant on the Soccer dataset. The difference between
the 3th and 10th iterations were not significant in ei-
ther case. Our co-learning method which integrated
the document-labeling and CSD tasks significantly
outperformed the baseline approaches ? which use
separate document cleaning and document labeling
steps ? on the three datasets.
On the clinical domains the automatically se-
lected indicators were disease names, symptom
names (e.g. high blood pressure), their spelling vari-
ants, synonyms (like hypertension) and their abbre-
viations (e.g htn). On the soccer domain club names,
synonyms (like The Saints) and stadium names (e.g.
Old Trafford) were selected. A label was indicated
by 3-4 indicator phrases.
Note that in these information-oriented docu-
ment multi-labeling tasks simple indicator selection-
based document labelers alone achieved results
comparable to the bag-of-words-based classifiers.
The learnt content shift detectors led to an average
improvement of 3.6% in the F-measure (i.e. a 24%
error reduction). The effect of further iterations is
various. As Table 4 shows, three iterations brought
an increase on the CMC and Obesity datasets but not
on the Soccer corpus. After a few iterations the set
of indicator phrases and the content shift detector
did not change substantially. The results achieved
by the MaxEnt document classifier employing the
?cleaned? training documents (last row of Table 4)
are significantly better (an average improvement of
1.9% in the F-measure and 12% error reduction)
than those by the CSSDB (row 2) but the indicator
selector approach performed even better.
8According to McNemar?s test with P-value of 0.001
6 Conclusions
In this paper, we dealt with information-oriented
document labeling tasks and investigated machine
learning approaches for local content shift detectors
from document-level labels. We demonstrated ex-
perimentally that a significant amount of false posi-
tive matches of indicator phrases can be recognized
by trained content shift detectors. Our trained CSD
does not use any task or domain specific knowledge
and exploits the false and true positive matches of in-
dicator phrases, i.e. it uses only document-level an-
notation. This task-independent approach achieved
competitive results with CSDs which were manually
fine-tuned for particular datasets. The empirical re-
sults also support the idea of generalized local CSD
(false positive removal) opposite to developing in-
dependent CSD for particular language phenomena
(like negation and speculation).
A co-learning framework for training local con-
tent shift detectors and indicator selection was in-
troduced as well. Our method integrates document
classification and CSD learning, which are tradi-
tionally used as independent submodules of appli-
cations. Experiments on three information-oriented
document-labeling datasets ? from two application
areas ? with simple indicator selection and syntactic
parse-based content shifter learning were performed
and the results show a clear improvement over the
bag-of-word-based document classification baseline
approaches.
However, the proposed content shift detec-
tor learning approach is tailored for information-
oriented document labeling tasks, i.e. it performs
well when not too many and reliable indicator
phrases are present. In the future, we plan to in-
vestigate and extend the framework for the general
document classification task where many indicators
with complex relationships among them determine
the labels of a document but local content shifters
can play an important role.
Acknowledgements
This work was partially founded by the Research
Group on Artificial Intelligence of the Hungarian
Academy of Sciences. Richa?rd Farkas was also
funded by Deutsche Forschungsgemeinschaft grant
SFB 732.
768
References
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. TEXT2TABLE: Medical Text Summa-
rization System Based on Named Entity Recognition
and Modality Identification. In Proceedings of the
BioNLP 2009 Workshop, pages 185?192.
Wendy W. Chapman, David Chu, and John N. Dowling.
2007. Context: an algorithm for identifying contextual
features from clinical text. In Proceedings of the ACL
Workshop on BioNLP 2007, pages 81?88.
Richa?rd Farkas and Gyo?rgy Szarvas. 2008. Automatic
construction of rule-based icd-9-cm coding systems.
BMC Bioinformatics, 9(Suppl 3):S10.
Richa?rd Farkas, Gyo?rgy Szarvas, Istva?n Hegedu?s, At-
tila Alma?si, Veronika Vincze, Ro?bert Orma?ndi, and
Ro?bert Busa-Fekete. 2009. Semi-automated construc-
tion of decision rules to predict morbidities from clini-
cal texts. Journal of the American Medical Informatics
Association, 16:601?605.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceedings
of the Fourteenth Conference on Computational Natu-
ral Language Learning (CoNLL-2010): Shared Task,
pages 1?12.
Eibe Frank and Ian H. Witten. 1998. Generating accu-
rate rule sets without global optimization. In Proc. of
Fifteenth International Conference on Machine Learn-
ing, pages 144?151.
Carol Friedman, Philip O. Alderson, John H. M. Austin,
James J. Cimino, and Stephen B. Johnson. 1994. A
General Natural-language Text Processor for Clinical
Radiology. Journal of the American Medical Infor-
matics Association, 1(2):161?174.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ing Wikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176.
Thorsten Joachims, 1999. Making large-scale support
vector machine learning practical, pages 169?184.
MIT Press, Cambridge, MA, USA.
Halil Kilicoglu and Sabine Bergler. 2009. Syntactic De-
pendency Based Heuristics for Biological Event Ex-
traction. In Proceedings of the BioNLP Workshop
Companion Volume for Shared Task, pages 119?127.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 Shared Task on Event Extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st ACL,
pages 423?430.
Leah S. Larkey and W. Bruce Croft. 1995. Automatic as-
signment of icd9 codes to discharge summaries. Tech-
nical report.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proc. of Biolink
2004, Linking Biological Literature, Ontologies and
Databases (HLT-NAACL Workshop:), pages 17?24.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ben Medlock and Ted Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature.
In Proceedings of the ACL, pages 992?999, June.
Roser Morante, V. Van Asch, and A. van den Bosch.
2009. Joint memory-based learning of syntactic and
semantic dependencies in multiple languages. In Pro-
ceedings of CoNLL, pages 25?30.
Arzucan ?Ozgu?r and Dragomir R. Radev. 2009. Detect-
ing Speculations and their Scopes in Scientific Text.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1398?1407.
John P. Pestian, Chris Brew, Pawel Matykiewicz,
DJ Hovermale, Neil Johnson, K. Bretonnel Cohen, and
Wlodzislaw Duch. 2007. A shared task involving
multi-label classification of clinical free text. In Pro-
ceedings of the ACL Workshop on BioNLP 2007, pages
97?104.
Roser Sauri and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Peter Scho?nhofen. 2006. Identifying document topics
using the wikipedia category network. In Proceedings
of the 2006 IEEE/WIC/ACM International Conference
on Web Intelligence, pages 456?462.
Ille?s Solt, Domonkos Tikk, Viktor Ga?l, and Zsolt Tivadar
Kardkova?cs. 2009. Semantic classification of diseases
in discharge summaries using a context-aware rule-
based classifier. J. Am. Med. Inform. Assoc., 16:580?
584, jul.
Stephanie Strassel, Mark A. Przybocki, Kay Peterson,
Zhiyi Song, and Kazuaki Maeda. 2008. Linguis-
tic resources and evaluation techniques for evaluation
of cross-document automatic content extraction. In
LREC.
Gyo?rgy Szarvas. 2008. Hedge Classification in Biomed-
ical Texts with a Weakly Supervised Selection of Key-
words. In Proceedings of ACL-08, pages 281?289.
769
O. Uzuner, Ira Goldstein, Yuan Luo, and Isaac Kohane.
2008. Identifying Patient Smoking Status from Medi-
cal Discharge Records. Journal of American Medical
Informatics Association, 15(1):14?24.
Ozlem Uzuner. 2009. Recognizing obesity and comor-
bidities in sparse data. Journal of American Medical
Informatics Association, 16(4):561?70.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope Corpus: Biomedical Texts Annotated for Un-
certainty, Negation and their Scopes. BMC Bioinfor-
matics, 9(Suppl 11):S9.
Ian H. Witten and Eibe Frank. 1999. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
770
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1038?1047, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Forest Reranking through Subtree Ranking
Richa?rd Farkas, Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart
{farkas,schmid}@ims.uni-stuttgart.de
Abstract
We propose the subtree ranking approach to
parse forest reranking which is a general-
ization of current perceptron-based reranking
methods. For the training of the reranker,
we extract competing local subtrees, hence
the training instances (candidate subtree sets)
are very similar to those used during beam-
search parsing. This leads to better param-
eter optimization. Another chief advantage
of the framework is that arbitrary learning to
rank methods can be applied. We evaluated
our reranking approach on German and En-
glish phrase structure parsing tasks and com-
pared it to various state-of-the-art reranking
approaches such as the perceptron-based for-
est reranker. The subtree ranking approach
with a Maximum Entropy model significantly
outperformed the other approaches.
1 Introduction
Reranking has become a popular technique for
solving various structured prediction tasks, such
as phrase-structure (Collins, 2000) and depen-
dency parsing (Hall, 2007), semantic role labeling
(Toutanova et al 2008) and machine translation
(Shen et al 2004). The idea is to (re)rank candi-
dates extracted by a base system exploiting a rich
feature set and operating at a global (usually sen-
tence) level. Reranking achieved significant gains
over the base system in many tasks because it has
access to information/features which are not com-
putable in the base system. Reranking also outper-
forms discriminative approaches which try to han-
dle the entire candidate universe (cf. Turian et al
(2006)) because the base system effectively and ef-
ficiently filters out many bad candidates and makes
the problem tractable.
The standard approach for reranking is the n-best
list ranking procedure, where the base system ex-
tracts its top n global-level candidates with associ-
ated goodness scores that define an initial ranking.
Then the task is to rerank these candidates by us-
ing a rich feature set. The bottleneck of this ap-
proach is the small number of candidates consid-
ered. Compared to n-best lists, packed parse forests
encode more candidates in a compact way. For-
est reranking methods have been proposed, which
can exploit the richer set of candidates and they
have been successfully applied for phrase-structure
(Huang, 2008), dependency (Hayashi et al 2011)
parsing and machine translation (Li and Khudanpur,
2009) as well.
Huang (2008) introduced the perceptron-based
forest reranking approach. The core of the algo-
rithm is a beam-search based decoder operating on
the packed forest in a bottom-up manner. It follows
the assumption that the feature values of the whole
structure are the sum of the feature values of the lo-
cal elements and they are designed to the usage of
the perceptron update. Under these assumptions a
1-best Viterbi or beam-search decoder can be effi-
ciently employed at parsing and training time. Dur-
ing training, it decodes the 1-best complete parse
then it makes the perceptron update against the or-
acle parse, i.e. the perceptron is trained at the global
(sentence) level.
We propose here a subtree ranker approach
which can be regarded as a generalization of this for-
1038
est reranking procedure. In contrast to updating on
a single (sub)tree per sentence using only the 1-best
parse (perceptron-based forest reranking), the sub-
tree ranker exploits subtrees of all sizes from a sen-
tence and trains a (re)ranker utilising several deriva-
tions of the constituent in question. During parsing
we conduct a beam-search extraction by asking the
ranker to select the k best subtrees among the pos-
sible candidates of each forest node. The chief mo-
tivation for this approach is that in this way, train-
ing and prediction are carried out on similar local
candidate lists which we expect to be favorable to
the learning mechanism. We empirically prove that
the trained discriminative rankers benefit from hav-
ing access to a larger amount of subtree candidates.
Moreover, in this framework any kind of learning
to rank methods can be chosen as ranker, including
pair-wise and list-wise classifiers (Li, 2011).
The contributions of this paper are the following:
? We extend the perceptron-based forest
rerankers to the subtree ranker forest reranking
framework which allows to replace the per-
ceptron update by any kind of learning to rank
procedure.
? We report experimental results on German
and English phrase-structure parsing compar-
ing subtree rerankers to various other rerankers
showing a significant improvement over the
perceptron-based forest reranker approach.
2 Related Work
Our method is closely related to the work of Huang
(2008), who introduced forest-based reranking for
phrase structure parsing. The proposed frame-
work can be regarded as an extension of this ap-
proach. It has several advantages compared with
the perceptron-based forest reranker. In this paper
we focus on the most important one ? and briefly
discuss two others in Section 5 ? which is enabling
the use of any kind of learning to rank approaches.
While the perceptron is fast to train, other machine
learning approaches usually outperform it. Most of
the existing learning to rank approaches are built on
linear models and evaluate the candidates indepen-
dently of each other (such as MaxEnt (Charniak and
Johnson, 2005), SVMRank (Joachims, 2002), Soft-
Rank (Guiver and Snelson, 2008)). Thus the choice
of the learning method does not influence parsing
time. We believe that the real bottleneck of parsing
applications is parsing time and not training time.
On the other hand, they can learn a better model
(at the cost of higher training time) than the Per-
ceptron. In theory, we can imagine learning to rank
approaches which can not be reduced to the indi-
vidual scoring of candidates at prediction time, for
instance a decision tree-based pairwise ranker. Al-
though such methods would also fit into the general
subtree framework, they are not employed in prac-
tice (Li, 2011).
The subtree ranking approach is a generalization
of the perceptron-based approach. If the ranking
algorithm is the Averaged Perceptron, the parsing
algorithm reduces to perceptron-based forest pars-
ing. If the ?selection strategy? utilizes the base sys-
tem ranking and training starts with a filtering step
which keeps only candidate sets from the root node
of the forest we get the offline version of the training
procedure of the perceptron-based forest reranker of
Huang (2008).
As our approach is based on local ranking (local
update in the online learning literature), it is highly
related to early update which looks for the first lo-
cal decision point where the oracle parse falls out
from the beam. Early update was introduced by
Collins and Roark (2004) for incremental parsing
and adopted to forest reranking by Wang and Zong
(2011).
Besides phrase structure parsing, the forest
reranking approach was successfully applied for de-
pendency parsing as well. Hayashi et al(2011) in-
troduced a procedure where the interpolation of a
generative and a forest-based discriminative parser
is exploited.
From the algorithmic point of view, our approach
is probably most closely related to Searn (Daume?
et al 2009) and Magerman (1995) as we also em-
ploy a particular machine learned model for a se-
quence of local decisions. The topological order of
the parse forest nodes can form the ?sequence of
choices? of Searn. The biggest differences between
our approach and Searn are that we propose an ap-
proach employing beam search and the ?policy? is a
ranker in our framework instead of a multiclass clas-
sifier as there are no ?actions? here, instead we have
to choose from candidate sets in the forest reranking
1039
framework. In a wider sense, our approach can be
regarded ? like Searn ? as an Inverse Reinforcement
Learning approach where ?one is given an environ-
ment and a set of trajectories and the problem is to
find a reward function such that an agent acting opti-
mally with respect to the reward function would fol-
low trajectories that match those in the training set?
(Neu and Szepesva?ri, 2009). Neu and Szepesva?ri
(2009) introduced the top-down parsing Markov De-
cision Processes and experiment with several inverse
reinforcement learning methods. The forest rerank-
ing approaches are bottom-up parsers which would
require a new (non-straightforward) definition of a
corresponding Markov Decision Process.
3 Subtree Ranking-based Forest
Reranking
A packed parse forest is a compact representation
of possible parses for a given sentence. A forest has
the structure of a hypergraph, whose nodes V are the
elementary units of the underlying structured predic-
tion problem and the hyperedges E are the possible
deductive steps from the nodes. In this paper we
experimented with phrase-structure parse reranking.
In this framework nodes correspond to constituents
spanning a certain scope of the input sentence and a
hyperedge e links a parent node head(e) to its chil-
dren tails(e) (i.e. a hyperedge is a CFG rule in con-
text).
The forest is extracted from the chart of a base
PCFG parser, usually employing a heavy pruning
strategy. Then the goal of a forest reranker is to find
the best parse of the input sentence exploiting a fea-
ture representation of (sub)trees.
We sketch the parsing procedure of the subtree
ranker in Algorithm 1. It is a bottom-up beam-
search parser operating on the hypergraph. At each
node v we store the k best subtrees S(v) headed by
the node. The S(v) lists contain the k top-ranked
subtrees by the ranker R among the candidates in the
beam. The set of candidate subtrees at a node is the
union of the candidates at the different hyperedges.
The set of candidate subtrees at a certain hyperedge,
in turn, is formed by the Cartesian product ?S(vi)
of the k-best subtrees stored at the child nodes vi.
The final output of forest ranking is the 1-best sub-
tree headed by the goal node S1(vgoal).
Algorithm 1 Subtree Ranking
Require: ?V,E? forest, R ranker
for all v ? V in bottom-up topological order do
C ? ?
for all e ? E, head(e) = v do
C ? C ? (?S(vi)) , vi ? tails(e)
end for
S(v)? Rk(C)
end for
return S1(vgoal)
For training the ranker we propose to extract lo-
cal candidate lists from the forests which share the
characteristics of the candidates at parsing time. Al-
gorithm 2 depicts the training procedure of the sub-
tree ranker.
As forests sometimes do not contain the gold stan-
dard tree, we extract an oracle tree instead, which
is the closest derivable tree in the forest to the gold
standard tree (Collins, 2000). Then we optimize the
parser for ranking the oracle tree at the top. This pro-
cedure is beneficial to training since the objective is
a reachable state. In Algorithm 2, we extract the ora-
cle tree from the parses encoded in the forest ?V,E?i
for the ith training sentence, which is the tree with
the highest F-score when compared to the gold stan-
dard tree yi. For each of the training sentences we
calculate the oracle subtrees for each node {Ov} of
the corresponding parse forest. We follow the dy-
namic programming approach of Huang (2008) for
the extraction of the forest oracle. The goal of this
algorithm is to extract the full oracle tree, but as a
side product it calculates the best possible subtree
for all nodes including the nodes outside of the full
oracle tree as well.
After computing the oracle subtrees, we crawl
the forests bottom-up and extract a training instance
?C,Ov? at each node v which consists of the candi-
date set C and the oracle Ov at that node. The cre-
ation of candidate lists is exactly the same as it was
at parsing time. Then we create training instances
from each of the candidate lists and form the set of
subtrees S(v) which is stored for candidate extrac-
tion at the higher levels of the forest (later steps in
the training instance extraction).
A crucial design question is how to form the S(v)
sets during training, which is the task of the selection
1040
Algorithm 2 Subtree Ranker Training
Require: {?V,E?i, yi}N1 , SS selection strategy
T ? ?
for all i? 1...N do
O ? oracle extractor(?V,E?i, yi)
for all v ? Vi in bottom-up topological order
do
C ? ?
for all e ? E, head(e) = v do
C ? C ? (?S(vj)) , vj ? tails(e)
end for
T ? T ? ?C,Ov?
S(v)? SS(C,Ov)
end for
end for
R? train reranker(T )
return R
strategy SS. One possible solution is to keep the k
best oracle subtrees, i.e. the k subtrees closest to the
gold standard parse, which is analogous to using the
gold standard labels in Maximum Entropy Markov
Models for sequence labeling problems (we refer
this selection strategy as ?oracle subtree? later on).
The problem with this solution is that if the rankers
have been trained on the oracle subtrees potentially
leads to a suboptimal performance as the outputs of
the ranker at prediction time are noisy. Note that
this approach is not a classical beam-based decod-
ing anymore as the ?beam? is maintained according
to the oracle parses and there is no model which in-
fluences that. An alternative solution ? beam-based
decoding ? is to use a ranker model to extract the
S(v) set in training time as well. In the general
reranking approach, we assume that the ranking of
the base parser is reliable. So we store the k best
subtrees according to the base system in S(v) (the
?base system ranking? selection strategy). Note that
the general framework keeps this question open and
lets the implementations define a selection strategy
SS.
After extracting the training instances T we can
train an arbitrary ranker R offline. Note that the
extraction of candidate lists is exactly the same in
Algorithm 1 and 2 while the creation of Sv can be
different.
4 Experiments
We carried out experiments on English and German
phrase-structure reranking. As evaluation metric, we
used the standard evalb implementation of PAR-
SEVAL on every sentence without length limitation
and we start from raw sentences without gold stan-
dard POS tagging. As the grammatical functions of
constituents are important from a downstream ap-
plication point of view ? especially in German ? we
also report PARSEVAL scores on the conflation of
constituent labels and grammatical functions. These
scores are shown in brackets in Table 2.
4.1 Datasets
We used the Wall Street Journal subcorpus of the
Ontonotes v4.0 corpus (Weischedel et al 2011)1 for
English. As usual sections 2-21, 23 and 24 served as
training set (30,060 sentences), test set (1,640 sen-
tences), and development set (1,336 sentences), re-
spectively. Using the Ontonotes version enables us
to assess parser robustness. To this end, we eval-
uated our models also on the weblog subcorpus of
the Ontonotes v4.0 corpus which consists of 15,103
sentences.
For German we used the Tiger treebank (Brants
et al 2002). We take the first 40,474 sentences of
the Tiger treebank as training data, the next 5,000
sentences as development data, and the last 5,000
sentences as test data.
4.2 Implementation of the Generic Framework
We investigate the Averaged Perceptron and a Maxi-
mum Entropy ranker as the reranker R in the subtree
ranking framework. The Maximum Entropy ranker
model is optimized with a loss function which is
the negative log conditional likelihood of the ora-
cle trees relative to the candidate sets. In the case of
multiple oracles we optimize for the sum of the ora-
cle trees? posterior probabilities (Charniak and John-
son, 2005).
In our setup the parsing algorithm is identical
to the perceptron-based forest reranker of Huang
(2008) because both the Averaged Perceptron and
the Maximum Entropy rankers score the local sub-
tree candidates independently of each other using
1Note that it contains less sentences and a slightly modified
annotation schema than the Penn Treebank.
1041
a linear model. There is no need to compute the
global normalization constant of the Maximum En-
tropy model because we only need the ranking and
not the probabilities. Hence the difference is in how
to train the ranker model.
We experimented with both the ?oracle subtree?
and the ?base system ranking? selection strategies
(see Section 3).
4.3 Five Methods for Forest-based Reranking
We conducted comparative experiments employing
the proposed subtree ranking approach and state-of-
the-art methods for forest reranking. Note that they
are equivalent in parsing time as each of them uses
beam-search with a linear classifier, on the other
hand they are radically different in their training.
? The original perceptron-based forest reranker
of Huang (2008) (?perceptron with global train-
ing?).
? The same method employing the early-update
updating mechanism instead of the global up-
date. Wang and Zong (2011) reported a signif-
icant gain using this update over the standard
global update (?perceptron with early update?).
? Similar to learning a perceptron at the global
level and then applying it at local decisions,
we can train a Maximum Entropy ranker at the
global level utilizing the n-best full parse can-
didates of the base parser, then use this model
for local decision making. So we train the
standard n-best rerankers (Charniak and John-
son, 2005) and then apply them in the beam-
search-based Viterbi parser (?n-best list train-
ing?). Applying the feature weights adjusted in
this approach in the forest-based decoding out-
performs the standard n-best list decoding by
an F-score of 0.3 on the German dataset.
? The subtree ranker method using the Averaged
Perceptron reranker. This is different from the
?perceptron with global training? as we conduct
updates at every local decision point and we do
offline training (?subtree ranking by AvgPer?).
? The subtree ranker method using Maximum
Entropy training (?subtree ranking by Max-
Ent?).
We (re)implemented these methods and used the
same forests and the same feature sets for the com-
parative experiments.
4.4 Implementation Details
We used the first-stage PCFG parser of Charniak
and Johnson (2005) for English and BitPar (Schmid,
2004) for German. BitPar employs a grammar engi-
neered for German (for details please refer to Farkas
et al(2011)). These two parsers are state-of-the-art
PCFG parsers for English and German, respectively.
For German the base parser and the reranker oper-
ate on the conflation of constituent labels and gram-
matical functions. For English, we used the forest
extraction and pruning code of Huang (2008). The
pruning removes hyperedges where the difference
between the cost of the best derivation using this hy-
peredge and the cost of the globally best derivation
is above some threshold. For German, we used the
pruned parse forest of Bitpar (Schmid, 2004). Af-
ter computing the posterior probability of each hy-
peredge given the input sentence, Bitpar prunes the
parse forest by deleting hyperedges whose posterior
probability is below some threshold. (We used the
threshold 0.01).
We employed an Averaged Perceptron (for ?per-
ceptron with global training?, ?perceptron with early
update? and ?subtree ranking by AvgPer?) and a
Maximum Entropy reranker (for ?subtree ranking
by MaxEnt? and ?n-best list training?). For the per-
ceptron reranker, we used the Joshua implementa-
tion2. The optimal number of iterations was deter-
mined on the development set. For the Maximum
Entropy reranker we used the RankMaxEnt imple-
mentation of the Mallet package (McCallum, 2002)
modified to use the objective function of Charniak
and Johnson (2005) and we optimized the L2 regu-
larizer coefficient on the development set.
The beam-size were set to 15 (the value suggested
by Huang (2008)) during parsing and the training
of the ?perceptron with global training? and ?percep-
tron with early update? models. We used k = 3 for
training the ?subtree ranking by AvgPer? and ?sub-
tree ranking by MaxEnt? rankers (see Section 5 for
a discussion on this).
In the English experiments, we followed (Huang,
2http://joshua.sourceforge.net/Joshua/
1042
Tiger test WSJ dev WSJ test WB
base system (1-best) 76.84 (65.91) 89.29 88.63 81.86
oracle tree 90.66 (80.38) 97.31 97.30 94.18
Table 1: The lower and upper bounds for rerankers on the four evaluation datasets. The numbers in brackets refers to
evaluation with grammatical function labels on the German dataset.
Tiger test WSJ dev WSJ test WB
perceptron with global training 78.39 (67.79) 90.58 89.60 82.87
perceptron with early update 78.83 (68.05) 90.81? 90.01 83.03?
n-best list training 78.75 (68.04) 90.89 90.11 83.55
subtree ranking by AvgPer 78.54? (67.97?) 90.65? 89.97 83.04?
subtree ranking by MaxEnt 79.36 (68.72) 91.14 90.32 83.83
Table 2: The results achieved by various forest rerankers. The difference between the scores marked by ? and the
?perceptron with global training? were not statistically significant with p < 0.005 according to the the McNemar test.
All other results are statistically different from this baseline.
2008) and selectively re-implemented feature tem-
plates from (Collins, 2000) and Charniak and John-
son (2005). For German we re-implemented the
feature templates of Versley and Rehbein (2009)
which is the state-of-the-art feature set for German.
It consists of features constructed from the lexical-
ized parse tree and its typed dependencies along
with features based on external statistical informa-
tion (such as the clustering of unknown words ac-
cording to their context of occurrence and PP attach-
ment statistics gathered from the automatically POS
tagged DE-WaC corpus, a 1.7G words sample of the
German-language WWW). We filtered out rare fea-
tures which occurred in less than 10 forests (we used
the same non-tuned threshold for the English and
German training sets as well).
We also re-implemented the oracle extraction pro-
cedure of Huang (2008) and extended its convolu-
tion and translation operators for using the base sys-
tem score as tie breaker.
4.5 Results
Table 1 shows the results of the 1-best parse of the
base system and the oracle scores ? i.e. the lower
and upper bounds for the rerankers ? for the four
evaluation datasets used in our experiments. The
German and the weblog datasets are more difficult
for the parsers.
The following table summarizes the characteris-
tics of the subtree ranker?s training sample of the
German and English datasets by employing the ?or-
acle subtree? selection strategy:
Tiger train WSJ train
#candidate lists 266,808 1,431,058
avg. size of cand. lists 3.2 5.7
#features before filtering 2,683,552 22,164,931
#features after filtering 94,164 858,610
Table 3: The sizes of the subtree ranker training datasets
at k = 3.
Using this selection strategy the training dataset
is smaller than the training dataset of the n-best list
rankers ? where offline trainers are employed as well
? as the total number of candidates is similar (and
even less in the Tiger corpus) while there are fewer
firing features at the subtrees than at full trees.
Table 2 summarizes the results achieved by vari-
ous forest rerankers. Both subtree rankers used the
oracle subtrees as the selection strategy of Algo-
rithm 2. The ?subtree ranking by MaxEnt? method
significantly outperformed the perceptron-based for-
est reranking algorithms at each of the datasets and
seems to be more robust as its advantage on the out-
domain data ?WB? is higher compared with the in-
domain ?WSJ? datasets. The early update improves
the perceptron based forest rerankers which is in line
with the results reported by Wang and Zong (2011).
The ?n-best list training? method works surprisingly
well. It outperforms both perceptron-based forest
1043
rerankers on the English datasets (while achieving a
smaller F-score than the perceptron with early up-
date on the Tiger corpus) which demonstrates the
potential of utilizing larger candidate lists for dis-
criminative training of rerankers. The comparison of
the ?subtree ranking by AvgPer? row and the ?subtree
ranking by MaxEnt? row shows a clear advantage of
the Maximum Entropy training mechanism over the
Averaged Perceptron.
Besides the ?oracle subtree? selection strategy we
also experimented with the ?base system ranking?
selection strategy with subtree Maximum Entropy
ranker. Table 4 compares the accuracies of the two
strategies. The difference between the two strate-
gies varies among datasets. In the German dataset,
they are competitive and the prediction of grammati-
cal functions benefits from the ?base system ranking?
strategy, while it performs considerably worse at the
English datasets.
Tiger test WSJ test WB
oracle SS 79.36 (68.72) 90.32 83.83
base sys SS 79.34 (68.84) 89.97 83.34
Table 4: The results of the two selection strategies. Using
the oracle trees proved to be better on each of the datasets.
Extracting candidate lists from each of the local
decision points might seem to be redundant. To gain
some insight into this question, we investigated the
effect of training instance filtering strategies on the
Tiger treebank. We removed the training instances
from the training sample T where the F-score of
the oracle (sub)tree against the gold standard tree is
less than a certain threshold (this data selection pro-
cedure was inspired by Li and Khudanpur (2008)).
The idea behind this data selection is to eliminate
bad training examples which might push the learner
into the wrong direction. Figure 1 depicts the results
on the Tiger treebank as a function of this data se-
lection threshold.
With this data selection strategy we could further
gain 0.22 F-score percentage points achieving 79.58
(68.87) and we can conclude that omitting candidate
sets far from the gold-standard tree helps training.
Figure 1 also shows that too strict filtering hurts the
performance. The result with threshold=90 is worse
than the result without filtering. We should note
that similar data selection methods can be applied
0 20 40 60 8079.2
5
79.3
5
79.4
5
79.5
5
data filtering threshold (F?score)
PAR
SEVA
L F?s
core
Figure 1: The effect of data selection on the Tiger test set.
to each of the baseline systems and the comparison
to them would be fair with conducting that. Thus
we consider our results without data selection to be
final.
5 Discussion
We experimentally showed in the previous section
that the subtree forest reranking approach with Max-
imum Entropy models significantly outperforms the
perceptron-based forest reranking approach. This
improvement must be the result of differences in the
training algorithms because there is no difference
between the two approaches at parse time, as we dis-
cussed in Section 4.2.
There are two sources of these improvements.
(i) We use local subtrees as training instances in-
stead of using the global parses exclusively. The
most important difference between the training of
the perceptron-based forest reranker and the subtree
forest reranker is that we train on subtrees (extract
candidate sets) outside of the Viterbi parses as well,
i.e. our intuition is that the training of the discrimi-
native model can benefit from seeing good and bad
subtrees far from the best parses as well. (ii) The
subtree ranker framework enables us to employ the
Maximum Entropy ranker on multiple candidates,
which usually outperforms the Averaged Perceptron.
The results of Table 2 can be considered as two
paths from the ?perceptron with global training?
to the ?subtree ranking by MaxEnt? applying these
1044
sources of improvements. If we use (i) and stay with
the Averaged Perceptron as learning algorithm we
get ?subtree ranking by AvgPer?. If we additionally
replace the Averaged Perceptron by Maximum En-
tropy ? i.e. follow (ii) ? we arrive at ?subtree ranking
by MaxEnt?. On the other hand, the ?n-best training?
uses global trees and Maximum Entropy for train-
ing, so the reason of the difference between ?per-
ceptron with global training? and ?n-best training? is
(ii). Then we arrive at ?subtree ranking by MaxEnt?
by (i). This line of thoughts and the figures of Ta-
ble 2 indicate that the added value of (i) and (ii) are
similar in magnitude.
5.1 Error Analysis
For understanding the added value of the proposed
subtree ranking method, we manually investigated
sentences from the German development set and
compared the parses of the ?perceptron with global
training? with the ?subtree ranking by MaxEnt?. We
could not found any linguistic phenomena which
was handled clearly better by the subtree ranker3,
but it made considerably more fixes than errors in
the following cases:
? the attachment of adverbs,
? the unary branching verbal phrases and
? extremely short sentences which does not con-
tain any verb (fragments).
5.2 Novel Opportunities with the Subtree
Ranking Framework
A generalization issue of the subtree ranking ap-
proach is that it allows to use any kind of feature
representation and arbitrary aggregation of local
features. The basic assumption of training on the
global (sentence) level in the perceptron reranking
framework is that the feature vector of a subtree is
the sum of the feature vectors of the children and
the features extracted from the root of the subtree
in question. This decomposability assumption pro-
vides a fine framework in the case of binary features
which fire if a certain linguistic phenomenon occurs.
On the other hand, this is not straightforward in the
3We believe that this might be the case only if we would
introduce new information (e.g. features) for the system.
presence of real valued features. For example, Ver-
sley and Rehbein (2009) introduce real-valued fea-
tures for supporting German PP-attachment recogni-
tion ? the mutual information of noun and preposi-
tion co-occurrence estimated from a huge unlabeled
corpus ? and this single feature template (about 80
features) could achieve a gain of 1 point in phrase
structure parsing accuracy while the same improve-
ment can be achieved by several feature templates
and millions of binary features. The aggregation of
such feature values can be different from summing,
for instance the semantics of the feature can demand
averaging, minimum, maximum or introducing new
features etc. Another opportunity for extending cur-
rent approaches is to employ utility functions on top
of the sum of the binary feature values. Each of these
extensions fits into the proposed framework.
The subtree ranking framework also enables the
usage of different models at different kinds of
nodes. For example, different models can be trained
for ranking subtress headed by noun phrases and for
verb phrases. This is not feasible in the perceptron-
based forest ranker which sums up features and up-
dates feature weights at the sentence level while the
ranker R in Algorithm 2 can refer to several models
because we handle local decisions separately. This
approach would not hurt parsing speed as one par-
ticular model is asked at each node, but it multiplies
memory requirements. This is an approach which
the subtree ranking framework allows, but which
would not fit to the global level updates of the per-
ceptron forest rerankers.
As a first step in this direction of research we ex-
perimented with training three different Maximum
Entropy models using the same feature representa-
tion, the first only on candidate lists extracted from
noun phrase nodes, the second on verb phrase nodes
and the third on all nodes (i.e. the third model is
equivalent to the ?subtree MaxEnt? model). Then at
prediction time, we ask that model (out of the three)
which is responsible for ranking the candidates of
the current type of node. This approach performed
worse than the single model approach achieving an
F-scores of 79.24 (68.46) on the Tiger test dataset.
This negative results ? compared with 79.36 (68.72)
achieved by a single model ? is probably due to data
sparsity problems. The amount of training samples
for noun phrases is 6% of the full training sample
1045
and it seems that a better model can be learned from
a much bigger but more heterogeneous dataset.
5.3 On the Efficiency of Subtree Ranking
In subtree ranking, we extract a larger number
of training instances (candidate lists) than the
perceptron-based approach which extracts exactly
one instance from a sentence. Moreover, the can-
didate lists are longer than the perceptron-based ap-
proach (where 2 ?candidates? are compared against
each other). Training on this larger set (refer Table 3
for concrete figures) consumes more space and time.
In our implementation, we keep the whole train-
ing dataset in the memory. With this implementation
the whole training process (feature extraction, can-
didate extraction and training the Maximum Entropy
ranker) takes 3 hours and uses 10GB of memory at
k = 1 and it takes 20 hours and uses 60GB of mem-
ory at k = 3 ((Huang, 2008) reported 5.3 and 27.3
hours at beam-sizes of 1 and 15 respectively but it
used only 1.2GB of memory). The in-depth investi-
gation of the effect of k is among our future plans.
6 Conclusions
We presented a subtree ranking approach to parse
forest reranking, which is a generalization of current
reranking methods. The main advantages of our ap-
proach are: (i) The candidate lists used during train-
ing are very similar to those used during parsing,
which leads to better parameter optimization. (ii)
Arbitrary ranking methods can be applied in our ap-
proach. (iii) The reranking models need not to be
decomposable.
We evaluated our parse reranking approach on
German and English phrase structure parsing tasks
and compared it to various state-of-the-art rerank-
ing approaches such as the perceptron-based for-
est reranker (Huang, 2008). The subtree reranking
approach with a Maximum Entropy model signifi-
cantly outperformed the other approaches.
We conjecture two reasons for this result: (i) By
training on all subtrees instead of Viterbi parses or
n-best parses only, we use the available training
data more effectively. (ii) The subtree ranker frame-
work allows us to use a standard Maximum Entropy
learner in parse-forest training instead of the Percep-
tron, which is usually superior.
Acknowledgements
We thank Liang Huang to provide us the modi-
fied version of the Charniak parser, which output a
packed forest for each sentence along with his forest
pruning code.
This work was founded by the Deutsche
Forschungsgemeinschaft grant SFB 732, project D4.
1046
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, pages 24?41.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 173?180.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages 111?
118.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of the Seven-
teenth International Conference onMachine Learning,
ICML ?00, pages 175?182.
Hal Daume?, III, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75(3):297?325, June.
Richa`rd Farkas, Bernd Bohnet, and Helmut Schmid.
2011. Features for phrase-structure reranking from
dependency parses. In Proceedings of the 12th Inter-
national Conference on Parsing Technologies, pages
209?214.
John Guiver and Edward Snelson. 2008. Learning to
rank with softrank and gaussian processes. In Pro-
ceedings of the 31st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, SIGIR ?08, pages 259?266.
Keith Hall. 2007. K-best spanning tree parsing. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 392?399, June.
Katsuhiko Hayashi, Taro Watanabe, Masayuki Asahara,
and Yuji Matsumoto. 2011. Third-order varia-
tional reranking on packed-shared dependency forests.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1479?1488.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (KDD), pages
133?142.
Zhifei Li and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statistical
machine translation. In Proceedings of the 8th AMTA
conference, pages 133?142.
Z. Li and S. Khudanpur, 2009. GALE book chapter on
?MT From Text?, chapter Forest reranking for machine
translation with the perceptron algorithm.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 276?283, June.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Gergely Neu and Csaba Szepesva?ri. 2009. Training
parsers by inverse reinforcement learning. Machine
Learning, 77(2?3):303?337.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proceedings of Coling 2004, pages 162?168.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
177?184.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Joseph P. Turian, Benjamin Wellington, and I. Dan
Melamed. 2006. Scalable discriminative learning for
natural language parsing and translation. In NIPS,
pages 1409?1416.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137.
Zhiguo Wang and Chengqing Zong. 2011. Parse rerank-
ing based on higher-order lexical dependencies. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 1251?1259.
Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch
Marcus, Robert Belvin, Sameer Pradhan, Lance
Ramshaw, and Nianwen Xue, 2011. Handbook of Nat-
ural Language Processing and Machine Translation.,
chapter OntoNotes: A Large Training Corpus for En-
hanced Processing.
1047
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 55?65,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Dependency Parsing of Hungarian: Baseline Results and Challenges
Richa?rd Farkas1, Veronika Vincze2, Helmut Schmid1
1Institute for Natural Language Processing, University of Stuttgart
{farkas,schmid}@ims.uni-stuttgart.de
2Research Group on Artificial Intelligence, Hungarian Academy of Sciences
vinczev@inf.u-szeged.hu
Abstract
Hungarian is a stereotype of morpholog-
ically rich and non-configurational lan-
guages. Here, we introduce results on de-
pendency parsing of Hungarian that em-
ploy a 80K, multi-domain, fully manu-
ally annotated corpus, the Szeged Depen-
dency Treebank. We show that the results
achieved by state-of-the-art data-driven
parsers on Hungarian and English (which is
at the other end of the configurational-non-
configurational spectrum) are quite simi-
lar to each other in terms of attachment
scores. We reveal the reasons for this and
present a systematic and comparative lin-
guistically motivated error analysis on both
languages. This analysis highlights that ad-
dressing the language-specific phenomena
is required for a further remarkable error re-
duction.
1 Introduction
From the viewpoint of syntactic parsing, the lan-
guages of the world are usually categorized ac-
cording to their level of configurationality. At one
end, there is English, a strongly configurational
language while Hungarian is at the other end of
the spectrum. It has very few fixed structures
at the sentence level. Leaving aside the issue of
the internal structure of NPs, most sentence-level
syntactic information in Hungarian is conveyed
by morphology, not by configuration (E?. Kiss,
2002).
A large part of the methodology for syntactic
parsing has been developed for English. How-
ever, parsing non-configurational and less config-
urational languages requires different techniques.
In this study, we present results on Hungarian de-
pendency parsing and we investigate this general
issue in the case of English and Hungarian.
We employed three state-of-the-art data-driven
parsers (Nivre et al 2004; McDonald et al 2005;
Bohnet, 2010), which achieved (un)labeled at-
tachment scores on Hungarian not so different
from the corresponding English scores (and even
higher on certain domains/subcorpora). Our in-
vestigations show that the feature representation
used by the data-driven parsers is so rich that they
can ? without any modification ? effectively learn
a reasonable model for non-configurational lan-
guages as well.
We also conducted a systematic and compar-
ative error analysis of the system?s outputs for
Hungarian and English. This analysis highlights
the challenges of parsing Hungarian and sug-
gests that the further improvement of parsers re-
quires special handling of language-specific phe-
nomena. We believe that some of our findings
can be relevant for intermediate languages on the
configurational-non-configurational spectrum.
2 Chief Characteristics of the
Hungarian Morphosyntax
Hungarian is an agglutinative language, which
means that a word can have hundreds of word
forms due to inflectional or derivational affixa-
tion. A lot of grammatical information is encoded
in morphology and Hungarian is a stereotype of
morphologically rich languages. The Hungarian
word order is free in the sense that the positions
of the subject, the object and the verb are not fixed
within the sentence, but word order is related to
information structure, e.g. new (or emphatic) in-
formation (the focus) always precedes the verb
55
and old information (the topic) precedes the focus
position. Thus, the position relative to the verb
has no predictive force as regards the syntactic
function of the given argument: while in English,
the noun phrase before the verb is most typically
the subject, in Hungarian, it is the focus of the
sentence, which itself can be the subject, object
or any other argument (E?. Kiss, 2002).
The grammatical function of words is deter-
mined by case suffixes as in gyerek ?child? ? gye-
reknek (child-DAT) ?for (a/the) child?. Hungarian
nouns can have about 20 cases1 which mark the
relationship between the head and its arguments
and adjuncts. Although there are postpositions
in Hungarian, case suffixes can also express re-
lations that are expressed by prepositions in En-
glish.
Verbs are inflected for person and number and
the definiteness of the object. Since conjugational
information is sufficient to deduce the pronominal
subject or object, they are typically omitted from
the sentence: Va?rlak (wait-1SG2OBJ) ?I am wait-
ing for you?. This pro-drop feature of Hungar-
ian leads to the fact that there are several clauses
without an overt subject or object.
Another peculiarity of Hungarian is that the
third person singular present tense indicative form
of the copula is phonologically empty, i.e. there
are apparently verbless sentences in Hungarian:
A ha?z nagy (the house big) ?The house is big?.
However, in other tenses or moods, the copula
is present as in A ha?z nagy lesz (the house big
will.be) ?The house will be big?.
There are two possessive constructions in
Hungarian. First, the possessive relation is only
marked on the possessed noun (in contrast, it is
marked only on the possessor in English): a fiu?
kutya?ja (the boy dog-POSS) ?the boy?s dog?. Sec-
ond, both the possessor and the possessed bear a
possessive marker: a fiu?nak a kutya?ja (the boy-
DAT the dog-POSS) ?the boy?s dog?. In the latter
case, the possessor and the possessed may not be
adjacent within the sentence as in A fiu?nak la?tta a
kutya?ja?t (the boy-DAT see-PAST3SGOBJ the dog-
POSS-ACC) ?He saw the boy?s dog?, which results
in a non-projective syntactic tree. Note that in
the first case, the form of the possessor coincides
1Hungarian grammars and morphological coding sys-
tems do not agree on the exact number of cases, some rare
suffixes are treated as derivational suffixes in one grammar
and as case suffixes in others; see e.g. Farkas et al(2010).
with that of a nominative noun while in the second
case, it coincides with a dative noun.
According to these facts, a Hungarian parser
must rely much more on morphological analysis
than e.g. an English one since in Hungarian it
is morphemes that mostly encode morphosyntac-
tic information. One of the consequences of this
is that Hungarian sentences are shorter in terms
of word numbers than English ones. Based on
the word counts of the Hungarian?English paral-
lel corpus Hunglish (Varga et al 2005), an En-
glish sentence contains 20.5% more words than its
Hungarian equivalent. These extra words in En-
glish are most frequently prepositions, pronomi-
nal subjects or objects, whose parent and depen-
dency label are relatively easy to identify (com-
pared to other word classes). This train of thought
indicates that the cross-lingual comparison of fi-
nal parser scores should be conducted very care-
fully.
3 Related work
We decided to focus on dependency parsing in
this study as it is a superior framework for non-
configurational languages. It has gained inter-
est in natural language processing recently be-
cause the representation itself does not require
the words inside of constituents to be consecu-
tive and it naturally represent discontinuous con-
structions, which are frequent in languages where
grammatical relations are often signaled by mor-
phology instead of word order (McDonald and
Nivre, 2011). The two main efficient approaches
for dependency parsing are the graph-based and
the transition-based parsers. The graph-based
models look for the highest scoring directed span-
ning tree in the complete graph whose nodes are
the words of the sentence in question. They solve
the machine learning problem of finding the opti-
mal scoring function of subgraphs (Eisner, 1996;
McDonald et al 2005). The transition-based ap-
proaches parse a sentence in a single left-to-right
pass over the words. The next transition in these
systems is predicted by a classifier that is based
on history-related features (Kudo and Matsumoto,
2002; Nivre et al 2004).
Although the available treebanks for Hungar-
ian are relatively big (82K sentences) and fully
manually annotated, the studies on parsing Hun-
garian are rather limited. The Szeged (Con-
stituency) Treebank (Csendes et al 2005) con-
56
sists of six domains ? namely, short business
news, newspaper, law, literature, compositions
and informatics ? and it is manually annotated
for the possible alternatives of words? morpho-
logical analyses, the disambiguated analysis and
constituency trees. We are aware of only two
articles on phrase-structure parsers which were
trained and evaluated on this corpus (Barta et al
2005; Iva?n et al 2007) and there are a few studies
on hand-crafted parsers reporting results on small
own corpora (Babarczy et al 2005; Pro?sze?ky et
al., 2004).
The Szeged Dependency Treebank (Vincze et
al., 2010) was constructed by first automatically
converting the phrase-structure trees into depen-
dency trees, then each of them was manually
investigated and corrected. We note that the
dependency treebank contains more information
than the constituency one as linguistic phenom-
ena (like discontinuous structures) were not anno-
tated in the former corpus, but were added to the
dependency treebank. To the best of our knowl-
edge no parser results have been published on this
corpus. Both corpora are available at www.inf.
u-szeged.hu/rgai/SzegedTreebank.
The multilingual track of the CoNLL-2007
Shared Task (Nivre et al 2007) addressed also
the task of dependency parsing of Hungarian. The
Hungarian corpus used for the shared task con-
sists of automatically converted dependency trees
from the Szeged Constituency Treebank. Several
issues of the automatic conversion tool were re-
considered before the manual annotation of the
Szeged Dependency Treebank was launched and
the annotation guidelines contained instructions
related to linguistic phenomena which could not
be converted from the constituency representa-
tion ? for a detailed discussion, see Vincze et al
(2010). Hence the annotation schemata of the
CoNLL-2007 Hungarian corpus and the Szeged
Dependency Treebank are rather different and the
final scores reported for the former are not di-
rectly comparable with our reported scores here
(see Section 5).
4 The Szeged Dependency Treebank
We utilize the Szeged Dependency Treebank
(Vincze et al 2010) as the basis of our experi-
ments for Hungarian dependency parsing. It con-
tains 82,000 sentences, 1.2 million words and
250,000 punctuation marks from six domains.
The annotation employs 16 coarse grained POS
tags, 95 morphological feature values and 29 de-
pendency labels. 19.6% of the sentences in the
corpus contain non-projective edges and 1.8% of
the edges are non-projective2, which is almost 5
times more frequent than in English and is the
same as the Czech non-projectivity level (Buch-
holz and Marsi, 2006). Here we discuss two an-
notation principles along with our modifications
in the dataset for this study which strongly influ-
ence the parsers? accuracies.
Named Entities (NEs) were treated as one to-
ken in the Szeged Dependency Treebank. Assum-
ing a perfect phrase recogniser on the whitespace
tokenised input for them is quite unrealistic. Thus
we decided to split them into tokens for this study.
The new tokens automatically got a proper noun
with default morphological features morphologi-
cal analysis except for the last token ? the head of
the phrase ?, which inherited the morphological
analysis of the original multiword unit (which can
contain various grammatical information). This
resulted in an N N N N POS sequence for Kova?cs
e?s ta?rsa kft. ?Smith and Co. Ltd.? which would
be annotated as N C N N in the Penn Treebank.
Moreover, we did not annotate any internal struc-
ture of Named Entities. We consider the last word
of multiword named entities as the head because
of morphological reasons (the last word of multi-
word units gets inflected in Hungarian) and all the
previous elements are attached to the succeeding
word, i.e. the penultimate word is attached to the
last word, the antepenultimate word to the penulti-
mate one etc. The reasons for these considerations
are that we believe that there are no downstream
applications which can exploit the information of
the internal structures of Named Entities and we
imagine a pipeline where a Named Entity Recog-
niser precedes the parsing step.
Empty copula: In the verbless clauses (pred-
icative nouns or adjectives) the Szeged Depen-
dency Treebank introduces virtual nodes (16,000
items in the corpus). This solution means that
a similar tree structure is ascribed to the same
sentence in the present third person singular and
all the other tenses / persons. A further argu-
ment for the use of a virtual node is that the vir-
tual node is always present at the syntactic level
2Using the transitive closure definition of Nivre and Nils-
son (2005).
57
corpus Malt MST Mate
ULA LAS ULA LAS ULA LAS
Hungarian
dev 88.3 (89.9) 85.7 (87.9) 86.9 (88.5) 80.9 (82.9) 89.7 (91.1) 86.8 (89.0)
test 88.7 (90.2) 86.1 (88.2) 87.5 (89.0) 81.6 (83.5) 90.1 (91.5) 87.2 (89.4)
English
dev 87.8 (89.1) 84.5 (86.1) 89.4 (91.2) 86.1 (87.7) 91.6 (92.7) 88.5 (90.0)
test 88.8 (89.9) 86.2 (87.6) 90.7 (91.8) 87.7 (89.2) 92.6 (93.4) 90.3 (91.5)
Table 1: Results achieved by the three parsers on the (full) Hungarian (Szeged Dependency Treebank) and
English (CoNLL-2009) datasets. The scores in brackets are achieved with gold-standard POS tagging.
since it is overt in all the other forms, tenses and
moods of the verb. Still, the state-of-the-art de-
pendency parsers cannot handle virtual nodes. For
this study, we followed the solution of the Prague
Dependency Treebank (Hajic? et al 2000) and vir-
tual nodes were removed from the gold standard
annotation and all of their dependents were at-
tached to the head of the original virtual node and
they were given a dedicated edge label (Exd).
Dataset splits: We formed training, develop-
ment and test sets from the corpus where each
set consists of texts from each of the domains.
We paid attention to the issue that a document
should not be separated into different datasets be-
cause it could result in a situation where a part of
the test document was seen in the training dataset
(which is unrealistic because of unknown words,
style and frequently used grammatical structures).
As the fiction subcorpus consists of three books
and the law subcorpus consists of two rules, we
took half of one of the documents for the test
and development sets and used the other part(s)
for training there. This principle was followed at
our cross-fold-validation experiments as well ex-
cept for the law subcorpus. We applied 3 folds for
cross-validation for the fiction subcorpus, other-
wise we used 10 folds (splitting at documentary
boundaries would yield a training fold consisting
of just 3000 sentences).3
5 Experiments
We carried out experiments using three state-of-
the-art parsers on the Szeged Dependency Tree-
bank (Vincze et al 2010) and on the English
datasets of the CoNLL-2009 Shared Task (Hajic?
et al 2009).
3Both the training/development/test and the cross-
validation splits are available at www.inf.u-szeged.
hu/rgai/SzegedTreebank.
Tools: We employed a finite state automata-
based morphological analyser constructed from
the morphdb.hu lexical resource (Tro?n et al
2006) and we used the MSD-style morphological
code system of the Szeged TreeBank (Alexin et
al., 2003). The output of the morphological anal-
yser is a set of possible lemma?morphological
analysis pairs. This set of possible morphologi-
cal analyses for a word form is then used as pos-
sible alternatives ? instead of open and closed tag
sets ? in a standard sequential POS tagger. Here,
we applied the Conditional Random Fields-based
Stanford POS tagger (Toutanova et al 2003) and
carried out 5-fold-cross POS training/tagging in-
side the subcorpora.4 For the English experiments
we used the predicted POS tags provided for the
CoNLL-2009 shared task (Hajic? et al 2009).
As the dependency parser we employed three
state-of-the-art data-driven parsers, a transition-
based parser (Malt) and two graph-based parsers
(MST and Mate parsers). The Malt parser (Nivre
et al 2004) is a transition-based system, which
uses an arc-eager system along with support vec-
tor machines to learn the scoring function for tran-
sitions and which uses greedy, deterministic one-
best search at parsing time. As one of the graph-
based parsers, we employed the MST parser (Mc-
Donald et al 2005) with a second-order feature
decoder. It uses an approximate exhaustive search
for unlabeled parsing, then a separate arc label
classifier is applied to label each arc. The Mate
parser (Bohnet, 2010) is an efficient second or-
der dependency parser that models the interaction
between siblings as well as grandchildren (Car-
reras, 2007). Its decoder works on labeled edges,
i.e. it uses a single-step approach for obtaining
labeled dependency trees. Mate uses a rich and
4The JAVA implementation of the morphological anal-
yser and the slightly modified POS tagger along with trained
models are available at http://www.inf.u-szeged.
hu/rgai/magyarlanc.
58
corpus #sent. length CPOS DPOS ULA all ULA LAS all LAS
newspaper 9189 21.6 97.2 96.5 88.0 (90.0) +0.8 84.7 (87.5) +1.0
short business 8616 23.6 98.0 97.7 93.8 (94.8) +0.3 91.9 (93.4) +0.4
fiction 9279 12.6 96.9 95.8 87.7 (89.4) -0.5 83.7 (86.2) -0.3
law 8347 27.3 98.3 98.1 90.6 (90.7) +0.2 88.9 (89.0) +0.2
computer 8653 21.9 96.4 95.8 91.3 (92.8) -1.2 88.9 (91.2) -1.6
composition 22248 13.7 96.7 95.6 92.7 (93.9) +0.3 88.9 (91.0) +0.3
Table 2: Domain results achieved by the Mate parser in cross-validation settings. The scores in brackets are
achieved with gold-standard POS tagging. The ?all? columns contain the added value of extending the training
sets with each of the five out-domain subcorpora.
well-engineered feature set and it is enhanced by
a Hash Kernel, which leads to higher accuracy.
Evaluation metrics: We apply the Labeled At-
tachment Score (LAS) and Unlabeled Attachment
Score (ULA), taking into account punctuation as
well for evaluating dependency parsers and the
accuracy on the main POS tags (CPOS) and a
fine-grained morphological accuracy (DPOS) for
evaluating the POS tagger. In the latter, the analy-
sis is regarded as correct if the main POS tag and
each of the morphological features of the token in
question are correct.
Results: Table 1 shows the results got by the
parsers on the whole Hungarian corpora and on
the English datasets. The most important point
is that scores are not different from the English
scores (although they are not directly compara-
ble). To understand the reasons for this, we man-
ually investigated the set of firing features with
the highest weights in the Mate parser. Although
the assessment of individual feature contributions
to a particular decoder decision is not straightfor-
ward, we observed that features encoding config-
urational information (i.e. the direction or length
of an edge, the words or POS tag sequences/sets
between the governor and the dependent) were
frequently among the highest weighted features
in English but were extremely rare in Hungarian.
For instance, one of the top weighted features for
a subject dependency in English was the ?there is
no word between the head and the dependent? fea-
ture while this never occurred among the top fea-
tures in Hungarian.
As a control experiment, we trained the Mate
parser only having access to the gold-standard
POS tag sequences of the sentences, i.e. we
switched off the lexicalization and detailed mor-
phological information. The goal of this experi-
ment was to gain an insight into the performance
of the parsers which can only access configura-
tional information. These parsers achieved worse
results than the full parsers by 6.8 ULA, 20.3 LAS
and 2.9 ULA, 6.4 LAS on the development sets
of Hungarian and English, respectively. As ex-
pected, Hungarian suffers much more when the
parser has to learn from configurational informa-
tion only, especially when grammatical functions
have to be predicted (LAS). Despite this, the re-
sults of Table 1 show that the parsers can practi-
cally eliminate this gap by learning from morpho-
logical features (and lexicalization). This means
that the data-driven parsers employing a very rich
feature set can learn a model which effectively
captures the dependency structures using feature
weights which are radically different from the
ones used for English.
Another cause of the relatively high scores is
that the CPOS accuracy scores on Hungarian
and English are almost equal: 97.2 and 97.3, re-
spectively. This also explains the small differ-
ence between the results got by gold-standard and
predicted POS tags. Moreover, the parser can
also exploit the morphological features as input
in Hungarian.
The Mate parser outperformed the other two
parsers on each of the four datasets. Comparing
the two graph-based parsers Mate and MST, the
gap between them was twice as big in LAS than in
ULA in Hungarian, which demonstrates that the
one-step approach looking for the maximum
labeled spanning tree is more suitable for Hun-
garian than the two-step arc labeling approach of
MST. This probably holds for other morpholog-
ically rich languages too as the decoder can ex-
ploit information from the labels of decoded arcs.
Based on these results, we decided to use only
Mate for our further experiments.
59
Table 2 provides an insight into the effect of
domain differences on POS tagging and pars-
ing scores. There is a noticeable difference be-
tween the ?newspaper? and the ?short business
news? corpora. Although these domains seem to
be close to each other at the first glance (both are
news), they have different characteristics. On the
one hand, short business news is a very narrow
domain consisting of 2-3 sentence long financial
short reports. It frequently uses the same gram-
matical structures (like ?Stock indexes rose X per-
cent at the Y Stock on Wednesday?) and the lexi-
con is also limited. On the other hand, the news-
paper subcorpus consists of full journal articles
covering various domains and it has a fancy jour-
nalist style.
The effect of extending the training dataset with
out-of-domain parses is not convincing. In spite
of the ten times bigger training datasets, there
are two subcorpora where they just harmed the
parser, and the improvement on other subcorpora
is less than 1 percent. This demonstrates well the
domain-dependence of parsing.
The parser and the POS tagger react to do-
main difficulties in a similar way, according to
the first four rows of Table 2. This observation
holds for the scores of the parsers working with
gold-standard POS tags, which suggests that do-
main difficulties harm POS tagging and parsing as
well. Regarding the two last subcorpora, the com-
positions consist of very short and usually simple
sentences and the training corpora are twice as big
compared with other subcorpora. Both factors are
probably the reasons for the good parsing perfor-
mance. In the computer corpus, there are many
English terms which are manually tagged with an
?unknown? tag. They could not be accurately pre-
dicted by the POS tagger but the parser could pre-
dict their syntactic role.
Table 2 also tells us that the difference between
CPOS and DPOS is usually less than 1 percent.
This experimentally supports that the ambigu-
ity among alternative morphological analyses
is mostly present at the POS-level and the mor-
phological features are efficiently identified by
our morphological analyser. The most frequent
morphological features which cannot be disam-
biguated at the word level are related to suffixes
with multiple functions or the word itself cannot
be unambiguously segmented into morphemes.
Although the number of such ambiguous cases is
low, they form important features for the parser,
thus we will focus on the more accurate handling
of these cases in future work.
Comparison to CoNLL-2007 results: The
best performing participant of the CoNLL-2007
Shared Task (Nivre et al 2007) achieved an ULA
of 83.6 and LAS of 80.3 (Hall et al 2007) on
the Hungarian corpus. The difference between the
top performing English and Hungarian systems
were 8.14 ULA and 9.3 LAS. The results reported
in 2007 were significantly lower and the gap be-
tween English and Hungarian is higher than our
current values. To locate the sources of difference
we carried out other experiments with Mate on
the CoNLL-2007 dataset using the gold-standard
POS tags (the shared task used gold-standard POS
tags for evaluation).
First we trained and evaluated Mate on the
original CoNLL-2007 datasets, where it achieved
ULA 84.3 and LAS 80.0. Then we used the sen-
tences of the CoNLL-2007 datasets but with the
new, manual annotation. Here, Mate achieved
ULA 88.6 and LAS 85.5, which means that the
modified annotation schema and the less erro-
neous/noisy annotation caused an improvement of
ULA 4.3 and LAS 5.5. The annotation schema
changed a lot: coordination had to be corrected
manually since it is treated differently after con-
version, moreover, the internal structure of ad-
jectival/participial phrases was not marked in the
original constituency treebank, so it was also
added manually (Vincze et al 2010). The im-
provement in the labeled attachment score is prob-
ably due to the reduction of the label set (from 49
to 29 labels), which step was justified by the fact
that some morphosyntactic information was dou-
bly coded in the case of nouns (e.g. ha?zzal (house-
INS) ?with the/a house?) in the original CoNLL-
2007 dataset ? first, by their morphological case
(Cas=ins) and second, by their dependency label
(INS).
Lastly, as the CoNLL-2007 sentences came
from the newspaper subcorpus, we can compare
these scores with the ULA 90.0 and LAS 87.5
of Table 2. The ULA 1.5 and LAS 2.0 differ-
ences are the result of the bigger training corpus
(9189 sentences on average compared to 6390 in
the CoNLL-2007 dataset).
60
Hungarian English
label attachment label attachment
virtual nodes 31.5% 39.5% multiword NEs 15.2% 17.6%
conjunctions and negation ? 11.2% PP-attachment ? 15.9%
noun attachment ? 9.6% non-canonical word order 6.4% 6.5%
more than 1 premodifier ? 5.1% misplaced clause ? 9.7%
coordination 13.5% 16.5% coordination 8.5% 12.5%
mislabeled adverb 16.3% ? mislabeled adverb 40.1% ?
annotation errors 10.7% 6.8% annotation errors 9.7% 8.5%
other 28.0% 11.3% other 20.1% 29.3%
TOTAL 100% 100% TOTAL 100% 100%
Table 3: The most frequent corpus-specific and general attachment and labeling error categories (based on a
manual investigation of 200?200 erroneous sentences).
6 A Systematic Error Analysis
In order to discover specialties and challenges of
Hungarian dependency parsing, we conducted an
error analysis of parsed texts from the newspaper
domain both in English and Hungarian. 200 ran-
domly selected erroneous sentences from the out-
put of Mate were investigated in both languages
and we categorized the errors on the basis of the
linguistic phenomenon responsible for the errors
? for instance, when an error occurred because of
the incorrect identification of a multiword Named
Entity containing a conjunction, we treated it as
a Named Entity error instead of a conjunction er-
ror ?, i.e. our goal was to reveal the real linguistic
sources of errors rather than deducing from auto-
matically countable attachment/labeling statistics.
We used the parses based on gold-standard
POS tagging for this analysis as our goal was to
identify the challenges of parsing independently
of the challenges of POS tagging. The error cate-
gories are summarized in Table 3 along with their
relative contribution to attachment and labeling
errors. This table contains the categories with
over 5% relative frequency.5
The 200 sentences contained 429/319 and
353/330 attachment/labeling errors in Hungarian
and English, respectively. In Hungarian, attach-
ment errors outnumber label errors to a great ex-
tent whereas in English, their distribution is basi-
cally the same. This might be attributed to the
higher level of non-projectivity (see Section 4)
and to the more fine-grained label set of the En-
glish dataset (36 against 29 labels in English and
5The full tables are available at www.inf.u-szeged.
hu/rgai/SzegedTreebank.
Hungarian, respectively).
Virtual nodes: In Hungarian, the most common
source of parsing errors was virtual nodes. As
there are quite a lot of verbless clauses in Hungar-
ian (see Section 2 on sentences without copula), it
might be difficult to figure out the proper depen-
dency relations within the sentence, since the verb
plays the central role in the sentence, cf. Tesnie`re
(1959). Our parser was not efficient in identify-
ing the structure of such sentences, probably due
to the lack of information for data-driven parsers
(each edge is labeled as Exd while they have sim-
ilar features to ordinary edges). We also note that
the output of the current system with Exd labels
does not contain too much information for down-
stream applications of parsing. The appropriate
handling of virtual nodes is an important direction
for future work.
Noun attachment: In Hungarian, the nomi-
nal arguments of infinitives and participles were
frequently erroneously attached to the main
verb. Take the following sentence: A Horn-
kabinet ideje?n jo?l beva?lt mo?dszerhez pro?ba?lnak
meg visszate?rni (the Horn-government time-
3SGPOSS-SUP well tried method-ALL try-3PL
PREVERB return-INF) ?They are trying to return
to the well-tried method of the Horn government?.
In this sentence, a Horn-kabinet ideje?n ?during
the Horn government? is a modifier of the past
participle beva?lt ?well-tried?, however, it is at-
tached to the main verb pro?ba?lnak ?they are try-
ing? by the parser. Moreover, mo?dszerhez ?to the
method? is an argument of the infinitive visszate?r-
ni ?to return?, but the parser links it to the main
61
verb. In free word order languages, the order of
the arguments of the infinitive and the main verb
may get mixed, which is called scrambling (Ross,
1986). This is not a common source of error in
English as arguments cannot scramble.
Article attachment: In Hungarian, if there is
an article before a prenominal modifier, it can be-
long to the head noun and to the modifier as well.
In a szoba ajtaja (the room door-3SGPOSS) ?the
door of the room? the article belongs to the modi-
fier but when the prenominal modifier cannot have
an article (e.g. a februa?rban indulo? projekt (the
February-INE starting project) ?the project start-
ing in February?), it is attached to the head noun
(i.e. to projekt ?project?). It was not always clear
for the parser which parent to select for the arti-
cle. In contrast, these cases are not problematic
in English since the modifier typically follows the
head and thus each article precedes its head noun.
Conjunctions or negation words ? most typ-
ically the words is ?too?, csak ?only/just? and
nem/sem ?not? ? were much more frequently at-
tached to the wrong node in Hungarian than in
English. In Hungarian, they are ambiguous be-
tween being adverbs and conjunctions and it is
mostly their conjunctive uses which are problem-
atic from the viewpoint of parsing. On the other
hand, these words have an important role in mark-
ing the information structure of the sentence: they
are usually attached to the element in focus posi-
tion, and if there is no focus, they are attached
to the verb. However, sentences with or with-
out focus can have similar word order but their
stress pattern is different. Dependency parsers
obviously cannot recognize stress patterns, hence
conjunctions and negation words are sometimes
erroneously attached to the verb in Hungarian.
English sentences with non-canonical word
order (e.g. questions) were often incorrectly
parsed, e.g. the noun following the main verb is
the object in sentences like Replied a salesman:
?Exactly.?, where it is the subject that follows the
verb for stylistic reasons. However, in Hungarian,
morphological information is of help in such sen-
tences, as it is not the position relative to the verb
but the case suffix that determines the grammati-
cal role of the noun.
In English, high or low PP-attachment was
responsible for many parsing ambiguities: most
typically, the prepositional complement which
follows the head was attached to the verb instead
of the noun or vice versa. In contrast, Hungarian
is a head-after-dependent language, which means
that dependents most often occur before the head.
Furthermore, there are no prepositions in Hungar-
ian, and grammatical relations encoded by prepo-
sitions in English are conveyed by suffixes or
postpositions. Thus, if there is a modifier before
the nominal head, it requires the presence of a
participle as in: Felvette a kirakatban levo? ruha?t
(take.on-PAST3SGOBJ the shop.window-INE be-
ing dress-ACC) ?She put on the dress in the shop
window?. The English sentence is ambiguous (ei-
ther the event happens in the shop window or the
dress was originally in the shop window) while
the Hungarian has only the latter meaning.6
General dependency parsing difficulties:
There were certain structures that led to typical
label and/or attachment errors in both languages.
The most frequent one among them is coordi-
nation. However, it should be mentioned that
syntactic ambiguities are often problematic even
for humans to disambiguate without contextual
or background semantic knowledge.
In the case of label errors, the relation between
the given node and its parent was labeled incor-
rectly. In both English and Hungarian, one of the
most common errors of this type was mislabeled
adverbs and adverbial phrases, e.g. locative ad-
verbs were labeled as ADV/MODE. However, the
frequency rate of this error type is much higher
in English than in Hungarian, which may be re-
lated to the fact that in the English corpus, there
is a much more balanced distribution of adverbial
labels than in the Hungarian one (where the cat-
egories MODE and TLOCY are responsible for
90% of the occurrences). Assigning the most fre-
quent label of the training dataset to each adverb
yields an accuracy of 82% in English and 93% in
Hungarian, which suggests that there is a higher
level of ambiguity for English adverbial phrases.
For instance, the preposition by may introduce an
adverbial modifier of manner (MNR) in by cre-
ating a bill and the agent in a passive sentence
(LGS). Thus, labeling adverbs seems to be a more
6However, there exists a head-before-dependent version
of the sentence (Felvette a ruha?t a kirakatban), whose pre-
ferred reading is ?She was in the shop window while dressing
up?, that is, the modifier belongs to the verb.
62
difficult task in English.7
Clauses were also often mislabeled in both lan-
guages, most typically when there was no overt
conjunction between clauses. Another source of
error was when more than one modifier occurred
before a noun (5.1% and 4.2% of attachment er-
rors in Hungarian and in English): in these cases,
the first modifier could belong to the noun (a
brown Japanese car) or to the second modifier (a
brown haired girl).
Multiword Named Entities: As we mentioned
in Section 4, members of multiword Named Enti-
ties had a proper noun POS-tag and an NE label
in our dataset. Hence when parsing is based on
gold standard POS-tags, their recognition is al-
most perfect while it is a frequent source or er-
rors in the CoNLL-2009 corpus. We investigated
the parse of our 200 sentences with predicted POS
tags at NEs and found that this introduces several
errors (about 5% of both attachment and labeling
errors) in Hungarian. On the other hand, the re-
sults are only slightly worse in English, i.e. iden-
tifying the inner structure of NEs does not depend
on whether the parser builds on gold standard or
predicted POS-tags since function words like con-
junctions or prepositions ? which mark grammat-
ical relations ? are tagged in the same way in both
cases. The relative frequency of this error type is
much higher in English even when the Hungar-
ian parser does not have access to the gold proper
noun POS tags. The reason for this is simple: in
the Penn Treebank the correct internal structure of
the NEs has to be identified beyond the ?phrase
boundaries? while in Hungarian their members
just form a chain.
Annotation errors: We note that our analysis
took into account only sentences which contained
at least one parsing error and we crawled only
the dependencies where the gold standard anno-
tation and the output of the parser did not match.
Hence, the frequency of annotation errors is prob-
ably higher than we found (about 1% of the en-
tire set of dependencies) during our investigation
as there could be annotation errors in the ?error-
free? sentences and also in the investigated sen-
tences where the parser agrees with that error.
7We would nevertheless like to point out that adverbial
labels have a highly semantic nature, i.e. it could be argued
that it is not the syntactic parser that should identify them but
a semantic processor.
7 Conclusions
We showed that state-of-the-art dependency
parsers achieve similar results ? in terms of at-
tachment scores ? on Hungarian and English. Al-
though the results with this comparison should be
taken with a pinch of salt ? as sentence lengths
(and information encoded in single words) differ,
domain differences and annotation schema diver-
gences are uncatchable ? we conclude that parsing
Hungarian is just as hard a task as parsing English.
We argued that this is due to the relatively good
POS tagging accuracy (which is a consequence
of the low ambiguity of alternative morphological
analyses of a sentence and the good coverage of
the morphological analyser) and the fact that data-
driven dependency parsers employ a rich feature
representation which enables them to learn differ-
ent kinds of feature weight profiles.
We also discussed the domain differences
among the subcorpora of the Szeged Dependency
Treebank and their effect on parsing results. Our
results support that there can be higher differences
in parsing scores among domains in one language
than among corpora from a similar domain but
different languages (which again marks pitfalls of
inter-language comparison of parsing scores).
Our systematic error analysis showed that han-
dling the virtual nodes (mostly empty copula) is
a frequent source of errors. We identified several
phenomena which are not typically listed as Hun-
garian syntax-specific features but are challeng-
ing for the current data-driven parsers, however,
they are not problematic in English (like the at-
tachment of conjunctions and negation words and
the attachment problem of nouns and articles).
We concluded ? based on our quantitative analy-
sis ? that a further notable error reduction is only
achievable if distinctive attention is paid to these
language-specific phenomena.
We intend to investigate the problem of vir-
tual nodes in dependency parsing in more depth
and to implement new feature templates for the
Hungarian-specific challenges as future work.
Acknowledgments
This work was supported in part by the Deutsche
Forschungsgemeinschaft grant SFB 732 and the
NIH grant (project codename MASZEKER) of
the Hungarian government.
63
References
Zolta?n Alexin, Ja?nos Csirik, Tibor Gyimo?thy, Ka?roly
Bibok, Csaba Hatvani, Ga?bor Pro?sze?ky, and La?szlo?
Tihanyi. 2003. Annotated Hungarian National Cor-
pus. In Proceedings of the EACL, pages 53?56.
Anna Babarczy, Ba?lint Ga?bor, Ga?bor Hamp, and
Andra?s Rung. 2005. Hunpars: a rule-based sen-
tence parser for Hungarian. In Proceedings of the
6th International Symposium on Computational In-
telligence.
Csongor Barta, Do?ra Csendes, Ja?nos Csirik, Andra?s
Ho?cza, Andra?s Kocsor, and Korne?l Kova?cs. 2005.
Learning syntactic tree patterns from a balanced
Hungarian natural language database, the Szeged
Treebank. In Proceedings of 2005 IEEE Interna-
tional Conference on Natural Language Processing
and Knowledge Engineering, pages 225 ? 231.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149?164.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 957?961.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and
Andra?s Kocsor. 2005. The Szeged Treebank. In
TSD, pages 123?131.
Katalin E?. Kiss. 2002. The Syntax of Hungarian.
Cambridge University Press, Cambridge.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics - Volume 1, COLING ?96, pages 340?
345.
Richa?rd Farkas, Da?niel Szeredi, Da?niel Varga, and
Veronika Vincze. 2010. MSD-KR harmoniza?cio? a
Szeged Treebank 2.5-ben [Harmonizing MSD and
KR codes in the Szeged Treebank 2.5]. In VII. Ma-
gyar Sza?m??to?ge?pes Nyelve?szeti Konferencia, pages
349?353.
Jan Hajic?, Alena Bo?hmova?, Eva Hajic?ova?, and Barbora
Vidova?-Hladka?. 2000. The Prague Dependency
Treebank: A Three-Level Annotation Scenario. In
Anne Abeille?, editor, Treebanks: Building and
Using Parsed Corpora, pages 103?127. Amster-
dam:Kluwer.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Depen-
dencies in Multiple Languages. In Proceedings of
the Thirteenth Conference on Computational Nat-
ural Language Learning (CoNLL 2009): Shared
Task, pages 1?18.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen
Eryigit, Bea?ta Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single Malt or Blended?
A Study in Multilingual Parser Optimization. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 933?939.
Szila?rd Iva?n, Ro?bert Orma?ndi, and Andra?s Kocsor.
2007. Magyar mondatok SVM alapu? szintaxis
elemze?se [SVM-based syntactic parsing of Hun-
garian sentences]. In V. Magyar Sza?m??to?ge?pes
Nyelve?szeti Konferencia, pages 281?283.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning - Volume 20, COLING-02, pages
1?7.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37:197?230.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings
of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 99?
106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-Based Dependency Parsing. In HLT-
NAACL 2004 Workshop: Eighth Conference
on Computational Natural Language Learning
(CoNLL-2004), pages 49?56.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task
on Dependency Parsing. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 915?932.
Ga?bor Pro?sze?ky, La?szlo? Tihanyi, and Ga?bor L. Ugray.
2004. Moose: A Robust High-Performance Parser
and Generator. In Proceedings of the 9th Workshop
of the European Association for Machine Transla-
tion.
John R. Ross. 1986. Infinite syntax! ABLEX, Nor-
wood, NJ.
Lucien Tesnie`re. 1959. E?le?ments de syntaxe struc-
turale. Klincksieck, Paris.
64
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, pages 173?180.
Viktor Tro?n, Pe?ter Hala?csy, Pe?ter Rebrus, Andra?s
Rung, Eszter Simon, and Pe?ter Vajda. 2006. Mor-
phdb.hu: Hungarian lexical database and morpho-
logical grammar. In Proceedings of 5th Inter-
national Conference on Language Resources and
Evaluation (LREC ?06).
Da?niel Varga, Pe?ter Hala?csy, Andra?s Kornai, Viktor
Nagy, La?szlo? Ne?meth, and Viktor Tro?n. 2005. Par-
allel corpora for medium density languages. In Pro-
ceedings of the RANLP, pages 590?596.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian Dependency Treebank. In Proceedings of the
Seventh Conference on International Language Re-
sources and Evaluation (LREC?10).
65
Cross-Genre and Cross-Domain Detection of
Semantic Uncertainty
Gyo?rgy Szarvas?
Technische Universita?t Darmstadt
Veronika Vincze??
Hungarian Academy of Sciences
Richa?rd Farkas?
Universita?t Stuttgart
Gyo?rgy Mo?ra?
University of Szeged
Iryna Gurevych?
Technische Universita?t Darmstadt
Uncertainty is an important linguistic phenomenon that is relevant in various Natural
Language Processing applications, in diverse genres from medical to community generated,
newswire or scientific discourse, and domains from science to humanities. The semantic un-
certainty of a proposition can be identified in most cases by using a finite dictionary (i.e., lexical
cues) and the key steps of uncertainty detection in an application include the steps of locating
the (genre- and domain-specific) lexical cues, disambiguating them, and linking them with the
units of interest for the particular application (e.g., identified events in information extraction).
In this study, we focus on the genre and domain differences of the context-dependent semantic
uncertainty cue recognition task.
We introduce a unified subcategorization of semantic uncertainty as different domain appli-
cations can apply different uncertainty categories. Based on this categorization, we normalized
the annotation of three corpora and present results with a state-of-the-art uncertainty cue
recognition model for four fine-grained categories of semantic uncertainty.
? Technische Universita?t Darmstadt, Ubiquitous Knowledge Processing (UKP) Lab, TU Darmstadt - FB 20
Hochschulstrasse 10, D-64289 Darmstadt, Germany. E-mail: {szarvas,gurevych}@tk.informatik
.tu-darmstadt.de.
?? Hungarian Academy of Sciences, Research Group on Artificial Intelligence, Tisza Lajos krt. 103,
6720 Szeged, Hungary. E-mail: vinczev@inf.u-szeged.hu.
? Universita?t Stuttgart, Institut fu?r Maschinelle Sprachverarbeitung. Azenbergstrasse 12, D-70174 Stuttgart,
Germany. E-mail: farkas@ims.uni-stuttgart.de.
? University of Szeged, Department of Informatics, A?rpa?d te?r 2, 6720 Szeged, Hungary.
E-mail: gymora@inf.u-szeged.hu.
Submission received: 6 April 2011; revised submission received: 1 October 2011; accepted for publication:
30 November 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
Our results reveal the domain and genre dependence of the problem; nevertheless, we
also show that even a distant source domain data set can contribute to the recognition
and disambiguation of uncertainty cues, efficiently reducing the annotation costs needed to
cover a new domain. Thus, the unified subcategorization and domain adaptation for training
the models offer an efficient solution for cross-domain and cross-genre semantic uncertainty
recognition.
1. Introduction
In computational linguistics, especially in information extraction and retrieval, it is
of the utmost importance to distinguish between uncertain statements and factual
information. In most cases, what the user needs is factual information, hence uncertain
propositions should be treated in a special way: Depending on the exact task, the
system should either ignore such texts or separate them from factual information. In
machine translation, it is also necessary to identify linguistic cues of uncertainty because
the source and the target language may differ in their toolkit to express uncertainty
(one language uses an auxiliary, the other uses just a morpheme). To cite another
example, in clinical document classification, medical reports can be grouped according
to whether the patient definitely suffers, probably suffers, or does not suffer from an
illness.
There are several linguistic phenomena that are referred to as uncertainty in the
literature. We consider propositions to which no truth value can be attributed, given
the speaker?s mental state, as instances of semantic uncertainty. In contrast, uncertainty
may also arise at the discourse level, when the speaker intentionally omits some infor-
mation from the statement, making it vague, ambiguous, or misleading. Determining
whether a given proposition is uncertain or not may involve using a finite dictionary of
linguistic devices (i.e., cues). Lexical cues (such as modal verbs or adverbs) are respon-
sible for semantic uncertainty whereas discourse-level uncertainty may be expressed by
lexical cues and syntactic cues (such as passive constructions) as well. We focus on four
types of semantic uncertainty in this study and henceforth the term cuewill be taken to
mean lexical cue.
The key steps of recognizing semantically uncertain propositions in a natural
language processing (NLP) application include the steps of locating lexical cues for
uncertainty, disambiguating them (as not all occurrences of the cues indicate uncer-
tainty), and finally linking them with the textual representation of the propositions
in question. The linking of a cue to the textual representation of the proposition can
be performed on the basis of syntactic rules that depend on the word class of the
lexical cue, but they are independent of the actual application domain or text type
where the cue is observed. The set of cues used and the frequency of their certain
and uncertain usages are domain and genre dependent, however, and this has to be
addressed if we seek to craft automatic uncertainty detectors. Here we interpret genre
as the basic style and formal characteristics of the writing that is independent of its topic
(e.g., scientific papers, newswire texts, or business letters), and domain as a particular
field of knowledge and is related to the topic of the text (e.g., medicine, archeology, or
politics).
Uncertainty cue candidates do not display uncertainty in all of their occurrences.
For instance, the mathematical sense of probable is dominant in mathematical texts
whereas its ratio can be relatively low in papers in the humanities. The frequency of
the two distinct meanings of the verb evaluate (which can be a synonym of judge [an
336
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
uncertain meaning] and calculate) is also different in the bioinformatics and cell biology
domains. Compare:
(1) To evaluateCUE the PML/RARalpha role in myelopoiesis, transgenic mice
expressing PML/RARalpha were engineered.
(2) Our method was evaluated on the Lindahl benchmark for fold recognition.
In this article we focus on the domain-dependent aspects of uncertainty detection
and we examine the recognition of uncertainty cues in context. We do not address the
problem of linking cues to propositions in detail (see, e.g., Chapman, Chu, and Dowling
[2007] and Kilicoglu and Bergler [2009] for the information extraction case).
For the empirical investigation of the domain dependent aspects, data sets are
required from various domains. To date, several corpora annotated for uncertainty have
been constructed for different genres and domains (BioScope, FactBank, WikiWeasel,
and MPQA, to name but a few). These corpora cover different aspects of uncertainty,
however, being grounded on different linguistic models, which makes it hard to exploit
cross-domain knowledge in applications. These differences in part stem from the varied
application needs across application domains. Different types of uncertainty and classes
of linguistic expressions are relevant for different domains. Although hypotheses and
investigations form a crucial part of the relevant cases in scientific applications, they
are less prominent in newswire texts, where beliefs and rumors play a major role. This
finding motivates a more fine-grained treatment of uncertainty. In order to bridge the
existing gaps between application goals, these typical cases need to be differentiated. A
fine-grained categorization enables the individual treatment of each subclass, which
is less dependent on domain differences than using one coarse-grained uncertainty
class. Moreover, this approach enables each particular application to identify and select
from a pool of models only those aspects of uncertainty that are relevant in the specific
domain.
As one of the main contributions of this study, we propose a uniform subcategoriza-
tion of semantic uncertainty in which all the previous corpus annotation works can be
placed, and which reveals the fundamental differences between the currently existing
resources. In addition, we manually harmonized the annotations of three corpora and
performed the fine-grained labeling according to the suggested subcategorization so as
to be able to perform cross-domain experiments.
An important factor in training robust cross-domain models is to focus on shallow
features that can be reliably obtained for many different domains and text types, and
to craft models that exploit the shared knowledge from different sources as much
as possible, making the adaptation to new domains efficient. The study of learning
efficient models across different domains is the subject of transfer learning and domain
adaptation research (cf. Daume? III and Marcu 2006; Pan and Yang 2010). The domain
adaptation setting assumes a target domain (for which an accurate model should be
learned with a limited amount of labeled training data), a source domain (with charac-
teristics different from the target and for which a substantial amount of labeled data is
available), and an arbitrary supervised learning model that exploits both the target and
source domain data in order to learn an improved target domain model.
The success of domain adaptation mainly depends on two factors: (i) the similarity
of the target and source domains (the two domains should be sufficiently similar to
allow knowledge transfer); and (ii) the application of an efficient domain adaptation
337
Computational Linguistics Volume 38, Number 2
method (which permits the learning algorithm to exploit the commonalities of the
domains while preserving the special characteristics of the target domain).
As our second main contribution, we study the impact of domain differences on
uncertainty detection, how this impact depends on the distance between target and
source domains concerning their domains and genres, and how these differences can
be reduced to produce accurate target domain models with limited annotation effort.
Because previously existing resources exhibited fundamental differences that made
domain adaptation difficult,1 to our knowledge this is the first study to analyze domain
differences and adaptability in the context of uncertainty detection in depth, and also
the first study to report consistently positive results in cross-training.
The main contributions of the current paper can be summarized as follows:
 We provide a uniform subcategorization of semantic uncertainty (with
definitions, examples, and test batteries for annotation) and classify all
major previous studies on uncertainty corpus annotation into the proposed
categorization system, in order to reveal and analyze the differences.
 We provide a harmonized, fine-grained reannotation of three corpora,
according to the suggested subcategorization, to allow an in-depth
analysis of the domain dependent aspects of uncertainty detection.
 We compare the two state-of-the-art approaches to uncertainty cue
detection (i.e., the one based on token classification and the one on
sequence labeling models), using a shared feature set, in the context of the
CoNLL-2010 shared task, to understand their strengths and weaknesses.2
 We train an accurate semantic uncertainty detector that distinguishes four
fine-grained categories of semantic uncertainty (epistemic, doxastic,
investigation, and condition types) and thus is better for future
applications in various domains than previous models. Our experiments
reveal that, similar to the best model of the CoNLL-2010 shared task for
biological texts but in a fine-grained context, shallow features provide
good results in recognizing semantic uncertainty. We also show that this
representation is less suited to detecting discourse-level uncertainty
(which was part of the CoNLL task for Wikipedia texts).
 We examine in detail the differences between domains and genres as
regards the language used to express semantic uncertainty, and learn how
the domain or genre distance affects uncertainty recognition in texts with
unseen characteristics.
 We apply domain adaptation techniques to fully exploit out-of-domain
data and minimize annotation costs to adapt to a new domain, and we
report successful results for various text domains and genres.
The rest of the paper is structured as follows. In Section 2, our classification of
uncertainty phenomena is presented in detail and it is compared with the concept of
1 Only 3 out of the more than 20 participants of the related CoNLL-2010 shared task (Farkas et al 2010)
managed to exploit out-of-domain data to improve their results, and only by a negligible margin.
2 The most successful CoNLL systems were based on these approaches, but different feature
representations make direct comparisons difficult.
338
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
uncertainty used in existing corpora. A framework for detecting semantic uncertainty
is then presented in Section 3. Relatedwork on cue detection is summarized in Section 4,
which is followed by a description of our cue recognition system and a presentation of
our experimental set-up using various source and target genre and domain pairs for
cross-domain learning and domain adaptation in Section 5. Our results are elaborated
on in Section 6 with a focus on the effect of domain similarities and on the annotation
effort needed to cover a new domain. We then conclude with a summary of our results
and make some suggestions for future research.
2. The Phenomenon Uncertainty
In order to be able to introduce and discuss our data sets, experiments, and findings,
we have to clarify our understanding of the term uncertainty. Uncertainty?in its most
general sense?can be interpreted as lack of information: The receiver of the information
(i.e., the hearer or the reader) cannot be certain about some pieces of information. In this
respect, uncertainty differs from both factuality and negation; as regards the former,
the hearer/reader is sure that the information is true and as for the latter, he is sure
that the information is not true. From the viewpoint of computer science, uncertainty
emerges due to partial observability, nondeterminism, or both (Russell and Norvig
2010). Linguistic theories usually associate the notion of modality with uncertainty:
Epistemic modality encodes how much certainty or evidence a speaker has for the
proposition expressed by his utterance (Palmer 1986) or it refers to a possible state of
the world in which the given proposition holds (Kiefer 2005). The common point in
these approaches is that in the case of uncertainty, the truth value/reliability of the
proposition cannot be decided because some other piece of information is missing.
Thus, uncertain propositions are those in our understanding whose truth value or
reliability cannot be determined due to lack of information.
In the following, we focus on semantic uncertainty and we suggest a tentative
classification of several types of semantic uncertainty. Our classification is grounded on
the knowledge of existing corpora and uncertainty recognition tools and our chief goal
here is to provide a computational linguistics-oriented classification. With this in mind,
our subclasses are intended to be well-defined and easily identifiable by automatic
tools. Moreover, this classification allows different applications to choose the subset of
phenomena to be recognized in accordance with their main task (i.e., we tried to avoid
an overly coarse or fine-grained categorization).
2.1 Classification of Uncertainty Types
Several corpora annotated for uncertainty have been published in different domains
such as biology (Medlock and Briscoe 2007; Kim, Ohta, and Tsujii 2008; Settles, Craven,
and Friedland 2008; Shatkay et al 2008; Vincze et al 2008; Nawaz, Thompson, and
Ananiadou 2010), medicine (Uzuner, Zhang, and Sibanda 2009), news media (Rubin,
Liddy, and Kando 2005; Wilson 2008; Saur?? and Pustejovsky 2009; Rubin 2010), and
encyclopedia (Farkas et al 2010). As can be seen from publicly available annotation
guidelines, there are many overlaps but differences as well in the understanding of
uncertainty, which is sometimes connected to domain- and genre-specific features of
the texts. Here we introduce a domain- and genre-independent classification of several
types of semantic uncertainty, which was inspired by both theoretical and computa-
tional linguistic considerations.
339
Computational Linguistics Volume 38, Number 2
Figure 1
Types of uncertainty. FB = FactBank; Genia = Genia Event; Rubin = the data set described
in Rubin, Liddy and Noriko (2005); META = the data set described in Nawaz, Thompson
and Ananiadou (2010); Medlock = the data set described in Medlock and Briscoe (2007);
Shatkay = the data set described in Shatkay et al (2008); Settles = the data set described in
Settles et al (2008).
2.1.1 A Tentative Classification. Based on corpus data and annotation principles, the
expression uncertainty can be used as an umbrella term for covering phenomena at the
semantic and discourse levels.3 Our classification of semantic uncertainty is assumed
to be language-independent, but our examples presented here come from the English
language, to keep matters simple.
Semantically uncertain propositions can be defined in terms of truth conditional
semantics. They cannot be assigned a truth value (i.e., it cannot be stated for sure
whether they are true or false) given the speaker?s current mental state.
Semantic level uncertainty can be subcategorized into epistemic and hypothetical
(see Figure 1). The main difference between epistemic and hypothetical uncertainty is
that whereas instances of hypothetical uncertainty can be true, false or uncertain, epis-
temically uncertain propositions are definitely uncertain?in terms of possible worlds,
hypothetical propositions allow that the proposition can be false in the actual world,
but in the case of epistemic uncertainty the factuality of the proposition is not known.
In the case of epistemic uncertainty, it is known that the proposition is neither true
nor false: It describes a possible world where the proposition holds but this possible
world does not coincide with the speaker?s actual world. In other words, it is certain
that the proposition is uncertain. Epistemic uncertainty is related to epistemic modality:
a sentence is epistemically uncertain if on the basis of our world knowledge we cannot
decide at the moment whether it is true or false (hence the name) (Kiefer 2005). The
source of an epistemically uncertain proposition cannot claim the uncertain proposition
and be sure about its opposite at the same time.
(3) EPISTEMIC: Itmay be raining.
3 The entire typology of semantic uncertainty phenomena and a test battery for their classification are
described in a supplementary file. Together with the corpora and the experimental software, they are
available at http://www.inf.u-szeged.hu/rgai/uncertainty.
340
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
As for hypothetical uncertainty, the truth value of the propositions cannot be
determined either and nothing can be said about the probability of their happening.
Propositions under investigation are an example of such statements: Until further anal-
ysis, the truth value of the proposition under question cannot be stated. Conditionals
can also be classified as instances of hypotheses. It is also common in these two types of
uncertain propositions that the speaker can utter them while it is certain (for others or
even for him) that its opposite holds hence they can be called instances of paradoxical
uncertainty.
Hypothetical uncertainty is connectedwith non-epistemic types ofmodality aswell.
Doxastic modality expresses the speaker?s beliefs?which may be known to be true or
false by others in the current state of the world. Necessity (duties, obligation, orders)
is the main objective of deontic modality; dispositional modality is determined by the
dispositions (i.e., physical abilities) of the person involved; and circumstantial modality
is defined by external circumstances. Buletic modality is related to wishes, intentions,
plans, and desires. An umbrella term for deontic, dispositional, circumstantial, and
buletic modality is dynamic modality (Kiefer 2005).
HYPOTHETICAL:
(4) DYNAMIC: I have to go.
(5) DOXASTIC: He believes that the Earth is flat.
(6) INVESTIGATION: We examined the role of NF-kappa B in protein activation.
(7) CONDITION: If it rains, we?ll stay in.
Conditions and instances of dynamic modality are related to the future: In the
future, they may happen but at the moment it is not clear whether they will take place
or not / whether they are true, false, or uncertain.
2.1.2 Comparison with other Classifications. The feasibility of the classification proposed in
this study can be justified by mapping the annotation schemes used in other existing
corpora to our subcategorizations of uncertainty. This systematic comparison also high-
lights the major differences between existing works and partly explains why examples
for successful cross-domain application of existing resources and models are hard to
find in the literature.
Most of the annotations found in biomedical corpora (Medlock and Briscoe 2007;
Settles, Craven, and Friedland 2008; Shatkay et al 2008; Thompson et al 2008; Nawaz,
Thompson, and Ananiadou 2010) fall into the epistemic uncertainty class. BioScope
(Vincze et al 2008) annotations mostly belong to the epistemic uncertainty category,
with the exception of clausal hypotheses (i.e., hypotheses that are expressed by a clause
headed by if or whether), which are instances of the investigation class. The probable
class of Genia Event (Kim, Ohta, and Tsujii 2008) is of the epistemically uncertain type
and the doubtful class belongs to the investigation class. Rubin, Liddy, and Kando (2005)
consider uncertainty as a phenomenon belonging to epistemicmodality: The high, mod-
erate, and low levels of certainty coincide with our epistemic uncertainty category. The
speculation annotations of the MPQA corpus also belong to the epistemic uncertainty
class, with four levels (Wilson 2008). The probable and possible classes found in FactBank
(Saur?? and Pustejovsky 2009) are of the epistemically uncertain type, events with a
generic source belong to discourse-level uncertainty, whereas underspecified events are
341
Computational Linguistics Volume 38, Number 2
classified as hypothetical uncertainty in our system as, by definition, their truth value
cannot be determined. WikiWeasel (Farkas et al 2010) contains annotation for epistemic
uncertainty, but discourse-level uncertainty is also annotated in the corpus (see Figure 1
for an overview). The categories used for themachine reading task described inMorante
and Daelemans (2011) also overlap with our fine-grained classes: Uncertain events in
their system fall into our epistemic uncertainty class. Their modal events expressing
purpose, need, obligation, or desire are instances of dynamic modality, whereas their
conditions are understood in a similar way to our condition class. The modality types
listed in Baker et al (2010) can be classified as types of dynamic modality, except for
their belief category. Instances of the latter category are either certain (It is certain that he
met the president) or epistemic or doxastic modality in our system.
2.2 Types of Semantic Uncertainty Cues
We assume that the nature of the lexical unit determines the type of uncertainty it
represents, that is, semantic uncertainty is highly lexical in nature. The part of speech of
the uncertainty cue candidates serves as the basis for categorization, similar to the ones
found in Hyland (1994, 1996, 1998) and Rizomilioti (2006). In English, modality is often
associated with modal auxiliaries (Palmer 1979), but, as Table 1 shows, there are many
other parts of speech that can express uncertainty. It should be added that there are
cues where it depends on the context, rather than the given lexical item, what subclass
of uncertainty the cue refers to, for example, may can denote epistemic modality (It may
rain. . . ) or dynamic modality (Now you may open the door). These categories are listed in
Table 1.
3. A Framework for Detecting Semantic Uncertainty
In our model, uncertainty detection is a standalone task that is largely independent of
the underlying application. In this section, we briefly discuss how uncertainty detection
Table 1
Uncertainty cues.
Adjectives / adverbs
probable, likely, possible, unsure, possibly, perhaps, etc. epistemic
Auxiliaries
may, might, can, would, should, could, etc. semantic
Verbs
speculative: suggest, question, seem, appear, favor, etc. epistemic
psych: think, believe, etc. doxastic
analytic: investigate, analyze, examine, etc. investigation
prospective: plan, want, order, allow, etc. dynamic
Conjunctions
if, whether, etc. investigation
Nouns
nouns derived speculation, proposal, consideration, etc. same as the verb
from uncertain verb:
other rumor, idea, etc. doxastic
uncertain nouns:
342
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
can be incorporated into an information extraction task, which is probably the most
relevant application area (see Kim et al [2009] for more details). In the information
extraction context, the key steps of recognizing uncertain propositions are locating the
cues, disambiguating them (as not all occurrences of the cues indicate uncertainty; recall
the example of evaluate), and finally linking them with the textual representation of
the propositions in question. We note here that marking the textual representations of
important propositions (often referred to as events in information extraction) is actually
the main goal of an information extraction system, hence we will not focus on their
identification and just assume that they are already marked in texts.
The following is an example that demonstrates the process of uncertainty detection:
(8) In this study we hypothesizedCUE that the phosphorylation of TRAF2
inhibitsEVENT binding to the CD40 cytoplasmic domain.
Here the EVENT mark-up is produced by the information extraction system, and
uncertainty detection consists of i) the recognition of the cue word hypothesized, and
determining whether it denotes uncertainty in this specific case (producing the CUE
mark-up) and ii) determining whether the cue hypothesizedmodifies the event triggered
by inhibits or not (positive example in this case).
3.1 Uncertainty Cue Detection and Disambiguation
The cue detection and disambiguation problem can be essentially regarded as a token
labeling problem. Here the task is to assign a label to each of the tokens of a sentence
in question according to whether it is the starting token of an uncertainty cue (B-
CUE TYPE), an inside token of a cue (I-CUE TYPE), or it is not part of any cue (O). Most
previous studies assume a binary classification task, namely, each token is either part of
an uncertainty cue, or it is not a cue. For fine-grained uncertainty detection, a different
label has to be used for each uncertainty type to be distinguished. This way, the label
sequence of a sentence naturally identifies all uncertainty cues (with their types) in the
sentence, and disambiguation is solved jointly with recognition.
Because the uncertainty cue vocabulary and the distribution of certain and uncer-
tain senses of cues vary in different domains and genres, uncertainty cue detection and
disambiguation are the main focus of the current study.
3.2 Linking Uncertainty Cues to Propositions
The task of linking the detected uncertainty cues to propositions can be formulated
as a binary classification task over uncertainty cue and event marker pairs. The relation
holds and is considered true if the cuemodifies the truth value (confidence) of the event;
it does not hold and is considered false if the cue does not have any impact on the
interpretation of the event. That is, the pair (hypothesized, inhibits) in Example (8) is an
instance of positive relation.
The linking of uncertainty cues and event markers can be established by using de-
pendency grammar rules (i.e., the problem is mainly syntax driven). As the grammatical
properties of the language are similar in various domains and genres, this task is largely
domain-independent, as opposed to the recognition and disambiguation task. Because
of this, we sketch the most important matching patterns, but do not address the linking
task in great detail here.
343
Computational Linguistics Volume 38, Number 2
The following are the characteristic rules that can be used to link uncertainty cues
to event markers. For practical implementations of heuristic cue/event matching, see
Chapman, Chu, and Dowling (2007) and Kilicoglu and Bergler (2009).
 If the event clue has an uncertain verb, noun, preposition, or auxiliary as a
(not necessarily direct) parent in the dependency graph of the sentence,
the event is regarded as uncertain.
 If the event clue has an uncertain adverb or adjective as its child, it is
treated as uncertain.
4. Related Work on Uncertainty Cue Detection
Herewe review the publishedworks related to uncertainty cue detection. Earlier studies
focused either on in-domain cue recognition for a single domain or on cue lexicon
extraction from large corpora. The latter approach is applicable tomultiple domains, but
does not address the disambiguation of uncertain and other meanings of the extracted
cue words. We are also aware of several studies that discussed the differences of cue
distributions in various domains, without developing a cue detector. To the best of
our knowledge, our study is the first to address the genre- and domain-adaptability of
uncertainty cue recognition systems and thus uncertainty detection in a general context.
We should add that there are plenty of studies on end-application oriented uncer-
tainty detection, that is, how to utilize the recognized cues (see, for instance, Kilicoglu
and Bergler [2008], Uzuner, Zhang, and Sibanda [2009] and Saur?? [2008] for information
extraction or Farkas and Szarvas [2008] for document labeling applications), and a
recent pilot task sought to exploit negation and hedge cue detectors in machine reading
(Morante and Daelemans 2011). As the focus of our paper is cue recognition, however,
we omit their detailed description here.
4.1 In-Domain Cue Detection
In-domain uncertainty detectors have been developed since the mid 1990s. Most of
these systems use hand-crafted lexicons for cue recognition and they treat each oc-
currence of the lexicon items as a cue?that is, they do not address the problem of
disambiguating cues (Friedman et al 1994; Light, Qiu, and Srinivasan 2004; Farkas and
Szarvas 2008; Saur?? 2008; Conway, Doan, and Collier 2009; Van Landeghem et al 2009).
ConText (Chapman, Chu, and Dowling 2007) uses regular expressions to define cues
and ?pseudo-triggers?. A pseudo-trigger is a superstring of a cue and it is basically
used for recognizing contexts where a cue does not imply uncertainty (i.e., it can be
regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and
Baldwin (2009) introduced a system which also used non-consecutive tokens as cues
(like not+as+yet).
Utilizing manually labeled corpora, machine learning?based uncertainty cue de-
tectors have also been developed (to the best of our knowledge each of them uses an
in-domain training data set). They use token classification (Morante and Daelemans
2009; Clausen 2010; Fernandes, Crestana, and Milidiu? 2010; Sa?nchez, Li, and Vogel
2010) or sequence labeling approaches (Li et al 2010; Rei and Briscoe 2010; Tang et al
2010; Zhang et al 2010). In both cases the tokens are labeled according to whether
they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of
344
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
tokens) thus it naturally deals with the context of a particular word. On the other hand,
context information for a token is built into the feature space of the token classification
approaches. O?zgu?r and Radev (2009) and Velldal (2010) match cues from a lexicon then
apply a binary classifier based on features describing the context of the cue candidate.
Each of these approaches uses a rich feature representation for tokens, which usu-
ally includes surface-level, part-of-speech, and chunk-level features. A few systems
have also used dependency relation types originating at the cue (Rei and Briscoe 2010;
Sa?nchez, Li, and Vogel 2010; Velldal, ?vrelid, and Oepen 2010; Zhang et al 2010); the
CoNLL-2010 Shared Task final ranking suggests that it has only a limited impact on the
performance of an entire system (Farkas et al 2010), however. O?zgu?r and Radev (2009)
further extended the feature set with the other cues that occur in the same sentence as
the cue, and positional features such as the section header of the article in which the cue
occurs (the latter is only defined for scientific publications). Velldal (2010) argues that
the dimensionality of the uncertainty cue detection feature space is too high and reports
improvements by using the sparse random indexing technique.
Ganter and Strube (2009) proposed a rather different approach for (weasel) cue
detection?exploiting weasel tags4 in Wikipedia articles given by editors. They used
syntax-based patterns to recognize the internal structure of the cues, which has proved
useful as discourse-level uncertainty cues are usually long and have a complex internal
structure (as opposed to semantic uncertainty cues).
As can be seen, uncertainty cue detectors have mostly been developed in the bio-
logical and medical domains. All of these studies, however, focus on only one domain,
namely, in-domain cue detection is carried out, which assumes the availability of a
training data set of sufficient size. The only exceptionwe are aware of is the CoNLL-2010
Shared Task (Farkas et al 2010), where participants had the chance to use Wikipedia
data on biomedical domain and vice versa. Probably due to the differences in the
annotated uncertainty types and the stylistic and topical characteristics of the texts,
very few participants performed cross-domain experiments and reported only limited
success (see Section 5.3.2 for more on this).
Overall, the findings of these studies indicate that disambiguating cue candidates is
an important aspect of uncertainty detection and that the domain specificity of disam-
biguation models and domain adaptation in general are largely unexplored problems
in uncertainty detection.
4.2 Weakly Supervised Extraction of Cue Lexicon
Similar to our approach, several studies have addressed the problem of developing
an uncertainty detector for a new domain using as little annotation effort as possible.
The aim of these studies is to identify uncertain sentences; this is carried out by semi-
automatic construction of cue lexicons. The weakly supervised approaches start with
very small seed sets of annotated certain and uncertain sentences, and use bootstrap-
ping to induce a suitable training corpus in an automatic way. Such approaches collect
potentially certain and uncertain sentences from a large unlabeled pool based on their
similarity to the instances in the seed sets (Medlock and Briscoe 2007), or based on the
known errors of an information extraction system that is itself sensitive to uncertain
texts (Szarvas 2008). Further instances are then collected (in an iterative fashion) on
the basis of their similarity to the current training instances. Based on the observation
4 See http://en.wikipedia.org/wiki/Wikipedia:Embrace_weasel_words.
345
Computational Linguistics Volume 38, Number 2
that uncertain sentences tend to contain more than one uncertainty cue, these models
successfully extend the seed sets with automatically labeled sentences, and can produce
an uncertainty classifier with a sentence-level F-score of 60?80% for the uncertain class,
given that the texts of the seed examples, the unlabeled pool, and the actual evaluation
data share very similar properties.
Szarvas (2008) showed that these models essentially learn the uncertainty lexicon
(set of cues) of the given domain, but are otherwise unable to disambiguate the potential
cue words?that is, to distinguish between the uncertain and certain uses of the previ-
ously seen cues. This deficiency of the derived models is inherent to the bootstrapping
process, which considers all occurrences of the cue candidates as good candidates for
positive examples (as opposed to unlabeled sentences without any previously seen cue
words).
Kilicoglu and Bergler (2008) proposed a semi-automatic method to expand a seed
cue lexicon. Their linguistically motivated approach is also based on the weakly super-
vised induction of a corpus of uncertain sentences. It exploits the syntactic patterns of
uncertain sentences to identify new cue candidates.
The previous studies on weakly supervised approaches to uncertainty detection
did not tackle the problem of disambiguating the certain and uncertain uses of cue
candidates, which is a major drawback from a practical point of view.
4.3 Cue Distribution Analyses
Besides automatic uncertainty recognition, several studies investigated the distribution
of hedge cues in scientific papers from different domains (Hyland 1998; Falahati 2006;
Rizomilioti 2006). The effect of different domains on the frequency of uncertain expres-
sions was examined in Rizomilioti (2006). Based on a previously defined dictionary of
hedge cues, she analyzed the linguistic tools expressing epistemic modality in research
papers from three domains, namely, archeology, literary criticism, and biology. Her
results indicated that archaeological papers tend to contain the most uncertainty cues
(which she calls downtoners) and the fewest uncertainty cues can be found in literary
criticism papers. Different academic disciplines were contrasted in Hyland (1998) from
the viewpoint of hedging: Papers belonging to the humanities contain significantly
more hedging devices than papers in sciences. It is interesting to note, however, that
in both studies, biological papers are situated in the middle as far as the percentage rate
of uncertainty cues is concerned. Falahati (2006) examined hedges in research articles in
medicine, chemistry, and psychology and concluded that it is psychology articles that
contain the most hedges.
Overall, these studies demonstrate that there are substantial differences in the way
different technical/scientific domains and different genres express uncertainty in gen-
eral, and in the use of semantic uncertainty in particular. Differences are found not just
in the use of different vocabulary for expressing uncertainty, but also in the frequency
of certain and uncertain usage of particular uncertainty cues. These findings underpin
the practical importance of domain portability and domain adaptation of uncertainty
detectors.
5. Uncertainty Cue Recognition
In this section, we present our uncertainty cue detector and the results of the cross-genre
and -domain experiments carried out by us. Before describing ourmodel and discussing
the results of the experiments, a short overview of the texts used as training and test
346
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
data sets will be given along with an empirical analysis of the sense distributions of the
most frequent cues.
5.1 Data Sets
In our investigations, we selected three corpora (i.e., BioScope, WikiWeasel, and Fact-
Bank) from different domains (biomedical, encyclopedia, and newswire, respectively).
Genres also vary in the corpora (in the scientific genre, there are papers and abstracts
whereas the other corpora contain pieces of news and encyclopedia articles). We pre-
ferred corpora on which earlier experiments had been carried out because this allowed
us to compare our results with those of previous studies. This selectionmakes it possible
to investigate domain and genre differences because each domain has its characteristic
language use (which might result in differences in cue distribution) and different genres
also require different writing strategies (e.g., in abstracts, implications of experimental
results are often emphasized, which usually involves the use of uncertain language).
The BioScope corpus (Vincze et al 2008) contains clinical texts as well as biological
texts from full papers and scientific abstracts; the texts were manually annotated for
hedge cues and their scopes. In our experiments, 15 other papers annotated for the
CoNLL-2010 Shared Task (Farkas et al 2010) were also added to the set of BioScope
papers. The WikiWeasel corpus (Farkas et al 2010) was also used in the CoNLL-2010
Shared Task and it was manually annotated for weasel cues and semantic uncertainty
in randomly selected paragraphs taken from Wikipedia articles. The FactBank corpus
contains texts from the newswire domain (Saur?? and Pustejovsky 2009). Events are
annotated in the data set and they are evaluated on the basis of their factuality from
the viewpoint of their sources.
Table 2 provides statistical data on the three corpora. Because in our experimental
set-up, texts belonging to different genres also play an important role, data on abstracts
and papers are included separately.
5.1.1 Genres and Domains. Texts found in the three corpora to be investigated can be
categorized into three genres, which can be further divided to subgenres at a finer level
of distinction. Figure 2 depicts this classification.
The majority of BioScope texts (papers and abstracts) belong to the scientific dis-
course genre. FactBank texts can be divided into broadcast and written news, and
Wikipedia texts belong to the encyclopedia genre.
As for the domain of the texts, there are three broad domains, namely, biology, news,
and encyclopedia. Once again, these domains can be further divided into narrower
Table 2
Data on the corpora. sent. = sentence; epist. = epistemic cue; dox. = doxastic cue;
inv. = investigation cue; cond. = condition cue.
Data Set #sent. #epist. #dox. #inv. #cond. Total
BioScope papers 7676 1373 220 295 187 2075
BioScope abstracts 11797 2478 200 784 24 3486
BioScope total 19473 3851 420 1079 211 5561
WikiWeasel 20756 1171 909 94 491 3265
FactBank 3123 305 201 36 178 720
Total 43352 5927 1530 1209 880 9546
347
Computational Linguistics Volume 38, Number 2
Figure 2
Genres of texts.
Figure 3
Domains of texts.
topics at a fine-grained level, which is shown in Figure 3. All abstracts and five papers
in BioScope are related to the MeSH terms human, blood cell, and transcription factor (hbc
in Figure 3). Nine BMC Bioinformatics papers come from the bioinformatics domain
(bmc in Figure 3), and ten papers describe some experimental results on the Drosophila
species (fly). FactBank news can be classified as stock news, political news, and
criminal news. Encyclopedia articles cover a broad range of topics, hence no detailed
classification is given here.
5.1.2 The Normalization of the Corpora. In order to uniformly evaluate our methods in
each domain and genre (and each corpus), the evaluation data sets were normalized.
This meant that cues had to be annotated in each data set and differentiated for types
of semantic uncertainty. This resulted in the reannotation of BioScope, WikiWeasel, and
FactBank.5 In BioScope, the originally annotated cues were separated into epistemic
cues and subtypes of hypothetical cues and instances of hypothetical uncertainty not
yet marked were also annotated. In FactBank, epistemic and hypothetical cues were
annotated: Uncertain events were matched with their uncertainty cues and instances
of hypothetical uncertainty that were originally not annotated were also marked in
the corpus. In the case of WikiWeasel, these two types of cues were separated from
discourse-level cues.
One class of hypothetical uncertainty (i.e., dynamic modality) was not annotated
in any of the corpora. Although dynamic modality seems to play a role in the news
domain, it is less important and less represented in the other two domains we investi-
gated here. The other subclasses are more of general interest for the applications. For
example, one of our training corpora comes from the scientific domain, where it is more
important to distinguish facts from hypotheses and propositions under investigation
5 The corpora are available at http://www.inf.u-szeged.hu/rgai/uncertainty.
348
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
(which can be later confirmed or rejected, compare the meta-knowledge annotation
scheme developed for biological events [Nawaz, Thompson, andAnaniadou 2010]), and
from propositions that depend on each other (conditions).
5.1.3 Uncertainty Cues in the Corpora. An analysis of the cue distributions reveals some
interesting trends that can be exploited in uncertainty detection across domains and
genres. The most frequent cue stems in the (sub)corpora used in our study can be seen
in Table 3 and they are responsible for about 74% of epistemic cue occurrences, 55% of
doxastic cue occurrences, 70% of investigation cue occurrences, and 91% of condition
cue occurrences.
As can be seen, one of the most frequent epistemic cues in each corpus is may. If,
possible, might, and suggest also occur frequently in our data set.
The distribution of the uncertainty cues was also analyzed from the perspective of
uncertainty classes in each corpus, which is presented in Figure 4. Inmost of the corpora,
epistemic cues are the most frequent (except for FactBank) and they vary the most:
Out of the 300 cue stems occurring in the corpora, 206 are epistemic cues. Comparing
the domains, it can readily be seen that in biological texts, doxastic uncertainty is not
frequent, which is especially true for abstracts, whereas in FactBank and WikiWeasel
they cover about 27% of the data. The most frequent doxastic keywords exhibit some
domain-specific differences, however: In BioScope, the most frequent ones include puta-
tive and hypothesis, which rarely occur in FactBank and WikiWeasel. Nevertheless, cues
belonging to the investigation class can be found almost exclusively in scientific texts
(89% of them are in BioScope), which can be expected because the aim of scientific pub-
lications is to examine whether a hypothesized phenomenon occurs. Among the most
Table 3
The most frequent cues in the corpora. epist. = epistemic cue; dox. = doxastic cue; inv. =
investigation cue; cond. = condition cue.
Global Abstracts Full papers BioScope FactBank WikiWeasel
Epist. may 1508 suggest 616 may 228 suggest 810 may 43 may 721
suggest 928 may 516 suggest 194 may 744 could 29 probable 112
indicate 421 indicate 301 indicate 103 indicate 404 possible 26 suggest 108
possible 304 appear 143 possible 84 appear 213 likely 24 possible 93
appear 260 or 119 might 83 or 197 might 23 likely 80
might 256 possible 101 or 78 possible 185 appear 15 might 78
likely 221 might 72 can 73 might 155 seem 11 seem 67
or 198 potential 72 appear 70 can 117 potential 10 could 55
could 196 likely 60 likely 57 likely 117 probable 10 perhaps 51
probable 157 could 56 could 56 could 112 suggest 10 appear 32
Dox. consider 276 putative 43 putative 37 putative 80 expect 75 consider 250
believe 222 think 43 hypothesis 33 hypothesis 77 believe 25 believe 173
expect 136 hypothesis 43 assume 24 think 66 think 24 allege 81
think 131 believe 14 think 24 assume 32 allege 8 think 61
putative 83 consider 10 expect 22 predict 26 accuse 7 regard 58
Invest. whether 247 investigate 177 whether 73 investigate 221 whether 26 whether 52
investigate 222 examine 160 investigate 44 examine 183 if 3 if 20
examine 183 whether 96 test 25 whether 169 remain to be seen 2 whether or not 7
study 102 study 88 examine 23 study 101 question 1 assess 3
determine 90 determine 67 determine 20 determine 87 determine 1 evaluate 3
Cond. if 418 if 14 if 85 if 99 if 65 if 254
would 238 would 6 would 46 would 52 would 50 would 136
will 80 until 2 will 20 will 20 will 21 will 39
until 40 could 1 should 11 should 11 until 16 until 15
could 30 unless 1 could 9 could 10 could 9 unless 14
349
Computational Linguistics Volume 38, Number 2
Figure 4
Cue type distributions in the corpora.
frequent cues, investigate, examine, and study belong to this group. These data reveal
that the frequency of doxastic and investigation cues is strongly domain-dependent,
and this explains the fact that the investigation vocabulary is very limited in Factbank
and WikiWeasel. Only about 10 cue stems belong to this uncertainty class in these cor-
pora. The set of condition cue stems, however, is very small in each corpus; altogether
18 condition cue stems can be found in the data, although if and would are responsible
for almost 75% of condition cue occurrences. It should also be mentioned that the
percentage of condition cues is higher in FactBank than in the other corpora.
Another interesting trend was observed when word forms were considered instead
of stemmed forms: Certain verbs in third person singular (e.g., expects or believes) occur
mostly in FactBank and WikiWeasel. The reason for this may be that when speaking
about someone else?s opinion in scientific discourse, the source of the opinion is usually
provided in the form of references or citations?usually at the end of the sentence?and
due to this, the verb is often used in the passive form, as in Example (9).
(9) It is currently believed that both RAG1 and RAG2 proteins were originally
encoded by the same transposon recruited in a common ancestor of jawed
vertebrates [3,12,13,16].
In contrast, impersonal constructions are hardly used in news media, where the ob-
jective is to inform listeners about the source of the news presented as well in order
to enable them to judge the reliability of a piece of news. Here, a clause including the
source and a communication verb is usually attached to the proposition.
A genre-related difference between scientific abstracts and full papers is that con-
dition cues can rarely be found in abstracts, although they occur more frequently in
papers (with the non-cue usage still being much more frequent). Another difference is
the percentage of cues of the investigation type, which may be related to the structure
350
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
of abstracts. Biological abstracts usually present the problem they examine and describe
methods they use. This entails the application of predicates belonging to the investiga-
tion class of uncertainty. It can be argued, however, that scientific papers also have these
characteristics but abstracts are much shorter than papers (generally, they contain about
10?12 sentences). Hence, investigation cues are responsible for a greater percentage of
cues.
There are some lexical differences among the corpora that are related to domain or
genre specificity. For instance, due to their semantics, the words charge, accuse, allege,
fear, worry, and rumor are highly unlikely to occur in scientific publications, but they
occur relatively often in news texts and in Wikipedia articles. As for lexical divergences
between abstracts and papers, many of them are related to verbs of investigation and
their different usage. In the corpora, verbs of investigations were marked only if it was
not clear whether the event/phenomenon would take place or not. If it has already
happened (The police are investigating the crime) or the existence of the thing under
investigation can be stated with certainty, independently of the investigation (The top
ten organisms were examined), then they are not instances of hypotheses, so they were
not annotated. As the data sets make clear, there were some candidates of investigation
verbs that occurred in the investigation sense mostly in abstracts but in another sense in
papers, especially in the bmc data set (e.g. assess or examine). Evaluate also had a special
mathematical sense in bmc papers, which did not occur in abstracts.
It can also be seen that some of the very frequent cues in papers do not occur (or
only relatively rarely) in abstracts. This is especially true for the bmc data set, where can,
if, would, could, and will are among the 15 most frequent cues and represent 23.21% of
cue occurrences, but only 3.85% in abstracts. It is also apparent that the rate of epistemic
cues is lower in bmc papers than in abstracts or other types of papers.
Genre-dependent characteristics can be analyzed if BioScope abstracts and hbc
papers are compared because their fine-grained domain is the same. Thus, it may be
assumed that differences between their cues are related to the genre. The sets of cues
used are similar, but the sense distributions may differ for certain ambiguous cues. For
instance, indicate mostly appears in the ?suggest? sense in abstracts, whereas in papers
it is used in the ?signal? sense. Another difference is that the percentage rate of doxastic
cues is almost twice as high in papers as in abstracts (10.6% and 5.7%, respectively).
Besides these differences, the two data sets are quite similar.
Domain-related differences can be analyzed when the three subdomains of biolog-
ical papers are contrasted. As stressed earlier, bmc papers contain fewer instances of
epistemic uncertainty, but condition cues occur more frequently in them. Nevertheless,
fly and hbc papers are rather similar in these respects but hbc papers contain more
investigation cues than the other two subcorpora. As regards lexical issues, the non-cue
usage of possible in comparative constructions is more frequent in the bmc data set than
in the other papers and many occurrences of if in bmc are related to definitions, which
were not annotated as uncertain. On the basis of this information, the fly and the hbc
domains seem to be more similar to each other than to the BMC data set from a linguistic
point of view.
From the perspective of genre and domain adaptation, the following points should
be highlighted concerning the distribution of uncertainty cues across corpora. Doxastic
uncertainty is of primary importance in the news and encyclopedia domains whereas
the investigation class is characteristic of the biological domain. Within the latter, there
is a genre-related difference as well: It is the epistemic and investigation classes that
are mainly present in abstracts whereas in papers cues belonging to other uncertainty
classes can also be found. Thus, when applying techniques developed for biological
351
Computational Linguistics Volume 38, Number 2
texts or abstracts to news texts, for example, doxastic uncertainty cues deserve special
attention as it might well be the case that there are insufficient training examples for this
class of uncertainty cues. The adaptation of an uncertainty cue detector constructed for
encyclopedia texts requires the special treatment of investigation cues, however, if, for
instance, scientific discourse is the target genre since they are underrepresented in the
source genre.
5.2 Evaluation Metrics
As evaluation metrics, we used cue-level and sentence-level F?=1 scores for the uncer-
tain class (the standard evaluation metrics of Task 1 of the CoNLL-2010 shared task)
and denote them by Fcue and Fsent, respectively. We report cue-level F?=1 scores on the
individual subcategories of uncertainty and the unlabeled (binary) F?=1 scores as well.
A sentence is treated as uncertain (in the gold standard and prediction) iff it contains at
least one cue. Note that the cue-level metric is quite strict as it is based on recognized
phrases?that is, only cues with perfect boundary matches are true positives. For the
sentence-level evaluation we simply labeled those sentences as uncertain that contained
at least one recognized cue.
5.3 Cross-Domain Cue Recognition Model
In order to minimize the development cost of a labeled corpus and an uncertainty
detector for a new genre/domain, we need to induce an accurate model from a minimal
amount of labeled data, or take advantage of existing corpora for different genres
and/or domains and use a domain adaptation approach. Experiments investigating the
value and sufficiency of existing corpora (which are usually out-of-domain) and simple
domain adaptation methods were carried out. For this purpose, we implemented a cue
recognition model, which is described in this section.
To train our models, we applied surface level (e.g., capitalization) and shallow
syntactic features (part-of-speech tags and chunks) and avoided the use of lexicon-based
features listing potential cue words, in order to reduce the domain dependence of the
learned models. Now we will introduce our model, which is competitive with the state-
of-the-art systems and focus on its domain adaptability. We will also describe the im-
plementation details of the learning model and the features employed. We should add
that the optimization of a cue detector was not the main focus of our study, however.
5.3.1 Feature Set.We extracted two types of features for each token to describe the token
itself, together with its local context in a window of limited size (1, 2, or no window,
depending on the feature).
The first group consists of features describing the surface form of the tokens. Here
we provide the list of the surface features with the corresponding window sizes:
 Stems of the tokens by the Porter stemmer in a window of size 2 (current
token and two tokens to the left and right).
 Surface pattern of the tokens in a window of size one (current token
and 1 token to the left and right). These patterns are similar to the word
shape feature described in Sun et al (2007). This feature can describe the
capitalization and other orthographic features as well. Patterns represent
352
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
character sequences of the same type with one single character for a
given word. There are six different pattern types denoting capitalized and
lowercased character sequences with the characters ?A? and ?a?, number
sequences with ?0?, Greek letter sequences with ?G? and ?g?, Roman
numerals with ?R? and ?r?, and non-alphanumerical characters with ?!?.
 Prefixes and suffixes of word forms from three to five characters long.
The second group of features describes the syntactic properties of the token and its
local context. The list of the syntactic features with the corresponding window sizes is
the following:
 Part-of-speech (POS) tags of the tokens by the C&C POS-tagger in a
window of size 2.
 Syntactic chunk of the tokens, as given by the C&C chunker,6 and the
chunk code of the tokens in a window of size 2.
 Concatenated stem, POS, and chunk labels similar to the features used
by Tang et al (2010). These feature strings were a combination of the stem
and the chunk code of the current token, the stem of the current token
combined with the POS-codes of the token left and right, and the chunk
code of the current token with the stems of the neighboring tokens.
5.3.2 CoNLL-2010 Experiments. The CoNLL-2010 shared task Learning to detect hedges and
their scope in natural language text focused on uncertainty detection. Two subtasks were
defined at the shared task: The first task sought to recognize sentences that contain some
uncertain language in two different domains and the second task sought to recognize
lexical cues together with their linguistic scope in biological texts (i.e., the text span in
terms of constituency grammar that covers the part of the sentence that is modified
by the cue). The lexical cue recognition subproblem of the second task7 is identical
to the problem setting used in this study, with the only major difference being the
types of uncertainty addressed: In the CoNLL-2010 task biological texts contained only
epistemic, doxastic, and investigation types of uncertainty. Apart from these differences,
the CoNLL-2010 shared task offers an excellent testbed for comparing our uncertainty
detection model with other state-of-the-art approaches for uncertainty detection and to
compare different classification approaches. Here we present our detailed experiments
using the CoNLL data sets, analyze the performance of our models, and select the most
suitable models for further experiments.
CoNLL systems. The uncertainty detection systems that were submitted to the CoNLL
shared task can be classified into three major types. The first set of systems treats the
problem as a sentence classification task, that is, one to decide whether a sentence
contains any uncertain element or not. These models operate at the sentence level and
are unsuitable for cue detection. The second group handles the problem as a token
6 POS-tagging and chunking were performed on all corpora using the C&C Tools (Curran, Clark, and
Bos 2007).
7 As an intermediate level, participants of the first task could submit the lexical cues found in sentences
for evaluation, without their scope, which gave some insight into the nature of cue detection on the
Wikipedia corpus (where scope annotation does not exist) as well.
353
Computational Linguistics Volume 38, Number 2
Table 4
Results on the original CoNLL-2010 data sets. The first three rows correspond to our baseline,
token-based, and sequence labeling models. The BEST/SEQ row shows the results of the best
sequence labeling approach of the CoNLL shared task (for both domains), the BEST/TOK rows
show the best token-based models, and the BEST/SENT rows show the best sentence-level
classifiers (these models did not produce cue-level results).
BIOLOGICAL WIKIPEDIA
Fcue Fsent Fcue Fsent
BASELINE 74.5 81.4 19.5 58.6
TOKEN/MAXENT 79.7 85.8 22.3 58.1
SEQUENCE/CRF 81.4 87.0 32.7 47.0
BEST/SEQ (Tang et al 2010) 81.3 86.4 36.5 55.0
BEST/TOK BIO (Velldal, ?vrelid, and Oepen 2010) 78.7 85.2 ? ?
BEST/TOKWIKI (Morante, Van Asch, and Daelemans 2010) 76.7 81.7 11.3 57.3
BEST/SENT BIO (Ta?ckstro?m et al 2010) ? 85.2 ? 55.4
BEST/SENTWIKI (Georgescul 2010) ? 78.5 ? 60.2
classification task, and classifies each token independently as uncertain (or not).
Contextual information is only included in the form of feature functions. The third
group of systems handled the task as a sequential token labeling problem, that is, de-
termined the most likely label sequence of a sentence in one step, taking the informa-
tion about neighboring labels into account. Sequence labeling and token classification
approaches performed best for biological texts and sentence-level models and token
classification approaches gave the best results for Wikipedia texts (see Table 6 in Farkas
et al [2010]). Here we compare a state-of-the-art token classification and sequence
labeling approach using a shared feature representation to decide which model to use
in further experiments.
Classifier models. We used a first-order linear chain conditional random fields (CRF)
model as a sequence labeler and a Maximum Entropy (Maxent) classifier model as a
token classifier, implemented in the Mallet (McCallum 2002) package for training the
uncertainty cue detectors. This choicewasmotivated by the fact that thesewere themost
popular classification approaches among the CoNLL-2010 participants, and that CRF
models are known to provide high accuracy for the detection of phrases with accurate
boundaries (e.g., in named entity recognition). We trained the CRF and Maxent models
with their default settings in Mallet for 200 iterations or until convergence (CRF), and
also until convergence (Maxent) in each experimental set-up.
As a baseline model, we applied a simple dictionary-based approach which clas-
sifies every uni- and bigram as uncertain that is tagged as uncertain in over 50% of
the cases in the training data. Hence, it is a similar system to that presented by Tjong
Kim Sang (2010), without tuning the decision threshold for predicting uncertainty.
CoNLL results. An overview of the results achieved on the CoNLL-2010 data sets can
be found in Table 4. A comparison of our models with the CoNLL systems reveals that
our uncertainty detection model is very competitive when applied on the biological
data set. Our CRF model trained on the official training data set of the shared task
achieved a cue-level F-score of 81.4 and sentence-level F-score of 87.0 on the biological
354
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
evaluation data set. These results would have come first in the shared task, with a
marginal difference compared to the top performing participant. In contrast, our model
is less competitive on the Wikipedia data set: The Maxent model achieved a cue-level
F-score of 22.3 and sentence-level F-score of 58.1 on the Wikipedia evaluation data
set, whereas our CRF model was not competitive with the best participating systems.
The observation that sequence-labeling models perform worse than token-based
approaches on Wikipedia, especially for sentence-level evaluation measures, coincides
with the findings of the shared task: The discourse-level uncertainty cues in the
Wikipedia data set are rather long and heterogeneous and sequence labeling models
often revert to not annotating any token in a sentence when the phrase boundaries are
hard to detect. Still, sequence labeling models have an advantage in terms of cue-level
accuracy. This is not surprising because CRF is a state-of-the-art model for chunking /
sequence labeling tasks.
We conclude from Table 4 that our model is competitive with the state-of-the-art
systems for detecting semantic uncertainty (which is closer to the biological subtask),
but it is less suited to recognizing discourse-level uncertainty. In the subsequent exper-
iments we used our CRF model, which performed best in detecting uncertainty cues in
natural language sentences.
5.3.3 Domain Adaptation Model. In supervised machine learning, the task is to learn how
to make predictions on previously unseen, new examples based on a statistical model
learned from a collection of labeled training examples (i.e., a set of examples coupled
with the desired output for them). The classification setting assumes a set of labels L, a
set of features X, and a probability distribution p(X) describing the examples in terms of
their features. Then the training examples are assumed to be given in the form of {xi, li}
pairs and the goal of classification is to estimate the label distribution p(L|X), which can
be used later on to predict the labels for unseen examples.
Domain adaptation focuses on the problem where the same (or a closely related)
learning task has to be solved in multiple domains which have different characteristics
in terms of their features: The set of features X may be different or the probability dis-
tributions p(X) describing the inputs may be different. When the target tasks are treated
as different (but related), the label distribution p(L|X) is dependent on the domain. That
is, given a domain d, the problem can be formalized as modeling p(L|X)d based on Xd,
p(X)d and a set of examples: {xi,d, li}.8 In the context of domain adaptation, there is a
target domain t and a source domain s, with labeled data available for both, and the
goal is to induce a more accurate target domain model p(L|X)t from {xi,t, li} ? {xi,s, li}
than the one learned from {xi,t, li} only. In practical scenarios, the goal is to exploit the
source data to acquire an accurate model from just limited target data which are alone
insufficient to train an accurate in-domain model, and thus to port the model to a new
domain with moderate annotation costs. The problem is difficult because it is nontrivial
for a learning method to account for the different data (and label) distributions between
target and source, which causes a remarkable drop in model accuracy when it is applied
to classifying examples taken from the target domain.
In our experimental context, both topic- and genre-related differences of texts pose
an adaptation problem as these factors have an impact on both the vocabulary (p(X))
and the sense distributions of the cues (p(L|X)) found in different texts. There is some
8 The literature also describes the case when the set of labels depends on the domain, but we omit this case
to simplify our notation and discussion. For details, see Pan and Yang (2010).
355
Computational Linguistics Volume 38, Number 2
confusion in the literature regarding the terminology describing the various domain
mismatches in the learning problem. For example, Daume? III (2007) describes a domain
adaptation method where he assumes that the label distribution is unchanged (we note
here that this assumption is not exploited in the method, and that the label distribution
changes in our problem), whereas Pan and Yang (2010) uses the term inductive transfer
learning to refer to our scenario (in their paper, domain adaptation refers to a different
setting).9 In this study we always use the term domain adaptation to refer to our problem
setting, that is, where both p(X) and p(L|X) are assumed to change.
In our experiments, we used various data sets taken from multiple genres and
domains (see Section 5.1.1 for an overview) and applied a simple but effective do-
main adaptation model (Daume? III 2007) for training our classifiers. In this model,
domain adaptation is carried out by defining each feature over the target and source
data sets twice?just once for target domain instances, and once for both the tar-
get and source domain instances. Formally, having a target domain t and a source
domain s and n features {f1, f2, . . . fn}, for each fi we have a target-only version fi,t
and a shared version fi,t+s. Each target domain example is described by 2n features:
{ f1,t, f2,t, . . . fn,t, f1,t+s, f2,t+s, . . . fn,t+s} and source domain examples are described by only
the n shared features: { f1,t+s, f2,t+s, . . . fn,t+s}. Using the union of the source and target
training data sets {xi,t, li} ? {xi,s, li} and this feature representation, any standard super-
vised machine learning technique can be used and it becomes possible for the algorithm
to learn target-dependent and shared patterns at the same time and handle the changes
in the underlying distributions. This easy domain adaptation technique has been found
to work well in many NLP-oriented tasks. We used the CRF models introduced herein
and in this way, we were able to exploit feature?label correspondences across domains
(for features that behave consistently across domains) and also to learn patterns specific
to the target domain.
5.4 Cross-Domain and Genre Experiments
We defined several settings (target and source pairs) with varied domain and genre
distances and target data set sizes. These experiments allowed us to study the po-
tential of transferring knowledge across existing corpora for the accurate detection of
uncertain language in a wide variety of text types. In our experiments, we used all the
combinations of genres and domains that we found plausible. News texts (and their
subdomains) were not used as source data because FactBank is significantly smaller
than the other corpora (WikiWeasel or scientific texts). As the source data set is typically
larger than the target data set in practical scenarios, news texts can only be used as target
data. Abstracts were only used as source data because information extraction typically
addresses full texts whereas abstracts just provide annotated data for development pur-
poses. Besides these restrictions, we experimented with all possible target and source
pairs.
We used four different machine-learning settings for each target?source pair in our
investigations. In the purely cross-domain (CROSS) setting, the model was trained on
the source domain and evaluated on the target (i.e., no labeled target domain data
sets were used for training). In the purely in-domain setting (TARGET), we performed
9 More on this can be found in Pan and Yang (2010) and at http://nlpers.blogspot.com/2007/11/
domain-adaptation-vs-transfer-learning.html.
356
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
Table 5
Experimental results on different target and source domain pairs. The third column contains the
ratio of the target train and source data sets? sizes in terms of sentences. DIST shows the distance
of the source and target domain/genre (?-? same; ?+? fine-grade difference; ?++? coarse-grade
difference; bio = biological; enc = encyclopedia; sci paper = scientific paper; sci abs = scientific
abstract; sci paper hbc = scientific papers on human blood cell experiments; sci paper fly =
scientific papers on Drosophila; sci paper bmc = scientific papers on bioinformatics).
CROSS TARGET DA/ALL DA/CUE
TARGET SOURCE SOURCE
TARGET
DIST Fcue Fsent Fcue Fsent Fcue Fsent Fcue Fsent
enc sci paper+ abs 0.9 ++/++ 68.0 74.2 82.4 87.4 82.6 87.6 82.6 87.6
news sci paper+ abs 6.2 ++/++ 64.4 70.5 68.7 77.1 72.7 79.5 73.8 81.0
news enc 6.6 ++/++ 68.2 74.8 68.7 77.1 73.7 81.2 73.1 80.0
sci paper enc 2.7 ++/++ 67.8 75.1 78.8 84.4 80.0 85.9 79.8 85.4
sci paper bmc sci abs hbc 4.3 +/+ 58.2 70.5 64.0 74.5 68.1 76.7 69.3 77.8
sci paper fly sci abs hbc 3.4 +/+ 70.5 79.1 80.0 85.1 83.3 88.2 82.9 87.8
sci paper hbc sci abs hbc 8.2 ?/+ 76.5 82.9 74.2 80.2 84.2 88.6 83.0 88.9
sci paper bmc sci paper fly+ hbc 1.8 +/? 69.8 77.6 64.0 74.5 70.0 78.2 69.4 78.1
sci paper fly sci paper bmc+ hbc 1.2 +/? 78.4 83.5 80.0 85.1 82.6 87.0 82.9 87.0
sci paper hbc sci paper bmc+ fly 4.4 +/? 81.7 85.9 74.2 80.2 80.7 86.9 80.7 85.9
AVERAGE: 70.4 77.4 73.5 80.6 77.8 84.0 77.8 84.0
10-fold cross-validation on the target data (i.e., no source domain data were used). In
the two domain adaptation settings, we again performed 10-fold cross-validation on
the target data but exploited the source data set (as described in Section 5.3). Here, we
either used each sentence of the source data set (DA/ALL) or only those sentences that
contained a cue observed in the target train data set (DA/CUE).
Table 5 lists the results obtained on various target and source domains in various
machine learning settings and Table 6 contains the absolute differences between a
particular result and the in-domain (TARGET) results.
Fine-grained semantic uncertainty classification results are summarized in Tables 7
and 8. Table 7 contrasts the coarse-grained Fcue with the unlabeled/binary Fcue of fine-
grained experiments, therefore it quantifies the difference in accuracy due to the more
difficult classification setting and the increased sparseness of the task. Table 8 shows
the per class Fcue scores, namely, how accurately our model recognizes the individual
uncertainty types.
Table 6
The absolute difference between the F-scores of Table 5 relative to the baseline TARGET setting.
CROSS TARGET DA/ALL DA/CUE
TARGET SOURCE SOURCE
TARGET
DIST Fcue Fsent Fcue Fsent Fcue Fsent Fcue Fsent
enc sci paper+ abs 0.9 ++/++ ?14.4 ?13.2 82.4 87.4 0.2 0.2 0.2 0.2
news sci paper+ abs 6.2 ++/++ ?4.3 ?6.6 68.7 77.1 4.0 2.4 5.1 3.9
news enc 6.6 ++/++ ?0.5 ?2.3 68.7 77.1 5.0 4.1 4.4 2.9
sci paper enc 2.7 ++/++ ?11.0 ?9.3 78.8 84.4 1.2 1.5 1.0 1.0
sci paper bmc sci abs hbc 4.3 +/+ ?5.8 ?4.0 64.0 74.5 4.1 2.2 5.3 3.3
sci paper fly sci abs hbc 3.4 +/+ ?9.5 ?6.0 80.0 85.1 3.3 3.1 2.9 2.7
sci paper hbc sci abs hbc 8.2 ?/+ 2.3 2.7 74.2 80.2 10.0 8.4 8.8 8.7
sci paper bmc sci paper fly+ hbc 1.8 +/? 5.8 3.1 64.0 74.5 6.0 3.7 5.4 3.6
sci paper fly sci paper bmc+ hbc 1.2 +/? ?1.6 ?1.6 80.0 85.1 2.6 1.9 2.9 1.9
sci paper hbc sci paper bmc+ fly 4.4 +/? 7.5 5.7 74.2 80.2 6.5 6.7 6.5 5.7
AVERAGE: ?3.1 ?3.2 73.5 80.6 4.3 3.4 4.3 3.4
357
Computational Linguistics Volume 38, Number 2
Table 7
Comparison of cue-level binary (Fbin) and unlabeled F-scores (Funl). Binary F-score corresponds
to coarse-grained classification (uncertain vs. certain), and unlabeled F-score is the fine-grained
classification converted to binary (disregarding the fine-grained category labels).
CROSS TARGET DA/ALL DA/CUE
TARGET SOURCE SOURCE
TARGET
DIST Fbin Funl Fbin Funl Fbin Funl Fbin Funl
enc sci paper+ abs 0.9 ++/++ 68.0 67.4 82.4 82.4 82.6 81.9 82.6 81.7
news sci paper+ abs 6.2 ++/++ 64.4 59.9 68.7 66.4 72.7 71.5 73.8 71.8
news enc 6.6 ++/++ 68.2 67.0 68.7 66.4 73.7 73.6 73.1 73.4
sci paper enc 2.7 ++/++ 67.8 67.2 78.8 78.3 80.0 80.2 79.8 79.5
sci paper bmc sci abs hbc 4.3 +/+ 58.2 66.3 64.0 61.9 68.1 68.5 69.3 67.9
sci paper fly sci abs hbc 3.4 +/+ 70.5 78.7 80.0 79.2 83.3 83.4 82.9 83.2
sci paper hbc sci abs hbc 8.2 ?/+ 76.5 83.6 74.2 69.3 84.2 83.1 83.0 83.4
sci paper bmc sci paper fly+ hbc 1.8 +/? 69.8 69.7 64.0 61.9 70.0 69.5 69.4 65.9
sci paper fly sci paper bmc+ hbc 1.2 +/? 78.4 77.7 80.0 79.2 82.6 82.1 82.9 82.5
sci paper hbc sci paper bmc+ fly 4.4 +/? 81.7 81.9 74.2 69.3 80.7 81.3 80.7 81.2
AVERAGE: 70.4 71.9 73.5 71.4 77.8 77.5 77.8 77.0
Table 8
The per class cue-level F-scores in fine-grained classification. Fcrs, Ftgt, and Fda correspond to the
CROSS, TARGET, and DA/CUE settings, respectively (same as previous). The DA/ALL setting
is not shown for space reasons and due to its similarity to the DA/CUE results.
EPISTEMIC INVESTIGATION DOXASTIC CONDITION
TARGET SOURCE Fcrs Ftgt Fda Fcrs Ftgt Fda Fcrs Ftgt Fda Fcrs Ftgt Fda
enc sci paper+ abs 75.9 83.4 82.8 67.3 67.5 70.4 48.8 89.2 88.1 54.4 62.6 61.2
news sci paper+ abs 70.9 65.4 75.2 79.5 75.9 83.1 39.1 68.9 71.3 47.2 57.1 57.5
news enc 65.4 65.4 74.5 74.6 75.9 87.5 76.3 68.9 78.0 50.6 57.1 56.7
sci paper enc 72.9 81.2 81.9 36.5 72.9 72.4 63.6 74.9 79.8 57.0 58.9 59.7
sci paper bmc sci abs hbc 71.5 68.3 72.6 56.1 37.7 58.1 68.1 61.9 69.4 45.5 45.0 49.5
sci paper fly sci abs hbc 82.9 82.1 85.3 69.0 68.6 76.6 75.1 71.7 75.4 28.6 63.4 64.1
sci paper hbc sci abs hbc 87.5 77.7 86.4 76.5 53.5 77.5 80.6 39.0 76.7 26.1 10.0 33.3
sci paper bmc sci paper fly+ hbc 74.4 68.3 69.2 55.9 37.7 57.4 63.7 61.9 64.7 57.3 45.0 50.7
sci paper fly sci paper bmc+ hbc 80.3 82.1 84.3 66.7 68.6 75.8 77.7 71.7 77.3 53.5 63.4 68.0
sci paper hbc sci paper bmc+ fly 85.2 77.7 86.0 74.0 53.5 70.3 75.9 39.0 70.2 58.1 10.0 41.4
AVERAGE: 76.7 75.2 79.8 65.6 61.2 72.9 66.9 64.7 75.1 47.8 47.3 54.2
The size of the target training data sets proved to be an important factor in these
investigations. Hence, we performed experiments with different target data set sizes.
We utilized the DA/ALL model (which is more robust for extremely small target data
sizes [e.g., 100-400 sentences]) and performed the same 10-fold cross validation on the
target data set as in Tables 5-8. For each fold of the cross-validation here, however, we
just used n sentences (x axis of the figures) from the target training data set and a fixed
set of 4,000 source sentences to alleviate the effect of varying data set sizes. Figure 5
depicts the learning curves for two target/source data set pairs.
6. Discussion
As Table 5 shows, incorporating labeled data from different genres and/or domains
consistently improves the performance. The successful applicability of domain adap-
tation tells us that the problem of detecting uncertainty has similar characteristics
across genres and domains. The uncertainty cue lexicons of different domains and
358
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
Figure 5
Learning curves: Results achieved with different target train sizes. The left and right figures
show two selected source/target pairs. The upper figures depict coarse-grained classification
results (Fcue); DA, CROSS, and TARGET with the same settings as in Table 5. The lower figures
show the per class Fcue of the DA/ALL model in the fine-grained classification.
genres indeed share a core vocabulary and despite the differences in sense distributions,
labeled data from a different source improves uncertainty classification in a new genre
and domain if the different data sets are annotated consistently. This justifies our aim
to create a consistent representation of uncertainty that can be applied to multiple
domains.
6.1 Domain Adaptation Results
The size of the target and source data sets largely influences to what extent external
data can improve results. The only case where domain adaptation had only a negligible
effect (an F-score gain less than 1%) is where the target data set is itself very large.
This is expected as the more target data one has, the less crucial it is to incorporate
additional data with some undesirable characteristics (difference in style, domain,
certain/uncertain sense distribution, etc.).
The performance scores for the CROSS setting clearly indicate the domain/genre
distance of the data sets: Themore distant the domain and genre of the source and target
data sets are, the more the CROSS performance (where no labeled target data is used)
degrades, compared with the TARGET model. In general, when the distance between
both the domain and the genre of texts is substantial (++/++ and +/+ rows in Tables 5
359
Computational Linguistics Volume 38, Number 2
and 6), this accounts for a 6?10% decrease in both the sentence and cue-level F-scores.
An exception is the case of encyclopedic source and news target domains. Here the
performance is very close to the target domain performance. This indicates that these
settings are not so different from each other as it might seem at the first glance. The
encyclopedic and news genres share quite a lot of commonalities (compare cue distribu-
tions in Figure 4, for instance). We verified this observation by using a knowledge-poor
quantitative estimator of similarity between domains (Van Asch and Daelemans 2010):
Using cosine as the similarity measure, the newswire and encyclopedia texts are found
to be the second most similar domain pair in our experiments, with a score comparable
to those obtained for the pairs of scientific article types bmc, hbc, and fly.
When there is a domain or genre match between source and target (?/+ and +/?
rows in Tables 5 and 6), however, and the distance regarding the other is just moderate,
the cross-training performance is close to or even better than the target-only results.
That is, the larger amount of source training data balances the differences between the
domains. These results indicate that the learned uncertainty classifiers can be directly
applied to slightly different data sets. This suitability is due to the learned disambigua-
tion models, which generalize well in similar settings. This is contrary to the findings
of earlier studies, which built the uncertainty detectors using seed examples and boot-
strapping. These models were not designed to learn any disambiguation models for
the cue words found, and their performance degraded even for slightly different data
(Szarvas 2008).
Comparing the two domain adaptation procedures DA/CUE and DA/ALL, adap-
tation via transferring only source sentences that contain a target domain cue is, on
average, comparable to transferring all the data from the source domain. In other words,
when we have a small but sufficient amount of target data available, it is enough to
account for source data corresponding to the uncertainty cues we saw in the limited
target data set. This observation has several consequences, namely:
 The source-only cues, or to be more precise, their disambiguation models,
are not helpful for the target domains as they cannot be adapted. This is
due to the differences in the source and target disambiguation models.
 Similarly, domain adaptation improves the disambiguation models for the
observed target cues, rather than introducing new vocabulary into the
target domain. This mechanism coincides with our initial goal of using
domain adaptation to learn better semantic models. This effect is the
opposite of how bootstrapping-based weakly supervised approaches
improve the performance in an underresourced domain. This observation
suggests a promising future direction of combining the two approaches to
maximize the gains while minimizing the annotation costs.
 In a general context, we can effectively extend the data for a given domain
if we have robust knowledge of the potential uncertainty vocabulary for
that domain. Given the wide variety of the domains and genres of our data
sets, it is reasonable to suppose that they represent uncertain language in
general quite well, and the joint vocabularies provide a good starting point
for a targeted data development for further domains.
As regards the fine-grained classification results, Table 7 demonstrates that the
fine-grained distinction results in only a small, or no, loss in performance. The coarse-
grained model is slightly more accurate than the fine-grained model (counting correctly
360
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
recognized but misclassified cues as true positives) in most settings. The most signif-
icant difference is observed for the target-only settings, where no out-of-domain data
are used for the training and thus the data sets are accordingly smaller. A noticeable
exception is when scientific abstracts are used for cross training: In those settings the
coarse-grained model performs poorly, due to its lower recall, which we attribute to
overfitting the special characteristics of abstracts. The fact that in fine-grained classi-
fication the CROSS results consistently outperform the TARGET models (see Table 8)
even for distant domain pairs, also underlines that the increased sparseness caused by
the differentiation of the various subtypes of uncertainty is an important factor only for
smaller data sets. The improvement by domain adaptation is clearly more prominent
in fine-grained than in coarse-grained classification, however: The individual cue types
benefit by 5?10% points in terms of the F-score from out-of-domain data and domain
adaptation. Moreover, as Table 8 shows, for the domain pairs and fine-grained classes
where a nice amount of positive examples are at hand, the per class Fcue scores are
also around 80% and above. This means that it is possible to accurately identify the
individual subtypes of semantic uncertainty, and thus it also proves the feasibility of
the subcategorization and annotation scheme proposed in this study (Section 2). Other
important observations here are that domain adaptation is even more significant in the
more difficult fine-grained classification setting, and that the condition class represents
a challenge for our model. The performance for the condition class is lower than that
for the other classes, which can only in part be attributed to the fact that this is the
least represented subtype in our data sets: as opposed to other cue types, condition cues
are typically used in many different contexts and they may belong to other uncertainty
classes as well.
6.2 The Required Amount of Annotation
Based on our experiments, we may conclude that a manually annotated training data
set consisting of 3,000?5,000 sentences is sufficient for training an accurate cue detector
for a new genre/domain. The results of our learning curve experiments (Figure 5)
illustrate the situations where only a limited amount of annotated data (fewer than 3,000
sentences) is available for the target domain. The feasibility of decreasing annotation
efforts and the real added value of domain adaptation are more prominent in this range.
It is easy to see that the TARGET results approach to DA results with more target data.
Figure 5 shows that the size of the target training data set where the supervised
TARGET setting outperforms the CROSS model (trained on 4,000 source sentences) is
around 1,000 sentences. Aswementioned earlier, even distant domain data can improve
the cue recognition model in the absence of a sufficient target data set. Figure 5 justifies
this observation, as the CROSS and DA settings outperform the TARGET setting on
each source?target data set pair. It can also be observed that the doxastic type is more
domain-dependent than the others and its results consistently improve by increasing
the size of the target domain annotation (which coincides with the cue frequency
investigations of Section 5.1.3). In the news target domain, however, the investigation
and epistemic classes benefit a lot from a small amount of annotated target data but their
performance scores increase just slightly after that. This indicates that most of the im-
portant domain-dependent (probably lexical) knowledge could be gathered from 100?
400 sentences. In the biological experiments, we may conclude that the investigation
class is already covered by the source domain (intuitively, the investigation cues are well
represented in the abstracts) and its results are not improved significantly by usingmore
361
Computational Linguistics Volume 38, Number 2
target data. The condition class is underrepresented in both the source and target data
sets and hence no reliable observations can bemade regarding this subclass (see Table 2).
Overall, if we would like to have an uncertainty cue detector for a new
genre/domain: (i) We can achieve performance around 60?70% by using cross training
depending on the difference between the domains (i.e., without any annotation effort);
(ii) By annotating around 3,000 sentences, we can have a performance of 70?80%,
depending on the level of difficulty of the texts; (iii) We can get the same 70?80% results
with annotating just 1,000 sentences and using domain adaptation.
6.3 Interesting Examples and Error Analysis
As might be expected, most of the erroneous cue predictions were due to vocabulary
differences, for example, fear or accuse occurred only in news texts, which is why they
were not recognized by models trained on biological or encyclopedia texts. Another
example is the case of or, which is a frequent cue in biological texts. Still, it is rarely
used as a cue in other domains but without domain adaptation, the model trained on
biological texts marks quite a few occurrences of or as cues in the news or encyclope-
dia domains. Many of these anomalies were eliminated by the application of domain
adaptation techniques, however.
Many errors were related to multi-class cues. These cues are especially hard to
disambiguate because not only can they refer to several classes of uncertainty, but
they typically have non-cue usage as well. For instance, the case of would is rather
complicated because it can fulfill several functions:
(10) EPISTEMIC USAGE (?IT IS HIGHLY PROBABLE?): Further biochemical studies
on the mechanism of action of purified kinesin-5 from multiple systems
would obviously be fruitful. (Corpus: fly)
(11) CONDITIONAL: ?If religion was a thing that money could buy,/The rich
would live and the poor would die.? (Corpus: WikiWeasel)
(12) FUTURE IN THE PAST: This Aarup can trace its history back to 1500, but it
would be 1860?s before it would become a town. (Corpus: WikiWeasel)
(13) REPEATED ACTION IN THE PAST (?USED TO?): ?Becker? was the next T.V.
Series for Paramount that Farrell would co-star in. (Corpus: WikiWeasel)
(14) DYNAMIC MODALITY: Individuals would first have a small lesion at the
site of the insect bite, which would eventually leave a small scar. (Corpus:
WikiWeasel)
(15) PRAGMATIC USAGE: Although some would dispute the fact, the joke
related to a peculiar smell that follows his person. (Corpus: WikiWeasel)
The epistemic uses of would are annotated as epistemic cues whereas its occurrences in
conditionals are marked as hypothetical cues. The habitual past meaning is not related
to uncertainty, hence it is not annotated. The future in the past meaning (i.e., past tense
of will), however, denotes an event of which it is known that happened later, so it is
certain. The dynamically modal would is similar to the future will (which is an instance
of dynamic modality as well), but it is not annotated in the corpora. The pragmatic
362
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
use of would does not refer to semantic uncertainty (the semantic value of the sentence
would be exactly the same without it or if it is replaced with may, might, will, etc., that
is, some will/may/might/? dispute the fact mean the same). It is rather a stylistic issue
to further express uncertainty at the discourse level (i.e., there are some unidentified
people who dispute the fact, hence the opinion cannot be associated with any definite
source).
The last two uses of would are not typically described in grammars of English
and seem to be characteristic primarily of the news and encyclopedia domains. Thus
it is advisable to explore such cases and treat them with special consideration when
adapting an algorithm trained and tested in a specific domain to another domain.
Another interesting example is may in its non-cue usage. Being (one of) the most
frequent cues in each subcorpus, its non-cue usage is rather limited but can be found
occasionally in FactBank and WikiWeasel. The following instance of may in FactBank
was correctly marked as non-cue by the cue detector when trained on Wikipedia texts.
On the other hand, it was marked as a cue when trained on biological texts since in this
case, there were insufficient training examples of may not being a cue:
(16) ?Wellmay we say ?God save the Queen,? for nothing will save the
republic,? outraged monarchist delegate David Mitchell said. (Corpus:
FactBank)
A final example to be discussed is concern. This word also has several uses:
(17) NOUN MEANING ?COMPANY?: The insurance concern said all conversion
rights on the stock will terminate on Nov. 30. (Corpus: FactBank)
(18) NOUN MEANING ?WORRY?: Concern about declines in other markets,
especially New York, caused selling pressure. (Corpus: FactBank)
(19) PREPOSITION: The company also said it continues to explore all options
concerning the possible sale of National Aluminum?s 54.5% stake in an
aluminum smelter in Hawesville, Ky. (Corpus: FactBank)
(20) VERB: Many of the predictions in these two data sets concern protein pairs
and proteins that are not present in other data sets. (Corpus: bmc)
Among these examples, only the second one should be annotated as uncertain. POS-
tagging seems to provide enough information for excluding the verbal and preposi-
tional uses of the word but in the case of nominal usage, additional information is also
required to enable the system to decide whether it is an uncertainty cue or not (in this
case, the noun in the ?company? sense cannot have an argument while in the ?worry?
sense, it can have [about declines]). Again, the frequency of the two senses depends
heavily on the domain of the texts, which should also be considered when adapting
the cue detector to a different domain. We should mention that the role of POS-tagging
is essential in cue detection because many ambiguities can be resolved on the basis of
POS-tags. Hence, POS-tagging errors can lead to a serious decline in performance.
We think that an analysis of similar examples can further support domain adapta-
tion and cue detection across genres and domains.
363
Computational Linguistics Volume 38, Number 2
7. Conclusions and Future Work
In this article, we introduced an uncertainty cue detection model that can perform well
across different domains and genres. Even though several types of uncertainty exist,
available corpora and resources focus only on some of the possible types and thereby
only cover particular aspects of the phenomenon. This means that uncertainty models
found in the literature are heterogeneous, and the results of experiments on different
corpora are hardly comparable. These facts motivated us to offer a unified model of
semantic uncertainty enhanced by linguistic and computer science considerations. In
accordance with this classification, we reannotated three corpora from several domains
and genres using our uniform annotation guidelines.
Our results suggest that simple cross training can be employed and it achieves a
reasonable performance (60?70% cue-level F-score) when no annotated data is at hand
for a new domain. When some annotated data is available (here somemeans fewer than
3,000 annotated sentences for the target domain), domain adaptation techniques are
the best choice: (i) they lead to a significant improvement compared to simple cross
training, and (ii) they can provide a reasonable performance with significantly less
annotation. In our experiments, the annotation of 3,000 sentences and training only on
these is roughly equivalent to the annotation of 1,000 sentences using external data and
domain adaptation. If the size of the training data set is sufficiently large (larger than
5,000 sentences) the effect of incorporating additional data?having some undesirable
characteristics?is not crucial.
Comparing different domain adaptation techniques, we found that similar results
could be attained when the source domain was filtered for sentences that contained
cues in the target domain. This tells us that models learn to better disambiguate the
cues seen in the target domain instead of finding new, unseen cues. In this sense, this
approach can be regarded as a complementarymethod toweakly supervised techniques
for lexicon extraction. A promising way to further minimize annotation costs while
maximizing performance would be the integration of the two approaches, which we
plan to investigate in the near future.
In our study, we did not pay attention to dynamic modality (due to the lack of an-
notated resources), but the detection of such phenomena is also desirable. For instance,
dynamically modal events cannot be treated as certain?that is, the event of buying
cannot be assigned the same truth value in They agreed to buy the company and They
bought the company. Whereas the second sentence expresses a fact, the first one informs
us about the intention of buying the company, which will be certainly carried out in a
world where moral or business laws are observed but at the moment it cannot be stated
whether the transaction takes place (i.e., that it is certain). Hence, in the future, we also
intend to integrate the identification of dynamically modal cues into our uncertainty
cue detector.
Acknowledgments
This work was supported in part by
the NIH grants (project codenames
MASZEKER and BELAMI) of the
Hungarian government, by the German
Ministry of Education and Research
under grant SiDiM (grant no. 01IS10054G),
and by the Volkswagen Foundation as
part of the Lichtenberg-Professorship
Program (grant no. I/82806). Richa?rd
Farkas was funded by the Deutsche
Forschungsgemeinschaft grant SFB 732.
References
Baker, Kathy, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Ed Hovy, Lori Levin,
Marjorie McShane, Teruko Mitamura,
364
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
Sergei Nirenburg, Christine Piatko,
Owen Rambow, and Gramm Richardson.
2010. Modality Annotation Guidelines.
Technical Report 4, Human Language
Technology Center of Excellence,
Baltimore, MD.
Chapman, Wendy W., David Chu, and
John N. Dowling. 2007. ConText: An
algorithm for identifying contextual
features from clinical text. In Proceedings
of the ACL Workshop on BioNLP 2007,
pages 81?88, Prague, Czech Republic.
Clausen, David. 2010. HedgeHunter:
A system for hedge detection and
uncertainty classification. In Proceedings of
the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010):
Shared Task, pages 120?125, Uppsala.
Conway, Mike, Son Doan, and Nigel Collier.
2009. Using hedges to enhance a disease
outbreak report text mining system. In
Proceedings of the BioNLP 2009 Workshop,
pages 142?143, Boulder, CO.
Curran, James, Stephen Clark, and Johan
Bos. 2007. Linguistically motivated
large-scale NLP with C&C and Boxer. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics
Companion Volume Proceedings of the Demo
and Poster Sessions, pages 33?36, Prague.
Daume? III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
45th Annual Meeting of the Association of
Computational Linguistics, pages 256?263,
Prague.
Daume? III, Hal and Daniel Marcu. 2006.
Domain adaptation for statistical
classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
Falahati, Reza. 2006. The use of hedging
across different disciplines and rhetorical
sections of research articles. In Proceedings
of the 22nd NorthWest Linguistics Conference
(NWLC22), pages 99?112, Burnaby.
Farkas, Richa?rd and Gyo?rgy Szarvas. 2008.
Automatic construction of rule-based
ICD-9-CM coding systems. BMC
Bioinformatics, 9:1?9.
Farkas, Richa?rd, Veronika Vincze, Gyo?rgy
Mo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.
2010. The CoNLL-2010 Shared Task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010):
Shared Task, pages 1?12, Uppsala.
Fernandes, Eraldo R., Carlos E. M. Crestana,
and Ruy L. Milidiu?. 2010. Hedge detection
using the RelHunter approach. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 64?69,
Uppsala.
Friedman, Carol, Philip O. Alderson,
John H. M. Austin, James J. Cimino, and
Stephen B. Johnson. 1994. A General
natural-language text processor for clinical
radiology. Journal of the American Medical
Informatics Association, 1(2):161?174.
Ganter, Viola and Michael Strube. 2009.
Finding hedges by chasing weasels: Hedge
detection using Wikipedia tags and
shallow linguistic features. In Proceedings
of the ACL-IJCNLP 2009 Conference Short
Papers, pages 173?176, Suntec.
Georgescul, Maria. 2010. A hedgehop over a
max-margin framework using hedge cues.
In Proceedings of the Fourteenth Conference
on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 26?31,
Uppsala.
Hyland, Ken. 1994. Hedging in academic
writing and EAP textbooks. English for
Specific Purposes, 13(3):239?256.
Hyland, Ken. 1996. Writing without
conviction? Hedging in scientific
research articles. Applied Linguistics,
17(4):433?454.
Hyland, Ken. 1998. Boosters, hedging and
the negotiation of academic knowledge.
Text, 18(3):349?382.
Kiefer, Ferenc. 2005. Leheto?se?g e?s
szu?kse?gszeru?se?g [Possibility and
necessity]. Tinta Kiado?, Budapest.
Kilicoglu, Halil and Sabine Bergler.
2008. Recognizing speculative
language in biomedical research articles:
A linguistically motivated perspective.
In Proceedings of the Workshop on Current
Trends in Biomedical Natural Language
Processing, pages 46?53, Columbus, OH.
Kilicoglu, Halil and Sabine Bergler. 2009.
Syntactic dependency based heuristics for
biological event extraction. In Proceedings
of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 119?127,
Boulder, CO.
Kim, Jin-Dong, Tomoko Ohta, Sampo
Pyysalo, Yoshinobu Kano, and Jun?ichi
Tsujii. 2009. Overview of BioNLP?09
Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop
Companion Volume for Shared Task,
pages 1?9, Boulder, OH.
Kim, Jin-Dong, Tomoko Ohta, and Jun?ichi
Tsujii. 2008. Corpus annotation for mining
biomedical events from literature. BMC
Bioinformatics, 9(Suppl 10).
365
Computational Linguistics Volume 38, Number 2
Li, Xinxin, Jianping Shen, Xiang Gao, and
Xuan Wang. 2010. Exploiting rich features
for detecting hedges and their scope. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 78?83,
Uppsala.
Light, Marc, Xin Ying Qiu, and Padmini
Srinivasan. 2004. The language of
bioscience: Facts, speculations, and
statements in between. In Proceedings
of the HLT-NAACL 2004 Workshop:
Biolink 2004, Linking Biological Literature,
Ontologies and Databases, pages 17?24,
Boston, Massachusetts, USA.
MacKinlay, Andrew, David Martinez, and
Timothy Baldwin. 2009. Biomedical event
annotation with CRFs and precision
grammars. In Proceedings of the Workshop
on Current Trends in Biomedical Natural
Language Processing: Shared Task,
BioNLP ?09, pages 77?85, Uppsala.
McCallum, Andrew Kachites. 2002.
MALLET: A Machine Learning for
Language Toolkit. Available at
http://mallet.cs.umass.edu.
Medlock, Ben and Ted Briscoe. 2007.
Weakly supervised learning for hedge
classification in scientific literature. In
Proceedings of the ACL, pages 992?999,
Prague.
Morante, Roser and Walter Daelemans.
2009. Learning the scope of hedge cues
in biomedical texts. In Proceedings of the
BioNLP 2009 Workshop, pages 28?36,
Boulder, CO.
Morante, Roser and Walter Daelemans. 2011.
Annotating modality and negation for a
machine reading evaluation. In Proceedings
of CLEF 2011, Amsterdam, Netherlands.
Morante, Roser, Vincent Van Asch, and
Walter Daelemans. 2010. Memory-based
resolution of in-sentence scopes of hedge
cues. In Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning (CoNLL-2010): Shared
Task, pages 40?47, Uppsala, Sweden.
Nawaz, Raheel, Paul Thompson, and
Sophia Ananiadou. 2010. Evaluating a
meta-knowledge annotation scheme for
bio-events. In Proceedings of the Workshop
on Negation and Speculation in Natural
Language Processing, pages 69?77, Uppsala.
O?zgu?r, Arzucan and Dragomir R. Radev.
2009. Detecting speculations and their
scopes in scientific text. In Proceedings
of the 2009 Conference on Empirical
Methods in Natural Language Processing,
pages 1398?1407, Singapore.
Palmer, Frank Robert. 1979.Modality and
the English Modals. Longman, London.
Palmer, Frank Robert. 1986.Mood and
Modality. Cambridge University Press,
Cambridge.
Pan, Sinno Jialin and Qiang Yang. 2010.
A survey on transfer learning. IEEE
Transactions on Knowledge and Data
Engineering, 22(10):1345?1359.
Rei, Marek and Ted Briscoe. 2010. Combining
manual rules and supervised learning
for hedge cue and scope detection. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 56?63,
Uppsala.
Rizomilioti, Vassiliki. 2006. Exploring
epistemic modality in academic discourse
using corpora. In Elisabet Arno? Macia,
Antonia Soler Cervera, and Carmen Rueda
Ramos, editors, Information Technology in
Languages for Specific Purposes, volume 7
of Educational Linguistics. Springer US,
New York, pages 53?71.
Rubin, Victoria L. 2010. Epistemic modality:
From uncertainty to certainty in the
context of information seeking as
interactions with texts. Information
Processing & Management, 46(5):533?540.
Rubin, Victoria L., Elizabeth D. Liddy,
and Noriko Kando. 2005. Certainty
identification in texts: Categorization
model and manual tagging results. In
James G. Shanahan, Yan Qu, and Janyce
Wiebe, editors, Computing Attitude and
Affect in Text: Theory and Applications (the
Information Retrieval Series), Springer
Verlag, New York, pages 61?76.
Russell, Stuart J. and Peter Norvig. 2010.
Artificial Intelligence?AModern Approach
(3rd international edition). Upper Saddle
River, NJ: Pearson Education.
Sa?nchez, Liliana Mamani, Baoli Li, and
Carl Vogel. 2010. Exploiting CCG
structures with tree kernels for speculation
detection. In Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning (CoNLL-2010): Shared
Task, pages 126?131, Uppsala.
Saur??, Roser. 2008. A Factuality Profiler
for Eventualities in Text. Ph.D. thesis,
Brandeis University, Waltham, MA.
Saur??, Roser and James Pustejovsky. 2009.
FactBank: A corpus annotated with
event factuality. Language Resources and
Evaluation, 43:227?268.
Settles, Burr, Mark Craven, and Lewis
Friedland. 2008. Active learning with
real annotation costs. In Proceedings of
366
Szarvas et al Cross-Genre and Cross-Domain Detection of Semantic Uncertainty
the NIPS Workshop on Cost-Sensitive
Learning, pages 1?10, Vancouver, Canada.
Shatkay, Hagit, Fengxia Pan, Andrey
Rzhetsky, and W. John Wilbur. 2008.
Multi-dimensional classification of
biomedical text: Toward automated,
practical provision of high-utility
text to diverse users. Bioinformatics,
24(18):2086?2093.
Sun, Chengjie, Lei Lin, Xiaolong Wang,
and Yi Guan. 2007. Using maximum
entropy model to extract protein-protein
interaction information from biomedical
literature. In De-Shuang Huang, Donald C.
Wunsch, Daniel S. Levine, and Kang-Hyun
Jo, editors, Advanced Intelligent Computing
Theories and Applications. With Aspects
of Theoretical and Methodological Issues.
Springer Verlag, Heidelberg,
pages 730?737.
Szarvas, Gyo?rgy. 2008. Hedge classification
in biomedical texts with a weakly
supervised selection of keywords.
In Proceedings of ACL-08: HLT,
pages 281?289, Columbus, OH.
Ta?ckstro?m, Oscar, Sumithra Velupillai,
Martin Hassel, Gunnar Eriksson,
Hercules Dalianis, and Jussi Karlgren.
2010. Uncertainty detection as
approximate max-margin sequence
labelling. In Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning (CoNLL-2010):
Shared Task, pages 84?91, Uppsala.
Tang, Buzhou, Xiaolong Wang, Xuan Wang,
Bo Yuan, and Shixi Fan. 2010. A cascade
method for detecting hedges and their
scope in natural language text. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 13?17,
Uppsala.
Thompson, Paul, Giulia Venturi, John
McNaught, Simonetta Montemagni, and
Sophia Ananiadou. 2008. Categorising
modality in biomedical texts. In Proceedings
of the LREC 2008 Workshop on Building and
Evaluating Resources for Biomedical Text
Mining, pages 27?34, Marrakech, Morocco.
Tjong Kim Sang, Erik. 2010. A baseline
approach for detecting sentences
containing uncertainty. In Proceedings
of the Fourteenth Conference on
Computational Natural Language
Learning (CoNLL-2010): Shared Task,
pages 148?150, Uppsala.
Uzuner, O?zlem, Xiaoran Zhang, and
Tawanda Sibanda. 2009. Machine
learning and rule-based approaches to
assertion classification. Journal of the
American Medical Informatics Association,
16(1):109?115.
Van Asch, Vincent and Walter Daelemans.
2010. Using domain similarity for
performance estimation. In Proceedings of
the 2010 Workshop on Domain Adaptation for
Natural Language Processing, pages 31?36,
Uppsala.
Van Landeghem, Sofie, Yvan Saeys,
Bernard De Baets, and Yves Van de Peer.
2009. Analyzing text in search of
bio-molecular events: A high-precision
machine learning framework. In
Proceedings of the BioNLP 2009 Workshop
Companion Volume for Shared Task,
pages 128?136, Boulder, CO.
Velldal, Erik. 2010. Detecting uncertainty
in biomedical literature: A simple
disambiguation approach using sparse
random indexing. In Proceedings of SMBM
2010, pages 75?83, Cambridge.
Velldal, Erik, Lilja ?vrelid, and Stephan
Oepen. 2010. Resolving speculation:
MaxEnt cue classification and
dependency-based scope rules.
In Proceedings of the Fourteenth Conference
on Computational Natural Language
Learning (CoNLL-2010): Shared Task,
pages 48?55, Uppsala.
Vincze, Veronika, Gyo?rgy Szarvas, Richa?rd
Farkas, Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008. The BioScope Corpus: Biomedical
texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics,
9(Suppl 11):S9.
Wilson, Theresa Ann. 2008. Fine-grained
Subjectivity and Sentiment Analysis:
Recognizing the Intensity, Polarity, and
Attitudes of Private States. Ph.D. thesis,
University of Pittsburgh, PA.
Zhang, Shaodian, Hai Zhao, Guodong Zhou,
and Bao-Liang Lu. 2010. Hedge detection
and scope finding by sequence labeling
with normalized feature selection. In
Proceedings of the Fourteenth Conference on
Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 92?99,
Uppsala.
367
Knowledge Sources for Constituent Parsing
of German, a Morphologically Rich and
Less-Configurational Language
Alexander Fraser?
Institute for NLP, University of Stuttgart
Helmut Schmid??
Institute for NLP, University of Stuttgart
Richa?rd Farkas?
Institute for NLP, University of Stuttgart
Renjing Wang?
Institute for NLP, University of Stuttgart
Hinrich Schu?tze?
Institute for NLP, University of Stuttgart
We study constituent parsing of German, a morphologically rich and less-configurational
language. We use a probabilistic context-free grammar treebank grammar that has been adapted
to the morphologically rich properties of German by markovization and special features added
to its productions. We evaluate the impact of adding lexical knowledge. Then we examine both
monolingual and bilingual approaches to parse reranking. Our reranking parser is the new state
of the art in constituency parsing of the TIGER Treebank. We perform an analysis, concluding
with lessons learned, which apply to parsing other morphologically rich and less-configurational
languages.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: fraser@ims.uni-stuttgart.de.
?? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: schmid@ims.uni-stuttgart.de.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: farkas@ims.uni-stuttgart.de.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany.
Submission received: October 1, 2011; revised submission received: May 30, 2012; accepted for publication:
August 3, 2012
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
1. Introduction
A large part of the methodology for parsing in natural language processing has been
developed for English and a majority of publications on parsing are about parsing
of English. English is a strongly configurational language. Nearly all of the syntactic
information needed by anyNLP application can be obtained by configurational analysis
(e.g., by having a correct constituent parse).
Many other languages of the world are fundamentally different from English in this
respect. At the other end of the configurational?nonconfigurational spectrum we find a
language like Hungarian that has very little fixed structure on the level of the sentence.
Leaving aside the issue of the internal structure of NPs, most sentence-level syntactic
information in Hungarian is conveyed by morphology, not by configuration.
In this paper, we address German, a third type of language that is intermediate
between English and Hungarian. German has strong configurational constraints (e.g.,
main clauses are verb-second) as well as rich derivational and inflectional morphology,
all of which must be modeled for high-quality parsing. German?s intermediate status
raises a number of interesting issues in parsing that are of particular prominence for a
mixed configurational/morphological language, but are?as we will argue?of general
relevance for morphologically rich languages. Partly this is the case because there are
few (if any) languages archetypical of being purely configurational and purely noncon-
figurational (e.g., morphology is also important for English and even Hungarian has
configurational constraints). For lack of a better termwe refer to intermediate languages
as typified by German as MR&LC for morphologically rich and less-configurational.
Part of the motivation for this special issue is that most work on parsing to date
has been done on English, a morphologically simple language. As computational lin-
guistics broadens its focus beyond English it becomes important to take a more general
approach to parsing that can handle languages that are typologically very different from
English. Rich morphology (RM) is one very salient characteristic of a language that
affects the design of parsing methods. We argue that there are two other properties
of languages that are relevant in a discussion of parsing RM languages: syncretism
and configurationality. These two properties are correlated typologically with RM and
should therefore be taken into account when we address parsing RM languages.1
We first define the three properties and explain their relevance for parsing. The
large number of languages for which this correlation holds can be ordered along a
single dimension that can be interpreted as degree of morphological complexity. We
give examples for a number of languages that are positioned at different points on this
scale. Finally, we argue that just as languages that are at the opposite end of the spectrum
from English (prototypical examples of morphological richness like Hungarian) require
parsing methods that can be quite different from those optimal for English, the same
is true for a language like German that is in the middle of the spectrum?and what is
required is in some respects different from what is optimal for one extreme (English) or
the other (Hungarian).
The three correlated properties are rich morphology, syncretism, and configura-
tionality. Morphological richness can be roughly measured by the number of different
morphological forms a word of a particular syntactic category can have; for example,
1 We note, however, that this relationship is not a language universal. It is instead a frequently observed
correlation; for Chinese, for instance, the correlation does not seem to hold as strongly.
58
Fraser et al Knowledge Sources for Parsing German
a typical English noun has two forms (singular and plural), a typical German noun
has eight forms (singular and plural in four different cases), and a typical Hungarian
noun has several hundreds of forms. Syncretism refers to the fact that different mor-
phological forms have identical surface realization; for example, the formMann (?man?
in German) can be the nominative, dative, or accusative singular of Mann depending
on context. Configurationality refers to the degree to which the arrangement of words
and phrases of a particular syntactic function in a sentence is fixed. English is highly
configurational: it has limited flexibility in how the major phrases in a sentence (subject,
verb, direct object, indirect object, etc.) can be ordered. Hungarian and Latin are highly
flexible: Even though there are pragmatic constraints, in principle a large number of
possible orderings are grammatical. German is less configurational. It has some strict
constraints (verb second in main clauses, verb final in subordinate clauses), but also
some properties of a nonconfigurational language; for example, ordering of phrases
in the mittelfeld (the part of the main clause enclosed by the two parts of the verbal
complex) is very flexible.
It is obvious why configurationality and rich morphology are typologically (neg-
atively) correlated. Rich morphology specifies the syntactic role of a phrase in the
sentence, so fixing a position is not required, and many morphologically rich languages
therefore do not fix the position. Conversely, simple morphology gives little specific
information about the role of words and phrases in the sentence. One device often used
by morphologically simple languages to address this problem and reduce widespread
ambiguity is to fix the order of words and phrases in the sentence.
Syncretism has an effect that is similar to simplification of complex morphology.
Simple morphology is unspecific about grammatical function because it uses a small
number of morphological categories. Syncretism is unspecific about grammatical func-
tion because it suffers from a high degree of ambiguity. Even though the number of
different morphological categories is potentially large, syncretic forms conflate many of
these categories, so that these forms are much less helpful in determining grammatical
function than forms in a nonsyncretic language with the same number of categories.
Again, to counteract the communicative difficulties that lack of morphological speci-
ficity would create, stricter constraints on ordering and configuration are often used by
syncretic languages.
We have used English and Hungarian as examples for the extremes and German
for the middle of the spectrum. We now give examples of other languages and their
positions on the scale. Dutch is similar to German in that it also is verb second in main
clauses and verb final in subordinate clauses. The order of arguments in the mittelfeld is
much more restricted than in German, however. At the same time, Dutch morphology
has been much more simplified in the last centuries than German morphology. This
nicely confirms the correlation between RM and configurationality. Thus, Dutch is
positioned between English and German on the scale.
Classical Arabic is somewhat similar to German: The number of different morpho-
logical forms is roughly comparable to German and it allows a number of different
word orders. Modern Standard Arabic speakers rarely mark case, however, at least not
in spontaneous speech. At the same time, Modern Standard Arabic speakers use SVO
order much more frequently and consistently than is the case in Classical Arabic. Thus,
Classical Arabic is roughly at the same position as German on the scale whereas spoken
Modern Standard Arabic may be more comparable to Dutch.
Finally, Modern Greek is a language that is intermediate between German and
Hungarian. It has richer morphology than German, but it has a fair amount of syn-
cretism and therefore more morphological ambiguity than Hungarian. SVO is the
59
Computational Linguistics Volume 39, Number 1
predominant word order in modern Greek, but other word orders can be used. The
order within the noun phrase is more flexible than in German: Adjectives can precede
or follow the noun.
In the examples we have given, the amount of information conveyed by a mor-
phological form is negatively correlated with the amount of information conveyed by
configuration. If morphology conveys a lot of information (due to a large number of
distinctions and the lack of syncretism), then word order is freer and conveys less
information. If morphology conveys less information (due to fewer distinctions or more
syncretism), then configuration is fixed and provides more information to the speaker.
This suggests that RM and configuration are important variables that should be taken
into account in the design of parsing methods. In addition to looking at the extremes of
the spectrum that are exemplified by English andHungarian, we should also investigate
the middle: morphologically (somewhat) rich languages that are less configurational. In
this article, we look at the example of German.
One key question for MR&LC parsing is which type of parsing formalism to adopt,
constituency or dependency. It is a widely held belief that dependency structures are
better suited to represent syntactic analyses for morphologically rich languages because
they allow non-projective structures (the equivalent of discontinuous constituents in
constituency parsing). As Tsarfaty et al (2010) point out, however, this is not the same
as proving that dependency parsers function better than constituency parsers for pars-
ing morphologically rich languages. In fact, most state-of-the-art dependency parsers
(McDonald and Pereira 2006; Hall and Nivre 2008; Seeker et al 2010a) generate purely
projective dependency structures that are optionally transformed into non-projective
structures in a post-processing step. Comparable post-processing techniques have been
used in English constituency parsing (Gabbard, Marcus, and Kulick 2006; Schmid 2006;
Cai, Chiang, and Goldberg 2011) to identify discontinuous constituents andmight work
for other languages, as well.
The overview paper of the Parsing German Shared Task (Ku?bler 2008) reports
higher accuracies for detecting grammatical functions with dependency parsers than
with constituent parsers, but the direct comparison is not fair as it required phrase
boundaries to be correct on the constituent side while the tokens were the unit of
evaluation on the dependency side.2 How to carry out an absolutely fair comparison
of the two representations is still an open research question.3
Constituent parses often provide more information than dependency parses. An
example is the coordination ambiguity in old men and women versus old men and children.
The correct constituent parse for the first expression contains a coordination at the
noun level whereas the parse for the second expression coordinates at the level of
NPs. The dependency structures of both expressions, on the other hand, are usually
identical and thus unable to reflect the fact that oldmodifies women but not children. It is
possible, in principle, to encode the difference in dependency trees (cf. Rambow 2010),
2 This is due to how the evalb tool used to calculate PARSEVAL works. If a constituent is not perfectly
matched, the grammatical function is considered to be wrong, even if there was a partial match (at the
token level). This is not a problem with dependency-based evaluation. For further discussion of the
PARSEVAL metric and dependency-based evaluation see, for example, Rehbein and van Genabith (2007)
and Tsarfaty, Nivre, and Andersson (2012).
3 Two possible solutions are to use TedEval (Tsarfaty, Nivre, and Andersson 2012), or to conduct an analysis
of grammatical functions at the token level in a consistent fashion for both dependency and constituent
parsers. In our case, the latter would require a high quality conversion from the Tiger constituency
representation to a dependency representation, which we hope to implement in future work.
60
Fraser et al Knowledge Sources for Parsing German
for example, by enriching the edge labels, but the constituent representation is simpler
for this phenomenon.
Finally, there are some applications that need constituent parses rather than depen-
dency parses. For instance, many hierarchical statistical machine translation systems
use constituency parses, requiring the output of a dependency parser to be transformed
into a constituent parse.4 We conclude that there is no clear evidence for preferring
dependency parsing over constituency parsing in analyzing languages with RM and
instead argue that research in both frameworks is important.
We view the detailed description of a constituency parsing system for a mor-
phologically rich language, a system that addresses the major problems that arise in
constituency parsing for MR&LC, as one of our main contributions in this paper.
The first problem we address is the proliferation of phrase structure rules in
MR&LC languages. For example, there are a large number of possible orderings of the
phrases in the German mittelfeld, and many orderings are exceedingly rare. A standard
constituency parser cannot estimate probabilities for the corresponding rules reliably.
The solution we adopt here is markovization?complex rules are decomposed
into small unidirectional rules that can be modeled and estimated more reliably than
complex rules. Although markovization in itself is not new, we stress its importance for
MR&LC languages here and present a detailed, reproducible account of how we use it
for German. Markovization combines the best of both worlds for MR&LC languages:
Preferential configurational information can be formalized and exploited by the parser
without incurring too large of a performance penalty due to sparse data problems.
The second problem that needs to be addressed in parsingmanyMR&LC languages
is widespread syncretism. We mainly address syncretism by using a high performance
finite-state automata-based morphological analyzer. Such an analyzer is of obvious
importance for any morphologically rich language because the productivity of mor-
phologically rich languages significantly increases the unknown-word rate in new text
versus morphologically poor languages. So the parser cannot simply memorize the
grammatical properties of words in the Treebank used for training. Instead we incorpo-
rate a complex guesser into our parser that, based on the input from the morphological
analyzer, predicts the grammatical properties of new words and (equally important)
unobserved grammatical properties of known words. With prevailing syncretism, this
task is muchmore complex than in a language where case, gender, number, and so forth,
can be deterministically deduced from morphology.
The morphological analyzer is based on (i) a finite state formalization of German
morphology and (ii) a large lexicon of morphologically analyzed German words. We
refer to these two components together as lexical knowledge. We show that lexical
knowledge is beneficial for parsing performance for an MR&LC language like German.
In addition to lexical knowledge, there is a second important aspect of syncretism
that needs to be addressed in MR&LC languages. Syntactic disambiguation in these
languages must always involve both systems of grammatical encoding, morphology
and configuration, acting together. The most natural way of doing this in a language
like German is to perform this integration of the two knowledge sources directly as part
of parsing.We do this by annotating constituent labels with grammatical functionwhere
appropriate. In contrast with syntactic parses of strongly configurational languages
like English, syntactic parses of German are not useful for most tasks without having
4 We do note, however, that there are a few translation systems which use a dependency representation
directly (e.g., Quirk, Menezes, and Cherry 2005; Shen, Xu, and Weischedel 2008; Tu et al 2010).
61
Computational Linguistics Volume 39, Number 1
grammatical functions indicated. It is not even possible to access the basic subcatego-
rization of the verb (such as determining the subject) without grammatical functions.
We argue that MR&LC languages like German should always be evaluated on labels-
cum-grammatical-function.
Our last main contribution in this paper concerns the fact that we believe that
MR&LC languages give rise to more ambiguity than languages that are predominantly
configurational or morphological. As an example consider the German sentence ?Die
[the] Katze [cat] jagt [hunts] die [the] Schlange [snake].? In German either the cat or the
snake can be the hunter. This type of ambiguity neither occurs in a strongly configu-
rational language like English (where configuration determines grammatical function)
nor in a morphologically rich language like Hungarian that has no or little syncretism
(where morphology determines grammatical function). Although morphology and
configuration in MR&LC languages often work hand in hand for complete disambigua-
tion, there are also many sentences where neither of the two provides the necessary
information for disambiguation. We believe that this distinguishing characteristic of
MR&LC languages makes it necessary to tap additional knowledge sources. In this
paper, we look at two such knowledge sources: monolingual reranking (which captures
global properties of well-formed parses for additional disambiguation) and bilingual
reranking (which exploits parallel text in a different language for disambiguation).
For monolingual reranking, we define a novel set of rich features based on sub-
categorization frames. We compare our compact feature set with a sparse feature set
designed for German previously by Versley and Rehbein (2009). We show that the
richer subcategorization-based framework for monolingual reranking is effective; it has
comparable performance to the sparse feature set?moreover, they complement each
other.
For bilingual reranking, we present our approach to bitext parsing, where a German
parse is found that minimizes syntactic divergence with an automatically generated
parse of its English translation. We pursue this approach for a number of reasons. First,
one limiting factor for syntactic approaches to statistical machine translation is parse
quality (Quirk and Corston-Oliver 2006). Improved parses of bitext should result in
improved machine translation. Second, as more and more texts are available in several
languages, it will be increasingly the case that a text to be parsed is itself part of a
bitext. Third, we hope that the improved parses of bitext can serve as higher quality
training data for improving monolingual parsing using a process similar to self-training
(McClosky, Charniak, and Johnson 2006a).
We show that the three different knowledge sources we use in this paper (lexical
knowledge, monolingual features, and bilingual features) are valuable separately. We
also show that the gain of the two sets of reranking features (monolingual and bilingual)
is additive, suggesting that they capture different types of information.
The resulting parser is currently the best constituent parser for German (with or
without bilingual features). In particular, we show that the baseline parser without
reranking is competitive with the previous state of the art (the Berkeley parser) and
that the re-ranking can add an important gain.
2. Previous Work
Constituent parsing for English is well studied. The best generative constituent parsers
are currently the Brown reranking parser (Charniak and Johnson 2005), the exten-
sion of this parser with self training by McClosky, Charniak, and Johnson (2006b),
and the parser of Petrov and Klein (2007), which is an unlexicalized probabilistic
62
Fraser et al Knowledge Sources for Parsing German
context-free grammar (PCFG) parser with latent feature annotations. Charniak and
Johnson (2005) and Huang (2008) have introduced a significant improvement by
feature-rich discriminative reranking as well.
The number of treebank constituent parsers for German is smaller. Dubey and
Keller (2003) adapted Collins?s (1997) lexicalized parser to German. An unlexicalized
PCFG parser similar to our generative parser was presented by Schiehlen (2004). The
best constituent parser participating in the ACL-08 Workshop on Parsing German
(Ku?bler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser
was also adapted to German (Rafferty andManning 2008). German dependency parsers
have been developed by Menzel and Schro?der (1998), Duchier and Debusmann (2001),
Hall and Nivre (2008), Henderson et al (2008), and Seeker et al (2010a), to name
a few.
There is also some previous work on German parse reranking. Forst (2007) pre-
sented a reranker for German LFG parsing, and Dreyer, Smith, and Smith (2006) applied
reranking to German dependency parsing. Versley and Rehbein (2009) developed a
reranking method for German constituent parsers. The work by Versley and Rehbein
and by Schiehlen (2004) is closest to ours. Like them, we rerank the unlexicalized BitPar
parser. We also refine treebank labels to increase parsing performance, but add more
information and achieve a larger improvement. We use the monolingual feature set of
Versley and Rehbein in our reranker, but add further monolingual features as well as
bilingual features.
3. Generative Parsing Framework
Our generative parser is an unlexicalized PCFG parser which is based on the BitPar
parser (Schmid 2004). BitPar uses a fast bitvector-based implementation of the well-
known Cocke-Younger-Kasami algorithm and stores the chart as a large bit vector.
This representation is memory efficient and allows full parsing (without search space
pruning) with large treebank grammars. BitPar is also quite fast because the basic
parsing operations are parallelized by means of (single-instruction) and-operations on
bitvectors. BitPar can either be used to compute the most likely parse (Viterbi parse), or
the full set of parses in the form of a parse forest, or the n-best parse trees.
3.1 Grammar
The grammar and lexicon used by our generative parser are extracted from the Tiger2
Treebank (Brants et al 2002). Similar to Johnson (1998) andKlein andManning (2003) we
improve the accuracy of the unlexicalized parser by refining the non-terminal symbols
of the grammar to encode relevant contextual information. This refinement weakens
the strong independence assumptions of PCFGs and improves parsing accuracy. The
extraction of the grammar and lexicon involves the following steps:
1. Discontinuous constituents are eliminated (Section 3.2).
2. Treebank annotations are transformed (Section 3.4) and augmented
(Section 3.5).
3. Grammar rules, lexical rules, and their frequencies are extracted from the
annotated parse trees.
4. The grammar is markovized (Section 3.6).
63
Computational Linguistics Volume 39, Number 1
.
S-TOP
PROPAV-OP-1
Daraus
This-from
VMFIN-HD
kann
can
VP-OC
VP-OC
PROAV-OP
*T*-1
VVPP-HD
gefolgert
concluded
VAINF-HD
werden
be
Figure 1
Projectivized parse tree for the sentence: Daraus kann gefolgert werden [From this can be
concluded].
3.2 Raising for Non-Projectivity
The Tiger2 Treebank that we used in our experiments contains discontinuous con-
stituents. As in other work on German parsing using the Tiger2 Treebank (Dubey
and Keller 2003; Schiehlen 2004; Ku?bler, Hinrichs, and Maier 2006), we eliminated
discontinuous constituents by raising those parts of the discontinuous constituent that
do not contain the head to the child position of an ancestor node of the discontinuous
constituent. Hsu (2010) compared three different Tiger2 conversion schemes and found
raising to be the most effective. The projective parse tree in Figure 1, for instance, is
obtained from a Tiger parse tree where the pronominal adverb Daraus was a dis-
continuous child of the lower VP-OC node.
The parse tree in Figure 1 shows a trace node and coreference indices (similar to
the Penn Treebank annotation style for discontinuous constituents). If slash features
are added to the nodes on the path between the PROAV node and its trace within the
VP, it is possible to restore discontinuous constituents (Schmid 2006). Due to sparse
data problems caused by the added slash features, however, the parsing accuracy
drops by 1.5% compared with the version without slash features (when evaluated on
projectivized parse trees). Traces are recognized with a precision of 53% and a recall of
33%. The correct antecedents are identified with a precision of 48% and a recall of 30%.
These figures indicate that the identification of discontinuous constituents in Tiger parse
trees is a harder task than in English Penn Treebank parses, considering the 84% F-score
for the recognition of empty constituents and the 77% F-score for the identification of
antecedents reported in Schmid (2006) for an analogous approach.
As the example in Figure 1 shows, the precise attachment point of constituents
is often not required: We can simply assume that all constituents appearing at the S
level are dependents of the main verb of the clause. Only for modifiers with scope
ambiguities (e.g., negation particles) is it relevant whether they are attached at the S
or VP level. These considerations suggest that it is better to recognize discontinuous
constituents in a post-processing step as in Johnson (2001), Campbell (2004), and Levy
and Manning (2004). In the rest of the paper, we will only work with parse trees from
which coreference indices and trace nodes have been removed.
3.3 Morphological Features and Grammatical Functions
The Tiger2 Treebank annotates non-terminals not only with syntactic categories but
also with grammatical function labels such as SB (subject), OA (accusative object), or
64
Fraser et al Knowledge Sources for Parsing German
MO (modifier). These labels provide important information that is necessary in order to
derive a semantic representation from a parse. It is not possible to infer the grammatical
role of a constituent from its position in the parse tree alone (as can be done in English,
for instance). Case information is needed in addition in order to help determine the
correct grammatical role. The Tiger2 Treebank provides case, number, degree (positive,
comparative, superlative), and gender information at the part-of-speech (POS) level.
Our parser concatenates the grammatical function labels as well as the case infor-
mation of the POS tags to the base labels similarly to Dubey (2004) and Versley (2005).
Our earlier experiments showed that adding case information increases F-score by 2.1%
absolute. Further enriching the grammar with morphological features, however, hurts
performance. Adding number features decreased F-score by 0.5%. Adding number,
gender, and degree decreased F-score by 1.6%. When grammatical functions are taken
into account in the evaluation, the performance drops by 1.5% when number, gender,
and degree features are incorporated. It seems that the additional information supplied
by the agreement features is not useful enough to outweigh sparse data problems
caused by the more fine-grained label set. Therefore we only use case, but designing
a smoothing procedure allowing us to use number, gender, and degree is interesting
future work.
3.4 Tree Transformations
Similarly to Schiehlen (2004), we automatically augment the Tiger2 annotation with ad-
ditional feature annotations. Our feature annotation set is larger than that of Schiehlen.
In addition to making feature annotations, we also perform some tree transformations
that reduce the complexity of the grammar. In all evaluations, we use the original
(projectivized) Tiger parse trees as gold standard and convert the parse trees generated
by our parser to the same format by undoing the transformations and removing the
additional features. In the rest of this section, we explain the tree transformations that
we used. The following section describes the feature annotations.5
Unary branching rules. The Tiger Treebank avoids unary branching nodes. NPs
and other phrasal categories which dominate just a single node are usually omitted.
The sentence Sie zo?gern [They hesitate], for instance, is analyzed as (S-TOP (PPER-SB
Sie) (VVFIN-HD zo?gern)) without an explicit NP or VP. The lack of unary branching
nodes increases the number of rules because now a rule S-TOP? PPER-SB VVFIN-HD
is needed in addition to the rule S-TOP? NP-SB VVFIN-HD, for instance.
In order to reduce sparse-data problems, we insert unary branching nodes and
transform this parse to (S-TOP (NP-SB (PPER-HD Sie)) (VVFIN-HD zo?gern)) by adding
an NP node with the grammatical function (GF) of the pronoun. The GF of the pronoun,
in turn, is replaced by HD (head). Such unary branching NPs are added on top of nouns
(NN), pronouns (PPER, PDS, PIS, PRELS), cardinals (CARD), and complex proper
names (PN) that are dominated by S, VP, TOP, or DL6 nodes.7 The transformation is
reversible, which allows the original annotation to be restored.
5 Descriptions of the different symbols used in the Tiger annotation scheme are available at
http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
6 DL is a discourse level constituent.
7 If a single proper name (NE) forms a noun phrase, we first add a PN node and then an NP node on top.
If a simple noun (NN) with a GF other than NK appears inside of an NP, PP, CNP, CO, or AP, we also add
an NP node on top of it. Similarly, we add a PN node on top of proper names (NE) in the same context.
65
Computational Linguistics Volume 39, Number 1
.
CPP-MO
KON-CD
weder
neither
PP-CJ
APPR-AC
in
in
NE-NK
Berlin
Berlin
KON-CD
noch
nor
PP-CJ
APPR-AC
in
in
NE-NK
Frankfurt
Frankfurt
.
CPP-MO
KON-CD/weder
weder
neither
PP-MO
APPR-AC/in
in
in
NE-HD
Berlin
Berlin
KON-CD/noch
noch
nor
PP-MO
APPR-AC/in
in
in
NE-HD
Frankfurt
Frankfurt
Figure 2
Parse of the phrase weder in Berlin noch in Frankfurt [neither in Berlin nor in Frankfurt] before
and after selective lexicalization of prepositions and conjunctions. This example also shows the
replacement of the grammatical function features CJ and NK discussed in the previous section.
The modified parts are printed in boldface.
By adding a unary-branching NP-SB node, for instance, we introduce an additional
independence assumption, namely, we assume that the expansion of the subject NP is
independent of the other arguments and adjuncts of the verb (a plausible assumption
that is confirmed by a performance improvement).
Elimination of NK. Tiger normally uses the grammatical function HD to mark the
head of a phrase. In case of NPs and PPs, however, the GF of the head is NK (noun
kernel). The same GF is also assigned to the adjectives and determiners of the noun
phrase. We replace NK by HD in order to reduce the set of symbols.8
Elimination of CJ. Tiger annotates each conjunct in a coordination with the spe-
cial grammatical function label CJ. We replace CJ by the grammatical function of the
coordinated phrase. This transformation is also reversible.
3.5 Additional Feature Annotations
Selective lexicalization. We mark the POS tags of the frequent prepositions in [in], von
[from, of], auf [on], durch [through, by means of], unter [under], um [around, at] and
their variants regarding capitalization (e.g., Unter) and incorporation of articles (e.g.,
unters, unterm) with a feature which identifies the preposition. This can be seen as a
restricted form of lexicalization. In the same way, we also ?lexicalize? the coordinating
conjunctions (KON-CD) sowohl [as well], als [as], weder [neither], noch [nor], entweder
[either], and oder [or] if preceded by entweder. Figure 2 shows an example.
Sentence punctuation. If a clause node (S) has a sibling node labeled with POS tag
?$.? that dominates a question mark or exclamation mark, then the clause node and the
POS tag are annotated with quest or excl, so the grammar models different clause types.
8 The original annotation can be restored: HD never occurs in NP or PP children in original Tiger parses.
66
Fraser et al Knowledge Sources for Parsing German
.
CS-RC
S-RC
NP-SB/rel
PRELS-HD-Nom
die
who
NP-OA
NN-HD-ACC
Surfen
surfing
VVFIN-HD
sagen
say
KON-CD
und
and
S-RC/norel/nosubj
NP-OA
NN-HD-Acc
Freiheit
freedom
VVFIN-HD
meinen
mean
Figure 3
Parse of the phrase die Surfen sagen und Freiheit meinen [who say surfing and mean freedom] before
and after annotation with relative clause features. This example also shows the nosubj feature,
which will be discussed later.
Adjunct attachment. Adjuncts often differ with respect to their preferred attach-
ment sites. Therefore, we annotate PPs and adverbials (AVP, ADV, ADJD) with one of
the features N, V, or 0 which indicate a nominal parent (NP or PP), a verbal parent
(VP, S), or anything else, respectively. In case of adverbial phrases (AVP), the label is
propagated to the head child.
Relative clause features. In many relative clauses (S-RC), the relative pronoun
(PRELS, PRELAT, PWAV, PWS) is embedded inside of another constituent. In this case,
all nodes on the path between the pronoun and the clause node are marked with the
feature rel. Furthermore, we add a feature norel to relative clauses if no relative pronoun
is found. Figure 3 shows an example.
Wh features. Similar to the feature rel assigned to phrases that dominate a relative
pronoun, we use a feature wh which is assigned to all NPs and PPs which immediately
dominate a wh-pronoun (PWAT, PWS, PWAV). This feature better restricts the positions
where such NPs and PPs can occur.
Noun sequence feature. If two nouns occur together within a GermanNP (as in drei
Liter Milch [three liters (of) milk] or Ende Januar [end (of) January]), then the first noun
is usually a kind of measure noun. We mark it with the feature seq.
Proper name chunks. Some noun phrases such as Frankfurter Rundschau, Junge
Union, Die Zeit are used as proper names. In this case, the grammatical function of the
NP is PNC. In order to restrict the nouns and adjectives that can occur inside of such
proper name chunks, we mark their POS tags with the feature name.
Predicative APs. Complex adjectival phrases (AP) are either attributively used as
noun modifiers inside of an NP or PP, or predicatively elsewhere. In order to better
model the two types of APs, we mark APs that dominate a predicative adjective (ADJD)
with the feature pred.9
Nominal heads of APs. Sometimes the head of an AP is a noun as in (AP drei
Millionen) Mark [three million Marks] or ein (AP politisch Verfolgter) [a politically
persecuted-person]. We mark these APs with the feature nom.
Year numbers.Years such as 1998 can appear in places where other numbers cannot.
Therefore POS tags of numbers between 1900 and 2019 are marked with year.10
Clause type feature for conjunctions. The type of a subordinate clause and the
subordinating conjunction are highly correlated. German object clauses (S-OC) usually
9 We also mark an AP parent of a node with the label AP-HD/pred in the same way.
10 For some texts, it might be advantageous to use a broader definition of year numbers.
67
Computational Linguistics Volume 39, Number 1
start with dass [that] or ob [whether]; modifier clauses (S-MO) often start with wenn
[if], weil [because], or als [when]. We mark subordinating conjunctions of argument
clauses (S-OC), modifier clauses (S-MO), subject clauses (S-SB), and dislocated clauses
(S-RE) with a feature (OC,MO, SB, or RE) identifying clause type. Without this feature,
argument clauses of nouns, for instance, are often misanalyzed as modifiers of the main
clause.
VP features. VPs that are headed by finite verbs, infinitives, past participles, imper-
atives, and zu infinitives are all used in different contexts. Therefore we mark object VPs
(VP-OC) with a corresponding feature. When parsing the sentence Alle Ra?ume mu?ssen
mehrfach gesa?ubert und desinfiziert werden [all rooms must multiply cleaned and disin-
fected be; all rooms must be ...], this feature allows the parser to correctly coordinate the
two past participle VPs mehrfach gesa?ubert and desinfiziert instead of the past participle
VP mehrfach gesa?ubert and the infinitival VP desinfiziert werden.
Phrases without a head. Some phrases in the Tiger corpus lack a head. This is
frequent in coordinations. All phrases that do not have a child node with one of the
grammatical functions HD, PNC, AC, AVC, NMC, PH, PD, ADC, UC, or DH aremarked
with the feature nohead.
Clauses without a subject. We also mark conjunct clauses with the feature nosubj
if they are neither headed by an imperative nor contain a child node with the gram-
matical function SB (subject) or EP (expletive). This is useful in order to correctly parse
coordinations where the subject is dropped in the second conjunct.
3.6 Markovization
The Tiger Treebank uses rather flat structures where nodes have up to 25 child nodes.
This causes sparse data problems because only some of the possible rules of that length
actually appear in the training corpus. The sparse data problem is solved bymarkoviza-
tion (Collins 1997; Klein and Manning 2003), which splits long rules into a set of shorter
rules. The shorter rules generate the child nodes of the original rule one by one. First,
the left siblings of the head child of the rule are generated from left to right, then the
right siblings are generated from right to left. Finally, the head is generated. Figure 4
shows the markovization of the rule NP? NMNN PP PP.
The auxiliary symbols that are used here encode information about the parent cat-
egory, the head child, and previously generated children. Because all auxiliary symbols
encode the head category, the head is already selected by the first rule, but only later
actually generated by the last rule.
.
NP
NM ?L:NP[NN]NN|NM?
?M:NP[NN]?
?R:NP[NN]PP|PP?
?R:NP[NN]NN|PP?
NN
PP
PP
Figure 4
Markovization of the rule NP? NMNN PP PP.
68
Fraser et al Knowledge Sources for Parsing German
The general form of the auxiliary symbols is ?direction:parent[head]next|previous?
where direction is either L, M, or R, parent is the symbol on the left hand side of the
rule, head is the head on the right hand side of the rule, next is the symbol which will
be generated next, and previous is the symbol that was generated before. Auxiliaries
starting with L generate the children to the left of the head. Auxiliaries starting with
R similarly generate the children to the right of the head and the head itself. The
auxiliary starting with M is used to switch from generating left children to generating
right children. Each rule contains information about the parent, the head, and (usually)
three child symbols (which may include an imaginary boundary symbol). The first rule
encodes the trigram left-boundary NM NN. The second rule is an exception which only
encodes the bigram NM NN. The third rule encodes the trigram PP PP right-boundary.
The last rule is an exception, again, and only encodes NN PP. There is no rule which
covers the trigram consisting of the head and its two immediate neighbors.
Our markovization strategy only transforms rules that occur less than 10 times in
the training data. If one of the auxiliary symbols introduced by the markovization (such
as ?L:NP[NN]NN?NM?) is used less than 20 times (the values of the two thresholds
were optimized on part of the development data) overall, it is replaced by a simpler
symbol ?L:NP[NN]NN? that encodes less context. In this way, we switch from a trigram
model (where the next child depends on the two preceding children) to a bigrammodel
(where it only depends on the preceding child) in order to avoid sparse data problems.
Themethod is similar to the markovization strategy of Klein andManning (2003) except
that they markovize all rules. We simulated their strategy by raising the rule frequency
threshold to a larger value, but obtained worse results. We also tried an alternative
markovization strategy that generates all children left to right (the auxiliary symbols
now lack the direction flag, and the rules cover all possible trigrams), but again obtained
worse results. A disadvantage of our markovization method are spurious ambiguities.
They arise because some of the rules which are not markovized are also covered by
markovization rules.
3.7 Dealing with Unknown Words and Unseen POS Tags
BitPar includes a sophisticated POS guesser that uses several strategies to deal with
unknown words and unseen POS tags of known words. Unknown words are divided
into eight classes11 based on regular expressions that are manually defined. These
classes distinguish between lower-case words, capitalized words, all upper-case words,
hyphenated words, numbers, and so forth. For each word class, BitPar builds a suffix
tree (Weischedel et al 1993; Schmid 1995; Brants 2000) from the suffixes of all words in
the lexicon up to a length of 7. At each node of the suffix tree, it sums up the conditional
POS probabilities (given the word) over all known words with that suffix. By summing
POS probabilities rather than frequencies, all words have the same weight, which is
appropriate here because we need to model the POS probabilities of infrequent words.
BitPar computes POS probability estimates for each node using the sum of probabilities
as a pseudo-frequency for each tag. The estimates are recursively smoothed with the
Witten-Bell method using the smoothed POS probabilities of the parent node as a
backoff probability distribution.12 The suffix trees are pruned by recursively removing
11 We also experimented with more complex classifications, but they failed to improve the results.
12 The number of ?observed? POS tags, which is needed by Witten-Bell smoothing, is defined as the
number of POS tags with a pseudo-frequency larger than 0.5.
69
Computational Linguistics Volume 39, Number 1
leaf nodes whose pseudo-frequency is below 5 or whose weighted information gain13
is below a threshold of 1.
Whenever an unknown word is encountered during parsing, BitPar determines the
word class and obtains the tag probability distribution from the corresponding suffix
tree. BitPar assumes that function words are completely covered by the lexicon and
never guesses function word POS classes for unknown words.
BitPar uses information from the unknown word POS guesser and (if available)
information from an external lexicon (generated by a computational morphology, for
instance, as we will discuss in Section 5.1) in order to predict unobserved POS tags
for known words. First the external lexicon and the lexicon extracted from the training
corpus are merged. Then smoothed probabilities are estimated using Witten-Bell
smoothing with a backoff distribution. The backoff distribution is the average of:
(1) the probability distribution returned by the unknown word POS guesser
if at least one possible POS tag of the word according to the lexicon is an
open-class POS tag,
(2) the average POS probability distribution of all words with exactly the same
set of possible POS tags as the given word14 if at least one of the possible
tags is unseen, and
(3) the prior POS probability distribution if no other word in the lexicon has
the same set of possible POS tags and at least one of the word?s possible
POS tags is unseen.
4. Evaluation of the Generative Parser
As we present each knowledge source, we would like to evaluate it against manually
annotated Treebanks. Our first evaluation shows that our generative parser introduced
in the previous section is comparable with the Berkeley generative parser. Before we
present this comparison in Section 4.1 we discuss evaluating parse accuracy.
In our evaluations, we use the Tiger Treebank (Brants et al 2002) and a small
Europarl Treebank (Pado? and Lapata 2009). We take the first 40,474 sentences of the
Tiger Treebank as training data (Tiger train), the next 5,000 sentences as development
data (Tiger dev), and the last 5,000 sentences as test data (Tiger test). The Europarl
data consists of 662 sentences15 and are either completely used as test data and not di-
vided up or we carried out seven-fold cross-validation experiments with our reranking
models.
All parsers are evaluated on projectivized parse trees. This means that we apply
step 1 of the grammar extraction process described in Section 3.1 to the test parses
and use the result as the gold standard (except for the Pado? set, which is already
projectivized). The test sentences are parsed and the resulting parse trees are converted
13 The weighted information gain is the difference between the entropy of the parent node and the entropy
of the current node, multiplied by the total frequency of the current node and divided by the number of
?observed? POS tags of the current node.
14 A similar pooling of lexicon entries was previously used in the POS tagger of Cutting et al (1992).
15 We use only the sentences in this set which had a single sentence as a translation, so that they could
be used in bilingual reranking, which will be discussed later.
70
Fraser et al Knowledge Sources for Parsing German
to the same format as the gold standard trees by undoing Steps 2, 3, and 4 of Section 3.1.
This conversion involves four steps:
1. Demarkovization removes all the auxiliary nodes introduced by
markovization and raises their children to the next non-auxiliary node.
2. The added unary-branching nodes are eliminated.
3. The original grammatical function labels NK inside of NPs and PPs,
and CJ inside of coordinated phrases, are restored.
4. All feature annotations are deleted.
We use PARSEVAL scores (Black et al 1991) and the standard evaluation tool evalb16
to compare the converted parse trees with the gold standard parse trees using labeled
F-score. We report accuracies for all test sentences and not just sentences of length up to
40. We do not evaluate parsers with gold standard POS tags, but instead automatically
infer them. These considerations make our evaluation setting as close to the real-world
setting as possible.
We report results for evaluations with and without grammatical functions. We
report PARSEVAL scores with grammatical functions inside parentheses after the
results using only basic constituent categories. We believe that grammatical functions
are an important part of the syntactic analysis for any downstream applications in less-
configurational languages such as German because crucial distinctions (e.g., the distinc-
tion between subject and object) are not feasible without them. We should mention that
our results are not directly comparable to previously published results on the Tiger2
corpus (Ku?bler 2008; Versley and Rehbein 2009; Seeker et al 2010b), because each of
the previous studies used different portions of the corpus and there are differences in
the evaluation metric as well. The transformed corpus (in our train, development, and
test split format) and the evaluation scripts we used are available,17 which we hope will
enable direct comparison with our results.
4.1 Comparison of BitPar and Berkeley
The best constituent parser participating in the Parsing German Shared Task (Ku?bler
2008) was the Berkeley parser (Petrov and Klein 2008) and to the best of our knowledge
it has achieved the best published accuracy for German constituency parsing so far.
The Berkeley parser takes an automated approach, in which each constituent symbol is
split into subsymbols applying an expectation-maximization method. We compare our
manually enriched grammar to this automatic approach.
We trained the Berkeley parser on Tiger train using the basic constituent categories
concatenated to the grammatical function labels as starting symbols. We found that it
achieved the best PARSEVAL scores on Tiger dev after the fourth iteration. This model
was used for parsing Tiger dev, Tiger test, and the Europarl corpus.
BitPar achieved 82.51 (72.46), 76.67 (65.61), and 77.13 (66.06), and the Berkeley
parser achieved 82.76 (73.20), 76.37 (65.66), and 75.51 (63.3) on the three corpora,
respectively. In general, these results indicate that these two parsers are competitive.
On the other hand, the fact that the results of the Berkeley parser are much worse than
16 http://nlp.cs.nyu.edu/evalb/, 2008 version.
17 See http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
71
Computational Linguistics Volume 39, Number 1
BitPar on the out-of-domain Europarl corpus indicates that it overfits to the domain
of the training corpus (Tiger2). Following a reviewer suggestion, we looked at the
sentences containing many words not occurring in the training data, and observed that
our lexical resource is strongly helpful for these sentences. Another disadvantage of
the automatic approach of the Berkeley parser is that the resulting subsymbols are not
easily interpretable, which can hinder defining features for parse reranking using them.
Based on these considerations, we decided to use BitPar in our reranking experiments.
The combination of the two radically different approaches (linguistically motivated
grammar extensions and automatic symbol splitting) is a rather promising area of
research for improving parsing accuracy, which we plan to address in future work.
5. Impact of Our Lexical Resource
5.1 Integration of SMOR with BitPar
There are a large number of inflectedword forms formanyGerman lemmas. This causes
sparse data problems if some forms are not observed in the training data. BitPar applies
the heuristics described in Section 3.7 to obtain POS probabilities for unseen words.
Although these heuristics seem to work quite well, we expect better results if the parser
has access to information from a morphological analyzer.
We use the German finite-state morphology SMOR (Schmid, Fitschen, and Heid
2004) to provide sets of possible POS tags for all words. SMOR covers inflection, deriva-
tion, and compounding and achieves good coverage in combination with the stem
lexicon IMSLex (Lezius, Dipper, and Fitschen 2000). SMOR is integrated into the parser
in the following way. We create a combined word list from the training and testing
data18 and analyze it with SMOR. The SMOR analyses are then mapped to the POS
tag set used by the parser, and supplied to BitPar as an external lexicon (see Section 3.7).
Consider the example word erlischt [goes out], which did not appear in the train-
ing corpus. SMOR produces the analysis erlo?schen.V.3.Sg.Pres.Ind, which is mapped
to VVFIN-HD and added to the lexicon. Using this entry, BitPar correctly parsed
the sentence Die Anzeige erlischt [The display goes out]. Without using SMOR, the
parser analysed erlischt as a past participle because scht is a frequent past participle
ending.
5.2 Effect on In-Domain and Out-of-Domain Parsing
In order to measure the effect of the integration of a German morphology on parsing
accuracy (see Section 5.1), we tested the BitPar parser on the Tiger data and on Europarl
data. The results are summarized in Table 1. They show that the morphology helps on
out-of-domain data (Europarl), but not so much on in-domain data (Tiger). The POS
tagging accuracy, however, also increases on Tiger data by 0.13%. When grammatical
functions are included in the evaluation, the performance improvement more than
doubles on Europarl data. As a result, we decided to use the finite-state morphology
in the rest of the experiments we conducted.
Table 1 also shows that the Tiger test data is harder to parse than the dev data. We
examined the two subcorpora and found that the test data contains longer sentences
18 Because we are only using the words here, and not their POS labels, this approach is methodologically
sound and could be applied to any unparsed data in the same way.
72
Fraser et al Knowledge Sources for Parsing German
Table 1
Effect of using finite-state morphology on parsing accuracy. The values in parentheses are
labeled F-scores from the evaluation with grammatical functions.
morphology Tiger dev Tiger test Europarl
without 82.51 (72.46) 76.67 (65.61) 76.81 (65.31)
with 82.42 (72.36) 76.84 (65.91) 77.13 (66.06)
difference ?0.09 (?0.10) +0.17 (+0.30) +0.32 (+0.75)
(18.4 vs. 15.3 words on average) and that the ratio of unknown words is higher (10.0%
vs. 7.6%).
6. Parse Reranking
The most successful supervised phrase-structure parsers are feature-rich discriminative
parsers that heavily depend on an underlying PCFG grammar (Charniak and Johnson
2005; Huang 2008). These approaches consist of two stages. At the first stage they apply
a PCFG grammar to extract possible parses. The full set of possible parses cannot be
iterated through in practice, and is usually pruned as a consequence. The n-best list
parsers keep just the 50?100 best parses according to the PCFG. Other methods remove
nodes and edges from the packed parse forest whose posterior probability is under a
pre-defined threshold (Charniak and Johnson 2005).
The task of the second stage is to select the best parse from the set of possible
parses (i.e., rerank this set). These methods use a large feature set (usually a few
million features) (Collins 2000; Charniak and Johnson 2005). The n-best list approaches
can straightforwardly use local and non-local features as well because they decide at
the sentence-level (Charniak and Johnson 2005). Involving non-local features is more
complicated in the forest-based approaches. The conditional random field methods
usually use only local features (Yusuke and Jun?ichi 2002; Finkel, Kleeman, and
Manning 2008). Huang (2008) introduced a beam-search and average perceptron-based
procedure incorporating non-local features in a forest-based approach. His empirical
results show only a minor improvement from incorporating non-local features,
however.
In this study, we experiment with n-best list reranking using a maximum entropy
machine learning model for (re)ranking along with local and non-local features. Our
reranking framework follows Charniak and Johnson (2005). At the first-stage of parsing,
we extract the 100 best parses for a sentence according to BitPar?s probability model.
At parsing time, a weight vector w is given for the feature vectors (which numerically
represent one possible parse) and we select the parse with the highest inner product
of these two vectors. The goal of training is to adjust w. In the maximum entropy
framework, this is achieved by solving the optimization problem of maximizing the
posterior probability of the oracle parse?the parse with the highest F-score.19 Our
method aims to select the oracle, as the gold standard parse is often not present in
the 100-best parses.20 Our preliminary experiments showed that parse candidates close
19 Ties are broken using the PCFG probabilities of the parses.
20 The oracle F-score (i.e., the upper limit of 100-best reranking on the Tiger development corpus) is 90.17.
73
Computational Linguistics Volume 39, Number 1
to the oracle confuse training. Hence during training, we removed all parses whose
F-score is closer than 1.0 to the score of the oracle.21
As we discussed in Section 1, the parsing output of morphologically rich languages
is useful only when it is additionally annotated with grammatical functions. The oracle
parses often change if the grammatical function labels are also taken into consideration
at the PARSEVAL score calculation. Hence slightly different objective functions are used
in the two cases. We will report results achieved by reranking models where the oracle
selection for training agrees with the evaluation metric utilized?that is, we trained
different models (which differ in the oracle selection) for the basic constituent label
evaluation and for the evaluation on grammatical functions.
During training we followed an eight-fold cross validation technique for candidate
extraction (Collins 2000). Here, one-eighth of the training corpus was parsed with a
PCFG extracted from seven-eighths of the data set. This provides realistic training
examples for the reranker as these parses were not seen during grammar extraction. We
used the ranking MaxEnt implementation of MALLET (McCallum 2002) with default
parameters.
7. Monolingual Reranking
7.1 Subcategorization-Based Monolingual Reranking Features
We introduce here several novel subcategorization-based features for monolingual
reranking. For this, we first describe our algorithm for extracting subcategorization
(subcat) information. We use our enriched version of the Tiger2 training set. In order
to extract verbal subcat frames we find all nodes labeled with the category S (clause)
or VP-MO (modifying VP) and extract their arguments. Arguments22 are nodes of the
categories shown in Table 2. The arguments of nouns are obtained by looking for NN
nodes which are either dominated by an NP or a PP, and which take a following node
of category PP, VP-OC, or S-OC as argument.
The feature functions we present are mostly lexicalized. This means we need access
to the head words of the arguments. The argument heads are extracted as follows: As
NP headwe take the last nodewhose function label is either HD,NK, or PH. If this node
is of category NP or PN, we recursively select the head of that constituent. Similarly, the
head of an AP is the last node with functional label HD. If it is an AP, the head is
searched inside of it. In the case of PPs, we extract two heads, namely, the preposition
(or postposition) as well as the nominal head of the PP, which is found using similar
rules as for NPs. We also extract the case of the nominal head.
The extraction of verbal heads is somewhat more complicated. In order to obtain
the correct verbal head of a clause irrespective of the verb position (verb-first, verb-
second, verb-final), we extract all verbs that are dominated by the clause and a possibly
empty sequence of VP-OC or VP-PD (statal passive) nodes and an optional VZ-HD
node. Then we take the first non-finite verb, or alternatively the first finite verb if all
verbs were finite. In order to avoid sparse data problems caused by the many different
inflections of German verbs, we lemmatize the verbs.
21 In Fraser, Wang, and Schu?tze (2009) we used Minimum Error Rate Training. Once we made this change
to maximum entropy the results on small feature sets became similar (details omitted).
22 An exception to this is that if a PP argument dominates a node of category PROAV-PH, it is considered
a PROAV-PH argument. An example is the sentence Er [he] wartet [waits] (PP-OP (PROAV-PH darauf
[for this]), (S-RE dass [that] sie [she] kommt [comes])).
74
Fraser et al Knowledge Sources for Parsing German
Table 2
Arguments used in extracted subcategorization frames.
NP-SB, PN-SB, CNP-SB, S-SB, VP-SB subjects
NP-OA, PN-OA, CNP-OA direct objects
NP-DA, PN-DA, CNP-DA indirect objects
PRF-OA reflexive direct objects
PRF-DA reflexive indirect objects
NP-PD, CNP-PD predicative NPs
ADJD-PD, AP-PD, CAP-PD predicative adjectives
S-OC, CS-OC argument clauses
PP-OP, CPP-OP PP arguments
VP-OC/zu infinitival complement clauses
PROAV-OP pronominal adverbs serving as PP proxies such as
daraus [out of this]
NP-EP expletive subjects
VP-RE, NP-RE VP/NP appearing in expletive constructions
In the case of coordinated phrases, we take the head of the first conjunct. Arguments
are sorted to put them in a well-defined order. An example is that given the correct
parse of the sentence Statt [instead of] Details [details] zu [to] nennen [name], hat [has]
er [he] unverdrossen [assiduously] die [the] ?Erfolgsformel? [formula of success] wiederholt
[repeated], meaning ?instead of naming the details, he assiduously repeated the formula
of success,? we extract the two subcat frames:
VP-MO OBJ:Details VZ-HD:zu:nennen
S-TOP VP-MO SUBJ:er OBJ:Erfolgsformel VVPP-HD:wiederholt
We can now describe our features. The features focus on subcat frames taken from
S nodes (VP-MO is treated as S), and on attachment of prepositions and conjunctions to
nouns. We define conditional probability and mutual information (MI) features.
The two conditional probability features are ProbPrepAttach and ProbAdverb-
Attach, which calculate the probability for each preposition or adverb to be attached
to its governor, given the label of the governor. We estimate this from the training
data as follows, for the example of the PP feature. In the feature scoring, we give
each preposition attachment a score which is the negative log10 of the probabil-
ity p(lex prep|label governor) = f (lex prep, label governor)/f (label governor) (with a
cutoff of 5).
For all of our other monolingual features, we use (negative) pointwise mutual
information: ?log10(p(a, b)/p(a)p(b)) (here we use cutoffs of 5 and ?5).
MI NounP and MI NounConj give an assessment of a preposition or a conjunction
being attached to a noun (given the lexicalized preposition and the lexicalized noun).
For the MI VSubcat feature, we use as a the frame (without lexicalization), and as
b the head verb. p(a) is estimated as the relative frequency of this frame over all frames
extracted from Tiger2 train. MI VSimpleSubcat is a simpler version of MI VSubcat.
PP is excluded from frames because PP is often an adjunct rather than an argument.
For the MI VArg feature, we use as a the argument function and the head word
of the argument (e.g., OBJ:Buch, which is ?book? used as an object). As b we again
use the head verb. The estimate of p(a) is frequency(OBJ:Buch)/(total number of
extracted frames).23 In addition, this feature is refined into individual features for
23 We make the assumption that every frame has an object, but that this object can be NULL.
75
Computational Linguistics Volume 39, Number 1
different kinds of arguments: MI VSubj, MI VObj, MI VIobj, MI VPP, MI VPRF,
MI VS OC, MI VVP, and MI VerbPROAV. As an example, the MI of ?lesen, OBJ:Buch?
(reading, object:Book) would be used for the MI VArg features and for the MI VObj
feature. For functions such as MI VPP which are headed by both a function word (here,
a preposition) and a content word, only the function word is used (and no case).
The last MI feature is MI VParticle. Some German verbs contain a separable parti-
cle, which can also be analyzed as an adverb but will then have a different meaning. For
the sentence ?Und [and] Frau [Mrs.] Ku?nast [(proper name)] bringt [brings] das [that] auch
[also] nicht [not] ru?ber [across],? if ?ru?ber? is analyzed as an adverb, the verb means to
carry/take/bring over [to another physical location], but if it is viewed as a particle, the
sentence means Frau Ku?nast is not able to explain this. The feature MI VParticle helps
with this kind of disambiguation.
7.2 The Versley and Rehbein Feature Set
We also carried out experiments with the feature set of Versley and Rehbein (2009),
which is specially designed for German. It consists of features constructed from the
lexicalized parse tree along with features based on external statistical information.
The features here are local in the sense that their values can be computed at the
constituent in question, its daughters, and its spanning words. All features except
the external statistical information are binary and indicate that a lexicalized pattern is
present in the parse. They were originally designed for forest-based reranking (Versley
and Rehbein 2009). Following Charniak and Johnson (2005) we sum up these local
feature values in the parse tree. Thus our versions count the number of times that a
particular pattern occurs in the entire parse tree.
The patterns used can be further subcategorized into three groups. The wordform-
based patterns are token?POS (e.g., one pattern is ?lesen-VVINF?) and the word class
of the token in question (word class comes from an automatic clustering of words based
on contextual features). The constituent-based patterns are the size of the constituent,
the constituent label, and the right-hand side of the derivational rule applied at the node
in question. The last and biggest group of the pattern features is formed by the bilexical
dependencies. They are based on the head word of the constituent node in question
and its daughters. Versley and Rehbein (2009) have also introduced features that exploit
statistical information gathered from an external data set and aim to resolve PP attach-
ment ambiguity. Mutual information values were gathered on the association between
nouns and immediately following prepositions, as well as between prepositions and
closely following verbs on the DE-WaC corpus (Baroni and Kilgarriff 2006). These
feature values were then used at NP?PP and VP?PP daughter attachments.
A total of 2.7 million features fired in the Tiger train. We ignored features firing
in less than five sentences for computational efficiency, resulting in 117,000 extremely
sparse features.
7.3 Monolingual Reranking Experiments
We rerank 100-best lists from BitPar (Schmid 2004), which uses the grammar extraction
procedure and lexical resources introduced in Section 3. In each of the experiments we
extracted the grammar from the Tiger train and used it to obtain the 100-best parses for
the sentences of the evaluation corpus.
We trained reranking models on the Tiger train as described in Section 6 using our
subcategorization-based features, the Versley09 feature set, and the union of these two
76
Fraser et al Knowledge Sources for Parsing German
Table 3
The PARSEVAL score of monolingual features to rerank the parses of Europarl (seven-way
cross-validation on 662 sentences) and Tiger2 (development and test sets).
Tiger dev Tiger test Europarl CROSS Europarl IN
Baseline 82.42 (72.36) 76.84 (65.91) 77.13 (66.06)
subcat 83.19 (73.63) 77.65 (67.21) 77.23 (66.13) 77.73 (66.95)
Versley09 83.56 (73.89) 78.57 (68.42) 77.82 (66.87) 77.62 (66.05)
subcat+Versley09 84.19 (74.96) 78.86 (69.04) 77.76 (66.84) 77.93 (66.75)
sets. We evaluated the models on Tiger dev, Tiger test, and Europarl. As the domains
of Tiger and Europarl are quite different, besides this cross-domain parser evaluation
(CROSS) we carried out an in-domain (IN) evaluation as well. In the latter we followed
the seven-fold cross-validation approach, that is, the reranking models were trained on
six-sevenths of Europarl. The results are presented in Table 3.
The results presented in Table 3 show that the reranking models achieve an im-
provement over the baseline parser using both our and the Versley09 feature sets. The
Versley09 feature set achieved better results than our monolingual features when a
training dataset with sufficient size is given (Tiger). On the other hand using our 16
rich features (compared with 117,000 sparse features) is more suitable for the settings
where only a limited amount of training instances are available (the training sets consist
of 567 sentences of Europarl in seven-fold cross-validation). The rerankingmodels using
the union of the feature sets obtain close to the sum of the improvements of the two in-
dividual feature sets. The subcategorization features model rich non-local information,
and the fine-grained features capture local distinctions well and the features based on
the Web corpus access additional knowledge.
We performed an experiment adding one feature at a time, and found that the
most effective features were ProbAdverbAttach, MI VPP, MI VPRF, MI VSubj, and
MI VArg. After this the variation caused by numeric instability was too high to see a
consistent incremental gain from the rest of the features. We conclude that these features
can be robustly estimated and have more discriminative power than the others, but we
emphasize that we used all features in our experiments.
Figure 5 shows a parse tree produced by the BitPar parser in which the noun phrase
diese Finanzierung is incorrectly classified as an accusative object. The monolingual
subcategorization features MI VSubcat, MI VSimpleSubcat, and MI VArg enable the
reranker to correctly analyze the noun phrase as a subject and to move it from the VP
level to the S level.
.
S-TOP
PWAV-MO
Woher
where-from
VMFIN-HD
soll
should
VP-OC
NP-OA
PDAT-HD
diese
this
NN-HD
Finanzierung
financing
VVINF-HD
kommen
come
Figure 5
Erroneous parse produced by BitPar that is corrected by monolingual features.
77
Computational Linguistics Volume 39, Number 1
8. Bilingual Reranking
We now present our bilingual reranking framework. This follows our previous work
(Fraser, Wang, and Schu?tze 2009), which defined feature functions for reranking
English parses, but now we will use these same feature functions (and three additional
feature functions introduced to capture phenomena higher in the syntactic tree) to
rerank German parses. The intuition for using this type of bitext projection feature is
that ambiguous structures in one language often correspond to unambiguous structures
in another. Our feature functions are functions on the hypothesized English parse e,
the German parse g, and the word alignment a, and they assign a score (varying
between 0 and infinity) that measures syntactic divergence. The alignment of a sentence
pair is a function that, for each English word, returns a set of German words with
which the English word is aligned. Feature function values are calculated either by
taking the negative log of a probability, or by using a heuristic function which scales
similarly.24
The bilingual feature functions we define are functions that measure differ-
ent types of syntactic divergence between an English parse and a German parse.
Charniak and Johnson (2005) defined the state of the art in discriminative n-best
constituency parsing of English syntax (without the use of self-training). The n-best
output of their generative parser is reranked discriminatively by a reranker. We call
this CJRERANK. We will use an array of feature functions measuring the syntactic
divergence of candidate German parses with the projection of the English parse
obtained from CJRERANK.
In our experiments we use the English text of the parallel Treebank extracted from
the Europarl corpus and annotated by Pado? and Lapata (2009). There are 662 German
sentences that are aligned to single English sentences; this is the set that we use. Due to
the limited number of trees, we perform cross-validation to measure performance.
The basic idea behind our feature functions is that any constituent in a sentence
should play approximately the same syntactic role and have a similar span as the corre-
sponding constituent in a translation. If there is an obvious disagreement, it is probably
caused by wrong attachment or other syntactic mistakes in parsing. Sometimes in
translation the syntactic role of a given semantic constituent changes; we assume that
our model penalizes all hypothesized parses equally in this case.
To determine which features to describe here we conducted a greedy feature addi-
tion experiment (adding one feature at a time), on top of our best monolingual system
(combining both subcat and Versley09 feature sets). All bilingual experiments use all of
the features (not just the features we describe here). Definitions are available.25
BitParLogProb (the only monolingual feature used in the bilingual-only experi-
ment) is the negative log probability assigned by BitPar to the German parse.
8.1 Count Feature Functions
Count feature functions count projection constraint violations.
Feature CrdBin counts binary events involving the heads of coordinated phrases. If
we have a coordination where the English CC is aligned only with a German KON, and
24 A probability of 1 is a feature value of 0, whereas a low probability is a feature value which is
 0.
25 See http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
78
Fraser et al Knowledge Sources for Parsing German
Table 4
Other projection features selected; see the previously mentioned Web page25 for precise
definitions.
POSParentPrjWordPerG2E Computes the span difference between all the parent constituents
of POS tags in a German parse and their respective coverage
in the corresponding English parse, measured using percentage
coverage of the sentence in words. The feature value is the sum
of all the differences. The projection direction is from German to
English.
AbovePOSPrjPer Projection direction is from English to German, and measured in
percentage sentence coverage using characters, not words. The
feature value is calculated over all constituents above the POS
level in the English tree.
AbovePOSPrjWord Calculates a length-based difference using words.
POSPar2Prj Only applies when the POS tag?s parent has two children (the
POS tag has only one sibling). Projects from English to German
and calculates a length-based difference in characters.
POSPar2PrjPer Calculates a percentage-based difference based on characters.
POSPar2PrjG2E Like POSPar2Prj except projects from German to English.
POSPar2PrjWordG2E Like POSPar2PrjG2E except uses word-based differences.
both have two siblings, then the value contributed toCrdBin is 1 (indicating a constraint
violation) unless the head of the English left conjunct is aligned with the head of the
German left conjunct and likewise the right conjuncts are aligned.
Feature Q simply captures a mismatch between questions and statements. If a
German sentence is parsed as a question but the parallel English sentence is not, or
vice versa, the feature value is 1; otherwise the value is 0.
Feature S-OC considers that a clausal object (OC) in a German parse should be
projected to a simple declarative clause in English. This feature counts violations.
EngPPinSVP checks whether a PP inside of a S or VP in English attaches to the
same (projected) constituent in German. If an English PP follows immediately a VP or
a single verb, and the whole constituent is labeled ?S? or ?VP,? then the PP should be
identified as governed by the VP. In this case the corresponding German PP should
attach as well to the German VP to which the English VP is projected (attachment in
German can be to the left or to the right). If the governor in German does not turn out to
be a VP or have a tag starting with ?V,? a value of 1 will be added to the feature for this
German parse.
EngLeftSVP checks whether the left sibling of S or VP in English attaches to the
same (projected) constituent in German (where attachment can be left or right). This
feature counts violations.
Span Projection Feature Functions. Span projection features calculate an absolute or
percentage difference between a constituent?s span and the span of its projection. Span
size is measured in characters or words. To project a constituent in a parse, we use the
word alignment to project all word positions covered by the constituent and then look
for the smallest covering constituent in the parse of the parallel sentence.
PPParentPrjWord checks the correctness of PP attachment. It projects all the parents
of PP constituents in an English parse to German, and sums all the span differences. It is
measured in words. In addition to PPParentPrjWord we implement two bonus features,
NonPPWord and NonPPPer. The former simply calculates the number of words that
79
Computational Linguistics Volume 39, Number 1
do not belong to PP phrases in the sentence, and the latter computes the non-PP
proportion in a character-based fashion. These can be thought of as tunable parameters
which adjust PPParentPrjWord to not disfavor large PPs. The other selected projection
features are described in Table 4.
Probabilistic Feature Functions. We use Europarl (Koehn 2005), from which we
extract a parallel corpus of approximately 1.22 million sentence pairs, to estimate
the probabilistic feature functions described in this section.
We describe the feature PTag, despite the fact that it was not selected by the feature
analysis, because several variations (described next) were selected. PTag measures
tagging inconsistency based on estimating the probability for each English word that
it has a particular POS tag, given the aligned German word?s POS tag. To avoid noisy
feature values due to outliers and parse errors, we bound the value of PTag at 5.26 We
use relative frequency to estimate this feature. When an English word is aligned with
two words, estimation is more complex. We heuristically give each English and German
pair one count. The value calculated by the feature function is the geometric mean27 of
the pairwise probabilities.
The feature PTagEParent measures tagging inconsistency based on estimating the
probability that the parent of the English word at position i has a particular tag, given
the aligned German word?s POS label. PTagBiGLeft measures tagging inconsistency
based on estimating the probability for each English word that it has a particular POS
tag, given the aligned German word?s label and the word to the left of the aligned
German word?s label. PTagBiGParent measures tagging inconsistency based on esti-
mating the probability for each English word that it has a particular POS tag, given the
aligned German word?s label and the German word?s parent?s label.
8.2 Bilingual Reranking Experiments
We performed experiments looking at bilingual reranking performance. To train the
parameters of the probabilistic feature functions, we use 1-best parses of the large
Europarl parallel corpus (from CJRERANK and BitPar). We work on the same 100-best
list (of the German sentences in the small Pado? set) as was used in the previous section.
We parse the English sentences of the small Europarl set with CJRERANK; this parse is
used as our bilingual knowledge source. Finally we rerank using the bilingual features
(results in the first row of Table 5).
We then combine the monolingual features with the bilingual features. We rerank
using both the monolingual and the bilingual features together, and the results are
presented in Table 5. The bilingual feature-based reranker achieved 1 percentage point
improvement over the baseline. This advantage was just slightly decreasedwhenmono-
lingual features are also present. This indicates again that themonolingual and bilingual
features can capture different linguistic phenomena and their information content is
rather different. As in the Europarl IN setting, using the large sparse Versley09 feature
set the reranker could not learn a meaningful model from a moderate-sized training
data set.
26 Throughout this paper, assume log(0) = ??.
27 Each English word has the same weight regardless of whether it was aligned with one or with more
German words.
80
Fraser et al Knowledge Sources for Parsing German
Table 5
PARSEVAL scores of bi+monolingual features to rerank the parses of Europarl (seven-way
cross-validation) and the added value of bilingual features over the results achieved by the
corresponding monolingual feature set.
Mono features without bilingual with bilingual added value
NONE 77.13 (66.06) 78.10 (67.12) +0.97 (+1.06)
subcat 77.73 (66.95) 78.54 (67.95) +0.78 (+1.00)
Versley09 77.62 (66.05) 77.71 (66.06) +0.09 (+0.01)
subcat+Versley09 77.93 (66.75) 78.70 (67.45) +0.78 (+0.70)
The parse tree in Figure 6 demonstrates the value of bilingual features. It was
produced by the monolingual reranker and it incorrectly combines the two adverbs aber
and ebenso into an adverbial phrase and places this under the VP. The bilingual reranker
instead attaches the two adverbs separately at the S level. The attachment to the S node
indicates that the two adverbs modify the modal verb kann and not the full verb sagen.
This is triggered by the feature POSPar2Prj.
8.3 Previous Work on Bitext Parsing
Bitext parsing was also addressed by Burkett and Klein (2008). In that work, they use
feature functions defined on triples of (English parse tree, Chinese parse tree, alignment)
which are combined in a log-linear model, much as we do. In later work (Burkett,
Blitzer, and Klein 2010), they developed a unified joint model for solving the same
problem using a weakly synchronized grammar. To train these models they use a small
parallel Treebank that contains gold standard trees for parallel sentences in Chinese
and English, whereas we only require gold standard trees for the language we are
reranking. Another important difference is that Burkett and Klein (2008) use a large
number of automatically generated features (defined in terms of feature generation
templates) whereas we use a small number of carefully designed features that we found
by linguistic analysis of parallel corpora. Burkett, Blitzer, and Klein (2010) use a subset
of the features of Burkett and Klein (2008) for synchronization, along with monolin-
gual parsing and alignment based features. Finally, self-training (McClosky, Charniak,
and Johnson 2006b) is another differentiator of our work. We use probabilities esti-
mated from aligned English CJRERANK parses and German BitPar parses of the large
Europarl corpus in our bilingual feature functions. These feature functions are used to
.
S-TOP
PIS-SB
Man
one
VMFIN-HD
kann
can
VP-OC
AVP-MO
ADV-MO
aber
but
ADV-HD
ebenso
just-as-well
VVINF-HD
sagen
say
,
,
S-OC
KOUS-CP
dass
that
PPER-SB
sie
they
ADJD-PD
anspruchsvoll
demanding
VAFIN-HD
sind
are
Figure 6
Erroneous parse produced by the reranker using only monolingual features, which is corrected
by bilingual features. The sentence means One can, however, just as well say that they are demanding.
81
Computational Linguistics Volume 39, Number 1
improve ranking of German BitPar parses in the held-out test sets, which is a form of
self-training.
Two other interesting studies in this area are those of Fossum and Knight (2008)
and of Huang, Jiang, and Liu (2009). They improve English prepositional phrase at-
tachment using features from a Chinese sentence. Unlike our approach, however, they
do not require a Chinese syntactic parse as the word order in Chinese is sufficient to
unambiguously determine the correct attachment point of the prepositional phrase in
the English sentence without using a Chinese syntactic parse.
We know of no other work that has investigated to what extent monolingual and
bilingual features in parse reranking are complementary. In particular, the work on bi-
text parsing by Burkett and Klein (2008) does not address the question as to whether the
effect of monolingual and bilingual features in parse reranking is (partially) additive.
We demonstrate bilingual improvement for a strong parser of German. Previously,
we showed bilingual improvement for parsing English with an unlexicalized parser
(Fraser, Wang, and Schu?tze 2009), using 34 of the 37 bilingual feature functions we use
in this work.
9. Conclusion
In this paper, we have focused on MR&LC languages like German?languages that
are morphologically rich, but also have a strong configurational component. We have
argued that constituency parsing is, perhaps contrary to conventional wisdom, an ap-
propriate parsing formalism for MR&LC because constituents capture configurational
constraints in a transparent way and because for many applications constituency pars-
ing is preferable to dependency parsing. Our detailed description of a constituency
parsing system for a morphologically rich language, a system that addresses the major
problems that arise in constituency parsing for MR&LC, is one main contribution of this
paper. Two of these problems are rule proliferation and syncretism. We have addressed
rule proliferation bymarkovization and syncretism by (i) deploying a high performance
finite-state-based morphological analyzer that is based on rich lexical knowledge and
(ii) encoding grammatical functions directly as part of the phrase labels. This direct
encoding allows us to directly combine morphological and configurational informa-
tion in parsing and arrive at a maximally disambiguated parse. We argued that this
is the right setup for MR&LC languages because applications must have access to
grammatical functions.
A large part of this paper was concerned with making available and evaluating
additional knowledge sources for improved parsing of the MR&LC language German.
Our motivation was that (as we argued) MR&LC languages have in general higher am-
biguity than purely configurational and purely morphological languages, in particular
with respect to grammatical functions. Apart from the lexical knowledge embedded
in the morphological analyzer, we presented work on two other knowledge sources to
address this type of additional ambiguity: monolingual reranking (which looks at global
sentence-wide constraints for disambiguation) and bitext reranking (which exploits
parallel text for disambiguation). We were able to improve the performance of a strong
baseline parser using these three knowledge sources and we showed that they are
largely complementary: Performance improvements were additive when we used them
together. The resulting parser is currently the best constituent parser for German (with
or without bilingual features).
New languages and even new domains can require new treebanks. To create such
a treebank for a MR&LC language, we would first annotate a small number of gold
82
Fraser et al Knowledge Sources for Parsing German
standard trees, using parallel text with English or another language if such text is
available. Next, wewould consider how to quickly differentiate constituents of the same
type using constituent labels plus grammatical functions, as we outlined in Section 3.
Following this, we would use BitPar to build a parser in the same way as we presented
here, and to determine the optimal level of markovization, which we assume would be
very high with a small number of gold standard training trees. Next, as more trees are
annotated in an active learning framework, we would begin to develop morphological
analysis. We would implement the bilingual framework following this (if we have
access to bitext). Then we would implement basic subcategorization extraction and add
monolingual features. Finally, as more gold standard trees are annotated, the reranking
framework should be constantly retrained. In particular, we expect that the effect of the
knowledge sources we have presented will be much stronger when starting with less
training data.
Our work in this paper will be of use to developers of German syntactic parsers
as we have state-of-the-art performance using linguistically motivated features that are
easy to understand. We also hope that our work can serve as a cookbook of ideas to try
for others working on parsers for other morphologically rich languages.
Acknowledgments
We would like to thank Sandra Ku?bler and
Yannick Versley. We gratefully acknowledge
Deutsche Forschungsgemeinschaft (DFG)
for funding this work (grants SCHU 2246/
6-1Morphosyntax for MT and SFB 732
D4Modular lexicalization of PCFGs). This
work was supported in part by the IST
Programme of the European Community,
under the PASCAL2 Network of Excellence,
IST-2007-216886. This publication only
reflects the authors? views.
References
Baroni, Marco and Adam Kilgarriff. 2006.
Large linguistically processed Web
corpora for multiple languages.
In EACL: Posters & Demonstrations,
pages 87?90, Trento.
Black, E., S. Abney, S. Flickenger,
C. Gdaniec, C. Grishman, P. Harrison,
D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus,
S. Roukos, B. Santorini, and
T. Strzalkowski. 1991. Procedure for
quantitatively comparing the syntactic
coverage of English grammars. In
Proceedings of the Workshop on Speech
and Natural Language, HLT ?91,
pages 306?311, Pacific Grove, CA.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen, Wolfgang Lezius, and George
Smith. 2002. The TIGER Treebank.
In Proceedings of the Workshop on
Treebanks and Linguistic Theories,
pages 24?41, Sozopol.
Brants, Thorsten. 2000. TnT?a statistical
part-of-speech tagger. In ANLP,
pages 224?231, Seattle, WA.
Burkett, David, John Blitzer, and Dan Klein.
2010. Joint parsing and alignment
with weakly synchronized grammars.
In HLT-NAACL, pages 127?135,
Los Angeles, CA.
Burkett, David and Dan Klein. 2008. Two
languages are better than one (for syntactic
parsing). In EMNLP, pages 877?886,
Honolulu, HI.
Cai, Shu, David Chiang, and Yoav Goldberg.
2011. Language-independent parsing with
empty elements. In ACL, pages 212?216,
Portland, OR.
Campbell, Richard. 2004. Using linguistic
principles to recover empty categories.
In ACL, pages 645?652, Barcelona.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt
discriminative reranking. In ACL,
pages 173?180, Ann Arbor, MI.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In ACL, pages 16?23, Madrid.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
In ICML, pages 25?70, Stanford, CA.
Cutting, Doug, Julian Kupiec, Jan Pedersen,
and Penelope Sibun. 1992. A practical
part-of-speech tagger. In ANLP,
pages 133?140, Trento.
Dreyer, Markus, David A. Smith, and
Noah A. Smith. 2006. Vine parsing and
minimum risk reranking for speed and
precision. In CoNLL, pages 201?205,
New York, NY.
83
Computational Linguistics Volume 39, Number 1
Dubey, Amit. 2004. Statistical Parsing for
German: Modeling Syntactic Properties
and Annotation Differences. Ph.D. thesis,
Saarland University.
Dubey, Amit and Frank Keller. 2003.
Probabilistic parsing for German using
sister-head dependencies. In ACL,
pages 96?103, Sapporo.
Duchier, Denys and Ralph Debusmann.
2001. Topological dependency trees:
a constraint-based account of linear
precedence. In ACL, pages 180?187,
Toulouse.
Finkel, Jenny Rose, Alex Kleeman, and
Christopher D. Manning. 2008. Efficient,
feature-based, conditional random
field parsing. In ACL, pages 959?967,
Columbus, OH.
Forst, Martin. 2007. Filling statistics
with linguistics?property design
for the disambiguation of German
LFG parses. In Proceedings of the ACL
Workshop on Deep Linguistic Processing,
pages 17?24, Prague.
Fossum, Victoria and Kevin Knight. 2008.
Using bilingual Chinese?English word
alignments to resolve PP-attachment
ambiguity in English. In AMTA,
pages 48?53, Honolulu, HI.
Fraser, Alexander, Renjing Wang,
and Hinrich Schu?tze. 2009. Rich
bitext projection features for parse
reranking. In EACL, pages 282?290,
Athens.
Gabbard, Ryan, Mitchell Marcus, and Seth
Kulick. 2006. Fully parsing the Penn
Treebank. In HLT-NAACL, pages 184?191,
Morristown, NJ.
Hall, Johan and Joakim Nivre. 2008.
A dependency-driven parser for
German dependency and constituency
representations. In Proceedings of the
Workshop on Parsing German, pages 47?54,
Columbus, OH.
Henderson, James, Paola Merlo, Gabriele
Musillo, and Ivan Titov. 2008. A latent
variable model of synchronous parsing
for syntactic and semantic dependencies.
In CoNLL, pages 178?182, Manchester.
Hsu, Yu-Yin. 2010. Comparing conversions
of discontinuity in PCFG parsing. In TLT,
pages 103?113, Tartu.
Huang, Liang. 2008. Forest reranking:
Discriminative parsing with non-local
features. In ACL, pages 586?594,
Columbus, OH.
Huang, Liang, Wenbin Jiang, and
Qun Liu. 2009. Bilingually constrained
(monolingual) shift-reduce parsing.
In EMNLP, pages 1,222?1,231,
Singapore.
Johnson, Mark. 1998. PCFG models
of linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Johnson, Mark. 2001. A simple pattern-
matching algorithm for recovering empty
nodes and their antecedents. In ACL,
pages 136?143, Philadelphia, PA.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing.
In ACL, pages 423?430, Sapporo.
Koehn, Philipp. 2005. Europarl: a parallel
corpus for statistical machine translation.
InMT Summit X, pages 79?86, Phuket.
Ku?bler, Sandra. 2008. The PaGe 2008 shared
task on parsing German. In Proceedings
of the Workshop on Parsing German,
pages 55?63, Columbus, OH.
Ku?bler, Sandra, Erhard W. Hinrichs, and
Wolfgang Maier. 2006. Is it really that
difficult to parse German? In EMNLP,
pages 111?119, Sydney.
Levy, Roger and Christopher D. Manning.
2004. Deep dependencies from context-free
statistical parsers: Correcting the surface
dependency approximation. In ACL,
pages 327?334, Barcelona.
Lezius, Wolfgang, Stefanie Dipper, and Arne
Fitschen. 2000. IMSLex?representing
morphological and syntactical information
in a relational database. In EURALEX,
pages 133?139, Stuttgart.
McCallum, Andrew Kachites. 2002. Mallet:
A machine learning for language toolkit.
http://mallet.cs.umass.edu.
McClosky, David, Eugene Charniak,
and Mark Johnson. 2006a. Effective
self-training for parsing. In HLT-NAACL,
pages 152?159, Morristown, NJ.
McClosky, David, Eugene Charniak,
and Mark Johnson. 2006b. Reranking
and self-training for parser adaptation.
In COLING-ACL, pages 337?344,
Sydney.
McDonald, Ryan and Fernando Pereira.
2006. Online learning of approximate
dependency parsing algorithms.
In EACL, pages 81?88, Trento.
Menzel, Wolfgang and Ingo Schro?der.
1998. Decision procedures for dependency
parsing using graded constraints.
In COLING-ACL Workshop on Processing
of Dependency-Based Grammars,
pages 78?87, Montreal.
Pado?, Sebastian and Mirella Lapata. 2009.
Cross-lingual annotation projection of
semantic roles. Journal of Artificial
Intelligence Research, 36:307?340.
84
Fraser et al Knowledge Sources for Parsing German
Petrov, Slav and Dan Klein. 2007. Improved
inference for unlexicalized parsing.
In HLT-NAACL, pages 404?411,
Rochester, NY.
Petrov, Slav and Dan Klein. 2008. Parsing
German with latent variable grammars.
In Proceedings of the Workshop on Parsing
German, pages 33?39, Columbus, OH.
Quirk, Chris and Simon Corston-Oliver.
2006. The impact of parse quality on
syntactically-informed statistical
machine translation. In EMNLP,
pages 62?69, Sydney.
Quirk, Chris, Arul Menezes, and
Colin Cherry. 2005. Dependency treelet
translation: Syntactically informed
phrasal SMT. In ACL, pages 271?279,
Oxford.
Rafferty, Anna and Christopher D. Manning.
2008. Parsing three German Treebanks:
Lexicalized and unlexicalized baselines.
In Proceedings of the Workshop on Parsing
German, pages 40?46, Columbus, OH.
Rambow, Owen. 2010. The simple truth
about dependency and phrase structure
representations: an opinion piece.
In HLT-NAACL, pages 337?340,
Los Angeles, CA.
Rehbein, Ines and Josef van Genabith.
2007. Evaluating evaluation measures.
In NODALIDA, pages 372?379, Tartu.
Schiehlen, Michael. 2004. Annotation
strategies for probabilistic parsing in
German. In COLING, pages 390?396,
Geneva.
Schmid, Helmut. 1995. Improvements in
part-of-speech tagging with an application
to German. In Proceedings of the ACL
SIGDAT-Workshop, pages 47?50, Dublin.
Schmid, Helmut. 2004. Efficient parsing
of highly ambiguous context-free
grammars with bit vectors. In COLING,
pages 162?168, Geneva.
Schmid, Helmut. 2006. Trace prediction
and recovery with unlexicalized PCFGs
and slash features. In COLING-ACL,
pages 177?184, Sydney.
Schmid, Helmut, Arne Fitschen, and
Ulrich Heid. 2004. SMOR: A German
computational morphology covering
derivation, composition and inflection.
In LREC, pages 1,263?1,266, Lisbon.
Seeker, Wolfgang, Bernd Bohnet, Lilja
?vrelid, and Jonas Kuhn. 2010a.
Informed ways of improving data-driven
dependency parsing for German. In
COLING: Posters, pages 1,122?1,130,
Beijing.
Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn,
and Josef Van Genabith. 2010b. Hard
constraints for grammatical function
labelling. In ACL, pages 1,087?1,097,
Uppsala.
Shen, Libin, Jinxi Xu, and Ralph Weischedel.
2008. A new string-to-dependency
machine translation algorithm with a
target dependency language model. In
ACL-HLT, pages 577?585, Columbus, OH.
Tsarfaty, Reut, Joakim Nivre, and Evelina
Andersson. 2012. Cross-framework
evaluation for statistical parsing.
In EACL, pages 44?54, Avignon.
Tsarfaty, Reut, Djame? Seddah, Yoav
Goldberg, Sandra Kuebler, Yannick
Versley, Marie Candito, Jennifer Foster,
Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing of morphologically
rich languages (SPMRL) what, how and
whither. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 1?12,
Los Angeles, CA.
Tu, Zhaopeng, Yang Liu, Young-Sook
Hwang, Qun Liu, and Shouxun Lin. 2010.
Dependency forest for statistical machine
translation. In COLING, pages 1,092?1,100,
Beijing.
Versley, Yannick. 2005. Parser evaluation
across text types. In Fourth Workshop on
Treebanks and Linguistic Theories (TLT),
pages 209?220, Barcelona.
Versley, Yannick and Ines Rehbein. 2009.
Scalable discriminative parsing for
German. In IWPT, pages 134?137, Paris.
Weischedel, Ralph, Marie Meteer, Richard
Schwartz, Lance Ramshaw, and Jeff
Palmucci. 1993. Coping with ambiguity
and unknown words through probabilistic
models. Computational Linguistics,
19(2):359?382.
Yusuke, Miyao and Tsujii Jun?ichi. 2002.
Maximum entropy estimation for
feature forests. In HLT, pages 292?297,
San Diego, CA.
85

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 255?261,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Identifying English and Hungarian Light Verb Constructions:
A Contrastive Approach
Veronika Vincze1,2, Istva?n Nagy T.2 and Richa?rd Farkas2
1Hungarian Academy of Sciences, Research Group on Artificial Intelligence
vinczev@inf.u-szeged.hu
2Department of Informatics, University of Szeged
{nistvan,rfarkas}@inf.u-szeged.hu
Abstract
Here, we introduce a machine learning-
based approach that allows us to identify
light verb constructions (LVCs) in Hun-
garian and English free texts. We also
present the results of our experiments on
the SzegedParalellFX English?Hungarian
parallel corpus where LVCs were manu-
ally annotated in both languages. With
our approach, we were able to contrast
the performance of our method and define
language-specific features for these typo-
logically different languages. Our pre-
sented method proved to be sufficiently ro-
bust as it achieved approximately the same
scores on the two typologically different
languages.
1 Introduction
In natural language processing (NLP), a signifi-
cant part of research is carried out on the English
language. However, the investigation of languages
that are typologically different from English is
also essential since it can lead to innovations that
might be usefully integrated into systems devel-
oped for English. Comparative approaches may
also highlight some important differences among
languages and the usefulness of techniques that are
applied.
In this paper, we focus on the task of identify-
ing light verb constructions (LVCs) in English and
Hungarian free texts. Thus, the same task will be
carried out for English and a morphologically rich
language. We compare whether the same set of
features can be used for both languages, we in-
vestigate the benefits of integrating language spe-
cific features into the systems and we explore how
the systems could be further improved. For this
purpose, we make use of the English?Hungarian
parallel corpus SzegedParalellFX (Vincze, 2012),
where LVCs have been manually annotated.
2 Light Verb Constructions
Light verb constructions (e.g. to give advice) are
a subtype of multiword expressions (Sag et al,
2002). They consist of a nominal and a verbal
component where the verb functions as the syn-
tactic head, but the semantic head is the noun. The
verbal component (also called a light verb) usu-
ally loses its original sense to some extent. Al-
though it is the noun that conveys most of the
meaning of the construction, the verb itself can-
not be viewed as semantically bleached (Apres-
jan, 2004; Alonso Ramos, 2004; Sanroma?n Vi-
las, 2009) since it also adds important aspects to
the meaning of the construction (for instance, the
beginning of an action, such as set on fire, see
Mel?c?uk (2004)). The meaning of LVCs can be
only partially computed on the basis of the mean-
ings of their parts and the way they are related to
each other, hence it is important to treat them in a
special way in many NLP applications.
LVCs are usually distinguished from productive
or literal verb + noun constructions on the one
hand and idiomatic verb + noun expressions on
the other (Fazly and Stevenson, 2007). Variativ-
ity and omitting the verb play the most significant
role in distinguishing LVCs from productive con-
structions and idioms (Vincze, 2011). Variativity
reflects the fact that LVCs can be often substituted
by a verb derived from the same root as the nomi-
nal component within the construction: productive
constructions and idioms can be rarely substituted
by a single verb (like make a decision ? decide).
Omitting the verb exploits the fact that it is the
nominal component that mostly bears the seman-
tic content of the LVC, hence the event denoted
by the construction can be determined even with-
out the verb in most cases. Furthermore, the very
same noun + verb combination may function as an
LVC in certain contexts while it is just a productive
construction in other ones, compare He gave her a
255
ring made of gold (non-LVC) and He gave her a
ring because he wanted to hear her voice (LVC),
hence it is important to identify them in context.
In theoretical linguistics, Kearns (2002) distin-
guishes between two subtypes of light verb con-
structions. True light verb constructions such as
to give a wipe or to have a laugh and vague ac-
tion verbs such as to make an agreement or to
do the ironing differ in some syntactic and se-
mantic features and can be separated by various
tests, e.g. passivization, WH-movement, pronom-
inalization etc. This distinction also manifests in
natural language processing as several authors pay
attention to the identification of just true light verb
constructions, e.g. Tu and Roth (2011). However,
here we do not make such a distinction and aim to
identify all types of light verb constructions both
in English and in Hungarian, in accordance with
the annotation principles of SZPFX.
The canonical form of a Hungarian light verb
construction is a bare noun + third person singular
verb. However, they may occur in non-canonical
versions as well: the verb may precede the noun,
or the noun and the verb may be not adjacent due
to the free word order. Moreover, as Hungarian
is a morphologically rich language, the verb may
occur in different surface forms inflected for tense,
mood, person and number. These features will be
paid attention to when implementing our system
for detecting Hungarian LVCs.
3 Related Work
Recently, LVCs have received special interest in
the NLP research community. They have been au-
tomatically identified in several languages such as
English (Cook et al, 2007; Bannard, 2007; Vincze
et al, 2011a; Tu and Roth, 2011), Dutch (Van de
Cruys and Moiro?n, 2007), Basque (Gurrutxaga
and Alegria, 2011) and German (Evert and Ker-
mes, 2003).
Parallel corpora are of high importance in the
automatic identification of multiword expressions:
it is usually one-to-many correspondence that is
exploited when designing methods for detecting
multiword expressions. Caseli et al (2010) de-
veloped an alignment-based method for extracting
multiword expressions from Portuguese?English
parallel corpora. Samardz?ic? and Merlo (2010) an-
alyzed English and German light verb construc-
tions in parallel corpora: they pay special attention
to their manual and automatic alignment. Zarrie?
and Kuhn (2009) argued that multiword expres-
sions can be reliably detected in parallel corpora
by using dependency-parsed, word-aligned sen-
tences. Sinha (2009) detected Hindi complex
predicates (i.e. a combination of a light verb and
a noun, a verb or an adjective) in a Hindi?English
parallel corpus by identifying a mismatch of the
Hindi light verb meaning in the aligned English
sentence. Many-to-one correspondences were also
exploited by Attia et al (2010) when identifying
Arabic multiword expressions relying on asym-
metries between paralell entry titles of Wikipedia.
Tsvetkov and Wintner (2010) identified Hebrew
multiword expressions by searching for misalign-
ments in an English?Hebrew parallel corpus.
To the best of our knowledge, parallel corpora
have not been used for testing the efficiency of an
MWE-detecting method for two languages at the
same time. Here, we investigate the performance
of our base LVC-detector on English and Hungar-
ian and pay special attention to the added value of
language-specific features.
4 Experiments
In our investigations we made use of the Szeged-
ParalellFX English-Hungarian parallel corpus,
which consists of 14,000 sentences and contains
about 1370 LVCs for each language. In addition,
we are aware of two other corpora ? the Szeged
Treebank (Vincze and Csirik, 2010) and Wiki50
(Vincze et al, 2011b) ?, which were manually an-
notated for LVCs on the basis of similar principles
as SZPFX, so we exploited these corpora when
defining our features.
To automatically identify LVCs in running
texts, a machine learning based approach was ap-
plied. This method first parsed each sentence and
extracted potential LVCs. Afterwards, a binary
classification method was utilized, which can au-
tomatically classify potential LVCs as an LVC or
not. This binary classifier was based on a rich fea-
ture set described below.
The candidate extraction method investi-
gated the dependency relation among the verbs
and nouns. Verb-object, verb-subject, verb-
prepositional object, verb-other argument (in the
case of Hungarian) and noun-modifier pairs were
collected from the texts. The dependency labels
were provided by the Bohnet parser (Bohnet,
2010) for English and by magyarlanc 2.0
(Zsibrita et al, 2013) for Hungarian.
256
The features used by the binary classifier can be
categorised as follows:
Morphological features: As the nominal com-
ponent of LVCs is typically derived from a verbal
stem (make a decision) or coincides with a verb
(have a walk), the VerbalStem binary feature fo-
cuses on the stem of the noun; if it had a verbal
nature, the candidates were marked as true. The
POS-pattern feature investigates the POS-tag se-
quence of the potential LVC. If it matched one pat-
tern typical of LVCs (e.g. verb + noun) the
candidate was marked as true; otherwise as false.
The English auxiliary verbs, do and have often
occur as light verbs, hence we defined a feature for
the two verbs to denote whether or not they were
auxiliary verbs in a given sentence.The POS code
of the next word of LVC candidate was also ap-
plied as a feature. As Hungarian is a morpholog-
ically rich language, we were able to define vari-
ous morphology-based features like the case of the
noun or its number etc. Nouns which were histor-
ically derived from verbs but were not treated as
derivation by the Hungarian morphological parser
were also added as a feature.
Semantic features: This feature also exploited
the fact that the nominal component is usually de-
rived from verbs. Consequently, the activity
or event semantic senses were looked for among
the upper level hyperonyms of the head of the
noun phrase in English WordNet 3.11 and in the
Hungarian WordNet (Miha?ltz et al, 2008).
Orthographic features: The suffix feature is
also based on the fact that many nominal compo-
nents in LVCs are derived from verbs. This feature
checks whether the lemma of the noun ended in
a given character bi- or trigram. The number of
words of the candidate LVC was also noted and
applied as a feature.
Statistical features: Potential English LVCs
and their occurrences were collected from 10,000
English Wikipedia pages by the candidate extrac-
tion method. The number of occurrences was used
as a feature when the candidate was one of the syn-
tactic phrases collected.
Lexical features: We exploit the fact that the
most common verbs are typically light verbs.
Therefore, fifteen typical light verbs were selected
from the list of the most frequent verbs taken from
the Wiki50 (Vincze et al, 2011b) in the case of En-
glish and from the Szeged Treebank (Vincze and
1http://wordnet.princeton.edu
Csirik, 2010) in the case of Hungarian. Then, we
investigated whether the lemmatised verbal com-
ponent of the candidate was one of these fifteen
verbs. The lemma of the noun was also applied
as a lexical feature. The nouns found in LVCs
were collected from the above-mentioned corpora.
Afterwards, we constructed lists of lemmatised
LVCs got from the other corpora.
Syntactic features: As the candidate extraction
methods basically depended on the dependency
relation between the noun and the verb, they could
also be utilised in identifying LVCs. Though the
dobj, prep, rcmod, partmod or nsubjpass
dependency labels were used in candidate extrac-
tion in the case of English, these syntactic relations
were defined as features, while the att, obj,
obl, subj dependency relations were used in the
case of Hungarian. When the noun had a deter-
miner in the candidate LVC, it was also encoded
as another syntactic feature.
Our feature set includes language-independent
and language-specific features as well. Language-
independent features seek to acquire general fea-
tures of LVCs while language-specific features can
be applied due to the different grammatical char-
acteristics of the two languages or due to the avail-
ability of different resources. Table 1 shows which
features were applied for which language.
We experimented with several learning algo-
rithms and decision trees have been proven per-
forming best. This is probably due to the fact that
our feature set consists of compact ? i.e. high-
level ? features. We trained the J48 classifier of the
WEKA package (Hall et al, 2009). This machine
learning approach implements the decision trees
algorithm C4.5 (Quinlan, 1993). The J48 classi-
fier was trained with the above-mentioned features
and we evaluated it in a 10-fold cross validation.
The potential LVCs which are extracted by the
candidate extraction method but not marked as
positive in the gold standard were classed as neg-
ative. As just the positive LVCs were annotated
on the SZPFX corpus, the F?=1 score interpreted
on the positive class was employed as an evalu-
ation metric. The candidate extraction methods
could not detect all LVCs in the corpus data, so
some positive elements in the corpora were not
covered. Hence, we regarded the omitted LVCs
as false negatives in our evaluation.
257
Features Base English Hungarian
Orthographical ? ? ?
VerbalStem ? ? ?
POS pattern ? ? ?
LVC list ? ? ?
Light verb list ? ? ?
Semantic features ? ? ?
Syntactic features ? ? ?
Auxiliary verb ? ? ?
Determiner ? ? ?
Noun list ? ? ?
POS After ? ? ?
LVC freq. stat. ? ? ?
Agglutinative morph. ? ? ?
Historical derivation ? ? ?
Table 1: The basic feature set and language-
specific features.
English Hungarian
ML 63.29/56.91/59.93 66.1/50.04/56.96
DM 73.71/29.22/41.67 63.24/34.46/44.59
Table 2: Results obtained in terms of precision, re-
call and F-score. ML: machine learning approach
DM: dictionary matching method.
5 Results
As a baseline, a context free dictionary matching
method was applied. For this, the gold-standard
LVC lemmas were gathered from Wiki50 and the
Szeged Treebank. Texts were lemmatized and if
an item on the list was found in the text, it was
treated as an LVC.
Table 2 lists the results got on the two differ-
ent parts of SZPFX using the machine learning-
based approach and the baseline dictionary match-
ing. The dictionary matching approach yielded the
highest precision on the English part of SZPFX,
namely 73.71%. However, the machine learning-
based approach proved to be the most successful
as it achieved an F-score that was 18.26 higher
than that with dictionary matching. Hence, this
method turned out to be more effective regard-
ing recall. At the same time, the machine learn-
ing and dictionary matching methods got roughly
the same precision score on the Hungarian part of
SZPFX, but again the machine learning-based ap-
proach achieved the best F-score. While in the
case of English the dictionary matching method
got a higher precision score, the machine learning
approach proved to be more effective.
An ablation analysis was carried out to exam-
ine the effectiveness of each individual feature of
the machine learning-based candidate classifica-
Feature English Hungarian
All 59.93 56.96
Lexical -19.11 -14.05
Morphological -1.68 -1.75
Orthographic -0.43 -3.31
Syntactic -1.84 -1.28
Semantic -2.17 -0.34
Statistical -2.23 ?
Language-specific -1.83 -1.05
Table 3: The usefulness of individual features in
terms of F-score using the SZPFX corpus.
tion. For each feature type, a J48 classifier was
trained with all of the features except that one. We
also investigated how language-specific features
improved the performance compared to the base
feature set. We then compared the performance to
that got with all the features. Table 3 shows the
contribution of each individual feature type on the
SZPFX corpus. For each of the two languages,
each type of feature contributed to the overall per-
formance. Lexical features were very effective in
both languages.
6 Discussion
According to the results, our base system is ro-
bust enough to achieve approximately the same
results on two typologically different languages.
Language-specific features further contribute to
the performance as shown by the ablation anal-
ysis. It should be also mentioned that some of
the base features (e.g. POS-patterns, which we
thought would be useful for English due to the
fixed word order) were originally inspired by one
of the languages and later expanded to the other
one (i.e. they were included in the base feature set)
since it was also effective in the case of the other
language. Thus, a multilingual approach may be
also beneficial in the case of monolingual applica-
tions as well.
The most obvious difference between the per-
formances on the two languages is the recall scores
(the difference being 6.87 percentage points be-
tween the two languages). This may be related to
the fact that the distribution of light verbs is quite
different in the two languages. While the top 15
verbs covers more than 80% of the English LVCs,
in Hungarian, this number is only 63% (and in or-
der to reach the same coverage, 38 verbs should be
included). Another difference is that there are 102
258
different verbs in English, which follow the Zipf
distribution, on the other hand, there are 157 Hun-
garian verbs with a more balanced distributional
pattern. Thus, fewer verbs cover a greater part of
LVCs in English than in Hungarian and this also
explains why lexical features contribute more to
the overall performance in English. This fact also
indicates that if verb lists are further extended, still
better recall scores may be achieved for both lan-
guages.
As for the effectiveness of morphological and
syntactic features, morphological features perform
better on a language with a rich morphologi-
cal representation (Hungarian). However, syntax
plays a more important role in LVC detection in
English: the added value of syntax is higher for
the English corpora than for the Hungarian one,
where syntactic features are also encoded in suf-
fixes, i.e. morphological information.
We carried out an error analysis in order to see
how our system could be further improved and
the errors reduced. We concluded that there were
some general and language-specific errors as well.
Among the general errors, we found that LVCs
with a rare light verb were difficult to recognize
(e.g. to utter a lie). In other cases, an originally
deverbal noun was used in a lexicalised sense to-
gether with a typical light verb ((e.g. buildings
are given (something)) and these candidates were
falsely classed as LVCs. Also, some errors in
POS-tagging or dependency parsing also led to
some erroneous predictions.
As for language-specific errors, English verb-
particle combinations (VPCs) followed by a noun
were often labeled as LVCs such as make up
his mind or give in his notice. In Hungar-
ian, verb + proper noun constructions (Hamletet
ja?tssza?k (Hamlet-ACC play-3PL.DEF) ?they are
playing Hamlet?) were sometimes regarded as
LVCs since the morphological analysis does not
make a distinction between proper and common
nouns. These language-specific errors may be
eliminated by integrating a VPC detector and a
named entity recognition system into the English
and Hungarian systems, respectively.
Although there has been a considerable amount
of literature on English LVC identification (see
Section 3), our results are not directly comparable
to them. This may be explained by the fact that dif-
ferent authors aimed to identify a different scope
of linguistic phenomena and thus interpreted the
concept of ?light verb construction? slightly dif-
ferently. For instance, Tu and Roth (2011) and Tan
et al (2006) focused only on true light verb con-
structions while only object?verb pairs are consid-
ered in other studies (Stevenson et al, 2004; Tan et
al., 2006; Fazly and Stevenson, 2007; Cook et al,
2007; Bannard, 2007; Tu and Roth, 2011). Several
other studies report results only on light verb con-
structions formed with certain light verbs (Steven-
son et al, 2004; Tan et al, 2006; Tu and Roth,
2011). In contrast, we aimed to identify all kinds
of LVCs, i.e. we did not apply any restrictions on
the nature of LVCs to be detected. In other words,
our task was somewhat more difficult than those
found in earlier literature. Although our results are
somewhat lower on English LVC detection than
those attained by previous studies, we think that
despite the difficulty of the task, our method could
offer promising results for identifying all types of
LVCs both in English and in Hungarian.
7 Conclusions
In this paper, we introduced our machine learning-
based approach for identifying LVCs in Hungar-
ian and English free texts. The method proved
to be sufficiently robust as it achieved approxi-
mately the same scores on two typologically dif-
ferent languages. The language-specific features
further contributed to the performance in both lan-
guages. In addition, some language-independent
features were inspired by one of the languages, so
a multilingual approach proved to be fruitful in the
case of monolingual LVC detection as well.
In the future, we would like to improve our sys-
tem by conducting a detailed analysis of the effect
of each feature on the results. Later, we also plan
to adapt the tool to other types of multiword ex-
pressions and conduct further experiments on lan-
guages other than English and Hungarian, the re-
sults of which may further lead to a more robust,
general LVC system. Moreover, we can improve
the method applied in each language by imple-
menting other language-specific features as well.
Acknowledgments
This work was supported in part by the European
Union and the European Social Fund through the
project FuturICT.hu (grant no.: TA?MOP-4.2.2.C-
11/1/KONV-2012-0013).
259
References
Margarita Alonso Ramos. 2004. Las construcciones
con verbo de apoyo. Visor Libros, Madrid.
Jurij D. Apresjan. 2004. O semantic?eskoj nepustote
i motivirovannosti glagol?nyx leksic?eskix funkcij.
Voprosy jazykoznanija, (4):3?18.
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel
Pecina, and Josef van Genabith. 2010. Automatic
Extraction of Arabic Multiword Expressions. In
Proceedings of the 2010 Workshop on Multiword
Expressions: from Theory to Applications, pages
19?27, Beijing, China, August. Coling 2010 Orga-
nizing Committee.
Colin Bannard. 2007. A measure of syntactic flexibil-
ity for automatically identifying multiword expres-
sions in corpora. In Proceedings of the Workshop
on a Broader Perspective on Multiword Expressions,
MWE ?07, pages 1?8, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97.
Helena de Medeiros Caseli, Carlos Ramisch, Maria das
Grac?as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expres-
sions. Language Resources and Evaluation, 44(1-
2):59?77.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: exploiting syntactic
forms for the automatic identification of idiomatic
expressions in context. In Proceedings of the Work-
shop on a Broader Perspective on Multiword Ex-
pressions, MWE ?07, pages 41?48, Morristown, NJ,
USA. Association for Computational Linguistics.
Stefan Evert and Hannah Kermes. 2003. Experiments
on candidate data for collocation extraction. In Pro-
ceedings of EACL 2003, pages 83?86.
Afsaneh Fazly and Suzanne Stevenson. 2007. Distin-
guishing Subtypes of Multiword Expressions Using
Linguistically-Motivated Statistical Measures. In
Proceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pages 9?16, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Antton Gurrutxaga and In?aki Alegria. 2011. Auto-
matic Extraction of NV Expressions in Basque: Ba-
sic Issues on Cooccurrence Techniques. In Proceed-
ings of the Workshop on Multiword Expressions:
from Parsing and Generation to the Real World,
pages 2?7, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18.
Kate Kearns. 2002. Light verbs in English.
Manuscript.
Igor Mel?c?uk. 2004. Verbes supports sans peine.
Lingvisticae Investigationes, 27(2):203?217.
Ma?rton Miha?ltz, Csaba Hatvani, Judit Kuti, Gyo?rgy
Szarvas, Ja?nos Csirik, Ga?bor Pro?sze?ky, and Tama?s
Va?radi. 2008. Methods and Results of the Hun-
garian WordNet Project. In Attila Tana?cs, Do?ra
Csendes, Veronika Vincze, Christiane Fellbaum, and
Piek Vossen, editors, Proceedings of the Fourth
Global WordNet Conference (GWC 2008), pages
311?320, Szeged. University of Szeged.
Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, CA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In Pro-
ceedings of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-2002, pages 1?15, Mexico City,
Mexico.
Tanja Samardz?ic? and Paola Merlo. 2010. Cross-lingual
variation of light verb constructions: Using parallel
corpora and automatic alignment for linguistic re-
search. In Proceedings of the 2010 Workshop on
NLP and Linguistics: Finding the Common Ground,
pages 52?60, Uppsala, Sweden, July. Association
for Computational Linguistics.
Begon?a Sanroma?n Vilas. 2009. Towards a seman-
tically oriented selection of the values of Oper1.
The case of golpe ?blow? in Spanish. In David
Beck, Kim Gerdes, Jasmina Milic?evic?, and Alain
Polgue`re, editors, Proceedings of the Fourth In-
ternational Conference on Meaning-Text Theory ?
MTT?09, pages 327?337, Montreal, Canada. Univer-
site? de Montre?al.
R. Mahesh K. Sinha. 2009. Mining Complex Predi-
cates In Hindi Using A Parallel Hindi-English Cor-
pus. In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation, Disam-
biguation and Applications, pages 40?46, Singa-
pore, August. Association for Computational Lin-
guistics.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.
2004. Statistical Measures of the Semi-Productivity
of Light Verb Constructions. In Takaaki Tanaka,
Aline Villavicencio, Francis Bond, and Anna Ko-
rhonen, editors, Second ACL Workshop on Multi-
word Expressions: Integrating Processing, pages 1?
8, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Yee Fan Tan, Min-Yen Kan, and Hang Cui. 2006.
Extending corpus-based identification of light verb
constructions using a supervised learning frame-
work. In Proceedings of the EACL Workshop on
260
Multi-Word Expressions in a Multilingual Contexts,
pages 49?56, Trento, Italy, April. Association for
Computational Linguistics.
Yulia Tsvetkov and Shuly Wintner. 2010. Extrac-
tion of multi-word expressions from small parallel
corpora. In Coling 2010: Posters, pages 1256?
1264, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Yuancheng Tu and Dan Roth. 2011. Learning English
Light Verb Constructions: Contextual or Statistical.
In Proceedings of the Workshop on Multiword Ex-
pressions: from Parsing and Generation to the Real
World, pages 31?39, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Tim Van de Cruys and Begon?a Villada Moiro?n. 2007.
Semantics-based multiword expression extraction.
In Proceedings of the Workshop on a Broader
Perspective on Multiword Expressions, MWE ?07,
pages 25?32, Morristown, NJ, USA. Association for
Computational Linguistics.
Veronika Vincze and Ja?nos Csirik. 2010. Hungar-
ian corpus of light verb constructions. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 1110?
1118, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Veronika Vincze, Istva?n Nagy T., and Ga?bor Berend.
2011a. Detecting Noun Compounds and Light Verb
Constructions: a Contrastive Study. In Proceedings
of the Workshop on Multiword Expressions: from
Parsing and Generation to the Real World, pages
116?121, Portland, Oregon, USA, June. ACL.
Veronika Vincze, Istva?n Nagy T., and Ga?bor Berend.
2011b. Multiword expressions and named entities in
the Wiki50 corpus. In Proceedings of RANLP 2011,
Hissar, Bulgaria.
Veronika Vincze. 2011. Semi-Compositional Noun
+ Verb Constructions: Theoretical Questions and
Computational Linguistic Analyses. Ph.D. thesis,
University of Szeged, Szeged, Hungary.
Veronika Vincze. 2012. Light Verb Constructions
in the SzegedParalellFX English?Hungarian Paral-
lel Corpus. In Nicoletta Calzolari, Khalid Choukri,
Thierry Declerck, Mehmet Ug?ur Dog?an, Bente
Maegaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, May. European
Language Resources Association (ELRA).
Sina Zarrie? and Jonas Kuhn. 2009. Exploiting Trans-
lational Correspondences for Pattern-Independent
MWE Identification. In Proceedings of the Work-
shop on Multiword Expressions: Identification,
Interpretation, Disambiguation and Applications,
pages 23?30, Singapore, August. Association for
Computational Linguistics.
Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.
2013. magyarlanc 2.0: szintaktikai elemze?s e?s fel-
gyors??tott szo?faji egye?rtelmu?s??te?s [magyarlanc 2.0:
Syntactic parsing and accelerated POS-tagging].
In Attila Tana?cs and Veronika Vincze, editors,
MSzNy 2013 ? IX. Magyar Sza?m??to?ge?pes Nyelve?szeti
Konferencia, pages 368?374, Szeged. Szegedi Tu-
doma?nyegyetem.
261
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 186?189,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SZTERGAK : Feature Engineering for Keyphrase Extraction
G
?
abor Berend
Department of Informatics
University of Szeged
2.
?
Arp?ad t?er Szeged, H-6720, Hungary
berendg@inf.u-szeged.hu
Rich
?
ard Farkas
Hungarian Academy of Sciences
103. Tisza Lajos k?or?ut
Szeged, H-6720, Hungary
rfarkas@inf.u-szeged.hu
Abstract
Automatically assigning keyphrases to
documents has a great variety of applica-
tions. Here we focus on the keyphrase
extraction of scientific publications and
present a novel set of features for the su-
pervised learning of keyphraseness. Al-
though these features are intended for ex-
tracting keyphrases from scientific papers,
because of their generality and robust-
ness, they should have uses in other do-
mains as well. With the help of these fea-
tures SZTERGAK achieved top results on
the SemEval-2 shared task on Automatic
Keyphrase Extraction from Scientific Arti-
cles and exceeded its baseline by 10%.
1 Introduction
Keyphrases summarize the content of documents
with the most important phrases. They can be
valuable in many application areas, ranging from
information retrieval to topic detection. However,
since manually assigned keyphrases are rarely pro-
vided and creating them by hand would be costly
and time-consuming, their automatic generation
is of great interest nowadays. Recent state-of-
the-art systems treat this kind of task as a super-
vised learning task, in which phrases of a docu-
ment should be classified with respect to their key
phrase characteristics based on manually labeled
corpora and various feature values.
This paper focuses on the task of keyphrase ex-
traction from scientific papers and we shall intro-
duce new features that can significantly improve
the overall performance. Although the experimen-
tal results presented here are solely based on sci-
entific articles, due to the robustness and univer-
sality of the features, our approach is expected to
achieve good results when applied on other do-
mains as well.
2 Related work
In keyphrase extraction tasks, phrases are ex-
tracted from one document that are the most char-
acteristic of its content (Liu et al, 2009; Wit-
ten et al, 1999). In these approaches keyphrase
extraction is treated as a classification task, in
which certain n-grams of a specific document act
as keyphrase candidates, and the task is to classify
them as proper keyphrases or not.
While Frank et al (1999) exploited domain spe-
cific knowledge to improve the quality of auto-
matic tagging, others like Liu et al (2009) analyze
term co-occurence graphs. It was Nguyen and Kan
(2007) who dealt with the special characteristics of
scientific papers and introduced the state-of-the-
art feature set to keyphrase extraction tasks. Here
we will follow a similar approach and make sig-
nificant improvements by the introduction of novel
features.
3 The SZTERGAK system
The SZTERGAK framework treats the reproduc-
tion of reader-assigned keyphrases as a supervised
learning task. In our setting a restricted set of to-
ken sequences extracted from the documents was
used as classification instances. These instances
were ranked regarding to their posteriori proba-
bilities of the keyphrase class, estimated by a
Na??ve Bayes classifier. Finally, we chose the top-
15 candidates as keyphrases.
Our features can be grouped into four main cat-
egories: those that were calculated solely from
the surface characteristics of phrases, those that
took into account the document that contained a
keyphrase, those that were obtained from the given
document set and those that were based on exter-
nal sources of information.
186
3.1 Preprocessing
Since there are parts of a document (e.g. tables
or author affiliations) that can not really contribute
to the keyphrase extractor, several preprocessing
steps were carried out. Preprocessing included the
elimination of author affiliations and messy lines.
The determination of the full title of an article
would be useful, however, it is not straightforward
because of multi-line titles. To solve this prob-
lem, a web query was sent with the first line of
a document and its most likely title was chosen
by simply selecting the most frequently occurring
one among the top 10 responses provided by the
Google API. This title was added to the document,
and all the lines before the first occurrence of the
line Abstract were omitted.
Lines unlikely to contain valuable information
were also excluded from the documents. These
lines were identified according to statistical data
of their surface forms (e.g. the average and
the deviation of line lengths) and regular expres-
sions. Lastly, section and sentence boundaries
were found in a rule-based way, and the POS and
syntactic tagging (using the Stanford parser (Klein
and Manning, 2003)) of each sentence were car-
ried out.
When syntactically parsed sentences were ob-
tained, keyphrase aspirants were extracted. The 1
to 4-long token sequences that did not start or end
with a stopword and consisted only of POS-codes
of an adjective, a noun or a verb were de-
fined to be possible keyphrases (resulting in classi-
fication instances). Tokens of key phrase aspirants
were stemmed to store them in a uniform way, but
they were also appended by the POS-code of the
derived form, so that the same root forms were dis-
tinguished if they came from tokens having differ-
ent POS-codes, like there shown in Table 1.
Textual Appearance Canonical form
regulations regul nns
Regulation regul nn
regulates regul vbz
regulated regul vbn
Table 1: Standardization of document terms.
3.2 The extended feature set
The features characterizing the extracted
keyphrase aspirants can be grouped into four
main types, namely phrase-, document-, corpus-
level and external knowledge-based features.
Below we will describe the different types of
features as well as those of KEA (Witten et al,
1999) which are cited as default features by most
of the literature dealing with keyphrase extraction.
3.2.1 Standard features
Features belonging to this set contain those of
KEA, namely Tf-idf and the first occurrence.
The Tf-idf feature assigns the tf-idf metric to
each keyphrase aspirant.
The first occurrence feature contains the rela-
tive first position for each keyphrase aspirant. The
feature value was obtained by dividing the abso-
lute first token position of a phrase by the number
of tokens of the document in question.
3.2.2 Phrase-level features
Features belonging to this group were calcu-
lated solely based on the keyphrase aspirants
themselves. Such features are able to get the
general characteristics of phrases functioning as
keyphrases.
Phrase length feature contains the number of
tokens a keyphrase aspirant consists of.
POS feature is a nominal one that stores
the POS-code sequence of each keyphrase aspi-
rant. (For example, for the phrase full JJ
space NN its value was JJ NN.)
Suffix feature is a binary feature that stores
information about whether the original form of
a keyphrase aspirant finished with some specific
ending according to a subset of the Michigan Suf-
ficiency Exams? Suffix List.
1
3.2.3 Document-level features
Since keyphrases should summarize the particular
document they represent, and phrase-level features
introduced above were independent of their con-
text, document-level features were also invented.
Acronymity feature functions as a binary fea-
ture that is assigned a true value iff a phrase is
likely to be an extended form of an acronym in the
same document. A phrase is treated as an extended
form of an acronym if it starts with the same letter
as the acronym present in its document and it also
contains all the letters of the acronym in the very
same order as they occur in the acronym.
PMI feature provides a measure of the mul-
tiword expression nature of multi-token phrases,
1
http://www.michigan-proficiency-exams.com/suffix-
list.html
187
and it is defined in Eq. (1), where p(t
i
) is the
document-level probability of the occurrence of
ith token in the phrase. This feature value is a gen-
eralized form of pointwise mutual information for
phrases with an arbitrary number of tokens.
pmi(t
1
, t
2
, ..., t
n
) =
log(
p(t
1
,t
2
,...,t
n
)
p(t
1
)?p(t
2
)?...?p(t
n
)
)
log(p(t
1
, t
2
, ..., t
n
))
n?1
(1)
Syntactic feature values refer to the average
minimal normalized depth of the NP-rooted parse
subtrees that contain a given keyphrase aspirant at
the leaf nodes in a given document.
3.2.4 Corpus-level features
Corpus-level features are used to determine the
relative importance of keyphrase aspirants based
on a comparison of corpus-level and document-
level frequencies.
The sf-isf feature was created to deal with logi-
cal positions of keyphrases and the formula shown
in Eq. (2) resembles that of tf-idf scores (hence
its name, i.e. Section Frequency-Inverted Section
Frequency). This feature value favors keyphrase
aspirants k that are included in several sections of
document d (sf ), but are present in a relatively
small number of sections in the overall corpus
(isf ). Phrases with higher sf-isf scores for a given
document are those that are more relevant with re-
spect to that document.
sfisf(k, d) = sf(k, d) ? isf(k) (2)
Keyphraseness feature is a binary one which
has a true value iff a phrase is one of the 785 dif-
ferent author-assigned keyphrases provided in the
training and test corpora.
3.2.5 External knowledge-based features
Apart from relying on the given corpus, further en-
hancements in performance can be obtained by re-
lying on external knowledge sources.
Wikipedia-feature is assigned a true value
for keyphrase aspirants for which there exists a
Wikipedia article with the same title. Preliminary
experiments showed that this feature is noisy, thus
we also investigated a relaxed version of it, where
occurrences of Wikipedia article titles were looked
for only in the title and abstract of a paper.
Besides using Wikipedia for feature calculation,
it was also utilized to retrieve semantic orienta-
tions of phrases. Making use of redirect links of
Wikipedia, the semantic relation of synonymity
Feature combinations F-score
Standard features (SF) 14.57
SF + phrase length feature 20.93
SF + POS feature 19.60
SF + suffix feature 16.35
SF + acronymity feature 16.87
SF + PMI feature 15.68
SF + syntactic feature 14.20
SF + sf-isf feature 14.79
SF + keyphraseness feature 15.17
SF + Wikipedia feature - full paper 14.37
SF + Wikipedia feature - abstract 16.50
SF + Wikipedia redirect 14.50
Shared Task best baseline 12.87
All features 23.82
All features - keyphraseness excluded 22.11
Table 2: Results obtained with different features.
can be exploited. For example, as there exists a
redirection between Wikipedia articles XML and
Extensible Markup Language, it may be
assumed that these phrases mean the same. For
this reason during the training phase we treated
a phrase equivalent to its redirected version, i.e.
if there is a keyphrase aspirant that is not as-
signed in the gold-standard reader annotation but
the Wikipedia article with the same title has a redi-
rection to such a phrase that is present among pos-
itive keyphrase instances of a particular document,
the original phrase can be treated as a positive in-
stance as well. In this way the ratio of positive ex-
amples could be increased from 0.99% to 1.14%.
4 Results and discussion
The training and test sets of the shared task (Kim
et al, 2010) consisted of 144 and 100 scien-
tific publications from the ACL repository, respec-
tively. Since the primary evaluation of the shared
task was based on the top-15 ranked automatic
keyphrases compared to the keyphrases assigned
by the readers of the articles, these results are re-
ported here. The evaluation results can be seen in
Table 2 where the individual effect of each feature
is given in combination with the standard features.
It is interesting to note the improvement ob-
tained by extending standard features with the
simple feature of phrase length. This indicates
that though the basic features were quite good,
they did not take into account the point that reader
188
keyphrases are likely to consist of several words.
Morphological features, such as POS or suffix
features were also among the top-performing ones,
which seems to show that most of the keyphrases
tend to have some common structure. In contrast,
the syntactic feature made some decrease in the
performance when it was combined just with the
standard ones. This can be due to the fact that the
input data were quite noisy, i.e. some inconsisten-
cies arose in the data during the pdf to text con-
version of articles, which made it difficult to parse
some sentences correctly.
It was also interesting to see that Wikipedia fea-
ture did not improve the result when it was applied
to the whole document. However, our previous ex-
periences on keyphrase extraction from scientific
abstracts showed that this feature can be very use-
ful. Hence, we relaxed the feature to handle occur-
rences just from the abstract. This modification of
the feature yielded a 14.8% improvement in the F-
measure. A possible explanation for this is that
Wikipedia has articles of very common phrases
(such as Calculation or Result) and the dis-
tribution of such non-keyphrase terms is higher in
the body of the articles than in abstracts.
The last row of Table 2 contains the result
achieved by the complete feature set excluding
keyphraseness. As keyphraseness exploits author-
assigned keyphrases and ? to the best of our
knowledge ? other participants of the shared task
did not utilize author-assigned keyphrases, this re-
sult is present in the final ranking of the shared
task systems. However, we believe that if the task
is to extract keyphrases from an article to gain se-
mantic meta-data for an NLP application (e.g. for
information retrieval or summarization), author-
assigned keyphrases are often present and can be
very useful. This latter statement was proved by
one of our experiments where we used the au-
thor keyphrases assigned to the document itself as
a binary feature (instead of using the pool of all
keyphrases). This feature set could achieve an F-
score of 27.44 on the evaluation set and we believe
that this should be the complete feature set in a
real-world semantic indexing application.
5 Conclusions
In this paper we introduced a wide set of new fea-
tures that are able to enhance the overall perfor-
mance of supervised keyphrase extraction applica-
tions. Our features include those calculated simply
on surface forms of keyphrase aspirants, those that
make use of the document- and corpus-level envi-
ronment of phrases and those that rely on exter-
nal knowledge. Although features were designed
to the specific task of extracting keyphrases from
scientific papers, due to their generality it is highly
assumable that they can be successfully utilized on
different domains as well.
The features we selected in SZTERGAK per-
formed well enough to actually achieve the
third place on the shared task by excluding the
keyphraseness feature and would be the first by
using any author-assigned keyphrase-based fea-
ture. It is also worth emphasizing that we think
that there are many possibilities to further extend
the feature set (e.g. with features that take the
semantic relatedness among keyphrase aspirants
into account) and significant improvement could
be achievable.
Acknowledgement
The authors would like to thank the annotators of
the shared task for the datasets used in the shared
task. This work was supported in part by the
NKTH grant (project codename TEXTREND).
References
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ing of 16th IJCAI, pages 668?673.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. Semeval-2010 task 5 : Au-
tomatic keyphrase extraction from scientific articles.
In Proc. of the 5th SIGLEX Workshop on Semantic
Evaluation.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 423?430.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on EMNLP.
Thuy Dung Nguyen and Minyen Kan. 2007.
Keyphrase extraction in scientific publications. In
Proc. of International Conference on Asian Digital
Libraries (ICADL 07), pages 317?326.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. Kea:
Practical automatic keyphrase extraction. In ACM
DL, pages 254?255.
189
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 549?553, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SZTE-NLP: Sentiment Detection on Twitter Messages
Viktor Hangya, Ga?bor Berend, Richa?rd Farkas
University of Szeged
Department of Informatics
hangyav@gmail.com, {berendg,rfarkas}@inf.u-szeged.hu
Abstract
In this paper we introduce our contribution
to the SemEval-2013 Task 2 on ?Sentiment
Analysis in Twitter?. We participated in ?task
B?, where the objective was to build mod-
els which classify tweets into three classes
(positive, negative or neutral) by their con-
tents. To solve this problem we basically fol-
lowed the supervised learning approach and
proposed several domain (i.e. microblog) spe-
cific improvements including text preprocess-
ing and feature engineering. Beyond the su-
pervised setting we also introduce some early
results employing a huge, automatically anno-
tated tweet dataset.
1 Introduction
In the past few years, the popularity of social me-
dia has increased. Many studies have been made in
the area (Jansen et al, 2009; O?Connor et al, 2010;
Bifet and Frank, 2010; Sang and Bos, 2012). Peo-
ple post messages on a variety of topics, for example
products, political issues, etc. Thus a big amount of
user generated data is created day-by-day. The man-
ual processing of this data is impossible, therefore
automatic procedures are needed.
In this paper we introduce an approach which is
able to assign sentiment labels to Twitter messages.
More precisely, it classifies tweets into positive, neg-
ative or neutral polarity classes. The system partici-
pated in the SemEval-2013 Task 2: Sentiment Anal-
ysis in Twitter, Task?B Message Polarity Classifica-
tion (Wilson et al, 2013). In our approach we used
a unigram based supervised model because it has
been shown that it works well on short messages like
tweets (Jiang et al, 2011; Barbosa and Feng, 2010;
Agarwal et al, 2011; Liu, 2010). We reduced the
size of the dictionary by normalizing the messages
and by stop word filtering. We also explored novel
features which gave us information on the polarity of
a tweet, for example we made use of the acronyms
in messages.
In the ?constrained? track of Task?B we used the
given training and development data only. For the
?unconstrained? track we downloaded tweets using
the Twitter Streaming API1 and automatically anno-
tated them. We present some preliminary results on
exploiting this huge dataset for training our classi-
fier.
2 Approach
At the beginning of our experiments we used a
unigram-based supervised model. Later on, we re-
alized that the size of our dictionary is huge, so
in the normalization phase we tried to reduce the
number of words in it. We investigated novel fea-
tures which contain information on the polarity of
the messages. Using these features we were able to
improve the precision of our classifier. For imple-
mentation we used the MALLET toolkit, which is a
Java-based package for natural language processing
(McCallum, 2002).
2.1 Normalization
One reason for the unusually big dictionary size is
that it contains one word in many forms, for exam-
1https://dev.twitter.com/docs/
streaming-apis/streams/public
549
ple in upper and lower case, in a misspelled form,
with character repetition, etc. On the other hand, it
contained numerous special annotations which are
typical for blogging, such as Twitter-specific anno-
tations, URL?s, smileys, etc. Keeping these in mind
we made the following normalization steps:
? First, in order to get rid of the multiple forms
of a single word we converted them into lower
case form then we stemmed them. For this pur-
pose we used the Porter Stemming Algorithm.
? We replaced the @ and # Twitter-specific tags
with the [USER] and [TAG] notations, respec-
tively. Besides we converted every URL in the
messages to the [URL] notation.
? Smileys in messages play an important role
in polarity classification. For this reason we
grouped them into positive and negative smi-
ley classes. We considered :), :-),: ), :D, =), ;),
; ), (: and :(, :-(, : (, ):, ) : smileys as positive
and negative, respectively.
? Since numbers do not contain information re-
garding a message polarity, we converted them
as well to the [NUMBER] form. In ad-
dition, we replaced the question and excla-
mation marks with the [QUESTION MARK]
and [EXCLAMATION MARK] notations. Af-
ter this we removed the unnecessary char-
acters ?"#$%&()*+,./:;<=>\?{}?, with
the exception that we removed the ? character
only if a word started or ended with it.
? In the case of words which contained character
repetitions ? more precisely those which con-
tained the same character at least three times
in a row ?, we reduced the length of this se-
quence to three. For instance, in the case
of the word yeeeeahhhhhhh we got the form
yeeeahhh. This way we unified these charac-
ter repetitions, but we did not loose this extra
information.
? Finally we made a stop word filtering in order
to get rid of the undesired words. To identify
these words we did not use a stop word dictio-
nary, rather we filtered out those words which
appeared too frequently in the training corpus.
We have chosen this method because we would
like to automatically detect those words which
are not relevant in the classification.
Before the normalization step, the dictionary con-
tained approximately 41, 000 words. After the above
introduced steps we managed to reduce the size of
the dictionary to 15, 000 words.
2.2 Features
After normalizing Twitter messages, we searched
for special features which characterize the polarity
of the tweets. One such feature is the polarity of
each word in a message. To determine the polarity
of a word, we used the SentiWordNet sentiment lex-
icon (Baccianella et al, 2010). In this lexicon, a pos-
itive, negative and an objective real value belong to
each word, which describes the polarity of the given
word. We consider a word as positive if the related
positive value is greater than 0.3, we consider it as
negative if the related negative value is greater than
0.2 and we consider it as objective if the related ob-
jective value is greater than 0.8. The threshold of the
objective value is high because most words are ob-
jective in this lexicon. After calculating the polarity
of each word we created three new features for each
tweet which are the number of positive, negative and
objective words, respectively. We also checked if a
negation word precedes a positive or negative word
and if so we inverted its polarity.
We also tried to group acronyms by their polarity.
For this purpose we used an acronym lexicon which
can be found on the www.internetslang.com
website. For each acronym we used the polarity of
each word in the acronym?s description and we de-
termined the polarity of the acronym by calculat-
ing the rate of positive and negative words in the
description. This way we created two new fea-
tures which are the number of positive and negative
acronyms in a given message.
Our intuition was that people like to use character
repetitions in their words for expressing their happi-
ness or sadness. Besides normalizing these tokens
(see Section 2.1), we created a new feature as well,
which represents the number of this kind of words
in a tweet.
Beyond character repetitions people like to write
words or a part of the text in upper case in order to
550
call the reader?s attention. Because of this we cre-
ated another feature which is the number of upper
case words in the given text.
3 Collected Data
In order to achieve an appropriate precision with su-
pervised methods we need a big amount of training
data. Creating this database manually is a hard and
time-consuming task. In many cases it is hard even
for humans to determine the polarity of a message,
for instance:
After a whole 5 hours away from work, I
get to go back again, I?m so lucky!
In the above tweet we cannot decide precisely the
polarity because the writer can be serious or just sar-
castic.
In order to increase the size of the training data
we acquired additional tweets, which we used in
the unconstrained run for Task?B. We created an ap-
plication which downloads tweets using the Twitter
Streaming API. The API supports language filter-
ing, which was used to get rid of non-English mes-
sages. Our manual investigations of the downloaded
tweets revealed, however, that this filter allows a big
amount of non-English tweets, which is probably
due to the fact that some Twitter users did not set
their language. We used Twitter4J2 API (which is
a Java library for the Twitter API) for downloading
these tweets. We automatically annotated the down-
loaded tweets using the Twitter Sentiment3 web ap-
plication, similar to Barbosa and Feng (2010) but
we used only one annotator. This web application
also supports language detection, but after this extra
filtration, our dataset still contained a considerable
amount of non-English messages. After 16 hours
of data collection we got 350, 000 annotated tweets,
where the distribution of neutral, positive and neg-
ative classes was approximately 60%, 20%, 20%,
respectively. For further testing purposes we have
chosen 10, 000 tweets from each class.
4 Results
We report results on the two official test sets of the
shared task. The ?twitter? test set consists of 3, 813
2http://twitter4j.org
3http://www.sentiment140.com
tweets while the ?sms? set consists of 2, 094 sms
messages. We evaluated both test databases in two
ways, in the so-called constrained run we only used
the official training database, while in the uncon-
strained run we also used a part of the additional
data, which was mentioned in the 3 section. The
official training database contained 4, 028 positive,
1, 655 negative and 3, 821 neutral tweets while for
the unconstrained run we used an additional 10, 000
tweets from each class. This way in each phase we
got four kinds of runs, which were evaluated with
the Na??ve Bayes and Maximum Entropy classifiers.
In Table 1 the evaluation of the unigram-based
model with the Na??ve Bayes learner can be seen.
The table contains the F-scores for the positive, neg-
ative and neutral labels for each of the four runs.
The avg column contains the average F-score for the
positive and negative labels, which was the official
evaluation metric for SemEval-2013 Task 2. We got
the best scores for the neutral label whilst the worst
scores are obtained for the negative label, which is
due to the fact that there were much less negative
instances in the training database. It can be seen
that the F-scores for the unconstrained run are better
both for the tweet and sms test databases. For the
unigram-based model the F-scores are higher when
we used the Maximum Entropy model (see Table 2).
pos neg neut avg
twitter-constrained 0.59 0.09 0.65 0.34
twitter-unconstrained 0.60 0.17 0.65 0.38
sms-constrained 0.46 0.16 0.63 0.31
sms-unconstrained 0.47 0.38 0.53 0.42
Table 1: Unigram-based model, Na??ve Bayes learner
pos neg neut avg
twitter-constrained 0.60 0.33 0.67 0.46
twitter-unconstrained 0.60 0.40 0.66 0.50
sms-constrained 0.47 0.31 0.69 0.39
sms-unconstrained 0.52 0.47 0.66 0.49
Table 2: Unigram-based model, Maximum Entropy
learner
In Tables 3 and 4 the evaluation results can be
seen for the normalized model. The normalization
551
step increased the precision for both learning al-
gorithms and the Maximum Entropy learner is still
better than Na??ve Bayes. Besides this we noticed
that for both learners in the case of the tweet test
database, the unconstrained run had lower scores
than the constrained whilst in the case of the sms
test database this phenomenon did not appear.
pos neg neut avg
twitter-constrained 0.65 0.32 0.67 0.48
twitter-unconstrained 0.62 0.21 0.63 0.41
sms-constrained 0.56 0.27 0.72 0.41
sms-unconstrained 0.52 0.35 0.66 0.43
Table 3: Normalized model, Na??ve Bayes learner
pos neg neut avg
twitter-constrained 0.66 0.40 0.68 0.53
twitter-unconstrained 0.61 0.42 0.64 0.51
sms-constrained 0.61 0.38 0.77 0.49
sms-unconstrained 0.57 0.47 0.72 0.52
Table 4: Normalized model, Maximum Entropy
learner
The evaluation results of the feature-based model
can be seen in Tables 5 and 6. In the case of the
Na??ve Bayes learner, the features did not increase the
F-scores, only for the sms-unconstrained run. For
the other runs the achieved scores decreased. In the
case of the Maximum Entropy learner the features
increased the F-scores, slightly for the constrained
runs and a bit more for the unconstrained runs.
From this analysis we can conclude that the nor-
malization of the messages yielded a considerable
increase in the F-scores. We discussed above that
this step also significantly reduced the size of the
dictionary. The features increased the precision too,
especially for the unconstrained run. This means
that these features represent properties which are
useful for those training data which are not from the
same corpus as the test messages. We compared two
machine learning algorithms and from the results we
concluded that the Maximum Entropy learner per-
forms better than the Na??ve Bayes on this task. Our
experiments also showed that the external, automat-
ically labeled training database helped only in the
classification of sms messages. This is due to the
fact that the smses and our external database are
from a different distribution than the official tweet
database.
pos neg neut avg
twitter-constrained 0.65 0.32 0.67 0.48
twitter-unconstrained 0.62 0.17 0.79 0.39
sms-constrained 0.56 0.38 0.74 0.47
sms-unconstrained 0.54 0.29 0.70 0.41
Table 5: Feature-based model, Na??ve Bayes learner
pos neg neut avg
twitter-constrained 0.66 0.41 0.69 0.54
twitter-unconstrained 0.63 0.43 0.65 0.53
sms-constrained 0.62 0.39 0.79 0.50
sms-unconstrained 0.61 0.49 0.75 0.55
Table 6: Feature-based model, Maximum Entropy
learner
5 Conclusions and Future Work
Recently, sentiment analysis on Twitter messages
has gained a lot of attention due to the huge amount
of Twitter users and their tweets. In this paper we ex-
amined different methods for classifying Twitter and
sms messages. We proposed special features which
characterize the polarity of the messages and we
concluded that due to the informality (slang, spelling
mistakes, etc.) of the messages it is crucial to nor-
malize them properly.
In the future, we plan to investigate the utility of
relations between Twitter users and between their
tweets and we are interested in topic-dependent sen-
timent analysis.
Acknowledgments
This work was supported in part by the Euro-
pean Union and the European Social Fund through
project FuturICT.hu (grant no.: TA?MOP-4.2.2.C-
11/1/KONV-2012-0013).
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment Analysis
552
of Twitter Data. In Proceedings of the Workshop on
Language in Social Media (LSM 2011), pages 30?38,
June.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. In Poster volume, Coling 2010, pages 36?44,
August.
Albert Bifet and Eibe Frank. 2010. Sentiment Knowl-
edge Discovery in Twitter Streaming Data.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter Power: Tweets as Electronic
Word of Mouth. In Journal of the American society
for information science and technology, pages 2169?
2188.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter Sentiment
Classification. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics, pages 151?160, June.
Bing Liu. 2010. Sentiment Analysis and Subjectivity. In
N. Indurkhya and F. J. Damerau, editors, Handbook of
Natural Language Processing.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From Tweets to Polls: Linking Text Sentiment to
Public Opinion Time Series. In Proceedings of the
International AAAI Conference on Weblogs and Social
Media, May.
Erik Tjong Kim Sang and Johan Bos. 2012. Predicting
the 2011 Dutch Senate Election Results with Twitter.
In Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 53?60, April.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 Task 2: Sentiment Analysis in Twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
553
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 1?12,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The CoNLL-2010 Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text
Richa?rd Farkas1,2, Veronika Vincze1, Gyo?rgy Mo?ra1, Ja?nos Csirik1,2, Gyo?rgy Szarvas3
1 University of Szeged, Department of Informatics
2 Hungarian Academy of Sciences, Research Group on Artificial Intelligence
3 Technische Universita?t Darmstadt, Ubiquitous Knowledge Processing Lab
{rfarkas,vinczev,gymora,csirik}@inf.u-szeged.hu,
szarvas@tk.informatik.tu-darmstadt.de
Abstract
The CoNLL-2010 Shared Task was dedi-
cated to the detection of uncertainty cues
and their linguistic scope in natural lan-
guage texts. The motivation behind this
task was that distinguishing factual and
uncertain information in texts is of essen-
tial importance in information extraction.
This paper provides a general overview
of the shared task, including the annota-
tion protocols of the training and evalua-
tion datasets, the exact task definitions, the
evaluation metrics employed and the over-
all results. The paper concludes with an
analysis of the prominent approaches and
an overview of the systems submitted to
the shared task.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
provides a competitive shared task for the Com-
putational Linguistics community. After a five-
year period of multi-language semantic role label-
ing and syntactic dependency parsing tasks, a new
task was introduced in 2010, namely the detection
of uncertainty and its linguistic scope in natural
language sentences.
In natural language processing (NLP) ? and in
particular, in information extraction (IE) ? many
applications seek to extract factual information
from text. In order to distinguish facts from unre-
liable or uncertain information, linguistic devices
such as hedges (indicating that authors do not
or cannot back up their opinions/statements with
facts) have to be identified. Applications should
handle detected speculative parts in a different
manner. A typical example is protein-protein in-
teraction extraction from biological texts, where
the aim is to mine text evidence for biological enti-
ties that are in a particular relation with each other.
Here, while an uncertain relation might be of some
interest for an end-user as well, such information
must not be confused with factual textual evidence
(reliable information).
Uncertainty detection has two levels. Auto-
matic hedge detectors might attempt to identify
sentences which contain uncertain information
and handle whole sentences in a different man-
ner or they might attempt to recognize in-sentence
spans which are speculative. In-sentence uncer-
tainty detection is a more complicated task com-
pared to the sentence-level one, but it has bene-
fits for NLP applications as there may be spans
containing useful factual information in a sentence
that otherwise contains uncertain parts. For ex-
ample, in the following sentence the subordinated
clause starting with although contains factual in-
formation while uncertain information is included
in the main clause and the embedded question.
Although IL-1 has been reported to con-
tribute to Th17 differentiation in mouse
and man, it remains to be determined
{whether therapeutic targeting of IL-1
will substantially affect IL-17 in RA}.
Both tasks were addressed in the CoNLL-2010
Shared Task, in order to provide uniform manu-
ally annotated benchmark datasets for both and to
compare their difficulties and state-of-the-art so-
lutions for them. The uncertainty detection prob-
lem consists of two stages. First, keywords/cues
indicating uncertainty should be recognized then
either a sentence-level decision is made or the lin-
guistic scope of the cue words has to be identified.
The latter task falls within the scope of semantic
analysis of sentences exploiting syntactic patterns,
as hedge spans can usually be determined on the
basis of syntactic patterns dependent on the key-
word.
1
2 Related Work
The term hedging was originally introduced by
Lakoff (1972). However, hedge detection has re-
ceived considerable interest just recently in the
NLP community. Light et al (2004) used a hand-
crafted list of hedge cues to identify specula-
tive sentences in MEDLINE abstracts and several
biomedical NLP applications incorporate rules for
identifying the certainty of extracted information
(Friedman et al, 1994; Chapman et al, 2007; Ara-
maki et al, 2009; Conway et al, 2009).
The most recent approaches to uncertainty de-
tection exploit machine learning models that uti-
lize manually labeled corpora. Medlock and
Briscoe (2007) used single words as input features
in order to classify sentences from biological ar-
ticles (FlyBase) as speculative or non-speculative
based on semi-automatically collected training ex-
amples. Szarvas (2008) extended the methodology
of Medlock and Briscoe (2007) to use n-gram fea-
tures and a semi-supervised selection of the key-
word features. Kilicoglu and Bergler (2008) pro-
posed a linguistically motivated approach based
on syntactic information to semi-automatically re-
fine a list of hedge cues. Ganter and Strube (2009)
proposed an approach for the automatic detec-
tion of sentences containing uncertainty based on
Wikipedia weasel tags and syntactic patterns.
The BioScope corpus (Vincze et al, 2008) is
manually annotated with negation and specula-
tion cues and their linguistic scope. It consists
of clinical free-texts, biological texts from full pa-
pers and scientific abstracts. Using BioScope for
training and evaluation, Morante and Daelemans
(2009) developed a scope detector following a su-
pervised sequence labeling approach while O?zgu?r
and Radev (2009) developed a rule-based system
that exploits syntactic patterns.
Several related works have also been published
within the framework of The BioNLP?09 Shared
Task on Event Extraction (Kim et al, 2009), where
a separate subtask was dedicated to predicting
whether the recognized biological events are un-
der negation or speculation, based on the GENIA
event corpus annotations (Kilicoglu and Bergler,
2009; Van Landeghem et al, 2009).
3 Uncertainty Annotation Guidelines
The shared task addressed the detection of uncer-
tainty in two domains. As uncertainty detection
is extremely important for biomedical information
extraction and most existing approaches have tar-
geted such applications, participants were asked
to develop systems for hedge detection in bio-
logical scientific articles. Uncertainty detection
is also important, e.g. in encyclopedias, where
the goal is to collect reliable world knowledge
about real-world concepts and topics. For exam-
ple, Wikipedia explicitly declares that statements
reflecting author opinions or those not backed up
by facts (e.g. references) should be avoided (see
3.2 for details). Thus, the community-edited en-
cyclopedia, Wikipedia became one of the subjects
of the shared task as well.
3.1 Hedges in Biological Scientific Articles
In the biomedical domain, sentences were manu-
ally annotated for both hedge cues and their lin-
guistic scope. Hedging is typically expressed by
using specific linguistic devices (which we refer to
as cues in this article) that modify the meaning or
reflect the author?s attitude towards the content of
the text. Typical hedge cues fall into the following
categories:
? auxiliaries: may, might, can, would, should,
could, etc.
? verbs of hedging or verbs with speculative
content: suggest, question, presume, suspect,
indicate, suppose, seem, appear, favor, etc.
? adjectives or adverbs: probable, likely, possi-
ble, unsure, etc.
? conjunctions: or, and/or, either . . . or, etc.
However, there are some cases where a hedge is
expressed via a phrase rather than a single word.
Complex keywords are phrases that express un-
certainty together, but not on their own (either the
semantic interpretation or the hedging strength of
its subcomponents are significantly different from
those of the whole phrase). An instance of a com-
plex keyword can be seen in the following sen-
tence:
Mild bladder wall thickening {raises
the question of cystitis}.
The expression raises the question of may be sub-
stituted by suggests and neither the verb raises nor
the noun question convey speculative meaning on
their own. However, the whole phrase is specula-
tive therefore it is marked as a hedge cue.
2
During the annotation process, a min-max strat-
egy for the marking of keywords (min) and their
scope (max) was followed. On the one hand, when
marking the keywords, the minimal unit that ex-
presses hedging and determines the actual strength
of hedging was marked as a keyword. On the other
hand, when marking the scopes of speculative key-
words, the scope was extended to the largest syn-
tactic unit possible. That is, all constituents that
fell within the uncertain interpretation were in-
cluded in the scope. Our motivation here was that
in this way, if we simply disregard the marked text
span, the rest of the sentence can usually be used
for extracting factual information (if there is any).
For instance, in the example above, we can be sure
that the symptom mild bladder wall thickening is
exhibited by the patient but a diagnosis of cystitis
would be questionable.
The scope of a speculative element can be de-
termined on the basis of syntax. The scopes of
the BioScope corpus are regarded as consecutive
text spans and their annotation was based on con-
stituency grammar. The scope of verbs, auxil-
iaries, adjectives and adverbs usually starts right
with the keyword. In the case of verbal elements,
i.e. verbs and auxiliaries, it ends at the end of the
clause or sentence, thus all complements and ad-
juncts are included. The scope of attributive ad-
jectives generally extends to the following noun
phrase, whereas the scope of predicative adjec-
tives includes the whole sentence. Sentential ad-
verbs have a scope over the entire sentence, while
the scope of other adverbs usually ends at the end
of the clause or sentence. Conjunctions generally
have a scope over the syntactic unit whose mem-
bers they coordinate. Some linguistic phenomena
(e.g. passive voice or raising) can change scope
boundaries in the sentence, thus they were given
special attention during the annotation phase.
3.2 Wikipedia Weasels
The chief editors of Wikipedia have drawn the at-
tention of the public to uncertainty issues they call
weasel1. A word is considered to be a weasel
word if it creates an impression that something im-
portant has been said, but what is really commu-
nicated is vague, misleading, evasive or ambigu-
ous. Weasel words do not give a neutral account
of facts, rather, they offer an opinion without any
1http://en.wikipedia.org/wiki/Weasel_
word
backup or source. The following sentence does
not specify the source of information, it is just the
vague term some people that refers to the holder of
this opinion:
Some people claim that this results in a
better taste than that of other diet colas
(most of which are sweetened with as-
partame alone).
Statements with weasel words usually evoke ques-
tions such as Who says that?, Whose opinion is
this? and How many people think so?.
Typical instances of weasels can be grouped in
the following way (we offer some examples as
well):
? Adjectives and adverbs
? elements referring to uncertainty: prob-
able, likely, possible, unsure, often, pos-
sibly, allegedly, apparently, perhaps,
etc.
? elements denoting generalization:
widely, traditionally, generally, broadly-
accepted, widespread, etc.
? qualifiers and superlatives: global, su-
perior, excellent, immensely, legendary,
best, (one of the) largest, most promi-
nent, etc.
? elements expressing obviousness:
clearly, obviously, arguably, etc.
? Auxiliaries
? may, might, would, should, etc.
? Verbs
? verbs with speculative content and their
passive forms: suggest, question, pre-
sume, suspect, indicate, suppose, seem,
appear, favor, etc.
? passive forms with dummy subjects: It
is claimed that . . . It has been men-
tioned . . . It is known . . .
? there is / there are constructions: There
is evidence/concern/indication that. . .
? Numerically vague expressions / quantifiers
? certain, numerous, many, most, some,
much, everyone, few, various, one group
of, etc. Experts say . . . Some people
think . . .More than 60% percent . . .
3
? Nouns
? speculation, proposal, consideration,
etc. Rumour has it that . . . Common
sense insists that . . .
However, the use of the above words or grammat-
ical devices does not necessarily entail their being
a weasel cue since their use may be justifiable in
their contexts.
As the main application goal of weasel detec-
tion is to highlight articles which should be im-
proved (by reformulating or adding factual is-
sues), we decided to annotate only weasel cues
in Wikipedia articles, but we did not mark their
scopes.
During the manual annotation process, the fol-
lowing cue marking principles were employed.
Complex verb phrases were annotated as weasel
cues since in some cases, both the passive con-
struction and the verb itself are responsible for the
weasel. In passive forms with dummy subjects and
there is / there are constructions, the weasel cue
included the grammatical subject (i.e. it and there)
as well. As for numerically vague expressions, the
noun phrase containing a quantifier was marked
as a weasel cue. If there was no quantifier (in the
case of a bare plural), the noun was annotated as
a weasel cue. Comparatives and superlatives were
annotated together with their article. Anaphoric
pronouns referring to a weasel word were also an-
notated as weasel cues.
4 Task Definitions
Two uncertainty detection tasks (sentence clas-
sification and in-sentence hedge scope detec-
tion) in two domains (biological publications and
Wikipedia articles) with three types of submis-
sions (closed, cross and open) were given to the
participants of the CoNLL-2010 Shared Task.
4.1 Detection of Uncertain Sentences
The aim of Task1 was to develop automatic proce-
dures for identifying sentences in texts which con-
tain unreliable or uncertain information. In par-
ticular, this task is a binary classification problem,
i.e. factual and uncertain sentences have to be dis-
tinguished.
As training and evaluation data
? Task1B: biological abstracts and full articles
(evaluation data contained only full articles)
from the BioScope corpus and
? Task1W: paragraphs from Wikipedia possi-
bly containing weasel information
were provided. The annotation of weasel/hedge
cues was carried out on the phrase level, and sen-
tences containing at least one cue were considered
as uncertain, while sentences with no cues were
considered as factual. The participating systems
had to submit a binary classification (certain vs.
uncertain) of the test sentences while marking cues
in the submissions was voluntary (but participants
were encouraged to do this).
4.2 In-sentence Hedge Scope Resolution
For Task2, in-sentence scope resolvers had to be
developed. The training and evaluation data con-
sisted of biological scientific texts, in which in-
stances of speculative spans ? that is, keywords
and their linguistic scope ? were annotated manu-
ally. Submissions to Task2 were expected to auto-
matically annotate the cue phrases and the left and
right boundaries of their scopes (exactly one scope
must be assigned to a cue phrase).
4.3 Evaluation Metrics
The evaluation for Task1 was carried out at the
sentence level, i.e. the cue annotations in the sen-
tence were not taken into account. The F?=1 mea-
sure (the harmonic mean of precision and recall)
of the uncertain class was employed as the chief
evaluation metric.
The Task2 systems were expected to mark cue-
and corresponding scope begin/end tags linked to-
gether by using some unique IDs. A scope-level
F?=1 measure was used as the chief evaluation
metric where true positives were scopes which ex-
actly matched the gold standard cue phrases and
gold standard scope boundaries assigned to the cue
word. That is, correct scope boundaries with in-
correct cue annotation and correct cue words with
bad scope boundaries were both treated as errors.
This scope-level metric is very strict. For in-
stance, the requirement of the precise match of the
cue phrase is questionable as ? from an application
point of view ? the goal is to find uncertain text
spans and the evidence for this is not so impor-
tant. However, the annotation of cues in datasets
is essential for training scope detectors since lo-
cating the cues usually precedes the identification
of their scope. Hence we decided to incorporate
cue matches into the evaluation metric.
4
Another questionable issue is the strict bound-
ary matching requirement. For example, includ-
ing or excluding punctuations, citations or some
bracketed expressions, like (see Figure 1) from
a scope is not crucial for an otherwise accurate
scope detector. On the other hand, the list of
such ignorable phenomena is arguable, especially
across domains. Thus, we considered the strict
boundary matching to be a straightforward and un-
ambiguous evaluation criterion. Minor issues like
those mentioned above could be handled by sim-
ple post-processing rules. In conclusion we think
that the uncertainty detection community may find
more flexible evaluation criteria in the future but
the strict scope-level metric is definitely a good
starting point for evaluation.
4.4 Closed and Open Challenges
Participants were invited to submit results in dif-
ferent configurations, where systems were allowed
to exploit different kinds of annotated resources.
The three possible submission categories were:
? Closed, where only the labeled and unla-
beled data provided for the shared task were
allowed, separately for each domain (i.e.
biomedical train data for biomedical test set
and Wikipedia train data for Wikipedia test
set). No further manually crafted resources
of uncertainty information (i.e. lists, anno-
tated data, etc.) could be used in any domain.
On the other hand, tools exploiting the man-
ual annotation of linguistic phenomena not
related to uncertainty (such as POS taggers
and parsers trained on labeled corpora) were
allowed.
? Cross-domain was the same as the closed one
but all data provided for the shared task were
allowed for both domains (i.e. Wikipedia
train data for the biomedical test set, the
biomedical train data for Wikipedia test set
or a union of Wikipedia and biomedical train
data for both test sets).
? Open, where any data and/or any additional
manually created information and resource
(which may be related to uncertainty) were
allowed for both domains.
The motivation behind the cross-domain and the
open challenges was that in this way, we could
assess whether adding extra (i.e. not domain-
specific) information to the systems can contribute
to the overall performance.
5 Datasets
Training and evaluation corpora were annotated
manually for hedge/weasel cues and their scope
by two independent linguist annotators. Any dif-
ferences between the two annotations were later
resolved by the chief annotator, who was also re-
sponsible for creating the annotation guidelines
and training the two annotators. The datasets
are freely available2 for further benchmark experi-
ments at http://www.inf.u-szeged.hu/
rgai/conll2010st.
Since uncertainty cues play an important role
in detecting sentences containing uncertainty, they
are tagged in the Task1 datasets as well to enhance
training and evaluation of systems.
5.1 Biological Publications
The biological training dataset consisted of the bi-
ological part of the BioScope corpus (Vincze et al,
2008), hence it included abstracts from the GE-
NIA corpus, 5 full articles from the functional ge-
nomics literature (related to the fruit fly) and 4 ar-
ticles from the open access BMC Bioinformatics
website. The automatic segmentation of the doc-
uments was corrected manually and the sentences
(14541 in number) were annotated manually for
hedge cues and their scopes.
The evaluation dataset was based on 15 biomed-
ical articles downloaded from the publicly avail-
able PubMedCentral database, including 5 ran-
dom articles taken from the BMC Bioinformat-
ics journal in October 2009, 5 random articles to
which the drosophila MeSH term was assigned
and 5 random articles having the MeSH terms
human, blood cells and transcription factor (the
same terms which were used to create the Genia
corpus). These latter ten articles were also pub-
lished in 2009. The aim of this article selection
procedure was to have a theme that was close to
the training corpus. The evaluation set contained
5003 sentences, out of which 790 were uncertain.
These texts were manually annotated for hedge
cues and their scope. To annotate the training and
the evaluation datasets, the same annotation prin-
ciples were applied.
2under the Creative Commons Attribute Share Alike li-
cense
5
For both Task1 and Task2, the same dataset was
provided, the difference being that for Task1, only
hedge cues and sentence-level uncertainty were
given, however, for Task2, hedge cues and their
scope were marked in the text.
5.2 Wikipedia Datasets
2186 paragraphs collected from Wikipedia
archives were also offered as Task1 training
data (11111 sentences containing 2484 uncertain
ones). The evaluation dataset contained 2346
Wikipedia paragraphs with 9634 sentences, out of
which 2234 were uncertain.
For the selection of the Wikipedia paragraphs
used to construct the training and evaluation
datasets, we exploited the weasel tags added by
the editors of the encyclopedia (marking unsup-
ported opinions or expressions of a non-neutral
point of view). Each paragraph containing weasel
tags (5874 different ones) was extracted from the
history dump of EnglishWikipedia. First, 438 ran-
domly selected paragraphs were manually anno-
tated from this pool then the most frequent cue
phrases were collected. Later on, two other sets
of Wikipedia paragraphs were gathered on the ba-
sis of whether they contained such cue phrases or
not. The aim of this sampling procedure was to
provide large enough training and evaluation sam-
ples containing weasel words and also occurrences
of typical weasel words in non-weasel contexts.
Each sentence was annotated manually for
weasel cues. Sentences were treated as uncer-
tain if they contained at least one weasel cue, i.e.
the scope of weasel words was the entire sentence
(which is supposed to be rewritten by Wikipedia
editors).
5.3 Unlabeled Data
Unannotated but pre-processed full biological arti-
cles (150 articles from the publicly available Pub-
MedCentral database) and 1 million paragraphs
from Wikipedia were offered to the participants as
well. These datasets did not contain any manual
annotation for uncertainty, but their usage permit-
ted data sampling from a large pool of in-domain
texts without time-wasting pre-processing tasks
(cleaning and sentence splitting).
5.4 Data Format
Both training and evaluation data were released
in a custom XML format. For each task, a sep-
arate XML file was made available containing the
whole document set for the given task. Evaluation
datasets were available in the same format as train-
ing data without any sentence-level certainty, cue
or scope annotations.
The XML format enabled us to provide more
detailed information about the documents such as
segment boundaries and types (e.g. section titles,
figure captions) and it is the straightforward for-
mat to represent nested scopes. Nested scopes
have overlapping text spans which may contain
cues for multiple scopes (there were 1058 occur-
rences in the training and evaluation datasets to-
gether). The XML format utilizes id-references
to determine the scope of a given cue. Nested
constructions are rather complicated to represent
in the standard IOB format, moreover, we did not
want to enforce a uniform tokenization.
To support the processing of the data files,
reader and writer software modules were devel-
oped and offered to the participants for the uCom-
pare (Kano et al, 2009) framework. uCompare
provides a universal interface (UIMA) and several
text mining and natural language processing tools
(tokenizers, POS taggers, syntactic parsers, etc.)
for general and biological domains. In this way
participants could configure and execute a flexible
chain of analyzing tools even with a graphical UI.
6 Submissions and Results
Participants uploaded their results through the
shared task website, and the official evaluation was
performed centrally. After the evaluation period,
the results were published for the participants on
the Web. A total of 23 teams participated in the
shared task. 22, 16 and 13 teams submitted output
for Task1B, Task1W and Task2, respectively.
6.1 Results
Tables 1, 2 and 3 contain the results of the submit-
ted systems for Task1 and Task2. The last name
of the first author of the system description pa-
per (published in these proceedings) is used here
as a system name3. The last column contains the
type of submission. The system of Kilicoglu and
Bergler (2010) is the only open submission. They
adapted their system introduced in Kilicoglu and
Bergler (2008) to the datasets of the shared task.
Regarding cross submissions, Zhao et al (2010)
and Ji et al (2010) managed to achieve a no-
ticeable improvement by exploiting cross-domain
3O?zgu?r did not publish a description of her system.
6
Name P / R / F type
Georgescul 72.0 / 51.7 / 60.2 C
Ji 62.7 / 55.3 / 58.7 X
Chen 68.0 / 49.7 / 57.4 C
Morante 80.6 / 44.5 / 57.3 C
Zhang 76.6 / 44.4 / 56.2 C
Zheng 76.3 / 43.6 / 55.5 C
Ta?ckstro?m 78.3 / 42.8 / 55.4 C
Mamani Sa?nchez 68.3 / 46.2 / 55.1 C
Tang 82.3 / 41.4 / 55.0 C
Kilicoglu 67.9 / 46.0 / 54.9 O
Tjong Kim Sang 74.0 / 43.0 / 54.4 C
Clausen 75.1 / 42.0 / 53.9 C
O?zgu?r 59.4 / 47.9 / 53.1 C
Zhou 85.3 / 36.5 / 51.1 C
Li 88.4 / 31.9 / 46.9 C
Prabhakaran 88.0 / 28.4 / 43.0 C
Ji 94.2 / 6.6 / 12.3 C
Table 1: Task1 Wikipedia results (type ?
{Closed(C), Cross(X), Open(O)}).
data. Zhao et al (2010) extended the biological
cue word dictionary of their system ? using it as
a feature for classification ? by the frequent cues
of the Wikipedia dataset, while Ji et al (2010)
used the union of the two datasets for training
(they have reported an improvement from 47.0 to
58.7 on the Wikipedia evaluation set after a post-
challenge bugfix).
Name P / R / F type
Morante 59.6 / 55.2 / 57.3 C
Rei 56.7 / 54.6 / 55.6 C
Velldal 56.7 / 54.0 / 55.3 C
Kilicoglu 62.5 / 49.5 / 55.2 O
Li 57.4 / 47.9 / 52.2 C
Zhou 45.6 / 43.9 / 44.7 O
Zhou 45.3 / 43.6 / 44.4 C
Zhang 46.0 / 42.9 / 44.4 C
Fernandes 46.0 / 38.0 / 41.6 C
Vlachos 41.2 / 35.9 / 38.4 C
Zhao 34.8 / 41.0 / 37.7 C
Tang 34.5 / 31.8 / 33.1 C
Ji 21.9 / 17.2 / 19.3 C
Ta?ckstro?m 2.3 / 2.0 / 2.1 C
Table 2: Task2 results (type ? {Closed(C),
Open(O)}).
Each Task2 and Task1W system achieved a
Name P / R / F type
Tang 85.0 / 87.7 / 86.4 C
Zhou 86.5 / 85.1 / 85.8 C
Li 90.4 / 81.0 / 85.4 C
Velldal 85.5 / 84.9 / 85.2 C
Vlachos 85.5 / 84.9 / 85.2 C
Ta?ckstro?m 87.1 / 83.4 / 85.2 C
Shimizu 88.1 / 82.3 / 85.1 C
Zhao 83.4 / 84.8 / 84.1 X
O?zgu?r 77.8 / 91.3 / 84.0 C
Rei 83.8 / 84.2 / 84.0 C
Zhang 82.6 / 84.7 / 83.6 C
Kilicoglu 92.1 / 74.9 / 82.6 O
Morante 80.5 / 83.3 / 81.9 X
Morante 81.1 / 82.3 / 81.7 C
Zheng 73.3 / 90.8 / 81.1 C
Tjong Kim Sang 74.3 / 87.1 / 80.2 C
Clausen 79.3 / 80.6 / 80.0 C
Szidarovszky 70.3 / 91.0 / 79.3 C
Georgescul 69.1 / 91.0 / 78.5 C
Zhao 71.0 / 86.6 / 78.0 C
Ji 79.4 / 76.3 / 77.9 C
Chen 74.9 / 79.1 / 76.9 C
Fernandes 70.1 / 71.1 / 70.6 C
Prabhakaran 67.5 / 19.5 / 30.3 X
Table 3: Task1 biological results (type ?
{Closed(C), Cross(X), Open(O)}).
higher precision than recall. There may be two
reasons for this. The systems may have applied
only reliable patterns, or patterns occurring in the
evaluation set may be imperfectly covered by the
training datasets. The most intense participation
was on Task1B. Here, participants applied vari-
ous precision/recall trade-off strategies. For in-
stance, Tang et al (2010) achieved a balanced pre-
cision/recall configuration, while Li et al (2010)
achieved third place thanks to their superior preci-
sion.
Tables 4 and 5 show the cue-level performances,
i.e. the F-measure of cue phrase matching where
true positives were strict matches. Note that it was
optional to submit cue annotations for Task1 (if
participants submitted systems for both Task2 and
Task1B with cue tagging, only the better score of
the two was considered).
It is interesting to see that Morante et al (2010)
who obtained the best results on Task2 achieved
a medium-ranked F-measure on the cue-level (e.g.
their result on the cue-level is lower by 4% com-
7
pared to Zhou et al (2010), while on the scope-
level the difference is 13% in the reverse direc-
tion), which indicates that the real strength of the
system of Morante et al (2010) is the accurate de-
tection of scope boundaries.
Name P / R / F
Tang 63.0 / 25.7 / 36.5
Li 76.1 / 21.6 / 33.7
O?zgu?r 28.9 / 14.7 / 19.5
Morante 24.6 / 7.3 / 11.3
Table 4: Wikipedia cue-level results.
Name P / R / F type
Tang 81.7 / 81.0 / 81.3 C
Zhou 83.1 / 78.8 / 80.9 C
Li 87.4 / 73.4 / 79.8 C
Rei 81.4 / 77.4 / 79.3 C
Velldal 81.2 / 76.3 / 78.7 C
Zhang 82.1 / 75.3 / 78.5 C
Ji 78.7 / 76.2 / 77.4 C
Morante 78.8 / 74.7 / 76.7 C
Kilicoglu 86.5 / 67.7 / 76.0 O
Vlachos 82.0 / 70.6 / 75.9 C
Zhao 76.7 / 73.9 / 75.3 X
Fernandes 79.2 / 64.7 / 71.2 C
Zhao 63.7 / 74.1 / 68.5 C
Ta?ckstro?m 66.9 / 58.6 / 62.5 C
O?zgu?r 49.1 / 57.8 / 53.1 C
Table 5: Biological cue-level results (type ?
{Closed(C), Cross(X), Open(O)}).
6.2 Approaches
The approaches to Task1 fall into two major cat-
egories. There were six systems which handled
the task as a classical sentence classification prob-
lem and employed essentially a bag-of-words fea-
ture representation (they are marked as BoW in
Table 6). The remaining teams focused on the
cue phrases and sought to classify every token if
it was a part of a cue phrase, then a sentence was
predicted as uncertain if it contained at least one
recognized cue phrase. Five systems followed a
pure token classification approach (TC) for cue de-
tection while others used sequential labeling tech-
niques (usually Conditional Random Fields) to
identify cue phrases in sentences (SL).
The feature set employed in Task1 systems typ-
ically consisted of the wordform, its lemma or
stem, POS and chunk codes and about the half of
the participants constructed features from the de-
pendency and/or constituent parse tree of the sen-
tences as well (see Table 6 for details).
It is interesting to see that the top ranked sys-
tems of Task1B followed a sequence labeling ap-
proach, while the best systems on Task1W applied
a bag-of-words sentence classification. This may
be due to the fact that biological sentences have
relatively simple patterns. Thus the context of the
cue words (token classification-based approaches
used features derived from a window of the token
in question, thus, they exploited the relationship
among the tokens and their contexts) can be uti-
lized while Wikipedia weasels have a diverse na-
ture. Another observation is that the top systems
in both Task1B and Task1W are the ones which
did not derive features from syntactic parsing.
Each Task2 system was built upon a Task1 sys-
tem, i.e. they attempted to recognize the scopes
for the predicted cue phrases (however, Zhang et
al. (2010) have argued that the objective functions
of Task1 and Task2 cue detection problems are
different because of sentences containing multiple
hedge spans).
Most systems regarded multiple cues in a sen-
tence to be independent from each other and
formed different classification instances from
them. There were three systems which incor-
porated information about other hedge cues (e.g.
their distance) of the sentence into the feature
space and Zhang et al (2010) constructed a cas-
cade system which utilized directly the predicted
scopes (it processes cue phrases from left to right)
during predicting other scopes in the same sen-
tence.
The identification of the scope for a certain cue
was typically carried out by classifying each to-
ken in the sentence. Task2 systems differ in the
number of class labels used as target and in the
machine learning approaches applied. Most sys-
tems ? following Morante and Daelemans (2009)
? used three class labels (F)IRST, (L)AST and
NONE. Two participants used four classes by
adding (I)NSIDE, while three systems followed
a binary classification approach (SCOPE versus
NONSCOPE). The systems typically included a
post-processing procedure to force scopes to be
continuous and to include the cue phrase in ques-
tion. The machine learning methods applied can
be again categorized into sequence labeling (SL)
8
NA
ME
ap
pro
ach
ma
ch
ine
fea
tur
e
fea
tur
es
em
plo
ye
d
lea
rne
r
sel
ect
ion
dic
t
ort
ho
lem
ma
/st
em
PO
S
ch
un
k
de
p
do
cp
art
oth
er
Cl
au
sen
Bo
W
Ma
xE
nt
+
+
he
dg
ec
ue
dis
tan
ce
Ch
en
Bo
W
Ma
xE
nt
sta
tis
tic
al
+
+
+
+
sen
ten
cel
en
gth
Fe
rna
nd
es
SL
ET
L
+
+
+
Ge
org
esc
ul
Bo
W
SV
M+
pa
ram
tun
ing
+
Ji
TC
Mo
dA
vg
Pe
rce
ptr
on
+
Ki
lic
og
lu
TC
ma
nu
al
+
+
+
ex
ter
na
ld
ict
Li
SL
CR
F+
po
stp
roc
gre
ed
yf
wd
+
+
+
Ma
ma
ni
Sa?
nc
he
z
Bo
W
SV
MT
ree
Ke
rne
l
+
+
+
+
+
Mo
ran
te
(w
iki
)
TC
SV
M+
po
stp
roc
sta
tis
tic
al
+
+
+
+
+
Mo
ran
te
(bi
o)
SL
KN
N
sta
tis
tic
al
+
+
+
+
+
+
Pra
bh
ak
ara
n
SL
CR
F
gre
ed
yf
wd
+
+
+
+
Le
vin
Cl
ass
Re
i
SL
CR
F
+
+
+
+
Sh
im
izu
SL
Ba
ye
sP
oin
tM
ach
ine
s
GA
+
+
+
+
NE
s,u
nla
be
led
da
ta
Sz
ida
rov
szk
y
SL
CR
F
ex
ha
ust
ive
+
+
+
Ta?
ck
str
o?m
Bo
W
SV
M
gre
ed
yf
wd
+
+
+
+
+
sen
ten
cel
en
gth
Ta
ng
SL
CR
F,S
VM
HM
M
sta
tis
tic
al
+
+
+
+
+
Tjo
ng
Ki
m
Sa
ng
TC
Na
ive
Ba
ye
s
Ve
lld
al
TC
Ma
xE
nt
ma
nu
al
+
+
+
+
Vl
ach
os
TC
Ba
ye
sia
nL
og
Re
g
ma
nu
al
+
+
+
+
Zh
an
g
SL
CR
F+
fea
tur
ec
om
bin
ati
on
gre
ed
yf
wd
+
+
+
+
+
NE
s
Zh
ao
SL
CR
F
sta
tis
tic
al
+
+
+
+
Zh
en
g
SL
CR
F,M
ax
En
t
ma
nu
al
+
+
+
+
Co
nst
itu
en
tP
ars
ing
Zh
ou
SL
CR
F
sta
tis
tic
al
+
+
+
+
Wo
rdN
et
Ta
ble
6:
Sy
ste
m
arc
hit
ect
ure
so
ve
rvi
ew
for
Ta
sk1
.A
pp
roa
ch
es:
seq
ue
nc
el
ab
eli
ng
(SL
),t
ok
en
cla
ssi
fic
ati
on
(T
C)
,b
ag
-of
-w
ord
sm
od
el
(B
oW
);M
ach
ine
lea
rne
rs:
En
tro
py
Gu
ide
dT
ran
sfo
rm
ati
on
Le
arn
ing
(E
TL
),A
ve
rag
ed
Pe
rce
ptr
on
(A
P),
k-n
ear
est
ne
igh
bo
ur
(K
NN
);F
eat
ure
sel
ect
ion
:g
ath
eri
ng
ph
ras
es
fro
m
the
tra
ini
ng
co
rpu
su
sin
gs
tat
ist
ica
lth
res
ho
lds
(st
ati
sti
cal
);F
eat
ure
s:
ort
ho
gra
ph
ica
lin
for
ma
tio
na
bo
ut
the
tok
en
(or
tho
),l
em
ma
or
ste
m
of
the
tok
en
(st
em
),P
art
-of
-Sp
eec
h
co
de
s(
PO
S),
syn
tac
tic
ch
un
ki
nfo
rm
ati
on
(ch
un
k),
de
pe
nd
en
cy
pa
rsi
ng
(de
p),
po
sit
ion
ins
ide
the
do
cu
me
nt
or
sec
tio
ni
nfo
rm
ati
on
(do
cp
os)
an
dt
ok
en
cla
ssi
fic
ati
on
(T
C)
ap
pro
ach
es
(se
eT
ab
le
7).
Th
ef
eat
ure
set
su
sed
he
re
are
the
sam
ea
sf
or
Ta
sk1
,e
xte
nd
ed
by
sev
era
lf
eat
ure
sd
esc
rib
ing
the
rel
ati
on
shi
pb
etw
een
the
cu
ep
hra
se
an
dt
he
tok
en
in
qu
est
ion
mo
stl
yb
yd
esc
rib
ing
the
de
pe
nd
en
cy
pa
th
be
tw
een
the
m.
9
NAME approach scope ML postproc tree dep multihedge
Fernandes TC FL ETL
Ji TC I AP +
Kilicoglu HC manual + + +
Li SL FL CRF, SVMHMM + + +
Morante TC FL KNN + +
Rei SL FIL manual+CRF + +
Ta?ckstro?m TC FI SVM +
Tang SL FL CRF + + +
Velldal HC manual +
Vlachos TC I Bayesian MaxEnt + +
Zhang SL FIL CRF + +
Zhao SL FL CRF +
Zhou SL FL CRF + +
Table 7: System architectures overview for Task2. Approaches: sequence labeling (SL), token clas-
sification (TC), hand-crafted rules (HC); Machine learners: Entropy Guided Transformation Learning
(ETL), Averaged Perceptron (AP), k-nearest neighbour (KNN); The way of identifying scopes: predict-
ing first/last tokens (FL), first/inside/last tokens (FIL), just inside tokens (I); Multiple Hedges: the system
applied a mechanism for handling multiple hedges inside a sentence
and token classification (TC) approaches (see Ta-
ble 7). The feature sets used here are the same
as for Task1, extended by several features describ-
ing the relationship between the cue phrase and the
token in question mostly by describing the depen-
dency path between them.
7 Conclusions
The CoNLL-2010 Shared Task introduced the
novel task of uncertainty detection. The challenge
consisted of a sentence identification task on un-
certainty (Task1) and an in-sentence hedge scope
detection task (Task2). In the latter task the goal
of automatic systems was to recognize speculative
text spans inside sentences.
The relatively high number of participants in-
dicates that the problem is rather interesting for
the Natural Language Processing community. We
think that this is due to the practical importance
of the task for (principally biomedical) applica-
tions and because it addresses several open re-
search questions. Although several approaches
were introduced by the participants of the shared
task and we believe that the ideas described in
this proceedings can serve as an excellent starting
point for the development of an uncertainty de-
tector, there is a lot of room for improving such
systems. The manually annotated datasets and
software tools developed for the shared task may
act as benchmarks for these future experiments
(they are freely available at http://www.inf.
u-szeged.hu/rgai/conll2010st).
Acknowledgements
The authors would like to thank Joakim Nivre
and Llu??s Ma?rquez for their useful suggestions,
comments and help during the organisation of the
shared task.
This work was supported in part by the
National Office for Research and Technol-
ogy (NKTH, http://www.nkth.gov.hu/)
of the Hungarian government within the frame-
work of the projects TEXTREND, BELAMI and
MASZEKER.
References
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. TEXT2TABLE: Medical Text Summa-
rization System Based on Named Entity Recogni-
tion and Modality Identification. In Proceedings of
the BioNLP 2009 Workshop, pages 185?192, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Wendy W. Chapman, David Chu, and John N. Dowl-
ing. 2007. ConText: An Algorithm for Identifying
Contextual Features from Clinical Text. In Proceed-
ings of the ACL Workshop on BioNLP 2007, pages
81?88.
Mike Conway, Son Doan, and Nigel Collier. 2009. Us-
ing Hedges to Enhance a Disease Outbreak Report
10
Text Mining System. In Proceedings of the BioNLP
2009 Workshop, pages 142?143, Boulder, Colorado,
June. Association for Computational Linguistics.
Carol Friedman, Philip O. Alderson, John H. M.
Austin, James J. Cimino, and Stephen B. Johnson.
1994. A General Natural-language Text Processor
for Clinical Radiology. Journal of the American
Medical Informatics Association, 1(2):161?174.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ingWikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 173?176, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Feng Ji, Xipeng Qiu, and Xuanjing Huang. 2010. De-
tecting Hedge Cues and their Scopes with Average
Perceptron. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2010): Shared Task, pages 139?146,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Yoshinobu Kano, William A. Baumgartner, Luke
McCrohon, Sophia Ananiadou, Kevin B. Cohen,
Lawrence Hunter, and Jun?ichi Tsujii. 2009. U-
Compare: Share and Compare Text Mining Tools
with UIMA. Bioinformatics, 25(15):1997?1998,
August.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing Speculative Language in Biomedical Research
Articles: A Linguistically Motivated Perspective.
In Proceedings of the Workshop on Current Trends
in Biomedical Natural Language Processing, pages
46?53, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2009. Syn-
tactic Dependency Based Heuristics for Biological
Event Extraction. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 119?127, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2010. A High-
Precision Approach to Detecting Hedges and Their
Scopes. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 103?110, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
George Lakoff. 1972. Linguistics and natural logic.
In The Semantics of Natural Language, pages 545?
665, Dordrecht. Reidel.
Xinxin Li, Jianping Shen, Xiang Gao, and Xuan
Wang. 2010. Exploiting Rich Features for Detect-
ing Hedges and Their Scope. In Proceedings of the
Fourteenth Conference on Computational Natural
Language Learning (CoNLL-2010): Shared Task,
pages 36?41, Uppsala, Sweden, July. Association
for Computational Linguistics.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The Language of Bioscience: Facts, Spec-
ulations, and Statements in Between. In Proceed-
ings of the HLT-NAACL 2004 Workshop: Biolink
2004, Linking Biological Literature, Ontologies and
Databases, pages 17?24.
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific
Literature. In Proceedings of the ACL, pages 992?
999, Prague, Czech Republic, June.
Roser Morante andWalter Daelemans. 2009. Learning
the Scope of Hedge Cues in Biomedical Texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36, Boulder, Colorado, June. Association for
Computational Linguistics.
Roser Morante, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based Resolution of In-
sentence Scopes of Hedge Cues. In Proceedings of
the Fourteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2010): Shared
Task, pages 48?55, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. De-
tecting Speculations and their Scopes in Scientific
Text. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1398?1407, Singapore, August. Associ-
ation for Computational Linguistics.
Gyo?rgy Szarvas. 2008. Hedge Classification in
Biomedical Texts with a Weakly Supervised Selec-
tion of Keywords. In Proceedings of ACL-08: HLT,
pages 281?289, Columbus, Ohio, June. Association
for Computational Linguistics.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A Cascade Method for De-
tecting Hedges and their Scope in Natural Language
Text. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 25?29, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Sofie Van Landeghem, Yvan Saeys, Bernard De Baets,
and Yves Van de Peer. 2009. Analyzing Text in
Search of Bio-molecular Events: A High-precision
Machine Learning Framework. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
11
Shared Task, pages 128?136, Boulder, Colorado,
June. Association for Computational Linguistics.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope Corpus: Biomedical Texts Annotated for Un-
certainty, Negation and their Scopes. BMC Bioin-
formatics, 9(Suppl 11):S9.
Shaodian Zhang, Hai Zhao, Guodong Zhou, and Bao-
liang Lu. 2010. Hedge Detection and Scope Find-
ing by Sequence Labeling with Procedural Feature
Selection. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 70?77, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Qi Zhao, Chengjie Sun, Bingquan Liu, and Yong
Cheng. 2010. Learning to Detect Hedges and their
Scope Using CRF. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning (CoNLL-2010): Shared Task, pages 64?
69, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Huiwei Zhou, Xiaoyan Li, Degen Huang, Zezhong Li,
and Yuansheng Yang. 2010. Exploiting Multi-
Features to Detect Hedges and Their Scope in
Biomedical Texts. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning (CoNLL-2010): Shared Task, pages 56?
63, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
12
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 49?55,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Data-driven Multilingual Coreference Resolution using Resolver Stacking
Anders Bjo?rkelund and Richa?rd Farkas
Institute for Natural Language Processing
University of Stuttgart
{anders,farkas}@ims.uni-stuttgart.de
Abstract
This paper describes our contribution to the
CoNLL 2012 Shared Task.1 We present a
novel decoding algorithm for coreference res-
olution which is combined with a standard
pair-wise coreference resolver in a stacking
approach. The stacked decoders are evaluated
on the three languages of the Shared Task. We
obtain an official overall score of 58.25 which
is the second highest in the Shared Task.
1 Introduction
In this paper we present our contribution to the
CoNLL 2012 Shared Task (Pradhan et al, 2012).
We follow the standard architecture where mentions
are extracted in the first step, then they are clustered
using a pair-wise classifier (see e.g., (Ng, 2010)).
For English, the set of extracted mentions is filtered
by removing non-referential occurrences of certain
pronouns. Our coreference resolver at its core re-
lies on a pair-wise classifier. To overcome the prob-
lems associated with the isolated pair-wise deci-
sions, we devised a novel decoding algorithm which
compares a mention to partially built clusters. For
our Shared Task contribution we combined this al-
gorithm with conventional pair-wise decoding algo-
rithms in a stacking approach.
In the Shared Task evaluation, our system re-
ceived an overall official score of 58.25, which is
the second highest among the sixteen participants.2
1The system is available for download on http://www.
ims.uni-stuttgart.de/?anders/
2The overall score is the average of MUC, B3, and CEAFE,
averaged over all three languages
2 Mention Extraction
Since all mentions are not annotated in Shared Task
data, but only mentions that take part in coreference
chains, training a general-purpose anaphoricity clas-
sifier is non-trivial. We thus implemented a high-
recall, low-precision mention extraction module that
allows the coreference resolver to see most of the
possible mentions, but has to learn to sort out the
non-referential mentions.
The mention extraction module relies mainly on
the syntactic parse tree, but also on named entities
(which were only provided for English in the pre-
dicted versions of the Shared Task data).
Since the part-of-speech tags vary a bit across the
languages, so do our extraction rules: For Arabic,
we extract all NP?s, and all terminals with part-of-
speech tags PRP and PRP$; for Chinese, we extract
all NP?s, and all terminals with part-of-speech tags
PN and NR; for English, we extract all NP?s, all ter-
minals with part-of-speech tags PRP and PRP$, and
all named entities.
Early experiments indicated that the English
coreference resolver frequently makes mistakes re-
lated to non-referential instances of the pronouns it
(often referred to as expletive or pleonastic in the lit-
erature), we, and you (generic mentions, which are
not annotated according to the OntoNotes annota-
tion guidelines). To address this issue, we developed
a referential/non-referential mention classifier in
order to identify these mentions. The classifier acts
as a filter after the mention extraction module and
removes clear cases of non-referential mentions.
Our basic assumption was that when these pro-
49
th = 0.5 th = 0.95
Precision Recall F1 Precision Recall F1 # occurrences
it 75.41 61.92 68 86.78 38.65 53.48 10,307
we 65.93 41.61 51.02 75.41 24.20 36.64 5,323
you 79.10 74.26 76.60 88.36 51.59 65.15 11,297
Average 75.73 63.05 68.81 86.17 41.04 55.60 26,927
Table 1: Performance of the non-referential classifier used for English. Precision, recall, and F-measure are broken
down by pronoun (top three rows), and the micro-average over all three (bottom row). The left side uses a probability
threshold of 0.5, and the right one a threshold of 0.95. The last column denotes the number of occurrences of the
corresponding token. All numbers are computed on the development set.
nouns do not participate in any coreference chain,
they are examples of non-referential mentions.
Based on this assumption, we extracted referential
and non-referential examples from the training set
and trained binary MaxEnt classifiers using the Mal-
let toolkit (McCallum, 2002).
Since the mentions filtered by these classifiers
are permanently removed, they are never presented
as potential mentions to the coreference resolver.
Hence, we aim for a classifier that yields few false
positives (i.e., mentions classified as non-referential
although they were not). False negatives, on the
other hand, may be passed on to the resolver, which,
ideally, does not assign them to a cluster. The pre-
cision/recall tradeoff can easily be controlled by ad-
justing the threshold of the posterior probability of
these classifiers, requiring a very high probability
that a mention is non-referential. Preliminary ex-
periments indicated that a threshold of 0.95 worked
best when the coreference resolver was trained and
evaluated on these filtered mentions.
We also found that the target pronouns should be
handled separately, i.e., instead of training one sin-
gle classifier we trained independent classifiers for
each of the target pronouns. The individual per-
formance of the classifiers, as well as the micro-
average over all three pronouns are shown in Ta-
ble 1, both using the default probability threshold
of 0.5, and the higher 0.95. In the final, fine-tuned
English coreference system, we found that the use
of the classifiers with the higher threshold improved
in all coreference metrics, and gave an increase of
about 0.5 in the official CoNLL score.
The feature set used by the classifiers describes
the (in-sentence) context of the pronoun. It consists
of the uni-, bi-, and trigrams of word forms and POS
tags in a window of ?5; the position inside the sen-
tence; the preceding and following verb and adjec-
tive; the distance to the following named entity; the
genre of the document; and whether the mention is
between quotes. For English, we additionally ex-
tended this general feature set by re-implementing
the features of Boyd et al (2005).
We investigated similar classifiers for Arabic and
Chinese as well. We selected targets based on the
frequency statistics of tokens being referential and
non-referential on the training set and used the gen-
eral feature set described above. However, these
classifiers did not contribute to the more complex
coreference system, hence the non-referential clas-
sifiers are included only in the English system.
3 Training Instance generation
To generate training instances for the pair-wise clas-
sifier, we employed the approach described by Soon
et al (2001). In this approach, for every extracted
anaphoric mention mj , we create a positive train-
ing instance with its closest preceding antecedent
mi: P = {(mi,mj)}. Negative examples are con-
structed by considering all the pairs of mj and the
(non-coreferent) mentions mk between mi and mj :
N = {(mk,mj)|i < k < j}. We extract the train-
ing examples on the version of the training set that
uses predicted information, and restrict the mentions
considered to the ones extracted by our mention ex-
traction module. Using these training examples, we
train a linear logistic regression classifier using the
LIBLINEAR package (Fan et al, 2008).
To create training examples for the English clas-
sifier, which uses the non-referential classifier for
pronouns, we made a 10-fold cross-annotation on
the training set with this classifier. I.e., the docu-
ments were partitioned into 10 sets D1, D2, ..., D10,
and when extracting training examples for docu-
50
ments in Dp, the non-referential classifier trained on
Dtp =
?
i 6=p
Di was applied.
4 Decoding
We implemented several decoding algorithms for
our resolver. The two most common decoding al-
gorithms often found in literature are the so-called
BestFirst (henceforth BF) and ClosestFirst (CF) al-
gorithms (Ng, 2010). Both work in a similar man-
ner and consider mentions linearly ordered as they
occur in the document. They proceed left-to-right
and for every mention mj , they consider all pairs
(mi,mj), where mi precedes mj , and queries the
classifier whether they are coreferent or not. The
main difference between the two algorithms is that
the CF algorithm selects the closest preceding men-
tion deemed coreferent with mj by the classifier,
while the BF algorithm selects the most probable
preceding mention. Most probable is determined
by some sort of confidence measure of how likely
two mentions are to corefer according to the classi-
fier. For both algorithms, the threshold can also be
tuned separately, e.g., requiring a probability larger
than a certain threshold thcoref in order to establish
a link between two mentions. Since the logistic clas-
sifiers we use directly model a probability distribu-
tion, we simply use the posterior probability of the
coref class as our confidence score.
Following Bjo?rkelund and Nugues (2011) we also
implemented a decoder that works differently de-
pending on whether mj is a pronoun or not. Specifi-
cally, for pronouns, the CF algorithm is used, other-
wise the BF algorithm is used. In the remainder, we
shall refer to this decoder as PronounsClosestFirst,
or simply PCF.
4.1 Disallowing transitive nesting
A specific kind of mistake we frequently saw in our
output is that two clearly disreferent nested mentions
are put in the same cluster. Although nestedness
can be used as a feature for the classifier, and this
appeared to improve performance, two nested men-
tions can still be put into the same cluster because
they are both classified as coreferent with a different,
preceding mention. The end result is that the two
nested mentions are inadvertently clustered through
transitivity.
For example, consider the two occurrences of the
phrase her mother in (1) below. The spans in the ex-
ample are labeled alphabetically according to their
linear order in the document.3 Before the resolver
considers the last mention d, it has already success-
fully placed (a, c) in the same cluster. The first pair
involving d is (c, d), which is correctly classified as
disreferent (here, the feature set informs the classi-
fier that (c, d) are nested). However, the pair (a, d)
is easily classified as coreferent since the head noun
of a agrees in gender and number with d (and they
are not nested).
A different problem is related to named entities
in possessive constructions. Consider (2), where our
mention extractor extracted e, because it was an NP,
and f , because it was tagged as a GPE by the named
entity recognizer. Again, the pair (e, f) is correctly
classified as disreferent, but both e and f are likely
to be classified as coreferent with preceding men-
tions of Taiwan, since our string matching feature
ignores possessive markers.
(1) ... she seemed to have such a good relation-
ship with [[her]b mother]a. Like [[her]d mother]c
treated her like a human being ...
(2) [[Taiwan]f ?s]e
To circumvent this problem, we let the decoders
build the clusters incrementally as they work their
way through a document and disallow this type of
transitive nesting. For instance, when the decoder is
trying to find an antecedent for d in (1), a and c have
already been clustered together, and when the pair
(c, d) is classified as disreferent, the decoder is con-
strained to skip over other members of c?s cluster as
it moves backwards in the document. This modifi-
cation gave an increase of about 0.6 in the CoNLL
score for English, and about 0.4 for Arabic and Chi-
nese, and we used this constraint whenever we use
the above-mentioned decoders.
4.2 A Cluster-Mention Decoding Algorithm
The pair-wise classifier architecture has, justifiably,
received much criticism as it makes decisions based
on single pairs of mentions only. We therefore de-
3We impose a total order on the mentions by sorting them
by starting point. For multiple mentions with the same starting
point, the longer is considered to precede the shorter.
51
vised a decoding algorithm that has a better perspec-
tive on entire clusters.
The algorithm works by incrementally merging
clusters as mentions are processed. Initially, every
mention forms its own cluster. When the next men-
tion mj is processed, it is compared to all the pre-
ceding mentions, M = {mi|i < j}. The score of
linking mj with mi is defined according to:
score(mi,mj) = (
?
mc?C
P (coref |(mc,mj)))
1/|C|
where P (coref |(mi,mj)) is the posterior probabil-
ity that mi and mj are coreferent according to the
pair-wise classifier, and C denotes the cluster that
mi belongs to.
After considering all preceding mentions, the
cluster of mj is merged with the cluster of the men-
tion with which it had the highest score, assuming
this score is higher than a given threshold thcoref .
Otherwise it remains in its own cluster.
The task of the score function is to capture
cluster-level information. When mj is compared to
a mention mi, the score is computed as the geo-
metric mean of the product of the probabilities of
linking mj to all mentions in the cluster that mi
belongs to. Also note that for two preceding men-
tions mi1 and mi2 that already belong to the same
cluster, score(mi1 ,mj) = score(mi2 ,mj). I.e., the
score is the same when mj is compared to all men-
tions belonging to the same cluster. Since this algo-
rithm works by maximizing the average probability
for linking a mention, we dub this algorithm Aver-
ageMaxProb, or AMP for short.
It should also be noted that other definitions
of the cluster score function score are conceiv-
able.4 However, initial experiments with other clus-
ter score functions performed worse than the defi-
nition above, and time prevented us from exploring
this conclusively.
Contrary to the pair-wise decoding algorithms
where pair-wise decisions are made in isolation, the
order in which mentions are processed make a dif-
ference to the AMP decoder. It is generally ac-
cepted that named entities are more informative and
4In the extreme case, one could take the maximum of the
link probabilities over the mentions that belong to the cluster
C, in which case the algorithm collapses into the BF algorithm.
easier to resolve than common noun phrases and
pronouns. To leverage this, we follow Sapena et
al. (2010) who reorder mentions based on mention
type. Specifically, we first process proper noun
phrases, then common noun phrases, and finally pro-
nouns. This implies that common noun phrases
have to have a reasonable agreement not only with
preceding proper noun phrases of a cluster, but all
proper noun phrases in a document (where reason-
able means that the geometric average of all poste-
rior probabilities stay reasonably high). Similarly,
pronouns are forced agree reasonably with all proper
and common nouns phrases in a given cluster, and
not only the preceding ones. Early experiments
showed an increase in performance using reorder-
ing, and we consequently used reordering for all lan-
guages in the experiments.
5 Features
An advantage of the pair-wise model and of the lin-
ear classifiers we use is that they can easily accom-
modate very large feature spaces, while still remain-
ing reasonably fast. We exploited this by building a
large number of parametrized feature templates, that
allowed us to experiment easily and quickly with
different feature sets. Additionally, since our clas-
sifiers are linear, we also evaluated a large number
of feature conjunctions, which proved to be crucial
to gain reasonable performance.
Due to space restrictions we can not list the com-
plete set of features used in this paper but mention
briefly what type of features we used. Most of them
are taken from previous work on coreference reso-
lution (Soon et al, 2001; Luo and Zitouni, 2005;
Sapena et al, 2010; Bjo?rkelund and Nugues, 2011).
For a complete list of features the reader can refer
to the download of the resolver, which includes the
feature sets and parameters used for every language.
One set of feature templates we use is based on
surface forms and part-of-speech tags of the first and
last, previous and following, and head tokens of the
spans that make up mentions. Another set of tem-
plates are based on the syntax trees, including both
subcategorization frames as well as paths in the syn-
tax tree. To extract head words of mentions, we
used the head percolation rules of Choi and Palmer
(2010) for Arabic and English, and those of Zhang
52
and Clark (2011) for Chinese.
While Chinese and English display no or rela-
tively small variety in morphological inflection, Ara-
bic has a very complex morphology. This means
that Arabic suffers from greater data sparseness with
respect to lexical features. This is exaggerated by
the fact that the Arabic training set is considerably
smaller than the Chinese and English ones. Hence,
we used the lemmas and unvocalised Buckwalter
forms that were provided in the Arabic dataset.
We also tried to extract number and gender in-
formation based on affixes of Arabic surface forms.
These features did, however, not help much. We
did however see a considerable increase in perfor-
mance when we added features that correspond to
the Shortest Edit Script (Myers, 1986) between sur-
face forms and unvocalised Buckwalter forms, re-
spectively. We believe that edit scripts are better at
capturing the differences in gender and number sig-
naled by certain morphemes than our hand-crafted
rules.
6 Resolver Stacking
In Table 2 we present a comparison of the BF, PCF,
and AMP resolvers. We omit the results of the CF
decoder, since it always did worse and the corre-
sponding numbers would not add more to the pic-
ture. The table shows F-measures of mention de-
tection (MD), the MUC metric, the B3 metric, and
the entity-based CEAF metric. The CoNLL score,
which is computed as the arithmetic mean of MUC,
B3, and CEAFE, is shown in the last row.
Comparing the AMP decoder to the pair-wise de-
coders, we find that it generally ? i.e., with respect
to the CoNLL average ? performs worse though it
always obtains higher scores with the CEAFE met-
ric. When we looked at the precision and recall for
mention detection, we also found that the AMP de-
coder suffers from lower recall, but higher precision.
This led us to conclude that this decoder is more con-
servative in terms of clustering mentions, and builds
smaller, but more consistent clusters. We could also
verify this when we computed average cluster sizes
on the output of the different decoders.
In order to combine the strengths of the AMP
decoder and the pair-wise decoders we employed
stacking, i.e., we feed the output of one resolver
Arabic BF PCF AMP Stacked
MD 58.63 58.49 58.21 60.51
MUC 45.8 45.4 43.2 46.66
B3 66.65 66.56 66.39 66.3
CEAFE 41.52 41.58 43.1 42.57
CoNLL 51.32 51.18 50.9 51.84
Chinese BF PCF AMP Stacked
MD 67.22 67.19 66.79 67.61
MUC 59.58 59.43 57.23 59.84
B3 72.9 72.82 72.7 73.35
CEAFE 46.99 46.98 48.25 47.7
CoNLL 59.82 59.74 59.39 60.30
English BF PCF AMP Stacked
MD 74.33 74.42 73.75 74.96
MUC 66.76 66.93 62.74 67.12
B3 70.96 71.11 68.05 71.18
CEAFE 45.46 45.83 46.49 46.84
CoNLL 61.06 61.29 59.09 61.71
Table 2: Performance of different decoders on the devel-
opment set for each language. The configuration of the
Stacked systems is described in detail in Section 7.
as input to a second. The second resolver is in-
formed about the decision of the first one by intro-
ducing an additional feature that encodes the deci-
sion of the first resolver. This feature can take five
values, depending on how the first resolver treated
the two mentions in question: NEITHER, when none
of the mentions were placed in a cluster; IONLY,
when only the first (antecedent) mention was placed
in a cluster; JONLY, when only the second (anaphor)
mention was placed in a cluster; COREF, when both
mentions were placed in the same cluster; and DIS-
REF, when both mentions were clustered, but in dif-
ferent clusters.
In addition to the stacking feature, the second re-
solver uses the exact same feature set as the first re-
solver. To generate the information for the stack fea-
ture for training, we made a 10-fold cross-annotation
on the training set, in the same way that we cross-
annotated the non-referential classifier for English.
In early stacking experiments, we experimented
with several combinations of the different decoders.
We found that stacking different pair-wise decoders
did not give any improvement. We believe the rea-
son for this is that these decoders are too similar and
hence can not really benefit from each other. How-
ever, when we used the AMP decoder as the first
53
step, and a pair-wise decoder as the second, we saw
an increase in performance, particularly with respect
to the CEAFE metric.
7 Feature and Parameter Tuning
For every language we tuned decoder parameters
and feature sets individually. The feature sets were
tuned semi-automatically by evaluating the addition
of a new feature template (or template conjunction)
to a baseline set. Ideally, we would add feature
templates to the baseline set incrementally one at a
time, following a cross-validation on the training set.
However, to reduce computational effort and time
consumption, we resorted to doing only one or two
folds out of a 4-fold cross-validation, and adding the
two to three most contributing templates in every it-
eration to the baseline set. The feature sets were op-
timized to maximize the official CoNLL score using
the standard BF decoder.
For the final submission we tuned the thresholds
for each decoder, and the choice of pair-wise de-
coder to use as the second decoder for each lan-
guage. Modifying the threshold of the AMP decoder
gave very small differences in overall score and we
kept the threshold for this decoder at 0.5. How-
ever, when we increased the probability threshold
for the second resolver, we found that performance
increased across all languages.
The choice of decoder for the second resolver, and
the probability threshold for this, was determined by
a 4-fold cross-validation on the training set. For our
final submission, as well as in the column Stacked
in Table 2, we used the following combinations: For
Arabic, the threshold was set to 0.60, and the PCF
decoder was used; for Chinese, the threshold was set
to 0.65, and the BF decoder was used; for English,
the threshold was set to 0.65, and the PCF decoder
was used.
8 Official Results
The final scores of our system are presented in Ta-
ble 3. The table also includes the results on the sup-
plementary tracks: gold mention boundaries (GB),
when the perfect boundaries of mentions were given;
and gold mentions (GM), when only the mentions in
the gold standard were given (with gold boundaries).
For all three settings we used the same model, which
Arabic PM GB GM
MD 60.55 60.61 76.43
MUC 47.82 47.90 60.81
B3 68.54 68.61 67.29
CEAFE 44.3 44 49.32
CoNLL 53.55 53.50 59.14
Chinese PM GB GM
MD 66.37 71.02 83.47
MUC 58.61 63.56 76.85
B3 73.10 74.52 76.30
CEAFE 48.19 50.20 56.61
CoNLL 59.97 62.76 69.92
English PM GB GM
MD 75.38 75.3 86.16
MUC 67.58 67.29 78.70
B3 70.26 69.70 72.67
CEAFE 45.87 45.27 53.23
CoNLL 61.24 60.75 68.20
Table 3: Performance on the shared task test set. Us-
ing predicted mentions (PM; i.e., the official evalua-
tion), gold mentions boundaries (GB), and gold mentions
(GM).
was trained on the concatenation of the training and
the development sets.
Compared to the results on the development set
(cf. Table 2), we see a slight drop for Chinese and
English, but a fairly big increase for Arabic. Given
that Chinese and English have the biggest training
sets, we speculate that the increase in Arabic might
stem from the increased lexical coverage provided
by training on both the training and the development
sets.
9 Conclusion
We have presented a novel cluster-based coreference
resolution algorithm. This algorithm was combined
with conventional pair-wise resolution algorithms in
a stacking approach. We applied our system to all
three languages in the Shared Task, and obtained an
official overall final score of 58.25 which was the
second highest in the Shared Task.
Acknowledgments
This work was supported by the Deutsche
Forschungsgemeinschaft (DFG) via the SFB 732
?Incremental Specification in Context?, projects D4
(PI Helmut Schmid) and D8 (PI Jonas Kuhn).
54
References
Anders Bjo?rkelund and Pierre Nugues. 2011. Explor-
ing lexicalized features for coreference resolution. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 45?50, June.
Adriane Boyd, Whitney Gegg-Harrison, and Donna By-
ron. 2005. Identifying non-referential it: A machine
learning approach incorporating linguistically moti-
vated patterns. In Proceedings of the ACL Workshop
on Feature Engineering for Machine Learning in Nat-
ural Language Processing, pages 40?47, June.
Jinho D. Choi and Martha Palmer. 2010. Robust
Constituent-to-Dependency Conversion for English.
In Proceedings of 9th Treebanks and Linguistic The-
ories Workshop (TLT), pages 55?66.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In Pro-
ceedings of Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing, pages 660?667, October.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Eugene W. Myers. 1986. An O(ND) difference algo-
rithm and its variations. Algorithmica, 1:251?266.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1396?1411, July.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012).
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2010. A
global relaxation labeling approach to coreference res-
olution. In Coling 2010: Posters, pages 1086?1094,
August.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Yue Zhang and Stephen Clark. 2011. Syntactic process-
ing using the generalized perceptron and beam search.
Computational Linguistics, 37(1):105?151.
55
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 122?127,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Munich-Edinburgh-Stuttgart Submissions of OSM Systems at WMT13
Nadir Durrani1, Helmut Schmid2, Alexander Fraser2,
Hassan Sajjad3, Richa?rd Farkas4
1University of Edinburgh ? dnadir@inf.ed.ac.uk
2Ludwig Maximilian University Munich ? schmid,fraser@cis.uni-muenchen.de
3Qatar Computing Research Institute ? hsajjad@qf.org.qa
4University of Szeged ? rfarkas@inf.u-szeged.hu
Abstract
This paper describes Munich-Edinburgh-
Stuttgart?s submissions to the Eighth
Workshop on Statistical Machine Transla-
tion. We report results of the translation
tasks from German, Spanish, Czech and
Russian into English and from English to
German, Spanish, Czech, French and Rus-
sian. The systems described in this paper
use OSM (Operation Sequence Model).
We explain different pre-/post-processing
steps that we carried out for different
language pairs. For German-English we
used constituent parsing for reordering
and compound splitting as preprocessing
steps. For Russian-English we transliter-
ated the unknown words. The translitera-
tion system is learned with the help of an
unsupervised transliteration mining algo-
rithm.
1 Introduction
In this paper we describe Munich-Edinburgh-
Stuttgart?s1 joint submissions to the Eighth Work-
shop on Statistical Machine Translation. We use
our in-house OSM decoder which is based on
the operation sequence N-gram model (Durrani
et al, 2011). The N-gram-based SMT frame-
work (Marin?o et al, 2006) memorizes Markov
chains over sequences of minimal translation units
(MTUs or tuples) composed of bilingual transla-
tion units. The OSM model integrates reordering
operations within the tuple sequences to form a
heterogeneous mixture of lexical translation and
1Qatar Computing Research Institute and University of
Szeged were partnered for RU-EN and DE-EN language pairs
respectively.
reordering operations and learns a Markov model
over a sequence of operations.
Our decoder uses the beam search algorithm in
a stack-based decoder like most sequence-based
SMT frameworks. Although the model is based
on minimal translation units, we use phrases dur-
ing search because they improve the search accu-
racy of our system. The earlier decoder (Durrani
et al, 2011) was based on minimal units. But we
recently showed that using phrases during search
gives better coverage of translation, better future
cost estimation and lesser search errors (Durrani
et al, 2013a) than MTU-based decoding. We have
therefore shifted to phrase-based search on top of
the OSM model.
This paper is organized as follows. Section 2
gives a short description of the model and search
as used in the OSM decoder. In Section 3 we
give a description of the POS-based operation se-
quence model that we test for our German-English
and English-German experiments. Section 4 de-
scribes our processing of the German and English
data for German-English and English-German ex-
periments. In Section 5 we describe the unsuper-
vised transliteration mining that has been done for
the Russian-English and English-Russian experi-
ments. In Section 6 we describe the sub-sampling
technique that we have used for several language
pairs. In Section 7 we describe the experimental
setup followed by the results. Finally we summa-
rize the paper in Section 8.
2 System Description
2.1 Model
Our systems are based on the OSM (Operation Se-
quence Model) that simultaneously learns trans-
lation and reordering by representing a bilingual
122
Figure 1: Bilingual Sentence with Alignments
sentence pair and its alignments as a unique se-
quence of operations. An operation either jointly
generates source and target words, or it performs
reordering by inserting gaps or jumping to gaps.
We then learn a Markov model over a sequence of
operations o1, o2, . . . , oJ that encapsulate MTUs
and reordering information as:
posm(o1, ..., oJ) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
By coupling reordering with lexical generation,
each (translation or reordering) decision depends
on n? 1 previous (translation and reordering) de-
cisions spanning across phrasal boundaries. The
reordering decisions therefore influence lexical se-
lection and vice versa. A heterogeneous mixture
of translation and reordering operations enables us
to memorize reordering patterns and lexicalized
triggers unlike the classic N-gram model where
translation and reordering are modeled separately.
2.2 Training
During training, each bilingual sentence pair is de-
terministically converted to a unique sequence of
operations.2 The example in Figure 1(a) is con-
verted to the following sequence of operations:
Generate(Beide, Both)? Generate(La?nder, coun-
tries)? Generate(haben, have)? Insert Gap?
Generate(investiert, invested)
At this point, the (partial) German and English
sentences look as follows:
Beide La?nder haben investiert
Both countries have invested
The translator then jumps back and covers the
skipped German words through the following se-
quence of operations:
Jump Back(1)?Generate(Millionen, millions)?
Generate(von, of)? Generate(Dollar, dollars)
2Please refer to Durrani et al (2011) for a list of opera-
tions and the conversion algorithm.
The generative story of the OSM model also
supports discontinuous source-side cepts and
source-word deletion. However, it doesn?t provide
a mechanism to deal with unaligned and discon-
tinuous target cepts. These are handled through
a 3-step process3 in which we modify the align-
ments to remove discontinuous and unaligned tar-
get MTUs. Please see Durrani et al (2011) for
details. After modifying the alignments, we con-
vert each bilingual sentence pair and its align-
ments into a sequence of operations as described
above and learn an OSM model. To this end,
a Kneser-Ney (Kneser and Ney, 1995) smoothed
9-gram model is trained with SRILM (Stolcke,
2002) while KenLM (Heafield, 2011) is used at
runtime.
2.3 Feature Functions
We use additional features for our model and em-
ploy the standard log-linear approach (Och and
Ney, 2004) to combine and tune them. We search
for a target string E which maximizes a linear
combination of feature functions:
E? = argmax
E
?
?
?
J?
j=1
?jhj(o1, ..., oJ)
?
?
?
where ?j is the weight associated with the fea-
ture hj(o1, ..., oj). Apart from the main OSM
feature we train 9 additional features: A target-
language model (see Section 7 for details), 2 lex-
ical weighting features, gap and open gap penalty
features, two distance-based distortion models and
2 length-based penalty features. Please refer to
Durrani et al (2011) for details.
2.4 Phrase Extraction
Phrases are extracted in the following way: The
aligned training corpus is first converted to an op-
eration sequence. Each subsequence of operations
that starts and ends with a translation operation, is
considered a ?phrase?. The translation operations
include Generate Source Only (X) operation which
deletes unaligned source word. Such phrases may
be discontinuous if they include reordering opera-
tions. We replace each subsequence of reordering
operations by a discontinuity marker.
3Durrani et al (2013b) recently showed that our post-
processing of alignments hurt the performance of the Moses
Phrase-based system in several language pairs. The solu-
tion they proposed has not been incorporated into the current
OSM decoder yet.
123
During decoding, we match the source tokens
of the phrase with the input. Whenever there is
a discontinuity in the phrase, the next source to-
ken can be matched at any position of the input
string. If there is no discontinuity marker, the next
source token in the phrase must be to the right of
the previous one. Finally we compute the number
of uncovered input tokens within the source span
of the hypothesized phrase and reject the phrase
if the number is above a threshold. We use a
threshold value of 2 which had worked well in
initial experiments. Once the positions of all the
source words of a phrase are known, we can com-
pute the necessary reordering operations (which
may be different from the ones that appeared in
the training corpus). This usage of phrases al-
lows the decoder to generalize from a seen trans-
lation ?scored a goal ? ein Tor schoss? (where
scored/a/goal and schoss/ein/Tor are aligned, re-
spectively) to ?scored a goal ? schoss ein Tor?.
The phrase can even be used to translate ?er schoss
heute ein Tor ? he scored a goal today? although
?heute? appears within the source span of the
phrase ?ein Tor schoss?. Without phrase-based
decoding, the unusual word translations ?schoss?
scored? and ?Tor?goal? (at least outside of the soc-
cer literature) are likely to be pruned.
The phrase tables are further filtered with
threshold pruning. The translation options with
a frequency less than x times the frequency of
the most frequent translation are deleted. We use
x = 0.02. We use additional settings to increase
this threshold for longer phrases. The phrase fil-
tering heuristic was used to speed up decoding. It
did not lower the BLEU score in our small scale
experiments (Durrani et al, 2013a), however we
could not test whether this result holds in a large
scale evaluation.
2.5 Decoder
The decoding framework used in the operation se-
quence model is based on Pharaoh (Koehn, 2004).
The decoder uses beam search to build up the
translation from left to right. The hypotheses are
arranged in m stacks such that stack i maintains
hypotheses that have already translated imany for-
eign words. The ultimate goal is to find the best
scoring hypothesis, that translates all the words
in the foreign sentence. During the hypothesis
extension each extracted phrase is translated into
a sequence of operations. The reordering opera-
tions (gaps and jumps) are generated by looking at
the position of the translator, the last foreign word
generated etc. (Please refer to Algorithm 1 in Dur-
rani et al (2011)). The probability of an opera-
tion depends on the n?1 previous operations. The
model is smoothed with Kneser-Ney smoothing.
3 POS-based OSM Model
Part-of-speech information is often relevant for
translation. The word ?stores? e.g. should be
translated to ?La?den? if it is a noun and to ?spei-
chert? when it is a verb. The sentence ?The small
child cries? might be incorrectly translated to ?Die
kleinen Kind weint? where the first three words
lack number, gender and case agreement.
In order to better learn such constraints which
are best expressed in terms of part of speech, we
add another OSM model as a new feature to the
log-linear model of our decoder, which is identi-
cal to the regular OSM except that all the words
have been replaced by their POS tags. The input
of the decoder consists of the input sentence with
automatically assigned part-of-speech tags. The
source and target part of the training data are also
automatically tagged and phrases with words and
POS tags on both sides are extracted. The POS-
based OSM model is only used in the German-to-
English and English-to-German experiments.4 So
far, we only used coarse POS tags without gender
and case information.
4 Constituent Parse Reordering
Our German-to-English system used constituent
parses for pre-ordering of the input. We parsed all
of the parallel German to English data available,
and the tuning, test and blind-test sets. We then
applied reordering rules to these parses. We used
the rules for reordering German constituent parses
of Collins et al (2005) together with the additional
rules described by Fraser (2009). These are ap-
plied as a preprocess to all German data (training,
tuning and test data). To produce the parses, we
started with the generative BitPar parser trained on
the Tiger treebank with optimizations of the gram-
mar, as described by (Fraser et al, 2013). We then
performed self-training using the high quality Eu-
roparl corpus - we parsed it, and then retrained the
parser on the output.
4This work is ongoing and we will present detailed exper-
iments in the future.
124
Following this, we performed linguistically-
informed compound splitting, using the system of
Fritzinger and Fraser (2010), which disambiguates
competing analyses from the high-recall Stuttgart
Morphological Analyzer SMOR (Schmid et al,
2004) using corpus statistics (Koehn and Knight,
2003). We also split portmanteaus like German
?zum? formed from ?zu dem? meaning ?to the?.
Due to time constraints, we did not address Ger-
man inflection. See Weller et al (2013) for further
details of the linguistic processing involved in our
German-to-English system.
5 Transliteration Mining/Handling
OOVs
The machine translation system fails to translate
out-of-vocabulary words (OOVs) as they are un-
known to the training data. Most of the OOVs
are named entities and simply passing them to
the output often produces correct translations if
source and target language use the same script.
If the scripts are different transliterating them to
the target language script could solve this prob-
lem. However, building a transliteration system
requires a list of transliteration pairs for training.
We do not have such a list and making one is a
cumbersome process. Instead, we use the unsu-
pervised transliteration mining system of Sajjad et
al. (2012) that takes a list of word pairs for train-
ing and extracts transliteration pairs that can be
used for the training of the transliteration system.
The procedure of mining transliteration pairs and
transliterating OOVs is described as follows:
We word-align the parallel corpus using
GIZA++ in both direction and symmetrize the
alignments using the grow-diag-final-and heuris-
tic. We extract all word pairs which occur as 1-
to-1 alignments (like Sajjad et al (2011)) and later
refer to them as the list of word pairs. We train the
unsupervised transliteration mining system on the
list of word pairs and extract transliteration pairs.
We use these mined pairs to build a transliteration
system using the Moses toolkit. The translitera-
tion system is applied in a post-processing step
to transliterate OOVs. Please refer to Sajjad et
al. (2013) for further details on our transliteration
work.
6 Sub-sampling
Because of scalability problems we were not able
to use the entire data made available for build-
ing the translation model in some cases. We used
modified Moore-Lewis sampling (Axelrod et al,
2011) for the language pairs es-en, en-es, en-fr,
and en-cs. In each case we included the News-
Commentary and Europarl corpora in their en-
tirety, and scored the sentences in the remaining
corpora (the selection corpus) using a filtering cri-
terion, adding 10% of the selection corpus to
the training data. We can not say with certainty
whether using the entire data will produce better
results with the OSM decoder. However, we know
that the same data used with the state-of-the-art
Moses produced worse results in some cases. The
experiments in Durrani et al (2013c) showed that
MML filtering decreases the BLEU scores in es-
en (news-test13: Table 19) and en-cs (news-test12:
Table 14). We can therefore speculate that being
able to use all of the data may improve our results
somewhat.
7 Experiments
Parallel Corpus: The amount of bitext used for
the estimation of the translation models is: de?en
? 4.5M and ru?en ? 2M parallel sentences. We
were able to use all the available data for cs-to-en
(? 15.6M sentences). However, sub-sampled data
was used for en-to-cs (? 3M sentences), en-to-fr
(? 7.8M sentences) and es?en (? 3M sentences).
Monolingual Language Model: We used all
the available training data (including LDC Giga-
word data) for the estimation of monolingual lan-
guage models: en? 287.3M sentences, fr? 91M,
es ? 65.7M, cs ? 43.4M and ru ? 21.7M sen-
tences. All data except for ru-en and en-ru was
true-cased. We followed the approach of Schwenk
and Koehn (2008) by training language models
from each sub-corpus separately and then linearly
interpolated them using SRILM with weights op-
timized on the held-out dev-set. We concatenated
the news-test sets from four years (2008-2011) to
obtain a large dev-set5 in order to obtain more sta-
ble weights (Koehn and Haddow, 2012).
Decoder Settings: For each extracted input
phrase only 15-best translation options were used
during decoding.6 We used a hard reordering limit
5For Russian-English and English-Russian language
pairs, we divided the tuning-set news-test 2012 into two
halves and used the first half for tuning and second for test.
6We could not experiment with higher n-best translation
options due to a bug that was not fixed in time and hindered
us from scaling.
125
of 16 words which disallows a jump beyond 16
source words. A stack size of 100 was used during
tuning and 200 for decoding the test set.
Results: Table 1 shows the uncased BLEU
scores along with the rank obtained on the sub-
mission matrix.7 We also show the results from
human evaluation.
Lang Evaluation
Automatic Human
BLEU Rank Win Ratio Rank
de-en 27.6 9/31 0.562 6-8
es-en 30.4 6/12 0.569 3-5
cs-en 26.4 3/11 0.581 2-3
ru-en 24.5 8/22 0.534 7-9
en-de 20.0 6/18
en-es 29.5 3/13 0.544 5-6
en-cs 17.6 14/22 0.517 4-6
en-ru 18.1 6/15 0.456 9-10
en-fr 30.0 7/26 0.541 5-9
Table 1: Translating into and from English
8 Conclusion
In this paper, we described our submissions to
WMT 13 in all the shared-task language pairs
(except for fr-en). We used an OSM-decoder,
which implements a model on n-gram of opera-
tions encapsulating lexical generation and reorder-
ing. For German-to-English we used constituent
parsing and applied linguistically motivated rules
to these parses, followed by compound splitting.
We additionally used a POS-based OSM model for
German-to-English and English-to-German exper-
iments. For Russian-English language pairs we
used unsupervised transliteration mining. Because
of scalability issues we could not use the entire
data in some language pairs and used only sub-
sampled data. Our Czech-to-English system that
was built from the entire data did better in both
automatic and human evaluation compared to the
systems that used sub-sampled data.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. We
would like to thank Philipp Koehn and Barry Had-
dow for providing data and alignments. Nadir
7http://matrix.statmt.org/
Durrani was funded by the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n ? 287658. Alexander
Fraser was funded by Deutsche Forschungsge-
meinschaft grant Models of Morphosyntax for
Statistical Machine Translation. Helmut Schmid
was supported by Deutsche Forschungsgemein-
schaft grant SFB 732. Richa?rd Farkas was
partially funded by the Hungarian National Ex-
cellence Program (TA?MOP 4.2.4.A/2-11-1-2012-
0001). This publication only reflects the authors?
views.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In ACL05, pages 531?540, Ann Arbor,
MI.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1045?1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a. Model With Minimal Translation Units, But
Decode With Phrases. In The 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013b. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013c. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Alexander Fraser, Helmut Schmid, Richa?rd Farkas,
Renjing Wang, and Hinrich Schu?tze. 2013. Knowl-
edge sources for constituent parsing of German, a
morphologically rich and less-configurational lan-
guage. Computational Linguistics - to appear.
126
Alexander Fraser. 2009. Experiments in Morphosyn-
tactic Processing for Translating to and from Ger-
man. In Proceedings of the EACL 2009 Fourth
Workshop on Statistical Machine Translation, pages
115?119, Athens, Greece, March.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the ACL 2010
Fifth Workshop on Statistical Machine Translation,
Uppsala, Sweden.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, United King-
dom, 7.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, vol-
ume I, pages 181?184, Detroit, Michigan, May.
Philipp Koehn and Barry Haddow. 2012. Towards Ef-
fective Use of Training Data in Statistical Machine
Translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 317?
321, Montre?al, Canada, June. Association for Com-
putational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 187?193, Morristown, NJ.
Philipp Koehn. 2004. Pharaoh: A Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In AMTA, pages 115?124.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrik Lambert, Jose? A. R. Fonol-
losa, and Marta R. Costa-jussa`. 2006. N-gram-
Based Machine Translation. Computational Lin-
guistics, 32(4):527?549.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(1):417?449.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised transliteration
mining with an application to word alignment. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, Portland, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and
semi-supervised transliteration mining. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, Jeju, Korea.
Hassan Sajjad, Svetlana Smekalova, Nadir Durrani,
Alexander Fraser, and Helmut Schmid. 2013.
QCRI-MES Submission at WMT13: Using Translit-
eration Mining to Improve Statistical Machine
Translation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition, and Inflec-
tion. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC).
Holger Schwenk and Philipp Koehn. 2008. Large and
Diverse Language Models for Statistical Machine
Translation. In International Joint Conference on
Natural Language Processing, pages 661?666, Jan-
uary 2008.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Richa?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart Submissions at WMT13: Mor-
phological and Syntactic Processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
127
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 232?239,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Munich-Edinburgh-Stuttgart Submissions at WMT13:
Morphological and Syntactic Processing for SMT
Marion Weller1, Max Kisselew1, Svetlana Smekalova1, Alexander Fraser2,
Helmut Schmid2, Nadir Durrani3, Hassan Sajjad4, Richa?rd Farkas5
1University of Stuttgart ? (wellermn|kisselmx|smekalsa)@ims.uni-stuttgart.de
2Ludwig-Maximilian University of Munich ? (schmid|fraser)@cis.uni-muenchen.de
3University of Edinburgh ? dnadir@inf.ed.ac.uk
4Qatar Computing Research Institute ? hsajjad@qf.org.qa
5University of Szeged ? rfarkas@inf.u-szeged.hu
Abstract
We present 5 systems of the Munich-
Edinburgh-Stuttgart1 joint submissions to
the 2013 SMT Shared Task: FR-EN, EN-
FR, RU-EN, DE-EN and EN-DE. The
first three systems employ inflectional gen-
eralization, while the latter two employ
parser-based reordering, and DE-EN per-
forms compound splitting. For our ex-
periments, we use standard phrase-based
Moses systems and operation sequence
models (OSM).
1 Introduction
Morphologically complex languages often lead to
data sparsity problems in statistical machine trans-
lation. For translation pairs with morphologically
rich source languages and English as target lan-
guage, we focus on simplifying the input language
in order to reduce the complexity of the translation
model. The pre-processing of the source-language
is language-specific, requiring morphological anal-
ysis (FR, RU) as well as sentence reordering (DE)
and dealing with compounds (DE). Due to time
constraints we did not deal with inflection for DE-
EN and EN-DE.
The morphological simplification process con-
sists in lemmatizing inflected word forms and deal-
ing with word formation (splitting portmanteau
prepositions or compounds). This needs to take
into account translation-relevant features (e.g. num-
ber) which vary across the different language pairs:
while French only has the features number and
gender, a wider array of features needs to be con-
sidered when modelling Russian (cf. table 6). In
addition to morphological reduction, we also apply
transliteration models learned from automatically
1The language pairs DE-EN and RU-EN were developed
in collaboration with the Qatar Computing Research Institute
and the University of Szeged.
mined transliterations to handle out-of-vocabulary
words (OOVs) when translating from Russian.
Replacing inflected word forms with simpler
variants (lemmas or the components of split com-
pounds) aims not only at reducing the general com-
plexity of the translation model, but also at decreas-
ing the amount of out-of-vocabulary words in the
input data. This is particularly the case with Ger-
man compounds, which are very productive and
thus often lack coverage in the parallel training
data, whereas the individual components can be
translated. Similarly, inflected word forms (e.g. ad-
jectives) benefit from the reduction to lemmas if
the full inflection paradigm does not occur in the
parallel training data.
For EN-FR, a translation pair with a morpho-
logically complex target language, we describe a
two-step translation system built on non-inflected
word stems with a post-processing component for
predicting morphological features and the genera-
tion of inflected forms. In addition to the advantage
of a more general translation model, this method
also allows the generation of inflected word forms
which do not occur in the training data.
2 Experimental setup
The translation experiments in this paper are car-
ried out with either a standard phrase-based Moses
system (DE-EN, EN-DE, EN-FR and FR-EN) or
with an operation sequence model (RU-EN, DE-
EN), cf. Durrani et al (2013b) for more details.
An operation sequence model (OSM) is a state-
of-the-art SMT-system that learns translation and
reordering patterns by representing a sentence pair
and its word alignment as a unique sequence of
operations (see e.g. Durrani et al (2011), Durrani
et al (2013a) for more details). For the Moses sys-
tems we used the old train-model perl scripts rather
than the EMS, so we did not perform Good-Turing
smoothing; parameter tuning was carried out with
batch-mira (Cherry and Foster, 2012).
232
1 Removal of empty lines
2 Conversion of HTML special characters like
&quot; to the corresponding characters
3 Unification of words that were written both
with an ? or with an oe to only one spelling
4 Punctuation normalization and tokenization
5 Putting together clitics and apostrophes like
l ? or d ? to l? and d?
Table 1: Text normalization for FR-EN.
Definite determiners la / l? / les ? le
Indefinite determiners un / une ? un
Adjectives Infl. form ? lemma
Portmanteaus e. g. au ? a` le
Verb participles Reduced to
inflected for gender non-inflected
and number verb participle form
ending in e?e/e?s/e?es ending in e?
Clitics and apostroph- d? ? de,
ized words are converted qu? ? que,
to their lemmas n? ? ne, ...
Table 2: Rules for morphological simplification.
The development data consists of the concate-
nated news-data sets from the years 2008-2011.
Unless otherwise stated, we use all constrained data
(parallel and monolingual). For the target-side lan-
guage models, we follow the approach of Schwenk
and Koehn (2008) and train a separate language
model for each corpus and then interpolate them
using weights optimized on development data.
3 French to English
French has a much richer morphology than English;
for example, adjectives in French are inflected with
respect to gender and number whereas adjectives
in English are not inflected at all. This causes data
sparsity in coverage of French inflected forms. We
try to overcome this problem by simplifying French
inflected forms in a pre-processing step in order to
adapt the French input better to the English output.
Processing of the training and test data The
pre-processing of the French input consists of two
steps: (1) normalizing not well-formed data (cf.
table 1) and (2) morphological simplification.
In the second step, the normalized training data
is annotated with Part-of-Speech tags (PoS-tags)
and word lemmas using RFTagger (Schmid and
Laws, 2008) which was trained on the French tree-
bank (Abeille? et al, 2003). French forms are then
simplified according to the rules given in table 2.
Data and experiments We trained a French to
English Moses system on the preprocessed and
System BLEU (cs) BLEU (ci)
Baseline 29.90 31.02
Simplified French* 29.70 30.83
Table 3: Results of the French to English system
(WMT-2012). The marked system (*) corresponds
to the system submitted for manual evaluation. (cs:
case-sensitive, ci: case-insensitive)
simplified constrained parallel data.
Due to tractability problems with word align-
ment, the 109 French-English corpus and the UN
corpus were filtered to a more manageable size.
The filtering criteria are sentence length (between
15 and 25 words), as well as strings indicating that
a sentence is neither French nor English, or other-
wise not well-formed, aiming to obtain a subset of
good-quality sentences. In total, we use 9M par-
allel sentences. For the English language model
we use large training data with 287.3M true-cased
sentences (including the LDC Giga-word data).
We compare two systems: a baseline with reg-
ular French text, and a system with the described
morphological simplifications. Results for the
WMT-2012 test set are shown in table 3. Even
though the baseline is better than the simplified
system in terms of BLEU, we assume that the trans-
lation model of the simplified system benefits from
the overall generalization ? thus, human annotators
might prefer the output of the simplified system.
For the WMT-2013 set, we obtain BLEU scores
of 29,97 (cs) and 31,05 (ci) with the system built
on simplified French (mes-simplifiedfrench).
4 English to French
Translating into a morphologically rich language
faces two problems: that of asymmetry of mor-
phological information contained in the source and
target language and that of data sparsity.
In this section we describe a two-step system de-
signed to overcome these types of problems: first,
the French data is reduced to non-inflected forms
(stems) with translation-relevant morphological fea-
tures, which is used to built the translation model.
The second step consists of predicting all neces-
sary morphological features for the translation out-
put, which are then used to generate fully inflected
forms. This two-step setup decreases the complex-
ity of the translation task by removing language-
specific features from the translation model. Fur-
thermore, generating inflected forms based on word
stems and morphological features allows to gener-
233
ate forms which do not occur in the parallel training
data ? this is not possible in a standard SMT setup.
The idea of separating the translation into two
steps to deal with complex morphology was in-
troduced by Toutanova et al (2008). Fraser et
al. (2012) applied this method to the language
pair English-German with an additional special
focus on word formation issues such as the split-
ting and merging of portmanteau prepositions and
compounds. The presented inflection prediction
systems focuses on nominal inflection; verbal in-
flection is not addressed.
Morphological analysis and resources The
morphological analysis of the French training data
is obtained using RFTagger, which is designed
for annotating fine-grained morphological tags
(Schmid and Laws, 2008). For generating inflected
forms based on stems and morphological features,
we use an extended version of the finite-state mor-
phology FRMOR (Zhou, 2007). Additionally, we
use a manually compiled list of abbreviations and
named entities (names of countries) and their re-
spective grammatical gender.
Stemming For building the SMT system, the
French data (parallel and monolingual) is trans-
formed into a stemmed representation. Nouns,
i.e. the heads of NPs or PPs, are marked with
inflection-relevant features: gender is considered
as part of the stem, whereas number is determined
by the source-side input: for example, we expect
source-language words in plural to be translated by
translated by stems with plural markup. This stem-
markup is necessary in order to guarantee that the
number information is not lost during translation.
For a better generalization, portmanteaus are split
into separate parts: au? a`+le (meaning, ?to the?).
Predicting morphological features For predict-
ing the morphological features of the SMT output
(number and gender), we use a linear chain CRF
(Lavergne et al, 2010) trained on data annotated
with these features using n-grams of stems and part-
of-speech tags within a window of 4 positions to
each side of the current word. Through the CRF,
the values specified in the stem-markup (number
and gender on nouns) are propagated over the rest
of the linguistic phrase, as shown in column 2 of
table 4. Based on the stems and the morphological
features, inflected forms can be generated using
FRMOR (column 3).
Post-processing As the French data has been
normalized, a post-processing step is needed in or-
der to generate correct French surface forms: split
portmanteaus are merged into their regular forms
based on a simple rule set. Furthermore, apostro-
phes are reintroduced for words like le, la, ne, ... if
they are followed by a vowel. Column 4 in table 4
shows post-processing including portmanteau for-
mation. Since we work on lowercased data, an
additional recasing step is required.
Experiments and evaluation We use the same
set of reduced parallel data as the FR-EN system;
the language model is built on 32M French sen-
tences. Results for the WMT-2012 test set are given
in table 5. Variant 1 shows the results for a small
system trained only on a part of the training data
(Europarl+News Commentary), whereas variant 2
corresponds to the submitted system. A small-scale
analysis indicated that the inflection prediction sys-
tem tends to have problems with subject-verb agree-
ment. We trained a factored system using addi-
tional PoS-tags with number information which
lead to a small improvement on both variants.
While the small model is significantly better than
the baseline2 as it benefits more from the general-
ization, the result for the full system is worse than
the baseline3. Here, given the large amount of
data, the generalization effect has less influence.
However, we assume that the more general model
from the inflection prediction system produces bet-
ter translations than a regular model containing a
large amount of irrelevant inflectional information,
particularly when considering that it can produce
well-formed inflected sequences that are inaccessi-
ble to the baseline. Even though this is not reflected
in terms of BLEU, humans might prefer the inflec-
tion prediction system.
For the WMT-2013 set, we obtain BLEU scores
of 29.6 (ci) and 28.30 (cs) with the inflection pre-
diction system mes-inflection (marked in table 5).
5 Russian-English
The preparation of the Russian data includes the
following stages: (1) tokenization and tagging and
(2) morphological reduction.
Tagging and tagging errors For tagging, we use
a version of RFTagger (Schmid and Laws, 2008)
2Pairwise bootstrap resampling with 1000 samples.
3However, the large inflection-prediction system has a
slightly better NIST score than the baseline (7.63 vs. 7.61).
234
SMT-output predicted generated after post- gloss
with stem-markup in bold print features forms processing
avertissement<Masc><Pl>[N] Masc.Pl avertissements avertissements warnings
sinistre[ADJ] Masc.Pl sinistres sinistres dire
de[P] ? de du from
le[ART] Masc.Sg le the
pentagone<Masc><Sg>[N] Masc.Sg pentagone pentagone pentagon
sur[P] ? sur sur over
de[P] ? de d? of
e?ventuel[ADJ] Fem.Pl e?ventuelles e?ventuelles potential
re?duction<Fem><Pl>[N] Fem.Pl re?ductions re?ductions reductions
de[P] ? de du of
le[ART] Masc.Sg le the
budget<Masc><Sg>[N] Masc.Sg budget budget budget
de[P] ? de de of
le[ART] Fem.Sg la la the
de?fense<Fem><Sg>[N] Fem.Sg de?fense de?fense de?fense
Table 4: Processing steps for the input sentence dire warnings from pentagon over potential defence cuts.
that has been developed based on data tagged with
TreeTagger (Schmid, 1994) using a model from
Sharoff et al (2008). The data processed by Tree-
Tagger contained errors such as wrong definition
of PoS for adverbs, wrong selection of gender for
adjectives in plural and missing features for pro-
nouns and adverbs. In order to train RFTagger, the
output of TreeTagger was corrected with a set of
empirical rules. In particular, the morphological
features of nominal phrases were made consistent
to train RFTagger: in contrast to TreeTagger, where
morphological features are regarded as part of the
PoS-tag, RFTagger allows for a separate handling
of morphological features and POS tags.
Despite a generally good tagging quality, some
errors seem to be unavoidable due to the ambiguity
of certain grammatical forms in Russian. A good
example of this are neuter nouns that have the same
form in all cases, or feminine nouns, which have
identical forms in singular genitive and plural nom-
inative (Sharoff et al, 2008). Since Russian has no
binding word order, and the case of nouns cannot
be determined on that basis, such errors cannot be
corrected with empirical rules implemented as post-
System BLEU (ci) BLEU (cs)
1 Baseline 24.91 23.40
InflPred 25.31 23.81
InflPred-factored 25.53 24.04
2 Baseline 29.32 27.65
InflPred* 29.07 27.40
InflPred-factored 29.17 27.46
Table 5: Results for French inflection prediction
on the WMT-2012 test set. The marked system (*)
corresponds to the system submitted for manual
evaluation.
processing. Similar errors occur when specifying
the case of adjectives, since the suffixes of adjec-
tives are even less varied as compared to the nouns.
In our application, we hope that this type of error
does not affect the result due to the following sup-
pression of a number of morphological attributes
including the case of adjectives.
Morphological reduction In comparison to
Slavic languages, English is morphologically poor.
For example, English has no morphological at-
tributes for nouns and adjectives to express gender
or case; verbs have no gender either. In contrast,
Russian is morphologically very rich ? there are
e.g. 6 cases and 3 grammatical genders, which
manifest themselves in different suffixes for nouns,
pronouns, adjectives and some verb forms. When
translating from Russian into English, many of
these attributes are (hopefully) redundant and are
therefore deleted from the training data. The mor-
phological reduction in our system was applied to
nouns, pronouns, verbs, adjectives, prepositions
and conjunctions. The rest of the POS (adverbs,
particles, interjections and abbreviations) have no
morphological attributes. The list of the original
and the reduced attributes is given in Table 6.
Transliteration mining to handle OOVs The
machine translation system fails to translate out-of-
vocabulary words (OOVs) as they are unknown to
the training data. Most of the OOVs are named en-
tities and transliterating them to the target language
script could solve this problem. The transliteration
system requires a list of transliteration pairs for
training. As we do not have such a list, we use
the unsupervised transliteration mining system of
Sajjad et al (2012) that takes a list of word pairs for
235
Part of Attributes Reduced
Speech RFTagger attributes
Noun Type Type
Gender Gender
Number Number
Case Case
nom,gen,dat,acc,instr,prep gen,notgen
Animate
Case 2
Pronoun Person Person
Gender Gender
Number Number
Case Case
nom,gen,dat,acc,instr,prep nom,notnom
Syntactic type
Animated
Verb Type Type
VForm VForm
Tense Tense
Person Person
Number Number
Gender
Voice Voice
Definiteness
Aspect Aspect
Case
Adjec- Type Type
tive Degree Degree
Gender
Number
Case
Definiteness
Prep- Type
osition Formation
Case
Conjunc- Type Type
tion Formation Formation
Table 6: Rules for simplifying the morphological
complexity for RU.
training and extracts transliteration pairs that can
be used for the training of the transliteration system.
The procedure of mining transliteration pairs and
transliterating OOVs is described as follows: We
word-align the parallel corpus using GIZA++ and
symmetrize the alignments using the grow-diag-
final-and heuristic. We extract all word pairs which
occur as 1-to-1 alignments (Sajjad et al, 2011) and
later refer to them as a list of word pairs. We train
the unsupervised transliteration mining system on
the list of word pairs and extract transliteration
pairs. We use these mined pairs to build a transliter-
ation system using the Moses toolkit. The translit-
eration system is applied as a post-processing step
to transliterate OOVs.
The morphological reduction of Russian (cf. sec-
tion 5) does not process most of the OOVs as they
are also unknown to the POS tagger. So OOVs that
we get are in their original form. When translit-
Original corpus
SYS WMT-2012 WMT-2013
GIZA++ 32.51 25.5
TA-GIZA++ 33.40 25.9*
Morph-reduced
SYS WMT-2012 WMT-2013
GIZA++ 31.22 24.3
TA-GIZA++ 31.40 24.45
Table 7: Russian to English machine translation
system evaluated on WMT-2012 and WMT-2013.
Human evaluation in WMT13 is performed on the
system trained using the original corpus with TA-
GIZA++ for alignment (marked with *).
erating them, the inflected forms generate wrong
English transliterations as inflectional suffixes get
transliterated too, specially OOV named entities.
We solved this problem by stemming the OOVs
based on a list of suffixes ( , , , , , ) and
transliterating the stemmed forms.
Experiments and results We trained the sys-
tems separately on GIZA++ and transliteration
augmented-GIZA++ (TA-GIZA++) to compare
their results; for more details see Sajjad et al
(2013). All systems are tuned using PROv1 (Nakov
et al, 2012). The translation output is post-
processed to transliterate OOVs.
Table 7 summarizes the results of RU-EN trans-
lation systems trained on the original corpus and
on the morph-reduced corpus. Using TA-GIZA++
alignment gives the best results for both WMT-
2012 and WMT-2013, leading to an improvement
of 0.4 BLEU points.
The system built on the morph-reduced data
leads to decreased BLEU results. However, the per-
centage of OOVs is reduced for both test sets when
using the morph-reduced data set compared to the
original data. An analysis of the output showed
that the morph-reduced system makes mistakes in
choosing the right tense of the verb, which might
be one reason for this outcome. In the future, we
would like to investigate this issue in detail.
6 German to English and English to
German
We submitted systems for DE-EN and EN-DE
which used constituent parses for pre-reordering.
For DE-EN we also deal with word formation is-
sues such as compound splitting. We did not per-
form inflectional normalization or generation for
German due to time constraints, instead focusing
236
our efforts on these issues for French and Russian
as previously described.
German to English German has a wider diver-
sity of clausal orderings than English, all of which
need to be mapped to the English SVO order. This
is a difficult problem to solve during inference, as
shown for hierarchical SMT by Fabienne Braune
and Fraser (2012) and for phrase-based SMT by
Bisazza and Federico (2012).
We syntactically parsed all of the source side
sentences of the parallel German to English data
available, and the tuning, test and blindtest sets.
We then applied reordering rules to these parses.
We use the rules for reordering German constituent
parses of Collins et al (2005) together with the
additional rules described by Fraser (2009). These
are applied as a preprocess to all German data.
For parsing the German sentences, we used the
generative phrase-structure parser BitPar with opti-
mizations of the grammar, as described by Fraser
et al (2013). The parser was trained on the Tiger
Treebank (Brants et al, 2002) along with utilizing
the Europarl corpus as unlabeled data. At the train-
ing of Bitpar, we followed the targeted self-training
approach (Katz-Brown et al, 2011) as follows. We
parsed the whole Europarl corpus using a grammar
trained on the Tiger corpus and extracted the 100-
best parse trees for each sentence. We selected the
parse tree among the 100 candidates which got the
highest usefulness scores for the reordering task.
Then we trained a new grammar on the concatena-
tion of the Tiger corpus and the automatic parses
from Europarl.
The usefulness score estimates the value of a
parse tree for the reordering task. We calculated
this score as the similarity between the word order
achieved by applying the parse tree-based reorder-
ing rules of Fraser (2009) and the word order indi-
cated by the automatic word alignment between
the German and English sentences in Europarl.
We used the Kendall?s Tau Distance as the simi-
larity metric of two word orderings (as suggested
by Birch and Osborne (2010)).
Following this, we performed linguistically-
informed compound splitting, using the system of
Fritzinger and Fraser (2010), which disambiguates
competing analyses from the high-recall Stuttgart
Morphological Analyzer SMOR (Schmid et al,
2004) using corpus statistics. We also split German
portmanteaus like zum? zu dem (meaning to the).
system BLEU BLEU system name
(ci) (cs)
DE-EN (OSM) 27.60 26.12 MES
DE-EN (OSM) 27.48 25.99 not submitted
BitPar not self-trained
DE-EN (Moses) 27.14 25.65 MES-Szeged-
reorder-split
DE-EN (Moses) 26.82 25.36 not submitted
BitPar not self-trained
EN-DE (Moses) 19.68 18.97 MES-reorder
Table 8: Results on WMT-2013 (blindtest)
English to German The task of mapping En-
glish SVO order to the different clausal orders in
German is difficult. For our English to German
systems, we solved this by parsing the English and
applying the system of Gojun and Fraser (2012) to
reorder English into the correct German clausal or-
der (depending on the clause type which is detected
using the English parse, see (Gojun and Fraser,
2012) for further details).
We primarily used the Charniak-Johnson gener-
ative parser (Charniak and Johnson, 2005) to parse
the English Europarl data and the test data. How-
ever, due to time constraints we additionally used
Berkeley parses of about 400K Europarl sentences
and the other English parallel training data. We
also left a small amount of the English parallel
training data unparsed, which means that it was
not reordered. For tune, test and blindtest (WMT-
2013), we used the Charniak-Johnson generative
parser.
Experiments and results We used all available
training data for constrained systems; results for
the WMT-2013 set are given in table 8. For the
contrastive BitPar results, we reparsed WMT-2013.
7 Conclusion
We presented 5 systems dealing with complex mor-
phology. For two language pairs with a morpho-
logically rich source language (FR and RU), the
input was reduced to a simplified representation
containing only translation-relevant morphologi-
cal information (e.g. number on nouns). We also
used reordering techniques for DE-EN and EN-DE.
For translating into a language with rich morphol-
ogy (EN-FR), we applied a two-step method that
first translates into a stemmed representation of
the target language and then generates inflected
forms based on morphological features predicted
on monolingual data.
237
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions, Daniel
Quernheim for providing Berkeley parses of some
of the English data, Stefan Ru?d for help with the
manual evalution, and Philipp Koehn and Barry
Haddow for providing data and alignments.
Nadir Durrani was funded by the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement n. 287658. Alexan-
der Fraser was funded by Deutsche Forschungs-
gemeinschaft grant Models of Morphosyntax for
Statistical Machine Translation and from the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement
n. 248005. Marion Weller was funded from the
European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement
n. 248005. Svetlana Smekalova was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Trans-
lation. Helmut Schmid and Max Kisselew were
supported by Deutsche Forschungsgemeinschaft
grant SFB 732. Richa?rd Farkas was supported by
the European Union and the European Social Fund
through project FuturICT.hu (grant n. TA?MOP-
4.2.2.C-11/1/KONV-2012-0013). This publication
only reflects the authors? views.
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Build-
ing a treebank for french. In A. Abeille?, editor, Tree-
banks. Kluwer, Dordrecht.
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of ACL WMT and MetricsMATR, Upp-
sala, Sweden.
Arianna Bisazza and Marcello Federico. 2012. Mod-
ified distortion matrices for phrase-based statistical
machine translation. In ACL, pages 478?487.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In ACL, pages 173?180, Ann Arbor, MI,
June. Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Porceedings of ACL 2005.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of ACL-HLT
2011, Portland, Oregon, USA.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a. Model With Minimal Translation Units, But
Decode With Phrases. In Proceedings of NAACL
2013, Atlanta, Georgia, USA.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richa?rd Farkas. 2013b. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Anita Gojun Fabienne Braune and Alexander Fraser.
2012. Long-distance reordering during search for
hierarchical phrase-based SMT. In Proceedings of
EAMT 2012.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proceedings of EACL 2012,
Avignon, France.
Alexander Fraser, Helmut Schmid, Richa?rd Farkas,
Renjing Wang, and Hinrich Schu?tze. 2013. Knowl-
edge sources for constituent parsing of German, a
morphologically rich and less-configurational lan-
guage. Computational Linguistics - to appear.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In EACL WMT.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to avoid burning ducks: Combining linguistic analy-
sis and corpus statistics for German compound pro-
cessing. In ACL WMT and Metrics MATR.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English-to-
German SMT. In Proceedings of EACL 2012.
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proceedings of EMNLP 2011, Edinburgh, Scotland.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL 2010, pages 504?513.
Preslav Nakov, Francisco Guzma?n, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. Mumbai, India.
238
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised transliteration
mining with an application to word alignment. In
Proceedings of ACL 2011, Portland, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and semi-
supervised transliteration mining. In Proceedings of
ACL 2012, Jeju, Korea.
Hassan Sajjad, Svetlana Smekalova, Nadir Durrani,
Alexander Fraser, and Helmut Schmid. 2013.
QCRI-MES Submission at WMT13: Using Translit-
eration Mining to Improve Statistical Machine
Translation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained pos tagging. In Pro-
ceedings of COLING 2008, Stroudsburg, PA, USA.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: a German Computational Morphology Cov-
ering Derivation, Composition, and Inflection. In
Proceedings of LREC 2004.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In Proceedings of IJCNLP 2008.
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating russian tagsets. In Proceedings of LREC
2008.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In Proceedings of ACL-HLT
2008.
Zhenxia Zhou. 2007. Entwicklung einer franzo?sischen
Finite-State-Morphologie. Diploma Thesis, Insti-
tute for Natural Language Processing, University of
Stuttgart.
239
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 62?67,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
LFG-based Features for Noun Number and Article Grammatical Errors
Ga?bor Berend1, Veronika Vincze2, Sina Zarriess3, Richa?rd Farkas1
1University of Szeged
Department of Informatics
{berendg,rfarkas}@inf.u-szeged.hu
2Research Group on Artificial Intelligence
Hungarian Academy of Sciences
vinczev@inf.u-szeged.hu
3University of Stuttgart
Institute for Natural Language Processing
zarriesa@ims.uni-stuttgart.de
Abstract
We introduce here a participating system
of the CoNLL-2013 Shared Task ?Gram-
matical Error Correction?. We focused on
the noun number and article error cate-
gories and constructed a supervised learn-
ing system for solving these tasks. We car-
ried out feature engineering and we found
that (among others) the f-structure of an
LFG parser can provide very informative
features for the machine learning system.
1 Introduction
The CoNLL-2013 Shared Task aimed at identify-
ing and correcting grammatical errors in the NU-
CLE learner corpus of English (Dahlmeier et al,
2013). This task has become popular in the natural
language processing (NLP) community in the last
few years (Dale and Kilgariff, 2010), which mani-
fested in the organization of shared tasks. In 2011,
the task Helping Our Own (HOO 2011) was held
(Dale and Kilgariff, 2011), which targeted the pro-
motion of NLP tools and techniques in improving
the textual quality of papers written by non-native
speakers of English within the field of NLP. The
next year, HOO 2012 (Dale et al, 2012) specifi-
cally focused on the correction of determiner and
preposition errors in a collection of essays writ-
ten by candidates sitting for the Cambridge ESOL
First Certificate in English (FCE) examination. In
2013, the CoNLL-2013 Shared Task has continued
this direction of research.
The CoNLL-2013 Shared Task is based on the
NUCLE corpus, which consists of about 1,400
student essays from undergraduate university stu-
dents at The National University of Singapore
(Dahlmeier et al, 2013). The corpus contains over
one million words and it is completely annotated
with grammatical errors and corrections. Among
the 28 error categories, this year?s shared task fo-
cused on the automatic detection and correction of
five specific error categories.
In this paper, we introduce our contribution of
the CoNLL-2013 Shared Task. We propose a su-
pervised learning-based approach. The main con-
tribution of this work is the exploration of several
feature templates for grammatical error categories.
We focused on the two ?nominal? error categories:
1.1 Article and Determiner Errors
This error type involved all kinds of errors
which were related to determiners and articles
(ArtOrDet). It required multiple correction
strategies. On the one hand, superfluous articles
or determiners should be deleted from the text.
On the other hand, missing articles or determin-
ers should be inserted and at the same time it was
sometimes also necessary to replace a certain type
of article or determiner to an other type. Here is
an example:
For nations like Iran and North Ko-
rea, the development of nuclear power
is mainly determined by the political
forces. ? For nations like Iran and
North Korea, the development of nu-
clear power is mainly determined by po-
litical forces.
62
1.2 Wrong Number of the Noun
The wrong number of nouns (Nn) meant that either
a singular noun should occur in the plural form or
a plural noun should occur in the singular form.
A special case of such errors was that sometimes
uncountable nouns were used in the plural, which
is ungrammatical. The correction involved here
the change of the number. Below we provide an
example:
All these measures are implemented to
meet the safety expectation of the op-
eration of nuclear power plant. ? All
these measures are implemented to meet
the safety expectation of the operation
of nuclear power plants.
2 System Description
Our approach for grammatical error detection was
to construct supervised classifiers for each candi-
date of grammatical error locations. In general,
our candidate extraction and features are based
on the output of the preprocessing step provided
by the organizers which contained both the POS-
tag sequences and the constituency phrase struc-
ture outputs for every sentence in the training and
test sets determined by Stanford libraries. We em-
ployed the Maximum Entropy based supervised
classification model using the MALLET API (Mc-
Callum, 2002), which was responsible for suggest-
ing the various corrections.
The most closely related approach to ours is
probably the work of De Felice and Pulman
(2008). We also employ a Maximum Entropy clas-
sifier and a syntax-motivated feature set. However,
we investigate deeper linguistic features (based on
the f-structure of an LFG parser).
In the following subsections we introduce our
correction candidate recognition procedure and
the features used for training and prediction of
the machine learning classifier. We employed the
same feature set for each classification task.
2.1 Candidate Locations
We used the following heuristics for the recogni-
tion of the possible locations of grammatical er-
rors. We also describe the task of various classi-
fiers at these candidate locations.
Article and Determiner Error category We
handled the beginning of each noun phrase
(NP) as a possible location for errors related
to articles or determiners. The NP was
checked if it started with any definite or
indefinite article. If it did, we asked our
three-class classifier whether to leave it
unmodified, change its type (i.e. an indefinite
to a definite one or vice versa) or simply
delete it. However, when there was no article
at all at the beginning of a noun phrase,
the decision made by a different three-class
classifier was whether to leave that position
empty or to put a definite or indefinite article
in that place.
Wrong Number of the Noun Error category
Every token tagged as a noun (either in plural
or singular) was taken into consideration at
this subtask. We constructed two ? i.e. one
for the word forms originally written in plu-
ral and singular ? binary classifiers whether
the number (i.e. plural or singular) of the
noun should be changed or left unchanged.
2.2 LFG parse-based features
We looked for the minimal governing NP for each
candidate location. We reparsed this NP with-
out context by a Lexical Functional Grammar
(LFG) parser and we acquired features from its
f-structure. In the following paragraph, LFG is
introduced briefly while Table 1 summarizes the
features extracted from the LFG parse.
Lexical Functional Grammar (LFG) (Bresnan,
2000) is a constraint-based theory of grammar. It
posits two levels of representation, c(onstituent)-
structure and f(unctional)-structure.
C-structure is represented by context free
phrase-structure trees, and captures surface gram-
matical configurations. F-structures approximate
basic predicate-argument and adjunct structures.
The experiments reported in this paper use the
English LFG grammar constructed as part of the
ParGram project (Butt et al, 2002). The gram-
mar is implemented in XLE, a grammar develop-
ment environment, which includes a very efficient
LFG parser. Within the spectrum of approaches to
natural language parsing, XLE can be considered
a hybrid system combining a hand-crafted gram-
mar with a number of automatic ambiguity man-
agement techniques:
(i) c-structure pruning where, based on informa-
tion from statistically obtained parses, some trees
are ruled out before f-structure unification (Cahill
et al, 2007)
63
COORD NP/PP is coordinated +/-
COORD-LEVEL syntactic category of coordi-
nated phrase
DEG-DIM dimension for comparitive NPs,
(?equative?/?pos?/?neg?)
DEGREE semantic type of adjec-
tival modifier (?posi-
tive?/?comparative?/?superlative?)
DET-TYPE type of determiner
(?def?/?indef?/?demon?)
LOCATION-TYPE marks locative NPs
NAME-TYPE ?first name?/?last name?
NSYN syntactic noun type (?com-
mon?/?proper?/?pronoun?)
PRON-TYPE syntactic pronoun type (e.g.
?pers?, ?refl?, ?poss?)
PROPER-TYPE type of proper noun (e.g. ?com-
pany?, ?location?, ?name?)
Table 1: Short characterization of the LFG fea-
tures incorporated in our models designed to cor-
rect noun phrase-related grammatical errors
(ii) an Optimality Theory-style constraint mecha-
nism for filtering and ranking competing analyses
(Frank et al, 2001),
and (iii) a stochastic disambiguation component
which is based on a log-linear probability model
(Riezler et al, 2002) and works on the packed rep-
resentations.
Although we use a deep, hand-crafted LFG
grammar for processing the data, our approach is
substantially different from other grammar-based
approaches to CALL. For instance, Fortmann and
Forst (2004) supplement a German LFG devel-
oped for newspaper text with so-called malrules
that accept marked or ungrammatical input of
some predefined types. In our work, we apply an
LFG parser developed for standard texts to get a
rich feature representation that can be exploited
by a classifier. While malrules would certainly be
useful for finding other error types, such as agree-
ment errors, the NP- and PP-errors are often ana-
lyzed as grammatical by the parser (e.g. ?the po-
litical forces? vs. ?political forces?). Thus, the
grammaticality of a phrase predicted by the gram-
mar is not necessarily a good indicator for correc-
tion in our case.
2.3 Phrase-based contextual features
Besides the LFG features describing the internal
structure of the minimal NP that dominates a can-
didate location, we defined features describing its
context as well. Phrase-based contextual features
searched for those minimal prepositional and noun
phrases that governed a token at a certain can-
Final results Corrected output
P 0.0552 0.1260
R 0.0316 0.0292
F 0.0402 0.0474
Table 2: Overall results aggregated over the five
error types
didate location of the sentence where a decision
was about to be taken. Then features encoding the
types of the phrases that preceded and succeeded
a given minimal governing noun or prepositional
phrase were extracted.
The length of those minimal governing noun
and prepositional phrases as well as those of the
preceding and succeeding ones were taken into
account as numeric features. The motivation be-
hind using the span size of the minimal governing
and neighboring noun and prepositional phrases
is that it was assumed that grammatical errors in
the sentence result in unusual constituency subtree
patterns that could manifest in minimal governing
phrases having too long spans for instance. The
relative position of the candidate position inside
the smallest dominating noun and prepositional
phrases was also incorporated as a feature since
this information might carry some information for
noun errors.
2.4 Token-based contextual features
A third group of features described the context of
the candidate location at the token level. Here, two
sets of binary features were introduced to mark the
fact if the token was present in the four token-sized
window to its left or right. Dedicated nominal fea-
tures were introduced to store the word form of
the token immediately preceding a decision point
within a sentence and the POS-tags at the preced-
ing and actual token positions.
Two lists were manually created which con-
sisted of entirely uncountable nouns (e.g. blood)
and nouns that are uncountable most of the times
(e.g. aid or dessert). When generating fea-
tures for those classifiers that can modify the plu-
rality of a noun, we marked the fact in a binary
feature if they were present in any of these lists.
Another binary feature indicated if the actual noun
to be classified could be found at an earlier point
of the document.
64
Only erroneous All sentences
P 0.1260 0.1061
R 0.0292 0.0085
F 0.0474 0.0158
Table 3: Overall results aggregated over the five
error types
Only erroneous All sentences
P 0.2500 0.0167
R 0.0006 0.0006
F 0.0012 0.0012
Table 4: Overall results aggregated over the five
error types, not using the LFG parser based fea-
tures
3 Results
It is important to note that our officially submit-
ted architecture included an unintended step which
meant that whenever our system predicted that at
a certain point an indefinite article should be in-
serted or (re-)written, the indefinite article an was
put at that place erroneously when the succeeding
token started with a consonant (e.g. outputting an
serious instead of a serious).
Since the output that contained this kind of error
served as the basis of the official ranking we in-
clude in Table 2 the results achieved with the out-
put affected by this unintended behavior, however,
in the following we present our results in such a
manner where this kind of error is eliminated from
the output of our system.
Upon training our systems we followed two
strategies. For the first approach we used all the
sentences regardless if they had any error in them
at all. However, in an alternative approach we uti-
lized only those sentences from the training corpus
that had at least one error in them from the five er-
ror categories to be dealt with in the shared task.
The different results achieved on the test set ac-
cording to the two approaches are detailed in Ta-
ble 3. Turning off the LFG features ended up in
the results detailed in Table 4.
Since our framework in its present state only
aims at the correction of errors explicitly re-
lated to noun phrases, no error categories besides
ArtOrDet and Nn (for more details see Sections
1.1 and 1.2, respectively) could be possibly cor-
rected by our system. Note that these two error
categories covered 66.1% of the corrections on the
test set, so with our approach this was the highest
possibly achievable score in recall.
In order to get a clearer picture on the effective-
ness of our proposed methodology on the two error
types that we aimed at, we present results focusing
on those two error classes.
Nn ArtOrDet
P 0.4783 (44/92) 0.0151 (4/263)
R 0.1111 (44/396) 0.0058 (4/690)
F 0.1803 0.0084
Table 5: The scores achieved and the number of
true positive, suggestions, real errors for the Noun
Number (Nn) and Article and Determiner Errors
(ArtOrDet) categories.
4 Error Analysis
In order to analyze the performance of our system
in more detail, we carried out an error analysis.
As our system was optimized for errors related to
nouns (i.e. Nn and ArtOrDet errors), we focus
on these error categories in our discussion and ne-
glect verbal and prepositional errors.
Some errors in our system?s output were due
to pronouns, which are conventionally tagged as
nouns (e.g. something), but were incorrectly put
in the plural, resulting in the erroneous correc-
tion somethings. These errors would have been
avoided by including a list of pronouns which
could not be used in the plural (even if they are
tagged as nouns).
Another common source of errors was that
countable and uncountable uses of nouns which
can have both features in different senses or
metonymic usage (e.g. coffee as a substance is un-
countable but coffee meaning ?a cup of coffee? is
countable) were hard to separate. Performance on
this class of nouns could be ameliorated by apply-
ing word sense disambiguation/discrimination or
a metonymy detector would also prove useful for
e.g. mass nouns.
A great number of nominal errors involved
cases where a singular noun occurred in the text
without any article or determiner. In English, this
is only grammatical in the case of uncountable
nouns which occur in generic sentences, for in-
stance:
Radio-frequency identification is a
technology which uses a wireless non-
contact system to scan and transfer the
data [...]
65
The above sentence offers a definition of radio-
frequency identification, hence it is a generic state-
ment and should be left as it is. In other cases,
two possible strategies are available for correc-
tion. First, the noun gets an article or a determiner.
The actual choice among the articles or determin-
ers depends on the context: if the noun has been
mentioned previously and thus is already known
(definite) in the context, it usually gets a definite
article (or a possessive determiner). If it is men-
tioned for the first time, it gets an indefinite arti-
cle (unless it is a unique thing such as the sun).
The difficulty of the problem lies in the fact that
in order to adequately assign an article or deter-
miner to the noun, it is not sufficient to rely only
on the sentence. Thus, is also necessary to go be-
yond the sentence and move on the level of text
or discourse, which requires natural language pro-
cessing techniques that we currently lack but are
highly needed. With the application of such tech-
niques, we would have probably achieved better
results but this remains now for future work.
Second, the noun could be put in the plural.
This strategy is usually applied when either there
are more than one of the thing mentioned or it is a
generic sentence (i.e. things are discussed in gen-
eral and no specific instances of things are spo-
ken of). In this case, the detection of generic sen-
tences/events would be helpful, which again re-
quires deep semantic processing of the discourse
and is also a possible direction for future work.
To conclude, the successful identification of
noun number and article errors would require a
much deeper semantic (and even pragmatic) anal-
ysis and representation of the texts in question.
5 Discussion and further work
Comparing the columns of Table 3 we can con-
clude that restricting the training sentences to only
those which had some kind of grammatical error
in them had a useful effect on the overall effec-
tiveness of our system.
In a similar way, it can be stated based on the
results in Table 4 that composing features from the
output of an LFG parser is essentially beneficial
for the determination of Nn-type errors. Table 5
reveals, however, that those features which work
relatively well on the correction of Nn type errors
are less useful on ArtOrDet-type errors without
any modification.
As our only target at this point was to suggest
error corrections related to noun phrases, our ob-
vious future plans include the extension of our sys-
tem to deal with error categories of different types.
Simultaneously, we are planning to utilize large
scale corpus statistics, such as the Google N-gram
Corpus to build a more effective system.
Acknowledgements
This work was supported in part by the European
Union and the European Social Fund through the
project FuturICT.hu (grant no.: TA?MOP-4.2.2.C-
11/1/KONV-2012-0013).
References
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell, Oxford.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The Parallel Grammar Project. In Proceedings of
COLING-2002 Workshop on Grammar Engineering
and Evaluation, Taipei, Taiwan.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2007. Speeding
up LFG Parsing using C-Structure Pruning. In Col-
ing 2008: Proceedings of the workshop on Grammar
Engineering Across Frameworks, pages 33 ? 40.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations (BEA 2013), Atlanta, Georgia, USA. Asso-
ciation for Computational Linguistics.
Robert Dale and Adam Kilgariff. 2010. Helping Our
Own: Text massaging for computational linguistics
as a new shared task. In Proceedings of the 6th Inter-
national Natural Language Generation Conference,
pages 261?265, Dublin, Ireland.
Robert Dale and Adam Kilgariff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the 13th European Workshop on Natural
Language Generation, Nancy, France.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?62,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Rachele De Felice and Stephen G. Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceed-
ings of the 22nd International Conference on Com-
66
putational Linguistics (Coling 2008), pages 169?
176.
Christian Fortmann and Martin Forst. 2004. An LFG
Grammar Checker for CALL. In Proceedings of
ICALL 2004.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell. 2001. Optimality Theory Style
Constraint Ranking in Large-Scale LFG Grammars.
In Peter Sells, editor, Formal and Empirical Issues in
Optimality Theoretic Syntax, pages 367?397. CSLI
Publications.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Stefan Riezler, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal us-
ing a Lexical-Functional Grammar and Discrimina-
tive Estimation Techniques. In Proceedings of ACL
2002.
67
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 135?145,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
(Re)ranking Meets Morphosyntax: State-of-the-art Results
from the SPMRL 2013 Shared Task?
Anders Bjo?rkelund?, O?zlem C?etinog?lu?, Richa?rd Farkas?, Thomas Mu?ller??, and Wolfgang Seeker?
?Institute for Natural Language Processing , University of Stuttgart, Germany
?Department of Informatics, University of Szeged, Hungary
?Center for Information and Language Processing, University of Munich, Germany
{anders,ozlem,muellets,seeker}@ims.uni-stuttgart.de
rfarkas@inf.u-szeged.hu
Abstract
This paper describes the IMS-SZEGED-CIS
contribution to the SPMRL 2013 Shared Task.
We participate in both the constituency and
dependency tracks, and achieve state-of-the-
art for all languages. For both tracks we make
significant improvements through high quality
preprocessing and (re)ranking on top of strong
baselines. Our system came out first for both
tracks.
1 Introduction
In this paper, we present our contribution to the 2013
Shared Task on Parsing Morphologically Rich Lan-
guages (MRLs). MRLs pose a number of interesting
challenges to today?s standard parsing algorithms,
for example a free word order and, due to their rich
morphology, greater lexical variation that aggravates
out-of-vocabulary problems considerably (Tsarfaty
et al, 2010).
Given the wide range of languages encompassed
by the term MRL, there is, as of yet, no clear con-
sensus on what approaches and features are gener-
ally important for parsing MRLs. However, devel-
oping tailored solutions for each language is time-
consuming and requires a good understanding of
the language in question. In our contribution to the
SPMRL 2013 Shared Task (Seddah et al, 2013), we
therefore chose an approach that we could apply to
all languages in the Shared Task, but that would also
allow us to fine-tune it for individual languages by
varying certain components.
?Authors in alphabetical order.
For the dependency track, we combined the n-
best output of multiple parsers and subsequently
ranked them to obtain the best parse. While this
approach has been studied for constituency parsing
(Zhang et al, 2009; Johnson and Ural, 2010; Wang
and Zong, 2011), it is, to our knowledge, the first
time this has been applied successfully within de-
pendency parsing. We experimented with different
kinds of features in the ranker and developed fea-
ture models for each language. Our system ranked
first out of seven systems for all languages except
French.
For the constituency track, we experimented
with an alternative way of handling unknown words
and applied a products of Context Free Grammars
with Latent Annotations (PCFG-LA) (Petrov et al,
2006), whose output was reranked to select the best
analysis. The additional reranking step improved
results for all languages. Our system beats vari-
ous baselines provided by the organizers for all lan-
guages. Unfortunately, no one else participated in
this track.
For both settings, we made an effort to automat-
ically annotate our data with the best possible pre-
processing (POS, morphological information). We
used a multi-layered CRF (Mu?ller et al, 2013) to
annotate each data set, stacking with the information
provided by the organizers when this was beneficial.
The high quality of our preprocessing considerably
improved the performance of our systems.
The Shared Task involved a variety of settings as
to whether gold or predicted part-of-speech tags and
morphological information were available, as well
as whether the full training set or a smaller (5k sen-
135
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
MarMoT 97.38/92.22 97.02/87.08 97.61/90.92 98.10/91.80 97.09/97.67 98.72/97.59 94.03/87.68 98.12/90.84 97.27/97.13
Stacked 98.23/89.05 98.56/92.63 97.83/97.62
Table 1: POS/morphological feature accuracies on the development sets.
tences) training set was used for training. Through-
out this paper we focus on the settings with pre-
dicted preprocessing information with gold segmen-
tation and the full1 training sets. Unless stated other-
wise, all given numbers are drawn from experiments
in this setting. For all other settings, we refer the
reader to the Shared Task overview paper (Seddah et
al., 2013).
The remainder of the paper is structured as fol-
lows: We present our preprocessing in Section 2 and
afterwards describe both our systems for the con-
stituency (Section 3) and for the dependency tracks
(Section 4). Section 5 discusses the results on the
Shared Task test sets. We conclude with Section 6.
2 Preprocessing
We first spent some time on preparing the data sets,
in particular we automatically annotated the data
with high-quality POS and morphological informa-
tion. We consider this kind of preprocessing to be an
essential part of a parsing system, since the quality
of the automatic preprocessing strongly affects the
performance of the parsers.
Because our tools work on CoNLL09 format, we
first converted the training data from the CoNLL06
format to CoNLL09. We thus had to decide whether
to use coarse or fine part-of-speech (POS) tags. In
a preliminary experiment we found that fine tags are
the better option for all languages but Basque and
Korean. For Korean the reason seems to be that the
fine tag set is huge (> 900) and that the same infor-
mation is also provided in the feature column.
We predict POS tags and morphological features
jointly using the Conditional Random Field (CRF)
tagger MarMoT2 (Mu?ller et al, 2013).
MarMoT incrementally creates forward-
backward lattices of increasing order to prune
the sizable space of possible morphological analy-
ses. We use MarMoT with the default parameters.
1Although, for Hebrew and Swedish only 5k sentences were
available for training, and the two settings thus coincide.
2https://code.google.com/p/cistern/
Since morphological dictionaries can improve au-
tomatic POS tagging considerably, we also created
such dictionaries for each language. For this, we an-
alyzed the word forms provided in the data sets with
language-specific morphological analyzers except
for Hebrew and German where we just extracted the
morphological information from the lattice files pro-
vided by the organizers. For the other languages
we used the following tools: Arabic: AraMorph
a reimplementation of Buckwalter (2002), Basque:
Apertium (Forcada et al, 2011), French: an IMS
internal tool,3 Hungarian: Magyarlanc (Zsibrita et
al., 2013), Korean: HanNanum (Park et al, 2010),
Polish: Morfeusz (Wolin?ski, 2006), and Swedish:
Granska (Domeij et al, 2000).
The created dictionaries were shared with the
other Shared Task participants. We used these dic-
tionaries as additional features for MarMoT.
For some languages we also integrated the pre-
dicted tags provided by the organizers into the fea-
ture model. These stacked models gave improve-
ments for Swedish, Polish and Basque (cf. Table 1
for accuracies).
For the full setting the training data was annotated
using 5-fold jackknifing. In the 5k setting, we addi-
tionally added all sentences not present in the parser
training data to the training data sets of the tagger.
This is similar to the predicted 5k files provided by
the organizers, where more training data than the 5k
was also used for prediction.
Table 3 presents a comparison between our graph-
based baseline parser using the preprocessing ex-
plained in this section (denoted mate) and the
preprocessing provided by the organizers (denoted
mate?). Our preprocessing yields improvements
for all languages but Swedish. The worse perfor-
mance for Swedish is due to the fact that the pre-
dictions provided by the organizers were produced
by models that were trained on a much larger data
3The French morphology was written by Zhenxia Zhou,
Max Kisselew and Helmut Schmid. It is an extension of Zhou
(2007) and implemented in SFST (Schmid, 2005).
136
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Berkeley 78.24 69.17 79.74 81.74 87.83 83.90 70.97 84.11 74.50
Replaced 78.70 84.33 79.68 82.74 89.55 89.08 82.84 87.12 75.52
Product 80.30 86.21 81.42 84.56 90.49 89.80 84.15 88.32 79.25
Reranked 81.24 87.35 82.49 85.01 90.49 91.07 84.63 88.40 79.53
Table 2: PARSEVAL scores on the development sets.
set. The comparison with other parsers demonstrates
that for some languages (e.g., Hebrew or Korean)
the improvements due to better preprocessing can
be greater than the improvements due to a better
parser. For instance, for Hebrew the parser trained
on the provided preprocessing is more than three
points (LAS) behind the three parsers trained on
our own preprocessing. However, the difference be-
tween these three parsers is less than a point.
3 Constituency Parsing
The phrase structure parsing pipeline is based on
products of Context Free Grammars with Latent An-
notations (PCFG-LA) (Petrov et al, 2006) and dis-
criminative reranking. We further replace rare words
by their predicted morphological analysis.
We preprocess the treebank trees by removing the
morphological annotation of the POS tags and the
function labels of all non-terminals. We also reduce
the 177 compositional Korean POS tags to their first
atomic tag, which results in a POS tag set of 9 tags.
PCFG-LAs are incrementally built by split-
ting non-terminals, refining parameters using EM-
training and reversing splits that only cause small
increases in likelihood.
Running the Berkeley Parser4 ? the reference im-
plementation of PCFG-LAs ? on the data sets results
in the PARSEVAL scores given in Table 2 (Berke-
ley). The Berkeley parser only implements a simple
signature-based unknown word model that seems to
be ineffective for some of the languages, especially
Basque and Korean.
We thus replace rare words (frequency < 20) by
the predicted morphological tags of Section 2 (or the
true morphological tag for the gold setup). The intu-
ition is that our discriminative tagger has a more so-
phisticated unknown word treatment than the Berke-
ley parser, taking for example prefixes, suffixes and
4http://code.google.com/p/
berkeleyparser/
the immediate lexical context into account. Further-
more, the morphological tag contains most of the
necessary syntactic information. An exception, for
instance, might be the semantic information needed
to disambiguate prepositional attachment. We think
that replacing rare words by tags has an advan-
tage over constraining the pre-terminal layer of the
parser, because the parser can still decide to assign
a different tag, for example in cases were the tag-
ger produces errors due to long-distance dependen-
cies. The used frequency threshold of 20 results
in token replacement rates of 18% (French) to 57%
(Korean and Polish), which correspond to 209 (for
Polish) to 3221 (for Arabic) word types that are not
replaced. The PARSEVAL scores for the described
method are again given in Table 2 (Replaced). The
method yields improvements for all languages ex-
cept for French where we observe a drop of 0.06.
The improvements range from 0.46 for Arabic to
1.02 for Swedish, 3.1 for Polish and more than 10
for Basque and Korean.
To further improve results, we employ the
product-of-grammars procedure (Petrov, 2010),
where different grammars are trained on the same
data set but with different initialization setups. We
trained 8 grammars and used tree-level inference.
In Table 2 (Product) we can see that this leads to
improvements from 0.72 for Hungarian to 3.73 for
Swedish.
On the 50-best output of the product parser,
we also carry out discriminative reranking. The
reranker is trained for the maximum entropy objec-
tive function of Charniak and Johnson (2005) and
use the standard feature set ? without language-
specific feature engineering ? from Charniak and
Johnson (2005) and Collins (2000). We use a
slightly modified version of the Mallet toolkit (Mc-
Callum, 2002) for reranking.
Improvements range from negligible differences
(< .1) for Hebrew and Polish to substantial differ-
ences (> 1.) for Basque, French, and Hungarian.
137
mate parser
best-first
parser
turboparser
merged list
of 50-100 best
trees/sentence
merged list
scored by
all parsers
ranker
ptb trees
Parsing Ranking
IN OUT
scores
scores
scores
features
Figure 1: Architecture of the dependency ranking system.
For our final submission, we used the reranker
output for all languages except French, Hebrew, Pol-
ish, and Swedish. This decision was based on an
earlier version of the evaluation setting provided by
the organizers. In this setup, reranking did not help
or was even harmful for these four languages. The
figures in Table 2 use the latest evaluation script and
are thus consistent with the test set results presented
in Section 5.
After the submission deadline the Shared Task
organizers made us aware that we had surprisingly
low exact match scores for Polish (e.g., 1.22 for
the reranked setup). The reason seems to be that
the Berkeley parser cannot produce unary chains of
length > 2. The gold development set contains 1783
such chains while the prediction of the reranked sys-
tem contains none. A particularly frequent unary
chain with 908 occurences in the gold data is ff ?
fwe ? formaczas. As this chain cannot be pro-
duced the parser leaves out the fwe phrase. Inserting
new fwe nodes between ff and formacszas nodes
raises the PARSEVAL scores of the reranked model
from 88.40 to 90.64 and the exact match scores to
11.34. This suggests that the Polish results could be
improved substantially if unary chains were properly
dealt with, for example by collapsing unary chains.5
4 Dependency Parsing
The core idea of our dependency parsing system
is the combination of the n-best output of several
5Thanks to Slav Petrov for pointing us to the unary chain
length limit.
parsers followed by a ranking step on the com-
bined list. Specifically, we first run two parsers that
each output their 50-best analyses for each sentence.
These 50-best analyses are merged together into one
single n-best list of between 50 and 100 analyses
(depending on the overlap between the n-best lists
of the two parsers). We then use the two parsers
plus an additional one to score each tree in the n-
best lists according to their parsing model, thus pro-
viding us with three different scores for each tree in
the n-best lists. The n-best lists are then given to
a ranker, which ranks the list using the three scores
and a small set of additional features in order to find
the best overall analysis. Figure 1 shows a schematic
of the process.
As a preprocessing step, we reduced the depen-
dency label set for the Hungarian training data.
The Hungarian dependency data set encodes ellipses
through composite edge labels which leads to a pro-
liferation of edge labels (more than 400). Since
many of these labels are extremely rare and thus hard
to learn for the parsers, we reduced the set of edge la-
bels during the conversion. Specifically, we retained
the 50 most frequent labels, while reducing the com-
posite labels to their base label.
For producing the initial n-best lists, we use
the mate parser6 (Bohnet, 2010) and a variant of
the EasyFirst parser (Goldberg and Elhadad, 2010),
which we here call best-first parser.
The mate parser is a state-of-the-art graph-based
dependency parser that uses second-order features.
6https://code.google.com/p/mate-tools
138
The parser works in two steps. First, it uses dy-
namic programming to find the optimal projective
tree using the Carreras (2007) decoder. It then
applies the non-projective approximation algorithm
proposed by McDonald and Pereira (2006) in or-
der to produce non-projective parse trees. The non-
projective approximation algorithm is a greedy hill
climbing algorithm that starts from the optimal pro-
jective parse and iteratively tries to reattach all to-
kens, one at a time, everywhere in the sentence as
long as the tree property holds. It halts when the in-
crease in the score of the tree according to the pars-
ing model is below a certain threshold.
n-best lists are obtained by applying the non-
projective approximation algorithm in a non-greedy
manner, exploring multiple possibilities. All trees
are collected in a list, and when no new trees are
found, or newer trees have a significantly lower
score than the currently best one, search halts. The
n best trees are then retrieved from the list. It
should be noted that, in the standard case, the non-
projective approximation algorithm may find a local
optimum, and that there may be other trees that have
a higher score which were not explored. Thus the
best parse in the greedy case may not necessarily
be the one with the highest score in the n-best list.
Since the parser is trained with the greedy version
of the non-projective approximation algorithm, the
greedily chosen output parse tree is of special in-
terest. We thus flag this tree as the baseline mate
parse, in order to use that for features in the ranker.
The baseline mate parse is also our overall baseline
in the dependency track.
The best-first parser deviates from the EasyFirst
parser in several small respects: The EasyFirst de-
coder creates dependency links between the roots of
adjacent substructures, which gives an O(n log n)
complexity, but restricts the output to projective
trees. The best-first parser is allowed to choose as
head any node of an adjacent substructure instead of
only the root, which increases complexity to O(n2),
but accounts for a big part of possible non-projective
structures. We additionally implemented a swap-
operation (Nivre, 2009; Tratz and Hovy, 2011) to
account for the more complex structures. The best-
first parser relies on a beam-search strategy7 to pur-
7Due to the nature of the decoder, the parser can produce
sue multiple derivations, which we also use to pro-
duce the n-best output.
In the scoring step, we additionally apply the tur-
boparser8 (Martins et al, 2010), which is based on
linear programming relaxations.9 We changed all
three parsers such that they would return a score for
a given tree. We use this to extract scores from each
parser for all trees in the n-best lists. It is impor-
tant to have a score from every parser for every tree,
as previously observed by Zhang et al (2009) in the
context of constituency reranking.
4.1 Ranking
Table 3 shows the performance of the individual
parsers measured on the development sets. It also
displays the oracle scores over the different n-best
lists, i.e., the maximal possible score over an n-best
list if the best tree is always selected.
The mate parser generally performs best followed
by turboparser, while the best-first parser comes last.
But we can see from the oracle scores that the best-
first parser often shows comparable or even higher
oracle scores than mate, and that the combination
of the n-best lists always adds substantial improve-
ments to the oracle scores. These findings show that
the mate and best-first parsers are providing differ-
ent sets of n-best lists. Moreover, all three parsers
rely on different parsing algorithms and feature sets.
For these reasons, we hypothesized that the parsers
contribute different views on the parse trees and that
their combination would result in better overall per-
formance.
In order to leverage the diversity between the
parsers we experimented with ranking10 on the
n-best lists. We used the same ranking model in-
troduced in Section 3 here as well. The model is
trained to select the best parse according to the la-
beled attachment score (LAS). The training data for
the ranker was created by 5-fold jackknifing on the
training sets. The feature sets for the ranker for
spurious ambiguities in the beam. If this occurs, only the one
with the higher score is kept.
8http://www.ark.cs.cmu.edu/TurboParser/
9Ideally we would also extract n-best lists from the tur-
boparser, however time prevented us from making the necessary
modifications.
10We refrain from calling it reranking in this setting, since
we are using merged n-best lists and the initial ranking is not
entirely clear to begin with.
139
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Baseline results for individual parsers
mate? 88.50/83.50 88.18/84.49 92.71/90.85 83.63/75.89 87.07/82.84 86.06/82.39 91.17/85.81 83.65/77.16
mate 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05
bf 87.61/85.32 84.07/75.90 87.45/83.92 92.90/91.10 86.10/79.57 83.85/75.94 86.54/83.97 90.10/83.75 82.27/75.36
turbo 87.82/85.35 88.88/83.84 88.24/84.57 93.59/91.54 85.74/78.95 86.86/82.80 88.35/86.23 90.97/85.55 83.24/76.15
Oracle scores for n-best lists
mate 90.85/88.74 93.39/89.85 90.99/87.81 97.14/95.84 89.05/83.03 91.41/88.19 94.86/92.96 95.19/91.67 87.19/81.66
bf 91.47/89.46 91.68/86.46 91.38/88.68 97.40/96.60 91.04/85.67 87.64/81.79 94.90/92.94 96.25/93.74 87.60/82.46
merged 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96
Table 3: Baseline performance and n-best oracle scores (UAS/LAS) on the development sets. mate? uses the prepro-
cessing provided by the organizers, the other parsers use the preprocessing described in Section 2.
each language were optimized manually via cross-
validation on the training sets. The features used for
each language, as well as a default (baseline) fea-
ture set, are shown in Table 4. We now outline the
features we used in the ranker:
Score from the base parsers ? denoted B, M,
T, for the best-first, mate, and turbo parsers, re-
spectively. We also have indicator features whether
a certain parse was the best according to a given
parser, denoted GB, GM, GT, respectively. Since
the mate parser does not necessarily assign the high-
est score to the baseline mate parse, the GM fea-
ture is a ternary feature which indicates whether a
parse is the same as the baseline mate parse, or bet-
ter, or worse. We also experimented with transfor-
mations and combinations of the scores from the
parsers. Specifically, BMProd denotes the product
of B and M; BMeProd denotes the sum of B and M
in e-space, i.e., eB+M ; reBMT, reBT, reMT denote
the normalized product of the corresponding scores,
where scores are normalized in a softmax fashion
such that all features take on values in the interval
(0, 1).
Projectivity features (Hall et al, 2007) ? the
number of non-projective edges in a tree, denoted
np. Whether a tree is ill-nested, denoted I. Since ill-
nested trees are extremely rare in the treebanks, this
helps the ranker filter out unlikely candidates from
the n-best lists. For a definition and further discus-
sion of ill-nestedness, we refer to (Havelka, 2007).
Constituent features ? from the constituent track
we also have constituent trees of all sentences which
can be used for feature extraction. Specifically, for
every head-dependent pair, we extract the path in the
constituent tree between the nodes, denoted ptbp.
Case agreement ? on head-dependent pairs that
both have a case value assigned among their mor-
phological features, we mark whether it is the same
case or not, denoted case.
Function label uniqueness ? on each training set
we extracted a list of function labels that generally
occur at most once as the dependent of a node, e.g.,
subjects or objects. Features are then extracted from
all nodes that have one or more dependents of each
label aimed at capturing mistakes such as double
subjects on a verb. This template is denoted FL.
In addition to the features mentioned above, we
experimented with a variety of feature templates, in-
cluding features drawn from previous work on de-
pendency reranking (Hall, 2007), e.g., lexical and
POS-based features over edges, ?subcategorization?
frames (i.e., the concatenation of POS-tags that are
headed by a certain node in the tree), etc, although
these features did not seem to help. For German we
created feature templates based on the constraints
used in the constraint-based parser by Seeker and
Kuhn (2013). This includes, e.g., violations in case
or number agreement between heads and depen-
dents, as well as more complex features that con-
sider labels on entire verb complexes. None of these
features yielded any clear improvements though. We
also experimented with features that target some
specific constructions (and specifics of annotation
schemes) which the parsers typically cannot fully
see, such as coordination, however, also here we saw
no clear improvements.
4.2 Effects of Ranking
In Table 5, we show the improvements from using
the ranker, both with the baseline and optimized fea-
tures sets for the ranker. For the sake of comparison,
140
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Baseline 87.68/85.42 89.11/84.43 88.30/84.84 93.15/91.46 86.05/79.37 88.03/84.41 87.91/85.76 91.51/86.30 83.53/77.05
Ranked-dflt 88.54/86.32 89.99/85.43 88.85/85.39 94.06/92.36 87.28/80.44 88.16/84.54 88.71/86.65 92.26/87.12 84.51/77.83
Ranked 88.93/86.74 89.95/85.61 89.37/85.96 94.20/92.68 87.63/81.02 88.38/84.77 89.20/87.12 93.02/87.69 85.04/78.57
Oracle 92.65/90.71 95.15/91.91 92.97/90.43 98.19/97.44 92.39/87.18 92.12/88.76 96.23/94.65 97.28/95.29 89.87/84.96
Table 5: Performance (UAS/LAS) of the reranker on the development sets. Baseline denotes our baseline. Ranked-dflt
and Ranked denote the default and optimized ranker feature sets, respectively. Oracle denotes the oracle scores.
default B, M, T, GB, GM, GT, I
Arabic B, M, T, GB, GM, I, ptbp, reBMT
Basque B, M, T, GB, GM, GT, I, ptbp, I, reMT, case
French B, M, T, GB, GM, GT, I, ptbp
German B, M, T, GM, I, BMProd, FL
Hebrew B, M, T, GB, GM, GT, I, ptbp, FL, BMeProd
Hungarian B, M, T, GB, GM, GT, I, ptbp, reBM, FL
Korean B, M, T, GB, GM, GT, I, ptbp, reMT, FL
Polish B, M, T, GB, GM, GT, I, ptbp, np
Swedish B, M, T, GB, GM, GT, I, ptbp, reBM, FL
Table 4: Feature sets for the dependency ranker for each
language. default denotes the default ranker feature set.
the baseline mate parses as well as the oracle parses
on the merged n-best lists are repeated from Table 3.
We see that ranking clearly helps, both with a tai-
lored feature set, as well as the default feature set.
The improvement in LAS between the baseline and
the tailored ranking feature sets ranges from 1.1%
(French) to 1.6% (Hebrew) absolute, with the excep-
tion of Hungarian, where improvements on the dev
set are more modest (contrary to the test set results,
cf. Section 5). Even with the default feature set, the
improvements range from 0.5% (French) to 1.1%
(Hebrew) absolute, again setting Hungarian aside.
We believe that this is an interesting result consid-
ering the simplicity of the default feature set.
5 Test Set Results
In this section we outline our final results on the test
sets. As previously, we focus on the setting with
predicted tags in gold segmentation and the largest
training set. We also present results on Arabic and
Hebrew for the predicted segmentation setting. For
the gold preprocessing and all 5k settings, we refer
the reader to the Shared Task overview paper (Sed-
dah et al, 2013).11
In Table 7, we present our results in the con-
11Or the results page online: http://www.spmrl.org/
spmrl2013-sharedtask-results.html
stituency track. Since we were the only participat-
ing team in the constituency track, we compare our-
selves with the best baseline12 provided by the or-
ganizers. Our system outperforms the baseline for
all languages in terms of PARSEVAL F1. Follow-
ing the trend on the development sets, reranking is
consistently helping across languages.13 Despite the
lack of other submissions in the shared task, we be-
lieve our numbers are generally strong and hope that
they can serve as a reference for future work on con-
stituency parsing on these data sets.
Table 8 displays our results in the dependency
track. We submitted two runs: a baseline, which
is the baseline mate parse, and the reranked trees.
The table also compares our results to the best per-
forming other participant in the shared task (denoted
Other) as well as the MaltParser (Nivre et al, 2007)
baseline provided by the shared task organizers (de-
noted ST Baseline). We obtain the highest scores
for all languages, with the exception of French. It is
also clear that we make considerable gains over our
baseline, confirming our results on the development
sets reported in Section 4. It is also noteworthy that
our baseline (i.e., the mate parser with our own pre-
processing) outperforms the best other system for 5
languages.
Arabic Hebrew
Other 90.75/8.48 88.33/12.20
Dep. Baseline 91.13/9.10 89.27/15.01
Dep. Ranked 91.74/9.83 89.47/16.97
Constituency 92.06/9.49 89.30/13.60
Table 6: Unlabeled TedEval scores (accuracy/exact
match) for the test sets in the predicted segmentation set-
ting. Only sentences of length ? 70 are evaluated.
12It should be noted that the Shared Task organizers com-
puted 2 different baselines on the test sets. The best baseline
results for each language thus come from different parsers.
13We remind the reader that our submission decisions are not
based on figures in Table 2, cf. Section 3.
141
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
ST Baseline 79.19 74.74 80.38 78.30 86.96 85.22 78.56 86.75 80.64
Product 80.81 87.18 81.83 80.70 89.46 90.58 83.49 87.55 83.99
Reranked 81.32 87.86 82.86 81.27 89.49 91.85 84.27 87.76 84.88
Table 7: Final PARSEVAL F1 scores for constituents on the test set for the predicted setting. ST Baseline denotes the
best baseline (out of 2) provided by the Shared Task organizers. Our submission is underlined.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
ST Baseline 83.18/80.36 79.77/70.11 82.49/77.98 81.51/77.81 76.49/69.97 80.72/70.15 85.72/82.06 82.19/75.63 80.29/73.21
Other 85.78/83.20 89.19/84.25 89.19/85.86 90.80/88.66 81.05/73.63 88.93/84.97 85.84/82.65 88.12/82.56 87.28/80.88
Baseline 86.96/84.81 89.32/84.25 87.87/84.37 90.54/88.37 85.88/79.67 89.09/85.31 87.41/85.51 90.30/85.51 86.85/80.67
Ranked 88.32/86.21 89.88/85.14 88.68/85.24 91.64/89.65 86.70/80.89 89.81/86.13 88.47/86.62 91.75/87.07 88.06/82.13
Table 8: Final UAS/LAS scores for dependencies on the test sets for the predicted setting. Other denotes the highest
scoring other participant in the Shared Task. ST Baseline denotes the MaltParser baseline provided by the Shared Task
organizers.
Table 6 shows the unlabeled TedEval (Tsarfaty et
al., 2012) scores (accuracy/exact match) on the test
sets for the predicted segmentation setting for Ara-
bic and Hebrew. Note that these figures only include
sentences of length less than or equal to 70. Since
TedEval enables cross-framework comparison, we
compare our submissions from the dependency track
to our submission from the constituency track. In
these runs we used the same systems that were used
for the gold segmentation with predicted tags track.
The predicted segmentation was provided by the
Shared Task organizers. We also compare our re-
sults to the best other system from the Shared Task
(denoted Other).
Also here we obtain the highest results for both
languages. However, it is unclear what syntactic
paradigm (dependencies or constituents) is better
suited for the task. All in all it is difficult to assess
whether the differences between the best and second
best systems for each language are meaningful.
6 Conclusion
We have presented our contribution to the 2013
SPMRL Shared Task. We participated in both the
constituency and dependency tracks. In both tracks
we make use of a state-of-the-art tagger for POS and
morphological features. In the constituency track,
we use the tagger to handle unknown words and em-
ploy a product-of-grammars-based PCFG-LA parser
and parse tree reranking. In the dependency track,
we combine multiple parsers output as input for a
ranker.
Since there were no other participants in the con-
stituency track, it is difficult to draw any conclusions
from our results. We do however show that the ap-
plication of product grammars, our handling of rare
words, and a subsequent reranking step outperforms
a baseline PCFG-LA parser.
In the dependency track we obtain the best re-
sults for all languages except French among 7 partic-
ipants. Our reranking approach clearly outperforms
a baseline graph-based parser. This is the first time
multiple parsers have been used in a dependency
reranking setup.
Aside from minor decisions made on the basis
of each language, our approach is language agnos-
tic and does not target morphology in any particu-
lar way as part of the parsing process. We show
that with a strong baseline and with no language
specific treatment it is possible to achieve state-of-
the-art results across all languages. Our architec-
ture for the dependency parsing track enables the use
of language-specific features in the ranker, although
we only had minor success with features that target
morphology. However, it may be the case that ap-
proaches from previous work on parsing MRLs, or
the approaches taken by other teams in the Shared
Task, can be successfully combined with ours and
improve parsing accuracy even more.
Acknowledgments
Richa?rd Farkas is funded by the European Union and
the European Social Fund through the project Fu-
turICT.hu (grant no.: TA?MOP-4.2.2.C-11/1/KONV-
142
2012-0013). Thomas Mu?ller is supported by a
Google Europe Fellowship in Natural Language
Processing. The remaining authors are funded by
the Deutsche Forschungsgemeinschaft (DFG) via
the SFB 732, projects D2 and D8 (PI: Jonas Kuhn).
We also express our gratitude to the treebank
providers for each language: Arabic (Maamouri et
al., 2004; Habash and Roth, 2009; Habash et al,
2009; Green and Manning, 2010), Basque (Aduriz
et al, 2003), French (Abeille? et al, 2003), He-
brew (Sima?an et al, 2001; Tsarfaty, 2010; Gold-
berg, 2011; Tsarfaty, 2013), German (Brants et al,
2002; Seeker and Kuhn, 2012), Hungarian (Csendes
et al, 2005; Vincze et al, 2010), Korean (Choi
et al, 1994; Choi, 2013), Polish (S?widzin?ski and
Wolin?ski, 2010), and Swedish (Nivre et al, 2006).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeille?, editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. D??az de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In TLT-03, pages 201?204.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, edi-
tors, Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT 2002), pages 24?41, So-
zopol, Bulgaria.
Tim Buckwalter. 2002. Buckwalter Arabic Morpholog-
ical Analyzer Version 1.0. Linguistic Data Consor-
tium, University of Pennsylvania, 2002. LDC Catalog
No.: LDC2002L49.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961, Prague, Czech Republic, June.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 173?180.
Key-Sun Choi, Young S Han, Young G Han, and Oh W
Kwon. 1994. Kaist tree bank project for korean:
Present and future development. In Proceedings of
the International Workshop on Sharable Natural Lan-
guage Resources, pages 7?14. Citeseer.
Jinho D. Choi. 2013. Preparing korean data for
the shared task on parsing morphologically rich lan-
guages. ArXiv e-prints.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of the Sev-
enteenth International Conference on Machine Learn-
ing, ICML ?00, pages 175?182.
Do?ra Csendes, Jano?s Csirik, Tibor Gyimo?thy, and Andra?s
Kocsor. 2005. The Szeged treebank. In Va?clav Ma-
tous?ek, Pavel Mautner, and Toma?s? Pavelka, editors,
Text, Speech and Dialogue: Proceedings of TSD 2005.
Springer.
Rickard Domeij, Ola Knutsson, Johan Carlberger, and
Viggo Kann. 2000. Granska-an efficient hybrid sys-
tem for Swedish grammar checking. In In Proceed-
ings of the 12th Nordic Conference in Computational
Linguistics.
Mikel L Forcada, Mireia Ginest??-Rosell, Jacob Nord-
falk, Jim O?Regan, Sergio Ortiz-Rojas, Juan An-
tonio Pe?rez-Ortiz, Felipe Sa?nchez-Mart??nez, Gema
Ram??rez-Sa?nchez, and Francis M Tyers. 2011. Aper-
tium: A free/open-source platform for rule-based ma-
chine translation. Machine Translation.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 742?750, Los Angeles, California, June.
Association for Computational Linguistics.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
Spence Green and Christopher D. Manning. 2010. Bet-
ter arabic parsing: Baselines, evaluations, and anal-
ysis. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 394?402, Beijing, China, August. Coling 2010
Organizing Committee.
Nizar Habash and Ryan Roth. 2009. Catib: The
columbia arabic treebank. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 221?
224, Suntec, Singapore, August. Association for Com-
putational Linguistics.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
143
Keith Hall, Jiri Havelka, and David A. Smith. 2007.
Log-Linear Models of Non-Projective Trees, k-best
MST Parsing and Tree-Ranking. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 962?966, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Keith Hall. 2007. K-best Spanning Tree Parsing. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 392?399, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Jiri Havelka. 2007. Beyond Projectivity: Multilin-
gual Evaluation of Constraints and Measures on Non-
Projective Structures. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 608?615, Prague, Czech Republic,
June. Association for Computational Linguistics.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 665?668, Los An-
geles, California, June. Association for Computational
Linguistics.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo Parsers: Depen-
dency Parsing by Approximate Variational Inference.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 34?
44, Cambridge, MA, October. Association for Compu-
tational Linguistics.
Andrew Kachites McCallum. 2002. ?mal-
let: A machine learning for language toolkit?.
http://mallet.cs.umass.edu.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 81?88, Trento, Italy. Asso-
ciation for Computational Linguistics.
Thomas Mu?ller, Helmut Schmid, and Hinrich Schu?tze.
2013. Efficient Higher-Order CRFs for Morphological
Tagging. In In Proceedings of EMNLP.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13:95?135, 6.
Joakim Nivre. 2009. Non-Projective Dependency Pars-
ing in Expected Linear Time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
351?359, Suntec, Singapore, August. Association for
Computational Linguistics.
S Park, D Choi, E-k Kim, and KS Choi. 2010. A plug-in
component-based Korean morphological analyzer. In
Proceedings of HCLT2010.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, pages 433?440. Associa-
tion for Computational Linguistics.
Slav Petrov. 2010. Products of Random Latent Variable
Grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Los Angeles, California, June. Associa-
tion for Computational Linguistics.
Helmut Schmid. 2005. A programming language for
finite state transducers. In FSMNLP.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Can-
dito, Jinho Choi, Richa?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola, Yoav Goldberg, Spence
Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan
Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolin?ski, and Alina Wro?blewska.
2013. Overview of the SPMRL 2013 Shared Task: A
Cross-Framework Evaluation of Parsing Morphologi-
cally Rich Languages. In Proceedings of the 4th Work-
shop on Statistical Parsing of Morphologically Rich
Languages: Shared Task, Seattle, WA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 3132?3139, Istanbul, Turkey. European
Language Resources Association (ELRA).
Wolfgang Seeker and Jonas Kuhn. 2013. Morphological
and Syntactic Case in Statistical Dependency Parsing.
Computational Linguistics, 39(1):23?55.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique des
Langues.
144
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Text,
Speech and Dialogue: 13th International Conference
(TSD), Lecture Notes in Artificial Intelligence, pages
197?204, Brno, Czech Republic. Springer.
Stephen Tratz and Eduard Hovy. 2011. A Fast, Ac-
curate, Non-Projective, Semantically-Enriched Parser.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1257?1268, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Reut Tsarfaty, Djame? Seddah, Yoav Goldberg, Sandra
Kuebler, Yannick Versley, Marie Candito, Jennifer
Foster, Ines Rehbein, and Lamia Tounsi. 2010. Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL) What, How and Whither. In Proc. of the
SPMRL Workshop of NAACL-HLT, pages 1?12, Los
Angeles, CA, USA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Joint Evaluation of Morphological Segmen-
tation and Syntactic Parsing. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
6?10, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A Unified Morpho-Syntactic
Scheme of Stanford Dependencies. Proceedings of
ACL.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian dependency treebank. In LREC.
Zhiguo Wang and Chengqing Zong. 2011. Parse Rerank-
ing Based on Higher-Order Lexical Dependencies. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 1251?1259, Chi-
ang Mai, Thailand, November. Asian Federation of
Natural Language Processing.
Marcin Wolin?ski. 2006. Morfeusz - A practical tool for
the morphological analysis of Polish. In Intelligent in-
formation processing and web mining, pages 511?520.
Springer.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-Best Combination of Syntactic Parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552?1560, Singapore, August. Association for Com-
putational Linguistics.
Zhenxia Zhou. 2007. Entwicklung einer franzo?sischen
Finite-State-Morphologie. Diplomarbeit, Institute for
Natural Language Processing, University of Stuttgart.
Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.
2013. Magyarlanc 2.0: Szintaktikai elemze?s e?s fel-
gyors??tott szo?faji egye?rtelmu?s??te?s. In IX. Magyar
Sza?m??to?ge?pes Nyelve?szeti Konferencia.
145
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
