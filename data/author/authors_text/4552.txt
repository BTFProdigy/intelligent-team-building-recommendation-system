Classifying Ellipsis in Dialogue: A Machine Learning Approach
Raquel FERNA?NDEZ, Jonathan GINZBURG and Shalom LAPPIN
Department of Computer Science
King?s College London
Strand, London WC2R 2LS, UK
{raquel,ginzburg,lappin}@dcs.kcl.ac.uk
Abstract
This paper presents a machine learning approach
to bare sluice disambiguation in dialogue. We ex-
tract a set of heuristic principles from a corpus-based
sample and formulate them as probabilistic Horn
clauses. We then use the predicates of such clauses
to create a set of domain independent features to an-
notate an input dataset, and run two different ma-
chine learning algorithms: SLIPPER, a rule-based
learning algorithm, and TiMBL, a memory-based
system. Both learners perform well, yielding simi-
lar success rates of approx 90%. The results show
that the features in terms of which we formulate our
heuristic principles have significant predictive power,
and that rules that closely resemble our Horn clauses
can be learnt automatically from these features.
1 Introduction
The phenomenon of sluicing?bare wh-phrases
that exhibit a sentential meaning?constitutes
an empirically important construction which
has been understudied from both theoretical
and computational perspectives. Most theoret-
ical analyses (e.g. (Ross, 1969; Chung et al,
1995)), focus on embedded sluices considered
out of any dialogue context. They rarely look
at direct sluices?sluices used in queries to re-
quest further elucidation of quantified parame-
ters (e.g. (1a)). With a few isolated exceptions,
these analyses also ignore a class of uses we refer
to (following (Ginzburg and Sag, 2001) (G&S))
as reprise sluices. These are used to request
clarification of reference of a constituent in a
partially understood utterance, as in (1b).
(1) a. Cassie: I know someone who?s a good kisser.
Catherine: Who? [KP4, 512]1
b. Sue: You were getting a real panic then.
Angela: When? [KB6, 1888]
Our corpus investigation shows that the com-
bined set of direct and reprise sluices constitutes
1This notation indicates the British National Corpus
file (KP4) and the sluice sentence number (512).
more than 75% of all sluices in the British Na-
tional Corpus (BNC). In fact, they make up ap-
prox. 33% of all wh-queries in the BNC.
In previous work (Ferna?ndez et al, to ap-
pear), we implemented G&S?s analysis of di-
rect sluices as part of an interpretation module
in a dialogue system. In this paper we apply
machine learning techniques to extract rules for
sluice classification in dialogue.
In Section 2 we present our corpus study of
classifying sluices into dialogue types and dis-
cuss the methodology we used in this study.
Section 3 analyses the distribution patterns we
identify and considers possible explanations for
these patterns. In Section 4 we identify a num-
ber of heuristic principles for classifying each
sluice dialogue type and formulate these prin-
ciples as probability weighted Horn clauses. In
Section 5, we then use the predicates of these
clauses as features to annotate our corpus sam-
ples of sluices, and run two machine learning
algorithms on these data sets. The first ma-
chine learner used, SLIPPER, extracts opti-
mised rules for identifying sluice dialogue types
that closely resemble our Horn clause principles.
The second, TiMBL, uses a memory-based ma-
chine learning procedure to classify a sluice by
generalising over similar environments in which
the sluice occurs in a training set. Both algo-
rithms performed well, yielding similar success
rates of approximately 90%. This suggests that
the features in terms of which we formulated our
heuristic principles for classifying sluices were
well motivated, and both learning algorithms
that we used are well suited to the task of dia-
logue act classification for fragments on the ba-
sis of these features. We finally present our con-
clusions and future work in Section 6.
2 Corpus Study
2.1 The Corpus
Our corpus-based investigation of bare sluices
has been performed using the ?10 million word
dialogue transcripts of the BNC. The corpus of
bare sluices has been constructed using SCoRE
(Purver, 2001), a tool that allows one to search
the BNC using regular expressions.
The dialogue transcripts of the BNC contain
5183 bare sluices (i.e. 5183 sentences consist-
ing of just a wh-word). We distinguish between
the following classes of bare sluices: what, who,
when, where, why, how and which. Given that
only 15 bare which were found, we have also
considered sluices of the form which N. Includ-
ing which N, the corpus contains a total of 5343
sluices, whose distribution is shown in Table 1.
The annotation was performed on two differ-
ent samples of sluices extracted from the total
found in the dialogue transcripts of the BNC.
The samples were created by arbitrarily select-
ing 50 sluices of each class (15 in the case of
which). The first sample included all instances
of bare how and bare which found, making up a
total of 365 sluices. The second sample con-
tained 50 instances of the remaining classes,
making up a total of 300 sluices.
what why who where
3045 1125 491 350
when which N how which
107 160 50 15
Total: 5343
Table 1: Total of sluices in the BNC
2.2 The Annotation Procedure
To classify the sluices in the first sample of our
sub-corpus we used the categories described be-
low. The classification was done by 3 expert
annotators (the authors) independently.
Direct The utterer of the sluice understands
the antecedent of the sluice without difficulty.
The sluice queries for additional information
that was explicitly or implicitly quantified away
in the previous utterance.
(2) Caroline: I?m leaving this school.
Lyne: When? [KP3, 538]
Reprise The utterer of the sluice cannot un-
derstand some aspect of the previous utterance
which the previous (or possibly not directly pre-
vious) speaker assumed as presupposed (typi-
cally a contextual parameter, except for why,
where the relevant ?parameter? is something
like speaker intention or speaker justification).
(3) Geoffrey: What a useless fairy he was.
Susan: Who? [KCT, 1753]
Clarification The sluice is used to ask for
clarification about the previous utterance as a
whole.
(4) June: Only wanted a couple weeks.
Ada: What? [KB1, 3312]
Unclear It is difficult to understand what
content the sluice conveys, possibly because the
input is too poor to make a decision as to its
resolution, as in the following example:
(5) Unknown : <unclear> <pause>
Josephine: Why? [KCN, 5007]
After annotating the first sample, we decided
to add a new category to the above set. The
sluices in the second sample were classified ac-
cording to a set of five categories, including the
following:
Wh-anaphor The antecedent of the sluice is
a wh-phrase.
(6) Larna: We?re gonna find poison apple and I
know where that one is.
Charlotte: Where? [KD1, 2371]
2.3 Reliability
To evaluate the reliability of the annotation, we
use the kappa coefficient (K) (Carletta, 1996),
which measures pairwise agreement between a
set of coders making category judgements, cor-
recting for expected chance agreement. 2
The agreement on the coding of the first
sample of sluices was moderate (K = 52).3
There were important differences amongst
sluice classes: The lowest agreement was on the
annotation for why (K = 29), what (K = 32)
and how (K = 32), which suggests that these
categories are highly ambiguous. Examina-
tion of the coincidence matrices shows that the
largest confusions were between reprise and
clarification in the case of what, and be-
tween direct and reprise for why and how.
On the other hand, the agreement on classi-
fying who was substantially higher (K = 71),
with some disagreements between direct and
reprise.
Agreement on the annotation of the 2nd sam-
ple was considerably higher although still not
entirely convincing (K = 61). Overall agree-
ment was improved in all classes, except for
2K = P (A)?P (E)/1?P (E), where P(A) is the pro-
portion of actual agreements and P(E) is the proportion
of expected agreement by chance, which depends on the
number of relative frequencies of the categories under
test. The denominator is the total proportion less the
proportion of chance expectation.
3All values are shown as percentages.
where and who. Agreement on what improved
slightly (K = 39), and it was substantially
higher on why (K = 52), when (K = 62) and
which N (K = 64).
Discussion Although the three coders may
be considered experts, their training and famil-
iarity with the data were not equal. This re-
sulted in systematic differences in their anno-
tations. Two of the coders (coder 1 and coder
2) had worked more extensively with the BNC
dialogue transcripts and, crucially, with the def-
inition of the categories to be applied. Leaving
coder 3 out of the coder pool increases agree-
ment very significantly: K = 70 in the first
sample, and K = 71 in the second one. The
agreement reached by the more expert pair of
coders was high and stable. It provides a solid
foundation for the current classification. It also
indicates that it is not difficult to increase an-
notation agreement by relatively light training
of coders.
3 Results: Distribution Patterns
In this section we report the results obtained
from the corpus study described in Section 2.
The study shows that the distribution of read-
ings is significantly different for each class of
sluice. Subsection 3.2 outlines a possible expla-
nation of such distribution.
3.1 Sluice/Interpretation Correlations
The distribution of interpretations for each class
of sluice is shown in Table 2. The distributions
are presented as percentages of pairwise agree-
ment (i.e. agreement between pairs of coders),
leaving aside the unclear cases. This allows
us to see the proportion made up by each in-
terpretation for each sluice class, together with
any correlations between sluice and interpreta-
tion. Distributions are similar over both sam-
ples, suggesting that corpus size is large enough
to permit the identification of repeatable pat-
terns.
Table 2 reveals interesting correlations be-
tween sluice classes and preferred interpreta-
tions. The most common interpretation for
what is clarification, making up 69% in the
first sample and 66% in the second one. Why
sluices have a tendency to be direct (57%,
83%). The sluices with the highest probability
of being reprise are who (76%, 95%), which
(96%), which N (88%, 80%) and where (75%,
69%). On the other hand, when (67%, 65%) and
how (87%) have a clear preference for direct
interpretations.
1st Sample 2nd Sample
Dir Rep Cla Dir Rep Cla Wh-a
what 9 22 69 7 23 66 4
why 57 43 0 83 14 0 3
who 24 76 0 0 95 0 5
where 25 75 0 22 69 0 9
when 67 33 0 65 29 0 6
which N 12 88 0 20 80 0 0
which 4 96 0 ? ? ? ?
how 87 8 5 ? ? ? ?
Table 2: Distributions as pairwise agr percentages
3.2 Explaining the Frequency Hierarchy
In order to gain a complete perspective on sluice
distribution in the BNC, it is appropriate to
combine the (averaged) percentages in Table 2
with the absolute number of sluices contained in
the BNC (see Table 1), as displayed in Table 3:
whatcla 2040 whichNrep 135
whydir 775 whendir 90
whatrep 670 whodir 70
whorep 410 wheredir 70
whyrep 345 howdir 45
whererep 250 whenrep 35
whatdir 240 whichNdir 24
Table 3: Sluice Class Frequency - Estim. Tokens
For instance, although more than 70% of why
sluices are direct, the absolute number of why
sluices that are reprise exceeds the total num-
ber of when sluices by almost 3 to 1. Explicating
the distribution in Table 3 is important in or-
der to be able to understand among other issues
whether we would expect a similar distribution
to occur in a Spanish or Mandarin dialogue cor-
pus; similarly, whether one would expect this
distribution to be replicated across different do-
mains. Here we restrict ourselves to sketching
an explanation of a couple of striking patterns
exhibited in Table 3.
One such pattern is the low frequency of when
sluices, particularly by comparison with what
one might expect to be its close cousin?where;
indeed the direct/reprise splits are almost
mirror images for when v. where. Another very
notable pattern, alluded to above, is the high
frequency of why sluices.4
The when v. where contrast provides one ar-
gument against (7), which is probably the null
4As we pointed out above, sluices are a common
means of asking wh?interrogatives; in the case of why?
interrogatives, this is even stronger?close to 50% of all
such interrogatives in the BNC are sluices.
hypothesis w/r to the distribution of reprise
sluices:
(7) Frequency of antecedent hypothesis:
The frequency of a class of reprise sluices
is directly correlated with the frequency of
the class of its possible antecedents.
Clearly locative expressions do not outnum-
ber temporal ones and certainly not by the
proportion the data in Table 3 would require
to maintain (7).5 (Purver, 2004) provides ad-
ditional data related to this?clarification re-
quests of all types in the BNC that pertain to
nominal antecedents outnumber such CRs that
relate to verbal antecedents by 40:1, which does
not correlate with the relative frequency of nom-
inal v. verbal antecedents (about 1.3:1).
A more refined hypothesis, which at present
we can only state quite informally, is (8):
(8) Ease of grounding of antecedent hy-
pothesis: The frequency of a class of
reprise sluices is directly correlated with
the ease with which the class of its possible
antecedents can be grounded (in the sense
of (Clark, 1996; Traum, 1994)).
This latter hypothesis offers a route towards
explaining the when v. where contrast. There
are two factors at least which make ground-
ing a temporal parameter significantly easier on
the whole than grounding a locative parameter.
The first factor is that conversationalists typi-
cally share a temporal ontology based on a clock
and/or calendar. Although well structured loca-
tive ontologies do exist (e.g. grid points in a
map), they are far less likely to be common cur-
rency. The natural ordering of clock/calendar-
based ontologies reflected in grammatical de-
vices such as sequence of tense is a second fac-
tor that favours temporal parameters over loca-
tives.
From this perspective, the high frequency of
why reprises is not surprising. Such reprises
query either the justification for an antecedent
assertion or the goal of an antecedent query.
Speakers usually do not specify these explicitly.
In fact, what requires explanation is why such
5A rough estimate concerning the BNC can be ex-
tracted by counting the words that occur more than 1000
times. Of these approx 35k tokens are locative in nature
and could serve as antecedents of where; the correspond-
ing number for temporal expressions and when yields
approx 80k tokens. These numbers are derived from a
frequency list (Kilgarriff, 1998) of the demographic por-
tion of the BNC.
reprises do not occur even more frequently than
they actually do. To account for this, one has
to appeal to considerations of the importance of
anchoring a contextual parameter.6
A detailed explication of the distribution
shown in Table 3 requires a detailed model of
dialogue interaction. We have limited ourselves
to suggesting that the distribution can be expli-
cated on the basis of some quite general princi-
ples that regulate grounding.
4 Heuristics for sluice
disambiguation
In this section we informally describe a set
of heuristics for assigning an interpretation to
bare sluices. In subsection 4.2, we show how
our heuristics can be formalised as probabilistic
sluice typing constraints.
4.1 Description of the heuristics
To maximise accuracy we have restricted our-
selves to cases of three-way agreement among
the three coders when considering the distri-
bution patterns from which we obtained our
heuristics. Looking at these patters we have
arrived at the following general principles for
resolving bare sluice types.
What The most likely interpretation is
clarification. This seems to be the case
when the antecedent utterance is a fragment, or
when there is no linguistic antecedent. Reprise
interpretations also provide a significant propor-
tion (about 23%). If there is a pronoun (match-
ing the appropriate semantic constraints) in the
antecedent utterance, then the preferred inter-
pretation is reprise:
(9) Andy: I don?t know how to do it.
Nick: What? Garlic bread? [KPR, 1763]
Why The interpretation of why sluices tends
to be direct. However, if the antecedent is a
non-declarative utterance, or a negative declar-
ative, the sluice is likely to be a reprise.
(10) Vicki: Were you buying this erm newspaper
last week by any chance?
Frederick: Why? [KC3, 3388]
Who Sluices of this form show a very strong
preference for reprise interpretation. In the
majority of cases, the antecedent is either a
proper name (11), or a personal pronoun.
6Another factor is the existence of default strategies
for resolving such parameters, e.g. assuming that the
question asked transparently expresses the querier?s pri-
mary goal.
(11) Patrick: [...] then I realised that it was Fennite
Katherine: Who? [KCV, 4694]
Which/Which N Both sorts of sluices ex-
hibit a strong tendency to reprise. In the
overwhelming majority of reprise cases for both
which and which N, the antecedent is a definite
description like ?the button? in (12).
(12) Arthur: You press the button.
June: Which one? [KSS, 144]
Where The most likely interpretation of
where sluices is reprise. In about 70% of
the reprise cases, the antecedent of the sluice
is a deictic locative pronoun like ?there? or
?here?. Direct interpretations are preferred
when the antecedent utterance is declarative
with no overt spatial location expression.
(13) Pat: You may find something in there actually.
Carole: Where? [KBH, 1817]
When If the antecedent utterance is a declar-
ative and there is no time-denoting expression
other than tense, the sluice will be interpreted
as direct, as in example (14). On the other
hand, deictic temporal expressions like ?then?
trigger reprise interpretations.
(14) Caroline: I?m leaving this school.
Lyne: When? [KP3, 538]
How This class of sluice exhibits a very strong
tendency to direct (87%). It appears that
most of the antecedent utterances contain an
accomplishment verb.
(15) Anthony: I?ve lost the, the whole work itself
Arthur: How? [KP1, 631]
4.2 Probabilistic Constraints
The problem we are addressing is typing of bare
sluice tokens in dialogue. This problem is anal-
ogous to part-of-speech tagging, or to dialogue
act classification.
We formulate our typing constraints as Horn
clauses to achieve the most general and declar-
ative expression of these conditions. The an-
tecedent of a constraint uses predicates corre-
sponding to dialogue relations, syntactic prop-
erties, and lexical content. The predicate of the
consequent represents a sluice typing tag, which
corresponds to a maximal type in the HPSG
grammar that we used in implementing our di-
alogue system. Note that these constraints can-
not be formulated at the level of the lexical en-
tries of the wh-words since these distributions
are specific to sluicing and not to non-elliptical
wh-interrogatives.7 As a first example, consider
the following rule:
sluice(x), where(x),
ant utt(y,x),
contains(y,?there?) ? reprise(x) [.78]
This rule states that if x is a sluice construction
with lexical head where, and its antecedent ut-
terance (identified with the latest move in the
dialogue) contains the word ?there?, then x is a
reprise sluice. Note that, as in a probabilistic
context-free grammar (Booth, 1969), the rule is
assigned a conditional probability. In the exam-
ple above, .78 is the probability that the context
described in the antecedent of the clause pro-
duces the interpretation specified in the conse-
quent.8
The following three rules are concerned with
the disambiguation of why sluice readings. The
structure of the rules is the same as before. In
this case however, the disambiguation is based
on syntactic and semantic properties of the an-
tecedent utterance as a whole (like polarity or
mood), instead of focusing on a particular lexi-
cal item contained in such utterance.
sluice(x), why(x),
ant utt(y,x), non decl(y) ? reprise(x) [.93]
sluice(x), why(x),
ant utt(y,x), pos decl(y) ? direct(x) [.95]
sluice(x), why(x),
ant utt(y,x), neg decl(y) ? reprise(x) [.40]
5 Applying Machine Learning
To evaluate our heuristics, we applied machine
learning techniques to our corpus data. Our
aim was to evaluate the predictive power of the
features observed and to test whether the intu-
itive constraints formulated in the form of Horn
clause rules could be learnt automatically from
these features.
5.1 SLIPPER
We use a rule-based learning algorithm called
SLIPPER (for Simple Learner with Iterative
Pruning to Produce Error Reduction). SLIP-
PER (Cohen and Singer, 1999) combines the
7Thus, whereas Table 2 shows that approx. 70% of
who-sluices are reprise, this is clearly not the case
for non-elliptical who?interrogatives. For instance, the
KB7 block in the BNC has 33 non-elliptical who?
interrogatives. Of these at most 3 serve as reprise ut-
terances.
8These probabilities have been extracted manually
from the three-way agreement data.
separate-and-conquer approach used by most
rule learners with confidence-rated boosting to
create a compact rule set.
The output of SLIPPER is a weighted rule
set, in which each rule is associated with a con-
fidence level. The rule builder is used to find
a rule set that separates each class from the re-
maining classes using growing and pruning tech-
niques. To classify an instance x, one computes
the sum of the confidences that cover x: if the
sum is greater than zero, the positive class is
predicted. For each class, the only rule with
a negative confidence rating is a single default
rule, which predicts membership in the remain-
ing classes.
We decided to use SLIPPER for two main
reasons: (1) it generates transparent, relatively
compact rule sets that can provide interesting
insights into the data, and (2) its if-then rules
closely resemble our Horn clause constraints.
5.2 Experimental Setup
To generate the input data we took all three-
way agreement instances plus those instances
where there is agreement between coder 1 and
coder 2, leaving out cases classified as unclear.
We reclassified 9 instances in the first sample as
wh-anaphor, and also included these data.9 The
total data set includes 351 datapoints. These
were annotated according to the set of features
shown in Table 4.
sluice type of sluice
mood mood of the antecedent utterance
polarity polarity of the antecedent utterance
frag whether the antecedent utterance is
a fragment
quant presence of a quantified expression
deictic presence of a deictic pronoun
proper n presence of a proper name
pro presence of a pronoun
def desc presence of a definite description
wh presence of a wh word
overt presence of any other potential
antecedent expression
Table 4: Features
We use a total of 11 features. All features are
nominal. Except for the sluice feature that in-
dicates the sluice type, they are all boolean, i.e.
they can take as value either yes or no. The
features mood, polarity and frag refer to syn-
tactic and semantic properties of the antecedent
9We reclassified those instances that had motivated
the introduction of the wh-anaphor category for the sec-
ond sample. Given that there were no disagreements in-
volving this category, such reclassification was straight-
forward.
utterance as a whole. The remaining features,
on the other hand, focus on a particular lexical
item or construction contained in such utter-
ance. They will take yes as a value if this ele-
ment or construction exists and, it matches the
semantic restrictions imposed by the sluice type.
The feature wh will take a yes value only if there
is a wh-word that is identical to the sluice type.
Unknown or irrelevant values are indicated by
a question mark. This allows us to express, for
instance, that the presence of a proper name is
irrelevant to determine the interpretation of a
where sluice, while it is crucial when the sluice
type is who. The feature overt takes no as value
when there is no overt antecedent expression. It
takes yes when there is an antecedent expres-
sion not captured by any other feature, and it
is considered irrelevant (question mark value)
when there is an antecedent expression defined
by another feature.
5.3 Accuracy Results
We performed a 10-fold cross-validation on the
total data set, obtaining an average success rate
of 90.32%. Using leave-one-out cross-validation
we obtained an average success rate of 84.05%.
For the holdout method, we held over 100 in-
stances as a testing data, and used the reminder
(251 datapoints) for training. This yielded
a success rate of 90%. Recall, precision and
f-measure values are reported in Table 5.
category recall precision f-measure
direct 96.67 85.29 90.62
reprise 88.89 94.12 91.43
clarification 83.33 71.44 76.92
wh anaphor 80.00 100 88.89
Table 5: SLIPPER - Results
Using the holdout procedure, SLIPPER gen-
erated a set of 23 rules: 4 for direct, 13
for reprise, 1 for clarification and 1 for
wh-anaphor, plus 4 default rules, one for each
class. All features are used except for frag,
which indicates that this feature does not play a
significant role in determining the correct read-
ing. The following rules are part of the rule set
generated by SLIPPER:
direct not reprise|clarification|wh anaphor :-
overt=no, polarity=pos (+1.06296)
reprise not direct|clarification|wh anaphor :-
deictic=yes (+3.31703)
reprise not direct|clarification|wh anaphor :-
mood=non decl, sluice=why (+1.66429)
5.4 Comparing SLIPPER and TiMBL
Although SLIPPER seems to be especially well
suited for the task at hand, we decided to run a
different learning algorithm on the same train-
ing and testing data sets and compare the re-
sults obtained. For this experiment we used
TiMBL, a memory-based learning algorithm de-
veloped at Tilburg University (Daelemans et
al., 2003). As with all memory-based machine
learners, TiMBL stores representations of in-
stances from the training set explicitly in mem-
ory. In the prediction phase, the similarity be-
tween a new test instance and all examples in
memory is computed using some distance met-
ric. The system will assign the most frequent
category within the set of most similar exam-
ples (the k-nearest neighbours). As a distance
metric we used information-gain feature weight-
ing, which weights each feature according to the
amount of information it contributes to the cor-
rect class label.
The results obtained are very similar to the
previous ones. TiMBL yields a success rate of
89%. Recall, precision and f-measure values are
shown in Table 6. As expected, the feature that
received a lowest weighting was frag.
category recall precision f-measure
direct 86.60 86.60 86.6
reprise 88.89 90.50 89.68
clarification 83.33 71.44 76.92
wh anaphor 100 100 100
Table 6: TiMBL -Results
6 Conclusion and Further Work
In this paper we have presented a machine
learning approach to bare sluice classification
in dialogue using corpus-based empirical data.
From these data, we have extracted a set of
heuristic principles for sluice disambiguation
and formulated such principles as probability
weighted Horn clauses. We have then used
the predicates of these clauses as features to
annotate an input dataset, and ran two dif-
ferent machine learning algorithms: SLIPPER,
a rule-based learning algorithm, and TiMBL,
a memory-based learning system. SLIPPER
has the advantage of generating transparent
rules that closely resemble our Horn clause con-
straints. Both algorithms, however, perform
well, yielding to similar success rates of approx-
imately 90%. This shows that the features we
used to formulate our heuristic principles were
well motivated, except perhaps for the feature
frag, which does not seem to have a signifi-
cant predictive power. The two algorithms we
used seem to be well suited to the task of sluice
classification in dialogue on the basis of these
features.
In the future we will attempt to construct
an automatic procedure for annotating a dia-
logue corpus with the features presented here,
to which both machine learning algorithms ap-
ply.
References
T. Booth. 1969. Probabilistic representation
of formal languages. In IEEE Conference
Record of the 1969 Tenth Annual Symposium
of Switching and Automata Theory.
J. Carletta. 1996. Assessing agreement on clas-
sification tasks: the kappa statistics. Compu-
tational Linguistics, 2(22):249?255.
S. Chung, W. Ladusaw, and J. McCloskey.
1995. Sluicing and logical form. Natural Lan-
guage Semantics, 3:239?282.
H. H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
W. Cohen and Y. Singer. 1999. A simple, fast,
and effective rule learner. In Proc. of the 16th
National Conference on AI.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2003. TiMBL: Tilburg
Memory Based Learner, Reference Guide.
Technical Report ILK-0310, U. of Tilburg.
R. Ferna?ndez, J. Ginzburg, H. Gregory, and
S. Lappin. (to appear). SHARDS: Frag-
ment resolution in dialogue. In H. Bunt and
R. Muskens, editors, Computing Meaning,
volume 3. Kluwer.
J. Ginzburg and I. Sag. 2001. Interrogative
Investigations. CSLI Publications, Stanford,
California.
A. Kilgarriff. 1998. BNC Database and
Word Frequency Lists. www.itri.bton.ac.uk/
?Adam.Kilgarriff/ bnc-readme.html.
M. Purver. 2001. SCoRE: A tool for searching
the BNC. Technical Report TR-01-07, Dept.
of Computer Science, King?s College London.
M. Purver. 2004. The Theory and Use of Clari-
fication in Dialogue. Ph.D. thesis, King?s Col-
lege, London, forthcoming.
J. Ross. 1969. Guess who. In Proc. of the 5th
annual Meeting of the Chicago Linguistics So-
ciety, pages 252?286, Chicago. CLS.
D. Traum. 1994. A Computational Theory of
Grounding in Natural Language Conversa-
tion. Ph.D. thesis, University of Rochester,
Department of Computer Science, Rochester.
Introduction to the Special Issue on 
Computational Anaphora Resolution 
Ruslan Mitkov* 
University of Wolverhampton 
Shalom Lappin* 
King's College, London 
Branimir Boguraev* 
IBM T. J. Watson Research Center 
Anaphora accounts for cohesion in texts and is a phenomenon under active study 
in formal and computational linguistics alike. The correct interpretation of anaphora 
is vital for natural anguage processing (NLP). For example, anaphora resolution is 
a key task in natural anguage interfaces, machine translation, text summarization, 
information extraction, question answering, and a number of other NLP applications. 
After considerable initial research, followed by years of relative silence in the early 
1980s, anaphora resolution has attracted the attention of many researchers in the last 10 
years and a great deal of successful work on the topic has been carried out. Discourse- 
oriented theories and formalisms uch as Discourse Representation Theory and Cen- 
tering Theory inspired new research on the computational treatment of anaphora. The 
drive toward corpus-based robust NLP solutions further stimulated interest in alterna- 
tive and/or data-enriched approaches. Last, but not least, application-driven research 
in areas uch as automatic abstracting and information extraction i dependently high- 
lighted the importance of anaphora nd coreference r solution, boosting research in 
this area. 
Much of the earlier work in anaphora resolution heavily exploited omain and lin- 
guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and 
Brown 1988), which was difficult both to represent and to process, and which required 
considerable human input. However, the pressing need for the development of robust 
and inexpensive solutions to meet the demands of practical NLP systems encouraged 
many researchers tomove away from extensive domain and linguistic knowledge and 
to embark instead upon knowledge-poor anaphora resolution strategies. A number of 
proposals in the 1990s deliberately imited the extent o which they relied on domain 
and/or linguistic knowledge and reported promising results in knowledge-poor per- 
ational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 
1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; 
Mitkov 1996, 1998b). 
The drive toward knowledge-poor and robust approaches was further motivated 
by the emergence of cheaper and more reliable corpus-based NLP tools such as part- 
of-speech taggers and shallow parsers, alongside the increasing availability of corpora 
and other NLP resources (e.g., ontologies). In fact, the availability of corpora, both raw 
and annotated with coreferential links, provided a strong impetus to anaphora resolu- 
* School of Humanities, Language and Social Sciences, Stafford Street, Wolverhampton WV1 1SB, UK. 
E-maih r.mitkov@wlv.ac.uk 
t 30 Saw Mill River Road, Hawthorne, NY 10532, USA. E-mail: bkb@watson.ibm.com 
~: Department of Computer Science, King's College, The Strand, London WC2R 2LS, UK. 
E-mail: lappin@dcs.kcl.ac.uk 
@ 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 4 
tion with regard to both training and evaluation. Corpora (especially when annotated) 
are an invaluable source not only for empirical research but also for automated learning 
(e.g., machine learning) methods aiming to develop new rules and approaches; they 
also provide an important resource for evaluation of the implemented approaches. 
From simple co-occurrence rules (Dagan and Itai 1990) through training decision trees 
to identify anaphor-antecedent pairs (Aone and Bennett 1995) to genetic algorithms to 
optimize the resolution factors (Or~san, Evans, and Mitkov 2000), the successful per- 
formance of more and more modern approaches was made possible by the availability 
of suitable corpora. 
While the shift toward knowledge-poor strategies and the use of corpora repre- 
sented the main trends of anaphora resolution in the 1990s, there are other signifi- 
cant highlights in recent anaphora resolution research. The inclusion of the corefer- 
ence task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and 
MUC-7) gave a considerable impetus to the development of coreference resolution 
algorithms and systems, such as those described in Baldwin et al (1995), Gaizauskas 
and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century 
saw a number of anaphora resolution projects for languages other than English such as 
French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background 
of a growing interest in multilingual NLP, multilingual anaphora/coreference reso- 
lution has gained considerable momentum in recent years (Aone and McKee 1993; 
Azzam, Humphreys, and Gaizauskas 1998; Harabagiu and Maiorano 2000; Mitkov 
and Barbu 2000; Mitkov 1999; Mitkov and Stys 1997; Mitkov, Belguith, and Stys 1998). 
Other milestones of recent research include the deployment of probabilistic and ma- 
chine learning techniques (Aone and Bennett 1995; Kehler 1997; Ge, Hale, and Char- 
niak 1998; Cardie and Wagstaff 1999; the continuing interest in centering, used either 
in original or in revised form (Abra~os and Lopes 1994; Strube and Hahn 1996; Hahn 
and Strube 1997; Tetreault 1999); and proposals related to the evaluation methodology 
in anaphora resolution (Mitkov 1998a, 2001b). For a more detailed survey of the state 
of the art in anaphora resolution, see Mitkov (forthcoming). 
The papers published in this issue reflect he major trends in anaphora resolution 
in recent years. Some of them describe approaches that do not exploit full syntactic 
knowledge (as in the case of Palomar et al's and Stuckardt's work) or that employ 
machine learning techniques (Soon, Ng, and Lira); others present centering-based pro- 
noun resolution (Tetreault) or discuss theoretical centering issues (Kibble). Almost all 
of the papers feature extensive valuation (including comparative valuation as in 
the case of Tetreault's and Palomar et al's work) or discuss general evaluation issues 
(Byron as well as Stuckardt). 
Palomar et al's paper describes an approach that works from the output of a 
partial parser and handles third person personal, demonstrative, reflexive, and zero 
pronouns, featuring among other things syntactic onditions on Spanish NP-pronoun 
noncoreference and an enhanced set of resolution preferences. The authors also im- 
plement several known methods and compare their performance with that of their 
own algorithm. An indirect conclusion from this work is that an algorithm requires 
semantic knowledge in order to hope for a success rate higher than 75%. 
Soon, Ng, and Lira describe a C5-based learning approach to coreference resolu- 
tion of noun phrases in unrestricted text. The approach learns from a small, annotated 
corpus and tackles pronouns, proper names, and definite descriptions. The coreference 
resolution module is part of a larger coreference resolution system that also includes 
sentence segmentation, tokenization, morphological analysis, part-of-speech tagging, 
noun phrase identification, named entity recognition, and semantic lass determina- 
tion (via WordNet). The evaluation is carried out on the MUC-6 and MUC-7 test 
474 
Mitkov, Boguraev, and Lappin Anaphora Resolution: Introduction 
corpora. The paper reports on experiments aimed at quantifying the contribution of 
each resolution factor and features error analysis. 
Stuckardt's work presents an anaphor esolution algorithm for systems where only 
partial syntactic information is available. Stuckardt applies Government and Bind- 
ing Theory principles A, B, and C to the task of coreference resolution on partially 
parsed texts. He also argues that evaluation of anaphora resolution systems hould 
take into account several factors beyond simple accuracy of resolution. In particular, 
both developer-oriented (e.g., related to the selection of optimal resolution factors) 
and application-oriented (e.g., related to the requirement of the application, as in the 
case of information extraction, where a proper name antecedent is needed) evaluation 
metrics should be considered. 
Tetreault's contribution features comparative valuation involving the author's 
own centering-based pronoun resolution algorithm called the Left-Right Centering 
algorithm (LRC) as well as three other pronoun resolution methods: Hobbs's naive 
algorithm (Hobbs 1978), BFP (Brennan, Friedman, and Pollard 1987), and Strube's S- 
list approach (Strube 1998). The LRC is an alternative to the original BFP algorithm in 
that it processes utterances incrementally. It works by first searching for an antecedent 
in the current sentence; if none can be found, it continues the search on the Cf-list of 
the previous and the other preceding utterances in a left-to-right fashion. 
In her squib, Byron maintains that additional kinds of information should be 
included in an evaluation in order to make the performance of algorithms on pronoun 
resolution more transparent. In particular, she suggests that the pronoun coverage be 
explicitly reported and proposes that the evaluation details be presented in a concise 
and compact tabular format called standard isclosure. Byron also proposes ameasure, 
the resolution rate, which is computed as the number of pronouns resolved correctly 
divided by the number of (only) referential pronouns. 
Finally, in his squib Kibble discusses a reformulation of the centering transitions 
(Continue, Retain, and Shift), which specify the center movement across sentences. 
Instead of defining a total preference ordering, Kibble argues that a partial ordering 
emerges from the interaction among cohesion (maintaining the same center), salience 
(realizing the center as subject), and cheapness (realizing the anticipated center of a 
following utterance as subject). 
The last years have seen considerable advances in the field of anaphora resolution, 
but a number of outstanding issues either remain unsolved or need more attention 
and, as a consequence, represent major challenges to the further development of the 
field (Mitkov 2001a). A fundamental question that needs further investigation is how 
far the performance of anaphora resolution algorithms can go and what the limitations 
of knowledge-poor methods are. In particular, more research should be carried out on 
the factors influencing the performance of these algorithms. One of the impediments 
to the evaluation or fuller utilization of machine learning techniques is the lack of 
widely available corpora annotated for anaphoric or coreferential links. More work 
toward the proposal of consistent and comprehensive evaluation is necessary; so too 
is work in multilingual contexts. Some of these challenges have been addressed in the 
papers published in this issue, but ongoing research will continue to address them in 
the near future. 
References 
Abra~os, Jose and Jos6 Lopes. 1994. 
Extending DRT with a focusing 
mechanism for pronominal anaphora nd 
ellipsis resolution. In Proceedings ofthe 15th 
International Conference on Computational 
Linguistics (COLING'94), pages 1128-1132, 
Kyoto, Japan. 
Aone, Chinatsu and Scott Bennett. 1995. 
Evaluating automated and manual 
475 
Computational Linguistics Volume 27, Number 4 
acquisition of anaphora resolution 
strategies. In Proceedings ofthe 33rd Annual 
Meeting of the Association for Computational 
Linguistics (ACU95), pages 122-129, Las 
Cruces, NM. 
Aone, Chinatsu and Douglas McKee. 1993. 
A language-independent anaphora 
resolution system for understanding 
multilingual texts. In Proceedings ofthe 31st 
Annual Meeting of the Association for 
Computational Linguistics (ACU93), 
pages 156-163, Columbus, OH. 
Azzam, Saliha, Kevin Humphreys, and 
Robert Gaizauskas. 1998. Coreference 
resolution in a multilingual information 
extraction. In Proceedings ofa Workshop on 
Linguistic Coreference, Granada, Spain. 
Baldwin, Breck. 1997. CogNIAC: High 
precision coreference with limited 
knowledge and linguistic resources. In 
Proceedings ofthe ACU97/EACU97 
Workshop on Operational Factors in Practical, 
Robust Anaphora Resolution for Unrestricted 
Texts, pages 38-45, Madrid, Spain. 
Baldwin, Breck, Jeff Reynar, Mike Collins, 
Jason Eisner, Adwait Ratnaparki, Joseph 
Rosenzweig, Anoop Sarkar, and Srivinas 
Bangalore. 1995. Description of the 
University of Pennsylvania system used 
for MUC-6. In Proceedings ofthe Sixth 
Message Understanding Conference 
(MUC-6), pages 177-191, Columbia, MD. 
Brennan, Susan, Marilyn Friedman, and 
Carl Pollard. 1987. A centering approach 
to pronouns. In Proceedings ofthe 25th 
Annual Meeting of the Association for 
Computational Linguistics (ACU87), 
pages 155-162, Stanford, CA. 
Carbonell, Jaime and Ralf Brown. 1988. 
Anaphora resolution: A multi-strategy 
approach. In Proceedings ofthe 12th 
International Conference on Computational 
Linguistics (COLING'88), volume 1, 
pages 96-101, Budapest, 
Hungary. 
Cardie, Claire and Kiri Wagstaff. 1999. 
Noun phrase coreference asclustering. In 
Proceedings ofthe 1999 Joint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora, 
pages 82-89, College Park, MD. 
Carter, David M. 1987. Interpreting Anaphors 
in Natural Language Texts. Ellis Horwood, 
Chichester, UK. 
Dagan, Ido and Alon Itai. 1990. Automatic 
processing of large corpora for the 
resolution of anaphora references. In
Proceedings ofthe 13th International 
Conference on Computational Linguistics 
(COLING'90), volume 3, pages 1-3, 
Helsinki, Finland. 
Dagan, Ido and Alon Itai. 1991. A statistical 
filter for resolving pronoun references. In
Yishai A. Feldman and Alfred Bruckstein, 
editors, Artifi'cial Intelligence and Computer 
Vision. Elsevier Science Publishers B.V. 
(North-Holland), Amsterdam, pages 
125-135. 
Gaizauskas, Robert and Kevin Humphreys. 
1996. Quantitative evaluation of 
coreference algorithms in an information 
extraction system. Presented at Discourse 
Anaphora nd Anaphor Resolution 
Colloquium (DAARC-1), Lancaster, UK. 
Reprinted in Simon Botley and Tony 
McEnery, editors, Corpus-Based and 
Computational Approaches to Discourse 
Anaphora. John Benjamins, Amsterdam, 
2000, pages 143-167. 
Ge, Niyu, John Hale, and Eugene Charniak. 
1998. A statistical approach to anaphora 
resolution. In Proceedings ofthe Sixth 
Workshop on Very Large Corpora, 
pages 161-170, Montreal, Canada. 
Hahn, Udo and Michael Strube. 1997. 
Centering-in-the-large: Computing 
referential discourse segments. In 
Proceedings ofthe 35th Annual Meeting of the 
Association for Computational Linguistics 
(ACU97/EACU97), pages 104-111, 
Madrid, Spain. 
Harabagiu, Sanda and Steven Maiorano. 
2000. Multilingual coreference r solution. 
In Proceedings ofConference on Applied 
Natural Language Processing~North American 
Chapter of the Association for Computational 
Linguistics (ANLP-NAACL2000), pages 
142-149, Seattle, WA. 
Hobbs, Jerry. 1978. Resolving pronoun 
references. Lingua, 44:311-338. 
Kameyama, Megumi. 1997. Recognizing 
referential links: An information 
extraction perspective. In Proceedings ofthe 
ACU97/EACL'97 Workshop on Operational 
Factors in Practical, Robust Anaphora 
Resolution for Unrestricted Texts, 
pages 46-53, Madrid, Spain. 
Kehler, Andrew. 1997. Probabilistic 
coreference in information extraction. In 
Proceedings ofthe 2nd Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-2), pages 163-173, 
Providence, RI. 
Kennedy, Christopher and Branimir 
Boguraev. 1996. Anaphora for everyone: 
Pronominal anaphora resolution without 
a parser. In Proceedings ofthe 16th 
International Conference on Computational 
Linguistics (COLING'96), pages 113-118, 
Copenhagen, Denmark. 
Lappin, Shalom and Herbert Leass. 1994. 
An algorithm for pronominal anaphora 
476 
Mitkov, Boguraev, and Lappin Anaphora Resolution: Introduction 
resolution. Computational Linguistics, 
20(4):535-561. 
Mitkov, Ruslan. 1996. Pronoun resolution: 
The practical alternative. Presented at the 
Discourse Anaphora nd Anaphor 
Resolution Colloquium (DAARC-1), 
Lancaster, UK. Reprinted in Simon Botley 
and Tony McEnery, editors, Corpus-Based 
and Computational Approaches to Discourse 
Anaphora. John Benjamins, Amsterdam, 
2000, 189-212. 
Mitkov, Ruslan. 1998a. Evaluating anaphora 
resolution approaches. In Proceedings ofthe 
Discourse Anaphora nd Anaphora Resolution 
Colloquium (DAARC-2), Lancaster, UK. 
Mitkov, Ruslan. 1998b. Robust pronoun 
resolution with limited knowledge. In 
Proceedings ofthe 36th Annual Meeting of the 
Association for Computational Linguistics and 
the 17th International Conference on 
Computational Linguistics 
(COLING'98/ACU98), pages 869-875, 
Montreal, Canada. 
Mitkov, Ruslan. 1999. Multilingual anaphora 
resolution. Machine Translation, 
14(3-4):281-299. 
Mitkov, Ruslan. 2001a. Outstanding issues 
in anaphora resolution. In Alexander 
Gelbukh, editor, Computational Linguistics 
and Intelligent Text Processing. Springer, 
Berlin, pages 110-125. 
Mitkov, Ruslan. 2001b. Towards a more 
consistent and comprehensive evaluation 
of anaphora resolution algorithms and 
systems. Applied Artificial Intelligence: An 
International Journal, 15:253-276. 
Mitkov, Ruslan. Forthcoming. Anaphora 
Resolution. Longman, Harlow, UK. 
Mitkov, Ruslan, Lamia Belguith, and 
Malgorzata Stys. 1998. Multilingual robust 
anaphora resolution. In Proceedings ofthe 
Third International Conference on Empirical 
Methods in Natural Language Processing 
(EMNLP-3), pages 7-16, Granada, Spain. 
Mitkov, Ruslan and Malgorzata Stys. 1997. 
Robust reference resolution with limited 
knowledge: High precision genre-specific 
approach for English and Polish. In 
Proceedings ofthe International Conference on 
Recent Advances in Natural Language 
Processing (RANLP'97), pages 74-81, 
Tzigov Chark, Bulgaria. 
Mitkov, Ruslan and Catalina Barbu. 2000. 
Improving pronoun resolution in two 
languages by means of bilingual corpora. 
In Proceedings ofthe Discourse, Anaphora nd 
Reference Resolution Conference (DAARC 
2000), pages 133-137, Lancaster, UK. 
Nasukawa, Tetsuya. 1994. Robust method of 
pronoun resolution using full-text 
information. In Proceedings ofthe 15th 
International Conference on Computational 
Linguistics (COLING'94), pages 1157-1163, 
Kyoto, Japan. 
Or~san, Constantin, Richard Evans, and 
Ruslan Mitkov. 2000. Enhancing 
preference-based anaphora resolution 
with genetic algorithms. In Proceedings of
NLP-2000, pages 185-195, Patras, Greece. 
Rich, Elaine and Susann LuperFoy. 1988. An 
architecture for anaphora resolution. In 
Proceedings ofthe Second Conference on 
Applied Natural Language Processing 
(ANLP-2), pages 18-24, Austin, TX. 
Sidner, Candace. 1979. Toward a 
computational theory of definite anaphora 
comprehension in English. Technical 
Report AI-TR-537, MIT, Cambridge, MA. 
Strube, Michael. 1998. Never look back: An 
alternative to centering. In Proceedings of
the 36th Annual Meeting of the Association for 
Computational Linguistics and the 17th 
International Conference on Computational 
Linguistics (COLING'98/ACL'98), 
pages 1251-1257, Montreal, Canada. 
Strube, Michael and Udo Hahn. 1996. 
Functional centering. In Proceedings ofthe 
34th Annual Meeting of the Association for 
Computational Linguistics (ACL'96), 
pages 270-277, Santa Cruz, CA. 
Tetreault, Joel. 1999. Analysis of 
syntax-based pronoun resolution 
methods. In Proceedings ofthe 37th Annual 
Meeting of the Association for Computational 
Linguistics (ACL'99), pages 602-605, 
College Park, MD. 
Williams, Sandra, Mark Harvey, and Keith 
Preston. 1996. Rule-based reference 
resolution for unrestricted text using 
part-of-speech tagging and noun phrase 
parsing. In Proceedings ofthe Discourse 
Anaphora nd Anaphora Resolution 
Colloquium (DAARC-1), pages 441-456, 
Lancaster, UK. 
477 

 	
		Proceedings of the EACL 2009 Workshop on Cognitive Aspects of Computational Language Acquisition, pages 26?33,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Another look at indirect negative evidence
Alexander Clark
Department of Computer Science
Royal Holloway, University of London
alexc@cs.rhul.ac.uk
Shalom Lappin
Department of Philosophy
King?s College, London
shalom.lappin@kcl.ac.uk
Abstract
Indirect negative evidence is clearly an im-
portant way for learners to constrain over-
generalisation, and yet a good learning
theoretic analysis has yet to be provided
for this, whether in a PAC or a proba-
bilistic identification in the limit frame-
work. In this paper we suggest a theoreti-
cal analysis of indirect negative evidence
that allows the presence of ungrammati-
cal strings in the input and also accounts
for the relationship between grammatical-
ity/acceptability and probability. Given
independently justified assumptions about
lower bounds on the probabilities of gram-
matical strings, we establish that a limited
number of membership queries of some
strings can be probabilistically simulated.
1 Introduction
First language acquisition has been studied for a
long time from a theoretical point of view, (Gold,
1967; Niyogi and Berwick, 2000), but a consen-
sus has not emerged as to the most appropriate
model for learnability. The two main competing
candidates, Gold-style identification in the limit
and PAC-learning both have significant flaws.
For most NLP researchers, these issues are sim-
ply not problems: for all empirical purposes, one
is interested in modelling the distribution of exam-
ples or the conditional distribution of labels given
examples and the obvious solution ? an  ? ?
bound on some suitable loss function such as the
Kullback-Leibler Divergence ? is sufficient (Horn-
ing, 1969; Angluin, 1988a). There may be some
complexity issues involved with computing these
approximations, but there is no debate about the
appropriateness of the learning paradigm.
However, such an approach is unappealing to
linguists for a number of reasons: it fails to draw
a distinction between grammatical and ungram-
matical sentences, and for many linguists the key
data are not the ?performance? data but rather the
?voice of competence? as expressed in grammat-
icality and acceptability judgments. Many of the
most interesting sentences for syntacticians are
comparatively rare and unusual and may occur
with negligible frequency in the data.
We do not want to get into this debate here: in
this paper, we will assume that there is a categori-
cal distinction between grammatical and ungram-
matical sentences. See (Schu?tze, 1996) for exten-
sive discussion.
Within this view learnability is technically quite
difficult to formalise in a realistic way. Children
clearly are provided with examples of the lan-
guage ? so-called positive data ? but the status
of examples not in the language ? negative data
? is one of the endless and rather circular de-
bates in the language acquisition literature (Mar-
cus, 1993). Here we do not look at the role of
corrections and other forms of negative data but
we focus on what has been called indirect nega-
tive evidence (INE). INE is the non-occurrence of
data in the primary linguistic data; informally, if
the child does not hear certain ungrammatical sen-
tences, then by their absence the child can infer
that those strings are ungrammatical.
Indirect negative evidence has long been recog-
nised as an important source of information
(Pinker, 1979). However it has been surpris-
ingly difficult to find an explicit learning theo-
retic account of INE. Indeed, in both the PAC
and IIL paradigms it can be shown, that under
the standard assumptions, INE cannot help the
learner. Thus in many of these models, there
is a sharp and implausible distinction between
learning paradigms where the learner is provided
systematically with every negative example, and
those where the learner is denied any negative ev-
idence at all. Neither of these is very realistic.
In this paper, we suggest a resolution for this
conflict, by re-examining the standard learnability
assumptions. We make three uncontroversial ob-
26
servations: first that the examples the child is pro-
vided with are unlabelled, secondly that there are
a small proportion of ungrammatical sentences in
the input to the child, and thirdly that in spite of
this, the child does in fact learn.
We then draw a careful distinction between
probability and grammaticality and propose a re-
striction on the class of distributions allowed to
take account of the fact that children are exposed
to some ungrammatical utterances. We call this
the Disjoint Distribution Assumption: the assump-
tion that the classes of distributions for different
languages must be disjoint. Based on this assump-
tion, we argue that the learner can infer lower
bounds on the probabilities of grammatical strings,
and that using these lower bounds allow a prob-
abilistic approximation to membership queries of
some strings.
On this basis we conclude that the learner does
have some limited access to indirect negative evi-
dence, and we discuss some of the limitations on
this data and the implications for learnability.
2 Background
The most linguistically influential learnability
paradigm is undoubtedly that of Gold (Gold,
1967). In this paradigm the learner is required to
converge to exactly the right answer after a finite
time. In one variant of the paradigm the learner
is provided with only positive examples, and must
learn on every presentation of the language. Un-
der this paradigm no suprafinite class of languages
is learnable. If alternatively the learner is pro-
vided with a presentation of labelled examples,
then pretty much anything is learnable, but clearly
this paradigm has little relevance to the course of
language acquisition.
The major problem with the Gold positive data
paradigm is that the learner is required to learn
under every presentation; given the minimal con-
straints on what counts as a presentation, this re-
sults in a model which is unrealistically hard. In
particular, it is difficult for the learner to recover
from an overly general hypothesis; since it is has
only positive examples, such a hypothesis will
never be directly contradicted.
Indirect negative evidence is the claim that the
absence of sentences in the PLD can allow a
learner to infer that those sentences are ungram-
matical. As (Chomsky, 1981, p. 9) says:
A not unreasonable acquisition sys-
tem can be devised with the opera-
tive principle that if certain structures
or rules fail to be exemplified in rel-
atively simple expressions, where they
would expect to be found, then a (pos-
sibly marked) option is selected exclud-
ing them in the grammar, so that a kind
of ?negative evidence? can be available
even without corrections, adverse reac-
tions etc.
While this informal argument has been widely
accepted, and is often appealed to, it has so far not
been incorporated explicitly into a formal model
of learnability. Thus there are no learning mod-
els that we are aware of where positive learning
results have been achieved using indirect negative
evidence. Instead positive learnability results have
typically used general probabilistic models of con-
vergence without explicitly modelling grammati-
cality.
In what follows we will use the following no-
tation. ? is a finite alphabet, and ?? is the
set of all finite strings over ?. A (formal) lan-
guage L is a subset of ??. A distribution D over
?? is a function pD from ?? to [0, 1] such that?
w??? pD(w) = 1. We will write D(?
?) for the
set of all distributions over ??. The support of a
distribution D is the set of strings with positive
probability supp(D) = {w|pD(w) > 0}.
3 Probabilistic learning
The solution is to recognise the probabilistic na-
ture of how the samples are generated. We can
assume they are generated by some stochastic pro-
cess. On its own this says nothing ? anything can
be modelled by a stochastic process. To get learn-
ability we will need to add some constraints.
Suppose the child has seen thousands of times
sentences of the type ?I am AP?, and ?He is
AP? where AP is an adjective phrase, but he has
never heard anybody say ?He am AP?. Intuitively
it seems reasonable in this case to assume that
the child can infer from this that sentences of the
form?He am AP? are ungrammatical. Now, in the
case of the Gold paradigm, the child can make
no such inference. No matter how many millions
or trillions of times he has heard other examples,
the Gold paradigm does not allow any inference
to be made from frequency. The teacher, or en-
vironment, is an adversary who might be deliber-
ately withholding this data in order to confuse the
27
learner. The learner has to ignore this information.
However, in a more plausible learning environ-
ment, the learner can reason as follows. First, the
number of times that the learner has observed sen-
tences of the form ?He am AP? is zero. From this,
the learner can infer that sentences of this type are
rare: i.e. that they are not very probable. Similarly
from the high frequency of examples of the type ?I
am AP? and so on in the observed data, the learner
can infer that the probability of these sentences is
high.
The second step is that the learner can con-
clude from the difference in probability of these
two similar sets of sentences, that there must be a
difference in grammaticality between ?He am AP?
and ?He is AP?, and thus that sentences of the type
?He am AP? are ungrammatical.
It is important to recognise that the inference
proceeds in two steps:
1. the first is the inference from low frequency
in the observed data to low probability and
2. the second is the inference from compara-
tively low probability to ungrammaticality.
Both of these steps need justification, but if they
are valid, then the learner can extract evidence
about what is not in the language from stochastic
evidence about what is in the language. The first
step will be justified by some obvious and reason-
able probabilistic assumptions about the presenta-
tion of the data; the second step is more subtle and
requires some assumptions about the way the dis-
tribution of examples relates to the language being
learned.
3.1 Stochastic assumptions
The basic assumption we make is that the sam-
ples are being generated randomly in some way;
here we will make the standard assumption that
each sentence is generated independently from
the same fixed distribution, the Independently and
Identically Distributed (IID) assumption. While
this is a very standard assumption in statistics and
probability, it has been criticised as a modelling
assumption for language acquisition (Chater and
Vita?nyi, 2007).
Here we are interested in the acquisition of syn-
tax. We are therefore modelling the dependencies
between words and phrases in sentences, but as-
suming that there are no dependencies between
different sentences in discourse. That is to say, we
assume that the probability that a child hears a par-
ticular sentence does not depend on the previously
occurring sentence. Clearly, there are dependen-
cies between sentences. After questions, come an-
swers; a polar interrogative is likely to be followed
by a ?yes? or a ?no?; topics relate consecutive
sentences semantically, and numerous other fac-
tors cause inter-sentential relationships and regu-
larities of various types. Moreover, acceptability
does depend a great deal on the immediate context.
?Where did who go?? is marginal in most con-
texts; following ?Where did he go?? it is perfectly
acceptable. Additionally, since there are multiple
people generating Child Directed Speech (CDS),
this also introduces dependencies: each person
speaks in a slightly different way; while a rela-
tive is visiting, there will be a higher probability
of certain utterances, and so on. These correspond
to a violation of the ?identically? part of the IID
assumption: the distribution will change in time.
The question is whether it is legitimate to ne-
glect these issues in order to get some mathemat-
ical insight: do these idealising assumptions criti-
cally affect learnability? All of the computational
work that we are aware of makes these assump-
tions, whether in a nativist paradigm, (Niyogi and
Berwick, 2000; Sakas and Fodor, 2001; Yang,
2002) or an empiricist one (Clark and Thollard,
2004). We do need to make some assumptions,
otherwise even learning the class of observed nat-
ural languages would be too hard. The minimal
assumptions if we wish to allow any learnability
under stochastic presentation are that the process
generating the data is stationary and mixing. All
we need is for the law of large numbers to hold,
and for there to be rapid convergence of the ob-
served frequency to the expectation. We can get
this easily with the IID assumption, or with a bit
more work using ergodic theory. Thus in what fol-
lows we will make the IID assumption; effectively
using it as a place-holder for some more realistic
assumption, based on ergodic processes. See for
example (Gamarnik, 2003) for a an extension of
PAC analysis in this direction. The inference from
low frequency to low probability follows from the
minimal assumptions, specifically the IID, which
we are making here.
4 Probability and Grammaticality
We now look at the second step in the probabilistic
inference: how can the child go from low probabil-
28
ity to ungrammaticality? More generally the ques-
tion is what is the relation between probability and
grammaticality. There are lots of factors that affect
probability other than grammaticality: length of
utterance, lexical frequency, semantic factors and
real world factors all can have an impact on prob-
ability.
Low probability on its own cannot imply un-
grammaticality: if there are infinitely many gram-
matical sentences then there cannot be a lower
bound on the probability: if all grammatical
sentences have probability at least  then there
could be at most 1/ grammatical sentences which
would make the language finite. A very long
grammatical sentence can have very low probabil-
ity, lower than a short ungrammatical sentence, so
a less naive approach is necessary: the key point is
that the probability must be comparatively low.
Since we are learning from unlabelled data, the
only information that the child has comes from
from the distribution of examples, and so the dis-
tribution must pick out the language precisely. To
see this more clearly, suppose that the learner had
access to an ?Oracle? that would tell it the true
probability of any string, and has no limit on how
many strings it sees. A learner in this unrealistic
model is clearly more powerful than any learner
that just looks at a finite sample of the data. If this
learner could not learn, then no real learner could
learn on the basis of finite data.
More precisely for any language L we will have
a corresponding set of distributions D(L), and we
require the learner to learn under any of these dis-
tributions. What we require is that if we have two
distinct languages L and L? then the two sets of
distributionsD(L) andD(L?) must be disjoint, i.e.
have no elements in common. If they did have a
distribution D in common, then no learner could
tell the two languages apart as the information be-
ing provided would be identical. Of course, given
two distinct languages L and L?, it is possible that
they intersect, that is to say that there are strings
w in L?L?; a natural language example would be
two related dialects of the same language such as
some dialect of British English and some dialect of
American; though the languages are distinct in for-
mal terms, they are not disjoint, as there are sen-
tences that are grammatical in both. When we con-
sider the sets of distributions that are allowed for
each language D(L) and D(L?), we may find that
there are elements D ? D(L) and D? ? D(L?),
whose supports overlap, or even whose supports
are identical, supp(D) = supp(D?), and we may
well find that there are even some strings whose
probabilities are identical; i.e. there may be a
string w such that pD(w) = pD?(w) > 0. But
what we do not allow is that we have a distribution
D that is an element of both D(L) and D(L?). If
there were such an element, then when the learner
was provided with samples drawn from this dis-
tribution, since the samples are unlabelled, there
is absolutely no way that the learner could work
out whether the target was L or L?; the distribu-
tion would not determine the language. Therefore
there must be a function from distributions to lan-
guages. We cannot have a single distribution that
could be from two different languages. Let?s call
this the disjoint distribution assumption (DDA):
the assumption that the sets of distributions for dis-
tinct languages are disjoint.
Definition 1 The Disjoint Distribution Assump-
tion: If L 6= L? then D(L) ? D(L?) = ?.
This assumption seems uncontroversial; indeed
every proposal for a formal probabilistic model of
language acquisition that we are aware of makes
this assumption implicitly.
Now consider the convergence criterion: we
wish to measure the error with respect to the distri-
bution. There are two error terms, corresponding
to false positives and false negatives. Suppose our
target language is T and our hypothesis is H . De-
fine PD(S) for some set S to be
?
w?S pD(s).
e+ = PD(H \ T ) (1)
e? = PD(T \H) (2)
We will require both of these error terms to con-
verge to zero rapidly, and uniformly, as the amount
of data the learner has increases.
5 Modelling the DDA
If we accept this assumption, then we will require
some constraints on the sets of distributions. There
are a number of ways to model this: the most ba-
sic way is to assume that strings have probability
greater than zero if and only if the string is in the
language. Formally, for all D in D(L)
pD(w) > 0 ? w ? L (3)
Here we clearly have a function from distribu-
tions to languages: we just take the support of the
29
distribution to be the language: for all D in D(L),
supp(D) = L. Under this assumption alone how-
ever, indirect negative evidence will not be avail-
able.
That is because, in this situation, low probabil-
ity does not imply ungrammaticality: only zero
probability implies ungrammaticality. The fact
that we have never seen a sentence in a finite sam-
ple of size n means that we can say that it is likely
to have probability less than about 1/n, but we
cannot say that its probability is likely to be zero.
Thus we can never conclude that a sentence is un-
grammatical, if we make the assumption in Equa-
tion 3, and assume that there are no other limita-
tions on the set of distributions. Since we have
to learn for any distribution, we must learn even
when the distribution is being picked adversari-
ally. Suppose we have never seen an occurrence
of a string; this could be because the probability
has been artificially lowered to some infinitesimal
quantity by the adversary to mislead us. Thus we
gain nothing. Since there is no non-trivial lower
bound on the probability of grammatical strings,
effectively there is no difference between the re-
quirement pD(w) > 0 ? w ? L and the weaker
condition pD(w) > 0 ? w ? L.
But this is not the only possibility: indeed, it is
not a very good model at all. First, the assump-
tion that ungrammatical strings have zero proba-
bility is false. Ungrammatical sentences, that is
strings w, such that w 6? L, do occur in the en-
vironment, albeit with low probability. There are
performance errors, poetry and songs, other chil-
dren with less than adult competence, foreigners
and many other potential sources of ungrammat-
ical sentences. The orthodox view is that CDS
is ?unswervingly well-formed? (Newport et al,
1977): this is a slight exaggeration as a quick look
at CHILDES (MacWhinney, 2000) will confirm.
However, if we allow probabilities to be non-zero
for ungrammatical sentences, and put no other re-
strictions on the distributions then the learner will
fail on everything, since any distribution could be
for any language.
Secondly, the convergence criterion becomes
vacuous. As the probability of ungrammatical sen-
tences is now zero, this means that PD(H \ T ) =
e+ = 0, and thus the vacuous learner that always
returns the hypothesis ?? will have zero error. The
normal way of dealing with this (Shvaytser, 1990)
is to require the learner to hypothesize a subset of
the target. This is extremely undesirable, as it fails
to account for the presence of over-generalisation
errors in the child ? or any form of production of
ungrammatical sentences. On the basis of these
arguments, we can see that this naive approach is
clearly inadequate.
There are a number of other arguments why dis-
tribution free approaches are inappropriate here,
even though they are desirable in standard appli-
cations of statistical estimation (Collins, 2005).
First, the distribution of examples causally de-
pends on the people who are uttering the examples
who are native speakers of the language the learner
is learning and use that knowledge to construct ut-
terances. Second, suppose that we are trying to
learn a class of languages that includes some in-
finite regular language Lr. For concreteness sup-
pose it consists of {a?b?c?}; any number of a?s fol-
lowed by any number of b?s followed by any num-
ber of c?s. The learner must learn under any dis-
tribution: in particular it will have to learn under
the distribution where every string except an in-
finitesimally small amount has the number of ?a?s
equal to the number of ?b?s, or under the distribu-
tion where the number of occurrences of all three
letters must be equal, or any other arbitrary subset
of the target language. The adversary can distort
the probabilities so that with probability close to
one, at a fixed finite time, the learner will only see
strings from this subset. In effect the learner has
to learn these arbitrary subsets, which could be of
much greater complexity than the language.
Indeed researchers doing computational or
mathematical modelling of language acquisition
often find it convenient to restrict the distribu-
tions in some way. For example (Niyogi and
Berwick, 2000), in some computational modelling
of a parameter-setting model of language acquisi-
tion say
In the earlier section we assumed
that the data was uniformly distributed.
. . . In particular we can choose a dis-
tribution which will make the conver-
gence time as large as we want. Thus
the distribution-free convergence time
for the three parameter system is infi-
nite.
However, finding an alternative is not easy.
There are no completely satisfactory ways of re-
stricting the class of distributions, while maintain-
ing the property that the support of the distribu-
30
tion is equal to the language. (Clark and Thollard,
2004) argue for limiting the class of distributions
to those defined by the probabilistic variants of the
standard Chomsky representations. While this is
sufficient to achieve some interesting learning re-
sults, the class of distributions seems too small,
and is primarily motivated by the requirements of
the learning algorithm, rather than an analysis of
the learning situation.
5.1 Other bounds
Rather than making the simplistic assumption that
the support of the distribution must equal the lan-
guage, we can instead make the more realistic as-
sumption that every sentence, grammatical or un-
grammatical, can in principle appear in the input
and have non zero probability. In this case then
we do not need to require the learner to produce a
hypothesis that is a subset of the target, because if
the learner overgeneralises, e+ will be non-zero.
However, we clearly need to add some con-
straints to enforce the DDA. We can model this as
a function from distributions to languages. It is ob-
vious that grammaticality is correlated with prob-
ability in the sense that grammatical sentences are,
broadly speaking, more likely than ungrammatical
sentences; a natural way of articulating this is to
say that that there must be a real valued threshold
function gD(w) such that if pD(w) > gD(w) then
w ? L. Using this we define the set of allowable
distributions for a language L to be:
D(L, g) = {D : pD(w) > gD(w) ? w ? L}
(4)
Clearly this will satisfy the DDA. On its own this
is vacuous ? we have just changed notation, but
this notation gives us a framework in which to
compare some alternatives.
The original assumption that the support is
equal to the languages in this framework then just
has the simple form gD(w) = 0. The naive con-
stant bound we rejected above would be to have
this threshold as a constant that depends neither on
D nor on w i.e. for all w , gD(w) =  > 0. Both
of these bounds are clearly false, in the sense that
they do not hold for natural distributions: the first
because there are ungrammatical sentences with
non-zero probability; the second because there are
grammatical sentences with arbitrarily low proba-
bility. But the bound here need not be a constant,
and indeed it can depend both on the distribution
D and the word w.
5.2 Functional bound
We now look at variants of these bounds that pro-
vide a more accurate picture of the set of distribu-
tions that the child is exposed to. Recall that what
we are trying to do is to characterise a range of dis-
tributions that is large enough to include those that
the child will be exposed to. A slightly more nu-
anced way would be to have this as a very simple
function ofw, that ignoresD, and is just a function
of length. For example, we could have a simple
uniform exponential model:
gD(w) = ?g?
|w|
g (5)
This is in some sense an application of Harris?s
idea of equiprobability (Harris, 1991):
whatever else there is to be said
about the form of language, a fun-
damental task is to state the depar-
tures from equiprobability in sound- and
word-sequences
Using this model, we do not assume that the
learner is provided with information about the
threshold g; rather the learner will have cer-
tain, presumably domain general mechanisms that
cause it to discard anomalies, and pay attention
to significant deviations from equiprobability. We
can view the threshold g as defining a bound on
equiprobability; the role of syntax is to charac-
terise these deviations from the assumption that all
sequences are in some sense equally likely.
A more realistic model would depend also on
D; for example once could define these thresholds
to depend on some simple observable properties of
the distribution that could take account of lexical
probabilities: more sophisticated versions of this
bound could be derived from a unigram model, or
a class-based model (Pereira, 2000).
Alternatively we could take account of the pre-
fix and suffix probability of a string: for example,
where for some ? < 1: 1
gD(w) = ? max
uv=w
pD(u?
?)pD(?
?v) (6)
6 Using the lower bound
Putting aside the specific proposal for the lower
bound g, and going back to the issue of indirect
1A prefix is just an initial segment of a string and has no
linguistic and similarly for a suffix as the final segment.
31
negative evidence, we can see that the bound g is
the missing piece in the inference: if we observe
that a string w has zero frequency in our data set,
then we can conclude it has low probability, say
p; if p is less than g(w), then the string will be
ungrammatical; therefore the inference from low
probability to ungrammaticality in this case will
be justified.
The bound here is justified independently:
given the indubitable fact that there is a non-zero
probability of ungrammatical strings in the child?s
input, and the DDA, which again seems unassail-
able, together with the fact that learners do learn
some languages, it is a logical necessity that there
is such a bound. This bound then justifies indirect
negative evidence.
It is important to realise how limited this neg-
ative evidence is: it does not give the learner un-
limited access to negative examples. The learner
can only find out about sentences that would be
frequent if they were grammatical; this may be
enough to constrain overgeneralisation.
The most straightforward way of formalising
this indirect negative evidence is with membership
queries (Valiant, 1984; Angluin, 1988b). Mem-
bership queries are a model of learning where the
learner, rather than merely passively receiving ex-
amples, can query an oracle about whether an ex-
ample is in the language or not. In the model we
propose, the learner can approximate a member-
ship query with high probability by seeing the fre-
quency of an example with a high g in a large sam-
ple. If the frequency is low, often zero, in this sam-
ple, then with high probability this example will be
ungrammatical.
In particular given a functional bound, and some
polynomial thresholds on the probability, and us-
ing Chernoff bounds we can simulate a polyno-
mial number of membership queries, using large
samples of data. Note that membership queries
were part of the original PAC model (Valiant,
1984). Thus we can precisely define a limited
form of indirect negative evidence.
In particular given a bound g, we can test to see
whether a polynomial number of strings are un-
grammatical by taking a large sample and examin-
ing their frequency.
The exact details here depend on the form of
gD(w); if the bound depends on D in some re-
spect the learner will need to estimate some aspect
of D to compute the bound. This corresponds to
working out how probable the sentence would be
if it were grammatical. In the cases we have con-
sidered here, given sufficient data, we can estimate
gD(w) with high probability to an accuracy of 1;
call the estimate g?D(w). We can also estimate the
actual probability of the string with high probabil-
ity again with accuracy 2: let us denote this es-
timate by p?D(w). If p?D(w) + 2 < g?D(w) ? 1,
then we can conclude that pD(w) < gD(w) and
therefore that the sentence is ungrammatical. Con-
versely, the fact that a string has been observed
once does not necessarily mean that it is grammat-
ical. It only means that the probability is non-zero.
For the learner to conclude that it is grammatical,
s/he needs to have seen it enough times to con-
clude that the probability is above threshold. This
will be if p?D(w)? 2 > g?D(w) + 1
Note that this may be slightly too weak and
we might want to have a separate lower bound
for grammaticality and upper bound for ungram-
maticality. Otherwise if the distribution is such
that many strings are very close to the boundary
it will not be possible for the learner to determine
whether they are grammatical or not.
We can thus define learnability with respect to a
bound g that defines a set of distributionsD(L,G).
Thus this model differs from the PACmodel in two
respects: first the data is unlabelled, and secondly
is is not distribution free.
Definition An algorithm A learns the class of
languagesL if there is a polynomial p such that for
every language L ? L, where n is the size of the
smallest representation of L, for all distributions
D ? D(L, g) for all , ? > 0, when the algorithm
A is provided with at least p(n, ?1, ??1,?) un-
labelled examples drawn IID from D, it produces
with probability at least 1?? a hypothesis H such
that the error PD(H \T ?T \H) <  and further-
more it runs in time polynomial in the total size of
the sample.
7 Discussion
The unrealistic assumptions of the Gold paradigm
were realised quite early on (Horning, 1969). It
is possible to modify the Gold paradigm by in-
corporating a probabilistic presentation in the data
and requiring the learner to learn with probabil-
ity one. Perhaps surprisingly this does not change
anything, if we put no constraints on the target dis-
tribution (Angluin, 1988a).
In particular given a presentation on which the
32
normal non-probabilistic learner fails, we can con-
struct a distribution on which the probabilistic
learner will fail. Thus allowing an adversary to
pick the distribution is just as bad as allowing an
adversary to pick the presentation. However, the
distribution free assumption with unlabelled data
cannot account for the real variety of distributions
of CDS. In this model we propose restrictions on
the class of distributions, motivated by the oc-
currence of ungrammatical sentences. This also
means that we do not require a separate bound for
over-generalisation. As a result, we conclude that
there are limited amounts of negative evidence,
and suggest that these can be formalised as a lim-
ited number of membership queries, of strings that
would occur infrequently if they were ungrammat-
ical.
To be clear, we are not claiming that this is a di-
rect model of how children learn languages: rather
we hope to get some insight into the fundamen-
tal limitations of learning from unlabelled data by
switching to a more nuanced model. Here we have
not presented any positive results using this model,
but we observe that distribution dependent results
for learning regular languages and some context
free languages could be naturally modified to learn
in this framework. We hope that the recognition of
the validity of indirect negative evidence will di-
rect attention away from the supposed problems of
controlling overgeneralisation and towards the real
problems: the computational complexity of infer-
ring complex models.
References
D. Angluin. 1988a. Identifying languages from
stochastic examples. Technical Report YALEU/
DCS/RR-614, Yale University, Dept. of Computer
Science, New Haven, CT.
D. Angluin. 1988b. Queries and concept learning.
Machine Learning, 2(4):319?342, April.
N. Chater and P. Vita?nyi. 2007. ?Ideal learning? of nat-
ural language: Positive results about learning from
positive evidence. Journal of Mathematical Psy-
chology, 51(3):135?163.
N. Chomsky. 1981. Lectures on Government and
Binding.
Alexander Clark and Franck Thollard. 2004. Par-
tially distribution-free learning of regular languages
from positive samples. In Proceedings of COLING,
Geneva, Switzerland.
M. Collins. 2005. Parameter estimation for statistical
parsing models: Theory and practice of distribution-
free methods. In Harry Bunt, John Carroll, and
Giorgio Satta, editors, New Developments In Pars-
ing Technology, chapter 2, pages 19?55. Springer.
D Gamarnik. 2003. Extension of the PAC framework
to finite and countable Markov chains. IEEE Trans-
actions on Information Theory, 49(1):338?345.
E. M. Gold. 1967. Language identification in the limit.
Information and control, 10(5):447 ? 474.
Z.S. Harris. 1991. A Theory of Language and Informa-
tion: A Mathematical Approach. Clarendon Press.
James Jay Horning. 1969. A study of grammatical
inference. Ph.D. thesis, Computer Science Depart-
ment, Stanford University.
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Associates
Inc, US.
G.F. Marcus. 1993. Negative evidence in language
acquisition. Cognition, 46(1):53?85.
E.L. Newport, H. Gleitman, and L.R. Gleitman. 1977.
Mother, I?d rather do it myself: Some effects and
non-effects of maternal speech style. In Talking
to children: Language input and acquisition, pages
109?149. Cambridge University Press.
Partha Niyogi and Robert C. Berwick. 2000. Formal
models for learning in the principle and parameters
framework. In Peter Broeder and Jaap Murre, ed-
itors, Models of Language Acquisition, pages 225?
243. Oxford University Press.
F. Pereira. 2000. Formal grammar and information
theory: Together again? In Philosophical Transac-
tions of the Royal Society, pages 1239-1253. Royal
Society, London.
Steven Pinker. 1979. Formal models of language
learning. Cognition, 7:217?282.
W. Sakas and J.D. Fodor. 2001. The structural triggers
learner. In Language Acquisition and Learnability,
pages 172?233. Cambridge University Press.
Carson T. Schu?tze. 1996. The Empirical Base of Lin-
guistics. University of Chicago Press.
H. Shvaytser. 1990. A necessary condition for learn-
ing from positive examples. Machine Learning,
5(1):101?113.
L. Valiant. 1984. A theory of the learnable. Communi-
cations of the ACM, 27(11):1134 ? 1142.
C.D. Yang. 2002. Knowledge and Learning in Natural
Language. Oxford University Press, USA.
33
Classifying Non-Sentential Utterances in
Dialogue: A Machine Learning Approach
Raquel Ferna?ndez?
Potsdam University
Jonathan Ginzburg??
King?s College London
Shalom Lappin?
King?s College London
In this article we use well-known machine learning methods to tackle a novel task, namely
the classification of non-sentential utterances (NSUs) in dialogue. We introduce a fine-grained
taxonomy of NSU classes based on corpus work, and then report on the results of several machine
learning experiments. First, we present a pilot study focused on one of the NSU classes in the
taxonomy?bare wh-phrases or ?sluices??and explore the task of disambiguating between
the different readings that sluices can convey. We then extend the approach to classify the
full range of NSU classes, obtaining results of around an 87% weighted F-score. Thus our
experiments show that, for the taxonomy adopted, the task of identifying the right NSU class
can be successfully learned, and hence provide a very encouraging basis for the more general
enterprise of fully processing NSUs.
1. Introduction
Non-sentential utterances (NSUs)?fragmentary utterances that do not have the form
of a full sentence according to most traditional grammars, but that nevertheless convey
a complete clausal meaning?are a common phenomenon in spoken dialogue. The
following are two examples of NSUs taken from the dialogue transcripts of the British
National Corpus (BNC) (Burnard 2000):
(1) a. A: Who wants Beethoven music?
B: Richard and James. [BNC: KB8 1024?1025]1
b. A: It?s Ruth?s birthday.
B: When? [BNC: KBW 13116?13117]
? Karl-Liebknecht Strasse 24-25, 14476 Golm, Germany. E-mail: raquel@ling.uni-potsdam.de.
?? The Strand, London WC2R 2LS, UK. E-mail: jonathan.ginzburg@kcl.ac.uk.
? The Strand, London WC2R 2LS, UK. E-mail: shalom.lappin@kcl.ac.uk.
1 This notation indicates the name of the file and the sentence numbers in the BNC.
Submission received: 24 September 2004; revised submission received: 10 November 2006; accepted for
publication: 9 March 2007.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 3
Arguably the most important issue in the processing of NSUs concerns their resolu-
tion, that is, the recovery of a full clausal meaning from a form which is standardly
considered non-clausal. In the first of the examples, the NSU in bold face is a typical
?short answer,? which despite having the form of a simple NP would most likely be
understood as conveying the proposition Richard and James want Beethoven music. The
NSU in (1b) is an example of what has been called a ?sluice.? Again, despite being
realized by a bare wh-phrase, the meaning conveyed by the NSU could be paraphrased
as the question When is Ruth?s birthday?
Although short answers and short queries like those in (1) are perhaps two of the
most prototypical NSU classes, recent corpus studies (Ferna?ndez and Ginzburg 2002;
Schlangen 2003) show that other less well-known types of NSUs?each with its own
resolution constraints?are also pervasive in real conversations. This variety of NSU
classes, together with their inherent concise form and their highly context-dependent
meaning, often make NSUs ambiguous. Consider, for instance, example (2):
(2) a. A: I left it on the table.
B: On the table.
b. A: Where did you leave it?
B: On the table.
c. A: I think I put it er. . .
B: On the table.
d. A: Should I put it back on the shelf?
B: On the table.
An NSU like B?s response in (2a) can be understood either as a clarification question
or as an acknowledgment, depending on whether it is uttered with raising intonation
or not. In (2b), on the other hand, the NSU is readily understood as a short answer,
whereas in (2c) it fills a gap left by the previous utterance. Yet in the context of (2d) it
will most probably be understood as a sort of correction or a ?helpful rejection,? as we
shall call this kind of NSU later on in this article.
As different NSU classes are typically related to different resolution constraints, in
order to resolve NSUs appropriately systems need to be equipped in the first place with
the ability of identifying the intended kind of NSU. How this ability can be developed is
precisely the issue we address in this article. We concentrate on the task of automatically
classifying NSUs, which we approach using machine learning (ML) techniques. Our aim
in doing so is to develop a classification model whose output can be fed into a dialogue
processing system?be it a full dialogue system or, for instance, an automatic dialogue
summarization system?to boost its NSU resolution capability.
As we shall see, to run the ML experiments we report in this article, we an-
notate our data with small sets of meaningful features, instead of using large sets
of arbitrary features as is common in some stochastic approaches. We do this
with the aim of obtaining a better understanding of the different classes of NSUs,
their distribution, and their properties. For training, we use four machine learn-
ing systems: the rule induction learner SLIPPER (Cohen and Singer 1999), the
memory-based learner TiMBL (Daelemans et al 2003), the maximum entropy algo-
rithm MaxEnt (Le 2003), and the Weka toolkit (Witten and Frank 2000). From the
Weka toolkit we use the J4.8 decision tree learner, as well as a majority class pre-
dictor and a one-rule classifier to derive baseline systems that help us to evaluate
398
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
the difficulty of the classification task and the ML results obtained. The main
advantage of using several systems that implement different learning techniques is that
this allows us to factor out any algorithm-dependent effects that may influence our
results.
The article is structured as follows. In Section 2, we introduce the taxonomy of NSU
classes we adopt, present a corpus study done using the BNC, and give an overview
of the theoretical approach to NSU resolution we assume. After these introductory
sections, in Section 3 we present a pilot study that focuses on bare wh-phrases or sluices.
This includes a small corpus study and a preliminary ML experiment that concentrates
on disambiguating between the different interpretations that sluices can convey. We
obtain very encouraging results: around 80% weighted F-score (an 8% improvement
over a simple one-rule baseline). After this, in Section 4, we move on to the full range
of NSUs. We present our main experiments, whereby the ML approach is extended to
the task of classifying the full range of NSU classes in our taxonomy. The results we
achieve on this task are decidedly positive: around an 87% weighted F-score (a 25%
improvement over a four-rule baseline where only four features are used). Finally, in
Section 5, we offer conclusions and some pointers for future work.
2. A Taxonomy of NSUs
We propose a taxonomy that offers a comprehensive inventory of the kinds of NSUs
that can be found in conversation. The taxonomy includes 15 NSU classes. With a few
modifications, these follow the corpus-based taxonomy proposed by Ferna?ndez and
Ginzburg (2002). In what follows we exemplify each of the categories we use in our
work and characterize them informally.
Clarification Ellipsis (CE). We use this category to classify reprise fragments used to
clarify an utterance that has not been fully comprehended.
(3) a. A: There?s only two people in the class
B: Two people? [BNC: KPP 352?354]
b. A: [. . . ] You lift your crane out, so this part would come up.
B: The end? [BNC: H5H 27?28]
Check Question. This NSU class refers to short queries, usually realized by convention-
alized forms like alright? and okay?, that are requests for explicit feedback.
(4) A: So <pause> I?m allowed to record you.
Okay?
B: Yes. [BNC: KSR 5?7]
Sluice. We consider as sluices all wh-question NSUs, thereby conflating under this form-
based NSU class reprise and direct sluices like those in (5a) and (5b), respectively.2 In the
taxonomy of Ferna?ndez and Ginzburg (2002) reprise sluices are classified as CE. In the
taxonomy used in the experiments we report in this article, however, CE only includes
clarification fragments that are not bare wh-phrases.
2 This distinction is due to Ginzburg and Sag (2001). More on it will be discussed in Section 2.2.
399
Computational Linguistics Volume 33, Number 3
(5) a. A: Only wanted a couple weeks.
B: What? [BNC: KB1 3311?3312]
b. A: I know someone who?s a good kisser.
B: Who? [BNC: KP4 511?512]
Short Answer. This NSU class refers to typical responses to (possibly embedded) wh-
questions (6a)/(6b). Sometimes, however, wh-questions are not explicit, as in the context
of a short answer to a CE question, for instance (6c).
(6) a. A: Who?s that?
B: My Aunty Peggy. [BNC: G58 33?35]
b. A: Can you tell me where you got that information from?
B: From our wages and salary department. [BNC: K6Y 94?95]
c. A: Vague and?
B: Vague ideas and people. [BNC: JJH 65?66]
Plain Affirmative Answer and Plain Rejection. The typical context of these two classes
of NSUs is a polar question (7a), which can be implicit as in CE questions like (7b). As
shown in (7c), rejections can also be used to respond to assertions.
(7) a. A: Did you bring the book I told you?
B: Yes./ No.
b. A: That one?
B: Yeah. [BNC: G4K 106?107]
c. A: I think I left it too long.
B: No no. [BNC: G43 26?27]
Both plain affirmative answers and rejections are strongly indicated by lexical
material, characterized by the presence of a ?yes? word ( yeah, aye, yep. . . ) or the negative
interjection no.
Repeated Affirmative Answer. We distinguish plain affirmative answers like the ones
in (7) from repeated affirmative answers like the one in (8), which respond affirmatively
to a polar question by verbatim repetition or reformulation of (a fragment of) the
query.
(8) A: Did you shout very loud?
B: Very loud, yes. [BNC: JJW 571-572]
Helpful Rejection. The context of helpful rejections can be either a polar question or
an assertion. In the first case, they are negative answers that provide an appropriate
alternative (9a). As responses to assertions, they correct some piece of information in
the previous utterance (9b).
(9) a. A: Is that Mrs. John <last or full name>?
B: No, Mrs. Billy. [BNC: K6K 67-68]
400
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
b. A: Well I felt sure it was two hundred pounds a, a week.
B: No fifty pounds ten pence per person. [BNC: K6Y 112?113]
Plain Acknowledgment. The class plain acknowledgment refers to utterances (like
yeah, mhm, ok) that signal that a previous declarative utterance was understood and/or
accepted.
(10) A: I know that they enjoy debating these issues.
B: Mhm. [BNC: KRW 146?147]
Repeated Acknowledgment. This class is used for acknowledgments that, as repeated
affirmative answers, also repeat a part of the antecedent utterance, which in this case is
a declarative.
(11) A: I?m at a little place called Ellenthorpe.
B: Ellenthorpe. [BNC: HV0 383?384]
Propositional and Factual Modifiers. These two NSU classes are used to classify propo-
sitional adverbs like (12a) and factual adjectives like (12b), respectively, in stand-alone
uses.
(12) a. A: I wonder if that would be worth getting?
B: Probably not. [BNC: H61 81?82]
b. A: There?s your keys.
B: Oh great! [BNC: KSR 137?138]
Bare Modifier Phrase. This class refers to NSUs that behave like adjuncts modifying a
contextual utterance. They are typically PPs or AdvPs.
(13) A: [. . . ] they got men and women in the same dormitory!
B: With the same showers! [BNC: KST 992?996]
Conjunct. This NSU class is used to classify fragments introduced by conjunctions.
(14) A: Alistair erm he?s, he?s made himself coordinator.
B: And section engineer. [BNC: H48 141?142]
Filler. Fillers are NSUs that fill a gap left by a previous unfinished utterance.
(15) A: [. . . ] twenty two percent is er <pause>
B: Maxwell. [BNC: G3U 292?293]
2.1 The Corpus Study
The taxonomy of NSUs presented herein has been tested in a corpus study carried out
using the dialogue transcripts of the BNC. The study, which we describe here briefly,
supplies the data sets used in the ML experiments we will present in Section 4.
The present corpus of NSUs includes and extends the subcorpus used in Ferna?ndez
and Ginzburg (2002). It was created by manual annotation of a randomly selected
section of 200-speaker-turns from 54 BNC files. Of these files, 29 are transcripts of
401
Computational Linguistics Volume 33, Number 3
conversations between two dialogue participants, and 25 files are multi-party tran-
scripts. The total of transcripts used covers a wide variety of domains, from free conver-
sation to meetings, tutorials and training sessions, as well as interviews and transcripts
of medical consultations. The examined subcorpus contains 14,315 sentences. Sentences
in the BNC are identified by the CLAWS segmentation scheme (Garside 1987) and each
unit is assigned an identifier number.
We found a total of 1,299 NSUs, which make up 9% of the total of sentences in the
subcorpus. These results are in line with the rates reported in other recent corpus studies
of NSUs: 11.15% in (Ferna?ndez and Ginzburg 2002), 10.2% in (Schlangen and Lascarides
2003), 8.2% in (Schlangen 2005).3
The NSUs found were labeled according to the taxonomy presented previously
together with an additional class Other introduced to catch all NSUs that did not fall
in any of the classes in the taxonomy. All NSUs that could be classified with the tax-
onomy classes were additionally tagged with the sentence number of their antecedent
utterance. The NSUs not covered by the classification only make up 1.2% (16 instances)
of the total of NSUs found. Thus, with a rate of 98.8% coverage, the present taxonomy
offers a satisfactory coverage of the data.
The labeling of the entire corpus of NSUs was done by one expert annotator. To
assess the reliability of the annotation, a small study with two additional, non-expert
annotators was conducted. These annotated a total of 50 randomly selected instances
(containing a minimum of two instances of each NSU class as labeled by the expert
annotator) with the classes in the taxonomy. The agreement obtained by the three
annotators is reasonably good, yielding a ? score of 0.76. The non-expert annotators
were also asked to identify the antecedent sentence of each NSU. Using the expert
annotation as a gold standard, they achieved 96% and 92% accuracy in this task.
The distribution of NSU classes that emerged after the annotation of the subcorpus
is shown in detail in Table 1. By far the most common class can be seen to be Plain
Acknowledgment, which accounts for almost half of all NSUs found. This is followed
in frequency by Short Answer (14.5%) and Plain Affirmative Answer (8%). CE is the
most common class among the NSUs that denote questions (i.e., CE, Sluice, and Check
Question), making up 6.3% of all NSUs found.
2.2 Resolving NSUs: Theoretical Background and Implementation
The theoretical background we assume with respect to the resolution of NSUs derives
from the proposal presented in Ginzburg and Sag (2001), which in turn is based on the
theory of context developed by Ginzburg (1996, 1999).
Ginzburg and Sag (2001) provide a detailed analysis of a number of classes of
NSUs?including Short Answer, Sluice, and CE?couched in the framework of Head-
driven Phrase Structure Grammar (HPSG). They take NSUs to be first-class gram-
matical constructions whose resolution is achieved by combining the contribution of
the NSU phrase with contextual information?concretely, with the current question
under discussion, or QUD, which roughly corresponds to the current conversational
topic.4
3 For a comparison of our NSU taxonomy and the one proposed by Schlangen (2003), see Ferna?ndez (2006).
4 An anonymous reviewer asked about the distinction between NSUs that are meaning complete and those
which are not. In fact we take all NSUs to be interpreted as full propositions or questions.
402
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 1
Distribution of NSU classes.
NSU class Total %
Plain Acknowledgment 599 46.1
Short Answer 188 14.5
Plain Affirmative Answer 105 8.0
Repeated Acknowledgment 86 6.6
Clarification Ellipsis 82 6.3
Plain Rejection 49 3.7
Factual Modifier 27 2.0
Repeated Affirmative Answer 26 2.0
Helpful Rejection 24 1.8
Check Question 22 1.7
Filler 18 1.4
Bare Modifier Phrase 15 1.1
Propositional Modifier 11 0.8
Sluice 21 1.6
Conjunct 10 0.7
Other 16 1.2
Total 1,299 100
The simplest way of exemplifying this strategy is perhaps to consider a direct short
answer to an explicit wh-question, like the one shown in (16a).
(16) a. A: Who?s making the decisions?
B: The fund manager. (= The fund manager is making the decisions.)
[BNC: JK7 119?120]
b. QUD: ?(x).Make decision(x, t)
Resolution: Make decision( fm,t)
In this dialogue, the current QUD corresponds to the content of the previous utterance?
the wh-question Who?s making the decisions? Assuming a representation of questions as
lambda abstracts, the resolution of the short answer amounts to applying this question
to the phrasal content of the NSU, as shown in (16b) in an intuitive notation.5
Ginzburg and Sag (2001) distinguish between direct and reprise sluices. For direct
sluicing, the current QUD is a polar question p?, where p is required to be a quan-
tified proposition.6 The resolution of the direct sluice consists in constructing a wh-
question by a process that replaces the quantification with a simple abstraction. For
instance:
(17) a. A: A student phoned.
B: Who? (= Which student phoned?)
5 To simplify matters, throughout the examples in this section we use lambda abstraction for wh-questions
and a simple question mark operator for polar questions. For a far more accurate representation of
questions in HPSG and Type Theory with Records, see Ginzburg and Sag (2001) and Ginzburg (2005),
respectively.
6 In Ginzburg?s theory of context an assertion of a proposition p raises the polar question p? for discussion.
403
Computational Linguistics Volume 33, Number 3
b. QUD: ??xPhone(x, t)
Resolution: ?(x).Phone(x, t)
In the case of reprise sluices and CE, the current QUD arises in a somewhat less direct
way, via a process of utterance coercion or accommodation (Larsson 2002; Ginzburg and
Cooper 2004), triggered by the inability to ground the previous utterance (Traum 1994;
Clark 1996). The output of the coercion process is a question about the content of a
(sub)utterance which the addressee cannot resolve. For instance, if the original utterance
is the question Did Bo leave? in (18a), with Bo as the unresolvable sub-utterance, one
possible output from the coercion operations defined by Ginzburg and Cooper (2004) is
the question in (18b), which constitutes the current QUD, as well as the resolved content
of the reprise sluice in (18a).
(18) a. A: Did Bo leave?
B: Who? (= Who are you asking if s/he left?)
b. QUD: ?(b).Ask(A, ?Leave(b, t))
Resolution: ?(b).Ask(A, ?Leave(b, t))
The interested reader will find further details of this approach to NSU resolution and its
extension to other NSU classes in Ginzburg (forthcoming) and Ferna?ndez (2006).
The approach sketched here has been implemented as part of the SHARDS system
(Ginzburg, Gregory, and Lappin 2001; Ferna?ndez et al, in press), which provides a pro-
cedure for computing the interpretation of some NSU classes in dialogue. The system
currently handles short answers, direct and reprise sluices, as well as plain affirmative
answers to polar questions. SHARDS has been extended to cover several types of
clarification requests and used as a part of the information-state-based dialogue system
CLARIE (Purver 2004b). The dialogue system GoDiS (Larsson et al 2000; Larsson 2002)
also uses a QUD-based approach to handle short answers.
3. Pilot Study: Sluice Reading Classification
The first study we present focuses on the different interpretations or readings that
sluices can convey. We first describe a corpus study that aims at providing empirical
evidence about the distribution of sluice readings and establishing possible correlations
between these readings and particular sluice types. After this, we report the results of
a pilot machine learning experiment that investigates the automatic disambiguation of
sluice interpretations.
3.1 The Sluicing Corpus Study
We start by introducing the corpus of sluices. The next subsections describe the annota-
tion scheme, the reliability of the annotation, and the corpus results obtained.
Because sluices have a well-defined surface form?they are bare wh-words?we
were able to use an automatic mechanism to reliably construct our subcorpus of sluices.
This was created using SCoRE (Purver 2001), a tool that allows one to search the BNC
using regular expressions.
404
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 2
Total of sluices in the BNC.
what why who where which N when how which Total
3,045 1,125 491 350 160 107 50 15 5,343
The dialogue transcripts of the BNC contain 5,183 bare sluices (i.e., 5,183 sentences
consisting of just a wh-word). We distinguish between the following classes of bare
sluices: what, who, when, where, why, how, and which. Given that only 15 bare which were
found, we also considered sluices of the form which N. Including which N, the corpus
contains a total of 5,343 sluices, whose distribution is shown in Table 2.
For our corpus study, we selected a sample of sluices extracted from the total found
in the dialogue transcripts of the BNC. The sample was created by selecting all instances
of bare how (50) and bare which (15), and arbitrarily selecting 100 instances of each of the
remaining sluice classes, making up a total of 665 sluices.
Note that the sample does not reflect the frequency of sluice types found in the full
corpus. The inclusion of sufficient instances of the lesser frequent sluice types would
have involved selecting a much larger sample. Consequently it was decided to abstract
over the true frequencies to create a balanced sample whose size was manageable
enough to make the manual annotation feasible. We will return to the issue of the true
frequencies in Section 3.1.3.
3.1.1 Sluice Readings. The sample of sluices was classified according to a set of four
semantic categories?drawn from the theoretical distinctions introduced by Ginzburg
and Sag (2001)?corresponding to different sluice interpretations. The typology reflects
the basic direct/reprise divide and incorporates other categories that cover additional
readings, including an Unclear class intended for those cases that cannot easily be
classified by any of the other categories. The typology of sluice readings used was the
following:
Direct. Sluices conveying a direct reading query for additional information that was
explicitely or implicitly quantified away in the antecedent, which is understood without
difficulty. The sluice in (19) is an example of a sluice with direct reading: It asks for
additional temporal information that is implicitly quantified away in the antecedent
utterance.
(19) A: I?m leaving this school.
B: When? [BNC: KP3 537?538]
Reprise. Sluices conveying a reprise reading emerge as a result of an understanding
problem. They are used to clarify a particular aspect of the antecedent utterance corre-
sponding to one of its constituents, which was not correctly comprehended. In (20) the
reprise sluice has as antecedent constituent the pronoun he, whose reference could not
be adequately grounded.
(20) A: What a useless fairy he was.
B: Who? [BNC: KCT 1752?1753]
405
Computational Linguistics Volume 33, Number 3
Clarification. As reprise, this category also corresponds to a sluice reading that deals
with understanding problems. In this case the sluice is used to request clarification of
the entire antecedent utterance, indicating a general breakdown in communication. The
following is an example of a sluice with a clarification interpretation:
(21) A: Aye and what money did you get on it?
B: What?
A: What money does the government pay you? [BNC: KDJ 1077?1079]
Wh-anaphor. This category is used for the reading conveyed by sluices like (22), which
are resolved to a (possibly embedded) wh-question present in the antecedent utterance.
(22) A: We?re gonna find poison apple and I know where that one is.
B: Where? [BNC: KD1 2370?2371]
Unclear. We use this category to classify those sluices whose interpretation is difficult
to grasp, possibly because the input is too poor to make a decision as to its resolution,
as in the following example:
(23) A: <unclear> <pause>
B: Why? [BNC: KCN 5007]
3.1.2 Reliability. The coding of sluice readings was done independently by three dif-
ferent annotators. Agreement was moderate (? = 0.59). There were important differ-
ences among sluice classes: The lowest agreement was on the annotation of how (0.32)
and what (0.36), whereas the agreement on classifying who was substantially higher
(0.74).
Although the three coders may be considered ?experts,? their training and famil-
iarity with the data were not equal. This resulted in systematic differences in their
annotations. Two of the coders had worked more extensively with the BNC dialogue
transcripts and, crucially, with the definition of the categories to be applied. Leaving the
third annotator out of the coder pool increases agreement very significantly (? = 0.71).
The agreement reached by the more expert pair of coders was acceptable and, we believe,
provides a solid foundation for the current classification.7
3.1.3 Distribution Patterns. The sluicing corpus study shows that the distribution of read-
ings is significantly different for each class of sluice. The distribution of interpretations
is shown in Table 3, presented as row counts and percentages of those instances where
7 Besides the difficulty of annotating fine-grained semantic distinctions, we think that one of the reasons
why the ? score we obtain is not too high is that, as shall become clear in the next section, the present
annotation is strongly affected by the prevalence problem, which occurs when the distributions for
categories are skewed (highly unequal instantiation across categories). In order to control for differences
in prevalence, Di Eugenio and Glass (2004) propose an additional measure called PABAK
(prevalence-adjusted bias-adjusted kappa). In our case, we obtain a PABAK score of 0.60 for agreement
amongst the three coders, and a PABAK score of 0.80 for agreement between the pair of more expert
coders. A more detailed discussion of these issues can be found in Ferna?ndez (2006).
406
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 3
Distribution patterns.
Sluice Direct n (%) Reprise n (%) Clarification n (%) Wh-anaphor n (%)
what 7 (9.60) 17 (23.3) 48 (65.7) 1 (1.3)
why 55 (68.7) 24 (30.0) 0 (0) 1 (1.2)
who 10 (13.0) 65 (84.4) 0 (0) 2 (2.6)
where 31 (34.4) 56 (62.2) 0 (0) 3 (3.3)
when 50 (63.3) 27 (34.1) 0 (0) 2 (2.5)
which 1 (8.3) 11 (91.6) 0 (0) 0 (0)
whichN 19 (21.1) 71 (78.8) 0 (0) 0 (0)
how 23 (79.3) 3 (10.3) 3 (10.3) 0 (0)
at least two annotators agree, labeled taking the majority class and leaving aside cases
classified as Unclear.
Table 3 reveals significant correlations between sluice classes and preferred interpre-
tations (a chi square test yields ?2 = 438.53, p ? 0.001). The most common interpretation
for what is Clarification, making up more than 65%. Why sluices have a tendency to be
Direct (68.7%). The sluices with the highest probability of being Reprise are who (84.4%),
which (91.6), which N (78.8%), and where (62.2%). On the other hand, when (63.3%) and
how (79.3%) have a clear preference for Direct interpretations.
As explained in Section 3.1, the sample used in the corpus study does not reflect
the overall frequencies of sluice types found in the BNC. Now, in order to gain a
complete perspective on sluice distribution in the full corpus, it is therefore appropriate
to combine the percentages in Table 3 with the absolute number of sluices contained in
the BNC. The number of estimated tokens is displayed in Table 4.
For instance, the combination of Tables 3 and 4 allows us to see that although
almost 70% of why sluices are Direct, the absolute number of why sluices that are Reprise
exceeds the total number of when sluices by almost 3 to 1. Another interesting pattern
revealed by this data is the low frequency of when sluices, particularly by comparison
with what one might expect to be its close cousin, where. Indeed the Direct/Reprise
splits are almost mirror images for when versus where. Explicating the distribution in
Table 4 is important in order to be able to understand among other issues whether we
would expect a similar distribution to occur in a Spanish or Mandarin dialogue corpus;
similarly, whether one would expect this distribution to be replicated across different
domains.
Table 4
Sluice class frequency (estimated tokens).
whatcla 2,040 whichNrep 135
whydir 775 whendir 90
whatrep 670 whodir 70
whorep 410 wheredir 70
whyrep 345 howdir 45
whererep 250 whenrep 35
whatdir 240 whichNdir 24
407
Computational Linguistics Volume 33, Number 3
We will not attempt to provide an explanation for these patterns here. The reader
is invited to check a sketch of such an explanation for some of the patterns exhibited in
Table 4 in Ferna?ndez, Ginzburg, and Lappin (2004).
3.2 Automatic Disambiguation
In this section, we report a pilot study where we use machine learning to automatically
disambiguate between the different sluice readings using data obtained in the corpus
study presented previously.
3.2.1 Data. The data set used in this experiment was selected from our classified corpus
of sluices. To generate the input data for the ML experiments, all three-way agreement
instances plus those instances where there is agreement between the two coders with the
highest agreement were selected, leaving out cases classified as Unclear. The total data
set includes 351 datapoints. Of these, 106 are classified as Direct, 203 as Reprise, 24 as
Clarification, and 18 as Wh-anaphor. Thus, the classes in the data set have significantly
skewed distributions. However, as we are faced with a very small data set, we cannot
afford to balance the classes by leaving out a subset of the data. Hence, in this pilot study
the 351 data points are used in the ML experiments with their original distributions.
3.2.2 Features and Feature Annotation. In this pilot study?as well as in the extended
experiment we will present later on?instances were annotated with a small set of
features extracted automatically using the POS information encoded in the BNC. The
annotation procedure involves a simple algorithm which employs string searching and
pattern matching techniques that exploit the SGML mark-up of the corpus. The BNC
was automatically tagged using the CLAWS system developed at Lancaster University
(Garside 1987). The ?100 million words in the corpus were annotated according to a
set of 57 POS codes (known as the C5 tag-set) plus 4 codes for punctuation tags. A
list of these codes can be found in Burnard (2000). The BNC POS annotation process is
described in detail in Leech, Garside, and Bryant (1994).
Unfortunately the BNC mark-up does not include any coding of intonation. Our fea-
tures can therefore not use any intonational data, which would presumably be a useful
Table 5
Sluice features and values.
Feature Description Values
sluice type of sluice what, why, who, . . .
mood mood of the antecedent utterance decl, n decl
polarity polarity of the antecedent utterance pos, neg, ?
quant presence of a quantified expression yes, no, ?
deictic presence of a deictic pronoun yes, no, ?
proper n presence of a proper name yes, no, ?
pro presence of a pronoun yes, no, ?
def desc presence of a definite description yes, no, ?
wh presence of a wh word yes, no, ?
overt presence of any other potential antecedent expression yes, no, ?
408
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
source of information to distinguish, for instance, between question- and proposition-
denoting NSUs, between Plain Acknowledgment and Plain Affirmative Answer, and
between Reprise and Direct sluices.
To annotate the sluicing data, a set of 11 features was used. An overview of the
features and their values is shown in Table 5. Besides the feature sluice, which indicates
the sluice type, all the other features are concerned with properties of the antecedent
utterance. The features mood and polarity refer to syntactic and semantic properties
of the antecedent utterance as a whole. The remaining features, on the other hand,
focus on a particular lexical type or construction contained in the antecedent. These
features (quant, deictic, proper n, pro, def desc, wh, and overt) are not annotated
independently, but conditionally on the sluice type. That is, they will take yes as a
value if the element or construction in question appears in the antecedent and it matches
the semantic restrictions imposed by the sluice type. For instance, when a sluice with
value where for the feature sluice is annotated, the feature deictic, which encodes
the presence of a deictic pronoun, will take value yes only if the antecedent utterance
contains a locative deictic like here or there. Similarly the feature wh takes a yes value
only if there is a wh-word in the antecedent that is identical to the sluice type.
Unknown or irrelevant values are indicated by a question mark (?) value. This
allows us to express, for instance, that the presence of a proper name is irrelevant to
determining the interpretation of say a when sluice, although it is crucial when the sluice
type is who. The feature overt takes no as value when there is no overt antecedent
expression. It takes yes when there is an antecedent expression not captured by any
other feature, and it is considered irrelevant (? value) when there is an antecedent
expression defined by another feature.
The 351 data points were automatically annotated with the 11 features shown in
Table 5. The automatic annotation procedure was evaluated against a manual gold
standard, achieving an accuracy of 86%.
3.2.3 Baselines. Because sluices conveying a Reprise reading make up more than 57%
of our data set, relatively high results can already be achieved with a majority class
baseline that always predicts the class Reprise. This yields a 42.4% weighted F-score.
A slightly more interesting baseline can be obtained by using a one-rule classifier.
We use the implementation of a one-rule classifier provided in the Weka toolkit. For each
feature, the classifier creates a single rule which generates a decision tree where the root
is the feature in question and the branches correspond to its different values. The leaves
are then associated with the class that occurs most often in the data, for which that value
holds. The classifier then chooses the feature which produces the minimum error.
Figure 1
One-rule tree.
409
Computational Linguistics Volume 33, Number 3
Table 6
Baselines? results.
Sluice reading Recall Precision F1
Majority class baseline Reprise 100 57.80 73.30
weighted score 57.81 33.42 42.40
One-rule baseline Direct 72.60 67.50 70.00
Reprise 79.30 80.50 79.90
Clarification 100 64.90 78.70
weighted score 73.61 71.36 72.73
In this case the feature with the minimum error chosen by the one-rule classifier is
sluice. The classifier produces the one-rule tree in Figure 1. The branches of the tree
correspond to the sluice types; the interpretation with the highest probability for each
type of sluice is then predicted.
By using the feature sluice the one-rule tree implements the correlations between
sluice type and preferred interpretation that were discussed in Section 3.1.3. There, we
pointed out that these correlations were statistically significant. We can see now that
they are indeed a good rough guide for predicting sluice readings. As shown in Table 6,
the one-rule baseline dependent on the distribution patterns of the different sluice types
yields a 72.73% weighted F-score.
All results reported (here and in the remainder of the article) were obtained by
performing 10-fold cross-validation. They are presented as follows: The tables show
the recall, precision, and F-measure for each class. To calculate the overall performance
of the algorithm, these scores are normalized according to the relative frequency of
each class. This is done by multiplying each score by the total of instances of the
corresponding class and then dividing by the total number of datapoints in the data
set. The weighted overall recall, precision, and F-measure, shown in boldface for each
baseline in Table 6, is then the sum of the corresponding weighted scores. For each of
the baselines, the sluice readings not shown in the table obtain null scores.
3.2.4 ML Results. Finally, the four machine learning algorithms were run on the data
set annotated with the 11 features. Here, as well as in the more extensive experiment
we will present in Section 4, we use the following parameter settings with each of the
learners. Weka?s J4.8 decision tree learner is run using the default parameter settings.
With SLIPPER we use the option unordered, which finds a rule set that separates each
class from the remaining classes using growing and pruning techniques and in our case
yields slightly better results than the default setting. As for TiMBL, we run it using the
modified value difference metric (which performs better than the default overlap metric),
and keep the default settings for the number of nearest neighbors (k = 1) and feature
weighting method (gain ratio). Finally, with MaxEnt we use 40 iterations of the default
L-BFGS parameter estimation (Malouf 2002).
Overall, in this pilot study we obtain results of around 80% weighted F-score,
although there are some significant differences amongst the learners. MaxEnt gives the
lowest score (73.24% weighted F-score)?hardly over the one-rule baseline, and more
than 8 points lower than the best results, obtained with Weka?s J4.8 (81.80% weighted
F-score). The size of the data set seems to play a role in these differences, indicating that
MaxEnt does not perform so well with small data sets. A summary of weighted F-scores
is given in Table 7.
410
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 7
Comparison of weighted F-scores.
System Weighted F-score
Majority class baseline 42.40
One rule baseline 72.73
MaxEnt 73.24
TiMBL 79.80
SLIPPER 81.62
J4.8 81.80
Detailed recall, precision, and F-measure results for each learner are shown in
Appendix A. The results yielded by MaxEnt are almost equivalent to the ones achieved
with the one-rule baseline. With the other three learners, the use of contextual features
improves the results for Reprise and Direct by around 5 points each with respect to the
one-rule baseline. The results obtained with the one-rule baseline for the Clarification
reading, however, are hardly improved upon by any of the learners. In the case of
TiMBL the score is in fact lower?72.16 versus 78.70 weighted F-score. This leads us to
conclude that the best strategy is to interpret al what sluices as conveying a Clarification
reading.
The class Wh-anaphora, which, not being the majority interpretation for any sluice
type, was not predicted by the one-rule baseline nor by MaxEnt, now gives positive
results with the other three learners. The best result for this class is obtained with Weka?s
J4.8: 80% F-score.
The decision tree generated by Weka?s J4.8 algorithm is displayed in Figure 2. The
root of the tree corresponds to the feature wh, which makes a first distinction between
Figure 2
Weka?s J4.8 tree.
411
Computational Linguistics Volume 33, Number 3
Wh-anaphor and the other readings. If the value of this feature is yes, the class Wh-
anaphor is predicted. A negative value for this feature leads to the feature sluice. The
class with the highest probability is the only clue used to predict the interpretation
of the sluice types what, where, which, and whichN in a way parallel to the one-rule
baseline. Additional features are used for when, why, and who. A Direct reading is
predicted for a when sluice if there is no overt antecedent expression, whereas a Reprise
reading is preferred if the feature overt takes as value yes. For why sluices the mood
of the antecedent utterance is used to disambiguate between Reprise and Direct: If the
antecedent is declarative, the sluice is classified as Direct; if it is non-declarative it is
interpreted as Reprise. In the classification of who sluices three features are taken into
account: quant, pro, and proper n. The basic strategy is as follows: If the antecedent
utterance contains a quantifier and neither personal pronouns nor proper names appear,
the predicted class is Direct, otherwise the sluice is interpreted as Reprise.
3.2.5 Feature Contribution. Note that not all features are used in the tree generated by
Weka?s J4.8. The missing features are polarity, deictic, and def desc. Although they
don?t make any contribution to the model generated by the decision tree, examination
of the rules generated by SLIPPER shows that they are all used in the rule set induced by
this algorithm, albeit in rules with low confidence level. Despite the fact that SLIPPER
uses all features, the contribution of polarity, deictic, and def desc does not seem
to be very significant. When they are eliminated from the feature set, SLIPPER yields
very similar results to the ones obtained with the full set of features: 81.22% weighted F-
score versus the 81.66% obtained before. TiMBL on the other hand goes down a couple of
points, from 79.80% to 77.32% weighted F-score. No variation is observed with MaxEnt,
which seems to be using just the sluice type as a clue for classification.
4. Classifying the Full Range of NSUs
So far we have presented a study that has concentrated on fine-grained semantic dis-
tinctions of one of the classes in our taxonomy, namely Sluice, and have obtained very
encouraging results?around 80% weighted F-score (an improvement of 8 points over
a simple one-rule baseline). In this section we show that the ML approach taken can
be successfully extended to the task of classifying the full range of NSU classes in our
taxonomy.
We first present an experiment run on a restricted data set that excludes the classes
Plain Acknowledgement and Check Question, and then, in Section 4.6, report on a
follow-up experiment where all NSU classes are included.
4.1 Data
The data used in the experiments was selected from the corpus of NSUs following some
simplifying restrictions. Firstly, we leave aside the 16 instances classified as Other in the
corpus study (see Table 1). Secondly, we restrict the experiments to those NSUs whose
antecedent is the immediately preceding utterance. This restriction, which makes the
feature annotation task easier, does not pose a significant coverage problem, given that
the immediately preceding utterance is the antecedent for the vast majority of NSUs
(88%). The set of all NSUs, excluding those classified as Other, whose antecedent is the
immediately preceding utterance, contains a total of 1123 datapoints. See Table 8.
412
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 8
NSU subcorpus.
NSU class Total
Plain Acknowledgment 582
Short Answer 105
Affirmative Answer 100
Repeated Acknowledgment 80
CE 66
Rejection 48
Repeated Affirmative Answer 25
Factual Modifier 23
Sluice 20
Helpful Rejection 18
Filler 16
Check Question 15
Bare Modifier Phrase 10
Propositional Modifier 10
Conjunct 5
Total data set 1,123
Finally, as mentioned previously, the last restriction adopted concerns the instances
classified as Plain Acknowledgment and Check Question. Taking the risk of end-
ing up with a considerably smaller data set, we decided to leave aside these meta-
communicative NSU classes given that (1) plain acknowledgments make up more than
50% of the subcorpus leading to a data set with very skewed distributions; (2) check
questions are realized by the same kind of expressions as plain acknowledgments (okay,
right, etc.) and would presumably be captured by the same feature; and (3) a priori these
two classes seem two of the easiest types to identify (a hypothesis that was confirmed
after a second experiment?see Section 4.6). We therefore exclude plain acknowledg-
ments and check questions and concentrate on a more interesting and less skewed data
set containing all remaining NSU classes. This makes up a total of 526 data points
(1123 ? 582 ? 15). In Subsection 4.6 we shall compare the results obtained using this
restricted data set with those of a second experiment in which plain acknowledgements
and check questions are incorporated.
4.2 Features
A small set of features that capture the contextual properties that are relevant for
NSU classification was identified. In particular three types of properties that play an
important role in the classification task were singled out. The first one has to do with
semantic, syntactic, and lexical properties of the NSUs themselves. The second one
refers to the properties of its antecedent utterance. The third concerns relations between
the antecedent and the fragment. Table 9 shows an overview of the nine features used.
4.2.1 NSU Features. A set of four features are related to properties of the NSUs. These
are nsu cont, wh nsu, aff neg, and lex. The feature nsu cont is intended to distin-
guish between question-denoting (q value) and proposition-denoting (p value) NSUs.
The feature wh nsu encodes the presence of a wh-phrase in the NSU?it is primarily
introduced to identify Sluices. The features aff neg and lex signal the appearance of
413
Computational Linguistics Volume 33, Number 3
Table 9
NSU features and values.
Feature Description Values
nsu cont content of the NSU (either prop or question) p, q
wh nsu presence of a wh word in the NSU yes, no
aff neg presence of a yes/no word in the NSU yes, no, e(mpty)
lex presence of different lexical items in the NSU p mod, f mod, mod, conj, e
ant mood mood of the antecedent utterance decl, n decl
wh ant presence of a wh word in the antecedent yes, no
finished (un)finished antecedent fin, unf
repeat repeated words in NSU and antecedent 0-3
parallel repeated tag sequences in NSU and antecedent 0-3
particular lexical items. They include a value e(mpty) which allows us to encode the
absence of the relevant lexical items as well. The values of the feature aff neg indicate
the presence of either a yes or a no word in the NSU. The values of lex are invoked by the
appearance of modal adverbs (p mod), factual adjectives (f mod), and prepositions (mod)
and conjunctions (conj) in initial positions. These features are expected to be crucial to
the identification of Plain/Repeated Affirmative Answer and Plain/Helpful Rejection
on the one hand, and Propositional Modifiers, Factual Modifiers, Bare Modifier Phrases,
and Conjuncts on the other.
Note that the feature lex could be split into four binary features, one for each of its
non-empty values. This option, however, leads to virtually the same results. Hence, we
opt for a more compact set of features. This also applies to the feature aff neg.
4.2.2 Antecedent Features. We use the features ant mood, wh ant, and finished to encode
properties of the antecedent utterance. The first of these features distinguishes between
declarative and non-declarative antecedents. The feature wh ant signals the presence
of a wh-phrase in the antecedent utterance, which seems to be the best cue for classi-
fying Short Answers. As for the feature finished, it should help the learners identify
Fillers. The value unf is invoked when the antecedent utterance has a hesitant ending
(indicated, for instance, by a pause) or when there is no punctuation mark signalling a
finished utterance.
4.2.3 Similarity Features. The last two features, repeat and parallel, encode similarity
relations between the NSU and its antecedent utterance. They are the only numer-
ical features in the feature set. The feature repeat, which indicates the appearance
of repeated words between NSU and antecedent, is introduced as a clue to identify
Repeated Affirmative Answers and Repeated Acknowledgments. The feature parallel,
on the other hand, is intended to capture the particular parallelism exhibited by Helpful
Rejections. It signals the presence of sequences of POS tags common to the NSU and its
antecedent.
As in the sluicing experiment, all features were extracted automatically from the
POS information encoded in the BNC mark-up. However, as with the feature mood
in the sluicing study, some features like nsu cont and ant mood are high level features
that do not have straightforward correlates in POS tags. Punctuation tags (that would
correspond to intonation patterns in spoken input) help to extract the values of these
features, but the correspondence is still not unique. For this reason the automatic
414
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Figure 3
One-rule tree.
feature annotation procedure was again evaluated against a small sample of manually
annotated data. The feature values were extracted manually for 52 instances (?10% of
the total) randomly selected from the data set. In comparison with this gold standard,
the automatic feature annotation procedure achieves 89% accuracy. Only automatically
annotated data is used for the learning experiments.
4.3 Baselines
We now turn to examine some baseline systems that will help us to evaluate the
classification task. As before, the simplest baseline we can consider is a majority class
baseline that always predicts the class with the highest probability in the data set. In
the restricted data set used for the first experiment, this is the class Short Answer. The
majority class baseline yields a 6.7% weighted F-score.
When a one-rule classifier is run, we see that the feature that yields the minimum
error is aff neg. The one-rule baseline produces the one-rule decision tree in Fig-
ure 3, which yields a 32.5% weighted F-score (see Table 10). Plain Affirmative Answer
is the class predicted when the NSU contains a yes-word, Rejection when it contains a
no-word, and Short Answer otherwise.
Finally, we consider a more substantial baseline that uses the four NSU features.
Running Weka?s J4.8 decision tree classifier with these features creates a decision tree
with four rules, one for each feature used. The tree is shown in Figure 4.
Table 10
Baselines? results.
NSU Class Recall Precision F1
Majority class baseline ShortAns 100.00 20.10 33.50
weighted score 19.92 4.00 6.67
One-rule baseline ShortAns 95.30 30.10 45.80
AffAns 93.00 75.60 83.40
Reject 100.00 69.60 82.10
weighted score 45.93 26.73 32.50
Four-rule baseline CE 96.97 96.97 96.97
Sluice 100.00 95.24 97.56
ShortAns 94.34 47.39 63.09
AffAns 93.00 81.58 86.92
Reject 100.00 75.00 85.71
PropMod 100.00 100.00 100.00
FactMod 100.00 100.00 100.00
BareModPh 80.00 72.73 76.19
Conjunct 100.00 71.43 83.33
weighted score 70.40 55.92 62.33
415
Computational Linguistics Volume 33, Number 3
Figure 4
Four-rule tree.
The root of the tree corresponds to the feature nsu cont. It makes a first distinction
between question-denoting (q branch) and proposition-denoting NSUs (p branch). Not
surprisingly, within the q branch the feature wh nsu is used to distinguish between
Sluice and CE. The feature lex is the first node in the p branch. Its different values
capture the classes Conjunct, Propositional Modifier, Factual Modifier, and Bare Modi-
fier Phrase. The e(mpty) value for this feature takes us to the last, most embedded node
of the tree, realized by the feature aff neg, which creates a sub-tree parallel to the one-
rule tree in Figure 3. This four-rule baseline yields a 62.33% weighted F-score. Detailed
results for the three baselines considered are shown in Table 10.
4.4 Feature Contribution
As can be seen in Table 10, the classes Sluice, CE, Propositional Modifier, and Factual
Modifier achieve very high F-scores with the four-rule baseline?between 97% and
100%. These results are not improved upon by incorporating additional features nor
by using more sophisticated learners, which indicates that NSU features are sufficient
indicators to classify these NSU classes. This is in fact not surprising, given that the
disambiguation of Sluice, Propositional Modifier, and Factual Modifier is tied to the
presence of particular lexical items that are relatively easy to identify (wh-phrases and
certain adverbs and adjectives), whereas CE acts as a default category within question-
denoting NSUs.
There are, however, four NSU classes that are not predicted at all when only NSU
features are used. These are Repeated Affirmative Answer, Helpful Rejection, Repeated
Acknowledgment, and Filler. Because they are not associated with any leaf in the
tree, they yield null scores and therefore don?t appear in Table 10. Examination of
the confusion matrices shows that around 50% of Repeated Affirmative Answers were
classified as Plain Affirmative Answers, whereas the remaining 50%?as well as the
overwhelming majority of the other three classes just mentioned?were classified as
Short Answer. Acting as the default class, Short Answers achieves the lowest score:
63.09% F-score.
In order to determine the contribution of the antecedent features (ant mood, wh ant,
finished), as a next step these were added to the NSU features used in the four-
rule tree. When the antecedent features are incorporated, two additional NSU classes
are predicted. These are Repeated Acknowledgment and Filler, which achieve rather
416
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Figure 5
Node on a tree using NSU and antecedent features.
positive results: 74.8% and 64% F-score, respectively. We do not show the full results
obtained when NSU and antecedent features are used together. Besides the addition
of these two NSU classes, the results are very similar to those achieved with just NSU
features. The tree obtained when the antecedent features are incorporated to the NSU
features can be derived by substituting for the last node in the tree in Figure 4 the
tree in Figure 5. As can be seen in Figure 5, the features ant mood and finished con-
tribute to distinguish Repeated Acknowledgment and Filler from Short Answer, whose
F-score consequently rises, from 63.09% to 79%, due to an improvement in precision.
Interestingly, the feature wh ant does not have any contribution at this stage (although
it will be used by the learners when the similarity features are added). The general
weighted F-score obtained when NSU and antecedent features are combined is 77.87%.
A comparison of all weighted F-scores obtained will be shown in the next section, in
Table 11.
The use of NSU features and antecedent features is clearly not enough to account
for Repeated Affirmative Answer and Helpful Rejection, which obtain null scores.
4.5 ML Results
In this section we report the results obtained when the similarity features are included,
thereby using the full feature set, and the four machine learning algorithms are trained
on the data.
Although the classification algorithms implement different machine learning tech-
niques, they all yield very similar results: around an 87% weighted F-score. The max-
imum entropy model performs best, although the difference between its results and
Table 11
Comparison of weighted F-scores.
System Weighted F-score
Majority class baseline 6.67
One rule baseline 32.50
Four rule baseline (NSU features) 62.33
NSU and antecedent features 77.83
Full feature set:
- SLIPPER 86.35
- TiMBL 86.66
- J4.8 87.29
- MaxEnt 87.75
417
Computational Linguistics Volume 33, Number 3
those of the other algorithms is not statistically significant. Detailed recall, precision,
and F-measure scores are shown in Appendix B.
As seen in previous sections, the four-rule baseline algorithm that uses only NSU
features yields a 62.33% weighted F-score, whereas the incorporation of antecedent
features yields a 77.83% weighted F-score. The best result, the 87.75% weighted F-score
obtained with the maximal entropy model using all features, shows a 10% improvement
over this last result. As promised, a comparison of the scores obtained with the different
baselines considered and all learners used is given in Table 11.
Short Answers achieve high recall scores with the baseline systems (more than
90%). In the three baselines considered, Short Answer acts as the default category.
Therefore, even though the recall is high (given that Short Answer is the class with
the highest probability), precision tends to be quite low. The precision achieved for
Short Answer when only NSU features are used is ?47%. When antecedent features
are incorporated precision goes up to ?72%. Finally, the addition of similarity features
raises the precision for this class to ?82%. Thus, by using features that help to identify
other categories with the machine learners, the precision for Short Answers is improved
by around 36%, and the precision of the overall classification system by almost 33%:
from 55.90% weighted precision obtained with the four-rule baseline, to the 88.41%
achieved with the maximum entropy model using all features.
With the addition of the similarity features (repeat and parallel), the classes
Repeated Affirmative Answer and Helpful Rejection are predicted by the learners.
Although this contributes to the improvement of precision for Short Answer, the scores
yielded by these two categories are lower than the ones achieved with other classes. Re-
peated Affirmative Answer achieves nevertheless decent F-score, ranging from 56.96%
with SLIPPER to 67.20% with MaxEnt. The feature wh ant, for instance, is used to
distinguish Short Answer from Repeated Affirmative Answer. Figure 6 shows one of
the sub-trees generated by the feature repeat when Weka?s J4.8 is used with the full
feature set.
The class with the lowest scores is clearly Helpful Rejection. TiMBL achieves a
39.92% F-score for this class. The maximal entropy model, however, yields only a 10.37%
F-score. Examination of the confusion matrices shows that ?27% of Help Rejections
were classified as Rejection, ?26% as Short Answer, and ?15% as Repeated Acknowl-
edgement. This indicates that the feature parallel, introduced to identify this type of
NSUs, is not a good enough cue.
Figure 6
Node on a tree using the full feature set.
418
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Figure 7
One-rule tree.
4.6 Incorporating Plain Acknowledgment and Check Question
As explained in Section 4.1, the data set used in the experiments reported in the previous
section excluded the instances classified as Plain Acknowledgment and Check Question
in the corpus study. The fact that Plain Acknowledgment is the category with the highest
probability in the subcorpus (making up more than 50% of our total data set?see
Table 8), and that it does not seem particularly difficult to identify could affect the
performance of the learners by inflating the results. Therefore it was left out in order
to work with a more balanced data set and to minimize the potential for misleading
results. As the expressions used in plain acknowledgments and check questions are
very similar and they would in principle be captured by the same feature values, check
questions were left out as well. In a second phase the instances classified as Plain
Acknowledgment and Check Question were incorporated to measure their effect on
the results. In this section we discuss the results obtained and compare them with the
ones achieved in the initial experiment.
To generate the annotated data set an additional value ack was added to the feature
aff neg. This value is invoked to encode the presence of expressions typically used in
plain acknowledgments and/or check questions (mhm, right, okay, etc.). The total data
set (1,123 data points) was automatically annotated with the features modified in this
way, and the machine learners were then run on the annotated data.
4.6.1 Baselines. Given the high probability of Plain Acknowledgment, a simple majority
class baseline gives relatively high results: 35.31% weighted F-score. The feature with
the minimum error used to derive the one-rule baseline is again aff neg, this time with
the new value ack as part of its possible values (see Figure 7). The one-rule baseline
yields a weighted F-score of 54.26%.
The four-rule tree that uses only NSU features goes up to a weighted F-score
of 67.99%. In this tree the feature aff neg is now also used to distinguish between
CE and Check Question. Figure 8 shows the q branch of the tree. As the last node of
Figure 8
Node on the four-rule tree.
419
Computational Linguistics Volume 33, Number 3
the four-rule tree now corresponds to the tree in Figure 7, the class Plain Affirmative
Answer is not predicted when only NSU features are used.
When antecedent features are incorporated, Plain Affirmative Answers, Repeated
Acknowledgments, and Fillers are predicted, obtaining very similar scores to the ones
achieved in the experiment with the restricted data set. The feature ant mood is now
also used to distinguish between Plain Acknowledgment and Plain Affirmative Answer.
The last node in the tree is shown in Figure 9. The combined use of NSU features and
antecedent features yields a weighted F-score of 85.44%.
4.6.2 ML Results. As in the previous experiment, when all features are used the results
obtained are very similar across learners (around 92% weighted F-score), if slightly
lower with Weka?s J4.8 (89.53%). Detailed scores for each class are shown in Appen-
dix C. As expected, the class Plain Acknowledgment obtains a high F-score (?95% with
all learners). The F-score for Check Question ranges from 73% yielded by MaxEnt to
90% obtained with SLIPPER. The high score of Plain Acknowledgment combined with
its high probability raises the overall performance of the systems almost four points
over the results obtained in the previous experiment: from ?87% to ?92% weighted
F-score. The improvement with respect to the baselines, however, is not as large: we
now obtain a 55% improvement over the simple majority class baseline (from 35.31% to
92.21%), whereas in the experiment with the restricted data set the improvement with
respect to the majority class baseline is 81% (from 6.67% to 87.75% weighted F-score.).
Table 12 shows a comparison of all weighted F-scores obtained in this second
experiment.
It is interesting to note that even though the overall performance of the algorithms
is slightly higher than before (due to the reasons mentioned previously), the scores for
some NSU classes are actually lower. The most striking cases are perhaps the classes
Helpful Rejection and Conjunct, for which the maximum entropy model now gives null
scores (see Appendix C). We have already pointed out the problems encountered with
Helpful Rejection. As for the class Conjunct, although it yields good results with the
other learners, the proportion of this class (0.4%, 5 instances only) is now probably too
low to obtain reliable results.
A more interesting case is the class Affirmative Answer, which in TiMBL goes down
more than 10 points (from 93.61% to 82.42% F-score). The tree in Figure 7 provides a
clue to the reason for this. When the NSU contains a yes-word (second branch of the
tree) the class with the highest probability is now Plain Acknowledgment, instead of
Plain Affirmative Answer as before (see tree in Figure 3). This is due to the fact that,
Figure 9
Node on a tree using NSU and antecedent features.
420
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Table 12
Comparison of weighted F-scores.
System Weighted F-score
Majority class baseline 35.31
One rule baseline 53.03
Four rule baseline (NSU features) 67.99
NSU and antecedent features 85.44
Full feature set:
- J4.8 89.53
- SLIPPER 92.01
- TiMBL 92.02
- MaxEnt 92.21
at least in English, expressions like yeah (considered here as yes-words) are potentially
ambiguous between acknowledgments and affirmative answers.8 This ambiguity and
the problems it entails are also noted by Schlangen (2005), who addresses the problem
of identifying NSUs automatically. As he points out, the ambiguity of yes-words is
one of the difficulties encountered when trying to distinguish between backchannels
(plain acknowledgments in our taxonomy) and non-backchannel fragments. This is a
tricky problem for Schlangen as his NSU identification procedure does not have access
to the context. Although in the present experiments we do use features that capture
contextual information, determining whether the antecedent utterance is declarative or
interrogative (which one would expect to be the best clue to disambiguate between Plain
Acknowledgement and Plain Affirmative Answer) is not always trivial.
5. Conclusions
In this article we have presented results of several machine learning experiments where
we have used well-known machine learning techniques to address the novel task of
classifying NSUs in dialogue.
We first introduced a comprehensive NSU taxonomy based on corpus work carried
out using the dialogue transcripts of the BNC, and then sketched the approach to NSU
resolution we assume.
We then presented a pilot study focused on sluices, one of the NSU classes in our
taxonomy. We analyzed different sluice interpretations and their distributions in a small
corpus study and reported on a machine learning experiment that concentrated on
the task of disambiguating between sluice readings. This showed that the observed
correlations between sluice type and preferred interpretation are a good rough guide
for predicting sluice readings, which yields a 72% weighted F-score. Using a small set
of features that refer to properties of the antecedent utterance, we were able to improve
this result by 8%.
In the second part of this article we extended the machine learning approach used
in the sluicing experiment to the full range of NSU classes in our taxonomy. In order
to work with a more balanced set of data, the first run of this second experiment was
carried out using a restricted data set that excluded the classes Plain Acknowledgment
8 Arguably this ambiguity would not arise in French given that, according to Beyssade (2005), in French
the expressions used to acknowledge an assertion are different from those used in affirmative answers to
polar questions.
421
Computational Linguistics Volume 33, Number 3
and Check Question. We identified a small set of features that capture properties of
the NSUs, their antecedents and relations between them, and employed a series of
simple baseline methods to evaluate the classification task. The most successful of these
consists of a four-rule decision tree that only uses features related to properties of
the NSUs themselves. This gives a 62% weighted F-score. Not surprisingly, with this
baseline very high scores (over 95%) could be obtained for NSU classes that are defined
in terms of lexical or construction types, like Sluice and Propositional/Factual Modifier.
We then applied four learning algorithms to the data set annotated with all features
and improved the result of the four-rule baseline by 25%, obtaining a weighted F-score
of around 87% for all learners. The experiment showed that the classes that are most
difficult to identify are those that rely on relational features, like Repeated Affirmative
Answer and especially Helpful Rejection.
In a second run of the experiment we incorporated the instances classified as Plain
Acknowledgment and Check Question in the data set and ran the machine learners
again. The results achieved are very similar to those obtained in the previous run, if
slightly higher due to the high probability of the class Plain Acknowledgment. The
experiment did show however a potential confusion between Plain Acknowledgment
and Plain Affirmative Answer (observed elsewhere in the literature) that obviously had
not shown up in the previous run.
As typically different NSU classes are subjected to different resolution constraints,
identifying the correct NSU class is a necessary step towards the goal of fully processing
NSUs in dialogue. Our results show that, for the taxonomy we have considered, this task
can be successfully learned.
There are, however, several aspects that deserve further investigation. One of them
is the choice of features employed to characterize the utterances. In this case we have
opted for rather high-level features instead of using simple surface features, as is com-
mon in robust approaches to language understanding. As pointed out by an anonymous
reviewer, it would be worth exploring to what extent the performance of our current
approach could be improved by incorporating more low-level features, for instance by
the presence of closed-class function words.
Besides identifying the right NSU class, the processing and resolution of NSUs in-
volves other tasks that have not been addressed in this article and that are subjects of our
future research. For instance, we have abstracted here from the issue of distinguishing
NSUs from other sentential utterances. In our experiments the input fed to the learners
was in all cases a vector of features associated with an utterance that had already been
singled out as an NSU. Deciding whether an utterance is or is not an NSU is not an easy
task. This has for instance been addressed by Schlangen (2005), who obtains rather low
scores (42% F-measure). There is therefore a lot of room for improvement in this respect,
and indeed in the future we plan to explore ways of combining the classification task
addressed here with the NSU identification task.
Identifying and classifying NSUs are necessary conditions for resolving them. In
order to actually resolve them, however, the output of the classifier needs to be fed into
some extra module that takes care of this task. A route we plan to take in the future
is to integrate our classification techniques with the information state-based dialogue
system prototype CLARIE (Purver 2004a), which implements a procedure for NSU
resolution based on the theoretical assumptions sketched in Section 2.2. The taxonomy
which we have tested and presented here will provide the basis for classifying NSUs in
this dialogue processing system. The classification system will determine the templates
and procedures for interpretation that the system will apply to an NSU once it has
recognized its fragment type.
422
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Appendix A: Detailed ML Results for the Sluice Reading Classification Task
Learner Sluice Reading Recall Precision F1
Weka?s J4.8 Direct 71.70 79.20 75.20
Reprise 85.70 83.70 84.70
Clarification 100.00 68.60 81.40
Wh anaphor 66.70 100.00 80.00
weighted score 81.47 82.14 81.80
SLIPPER Direct 81.01 71.99 76.23
Reprise 83.85 86.49 85.15
Clarification 71.17 94.17 81.07
Wh anaphor 77.78 62.96 69.59
weighted score 81.81 81.43 81.62
TiMBL Direct 78.72 75.24 76.94
Reprise 83.08 83.96 83.52
Clarification 75.83 68.83 72.16
Wh anaphor 55.56 77.78 64.81
weighted score 79.85 79.98 79.80
MaxEnt Direct 65.22 75.56 70.01
Reprise 85.74 76.38 80.79
Clarification 89.17 70.33 78.64
Wh anaphor 0.00 0.00 0.00
weighted score 75.38 76.93 73.24
423
Computational Linguistics Volume 33, Number 3
Appendix B: Detailed ML Results for the Restricted NSU Classification Task
Weka?s J4.8 SLIPPER
NSU Class Recall Precision F1 Recall Precision F1
CE 97.00 97.00 97.00 93.64 97.22 95.40
Sluice 100.00 95.20 97.60 96.67 91.67 94.10
ShortAns 89.60 82.60 86.00 83.93 82.91 83.41
AffAns 92.00 95.80 93.90 93.13 91.63 92.38
Reject 95.80 80.70 87.60 83.60 100.00 91.06
RepAffAns 68.00 63.00 65.40 53.33 61.11 56.96
RepAck 85.00 89.50 87.20 85.71 89.63 87.62
HelpReject 22.20 33.30 26.70 28.12 20.83 23.94
PropMod 100.00 100.00 100.00 100.00 90.00 94.74
FactMod 100.00 100.00 100.00 100.00 100.00 100.00
BareModPh 80.00 100.00 88.90 100.00 80.56 89.23
ConjFrag 100.00 71.40 83.30 100.00 100.00 100.00
Filler 56.30 100.00 72.00 100.00 62.50 76.92
weighted score 87.62 87.68 87.29 86.21 86.49 86.35
TiMBL MaxEnt
NSU Class Recall Precision F1 Recall Precision F1
CE 94.37 91.99 93.16 96.11 96.39 96.25
Sluice 94.17 91.67 92.90 100.00 95.83 97.87
ShortAns 88.21 83.00 85.52 89.35 83.59 86.37
AffAns 92.54 94.72 93.62 92.79 97.00 94.85
Reject 95.24 81.99 88.12 100.00 81.13 89.58
RepAffAns 63.89 60.19 61.98 68.52 65.93 67.20
RepAck 86.85 91.09 88.92 84.52 81.99 83.24
HelpReject 35.71 45.24 39.92 5.56 77.78 10.37
PropMod 90.00 100.00 94.74 100.00 100.00 100.00
FactMod 97.22 100.00 98.59 97.50 100.00 98.73
BareModPh 80.56 100.00 89.23 69.44 100.00 81.97
ConjFrag 100.00 100.00 100.00 100.00 100.00 100.00
Filler 48.61 91.67 63.53 62.50 90.62 73.98
weighted score 86.71 87.25 86.66 87.11 88.41 87.75
424
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
Appendix C: Detailed ML Results for the Full NSU Classification Task
Weka?s J4.8 SLIPPER
NSU Class Recall Precision F1 Recall Precision F1
Ack 95.00 96.80 95.90 96.67 95.71 96.19
CheckQu 100.00 83.30 90.90 86.67 100.00 92.86
CE 92.40 95.30 93.80 96.33 93.75 95.02
Sluice 100.00 95.20 97.60 94.44 100.00 97.14
ShortAns 83.00 80.70 81.90 85.25 84.46 84.85
AffAns 86.00 82.70 84.30 82.79 87.38 85.03
Reject 100.00 76.20 86.50 77.60 100.00 87.39
RepAffAns 68.00 65.40 66.70 67.71 72.71 70.12
RepAck 86.30 84.10 85.20 84.04 92.19 87.93
HelpReject 33.30 46.20 38.70 29.63 18.52 22.79
PropMod 60.00 100.00 75.00 100.00 100.00 100.00
FactMod 91.30 100.00 95.50 100.00 100.00 100.00
BareModPh 70.00 100.00 82.40 83.33 69.44 75.76
ConjFrag 100.00 71.40 83.30 100.00 100.00 100.00
Filler 37.50 50.00 42.90 70.00 56.33 62.43
weighted score 89.67 89.78 89.53 91.57 92.70 92.01
TiMBL MaxEnt
NSU Class Recall Precision F1 Recall Precision F1
Ack 95.71 95.58 95.64 95.54 94.59 95.06
CheckQu 77.78 71.85 74.70 63.89 85.19 73.02
CE 93.32 94.08 93.70 88.89 94.44 91.58
Sluice 100.00 94.44 97.14 88.89 94.44 91.58
ShortAns 87.79 88.83 88.31 88.46 84.91 86.65
AffAns 85.00 85.12 85.06 86.83 81.94 84.31
Reject 98.33 80.28 88.39 100.00 78.21 87.77
RepAffAns 58.70 55.93 57.28 69.26 62.28 65.58
RepAck 86.11 80.34 83.12 86.95 77.90 82.18
HelpReject 22.67 40.00 28.94 00.00 00.00 00.00
PropMod 100.00 100.00 100.00 44.44 100.00 61.54
FactMod 97.50 100.00 98.73 93.33 100.00 96.55
BareModPh 69.44 83.33 75.76 58.33 100.00 73.68
ConjFrag 100.00 100.00 100.00 00.00 00.00 00.00
Filler 44.33 55.00 49.09 62.59 100.00 76.99
weighted score 91.49 90.75 91.02 91.96 93.17 91.21
425
Computational Linguistics Volume 33, Number 3
Acknowledgments
This work was funded by grant
RES-000-23-0065 from the Economic and
Social Council of the United Kingdom and it
was undertaken while all three authors were
members of the Department of Computer
Science at King?s College London. We wish
to thank Lief Arda Nielsen and Matthew
Purver for useful discussion and suggestions
regarding machine learning algorithms. We
are grateful to two anonymous reviewers for
very helpful comments on an earlier draft of
this article. Their insights and suggestions
have resulted in numerous improvements.
Of course we remain solely responsible for
the ideas presented here, and for any errors
that may remain.
References
Beyssade, Claire and Jean-Marie Marandin.
2005. Contour meaning and dialogue
structure. Paper presented at the
workshop Dialogue Modelling and
Grammar, Paris, France.
Burnard, Lou. 2000. Reference Guide for the
British National Corpus (World Edition).
Oxford University Computing Services.
Available from ftp:
//sable.ox.ac.uk/pub/ota/BNC/.
Clark, Herbert H. 1996. Using Language.
Cambridge University Press, Cambridge.
Cohen, William and Yoram Singer. 1999.
A simple, fast, and effective rule learner.
In Proceedings of the 16th National
Conference on Artificial Intelligence,
pages 335?342, Orlando, FL.
Daelemans, Walter, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 2003.
TiMBL: Tilburg Memory-Based Learner,
v. 5.0, Reference Guide. Technical
Report ILK-0310, University of
Tilburg.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
Ferna?ndez, Raquel. 2006. Non-Sentential
Utterances in Dialogue: Classification,
Resolution and Use. Ph.D. thesis,
Department of Computer Science, King?s
College London, University of London.
Ferna?ndez, Raquel and Jonathan Ginzburg.
2002. Non-sentential utterances: A corpus
study. Traitement automatique des languages,
43(2):13?42.
Ferna?ndez, Raquel, Jonathan Ginzburg,
Howard Gregory, and Shalom Lappin.
In press. SHARDS: Fragment resolution
in dialogue. In H. Bunt and R. Muskens,
editors, Computing Meaning, volume 3.
Kluwer, Amsterdam.
Ferna?ndez, Raquel, Jonathan Ginzburg, and
Shalom Lappin. 2004. Classifying Ellipsis
in Dialogue: A Machine Learning
Approach. In Proceedings of the 20th
International Conference on Computational
Linguistics, pages 240?246, Geneva,
Switzerland.
Garside, Roger. 1987. The CLAWS
word-tagging system. In R. Garside,
G. Leech, and G. Sampson, editors, The
Computational Analysis of English: A
Corpus-Based Approach. Longman, Harlow,
pages 30?41.
Ginzburg, Jonathan. 1996. Interrogatives:
Questions, facts, and dialogue. In Shalom
Lappin, editor, Handbook of Contemporary
Semantic Theory. Blackwell, Oxford,
pages 385?422.
Ginzburg, Jonathan. 1999. Ellipsis resolution
with syntactic presuppositions. In
H. Bunt and R. Muskens, editors,
Computing Meaning: Current Issues in
Computational Semantics. Kluwer,
Amsterdam, pages 255?279.
Ginzburg, Jonathan. 2005. Abstraction and
ontology: Questions as propositional
abstracts in type theory with records.
Journal of Logic and Computation,
2(15):113?118.
Ginzburg, Jonathan. forthcoming. Semantics
and Interaction in Dialogue. CSLI
Publications and University of Chicago
Press, Stanford, California. Draft chapters
available from http://www.dcs.kcl.
ac.uk/staff/ginzburg.
Ginzburg, Jonathan and Robin Cooper.
2004. Clarification, ellipsis, and the nature
of contextual updates. Linguistics and
Philosophy, 27(3):297?366.
Ginzburg, Jonathan, Howard Gregory,
and Shalom Lappin. 2001. SHARDS:
Fragment resolution in dialogue. In
H. Bunt, I. van der Suis, and E. Thijsse,
editors, Proceedings of the Fourth
International Workshop on Computational
Semantics, pages 156?172, Tilburg,
The Netherlands.
Ginzburg, Jonathan and Ivan Sag. 2001.
Interrogative Investigations. CSLI
Publications, Stanford, California.
Larsson, Staffan. 2002. Issue-based Dialogue
Management. Ph.D. thesis, Go?teborg
University, Sweden.
Larsson, Staffan, Peter Ljunglo?f, Robin
Cooper, Elisabet Engdahl, and Stina
Ericsson. 2000. GoDiS: An accommodating
dialogue system. In Proceedings of
426
Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogue
ANLP/NAACL-2000 Workshop on
Conversational Systems, pages 7?10,
Seattle, WA.
Le, Zhang. 2003. Maximum entropy
modeling toolkit for Python and
C++. Available from http://
homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
Leech, Geoffrey, Roger Garside, and
Michael Bryant. 1994. The large-scale
grammatical tagging of text: Experience
with the British National Corpus. In
N. Oostdijk and P. de Haan, editors,
Corpus-Based Research into Language.
Rodopi, Amsterdam, pages 47?63.
Malouf, Robert. 2002. A comparision of
algorithm for maximum entropy
parameter estimation. In Proceedings
of the Sixth Conference on Natural
Language Learning, pages 49?55, Taipei,
Taiwan.
Purver, Matthew. 2001. SCoRE: A tool for
searching the BNC. Technical Report
TR-01-07, Department of Computer
Science, King?s College London.
Purver, Matthew. 2004a. CLARIE: The
Clarification Engine. In J. Ginzburg and
E. Vallduv??, editors, Proceedings of the 8th
Workshop on the Semantics and Pragmatics of
Dialogue (Catalog), pages 77?84, Barcelona,
Spain.
Purver, Matthew. 2004b. The Theory and
Use of Clarification Requests in Dialogue.
Ph.D. thesis, King?s College, University
of London.
Schlangen, David. 2003. A Coherence-Based
Approach to the Interpretation of
Non-Sentential Utterances in Dialogue. Ph.D.
thesis, University of Edinburgh, Scotland.
Schlangen, David. 2005. Towards finding and
fixing fragments: Using ML to identify
non-sentential utterances and their
antecedents in multi-party dialogue. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 247?254, Ann Arbor, MI.
Schlangen, David and Alex Lascarides. 2003.
The interpretation of non-sentential
utterances in dialogue. In Proceedings of the
4th SIGdial Workshop on Discourse and
Dialogue, pages 62?71, Sapporo, Japan.
Traum, David. 1994. A Computational Theory
of Grounding in Natural Language
Conversation. Ph.D. thesis, University of
Rochester, Department of Computer
Science, Rochester, NY.
Witten, Ian H. and Eibe Frank. 2000.
Data Mining: Practical Machine
Learning Tools with Java Implementations.
Morgan Kaufmann, San Francisco.
Available from http://
www.cs.waikato.ac.nz/ml/weka.
427

Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 28?36,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Statistical Representation of Grammaticality Judgements: the Limits of
N-Gram Models
Alexander Clark, Gianluca Giorgolo, and Shalom Lappin
Department of Philosophy, King?s College London
firstname.lastname@kcl.ac.uk
Abstract
We use a set of enriched n-gram models to track
grammaticality judgements for different sorts of
passive sentences in English. We construct these
models by specifying scoring functions to map the
log probabilities (logprobs) of an n-gram model for
a test set of sentences onto scores which depend
on properties of the string related to the parame-
ters of the model. We test our models on classifica-
tion tasks for different kinds of passive sentences.
Our experiments indicate that our n-gram models
achieve high accuracy in identifying ill-formed pas-
sives in which ill-formedness depends on local rela-
tions within the n-gram frame, but they are far less
successful in detecting non-local relations that pro-
duce unacceptability in other types of passive con-
struction. We take these results to indicate some of
the strengths and the limitations of word and lexical
class n-gram models as candidate representations of
speakers? grammatical knowledge.
1 Introduction
Most advocates (Pereira, 2000; Bod et al, 2003)
and critics (Chomsky, 1957; Fong et al, 2013) of a
probabilistic view of grammatical knowledge have
assumed that this view identifies the grammatical
status of a sentence directly with the probability of
its occurrence. By contrast, we seek to character-
ize grammatical knowledge statistically, but with-
out reducing grammaticality directly to probabil-
ity. Instead we specify a set of scoring procedures
for mapping the logprob value of a sentence into
a relative grammaticality score, on the basis of the
properties of the sentence and of the logprobs that
an n-gram word model generates for the corpus
containing the sentence. A scoring procedure in
this set generates scores in terms of which we con-
struct a grammaticality classifier, using a param-
eterized standard deviation from the mean value.
The classifier provides a procedure for testing the
accuracy of different scoring criteria in separat-
ing grammatical from ungrammatical passive sen-
tences.
We evaluate this approach by applying it to
the task of distinguishing well and ill-formed sen-
tences with passive constructions headed by four
different sorts of verbs: intransitives (appear,
last), pseudo-transitives, which take a restricted
set of notional objects (laugh a hearty laugh,
weigh 10 kg), ambiguous transitives, which allow
both agentive and thematic subjects (the jeans /
the tailor fitted John), and robust transitives that
passivize freely (write, move). Intransitives and
pseudo-transitives generally yield ill-formed pas-
sives. Passives formed from ambiguous transitives
tend to be well-formed only on the agentive read-
ing. Robust transitives, for the most part, yield
acceptable passives, even if they are semantically
(or pragmatically) odd.
Experimenting with several scoring procedures
and alternative values for our standard deviation
parameter, we found that our classifier can distin-
guish pairwise between elements of the first two
classes of passives and those of the latter two with
a high degree of accuracy. However, its perfor-
mance is far less reliable in identifying the differ-
ence between ambiguous and robust transitive pas-
sives. The first classification task relies on local
lexical patterns that can be picked up by n-gram
models, while the second requires identification of
anomalous relations between passivized verbs and
by-phrases, which are not generally accessible to
measurement within the range of an n-gram.
We also observed that as we increased the size
of the training corpus, the performance of our en-
riched models on the classification task also in-
creased. This result suggests that better n-gram
language models are more sensitive to the sorts of
patterns that our scoring procedures rely on to gen-
erate accurate grammaticality classifications.
We note the important difference between
28
grammaticality and acceptability. Following stan-
dard assumptions, we take grammaticality to be
a theoretical notion, and acceptability to be an
empirically testable property. Acceptability is, in
part, determined by grammaticality, but also by
factors such as sentence length, processing limi-
tations, semantic acceptability and many other el-
ements. Teasing apart these two concepts, and ex-
plicating their precise relationship raises a host of
subtle methodological issues that we will not ad-
dress here. Oversimplifying somewhat, we are try-
ing to reconstruct a gradient notion of grammati-
cality which is derived from probabilistic models,
that can serve as a core component of a full model
of acceptability.
We distinguish our task from the standard task
of error detection in NLP (e.g. Post (2011)),
that can be used in various language processing
systems, such as machine translation (Pauls and
Klein, 2012), language modeling and so on. In
error detection, the problem is a supervised learn-
ing task. Given a corpus of examples labeled as
grammatical or ungrammatical, the problem is to
learn a classifier to distinguish them. We use su-
pervised learning as well, but only to measure the
upper bound of an unsupervised learning method.
We assume that native speakers do not, in general,
have access to systematic sets of ungrammatical
sentences that they can use to calibrate their judge-
ment of acceptability. Rather ungrammatical sen-
tences are unusual or unlikely. However, we use
some ungrammatical sentences to set an optimal
threshold for our scoring procedures.
2 Enriched N-Gram Language Models
We assume that we have some high quality lan-
guage model which defines a probability distri-
bution over whole sentences. As has often been
noted, it is not possible to reduce grammatical-
ity directly to a probability of this type, for sev-
eral reasons. First, if one merely specifies a fixed
probability value as a threshold for grammatical-
ity, where strings are deemed to be grammatical
if and only if their probability is higher than the
threshold, then one is committed to the existence
of only a finite number of grammatical sentences.
The probabilities of the possible strings of words
in a language sum to 1, and so at most 1/ sen-
tences can have a probability of at least . Second,
probability can be affected by factors that do not
influence grammaticality. For example, the word
?yak? is rarer (and therefore less probable) than the
word ?horse?, but this does not affect the relative
grammaticality of ?I saw a horse? versus ?I saw a
yak?. Third, a short ungrammatical sentence may
have a higher probability than a long grammatical
sentence with many rare words.
In spite of these arguments against a naive re-
duction of grammaticality, probabilistic inference
does play a role in linguistic judgements, as in-
dicated by the fact that they are often gradient.
Probabilistic inference is pervasive throughout all
domains of cognition (Chater et al, 2006), and
therefore it is plausible to assume that knowledge
of language is also probabilistic in nature. More-
over language models do seem to play a crucial
role in speech recognition and sentence process-
ing. Without them we would not be able to under-
stand speech in a noisy environment.
We propose to accommodate these different
considerations by using a scoring function to map
probabilities to grammaticality rankings. This
function does not apply directly to probabilities,
but rather to the parameters of the language model.
The probability of a particular sentence with re-
spect to a log-linear language model will be the
product of certain parameters: in log space, the
sum. We define scores that operate on this collec-
tion of parameters.
2.1 Scores
We have experimented with scores of two differ-
ent types that correlate with the grammaticality
of a sentence. Those of the first type are dif-
ferent implementations of the idea of normaliz-
ing the logprob assigned by an n-gram model to
a string by eliminating the significance of factors
that do not influence the grammatical status of a
sentence, such as sentence length and word fre-
quency. Scores of the second type are based on the
intuition that the (un)grammaticality of a sentence
is largely determined by its problematic compo-
nents. These scores are functions of the lowest
scoring n-grams in the sentence.
Mean logprob (ML) This score is the logprob
of the entire sentence divided by the length of the
sentence, or equivalently the mean of the logprobs
for the single trigrams:
ML = 1n logPTRIGRAM(?w1, . . . , wn?)
By normalizing the logprob for the entire sentence
by its length we eliminate the effect of sentence
length on the acceptability score.
29
Weighted mean logprob (WML) This score is
calculated by dividing the logprob of the entire
sentence by the sum of the unigram probabilities
of the lexical items that compose the sentence:
WML = logPTRIGRAM(?w1,...,wn?)logPUNIGRAM(?w1,...,wn?)
This score eliminates at the same time the effect of
the length of the sentence and the lower probabil-
ity assigned to sentences with rare lexical items.
Synctactic log odds ratio (SLOR) This score
was first used by Pauls and Klein (2012) and
performs a normalization very similar to WML
(we will see below that in fact the two scores are
basically equivalent):
SLOR =
logPTRIGRAM(?w1,...,wn?)?logPUNIGRAM(?w1,...,wn?)
n
Minimum (Min) This score is equal to the low-
est logprob assigned by the model to the n-grams
of the sentence divided by the unigram logprob of
the lexical item heading the n-gram:
Min = mini
[
logP (wi|wi?2wi?1)
logP (wi)
]
In this way, if a single n-gram is assigned a low
probability (normalized for the frequency of its
head lexical item), then this low score is in some
sense propagated to the whole sentence.
Mean of the first quartile (MFQ) This score
is a generalization of the Min score. We order
the single n-gram logprobs from the lowest to the
highest, and we consider the first (lowest) quar-
tile. We then normalize the logprobs for these n-
grams by the unigram probability of the head lex-
ical item, and we take the mean of these scores.
In this way we obtain a score that is more robust
than the simple Min, as, in general, a grammatical
anomaly influences the logprob of more than one
n-gram.
2.2 N-Gram Models
We are using n-gram models on the understand-
ing that they are fundamentally inadequate for de-
scribing natural languages in their full syntactic
complexity. In spite of their limitations, they are a
good starting point, as they perform well as lan-
guage models across a wide range of language
modeling tasks. They are easy to train, as they
do not require annotated training data.
We do not expect that our n-gram based gram-
maticality scores will be able to idenitfy all of the
cases of ungrammaticality that we encounter. Our
working hyposthesis is that they can capture cases
of ill-formedness that depend on local factors, that
can be identified within n-gram frames, as op-
posed to those which involve non-local relations.
If these models can detect local grammaticality vi-
olations, then we will have a basis for thinking
that richer, more structured language models can
recognize non-local as well as local sources of un-
grammaticality.
3 Experiments with Passives
Rather than trying to test the performance of these
models over all types of ungrammaticality, we
limit ourselves to a case study of the passive. By
tightly controlling the verb types and grammat-
ical construction to which we apply our models
we are better able to study the power and the lim-
its of these models as candidate representations of
grammatical knowledge.
3.1 Types of Passives
Our controlled experiments on passives are, in
part, inspired by speakers? judgments discussed in
Ambridge et al (2008). Their experimental work
measures the acceptability of various passive sen-
tences.
The active-passive alternation in English is ex-
emplified by the pair of sentences
? John broke the window.
? The window was broken by John.
The acceptability of the passive sentence de-
pends largely on lexical properties of the verb.
Some verbs do not allow the formation of the pas-
sive, as in the case of pure intransitive verbs like
appear, discussed below, which permit neither the
active transitive, nor the passive.
We conducted some prelimiary experiments,
not reported here, on modelling the data on pas-
sives from recent work in progress that Ben Am-
bridge and his colleagues are doing, and which
he was kind enough to make available to us. We
observed that the scores we obtained for our lan-
guage models did not fully track these judgements,
but we did notice that we obtained much better
correlation at the low end of the judgment distri-
bution. In Ambridge?s current data this judgement
range corresponds to passives constructed with in-
transitive verbs.
The Ambridge data indicates that the capacity
of verbs to yield well-formed passive verb phrases
30
forms a continuum. Studying the judgement pat-
terns in this data we identified four reasonably
salient points along this hierarchial continuum.
First, at the low end, we have intransitives
like appear: (*John appeared the book. *The
book was appeared). Next we have what may be
described as pseudo-transitives verbs like laugh,
which permit only notional NP objects and do not
easily passivize (Mary laughed a hearty laugh/*a
joke. ?A hearty laugh/*A joke was laughed by
Mary) above them. These are followed by cases
of ambiguous transitives like fit, which, in active
form, carry two distinct readings that correspond
to an agentive and a thematic subject, respectively.
? The tailor fitted John for a new suit.
? The jeans fitted John
Only the agentive reading can be passivized.
? John was fitted by the tailor.
? *John was fitted by the jeans.
Finally, the most easily passivized verbs are ro-
bust transitives, which take the widest selection of
NP subjects in passive form (John wrote the book.
The book was written by John).
This continuum causes well-formedness in pas-
sivization to be a gradient property, as the Am-
bridge data illustrates. Passives tend to be more
or less acceptable along this spectrum. The gradi-
ence of acceptability for passives implies the par-
tial overlap of the score distributions for the differ-
ent types of passives that our experiments show.
The experiments were designed to test our hy-
pothesis that n-gram based language models are
capable of detecting ungrammatical patterns only
in cases where they do not depend on relations
between words that cross the n-word boundary
applied in training. Therefore we expect such a
model to be capable of detecting the ungrammati-
cality of a sentence like A horrible death was died
by John, because the trigrams death was died, was
died by and died by John are unlikely to appear
in any corpus of English. On the other hand, we
do not expect a trigram model to store the infor-
mation necessary to identify the relative anomaly
of a sentence like Two hundred people were held
by the theater, because all the trigrams (as well as
the bigrams and the unigrams) that constitute the
sentence are likely to appear with reasonable fre-
quency in a large corpus of English.
The experiments generalize this observation
and test the performance of n-gram models on a
wider range of verb types. To quantify the per-
formance of the different models we derive simple
classifiers using the scores we have defined and
testing them in a binary classification task. This
task measures the ability of the classifier to dis-
tinguish between grammatical sentences, and sen-
tences containing different types of grammatical
errors.
The models are trained in an unsupervised man-
ner using only corpus data, which we assume to be
uniformly grammatical. In order to evaluate the
scoring methods, we use some supervised data to
set the optimal value of a simple threshold. This is
not however a supervised classification task: we
want to see how well the scores could be used
to separate grammatical and ungrammatical data,
and though unorthodox, this seems a more direct
way of measuring this conditional property than
stipulating some fixed threshold.
3.2 Training data
We used the British National Corpus (BNC) (BNC
Consortium, 2007) to obtain our training data. We
trained six different language models, using six
different subcorpora of the BNC. The first model
used the entire collection of written texts anno-
tated in the BNC, for a total of approximately 100
million words. The other models were trained on
increasingly smaller portions of the written texts
collection: 40 million words, 30 million words, 15
million words, 7.6 million words, and 3.8 million
words. We constructed these corpora by randomly
sampling an appropriate number of complete sen-
tences.
All models were trained on word sequences.
For smoothing the n-gram probability distribu-
tions we used Kneser-Ney interpolation, as de-
scribed in Goodman (2001).
3.3 Test data
We constructed the test data for our hypothesis in
a controlled fashion. We first compiled a list of
verbs for each of the four verb types that we con-
sider (intransitives, pseudo-transitives, ambiguous
transitives, and robust transitives). We selected
verbs from the BNC that appeared at least 100
times in their past participle form in the entire cor-
pus in order to ensure a sufficient number of pas-
31
sive uses in the training data.1 We selected 40 in-
transitive verbs, 13 pseudo transitives, 23 ambigu-
ous transitives and 40 transitive verbs. To clas-
sify the verbs we relied on our intuitions as native
speakers of English.
Using these lists we automatically generated
four corpora by selecting an agent and a patient
from a predefined pool of NPs, randomly select-
ing a determiner (if necessary) and a number (if
the NP allows plurals). The resulting corpora are
of the following sizes:
? intransitive verbs ? 24480 words, 3240 sen-
tences,
? pseudo transitive verbs ? 7956 words, 1053
sentences,
? ambiguous transitive verbs ? 14076 words,
1863 sentences,
? robust transitive verbs ? 24480 words, 3240
sentences.
Each corpus was evaluated by the six models.
We computed our derived scores for each sentence
on the basis of the logprobs that the language mod-
els assigns.
3.4 Binary classifiers
For each model and for each score we constructed
a set of simple binary classifiers on the basis of
the results obtained for the transitive verb corpus.
We took the mean of each score assigned by the
model to the transitive sentences, and we set dif-
ferent thresholds by subtracting from this value
a number of standard deviations ranging from 0
to 2.75. The rationale behind these classifiers is
that, assuming the passives of the robust transi-
tives to be grammatical, the scores for the other
cases should be comparatively lower. Therefore
by setting a threshold ?to the left? of the mean we
should be able to distinguish between grammati-
cal sentences, whose score is to the right of the
threshold, and ungrammatical ones, expected to a
have a score lower than the threshold. Formally
the classifier is defined as follows:
cs(w) =
{
+ if s(w) ? m? S ? ?
? otherwise
(1)
1Notice that in most cases the past participle form is the
same as the simple past form, and for this reason we set the
threshold to such a high value.
where s is one of our scores, w is the sentence to
be classified, s(w) represents the value assigned
by the score to sentence w, m is the mean for
the score in the transitive condition, ? is the stan-
dard deviation for the score again in the transitive
condition, and S is a factor by which we move
the threshold away from the mean. The classi-
fier assigns the grammatical (+) tag only to those
sentences that are assigned values higher than the
threshold m? S ? ?.
Alternatively in terms of the widely used z-
score, defined as zs(w) = (s(w) ?m)/? we can
say that w is classified as grammatical iff zs(w) ?
?S.
4 Results
For reasons of space we will limit the presenta-
tion of our detailed results to the 100 million word
model, as it offers the sharpest effects. We will,
however, also report comparisons on the most im-
portant metrics for the complete set of models.
In Figure 1 we show the distribution of the five
scores for the four different corpora (transitive,
ambiguous, pseudo, and intransitive) obtained us-
ing the 100 million word model. In all cases we
observe the same general pattern: the sentences in
the corpus generated with robust transitives are as-
signed comparatively high scores, and these grad-
ually decrease when we consider the ambiguous,
the pseudo and the intransitive conditions. Inter-
estingly, this order reflects the degree of ?transi-
tivity? that these verb types exhibit. Notice, how-
ever, that the four conditions seem to group into
two different macro-distributions. On the right
we have the transitive-ambiguous sentences and
on the left the pseudo-intransitive cases. This par-
tially confirms our hypothesis that n-gram mod-
els have problems recognizing lexical dependen-
cies that determine the felicitousness of passives
constructed using ambiguous transitive verbs, as
these are, for the most part, non-local. Neverthe-
less, it is important to note that the overlap of the
distributions for these two cases is also due to the
fact that many cases in the ambiguous transitive
corpus are indeed grammatical.
Figure 2 summarizes the (balanced) accuracies
obtained by our classifiers for each comparison,
by each model. These results confirm our hy-
pothesis that the classifiers tend to perform better
when distinguishing passive sentences constructed
with a robust transitive verbs from those headed by
32
Logprob ML WML
SLOR Min MFQ
0.00
0.05
0.10
0.15
0.0
0.5
1.0
1.5
2.0
0
2
4
6
0.0
0.5
1.0
1.5
2.0
0
2
4
6
0
1
2
3
4
?30 ?25 ?20 ?15 ?2.5 ?2.0 ?1.5 ?1.0 ?0.9 ?0.8 ?0.7 ?0.6 ?0.5
0.0 0.5 1.0 ?1.75 ?1.50 ?1.25 ?1.00 ?0.75 ?1.75 ?1.50 ?1.25 ?1.00 ?0.75
den
sity
condition
transitive
ambiguous
pseudo
intransitive
Figure 1: Distributions of the six scores Logprob, ML, WML, SLOR, Min and MFQ for the four differ-
ent conditions (robust transitive passives, ambiguous transitive passives, pseudo transitive passives and
intransitive passives) for the 100 million words language model.
pseudo-transitives and intransitives.
In the comparison between transitive and am-
biguous transitive sentences, the classifiers are
?stuck? at around 60% accuracy. Using larger
training corpora produces only a marginal im-
provement. This contrasts with what we observe
for the transitive/pseudo and transitive/intransitive
classification tasks. In the transitive/pseudo task,
we already obtain reasonable accuracy with the
model trained with the smallest BNC subset.
Oddly, the overall best result is achieved with 30
million words, although the result obtained with
the model trained on the full BNC corpus is not
much lower. For the transitive/intransitive classifi-
cation task we observe a much steadier and larger
growth in accuracy, reaching the overall best result
of 85.1%. Table 1 reports the best results for each
comparison by each language model. For each
condition we report the best accuracy obtained, the
corresponding F1 score, the score that achieves the
best result, and the best accuracy obtained by just
using the logprobs. These results are obtained us-
ing different values for the S parameter. However,
in general the best results are obtained when the S
parameter is set to a value in the interval [0.5, 1.5].
In comparing the performance of the individ-
ual scores, we first notice that, while for the tran-
sitive/ambiguous comparison all scores perform
pretty much at the same level, there is a clear hier-
archy between scores for the other comparisons.
We observe that the baseline raw logprob as-
signed by the n-grams models performs much
worse than the scores, resulting in roughly 10%
less accuracy than the best performing score in ev-
ery condition. ML performs slightly better, obtain-
ing around 5% greater accuracy than logprob as a
predictor. This shows that even though the length
of the sentences in our test data is relatively con-
stant (between 9 and 11 words), there is still an
improvement if we take this structural factor into
account. The two scores WML and SLOR display
the same pattern, showing that they are effectively
equivalent. This is not surprising given that they
are designed to modify the raw logprob by tak-
33
transitive
ambiguous transitivepseudo transitiveintransitive
l l l l l l l l l l l l
l l l l l l
l l l l l l l l ll l l l
l l l
l l l
l l
l l l
l l l
l l
l l l l l l l l l l l l
l l l l l l l l l
l l l l l l l
l l l
l
l l l l l l l l
l
l
l l l
l l l l l
l
l
l l l l l l l l l l l l
l l l l l l l l
l l l l l l l
l l
l l
l l l l l l
l l l l
l l l
l l l
l l l l l l l l l l l l
l l l l l l l l l
l l
l l l l l l l l l
l l
l l
l l l l
l l l l
l l l
l l l l l
l
l l l
l l l l l l l l l l l l
l l l l l l l l l l
l l l
l l l
l l l l l l
l l l
l l l
l l l l l
l l l l
l
l
l l l
l l
l
l
l l l l l l l l l l l l
l l l l l l l l l l
l l
l l l
l l l l l l l
l l l l l l l l l l
l
l l
l l l
l l l l l
l l l
l l l l
l l l l l
l
l
l
l l
l
l l l l l
l
l
l
l l
l
l
l
l l l
l l l l l
l l l l
l
l
l
l
l
l
l
l l l
l l
l l l l l
l l
l
l
l l l
l l
l l l
l
l
l
l
l
l l l l l
l
l
l
l
l
l
l l
l l l l l l
l l
l
l
l
l
l
l l
l
l
l
l
l l
l l l l l l
l l l
l l l
l l l l l l
l
l
l
l
l l l l l
l
l
l
l
l
l
l
l
l l l l l l
l l l
l
l
l l
l l l
l
l
l
l
l l
l l
l l l
l l
l
l l
l l
l
l l l l l
l
l
l
l
l l
l
l
l l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l l
l l
l
l
l
l
l
l
l
l l l l
l
l
l l l l l l
l
l l
l l l
l l
l l l l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l l l
l l l
l
l
l
l
l
l l
l
l
l
l l
l l l
l
l
l
l l
l l
l
l l
l l l
l
l
l
l
l
l
l l l l
l
l
l
l
l
l
l l
l l
l l l
l
l
l
l l
l
l l l
l
l
l
l l
l l l l
l
l
l
l l l l l l
l l
l l l l
l l l l l
l
l
l
l l l
l
l l l l
l
l
l
l
l l
l
l
l
l
l l
l l l l l
l
l
l
l
l
l
l l l l
l
l
l l l
l l l l l l l
l l l l l
l l
l l l
l
l
l
l
l
l
l l l l l
l
l
l
l l
l
l l
l l l l l l
l l
l
l
l
l
l
l l l
l
l
l l l
l l l l l l
l
l
l
l l l
l l
l l l
l
l
l
l
l
l
l
l l
l l l
l
l
l
l
l
l
l l
l
l
l
l l l l l
l
l
l
l
l
l
l
l l
l l
l
l
l
l l
l l
l l l l l
l
l
l
l l
l
l l l l l
l
l
l
l
l
l
l
l
l l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l l
l
l l
l
l
l
l
l
l
l l l l
l
l
l
l l
l l
l l l l
l
l
l
l l l
l
l l
l l l
l
l
l
l
l
l
l
l
l l l l
l
l
l
l
l
l
l
l
l l
l
l
l
l l l l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l l
l l l
l
l
l
l
l
l l
l
l
l l l l
l
l
l
l
l
l
l
l l
l l
l
l
l
l
l
l
l
l
l l l l l
l
l
l
l
l
l
l
l
l
l
l l l
l
l
l
l
l
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
3.8M
7.6M
15M
30M
40M
100M
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75S
Bala
nced
 acc
urac
y Scorel
l
l
l
l
l
LogprobMLWMLSLORMinMFQ
Figure 2: Accuracies for the classifiers for each model. S represents the number of standard deviations
?to the left? of the mean of the transitive condition score, used to set the threshold.
ing into account exactly the same factors (length
of the sentence and frequency of the unigrams that
compose the sentence). These two scores perform
generally better in the transitive/ambiguous com-
parison, and they achieve good performance when
the size of the training model is small. However,
for the most part, the two scores derived from the
logprobs of the least probable n-grams in the sen-
tence, Min and MFQ, get the best results. Min
exhibits erratic behavior (mainly due to its non-
normal distribution for each condition, as shown
in figure 1), and it seems to be more stable only
in the presence of a large training set. MFQ has
a much more robust contour, as it is significantly
less dependent on the choice of S.
5 Conclusions and Future Work
In Clark and Lappin (2011) we propose a model
of negative evidence that uses probability of oc-
currence in primary linguistic data as the basis for
estimating non-grammaticality through relatively
34
Model Comparison Best accuracy F1 Best performing score Logprob accuracy
transitive/ambiguous 60.9% 0.7 SLOR 57.3%
3.8M transitive/pseudo 77% 0.81 MFQ 67.6%
transitive/intransitive 73.8% 0.72 SLOR 65.6%
transitive/ambiguous 62.9% 0.68 MFQ 57.8%
7.6M transitive/pseudo 78.5% 0.76 MFQ 69.1%
transitive/intransitive 75.8% 0.72 MFQ 67.3%
transitive/ambiguous 62.3% 0.66 WML 57.8%
15M transitive/pseudo 72.6% 0.78 SLOR 66.5%
transitive/intransitive 79.5% 78.3 MFQ 69.5%
transitive/ambiguous 63.3% 0.75 WML 58.9%
30M transitive/pseudo 83.1% 0.88 Min 71.2%
transitive/intransitive 81.8% 0.82 MFQ 72.2%
transitive/ambiguous 63.8% 0.75 SLOR 59.5%
40M transitive/pseudo 80.1% 0.86 Min 69.7%
transitive/intransitive 83.5% 0.83 SLOR 72.6%
transitive/ambiguous 63.3% 0.75 SLOR 58.4%
100M transitive/pseudo 80.3% 0.9 MFQ 71.3%
transitive/intransitive 85.1% 0.85 SLOR 73.8%
Table 1: Best accuracies
low frequency in a sample of this data. Here we
follow Clark et al (2013) in effectively inverting
this strategy.
We identify a set of scoring functions based on
parameters of probabilistic models that we use to
define a grammaticality threshold, which we use
to classify strings as grammatical or ill-formed.
This model offers a stochastic characterisation of
grammaticality without reducing grammaticality
to probability.
We expect enriched lexical n-gram models of
the kind that we use here to be capable of rec-
ognizing the distinction between grammatical and
ungrammatical sentences when it depends on local
factors within the frame of the n-grams on which
they are trained. We further expect them not to be
able to identify this distinction when it depends on
non-local relations that fall outside of the n-gram
frame.
It might be thought that this hypothesis con-
cerning the capacities and limitations of n-gram
models is too obvious to require experimental sup-
port. In fact, this is not the case. Reali and Chris-
tiansen (2005) show that n-gram models can be
used to distinguish grammatical from ungrammat-
ical auxiliary fronted polar questions with a high
degree of success. More recently Frank et al
(2012) argue for the view that a purely sequen-
tial, non-hierarchical view of linguistic structure is
adequate to account for most aspects of linguistic
knowledge and processing.
We have constructed an experiment with differ-
ent (pre-identified) passive structures that provides
significant support for our hypothesis that lexical
n-gram models are very good at capturing local
syntactic relations, but cannot handle more distant
dependencies.
In future work we will be experimenting with
more expressive language models that can repre-
sent non-local syntactic relations. We will pro-
ceed conservatively by first extending our enriched
lexical n-gram models to chunking models, and
then to dependency grammar models, using only
as much syntactic structure as is required to iden-
tify the judgement patterns that we are studying.
To the extent that this research is successful it
will provide motivation for the view that syntactic
knowledge is inherently probabilistic in nature.
Acknowledgments
The research described in this paper was done in the
framework of the Statistical Models of Grammaticality
(SMOG) project at King?s College London, funded by grant
ES/J022969/1 from the Economic and Social Research Coun-
cil of the UK. We are grateful to Ben Ambridge for providing
us with the data from his experiments and for helpful dis-
cussion of the issues that we address in this paper. We also
thank the three anonymous CMCL 2013 reviewers for useful
comments and suggestions, that we have taken account of in
preparing the final version of the paper.
35
References
Ben Ambridge, Julian M Pine, Caroline F Rowland, and
Chris R Young. 2008. The effect of verb semantic
class and verb frequency (entrenchment) on childrens and
adults graded judgements of argument-structure overgen-
eralization errors. Cognition, 106(1):87?129.
BNC Consortium. 2007. The British National Corpus, ver-
sion 3 (BNC XML Edition). Distributed by Oxford Uni-
versity Computing Services on behalf of the BNC Consor-
tium.
R. Bod, J. Hay, and S. Jannedy. 2003. Probabilistic linguis-
tics. MIT Press.
N. Chater, J.B. Tenenbaum, and A. Yuille. 2006. Probabilis-
tic models of cognition: Conceptual foundations. Trends
in Cognitive Sciences, 10(7):287?291.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
A. Clark and S. Lappin. 2011. Linguistic Nativism and the
Poverty of the Stimulus. Wiley-Blackwell, Malden, MA.
A. Clark, G. Giorgolo, and S. Lappin. 2013. Towards a sta-
tistical model of grammaticality. In Proceedings of the
35th Annual Conference of the Cognitive Science Society.
Sandiway Fong, Igor Malioutov, Beracah Yankama, and
Robert C. Berwick. 2013. Treebank parsing and
knowledge of language. In Aline Villavicencio, Thierry
Poibeau, Anna Korhonen, and Afra Alishahi, editors, Cog-
nitive Aspects of Computational Language Acquisition,
Theory and Applications of Natural Language Processing,
pages 133?172. Springer Berlin Heidelberg.
Stefan Frank, Rens Bod, and Morten Christiansen. 2012.
How hierarchical is language use? In Proceedings of the
Royal Society B, number doi: 10.1098/rspb.2012.1741.
J.T. Goodman. 2001. A bit of progress in language model-
ing. Computer Speech & Language, 15(4):403?434.
A. Pauls and D. Klein. 2012. Large-scale syntactic language
modeling with treelets. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguistics,
pages 959?968. Jeju, Korea.
F. Pereira. 2000. Formal grammar and information theory:
together again? Philosophical Transactions of the Royal
Society of London. Series A: Mathematical, Physical and
Engineering Sciences, 358(1769):1239?1253.
M. Post. 2011. Judging grammaticality with tree substitution
grammar derivations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 217?222.
F. Reali and M.H. Christiansen. 2005. Uncovering the rich-
ness of the stimulus: Structure dependence and indirect
statistical evidence. Cognitive Science, 29(6):1007?1028.
36
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 72?79,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Probabilistic Rich Type Theory for Semantic Interpretation
Robin Cooper
1
, Simon Dobnik
1
, Shalom Lappin
2
, and Staffan Larsson
1
1
University of Gothenburg,
2
King?s College London
{cooper,sl}@ling.gu.se, simon.dobnik@gu.se, shalom.lappin@kcl.ac.uk
Abstract
We propose a probabilistic type theory in which a
situation s is judged to be of a type T with probabil-
ity p. In addition to basic and functional types it in-
cludes, inter alia, record types and a notion of typ-
ing based on them. The type system is intensional
in that types of situations are not reduced to sets
of situations. We specify the fragment of a com-
positional semantics in which truth conditions are
replaced by probability conditions. The type sys-
tem is the interface between classifying situations
in perception and computing the semantic interpre-
tations of phrases in natural language.
1 Introduction
Classical semantic theories (Montague, 1974), as
well as dynamic (Kamp and Reyle, 1993) and un-
derspecified (Fox and Lappin, 2010) frameworks
use categorical type systems. A type T identifies
a set of possible denotations for expressions in T ,
and the system specifies combinatorial operations
for deriving the denotation of an expression from
the values of its constituents.
These theories cannot represent the gradience
of semantic properties that is pervasive in speak-
ers? judgements concerning truth, predication, and
meaning relations. In general, predicates do not
have determinate extensions (or intensions), and
so, in many cases, speakers do not make categor-
ical judgements about the interpretation of an ex-
pression. Attributing gradience effects to perfor-
mance mechanisms offers no help, unless one can
show precisely how these mechanisms produce the
observed effects.
Moreover, there is a fair amount of evidence in-
dicating that language acquisition in general cru-
cially relies on probabilistic learning (Clark and
Lappin, 2011). It is not clear how a reasonable
account of semantic learning could be constructed
on the basis of the categorical type systems that ei-
ther classical or revised semantic theories assume.
Such systems do not appear to be efficiently learn-
able from the primary linguistic data (with weak
learning biases), nor is there much psychological
data to suggest that they provide biologically de-
termined constraints on semantic learning.
A semantic theory that assigns probability
rather than truth conditions to sentences is in a
better position to deal with both of these issues.
Gradience is intrinsic to the theory by virtue of
the fact that speakers assign values to declarative
sentences in the continuum of real numbers [0,1],
rather than Boolean values in {0,1}. In addition,
a probabilistic account of semantic learning is fa-
cilitated if the target of learning is a probabilistic
representation of meaning. Both semantic repre-
sentation and learning are instances of reasoning
under uncertainty.
Probability theorists working in AI often de-
scribe probability judgements as involving distri-
butions over worlds. In fact, they tend to limit
such judgements to a restricted set of outcomes
or events, each of which corresponds to a par-
tial world which is, effectively, a type of situa-
tion (Halpern, 2003). A classic example of the re-
duction of worlds to situation types in probability
theory is the estimation of the likelihood of heads
vs tails in a series of coin tosses. Here the world
is held constant except along the dimension of a
binary choice between a particular set of possi-
ble outcomes. A slightly more complex case is
the probability distribution for possible results of
throwing a single die, which allows for six pos-
sibilities corresponding to each of its numbered
faces. This restricted range of outcomes consti-
tutes the sample space.
We are making explicit the assumption, com-
mon to most probability theories used in AI, with
clearly defined sample spaces, that probability
is distributed over situation types (Barwise and
Perry, 1983), rather than over sets of entire worlds.
An Austinian proposition is a judgement that a
72
situation is of a particular type, and we treat it
as probabilistic. In fact, it expresses a subjec-
tive probability in that it encodes the belief of an
agent concerning the likelihood that a situation is
of that type. The core of an Austinian proposi-
tion is a type judgement of the form s : T , which
states that a situation s is of type T . On our ac-
count this judgement is expressed probabilistically
as p(s : T ) = r, where r ? [0,1].
1
On the probabilistic type system that we pro-
pose situation types are intensional objects over
which probability distributions are specified. This
allows us to reason about the likelihood of alter-
native states of affairs without invoking possible
worlds.
Complete worlds are not tractably repre-
sentable. Assume that worlds are maximal con-
sistent sets of propositions (Carnap, 1947). If
the logic of propositions is higher-order, then the
problem of determining membership in such a set
is not complete. If the logic is classically first-
order, then the membership problem is complete,
but undecidable.
Alternatively, we could limit ourselves to
propositional logic, and try to generate a maxi-
mally consistent set of propositions from a single
finite proposition P in Conjunctive Normal Form
(CNF, a conjunction of disjunctions), by simply
adding conjuncts to P . But it is not clear what
(finite) set of rules or procedures we could use to
decide which propositions to add in order to gen-
erate a full description of a world in a systematic
way. Nor is it obvious at what point the conjunc-
tion will constitute a complete description of the
world.
Moreover, all the propositions that P entails
must be added to it, and all the propositions with
which P is inconsistent must be excluded, in or-
der to obtain the maximal consistent set of propo-
sitions that describe a world. But then testing the
satisfiability of P is an instance of the ksat prob-
lem, which, in the general case, is NP-complete.
2
1
Beltagy et al. (2013) propose an approach on which clas-
sical logic-based representations are combined with distribu-
tional lexical semantics and a probabilistic Markov logic, in
order to select among the set of possible inferences from a
sentence. Our concern here is more foundational. We seek to
replace classical semantic representations with a rich proba-
bilistic type theory as the basis of both lexical and composi-
tional interpretation.
2
The ksat problem is to determine whether a formula in
propositional logic has a satisfying set of truth-value assign-
ments. For the complexity results of different types of ksat
problem see Papadimitriou (1995).
By contrast situation types can be as large or as
small as we need them to be. They are not max-
imal in the way that worlds are, and so the issue
of completeness of specification does not arise.
Therefore, they can, in principle, be tractably rep-
resented.
2 Rich Type Theory and Probability
Central to standard formulations of rich type the-
ories (for example, (Martin-L?of, 1984)) is the no-
tion of a judgement a : T , that object a is of type
T . We represent the probability of this judgement
as p(a : T ). Our system (based on Cooper (2012))
includes the following types.
Basic Types are not constructed out of other ob-
jects introduced in the theory. If T is a basic type,
p(a : T ) for any object a is provided by a probabil-
ity model, an assignment of probabilities to judge-
ments involving basic types.
PTypes are constructed from a predicate and
an appropriate sequence of arguments. An exam-
ple is the predicate ?man? with arity ?Ind ,Time?
where the types Ind and Time are the basic type
of individuals and of time points respectively.
Thus man(john,18:10) is the type of situation (or
eventuality) where John is a man at time 18:10.
A probability model provides probabilities p(e :
r(a
1
, . . . , a
n
)) for ptypes r(a
1
, . . . , a
n
). We take
both common nouns and verbs to provide the com-
ponents out of which PTypes are constructed.
Meets and Joins give, for T
1
and T
2
, the meet,
T
1
? T
2
and the join T
1
? T
2
, respectively. a :
T
1
? T
2
just in case a : T
1
and a : T
2
. a : T
1
?
T
2
just in case either a : T
1
or a : T
2
(possibly
both).
3
The probabilities for meet and joint types
are defined by the classical (Kolmogorov, 1950)
equations p(a : T
1
? T
2
) = p(a : T
1
)p(a : T
2
| a : T
1
)
(equivalently, p(a : T
1
? T
2
) = p(a : T
1
, a : T
2
)), and
p(a : T
1
? T
2
) = p(a : T
1
) + p(a : T
2
) ? p(a : T
1
? T
2
),
respectively.
Subtypes A type T
1
is a subtype of type T
2
,
T
1
v T
2
, just in case a : T
1
implies a : T
2
no mat-
ter what we assign to the basic types. If T
1
v T
2
then a : T
1
?T
2
iff a : T
1
and a : T
1
?T
2
iff a : T
2
.
Similarly, if T
2
v T
1
then a : T
1
? T
2
iff a : T
2
and a : T
1
? T
2
iff a : T
1
.
If T
2
v T
1
, then p(a : T
1
? T
2
) = p(a : T
2
),
and p(a : T
1
? T
2
) = p(a : T
1
). If T
1
v T
2
,
3
This use of intersection and union types is not standard in
rich type theories, where product and disjoint union are pre-
ferred following the Curry-Howard correspondence for con-
junction and disjunction.
73
then p(a : T
1
) ? p(a : T
2
). These definitions
also entail that p(a : T
1
? T
2
) ? p(a : T
1
), and
p(a : T
1
) ? p(a : T
1
? T
2
).
We generalize probabilistic meet and join types
to probabilities for unbounded conjunctive and
disjunctive type judgements, again using the clas-
sical equations.
Let
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) be the conjunctive
probability of judgements a
0
: T
0
, . . . , a
n
: T
n
.
Then
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) =
?
p
(a
0
: T
0
, . . . , a
n?1
:
T
n?1
)p(a
n
: T
n
| a
0
: T
0
, . . . , a
n?1
: T
n?1
). If n = 0,
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) = 1.
We interpret universal quantification as an un-
bounded conjunctive probability, which is true if
it is vacuously satisfied (n = 0) (Paris, 2010).
Let
?
p
(a
0
: T
0
, a
1
: T
1
, . . .) be the disjunctive
probability of judgements a
0
: T
0
, a
1
: T
1
, . . ..
It is computed by
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) =
?
p
(a
0
: T
0
, . . . , a
n?1
: T
n?1
) + p(a
n
: T
n
) ?
?
p
(a
0
:
T
0
, . . . , a
n?1
: T
n?1
)p(a
n
: T
n
| a
0
: T
0
, . . . , a
n?1
:
T
n?1
). If n = 0,
?
p
(a
0
: T
0
, . . . , a
n
: T
n
) = 0.
We take existential quantification to be an un-
bounded disjunctive probability, which is false if it
lacks a single non-nil probability instance (n = 0).
Conditional Conjunctive Probabilities are
computed by
?
p
(a
0
: T
0
, . . . , a
n
: T
n
| a : T ) =
?
p
(a
0
: T
0
, . . . , a
n?1
: T
n?1
| a : T )p(a
n
: T
n
|
a
0
: T
0
, . . . , a
n?1
: T
n?1
, a : T )). If n = 0,
?
p
(a
0
:
T
0
, . . . , a
n
: T
n
| a : T ) = 1.
Function Types give, for any types T
1
and T
2
,
the type (T
1
? T
2
). This is the type of total func-
tions with domain the set of all objects of type
T
1
and range included in objects of type T
2
. The
probability that a function f is of type (T
1
? T
2
)
is the probability that everything in its domain is of
type T
1
and that everything in its range is of type
T
2
, and furthermore that everything not in its do-
main which has some probability of being of type
T
1
is not in fact of type T
1
. p(f : (T
1
? T
2
)) =
?
a?dom(f)
p
(a : T
1
, f(a) : T
2
)(1?
?
a6?dom(f)
p
(a : T
1
))
Suppose that T
1
is the type of event where there
is a flash of lightning and T
2
is the type of event
where there is a clap of thunder. Suppose that f
maps lightning events to thunder events, and that
it has as its domain all events which have been
judged to have probability greater than 0 of being
lightning events. Let us consider that all the puta-
tive lightning events were clear examples of light-
ning (i.e. judged with probability 1 to be of type
T
1
) and are furthermore associated by f with clear
events of thunder (i.e. judged with probability 1 to
be of type T
2
). Suppose there were four such pairs
of events. Then the probability of f being of type
(T
1
? T
2
) is (1? 1)
4
, that is, 1.
Suppose, alternatively, that for one of the four
events f associates the lightning event with a silent
event, that is, one whose probability of being of
T
2
is 0. Then the probability of f being of type
(T
1
? T
2
) is (1 ? 1)
3
? (1 ? 0) = 0. One clear
counterexample is sufficient to show that the func-
tion is definitely not of the type.
In cases where the probabilities of the an-
tecedent and the consequent type judgements are
higher than 0, the probability of the entire judge-
ment on the existence of a functional type f will
decline in proportion to the size of dom(f). As-
sume, for example that there are k elements a ?
dom(f), where for each such a p(a : T
1
) =
p(f(a) : T
2
) ? .5. Every a
i
that is added to
dom(f) will reduce the value of p(f : (T
1
?
T
2
)), even if it yields higher values for p(a : T
1
)
and p(f(a) : T
2
). This is due to the fact that we
are treating the probability of p(f : (T
1
? T
2
))
as the likelihood of there being a function that is
satisfied by all objects in its domain. The larger
the domain, the less probable that all elements in
it fulfill the functional relation.
We are, then, interpreting a functional type
judgement of this kind as a universally quantified
assertion over the pairing of objects in dom(f)
and range(f). The probability of such an asser-
tion is given by the conjunction of assertions cor-
responding to the co-occurrence of each element a
in f ?s domain as an instance of T
1
with f(a) as an
instance of T
2
. This probability is the product of
the probabilities of these individual assertions.
This seems reasonable, but it only deals with
functions whose domain is all objects which have
been judged to have some probability, however
low, of being of type T
1
. Intuitively, functions
which leave out some of the objects with lower
likelihood of being of type T
1
should also have a
probability of being of type (T
1
? T
2
). This fac-
tor in the probability is represented by the second
element of the product in the formula.
74
Negation ?T , of type T , is the function type
(T ? ?), where ? is a necessarily empty type
and p(?) = 0. It follows from our rules for func-
tion types that p(f : ?T ) = 1 if dom(f) = ?, that
is T is empty, and 0 otherwise.
We also assign probabilities to judgements con-
cerning the (non-)emptiness of a type, p(T ). we
pass over the details of how we compute the prob-
abilities of such judgements, but we note that our
account of negation entails that p(T ? ?T ) = 1,
and (ii) p(??T ) = p(T ). Therefore, we sustain
classical Boolean negation and disjunction, in con-
trast to Martin-L?of?s (1984) intuitionistic type the-
ory.
Dependent Types are functions from objects to
types. Given appropriate arguments as functions
they will return a type. Therefore, the account of
probabilities associated with functions above ap-
plies to dependent types.
Record Types A record in a type system asso-
ciated with a set of labels is a set of ordered pairs
(fields) whose first member is a label and whose
second member is an object of some type (possibly
a record). Records are required to be functional on
labels (each label in a record can only occur once
in the record?s left projection).
A dependent record type is a set of fields (or-
dered pairs) consisting of a label ` followed by T
as above. The set of record types is defined by:
1. [], that is the empty set or Rec, is a record type. r : Rec
just in case r is a record.
2. If T
1
is a record type, ` is a label not occurring in T
1
,
and T
2
is a type, then T
1
? {?`, T
2
?} is a record type.
r : T
1
? {?`, T
2
?} just in case r : T
1
, r.` is defined (`
occurs as a label in r) and r.` : T
2
.
3. If T is a record type, ` is a label not occuring in
T , T is a dependent type requiring n arguments, and
?pi
1
, . . . , pi
n
? is an n-place sequence of paths in T ,
4
then T ? {?`, ?T , ?pi
1
, . . . , pi
n
???} is a record type.
r : T ? {?`, ?T , ?pi
1
, . . . , pi
n
???} just in case r : T ,
r.` is defined and r.` : T (r.pi
1
, . . . , r.pi
n
).
The probability that an object r is of a record
type T is given by the following clauses:
1. p(r : Rec) = 1 if r is a record, 0 otherwise
2. p(r : T
1
? {?`, T
2
?}) =
?
p
(r : T
1
, r.` : T
2
)
3. If T : (T
1
? (. . . ? (T
n
? Type) . . .)), then
p(r : T ? {?`, ?T , ?pi
1
, . . . , pi
n
???}) =
?
p
(r : T, r.` :
T (r.pi
1
, . . . , r.pi
n
) | r.pi
1
: T
1
, . . . , r.pi
n
: T
n
)
4
In the full version of TTR we also allow absolute paths
which point to particular records, but we will not include
them here.
3 Compositional Semantics
Montague (1974) determines the denotation of a
complex expression by applying a function to an
intensional argument (as in [[ NP ]]([[
?
VP ]])). We
employ a variant of this general strategy by ap-
plying a probabilistic evaluation function [[ ? ]]
p
to
a categorical (non-probabilistic) semantic value.
For semantic categories that are interpreted as
functions, [[ ? ]]
p
yields functions from categorical
values to probabilities. For sentences it produces
probability values.
The probabilistic evaluation function [[ ? ]]
p
pro-
duces a probabilistic interpretation based on a
classical compositional semantics. For sentences
it will return the probability that the sentence is
true. For categories that are interpreted as func-
tions it will return functions from (categorical) in-
terpretations to probabilities. We are not propos-
ing strict compositionality in terms of probabili-
ties. Probabilities are like truth-values (or rather,
truth-values are the limit cases of probabilities).
We would not expect to be able to compute the
probability associated with a complex constituent
on the basis of the probabilities associated with its
immediate constituents, any more than we would
expect to be able to compute a categorical inter-
pretation entirely in terms of truth-functions and
extensions. However, the simultaneous computa-
tion of categorical and probabilistic interpretations
provides us with a compositional semantic system
that is closely related to the simultaneous com-
putation of intensions and extensions in classical
Montague semantics.
The following definition of [[ ? ]]
p
for a fragment
of English is specified on the basis of our proba-
bilistic type system and a non-probabilistic inter-
pretation function [[ ? ]], which we do not give in
this version of the paper. (It?s definition is given
by removing the probability p from the definition
below.)
[[ [
S
S
1
and S
2
] ]]
p
= p(
[
e
1
:[[ S
1
]]
e
2
:[[ S
2
]]
]
)
[[ [
S
S
1
or S
2
] ]]
p
= p(
[
e:[[ S
1
]]?[[ S
2
]]
]
)
[[ [
S
Neg S] ]]
p
= [[ Neg ]]
p
([[ S ]])
[[ [
S
NP VP] ]]
p
= [[ NP ]]
p
([[ VP ]])
[[ [
NP
Det N] ]]
p
= [[ Det ]]
p
([[ N ]])
[[ [
NP
N
prop
] ]]
p
= [[ N
prop
]]
p
[[ [
VP
V
t
NP] ]]
p
= [[ V
t
]]
p
([[ NP ]])
[[ [
VP
V
i
] ]]
p
= [[ V
i
]]
p
[[ [
Neg
?it?s not true that?] ]]
p
= ?T :RecType(p(
[
e:?T
]
))
[[ [
Det
?some?] ]]
p
= ?Q:Ppty(?P :Ppty(p(
[
e:some(Q, P )
]
)))
[[ [
Det
?every?] ]]
p
= ?Q:Ppty(?P :Ppty(p(
[
e:every(Q, P )
]
)))
[[ [
Det
?most?] ]]
p
= ?Q:Ppty(?P :Ppty(p(
[
e:most(Q, P )
]
)))
75
[[ [
N
?boy?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:boy(r.x)
]
))
[[ [
N
?girl?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:girl(r.x)
]
))
[[ [
Adj
?green?] ]]
p
=
?P :Ppty(?r:
[
x:Ind
]
(p((
[
e:green(r.x,P )
]
)))))
[[ [
Adj
?imaginary?] ]]
p
=
?P :Ppty(?r:
[
x:Ind
]
(p((
[
e:imaginary(r.x,P )
]
)))))
5
[[ [
N
prop
?Kim?] ]]
p
= ?P :Ppty(p(P (
[
x=kim
]
)))
[[ [
N
prop
?Sandy?] ]]
p
= ?P :Ppty(p(P (
[
x=sandy
]
)))
[[ [
V
t
?knows?] ]]
p
=
?P:Quant(?r
1
:
[
x:Ind
]
(p(P(?r
2
:(
[
e:know(r
1
.x,r
2
.x)
]
)))))
[[ [
V
t
?sees?] ]]
p
=
?P:Quant(?r
1
:
[
x:Ind
]
(p(P(?r
2
:(
[
e:see(r
1
.x,r
2
.x)
]
)))))
[[ [
V
i
?smiles?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:smile(r.x)
]
))
[[ [
V
i
?laughs?] ]]
p
= ?r:
[
x:Ind
]
(p(
[
e:laugh(r.x)
]
))
A probability distribution d for this fragment,
based on a set of situations S, is such that:
p
d
(a : Ind) = 1 if a is kim or sandy
6
p
d
(s : T ) ? [0, 1] if s ? S and T is a ptype
p
d
(s : T ) = 0 if s 6? S and T is a ptype
7
p
d
(a : [
?
P ]) = p
d
(P (
[
x=a
]
))
p
d
(some(P,Q)) = p
d
([
?
P ] ? [
?
Q])
p
d
(every(P,Q)) = p
d
([
?
P ]? [
?
Q])
p
d
(most(P,Q)) = min(1,
p
d
([
?
P ]?[
?
Q]
?
most
p
d
([
?
P ])
)
The probability that an event e is of the type in
which the relation some holds of the properties P
andQ is the probability that e is of the conjunctive
type P ?Q. The probability that e is of the every
type for P and Q is the likelihood that it instanti-
ates the functional type P ? Q. As we have de-
fined the probabilities associated with functional
types in terms of universal quantification (an un-
bounded conjunction of the pairings between the
elements of the domain P of the function and its
range Q), this definition sustains the desired read-
ing of every. The likelihood that e is of the type
most for P and Q is the likelihood that e is of
type P ?Q, factored by the product of the contex-
tually determined parameter ?
most
and the likeli-
hood that e is of type P , where this fraction is less
than 1, and 1 otherwise.
Consider a simple example.
[[ [
S
[
NP
[
N
prop
Kim]] [
VP
[
V
i
smiles]]] ]]
p
=
?P :Ppty(p(P (
[
x=kim
]
)))(?r:
[
x:Ind
]
(
[
e:smile(r.x)
]
)) =
p(?r:
[
x:Ind
]
(
[
e:smile(r.x)
]
)(
[
x=kim
]
)) =
p(
[
e:smile(kim)
]
)
5
Notice that we characterize adjectival modifiers as rela-
tions between records of individuals and properties. We can
then invoke subtyping to capture the distinction between in-
tersective and non-intersective modifier relations.
6
This seems an intuitive assumption, though not a neces-
sary one.
7
Again this seems an intuitive, though not a necessary as-
sumption.
Suppose that p
d
(s
1
:smile(kim)) = .7,
p
d
(s
2
:smile(kim)) = .3, p
d
(s
3
:smile(kim)) =
.4, and there are no other situations s
i
such
that p
d
(s
i
:smile(kim)) > 0. Furthermore, let
us assume that these probabilities are indepen-
dent of each other, that is, p
d
(s
3
:smile(kim)) =
p
d
(s
3
:smile(kim) | s
1
:smile(kim), s
2
:smile(kim))
and so on. Then
p
d
(smile(kim))=
?
p
d
(s
1
: smile(kim), s
2
: smile(kim), s
3
: smile(kim)) =
?
p
d
(s
1
: smile(kim), s
2
: smile(kim)) + .4 ? .4
?
p
d
(s
1
:
smile(kim), s
2
: smile(kim)) =
(.7 + .3? .7? .3) + .4? .4(.7 + .3? .7? .3) =
.874
This means that p
d
(
[
e:smile(kim)
]
) = .874.
Hence [[ [
S
[
NP
[
N
prop
Kim]] [
VP
[
V
i
smiles]]] ]]
p
d
= .874
(where [[ ? ]]
p
d
is the result of computing [[ ? ]]
p
with respect to the probability distribution d).
Just as for categorical semantics, we can con-
struct type theoretic objects corresponding to
probabilistic judgements. We call these proba-
bilistic Austinian propositions. These are records
of type?
?
sit : Sit
sit-type : Type
prob : [0,1]
?
?
where [0,1] is used to represent the type of real
numbers between 0 and 1. They assert that the
probability that a situation s is of type Type is the
value of prob.
The definition of [[ ? ]]
p
specifies a compositional
procedure for generating an Austinian proposition
(record) of this type from the meanings of the syn-
tactic constituents of a sentence.
4 An Outline of Semantic Learning
We outline a schematic theory of semantic learn-
ing on which agents acquire classifiers that form
the basis for our probabilistic type system. For
simplicity and ease of presentation we take these
to be Naive Bayes classifiers, which an agent ac-
quires from observation. In future developments
of this theory we will seek to extend the approach
to Bayesian networks (Pearl, 1990).
We assume that agents keep records of observed
situations and their types, modelled as probabilis-
tic Austinian propositions. For example, an obser-
vation of a man running might yield the following
Austinian proposition for some a:Ind, s
1
:man(a),
s
2
:run(a):
76
??
?
?
?
?
?
?
?
sit =
?
?
ref = a
c
man
= s
1
c
run
= s
2
?
?
sit-type =
?
?
ref : Ind
c
man
: man(ref)
c
run
: run(ref)
?
?
prob = 0.7
?
?
?
?
?
?
?
?
?
An agent, A, makes judgements based on a
finite string of probabilistic Austinian proposi-
tions, J, corresponding to prior judgements held
in memory. For a type, T , J
T
represents that set of
Austinian propositions j such that j.sit-type v T .
If T is a type and J a finite string of probabilis-
tic Austinian propositions, then || T ||
J
represents
the sum of all probabilities associated with T in J
(
?
j?J
T
j.prob). P(J) is the sum of all probabilities
in J (
?
j?J
j.prob).
We use prior
J
(T ) to represent the prior proba-
bility that anything is of type T given J, that is
||T ||
J
P(J)
if P(J) > 0, and 0 otherwise.
p
A,J
(s : T ) denotes the probability that agent A
assigns with respect to prior judgements J to s be-
ing of type T . Similarly, p
A,J
(s : T
1
| s : T
2
) is
the probability that agent A assigns with respect
to prior judgements J to s being of type T
1
, given
that A judges s to be of type T
2
.
When an agent A encounters a new situation
s and considers whether it is of type T , he/she
uses probabilistic reasoning to determine the value
of p
A,J
(s : T ). A uses conditional probabilities
to calculate this value, where A computes these
conditional probabilities with the equation p
A,J
(s :
T
1
| s : T
2
) =
||T
1
?T
2
||
J
||T
2
||
J
, if || T
2
||
J
6= 0. Otherwise,
p
A,J
(s : T
1
| s : T
2
) = 0.
This is our type theoretic variant of the stan-
dard Bayesian formula for conditional probabili-
ties: p(A | B) =
|A&B|
|B|
. But instead of counting
categorical instances, we sum the probabilities of
judgements. This is because our ?training data? is
not limited to categorical observations. Instead it
consists of probabilistic observational judgements
that situations are of particular types.
8
Assume that we have the following types:
T
man
=
[
ref : Ind
c
man
: man(ref)
]
and
T
run
=
[
ref : Ind
c
run
: run(ref)
]
8
As a reviewer observes, by using an observer?s previous
judgements for the probability of an event being of a partic-
ular type, as the prior for the rule that computes the proba-
bility of a new event being of that type, we have, in effect,
compressed information that properly belongs in a Bayesian
network into our specification of a naive Bayesian classifier.
This is a simplification that we adopt here for ease of expo-
sition. In future work, we will characterise classifier learning
through full Bayesian networks.
Assume also that J
T
man
?T
run
has three members,
corresponding to judgements by A that a man was
running in three observed situations s
1
, s
3
, and
s
4
, and that these Austinian propositions have the
probabilities 0.6, 0.6. and 0.5 respectively.
Take J
T
man
to have five members correspond-
ing to judgements by A that there was a man in
s
1
, . . . , s
5
, and that the Austinian propositions as-
signing T
man
to s
1
, . . . , s
5
all have probability 0.7.
Given these assumptions, the conditional probabil-
ity that A will assign on the basis of J to someone
runs, given that he is a man is p
A,J
(r : T
run
| r :
T
man
) =
||T
man
?T
run
||
J
||T
man
||
J
=
0.6+0.6+0.5
0.7+0.7+0.7+0.7+0.7
= .486
We use conditional probabilities to construct a
Naive Bayes classifier. A classifies a new situa-
tion s based on the prior judgements J, and what-
ever evidence A can acquire about s. This evi-
dence has the form p
A,J
(s : T
e
1
), . . ., p
A,J
(s : T
e
n
),
where T
e
1
, . . . , T
e
n
are the evidence types. The
Naive Bayes classifier assumes that the evidence is
independent, in that the probability of each piece
of evidence is independent of every other piece of
evidence.
We first formulate Bayes? rule of conditional
probability. This rule defines the conditional prob-
ability of a conclusion r : T
c
, given evidence r :
T
e
1
, r : T
e
2
, . . . , r : T
e
n
, in terms of conditional prob-
abilities of the form p(s
i
: T
e
i
| s
i
: T
c
), 1 ? i ? n,
and priors for conclusion and evidence:
p
A,J
(r : T
c
| r : T
e
1
, . . . , r : T
e
n
) =
prior
J
(T
c
)
||T
e
1
?T
c
||
J
||T
c
||
J
...
||T
e
n
?T
c
||
J
||T
c
||
J
prior
J
(T
e
1
)...prior
J
(T
e
n
)
The conditional probabilities are computed
from observations as indicated above. The rule of
conditional probability allows the combination of
several pieces of evidence, without requiring pre-
vious observation of a situation involving all the
evidence types.
We formulate a Naive Bayes classifier as a func-
tion from evidence types T
e
1
, T
e
2
, . . . , T
e
n
(i.e. from
a record of type T
e
1
? T
e
2
? . . . ? T
e
n
) to conclusion
types T
c
1
, T
c
2
, . . . , T
c
m
. The conclusion is a disjunc-
tion of one or more T ? {T
c
1
, T
c
2
, . . . , T
c
m
}, where
m ranges over all possible non-disjunctive conclu-
sions distinguished by the classifier. This function
is specified as follows.
? : (T
e
1
? . . .?T
e
n
)? (T
c
1
? . . .?T
c
m
) such that ?(r) =
(
?
argmax
T??T
c
1
,...,T
c
m
?
p
A,J
(r : T | r : T
e
1
, . . . , r : T
e
n
)
The classifier returns the type T which max-
imises the conditional probability of r : T given
77
the evidence provided by r. The argmax operator
here takes a sequence of arguments and a func-
tion and yields a sequence containing the argu-
ments which maximise the function (if there are
more than one).
The classifier will output a disjunction in case
both possibilities have the same probability. The
?
operator takes a sequence and returns the dis-
junction of all elements of the sequence.
In addition to computing the conclusion which
receives the highest probability given the evi-
dence, we also want the posterior probability of
the judgement above, i.e. the probability of the
judgement in light of the evidence. We obtain the
non-normalised probabilities (p
nn
A,J
) of the different
possible conclusions by factoring in the probabili-
ties of the evidence:
p
nn
A,J
(r : ?(r)) =
?
T?
?
?1
?(r)
p
A,J
(r : T | r : T
e
1
, . . . , r : T
e
n
)p
A,J
(r :
T
e
1
) . . . p
A,J
(r : T
e
n
)
where
?
?1
is the inverse of
?
, i.e. a function that
takes a disjunction and returns the set of disjuncts.
We then take the probability of r : ?(r) and
normalise over the sum of the probabilities of
all the possible conclusions. This gives us the
normalised probability of the judgement resulting
from classification p(r : ?(r)) =
p
nn
A,J
(r:?(r))
?
1?i?m
p
nn
A,J
(r:T
c
i
)
.
However, since the probabilities of the evidence
are identical for all possible conclusions, we can
ignore them and instead compute the normalised
probability with the following equation (where m
ranges over all possible non-disjunctive conclu-
sions distinguished by the classifier, as above).
p
A,J
(r : ?(r)) =
?
T?
?
?1
?(r)
p
A,J
(r:T |r:T
e
1
,...,r:T
e
n
)
?
1?i?m
p
A,J
(r:T
c
i
|r:T
e
1
,...,r:T
e
n
)
The result of classification can be represented as
an Austinian proposition
?
?
sit = s
sit-type = ?(s)
prob = p
A,J
(s : ?(s))
?
?
which A adds to J as a result of observing and
classifying s, and is thus made available for sub-
sequent probabilistic reasoning.
5 Conclusions and Future Work
We have presented a probabilistic version of a rich
type theory with records, relying heavily on classi-
cal equations for types formed with meet, join, and
negation. This has permitted us to sustain classi-
cal equivalences and Boolean negation for com-
plex types within an intensional type theory. We
have replaced the truth of a type judgement with
the probability of it being the case, and we have
applied this approach to judgements that a situa-
tion if of type T .
Our probabilistic formulation of a rich type the-
ory with records provides the basis for a compo-
sitional semantics in which functions apply to cat-
egorical semantic objects in order to return either
functions from categorical interpretations to prob-
abilistic judgements, or, for sentences, to proba-
bilistic Austinian propositions. One of the inter-
esting ways in which this framework differs from
classical model theoretic semantics is that the ba-
sic types and type judgements at the foundation of
the type system correspond to perceptual judge-
ments concerning objects and events in the world,
rather than to entities in a model and set theoretic
constructions defined on them.
We have offered a schematic view of semantic
learning. On this account observations of situa-
tions in the world support the acquisition of naive
Bayesian classifiers from which the basic proba-
bilistic types of our type theoretical semantics are
extracted. Our type theory is, then, the interface
between observation-based learning of classifiers
for objects and the situations in which they figure
on one hand, and the computation of complex se-
mantic values for the expressions of a natural lan-
guage from these simple probabilistic types and
type judgements on the other. Therefore our gen-
eral model of interpretation achieves a highly in-
tegrated bottom-up treatment of linguistic mean-
ing and perceptually-based cognition that situates
meaning in learning how to make observational
judgements concerning the likelihood of situations
obtaining in the world.
The types of our semantic theory are inten-
sional. They constitute ways of classifying situ-
ations, and they cannot be reduced to set of situa-
tions. The theory achieves fine-grained intension-
ality through a rich and articulated type system,
where the foundation of this system is anchored in
perceptual observation.
The meanings of expressions are acquired on
the basis of speakers? experience in the applica-
tion of classifiers to objects and events that they
encounter. Meanings are dynamic and updated in
light of subsequent experience.
78
Probability is distributed over alternative situ-
ation types. Possible worlds, construed as maxi-
mal consistent sets of propositions (ultrafilters in a
proof theoretic lattice of propositions) play no role
in this framework.
Bayesian reasoning from observation provides
the incremental basis for learning and refining
predicative types. These types feed the combina-
torial semantic procedures for interpreting the sen-
tences of a natural language.
In future work we will explore implementations
of our learning theory in order to study the viabil-
ity of our probabilistic type theory as an interface
between perceptual judgement and compositional
semantics. We hope to show that, in addition to
its cognitive and theoretical interest, our proposed
framework will yield results in robotic language
learning, and dialogue modelling.
Acknowledgments
We are grateful to two anonymous reviewers for
very helpful comments on an earlier draft of this
paper. We also thank Alex Clark, Jekaterina
Denissova, Raquel Fern?andez, Jonathan Ginzburg,
Noah Goodman, Dan Lassiter, Michiel van Lam-
balgen, Poppy Mankowitz, Aarne Ranta, and Pe-
ter Sutton for useful discussion of ideas presented
in this paper. Shalom Lappin?s participation in
the research reported here was funded by grant
ES/J022969/1 from the Economic and Social Re-
search Council of the UK, and a grant from the
Wenner-Gren Foundations. We also gratefully ac-
knowledge the support of Vetenskapsr?adet, project
2009-1569, Semantic analysis of interaction and
coordination in dialogue (SAICD); the Depart-
ment of Philosophy, Linguistics, and Theory of
Science; and the Centre for Language Technology
at the University of Gothenburg.
References
Jon Barwise and John Perry. 1983. Situations and
Attitudes. Bradford Books. MIT Press, Cambridge,
Mass.
I. Beltagy, C. Chau, G. Boleda, D. Garrette, K. Erk,
and R. Mooney. 2013. Montague meets markov:
Deep semantics with probabilistic logical form. In
Second Joint Conference on Lexical and Computa-
tional Semantics, Vol. 1, pages 11?21. Association
of Computational Linguistics, Atlanta, GA.
R. Carnap. 1947. Meaning and Necessity. University
of Chicago Press, Chicago.
A. Clark and S. Lappin. 2011. Linguistic Nativism
and the Poverty of the Stimulus. Wiley-Blackwell,
Chichester, West Sussex, and Malden, MA.
Robin Cooper. 2012. Type theory and semantics in
flux. In Ruth Kempson, Nicholas Asher, and Tim
Fernando, editors, Handbook of the Philosophy of
Science, volume 14: Philosophy of Linguistics. El-
sevier BV, 271?323. General editors: Dov M. Gab-
bay, Paul Thagard and John Woods.
C. Fox and S. Lappin. 2010. Expressiveness and
complexity in underspecified semantics. Linguistic
Analysis, Festschrift for Joachim Lambek, 36:385?
417.
J. Halpern. 2003. Reasoning About Uncertainty. MIT
Press, Cambridge MA.
H. Kamp and U. Reyle. 1993. From Discourse to
Logic: Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Kluwer, Dordrecht.
A.N. Kolmogorov. 1950. Foundations of Probability.
Chelsea Publishing, New York.
Per Martin-L?of. 1984. Intuitionistic Type Theory. Bib-
liopolis, Naples.
Richard Montague. 1974. Formal Philosophy: Se-
lected Papers of Richard Montague. Yale University
Press, New Haven. ed. and with an introduction by
Richmond H. Thomason.
C. Papadimitriou. 1995. Computational Complexity.
Addison-Wesley Publishing Co., Readin, MA.
J. Paris. 2010. Pure inductive logic. Winter School in
Logic, Guangzhou, China.
J. Pearl. 1990. Bayesian decision methods. In
G. Shafer and J. Pearl, editors, Readings in Uncer-
tain Reasoning, pages 345?352. Morgan Kaufmann.
79
