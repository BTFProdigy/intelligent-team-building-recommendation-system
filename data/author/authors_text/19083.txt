Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 346?351,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automatically Predicting Sentence Translation Difficulty
Abhijit Mishra?, Pushpak Bhattacharyya?, Michael Carl?
? Department of Computer Science and Engineering, IIT Bombay, India
{abhijitmishra,pb}@cse.iitb.ac.in
? CRITT, IBC, Copenhagen Business School, Denmark,
mc.ibc@cbs.dk
Abstract
In this paper we introduce Translation Dif-
ficulty Index (TDI), a measure of diffi-
culty in text translation. We first de-
fine and quantify translation difficulty in
terms of TDI. We realize that any mea-
sure of TDI based on direct input by trans-
lators is fraught with subjectivity and ad-
hocism. We, rather, rely on cognitive ev-
idences from eye tracking. TDI is mea-
sured as the sum of fixation (gaze) and
saccade (rapid eye movement) times of
the eye. We then establish that TDI is
correlated with three properties of the in-
put sentence, viz. length (L), degree of
polysemy (DP) and structural complexity
(SC). We train a Support Vector Regres-
sion (SVR) system to predict TDIs for
new sentences using these features as in-
put. The prediction done by our frame-
work is well correlated with the empiri-
cal gold standard data, which is a repos-
itory of < L,DP, SC > and TDI pairs
for a set of sentences. The primary use of
our work is a way of ?binning? sentences
(to be translated) in ?easy?, ?medium? and
?hard? categories as per their predicted
TDI. This can decide pricing of any trans-
lation task, especially useful in a scenario
where parallel corpora for Machine Trans-
lation are built through translation crowd-
sourcing/outsourcing. This can also pro-
vide a way of monitoring progress of sec-
ond language learners.
1 Introduction
Difficulty in translation stems from the fact that
most words are polysemous and sentences can be
long and have complex structure. While length of
sentence is commonly used as a translation diffi-
culty indicator, lexical and structural properties of
a sentence also contribute to translation difficulty.
Consider the following example sentences.
1. The camera-man shot the policeman
with a gun. (length-8)
2. I was returning from my old office
yesterday. (length-8)
Clearly, sentence 1 is more difficult to process
and translate than sentence 2, since it has lexical
ambiguity (?Shoot? as an act of firing a shot or
taking a photograph?) and structural ambiguity
(Shot with a gun or policeman with a gun?). To
produce fluent and adequate translations, efforts
have to be put to analyze both the lexical and syn-
tactic properties of the sentences.
The most recent work on studying translation
difficulty is by Campbell and Hale (1999) who
identified several areas of difficulty in lexis and
grammar. ?Reading? researchers have focused on
developing readability formulae, since 1970. The
Flesch-Kincaid Readability test (Kincaid et al,
1975), the Fry Readability Formula (Fry, 1977)
and the Dale-Chall readability formula (Chall and
Dale, 1999) are popular and influential. These for-
mulae use factors such as vocabulary difficulty (or
semantic factors) and sentence length (or syntac-
tic factors). In a different setting, Malsburg et
al. (2012) correlate eye fixations and scanpaths
of readers with sentence processing. While these
approaches are successful in quantifying readabil-
ity, they may not be applicable to translation sce-
narios. The reason is that, translation is not
merely a reading activity. Translation requires
co-ordination between source text comprehension
and target text production (Dragsted, 2010). To
the best of our knowledge, our work on predicting
TDI is the first of its kind.
The motivation of the work is as follows. Cur-
rently, for domain specific Machine Translation
systems, parallel corpora are gathered through
translation crowdsourcing/outsourcing. In such
346
Figure 1: Inherent sentence complexity and per-
ceived difficulty during translation
a scenario, translators are paid on the basis of
sentence length, which ignores other factors con-
tributing to translation difficulty, as stated above.
Our proposed Translation Difficulty Index (TDI)
quantifies the translation difficulty of a sentence
considering both lexical and structural proper-
ties. This measure can, in turn, be used to clus-
ter sentences according to their difficulty levels
(viz. easy, medium, hard). Different payment and
schemes can be adopted for different such clusters.
TDI can also be useful for training and evalu-
ating second language learners. For example, ap-
propriate examples at particular levels of difficulty
can be chosen for giving assignments and monitor-
ing progress.
The rest of the paper is organized in the fol-
lowing way. Section 2 describes TDI as func-
tion of translation processing time. Section 3 is
on measuring translation processing time through
eye tracking. Section 4 gives the correlation of
linguistic complexity with observed TDI. In sec-
tion 5, we describe a technique for predicting TDIs
and ranking unseen sentences using Support Vec-
tor Machines. Section 6 concludes the paper with
pointers to future work.
2 Quantifying Translation Difficulty
As a first approximation, TDI of a sentence can
be the time taken to translate the sentence, which
can be measured through simple translation exper-
iments. This is based on the assumption that more
difficult sentences will require more time to trans-
late. However, ?time taken to translate? may not
be strongly related to the translation difficulty for
two reasons. First, it is difficult to know what
fraction of the total translation time is actually
spent on the translation-related-thinking. For ex-
ample, translators may spend considerable amount
of time typing/writing translations, which is ir-
relevant to the translation difficulty. Second, the
translation time is sensitive to distractions from
the environment. So, instead of the ?time taken
to translate?, we are more interested in the ?time
for which translation related processing is carried
out by the brain?. This can be termed as the Trans-
lation Processing Time (Tp). Mathematically,
Tp = Tp comp + Tp gen (1)
Where Tp comp and Tp gen are the processing times
for source text comprehension and target text gen-
eration respectively. The empirical TDI, is com-
puted by normalizing Tp with sentence length.
TDI = Tpsentencelength (2)
Measuring Tp is a difficult task as translators of-
ten switch between thinking and writing activities.
Here comes the role of eye tracking.
3 Measuring Tp by eye-tracking
We measure Tp by analyzing the gaze behavior
of translators through eye-tracking. The rationale
behind using eye-tracking is that, humans spend
time on what they see, and this ?time? is corre-
lated with the complexity of the information being
processed, as shown in Figure 1. Two fundamental
components of eye behavior are (a) Gaze-fixation
or simply, Fixation and (b) Saccade. The former
is a long stay of the visual gaze on a single loca-
tion. The latter is a very rapid movement of the
eyes between positions of rest. An intuitive feel
for these two concepts can be had by consider-
ing the example of translating the sentence The
camera-man shot the policeman with a gun men-
tioned in the introduction. It is conceivable that
the eye will linger long on the word ?shot? which
is ambiguous and will rapidly move across ?shot?,
?camera-man? and ?gun? to ascertain the clue for
disambiguation.
The terms Tp comp and Tp gen in (1) can now be
looked upon as the sum of fixation and saccadic
durations for both source and target sentences re-
spectively.
Modifying 1
Tp =
?
f?Fs
dur(f) +
?
s?Ss
dur(s)
+
?
f?Ft
dur(f) +
?
s?St
dur(s)
(3)
347
Figure 2: Screenshot of Translog. The circles rep-
resent fixations and arrow represent saccades.
Here, Fs and Ss correspond to sets of fixations and
saccades for source sentence and Ft and St corre-
spond to those for the target sentence respectively.
dur is a function returning the duration of fixations
and saccades.
3.1 Computing TDI using eye-tracking
database
We obtained TDIs for a set of sentences from
the Translation Process Research Database (TPR
1.0)(Carl, 2012). The database contains trans-
lation studies for which gaze data is recorded
through the Translog software1(Carl, 2012). Fig-
ure 2 presents a screendump of Translog. Out of
the 57 available sessions, we selected 40 transla-
tion sessions comprising 80 sentence translations2.
Each of these 80 sentences was translated from
English to three different languages, viz. Span-
ish, Danish and Hindi by at least 2 translators.
The translators were young professional linguists
or students pursuing PhD in linguistics.
The eye-tracking data is noisy and often ex-
hibits systematic errors (Hornof and Halverson,
2002). To correct this, we applied automatic er-
ror correction technique (Mishra et al, 2012) fol-
lowed by manually correcting incorrect gaze-to-
word mapping using Translog. Note that, gaze and
saccadic durations may also depend on the transla-
tor?s reading speed. We tried to rule out this effect
by sampling out translations for which the vari-
ance in participant?s reading speed is minimum.
Variance in reading speed was calculated after tak-
ing a samples of source text for each participant
and measuring the time taken to read the text.
After preprocessing the data, TDI was com-
puted for each sentence by using (2) and (3).The
observed unnormalized TDI score3 ranges from
0.12 to 0.86. We normalize this to a [0,1] scale
1http://www.translog.dk
220% of the translation sessions were discarded as it was
difficult to rectify the gaze logs for these sessions.
3Anything beyond the upper bound is hard to translate and
can be assigned with the maximum score.
Figure 3: Dependency graph used for computing
SC
using MinMax normalization.
If the ?time taken to translate? and Tp were
strongly correlated, we would have rather opted
?time taken to translate? for the measurement of
TDI. The reason is that ?time taken to translate?
is relatively easy to compute and does not require
expensive setup for conducting ?eye-tracking? ex-
periments. But our experiments show that there
is a weak correlation (coefficient = 0.12) between
?time taken to translate? and Tp. This makes us
believe that Tp is still the best option for TDI mea-
surement.
4 Relating TDI to sentence features
Our claim is that translation difficulty is mainly
caused by three features: Length, Degree of Poly-
semy and Structural Complexity.
4.1 Length
It is the total number of words occurring in a sen-
tence.
4.2 Degree of Polysemy (DP)
The degree of polysemy of a sentence is the sum of
senses possessed by each word in the Wordnet nor-
malized by the sentence length. Mathematically,
DPsentence =
?
w?W Senses(w)
length(sentence) (4)
Here, Senses(w) retrieves the total number senses
of a word P from the Wordnet. W is the set of
words appearing in the sentence.
4.3 Structural Complexity (SC)
Syntactically, words, phrases and clauses are at-
tached to each other in a sentence. If the attach-
ment units lie far from each other, the sentence
has higher structural complexity. Lin (1996) de-
fines it as the total length of dependency links in
the dependency structure of the sentence.
348
Figure 4: Prediction of TDI using linguistic prop-
erties such as Length(L), Degree of Polysemy
(DP) and Structural Complexity (SC)
Example: The man who the boy attacked
escaped.
Figure 3 shows the dependency graph for the
example sentence. The weights of the edges cor-
respond how far the two connected words lie from
each other in the sentence. Using Lin?s formula,
the SC score for the example sentence turns out to
be 15.
Lin?s way of computing SC is affected by sen-
tence length since the number of dependency links
for a sentence depends on its length. So we nor-
malize SC by the length of the sentence. After
normalization, the SC score for the example given
becomes 15/7 = 2.14
4.4 How are TDI and linguistic features
related
To validate that translation difficulty depends on
the above mentioned linguistic features, we tried
to find out the correlation coefficients between
each feature and empirical TDI. We extracted
three sets of sample sentences. For each sample,
sentence selection was done with a view to vary-
ing one feature, keeping the other two constant.
The Correlation Coefficients between L, DP and
SC and the empirical TDI turned out to be 0.72,
0.41 and 0.63 respectively. These positive correla-
tion coefficients indicate that all the features con-
tribute to the translation difficulty.
5 Predicting TDI
Our system predicts TDI from the linguistic prop-
erties of a sentence as shown in Figure 4.
The prediction happens in a supervised setting
through regression. Training such a system re-
quires a set sentences annotated with TDIs. In
our case, direct annotation of TDI is a difficult and
unintuitive task. So, we annotate TDI by observ-
Kernel(C=3.0) MSE (%) Correlation
Linear 20.64 0.69
Poly (Deg 2) 12.88 0.81
Poly (Deg 3) 13.35 0.78
Rbf (default) 13.32 0.73
Table 1: Relative MSE and Correlation with ob-
served data for different kernels used for SVR.
ing translator?s behavior (using equations (1) and
(2))instead of asking people to rate sentences with
TDI.
We are now prepared to give the regression sce-
nario for predicting TDI.
5.1 Preparing the dataset
Our dataset contains 80 sentences for which TDI
have been measured (Section 3.1). We divided this
data into 10 sets of training and testing datasets in
order to carry out a 10-fold evaluation. DP and SC
features were computed using Princeton Wordnet4
and Stanford Dependence Parser5.
5.2 Applying Support Vector Regression
To predict TDI, Support Vector Regression (SVR)
technique (Joachims et al, 1999) was preferred
since it facilitates multiple kernel-based methods
for regression. We tried using different kernels us-
ing default parameters. Error analysis was done
by means of Mean Squared Error estimate (MSE).
We also measured the Pearson correlation coeffi-
cient between the empirical and predicted TDI for
our test-sets.
Table 1 indicates Mean Square Error percent-
ages for different kernel methods used for SVR.
MSE (%) indicates by what percentage the pre-
dicted TDIs differ from the observed TDIs. In our
setting, quadratic polynomial kernel with c=3.0
outperforms other kernels. The predicted TDIs are
well correlated with the empirical TDIs. This tells
us that even if the predicted scores are not as ac-
curate as desired, the system is capable of ranking
sentences in correct order. Table 2 presents exam-
ples from the test dataset for which the observed
TDI (TDIO) and the TDI predicted by polynomial
kernel based SVR (TDIP ) are shown.
Our larger goal is to group unknown sentences
into different categories by the level of transla-
4http://www.wordnet.princeton.edu
5http://www.nlp.stanford.edu/software/
lex-parser.html
349
Example L DP SC TDIO TDIP Error
1. American Express recently
announced a second round
of job cuts. 10 10 1.8 0.24 0.23 4%
2. Sociology is a relatively
new academic discipline. 7 6 3.7 0.49 0.53 8%
Table 2: Example sentences from the test dataset.
tion difficulty. For that, we tried to manually as-
sign three different class labels to sentences viz.
easy, medium and hard based on the empirical
TDI scores. The ranges of scores chosen for easy,
medium and hard categories were [0-0.3], [0.3-
0.75] and [0.75-1.0] respectively (by trial and er-
ror). Then we trained a Support Vector Rank
(Joachims, 2006) with default parameters using
different kernel methods. The ranking framework
achieves a maximum 67.5% accuracy on the test
data. The accuracy should increase by adding
more data to the training dataset.
6 Conclusion
This paper introduces an approach to quantify-
ing translation difficulty and automatically assign-
ing difficulty levels to unseen sentences. It estab-
lishes a relationship between the intrinsic senten-
tial properties, viz., length (L), degree of polysemy
(DP) and structural complexity (SC), on one hand
and the Translation Difficulty Index (TDI), on the
other. Future work includes deeper investigation
into other linguistic factors such as presence of do-
main specific terms, target language properties etc.
and applying more sophisticated cognitive analy-
sis techniques for more reliable TDI score. We
would like to make use of inter-annotator agree-
ment to decide the boundaries for the translation
difficulty categories. Extending the study to differ-
ent language pairs and studying the applicability
of this technique for Machine Translation Quality
Estimation are also on the agenda.
Acknowledgments
We would like to thank the CRITT, CBS group for
their help in manual correction of TPR data. In
particular, thanks to Barto Mesa and Khristina for
helping with Spanish and Danish dataset correc-
tions.
References
Campbell, S., and Hale, S. 1999. What makes a text
difficult to translate? Refereed Proceedings of the
23rd Annual ALAA Congress.
Carl, M. 2012. Translog-II: A Program for Record-
ing User Activity Data for Empirical Reading and
Writing Research In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation, European Language Resources Associ-
ation (ELRA)
Carl, M. 2012 The CRITT TPR-DB 1.0: A Database
for Empirical Human Translation Process Research.
AMTA 2012 Workshop on Post-Editing Technology
and Practice (WPTP-2012).
Chall, J. S., and Dale, E. 1995. Readability revisited:
the new Dale-Chall readability formula Cambridge,
Mass.: Brookline Books.
Dragsted, B. 2010. Co-ordination of reading andwrit-
ing processes in translation. Contribution to Trans-
lation and Cognition, Shreve, G. and Angelone,
E.(eds.)Cognitive Science Society.
Fry, E. 1977 Fry?s readability graph: Clarification,
validity, and extension to level 17 Journal of Read-
ing, 21(3), 242-252.
Hornof, A. J. and Halverson, T. 2002 Cleaning up sys-
tematic error in eye-tracking data by using required
fixation locations. Behavior Research Methods, In-
struments, and Computers, 34, 592604.
Joachims, T., Schlkopf, B. ,Burges, C and A. Smola
(ed.). 1999. Making large-Scale SVM Learning
Practical. Advances in Kernel Methods - Support
Vector Learning. MIT-Press, 1999,
Joachims, T. 2006 Training Linear SVMs in Lin-
ear Time Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Kincaid, J. P., Fishburne, R. P., Jr., Rogers, R. L., and
Chissom, B. S. 1975. Derivation of New Read-
ability Formulas (Automated Readability Index, Fog
Count and Flesch Reading Ease Formula) for Navy
Enlisted Personnel Millington, Tennessee: Naval
Air Station Memphis,pp. 8-75.
350
Lin, D. 1996 On the structural complexity of natural
language sentences. Proceeding of the 16th Inter-
national Conference on Computational Linguistics
(COLING), pp. 729733.
Mishra, A., Carl, M, Bhattacharyya, P. 2012 A
heuristic-based approach for systematic error cor-
rection of gaze datafor reading. In MichaelCarl, P.B.
and Choudhary, K.K., editors, Proceedings of the
First Workshop on Eye-tracking and Natural Lan-
guage Processing, Mumbai, India. The COLING
2012 Organizing Committee
von der Malsburg, T., Vasishth, S., and Kliegl, R. 2012
Scanpaths in reading are informative about sen-
tence processing. In MichaelCarl, P.B. and Choud-
hary, K.K., editors, Proceedings of the First Work-
shop on Eye-tracking and Natural Language Pro-
cessing, Mumbai, India. The COLING 2012 Orga-
nizing Committee
351
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 175?180,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
TransDoop: A Map-Reduce based Crowdsourced Translation for
Complex Domains
Anoop Kunchukuttan?, Rajen Chatterjee?, Shourya Roy?, Abhijit Mishra?,
Pushpak Bhattacharyya?
? Department of Computer Science and Engineering, IIT Bombay,
{anoopk,abhijitmishra,pb}@cse.iitb.ac.in, rajen.k.chatterjee@gmail.com
? Xerox India Research Centre,
Shourya.Roy@xerox.com
Abstract
Large amount of parallel corpora is re-
quired for building Statistical Machine
Translation (SMT) systems. We describe
the TransDoop system for gathering trans-
lations to create parallel corpora from on-
line crowd workforce who have familiar-
ity with multiple languages but are not
expert translators. Our system uses a
Map-Reduce-like approach to translation
crowdsourcing where sentence translation
is decomposed into the following smaller
tasks: (a) translation of constituent phrases
of the sentence; (b) validation of qual-
ity of the phrase translations; and (c)
composition of complete sentence trans-
lations from phrase translations. Trans-
Doop incorporates quality control mech-
anisms and easy-to-use worker user in-
terfaces designed to address issues with
translation crowdsourcing. We have eval-
uated the crowd?s output using the ME-
TEOR metric. For a complex domain like
judicial proceedings, the higher scores ob-
tained by the map-reduce based approach
compared to complete sentence translation
establishes the efficacy of our work.
1 Introduction
Crowdsourcing is no longer a new term in the do-
main of Computational Linguistics and Machine
Translation research (Callison-Burch and Dredze,
2010; Snow et al, 2008; Callison-Burch, 2009).
Crowdsourcing - basically where task outsourcing
is delegated to a largely unknown Internet audi-
ence - is emerging as a new paradigm of human
in the loop approaches for developing sophisti-
cated techniques for understanding and generat-
ing natural language content. Amazon Mechanical
Turk(AMT) and CrowdFlower 1 are representative
general purpose crowdsourcing platforms where
as Lingotek and Gengo2 are companies targeted
at localization and translation of content typically
leveraging freelancers.
Our interest is towards developing a crowd-
sourcing based system to enable general, non-
expert crowd-workers generate natural language
content equivalent in quality to that of expert lin-
guists. Realization of the potential of attaining
great scalability and cost-benefit of crowdsourcing
for natural language tasks is limited by the abil-
ity of novice multi-lingual workers generate high
quality translations. We have specific interest in
Indian languages due to the large linguistic diver-
sity as well as the scarcity of linguistic resources in
these languages when compared to European lan-
guages. Crowdsourcing is a promising approach
as many Indian languages are spoken by hundreds
of Millions of people (approximately, Hindi-Urdu
by 500M, Bangla by 200M, Punjabi by over 100M
3) coupled with the fact that representation of In-
dian workers in online crowdsourcing platforms is
very high (close to 40% in Amazon Mechanical
Turk (AMT)).
However, this is a non-trivial task owing to lack
of expertise of novice crowd workers in transla-
tion of content. It is well understood that famil-
iarity with multiple languages might not be good
enough for people to generate high quality transla-
tions. This is compounded by lack of sincerity and
in certain cases, dishonest intention of earning re-
wards disproportionate to the effort and time spent
for online tasks. Common techniques for quality
control like gold data based validation and worker
reputation are not effective for a subjective task
1http://www.mturk.com,http://www.
crowdflower.com
2http://www.lingotek.com,http:///www.
gengo.com
3http://en.wikipedia.org/wiki/List_of_
languages_by_total_number_of_speakers
175
like translation which does not have any task spe-
cific measurements. Having expert linguists man-
ually validate crowd generated content defies the
purpose of deploying crowdsourcing on a large
scale.
In this work, we propose a technique, based
on the Divide-and-Conquer principle. The tech-
nique can be considered similar to a Map-Reduce
task run on crowd processors, where the transla-
tion task is split into simpler tasks distributed to
the crowd (the map stage) and the results are later
combined in a reduce stage to generate complete
translations. The attempt is to make translation
tasks easy and intuitive for novice crowd-workers
by providing translations aids to help them gen-
erate high quality of translations. Our contribu-
tion in this work is a end-to-end, crowdsourcing-
platform-independent, translation crowdsourcing
system that completely automates the translation
crowdsourcing task by (i) managing the transla-
tion pipeline through software components and the
crowd; (ii) performing quality control on work-
ers? output; and (iii) interfacing with crowdsourc-
ing service providers. The multi-stage, Map-
reduce approach simplifies the translation task for
crowd workers, while novel design of user inter-
face makes the task convenient for the worker and
discourages spamming. The system thus offers the
potential to generate high quality parallel corpora
on a large scale.
We discuss related work in Section 2 and the
multi-staged approach which is central to our sys-
tem in Section 3. Section 4 describes the sys-
tem architecture and workflow, while Section 5
presents important aspects of the user interfaces
in the system. We present our preliminary exper-
iments and observations in Section 6. Section 7
concludes the paper, pointing to future directions.
2 Related Work
Lately, crowdsourcing has been explored as a
source for generating data for NLP tasks (Snow
et al, 2008; Callison-Burch and Dredze, 2010).
Specifically, it has been explored as a channel for
collecting different resources for SMT - evalua-
tions of MT output (Callison-Burch, 2009), word
alignments in parallel sentences (Gao et al, 2010)
and post-edited versions of MT output (Aikawa et
al., 2012). Ambati and Vogel (2010), Kunchukut-
tan et al (2012) have shown the feasibility of
crowdsourcing for collecting parallel corpora and
pointed out that quality assurance is a major issue
for successful translation crowdsourcing.
The most popular methods for quality control
of crowdsourced tasks are based on sampling and
redundancy. For translation crowdsourcing, Am-
bati et al (2010) use inter-translator agreement for
selection of a good translation from multiple, re-
dundant worker translations. Zaidan and Callison-
Burch (2011) score translations using a feature
based model comprising sentence level, worker
level and crowd ranking based features. However,
automatic evaluation of translation quality is diffi-
cult, such automatic methods being either inaccu-
rate or expensive. Post et al (2012) have collected
Indic language corpora data utilizing the crowd for
collecting translations as well as validations. The
quality of the validations is ensured using gold-
standard sentence translations. Our approach to
quality control is similar to Post et al (2012), but
we work at the level of phrases.
While most crowdsourcing activities for data
gathering has been concerned with collecting sim-
ple annotations like relevance judgments, there has
been work to explore the use of crowdsourcing
for more complex tasks, of which translation is
a good example. Little et al (2010) propose that
many complex tasks can be modeled either as iter-
ative workflows (where workers iteratively build
on each other?s works) or as parallel workflows
(where workers solve the tasks in parallel, with the
best result voted upon later). Kittur et al (2011)
suggest a map-and-reduce approach to solve com-
plex problems, where a problem is decomposed
into smaller problems, which are solved in the map
stage and the results are combined in the reduce
stage. Our method can be seen as an instance
of the map-reduce approach applied to translation
crowdsourcing, with two map stages (phrase trans-
lation and translation validation) and one reduce
stage (sentence combination).
3 Multi-Stage Crowdsourcing Pipeline
Our system is based on a multi-stage pipeline,
whose central idea is to simplify the translation
task into smaller tasks. The high level block di-
agram of the system is shown in Figure 1. Source
language documents are sentencified using stan-
dard NLP tokenizers and sentence splitters. Ex-
tracted sentences are then split into phrases us-
ing a standard chunker and rule-based merging
of small chunks. This step creates small phrases
176
Figure 1: Multistage crowdsourced translation
from complex sentences which can be easily and
independently translated. This leads to a crowd-
sourcing pipeline, with three stages of tasks for the
crowd: Phrase Translation (PT), Phrase Transla-
tion Validation (PV), Sentence Composition (SC).
A group of crowd workers translate source lan-
guage phrases, the translations are validated by a
different group of workers and finally a third group
of workers put the phrase translation together to
create target language sentences. The validation
is done by workers by providing ratings on a k-
point scale. This kind of divide and conquer ap-
proach helps to tackle the complexity of crowd-
sourcing translations since: (1) the tasks are sim-
pler for workers; (2) uniformity of smaller tasks
brings about efficiency as in any industrial assem-
bly line; (3) pricing can be controlled for each
stage depending on the complexity; and (4) quality
control can be performed better for smaller tasks.
4 System Architecture
Figure 2 shows the architecture of TransDoop,
which implements the 3-stage pipeline. The major
design considerations were: (i) translation crowd-
sourcing pipeline should be independent of spe-
cific crowdsourcing platforms; (ii) support multi-
ple crowdsourcing platforms; (iii) customize job
parameters like pricing, quality control method
and task design; and (iv) support multiple lan-
guages and domains.
The core component in the system is the
Crowdsourcing Engine. The engine manages the
execution of the crowdsourcing pipeline, lifecycle
of jobs and quality control of submitted tasks. The
Engine exposes its capabilities through the Re-
quester API, which can be used by clients for
setting up, customizing and monitoring transla-
tion crowdsourcing jobs and controlling their exe-
cution. These capabilities are made available to
requesters via the Requester Portal. In order
to make the crowdsourcing engine independent
of any specific crowdsourcing platform, platform
specific Connectors are developed. The Crowd-
sourcing system makes the tasks to be crowd-
sourced available through the Connector API.
The connectors are responsible for polling the en-
gine for tasks to be crowdsourced, pushing the
tasks to crowdsourcing platforms, hosting worker
interfaces for the tasks and pushing the results
back to the engine after they have been completed
by workers on the crowdsourcing platform. Cur-
rently the system supports the AMT crowdsourc-
ing platform.
Figure 3 depicts the lifecycle of a translation
crowdsourcing job. The requester initiates a trans-
lation job for a document (a set of sentences). The
Crowdsourcing Engine schedules the job for exe-
cution. It first splits each sentence into phrases.
For the job, PT tasks are created and made avail-
able through the Connector API. The connector
for the specified platform periodically polls the
Crowdsourcing Engine via the Connector API.
Once the connector has new PT tasks for crowd-
sourcing, it interacts with the crowdsourcing plat-
form to request crowdsourcing services. The con-
nector monitors the progress of the tasks and on
completion provides the results and execution sta-
tus to the Crowdsourcing Engine. Once all the PT
tasks for the job are completed, the crowdsourcing
Engine initiates the PV task to obtain validations
for the translations. The Quality Control system
kicks in when all the PV tasks for the job have
been completed.
The quality control (QC) relies on a combina-
tion of sampling and redundancy. Each PV task
has a few gold-standard phrase translation pairs,
which is used to ensure that the validators are hon-
estly doing their tasks. The judgments from the
177
Figure 2: Architecture of TransDoop
Figure 3: Lifecycle of a Translation Job
good validators are used to determine the quality
of the phrase translation, based on majority voting,
average rating, etc. using multiple judgments col-
lected for each phrase translation. If any phrase
validations or translations are incorrect, then the
corresponding phrases/translations are again sent
to the PT/PV stage as the case may be. This will
continue until all phrase translations in the job are
correctly translated or a pre-configured number of
iterations are done.
Once phrase translations are obtained for all
phrases in a sentence, the Crowdsourcing Engine
creates SC tasks, where the workers are asked
to compose a single correct, coherent translation
from the phrase translation obtained in the previ-
ous stages.
5 User Interfaces
5.1 Worker User Interfaces
This section describes the worker user interfaces
for each stage in the pipeline. These are man-
aged by the Connector and have been designed to
make the task convenient for the worker and pre-
vent spam submissions. In the rest of the section,
we describe the salient features of the PT and SC
UI?s. PV UI is similar to k-scale voting tasks com-
monly found in crowdsourcing platforms.
? Translation UI: Figure 4a shows the trans-
lation UI for the PT stage. The user in-
terface discourages spamming by: (a) dis-
playing source text as images; and (b) alert-
ing workers if they don?t provide a transla-
tion or spend very little time on a task. The
UI also provides transliteration support for
non-Latin scripts (especially helpful for Indic
scripts). A Vocabulary Support, which shows
translation suggestions for word sequences
appearing in the source phrase, is also avail-
able. Suggested translations can be copied to
the input area with ease and speed.
? Sentence Translation Composition UI: The
sentence translation composition UI (shown
in Figure 4b) facilitates composition of sen-
tence translations from phrase translations.
First, the worker can drag and rearrange the
translated phrases into the right order, fol-
lowed by reordering of individual words.
This is important because many Indian lan-
guages have different constituent order ( S-O-
V) with respect to English (S-V-O). Finally,
the synthesized language sentence can be
post-edited to correct spelling, case marking,
inflectional errors, etc. The system also cap-
tures the reordering performed by the worker,
an important byproduct, which can be used
for training reordering models for SMT.
5.2 Requester UI
The system provides a Requester Portal through
which the requester can create, control and mon-
itor jobs and retrieve results. The portal allows
the requester to customize the job during creation
by configuring various parameters: (a) domain
and language pair (b) entire sentence vs multi-
stage translation (c) price for task at each stage
(d) task design (number of tasks in a task group,
etc.) (e) translation redundancy (f) validation qual-
ity parameters. Translation redundancy refers to
the number of translations requested for a source
phrase. Validation redundancy refers to the num-
ber of validations collected for each phrase trans-
lation pair and the redundancy based acceptance
criteria for phrase translations (majority, consen-
sus, threshold, etc.)
178
(a) Phrase Translation UI (b) Sentence Composition UI
Figure 4: Worker User Interfaces
6 Experiments and Observations
Using TransDoop, we conducted a set of small-
scale, preliminary translation experiments. We ob-
tained translations for English-Hindi and English-
Marathi language pairs for the Judicial and
Tourism domains. For each experiment, 15 sen-
tences were given as input to the pipeline. For
evaluation, we chose METEOR, a well-known
translation evaluation metric (Banerjee and Lavie,
2005). We compared the results obtained from the
crowdsourcing system with a expert human trans-
lation and the output of Google Translate. We also
compared two expert translations using METEOR
to establish a skyline for the translation accuracy.
Table 1 summarizes the results of our experiments.
The translations with Quality Control and mul-
tistage pipeline are better than Google translations
and translations obtained from the crowd without
any quality control, as evaluated by METEOR.
Multi-stage translation yields better than complete
sentence translation. Moreover, the translation
quality is comparable to that of expert human
translation. This behavior is observed across the
two language pairs and domains. This can be seen
in some examples of crowdsourced translations
obtained through the system which are shown in
Table 2.
Incorrect splitting of sentences can cause diffi-
culties in translation for the worker. For instance,
discontinuous phrases will not be available to the
worker as a single translation unit. In the English
interrogative sentence, the noun phrase splits the
verb phrase, therefore the auxiliary and main verb
could be in different translation units. e.g.
Why did you buy the book?
In addition, the phrase structures of the source
and target languages may not map, making trans-
lation difficult. For instance, the vaala modifier in
Hindi translates to a clause in English. It does not
contain any tense information, therefore the tense
of the English clause cannot be determined by the
worker. e.g.
Lucknow vaalaa ladkaa
could translate to any one of:
the boy who lives/lived/is living in Lucknow
We rely on the worker in sentence composition
stage to correct mistakes due to these inadequacies
and compose a good translation. In addition, the
worker in the PT stage could be provided with the
sentence context for translation. However, there
is a tradeoff between the cognitive load of context
processing versus uncertainty in translation. More
elaborately, to what extent can the cognitive load
be reduced before uncertainty of translation sets
in? Similarly, how much of context can be shown
before the cognitive load becomes pressing?
7 Conclusions
In this system demonstration, we present Trans-
Doop as a translation crowdsourcing system which
has the potential to harness the strength of the
crowd to collect high quality human translations
on a large scale. It simplifies the tedious trans-
lation tasks by decomposing them into several
?easy-to-solve? subtasks while ensuring quality.
Our evaluation on small scale data shows that
the multistage approach performs better than com-
plete sentence translation. We would like to exten-
sively use this platform for large scale experiments
on more language pairs and complex domains like
Health, Parliamentary Proceedings, Technical and
Scientific literature etc. to establish the utility of
179
Language Pair Domain Google No QC Translation with QC Reference
Translate single stage multi stage Human
en-mr Tourism 0.227? 0.30 0.368 0.372 0.48
en-hi Tourism 0.292 0.363 0.387 0.422 0.51
en-hi Judicial 0.252 0.30 0.388 0.436 0.49
Table 1: Experimental Results: Comparison of METEOR scores for different techniques, language pairs and domains
?Translated by an internal Moses-based SMT system
Accordingly the penalty imposed by AO is not justified and the same is cancelled.
isk an  sAr e aO ?ArA lgAy gy d\X uEcta nhF\ h{ aOr ek hF r? kr EdyA h{
Accordingly A O by imposed penalty justified not is and one also cancel did
tadAn  sAr e ao ?ArA lgAyA gyA d\X jAy) nhF\ h{ aOr us r? kr EdyA h{
Accordingly A O by imposed penalty justified not is and that cancel did
(a) English-Hindi Judicial Translation
A crowd of devotees engulf Haridwar during the time of daily prayer in the evening
fAm m\ d{Enk ?ATnA k smy k dOrAn B?o\ ko apnF cpV m\ l hEr?Ar kF BFX
evening in daily prayer of time during devotees its engulf in take Haridwar of crowd
??Al  ao\ kF BFX fAm m\ d{Enk ?ATnA k smy hEr?Ar ko apnF cpV m\ ltaF h{
devotees of crowd evening in daily prayer of time haridwar its engulf in take
(b) English-Hindi Tourism Translation
Table 2: Examples of translation from Google and three
staged pipeline for source sentence (2nd, 3rd and 1st rows
of each table respectively). Domains and languages are indi-
cated above.
the method for collection of parallel corpora on a
large scale.
References
Takako Aikawa, Kentaro Yamamoto, and Hitoshi Isa-
hara. 2012. The impact of crowdsourcing post-
editing with the collaborative translation frame-
work. In Advances in Natural Language Processing.
Springer Berlin Heidelberg.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation sys-
tems? In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. Language Resources and Evalua-
tion LREC.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with amazon?s me-
chanical turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing.
Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A
semi-supervised word alignment algorithm with par-
tial manual alignments. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR.
Aniket Kittur, Boris Smus, Susheel Khamkar, and
Robert E Kraut. 2011. Crowdforge: Crowdsourc-
ing complex work. In Proceedings of the 24th an-
nual ACM symposium on User interface software
and technology.
Anoop Kunchukuttan, Shourya Roy, Pratik Patel,
Kushal Ladha, Somya Gupta, Mitesh Khapra, and
Pushpak Bhattacharyya. 2012. Experiences in re-
source generation for machine translation through
crowdsourcing. Language Resources and Evalua-
tion LREC.
Greg Little, Lydia B Chilton, Max Goldman, and
Robert C Miller. 2010. Exploring iterative and par-
allel human computation processes. In Proceedings
of the ACM SIGKDD workshop on human computa-
tion.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Omar Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
180
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 36?41,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Measuring Sentiment Annotation Complexity of Text
Aditya Joshi
1,2,3?
Abhijit Mishra
1
Nivvedan Senthamilselvan
1
Pushpak Bhattacharyya
1
1
IIT Bombay, India,
2
Monash University, Australia
3
IITB-Monash Research Academy, India
{adityaj, abhijitmishra, nivvedan, pb}@cse.iitb.ac.in
Abstract
The effort required for a human annota-
tor to detect sentiment is not uniform for
all texts, irrespective of his/her expertise.
We aim to predict a score that quantifies
this effort, using linguistic properties of
the text. Our proposed metric is called
Sentiment Annotation Complexity (SAC).
As for training data, since any direct judg-
ment of complexity by a human annota-
tor is fraught with subjectivity, we rely on
cognitive evidence from eye-tracking. The
sentences in our dataset are labeled with
SAC scores derived from eye-fixation du-
ration. Using linguistic features and anno-
tated SACs, we train a regressor that pre-
dicts the SAC with a best mean error rate of
22.02% for five-fold cross-validation. We
also study the correlation between a hu-
man annotator?s perception of complexity
and a machine?s confidence in polarity de-
termination. The merit of our work lies in
(a) deciding the sentiment annotation cost
in, for example, a crowdsourcing setting,
(b) choosing the right classifier for senti-
ment prediction.
1 Introduction
The effort required by a human annotator to de-
tect sentiment is not uniform for all texts. Com-
pare the hypothetical tweet ?Just what I wanted: a
good pizza.? with ?Just what I wanted: a cold
pizza.?. The two are lexically and structurally
similar. However, because of the sarcasm in the
second tweet (in ?cold? pizza, an undesirable sit-
uation followed by a positive sentiment phrase
?just what I wanted?, as discussed in Riloff et al
(2013)), it is more complex than the first for senti-
ment annotation. Thus, independent of how good
?
- Aditya is funded by the TCS Research Fellowship Pro-
gram.
the annotator is, there are sentences which will be
perceived to be more complex than others. With
regard to this, we introduce a metric called senti-
ment annotation complexity (SAC). The SAC of a
given piece of text (sentences, in our case) can be
predicted using the linguistic properties of the text
as features.
The primary question is whether such complex-
ity measurement is necessary at all. Fort et al
(2012) describe the necessity of annotation com-
plexity measurement in manual annotation tasks.
Measuring annotation complexity is beneficial in
annotation crowdsourcing. If the complexity of
the text can be estimated even before the annota-
tion begins, the pricing model can be fine-tuned
(pay less for sentences that are easy to annotate,
for example). Also, in terms of an automatic SA
engine which has multiple classifiers in its ensem-
ble, a classifier may be chosen based on the com-
plexity of sentiment annotation (for example, use
a rule-based classifier for simple sentences and a
more complex classifier for other sentences). Our
metric adds value to sentiment annotation and sen-
timent analysis, in these two ways. The fact that
sentiment expression may be complex is evident
from a study of comparative sentences by Gana-
pathibhotla and Liu (2008), sarcasm by Riloff et
al. (2013), thwarting by Ramteke et al (2013) or
implicit sentiment by Balahur et al (2011). To
the best of our knowledge, there is no general ap-
proach to ?measure? how complex a piece of text
is, in terms of sentiment annotation.
The central challenge here is to annotate a data
set with SAC. To measure the ?actual? time spent
by an annotator on a piece of text, we use an eye-
tracker to record eye-fixation duration: the time
for which the annotator has actually focused on
the sentence during annotation. Eye-tracking an-
notations have been used to study the cognitive as-
pects of language processing tasks like translation
by Dragsted (2010) and sense disambiguation by
36
Joshi et al (2011). Mishra et al (2013) present a
technique to determine translation difficulty index.
The work closest to ours is by Scott et al (2011)
who use eye-tracking to study the role of emotion
words in reading.
The novelty of our work is three-fold: (a) The
proposition of a metric to measure complexity of
sentiment annotation, (b) The adaptation of past
work that uses eye-tracking for NLP in the con-
text of sentiment annotation, (c) The learning of
regressors that automatically predict SAC using
linguistic features.
2 Understanding Sentiment Annotation
Complexity
The process of sentiment annotation consists of
two sub-processes: comprehension (where the an-
notator understands the content) and sentiment
judgment (where the annotator identifies the sen-
timent). The complexity in sentiment annotation
stems from an interplay of the two and we expect
SAC to capture the combined complexity of both
the sub-processes. In this section, we describe
how complexity may be introduced in sentiment
annotation in different classical layers of NLP.
The simplest form of sentiment annotation com-
plexity is at the lexical level. Consider the sen-
tence ?It is messy, uncouth, incomprehensible, vi-
cious and absurd?. The sentiment words used
in this sentence are uncommon, resulting in com-
plexity.
The next level of sentiment annotation com-
plexity arises due to syntactic complexity. Con-
sider the review: ?A somewhat crudely con-
structed but gripping, questing look at a person so
racked with self-loathing, he becomes an enemy to
his own race.?. An annotator will face difficulty
in comprehension as well as sentiment judgment
due to the complicated phrasal structure in this re-
view. Implicit expression of sentiment introduces
complexity at the semantic and pragmatic level.
Sarcasm expressed in ?It?s like an all-star salute to
disney?s cheesy commercialism? leads to difficulty
in sentiment annotation because of positive words
like ?an all-star salute?.
Manual annotation of complexity scores may
not be intuitive and reliable. Hence, we use a cog-
nitive technique to create our annotated dataset.
The underlying idea is: if we monitor annotation
of two textual units of equal length, the more com-
plex unit will take longer to annotate, and hence,
should have a higher SAC. Using the idea of ?an-
notation time? linked with complexity, we devise a
technique to create a dataset annotated with SAC.
It may be thought that inter-annotator agree-
ment (IAA) provides implicit annotation: the
higher the agreement, the easier the piece of text
is for sentiment annotation. However, in case of
multiple expert annotators, this agreement is ex-
pected to be high for most sentences, due to the
expertise. For example, all five annotators agree
with the label for 60% sentences in our data set.
However, the duration for these sentences has a
mean of 0.38 seconds and a standard deviation of
0.27 seconds. This indicates that although IAA is
easy to compute, it does not determine sentiment
annotation complexity of text in itself.
3 Creation of dataset annotated with
SAC
We wish to predict sentiment annotation complex-
ity of the text using a supervised technique. As
stated above, the time-to-annotate is one good can-
didate. However, ?simple time measurement? is
not reliable because the annotator may spend time
not doing any annotation due to fatigue or distrac-
tion. To accurately record the time, we use an
eye-tracking device that measures the ?duration of
eye-fixations
1
?. Another attribute recorded by the
eye-tracker that may have been used is ?saccade
duration
2
?. However, saccade duration is not sig-
nificant for annotation of short text, as in our case.
Hence, the SAC labels of our dataset are fixation
durations with appropriate normalization.
It may be noted that the eye-tracking device is
used only to annotate training data. The actual
prediction of SAC is done using linguistic features
alone.
3.1 Eye-tracking Experimental Setup
We use a sentiment-annotated data set consisting
of movie reviews by (Pang and Lee, 2005) and
tweets from http://help.sentiment140.
com/for-students. A total of 1059 sen-
tences (566 from a movie corpus, 493 from a twit-
ter corpus) are selected.
We then obtain two kinds of annotation from
five paid annotators: (a) sentiment (positive, nega-
tive and objective), (b) eye-movement as recorded
1
A long stay of the visual gaze on a single location.
2
A rapid movement of the eyes between positions of rest
on the sentence.
37
Figure 1: Gaze-data recording using Translog-II
by an eye-tracker. They are given a set of instruc-
tions beforehand and can seek clarifications. This
experiment is conducted as follows:
1. A sentence is displayed to the annotator on
the screen. The annotator verbally states the
sentiment of this sentence, before (s)he can
proceed to the next.
2. While the annotator reads the sentence, a
remote eye-tracker (Model: Tobii TX 300,
Sampling rate: 300Hz) records the eye-
movement data of the annotator. The eye-
tracker is linked to a Translog II soft-
ware (Carl, 2012) in order to record the data.
A snapshot of the software is shown in fig-
ure 1. The dots and circles represent position
of eyes and fixations of the annotator respec-
tively.
3. The experiment then continues in modules of
50 sentences at a time. This is to prevent fa-
tigue over a period of time. Thus, each an-
notator participates in this experiment over a
number of sittings.
We ensure the quality of our dataset in different
ways: (a) Our annotators are instructed to avoid
unnecessary head movements and eye-movements
outside the experiment environment. (b) To min-
imize noise due to head movements further, they
are also asked to state the annotation verbally,
which was then manually recorded, (c) Our an-
notators are students between the ages 20-24 with
English as the primary language of academic in-
struction and have secured a TOEFL iBT score of
110 or above.
We understand that sentiment is nuanced- to-
wards a target, through constructs like sarcasm and
presence of multiple entities. However, we want to
capture the most natural form of sentiment anno-
tation. So, the guidelines are kept to a bare mini-
mum of ?annotating a sentence as positive, nega-
tive and objective as per the speaker?. This exper-
iment results in a data set of 1059 sentences with
a fixation duration recorded for each sentence-
annotator pair
3
The multi-rater kappa IAA for sen-
timent annotation is 0.686.
3.2 Calculating SAC from eye-tracked data
We now need to annotate each sentence with a
SAC. We extract fixation durations of the five an-
notators for each of the annotated sentences. A
single SAC score for sentence s for N annotators
is computed as follows:
SAC(s) =
1
N
N
?
n=1
z(n,dur(s,n))
len(s)
where,
z(n, dur(s, n)) =
dur(s,n)??(dur(n))
?(dur(n))
(1)
In the above formula, N is the total number of an-
notators while n corresponds to a specific annota-
tor. dur(s, n) is the fixation duration of annotator
n on sentence s. len(s) is the number of words
in sentence s. This normalization over number
of words assumes that long sentences may have
high dur(s, n) but do not necessarily have high
SACs. ?(dur(n)), ?(dur(n)) is the mean and
standard deviation of fixation durations for anno-
tator n across all sentences. z(n, .) is a function
that z-normalizes the value for annotator n to stan-
dardize the deviation due to reading speeds. We
convert the SAC values to a scale of 1-10 using
min-max normalization. To understand how the
formula records sentiment annotation complexity,
consider the SACs of examples in section 2. The
sentence ?it is messy , uncouth , incomprehensi-
ble , vicious and absurd? has a SAC of 3.3. On the
other hand, the SAC for the sarcastic sentence ?it?s
like an all-star salute to disney?s cheesy commer-
cialism.? is 8.3.
4 Predictive Framework for SAC
The previous section shows how gold labels for
SAC can be obtained using eye-tracking experi-
ments. This section describes our predictive for
SAC that uses four categories of linguistic fea-
tures: lexical, syntactic, semantic and sentiment-
related in order to capture the subprocesses of an-
notation as described in section 2.
4.1 Experiment Setup
The linguistic features described in Table 3.2 are
extracted from the input sentences. Some of these
3
The complete eye-tracking data is available at:http://
www.cfilt.iitb.ac.in/
?
cognitive-nlp/.
38
Feature Description
Lexical
- Word Count
- Degree of polysemy Average number of Wordnet senses per word
- Mean Word Length Average number of characters per word (commonly used in readability studies
as in the case of Pascual et al (2005))
- %ge of nouns and adjs.
- %ge of Out-of-
vocabulary words
Syntactic
- Dependency Distance Average distance of all pairs of dependent words in the sentence (Lin, 1996)
- Non-terminal to Ter-
minal ratio
Ratio of the number of non-terminals to the number of terminals in the con-
stituency parse of a sentence
Semantic
- Discourse connectors Number of discourse connectors
- Co-reference distance Sum of token distance between co-referring entities of anaphora in a sentence
- Perplexity Trigram perplexity using language models trained on a mixture of sentences
from the Brown corpus, the Amazon Movie corpus and Stanford twitter corpus
(mentioned in Sections 3 and 5)
Sentiment-related (Computed using SentiWordNet (Esuli et al, 2006))
- Subjective Word
Count
- Subjective Score Sum of SentiWordNet scores of all words
- Sentiment Flip Count A positive word followed in sequence by a negative word, or vice versa counts
as one sentiment flip
Table 1: Linguistic Features for the Predictive Framework
features are extracted using Stanford Core NLP
4
tools and NLTK (Bird et al, 2009). Words that
do not appear in Academic Word List
5
and Gen-
eral Service List
6
are treated as out-of-vocabulary
words. The training data consists of 1059 tuples,
with 13 features and gold labels from eye-tracking
experiments.
To predict SAC, we use Support Vector Regres-
sion (SVR) (Joachims, 2006). Since we do not
have any information about the nature of the rela-
tionship between the features and SAC, choosing
SVR allows us to try multiple kernels. We carry
out a 5-fold cross validation for both in-domain
and cross-domain settings, to validate that the re-
gressor does not overfit. The model thus learned is
evaluated using: (a) Error metrics namely, Mean
Squared Error estimate, Mean Absolute Error esti-
mate and Mean Percentage Error. (b) the Pearson
correlation coefficient between the gold and pre-
4
http://nlp.stanford.edu/software/
corenlp.shtml
5
www.victoria.ac.nz/lals/resources/academicwordlist/
6
www.jbauman.com/gsl.html
dicted SAC.
4.2 Results
The results are tabulated in Table 2. Our obser-
vation is that a quadratic kernel performs slightly
better than linear. The correlation values are pos-
itive and indicate that even if the predicted scores
are not as accurate as desired, the system is capa-
ble of ranking sentences in the correct order based
on their sentiment complexity. The mean percent-
age error (MPE) of the regressors ranges between
22-38.21%. The cross-domain MPE is higher than
the rest, as expected.
To understand how each of the features per-
forms, we conducted ablation tests by con-
sidering one feature at a time. Based on
the MPE values, the best features are: Mean
word length (MPE=27.54%), Degree of Polysemy
(MPE=36.83%) and %ge of nouns and adjectives
(MPE=38.55%). To our surprise, word count per-
forms the worst (MPE=85.44%). This is unlike
tasks like translation where length has been shown
39
Kernel Linear Quadratic Cross Domain Linear
Domain Mixed Movie Twitter Mixed Movie Twitter Movie Twitter
MSE 1.79 1.55 1.99 1.68 1.53 1.88 3.17 2.24
MAE 0.93 0.89 0.95 0.91 0.88 0.93 1.39 1.19
MPE 22.49% 23.8% 25.45% 22.02% 23.8% 25% 35.01% 38.21%
Correlation 0.54 0.38 0.56 0.57 0.37 0.6 0.38 0.46
Table 2: Performance of Predictive Framework for 5-fold in-domain and cross-domain validation using
Mean Squared Error (MSE), Mean Absolute Error (MAE) and Mean Percentage Error (MPE) estimates
and correlation with the gold labels.
to be one of the best predictors in translation dif-
ficulty (Mishra et al, 2013). We believe that for
sentiment annotation, longer sentences may have
more lexical clues that help detect the sentiment
more easily. Note that some errors may be intro-
duced in feature extraction due to limitations of
the NLP tools.
5 Discussion
Our proposed metric measures complexity of sen-
timent annotation, as perceived by human annota-
tors. It would be worthwhile to study the human-
machine correlation to see if what is difficult for
a machine is also difficult for a human. In other
words, the goal is to show that the confidence
scores of a sentiment classifier are negatively cor-
related with SAC.
We use three sentiment classification tech-
niques: Na??ve Bayes, MaxEnt and SVM with un-
igrams, bigrams and trigrams as features. The
training datasets used are: a) 10000 movie reviews
from Amazon Corpus (McAuley et. al, 2013) and
b) 20000 tweets from the twitter corpus (same as
mentioned in section 3). Using NLTK and Scikit-
learn
7
with default settings, we generate six posi-
tive/negative classifiers, for all possible combina-
tions of the three models and two datasets.
The confidence score of a classifier
8
for given
text t is computed as follows:
P : Probability of predicted class
Confidence(t) =
?
?
?
P if predicted
polarity is correct
1? P otherwise
(2)
7
http://scikit-learn.org/stable/
8
In case of SVM, the probability of predicted class is com-
puted as given in Platt (1999).
Classifier (Corpus) Correlation
Na??ve Bayes (Movie) -0.06 (73.35)
Na??ve Bayes (Twitter) -0.13 (71.18)
MaxEnt (Movie) -0.29 (72.17)
MaxEnt (Twitter) -0.26 (71.68)
SVM (Movie) -0.24 (66.27)
SVM (Twitter) -0.19 (73.15)
Table 3: Correlation between confidence of the
classifiers with SAC; Numbers in parentheses in-
dicate classifier accuracy (%)
Table 3 presents the accuracy of the classifiers
along with the correlations between the confidence
score and observed SAC values. MaxEnt has the
highest negative correlation of -0.29 and -0.26.
For both domains, we observe a weak yet nega-
tive correlation which suggests that the perception
of difficulty by the classifiers are in line with that
of humans, as captured through SAC.
6 Conclusion & Future Work
We presented a metric called Sentiment Annota-
tion Complexity (SAC), a metric in SA research
that has been unexplored until now. First, the pro-
cess of data preparation through eye tracking, la-
beled with the SAC score was elaborated. Using
this data set and a set of linguistic features, we
trained a regression model to predict SAC. Our
predictive framework for SAC resulted in a mean
percentage error of 22.02%, and a moderate corre-
lation of 0.57 between the predicted and observed
SAC values. Finally, we observe a negative corre-
lation between the classifier confidence scores and
a SAC, as expected. As a future work, we would
like to investigate how SAC of a test sentence can
be used to choose a classifier from an ensemble,
and to determine the pre-processing steps (entity-
relationship extraction, for example).
40
References
Balahur, Alexandra and Hermida, Jes?us M and Mon-
toyo, Andr?es. 2011. Detecting implicit expressions
of sentiment in text based on commonsense knowl-
edge. Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis,53-60.
Batali, John and Searle, John R. 1995. The Rediscov-
ery of the Mind. Artif. Intell., Vol. 77, 177-193.
Steven Bird and Ewan Klein and Edward Loper. 2009.
Natural Language Processing with Python O?Reilly
Media.
Carl, M. 2012. Translog-II: A Program for Record-
ing User Activity Data for Empirical Reading and
Writing Research. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation, European Language Resources Associ-
ation.
Dragsted, B. 2010. 2010. Co-ordination of reading
and writing processes in translation. Contribution
to Translation and Cognition. Shreve, G. and An-
gelone, E.(eds.)Cognitive Science Society.
Esuli, Andrea and Sebastiani, Fabrizio. 2006. Sen-
tiwordnet: A publicly available lexical resource for
opinion mining. Proceedings of LREC, vol. 6, 417-
422.
Fellbaum, Christiane 1998. WordNet: An electronic
lexical database. 1998. Cambridge. MA: MIT Press.
Fort, Kar?en and Nazarenko, Adeline and Rosset, So-
phie et al2012. Modeling the complexity of manual
annotation tasks: A grid of analysis Proceedings of
the International Conference on Computational Lin-
guistics.
Ganapathibhotla, G and Liu, Bing. 2008. Identifying
preferred entities in comparative sentences. 22nd In-
ternational Conference on Computational Linguis-
tics (COLING).
Gonz?alez-Ib?a?nez, Roberto and Muresan, Smaranda and
Wacholder, Nina 2011. Identifying Sarcasm in
Twitter: A Closer Look. ACL (Short Papers) 581-
586.
Joachims, T. 2006 Training Linear SVMs in Lin-
ear Time Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Lin, D. 1996 On the structural complexity of natural
language sentences. Proceeding of the 16th Inter-
national Conference on Computational Linguistics
(COLING), pp. 729733.
Mart?nez-G?omez, Pascual and Aizawa, Akiko. 2013.
Diagnosing Causes of Reading Difficulty using
Bayesian Networks International Joint Conference
on Natural Language Processing, 13831391.
McAuley, Julian John and Leskovec, Jure 2013 From
amateurs to connoisseurs: modeling the evolution of
user expertise through online reviews. Proceedings
of the 22nd international conference on World Wide
Web.
Mishra, Abhijit and Bhattacharyya, Pushpak and Carl,
Michael. 2013. Automatically Predicting Sentence
Translation Difficulty Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), 346-351.
Narayanan, Ramanathan and Liu, Bing and Choudhary,
Alok 2009. Sentiment Analysis of Conditional Sen-
tences. Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
180-189.
Pang, Bo and Lee, Lillian. 2008. Opinion mining and
sentiment analysis Foundations and trends in infor-
mation retrieval, vol. 2, 1-135.
Pang, Bo and Lee, Lillian. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, 115-124.
Platt, John and others. 1999. Probabilistic outputs for
support vector machines and comparisons to regular-
ized likelihood methods Advances in large margin
classifiers, vol. 10, 61-74.
Ramteke, Ankit and Malu, Akshat and Bhattacharyya,
Pushpak and Nath, J. Saketha 2013. Detect-
ing Turnarounds in Sentiment Analysis: Thwarting
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), 860-865.
Riloff, Ellen and Qadir, Ashequl and Surve, Prafulla
and De Silva, Lalindra and Gilbert, Nathan and
Huang, Ruihong 2013. Sarcasm as Contrast be-
tween a Positive Sentiment and Negative Situation
Conference on Empirical Methods in Natural Lan-
guage Processing, Seattle, USA.
Salil Joshi, Diptesh Kanojia and Pushpak Bhat-
tacharyya. 2013. More than meets the eye: Study
of Human Cognition in Sense Annotation. NAACL
HLT 2013, Atlanta, USA.
Scott G. , O Donnell P and Sereno S. 2012. Emotion
Words Affect Eye Fixations During Reading. Jour-
nal of Experimental Psychology:Learning, Memory,
and Cognition 2012, Vol. 38, No. 3, 783-792
Siegel, Sidney and N. J. Castellan, Jr. 1988. Nonpara-
metric Statistics for the Behavioral Sciences. Second
edition. McGraw-Hill.
41
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 142?146,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
A cognitive study of subjectivity extraction in sentiment annotation
Abhijit Mishra
1
Aditya Joshi
1,2,3
Pushpak Bhattacharyya
1
1
IIT Bombay, India
2
Monash University, Australia
3
IITB-Monash Research Academy, India
{abhijitmishra, adityaj, pb}@cse.iitb.ac.in
Abstract
Existing sentiment analysers are weak AI
systems: they try to capture the function-
ality of human sentiment detection faculty,
without worrying about how such faculty
is realized in the hardware of the human.
These analysers are agnostic of the actual
cognitive processes involved. This, how-
ever, does not deliver when applications
demand order of magnitude facelift in ac-
curacy, as well as insight into characteris-
tics of sentiment detection process.
In this paper, we present a cognitive study
of sentiment detection from the perspec-
tive of strong AI. We study the sentiment
detection process of a set of human ?sen-
timent readers?. Using eye-tracking, we
show that on the way to sentiment de-
tection, humans first extract subjectivity.
They focus attention on a subset of sen-
tences before arriving at the overall senti-
ment. This they do either through ?antici-
pation? where sentences are skipped dur-
ing the first pass of reading, or through
?homing? where a subset of the sentences
are read over multiple passes, or through
both. ?Homing? behaviour is also ob-
served at the sub-sentence level in com-
plex sentiment phenomena like sarcasm.
1 Introduction
Over the years, supervised approaches using
polarity-annotated datasets have shown promise
for SA (Pang and Lee, 2008). However, an al-
ternate line of thought has co-existed. Pang and
Lee (2004) showed that for SA, instead of a doc-
ument in its entirety, an extract of the subjec-
tive sentences alone can be used. This process
of generating a subjective extract is referred to
as subjectivity extraction. Mukherjee and Bhat-
tacharyya (2012) show that for sentiment predic-
tion of movie reviews, subjectivity extraction may
be used to discard the sentences describing movie
plots since they do not contribute towards the
speaker?s view of the movie.
While subjectivity extraction helps sentiment
classification, the reason has not been sufficiently
examined from the perspective of strong AI. The
classical definition of strong AI suggests that a
machine must be perform sentiment analysis in
a manner and accuracy similar to human beings.
Our paper takes a step in this direction. We study
the cognitive processes underlying sentiment an-
notation using eye-fixation data of the participants.
Our work is novel in two ways:
? We view documents as a set of sentences
through which sentiment changes. We show
that the nature of these polarity oscillations
leads to changes in the reading behavior.
? To the best of our knowledge, the idea of us-
ing eye-tracking to validate assumptions is
novel in case of sentiment analysis and many
NLP applications.
2 Sentiment oscillations & subjectivity
extraction
We categorize subjective documents as linear and
oscillating. A linear subjective document is the
one where all or most sentences have the same po-
larity. On the other hand, an oscillating subjective
document contains sentences of contrasting polar-
ity (viz. positive and negative). Our discussions
on two forms of subjectivity extraction use the
concepts of linear and oscillating subjective doc-
uments.
Consider a situation where a human reader
needs to annotate two documents with sentiment.
Assume that the first document is linear subjec-
tive - with ten sentences, all of them positive. In
142
case of this document, when he/she reads a cou-
ple of sentences with the same polarity, he/she be-
gins to assume that the next sentence will have the
same sentiment and hence, skips through it. We
refer to this behavior as anticipation. Now, let the
second document be an oscillating subjective doc-
ument with ten sentences, the first three positive,
the next four negative and the last three positive.
In this case, when a human annotator reads this
document and sees the sentiment flip early on, the
annotator begins to carefully read the document.
After completing a first pass of reading, the anno-
tator moves back to read certain crucial sentences.
We refer to this behavior as homing.
The following sections describe our observa-
tions in detail. Based on our experiments, we ob-
serve these two kinds of subjectivity extraction in
our participants: subjectivity extraction as a result
of anticipation and subjectivity extraction as a re-
sult of homing - for linear and oscillating docu-
ments respectively.
3 Experiment Setup
This section describes the framework used for our
eye-tracking experiment. A participant is given
the task of annotating documents with one out of
the following labels: positive, negative and ob-
jective. While she reads the document, her eye-
fixations are recorded.
To log eye-fixation data, we use Tobii T120
remote eye-tracker with Translog(Carl, 2012).
Translog is a freeware for recording eye move-
ments and keystrokes during translation. We con-
figure Translog for reading with the goal of senti-
ment.
3.1 Document description
We choose three movie reviews in English from
IMDB (http://www.imdb.com) and indicate them
as D0, D1 and D2. The lengths of D0, D1 and
D2 are 10, 9 and 13 sentences respectively. Using
the gold-standard rating given by the writer, we
derive the polarity of D0, D1 and D2 as positive,
negative and positive respectively. The three doc-
uments represent three different styles of reviews:
D0 is positive throughout (linear subjective), D1
contains sarcastic statements (linear subjective but
may be perceived as oscillating due to linguistic
difficulty) while D2 consists of many flips in sen-
timent (oscillating subjective).
It may seem that the data set is small and
may not lead to significant findings. However,
we wished to capture the most natural form of
sentiment-oriented reading. A larger data set
would have weakened the experiment because: (i)
Sentiment patterns (linear v/s subjective) begin to
become predictable to a participant if she reads
many documents one after the other. (ii) There
is a possibility that fatigue introduces unexpected
error. To ensure that our observations were signif-
icant despite the limited size of the data set, we
increased the number of our participants to 12.
3.2 Participant description
Our participants are 24-30 year-old graduate stu-
dents with English as the primary language of aca-
demic instruction. We represent them as P0, P1
and so on. The polarity for the documents as re-
ported by the participants are shown in Table 1.
All participants correctly identified the polarity of
document D0. Participant P9 reported that D1 is
confusing. 4 out of 12 participants were unable to
detect correct opinion in D2.
3.3 Experiment Description
We obtain two kinds of annotation from our an-
notators: (a) sentiment (positive, negative and ob-
jective), (b) eye-movement as recorded by an eye-
tracker. They are given a set of instructions before-
hand and can seek clarifications. This experiment
is conducted as follows:
1. A complete document is displayed on the
screen. The font size and line separation are
set to 17pt and 1.5 cm respectively to ensure
clear visibility and minimize recording error.
2. The annotator verbally states the sentiment of
this sentence, before (s)he can proceed to the
next.
3. While the annotator is reading the sentence,
a remote eye-tracker (Model: Tobii TX 300,
Sampling rate: 300Hz) records the eye-
movement data of the annotator. The eye-
tracker is linked to Translog II software (Carl,
2012) in order to record the data. A snap-
shot of the software is shown in figure 1. The
dots and circles represent position of eyes and
fixations of the annotator respectively. Each
eye-fixation that is recorded consists of: co-
ordinates, timestamp and duration. These
three parameters have been used to generate
sentence progression graphs.
143
Document Orig P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11
D0 +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve
D1 -ve -ve +ve -ve -ve -ve -ve -ve -ve -ve Neu/-ve -ve -ve
D2 +ve +ve +ve -ve +ve +ve Neu +ve Neu Neu +ve +ve +ve
Table 1: Polarity of documents as perceived by the writer (original) and the participants +ve, -ve and
Neu represent positive, negative and neutral polarities respectively.
Figure 1: Gaze-data recording using Translog-II
Figure 2: Sentence progression graph for partici-
pant P7 document D0
4 Observations: Subjectivity extraction
through anticipation
In this section, we describe a case in which partic-
ipants skip sentences. We show that anticipation
of sentiment is linked with subjectivity extraction.
Table 2 shows the number of unique and non-
unique sentences that participants read for each
document. The numbers in the last column in-
dicate average values. The table can be read as:
participant P1 reads 8 unique sentences of docu-
ment D0 (thus skipping two sentences) and includ-
ing repetitions, reads 26 sentences. Participant P0
skips as many as six sentences in case of document
D1.
The number of unique sentences read is lower
than sentence count for four out of twelve partic-
ipants in case of document D0. This skipping is
negligible in case of document D1 and D2. Also,
the average non-unique sentence fixations are 21
in case of D0 and 33.83 for D1 although the total
number of sentences in D0 and D1 is almost the
same. This verifies that participants tend to skip
sentences while reading D0.
Figure 2 shows sentence progression graph for
participant P7. The participant reads a series of
sentences and then skips two sentences. This im-
plies that anticipation behaviour was triggered af-
ter reading sentences of the same polarity. Sim-
ilar traits are observed in other participants who
skipped sentences while reading document D0.
5 Observations: Subjectivity extraction
through homing
This section presents a contrasting case of sub-
jectivity extraction. We refer to a reading pattern
as homing
1
when a participant reads a document
completely and returns to read a selected subset of
sentences. We believe that during sentiment an-
notation, this subset is the subjective extract that
the user has created in her mind. We observe this
phenomenon in reading patterns of documents D1
and D2. The former contains sarcasm because of
which parts of sentences may appear to be of con-
trasting polarity while the latter is an oscillating
subjective document.
1
The word is derived from missile guidance systems. The
definition
2
of homing is ?the process of determining the lo-
cation of something, sometimes the source of a transmission,
and going to it.?
144
Document P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 Avg.
D0
Non-unique 9 26 23 17 18 18 35 16 33 19 15 23 21
Unique 8 8 10 10 10 10 10 8 10 8 10 10
D1
Non-unique 5 23 46 13 15 44 35 26 56 57 40 46 33.83
Unique 3 9 9 9 9 9 8 9 9 9 9 9
D2
Non-unique 36 29 67 21 23 51 64 48 54 59 73 80 50.42
Unique 13 13 13 13 13 13 13 13 13 13 13 13
Table 2: Number of unique and non-unique sentences read by each participant
Figure 3: Sentence progression graph of partici-
pant P2 for document D1 (left) and document D2
(right)
Figure 3 shows sentence progression graphs of
participant P2 for documents D1 and D2. For doc-
ument D1, the participant performs one pass of
reading until sequence number 30. A certain sub-
set of sentences are re-visited in the second pass.
On analyzing sentences in the second pass of read-
ing, we observe a considerable overlap in case of
our participants. We also confirm that all of these
sentences are subjective. This means that the sen-
tences that are read after sequence number 30 form
the subjective extract of document D1.
Similar behaviour is observed in case of docu-
ment D2. The difference in this case is that there
is less overlap of sentences read in the second pass
among participants. This implies , for oscillat-
ing subjective documents, the subjective extract is
user/document-specific.
It may be argued that fixations corresponding
Participant TFD-SE PTFD TFC-SE
(secs) (%)
P5 7.3 8 21
P7 3.1 5 11
P9 51.94 10 26
P11 116.6 16 56
Table 3: Reading statistics for second pass reading
for document D1; TFD: Total fixation duration for
subjective extract; PTFD: Proportion of total fix-
ation duration = (TFD)/(Total duration); TFC-SE:
Total fixation count for subjective extract
to second pass reading are stray fixations and not
subjective extracts. Hence, for the second pass
reading of document D1, we tabulate fixation du-
ration, fixation count and proportion of total dura-
tion in Table 3. The fixation duration and fixation
count are both recorded by the eye-tracker. The
fixation counts are substantial and the participants
spend around 5-15% of the total reading time in
the second pass reading. We also confirm that all
of these sentences are subjective. This means that
these portions indeed correspond to subjective ex-
tracts as a result of homing.
6 A note on linguistic challenges
Our claim is that regression after reading an en-
tire document corresponds to the beginning of
a subjective extract. However, we observe that
some regressions may also happen due to senti-
ment changes at the sub-sentence level. Some of
these are as follows.
1. Sarcasm: Sarcasm involves an implicit flip
in the sentiment. Participant P9 does not cor-
rectly predict sentiment of Document D1. On
analyzing her data, we observe multiple re-
gressions on the sentence ?Add to this mess
some of the cheesiest lines and concepts, and
145
there you have it; I would call it a complete
waste of time, but in some sense it is so bad
it is almost worth seeing.? This sentence has
some positive words but is negative towards
the movie. Hence, the participant reads this
portion back and forth.
2. Thwarted expectations: Thwarted expecta-
tions are expressions with a sentiment rever-
sal within a sentence/snippet. Homing is ob-
served in this case as well. Document D2
has a case of thwarted expectations from sen-
tences 10-12 where there is an unexpected
flip of sentiment. In case of some partici-
pants, we observe regression on these sen-
tences multiple times.
7 Related Work
The work closest to ours is by Scott et al. (2011)
who study the role of emotion words in read-
ing using eye-tracking. They show that the eye-
fixation duration for emotion words is consistently
less than neutral words with the exception of high-
frequency negative words. Eye-tracking
3
technol-
ogy has also been used to study the cognitive as-
pects of language processing tasks like translation
and sense disambiguation. Dragsted (2010) ob-
serve co-ordination between reading and writing
during human translation. Similarly, Joshi et al.
(2011) use eye-tracking to correlate fixation dura-
tion with polysemy of words during word sense
disambiguation.
8 Conclusion & Future work
We studied sentiment annotation in the context of
subjectivity extraction using eye-tracking. Based
on how sentiment changes through a document,
humans may perform subjectivity extraction as a
result of either: (a) anticipation or (b) homing.
These observations are in tandem with the past
work that shows benefit of subjectivity extraction
for automatic sentiment classification.
Our study is beneficial in three perspectives: (i)
Sentiment classifiers may use interaction between
sentiment of sentences. Specifically, this can be
modeled using features like sentiment run length
(i.e. maximal span of sentences bearing same
3
Related Terms:
Eye-fixation: Long stay of visual gaze on a single location
Regression: Revisiting a previously read segment
Sentence Progression Graph: Graph showing reading se-
quence of sentences
sentiment) or sentiment flips (i.e. instances where
consecutive sentences bear opposite polarity),
(ii) Crowd-sourced sentiment annotation can
devise variable pricing models based on our study.
Based on anticipation and homing information
about documents, documents can be grouped into
difficulty categories and priced accordingly.
Acknowledgment
We thank Tobii Corporation for lending us their
eye-tracker for this study, and our annotators from
CFILT, IIT Bombay. Aditya is funded by the TCS
Research Fellowship Program.
References
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Sum-
marization Based on Minimum Cuts In Proceedings
of the ACL, 271-278.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis Foundations and Trends in In-
formation Retrieval, 2008, vol. 2, nos.12 1135.
B Dragsted. 2010. Co-ordination of reading and writ-
ing processes in translation. Contribution to Trans-
lation and Cognition. Shreve, G. and Angelone,
E.(eds.)Cognitive Science Society.
Michael Carl. 2012. Translog-II: A Program for
Recording User Activity Data for Empirical Reading
and Writing Research. In Proceedings of the Eight
International Conference on Language Resources
and Evaluation, European Language Resources As-
sociation.
Scott G. , ODonnell P and Sereno S. 2012. Emotion
Words Affect Eye Fixations During Reading. Jour-
nal of Experimental Psychology:Learning, Memory,
and Cognition 2012, Vol. 38, No. 3, 783792.
Salil Joshi, Diptesh Kanojia and Pushpak Bhat-
tacharyya. 2013. More than meets the eye: Study
of Human Cognition in Sense Annotation. NAACL
HLT 2013, Atlanta, USA.
Subhabrata Mukherjee and Pushpak Bhattacharyya.
2012. WikiSent : Weakly Supervised Senti-
ment Analysis Through Extractive Summarization
With Wikipedia European Conference on Machine
Learning (ECML PKDD 2012), Bristol, U.K.,
146
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 90?96,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The IIT Bombay Hindi?English Translation System at WMT 2014
Piyush Dungarwal, Rajen Chatterjee, Abhijit Mishra, Anoop Kunchukuttan,
Ritesh Shah, Pushpak Bhattacharyya
Department of Computer Science and Engineering
Indian Institute of Technology, Bombay
{piyushdd,rajen,abhijitmishra,anoopk,ritesh,pb}@cse.iitb.ac.in
Abstract
In this paper, we describe our English-
Hindi and Hindi-English statistical sys-
tems submitted to the WMT14 shared task.
The core components of our translation
systems are phrase based (Hindi-English)
and factored (English-Hindi) SMT sys-
tems. We show that the use of num-
ber, case and Tree Adjoining Grammar
information as factors helps to improve
English-Hindi translation, primarily by
generating morphological inflections cor-
rectly. We show improvements to the
translation systems using pre-procesing
and post-processing components. To over-
come the structural divergence between
English and Hindi, we preorder the source
side sentence to conform to the target lan-
guage word order. Since parallel cor-
pus is limited, many words are not trans-
lated. We translate out-of-vocabulary
words and transliterate named entities in
a post-processing stage. We also investi-
gate ranking of translations from multiple
systems to select the best translation.
1 Introduction
India is a multilingual country with Hindi be-
ing the most widely spoken language. Hindi and
English act as link languages across the coun-
try and languages of official communication for
the Union Government. Thus, the importance of
English?Hindi translation is obvious. Over the
last decade, several rule based (Sinha, 1995) , in-
terlingua based (Dave et. al., 2001) and statistical
methods (Ramanathan et. al., 2008) have been ex-
plored for English-Hindi translation.
In the WMT 2014 shared task, we undertake
the challenge of improving translation between the
English and Hindi language pair using Statisti-
cal Machine Translation (SMT) techniques. The
WMT 2014 shared task has provided a standard-
ized test set to evaluate multiple approaches and
avails the largest publicly downloadable English-
Hindi parallel corpus. Using these resources,
we have developed a phrase-based and a factored
based system for Hindi-English and English-Hindi
translation respectively, with pre-processing and
post-processing components to handle structural
divergence and morphlogical richness of Hindi.
Section 2 describes the issues in Hindi?English
translation.
The rest of the paper is organized as follows.
Section 3 describes corpus preparation and exper-
imental setup. Section 4 and Section 5 describe
our English-Hindi and Hindi-English translation
systems respectively. Section 6 describes the post-
processing operations on the output from the core
translation system for handling OOV and named
entities, and for reranking outputs from multiple
systems. Section 7 mentions the details regarding
our systems submitted to WMT shared task. Sec-
tion 8 concludes the paper.
2 Problems in Hindi?English
Translation
Languages can be differentiated in terms of
structural divergences and morphological mani-
festations. English is structurally classified as
a Subject-Verb-Object (SVO) language with a
poor morphology whereas Hindi is a morpho-
logically rich, Subject-Object-Verb (SOV) lan-
guage. Largely, these divergences are responsi-
ble for the difficulties in translation using a phrase
based/factored model, which we summarize in this
section.
2.1 English-to-Hindi
The fundamental structural differences described
earlier result in large distance verb and modi-
fier movements across English-Hindi. Local re-
ordering models prove to be inadequate to over-
90
come the problem; hence, we transformed the
source side sentence using pre-ordering rules to
conform to the target word order. Availability of
robust parsers for English makes this approach for
English-Hindi translation effective.
As far as morphology is concerned, Hindi is
more richer in terms of case-markers, inflection-
rich surface forms including verb forms etc. Hindi
exhibits gender agreement and syncretism in in-
flections, which are not observed in English. We
attempt to enrich the source side English corpus
with linguistic factors in order to overcome the
morphological disparity.
2.2 Hindi-to-English
The lack of accurate linguistic parsers makes it dif-
ficult to overcome the structural divergence using
preordering rules. In order to preorder Hindi sen-
tences, we build rules using shallow parsing infor-
mation. The source side reordering helps to reduce
the decoder?s search complexity and learn better
phrase tables. Some of the other challenges in gen-
eration of English output are: (1) generation of ar-
ticles, which Hindi lacks, (2) heavy overloading of
English prepositions, making it difficult to predict
them.
3 Experimental Setup
We process the corpus through appropriate filters
for normalization and then create a train-test split.
3.1 English Corpus Normalization
To begin with, the English data was tokenized us-
ing the Stanford tokenizer (Klein and Manning,
2003) and then true-cased using truecase.perl pro-
vided in MOSES toolkit.
3.2 Hindi Corpus Normalization
For Hindi data, we first normalize the corpus us-
ing NLP Indic Library (Kunchukuttan et. al.,
2014)
1
. Normalization is followed by tokeniza-
tion, wherein we make use of the trivtokenizer.pl
2
provided with WMT14 shared task. In Table 1, we
highlight some of the post normalization statistics
for en-hi parallel corpora.
1
https://bitbucket.org/anoopk/indic_
nlp_library
2
http://ufallab.ms.mff.cuni.cz/~bojar/
hindencorp/
English Hindi
Token 2,898,810 3,092,555
Types 95,551 118,285
Total Characters 18,513,761 17,961,357
Total sentences 289,832 289,832
Sentences (word
count ? 10)
188,993 182,777
Sentences (word
count > 10)
100,839 107,055
Table 1: en-hi corpora statistics, post normalisa-
tion.
3.3 Data Split
Before splitting the data, we first randomize the
parallel corpus. We filter out English sentences
longer than 50 words along with their parallel
Hindi translations. After filtering, we select 5000
sentences which are 10 to 20 words long as the test
data, while remaining 284,832 sentences are used
for training.
4 English-to-Hindi (en-hi) translation
We use the MOSES toolkit (Koehn et. al., 2007a)
for carrying out various experiments. Starting with
Phrase Based Statistical Machine Translation (PB-
SMT)(Koehn et. al., 2003) as baseline system we
go ahead with pre-order PBSMT described in Sec-
tion 4.1. After pre-ordering, we train a Factor
Based SMT(Koehn, 2007b) model, where we add
factors on the pre-ordered source corpus. In Fac-
tor Based SMT we have two variations- (a) using
Supertag as factor described in Section 4.2 and (b)
using number, case as factors described in Section
4.3.
4.1 Pre-ordering source corpus
Research has shown that pre-ordering source lan-
guage to conform to target language word order
significantly improves translation quality (Collins
et. al, 2005). There are many variations of pre-
ordering systems primarily emerging from either
rule based or statistical methods. We use rule
based pre-ordering approach developed by (Pa-
tel et. al., 2013), which uses the Stanford parser
(Klein and Manning, 2003) for parsing English
sentences. This approach is an extension to an ear-
lier approach developed by (Ramanathan et. al.,
2008). The existing source reordering system re-
quires the input text to contain only surface form,
however, we extended it to support surface form
91
along with its factors like POS, lemma etc.. An
example of improvement in translation after pre-
ordering is shown below:
Example: trying to replace bad ideas with good
ideas .
Phr: replace b  r EvcAro\ ko aQC EvcAro\ k
sAT
(replace bure vichaaron ko acche vichaaron ke
saath)
Gloss: replace bad ideas good ideas with
Pre-order PBSMT: aQC EvcAro\ s b  r EvcAro\
ko bdln kF koEff kr rh h{\
(acche vichaaron se bure vichaaron ko badalane
ki koshish kara rahe hain)
Gloss: good ideas with bad ideas to replace trying
4.2 Supertag as Factor
The notion of Supertag was first proposed by
Joshi and Srinivas (1994). Supertags are elemen-
tary trees of Lexicalized Tree Adjoining Grammar
(LTAG) (Joshi and Schabes, 1991). They provide
syntactic as well as dependency information at the
word level by imposing complex constraints in a
local context. These elementary trees are com-
bined in some manner to form a parse tree, due
to which, supertagging is also known as ?An ap-
proach to almost parsing?(Bangalore and Joshi,
1999). A supertag can also be viewed as frag-
ments of parse trees associated with each lexi-
cal item. Figure 1 shows an example of su-
pertagged sentence ?The purchase price includes
taxes?described in (Hassan et. al., 2007). It clearly
shows the sub-categorization information avail-
able in the verb include, which takes subject NP
to its left and an object NP to its right.
Figure 1: LTAG supertag sequence obtained using
MICA Parser.
Use of supertags as factors has already been
studied by Hassan (2007) in context of Arabic-
English SMT. They use supertag language model
along with supertagged English corpus. Ours
is the first study in using supertag as factor
for English-to-Hindi translation on a pre-ordered
source corpus.
We use MICA Parser (Bangalore et. al., 2009)
for obtaining supertags. After supertagging we run
pre-ordering system preserving the supertags in it.
For translation, we create mapping from source-
word|supertag to target-word. An example of im-
provement in translation by using supertag as fac-
tor is shown below:
Example: trying to understand what your child is
saying to you
Phr: aApkA b?A aAps ?A kh rhA h{ yh
(aapkaa bacchaa aapse kya kaha rahaa hai yaha)
Gloss: your child you what saying is this
Supertag Fact: aApkA b?A aAps ?A kh rhA
h{ , us smJn kF koEff krnA
(aapkaa bacchaa aapse kya kaha rahaa hai, use
samajhane kii koshish karnaa)
Gloss: your child to you what saying is , that un-
derstand try
4.3 Number, Case as Factor
In this section, we discuss how to generate correct
noun inflections while translating from English to
Hindi. There has been previous work done in order
to solve the problem of data sparsity due to com-
plex verb morphology for English to Hindi trans-
lation (Gandhe, 2011). Noun inflections in Hindi
are affected by the number and case of the noun
only. Number can be singular or plural, whereas,
case can be direct or oblique. We use the factored
SMT model to incorporate this linguistic informa-
tion during training of the translation models. We
attach root-word, number and case as factors to
English nouns. On the other hand, to Hindi nouns
we attach root-word and suffix as factors. We de-
fine the translation and generation step as follows:
? Translation step (T0): Translates English
root|number|case to Hindi root|suffix
? Generation step (G0): Generates Hindi sur-
face word from Hindi root|suffix
An example of improvement in translation by
using number and case as factors is shown below:
Example: Two sets of statistics
Phr: do k aA kw
(do ke aankade)
Gloss: two of statistics
Num-Case Fact: aA kwo\ k do sV
(aankadon ke do set)
Gloss: statistics of two sets
92
4.3.1 Generating number and case factors
With the help of syntactic and morphological
tools, we extract the number and case of the En-
glish nouns as follows:
? Number factor: We use Stanford POS tag-
ger
3
to identify the English noun entities
(Toutanova, 2003). The POS tagger itself dif-
ferentiates between singular and plural nouns
by using different tags.
? Case factor: It is difficult to find the
direct/oblique case of the nouns as En-
glish nouns do not contain this information.
Hence, to get the case information, we need
to find out features of an English sentence
that correspond to direct/oblique case of the
parallel nouns in Hindi sentence. We use
object of preposition, subject, direct object,
tense as our features. These features are
extracted using semantic relations provided
by Stanford?s typed dependencies (Marneffe,
2008).
4.4 Results
Listed below are different statistical systems
trained using Moses:
? Phrase Based model (Phr)
? Phrase Based model with pre-ordered source
corpus (PhrReord)
? Factor Based Model with factors on pre-
ordered source corpus
? Supertag as factor (PhrReord+STag)
? Number, Case as factor (PhrReord+NC)
We evaluated translation systems with BLEU and
TER as shown in Table 2. Evaluation on the devel-
opment set shows that factor based models achieve
competitive scores as compared to the baseline
system, whereas, evaluation on the WMT14 test
set shows significant improvement in the perfor-
mance of factor based models.
5 Hindi-to-English (hi-en) translation
As English follows SVO word order and Hindi fol-
lows SOV word order, simple distortion penalty in
phrase-based models can not handle the reordering
well. For the shared task, we follow the approach
3
http://nlp.stanford.edu/software/tagger.shtml
Development WMT14
Model BLEU TER BLEU TER
Phr 27.62 0.63 8.0 0.84
PhrReord 28.64 0.62 8.6 0.86
PhrReord+STag 27.05 0.64 9.8 0.83
PhrReord+NC 27.50 0.64 10.1 0.83
Table 2: English-to-Hindi automatic evaluation on
development set and on WMT14 test set.
that pre-orders the source sentence to conform to
target word order.
A substantial volume of work has been done
in the field of source-side reordering for machine
translation. Most of the experiments are based on
applying reordering rules at the nodes of the parse
tree of the source sentence. These reordering rules
can be automatically learnt (Genzel, 2010). But,
many source languages do not have a good robust
parser. Hence, instead we can use shallow pars-
ing techniques to get chunks of words and then
reorder them. Reordering rules can be learned au-
tomatically from chunked data (Zhang, 2007).
Hindi does not have a functional constituency
or dependency parser available, as of now. But,
a shallow parser
4
is available for Hindi. Hence,
we follow a chunk-based pre-ordering approach,
wherein, we develop a set of rules to reorder
the chunks in a source sentence. The follow-
ing are the chunks tags generated by this shallow
parser: Noun chunks (NP), Verb chunks (VGF,
VGNF, VGNN), Adjectival chunks (JJP), Ad-
verb chunks (RBP), Negatives (NEGP), Conjuncts
(CCP), Chunk fragments (FRAGP), and miscella-
neous entities (BLK) (Bharati, 2006).
5.1 Development of rules
After chunking an input sentence, we apply hand-
crafted reordering rules on these chunks. Follow-
ing sections describe these rules. Note that we ap-
ply rules in the same order they are listed below.
5.1.1 Merging of chunks
After chunking, we merge the adjacent chunks, if
they follow same order in target language.
1. Merge {JJP VGF} chunks (Consider this
chunk as a single VGF chunk)
e.g., vEZta h{ (varnit hai), E-Tta h{ (sthit hai)
4
http://ltrc.iiit.ac.in/showfile.php?
filename=downloads/shallow_parser.php
93
2. Merge adjacent verb chunks (Consider this
chunk as a single verb chunk)
e.g., EgrtaA h{ (girataa hai), l  BAtaA h{ (lub-
haataa hai)
3. Merge NP and JJP chunks separated by com-
mas and CCP (Consider this chunk as a single
NP chunk)
e.g., bwA aOr ahm (badaa aur aham)
5.1.2 Preposition chunk reordering
Next we find sequence of contiguous chunks sep-
arated by prepositions (Can end in verb chunks).
We apply following reordering rules on these con-
tiguous chunks:
1. Reorder multi-word preposition locally by re-
versing the order of words in that chunk
e.g., k alAvA (ke alaawaa) ? alAvA k,
k sAmn (ke saamane)? sAmn k
2. Reorder contiguous preposition chunk by re-
versing the order of chunks (Consider this
chunk as a single noun chunk)
e.g., Eh\d Dm m\ taFT kA bwA mh(v (hinduu
dharma me tirtha ka badaa mahatva)? bwA
mh(v kA taFT m\ Eh\d Dm
5.1.3 Verb chunk reordering
We find contiguous verb chunks and apply follow-
ing reordering rules:
1. Reorder chunks locally by reversing the order
of the chunks
e.g., vEZta h{ (varnit hai)? h{ vEZta
2. Verb chunk placement: We place the new
verb chunk after first NP chunk. Same rule
applies for all verb chunks in a sentence, i.e.,
we place each verb chunk after first NP chunk
of the clause to which the verb belongs.
Note that, even though placing verb chunk af-
ter first NP chunk may be wrong reordering.
But we also use distortion window of 6 to 20
while using phrase-based model. Hence, fur-
ther reordering of verb chunks can be some-
what handled by phrase-based model itself.
Thus, using chunker and reordering rules, we
get a source-reordered Hindi sentence.
5.2 Results
We trained two different translation models:
? Phrase-based model without source reorder-
ing (Phr)
? Phrase-based model with chunk-based source
reordering (PhrReord)
Development WMT14
Model BLEU TER BLEU TER
Phr 27.53 0.59 13.5 0.87
PhrReord 25.06 0.62 13.7 0.90
Table 3: Hindi-to-English automatic evaluation on
development set and on WMT14 test set.
Table 3 shows evaluation scores for develop-
ment set and WMT14 test set. Even though we do
not see significant improvement in automatic eval-
uation of PhrReord, but this model contributes in
improving translation quality after ranking, as dis-
cussed in Section 5. In subjective evaluation we
found many translation to be better in PhrReord
model as shown in the following examples:
Example 1: sn 2004 s v kI bAr coVg}-ta
rh h{\ |
(sana 2004 se ve kaii baar chotagrasta rahe hain.)
Phr: since 2004 he is injured sometimes .
PhrReord: he was injured many times since 2004
.
Example 2: aobAmA kA rA?~ pEta pd k c  nAv
?cAr hta  bnAyA aAEDkAErk jAl-Tl
(obama ka rashtrapti pad ke chunaav prachaar
hetu banaayaa aadhikarik jaalsthal)
Phr: of Obama for election campaign
PhrReord: official website of Obama created for
President campaign
6 Post processing
All experimental results reported in this paper are
after post processing the translation output. In post
processing, we remove some Out-of-Vocabulary
(OOV) words as described in subsection 6.1, after
which we transliterate the remaining OOV words.
6.1 Removing OOV
We noticed, there are many words in the training
corpus which were not present in the phrase ta-
ble, but, were present in the lexical tranlsation ta-
ble. So we used the lexical table as a dictionary
to lookup bilingual translations. Table 4 gives the
statistics of number of OOV reduced.
94
Model Before After
Phrased Based 2313 1354
Phrase Based (pre-order) 2256 1334
Supertag as factor 4361 1611
Num-Case as factor 2628 1341
Table 4: Statistics showing number of OOV be-
fore and after post processing the English-to-Hindi
translation output of Development set.
6.2 Transliteration of Untranslated Words
OOV words which were not present in the lexi-
cal translation table were then transliterated using
a naive transliteration system. The transliteration
step was applied on Hindi-to-English translation
outputs only. After transliteration we noticed frac-
tional improvements in BLEU score varying from
0.1 to 0.5.
6.3 Ranking of Ensemble MT Output
We propose a ranking framework to select the best
translation output from an ensemble of multiple
MT systems. In order to exploit the strength of
each system, we augment the translation pipeline
with a ranking module as a post processing step.
For English-to-Hindi ranking we combine the
output of both factor based models, whereas,
for Hindi-to-English ranking we combine phrase
based and phrase based with pre-ordering outputs.
For most of the systems, the output translations
are adequate but not fluent enough. So, based on
their fluency scores, we decided to rank the candi-
date translations. Fluency is well quantified by LM
log probability score and Perplexity. For a given
translation , we compute these scores by querying
the 5-gram language model built using SRILM.
Table 5 shows more than 4% relative improvement
in BLEU score for en-hi as well as hi-en transla-
tion system after applying ranking module.
Model BLEU METEOR TER
Phr(en-hi) 27.62 0.41 0.63
After Ranking (en-hi) 28.82 0.42 0.63
Phr(hi-en) 27.53 0.27 0.59
After Ranking (hi-en) 28.69 0.27 0.59
Table 5: Comparision of ranking score with base-
line
7 Primary Systems in WMT14
For English-to-Hindi, we submitted the ranked
output of factored models trained on pre-ordered
source corpus. For Hindi-to-English, we submit-
ted the ranked output of phrase based and pre-
ordered phrase based models. Table 6 shows eval-
uation scores of these systems on WMT14 test set.
Lang. pair BLEU TER
en-hi 10.4 0.83
hi-en 14.5 0.89
Table 6: WMT14 evaluation for en-hi and hi-en.
8 Conclusion
We conclude that the difficulties in English-Hindi
MT can be tackled by the use of factor based SMT
and various pre-processing and post processing
techniques. Following are our primary contribu-
tions towards English-Hindi machine translation:
? Use of supertag factors for better translation
of structurally complex sentences
? Use of number-case factors for accurately
generating noun inflections in Hindi
? Use of shallow parsing for pre-ordering Hindi
source corpus
We also observed that simple ranking strategy ben-
efits in getting the best translation from an ensem-
ble of translation systems.
References
Avramidis, Eleftherios, and Philipp Koehn. 2008. En-
riching Morphologically Poor Languages for Statis-
tical Machine Translation. ACL.
Banerjee, Satanjeev, and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation with
improved correlation with human judgments. Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational linguistics.
Srinivas Bangalore, Pierre Boulllier, Alexis Nasr,
Owen Rambow, and Beno?
?
ot Sagot. 2009. MICA:
a probabilistic dependency parser based on tree in-
sertion grammars application note. Proceedings of
95
Human Language Technologies The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Associ-
ation for Computational Linguistics.
A. Bharati, R. Sangal, D. M. Sharma and L. Bai.
2006. AnnCorra: Annotating Corpora Guidelines
for POS and Chunk Annotation for Indian Lan-
guages. Technical Report (TR-LTRC-31), LTRC,
IIIT-Hyderabad.
Dave, Shachi and Parikh, Jignashu and Bhattacharyya,
Pushpak. 2001. Interlingua-based English?Hindi
Machine Translation and Language Divergence
Journal Machine Translation
Gandhe, Ankur, Rashmi Gangadharaiah, Karthik
Visweswariah, and Ananthakrishnan Ramanathan.
2011. Handling verb phrase morphology in highly
inflected Indian languages for Machine Translation.
IJCNLP.
Genzel, Dmitriy. 2010. Automatically learning
source-side reordering rules for large scale machine
translation Proceedings of the 23rd international
conference on computational linguistics. Associa-
tion for Computational Linguistics
Hany Hassan, Khalil Sima?an, and Andy Way 2007.
Supertagged phrase-based statistical machine trans-
lation. Proceedings of the Association for Compu-
tational Linguistics Association for Computational
Linguistics.
Aravind K. Joshi and Yves Schabes 1991. Tree-
adjoining grammars and lexicalized grammars.
Technical Report No. MS-CIS-91-22
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. Pro-
ceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1. Association for Computational Linguis-
tics
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?
?
Zej Bojar, Alexan-
dra Constantin and Evan Herbst. 2007. Moses:
open source toolkit for statistical machine transla-
tion. Proceedings of the Second Workshop on Hy-
brid Approaches to Translation. Association for
Computational Linguistics.
Philipp Koehn and Hieu Hoang 2007. Factored Trans-
lation Models Conference on Empirical Methods in
Natural Language Processing.
Anoop Kunchukuttan, Abhijit Mishra, Rajen Chatter-
jee,Ritesh Shah, and Pushpak Bhattacharyya. 2014.
Sata-Anuvadak: Tackling Multiway Translation of
Indian Languages. Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation Conference
De Marneffe, Marie-Catherine, and Christopher
D. Manning. 2008. Stanford typed de-
pendencies manual. URL http://nlp. stanford.
edu/software/dependencies manual. pdf (2008).
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. Proceed-
ings of the 40th annual meeting on association for
computational linguistics. Association for Compu-
tational Linguistics.
Raj Nath Patel, Rohit Gupta, Prakash B. Pimpale and
Sasikumar M. 2013. Reordering rules for English-
Hindi SMT. Proceedings of the Second Workshop
on Hybrid Approaches to Translation. Association
for Computational Linguistics.
Ananthakrishnan Ramanathan, Pushpak Bhat-
tacharyya, Jayprasad Hegde, Ritesh M. Shah,
and M. Sasikumar. 2008. Simple syntactic and
morphological processing can help English-Hindi
statistical machine translation. In International
Joint Conference on NLP.
Sinha, RMK and Sivaraman, K and Agrawal, A and
Jain, R and Srivastava, R and Jain, A. 1995.
ANGLABHARTI: a multilingual machine aided
translation project on translation from English to In-
dian languages IEEE International Conference on
Systems, Man and Cybernetics
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1. Association for Computational Linguis-
tics.
Zhang, Yuqi, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sen-
tences with automatically learned rules for sta-
tistical machine translation Proceedings of the
NAACL-HLT 2007/AMTA Workshop on Syntax
and Structure in Statistical Translation. Association
for Computational Linguistics
Collins, Michael, Philipp Koehn, and Ivona Ku
?
cerova
2005 Clause restructuring for statistical machine
translation. Proceedings of the 43rd annual meeting
on association for computational linguistics. Asso-
ciation for Computational Linguistics
96
