Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 644?651, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Robust Combination Strategy for Semantic Role Labeling
Llu??s Ma`rquez, Mihai Surdeanu, Pere Comas, and Jordi Turmo
Technical University of Catalunya
Barcelona, Spain
{lluism,surdeanu,pcomas,turmo}@lsi.upc.edu
Abstract
This paper focuses on semantic role la-
beling using automatically-generated syn-
tactic information. A simple and robust
strategy for system combination is pre-
sented, which allows to partially recover
from input parsing errors and to signif-
icantly boost results of individual sys-
tems. This combination scheme is also
very flexible since the individual systems
are not required to provide any informa-
tion other than their solution. Extensive
experimental evaluation in the CoNLL-
2005 shared task framework supports our
previous claims. The proposed architec-
ture outperforms the best results reported
in that evaluation exercise.
1 Introduction
The task of Semantic Role Labeling (SRL), i.e.
the process of detecting basic event structures
such as who did what to whom, when and where,
has received considerable interest in the past few
years (Gildea and Jurafsky, 2002; Surdeanu et al,
2003; Xue and Palmer, 2004; Pradhan et al, 2005a;
Carreras and Ma`rquez, 2005). It was shown that
the identification of such event frames has a signif-
icant contribution for many Natural Language Pro-
cessing (NLP) applications such as Information Ex-
traction (Surdeanu et al, 2003) and Question An-
swering (Narayanan and Harabagiu, 2004).
Most current SRL approaches can be classified
in one of two classes: approaches that take ad-
vantage of complete syntactic analysis of text, pi-
oneered by Gildea and Jurafsky (2002), and ap-
proaches that use partial syntactic analysis, cham-
pioned by previous evaluations performed within
the Conference on Computational Natural Language
Learning (CoNLL) (Carreras and Ma`rquez, 2004).
The wisdom extracted from this volume of work in-
dicates that full syntactic analysis has a significant
contribution to the SRL performance, when using
hand-corrected syntactic information.
On the other hand, when only automatically-
generated syntax is available, the quality of the in-
formation provided through full syntax decreases
because the state-of-the-art of full parsing is less
robust and performs worse than the tools used for
partial syntactic analysis. Under such real-world
conditions, the difference between the two SRL ap-
proaches (with full or partial syntax) is not that high.
More interestingly, the two SRL strategies perform
better for different semantic roles. For example,
models that use full syntax recognize better agent
and theme roles, whereas models based on partial
syntax are better at recognizing explicit patient roles,
which tend to be farther from the predicate and accu-
mulate more parsing errors (Ma`rquez et al, 2005).
The above observations motivate the work pre-
sented in this paper. We introduce a novel semantic
role labeling approach that combines several indi-
vidual SRL systems. Intuitively, our approach can
be separated in two stages: a candidate generation
phase, where the solutions provided by several indi-
vidual models are combined into a pool of candidate
arguments, and an inference phase, where the candi-
dates are filtered using a binary classifier, and possi-
644
The luxury auto maker last year sold 1,214 cars in the U.S.
PPNP
VPNPNP
S
ARG0 ARGM?TMP P ARG1 ARGM?LOC
Figure 1: Sample PropBank sentence.
ble conflicts with domain knowledge constraints are
resolved to obtain the final solution.
For robustness, the inference model uses only
global attributes extracted from the solutions pro-
vided by the individual systems, e.g., the sequence
of role labels generated by each system for the cur-
rent predicate. We do not use any attributes spe-
cific to the individual models, not even the confi-
dence assigned by the individual classifiers. Besides
simplicity, the consequence of this decision is that
our approach does not impose any restrictions on the
individual SRL strategies, as long as one solution
is provided for each predicate. On the other hand,
probabilistic inference processes, which have been
successfully used for SRL (Koomen et al, 2005),
mandate that each individual candidate argument be
associated with its raw activation, or confidence, in
the given model. However, this information is not
directly available in two out of three of our individ-
ual models, which classify argument chunks and not
entire arguments.
Despite its simplicity, our approach obtains en-
couraging results: the combined system outperforms
any of the individual systems and, using exactly the
same data, it is also competitive with the best SRL
systems that participated in the latest CoNLL shared
task evaluation (Carreras and Ma`rquez, 2005).
2 Semantic Corpora
In this paper we report results using PropBank, an
approximately one-million-word corpus annotated
with predicate-argument structures (Kingsbury et
al., 2002). To date, PropBank addresses mainly
predicates lexicalized by verbs and a small num-
ber of predicates lexicalized by verb nominalizations
and adjectives.
The arguments of each predicate are numbered se-
quentially from ARG0 to ARG5. Generally, ARG0
stands for agent, ARG1 for theme or direct ob-
ject, and ARG2 for indirect object, benefactive or
instrument, but mnemonics tend to be verb spe-
cific. Additionally, predicates might have ?adjunc-
tive arguments?, referred to as ARGMs. For example,
ARGM-LOC indicates a locative and ARGM-TMP in-
dicates a temporal. Figure 1 shows a sample sen-
tence where one predicate (?sold?) has 4 arguments.
In a departure from ?traditional? SRL approaches
that train on the hand-corrected syntactic trees as-
sociated with PropBank, we do not use any syn-
tactic information from PropBank. Instead, we
develop our models using automatically-generated
syntax and named-entity (NE) labels, made avail-
able by the CoNLL shared task evaluation (Carreras
and Ma`rquez, 2005). From the CoNLL data, our
individual models based on full syntactic analysis
use the trees generated by the Charniak parser. The
partial-syntax model uses the chunk? i.e. basic syn-
tactic phrase ? labels and clause boundaries. All in-
dividual models make use of the provided NE labels.
Following the CoNLL-2005 setting we evaluated
our system also on a fresh test set, derived from the
Brown corpus. This second evaluation allows us to
re-enforce our robustness claim.
3 Approach Overview
The proposed architecture, summarized in Figure 2,
consists of two stages: a candidate generation phase
and an inference stage.
In the candidate generation step, we merge the so-
lutions of three individual SRL models into a unique
pool of candidate arguments. The proposed models
range from complete reliance on full parsing to us-
ing only partial syntactic information. The first two
models, Model 1 and 2, are developed as sequential
taggers (using the BIO tagging scheme) on a shared
framework. The major difference between the two
models is that Model 1 uses only partial syntactic
information (basic phrases and clause boundaries),
whereas Model 2 uses complete syntactic informa-
tion. To maximize diversity, Model 3 implements
a different strategy: it models only arguments that
map into exactly one syntactic constituent. Section 4
details all three individual models.
The inference stage starts with candidate filtering,
645
Candidate Filtering
Reliance on full syntax
Model 1 Model 2 Model 3
Conflict Resolution
Inference
Candidate
Generation
Figure 2: Architecture of the proposed system.
which reduces the number of candidate arguments
in the pool using a single binary classifier. Using
this classifier?s confidence values and a number of
domain-specific constraints, e.g. no two arguments
can overlap, the conflict resolution component en-
forces the consistency of the final solution using a
straightforward greedy strategy. The complete in-
ference model is detailed in Section 5.
4 Individual SRL Models
Models 1 and 2. These models approach SRL as
a sequential tagging task. In a pre-process step, the
input syntactic structures are traversed in order to
select a subset of constituents organized sequentially
(i.e. non embedding). Model 1 makes use only of
the partial tree defined by base chunks and clause
boundaries, while Model 2 explores full parse trees.
Precisely, the sequential tokens are selected as fol-
lows. First, the input sentence is splitted into dis-
joint segments by considering the clause boundaries
given by the syntactic structure. Second, for each
segment, the set of top-most non-overlapping syn-
tactic constituents completely falling inside the seg-
ment are selected as tokens. Note that this strategy
provides a set of sequential tokens covering the com-
plete sentence. Also, it is independent of the syn-
tactic annotation explored, given it provides clause
boundaries ? see (Ma`rquez et al, 2005) for more
details.
Due to this pre-processing stage, the upper-bound
recall figures are 95.67% for Model 1 and 90.32%
for Model 2 using the datasets defined in Section 6.
The nodes selected are labeled with B-I-O tags
(depending if they are at the beginning, inside, or
outside of a predicate argument) and they are con-
verted into training examples by considering a rich
set of features, mainly borrowed from state-of-the-
art systems. These features codify properties from:
(a) the argument constituent, (b) the target predicate,
Constituent type and head: extracted using common head-
word rules. If the first element is a PP chunk, then the
head of the first NP is extracted.
First and last words and POS tags of the constituent.
POS sequence: if it is less than 5 tags long.
2/3/4-grams of the POS sequence.
Bag-of-words of nouns, adjectives, and adverbs.
TOP sequence: sequence of types of the top-most syntactic
elements in the constituent (if it is less than 5 elements long).
In the case of full parsing this corresponds to the right-hand
side of the rule expanding the constituent node.
2/3/4-grams of the TOP sequence.
Governing category as in (Gildea and Jurafsky, 2002).
NamedEnt, indicating if the constituent embeds or
strictly matches a named entity along with its type.
TMP, indicating if the constituent embeds or strictly matches
a temporal keyword (extracted from AM-TMP arguments of
the training set).
Previous and following words and POS of the constituent.
The same features characterizing focus constituents are
extracted for the two previous and following tokens, provided
they are inside the clause boundaries of the codified region.
Table 1: Constituent structure features: Models 1/2
Predicate form, lemma, and POS tag.
Chunk type and type of verb phrase in which verb is
included: single-word or multi-word.
The predicate voice. We currently distinguish five voice
types: active, passive, copulative, infinitive, and progressive.
Binary flag indicating if the verb is a start/end of a clause.
Sub-categorization rule, i.e. the phrase structure rule that
expands the predicate immediate parent.
Table 2: Predicate structure features: Models 1/2
and (c) the distance between the argument and pred-
icate. The three feature sets are listed in Tables 1, 2,
and 3, respectively.1
Regarding the learning algorithm, we used gener-
alized AdaBoost with real-valued weak classifiers,
which constructs an ensemble of decision trees of
fixed depth (Schapire and Singer, 1999). We con-
sidered a one-vs-all decomposition into binary prob-
lems to address multi-class classification. AdaBoost
binary classifiers are then used for labeling test se-
quences, from left to right, using a recurrent sliding
window approach with information about the tag as-
signed to the preceding token. This tagging module
enforces some basic constraints, e.g., BIO correct
structure, arguments cannot overlap with clause nor
chunk boundaries, discard ARG0-5 arguments not
present in PropBank frames for a certain verb, etc.
1Features extracted from partial parsing and Named Entities
are common to Model 1 and 2, while features coming from full
parse trees only apply to Model 2.
646
Relative position, distance in words and chunks, and level of
embedding (in #clause-levels) with respect to the constituent.
Constituent path as described in (Gildea and Jurafsky, 2002)
and all 3/4/5-grams of path constituents beginning at the
verb predicate or ending at the constituent.
Partial parsing path as described in (Carreras et al, 2004)
and all 3/4/5-grams of path elements beginning at the
verb predicate or ending at the constituent.
Syntactic frame as described by Xue and Palmer (2004)
Table 3: Predicate?constituent features: Models 1/2
The syntactic label of the candidate constituent.
The constituent head word, suffixes of length 2, 3, and 4,
lemma, and POS tag.
The constituent content word, suffixes of length 2, 3, and
4, lemma, POS tag, and NE label. Content words, which
add informative lexicalized information different from
the head word, were detected using the heuristics
of (Surdeanu et al, 2003).
The first and last constituent words and their POS tags.
NE labels included in the candidate phrase.
Binary features to indicate the presence of temporal cue
words, i.e. words that appear often in AM-TMP phrases
in training.
For each TreeBank syntactic label we added a feature to
indicate the number of such labels included in the
candidate phrase.
The sequence of syntactic labels of the constituent
immediate children.
The phrase label, head word and POS tag of the
constituent parent, left sibling, and right sibling.
Table 4: Constituent structure features: Model 3
Model 3. The third individual SRL model makes
the strong assumption that each predicate argument
maps to one syntactic constituent. For example, in
Figure 1 ARG0 maps to a noun phrase, ARGM-LOC
maps to a prepositional phrase etcetera. This as-
sumption holds well on hand-corrected parse trees
and simplifies significantly the SRL process because
only one syntactic constituent has to be correctly
classified in order to recognize one semantic argu-
ment. On the other hand, this approach is limited
when using automatically-generated syntactic trees.
For example, only slightly over 91% of the argu-
ments can be mapped to one of the syntactic con-
stituents produced by the Charniak parser.
Using a bottom-up approach, Model 3 maps each
argument to the first syntactic constituent that has
the exact same boundaries and then climbs as high as
possible in the tree across unary production chains.
We currently ignore all arguments that do not map
to a single syntactic constituent.
The predicate word and lemma.
The predicate voice. Same definition as Models 1 and 2.
A binary feature to indicate if the predicate is frequent
(i.e., it appears more than twice in the training data) or not.
Sub-categorization rule. Same def. as Models 1 and 2.
Table 5: Predicate structure features: Model 3
The path in the syntactic tree between the argument phrase
and the predicate as a chain of syntactic labels along with
the traversal direction (up or down).
The length of the above syntactic path.
The number of clauses (S* phrases) in the path.
The number of verb phrases (VP) in the path.
The subsumption count, i.e. the difference between the
depths in the syntactic tree of the argument and predicate
constituents. This value is 0 if the two phrases share the
same parent.
The governing category, which indicates if NP
arguments are dominated by a sentence (typical for
subjects) or a verb phrase (typical for objects).
We generalize syntactic paths with more than 3
elements using two templates:
(a) Arg ? Ancestor ? Ni ? Pred, where Arg is the
argument label, Pred is the predicate label, Ancestor
is the label of the common ancestor, and Ni is instantiated
with all the labels between Pred and Ancestor in
the full path; and
(b) Arg ? Ni ? Ancestor ? Pred, where Ni is
instantiated with all the labels between Arg and
Ancestor in the full path.
The surface distance between the predicate and the
argument phrases encoded as: the number of tokens, verb
terminals (VB*), commas, and coordinations (CC) between
the argument and predicate phrases, and a binary feature to
indicate if the two constituents are adjacent.
A binary feature to indicate if the argument starts with a
predicate particle, i.e. a token seen with the RP* POS
tag and directly attached to the predicate in training.
Table 6: Predicate?constituent features: Model 3
Once the mapping process completes, Model 3
extracts a rich set of lexical, syntactic, and seman-
tic features. Tables 4, 5, and 6 present these features
organized in the same three categories as the previ-
ous Models 1 and 2 ? see (Surdeanu and Turmo,
2005) for more details.
Similarly with Models 1 and 2, Model 3 trains
one-vs-all classifiers using AdaBoost for the most
common argument labels. To reduce the sample
space, Model 3 selects training examples (both posi-
tive and negative) only from: (a) the first clause that
includes the predicate, or (b) from phrases that ap-
pear to the left of the predicate in the sentence. More
than 98% of the argument constituents fall into one
of these classes.
At prediction time the classifiers are combined us-
ing a simple greedy technique that iteratively assigns
647
to each predicate the argument classified with the
highest confidence. For each predicate we consider
as candidates all AM attributes, but only numbered
attributes indicated in the corresponding PropBank
frame. Additionally, this greedy strategy enforces a
limited number of domain knowledge constraints in
the generated solution: (a) arguments can not over-
lap in any form, and (b) no duplicate arguments are
allowed for ARG0-5.
5 The Inference Model
The most important component of our inference
model is candidate filtering, which decides if a can-
didate argument should be maintained in the global
solution or not. Candidate filtering is implemented
as a single binary classifier that uses only features
extracted from the solutions provided by the individ-
ual systems. For robustness, we do not use any fea-
tures that are specific to any of the individual mod-
els, nor the confidence value of their classifiers.
Table 7 lists the features extracted from each can-
didate argument by the filtering classifier. For sim-
plicity we have focused only on attributes that: (a)
are readily available in the solutions proposed by the
individual classifiers, and (b) allow the gathering of
simple and robust statistics. For example, the fil-
tering classifier might learn that a candidate is to be
trusted if: (a) two individual systems proposed it, (b)
if its label is ARG2 and it was generated by Model 1,
or (c) if it was proposed by Model 2 within a certain
argument sequence.
The candidate arguments that pass the filtering
stage are incorporated in the global solution by the
conflict resolution module, which enforces several
domain specific constraints. We have currently im-
plemented two constraints: (a) arguments can not
overlap or embed other arguments; and (b) no du-
plicate arguments are allowed for the numbered ar-
guments ARG0-5. Theoretically, the set of con-
straints can be extended with any other rules, but in
our particular case, we know that some constraints,
e.g. providing only arguments indicated in the cor-
responding PropBank frame, are already guaranteed
by the individual models. Conflicts are solved with
a straightforward greedy strategy: the pool of candi-
date arguments is inspected in descending order of
the confidence values assigned by the filtering clas-
The label of the candidate argument.
The number of systems that generated an argument with
this label and span.
The unique ids, e.g. M1 and M2, of all the systems that
generated an argument with this label and span.
The argument sequence for this predicate for all the systems
that generated an argument with this label and span. For
example, the argument sequence for the proposition
illustrated in Figure 1 is: ARG0 - ARGM-TMP - P -
ARG1 - ARGM-LOC.
The number and unique ids of all the systems that generated
an argument with the same span but different label.
The number and unique ids of all the systems that generated
an argument included in the current argument.
The number and unique ids of all the systems that generated
an argument that contains the current argument.
The number and unique ids of all the systems that generated
an argument that overlaps the current argument.
Table 7: Features used by the candidate filtering
classifier.
sifier, and candidates are appended to the global so-
lution only if they do not violate any of the domain
constraints with the arguments already selected. Our
inference system currently has a sequential architec-
ture, i.e. no feedback is sent from the conflict reso-
lution module to candidate filtering.
6 Experimental Results
We trained the individual models using the complete
CoNLL-2005 training set (PropBank/TreeBank sec-
tions 2 to 21). All models were developed using
AdaBoost with decision trees of depth 4 (i.e. each
branch may represent a conjunction of at most 4 ba-
sic features). Each classification model was trained
for up to 2,000 rounds.
We applied some simplifications to keep training
times and memory requirements inside admissible
bounds: (a) we have limited the number of nega-
tive examples in Model 3 to the first 500,000; (b)
we have trained only the most frequent argument la-
bels: top 41 for Model 1, top 35 for Model 2, and
top 24 for Model 3; and (c) we discarded all features
occurring less than 15 times in the training set.
The models were tuned on a separate develop-
ment partition (TreeBank section 24) and evaluated
on two corpora: (a) TreeBank section 23, which
consists of Wall Street Journal (WSJ) documents,
and (b) on three sections of the Brown corpus, se-
mantically annotated by the PropBank team for the
CoNLL 2005 shared task evaluation. Table 8 sum-
648
WSJ PProps Precision Recall F?=1
Model 1 48.45% 78.76% 72.44% 75.47 ?0.8
Model 2 52.04% 79.65% 74.92% 77.21 ?0.8
Model 3 45.28% 80.32% 72.95% 76.46 ?0.6
Brown PProps Precision Recall F?=1
Model 1 30.85% 67.72% 58.29% 62.65 ?2.1
Model 2 36.44% 71.82% 64.03% 67.70 ?1.9
Model 3 29.48% 72.41% 59.67% 65.42 ?2.1
Table 8: Overall results of the individual models on
the WSJ and Brown test sets.
marizes the results of the three models on the WSJ
and Brown corpora. In that table we include the
percentage of perfect propositions detected by each
model (?PProps?), i.e. predicates recognized with
all their arguments, the overall precision, recall, and
F?=1 measure2.
The results summarized in Table 8 indicate that
all individual systems have a solid performance. Al-
though none of them would rank in the top 3 in this
year?s CoNLL evaluation (Carreras and Ma`rquez,
2005), their performance is comparable to the best
individual systems presented at that evaluation exer-
cise3. As expected, the models based on full parsing
(2 and 3) perform better than the model based on
partial syntax. But, interestingly, the difference is
not large (e.g., less than 2 points in F?=1 in the WSJ
corpus), evincing that having base syntactic chunks
and clause boundaries is enough to obtain a compet-
itive performance with a simple system.
Consistently with other systems evaluated on the
Brown corpus, all our models experience a severe
performance drop in this corpus, due to the lower
performance of the linguistic processors.
6.1 Performance of Combination Systems
We have trained the candidate filtering binary classi-
fier on one third of the training partition. Its training
data was generated using individual models trained
on the other two thirds of the training partition. The
classifier was developed using Support Vector Ma-
chines (SVM) with a polynomial kernel of degree 2.
We trained combined models for all 4 possible com-
binations of our 3 individual models.
2The significance intervals for the F1 measure have been ob-
tained using bootstrap resampling (Noreen, 1989). F1 rates out-
side of these intervals are assumed to be significantly different
from the related F1 rate (p < 0.05).
3The best performing SRL systems at CoNLL were a com-
bination of several subsystems. See section 7 for details.
Table 9 summarizes the performance of the com-
bined systems on the WSJ and Brown corpora.4
The combined systems are compared against a base-
line combination system, which merges all the argu-
ments generated by the individual systems. For con-
flict resolution, the baseline uses the greedy strategy
introduced in Section 5, but using as argument or-
dering criterion a radix sort that orders the candidate
arguments in descending order of: number of mod-
els that agreed on this argument; argument length in
tokens; and performance of the individual system5.
Table 9 indicates that our combination strategy is
always successful: the results of all combined sys-
tems improve upon their individual models and they
are better the baseline when using the same num-
ber of individual models. As expected, the highest
scoring combined system includes all three individ-
ual models. Its F?=1 measure is 2.35 points higher
than the best individual model (Model 2) in the WSJ
test set and 1.30 points higher in the Brown test
set. Somewhat surprisingly, the highest percentage
of perfect propositions is not obtained by the over-
all best combination, but by the system that com-
bines the two models based on full parsing (Models
2 and 3). This happens because Model 1 is the weak-
est performing of the bunch, hence its arguments,
while providing useful information to the filtering
classifier, decrease the number of perfect proposi-
tions when selected.
We consider these results encouraging given the
simplicity of our inference model and the limited
amount of training data used to train the candidate
filtering classifier. Additionally, they compare fa-
vorably with respect to the best performing systems
at CoNLL-2005 shared task (see Section 7).
6.2 Upper Limit of the Combination Strategy
To explore the potential of our approach we have
constructed a hypothetical system where our candi-
date filtering module is replaced with a perfect clas-
sifier that selects only correct arguments and dis-
cards all others. Table 10 lists the results obtained
on the WSJ and Brown corpora by this hypothetical
system using all three individual models.
4For conciseness, in Table 9 we introduced the notation
M1+2+3 to indicate the combination of Models 1, 2, and 3
5This combination produced the highest-scoring baseline
model.
649
WSJ PProps Prec. Recall F?=1
M1+2 51.30% 81.30% 74.13% 77.55 ?0.7
M1+3 47.26% 81.21% 73.36% 77.08 ?0.8
M2+3 52.65% 81.55% 75.30% 78.30 ?0.7
M1+2+3 51.64% 84.89% 74.87% 79.56 ?0.7
baseline 51.09% 77.29% 78.67% 77.98 ?0.7
Brown PProps Prec. Recall F?=1
M1+2 35.95% 73.70% 62.93% 67.89 ?2.0
M1+3 28.98% 72.83% 58.84% 65.09 ?2.2
M2+3 37.06% 73.89% 63.30% 68.18 ?2.2
M1+2+3 34.20% 78.66% 61.46% 69.00 ?2.1
baseline 33.58% 67.66% 66.01% 66.82 ?1.8
Table 9: Overall results of the combination models
on the WSJ and Brown test sets.
Perfect props Precision Recall F?=1
WSJ 70.76% 99.12% 85.22% 91.64
Brown 51.87% 99.63% 74.32% 85.14
Table 10: Performance upper limit on the test sets.
Table 10 indicates that the upper limit of proposed
approach is relatively high: the F?=1 of this hy-
pothetical system is over 12 points higher than our
best combined system in the WSJ test set, and over
16 points higher in the Brown corpus. These re-
sults indicate that the potential of our combination
strategy is high, especially when compared with re-
ranking strategies, which are limited to the perfor-
mance of the best complete solution in the candidate
pool. By allowing the re-combination of arguments
from the individual candidate solutions we raise this
threshold significantly. Table 11 lists the contribu-
tion of the individual models to this upper limit on
the WSJ corpus. For conciseness, we list only the
?core? numbered arguments. ?? of 3? indicates the
percentage of correct arguments where all 3 mod-
els agreed, ?? of 2? indicates the percentage of cor-
rect arguments where any 2 models agreed, and the
other columns indicate the percentage of correct ar-
guments detected by a single model. Table 11 indi-
cates that, as expected, two or more individual mod-
els agreed on a large percentage of the correct argu-
ments. Nevertheless, a significant number of correct
arguments, e.g. over 22% of ARG3, come from a
single individual system. This proves that, in order
to achieve maximum performance, one has to look
beyond simple voting strategies that favor arguments
with high agreement between individual systems.
? of 3 ? of 2 M1 M2 M3
ARG0 80.45% 12.10% 3.47% 2.14% 1.84%
ARG1 69.82% 17.83% 7.45% 2.77% 2.13%
ARG2 56.04% 22.32% 12.20% 4.95% 4.49%
ARG3 56.03% 21.55% 12.93% 5.17% 4.31%
ARG4 65.85% 20.73% 6.10% 2.44% 4.88%
Table 11: Contribution of the individual systems to
the upper limit, for ARG0?ARG4 in the WSJ test set.
WSJ Brown
PProps F?=1 PProps F?=1
koomen 53.79% 79.44 ?0.8 32.34% 67.75 ?1.8
haghighi 56.52% 78.45 ?0.8 37.06% 67.71 ?2.0
pradhan 50.14% 77.37 ?0.7 36.44% 67.07 ?2.0
Table 12: Results of the best combined systems at
CoNLL-2005.
7 Related Work
The best performing systems at the CoNLL-2005
shared task included a combination of different base
subsystems to increase robustness and to gain cover-
age and independence from parse errors. Therefore,
they are closely related to the work of this paper.
Table 12 summarizes their results under exactly the
same experimental setting.
Koomen et al (2005) used a 2 layer architecture
similar to ours. The pool of candidates is generated
by running a full syntax SRL system on alternative
input information (Collins parsing, and 5-best trees
from Charniak?s parser). The combination of can-
didates is performed in an elegant global inference
procedure as constraint satisfaction, which, formu-
lated as Integer Linear Programming, can be solved
efficiently. Interestingly, the generalized inference
layer allows to include in the objective function,
jointly with the candidate argument scores, a num-
ber of linguistically-motivated constraints to obtain
a coherent solution. Differing from the strategy pre-
sented in this paper, their inference layer does not
include learning. Also, they require confidence val-
ues from individual classifiers. This is the best per-
forming system at CoNLL-2005.
Haghighi et al (2005) implemented a double re-
ranking model on top of the base SRL models to se-
lect the most probable solution among a set of can-
didates. The re-ranking is performed, first, on a set
of n-best solutions obtained by the base system run
on a single parse tree, and, then, on the set of best-
candidates coming from the n-best parse trees. The
650
re-ranking approach allows to define global complex
features applying to complete candidate solutions to
train the rankers. The main drawback, compared to
our approach, is that re-ranking does not permit to
combine different solutions since it is forced to se-
lect a complete candidate solution. This fact implies
that the performance upper limit strongly depends
on the ability of the base model to generate the com-
plete correct solution in the set of n-best candidates.
Finally, Pradhan et al (2005b) followed a stack-
ing approach by learning two individual systems
based on full syntax, whose outputs are used to
generate features to feed the training stage of a fi-
nal chunk-by-chunk SRL system. Although the fine
granularity of the chunking-based system allows to
recover from parsing errors, we find this combina-
tion scheme quite ad-hoc because it forces to break
argument candidates into chunks in the last stage.
8 Conclusions
This paper introduces a novel, robust combination
strategy for semantic role labeling. Our approach
is separated in two stages: a candidate generation
phase, which combines the solutions generated by
several individual models into a pool of candidate ar-
guments, followed by a simple inference model that
filters the candidate arguments using a single binary
classifier and then enforces an arbitrary number of
domain-specific constraints.
The proposed approach has several advantages.
First, because it combines the solutions provided by
the individual models, the inference model can re-
cover from errors produced in the generation phase.
Second, due to the diversity of the individual models
employed, the candidate pool contains a high per-
centage of the correct arguments. And lastly, our
approach is flexible and robust: it can incorporate
any SRL model in the candidate generation stage
because it does not require that the individual SRL
models provide any information, e.g. classification
confidence values, other than an argument solution.
Our results are better than the state of the art us-
ing automatically-generated syntactic information.
These results are encouraging considering the sim-
plicity of the proposed approach.
Acknowledgments
This research has been partially supported by the
European Commission (CHIL project, IP-506909).
Mihai Surdeanu is a research fellow within the
Ramo?n y Cajal program of the Spanish Ministry of
Education and Science.
References
X. Carreras and L. Ma`rquez. 2004. Introduction to the CoNLL-
2004 shared task: Semantic role labeling. In Proceedings of
CoNLL 2004.
X. Carreras and L. Ma`rquez. 2005. Introduction to the CoNLL-
2005 Shared Task: Semantic Role Labeling. In Proceedings
of CoNLL-2005.
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hierarchical
recognition of propositional arguments with perceptrons. In
Proceedings of CoNLL 2004 shared task.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3).
A. Haghighi, K. Toutanova, and C. Manning. 2005. A joint
model for semantic role labeling. In Proceedings of CoNLL-
2005 shared task.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding se-
mantic annotation to the Penn Treebank. In Proceedings of
the Human Language Technology Conference.
P. Koomen, V. Punyakanok, D. Roth, and W. Yih. 2005. Gen-
eralized inference with multiple semantic role labeling sys-
tems. In Proceedings of CoNLL-2005 shared task.
L. Ma`rquez, P. Comas, J. Gime?nez, and N. Catala`. 2005. Se-
mantic role labeling as sequential tagging. In Proceedings of
CoNLL-2005 shared task.
S. Narayanan and S. Harabagiu. 2004. Question answering
based on semantic structures. In International Conference
on Computational Linguistics (COLING 2004).
E. W. Noreen. 1989. Computer-Intensive Methods for Testing
Hypotheses. John Wiley & Sons.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin, and
D. Jurafsky. 2005a. Support vector learning for semantic
argument classification. Machine Learning, to appear.
S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and D. Juraf-
sky. 2005b. Semantic role chunking combining complemen-
tary syntactic views. In Proceedings of CoNLL-2005.
R. E. Schapire and Y. Singer. 1999. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3).
M. Surdeanu and J. Turmo. 2005. Semantic role labeling using
complete syntactic analysis. In Proceedings of CoNLL-2005
shared task.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures for information extrac-
tion. In Proceedings of ACL 2003.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proceedings of EMNLP-2004.
651
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 193?196, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling as Sequential Tagging
Llu??s Ma`rquez, Pere Comas, Jesu?s Gime?nez and Neus Catala`
TALP Research Centre
Technical University of Catalonia (UPC)
{lluism,pcomas,jgimenez,ncatala}@lsi.upc.edu
Abstract
In this paper we present a semantic role
labeling system submitted to the CoNLL-
2005 shared task. The system makes
use of partial and full syntactic informa-
tion and converts the task into a sequen-
tial BIO-tagging. As a result, the label-
ing architecture is very simple . Build-
ing on a state-of-the-art set of features, a
binary classifier for each label is trained
using AdaBoost with fixed depth decision
trees. The final system, which combines
the outputs of two base systems performed
F1=76.59 on the official test set. Addi-
tionally, we provide results comparing the
system when using partial vs. full parsing
input information.
1 Goals and System Architecture
The goal of our work is twofold. On the one hand,
we want to test whether it is possible to implement
a competitive SRL system by reducing the task to a
sequential tagging. On the other hand, we want to
investigate the effect of replacing partial parsing in-
formation by full parsing. For that, we built two dif-
ferent individual systems with a shared sequential
strategy but using UPC chunks-clauses, and Char-
niak?s parses, respectively. We will refer to those
systems as PPUPC and FPCHA, hereinafter.
Both partial and full parsing annotations provided
as input information are of hierarchical nature. Our
system navigates through these syntactic structures
in order to select a subset of constituents organized
sequentially (i.e., non embedding). Propositions are
treated independently, that is, each target verb gen-
erates a sequence of tokens to be annotated. We call
this pre-processing step sequentialization.
The sequential tokens are selected by exploring
the sentence spans or regions defined by the clause
boundaries1. The top-most syntactic constituents
falling inside these regions are selected as tokens.
Note that this strategy is independent of the input
syntactic annotation explored, provided it contains
clause boundaries. It happens that, in the case of
full parses, this node selection strategy is equivalent
to the pruning process defined by Xue and Palmer
(2004), which selects sibling nodes along the path of
ancestors from the verb predicate to the root of the
tree2. Due to this pruning stage, the upper-bound re-
call figures are 95.67% for PPUPC and 90.32% for
FPCHA. These values give F1 performance upper
bounds of 97.79 and 94.91, respectively, assuming
perfect predictors (100% precision).
The nodes selected are labeled with B-I-O tags
depending if they are at the beginning, inside, or out-
side of a verb argument. There is a total of 37 argu-
ment types, which amount to 37*2+1=75 labels.
Regarding the learning algorithm, we used gen-
eralized AdaBoost with real-valued weak classifiers,
which constructs an ensemble of decision trees of
fixed depth (Schapire and Singer, 1999). We con-
sidered a one-vs-all decomposition into binary prob-
1Regions to the right of the target verb corresponding to an-
cestor clauses are omitted in the case of partial parsing.
2With the unique exception of the exploration inside sibling
PP constituents proposed by (Xue and Palmer, 2004).
193
lems to address multi-class classification.
AdaBoost binary classifiers are used for labeling
test sequences in a left-to-right tagging scheme us-
ing a recurrent sliding window approach with infor-
mation about the tag assigned to the preceding to-
ken. This tagging module ensures some basic con-
straints, e.g., BIO correct structure, arguments do
not cross clause boundaries nor base chunk bound-
aries, A0-A5 arguments not present in PropBank
frames for a certain verb are not allowed, etc. We
also tried beam search on top of the classifiers? pre-
dictions to find the sequence of labels with highest
sentence-level probability (as a summation of indi-
vidual predictions). But the results did not improve
the basic greedy tagging.
Regarding feature representation, we used all
input information sources, with the exception of
verb senses and Collins? parser. We did not con-
tribute with significantly original features. Instead,
we borrowed most of them from the existing liter-
ature (Gildea and Jurafsky, 2002; Carreras et al,
2004; Xue and Palmer, 2004). Broadly speaking, we
considered features belonging to four categories3:
(1) On the verb predicate:
? Form; Lemma; POS tag; Chunk type and Type of
verb phrase in which verb is included: single-word or
multi-word; Verb voice: active, passive, copulative, in-
finitive, or progressive; Binary flag indicating if the verb
is a start/end of a clause.
? Subcategorization, i.e., the phrase structure rule expand-
ing the verb parent node.
(2) On the focus constituent:
? Type; Head: extracted using common head-word rules;
if the first element is a PP chunk, then the head of the first
NP is extracted;
? First and last words and POS tags of the constituent.
? POS sequence: if it is less than 5 tags long; 2/3/4-grams
of the POS sequence.
? Bag-of-words of nouns, adjectives, and adverbs in the
constituent.
? TOP sequence: sequence of types of the top-most syn-
tactic elements in the constituent (if it is less than 5 ele-
ments long); in the case of full parsing this corresponds to
the right-hand side of the rule expanding the constituent
node; 2/3/4-grams of the TOP sequence.
? Governing category as described in (Gildea and Juraf-
sky, 2002).
3Features extracted from partial parsing and Named Enti-
ties are common to PPUPC and FPCHA models, while features
coming from Charniak parse trees are implemented exclusively
in the FPCHA model.
? NamedEnt, indicating if the constituent embeds or
strictly-matches a named entity along with its type.
? TMP, indicating if the constituent embeds or strictly
matches a temporal keyword (extracted from AM-TMP ar-
guments of the training set).
(3) Context of the focus constituent:
? Previous and following words and POS tags of the con-
stituent.
? The same features characterizing focus constituents are
extracted for the two previous and following tokens,
provided they are inside the clause boundaries of the cod-
ified region.
(4) Relation between predicate and constituent:
? Relative position; Distance in words and chunks; Level
of embedding with respect to the constituent: in number
of clauses.
? Constituent path as described in (Gildea and Jurafsky,
2002); All 3/4/5-grams of path constituents beginning at
the verb predicate or ending at the constituent.
? Partial parsing path as described in (Carreras et al,
2004); All 3/4/5-grams of path elements beginning at the
verb predicate or ending at the constituent.
? Syntactic frame as described by Xue and Palmer (2004)
2 Experimental Setting and Results
We trained the classification models using the com-
plete training set (sections from 02 to 21). Once con-
verted into one sequence per target predicate, the re-
sulting set amounts 1,049,049 training examples in
the PPUPC model and 828,811 training examples in
the FPCHA model. The average number of labels per
argument is 2.071 and 1.068, respectively. This fact
makes ?I? labels very rare in the FPCHA model.
When running AdaBoost, we selected as weak
rules decision trees of fixed depth 4 (i.e., each branch
may represent a conjunction of at most 4 basic fea-
tures) and trained a classification model per label for
up to 2,000 rounds.
We applied some simplifications to keep training
times and memory requirements inside admissible
bounds. First, we discarded all the argument la-
bels that occur very infrequently and trained only
the 41 most frequent labels in the case of PPUPC
and the 35 most frequent in the case of FPCHA.
The remaining labels where joined in a new label
?other? in training and converted into ?O? when-
ever the SRL system assigns a ?other? label dur-
ing testing. Second, we performed a simple fre-
quency filtering by discarding those features occur-
ring less than 15 times in the training set. As an
194
exception, the frequency threshold for the features
referring to the verb predicate was set to 3. The final
number of features we worked with is 105,175 in the
case of PPUPC and 80,742 in the case of FPCHA.
Training with these very large data and feature
sets becomes an issue. Fortunately, we could split
the computation among six machines in a Linux
cluster. Using our current implementation combin-
ing Perl and C++ we could train the complete mod-
els in about 2 days using memory requirements be-
tween 1.5GB and 2GB. Testing with the ensembles
of 2,000 decision trees per label is also not very effi-
cient, though the resulting speed is admissible, e.g.,
the development set is tagged in about 30 minutes
using a standard PC.
The overall results obtained by our individual
PPUPC and FPCHA SRL systems are presented in ta-
ble 1, with the best results in boldface. As expected,
the FPCHA system significantly outperformed the
PPUPC system, though the results of the later can
be considered competitive. This fact is against the
belief, expressed as one of the conclusions of the
CoNLL-2004 shared task, that full-parsing systems
are about 10 F1 points over partial-parsing systems.
In this case, we obtain a performance difference of
2.18 points in favor of FPCHA.
Apart from resulting performance, there are addi-
tional advantages when using the FPCHA approach.
Due to the coarser granularity of sequence tokens,
FPCHA sequences are shorter. There are 21% less
training examples and a much lower quantity of ?I?
tags to predict (the mapping between syntactic con-
stituents and arguments is mostly one-to-one). As
a consequence, FPCHA classifiers train faster with
less memory requirements, and achieve competitive
results (near the optimal) with much less rounds of
boosting. See figure 1. Also related to the token
granularity, the number of completely correct out-
puts is 4.13 points higher in FPCHA, showing that
the resulting labelings are structurally better than
those of PPUPC.
Interestingly, the PPUPC and FPCHA systems
make quite different argument predictions. For in-
stance, FPCHA is better at recognizing A0 and A1
arguments since parse constituents corresponding to
these arguments tend to be mostly correct. Compar-
atively, PPUPC is better at recognizing A2-A4 argu-
ments since they are further from the verb predicate
 64
 66
 68
 70
 72
 74
 76
 78
 200  400  600  800  1000  1200  1400  1600  1800  2000
O
ve
ra
ll F
1
Number of rounds
PP-upc
FP-cha
PP best
FP-cha best
Figure 1: Overall F1 performance of individual sys-
tems on the development set with respect to the num-
ber of learning rounds
Perfect props Precision Recall F?=1
PPUPC 47.38% 76.86% 70.55% 73.57
FPCHA 51.51% 78.08% 73.54% 75.75
Combined 51.39% 78.39% 75.53% 76.93
Table 1: Overall results of the individual systems on
the development set.
and tend to accumulate more parsing errors, while
the fine granularity of the PPUPC sequences still al-
low to capture them4. Another interesting observa-
tion is that the precision of both systems is much
higher than the recall.
The previous two facts suggest that combining the
outputs of the two systems may lead to a significant
improvement. We experimented with a greedy com-
bination scheme for joining the maximum number of
arguments from both solutions in order to increase
coverage and, hopefully, recall. It proceeds depart-
ing from an empty solution by: First, adding all the
arguments from FPCHA in which this method per-
forms best; Second, adding all the arguments from
PPUPC in which this method performs best; and
Third, making another loop through the two meth-
ods adding the arguments not considered in the first
loop. At each step, we require that the added argu-
ments do not overlap/embed with arguments in the
current solution and also that they do not introduce
repetitions of A0-A5 arguments. The results on the
4As an example, the F1 performance of PPUPC on A0 and
A2 arguments is 79.79 and 65.10, respectively. The perfor-
mance of FPCHA on the same arguments is 84.03 and 62.36.
195
Precision Recall F?=1
Development 78.39% 75.53% 76.93
Test WSJ 79.55% 76.45% 77.97
Test Brown 70.79% 64.35% 67.42
Test WSJ+Brown 78.44% 74.83% 76.59
Test WSJ Precision Recall F?=1
Overall 79.55% 76.45% 77.97
A0 87.11% 86.28% 86.69
A1 79.60% 76.72% 78.13
A2 69.18% 67.75% 68.46
A3 76.38% 56.07% 64.67
A4 79.78% 69.61% 74.35
A5 0.00% 0.00% 0.00
AM-ADV 59.15% 52.37% 55.56
AM-CAU 73.68% 57.53% 64.62
AM-DIR 71.43% 35.29% 47.24
AM-DIS 77.14% 75.94% 76.54
AM-EXT 63.64% 43.75% 51.85
AM-LOC 62.74% 54.27% 58.20
AM-MNR 54.33% 52.91% 53.61
AM-MOD 96.16% 95.46% 95.81
AM-NEG 99.13% 98.70% 98.91
AM-PNC 53.49% 40.00% 45.77
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 77.68% 78.75% 78.21
R-A0 86.84% 88.39% 87.61
R-A1 75.32% 76.28% 75.80
R-A2 54.55% 37.50% 44.44
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 69.81% 71.15% 70.48
V 99.16% 99.16% 99.16
Table 2: Overall results (top) and detailed results on
the WSJ test (bottom).
development set (presented in table 1) confirm our
expectations, since a performance increase of 1.18
points over the best individual system was observed,
mainly caused by recall improvement. The final sys-
tem we presented at the shared task performs exactly
this solution merging procedure. When applied on
the WSJ test set, the combination scheme seems to
generalize well, since an improvement is observed
with respect to the development set. See the offi-
cial results of our system, which are presented in ta-
ble 2. Also from that table, it is worth noting that the
F1 performance drops by more than 9 points when
tested on the Brown test set, indicating that the re-
sults obtained on the WSJ corpora do not generalize
well to corpora with other genres. The study of the
sources of this lower performance deserves further
investigation, though we do not believe that it is at-
tributable to the greedy combination scheme.
3 Conclusions
We have presented a simple SRL system submit-
ted to the CoNLL-2005 shared task, which treats
the SRL problem as a sequence tagging task (us-
ing a BIO tagging scheme). Given the simplic-
ity of the approach, we believe that the results are
very good and competitive compared to the state-
of-the-art. We also provided a comparison between
two SRL systems sharing the same architecture, but
build on partial vs. full parsing, respectively. Al-
though the full parsing approach obtains better re-
sults and has some implementation advantages, the
partial parsing system shows also a quite competi-
tive performance. The results on the development
set differ in 2.18 points, but the outputs generated
by the two systems are significantly different. The
final system, which scored F1=76.59 in the official
test set, is a combination of both individual systems
aiming at increasing coverage and recall.
Acknowledgements
This research has been partially supported by the
European Commission (CHIL project, IP-506909).
Jesu?s Gime?nez is a research fellow from the Span-
ish Ministry of Science and Technology (ALIADO
project, TIC2002-04447-C02). We would like to
thank also Xavier Carreras for providing us with
many software components and Mihai Surdeanu for
fruitful discussions on the problem and feature engi-
neering.
References
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hierarchical
recognition of propositional arguments with perceptrons. In
Proceedings of CoNLL-2004.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
R. E. Schapire and Y. Singer. 1999. Improved Boosting Algo-
rithms Using Confidence-rated Predictions. Machine Learn-
ing, 37(3).
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP).
196
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 20?29,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Full Machine Translation for Factoid Question Answering
Cristina Espan?a-Bonet and Pere R. Comas
TALP Research Center
Universitat Polite`cnica de Catalunya (UPC)
{cristinae,pcomas}@lsi.upc.edu
Abstract
In this paper we present an SMT-based ap-
proach to Question Answering (QA). QA
is the task of extracting exact answers in
response to natural language questions. In
our approach, the answer is a translation of
the question obtained with an SMT system.
We use the n-best translations of a given
question to find similar sentences in the
document collection that contain the real
answer. Although it is not the first time that
SMT inspires a QA system, it is the first
approach that uses a full Machine Transla-
tion system for generating answers. Our ap-
proach is validated with the datasets of the
TREC QA evaluation.
1 Introduction
Question Answering (QA) is the task of extract-
ing short, relevant textual answers from a given
document collection in response to natural lan-
guage questions. QA extends IR techniques be-
cause it outputs concrete answers to a question
instead of references to full documents which are
relevant to a query. QA has attracted the attention
of researchers for some years, and several pub-
lic evaluations have been recently carried in the
TREC, CLEF, and NTCIR conferences (Dang et
al., 2007; Pen?as et al, 2011; Sakai et al, 2008).
All the example questions of this paper are ex-
tracted from the TREC evaluations.
QA systems are usually classified according to
what kind of questions they can answer; factoid,
definitional, how to or why questions are treated in
a distinct way. This work focuses on factoid ques-
tions, that is, those questions whose answers are
semantic entities (e.g., organisation names, per-
son names, numbers, dates, objects, etc.). For ex-
ample, the question Q1545: What is a female rab-
bit called? is factoid and its answer, ?doe?, is a
semantic entity (although not a named entity).
Factoid questions written in natural language
contain implicit information about the relations
between the concepts expressed and the expected
outcomes of the search, and QA explicitly ex-
ploits this information. Using an IR engine to
look up a boolean query would not consider the
relations therefore losing important information.
Consider the question Q0677: What was the name
of the television show, starring Karl Malden, that
had San Francisco in the title? and the candi-
date answer A. In this question, two types of
constraints are expressed over the candidate an-
swers. One is that the expected type of A is a
kind of ?television show.? The rest of the ques-
tion indicates that ?Karl Malden? is related to A
as being ?starred? by, and that ?San Francisco?
is a substring of A. Many factoid questions ex-
plicitly express an hyponymy relation about the
answer type, and also several other relations de-
scribing its context (i.e. spatial, temporal, etc.).
The QA problem can be approached from sev-
eral points of view, ranging from simple surface
pattern matching (Ravichandran and Hovy, 2002),
to automated reasoning (Moldovan et al, 2007)
or supercomputing (Ferrucci et al, 2010). In
this work, we propose to use Statistical Machine
Translation (SMT) for the task of factoid QA. Un-
der this perspective, the answer is a translation of
the question. It is not the first time that SMT is
used for QA tasks, several works have been us-
ing translation models to determine the answers
(Berger et al, 2000; Cui et al, 2005; Surdeanu
et al, 2011). But to our knowledge this is the first
20
approach that uses a full Machine Translation sys-
tem for generating answers.
The paper is organised as follows: Section 2
reviews the previous usages of SMT in QA, Sec-
tion 3 reports our theoretical approach to the task,
Section 4 describes our QA system, Section 5
presents the experimental setting, Section 6 anal-
yses the results and Section 7 draws conclusions.
2 Translation Models in QA
The use of machine translation in IR is not new.
Berger and Lafferty (1999) firstly propose a prob-
abilistic approach to IR based on methods of
SMT. Under their perspective, the human user has
an information need that is satisfied by an ?ideal?
theoretical document d from which the user draws
important query words q. This process can be
mirrored by a translation model: given the query
q, they find the documents in the collection with
words a most likely to translate to q. The key
ingredient is the set of translation probabilities
p(q|a) from IBM model 1 (Brown et al, 1993).
In a posterior work, Berger et al also intro-
duce the formulation of the QA problem in terms
of SMT (Berger et al, 2000). They estimate the
likelihood that a given answer containing a word
ai corresponds to a question containing word
qj . This estimation relies on an IBM model 1.
The method is tested with a collection of closed-
domain Usenet and call-center questions, where
each question must be paired with one of the
recorded answers. Soricut and Brill (2004) im-
plement a similar strategy but with a richer for-
mulation and targeted to open-domain QA. Given
a question Q, a web-search engine is used to
retrieve 3-sentence-long answer texts from FAQ
pages. These texts are later ranked with the like-
lihood of containing the answer to Q, and this
likelihood is estimated via a noisy-channel archi-
tecture. The work of Murdock and Croft (2005)
applies the same strategy to TREC data. They
evaluate the TREC 2003 passage retrieval task.
In this task, the system must output a single sen-
tence containing the answer to a factoid ques-
tion. Murdock and Croft tackle the length dis-
parity in question-answer pairs and show that this
MT-based approach outperforms traditional query
likelihood techniques.
Riezler et al (2007) define the problem of an-
swer retrieval from FAQ and social Q/A websites
as a query expansion problem. SMT is used to
translate the original query terms to the language
of the answers, thus obtaining an expanded list of
terms usable in standard IR techniques. They also
use SMT to perform question paraphrasing. In the
same context, Lee et al (2008) study methods for
improving the translation quality removing noise
from the parallel corpus.
SMT can be also applied to sentence represen-
tations different than words. Cui et al (2005)
approach the task of passage retrieval for QA
with translations of dependency parsing relations.
They extract the sequences of relations that link
each pair of words in the question and, using the
IBM translation model 1, score their similarity
to the relations extracted from the candidate pas-
sage. Thus, an approximate relation matching
score is obtained. Surdeanu et al (2011) extend
the scope of this approach by combining together
the translation probabilities of words, dependency
relations, and semantic roles in the context of an-
swer searching in FAQ collections.
The works we have described so far use
archives of question-answer pairs as information
sources. They are really doing document re-
trieval and sentence retrieval rather than question
answering, because every document/sentence is
known to be the answer of a question written in
the form of an answer, and no further information
extraction is necessary, they just select the best
answer from a given pool of answers. The dif-
ference with a standard IR task is that these sys-
tems are not searching for relevant documents but
for answer documents. In contrast, Echihabi and
Marcu (2003) introduce an SMT-based method
for extracting the concrete answer in factoid QA.
First, they use a standard IR engine to retrieve
candidate sentences and process them with a con-
stituent parser. Then, an elaborated process sim-
plifies these parse trees converting them into se-
quences of relevant words and/or syntactic tags.
This process reduces the length disparity between
questions and answers. For the answer extraction,
a special tag marking the position of the answer
is sequentially added to all suitable positions in
the sentence, thus yielding several candidate an-
swers for each sentence. Finally, each answer is
rated according to its likelihood of being a trans-
lation of the question, according to an IBM model
4 trained on a corpus of TREC and web-based
question-answer pairs.
With the exception of the query expansion ap-
21
proaches (Riezler et al, 2007), all works dis-
cussed here use some form of noisy-channel
model (translation model and target language
model) but do not perform the decoding part of
the SMT process to generate translations, nor use
the rich set of features of a full SMT. In fact, the
formulation of the noisy-channel in these works
has very few differences with pure language mod-
elling approaches to QA like the one of Heie et al
(2011), where two different models for retrieval
and filtering are learnt from a corpus of question-
answer pairs.
3 Question-to-Answer Translation
The core of our QA system is an SMT system for
the Question-to-Answer language pair. In SMT,
the best translation for a given source sentence is
the most probable one, and the probability of each
translation is given by the Bayes theorem. In our
case, the source sentence corresponds to the ques-
tion Q and the target or translation is the sentence
containing the answer A. With this correspon-
dence, the fundamental equation of SMT can be
written as:
A(Q) = A? = argmaxA P (A|Q)
= argmaxA P (Q|A)P (A), (1)
where P (Q|A) is the translation model and P (A)
is the language model, and each of them can be
understood as the sum of the probabilities for each
of the segments or phrases that conform the sen-
tence. The translation model quantifies the appro-
priateness of each segment of Q being answered
by A; the language model is a measure of the flu-
ency of the answer sentence and does not take into
account which is the question. Since we are in-
terested in identifying the concrete string that an-
swers the question and not a full sentence, this
probability is not as important as it is in the trans-
lation problem.
The log-linear model (Och and Ney, 2002), a
generalisation of the original noisy-channel ap-
proach (Eq. 1), estimates the final probability as
the logarithmic sum of several terms that depend
on both the question Q and the answer sentence
A. Using just two of the features, the model re-
produces the noisy-channel approach but written
in this way one can include as many features as
desired at the cost of introducing the same number
of free parameters. The model in its traditional
form includes 8 terms:
A(Q) = A? = argmaxA logP (A|Q) =
+ ?lm logP (A) + ?d logPd(A,Q)
+ ?lg log lex(Q|A) + ?ld log lex(A|Q)
+ ?g logPt(Q|A) + ?d logPt(A|Q)
+ ?ph log ph(A) + ?w logw(A) , (2)
where P (A) is the language model probabil-
ity, lex(Q|A) and lex(A|Q) are the generative
and discriminative lexical translation probabilities
respectively, Pt(Q|A) the generative translation
model, Pt(A|Q) the discriminative one, Pd(A,Q)
the distortion model, and ph(A) and w(A) corre-
spond to the phrase and word penalty models. We
start by using this form for the answer probabil-
ity and analyse the importance and validity of the
terms in the experiments Section. The ? weights,
which account for the relative importance of each
feature in the log-linear probabilistic model, are
commonly estimated by optimising the translation
performance on a development set. For this opti-
misation one may use Minimum Error Rate Train-
ing (MERT) (Och, 2003) where BLEU (Papineni
et al, 2002) is the reference evaluation.
Once the weights are determined and the prob-
abilities estimated from a corpus of question-
answer pairs (a parallel corpus in this task), a de-
coder uses Eq. 2 to score the possible outputs and
to find the best answer sentence given a question
or, in general, an n-best list of answers.
This formulation, although possible from an
abstract point of view, is not feasible in prac-
tice. The corpus from which probabilities are es-
timated is finite, and therefore new questions may
not be represented. There is no chance that SMT
can generate ex nihilo the knowledge necessary to
answer questions such as Q1201: What planet has
the strongest magnetic field of all the planets?.
So, rather than generating answers via translation,
we use translations as indicators of the sentence
context where an answer can be found. Context
here has not only the meaning of near words but
also a context at a higher level of abstraction.
To achieve this, we use two different represen-
tations of the question-answer pairs and two dif-
ferent SMT models in our QA system. We call
Level1 representation the original strings of text
of the question-answer pairs. The Level2 repre-
sentation, that aims at being more abstract, more
general and more useful in SMT, is constructed
22
applying this sequence of transformations: 1)
Quoted expressions in the question are identified,
paired with their counterpart in the answer (in
case any exists) and substituted by a special tag
QUOTED. 2) Each named entity is substituted
by its entity class (e.g., ?Karl Malone? by PER-
SON). 3) Each noun and verb is substituted by
their WordNet supersense1 (e.g. ?nickname? by
COMMUNICATION). 4) Any remaining word,
such as adjectives, adverbs and stop words, is left
as is. Additionally, in the answer sentence string,
the correct answer entity is substituted by a spe-
cial tag ANSWER. An example of this annotation
is given in Figure 1.
An SMT system trained with Level1 examples
will translate Q to answer sentences with vocab-
ulary and structure similar to the learning exam-
ples. The Level2 system will translate to a mix of
named entities, WordNet supersenses, bare words,
and ANSWER markers that represent the abstract
structure of the answer sentence. We call patterns
to the Level2 translations. The rationale of this
process is that the SMT model can learn the con-
text where answers appear depending of the struc-
ture of the question. The obtained translations
from both levels can be searched in the document
collection to find sentences that are very similar.
Note that in Level2, the vocabulary size of
the question-answer pairs is dramatically reduced
with respect to the original Level1 sentences, as
seen in Table 2. Thus, the sparseness is reduced,
and the translation model gains in coverage; pat-
terns are also easier to find than Level1 sentences,
and give flexibility and generality to the transla-
tion. And the most important feature, patterns
capture the context of the answer, pinpointing it
with accuracy.
These Level1 and Level2 translations are the
core of our QA system that is presented in the fol-
lowing Section.
4 The Question Answering System
Our QA system is a pipeline of three modules.
In the first one, the question is analysed and an-
notated with several linguistic processors. This
information is used by the rest of the modules.
In the second one, relevant documents are ob-
1WordNet noun synsets are organised in 26 semantic cat-
egories based on logical groupings, e.g., ARTIFACT, ANI-
MAL, BODY, COMMUNICATION. . . The verbs are organ-
ised in 15 categories. (Fellbaum, 1998)
Level1 Q: What is Karl Malone?s nickname ?
Level1 A: Malone , whose overall consistency has earned
him the nickname ANSWER , missed both of them with nine
seconds remaining .
Level2 Q: What STATIVE B-PERSON ?s COMMUNICA-
TION ?
Level2 A: B-PERSON , whose overall ATTRIBUTE POS-
SESSION POSSESSION him the COMMUNICATION
ANSWER , PERCEPTION both of them with B-NUM TIME
CHANGE .
Figure 1: Example of the two annotation levels used.
tained from the document collection with straight-
forward IR techniques and a list of candidate an-
swers is generated. Finally, these candidate an-
swers are filtered and ranked to obtain a final list
of proposed answers. This pipeline is a common
architecture for a simple QA system.
4.1 Question Analysis
Questions are processed with a tokeniser, a POS
tagger, a chunker, and a NERC. Besides, each
word is tagged with its most frequent sense in
WordNet. Then, a maximum-entropy classi-
fier determines the most probable expected an-
swer types for the question (EAT). This classi-
fier is built following the approach of Li and Roth
(2005), it can classify questions into 53 different
answer types and belongs to our in-house QA sys-
tem. Finally, a weighted list of relevant keywords
is extracted from the question. Their saliences are
heuristically determined: the most salient tokens
are the quoted expressions, followed by named
entities, then sequences of nouns and adjectives,
then nouns, and finally verbs and any remaining
non-stop word. This list is used in the candidate
answer generation module.
4.2 Candidate Answer Generation
The candidate answer generation comprises two
steps. First a set of passages is retrieved from the
document collection, and then the candidate an-
swers are extracted from the text.
For the retrieval, we have used the passage
retrieval module of our in-house QA system.
The passage retrieval algorithm initially creates
a boolean query with all nouns and more salient
words, and sets a threshold t to 50. It uses the
Lucene IR engine2 to fetch the documents match-
2http://lucene.apache.org
23
ing the current query and a subsequent passage
construction module extracts passages as docu-
ment segments where two consecutive keyword
occurrences are separated by at most t words.
If too few or too many passages are obtained
this way, a relaxation procedure is applied. The
process iteratively adjusts the salience level of
the keywords used in the query by dropping low
salient words when too few are obtained or adding
them when too many, and it also adjusts their
proximity threshold until the quality of the recov-
ered information is satisfactory (see ?) for further
details).
When the passages have been gathered, they
are split into sentences and processed with POS
tagging, chunking and a NERC. The candidate an-
swer list is composed of all named entities and
all phrases containing a noun. Each candidate is
associated to the sentence it has been extracted
from.
4.3 Answer Ranking
This module selects the best answers from the
candidates previously generated. It employs three
families of scores to rank them.
Context scores B and R: The n-best list of
Level2 question translations is generated. In this
step retrieved sentences are also transformed to
the Level2 representation. Then, each candidate
answer is replaced by the special ANSWER tag in
the associated sentence, thus, each sentence has a
unique ANSWER tag, as in the training examples.
Finally, each candidate is evaluated assessing the
similarity of the source sentence with the n-best
translations.
For this assessment we use two different met-
rics. One of them is a lexical metric commonly
used in machine translation, BLEU (Papineni et
al., 2002). A smoothed version is used to evalu-
ate the pairs at sentence level yielding the score B.
The other metric is ROUGE (Lin and Och, 2004),
here named R. We use the skip-bigram overlap-
ping measure with a maximum skip distance of
4 unigrams (ROUGE-S4). Contrary to BLEU,
ROUGE-S does not require consecutive matches
but is still sensitive to word order.
Both BLEU and ROUGE are well-known met-
rics that are useful for finding partial matchings in
long strings of words. Therefore it is an easy way
of implementing an approximated pattern match-
ing algorithm with off-the-shelf components.
Although these scores can determine if a sen-
tence is a candidate for asserting a certain prop-
erty of a certain object, they do not have the power
to discriminate if these objects are the actually re-
quired by the question. Level2 representation is
very coarse and, for example, treats all named en-
tities of the same categories as the same word.
Thus, it is prone to introduce noise in the form
of totally irrelevant answers. For example, con-
sider the questions Q1760: Where was C.S. Lewis
born? and Q1519: Where was Hans Christian
Anderson born?. Both questions have the same
Level2 representation: Where STATIVE PERSON
STATIVE?, and the same n-best list of transla-
tions. Any sentence stating the birthplace (or even
deathplace) of any person is equally likely to be
the correct answer of both questions because the
lexicalisation of Lewis and Anderson is lost.
On the other hand, B and R also show another
limitation. Since they are based on n-gram match-
ing, they cannot be discriminative enough when
there is only one different token between options,
and that happens when a same sentence has differ-
ent candidates for the answer. In this case the sys-
tem would be able to distinguish among answer
sentences but then all the variations with the an-
swer in a different position would have too much
similar scores. In order to mitigate these draw-
backs, we consider two other scores.
Language scores Lb, Lr, Lf : To alleviate the
discriminative problem of the context matching
metrics, we calculate the same B and R scores
but with Level1 translations and the original lexi-
calised question. These are the Lb and Lr scores.
Additionally, we introduce a new score Lf that
does not take into account the n-gram structure
of the sentences: after the n-best list of Level1
question translations is generated, the frequency
of each word present in the translations is com-
puted. Then, the words in the candidate answer
sentence are scored according to their normalised
frequency in the translations list and added up to-
gether. This score lies in the [0, 1] range.
Expected answer type score E: This score
checks if the type of the answer we are evalu-
ating matches the expected types we have deter-
mined in the question analysis. For this task, the
expected answer types are mapped to named enti-
ties and/or supersenses (e.g., type ENTY:product
24
is mapped to ARTIFACT). If the candidate answer
is a named entity of the expected type, or con-
tains a noun of the expected supersense, then this
candidate receives a score E equal to the confi-
dence of the question classification (the scores of
the ME classifier have been previously normalised
to probabilities).
These three families of scores can be combined
in several ways in order to produce a ranked list
of answers. In Section 6 the combination methods
are discussed.
5 Experiments
5.1 Training and Test Corpora
We have used the datasets from the Question
Answering Track of the TREC evaluation cam-
paigns3 ranging from TREC-9 to TREC-16 in our
experiments. These datasets provide both a robust
testbed for evaluation, and a source of question-
answer pairs to use as a parallel corpus for train-
ing our SMT system. Each TREC evaluation
provides a collection of documents composed of
newspaper texts (three different collections have
been used over the years), a set of new ques-
tions, and an answer key providing both the an-
swer string and the source document. Descrip-
tion of these collections can be found in the TREC
overviews (Voorhees, 2002; Dang et al, 2007).
We use the TREC-11 questions for test pur-
poses, the remaining sets are used for training un-
less some parts of TREC-9, TREC-10 and TREC-
12 that are kept for fitting the weights of our SMT
system. To gather the SMT corpus, we select all
the factoid questions whose answer can be found
in the documents and extract the full sentence that
contains the answer. With this methodology, a
parallel corpus with 12,335 question-answer pairs
is obtained. We have divided it into two subsets:
the pairs with only a single answer found in the
documents are used for the development set, and
the remaining pairs (i.e. having multiple occur-
rences of the correct answer) are used for train-
ing. The test set are the 500 TREC-11 questions,
452 out of them have a correct answer in the doc-
uments. The numbers are summarised in Table 1.
In order to obtain the Level2 representation of
these corpora, the documents and the test sets
must be annotated. For the annotation pipeline
3http://trec.nist.gov/data/qamain.html
Q A TRECs
Train 2264 12116 9,10,12,13,14,15,16
Dev 219 219 9,10,12
Test 500 2551 11
Table 1: Number of Questions and Answers in our data
sets. The number of TREC evaluation from which are
obtained is indicated.
Tokens Vocabulary
Q A Q A
TrainL1 97028 393978 3232 32013
TrainL2 91567 373008 540 9130
Table 2: Statistics for the 12,116 Q-A pairs in the train-
ing corpus according to the annotation level.
we use the TnT POS tagger (Brants, 2000),
WordNet (Fellbaum, 1998), the YamCha chun-
ker (Kudo and Matsumoto, 2003), the Stanford
NERC (Finkel et al, 2005), and an in-house tem-
poral expressions recogniser.
Table 2 shows some statistics for the parallel
corpus and the two different levels of annotation.
From the SMT point of view the corpus is small
in order to estimate the translation probabilities in
a reliable way but, as stated before, Level2 repre-
sentation diminishes the vocabulary considerably
and alleviates the problem.
5.2 SMT system
The statistical system is a state-of-the-art phrase-
based SMT system trained on the previously
introduced corpus. Its development has been
done using standard freely available software.
The language model is estimated using interpo-
lated Kneser-Ney discounting with SRILM (Stol-
cke, 2002). Word alignment is done with
GIZA++ (Och and Ney, 2003) and both phrase
extraction and decoding are done with the Moses
package (Koehn et al, 2007). The model weights
are optimised with Moses? script of MERT
against the BLEU evaluation metric.
For the full model, we consider the language
model, direct and inverse phrase probabilities, di-
rect and inverse lexical probabilities, phrase and
word penalties, and a non-lexicalised reordering.
5.3 QA system
The question answering system has three differ-
ent modules as explained in Section 4. For the
25
T1 T50 MRR
QA 0.006 (4) 0.206 (14) 0.024 (4)
SR 0.066 (8) 0.538 (9) 0.142 (8)
Upper bound 0.677 0.677 0.677
Table 3: Mean and standard deviation for 1000 real-
isations of the random baseline for QA and SR. The
upper bound is also shown.
first module, questions are annotated using the
same tools introduced in the corpora Section. The
second module generates 2,866,098 candidate an-
swers (373,323 different sentences), that is to say,
a mean of 5,700 answers per question (750 sen-
tences per question). These candidates are made
available to the third module resulting in the ex-
periments that will be discussed in Section 6.
The global QA system performance is evalu-
ated with three measures. T1 is a measure of
the system?s precision and gives the percentage
of correct answers in the first position; T50 gives
the number of correct answers in the first 50 po-
sitions, in some cases that corresponds to all can-
didate answers; finally the Mean Reciprocal Rank
(MRR) is a measure of the ranking capability of
the system and is estimated as the mean of the in-
verse ranking of the first correct answer for every
question: MRR= Q?1
?
i rank
?1
i .
6 Results Analysis
Given the set of answers retrieved by the candi-
date answer generation module, a na??ve baseline
system is estimated by selecting randomly 50 an-
swers for each of the questions. Table 3 shows
the mean of the three measures after applying this
random process 1000 times. The upper bound
of this task is the oracle that selects always the
correct answer/sentence if it is present in the re-
trieved passages. An answer is considered correct
if it perfectly matches the official TREC?s answer
key and a sentence is correct if it contains a cor-
rect answer. The random baseline has a precision
of 0.6%.
We also evaluate a second task, sentence re-
trieval for QA (SR). In this task, the system has
to provide a sentence that contains the answer, but
not to extract it. Within our SMT approach, both
tasks are done simultaneously, because the answer
is extracted according to its context sentence. A
random baseline for this second task, where only
QA SR
Metric T1 T50 MRR T1 T50 MRR
B 0.018 0.292 0.049 0.084 0.540 0.164
R 0.018 0.283 0.045 0.119 0.608 0.209
B+R 0.022 0.294 0.053 0.097 0.573 0.180
BR 0.027 0.294 0.057 0.137 0.591 0.211
Table 4: System performance using an SMT that gen-
erates a 100-best list, uses a 5-gram LM and all the
features of the TM.
1st best: The B-ORGANIZATION B-LOCATION ,
B-DATE ( B-ORGANIZATION ) - B-PERSON , whose
COMMUNICATION STATIVE ? ANSWER . ?
50th best: The ANSWER ANSWER , B-DATE ( B-ORGA-
NIZATION ) - B-PERSON , the PERSON of ANSWER
, the most popular ARTIFACT , serenely COGNITION
COMMUNICATION .
100th best: The B-LOCATION , B-DATE ( B-ORGANIZA-
TION ) - B-PERSON , the PERSON of ANSWER , COM-
MUNICATION B-LOCATION ?s COMMUNICATION .
Figure 2: Example of patterns found in an n-best list.
full sentences without marked answers are taken
into account, can also be read in Table 3.
We begin this analysis studying the perfor-
mance of the SMT-based parts alone. Table 4
shows the results when using an SMT decoder
that generates a 100-best list, uses a 5-gram lan-
guage model and all the features of the transla-
tion model. An example of the generated patterns
in Level2 representation can be found in Figure 2
for the question of Figure 1, Q1565: What is Karl
Malone?s nickname?.
Candidate answer sentences are ranked accord-
ing to the similarity with the patterns generated by
translation as measured by BLEU (B), ROUGE-
S4 (R) or combinations of them. To calcu-
late these metrics the n-best list with patterns is
considered to be a list of reference translations
(Fig. 2) to every candidate (Fig. 1). In general,
a combination of both metrics is more powerful
than any of them alone and the product outper-
forms the sum given that in most cases BLEU is
larger than ROUGE and smooths its effect. The
inclusion of the SMT patterns improves the base-
line but it does not imply a quantum leap. T1 is
at least three times better than the baseline?s one
but still the system answers less than a 3% of the
questions. In the first 50 positions the answer is
26
SMT Features T1 T50 MRR
Lex, LM5, 100-best 0.027 0.294 0.057
noLex, LM5, 100-best 0.015 0.281 0.045
Lex, LM3, 100-best 0.015 0.257 0.041
Lex, LM7, 100-best 0.033 0.288 0.050
Lex, LM5, 10-best 0.024 0.310 0.056
Lex, LM5, 1000-best 0.027 0.301 0.061
Lex, LM5, 10000-best 0.011 0.290 0.045
Table 5: System performance with different combina-
tions of the SMT features used in decoding. BR is the
metric used to score the answers.
found a 30% of the times. In the sentence re-
trieval task, results grow up to 14% and 59% re-
spectively. Its difference between tasks shows one
of the limitations of these metrics commented be-
fore, they are not discriminative enough when the
only difference among options is the position of
the ANSWER tag inside the sentence. This is the
empirical indication of the need for a score like
E. On the other hand, each question has a mean
of 5,732 candidate answers, and although T50 is
not a significant measure, its good results indicate
that the context scores metrics are doing their job.
The highest T50, 0.608, is reached by R and it is
very close to the upper bound 0.667.
Taking BR as a reference measure, we investi-
gate the impact of three features of the SMT in
Table 5. Regarding the length of the language
model used in the statistical translation, there is
a trend to improve the accuracy with longer lan-
guage models (T1 is 0.015 for a LM3, 0.027 for
LM5 and 0.033 for LM7 with the product of met-
rics) but recall is not very much affected and the
best values are obtained for LM5.
Second, the number of features in the trans-
lation model indicates that the best scores are
reached when one reproduces the same number
of features as a standard translation system. That
is, all of the measures when the lexical trans-
lation probabilities are ignored are significantly
lower than when the eight features are used. In
a counterintuitive way, the token to token transla-
tion probability helps to improve the final system
although word alignments here can be meaning-
less or nonexistent given the difference in length
and structure between question and answer.
Finally, the length of the n-best list is not a de-
cisive factor to take into account. Since the ele-
QA SR
Metric T1 T50 MRR T1 T50 MRR
Lf 0.016 0.286 0.046 0.137 0.605 0.236
Lb 0.022 0.304 0.054 0.100 0.581 0.192
Lr 0.018 0.326 0.060 0.131 0.627 0.225
Lbrf 0.038 0.330 0.079 0.147 0.622 0.238
E 0.044 0.373 0.096 0.058 0.579 0.142
ELbrf 0.018 0.293 0.048 0.118 0.623 0.214
BLbrf 0.051 0.337 0.091 0.184 0.616 0.271
RLbrf 0.033 0.346 0.069 0.191 0.618 0.279
BRLbrf 0.042 0.350 0.082 0.182 0.616 0.273
(B+R)Lbrf 0.044 0.346 0.085 0.187 0.618 0.273
BE 0.035 0.384 0.084 0.086 0.579 0.179
RE 0.035 0.377 0.086 0.131 0.630 0.228
BRE 0.049 0.377 0.098 0.135 0.608 0.220
(B+R)E 0.040 0.386 0.091 0.102 0.596 0.196
BELbrf 0.093 0.379 0.137 0.200 0.621 0.283
RELbrf 0.071 0.377 0.123 0.208 0.619 0.294
BRELbrf 0.091 0.379 0.132 0.200 0.622 0.287
(B+R)ELbrf 0.100 0.377 0.141 0.204 0.621 0.286
Table 6: System performance according to three dif-
ferent ranking strategies: context score (B and R), the
language scores (Lx) and EAT type checking (E).
ments in a n-best list usually differ very little, and
this is even more important for a system with a
reduced vocabulary, increasing the size of the list
does not enrich in a substantial way the variety of
the generated answers and results show no signif-
icant variances. Given these observations, we fix
an SMT system with a 5-gram language model,
the full set of translation model features and the
generation of a 100-best list for obtaining B and
R scores.
Each score approaches different problems of
the task and therefore, complement each other
rather than overlapping. Table 6 introduces the
results of a selected group of score combinations,
where Lbrf =LbLrLf .
The scores Lbrf and E alone are not very useful
because Lbrf gives the same score to all candi-
dates in the same sentence and E gives the same
score to all candidates of the same type. Exper-
imental results confirm that, as expected, Lbrf is
more appropriate for the SR task and E for the
QA task, although the figures are very low. When
joining E and the Ls together, no improvement is
obtained, and the results for the QA task are worse
than Lbrf alone, thus demonstrating that Level1
translations are not good enough for the QA task.
27
A better system combines all the metrics together.
The best results are achieved when adding B
and R scores to the combination. All of these
combinations (i.e. B, R, BR and B+R) are bet-
ter when are multiplied by both E and Lbrf than
by only one of them alone. Otherwise, combina-
tions of only E and Lbrf yield very poor results.
Thus, the Level2 representation boosts T1 scores
from 0.018 (ELbrf ) to 0.100 ((B+R)ELbrf ) in QA
and almost doubles it in SR. As a general trend,
we see that combinations involving R but not B
are better in the SR task than in the QA task. In
fact the best results for SR are obtained with the
RELbrf combination. The best MRR scores are
achieved also with the best T1 scores.
7 Discussion and Conclusions
The results here presented are our approach to
consider question answering a translation prob-
lem. Questions in an abstract representation
(Level2) are translated into an abstract represen-
tation of the answer, and these generated answers
are matched against all the candidates obtained
with the retrieval module. The candidates are then
ranked according to their similarity with the n-
best list of translations as measured by three fam-
ilies of metrics that include R, B, E and L.
The best combination of metrics is able to
answer a 10.0% of the questions in first place
(T1). This result is in the lowest part of the ta-
ble reported by the official TREC-11 overview
(Voorhees, 2002). The approach of Echihabi
and Marcu (2003) that uses translation proba-
bilities to rank the answers achieves higher re-
sults on the same data set (an MRR of 0.325
versus our 0.141). Although both works use
SMT techniques, the approach is quite different.
In fact, our system is more similar in spirit to
that of Ravichandran and Hovy (2002), which
learns regular expressions to find answer contexts
and shows significant improvements for out-of-
domain test sets, that is web data. Besides the fact
that Echihabi and Marcu use translation models
instead of a full translation system, they explicitly
treat the problem of the difference of length be-
tween the question and the answer. In our work,
this is not further considered than by the word and
phrase penalty features of the translation model.
Future work will address this difficulty.
The results of sentence ranking of our system
are similar to those obtained by Murdock and
Croft (2005), however, since test sets are different
they are not directly comparable. This is notable
because we tackle QA, and sentence retrieval is
obtained as collateral information.
Possible lines of future research include the
study abstraction levels different from Level2.
The linguistic processors provide us with interme-
diate information such as POS that is not currently
used as it is WordNet and named entities. Sev-
eral other levels combining this information can
be also tested in order to find the most appropri-
ate degree of abstraction for each kind of word.
The development part of the SMT system is a
delicate issue. MERT is currently optimising to-
wards BLEU, but the final score for ranking the
answers is a combination of a smoothed BLEU,
ROUGE, L and E. It has been shown that opti-
mising towards the same metric used to evaluate
the system is beneficial for translation, but also
that BLEU is one of the most robust metrics to
be used (Cer et al, 2010), so the issue has to
be investigated for the QA problem. Also, refin-
ing BLEU and ROUGE for this specific problem
can be useful. A first approximation could be an
adaptation of the n-gram counting of BLEU and
ROUGE so that it is weighted by its distance to
the answer; this way sentences that differ only be-
cause of the candidate answer string would be bet-
ter differentiated.
Related to this, the generation of the candidate
answer strings is exhaustive; the suppression of
the less frequent candidates could help to elimi-
nate noise in the form of irrelevant answer sen-
tences. Besides, the system correlates these an-
swer strings with the expected answer type of
the question (coincidence measured with E). This
step should be replaced by an SMT-based mech-
anism to build a full system only based on SMT.
Furthermore, we plan to include the Level1 trans-
lations into the candidate answer generation mod-
ule in order to do query expansion in the style of
Riezler et al (2007).
Acknowledgements
This work has been partially funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (MOLTO project, FP7-ICT-2009-4-
247914) and the Spanish Ministry of Science
and Innovation projects (OpenMT-2, TIN2009-
14675-C03-01 and KNOW-2, TIN2009-14715-
C04-04).
28
References
A. Berger and J. Lafferty. 1999. Information retrieval
as statistical translation. In Proceedings of ACM SI-
GIR Conference.
A. Berger, R. Caruana, D. Cohn, D. Freitag, and
V. Mittal. 2000. Bridging the lexical chasm: statis-
tical approaches to answer-finding. In Proceedings
of the ACM SIGIR Conference.
T. Brants. 2000. TnT ? a statistical part-of-speech
tagger. In Proceedings ANLP Conference.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2).
D. Cer, C. D. Manning, and D. Jurafsky. 2010. The
best lexical metric for phrase-based statistical MT
system optimization. In Proceeding of the HLT
Conference.
H. Cui, R. Sun, K. Li, M.Y. Kan, and T.S. Chua. 2005.
Question answering passage retrieval using depen-
dency relations. In Proceedings of the ACM SIGIR
Conference.
H.T. Dang, D. Kelly, and J. Lin. 2007. Overview of
the TREC 2007 question answering track. In Pro-
ceedings of the Text REtrieval Conference, TREC.
A. Echihabi and D. Marcu. 2003. A noisy-channel
approach to question answering. In Proceedings of
the ACL Conference. ACL.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA: MIT Press.
D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan,
D. Gondek, A. Kalyanpur, A. Lally, J. Murdock,
E. Nyberg, J. Prager, N. Schlaefer, and C. Welty.
2010. Building Watson: An overview of the
DeepQA project. AI Magazine, 31(3):59?79.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005.
Incorporating non-local information into informa-
tion extraction systems by gibbs sampling. In ACL.
M.H. Heie, E.W.D. Whittaker, and S. Furui. 2011.
Question answering using statistical language mod-
elling. Computer Speech & Language.
P. Koehn, H. Hoang, A. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the ACL, Demonstration Session.
T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernelbased text analysis. In Proceedings of ACL
Conference.
J.T. Lee, S.B. Kim, Y.I. Song, and H.C. Rim. 2008.
Bridging lexical gaps between queries and ques-
tions on large online Q&A collections with compact
translation models. In Proceedings of the EMNLP
Conference. ACL.
X. Li and D. Roth. 2005. Learning question classi-
fiers: The role of semantic information. Journal of
Natural Language Engineering.
C-Y. Lin and F. Och. 2004. Automatic Evaluation of
Machine Translation Quality Using Longest Com-
mon Subsequence and Skip-Bigram Statics. In Pro-
ceedings of the ACL Conference.
D. Moldovan, C. Clark, S. Harabagiu, and D. Hodges.
2007. Cogex: A semantically and contextually en-
riched logic prover for question answering. Journal
of Applied Logic, 5(1).
V. Murdock and W.B. Croft. 2005. Simple translation
models for sentence retrieval in factoid question an-
swering. In Procedings of the ACM SIGIR Confer-
ence.
F. Och and H. Ney. 2002. Discriminative Training and
Maximum Entropy Models for Statistical Machine
Translation. In Proceedings of the ACL Conference.
F. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1).
F. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of the ACL
Conference.
K. Papineni, S. Roukos, T. Ward, and W-J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the ACL Con-
ference.
A. Pen?as, E. Hovy, P. Forner, A?. Rodrigo, R. Sutcliffe,
C. Forascu, and C. Sporleder. 2011. Overview
of QA4MRE at CLEF 2011: Question answering
for machine reading evaluation. Working Notes of
CLEF.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In
Proceedings of the ACL Conference.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mit-
tal, and Y. Liu. 2007. Statistical machine trans-
lation for query expansion in answer retrieval. In
Proceedings of the ACL Conference.
T. Sakai, N. Kando, C.J. Lin, T. Mitamura, H. Shima,
D. Ji, K.H. Chen, and E. Nyberg. 2008. Overview
of the NTCIR-7 ACLIA IR4QA task. In Proceed-
ings of NTCIR Conference.
R. Soricut and E. Brill. 2004. Automatic question
answering: Beyond the factoid. In Proceedings of
HLT-NAACL Conference.
A. Stolcke. 2002. SRILM ? An extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken
Language Processing.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2011.
Learning to rank answers to non-factoid questions
from web collections. Computational Linguistics,
37(2).
E.M. Voorhees. 2002. Overview of the TREC 2002
Question Answering track. In In Proceedings of the
Text REtrieval Conference, TREC.
29
