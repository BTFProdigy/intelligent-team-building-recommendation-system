Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 479?489,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Building Specialized Bilingual Lexicons Using Large-Scale Background
Knowledge
Dhouha Bouamor1, Adrian Popescu1, Nasredine Semmar1, Pierre Zweigenbaum2
1 CEA, LIST, Vision and Content Engineering Laboratory, 91191
Gif-sur-Yvette CEDEX, France; firstname.lastname@cea.fr
2LIMSI-CNRS, F-91403 Orsay CEDEX, France; pz@limsi.fr
Abstract
Bilingual lexicons are central components of
machine translation and cross-lingual infor-
mation retrieval systems. Their manual con-
struction requires strong expertise in both lan-
guages involved and is a costly process. Sev-
eral automatic methods were proposed as an
alternative but they often rely on resources
available in a limited number of languages
and their performances are still far behind
the quality of manual translations. We intro-
duce a novel approach to the creation of spe-
cific domain bilingual lexicon that relies on
Wikipedia. This massively multilingual en-
cyclopedia makes it possible to create lexi-
cons for a large number of language pairs.
Wikipedia is used to extract domains in each
language, to link domains between languages
and to create generic translation dictionaries.
The approach is tested on four specialized do-
mains and is compared to three state of the art
approaches using two language pairs: French-
English and Romanian-English. The newly in-
troduced method compares favorably to exist-
ing methods in all configurations tested.
1 Introduction
The plethora of textual information shared on the
Web is strongly multilingual and users? information
needs often go well beyond their knowledge of for-
eign languages. In such cases, efficient machine
translation and cross-lingual information retrieval
systems are needed. Machine translation already has
a decades long history and an array of commercial
systems were already deployed, including Google
Translate 1 and Systran 2. However, due to the intrin-
sic difficulty of the task, a number of related prob-
lems remain open, including: the gap between text
semantics and statistically derived translations, the
scarcity of resources in a large majority of languages
and the quality of automatically obtained resources
and translations. While the first challenge is general
and inherent to any automatic approach, the second
and the third can be at least partially addressed by
an appropriate exploitation of multilingual resources
that are increasingly available on the Web.
In this paper we focus on the automatic creation of
domain-specific bilingual lexicons. Such resources
play a vital role in Natural Language Processing
(NLP) applications that involve different languages.
At first, research on lexical extraction has relied on
the use of parallel corpora (Och and Ney, 2003).
The scarcity of such corpora, in particular for spe-
cialized domains and for language pairs not involv-
ing English, pushed researchers to investigate the
use of comparable corpora (Fung, 1998; Chiao and
Zweigenbaum, 2003). These corpora include texts
which are not exact translation of each other but
share common features such as domain, genre, sam-
pling period, etc.
The basic intuition that underlies bilingual lexi-
con creation is the distributional hypothesis (Harris,
1954) which puts that words with similar meanings
occur in similar contexts. In a multilingual formu-
lation, this hypothesis states that the translations of
a word are likely to appear in similar lexical envi-
ronments across languages (Rapp, 1995). The stan-
dard approach to bilingual lexicon extraction builds
1http://translate.google.com/
2http://www.systransoft.com/
479
on the distributional hypothesis and compares con-
text vectors for each word of the source and tar-
get languages. In this approach, the comparison of
context vectors is conditioned by the existence of a
seed bilingual dictionary. A weakness of the method
is that poor results are obtained for language pairs
that are not closely related (Ismail and Manandhar,
2010). Another important problem occurs whenever
the size of the seed dictionary is small due to ignor-
ing many context words. Conversely, when dictio-
naries are detailed, ambiguity becomes an important
drawback.
We introduce a bilingual lexicon extraction ap-
proach that exploits Wikipedia in an innovative
manner in order to tackle some of the problems
mentioned above. Important advantages of using
Wikipedia are:
? The resource is available in hundreds of lan-
guages and it is structured as unambiguous con-
cepts (i.e. articles).
? The languages are explicitly linked through
concept translations proposed by Wikipedia
contributors.
? It covers a large number of domains and is thus
potentially useful in order to mine a wide array
of specialized lexicons.
Mirroring the advantages, there are a number of
challenges associated with the use of Wikipedia:
? The comparability of concept descriptions in
different languages is highly variable.
? The translation graph is partial since, when
considering any language pair, only a part of
the concepts are available in both languages
and explicitly connected.
? Domains are unequally covered in Wikipedia
(Halavais and Lackaff, 2008) and efficient do-
main targeting is needed.
The approach introduced in this paper aims to
draw on Wikipedia?s advantages while appropri-
ately addressing associated challenges. Among
the techniques devised to mine Wikipedia content,
we hypothesize that an adequate adaptation of Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007) is fitted to our application con-
text. ESA was already successfully tested in differ-
ent NLP tasks, such as word relatedness estimation
or text classification, and we modify it to mine spe-
cialized domains, to characterize these domains and
to link them across languages.
The evaluation of the newly introduced approach
is realized on four diversified specialized domains
(Breast Cancer, Corporate Finance, Wind Energy
and Mobile Technology) and for two pairs of lan-
guages: French-English and Romanian-English.
This choice allows us to study the behavior of dif-
ferent approaches for a pair of languages that are
richly represented and for a pair that includes Roma-
nian, a language that has fewer associated resources
than French and English. Experimental results show
that the newly introduced approach outperforms the
three state of the art methods that were implemented
for comparison.
2 Related Work
In this section, we first give a review of the stan-
dard approach and then introduce methods that build
upon it. Finally, we discuss works that rely on Ex-
plicit Semantic Analysis to solve other NLP tasks.
2.1 Standard Approach (SA)
Most previous approaches that address bilingual lex-
icon extraction from comparable corpora are based
on the standard approach (Fung, 1998; Chiao and
Zweigenbaum, 2002; Laroche and Langlais, 2010).
This approach is composed of three main steps:
1. Building context vectors: Vectors are first
extracted by identifying the words that ap-
pear around the term to be translated Wcand
in a window of n words. Generally, asso-
ciation measures such as the mutual infor-
mation (Morin and Daille, 2006), the log-
likelihood (Morin and Prochasson, 2011) or the
Discounted Odds-Ratio (Laroche and Langlais,
2010) are employed to shape the context vec-
tors.
2. Translation of context vectors: To enable the
comparison of source and target vectors, source
vectors are translated intoto the target language
by using a seed bilingual dictionary. When-
ever several translations of a context word exist,
480
all translation variants are taken into account.
Words not included in the seed dictionary are
simply ignored.
3. Comparison of source and target vectors:
Given Wcand, its automatically translated con-
text vector is compared to the context vectors
of all possible translations from the target lan-
guage. Most often, the cosine similarity is
used to rank translation candidates but alterna-
tive metrics, including the weighted Jaccard in-
dex (Prochasson et al, 2009) and the city-block
distance (Rapp, 1999), were studied.
2.2 Improvements of the Standard Approach
Most of the improvements of the standard approach
are based on the observation that the more repre-
sentative the context vectors of a candidate word
are, the better the bilingual lexicon extraction is. At
first, additional linguistic resources, such as special-
ized dictionaries (Chiao and Zweigenbaum, 2002) or
transliterated words (Prochasson et al, 2009), were
combined with the seed dictionary to translate con-
text vectors.
The ambiguities that appear in the seed bilingual
dictionary were taken into account more recently.
(Morin and Prochasson, 2011) modify the standard
approach by weighting the different translations ac-
cording to their frequency in the target corpus. In
(Bouamor et al, 2013), we proposed a method that
adds a word sense disambiguation process relying
on semantic similarity measurement from WordNet
to the standard approach. Given a context vector in
the source language, the most probable translation of
polysemous words is identified and used for build-
ing the corresponding vector in the target language.
The most probable translation is identified using the
monosemic words that appear in the same lexical en-
vironment.
On specialized French-English comparable cor-
pora, this approach outperforms the one proposed in
(Morin and Prochasson, 2011), which is itself bet-
ter than the standard approach. The main weakness
of (Bouamor et al, 2013) is that the approach relies
on WordNet and its application depends on the ex-
istence of this resource in the target language. Also,
the method is highly dependent on the coverage of
the seed bilingual dictionary.
2.3 Explicit Semantic Analysis
Explicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007) is a method that maps textual
documents onto a structured semantic space using
classical text indexing schemes such as TF-IDF. Ex-
amples of semantic spaces used include Wikipedia
or the Open Directory Project but, due to superior
performances, Wikipedia is most frequently used.
In the original evaluation, ESA outperformed state
of the art methods in a word relatedness estimation
task.
Subsequently, ESA was successfully exploited in
other NLP tasks and in information retrieval. Radin-
sky and al. (2011) added a temporal dimension to
word vectors and showed that this addition improves
the results of word relatedness estimation. (Hassan
and Mihalcea, 2011) introduced Salient Semantic
Analysis (SSA), a development of ESA that relies
on the detection of salient concepts prior to map-
ping words to concepts. SSA and the original ESA
implementation were tested on several word related-
ness datasets and results were mixed. Improvements
were obtained for text classification when compar-
ing SSA with the authors? in-house representation
of the method. ESA has weak language depen-
dence and was already deployed in multilingual con-
texts. (Sorg and Cimiano, 2012) extended ESA to
other languages and showed that it is useful in cross-
lingual and multilingual retrieval task. Their focus
was on creating a language independent conceptual
space in which documents would be mapped and
then retrieved.
Some open ESA topics related to bilingual lex-
icon creation include: (1) the document represen-
tation which is simply done by summing individ-
ual contributions of words, (2) the adaptation of the
method to specific domains and (3) the coverage of
the underlying resource in different language.
3 ESA for Bilingual Lexicon Extraction
The main objective of our approach is to devise lex-
icon translation methods that are easily applicable
to a large number of language pairs, while preserv-
ing the overall quality of results. A subordinated
objective is to exploit large scale background mul-
tilingual knowledge, such as the encyclopedic con-
tent available in Wikipedia. As we mentioned, ESA
481
Figure 1: Overview of the Explicit Semantic
Analysis enabled bilingual lexicon extraction.
(Gabrilovich and Markovitch, 2007) was exploited
in a number of NLP tasks but not in bilingual lexi-
con extraction.
Figure 1 shows the overall architecture of the lex-
ical extraction process we propose. The process is
completed in the following three steps:
1. Given a word to be translated and its con-
text vector in the source language, we derive
a ranked list of similar Wikipedia concepts (i.e.
articles) using the ESA inverted index.
2. Then, a translation graph is used to retrieve the
corresponding concepts in the target language.
3. Candidate translations are found through a sta-
tistical processing of concept descriptions from
the ESA direct index in the target language.
In this section, we first introduce the elements of
the original formulation of ESA necessary in our ap-
proach. Then, we detail the three steps that com-
pose the main bilingual lexicon extraction method
illustrated in Figure 1. Finally, as a complement to
the main method we introduce a measure for domain
word specificity and present a method for extracting
generic translation lexicons.
3.1 ESA Word and Concept Representation
Given a semantic space structured using a set of M
concepts and including a dictionary of N words,
a mapping between words and concepts can be
expressed as the following matrix:
w(W1, C1) w(W2, C1) ... w(WN , C1)
w(W1, C2) w(W2, C2) ... w(WN , C2)
... ... ...
w(W1, CM ) w(W2, CM ) ... w(WN , CM )
When Wikipedia is exploited concepts are
equated to Wikipedia articles and the texts of the ar-
ticles are processed in order to obtain the weights
that link words and concepts. In (Gabrilovich and
Markovitch, 2007), the weights w that link words
and concepts were obtained through a classical TF-
IDF weighting of Wikipedia articles. A series of
tweaks destined to improve the method?s perfor-
mance were used and disclosed later3. For instance,
administration articles, lists, articles that are too
short or have too few links are discarded. Higher
weight is given to words in the article title and
more longer articles are favored over shorter ones.
We implemented a part of these tweaks and tested
our own version of ESA with the Wikipedia ver-
sion used in the original implementation. The cor-
relation with human judgments of word relatedness
was 0.72 against 0.75 reported by (Gabrilovich and
Markovitch, 2007). The ESA matrix is sparse since
the N size of the dictionary, is usually in the range
of hundreds of thousands and each concept is usu-
ally described by hundreds of distinct words. The
direct ESA index from Figure 1 is obtained by read-
ing the matrix horizontally while the inverted ESA
index is obtained by reading the matrix vertically.
3https://github.com/faraday/
wikiprep-esa/wiki/roadmap
482
Terme Concepts
action e?valuation d?action, communisme, actionnaire activiste, socialisme,
de?velopement durable . . .
de?ficit crise de la dette dans la zone euro, dette publique, re`gle d?or budge?taire,
de?ficit, trouble du de?ficit de l?attention . . .
cisaillement taux de cisaillement, zone de cisaillement, cisaillement, contrainte de cisaille-
ment, viscoanalyseur . . .
turbine ffc turbine potsdam, turbine a` gaz, turbine, urbine hydraulique, coge?ne?ration
. . .
cryptage TEMPEST, chiffrement, liaison 16, Windows Vista, transfert de fichiers . . .
protocole Ad-hoc On-demand Distance Vector, protocole de Kyoto, optimized link state
routing protocol, liaison 16, IPv6 . . .
biopsie biopsie, maladie de Horton, cancer du sein, cancer du poumon, imagerie par
re?sonance magne?tique . . .
palpation cancer du sein, cellulite, examen clinique, appendicite, te?nosynovite . . .
Table 1: The five most similar Wikipedia concepts to the French terms action[share], de?ficit[deficit], ci-
saillement[shear], turbine[turbine], cryptage[encryption], biopsie[biopsie] and palpation[palpation] and
their context vectors.
3.2 Source Language Processing
The objective of the source language processing is
to obtain a ranked list of similar Wikipedia concepts
for each candidate word (Wcand) in a specialized do-
main. To do this, a context vector is first built for
each Wcand from a specialized monolingual corpus.
The association measure between Wcand and context
words is obtained using the Odds-Ratio (defined in
equation 5). Wikipedia concepts in the source lan-
guage Cs that are similar to Wcand and to a part of its
context words are extracted and ranked using equa-
tion 1.
Rank(Cs) = (10 ?max(Odds
Wcand
Wsi
)
?w(Wcand, Cs)) +
n?
i=1
OddsWcandWsi
?w(Wsi , Cs)
(1)
where max(OddsWcandWsi
) is the highest Odds-Ratio
association between Wcand and any of its context
words Wsi ; the factor 10 was empirically set to
give more importance to Wcand over context words;
w(Wcand, Cs) is the weight of the association be-
tween Wcand and Cs from the ESA matrix; n is the
total number of words Wsi in the context vector of
Wcand; Odds
Wcand
Wsi
is the association value between
Wcand and Wsi and w(Wsi , Cs) are the weights of
the associations between each context word Wsi and
Cs from the ESA matrix. The use of contextual in-
formation in equation 1 serves to characterize the
candidate word in the target domain.
In table 1, we present the five most similar
Wikipedia concepts to the French terms action,
de?ficit, cisaillement, turbine, cryptage, biopsie and
palpation and their context vectors. These terms are
part of the four specialized domains we are studying
here. From observing these examples, we note that
despite the difference between the specialized do-
mains and word ambiguity (words action and proto-
cole), our method has the advantage of successfully
representing each word to be translated by relevant
conceptual spaces.
3.3 Translation Graph Construction
To bridge the gap between the source and target lan-
guages, a concept translation graph that enables the
multilingual extension of ESA is used. This con-
cept translation graph is extracted from the explicit
translation links available in Wikipedia articles and
is exploited in order to connect a word?s conceptual
space in the source language with the correspond-
ing conceptual space in the target language. Only a
part of the articles have translations and the size of
483
the conceptual space in the target language is usu-
ally smaller than the space in the source language.
For instance, the French-English translation graph
contains 940,215 pairs of concepts while the French
and English Wikipedias contain approximately 1.4
million articles, respectively 4.25 million articles.
3.4 Target Language Processing
The third step of the approach takes place in the tar-
get language. Using the translation graph, we select
the 100 most similar concept translations (thresh-
old determined empirically after preliminary exper-
iments) from the target language and use their di-
rect ESA representations in order to retrieve poten-
tial translations for the candidate word Wcand from
source language. These candidate translations Wt
are ranked using equation 2.
Rank(Wt) = (
n?
i=1
w(Wt, Cti)
avg(Cti)
)
? log(count(Wt,S)) (2)
with w(Wt, Cti) is the weight of the translation can-
didate WT for concept Cti from the ESA matrix
in the target language; avg(Cti) is the average TF-
IDF score of words that appear in Cti ; S is the set
of similar concepts Cti in the target language and
count(Wt,S) accounts for the number of different
concepts from S in which the candidate translation
WT appears.
The accumulation of weights w(Wt, Cti) fol-
lows the way original ESA text representations
are calculated (Gabrilovich and Markovitch, 2007)
and avg(Cti) is used in order to correct the
bias of the TF-IDF scheme towards short articles.
log(count(Wt,S)) is used to favor words that are
associated with a larger number of concepts. log
weighting was chosen after preliminary experiments
with a wide range of functions.
3.5 Domain Specificity
In previous works, ESA was usually exploited
in generic tasks that did not require any domain
adaptation. Here we process information from
specific domains and we need to measure the
specificity of words in those domains. The domain
extraction is seeded by using Wikipedia concepts
(noted Cseed) that best describes the domain in
the target language. For instance, in English,
the Corporate Finance domain is seeded with
https://en.wikipedia.org/wiki/Corporate finance.
We extract a set of 10 words with the highest
TF-IDF score from this article (noted SW ) and use
them to retrieve a domain ranking of concepts in the
target language Rankdom(Ct) with equation 3.
Rankdom(Ct) = (
n?
i=1
w(Wti , Ct)
? w(Cseed,Wti)) ? count(SW,Ct) (3)
where n is size of the seed list of words (i.e. 10
items), w(Wti , Ct) is the weight of the domain
words in the concept Ct ; w(Cseed,Wti) is the
weight of Wti in Cseed, the seed concept of the do-
main, and count(SW,Ct) is the number of distinct
seed words from SW that appear in Ct.
The first part of equation 3 sums up the contribu-
tions of different words from SW that appear in Ct
while the second part is meant to further reinforce
articles that contain a larger number of domain key-
words from SW .
Domain delimitation is performed by retaining
articles whose Rankdom(Ct) is at least 1% or the
score of the top Rankdom(Ct) score. This threshold
was set up during preliminary experiments. Given
the delimitation obtained with equation 3, we calcu-
late a domain specificity score (specifdom(Wt)) for
each word that occurs in the domain ( equation 4).
specifdom(Wt) estimates how much of a word?s use
in an underlying corpus is related to a target domain.
specifdom(Wt) =
DFdom(Wt)
DFgen(Wt)
(4)
where DFdom and DFgen stand for the domain and
the generic document frequency of the word Wt.
specifdom(Wt) will be used to favor words with
greater domain specificity over more general ones
when several translations are available in a seed
generic translation lexicon. For instance, the French
word action is ambiguous and has English transla-
tions such as action, stock, share etc. In a general
case, the most frequent translation is action whereas
in a corporate finance context, share or stock are
more relevant. The specificity of the three transla-
tions, from highest to lowest, is: share, stock and ac-
tion and is used to rank these potential translations.
484
3.6 Generic Dictionaries
Generic translation dictionaries, already used by ex-
isting bilingual lexicon extraction approaches, can
also be integrated in the newly proposed approach.
The Wikipedia translation graph is transformed into
a translation dictionary by removing the disam-
biguation marks from ambiguous concept titles, as
well as lists, categories and other administration
pages. Moreover, since the approach does not han-
dle multiword units, we retain only translation pairs
that are composed of unigrams in both languages.
When existing, unigram redirections are also added
in each language.
The obtained dictionaries are incomplete since:
(1) Wikipedia focuses on concepts that are most of-
ten nouns, (2) specialized domain terms often do not
have an associated Wikipedia entry and (3) the trans-
lation graph covers only a fraction of the concepts
available in a language. For instance, the result-
ing translation dictionaries have 193,543 entries for
French-English and 136,681 entries for Romanian-
English. They can be used in addition to or instead
of other resources available and are especially useful
when there are only few other resources that link the
pair of languages processed.
4 Evaluation
The performances of our approach are evaluated
against the standard approach and its developments
proposed by (Morin and Prochasson, 2011) and
(Bouamor et al, 2013). In this section, we first
describe the data and resources we used in our ex-
periments. We then present differents parameters
needed in the implementation of the different meth-
ods tested. Finally, we discuss the obtained results.
4.1 Data and Resources
Comparable corpora
We conducted our experiments on four French-
English and Romanian-English specialized compa-
rable corpora: Corporate Finance, Breast Can-
cer, Wind Energy and Mobile Technology. For
the Romanian-English language pair, we used
Wikipedia to collect comparable corpora for all do-
mains since they were not already available. The
Wikipedia corpora are harvested using a category-
based selection. We consider the topic in the source
Domain FR EN
Corporate Finance 402,486 756,840
Breast Cancer 396,524 524,805
Wind Energy 145,019 345,607
Mobile Technology 197,689 144,168
Domain RO EN
Corporate Finance 206,169 524,805
Breast Cancer 22,539 322,507
Wind Energy 121,118 298,165
Mobile Technology 200,670 124,149
Table 2: Number of content words in the
comparable corpora.
language (for instance Cancer Mamar [Breast Can-
cer]) as a query to Wikipedia and extract all its sub-
topics (i.e., sub-categories) to construct a domain-
specific category tree. Then, based on the con-
structed tree, we collect all Wikipedia articles be-
longing to at least one of these categories and use
inter-language links to build the comparable cor-
pora.
Concerning the French-English pair, we followed
the strategy described above to extract the compa-
rable corpora related to the Corporate Finance and
Breast Cancer domains since they were otherwise
unavailable. For the two other domains, we used
the corpora released in the TTC project4. All cor-
pora were normalized through the following linguis-
tic preprocessing steps: tokenization, part-of-speech
tagging, lemmatization, and function word removal.
The resulting corpora5 sizes are presented in Table
2. The size of the domain corpora vary within and
across languages, with the corporate finance domain
being the richest in both languages. In Romanian,
Breast Cancer is particularly small, with approxi-
mately 22,000 tokens included. This variability will
allow us to test if there is a correlation between cor-
pus size and quality of results.
Bilingual dictionary
The seed generic French-English dictionary used
to translate French context vectors consists of an
in-house manually built resource which contains
approximately 120,000 entries. For Romanian-
4http://www.ttc-project.eu/index.php/
releases-publications
5Comparable corpora will be shared publicly
485
Domain FR-EN RO-EN
Corporate Finance 125 69
Breast Cancer 96 38
Wind Energy 89 38
Mobile Technology 142 94
Table 3: Sizes of the evaluation lists.
English, we used the generic dictionary extracted
following the procedure described in Subsection 3.6.
Gold standard
In bilingual terminology extraction from compara-
ble corpora, a reference list is required to evaluate
the performance of the alignment. Such lists are usu-
ally composed of around 100 single terms (Hazem
and Morin, 2012; Chiao and Zweigenbaum, 2002).
Reference lists6 were created for the four specialized
domains and the two pairs of languages. For the
French-English, reference words from the Corpo-
rate Finance domain were extracted from the glos-
sary of bilingual micro-finance terms7. For Breast
Cancer, the list is derived from the MESH and the
UMLS thesauri8. Concerning Wind Energy and Mo-
bile Technology, lists were extracted from special-
ized glossaries found on the Web. The Romanian-
English gold standard was manually created by a na-
tive speaker starting from the French-English lists.
Table 3 displays the sizes of the obtained lists. Ref-
erence terms pairs were retained if each word com-
posing them appeared at least five times in the com-
parable domain corpora.
4.2 Experimental setup
Aside from those already mentioned, three param-
eters need to be set up: (1) the window size that
defines contexts, (2) the association measure that
measures the strength of the association between
words and the (3) similarity measure that ranks can-
didate translations for state of the art methods. Con-
text vectors are defined using a seven-word window
which approximates syntactic dependencies. The
association and the similarity measures (Discounted
Log-Odds ratio (equation 5) and the cosine simi-
6Reference lists will be shared publicly
7http://www.microfinance.lu/en/
8http://www.nlm.nih.gov/
larity) were set following Laroche and Langlais
(2010), a comprehensive study of the influence of
these parameters on the bilingual alignment.
Odds-Ratiodisc = log
(O11 + 12 )(O22 +
1
2 )
(O12 + 12 )(O21 +
1
2 )
(5)
where Oij are the cells of the 2? 2 contingency ma-
trix of a token s co-occurring with the term S within
a given window size.
The F-measure of the Top 20 results (F-
Measure@20), which measures the harmonic mean
of precision and recall, is used as evaluation metric.
Precision is the total number of correct translations
divided by the number of terms for which the system
returned at least one answer. Recall is equal to the
ratio between the number of correct translation and
the total number of words to translate (Wcand).
4.3 Results and discussion
In addition to the basic approach based on ESA
(denoted ESA), we evaluate the performances of
a method so-called DicoSpec in which the transla-
tions are extracted from a generic dictionary and
a method we called ESASpec which combine ESA
and DicoSpec. DICOSpec is based on the generic
dictionary we presented in subsection 3.6 and pro-
ceeds as follows: we extract a list of translations for
each word to be translated from the generic dictio-
nary. The domain specificity introduced in subsec-
tion 3.5 is then used to rank these translations. For
instance, the french term port referring in the Mobile
Technology domain, to the system that allows com-
puters to receive and transmit information is trans-
lated into port and seaport. According to domain
specificity values, the following ranking is obtained:
the English term port obtain the highest specificity
value (0.48). seaport comes next with a specificity
value of 0.01. In ESASpec, the translations set out in
the translations lists proposed by both ESA and the
generic dictionary are weighted according to their
domain specificity values. The main intuition be-
hind this method is that by adding the information
about the domain specificity, we obtain a new rank-
ing of the bilingual extraction results.
The obtained results are displayed in table 4. The
comparison of state of the art method shows that
BA13 performs better than STAPP and MP11 for
French-English and has comparable performances
486
a)
F
R
-E
N
Method
F-Measure@20
Breast Cancer Corporate Finance Wind Eenrgy Mobile Technology
STAPP 0.49 0.17 0.08 0.06
MP11 0.55 0.33 0.24 0.05
BA13 0.61 0.37 0.30 0.24
Dicospec 0.50 0.20 0.36 0.25
ESA 0.74 0.50 0.83 0.72
ESAspec 0.81 0.56 0.86 0.75
b)
R
O
-E
N
Method
F-Measure@20
Breast Cancer Corporate Finance Wind Eenrgy Mobile Technology
STAPP 0.21 0.13 0.08 0.16
MP11 0.21 0.13 0.08 0.16
BA13 0.21 0.14 0.08 0.17
Dicospec 0.44 0.11 0.21 0.16
ESA 0.76 0.17 0.58 0.53
ESAspec 0.78 0.24 0.58 0.55
Table 4: Results of the specialized dictionary creation on four specific domains, two pairs of languages.Three
state of the art methods were used for comparison: STAPP is the standard approach, MP11 is the improve-
ment of the standard approach introduced in (Morin and Prochasson, 2011), BA13 is a recent method that
we developed (Bouamor et al, 2013). Dicospec exploits a generic dictionary, combined with the use of do-
main specificity (see Subsection 3.5). ESA stands for the ESA based approach introduced in this paper (see
Figure 1). ESAspec combines the results of Dicospec and ESA.
for RO-EN. Consequently, we will use BA13 as the
main baseline for discussing the newly introduced
approach. The results presented in Table 4 show
that ESAspec clearly outperforms the three base-
lines for the four domains and the two pairs of lan-
guages tested. When comparing ESAspec to BA13
for French-English, improvements range between
0.19 for Corporate Finance and 0.56 for Wind En-
ergy. For RO-EN, the improvements vary from 0.1
for Corporate Finance to 0.5 for Wind Energy. Also,
except for the Corporate Finance domain in Roma-
nian, the performance variation across domains is
much smaller for ESAspec than for the three state
of the art methods. This shows that ESAspec is more
robust to domain change and thus more generic.
The results obtained with ESA are signifi-
cantly better than those obtained with Dicospec and
ESAspec, their combination, further improves the
results. The main contribution to ESAspec perfor-
mances comes from ESA, a finding that validates
our assumption that the adequate use of a rich multi-
lingual resource such as Wikipedia is appropriate for
specialized lexicon translation. Dicospec is a sim-
ple method that ranks the different meanings of a
candidate word available in a generic dictionary but
its average performances are comparable to those
of BA13 for FR-EN and higher for RO-EN. This
finding advocates for the importance of good qual-
ity generic dictionaries in specialized lexicon trans-
lation approaches. However, it is clear that such
dictionaries are far from being sufficient in order
to cover all possible domains. There is no clear
correlation between domain size and quality of re-
sults. Although richer than the other three domains,
Corporate Finance has the lowest associated per-
formances. This finding is probably explained by
the intrinsic difficulty of each domain. When pass-
ing from FR-EN to RO-EN the average performance
drop is more significant for BA13 than for the ESA
based methods. The result indicates that our ap-
proach is more robust to language change.
5 Conclusion
We have presented a new approach to the creation
of specialized bilingual lexicons, one of the central
487
building blocks of machine translation systems. The
proposed approach directly tackles two of the ma-
jor challenges identified in the Introduction. The
scarcity of resources is addressed by an adequate
exploitation of Wikipedia, a resource that is avail-
able in hundreds of languages. The quality of auto-
matic translations was improved by appropriate do-
main delimitation and linking across languages, as
well as by an adequate statistical processing of con-
cepts similar to a word in a given context.
The main advantages of our approach compared
to state of the art methods come from: the increased
number of languages that can be processed, from
the smaller sensitivity to structured resources and
the appropriate domain delimitation. Experimental
validation is obtained through evaluation with four
different domains and two pairs of languages which
shows consistent performance improvement. For
French-English, two languages that have rich asso-
ciated Wikipedia representations, performances are
very interesting and are starting to approach those of
manual translations for three domains out of four (F-
Measure@20 around 0.8). For Romanian-English, a
pair involving a language with a sparser Wikipedia
representation, the performances of our method drop
compared to French-English . However, they do not
decrease to the same extent as those of the best state
of the art method tested. This finding indicates that
our approach is more general and, given its low lan-
guage dependence, it can be easily extended to a
large number of language pairs.
The results presented here are very encouraging
and we will to pursue work in several directions.
First, we will pursue the integration of our method,
notably through comparable corpora creation using
the data driven domain delimitation technique de-
scribed in Subsection 3.5. Equally important, the
size of the domain can be adapted so as to find
enough context for all the words in domain reference
lists. Second, given a word in a context, we currently
exploit all similar concepts from the target language.
Given that comparability of article versions in the
source and the target language varies, we will eval-
uate algorithms for filtering out concepts from the
target language that have low alignment with their
source language versions. A final line of work is
constituted by the use of distributional properties of
texts in order to automatically rank parts of concept
descriptions (i.e. articles) by their relatedness to the
candidate word. Similar to the second direction, this
process involves finding comparable text blocks but
rather at a paragraph or sentence level than at the
article level.
References
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2013. Context vector disambiguation
for bilingual lexicon extraction. In Proceedings of the
51st Association for Computational Linguistics (ACL-
HLT), Sofia, Bulgaria, August.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th international conference on Computational lin-
guistics - Volume 2, COLING ?02, pages 1?5. Associ-
ation for Computational Linguistics.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The
effect of a general lexicon in corpus-based identifi-
cation of french-english medical word translations.
In Proceedings Medical Informatics Europe, volume
95 of Studies in Health Technology and Informatics,
pages 397?402, Amsterdam.
Pascale Fung. 1998. A statistical view on bilingual lexi-
con extraction: From parallel corpora to non-parallel
corpora. In Parallel Text Processing, pages 1?17.
Springer.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Alexander Halavais and Derek Lackaff. 2008. An Anal-
ysis of Topical Coverage of Wikipedia. Journal of
Computer-Mediated Communication, 13(2):429?440.
Z.S. Harris. 1954. Distributional structure. Word.
Samer Hassan and Rada Mihalcea. 2011. Semantic re-
latedness using salient semantic analysis. In AAAI.
Amir Hazem and Emmanuel Morin. 2012. Adaptive dic-
tionary for bilingual lexicon extraction from compara-
ble corpora. In Proceedings, 8th international confer-
ence on Language Resources and Evaluation (LREC),
Istanbul, Turkey, May.
Azniah Ismail and Suresh Manandhar. 2010. Bilin-
gual lexicon extraction from comparable corpora us-
ing in-domain terms. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Posters, COLING ?10, pages 481?489. Association for
Computational Linguistics.
488
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 617?625, Beijing, China, Aug.
Emmanuel Morin and Be?atrice Daille. 2006. Compara-
bilite? de corpus et fouille terminologique multilingue.
In Traitement Automatique des Langues (TAL).
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable corpora
enhanced with parallel corpora. In Proceedings, 4th
Workshop on Building and Using Comparable Cor-
pora (BUCC), page 27?34, Portland, Oregon, USA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19?51, March.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Translation
Summit (MT Summit XII), page 284?291, Ottawa, On-
tario, Canada.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,
and Shaul Markovitch. 2011. A word at a time: com-
puting word relatedness using temporal semantic anal-
ysis. In Proceedings of the 20th international confer-
ence on World wide web, WWW ?11, pages 337?346,
New York, NY, USA. ACM.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
ACL ?95, pages 320?322. Association for Computa-
tional Linguistics.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Compu-
tational Linguistics, ACL ?99, pages 519?526. Asso-
ciation for Computational Linguistics.
P. Sorg and P. Cimiano. 2012. Exploiting wikipedia for
cross-lingual and multilingual information retrieval.
Data Knowl. Eng., 74:26?45, April.
489
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759?764,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Context Vector Disambiguation for Bilingual Lexicon Extraction from
Comparable Corpora
Dhouha Bouamor
CEA, LIST, Vision and
Content Engineering Laboratory,
91191 Gif-sur-Yvette CEDEX
France
dhouha.bouamor@cea.fr
Nasredine Semmar
CEA, LIST, Vision and Content
Engineering Laboratory,
91191 Gif-sur-Yvette
CEDEX France
nasredine.semmar@cea.fr
Pierre Zweigenbaum
LIMSI-CNRS,
F-91403 Orsay CEDEX
France
pz@limsi.fr
Abstract
This paper presents an approach that ex-
tends the standard approach used for bilin-
gual lexicon extraction from comparable
corpora. We focus on the unresolved prob-
lem of polysemous words revealed by the
bilingual dictionary and introduce a use of
a Word Sense Disambiguation process that
aims at improving the adequacy of con-
text vectors. On two specialized French-
English comparable corpora, empirical ex-
perimental results show that our method
improves the results obtained by two state-
of-the-art approaches.
1 Introduction
Over the years, bilingual lexicon extraction from
comparable corpora has attracted a wealth of re-
search works (Fung, 1998; Rapp, 1995; Chiao
and Zweigenbaum, 2003). The basic assumption
behind most studies is a distributional hypothe-
sis (Harris, 1954), which states that words with a
similar meaning are likely to appear in similar con-
texts across languages. The so-called standard ap-
proach to bilingual lexicon extraction from com-
parable corpora is based on the characterization
and comparison of context vectors of source and
target words. Each element in the context vector
of a source or target word represents its associa-
tion with a word which occurs within a window
of N words. To enable the comparison of source
and target vectors, words in the source vectors are
translated into the target language using an exist-
ing bilingual dictionary.
The core of the standard approach is the bilin-
gual dictionary. Its use is problematic when a word
has several translations, whether they are synony-
mous or polysemous. For instance, the French
word action can be translated into English as
share, stock, lawsuit or deed. In such cases, it
is difficult to identify in flat resources like bilin-
gual dictionaries which translations are most rel-
evant. The standard approach considers all avail-
able translations and gives them the same impor-
tance in the resulting translated context vectors in-
dependently of the domain of interest and word
ambiguity. Thus, in the financial domain, trans-
lating action into deed or lawsuit would introduce
noise in context vectors.
In this paper, we present a novel approach that
addresses the word polysemy problem neglected
in the standard approach. We introduce a Word
Sense Disambiguation (WSD) process that iden-
tifies the translations of polysemous words that
are more likely to give the best representation of
context vectors in the target language. For this
purpose, we employ five WordNet-based semantic
similarity and relatedness measures and use a data
fusion method that merges the results obtained by
each measure. We test our approach on two spe-
cialized French-English comparable corpora (fi-
nancial and medical) and report improved results
compared to two state-of-the-art approaches.
2 Related Work
Most previous works addressing the task of bilin-
gual lexicon extraction from comparable corpora
are based on the standard approach. In order to
improve the results of this approach, recent re-
searches based on the assumption that more the
context vectors are representative, better is the
bilingual lexicon extraction were conducted. In
these works, additional linguistic resources such
as specialized dictionaries (Chiao and Zweigen-
baum, 2002) or transliterated words (Prochasson
et al, 2009) were combined with the bilingual dic-
759
tionary to translate context vectors. Few works
have however focused on the ambiguity problem
revealed by the seed bilingual dictionary. (Hazem
and Morin, 2012) propose a method that filters the
entries of the bilingual dictionary on the base of
a POS-Tagging and a domain relevance measure
criteria but no improvements have been demon-
strated. Gaussier et al (2004) attempted to solve
the problem of word ambiguities in the source and
target languages. They investigated a number of
techniques including canonical correlation analy-
sis and multilingual probabilistic latent semantic
analysis. The best results, with an improvement of
the F-Measure (+0.02 at Top20) were reported for
a mixed method. Recently, (Morin and Prochas-
son, 2011) proceed as the standard approach but
weigh the different translations according to their
frequency in the target corpus. Here, we propose a
method that differs from Gaussier et al (2004) in
this way: If they focus on words ambiguities on
source and target languages, we thought that it
would be sufficient to disambiguate only trans-
lated source context vectors.
3 Context Vector Disambiguation
3.1 Semantic similarity measures
A large number of WSD techniques were pro-
posed in the literature. The most widely used ones
are those that compute semantic similarity1 with
the help of WordNet. WordNet has been used in
many tasks relying on word-based similarity, in-
cluding document (Hwang et al, 2011) and im-
age (Cho et al, 2007; Choi et al, 2012) retrieval
systems. In this work, we use it to derive a se-
mantic similarity between lexical units within the
same context vector. To the best of our knowledge,
this is the first application of WordNet to bilingual
lexicon extraction from comparable corpora.
Among semantic similarity measures using
WordNet, we distinguish: (1) measures based on
path length which simply counts the distance be-
tween two words in the WordNet taxonomy, (2)
measures relying on information content in which
a semantically annotated corpus is needed to com-
pute frequencies of words to be compared and (3)
the ones using gloss overlap which are designed
to compute semantic relatedness. In this work,
we use five similarity measures and compare
their performances. These measures include three
1For consiseness, we often use ?semantic similarity? to
refer collectively to both similarity and relatedness.
path-based semantic similarity measures denoted
PATH,WUP (Wu and Palmer, 1994) and LEA-
COCK (Leacock and Chodorow, 1998). PATH is
a baseline that is equal to the inverse of the short-
est path between two words. WUP finds the depth
of the least common subsumer of the words, and
scales that by the sum of the depths of individual
words. The depth of a word is its distance to the
root node. LEACOCK finds the shortest path be-
tween two words, and scales that by the maximum
path length found in the is?a hierarchy in which
they occur. Path length measures have the advan-
tage of being independent of corpus statistics, and
therefor uninfluenced by sparse data.
Since semantic relatedness is considered to be
more general than semantic similarity, we also
use two relatedness measures: LESK (Banerjee
and Pedersen, 2002) and VECTOR (Patwardhan,
2003). LESK finds overlaps between the glosses
of word pairs, as well as words? hyponyms. VEC-
TOR creates a co-occurrence matrix for each gloss
token. Each gloss is then represented as a vector
that averages token co-occurrences.
3.2 Disambiguation process
Once translated into the target language, the con-
text vectors disambiguation process intervenes.
This process operates locally on each context vec-
tor and aims at finding the most prominent trans-
lations of polysemous words. For this purpose,
we use monosemic words as a seed set of dis-
ambiguated words to infer the polysemous word?s
translations senses. We hypothesize that a word is
monosemic if it is associated to only one entry in
the bilingual dictionary. We checked this assump-
tion by probing monosemic entries of the bilingual
dictionary against WordNet and found that 95% of
the entries are monosemic in both resources. Ac-
cording to the above-described semantic similarity
measures, a similarity value SimV alue is derived
between all the translations provided for each pol-
ysemous word by the bilingual dictionary and all
monosemic words appearing within the same con-
text vector. In practice, since a word can belong to
more than one synset2 in WordNet, the semantic
similarity between two words w1 and w2 is defined
as the maximum of SimV alue between the synset
or the synsets that include the synsets(w1) and
2a group of a synonymous words in WordNet
760
synsets(w2) according to the following equation:
SemSim(w1, w2) = max{SimV alue(s1, s2);
(s1, s2) ? synsets(w1)? synsets(w2)} (1)
Then, to identify the most prominent transla-
tions of each polysemous unit wp, an average sim-
ilarity is computed for each translation wjp of wp:
Ave Sim(wjp) =
1
N
NX
i=1
SemSim(wi, wjp) (2)
where N is the total number of monosemic words
in each context vector and SemSim is the simi-
larity value of wjp and the ith monosemic word.
Hence, according to average similarity values
Ave Sim(wjp), we obtain for each polysemous
word wp an ordered list of translations w1p . . . wnp .
4 Experiments and Results
4.1 Resources and Experimental Setup
We conducted our experiments on two French-
English comparable corpora specialized on the
corporate finance and the breast cancer sub-
domains. Both corpora were extracted from
Wikipedia3. We consider the domain topic in
the source language (for instance cancer du sein
[breast cancer]) as a query to Wikipedia and
extract all its sub-topics (i.e., sub-categories in
Wikipedia) to construct a domain-specific cate-
gories tree. Then we collected all articles belong-
ing to one of these categories and used inter-
language links to build the comparable corpus.
Both corpora have been normalized through the
following linguistic preprocessing steps: tokeni-
sation, part-of-speech tagging, lemmatisation and
function words removal. The resulting corpora4
sizes as well as their polysemy rate PR are given
in Table 1. The polysemy rate indicates how much
words in the comparable corpora are associated
to more than one translation in the seed bilingual
dictionary. The dictionary consists of an in-house
bilingual dictionary which contains about 120,000
entries belonging to the general language with an
average of 7 translations per entry.
In bilingual terminology extraction from com-
parable corpora, a reference list is required to
evaluate the performance of the alignment. Such
lists are often composed of about 100 single
3http://dumps.wikimedia.org/
4Comparable corpora will be shared publicly
Corpus French English PR
Corporate finance 402.486 756.840 41%
Breast cancer 396.524 524.805 47%
Table 1: Comparable corpora sizes in term of
words and polysemy rates (PR) associated to each
corpus
terms (Hazem and Morin, 2012; Chiao and
Zweigenbaum, 2002). Here, we created two ref-
erence lists5 for the corporate finance and the
breast cancer sub-domains. The first list is com-
posed of 125 single terms extracted from the glos-
sary of bilingual micro-finance terms6. The second
list contains 79 terms extracted from the French-
English MESH and the UMLS thesauri7. Note
that reference terms pairs appear more than five
times in each part of both comparable corpora.
Three other parameters need to be set up,
namely the window size, the association measure
and the similarity measure. We followed (Laroche
and Langlais, 2010) to define these parame-
ters. They carried out a complete study of the
influence of these parameters on the bilingual
alignment. The context vectors were defined by
computing the Discounted Log-Odds Ratio (equa-
tion 3) between words occurring in the same con-
text window of size 7.
Odds-Ratiodisc = log (O11 +
1
2 )(O22 + 12 )
(O12 + 12 )(O21 + 12 )
(3)
where Oij are the cells of the 2 ? 2 contingency
matrix of a token s co-occurring with the term S
within a given window size. As similarity mea-
sure, we chose to use the cosine measure.
4.2 Results of bilingual lexicon extraction
To evaluate the performance of our approach, we
used both the standard approach (SA) and the ap-
proach proposed by (Morin and Prochasson, 2011)
(henceforth MP11) as baselines. The experiments
were performed with respect to the five semantic
similarity measures described in section 3.1. Each
measure provides, for each polysemous word, a
ranked list of translations. A question that arises
here is whether we should introduce only the top-
ranked translation into the context vector or con-
sider a larger number of translations, mainly when
a translation list contains synonyms. For this
5Reference lists will be shared publicly
6http://www.microfinance.lu/en/
7http://www.nlm.nih.gov/
761
a)
Co
rpo
rat
eF
ina
nc
e Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7Standard Approach (SA) 0.172
MP11 0.336
Sin
gle
me
asu
re
WUP 0.241 0.284 0.301 0.275 0.258 0.215 0.224
PATH 0.250 0.284 0.301 0.284 0.258 0.215 0.215
LEACOCK 0.250 0.293 0.301 0.275 0.275 0.241 0.232
LESK 0.272 0.293 0.293 0.275 0.258 0.250 0.215
VECTOR 0.267 0.310 0.284 0.284 0.232 0.232 0.232
CONDORCETMerge 0.362 0.379 0.353 0.362 0.336 0.275 0.267
b)
Br
eas
tC
an
cer
Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7
Standard Approach (SA) 0.493
MP11 0.553
Sin
gle
me
asu
re
WUP 0.481 0.566 0.566 0.542 0.554 0.542 0.554
PATH 0.542 0.542 0.554 0.566 0.578 0.554 0.554
LEACOCK 0.506 0.578 0.554 0.566 0.542 0.554 0.542
LESK 0.469 0.542 0.542 0.590 0.554 0.554 0.542
VECTOR 0.518 0.566 0.530 0.566 0.542 0.566 0.554
CONDORCETMerge 0.566 0.614 0.600 0.590 0.600 0.578 0.578
Table 2: F-Measure at Top20 for the two domains; MP11 = (Morin and Prochasson, 2011). In each
column, italics shows best single similarity measure, bold shows best result. Underline shows best result
overall.
reason, we take into account in our experiments
different numbers of translations, noted WN-Ti,
ranging from the pivot translation (i = 1) to the
seventh word in the translation list. This choice is
motivated by the fact that words in both corpora
have on average 7 translations in the bilingual dic-
tionary. Both baseline systems use all translations
associated to each entry in the bilingual dictionary.
The only difference is that in MP11 translations
are weighted according to their frequency in the
target corpus.
The results of different works focusing on bilin-
gual lexicon extraction from comparable corpora
are evaluated on the number of correct candidates
found in the first N first candidates output by the
alignment process (the TopN ). Here, we use the
Top20 F-measure as evaluation metric. The results
obtained for the corporate finance corpus are pre-
sented in Table 2a. The first notable observation is
that disambiguating context vectors using seman-
tic similarity measures outperforms the SA. The
highest F-measure is reported by VECTOR. Us-
ing the top two words (WN-T2) in context vec-
tors increases the F-measure from 0.172 to 0.310.
However, compared to MP11, no improvement
is achieved. Concerning the breast cancer cor-
pus, Table 2b shows improvements in most cases
over both the SA and MP11. The maximum F-
measure was obtained by LESK when for each
polysemous word up to four translations (WN-T4)
are considered in context vectors. This method
achieves an improvement of respectively +0.097
and +0.037% over SA and MP11.
Each of the tested 5 semantic similarity mea-
sures provides a different view of how to rank
the translations of a given test word. Combining
the obtained ranked lists should reinforce the con-
fidence in consensus translations, while decreas-
ing the confidence in non-consensus translations.
We have therefore tested their combination. For
this, we used a voting method, and chose one in
the Condorcet family the Condorcet data fusion
method. This method was widely used to combine
document retrieval results from information re-
trieval systems (Montague and Aslam, 2002; Nu-
ray and Can, 2006). It is a single-winner election
method that ranks the candidates in order of pref-
erence. It is a pairwise voting, i.e. it compares ev-
ery possible pair of candidates to decide the pref-
erence of them. A matrix can be used to present
the competition process. Every candidate appears
in the matrix as a row and a column as well. If
there are m candidates, then we need m2 elements
in the matrix in total. Initially 0 is written to all the
elements. If di is preferred to dj , then we add 1 to
the element at row i and column j (aij). The pro-
762
cess is repeated until all the ballots are processed.
For every element aij , if aij > m/2 , then di
beats dj ; if aij < m/2, then dj beats di; other-
wise (aij = m/2), there is a draw between di and
dj . The total score of each candidate is quantified
by summing the raw scores it obtains in all pair-
wise competitions. Finally the ranking is achiev-
able based on the total scores calculated.
Here, we view the ranking of the extraction re-
sults from different similarity measures as a spe-
cial instance of the voting problem where the
Top20 extraction results correspond to candidates
and different semantic similarity measures are the
voters. The combination method referred to as
CONDORCETMerge outperformed all the others
(see Tables 2a and 2b): (1) individual measures,
(2) SA, and (3) MP11. Even though the two cor-
pora are fairly different (subject and polysemy
rate), the optimal results are obtained when con-
sidering up to two most similar translations in con-
text vectors. This behavior shows that the fusion
method is robust to domain change. The addition
of supplementary translations, which are probably
noisy in the given domain, degrades the overall re-
sults. The F-measure gains with respect to SA are
+0.207 for corporate finance and +0.121 for the
breast cancer corpus. More interestingly, our ap-
proach outperforms MP11, showing that the role
of disambiguation is more important than that of
feature weighting.
5 Conclusion
We presented in this paper a novel method that
extends the standard approach used for bilingual
lexicon extraction. This method disambiguates
polysemous words in context vectors by selecting
only the most relevant translations. Five seman-
tic similarity and relatedness measures were used
for this purpose. Experiments conducted on two
specialized comparable corpora indicate that the
combination of similarity metrics leads to a better
performance than two state-of-the-art approaches.
This shows that the ambiguity present in special-
ized comparable corpora hampers bilingual lexi-
con extraction, and that methods such as the one
introduced here are needed. The obtained results
are very encouraging and can be improved in a
number of ways. First, we plan to mine much
larger specialized comparable corpora and focus
on their quality (Li and Gaussier, 2010). We also
plan to test our method on bilingual lexicon extrac-
tion from general-domain corpora, where ambigu-
ity is generally higher and disambiguation meth-
ods should be all the more needed.
References
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In Proceedings of the Third In-
ternational Conference on Computational Linguis-
tics and Intelligent Text Processing, CICLing ?02,
pages 136?145, London, UK, UK. Springer-Verlag.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings of
the 19th international conference on Computational
linguistics - Volume 2, COLING ?02, pages 1?5. As-
sociation for Computational Linguistics.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003.
The effect of a general lexicon in corpus-based iden-
tification of french-english medical word transla-
tions. In Proceedings Medical Informatics Europe,
volume 95 of Studies in Health Technology and In-
formatics, pages 397?402, Amsterdam.
Miyoung Cho, Chang Choi, Hanil Kim, Jungpil Shin,
and PanKoo Kim. 2007. Efficient image retrieval
using conceptualization of annotated images. Lec-
ture Notes in Computer Science, pages 426?433.
Springer.
Dongjin Choi, Jungin Kim, Hayoung Kim, Myungg-
won Hwang, and Pankoo Kim. 2012. A method for
enhancing image retrieval based on annotation using
modified wup similarity in wordnet. In Proceed-
ings of the 11th WSEAS international conference
on Artificial Intelligence, Knowledge Engineering
and Data Bases, AIKED?12, pages 83?87, Stevens
Point, Wisconsin, USA. World Scientific and Engi-
neering Academy and Society (WSEAS).
Pascale Fung. 1998. A statistical view on bilingual
lexicon extraction: From parallel corpora to non-
parallel corpora. In Parallel Text Processing, pages
1?17. Springer.
E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from compara-
ble corpora. In ACL, pages 526?533.
Z.S. Harris. 1954. Distributional structure. Word.
Amir Hazem and Emmanuel Morin. 2012. Adap-
tive dictionary for bilingual lexicon extraction from
comparable corpora. In Proceedings, 8th interna-
tional conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey, May.
Myunggwon Hwang, Chang Choi, and Pankoo Kim.
2011. Automatic enrichment of semantic relation
763
network and its application to word sense disam-
biguation. IEEE Transactions on Knowledge and
Data Engineering, 23:845?858.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In 23rd
International Conference on Computational Lin-
guistics (Coling 2010), pages 617?625, Beijing,
China, Aug.
Claudia Leacock and Martin Chodorow, 1998. Com-
bining local context and WordNet similarity for word
sense identification, pages 305?332. In C. Fellbaum
(Ed.), MIT Press.
Bo Li and E?ric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In 23rd International Confer-
ence on Computational Linguistics (Coling 2010),
Beijing, China, Aug.
Mark Montague and Javed A. Aslam. 2002. Con-
dorcet fusion for improved retrieval. In Proceedings
of the eleventh international conference on Informa-
tion and knowledge management, CIKM ?02, pages
538?548, New York, NY, USA. ACM.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceed-
ings, 4th Workshop on Building and Using Compa-
rable Corpora (BUCC), page 27?34, Portland, Ore-
gon, USA.
Rabia Nuray and Fazli Can. 2006. Automatic ranking
of information retrieval systems using data fusion.
Inf. Process. Manage., 42(3):595?614, May.
Siddharth Patwardhan. 2003. Incorporating Dictio-
nary and Corpus Information into a Context Vector
Measure of Semantic Relatedness. Master?s thesis,
University of Minnesota, Duluth, August.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Transla-
tion Summit (MT Summit XII), page 284?291, Ot-
tawa, Ontario, Canada.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, ACL ?95, pages 320?322. Association for
Computational Linguistics.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138. Association
for Computational Linguistics.
764
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 16?23,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Using WordNet and Semantic Similarity for Bilingual Terminology
Mining from Comparable Corpora
Dhouha Bouamor
CEA, LIST, Vision and
Content Engineering Laboratory,
91191 Gif-sur-Yvette CEDEX
France
dhouha.bouamor@cea.fr
Nasredine Semmar
CEA, LIST, Vision and Content
Engineering Laboratory,
91191 Gif-sur-Yvette
CEDEX France
nasredine.semmar@cea.fr
Pierre Zweigenbaum
LIMSI-CNRS,
F-91403 Orsay CEDEX
France
pz@limsi.fr
Abstract
This paper presents an extension of the
standard approach used for bilingual lex-
icon extraction from comparable corpora.
We study of the ambiguity problem re-
vealed by the seed bilingual dictionary
used to translate context vectors. For
this purpose, we augment the standard ap-
proach by a Word Sense Disambiguation
process relying on a WordNet-based se-
mantic similarity measure. The aim of
this process is to identify the translations
that are more likely to give the best rep-
resentation of words in the target lan-
guage. On two specialized French-English
comparable corpora, empirical experimen-
tal results show that the proposed method
consistently outperforms the standard ap-
proach.
1 Introduction
Bilingual lexicons play a vital role in many Natu-
ral Language Processing applications such as Ma-
chine Translation (Och and Ney, 2003) or Cross-
Language Information Retrieval (Shi, 2009). Re-
search on lexical extraction from multilingual cor-
pora have largely focused on parallel corpora. The
scarcity of such corpora in particular for special-
ized domains and for language pairs not involv-
ing English pushed researchers to investigate the
use of comparable corpora (Fung, 1998; Chiao
and Zweigenbaum, 2003). These corpora are com-
prised of texts which are not exact translation of
each other but share common features such as do-
main, genre, sampling period, etc.
The main work in this research area could be
seen as an extension of Harris?s distributional hy-
pothesis (Harris, 1954). It is based on the sim-
ple observation that a word and its translation are
likely to appear in similar contexts across lan-
guages (Rapp, 1995). Based on this assumption,
the alignment method, known as the standard ap-
proach builds and compares context vectors for
each word of the source and target languages.
A particularity of this approach is that, to enable
the comparison of context vectors, it requires the
existence of a seed bilingual dictionary to translate
source context vectors. The use of the bilingual
dictionary is problematic when a word has sev-
eral translations, whether they are synonymous or
polysemous. For instance, the French word action
can be translated into English as share, stock, law-
suit or deed. In such cases, it is difficult to iden-
tify in flat resources like bilingual dictionaries,
wherein entries are usually unweighted and un-
ordered, which translations are most relevant. The
standard approach considers all available trans-
lations and gives them the same importance in
the resulting translated context vectors indepen-
dently of the domain of interest and word ambigu-
ity. Thus, in the financial domain, translating ac-
tion into deed or lawsuit would probably introduce
noise in context vectors.
In this paper, we present a novel approach
which addresses the word ambiguity problem ne-
glected in the standard approach. We introduce a
use of a WordNet-based semantic similarity mea-
sure permitting the disambiguation of translated
context vectors. The basic intuition behind this
method is that instead of taking all translations
of each seed word to translate a context vector,
we only use the translations that are more likely
to give the best representation of the context vec-
tor in the target language. We test the method on
two specialized French-English comparable cor-
16
pora (financial and medical) and report improved
results, especially when many of the words in the
corpus are ambiguous.
The remainder of the paper is organized as fol-
lows: Section 2 presents the standard approach
and recalls in some details previous work address-
ing the task of bilingual lexicon extraction from
comparable corpora. In section 3 we present our
context disambiguation process. Before conclud-
ing and presenting directions for future work, we
describe in section 4 the experimental protocol we
followed and discuss the obtained results.
2 Bilingual lexicon extraction
2.1 Standard Approach
Most previous works addressing the task of bilin-
gual lexicon extraction from comparable corpora
are based on the standard approach (Fung, 1998;
Chiao and Zweigenbaum, 2002; Laroche and
Langlais, 2010). Formally, this approach is com-
posed of the following three steps:
1. Building context vectors: Vectors are first
extracted by identifying the words that appear
around the term to be translated S in a win-
dow of N words. Generally, an association
measure like the mutual information (Morin
and Daille, 2006), the log-likelihood (Morin
and Prochasson, 2011) or the Discounted
Odds-Ratio (Laroche and Langlais, 2010) are
employed to shape the context vectors.
2. Translation of context vectors: To enable
the comparison of source and target vectors,
source terms vectors are translated in the tar-
get language by using a seed bilingual dic-
tionary. Whenever it provides several trans-
lations for an element, all proposed transla-
tions are considered. Words not included in
the bilingual dictionary are simply ignored.
3. Comparison of source and target vectors:
Translated vectors are compared to target
ones using a similarity measure. The most
widely used is the cosine similarity, but
many authors have studied alternative metrics
such as the Weighted Jaccard index (Prochas-
son et al, 2009) or the City-Block dis-
tance (Rapp, 1999). According to similarity
values, a ranked list of translations for S is
obtained.
2.2 Related Work
Recent improvements of the standard approach are
based on the assumption that the more the con-
text vectors are representative, the better the bilin-
gual lexicon extraction is. Prochasson et al (2009)
used transliterated words and scientific compound
words as ?anchor points?. Giving these words
higher priority when comparing target vectors im-
proved bilingual lexicon extraction. In addition to
transliteration, Rubino and Linare`s (2011) com-
bined the contextual representation within a the-
matic one. The basic intuition of their work is that
a term and its translation share thematic similari-
ties. Hazem and Morin (2012) recently proposed a
method that filters the entries of the bilingual dic-
tionary based upon POS-tagging and domain rel-
evance criteria, but no improvements was demon-
strated.
Gaussier et al (2004) attempted to solve the
problem of different word ambiguities in the
source and target languages. They investigated a
number of techniques including canonical corre-
lation analysis and multilingual probabilistic la-
tent semantic analysis. The best results, with a
very small improvement were reported for a mixed
method. One important difference with Gaussier
et al (2004) is that they focus on words ambigu-
ities on source and target languages, whereas we
consider that it is sufficient to disambiguate only
translated source context vectors.
A large number of Word Sense Disambigua-
tion WSD techniques were previously proposed
in the literature. The most popular ones are those
that compute semantic similarity with the help
of existing thesauri such as WordNet (Fellbaum,
1998). This resource groups English words into
sets of synonyms called synsets, provides short,
general definitions and records various semantic
relations (hypernymy, meronymy, etc.) between
these synonym sets. This thesaurus has been ap-
plied to many tasks relying on word-based sim-
ilarity, including document (Hwang et al, 2011)
and image (Cho et al, 2007; Choi et al, 2012)
retrieval systems. In this work, we use this re-
source to derive a semantic similarity between lex-
ical units within the same context vector. To the
best of our knowledge, this is the first application
of WordNet to the task of bilingual lexicon extrac-
tion from comparable corpora.
17
  
Word?to?be?translated?(source?language)
Building?Context?Vector
Context?vector?  Translated?Context?vector
Bilingual?Dictionary WordNet
Disambiguated??Context?vector
Context?Vectors?(Target?language)
Figure 1: Overall architecture of the lexical extraction approach
3 Context Vector Disambiguation
The approach we propose includes the three steps
of the standard approach. As it was mentioned in
section 1, when lexical extraction applies to a spe-
cific domain, not all translations in the bilingual
dictionary are relevant for the target context vec-
tor representation. For this reason, we introduce
a WordNet-based WSD process that aims at im-
proving the adequacy of context vectors and there-
fore improve the results of the standard approach.
Figure 1 shows the overall architecture of the lexi-
cal extraction process. Once translated into the tar-
get language, the context vectors disambiguation
process intervenes. This process operates locally
on each context vector and aims at finding the
most prominent translations of polysemous words.
For this purpose, we use monosemic words as a
seed set of disambiguated words to infer the pol-
ysemous word?s translations senses. We hypoth-
esize that a word is monosemic if it is associated
to only one entry in the bilingual dictionary. We
checked this assumption by probing monosemic
entries of the bilingual dictionary against WordNet
and found that 95% of the entries are monosemic
in both resources.
Formally, we derive a semantic similarity value
between all the translations provided for each pol-
ysemous word by the bilingual dictionary and
all monosemic words appearing whithin the same
context vector. There is a relatively large number
of word-to-word similarity metrics that were pre-
viously proposed in the literature, ranging from
path-length measures computed on semantic net-
works, to metrics based on models of distribu-
tional similarity learned from large text collec-
tions. For simplicity, we use in this work, the Wu
and Palmer (1994) (WUP) path-length-based se-
mantic similarity measure. It was demonstrated by
(Lin, 1998) that this metric achieves good perfor-
mances among other measures. WUP computes a
score (equation 1) denoting how similar two word
senses are, based on the depth of the two synsets
(s1 and s2) in the WordNet taxonomy and that of
their Least Common Subsumer (LCS), i.e., the
most specific word that they share as an ancestor.
WupSim(s1, s2) =
2? depth(LCS)
depth(s1) + depth(s2)
(1)
In practice, since a word can belong to more
than one synset in WordNet, we determine the
semantic similarity between two words w1 and
w2 as the maximum WupSim between the synset
or the synsets that include the synsets(w1) and
synsets(w2) according to the following equation:
SemSim(w1, w2) = max{WupSim(s1, s2);
(s1, s2) ? synsets(w1)? synsets(w2)} (2)
18
Context Vector Translations Comparison Ave Sim
liquidite? liquidity ? ?
action
act SemSim(act,liquidity), SemSim(act,dividend) 0.2139
action SemSim(action,liquidity), SemSim(action,dividend) 0.4256
stock SemSim(stock,liquidity), SemSim(stock,dividend) 0.5236
deed SemSim(deed,liquidity), SemSim(deed,dividend) 0.1594
lawsuit SemSim(lawsuit,liquidity), SemSim(lawsuit,dividend) 0.1212
fact SemSim(fact,liquidity), SemSim(fact,dividend) 0.1934
operation SemSim(operation,liquidity), SemSim(operation,dividend) 0.2045
share SemSim(share,liquidity), SemSim(share,dividend) 0.5236
plot SemSim(plot,liquidity), SemSim(plot,dividend) 0.2011
dividende dividend ? ?
Table 1: Disambiguation of the context vector of the French term be?ne?fice [income] in the corporate
finance domain. liquidite? and dividende are monosemic and are used to infer the most similar translations
of the term action.
Then, to identify the most prominent translations
of each polysemous unit wp, an average similarity
is computed for each translation wjp of wp:
Ave Sim(wjp) =
?N
i=1 SemSim(wi, w
j
p)
N
(3)
where N is the total number of monosemic words
and SemSim is the similarity value of w
j
p and the
ith monosemic word. Hence, according to average
relatedness values Ave Sim(wjp), we obtain for
each polysemous word wp an ordered list of trans-
lations w1p . . . w
n
p . This allows us to select trans-
lations of words which are more salient than the
others to represent the word to be translated.
In Table 1, we present the results of the dis-
ambiguation process for the context vector of the
French term be?ne?fice in the corporate finance cor-
pus. This vector contains the words action, div-
idende, liquidite? and others. The bilingual dic-
tionary provides the following translations {act,
stock, action, deed, lawsuit, fact, operation, plot,
share} for the French polysemous word action.
We use the monosemic words dividende and liq-
uidite? to disambiguate the word action. From ob-
serving average similariy values (Ave Sim), we
notice that the words share and stock are on the
top of the list and therefore are most likely to rep-
resent the source word action in this context.
Corpus French English
Corporate finance 402, 486 756, 840
Breast cancer 396, 524 524, 805
Table 2: Comparable corpora sizes in term of
words.
4 Experiments and Results
4.1 Resources
4.1.1 Comparable corpora
We conducted our experiments on two French-
English comparable corpora specialized on
the corporate finance and the breast cancer
domains. Both corpora were extracted from
Wikipedia1. We consider the topic in the source
language (for instance finance des entreprises
[corporate finance]) as a query to Wikipedia
and extract all its sub-topics (i.e., sub-categories
in Wikipedia) to construct a domain-specific
category tree. A sample of the corporate fi-
nance sub-domain?s category tree is shown in
Figure 2. Then, based on the constructed tree,
we collect all Wikipedia pages belonging to one
of these categories and use inter-language links
to build the comparable corpus. Both corpora
were normalized through the following linguistic
preprocessing steps: tokenisation, part-of-speech
tagging, lemmatisation, and function word re-
moval. The resulting corpora2 sizes are given in
Table 2.
1http://dumps.wikimedia.org/
2Comparable corpora will be shared publicly
19
  
Finance?des?entreprise?[Corporate?Finance]
Analyse?Financi?re?[Financial?Analysis] Comptabilit??g?n?rale[Financial?accountancy] Indicateur?Financier[Financial?ratios]
Risque[Risk] Cr?dit[Credit]Actifs[Asset] Bilan[Balance?sheet] Salaire[Salary]Solde[Balance]
B?n?fice[profit] Revenu[Income]
...?...?
Figure 2: Wikipedia categories tree of the corporate finance sub-domain.
4.1.2 Bilingual dictionary
The bilingual dictionary used to translate context
vectors consists of an in-house manually revised
bilingual dictionary which contains about 120,000
entries belonging to the general domain. It is im-
portant to note that words on both corpora has on
average, 7 translations in the bilingual dictionary.
4.1.3 Evaluation list
In bilingual terminology extraction from compa-
rable corpora, a reference list is required to eval-
uate the performance of the alignment. Such
lists are usually composed of about 100 sin-
gle terms (Hazem and Morin, 2012; Chiao and
Zweigenbaum, 2002). Here, we created two refer-
ence lists3 for the corporate finance and the breast
cancer domains. The first list is composed of 125
single terms extracted from the glossary of bilin-
gual micro-finance terms4. The second list con-
tains 96 terms extracted from the French-English
MESH and the UMLS thesauri5. Note that refer-
ence terms pairs appear at least five times in each
part of both comparable corpora.
4.2 Experimental setup
Three other parameters need to be set up: (1) the
window size, (2) the association measure and the
(3) similarity measure. To define context vectors,
we use a seven-word window as it approximates
syntactic dependencies. Concerning the rest of the
3Reference lists will be shared publicly
4http://www.microfinance.lu/en/
5http://www.nlm.nih.gov/
parameters, we followed Laroche and Langlais
(2010) for their definition. The authors carried out
a complete study of the influence of these param-
eters on the bilingual alignment and showed that
the most effective configuration is to combine the
Discounted Log-Odds ratio (equation 4) with the
cosine similarity. The Discounted Log-Odds ratio
is defined as follows:
Odds-Ratiodisc = log
(O11 + 12)(O22 +
1
2)
(O12 + 12)(O21 +
1
2)
(4)
where Oij are the cells of the 2 ? 2 contingency
matrix of a token s co-occurring with the term S
within a given window size.
4.3 Results and discussion
It is difficult to compare results between different
studies published on bilingual lexicon extraction
from comparable corpora, because of difference
between (1) used corpora (in particular their con-
struction constraints and volume), (2) target do-
mains, and also (3) the coverage and relevance of
linguistic resources used for translation. To the
best of our knowledge, there is no common bench-
mark that can serve as a reference. For this reason,
we use the results of the standard approach (SA)
described in section 2.1 as a reference. We evalu-
ate the performance of both the SA and ours with
respect to TopN precision (PN ), recall (RN ) and
Mean Reciprocal Rank (MRR) (Voorhees, 1999).
Precision is the total number of correct translations
divided by the number of terms for which the sys-
tem gave at least one answer. Recall is equal to
20
a)
C
or
po
ra
te
F
in
an
ce
Method P1 P10 P20 R1 R10 R20 MRR
Standard Approach (SA) 0.046 0.140 0.186 0.040 0.120 0.160 0.064
WN-T1 0.065 0.196 0.261 0.056 0.168 0.224 0.089
WN-T2 0.102 0.252 0.308 0.080 0.216 0.264 0.122
WN-T3 0.102 0.242 0.327 0.088 0.208 0.280 0.122
WN-T4 0.112 0.224 0.299 0.090 0.190 0.250 0.124
WN-T5 0.093 0.205 0.280 0.080 0.176 0.240 0.110
WN-T6 0.084 0.205 0.233 0.072 0.176 0.200 0.094
WN-T7 0.074 0.177 0.242 0.064 0.152 0.208 0.090
b)
B
re
as
tC
an
ce
r
Method P1 P10 P20 R1 R10 R20 MRR
Standard Approach (SA) 0.342 0.542 0.585 0.250 0.395 0.427 0.314
WN-T1 0.257 0.500 0.571 0.187 0.364 0.416 0.257
WN-T2 0.314 0.614 0.671 0.229 0.447 0.489 0.313
WN-T3 0.342 0.628 0.671 0.250 0.458 0.489 0.342
WN-T4 0.342 0.571 0.642 0.250 0.416 0.468 0.332
WN-T5 0.357 0.571 0.657 0.260 0.416 0.479 0.348
WN-T6 0.357 0.571 0.652 0.260 0.416 0.468 0.347
WN-T7 0.357 0.585 0.657 0.260 0.427 0.479 0.339
Table 3: Precision, Recall at TopN (N=1,10,20) and MRR at Top20 for the two domains. In each column,
bold show best results. Underline show best results overall.
the ratio of correct translation to the total number
of terms. The MRR takes into account the rank
of the first good translation found for each entry.
Formally, it is defined as:
MRR =
1
Q
i=1?
|Q|
1
ranki
(5)
where Q is the total number of terms to be trans-
lated and ranki is the position of the first correct
translation in the translations candidates.
Our method provides a ranked list of transla-
tions for each polysemous word. A question that
arises here is whether we should introduce only
the best ranked translation in the context vector
or consider a larger number of words, especially
when a translations list contain synonyms (share
and stock in Table 1). For this reason, we take
into account in our experiments different number
of translations, noted WN-Ti, ranging from the
pivot translation (i = 1) to the seventh word in the
translations list. This choice is motivated by the
fact that words in both corpora have on average 7
translations in the bilingual dictionary. The base-
line (SA) uses all translations associated to each
entry in the bilingual dictionary. Table 3a displays
the results obtained for the corporate finance cor-
pus. The first substantial observation is that our
method which consists in disambiguating polyse-
mous words within context vectors consistently
outperforms the standard approach (SA) for all
configurations. The best MRR is reported when
for each polysemous word, we keep the most simi-
lar four translations (WN-T4) in the context vector
of the term to be translated. However, the highest
Top20 precision and recall are obtained by WN-
T3. Using the top three word translations in the
vector boosts the Top20 precision from 0.186 to
0.327 and the Top20 recall from 0.160 to 0.280.
Concerning the Breast Cancer corpus, slightly dif-
ferent results were obtained. As Table 3b show,
when the context vectors are totally disambiguated
(i.e. each source unit is translated by at most one
word in context vectors), all TopN precision, re-
call and MRR decrease. However, we report im-
provements against the SA in most other cases.
For WN-T5, we obtain the maximum MRR score
with an improvement of +0.034 over the SA. But,
as for the corporate finance corpus, the best Top20
precision and recall are reached by the WN-T3
method, with a gain of +0.082 in both Top10 and
Top20 precision and of about +0.06 in Top10 and
Top20 recall.
From observing result tables of both corporate
finance and breast cancer domains, we notice that
our approach performs better than the SA but with
different degrees. The improvements achieved in
21
Corpus Corpus PR Vectors PR
Corporate finance 41% 91, 6%
Breast cancer 47% 85, 1%
Table 4: Comparable corpora?s and context vec-
tor?s Polysemy Rates PR.
the corporate finance domain are higher than those
reported in the breast cancer domain. The reason
being that the vocabulary used in the breast cancer
corpus is more specific and therefore less ambigu-
ous than that used in corporate finance texts. The
results given in table 4 validate this assumption. In
this table, we give the polysemy rates of the com-
parable corpora (Corpus PR) and that of context
vectors (Vectors PR). PR indicates the percent-
age of words that are associated to more than one
translation in the bilingual dictionary. The results
show that breast cancer corpus is more polysemic
than that of the corporate finance. Nevertheless,
even if in both corpora, the candidates? context
vectors are highly polysemous, breast cancer?s
context vectors are less polysemous than those of
the corporate finance texts. In this corpus, 91, 6%
of the words used as entries to define context vec-
tors are polysemous. This shows that the ambi-
guity present in specialized comparable corpora
hampers bilingual lexicon extraction, and that dis-
ambiguation positively affects the overall results.
Even though the two corpora are fairly different
(subject and polysemy rate), the optimal Top20
precision and recall results are obtained when con-
sidering up to three most similar translations in
context vectors. This behavior shows that the dis-
ambiguation method is relatively robust to domain
change. We notice also that the addition of supple-
mentary translations, which are probably noisy in
the given domain, degrades the overall results but
remains greater than the SA.
5 Conclusion
We presented in this paper a novel method that
extends the standard approach used for bilin-
gual lexicon extraction from comparable corpora.
The proposed method disambiguates polysemous
words in context vectors and selects only the trans-
lations that are most relevant to the general con-
text of the corpus. Conducted experiments on two
highly polysemous specialized comparable cor-
pora show that integrating such process leads to
a better performance than the standard approach.
Although our initial experiments are positive, we
believe that they could be improved in a number
of ways. In addition to the metric defined by (Wu
and Palmer, 1994), we plan to apply other seman-
tic similarity and relatedness measures and com-
pare their performance. It would also be interest-
ing to mine much more larger comparable corpora
and focus on their quality as presented in (Li and
Gaussier, 2010). We want also to test our method
on bilingual lexicon extraction for a larger panel of
specialized corpora, where disambiguation meth-
ods are needed to prune translations that are irrel-
evant to the domain.
References
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings of
the 19th international conference on Computational
linguistics - Volume 2, COLING ?02, pages 1?5. As-
sociation for Computational Linguistics.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003.
The effect of a general lexicon in corpus-based iden-
tification of french-english medical word transla-
tions. In Proceedings Medical Informatics Europe,
volume 95 of Studies in Health Technology and In-
formatics, pages 397?402, Amsterdam.
Miyoung Cho, Chang Choi, Hanil Kim, Jungpil Shin,
and PanKoo Kim. 2007. Efficient image retrieval
using conceptualization of annotated images. Lec-
ture Notes in Computer Science, pages 426?433.
Springer.
Dongjin Choi, Jungin Kim, Hayoung Kim, Myungg-
won Hwang, and Pankoo Kim. 2012. A method for
enhancing image retrieval based on annotation using
modified wup similarity in wordnet. In Proceed-
ings of the 11th WSEAS international conference
on Artificial Intelligence, Knowledge Engineering
and Data Bases, AIKED?12, pages 83?87, Stevens
Point, Wisconsin, USA. World Scientific and Engi-
neering Academy and Society (WSEAS).
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Pascale Fung. 1998. A statistical view on bilingual
lexicon extraction: From parallel corpora to non-
parallel corpora. In Parallel Text Processing, pages
1?17. Springer.
E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from compara-
ble corpora. In ACL, pages 526?533.
Z.S. Harris. 1954. Distributional structure. Word.
22
Amir Hazem and Emmanuel Morin. 2012. Adap-
tive dictionary for bilingual lexicon extraction from
comparable corpora. In Proceedings, 8th interna-
tional conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey, May.
Myunggwon Hwang, Chang Choi, and Pankoo Kim.
2011. Automatic enrichment of semantic relation
network and its application to word sense disam-
biguation. IEEE Transactions on Knowledge and
Data Engineering, 23:845?858.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In 23rd
International Conference on Computational Lin-
guistics (Coling 2010), pages 617?625, Beijing,
China, Aug.
Bo Li and E?ric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In 23rd International Confer-
ence on Computational Linguistics (Coling 2010),
Beijing, China, Aug.
Dekang Lin. 1998. An information-theoretic def-
inition of similarity. In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing, ICML ?98, pages 296?304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Emmanuel Morin and Be?atrice Daille. 2006. Com-
parabilite? de corpus et fouille terminologique mul-
tilingue. In Traitement Automatique des Langues
(TAL).
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceed-
ings, 4th Workshop on Building and Using Compa-
rable Corpora (BUCC), page 27?34, Portland, Ore-
gon, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Transla-
tion Summit (MT Summit XII), page 284?291, Ot-
tawa, Ontario, Canada.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, ACL ?95, pages 320?322. Association for
Computational Linguistics.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, ACL ?99, pages 519?
526. Association for Computational Linguistics.
Raphae?l Rubino and Georges Linare`s. 2011. A multi-
view approach for term translation spotting. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
29?40.
Lei Shi. 2009. Adaptive web mining of bilingual
lexicons for cross language information retrieval.
In Proceedings of the 18th ACM conference on In-
formation and knowledge management, CIKM ?09,
pages 1561?1564, New York, NY, USA. ACM.
Ellen M. Voorhees. 1999. The trec-8 question an-
swering track report. In In Proceedings of TREC-8,
pages 77?82.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138. Association
for Computational Linguistics.
23
