Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 205?213, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Spectral Dependency Parsing with Latent Variables
Paramveer S. Dhillon1, Jordan Rodu2, Michael Collins3, Dean P. Foster2
and Lyle H. Ungar1
1Computer & Information Science/ 2Statistics, University of Pennsylvania, Philadelphia, PA, U.S.A
3 Computer Science, Columbia University, New York, NY, U.S.A
{dhillon|ungar@cis.upenn.edu}, {jrodu|foster@wharton.upenn.edu}
mcollins@cs.columbia.edu
Abstract
Recently there has been substantial interest in
using spectral methods to learn generative se-
quence models like HMMs. Spectral meth-
ods are attractive as they provide globally con-
sistent estimates of the model parameters and
are very fast and scalable, unlike EM meth-
ods, which can get stuck in local minima. In
this paper, we present a novel extension of
this class of spectral methods to learn depen-
dency tree structures. We propose a simple
yet powerful latent variable generative model
for dependency parsing, and a spectral learn-
ing method to efficiently estimate it. As a pi-
lot experimental evaluation, we use the spec-
tral tree probabilities estimated by our model
to re-rank the outputs of a near state-of-the-
art parser. Our approach gives us a moderate
reduction in error of up to 4.6% over the base-
line re-ranker.
1 Introduction
Markov models have been for two decades a
workhorse of statistical pattern recognition with ap-
plications ranging from speech to vision to lan-
guage. Adding latent variables to these models gives
us additional modeling power and have shown suc-
cess in applications like POS tagging (Merialdo,
1994), speech recognition (Rabiner, 1989) and ob-
ject recognition (Quattoni et al2004). However,
this comes at the cost that the resulting parameter
estimation problem becomes non-convex and tech-
niques like EM (Dempster et al1977) which are
used to estimate the parameters can only lead to lo-
cally optimal solutions.
Recent work by Hsu et al2008) has shown that
globally consistent estimates of the parameters of
HMMs can be found by using spectral methods, par-
ticularly by singular value decomposition (SVD) of
appropriately defined linear systems. They avoid the
NP Hard problem of the global optimization prob-
lem of the HMM parameters (Terwijn, 2002), by
putting restrictions on the smallest singular value
of the HMM parameters. The main intuition be-
hind the model is that, although the observed data
(i.e. words) seem to live in a very high dimensional
space, but in reality they live in a very low dimen-
sional space (size k ? 30 ? 50) and an appropriate
eigen decomposition of the observed data will re-
veal the underlying low dimensional dynamics and
thereby revealing the parameters of the model. Be-
sides ducking the NP hard problem, the spectral
methods are very fast and scalable to train compared
to EM methods.
In this paper we generalize the approach of Hsu
et al2008) to learn dependency tree structures with
latent variables.1 Petrov et al2006) and Musillo
and Merlo (2008) have shown that learning PCFGs
and dependency grammars respectively with latent
variables can produce parsers with very good gen-
eralization performance. However, both these ap-
proaches rely on EM for parameter estimation and
can benefit from using spectral methods.
We propose a simple yet powerful latent vari-
able generative model for use with dependency pars-
1Actually, instead of using the model by Hsu et al2008)
we work with a related model proposed by Foster et al2012)
which addresses some of the shortcomings of the earlier model
which we detail below.
205
ing which has one hidden node for each word in
the sentence, like the one shown in Figure 1 and
work out the details for the parameter estimation
of the corresponding spectral learning model. At
a very high level, the parameter estimation of our
model involves collecting unigram, bigram and tri-
gram counts sensitive to the underlying dependency
structure of the given sentence.
Recently, Luque et al2012) have also proposed
a spectral method for dependency parsing, however
they deal with horizontal markovization and use hid-
den states to model sequential dependencies within a
word?s sequence of children. In contrast with that, in
this paper, we propose a spectral learning algorithm
where latent states are not restricted to HMM-like
distributions of modifier sequences for a particular
head, but instead allow information to be propagated
through the entire tree.
More recently, Cohen et al2012) have proposed
a spectral method for learning PCFGs.
Its worth noting that recent work by Parikh et al
(2011) also extends Hsu et al2008) to latent vari-
able dependency trees like us but under the restric-
tive conditions that model parameters are trained for
a specified, albeit arbitrary, tree topology.2 In other
words, all training sentences and test sentences must
have identical tree topologies. By doing this they al-
low for node-specific model parameters, but must re-
train the model entirely when a different tree topol-
ogy is encountered. Our model on the other hand al-
lows the flexibility and efficiency of processing sen-
tences with a variety of tree topologies from a single
training run.
Most of the current state-of-the-art dependency
parsers are discriminative parsers (Koo et al2008;
McDonald, 2006) due to the flexibility of represen-
tations which can be used as features leading to bet-
ter accuracies and the ease of reproducibility of re-
sults. However, unlike discriminative models, gen-
erative models can exploit unlabeled data. Also, as
is common in statistical parsing, re-ranking the out-
puts of a parser leads to significant reductions in er-
ror (Collins and Koo, 2005).
Since our spectral learning algorithm uses a gen-
2This can be useful in modeling phylogeny trees for in-
stance, but precludes most NLP applications, since there is a
need to model the full set of different tree topologies possible
in parsing.
h0
h1 h2
was
Kilroy here
Figure 1: Sample dependency parsing tree for ?Kilroy
was here?
erative model of words given a tree structure, it can
score a tree structure i.e. its probability of genera-
tion. Thus, it can be used to re-rank the n-best out-
puts of a given parser.
The remainder of the paper is organized as fol-
lows. In the next section we introduce the notation
and give a brief overview of the spectral algorithm
for learning HMMs (Hsu et al2008; Foster et al
2012). In Section 3 we describe our proposed model
for dependency parsing in detail and work out the
theory behind it. Section 4 provides experimental
evaluation of our model on Penn Treebank data. We
conclude with a brief summary and future avenues
for research.
2 Spectral Algorithm For Learning HMMs
In this section we describe the spectral algorithm for
learning HMMs.3
2.1 Notation
The HMM that we consider in this section is a se-
quence of hidden states h ? {1, . . . , k} that follow
the Markov property:
p(ht|h1, . . . , ht?1) = p(ht|ht?1)
and a sequence of observations x ? {1, . . . , n} such
that
p(xt|x1, . . . , xt?1, h1, . . . , ht) = p(xt|ht)
3As mentioned earlier, we use the model by Foster et al
(2012) which is conceptually similar to the one by Hsu et al
(2008), but does further dimensionality reduction and thus has
lower sample complexity. Also, critically, the fully reduced di-
mension model that we use generalizes much more cleanly to
trees.
206
The parameters of this HMM are:
? A vector pi of length k where pii = p(h1 = i):
The probability of the start state in the sequence
being i.
? A matrix T of size k ? k where
Ti,j = p(ht+1 = i|ht = j): The probability of
transitioning to state i, given that the previous
state was j.
? A matrix O of size n? k where
Oi,j = p(x = i|h = j): The probability of
state h emitting observation x.
Define ?j to be the vector of length n with a 1 in
the jth entry and 0 everywhere else, and diag(v) to
be the matrix with the entries of v on the diagonal
and 0 everywhere else.
The joint distribution of a sequence of observa-
tions x1, . . . , xm and a sequence of hidden states
h1, . . . , hm is:
p(x1, . . . ,xm, h1, . . . , hm)
= pih1
m?1?
j=2
Thj ,hj?1
m?
j=1
Oxj ,hj
Now, we can write the marginal probability of a
sequence of observations as
p(x1, . . . xm)
=
?
h1,...,hm
p(x1, . . . , xm, h1, . . . , hm)
which can be expressed in matrix form4 as:
p(x1, . . . , xm) = 1>AxmAxm?1 ? ? ?Am1pi
where Axm ? Tdiag(O
>?xm), and 1 is a k-
dimensional vector with every entry equal to 1.
A is called an ?observation operator?, and is ef-
fectively a third order tensor, and Axm which is a
matrix, gives the distribution vector over states at
timem+1 as a function of the state distribution vec-
tor at the current time m and the current observation
?xm . SinceAxm depends on the hidden state, it is not
observable, and hence cannot be directly estimated.
4This is essentially the matrix form of the standard dynamic
program (forward algorithm) used to estimate HMMs.
However, Hsu et al2008) and Foster et al2012)
showed that under certain conditions there exists a
fully observable representation of the observable op-
erator model.
2.2 Fully observable representation
Before presenting the model, we need to address a
few more points. First, let U be a ?representation
matrix? (eigenfeature dictionary) which maps each
observation to a reduced dimension space (n ? k)
that satisfies the conditions:
? U>O is invertible
? |Uij | < 1.
Hsu et al2008); Foster et al2012) discuss U
in more detail, but U can, for example, be obtained
by the SVD of the bigram probability matrix (where
Pij = p(xt+1 = i|xt = j)) or by doing CCA on
neighboring n-grams (Dhillon et al2011).
Letting yi = U>?xi , we have
p(x1, . . . , xm)
= c>?C(ym)C(ym?1) . . . C(y1)c1 (1)
where
c1 = ?
c? = ?
>??1
C(y) = K(y)??1
and ?, ? and K, described in more detail below, are
quantities estimated by frequencies of unigrams, bi-
grams, and trigrams in the observed (training) data.
Under the assumption that data is generated by
an HMM, the distribution p? obtained by substituting
the estimated values c?1, c??, and C?(y) into equation
(1) converges to p sufficiently fast as the amount of
training data increases, giving us consistent param-
eter estimates. For details of the convergence proof,
please see Hsu et al2008) and Foster et al2012).
3 Spectral Algorithm For Learning
Dependency Trees
In this section, we first describe a simple latent vari-
able generative model for dependency parsing. We
then define some extra notation and finally present
207
the details of the corresponding spectral learning al-
gorithm for dependency parsing, and prove that our
learning algorithm provides a consistent estimation
of the marginal probabilities.
It is worth mentioning that an alternate way of ap-
proaching the spectral estimation of latent states for
dependency parsing is by converting the dependency
trees into linear sequences from root-to-leaf and do-
ing a spectral estimation of latent states using Hsu
et al2008). However, this approach would not
give us the correct probability distribution over trees
as the probability calculations for different paths
through the trees are not independent. Thus, al-
though one could calculate the probability of a path
from the root to a leaf, one cannot generalize from
this probability to say anything about the neighbor-
ing nodes or words. Put another way, when a par-
ent has more than the one descendant, one has to be
careful to take into account that the hidden variables
at each child node are all conditioned on the hidden
variable of the parent.
3.1 A latent variable generative model for
dependency parsing
In the standard setting, we are given training exam-
ples where each training example consists of a se-
quence of words x1, . . . , xm together with a depen-
dency structure over those words, and we want to
estimate the probability of the observed structure.
This marginal probability estimates can then be used
to build an actual generative dependency parser or,
since the marginal probability is conditioned on the
tree structure, it can be used re-rank the outputs of a
parser.
As in the conventional HMM described in the pre-
vious section, we can define a simple latent variable
first order dependency parsing model by introduc-
ing a hidden variable hi for each word xi. The
joint probability of a sequence of observed nodes
x1, . . . , xm together with hidden nodes h1, . . . , hm
can be written as
p(x1, . . . ,xm, h1, . . . , hm)
= pih1
m?
j=2
td(j)(hj |hpa(j))
m?
j=1
o(xj |hj)
(2)
h1
h2 h3
y1
y2 y3
Figure 2: Dependency parsing tree with observed vari-
ables y1, y2, and y3.
where pa(j) is the parent of node j and d(j) ?
{L,R} indicates whether hj is a left or a right node
of hpa(j). For simplicity, the number of hidden and
observed nodes in our tree are the same, however
they are not required to be so.
As is the case with the conventional HMM, the
parameters used to calculate this joint probability
are unobservable, but it turns out that under suitable
conditions a fully observable model is also possible
for the dependency tree case with the parameteriza-
tion as described below.
3.2 Model parameters
We will define both the theoretical representations
of our observable parameters, and the sampling ver-
sions of these parameters. Note that in all the cases,
the estimated versions are unbiased estimates of the
theoretical quantities.
Define Td and T ud where d ? {L,R} to be the
hidden state transition matrices from parent to left
or right child, and from left or right child to parent
(hence the u for ?up?), respectively. In other words
(referring to Figure 2)
TR = t(h3|h1)
TL = t(h2|h1)
T uR = t(h1|h3)
T uL = t(h1|h2)
Let Ux(i) be the i
th entry of vector U>?x andG =
U>O. Further, recall the notation diag(v), which is
a matrix with elements of v on its diagonal, then:
? Define the k-dimensional vector ? (unigram
208
counts):
? = Gpi
[??]i =
n?
u=1
c?(u)Uu(i)
where c?(u) = c(u)N1 , c(u) is the count of ob-
servation u in the training sample, and N1 =?
u?n c(u).
? Define the k?k matrices ?L and ?R (left child-
parent and right child-parent bigram counts):
[??L]i,j =
n?
u=1
n?
v=1
c?L(u, v)Uu(j)Uv(i)
?L = GT
u
Ldiag(pi)G
>
[??R]i,j =
n?
u=1
n?
v=1
c?R(u, v)Uu(j)Uv(i)
?R = GT
u
Rdiag(pi)G
>
where c?L(u, v) =
cL(u,v)
N2L
, cL(u, v) is the count
of bigram (u, v) where u is the left child and
v is the parent in the training sample, and
N2L =
?
(u,v)?n?n cL(u, v). Define c?R(u, v)
similarly.
? Define k ? k ? k tensor K (left child-parent-
right child trigram counts):
K?i,j,l =
n?
u=1
n?
v=1
n?
w=1
c?(u, v, w)Uw(i)Uu(j)Uv(l)
K(y) = GTLdiag(G
>y)T uRdiag(pi)G
>
where c?(w, u, v) = c(w,u,v)N3 , c(w, u, v) is
the count of bigram (w, u, v) where w is
the left child, u is the parent and v is the
right child in the training sample, and N3 =?
(w,u,v)?n?n?n c(w, u, v).
? Define k?k matrices ?L and ?R (skip-bigram
counts (left child-right child) and (right child-
left child)) 5:
[??L]i,j =
n?
u=1
n?
v=1
n?
w=1
c?(u, v, w)Uw(i)Uu(j)
?L = GTLT
u
Rdiag(pi)G
>
[??R]i,j =
n?
u=1
n?
v=1
n?
w=1
c?(u, v, w)Uw(j)Uu(i)
?R = GTRT
u
Ldiag(pi)G
>
3.3 Parameter estimation
Using the above definitions, we can estimate the pa-
rameters of the model, namely ?,?L,?R,?L,?R
andK, from the training data and define observables
useful for the dependency model as6
c1 = ?
cT? = ?
T??1R
EL = ?L?
?1
R
ER = ?R?
?1
L
D(y) = E?1L K(y)?
?1
R
As we will see, these quantities allow us to recur-
sively compute the marginal probability of the de-
pendency tree, p?(x1, . . . , xm), in a bottom up man-
ner by using belief propagation.
To see this, let hch(i) be the set of hidden chil-
dren of hidden node i (in Figure 2 for instance,
hch(1) = {2, 3}) and let och(i) be the set of ob-
served children of hidden node i (in the same figure
och(i) = {1}). Then compute the marginal proba-
bility p(x1, . . . , xm) from Equation 2 as
ri(h) =
?
j?hch(i)
?j(h)
?
j?och(i)
o(xj |h) (3)
where ?i(h) is defined by summing over all
the hidden random variables i.e., ?i(h) =?
h? p(h
?|h)ri(h?).
This can be written in a compact matrix form as
??ri
> = 1>
?
j?hch(i)
diag(T>dj
??rj )
?
?
j?och(i)
diag(O>?xj ) (4)
5Note than ?R = ?TL , which is not immediately obvious
from the matrix representations.
6The details of the derivation follow directly from the matrix
versions of the variables.
209
where ??ri is a vector of size k (the dimensionality of
the hidden space) of values ri(h). Note that since in
Equation 2 we condition on whether xj is the left or
right child of its parent, we have separate transition
matrices for left and right transitions from a given
hidden node dj ? {L,R}.
The recursive computation can be written in terms
of observables as:
??ri
> = c>?
?
j?hch(i)
D(E>dj
??rj )
?
?
j?och(i)
D((U>U)?1U>?xj )
The final calculation for the marginal probability
of a given sequence is
p?(x1, . . . , xm) =
??r1
>c1 (5)
The spectral estimation procedure is described be-
low in Algorithm 1.
Algorithm 1 Spectral dependency parsing (Comput-
ing marginal probability of a tree.)
1: Input: Training examples- x(i) for i ? {1, . . . ,M}
along with dependency structures where each se-
quence x(i) = x(i)1 , . . . , x
(i)
mi .
2: Compute the spectral parameters ??, ??R, ??L, ??R,
??L, and K?
#Now, for a given sentence, we can recursively com-
pute the following:
3: for x(i)j for j ? {mi, . . . , 1} do
4: Compute:
??ri
> = c>?
?
j?hch(i)
D(E>dj
??rj )
?
?
j?och(i)
D((U>U)?1U>?xj )
5: end for
6: Finally compute
p?(x1, . . . , xmi) =
??r1
>c1
#The marginal probability of an entire tree.
3.4 Sample complexity
Our main theoretical result states that the above
scheme for spectral estimation of marginal proba-
bilities provides a guaranteed consistent estimation
scheme for the marginal probabilities:
Theorem 3.1. Let the sequence {x1, . . . , xm} be
generated by an k ? 2 state HMM. Suppose we are
given a U which has the property that U>O is in-
vertible, and |Uij | ? 1. Suppose we use equation
(5) to estimate the probability based on N indepen-
dent triples. Then
N ? Cm
k2
2
log
(
k
?
)
(6)
where Cm is specified in the appendix, implies that
1?  ?
?
?
?
?
p?(x1, . . . , xm)
p(x1, . . . , xm)
?
?
?
? ? 1 + 
holds with probability at least 1? ?.
Proof. A sketch of the proof, in the case without di-
rectional transition parameters, can be found in the
appendix. The proof with directional transition pa-
rameters is almost identical.
4 Experimental Evaluation
Since our algorithm can score any given tree struc-
ture by computing its marginal probability, a natu-
ral way to benchmark our parser is to generate n-
best dependency trees using some standard parser
and then use our algorithm to re-rank the candidate
dependency trees, e.g. using the log spectral prob-
ability as described in Algorithm 1 as a feature in a
discriminative re-ranker.
4.1 Experimental Setup
Our base parser was the discriminatively trained
MSTParser (McDonald, 2006), which implements
both first and second order parsers and is trained
using MIRA (Crammer et al2006) and used the
standard baseline features as described in McDon-
ald (2006).
We tested our methods on the English Penn Tree-
bank (Marcus et al1993). We use the standard
splits of Penn Treebank; i.e., we used sections 2-21
for training, section 22 for development and section
23 for testing. We used the PennConverter7 tool to
convert Penn Treebank from constituent to depen-
dency format. Following (McDonald, 2006; Koo
7http://nlp.cs.lth.se/software/treebank_
converter/
210
et al2008), we used the POS tagger by Ratnaparkhi
(1996) trained on the full training data to provide
POS tags for development and test sets and used 10-
way jackknifing to generate tags for the training set.
As is common practice we stripped our sentences of
all the punctuation. We evaluated our approach on
sentences of all lengths.
4.2 Details of spectral learning
For the spectral learning phase, we need to just col-
lect word counts from the training data as described
above, so there are no tunable parameters as such.
However, we need to have access to an attribute dic-
tionary U which contains a k dimensional represen-
tation for each word in the corpus. A possible way
of generating U as suggested by Hsu et al2008) is
by performing SVD on bigrams P21 and using the
left eigenvectors as U . We instead used the eigen-
feature dictionary proposed by Dhillon et al2011)
(LR-MVL) which is obtained by performing CCA
on neighboring words and has provably better sam-
ple complexity for rare words compared to the SVD
alternative.
We induced the LR-MVL embeddings for words
using the Reuters RCV1 corpus which contains
about 63 million tokens in 3.3 million sentences and
used their context oblivious embeddings as our esti-
mate of U . We experimented with different choices
of k (the size of the low dimensional projection)
on the development set and found k = 10 to work
reasonably well and fast. Using k = 10 we were
able to estimate our spectral learning parameters
?,?L,R,?L,R,K from the entire training data in un-
der 2 minutes on a 64 bit Intel 2.4 Ghz processor.
4.3 Re-ranking the outputs of MST parser
We could not find any previous work which de-
scribes features for discriminative re-ranking for de-
pendency parsing, which is due to the fact that un-
like constituency parsing, the base parsers for depen-
dency parsing are discriminative (e.g. MST parser)
which obviates the need for re-ranking as one could
add a variety of features to the baseline parser itself.
However, parse re-ranking is a good testbed for our
spectral dependency parser which can score a given
tree. So, we came up with a baseline set of features
to use in an averaged perceptron re-ranker (Collins,
2002). Our baseline features comprised of two main
Method Accuracy Complete
I Order
MST Parser (No RR) 90.8 37.2
RR w. Base. Features 91.3 37.5
RR w. Base. Features +log p? 91.7 37.8
II Order
MST Parser (No RR) 91.8 40.6
RR w. Base. Features 92.4 41.0
RR w. Base. Features +log p? 92.7 41.3
Table 1: (Unlabeled) Dependency Parse re-ranking re-
sults for English test set (Section 23). Note: 1). RR =
Re-ranking 2). Accuracy is the number of words which
correctly identified their parent and Complete is the num-
ber of sentences for which the entire dependency tree was
correct. 3). Base. Features are the two re-ranking fea-
tures described in Section 4.3. 4). log p? is the spectral log
probability feature.
features which capture information that varies across
the different n-best parses and moreover were not
used as features by the baseline MST parser, ?POS-
left-modifier ? POS-head ? POS-right-modifier?
and ?POS-left/right-modifier ? POS-head ? POS-
grandparent?8. In addition to that we used the log of
spectral probability (p?(x1, . . . , xm) - as calculated
using Algorithm 1) as a feature.
We used the MST parser trained on entire training
data to generate a list of n-best parses for the devel-
opment and test sets. The n-best parses for the train-
ing set were generated by 3-fold cross validation,
where we train on 2 folds to get the parses for the
third fold. In all our experiments we used n = 50.
The results are shown in Table 1. As can be seen,
the best results give up to 4.6% reduction in error
over the re-ranker which uses just the baseline set of
features.
5 Discussion and Future Work
Spectral learning of structured latent variable mod-
els in general is a promising direction as has been
shown by the recent interest in this area. It al-
lows us to circumvent the ubiquitous problem of get-
ting stuck in local minima when estimating the la-
tent variable models via EM. In this paper we ex-
8One might be able to come up with better features for de-
pendency parse re-ranking. Our goal in this paper was just to
get a reasonable baseline.
211
tended the spectral learning ideas to learn a simple
yet powerful dependency parser. As future work, we
are working on building an end-to-end parser which
would involve coming up with a spectral version of
the inside-outside algorithm for our setting. We are
also working on extending it to learn more power-
ful grammars e.g. split head-automata grammars
(SHAG) (Eisner and Satta, 1999).
6 Conclusion
In this paper we proposed a novel spectral method
for dependency parsing. Unlike EM trained gen-
erative latent variable models, our method does not
get stuck in local optima, it gives consistent param-
eter estimates, and it is extremely fast to train. We
worked out the theory of a simple yet powerful gen-
erative model and showed how it can be learned us-
ing a spectral method. As a pilot experimental evalu-
ation we showed the efficacy of our approach by us-
ing the spectral probabilities output by our model for
re-ranking the outputs of MST parser. Our method
reduced the error of the baseline re-ranker by up to
a moderate 4.6%.
7 Appendix
This appendix offers a sketch of the proof of The-
orem 1. The proof uses the following definitions,
which are slightly modified from those of Foster
et al2012).
Definition 1. Define ? as the smallest element of ?,
??1, ??1, and K(). In other words,
? ?min{min
i
|?i|,min
i,j
|??1ij |,mini,j
|??1ij |,
min
i,j,k
|Kijk|,min
i,j
|?ij |,min
i,j
|?ij |, }
where Kijk = K(?j)ik are the elements of the ten-
sor K().
Definition 2. Define ?k as the smallest singular
value of ? and ?.
The proof relies on the fact that a row vector mul-
tiplied by a series of matrices, and finally multiplied
by a column vector amounts to a sum over all possi-
ble products of individual entries in the vectors and
matrices. With this in mind, if we bound the largest
relative error of any particular entry in the matrix by,
say, ?, and there are, say, s parameters (vectors and
matrices) being multiplied together, then by simple
algebra the total relative error of the sum over the
products is bounded by ?s.
The proof then follows from two basic steps.
First, one must bound the maximal relative error, ?
for any particular entry in the parameters, which can
be done using central limit-type theorems and the
quantity ? described above. Then, to calculate the
exponent s one simply counts the number of param-
eters multiplied together when calculating the prob-
ability of a particular sequence of observations.
Since each hidden node is associated with exactly
one observed node, it follows that s = 12m + 2L,
where L is the number of levels (for instance in our
example ?Kilroy was here? there are two levels). s
can be easily computed for arbitrary tree topologies.
It follows from Foster et al2012) that we achieve
a sample complexity
N ?
128k2s2
2 ?2?4k
log
(
2k
?
)
?
?1
? ?? ?
2/s2
( s
?
1 + ? 1)2
(7)
leading to the theorem stated above.
Lastly, note that in reality one does not see ? and
?k but instead estimates of these quantities; Foster
et al2012) shows how to incorporate the accuracy
of the estimates into the sample complexity.
Acknowledgement: We would like to thank
Emily Pitler for valuable feedback on the paper.
References
Shay Cohen, Karl Stratos, Michael Collins, Dean
Foster, and Lyle Ungar. Spectral learning of
latent-variable pcfgs. In Association of Compu-
tational Linguistics (ACL), volume 50, 2012.
Michael Collins. Ranking algorithms for named-
entity extraction: boosting and the voted percep-
tron. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguis-
tics, ACL ?02, pages 489?496, Stroudsburg, PA,
USA, 2002. Association for Computational Lin-
guistics. URL http://dx.doi.org/10.
3115/1073083.1073165.
Michael Collins and Terry Koo. Discriminative
reranking for natural language parsing. Comput.
212
Linguist., 31(1):25?70, March 2005. ISSN 0891-
2017.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. Online
passive-aggressive algorithms. Journal of Ma-
chine Learning Research, 7:551?585, 2006.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-
imum likelihood from incomplete data via the em
algorithm. JRSS, SERIES B, 39(1):1?38, 1977.
Paramveer S. Dhillon, Dean Foster, and Lyle Un-
gar. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Process-
ing Systems (NIPS), volume 24, 2011.
Jason Eisner and Giorgio Satta. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 457?464, Univer-
sity of Maryland, June 1999. URL http://cs.
jhu.edu/?jason/papers/#acl99.
Dean Foster, Jordan Rodu, and Lyle Ungar. Spec-
tral dimensionality reduction for HMMs. ArXiV
http://arxiv.org/abs/1203.6130, 2012.
D Hsu, S M. Kakade, and Tong Zhang. A spec-
tral algorithm for learning hidden markov models.
arXiv:0811.4413v2, 2008.
Terry Koo, Xavier Carreras, and Michael Collins.
Simple semi-supervised dependency parsing. In
In Proc. ACL/HLT, 2008.
F. Luque, A. Quattoni, B. Balle, and X. Carreras.
Spectral learning for non-deterministic depen-
dency parsing. In EACL, 2012.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. Building a large annotated
corpus of english: the penn treebank. Comput.
Linguist., 19:313?330, June 1993. ISSN 0891-
2017.
Ryan McDonald. Discriminative learning and span-
ning tree algorithms for dependency parsing. PhD
thesis, University of Pennsylvania, Philadelphia,
PA, USA, 2006. AAI3225503.
Bernard Merialdo. Tagging english text with a prob-
abilistic model. Comput. Linguist., 20:155?171,
June 1994. ISSN 0891-2017.
Gabriele Antonio Musillo and Paola Merlo. Un-
lexicalised hidden variable models of split de-
pendency grammars. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technolo-
gies: Short Papers, HLT-Short ?08, pages 213?
216, Stroudsburg, PA, USA, 2008. Association
for Computational Linguistics.
Ankur P. Parikh, Le Song, and Eric P. Xing. A spec-
tral algorithm for latent tree graphical models. In
ICML, pages 1065?1072, 2011.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433?440, Stroudsburg, PA, USA, 2006. As-
sociation for Computational Linguistics.
Ariadna Quattoni, Michael Collins, and Trevor Dar-
rell. Conditional random fields for object recog-
nition. In In NIPS, pages 1097?1104. MIT Press,
2004.
Lawrence R. Rabiner. A tutorial on hidden markov
models and selected applications in speech recog-
nition. In Proceedings of the IEEE, pages 257?
286, 1989.
Adwait Ratnaparkhi. A Maximum Entropy Model
for Part-Of-Speech Tagging. In Eric Brill and
Kenneth Church, editors, Proceedings of the Em-
pirical Methods in Natural Language Processing,
pages 133?142, 1996.
Sebastiaan Terwijn. On the learnability of hidden
markov models. In Proceedings of the 6th Inter-
national Colloquium on Grammatical Inference:
Algorithms and Applications, ICGI ?02, pages
261?268, London, UK, UK, 2002. Springer-
Verlag. ISBN 3-540-44239-1.
213
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 223?231,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Spectral Learning of Latent-Variable PCFGs
Shay B. Cohen1, Karl Stratos1, Michael Collins1, Dean P. Foster2, and Lyle Ungar3
1Dept. of Computer Science, Columbia University
2Dept. of Statistics/3Dept. of Computer and Information Science, University of Pennsylvania
{scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu
Abstract
We introduce a spectral learning algorithm for
latent-variable PCFGs (Petrov et al, 2006).
Under a separability (singular value) condi-
tion, we prove that the method provides con-
sistent parameter estimates.
1 Introduction
Statistical models with hidden or latent variables are
of great importance in natural language processing,
speech, and many other fields. The EM algorithm is
a remarkably successful method for parameter esti-
mation within these models: it is simple, it is often
relatively efficient, and it has well understood formal
properties. It does, however, have a major limitation:
it has no guarantee of finding the global optimum of
the likelihood function. From a theoretical perspec-
tive, this means that the EM algorithm is not guar-
anteed to give consistent parameter estimates. From
a practical perspective, problems with local optima
can be difficult to deal with.
Recent work has introduced polynomial-time
learning algorithms (and consistent estimation meth-
ods) for two important cases of hidden-variable
models: Gaussian mixture models (Dasgupta, 1999;
Vempala and Wang, 2004) and hidden Markov mod-
els (Hsu et al, 2009). These algorithms use spec-
tral methods: that is, algorithms based on eigen-
vector decompositions of linear systems, in particu-
lar singular value decomposition (SVD). In the gen-
eral case, learning of HMMs or GMMs is intractable
(e.g., see Terwijn, 2002). Spectral methods finesse
the problem of intractibility by assuming separabil-
ity conditions. For example, the algorithm of Hsu
et al (2009) has a sample complexity that is polyno-
mial in 1/?, where ? is the minimum singular value
of an underlying decomposition. These methods are
not susceptible to problems with local maxima, and
give consistent parameter estimates.
In this paper we derive a spectral algorithm
for learning of latent-variable PCFGs (L-PCFGs)
(Petrov et al, 2006; Matsuzaki et al, 2005). Our
method involves a significant extension of the tech-
niques from Hsu et al (2009). L-PCFGs have been
shown to be a very effective model for natural lan-
guage parsing. Under a separation (singular value)
condition, our algorithm provides consistent param-
eter estimates; this is in contrast with previous work,
which has used the EM algorithm for parameter es-
timation, with the usual problems of local optima.
The parameter estimation algorithm (see figure 4)
is simple and efficient. The first step is to take
an SVD of the training examples, followed by a
projection of the training examples down to a low-
dimensional space. In a second step, empirical av-
erages are calculated on the training example, fol-
lowed by standard matrix operations. On test ex-
amples, simple (tensor-based) variants of the inside-
outside algorithm (figures 2 and 3) can be used to
calculate probabilities and marginals of interest.
Our method depends on the following results:
? Tensor form of the inside-outside algorithm.
Section 5 shows that the inside-outside algorithm for
L-PCFGs can be written using tensors. Theorem 1
gives conditions under which the tensor form calcu-
lates inside and outside terms correctly.
? Observable representations. Section 6 shows
that under a singular-value condition, there is an ob-
servable form for the tensors required by the inside-
outside algorithm. By an observable form, we fol-
low the terminology of Hsu et al (2009) in referring
to quantities that can be estimated directly from data
where values for latent variables are unobserved.
Theorem 2 shows that tensors derived from the ob-
servable form satisfy the conditions of theorem 1.
? Estimating the model. Section 7 gives an al-
gorithm for estimating parameters of the observable
representation from training data. Theorem 3 gives a
sample complexity result, showing that the estimates
converge to the true distribution at a rate of 1/
?
M
where M is the number of training examples.
The algorithm is strikingly different from the EM
algorithm for L-PCFGs, both in its basic form, and
in its consistency guarantees. The techniques de-
223
veloped in this paper are quite general, and should
be relevant to the development of spectral methods
for estimation in other models in NLP, for exam-
ple alignment models for translation, synchronous
PCFGs, and so on. The tensor form of the inside-
outside algorithm gives a new view of basic calcula-
tions in PCFGs, and may itself lead to new models.
2 Related Work
For work on L-PCFGs using the EM algorithm, see
Petrov et al (2006), Matsuzaki et al (2005), Pereira
and Schabes (1992). Our work builds on meth-
ods for learning of HMMs (Hsu et al, 2009; Fos-
ter et al, 2012; Jaeger, 2000), but involves sev-
eral extensions: in particular in the tensor form of
the inside-outside algorithm, and observable repre-
sentations for the tensor form. Balle et al (2011)
consider spectral learning of finite-state transducers;
Lugue et al (2012) considers spectral learning of
head automata for dependency parsing. Parikh et al
(2011) consider spectral learning algorithms of tree-
structured directed bayes nets.
3 Notation
Given a matrix A or a vector v, we write A? or v?
for the associated transpose. For any integer n ? 1,
we use [n] to denote the set {1, 2, . . . n}. For any
row or column vector y ? Rm, we use diag(y) to
refer to the (m?m) matrix with diagonal elements
equal to yh for h = 1 . . . m, and off-diagonal ele-
ments equal to 0. For any statement ?, we use [[?]]
to refer to the indicator function that is 1 if ? is true,
and 0 if ? is false. For a random variable X, we use
E[X] to denote its expected value.
We will make (quite limited) use of tensors:
Definition 1 A tensor C ? R(m?m?m) is a set of
m3 parameters Ci,j,k for i, j, k ? [m]. Given a ten-
sor C , and a vector y ? Rm, we define C(y) to be
the (m ? m) matrix with components [C(y)]i,j =
?
k?[m]Ci,j,kyk. Hence C can be interpreted as a
function C : Rm ? R(m?m) that maps a vector
y ? Rm to a matrix C(y) of dimension (m?m).
In addition, we define the tensor C? ? R(m?m?m)
for any tensor C ? R(m?m?m) to have values
[C?]i,j,k = Ck,j,i
Finally, for vectors x, y, z ? Rm, xy?z? is the
tensor D ? Rm?m?m where Dj,k,l = xjykzl (this
is analogous to the outer product: [xy?]j,k = xjyk).
4 L-PCFGs: Basic Definitions
This section gives a definition of the L-PCFG for-
malism used in this paper. An L-PCFG is a 5-tuple
(N ,I,P,m, n) where:
? N is the set of non-terminal symbols in the
grammar. I ? N is a finite set of in-terminals.
P ? N is a finite set of pre-terminals. We assume
that N = I ? P, and I ? P = ?. Hence we have
partitioned the set of non-terminals into two subsets.
? [m] is the set of possible hidden states.
? [n] is the set of possible words.
? For all a ? I , b ? N , c ? N , h1, h2, h3 ? [m],
we have a context-free rule a(h1) ? b(h2) c(h3).
? For all a ? P, h ? [m], x ? [n], we have a
context-free rule a(h) ? x.
Hence each in-terminal a ? I is always the left-
hand-side of a binary rule a ? b c; and each pre-
terminal a ? P is always the left-hand-side of a
rule a ? x. Assuming that the non-terminals in
the grammar can be partitioned this way is relatively
benign, and makes the estimation problem cleaner.
We define the set of possible ?skeletal rules? as
R = {a ? b c : a ? I, b ? N , c ? N}. The
parameters of the model are as follows:
? For each a? b c ? R, and h ? [m], we have
a parameter q(a ? b c|h, a). For each a ? P,
x ? [n], and h ? [m], we have a parameter
q(a ? x|h, a). For each a ? b c ? R, and
h, h? ? [m], we have parameters s(h?|h, a ? b c)
and t(h?|h, a? b c).
These definitions give a PCFG, with rule proba-
bilities
p(a(h1) ? b(h2) c(h3)|a(h1)) =
q(a? b c|h1, a)? s(h2|h1, a? b c)? t(h3|h1, a? b c)
and p(a(h) ? x|a(h)) = q(a? x|h, a).
In addition, for each a ? I , for each h ? [m], we
have a parameter ?(a, h) which is the probability of
non-terminal a paired with hidden variable h being
at the root of the tree.
An L-PCFG defines a distribution over parse trees
as follows. A skeletal tree (s-tree) is a sequence of
rules r1 . . . rN where each ri is either of the form
a ? b c or a ? x. The rule sequence forms
a top-down, left-most derivation under a CFG with
skeletal rules. See figure 1 for an example.
A full tree consists of an s-tree r1 . . . rN , together
with values h1 . . . hN . Each hi is the value for
224
S1
NP2
D3
the
N4
dog
VP5
V6
saw
P7
him
r1 = S ? NP VP
r2 = NP ? D N
r3 = D ? the
r4 = N ? dog
r5 = VP ? V P
r6 = V ? saw
r7 = P ? him
Figure 1: An s-tree, and its sequence of rules. (For con-
venience we have numbered the nodes in the tree.)
the hidden variable for the left-hand-side of rule ri.
Each hi can take any value in [m].
Define ai to be the non-terminal on the left-hand-
side of rule ri. For any i ? {2 . . . N} define pa(i)
to be the index of the rule above node i in the tree.
Define L ? [N ] to be the set of nodes in the tree
which are the left-child of some parent, and R ?
[N ] to be the set of nodes which are the right-child of
some parent. The probability mass function (PMF)
over full trees is then
p(r1 . . . rN , h1 . . . hN ) = ?(a1, h1)
?
N
?
i=1
q(ri|hi, ai)?
?
i?L
s(hi|hpa(i), rpa(i))
?
?
i?R
t(hi|hpa(i), rpa(i)) (1)
The PMF over s-trees is p(r1 . . . rN ) =
?
h1...hN p(r1 . . . rN , h1 . . . hN ).
In the remainder of this paper, we make use of ma-
trix form of parameters of an L-PCFG, as follows:
? For each a? b c ? R, we define Qa?b c ?
Rm?m to be the matrix with values q(a ? b c|h, a)
for h = 1, 2, . . . m on its diagonal, and 0 values for
its off-diagonal elements. Similarly, for each a ? P,
x ? [n], we define Qa?x ? Rm?m to be the matrix
with values q(a ? x|h, a) for h = 1, 2, . . . m on its
diagonal, and 0 values for its off-diagonal elements.
? For each a ? b c ? R, we define Sa?b c ?
Rm?m where [Sa?b c]h?,h = s(h?|h, a? b c).
? For each a ? b c ? R, we define T a?b c ?
Rm?m where [T a?b c]h?,h = t(h?|h, a? b c).
? For each a ? I , we define the vector ?a ? Rm
where [?a]h = ?(a, h).
5 Tensor Form of the Inside-Outside
Algorithm
Given an L-PCFG, two calculations are central:
Inputs: s-tree r1 . . . rN , L-PCFG (N , I,P ,m, n), parameters
? Ca?b c ? R(m?m?m) for all a? b c ? R
? c?a?x ? R(1?m) for all a ? P , x ? [n]
? c1a ? R(m?1) for all a ? I.
Algorithm: (calculate the f i terms bottom-up in the tree)
? For all i ? [N ] such that ai ? P , f i = c?ri
? For all i ? [N ] such that ai ? I, f i = f?Cri(f?) where
? is the index of the left child of node i in the tree, and ?
is the index of the right child.
Return: f1c1a1 = p(r1 . . . rN)
Figure 2: The tensor form for calculation of p(r1 . . . rN ).
1. For a given s-tree r1 . . . rN , calculate
p(r1 . . . rN ).
2. For a given input sentence x = x1 . . . xN , cal-
culate the marginal probabilities
?(a, i, j) =
?
??T (x):(a,i,j)??
p(?)
for each non-terminal a ? N , for each (i, j)
such that 1 ? i ? j ? N .
Here T (x) denotes the set of all possible s-trees for
the sentence x, and we write (a, i, j) ? ? if non-
terminal a spans words xi . . . xj in the parse tree ? .
The marginal probabilities have a number of uses.
Perhaps most importantly, for a given sentence x =
x1 . . . xN , the parsing algorithm of Goodman (1996)
can be used to find
arg max
??T (x)
?
(a,i,j)??
?(a, i, j)
This is the parsing algorithm used by Petrov et al
(2006), for example. In addition, we can calcu-
late the probability for an input sentence, p(x) =
?
??T (x) p(?), as p(x) =
?
a?I ?(a, 1, N).
Variants of the inside-outside algorithm can be
used for problems 1 and 2. This section introduces a
novel form of these algorithms, using tensors. This
is the first step in deriving the spectral estimation
method.
The algorithms are shown in figures 2 and 3. Each
algorithm takes the following inputs:
1. A tensor Ca?b c ? R(m?m?m) for each rule
a? b c.
2. A vector c?a?x ? R(1?m) for each rule a? x.
225
3. A vector c1a ? R(m?1) for each a ? I .
The following theorem gives conditions under
which the algorithms are correct:
Theorem 1 Assume that we have an L-PCFG with
parameters Qa?x, Qa?b c, T a?b c, Sa?b c, ?a, and
that there exist matrices Ga ? R(m?m) for all a ?
N such that each Ga is invertible, and such that:
1. For all rules a? b c, Ca?b c(y) =
GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?1
2. For all rules a? x, c?a?x = 1?Qa?x(Ga)?1
3. For all a ? I , c1a = Ga?a
Then: 1) The algorithm in figure 2 correctly com-
putes p(r1 . . . rN ) under the L-PCFG. 2) The algo-
rithm in figure 3 correctly computes the marginals
?(a, i, j) under the L-PCFG.
Proof: See section 9.1.
6 Estimating the Tensor Model
A crucial result is that it is possible to directly esti-
mate parameters Ca?b c, c?a?x and c1a that satisfy the
conditions in theorem 1, from a training sample con-
sisting of s-trees (i.e., trees where hidden variables
are unobserved). We first describe random variables
underlying the approach, then describe observable
representations based on these random variables.
6.1 Random Variables Underlying the Approach
Each s-tree with N rules r1 . . . rN has N nodes. We
will use the s-tree in figure 1 as a running example.
Each node has an associated rule: for example,
node 2 in the tree in figure 1 has the rule NP? D N.
If the rule at a node is of the form a? b c, then there
are left and right inside trees below the left child and
right child of the rule. For example, for node 2 we
have a left inside tree rooted at node 3, and a right
inside tree rooted at node 4 (in this case the left and
right inside trees both contain only a single rule pro-
duction, of the form a ? x; however in the general
case they might be arbitrary subtrees).
In addition, each node has an outside tree. For
node 2, the outside tree is
S
NP VP
V
saw
P
him
Inputs: Sentence x1 . . . xN , L-PCFG (N , I,P ,m, n), param-
eters Ca?b c ? R(m?m?m) for all a? b c ? R, c?a?x ?
R(1?m) for all a ? P , x ? [n], c1a ? R(m?1) for all a ? I.
Data structures:
? Each ?a,i,j ? R1?m for a ? N , 1 ? i ? j ? N is a
row vector of inside terms.
? Each ?a,i,j ? Rm?1 for a ? N , 1 ? i ? j ? N is a
column vector of outside terms.
? Each ?(a, i, j) ? R for a ? N , 1 ? i ? j ? N is a
marginal probability.
Algorithm:
(Inside base case) ?a ? P , i ? [N ], ?a,i,i = c?a?xi
(Inside recursion) ?a ? I, 1 ? i < j ? N,
?a,i,j =
j?1
?
k=i
?
a?b c
?c,k+1,jCa?b c(?b,i,k)
(Outside base case) ?a ? I, ?a,1,n = c1a
(Outside recursion) ?a ? N , 1 ? i ? j ? N,
?a,i,j =
i?1
?
k=1
?
b?c a
Cb?c a(?c,k,i?1)?b,k,j
+
N
?
k=j+1
?
b?a c
Cb?a c? (?c,j+1,k)?b,i,k
(Marginals) ?a ? N , 1 ? i ? j ? N,
?(a, i, j) = ?a,i,j?a,i,j =
?
h?[m]
?a,i,jh ?
a,i,j
h
Figure 3: The tensor form of the inside-outside algorithm,
for calculation of marginal terms ?(a, i, j).
The outside tree contains everything in the s-tree
r1 . . . rN , excluding the subtree below node i.
Our random variables are defined as follows.
First, we select a random internal node, from a ran-
dom tree, as follows:
? Sample an s-tree r1 . . . rN from the PMF
p(r1 . . . rN ). Choose a node i uniformly at ran-
dom from [N ].
If the rule ri for the node i is of the form a? b c,
we define random variables as follows:
? R1 is equal to the rule ri (e.g., NP ? D N).
? T1 is the inside tree rooted at node i. T2 is the
inside tree rooted at the left child of node i, and T3
is the inside tree rooted at the right child of node i.
? H1,H2,H3 are the hidden variables associated
with node i, the left child of node i, and the right
child of node i respectively.
226
? A1, A2, A3 are the labels for node i, the left
child of node i, and the right child of node i respec-
tively. (E.g., A1 = NP, A2 = D, A3 = N.)
? O is the outside tree at node i.
? B is equal to 1 if node i is at the root of the tree
(i.e., i = 1), 0 otherwise.
If the rule ri for the selected node i is of
the form a ? x, we have random vari-
ables R1, T1,H1, A1, O,B as defined above, but
H2,H3, T2, T3, A2, and A3 are not defined.
We assume a function ? that maps outside trees o
to feature vectors ?(o) ? Rd? . For example, the fea-
ture vector might track the rule directly above the
node in question, the word following the node in
question, and so on. We also assume a function ?
that maps inside trees t to feature vectors ?(t) ? Rd.
As one example, the function ? might be an indica-
tor function tracking the rule production at the root
of the inside tree. Later we give formal criteria for
what makes good definitions of ?(o) of ?(t). One
requirement is that d? ? m and d ? m.
In tandem with these definitions, we assume pro-
jection matices Ua ? R(d?m) and V a ? R(d??m)
for all a ? N . We then define additional random
variables Y1, Y2, Y3, Z as
Y1 = (Ua1)??(T1) Z = (V a1)??(O)
Y2 = (Ua2)??(T2) Y3 = (Ua3)??(T3)
where ai is the value of the random variable Ai.
Note that Y1, Y2, Y3, Z are all in Rm.
6.2 Observable Representations
Given the definitions in the previous section, our
representation is based on the following matrix, ten-
sor and vector quantities, defined for all a ? N , for
all rules of the form a? b c, and for all rules of the
form a? x respectively:
?a = E[Y1Z?|A1 = a]
Da?b c = E
[
[[R1 = a? b c]]Y3Z?Y ?2 |A1 = a
]
d?a?x = E
[
[[R1 = a? x]]Z?|A1 = a
]
Assuming access to functions ? and ?, and projec-
tion matrices Ua and V a, these quantities can be es-
timated directly from training data consisting of a
set of s-trees (see section 7).
Our observable representation then consists of:
Ca?b c(y) = Da?b c(y)(?a)?1 (2)
c?a?x = d?a?x(?a)?1 (3)
c1a = E [[[A1 = a]]Y1|B = 1] (4)
We next introduce conditions under which these
quantities satisfy the conditions in theorem 1.
The following definition will be important:
Definition 2 For all a ? N , we define the matrices
Ia ? R(d?m) and Ja ? R(d??m) as
[Ia]i,h = E[?i(T1) | H1 = h,A1 = a]
[Ja]i,h = E[?i(O) | H1 = h,A1 = a]
In addition, for any a ? N , we use ?a ? Rm to
denote the vector with ?ah = P (H1 = h|A1 = a).
The correctness of the representation will rely on
the following conditions being satisfied (these are
parallel to conditions 1 and 2 in Hsu et al (2009)):
Condition 1 ?a ? N , the matrices Ia and Ja are
of full rank (i.e., they have rank m). For all a ? N ,
for all h ? [m], ?ah > 0.
Condition 2 ?a ? N , the matrices Ua ? R(d?m)
and V a ? R(d??m) are such that the matrices Ga =
(Ua)?Ia and Ka = (V a)?Ja are invertible.
The following lemma justifies the use of an SVD
calculation as one method for finding values for Ua
and V a that satisfy condition 2:
Lemma 1 Assume that condition 1 holds, and for
all a ? N define
?a = E[?(T1) (?(O))? |A1 = a] (5)
Then if Ua is a matrix of the m left singular vec-
tors of ?a corresponding to non-zero singular val-
ues, and V a is a matrix of the m right singular vec-
tors of ?a corresponding to non-zero singular val-
ues, then condition 2 is satisfied.
Proof sketch: It can be shown that ?a =
Iadiag(?a)(Ja)?. The remainder is similar to the
proof of lemma 2 in Hsu et al (2009).
The matrices ?a can be estimated directly from a
training set consisting of s-trees, assuming that we
have access to the functions ? and ?.
We can now state the following theorem:
227
Theorem 2 Assume conditions 1 and 2 are satisfied.
For all a ? N , define Ga = (Ua)?Ia. Then under
the definitions in Eqs. 2-4:
1. For all rules a? b c, Ca?b c(y) =
GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?1
2. For all rules a? x, c?a?x = 1?Qa?x(Ga)?1.
3. For all a ? N , c1a = Ga?a
Proof: The following identities hold (see sec-
tion 9.2):
Da?b c(y) = (6)
GcT a?b cdiag(yGbSa?b c)Qa?b cdiag(?a)(Ka)?
d?a?x = 1?Qa?xdiag(?a)(Ka)? (7)
?a = Gadiag(?a)(Ka)? (8)
c1a = Gapia (9)
Under conditions 1 and 2, ?a is invertible, and
(?a)?1 = ((Ka)?)?1(diag(?a))?1(Ga)?1. The
identities in the theorem follow immediately.
7 Deriving Empirical Estimates
Figure 4 shows an algorithm that derives esti-
mates of the quantities in Eqs 2, 3, and 4. As
input, the algorithm takes a sequence of tuples
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for i ? [M ].
These tuples can be derived from a training set
consisting of s-trees ?1 . . . ?M as follows:
? ?i ? [M ], choose a single node ji uniformly at
random from the nodes in ?i. Define r(i,1) to be the
rule at node ji. t(i,1) is the inside tree rooted at node
ji. If r(i,1) is of the form a? b c, then t(i,2) is the
inside tree under the left child of node ji, and t(i,3)
is the inside tree under the right child of node ji. If
r(i,1) is of the form a ? x, then t(i,2) = t(i,3) =
NULL. o(i) is the outside tree at node ji. b(i) is 1 if
node ji is at the root of the tree, 0 otherwise.
Under this process, assuming that the s-trees
?1 . . . ?M are i.i.d. draws from the distribution
p(?) over s-trees under an L-PCFG, the tuples
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) are i.i.d. draws
from the joint distribution over the random variables
R1, T1, T2, T3, O,B defined in the previous section.
The algorithm first computes estimates of the pro-
jection matrices Ua and V a: following lemma 1,
this is done by first deriving estimates of ?a,
and then taking SVDs of each ?a. The matrices
are then used to project inside and outside trees
t(i,1), t(i,2), t(i,3), o(i) down to m-dimensional vec-
tors y(i,1), y(i,2), y(i,3), z(i); these vectors are used to
derive the estimates of Ca?b c, c?a?x, and c1a.
We now state a PAC-style theorem for the learning
algorithm. First, for a given L-PCFG, we need a
couple of definitions:
? ? is the minimum absolute value of any element
of the vectors/matrices/tensors c1a, d?a?x, Da?b c,
(?a)?1. (Note that ? is a function of the projec-
tion matrices Ua and V a as well as the underlying
L-PCFG.)
? For each a ? N , ?a is the value of the m?th
largest singular value of ?a. Define ? = mina ?a.
We then have the following theorem:
Theorem 3 Assume that the inputs to the algorithm
in figure 4 are i.i.d. draws from the joint distribution
over the random variables R1, T1, T2, T3, O,B, un-
der an L-PCFG with distribution p(r1 . . . rN ) over
s-trees. Define m to be the number of latent states
in the L-PCFG. Assume that the algorithm in fig-
ure 4 has projection matrices U?a and V? a derived as
left and right singular vectors of ?a, as defined in
Eq. 5. Assume that the L-PCFG, together with U?a
and V? a, has coefficients ? > 0 and ? > 0. In addi-
tion, assume that all elements in c1a, d?a?x, Da?b c,
and ?a are in [?1,+1]. For any s-tree r1 . . . rN de-
fine p?(r1 . . . rN ) to be the value calculated by the
algorithm in figure 3 with inputs c?1a, c??a?x, C?a?b c
derived from the algorithm in figure 4. Define R to
be the total number of rules in the grammar of the
form a? b c or a ? x. Define Ma to be the num-
ber of training examples in the input to the algorithm
in figure 4 where ri,1 has non-terminal a on its left-
hand-side. Under these assumptions, if for all a
Ma ?
128m2
( 2N+1?1 + ?? 1
)2 ?2?4
log
(2mR
?
)
Then
1? ? ?
?
?
?
?
p?(r1 . . . rN )
p(r1 . . . rN )
?
?
?
?
? 1 + ?
A similar theorem (omitted for space) states that
1? ? ?
?
?
?
??(a,i,j)
?(a,i,j)
?
?
?
? 1 + ? for the marginals.
The condition that U?a and V? a are derived from
?a, as opposed to the sample estimate ??a, follows
Foster et al (2012). As these authors note, similar
techniques to those of Hsu et al (2009) should be
228
applicable in deriving results for the case where ??a
is used in place of ?a.
Proof sketch: The proof is similar to that of Foster
et al (2012). The basic idea is to first show that
under the assumptions of the theorem, the estimates
c?1a, d??a?x, D?a?b c, ??a are all close to the underlying
values being estimated. The second step is to show
that this ensures that p?(r1...rN? )p(r1...rN? ) is close to 1.
The method described of selecting a single tuple
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for each s-tree en-
sures that the samples are i.i.d., and simplifies the
analysis underlying theorem 3. In practice, an im-
plementation should most likely use all nodes in all
trees in training data; by Rao-Blackwellization we
know such an algorithm would be better than the
one presented, but the analysis of how much better
would be challenging. It would almost certainly lead
to a faster rate of convergence of p? to p.
8 Discussion
There are several potential applications of the
method. The most obvious is parsing with L-
PCFGs.1 The approach should be applicable in other
cases where EM has traditionally been used, for ex-
ample in semi-supervised learning. Latent-variable
HMMs for sequence labeling can be derived as spe-
cial case of our approach, by converting tagged se-
quences to right-branching skeletal trees.
The sample complexity of the method depends on
the minimum singular values of ?a; these singular
values are a measure of how well correlated ? and
? are with the unobserved hidden variable H1. Ex-
perimental work is required to find a good choice of
values for ? and ? for parsing.
9 Proofs
This section gives proofs of theorems 1 and 2. Due
to space limitations we cannot give full proofs; in-
stead we provide proofs of some key lemmas. A
long version of this paper will give the full proofs.
9.1 Proof of Theorem 1
First, the following lemma leads directly to the cor-
rectness of the algorithm in figure 2:
1Parameters can be estimated using the algorithm in
figure 4; for a test sentence x1 . . . xN we can first
use the algorithm in figure 3 to calculate marginals
?(a, i, j), then use the algorithm of Goodman (1996) to find
argmax??T (x)
?
(a,i,j)?? ?(a, i, j).
Inputs: Training examples (r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i))
for i ? {1 . . .M}, where r(i,1) is a context free rule; t(i,1),
t(i,2) and t(i,3) are inside trees; o(i) is an outside tree; and
b(i) = 1 if the rule is at the root of tree, 0 otherwise. A function
? that maps inside trees t to feature-vectors ?(t) ? Rd. A func-
tion ? that maps outside trees o to feature-vectors ?(o) ? Rd? .
Algorithm:
Define ai to be the non-terminal on the left-hand side of rule
r(i,1). If r(i,1) is of the form a? b c, define bi to be the non-
terminal for the left-child of r(i,1), and ci to be the non-terminal
for the right-child.
(Step 0: Singular Value Decompositions)
? Use the algorithm in figure 5 to calculate matrices U?a ?
R(d?m) and V? a ? R(d??m) for each a ? N .
(Step 1: Projection)
? For all i ? [M ], compute y(i,1) = (U?ai)??(t(i,1)).
? For all i ? [M ] such that r(i,1) is of the form
a? b c, compute y(i,2) = (U?bi)??(t(i,2)) and y(i,3) =
(U?ci)??(t(i,3)).
? For all i ? [M ], compute z(i) = (V? ai)??(o(i)).
(Step 2: Calculate Correlations)
? For each a ? N , define ?a = 1/
?M
i=1[[ai = a]]
? For each rule a? b c, compute D?a?b c = ?a ?
?M
i=1[[r(i,1) = a? b c]]y(i,3)(z(i))?(y(i,2))?
? For each rule a ? x, compute d??a?x = ?a ?
?M
i=1[[r(i,1) = a? x]](z(i))?
? For each a ? N , compute ??a = ?a ?
?M
i=1[[ai = a]]y(i,1)(z(i))?
(Step 3: Compute Final Parameters)
? For all a? b c, C?a?b c(y) = D?a?b c(y)(??a)?1
? For all a? x, c??a?x = d??a?x(??a)?1
? For all a ? I, c?1a =
?M
i=1[[ai=a and b(i)=1]]y(i,1)
?M
i=1[[b(i)=1]]
Figure 4: The spectral learning algorithm.
Inputs: Identical to algorithm in figure 4.
Algorithm:
? For each a ? N , compute ??a ? R(d??d) as
??a =
?M
i=1[[ai = a]]?(t(i,1))(?(o(i)))?
?M
i=1[[ai = a]]
and calculate a singular value decomposition of ??a.
? For each a ? N , define U?a ? Rm?d to be a matrix of the left
singular vectors of ??a corresponding to the m largest singular
values. Define V? a ? Rm?d? to be a matrix of the right singular
vectors of ??a corresponding to the m largest singular values.
Figure 5: Singular value decompositions.
229
Lemma 2 Assume that conditions 1-3 of theorem 1
are satisfied, and that the input to the algorithm in
figure 2 is an s-tree r1 . . . rN . Define ai for i ? [N ]
to be the non-terminal on the left-hand-side of rule
ri, and ti for i ? [N ] to be the s-tree with rule ri
at its root. Finally, for all i ? [N ], define the row
vector bi ? R(1?m) to have components
bih = P (Ti = ti|Hi = h,Ai = ai)
for h ? [m]. Then for all i ? [N ], f i = bi(G(ai))?1.
It follows immediately that
f1c1a1 = b
1(G(a1))?1Ga1?a1 = p(r1 . . . rN )
This lemma shows a direct link between the vec-
tors f i calculated in the algorithm, and the terms bih,
which are terms calculated by the conventional in-
side algorithm: each f i is a linear transformation
(through Gai) of the corresponding vector bi.
Proof: The proof is by induction.
First consider the base case. For any leaf?i.e., for
any i such that ai ? P?we have bih = q(ri|h, ai),
and it is easily verified that f i = bi(G(ai))?1.
The inductive case is as follows. For all i ? [N ]
such that ai ? I , by the definition in the algorithm,
f i = f?Cri(f?)
= f?Ga?T ridiag(f?Ga?Sri)Qri(Gai)?1
Assuming by induction that f? = b?(G(a? ))?1 and
f? = b?(G(a?))?1, this simplifies to
f i = ?rdiag(?l)Qri(Gai)?1 (10)
where ?r = b?T ri , and ?l = b?Sri . ?r is a row
vector with components ?rh =
?
h??[m] b
?
h?T
ri
h?,h =
?
h??[m] b
?
h?t(h?|h, ri). Similarly, ?l is a row vector
with components equal to ?lh =
?
h??[m] b
?
h?S
ri
h?,h =
?
h??[m] b
?
h?s(h?|h, ri). It can then be verified that
?rdiag(?l)Qri is a row vector with components
equal to ?rh?lhq(ri|h, ai).
But bih = q(ri|h, ai)?
(
?
h??[m] b
?
h?t(h?|h, ri)
)
?
(
?
h??[m] b
?
h?s(h?|h, ri)
)
= q(ri|h, ai)?rh?lh, hence
?rdiag(?l)Qri = bi and the inductive case follows
immediately from Eq. 10.
Next, we give a similar lemma, which implies the
correctness of the algorithm in figure 3:
Lemma 3 Assume that conditions 1-3 of theorem 1
are satisfied, and that the input to the algorithm in
figure 3 is a sentence x1 . . . xN . For any a ? N , for
any 1 ? i ? j ? N , define ??a,i,j ? R(1?m) to have
components ??a,i,jh = p(xi . . . xj|h, a) for h ? [m].
In addition, define ??a,i,j ? R(m?1) to have compo-
nents ??a,i,jh = p(x1 . . . xi?1, a(h), xj+1 . . . xN ) for
h ? [m]. Then for all i ? [N ], ?a,i,j = ??a,i,j(Ga)?1
and ?a,i,j = Ga??a,i,j . It follows that for all (a, i, j),
?(a, i, j) = ??a,i,j(Ga)?1Ga??a,i,j = ??a,i,j ??a,i,j
=
?
h
??a,i,jh ??
a,i,j
h =
?
??T (x):(a,i,j)??
p(?)
Thus the vectors ?a,i,j and ?a,i,j are linearly re-
lated to the vectors ??a,i,j and ??a,i,j , which are the
inside and outside terms calculated by the conven-
tional form of the inside-outside algorithm.
The proof is by induction, and is similar to the
proof of lemma 2; for reasons of space it is omitted.
9.2 Proof of the Identity in Eq. 6
We now prove the identity in Eq. 6, used in the proof
of theorem 2. For reasons of space, we do not give
the proofs of identities 7-9: the proofs are similar.
The following identities can be verified:
P (R1 = a? b c|H1 = h,A1 = a) = q(a? b c|h, a)
E [Y3,j|H1 = h,R1 = a? b c] = Ea?b cj,h
E [Zk|H1 = h,R1 = a? b c] = Kak,h
E [Y2,l|H1 = h,R1 = a? b c] = F a?b cl,h
where Ea?b c = GcT a?b c, F a?b c = GbSa?b c.
Y3, Z and Y2 are independent when conditioned
on H1, R1 (this follows from the independence as-
sumptions in the L-PCFG), hence
E [[[R1 = a? b c]]Y3,jZkY2,l | H1 = h,A1 = a]
= q(a? b c|h, a)Ea?b cj,h Kak,hF a?b cl,h
Hence (recall that ?ah = P (H1 = h|A1 = a)),
Da?b cj,k,l = E [[[R1 = a? b c]]Y3,jZkY2,l | A1 = a]
=
?
h
?ahE [[[R1 = a? b c]]Y3,jZkY2,l | H1 = h,A1 = a]
=
?
h
?ahq(a? b c|h, a)Ea?b cj,h Kak,hF a?b cl,h (11)
from which Eq. 6 follows.
230
Acknowledgements: Columbia University gratefully ac-
knowledges the support of the Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not nec-
essarily reflect the view of DARPA, AFRL, or the US
government. Shay Cohen was supported by the National
Science Foundation under Grant #1136996 to the Com-
puting Research Association for the CIFellows Project.
Dean Foster was supported by National Science Founda-
tion grant 1106743.
References
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
Proceedings of FOCS.
Dean P. Foster, Jordan Rodu, and Lyle H. Ungar.
2012. Spectral dimensionality reduction for hmms.
arXiv:1203.6130v1.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the 34th annual meeting on Associ-
ation for Computational Linguistics, pages 177?183.
Association for Computational Linguistics.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden Markov models. In
Proceedings of COLT.
H. Jaeger. 2000. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6).
F. M. Lugue, A. Quattoni, B. Balle, and X. Carreras.
2012. Spectral learning for non-deterministic depen-
dency parsing. In Proceedings of EACL.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 75?82. Association for
Computational Linguistics.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral al-
gorithm for latent tree graphical models. In Proceed-
ings of The 28th International Conference on Machine
Learningy (ICML 2011).
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In Proceed-
ings of the 30th Annual Meeting of the Association for
Computational Linguistics, pages 128?135, Newark,
Delaware, USA, June. Association for Computational
Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 433?440, Sydney, Australia, July.
Association for Computational Linguistics.
S. A. Terwijn. 2002. On the learnability of hidden
markov models. In Grammatical Inference: Algo-
rithms and Applications (Amsterdam, 2002), volume
2484 of Lecture Notes in Artificial Intelligence, pages
261?268, Berlin. Springer.
S. Vempala and G. Wang. 2004. A spectral algorithm for
learning mixtures of distributions. Journal of Com-
puter and System Sciences, 68(4):841?860.
231
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 679?683,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Penn: Using Word Similarities to better Estimate Sentence Similarity
Sneha Jha and H. Andrew Schwartz and Lyle H. Ungar
University of Pennsylvania
Philadelphia, PA, USA
{jhasneha, hansens, ungar}@seas.upenn.edu
Abstract
We present the Penn system for SemEval-
2012 Task 6, computing the degree of seman-
tic equivalence between two sentences. We
explore the contributions of different vector
models for computing sentence and word sim-
ilarity: Collobert and Weston embeddings as
well as two novel approaches, namely eigen-
words and selectors. These embeddings pro-
vide different measures of distributional simi-
larity between words, and their contexts. We
used regression to combine the different simi-
larity measures, and found that each provides
partially independent predictive signal above
baseline models.
1 Introduction
We compute the semantic similarity between pairs
of sentences by combining a set of similarity met-
rics at various levels of depth, from surface word
similarity to similarities derived from vector mod-
els of word or sentence meaning. Regression is then
used to determine optimal weightings of the differ-
ent similarity measures. We use this setting to as-
sess the contributions from several different word
embeddings.
Our system is based on similarities computed us-
ing multiple sets of features: (a) naive lexical fea-
tures, (b) similarity between vector representations
of sentences, and (c) similarity between constituent
words computed using WordNet, using the eigen-
word vector representations of words , and using se-
lectors, which generalize words to a set of words that
appear in the same context.
2 System Description
This section briefly describes the feature sets used to
arrive at a similarity measure between sentences. We
compare the use of word similarities based on three
different embeddings for words neural embeddings
using recursive autoencoders, eigenwords and selec-
tors.
2.1 Neural Models of Word Representation
An increasingly popular approach is to learn repre-
sentational embeddings for words from a large col-
lection of unlabeled data (typically using a genera-
tive model), and to use these embeddings to augment
the feature set of a supervised learner. These models
are based on the distributional hypothesis in linguis-
tics that words that occur in similar contexts tend
to have similar meanings. The similarities between
these vectors indicate similarity in the meanings of
corresponding words.
The state of the art model in paraphrase detection
uses an unsupervised recursive autoencoder (RAE)
model based on an unfolding objective that learn
feature vectors for phrases in syntactic parse trees
(Socher et al, 2011). The idea of neural language
models is to jointly learn an embedding of words
into an n-dimensional vector space that capture dis-
tributional syntactic and semantic information via
the words co-occurrence statistics. Further details
and evaluations of these embeddings are discussed
in Turian et al (2010).
Once the distributional syntactic and semantic
matrix is learned on an unlabeled corpus, one can
use it for subsequent tasks by using each words vec-
tor to represent that word. For initial word embed-
dings, we used the 100-dimensional vectors com-
679
puted via the unsupervised method of Collobert and
Weston (2008). These word embeddings are matri-
ces of size |V | ? n where |V | is the size of the vo-
cabulary and n is the dimensionality of the semantic
space. This matrix usually captures co-occurrence
statistics and its values are learned. We used the
embeddings provided by Socher et al (2011). Al-
though the original paper employed a dynamic pool-
ing layer in addition to the RAE that captures the
global structure of the similarity matrix, we found
the resulting sentence-level RAE itself was useful.
In turn, we use these vector representations at the
sentence level where the cosine similarity between
the sentence vectors serves as a measure of sentence
similarity. All parameters for the RAE layer are kept
same as described by Socher et al (2011).
2.2 Eigenword Similarity
Recent spectral methods use large amounts of un-
labeled data to learn word representations, which
can then be used as features in supervised learners
for linguistic tasks. Eigenwords, a spectral method
for computing word embeddings based on context
words that characterize the meanings of words, can
be efficiently computed by a set of methods based on
singular value decomposition (Dhillon et al, 2011).
Such representations are dense, low dimensional
and real-valued like the vector representations in the
previous section except that they are induced us-
ing eigen-decomposition of the word co-occurrence
matrix instead of neural networks. This method
uses Canonical Correlation Analysis (CCA) be-
tween words and their immediate contexts to es-
timate word representations from unlabeled data.
CCA is the analog to Principal Component Analysis
(PCA) for pairs of matrices. It computes the direc-
tions of maximal correlation between a pair of matri-
ces. CCAs invariance to linear data transformations
enables proofs showing that keeping the dominant
singular vectors faithfully captures any state infor-
mation. (For this work, we used the Google n-gram
collection of web three-grams as the unlabeled data.)
Each dimension of these representations captures la-
tent information about a combination of syntactic
and semantic word properties. In the original paper,
the word embeddings are context-specific. For this
task, we only use context-oblivious embeddings i.e.
one embedding per word type for this task, based
on their model. Word similarity can then be cal-
culated as cosine similarity between the eigenword
representation vectors for any two words.
To move from word-level similarity to sentence-
level a few more steps are necessary. We adapted
the method of matrix similarity given by Stevenson
and Greenwood (2005). One calculates similarity
between all pairs of words, and each sentence is rep-
resented as a binary vector (with elements equal to 1
if a word is present and 0 otherwise). The similarity
between these sentences vectors ~a and~b is given by:
s(~a,~b) =
~aW~b
|~a||~b|
(1)
where W is a semantic similarity matrix contain-
ing information about the similarity of word pairs.
Each element in matrix W represents the similarity
of words according to some lexical or spectral simi-
larity measure.
2.3 Selector Similarity
Another novel method to account for the similarity
between words is via comparison of Web selectors
(Schwartz and Gomez, 2008). Selectors are words
that take the place of an instance of a target word
within its local context. For example, in ?he ad-
dressed the strikers at the rally?, selectors for ?strik-
ers? might be ?crowd?, ?audience?, ?workers?, or ?stu-
dents? words which can realize the same constituent
position as the target word. Since selectors are de-
termined based on the context, a set of selectors is an
abstraction for the context of a word instance. Thus,
comparing selector sets produces a measure of word
instance similarity. A key difference between selec-
tors and the eigenwords used in this paper are that
selectors are instance specific. This has the benefit
that selectors can distinguish word senses, but the
drawback that each word instance requires its own
set of selectors to be acquired.
Although selectors have previously only been
used for worse sense disambiguation, one can also
use them to compute similarity between two word
instances by taking the cosine similarity of vectors
containing selectors for each instance. In our case,
we compute the cosine similarity for each pair of
noun instances and populate the semantic similarity
matrix in formula (1) to generate a sentence-level
680
similarity estimate. Combining web selector- based
word similarity features with the word embeddings
from the neural model gave us the best overall per-
formance on the aggregated view of the data sets.
2.4 Other Similarity Metrics
Knowledge-Based. We use WordNet to calculate
semantic distances between all open-class words in
the sentence pairs. There are three classifications
of similarity metrics over WordNet: path-based,
information- content based, and gloss-based (Ped-
erson et al, 2004). We chose to incorporate those
measures performing best in the Schwartz & Gomez
(2011) application-oriented evaluation: (a) the path-
based measure of Schwartz & Gomez (2008); (b)
the information-content measure of Jiang & Conrath
(1997) utilizing the difference in information con-
tent between concepts and their point of intersection;
(c) the gloss-based measure of Patwardhan & Peder-
sen (2006). By including metrics utilizing different
sources of information, we suspect they will each
have something novel to contribute.
Because WordNet provides similarity between
concepts (word senses), we take the maximum simi-
larity between all senses of each word to be the sim-
ilarity between the two words. Such similarity can
then be computed between multiple pairs of words
to populate the semantic similarity matrix W in for-
mula (1) and generate sentence-level similarity esti-
mates as described above. The information-content
and path-based measures are restricted to compar-
ing nouns and verbs and only across the same part
of speech. On the other hand, the gloss-based mea-
sure, which relies on connections through concept
definitions, is more general and can compare words
across parts of speech.
Surface Metrics. We added the following set of
lexical features to incorporate some surface infor-
mation lost in the vector-based representations.
? difference in the lengths of the two sentences
? average length of the sentences
? number of common words based on exact
string match
? number of content words in common
? number of common words in base form
? number of similar numerals in the sentences
3 Evaluation and Results
We combine the similarity metrics discussed previ-
ously via regression (Pedregosa et al, 2011). We
included the following sets of features:
? System-baseline: surface metrics, knowledge-
based metrics. (discussed in section 2.4).
? Neu: Neural Model similarity (section 2.1)
? Ew: Eigenword similarity (section 2.2)
? Sel: Selector similarity (section 2.3)
To capture possible non-linear relations, we added
a squared and square-rooted column corresponding
to each feature in the feature matrix. We also tried
to combine all the features to form composite mea-
sures by defining multiple interaction terms. Both
these sets of additional features improved the per-
formance of our regression model. We used all fea-
tures to train both a linear regression model and a
regularized model based on ridge regression. The
regularization parameter for ridge regression was set
via cross-validation over the training set. All pre-
dictions of similarity values were capped within the
range [0,1]. Our systems were trained on the follow-
ing data sets:
? MSR-Paraphrase, Microsoft Research Para-
phrase Corpus-750 pairs of sentences.
? MSR-Video, Microsoft Research Video De-
scription Corpus-750 pairs of sentences.
? SMT-Europarl, WMT2008 development data
set (Europarl section)-734 pairs of sentences.
Our performance in the official submission for the
SemEval task can be seen in Table 1. LReg indi-
cates the run with linear regression, ELReg adds
the eigenwords feature and ERReg also uses eigen-
words but with ridge regression. At the time of sub-
mission, we were not ready to test with the selector
features yet. Ridge regression consistently outper-
formed linear regression for every run of our sys-
tem, but overall Pearson score for our system using
linear regression scored the highest. Table 2 presents
a more thorough examination of results.
681
MSRpar MSRvid SMT-eur On-WN SMT-news ALLnrm Mean ALL
task-baseline .4334 .2996 .4542 .5864 .3908 .6732 (85) .4356 (70) .3110 (87)
LReg .5460 .7818 .3547 .5969 .4137 .8043 (36) .5699 (41) .6497 (33)
ELReg .5480 .7844 .3513 .6040 .3607 .8048 (34) .5654 (44) .6622 (27)
ERReg .5610 .7857 .3568 .6214 .3732 .8083 (28) .5755 (37) .6573 (28)
Table 1: Pearson?s r scores for the official submission. ALLnrm: Pearson correlation after the system outputs for each
dataset are fitted to the gold standard using least squares, and corresponding rank. Mean: Weighted mean across the
5 datasets, where the weight depends on the number of pairs in the dataset. ALL: Pearson correlation with the gold
standard for the five datasets, and corresponding rank. Parentheses indicate official rank out of 87 systems.
MSRpar MSRvid SMT-eur On-WN SMT-news Mean ALL
system-baseline .5143 .7736 .3574 .5017 .3867 .5343 .6542
+Neu .5243 .7811 .3772 .4860 .3410 .5318 .6643
+Ew .5267 .7787 .3853 .5237 .4495 .5560 .6724
+Sel .4973 .7684 .3129 .4812 .4016 .5306 .6492
+Neu, +Ew .5481 .7831 .2751 .5576 .3424 .5404 .6647
+Neu, +Sel .5230 .7775 .3724 .5327 .3787 .5684 .6818
+Ew, +Sel .5239 .7728 .2842 .5191 .4038 .5320 .6554
+Neu, +Ew, +Sel .5441 .7835 .2644 .5877 .3578 .5472 .6645
Table 2: Pearson?s r scores for runs based on various combinations of features. Mean: Weighted mean across the 5
datasets, where the weight depends on the number of pairs in the dataset. ALL: Pearson correlation with the gold
standard for the five datasets, and corresponding rank.
Discussion. In the aggregate, we see that each of
the similarity metrics has the ability to improve re-
sults when used with the right combination of other
features. For example, while selector similarity by
itself does not seem to help overall, using this met-
ric in conjunction with the neural model of similar-
ity gives us our best results. Interestingly, the oppo-
site is true of eigenword similarity, where the best
results are seen when they are independent of selec-
tors or the neural models. The decreased correla-
tions can be accounted for by the new features intro-
ducing over fitting, and one should note that no such
reductions in performance are significant compared
to the baseline, where as our best performance is a
significant (p < 0.05) improvement.
There are a few potential directions for future im-
provements. We did not tune our system differently
for different data sets although there is evidence of
specific features favoring certain data sets. In the
case of the neural model of similarity we expect
that deriving phrase level representations from the
sentences and utilizing the dynamic pooling layer
should give us a more thorough measure of simi-
larity beyond the sentence-level vectors we used in
this work. For eigenwords, we would like to experi-
ment with context-aware vectors as was described in
(Dhillon et. al, 2011). Lastly, we were only able to
acquire selectors for nouns, but we believe introduc-
ing selectors for other parts of speech will increase
the power of the selector similarity metric.
4 Conclusion
In this paper, we described two novel word-level
similarity metrics, namely eigenword similarity and
selector similarity, that leverage Web-scale corpora
in order to build word-level vector representations.
Additionally, we explored the use of a vector-model
at the sentence-level by unfolding a neural model of
semantics. We utilized these metrics in addition to
knowledge-based similarity, and surface-level simi-
larity metrics in a regression system to estimate sim-
ilarity at the sentence level. The performance of the
features varies significantly across corpora but at the
aggregate, eigenword similarity, selector similarity,
and the neural model of similarity all are shown to be
capable of improving performance beyond standard
surface-level and WordNet similarity metrics alone.
682
References
Eneko Agirre, Daniel Cer, Mona Diab and Aitor
Gonzalez. 2012. The SemEval-2012 Task-6 : A
Pilot on Semantic Textual Similarity. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012).
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing : deep
neural networks with multitask learning. In Inter-
national Conference on Machine Learning. Pages
160-167.
Paramveer Dhillon, Dean Foster and Lyle Ungar.
2011. Multiview learning of word embeddings via
CCA. In Proceedings of Neural Information Pro-
cessing Systems.
Jay Jiang and David Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference
on Research in Computational Linguistics, pages
1933.
Dekang Lin. 1997. Using syntactic dependency as
local context to resolve word sense ambiguity. In
Proceedings of the 35th annual meeting of Associa-
tion for Computational Linguistics, pages 64-71.
Ted Pedersen, Siddharth Patwardhan and Jason
Michelizzi. 2004. WordNet::Similarity-measuring
the relatedness of concepts. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics.
F. Pedregosa, G. Varoquaux, A. Gramfort, V.
Michel, B. Thirion, G. Grisel, M. Blondel, P. Pret-
tenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A.
Passos, D. Cournapeau, M. Brucher, M. Perrot, E.
Duchesnay. 2011. Scikit-learn: Machine Learning
in Python. Journal of Machine Learning Research.
Vol 12.2825-2830
Hansen A. Schwartz and Fernando Gomez. 2008.
Acquiring knowledge from the web to be used as se-
lectors for noun sense disambiguation. In Proceed-
ings of the Twelfth Conference on Computational
Natural Language Learning.
Hansen A. Schwartz and Fernando Gomez. 2011.
Evaluating semantic metrics on tasks of concept
similarity. In Proceedings of the twenty-fourth
Florida Artificial Intelligence Research Society.
Palm Beach, Florida: AAAI Press.
Richard Socher, Eric H. Huang, Jeffrey Penning-
ton, Andrew Y. Ng and Christopher Manning. 2011.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In Advances in
Neural Information Processing Systems.
Mark Stevenson and Mark A. Greenwood. 2005. A
Semantic Approach to IE Pattern Induction. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 379386.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and general
method for semi-supervised learning. In Proceed-
ings of the annual meeting of Association for Com-
putational Linguistics.
683
