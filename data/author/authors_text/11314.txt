Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 56?64,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Discriminative Latent Variable Chinese Segmenter
with Hybrid Word/Character Information
Xu Sun
Department of Computer Science
University of Tokyo
sunxu@is.s.u-tokyo.ac.jp
Yaozhong Zhang
Department of Computer Science
University of Tokyo
yaozhong.zhang@is.s.u-tokyo.ac.jp
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
matuzaki@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka
School of Computer Science
University of Manchester
yoshimasa.tsuruoka@manchester.ac.uk
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo, Japan
School of Computer Science, University of Manchester, UK
National Centre for Text Mining, UK
tsujii@is.s.u-tokyo.ac.jp
Abstract
Conventional approaches to Chinese word
segmentation treat the problem as a character-
based tagging task. Recently, semi-Markov
models have been applied to the problem, in-
corporating features based on complete words.
In this paper, we propose an alternative, a
latent variable model, which uses hybrid in-
formation based on both word sequences and
character sequences. We argue that the use of
latent variables can help capture long range
dependencies and improve the recall on seg-
menting long words, e.g., named-entities. Ex-
perimental results show that this is indeed the
case. With this improvement, evaluations on
the data of the second SIGHAN CWS bakeoff
show that our system is competitive with the
best ones in the literature.
1 Introduction
For most natural language processing tasks, words
are the basic units to process. Since Chinese sen-
tences are written as continuous sequences of char-
acters, segmenting a character sequence into a word
sequence is the first step for most Chinese process-
ing applications. In this paper, we study the prob-
lem of Chinese word segmentation (CWS), which
aims to find these basic units (words1) for a given
sentence in Chinese.
Chinese character sequences are normally am-
biguous, and out-of-vocabulary (OOV) words are a
major source of the ambiguity. Typical examples
of OOV words include named entities (e.g., orga-
nization names, person names, and location names).
Those named entities may be very long, and a dif-
ficult case occurs when a long word W (|W | ? 4)
consists of some words which can be separate words
on their own; in such cases an automatic segmenter
may split the OOV word into individual words. For
example,
(Computer Committee of International Federation of
Automatic Control) is one of the organization names
in the Microsoft Research corpus. Its length is 13
and it contains more than 6 individual words, but it
should be treated as a single word. Proper recogni-
tion of long OOV words are meaningful not only for
word segmentation, but also for a variety of other
purposes, e.g., full-text indexing. However, as is il-
lustrated, recognizing long words (without sacrific-
ing the performance on short words) is challenging.
Conventional approaches to Chinese word seg-
mentation treat the problem as a character-based la-
1Following previous work, in this paper, words can also refer
to multi-word expressions, including proper names, long named
entities, idioms, etc.
56
beling task (Xue, 2003). Labels are assigned to each
character in the sentence, indicating whether the
character xi is the start (Labeli = B), middle or end
of a multi-character word (Labeli = C). A popu-
lar discriminative model that have been used for this
task is the conditional random fields (CRFs) (Laf-
ferty et al, 2001), starting with the model of Peng
et al (2004). In the Second International Chinese
Word Segmentation Bakeoff (the second SIGHAN
CWS bakeoff) (Emerson, 2005), two of the highest
scoring systems in the closed track competition were
based on a CRF model (Tseng et al, 2005; Asahara
et al, 2005).
While the CRF model is quite effective compared
with other models designed for CWS, it may be lim-
ited by its restrictive independence assumptions on
non-adjacent labels. Although the window can in
principle be widened by increasing the Markov or-
der, this may not be a practical solution, because
the complexity of training and decoding a linear-
chain CRF grows exponentially with the Markov or-
der (Andrew, 2006).
To address this difficulty, a choice is to relax the
Markov assumption by using the semi-Markov con-
ditional random field model (semi-CRF) (Sarawagi
and Cohen, 2004). Despite the theoretical advan-
tage of semi-CRFs over CRFs, however, some pre-
vious studies (Andrew, 2006; Liang, 2005) explor-
ing the use of a semi-CRF for Chinese word seg-
mentation did not find significant gains over the
CRF ones. As discussed in Andrew (2006), the rea-
son may be that despite the greater representational
power of the semi-CRF, there are some valuable fea-
tures that could be more naturally expressed in a
character-based labeling model. For example, on
a CRF model, one might use the feature ?the cur-
rent character xi is X and the current label Labeli
is C?. This feature may be helpful in CWS for gen-
eralizing to new words. For example, it may rule
out certain word boundaries if X were a character
that normally occurs only as a suffix but that com-
bines freely with some other basic forms to create
new words. This type of features is slightly less nat-
ural in a semi-CRF, since in that case local features
?(yi, yi+1, x) are defined on pairs of adjacent words.
That is to say, information about which characters
are not on boundaries is only implicit. Notably, ex-
cept the hybrid Markov/semi-Markov system in An-
drew (2006)2, no other studies using the semi-CRF
(Sarawagi and Cohen, 2004; Liang, 2005; Daume?
III and Marcu, 2005) experimented with features of
segmenting non-boundaries.
In this paper, instead of using semi-Markov mod-
els, we describe an alternative, a latent variable
model, to learn long range dependencies in Chi-
nese word segmentation. We use the discrimina-
tive probabilistic latent variable models (DPLVMs)
(Morency et al, 2007; Petrov and Klein, 2008),
which use latent variables to carry additional infor-
mation that may not be expressed by those original
labels, and therefore try to build more complicated
or longer dependencies. This is especially meaning-
ful in CWS, because the used labels are quite coarse:
Label(y) ? {B,C}, where B signifies beginning a
word and C signifies the continuation of a word.3
For example, by using DPLVM, the aforementioned
feature may turn to ?the current character xi is X ,
Labeli = C, and LatentV ariablei = LV ?. The
current latent variable LV may strongly depend on
the previous one or many latent variables, and there-
fore we can model the long range dependencies
which may not be captured by those very coarse la-
bels. Also, since character and word information
have their different advantages in CWS, in our latent
variable model, we use hybrid information based on
both character and word sequences.
2 A Latent Variable Segmenter
2.1 Discriminative Probabilistic Latent
Variable Model
Given data with latent structures, the task is to
learn a mapping between a sequence of observa-
tions x = x1, x2, . . . , xm and a sequence of labels
y = y1, y2, . . . , ym. Each yj is a class label for the
j?th character of an input sequence, and is a mem-
ber of a set Y of possible class labels. For each se-
quence, the model also assumes a sequence of latent
variables h = h1, h2, . . . , hm, which is unobserv-
able in training examples.
The DPLVM is defined as follows (Morency et al,
2The system was also used in Gao et al (2007), with an
improved performance in CWS.
3In practice, one may add a few extra labels based on lin-
guistic intuitions (Xue, 2003).
57
2007):
P (y|x,?) =?
h
P (y|h,x,?)P (h|x,?), (1)
where ? are the parameters of the model. DPLVMs
can be seen as a natural extension of CRF models,
and CRF models can be seen as a special case of
DPLVMs that have only one latent variable for each
label.
To make the training and inference efficient, the
model is restricted to have disjoint sets of latent vari-
ables associated with each class label. Each hj is a
member in a set Hyj of possible latent variables for
the class label yj . H is defined as the set of all pos-
sible latent variables, i.e., the union of all Hyj sets.
Since sequences which have any hj /? Hyj will by
definition have P (y|x,?) = 0, the model can be
further defined4 as:
P (y|x,?) = ?
h?Hy1?...?Hym
P (h|x,?), (2)
where P (h|x,?) is defined by the usual conditional
random field formulation:
P (h|x,?) = exp??f(h,x)?
?h exp??f(h,x)
, (3)
in which f(h,x) is a feature vector. Given a training
set consisting of n labeled sequences, (xi,yi), for
i = 1 . . . n, parameter estimation is performed by
optimizing the objective function,
L(?) =
n?
i=1
log P (yi|xi,?) ? R(?). (4)
The first term of this equation is the conditional log-
likelihood of the training data. The second term is
a regularizer that is used for reducing overfitting in
parameter estimation.
For decoding in the test stage, given a test se-
quence x, we want to find the most probable label
sequence, y?:
y? = argmaxyP (y|x,??). (5)
For latent conditional models like DPLVMs, the best
label path y? cannot directly be produced by the
4It means that Eq. 2 is from Eq. 1 with additional definition.
Viterbi algorithm because of the incorporation of
hidden states. In this paper, we use a technique
based on A? search and dynamic programming de-
scribed in Sun and Tsujii (2009), for producing the
most probable label sequence y? on DPLVM.
In detail, an A? search algorithm5 (Hart et al,
1968) with a Viterbi heuristic function is adopted to
produce top-n latent paths, h1,h2, . . .hn. In addi-
tion, a forward-backward-style algorithm is used to
compute the exact probabilities of their correspond-
ing label paths, y1,y2, . . .yn. The model then tries
to determine the optimal label path based on the
top-n statistics, without enumerating the remaining
low-probability paths, which could be exponentially
enormous.
The optimal label path y? is ready when the fol-
lowing ?exact-condition? is achieved:
P (y1|x,?) ? (1 ?
?
yk?LPn
P (yk|x,?)) ? 0, (6)
where y1 is the most probable label sequence in
current stage. It is straightforward to prove that
y? = y1, and further search is unnecessary. This
is because the remaining probability mass, 1 ??
yk?LPn P (yk|x,?), cannot beat the current op-timal label path in this case. For more details of the
inference, refer to Sun and Tsujii (2009).
2.2 Hybrid Word/Character Information
We divide our main features into two types:
character-based features and word-based features.
The character-based features are indicator functions
that fire when the latent variable label takes some
value and some predicate of the input (at a certain
position) corresponding to the label is satisfied. For
each latent variable label hi (the latent variable la-
bel at position i), we use the predicate templates as
follows:
? Input characters/numbers/letters locating at po-
sitions i ? 2, i ? 1, i, i + 1 and i + 2
? The character/number/letter bigrams locating
at positions i ? 2, i ? 1, i and i + 1
5A? search and its variants, like beam-search, are widely
used in statistical machine translation. Compared to other
search techniques, an interesting point of A? search is that it
can produce top-n results one-by-one in an efficient manner.
58
? Whether xj and xj+1 are identical, for j = (i?
2) . . . (i + 1)
? Whether xj and xj+2 are identical, for j = (i?
3) . . . (i + 1)
The latter two feature templates are designed to de-
tect character or word reduplication, a morphologi-
cal phenomenon that can influence word segmenta-
tion in Chinese.
The word-based features are indicator functions
that fire when the local character sequence matches
a word or a word bigram. A dictionary containing
word and bigram information was collected from the
training data. For each latent variable label unigram
hi, we use the set of predicate template checking for
word-based features:
? The identity of the string xj . . . xi, if it matches
a word A from the word-dictionary of training
data, with the constraint i?6 < j < i; multiple
features will be generated if there are multiple
strings satisfying the condition.
? The identity of the string xi . . . xk, if it matches
a word A from the word-dictionary of training
data, with the constraint i < k < i+6; multiple
features could be generated.
? The identity of the word bigram (xj . . . xi?1,
xi . . . xk), if it matches a word bigram in the
bigram dictionary and satisfies the aforemen-
tioned constraints on j and k; multiple features
could be generated.
? The identity of the word bigram (xj . . . xi,
xi+1 . . . xk), if it matches a word bigram in the
bigram dictionary and satisfies the aforemen-
tioned constraints on j and k; multiple features
could be generated.
All feature templates were instantiated with val-
ues that occur in positive training examples. We
found that using low-frequency features that occur
only a few times in the training set improves perfor-
mance on the development set. We hence do not do
any thresholding of the DPLVM features: we simply
use all those generated features.
The aforementioned word based features can in-
corporate word information naturally. In addition,
following Wang et al (2006), we found using a
very simple heuristic can further improve the seg-
mentation quality slightly. More specifically, two
operations, merge and split, are performed on the
DPLVM/CRF outputs: if a bigram A B was not ob-
served in the training data, but the merged one AB
was, then A B will be simply merged into AB; on
the other hand, if AB was not observed but A B ap-
peared, then it will be split into A B. We found this
simple heuristic on word information slightly im-
proved the performance (e.g., for the PKU corpus,
+0.2% on the F-score).
3 Experiments
We used the data provided by the second Inter-
national Chinese Word Segmentation Bakeoff to
test our approaches described in the previous sec-
tions. The data contains three corpora from different
sources: Microsoft Research Asia (MSR), City Uni-
versity of Hong Kong (CU), and Peking University
(PKU).
Since the purpose of this work is to evaluate the
proposed latent variable model, we did not use ex-
tra resources such as common surnames, lexicons,
parts-of-speech, and semantics. For the generation
of word-based features, we extracted a word list
from the training data as the vocabulary.
Four metrics were used to evaluate segmentation
results: recall (R, the percentage of gold standard
output words that are correctly segmented by the de-
coder), precision (P , the percentage of words in the
decoder output that are segmented correctly), bal-
anced F-score (F ) defined by 2PR/(P + R), recall
of OOV words (R-oov). For more detailed informa-
tion on the corpora and these metrics, refer to Emer-
son (2005).
3.1 Training the DPLVM Segmenter
We implemented DPLVMs in C++ and optimized
the system to cope with large scale problems, in
which the feature dimension is beyond millions. We
employ the feature templates defined in Section 2.2,
taking into account those 3,069,861 features for the
MSR data, 2,634,384 features for the CU data, and
1,989,561 features for the PKU data.
As for numerical optimization, we performed
gradient decent with the Limited-Memory BFGS
59
(L-BFGS)6 optimization technique (Nocedal and
Wright, 1999). L-BFGS is a second-order Quasi-
Newton method that numerically estimates the cur-
vature from previous gradients and updates. With
no requirement on specialized Hessian approxima-
tion, L-BFGS can handle large-scale problems in an
efficient manner.
Since the objective function of the DPLVM model
is non-convex, we randomly initialized parameters
for the training.7 To reduce overfitting, we employed
an L2 Gaussian weight prior8 (Chen and Rosen-
feld, 1999). During training, we varied the L2-
regularization term (with values 10k, k from -3 to
3), and finally set the value to 1. We use 4 hidden
variables per label for this task, compromising be-
tween accuracy and efficiency.
3.2 Comparison on Convergence Speed
First, we show a comparison of the convergence
speed between the objective function of DPLVMs
and CRFs. We apply the L-BFGS optimization algo-
rithm to optimize the objective function of DPLVM
and CRF models, making a comparison between
them. We find that the number of iterations required
for the convergence of DPLVMs are fewer than for
CRFs. Figure 1 illustrates the convergence-speed
comparison on the MSR data. The DPLVM model
arrives at the plateau of convergence in around 300
iterations, with the penalized loss of 95K when
#passes = 300; while CRFs require 900 iterations,
with the penalized loss of 98K when #passes =
900.
However, we should note that the time cost of the
DPLVM model in each iteration is around four times
higher than the CRF model, because of the incorpo-
ration of hidden variables. In order to speed up the
6For numerical optimization on latent variable models, we
also experimented the conjugate-gradient (CG) optimization al-
gorithm and stochastic gradient decent algorithm (SGD). We
found the L-BFGS with L2 Gaussian regularization performs
slightly better than the CG and the SGD. Therefore, we adopt
the L-BFGS optimizer in this study.
7For a non-convex objective function, different parame-
ter initializations normally bring different optimization results.
Therefore, to approach closer to the global optimal point, it
is recommended to perform multiple experiments on DPLVMs
with random initialization and then select a good start point.
8We also tested the L-BFGS with L1 regularization, and we
found the L-BFGS with L2 regularization performs better in
this task.
0
300K
600K
900K
1200K
1500K
1800K
 100  200  300  400  500  600  700  800  900
O
bj.
 Fu
nc
. V
alu
e
Forward-Backward Passes
DPLVM
CRF
Figure 1: The value of the penalized loss based on the
number of iterations: DPLVMs vs. CRFs on the MSR
data.
Style #W.T. #Word #C.T. #Char
MSR S.C. 88K 2,368K 5K 4,050K
CU T.C. 69K 1,455K 5K 2,403K
PKU S.C. 55K 1,109K 5K 1,826K
Table 1: Details of the corpora. W.T. represents word
types; C.T. represents character types; S.C. represents
simplified Chinese; T.C. represents traditional Chinese.
training speed of the DPLVM model in the future,
one solution is to use the stochastic learning tech-
nique9. Another solution is to use a distributed ver-
sion of L-BFGS to parallelize the batch training.
4 Results and Discussion
Since the CRF model is one of the most successful
models in Chinese word segmentation, we compared
DPLVMs with CRFs. We tried to make experimen-
tal results comparable between DPLVMs and CRF
models, and have therefore employed the same fea-
ture set, optimizer and fine-tuning strategy between
the two. We also compared DPLVMs with semi-
CRFs and other successful systems reported in pre-
vious work.
4.1 Evaluation Results
Three training and test corpora were used in the test,
including the MSR Corpus, the CU Corpus, and the
9We have tried stochastic gradient decent, as described pre-
viously. It is possible to try other stochastic learning methods,
e.g., stochastic meta decent (Vishwanathan et al, 2006).
60
MSR data P R F R-oov
DPLVM (*) 97.3 97.3 97.3 72.2
CRF (*) 97.1 96.8 97.0 72.0
semi-CRF (A06) N/A N/A 96.8 N/A
semi-CRF (G07) N/A N/A 97.2 N/A
CRF (Z06-a) 96.5 96.3 96.4 71.4
Z06-b 97.2 96.9 97.1 71.2
ZC07 N/A N/A 97.2 N/A
Best05 (T05) 96.2 96.6 96.4 71.7
CU data P R F R-oov
DPLVM (*) 94.7 94.4 94.6 68.8
CRF (*) 94.3 93.9 94.1 65.8
CRF (Z06-a) 95.0 94.2 94.6 73.6
Z06-b 95.2 94.9 95.1 74.1
ZC07 N/A N/A 95.1 N/A
Best05 (T05) 94.1 94.6 94.3 69.8
PKU data P R F R-oov
DPLVM (*) 95.6 94.8 95.2 77.8
CRF (*) 95.2 94.2 94.7 76.8
CRF (Z06-a) 94.3 94.6 94.5 75.4
Z06-b 94.7 95.5 95.1 74.8
ZC07 N/A N/A 94.5 N/A
Best05 (C05) 95.3 94.6 95.0 63.6
Table 2: Results from DPLVMs, CRFs, semi-CRFs, and
other systems.
PKU Corpus (see Table 1 for details). The results
are shown in Table 2. The results are grouped into
three sub-tables according to different corpora. Each
row represents a CWS model. For each group, the
rows marked by ? represent our models with hy-
brid word/character information. Best05 represents
the best system of the Second International Chinese
Word Segmentation Bakeoff on the corresponding
data; A06 represents the semi-CRF model in An-
drew (2006)10, which was also used in Gao et al
(2007) (denoted as G07) with an improved perfor-
mance; Z06-a and Z06-b represents the pure sub-
word CRF model and the confidence-based com-
bination of CRF and rule-based models, respec-
tively (Zhang et al, 2006); ZC07 represents the
word-based perceptron model in Zhang and Clark
(2007); T05 represents the CRF model in Tseng et
al. (2005); C05 represents the system in Chen et al
10It is a hybrid Markov/semi-Markov CRF model which
outperforms conventional semi-CRF models (Andrew, 2006).
However, in general, as discussed in Andrew (2006), it is essen-
tially still a semi-CRF model.
(2005). The best F-score and recall of OOV words
of each group is shown in bold.
As is shown in the table, we achieved the best
F-score in two out of the three corpora. We also
achieved the best recall rate of OOV words on those
two corpora. Both of the MSR and PKU Corpus use
simplified Chinese, while the CU Corpus uses the
traditional Chinese.
On the MSR Corpus, the DPLVM model reduced
more than 10% error rate over the CRF model us-
ing exactly the same feature set. We also compared
our DPLVM model with the semi-CRF models in
Andrew (2006) and Gao et al (2007), and demon-
strate that the DPLVM model achieved slightly bet-
ter performance than the semi-CRF models. Andrew
(2006) and Gao et al (2007) only reported the re-
sults on the MSR Corpus.
In summary, tests for the Second International
Chinese Word Segmentation Bakeoff showed com-
petitive results for our method compared with the
best results in the literature. Our discriminative la-
tent variable models achieved the best F-scores on
the MSR Corpus (97.3%) and PKU Corpus (95.2%);
the latent variable models also achieved the best re-
calls of OOV words over those two corpora. We will
analyze the results by varying the word-length in the
following subsection.
4.2 Effect on Long Words
One motivation of using a latent variable model for
CWS is to use latent variables to more adequately
learn long range dependencies, as we argued in Sec-
tion 1. In the test data of the MSR Corpus, 19% of
the words are longer than 3 characters; there are also
8% in the CU Corpus and 11% in the PKU Corpus,
respectively. In the MSR Corpus, there are some ex-
tremely long words (Length > 10), while the CU
and PKU corpus do not contain such extreme cases.
Figure 2 shows the recall rate on different groups
of words categorized by their lengths (the number
of characters). As we expected, the DPLVM model
performs much better on long words (Length ? 4)
than the CRF model, which used exactly the same
feature set. Compared with the CRF model, the
DPLVM model exhibited almost the same level of
performance on short words. Both models have
the best performance on segmenting the words with
the length of two. The performance of the CRF
61
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-M
SR
 (%
)
Length of Word (MSR)
DPLVM
CRF
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-C
U 
(%
)
Length of Word (CU)
DPLVM
CRF
 40
 50
 60
 70
 80
 90
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-P
KU
 (%
)
Length of Word (PKU)
DPLVM
CRF
Figure 2: The recall rate on words grouped by the length.
model deteriorates rapidly as the word length in-
creases, which demonstrated the difficulty on mod-
eling long range dependencies in CWS. Compared
with the CRF model, the DPLVM model performed
quite well in dealing with long words, without sacri-
ficing the performance on short words. All in all, we
conclude that the improvement of using the DPLVM
model came from the improvement on modeling
long range dependencies in CWS.
4.3 Error Analysis
Table 3 lists the major errors collected from the la-
tent variable segmenter. We examined the collected
errors and found that many of them can be grouped
into four types: over-generalization (the top row),
errors on named entities (the following three rows),
errors on idioms (the following three rows) and er-
rors from inconsistency (the two rows at the bottom).
Our system performed reasonably well on very
complex OOV words, such as
(Agricultural Bank of China,
Gold Segmentation Segmenter Output
//
Co-allocated org. names
(Chen Yao) //
(Chen Fei) //
(Vasillis) //
//
//
// //
Idioms
// (propagandist)
(desertification) //
Table 3: Error analysis on the latent variable seg-
menter. The errors are grouped into four types: over-
generalization, errors on named entities, errors on idioms
and errors from data-inconsistency.
Shijiazhuang-city Branch, the second sales depart-
ment) and (Science
and Technology Commission of China, National In-
stitution on Scientific Information Analysis). How-
ever, it sometimes over-generalized to long words.
For example, as shown in the top row,
(National Department of Environmental Protection)
and (The Central Propaganda Department)
are two organization names, but they are incorrectly
merged into a single word.
As for the following three rows, (Chen Yao)
and (Chen Fei) are person names. They are
wrongly segmented because we lack the features to
capture the information of person names (such use-
ful knowledge, e.g., common surname list, are cur-
rently not used in our system). In the future, such
errors may be solved by integrating open resources
into our system. (Vasillis) is a transliter-
ated foreign location name and is also wrongly seg-
mented.
For the corpora that considered 4 character idioms
as a word, our system successfully combined most
of new idioms together. This differs greatly from the
results of CRFs. However, there are still a number
of new idioms that failed to be correctly segmented,
as listed from the fifth row to the seventh row.
Finally, some errors are due to inconsistencies in
the gold segmentation. For example, // (pro-
pagandist) is two words, but a word with similar
62
structure, (theorist), is one word.
(desertification) is one word, but its synonym,
// (desertification), is two words in the gold seg-
mentation.
5 Conclusion and Future Work
We presented a latent variable model for Chinese
word segmentation, which used hybrid information
based on both word and character sequences. We
discussed that word and character information have
different advantages, and could be complementary
to each other. Our model is an alternative to the ex-
isting word based models and character based mod-
els.
We argued that using latent variables can better
capture long range dependencies. We performed
experiments and demonstrated that our model can
indeed improve the segmentation accuracy on long
words. With this improvement, tests on the data
of the Second International Chinese Word Segmen-
tation Bakeoff show that our system is competitive
with the best in the literature.
Since the latent variable model allows a wide
range of features, so the future work will consider
how to integrate open resources into our system. The
latent variable model handles latent-dependencies
naturally, and can be easily extended to other label-
ing tasks.
Acknowledgments
We thank Kun Yu, Galen Andrew and Xiaojun Lin
for the enlightening discussions. We also thank the
anonymous reviewers who gave very helpful com-
ments. This work was partially supported by Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan).
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
Proceedings of EMNLP?06, pages 465?472.
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takahashi Tsuzuki. 2005. Combination of machine
learning methods for optimum chinese word segmen-
tation. Proceedings of the fourth SIGHAN workshop,
pages 134?137.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMU-CS-99-108, CMU.
Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun.
2005. Unigram language model for chinese word seg-
mentation. Proceedings of the fourth SIGHAN work-
shop.
Hal Daume? III and Daniel Marcu. 2005. Learn-
ing as search optimization: approximate large mar-
gin methods for structured prediction. Proceedings of
ICML?05.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. Proceedings of the
fourth SIGHAN workshop, pages 123?133.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics (ACL?07), pages 824?831.
P.E. Hart, N.J. Nilsson, and B. Raphael. 1968. A formal
basis for the heuristic determination of minimum cost
path. IEEE Trans. On System Science and Cybernet-
ics, SSC-4(2):100?107.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. Proceed-
ings of ICML?01, pages 282?289.
Percy Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-dynamic discriminative mod-
els for continuous gesture recognition. Proceedings of
CVPR?07, pages 1?8.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
optimization. Springer.
F. Peng and A. McCallum. 2004. Chinese segmenta-
tion and new word detection using conditional random
fields. Proceedings of COLING?04.
Slav Petrov and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. Proceedings of
NIPS?08.
Sunita Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Proceedings of ICML?04.
Xu Sun and Jun?ichi Tsujii. 2009. Sequential labeling
with latent variables: An exact inference algorithm and
its efficient approximation. Proceedings of the 12th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL?09).
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bakeoff
63
2005. Proceedings of the fourth SIGHAN workshop,
pages 168?171.
S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
meta-descent. Proceedings of ICML?06, pages 969?
976.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of the fifth SIGHAN workshop, pages
138?141, July.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Computa-
tional Linguistics and Chinese Language Processing,
8(1).
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. Pro-
ceedings of ACL?07.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. Proceedings of
HLT/NAACL?06 companion volume short papers.
64
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 210?213,
Paris, October 2009. c?2009 Association for Computational Linguistics
HPSG Supertagging: A Sequence Labeling View
Yao-zhong Zhang ? Takuya Matsuzaki ?
? Department of Computer Science, University of Tokyo
? School of Computer Science, University of Manchester
?National Centre for Text Mining, UK
{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii???
Abstract
Supertagging is a widely used speed-up
technique for deep parsing. In another
aspect, supertagging has been exploited
in other NLP tasks than parsing for
utilizing the rich syntactic information
given by the supertags. However, the
performance of supertagger is still a
bottleneck for such applications. In this
paper, we investigated the relationship
between supertagging and parsing, not
just to speed up the deep parser; We
started from a sequence labeling view
of HPSG supertagging, examining how
well a supertagger can do when separated
from parsing. Comparison of two types
of supertagging model, point-wise model
and sequential model, showed that the
former model works competitively well
despite its simplicity, which indicates
the true dependency among supertag
assignments is far more complex than the
crude first-order approximation made in
the sequential model. We then analyzed
the limitation of separated supertagging
by using a CFG-filter. The results showed
that big gains could be acquired by resort-
ing to a light-weight parser.
1 Introduction
Supertagging is an important part of lexicalized
grammar parsing. A high performance supertag-
ger greatly reduces the load of a parser and ac-
celerates its speed. A supertag represents a lin-
guistic word category, which encodes syntactic be-
havior of the word. The concept of supertagging
was first proposed for lexicalized tree adjoining
grammar (LTAG) (Bangalore and Joshi, 1999) and
then extended to other lexicalized grammars, such
as combinatory categorial grammar (CCG) (Clark,
2002) and Head-driven phrase structure grammar
(HPSG) (Ninomiya et al, 2006). Recently, syn-
tactic information in supertags has been exploited
for NLP tasks besides parsing, such as NP chunk-
ing (Shen and Joshi, 2003), semantic role label-
ing (Chen and Rambow, 2003) and machine trans-
lation (Hassan et al, 2007). Supertagging serves
there as an implicit and convenient way to incor-
porate rich syntactic information in those tasks.
Improving the performance of supertagging can
thus benefit these two aspects: as a preproces-
sor for deep parsing and as an independent, al-
ternative technique for ?almost? parsing. How-
ever, supertags are derived from a grammar and
thus have a strong connection to parsing. To fur-
ther improve the supertagging accuracy, the rela-
tion between supertagging and parsing is crucial.
With this motivation, we investigate how well a se-
quence labeling model can do when it is separated
from a parser, and to what extent the ignorance of
long distance dependencies in the sequence label-
ing formulation affects the supertagging results.
Specifically, we evaluated two different types
of supertagging model, point-wise model and se-
quential model, for HPSG supertagging. CFG-
filter was then used to empirically evaluate the
effect of long distance dependencies in supertag-
ging. The point-wise model achieved competitive
result of 92.53% accuracy on WSJ-HPSG tree-
bank with fast training speed, while the sequen-
tial model augmented with supertag edge features
did not give much further improvement over the
point-wise model. Big gains acquired by using
CFG-filter indicates that further improvement may
be achieved by resorting to a light-weight parser.
2 HPSG Supertags
HPSG (Pollard and Sag, 1994) is a kind of lexi-
calized grammar. In HPSG, many lexical entries
are used to express word-specific characteristics,
210
while only small amount of rule schemas are used
to describe general constructions. A supertag in
HPSG corresponds to a template of lexical entry.
For example, one possible supertag for ?big? is
?[<ADJP>]N lxm?, which indicates that the syn-
tactic category of ?big? is adjective and it modi-
fies a noun to its right. The number of supertags
is generally much larger than the number of labels
used in other sequence labeling tasks; Comparing
to 45 POS tags used in PennTreebank, the HPSG
grammar used in our experiments includes 2,308
supertags. Because of this, it is often very hard or
even impossible to apply computationary demand-
ing methods to HPSG supertagging.
3 Perceptron and Bayes Point Machine
Perceptron is an efficient online discriminative
training method. We used perceptron with weight-
averaging (Collins, 2002) as the basis of our su-
pertagging model. We also use perceptron-based
Bayes point machine (BPM) (Herbrich et al,
2001) in some of the experiments. In short, a BPM
is an average of a number of averaged perceptrons?
weights. We use average of 10 averaged percep-
trons, each of which is trained on a different ran-
dom permutation of the training data.
3.1 Formulation
Here we follow the definition of Collins? per-
ceptron to learn a mapping from the input space
(w, p) ? W ? P to the supertag space s ? S. We
use function GEN(w,p) to indicate all candidates
given input (w, p). Feature function f maps a train-
ing sample (w, p, s) ?W ?P ?S to a point in the
feature space Rd. To get feature weights ? ? Rd
of feature function, we used the averaged percep-
tron training method described in (Collins, 2002),
and the average of its 10 different runs (i.e., BPM).
For decoding, given an input (w, p) and a vector
of feature weights ?, we want to find an output s
which satisfies:
F (w, p) = argmax
s?GEN(w, p)
? ? f(w, p, s)
For the input (w, p), we treat it in two fash-
ions: one is (w, p) representing a single word
and a POS tag. Another is (w, p) representing
whole word and POS tags sequence. We call them
point-wise model and sequential model respec-
tively. Viterbi algorithm is used for decoding in
sequential model.
template type template
Word wi,wi?1,wi+1,
wi?1&wi, wi&wi+1
POS pi, pi?1, pi?2, pi+1,
pi+2, pi?1&pi, pi?2&pi?1,
pi?1&pi+1, pi&pi+1, pi+1&pi+2
Word-POS pi?1&wi, pi&wi, pi+1&wi
Supertag? si?1 , si?2&si?1
Substructure {ssi,1, ..., ssi,N}?Word
{ssi,1, ..., ssi,N}? POS
{ssi,1, ..., ssi,N}?Word-POS
{ssi?1,1, ..., ssi?1,N}?
{ssi,1, ..., ssi,N}?
Table 1: Feature templates for point-wise model
and sequential model. Templates with ? are only
used by sequential model. ssi,j represents j-th
substructure of supertag at i. For briefness, si is
omitted for each template. ??? means set-product.
e.g., {a,b}?{A,B}={a&A,a&B,b&A,b&B}
3.2 Features
Feature templates are listed in Table 1. To make
the results comparable with previous work, we
adopt the same feature templates as Matsuzaki et.
al. (2007). For sequential model, supertag con-
texts are added to the features. Because of the
large number of supertags, those supertag edge
features could be very sparse. To alleviate this
sparseness, we extracted sub-structures from the
lexical template of each supertag, and use them for
making generalized node/edge features as shown
in Table 1. The sub-structures we used include
subcategorization frames (e.g., subject=NP, ob-
ject=NP PP), direction and category of modifiee
phrase (e.g., mod left=VP), voice and tense of a
verb (e.g., passive past).
3.3 CFG-filter
Long distance dependencies are also encoded in
supertags. For example, when a transitive verb
gets assigned a supertag that specifies it has a PP-
object, in most cases a preposition to its right must
be assigned an argument (not adjunct) supertag,
and vice versa. Such kind of long distance context
information might be important for supertag dis-
ambiguation, but is not easy to incorporate into a
sequence labeling model separated from a parser.
To examine the limitation of supertagging sep-
arated from a parser, we used CFG-filter as an ap-
211
Model Name Acc%
PW-AP 92.29
SEQ-AP 92.53
PW-AP+CFG 93.57
SEQ-AP+CFG 93.68
Table 2: Averaged 10-cross validation of averaged
perceptron on Section 02-21.
proximation of an HPSG parser. We firstly cre-
ated a CFG that approximates the original HPSG
grammar, using the iterative method by Kiefer
and Krieger (2000). Given the supertags as pre-
terminals, the approximating CFG was then used
for finding a maximally scored sequence of su-
pertags which satisfies most of the grammatical
constraints in the original HPSG grammar (Mat-
suzaki et al, 2007). By comparing the supertag-
ging results before and after CFG-filtering, we can
quantify how many errors are caused by ignorance
of the long-range dependencies in supertagger.
4 Experiments and Analysis
We conducted experiments on WSJ-HPSG tree-
bank corpus (Miyao, 2006), which was semi-
automatically converted from the WSJ portion of
PennTreebank. The number of training iterations
was set to 5 for all models. Gold-standard POS
tags are used as input. The performance is evalu-
ated by accuracy1 and speed of supertagging on an
AMD Opteron 2.4GHz server.
Table 2 shows the averaged results of 10-
fold cross-validation of averaged perceptron (AP)
models2 on section 02-21. We can see the dif-
ference between point-wise AP model and se-
quential AP model is small (0.24%). It becomes
even smaller after CFG-filtering (0.11%). Table
3 shows the supertagging accuracy on section 22
based on BPM. Although not statistically signif-
icantly different from previous ME model (Mat-
suzaki et al, 2007), point-wise model (PW-BPM)
achieved competitive result 92.53% with faster
training. In addition, 0.27% and 0.29% gains were
brought by using BPM from PW-AP (92.26%) and
PW-SEQ (92.54%) with P-values less than 0.05.
The improvement by using sequential mod-
els (PW-AP?SEQ-AP: 0.24%, PW-BPM?SEQ-
BPM: 0.3%, statistically significantly different),
1?UNK? supertags are ignored in evaluation as previous.
2For time limitation, cross validation for BPM was not
conducted.
Model Name Acc% Training/
Testing Time ?
ME (Matsuzaki 07?) 92.45 ? 3h / 12s
PW-BPM 92.53 285s / 10s
SEQ-BPM 92.83 1721s / 13s
PW-BPM+SUB 92.68 1275s / 25s
SEQ-BPM+SUB 92.99 9468s / 107s
PW-BPM+CFG 93.60 285s / 78s
SEQ-BPM+CFG 93.70 1721s / 195s
PW-BPM+SUB+CFG 93.72 1275s / 170s
SEQ-BPM+SUB+CFG 93.88 9468s / 1011s
Table 3: Supertagging accuracy and training&
testing speed on section 22. (?) Test time was cal-
culated on totally 1648 sentences.
compared to point-wise models, were not so large,
but the training time was around 6 times longer.
We think the reason is twofold. First, as previous
research showed, POS sequence is very informa-
tive in supertagging (Clark, 2004). A large amount
of local syntactic information can be captured in
POS tags of surrounding words, although a few
long-range dependencies are of course not. Sec-
ond, the number of supertags is large and the su-
pertag edge features used in sequential model are
inevitably suffered from data sparseness. To alle-
viate this, we extracted sub-structure from lexical
templates (i.e., lexical items corresponding to su-
pertags) to augment the supertag edge features, but
only got 0.16% improvement (SEQ-BPM+SUB).
Furthermore, we also got 0.15% gains with P-
value less than 0.05 by incorporating the sub-
structure features into point-wise model (PW-
BPM+SUB). We hence conclude that the contri-
bution of the first-order edge features is not large
in sequence modeling for HPSG supertagging.
As we explained in Section 3.3, sequence label-
ing models have inherent limitation in the ability
to capture long distance dependencies between su-
pertags. This kind of ambiguity could be easier to
solve in a parser. To examine this, we added CFG-
filter which works as an approximation of a full
HPSG parser, after the sequence labeling model.
As expected, there came big gains of 1.26% (from
PW-AP to PW-AP+CFG) and 1.15% (from PW-
BPM to PW-BPM+CFG). Even for the sequen-
tial model we also got 1.15% (from SEQ-AP to
SEQ-AP+CFG) and 0.87% (from SEQ-BPM to
SEQ-BPM+CFG) respectively. All these models
were statistically significantly different from orig-
212
inal ones.
We also gave error analysis on test results.
Comparing SEQ-AP with SEQ-AP+CFG, one of
the most frequent types of ?correct supertag? by
the CFG-filter was for word ?and?, wherein a su-
pertag for NP-coordination (?NP and NP?) was
corrected to one for VP-coordination (?VP and
VP? or ?S and S?). It means the disambiguation
between the two coordination type is difficult for
supertaggers, presumably because they looks very
similar with a limited length of context since the
sequence of the NP-object of left conjunct, ?and?,
the NP subject of right conjunct looks very similar
to a NP coordination. The different assignments
by SEQ-AP+CFG from SEQ-AP include 725 right
corrections, while it changes 298 correct predic-
tions by SEQ-AP to wrong assignments. One pos-
sible reason for some of ?wrong correction? is re-
lated to the approximation of grammar. But this
gives clue that for supertagging task: just using
sequence labeling models is limited, and we can
resort to use some light-weight parser to handle
long distance dependencies.
Although some of the ambiguous supertags
could be left for deep parsing, like multi-tagging
technique (Clark, 2004), we also consider the
tasks where supertags can be used while conduct-
ing deep parsing is too computationally costly. Al-
ternatively, focusing on supertagging, we could
treat it as a sequence labeling task, while a conse-
quent light-weight parser is a disambiguator with
long distance constraint.
5 Conclusions
In this paper, through treating HPSG supertag-
ging in a sequence labeling way, we examined
the relationship between supertagging and parsing
from an angle. In experiment, even for sequential
models, CFG-filter gave much larger improvement
than one gained by switching from a point-wise
model to a sequential model. The accuracy im-
provement given by the CFG-filter suggests that
we could gain further improvement by combining
a supertagger with a light-weight parser.
Acknowledgments
Thanks to the anonymous reviewers for valuable
comments. The first author was partially sup-
ported by University of Tokyo Fellowship (UT-
Fellowship). This work was partially supported
by Grant-in-Aid for Specially Promoted Research
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25:237?265.
John Chen and Owen Rambow. 2003. Use of deep
linguistic features for the recognition and labeling
of semantic arguments. In Proceedings of EMNLP-
2003, pages 41?48.
Stephen Clark. 2002. Supertagging for combinatory
categorial grammar. In Proceedings of the 6th In-
ternational Workshop on Tree Adjoining Grammars
and Related Frameworks (TAG+ 6), pages 19?24.
Stephen Clark. 2004. The importance of supertagging
for wide-coverage ccg parsing. In Proceedings of
COLING-04, pages 282?288.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. pages 1?8.
Hany Hassan, Mary Hearne, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine transla-
tion. In Proceedings of ACL 2007, pages 288?295.
Ralf Herbrich, Thore Graepel, and Colin Campbell.
2001. Bayes point machines. Journal of Machine
Learning Research, 1:245?279.
Bernd Kiefer and Hans-Ulrich Krieger. 2000. A
context-free approximation of head-driven phrase
structure grammar. In Proceedings of IWPT-2000,
pages 135?146.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient hpsg parsing with supertagging
and cfg-filtering. In Proceedings of IJCAI-07, pages
1671?1676.
Yusuke Miyao. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. Disserta-
tion, The University of Tokyo.
Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Mat-
suzaki, and Yusuke Miyao. 2006. Extremely lex-
icalized models for accurate and fast hpsg parsing.
In Proceedings of EMNLP-2006, pages 155?163.
Carl Pollard and Ivan A. Sag. 1994. Head-driven
Phrase Structure Grammar. University of Chicago /
CSLI.
Libin Shen and Aravind K. Joshi. 2003. A snow based
supertagger with application to np chunking. In Pro-
ceedings of ACL 2003, pages 505?512.
213
An Improved CRF based Chinese Language Processing System for SIGHAN
Bakeoff 2007
Xihong Wu, Xiaojun Lin, Xinhao Wang, Chunyao Wu, Yaozhong Zhang and Dianhai Yu
Speech and Hearing Research Center
State Key Laboratory of Machine Perception,
Peking University, China, 100871
{wxh,linxj,wangxh,wucy,zhangyaoz,yudh}@cis.pku.edu.cn
Abstract
This paper describes three systems: the
Chinese word segmentation (WS) system,
the named entity recognition (NER) sys-
tem and the Part-of-Speech tagging (POS)
system, which are submitted to the Fourth
International Chinese Language Processing
Bakeoff. Here, Conditional Random Fields
(CRFs) are employed as the primary mod-
els. For the WS and NER tracks, the n-
gram language model is incorporated in our
CRFs based systems in order to take into ac-
count the higher level language information.
Furthermore, to improve the performances
of our submitted systems, a transformation-
based learning (TBL) technique is adopted
for post-processing.
1 Introduction
Among 24 closed and open tracks in this bakeoff, we
participated in 23 tracks, except the open NER track
of MSRA. Our systems are ranked 1st in 6 tracks,
and get close to the top level in several other tracks.
Recently, Maximum Entropy model(ME) and
CRFs (Low et al, 2005)(Tseng et al, 2005) (Hai
Zhao et al, 2006) turned out to be promising in natu-
ral language processing tracks, and obtain excellent
performances on most of the test corpora of Bake-
off 2005 and Bakeoff 2006. Compared to the gen-
erative models, like HMM, the primary advantage
of CRFs is that it relaxes the independence assump-
tions, which makes it able to handle multiple inter-
acting features between observation elements (Wal-
lach et al, 2004).
However, the ME and CRFs emphasize the rela-
tion of the basic units of sequence, like the Chinese
characters in these tracks. While, the higher level
information, like the relationship of the words is ig-
nored. From this point of view, the n-gram language
model is incorporated in our CRFs based systems in
order to cover the word level language information.
Based on several pilot-experimental results, we
found that the tagging errors always follow some
patterns. In order to find those error patterns and cor-
rect the similar errors, we integrated the TBL post-
processor in our systems. In addition, extra train-
ing data, which is transformed from People Daily
Corpus (Shiwen Yu et al, 2000) with some auto-
extracted transition rules, is used in each corpus for
the open tracks of WS.
The remainder of this paper is organized as fol-
lows. The scheme of our three developed systems
are described in section 2, 3 and 4, respectively. In
section 5, evaluation results based on these systems
are enumerated and discussed. Finally some conclu-
sions are drawn in section 6.
2 Word Segmentation
The WS system mainly consists of three compo-
nents, CRFs, n-gram language model and post-
processing strategies.
2.1 Conditional Random Fields
Conditional Random Fields, as the statistical se-
quence labeling models, achieve great success in
natural language processing, such as chunking (Fei
Sha et al, 2003) and word segmentation (Hai Zhao
et al, 2006). Different from traditional generative
155
Sixth SIGHAN Workshop on Chinese Language Processing
model, CRFs relax the constraint of the indepen-
dence assumptions, and therefore turn out to be more
suitable for natural language tasks.
CRFs model the conditional distribution p(Y |X)
of the labels Y given the observations X directly
with the formulation:
P?(Y |X) = 1Z(X)exp{
?
c?C
?
k
?kfk(Yc, X, c)}
(1)
Y is the label sequence, X is the observation se-
quence, Z(X) is a normalization term, fk is a fea-
ture function, and c is the set of cliques in Graphic.
In our tasks, C = {(yi?1, yi)}, X is the Chinese
character sequence of a sentence.
To label a Chinese character, we need to define
the label tags. Here we have six types of tags ac-
cording to character position in a word (Hai Zhao et
al., 2006):
tag = {B1, B2, B3, I, E, S}
?B1, B2, B3, I, E? represent the first, second, third,
continue, and end character positions in a multi-
character word, and ?S? is the single-character word
tag.
The unigram feature templates used here are:
Cn (n = ?2,?1, 0, 1, 2)
CnCn+1 (n = ?2,?1, 0)
CnCn+1Cn+2 (n = ?1)
Where C0 refers to the current character and
C?n(Cn) is the nth character to the left(right) of the
current character. We also use the basic bigram fea-
ture template which denotes the dependency on the
previous tag and current tag.
2.2 Multi-Model Integration
In order to integrate multi-model information, we
use a log-linear model(Och et al, 2002) to compute
the posterior probability:
Pr (W |C) = p?M1 (W |C)
= exp[
?M
m=1 ?mhm(W,C)]
?
W ? exp[
?M
m=1 ?mhm(W ?, C)]
(2)
Where W is the word sequence, and C is the char-
acter sequence. The decision rule here is:
W0 = argmaxW {Pr(W |C)}
= argmaxW {
M
?
m=1
?mhm(W,C)} (3)
The parameters ?M1 of this model can be opti-
mized by standard approaches, such as the Mini-
mum Error Rate Training used in machine transla-
tion (Och, 2003). In fact, the CRFs approach is
a special case of this framework when we define
M = 1 and use the following feature function:
h1(W,C) = logP?(Y |X) (4)
In our approach, the logarithms of the scores gen-
erated by the two kinds of models are used as feature
functions:
h1(W,C) = logPcrf (W,C)
= log
?
w
i
P?(wi|C) (5)
h2(W,C) = logPlm(W ) (6)
The first feature function(Eq.5) comes from CRFs.
Instead of computing the score of the whole la-
bel sequence Y with character sequence X through
P?(Y |X) directly, we try to get the posterior prob-
ability of a sub-sequence to be tagged as one whole
word P?(wi|C). Then we combine all the score of
words together. The second feature function(Eq.6)
comes from n-gram language model, which aims to
catch the words information.
The log-linear model with the feature functions
described above allows the dynamic programming
search algorithm for efficient decoding. The system
generates the word lattice with posterior probability
P?(wi|C). Then the best word sequence is searched
on the word lattice with the decision rule(Eq.3).
Since arbitrary sub-sequence can be viewed as a
candidate word in word lattice, we need to deal with
the problem of OOV words. The unigram of an OOV
word is estimated as:
Unigram(OOV Word) = pl (7)
where p is the minimal value of unigram scores in
the language model; l is the length of the OOV
word, which is used as a punishment factor to
avoid overemphasizing the long OOV words (Xin-
hao Wang et al, 2006).
2.3 Post-Processing Strategies
The division and combination rule, which has been
proved to be useful in our system of Bakeoff 2006
(Xinhao Wang et al, 2006), is adopted for the post-
processing in the system.
156
Sixth SIGHAN Workshop on Chinese Language Processing
2.4 Training Data Transition
For the WS open tracks, the unique difference from
closed tracks is that the additional training data is
supplemented for model refinement.
For the Simplified Chinese tracks, the additional
training data are collected from People Daily Cor-
pus with a set of auto-extracted transition rules. This
process is performed in a heuristic strategy and con-
tains five steps as follows:
(1) Segment the raw People Daily texts with the cor-
responding system for the closed track of each cor-
pus.
(2) Compare the result of step 1 with People Daily
Corpus to get the conflict pairs. For example,
{pair1: ??? vs. ???}
(Zhemin Jiang)
{pair2: ??? vs. ???}
(catch with two hands)
In each pair, the left phrase follows the People Daily
Corpus segmentation guideline, while the right one
is the phrase obtained from step 1.
(3) Divide the pairs into two sets: the first set con-
tains the pairs with right phrase appearing in the tar-
get training data; the other pairs are in the second
set.
(4) Select sentences which contain the left phrase of
the pairs in the second set from People Daily Cor-
pus.
(5) Transform these selected sentences by replacing
their phrase in the left side of the pair in the first set
to the right one. This is used as our transition rules.
3 Named Entity Recognition
The named entity recognition track is viewed as a
character sequence tagging problem in our NER sys-
tem and the log-linear model mentioned above is
employed again to integrate multi-model informa-
tion. To find the error patterns and correct them,
a TBL strategy is then used in the post-processing
module.
3.1 Model Description
In this NER track, we employe the log-linear model
and use the logarithms of the scores generated by the
two types of models as feature functions. Besides
CRFs, another model is the class-based n-gram lan-
guage model:
h1(Y, X) = logPcrf (Y, X)
= logP?(Y |X) (8)
h2(Y, X) = logPclm(Y, X) (9)
Y is the label sequence and X is the character se-
quence.
CRFs are used to generate the N-best tagging re-
sults with the scores of whole label sequence Y on
character sequence X by P?(Y |X). And then, the
log-linear model is used to reorder the N-best tag-
ging results by integrating the CRFs score and the
class-based n-gram language model score together.
CRFs
In this track, one Chinese character is labeled by
a tag of ten classes, which denoting the beginning,
continue, ending character of a specified named en-
tity or a non-entity character. There are three types
of named entities in these tracks, including person
name, location name and organization name.
In CRFs, the basic features used here are:
Cn (n = ?2,?1, 0, 1, 2)
CnCn+1 (n = ?2,?1, 0, 1)
CnCn+2 (n = ?1)
Besides basic unigram features, the bigram transi-
tion features considering the previous tag is adopted
with template Cn (n = ?2,?1, 0, 1, 2).
Class-Based N-gram Language Model
For the class-based n-gram language model, we
define that each character is a single class, while
each type of named entity is viewed as a single class.
With the character sequence and label sequence, the
class sequence can be generated. Take this sentence
for instance:
???????????
(But Ibrahimov is not satisfied)
Table 1 shows its class sequence. Class-based n-
gram language model can be trained with class se-
quence.
3.2 TBL
Since the analysis on our experiments shows that the
tagging errors always follow some patterns in NER
track, TBL strategy is adopted in our system to find
these patterns and correct the similar errors.
157
Sixth SIGHAN Workshop on Chinese Language Processing
character sequence ? ? ? ? ? ? ? ? ? ? ?
label sequence N Per-B Per-C Per-C Per-C Per-C Per-E N N N N
class sequence ? PERSON ? ? ? ?
Table 1: A class sequence example
Transformation-based learning is a symbolic ma-
chine learning method, introduced by (Eric Brill,
1995). The main idea in TBL is to generate a set of
transformation rules that can correct tagging errors
produced by the initial process.
There are four main procedures in our TBL
framework: An initial state assignment which is op-
erated by the system we described above; a set of al-
lowable templates for rules, ranging from words in
a 3 positions windows and name entity information
in a 3-word window with their combinations consid-
ered, and rules which are learned according to the
tagging differences between training data and results
generated by our system, at last, those rules are in-
troduced to correct similar errors.
4 POS Tagging
The POS tagging track is to assign the part-of-
speech sequence for the correctly segmented word
sequence. In our system, for the CTB corpus, the
CRFs are adopted; however for the other four cor-
pora, considering the limitations of resources and
time, the ME model is adopted. To improve the per-
formance of ME model, the POS tag of the previous
word is taken as a feature and the dynamic program-
ming strategy is used in decoding.
In the closed track, the features include the basic
features and their combined features. Firstly the pre-
vious and next words of the current word are taken
as the basic features. Secondly, based on the anal-
ysis of the OOV words, the first and last characters
of the current word, as well as the length of the cur-
rent word are proven to be effective features for the
OOV POS. Furthermore since the long distance con-
straint word may impact the POS of current word
(Yan Zhao et al, 2006), in the open track, a Chi-
nese parser is imported and the word depended on
the current word is extracted as feature.
5 Experiments and Results
We have participated in 23 tracks, except the open
NER track of MSRA. CRFs, ME model and n-gram
language model are adopted in these systems. Our
implementation uses the CRF++ package1 provided
by Taku Kudo, the Maximum Entropy Toolkit2 pro-
vided by Zhang Le, and the SRILM Toolkit provided
by Andreas Stolcke (Andreas Stolcke et al, 2002).
5.1 Chinese Word Segmentation
In the closed tracks, CRFs and bigram language
model are trained on the given training data for each
corpus. In order to integrate these two models, it is
necessary to train the corresponding parameter ?M1
with Minimum Error Rate Training approache based
on a development data. Since the development data
is not provided in this bakeoff, a ten-fold cross val-
idation approach is employed to implement the pa-
rameter training. A set of parameters can be trained
independently, and then the mean value is calculated
as the estimation of each parameter.
Table 2 gives the results of our WS system for
closed tracks.
baseline +LM +LM+Post
CTB 94.7 94.7 94.8
NCC 92.6 92.4 92.9
SXU 94.7 95.7 95.8
CITYU 92.9 93.7 93.9
CKIP 93.2 93.7 93.7
Table 2: Word segmentation performance on F-
value with different approach for the closed tracks
In the open tracks, as we do not have enough time
to finish the parameter estimation on the new data,
our system adopt the same parameters ?M1 used in
closed tracks. The unique difference from closed
1http://chasen.org/taku/software/CRF++
2http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
158
Sixth SIGHAN Workshop on Chinese Language Processing
tracks is that extra training data is added for each
corpus to improve the performance. For the Sim-
plified Chinese tracks, additional data comes from
People Daily Corpus which is transformed by our
transition strategy. At the same time, for the Tra-
ditional Chinese tracks, additional data comes from
the training and testing data used in the early Bake-
off. However, we implement two systems for the
CTB open track. The system (a) takes the training
and testing data used in the early Bakeoff as addi-
tional data, and System (b) takes the translated Peo-
ple Daily Corpus as additional data. Table 3 gives
the results of our open WS system.
baseline +LM +LM+Post
CTB(a) 99.2 99.2 99.3
CTB(b) 95.6 95.1 97.0
NCC 93.7 93.0 92.9
SXU 96.4 87.0 95.8
CITYU 95.8 90.6 91.0
CKIP 94.5 94.8 95.1
Table 3: Word segmentation performance on F-
value with different approach for the open tracks
The result shows that the system performance is
sensitive to the parameters ?M1 . Although we train
the useful parameter for closed tracks, it plays a bad
role in open tracks as we do not adapt it for the ad-
ditional training data.
5.2 Named Entity Recognition
In the closed NER tracks, CRFs and class-based tri-
gram language model are trained on the given train-
ing data for each corpus. The same approach em-
ployed in the WS tracks is adopted to train the corre-
sponding parameter ?M1 in our NER systems. Mean-
while, the TBL rules trained via five-fold cross val-
idation approach are also used in post-processing
procedure. Table 4 reports the results of our closed
NER system.
5.3 POS Tagging
The experiments show that the CRFs/ME method is
superior to the TBL method, and the concurrent er-
rors for these two methods are less than 60%. There-
fore we adopted TBL to correct the output results
of CRFs/ME: If the output tags of CRFs/ME and
baseline +LM +LM+Post
MSRA 89.3 89.7 89.9
CITYU 79.3 80.6 80.5
Table 4: Named entity recognition F-value through
different approaches for the closed tracks
TBL are not consistent and the output probability
of CRFs/ME is below a certain threshold, the TBL
results are fixed. Here the 90% of the training set
is taken as the training data and remained 10% is
separated as the development data to get the thresh-
old, which is 0.60 for the CRFs, and 0.90 for the
ME. In addition, the POS tagged corpus of the Chi-
nese Treebank 5.0 from LDC is added to the training
data for CTB open track. In our system, the Berke-
ley Parser (Slav Petrov et al, 2006) is adopted to
obtain the long distance constraint words. The per-
formance achieved by the methods described above
on each corpus are reported in Table 5.
CRFs/ME CRFs/ME
CRFs/ME TBL +TBL +TBL
+Syntax
CTIYU 88.7 87.7 89.1 89.0
CKIP 91.8 91.4 92.2 92.1
CTB 94.0 92.7 94.3 96.5
NCC 94.6 94.3 94.9 95.0
PKU 93.5 93.2 94.0 94.1
Table 5: POS tagging performance on total-accuracy
with different approach
6 Conclusion
In this paper, we have briefly described our systems
participating in the Bakeoff 2007. In the WS and
NER systems, the log-linear model is adopted to in-
tegrate CRFs and language model, which improves
the system performances effectively. At the same
time, system integration approach used in the POS
system also proves its validity. In addition, a heuris-
tic strategy is imported to generate additional train-
ing data for the open WS tracks. Finally, several
post-processing strategies are used to further im-
prove our systems.
159
Sixth SIGHAN Workshop on Chinese Language Processing
References
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word Seg-
mentation. Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing. pp. 161-164.
Jeju Island, Korea.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, Christopher Manning. 2005. A Conditional
Random Field Word Segmenter for Sighan Bakeoff
2005. Proceedings of the Fourth SIGHAN Workshop
on Chinese Language Processing. pp. 168-171. Jeju
Island, Korea.
Hai Zhao, Chang-Ning Huang and Mu Li. 2006. An
Improved Chinese Word Segmentation System with
Conditional Random Field. Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Processing.
pp. 162-165. Sydney, Australia.
Hanna M. Wallach. 2004. Conditional Random Fields:
An Introduction. Technical Report, UPenn CIS TR
MS-CIS-04-21.
Shiwen Yu, Xuefeng Zhu and Huiming Duan. 2000.
Specification of large-scale modern Chinese corpus.
Proceedings of ICMLP?2001. pp. 18-24. Urumqi,
China.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. Proceedings of Hu-
man Language Technology/NAACL. pp. 213-220. Ed-
monton, Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL). pp. 295-302. Philadelphia, PA.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. Proceedings of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL). pp. 160-167. Sapporo,
Japan.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, Xi-
hong Wu. 2006. Chinese Word Segmentation with
Maximum Entropy and N-gram Language Model. the
Fifth SIGHAN Workshop on Chinese Language Pro-
cessing. pp. 138-141. Sydney, Australia.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in Part-of-Speech tagging. Computational Lingusitics.
21(4).
Yan Zhao, Xiaolong Wang, Bingquan Liu, and Yi Guan.
2006. Fusion of Clustering Trigger-Pair Features for
POS Tagging Based on Maximum Entropy Model.
Journal of Computer Research and Development.
43(2). pp. 268-274.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. Proceedings of International
Conference on Spoken Language Processing. pp. 901-
904. Denver, Colorado.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the ACL. pp. 433-440.
Sydney, Australia.
160
Sixth SIGHAN Workshop on Chinese Language Processing
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 645?648,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Simple Approach for HPSG Supertagging Using Dependency Information
Yao-zhong Zhang ? Takuya Matsuzaki ?
? Department of Computer Science, University of Tokyo
? School of Computer Science, University of Manchester
?National Centre for Text Mining, UK
{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii???
Abstract
In a supertagging task, sequence labeling
models are commonly used. But their lim-
ited ability to model long-distance informa-
tion presents a bottleneck to make further im-
provements. In this paper, we modeled this
long-distance information in dependency for-
malism and integrated it into the process of
HPSG supertagging. The experiments showed
that the dependency information is very in-
formative for supertag disambiguation. We
also evaluated the improved supertagger in the
HPSG parser.
1 Introduction
Supertagging is a widely used speed-up technique
for lexicalized grammar parsing. It was first
proposed for lexicalized tree adjoining grammar
(LTAG) (Bangalore and Joshi, 1999), then extended
to combinatory categorial grammar (CCG) (Clark,
2002) and head-driven phrase structure grammar
(HPSG) (Ninomiya et al, 2006). For deep parsing,
supertagging is an important preprocessor: an ac-
curate supertagger greatly reduces search space of
a parser. Not limited to parsing, supertags can be
used for NP chunking (Shen and Joshi, 2003), se-
mantic role labeling (Chen and Rambow, 2003) and
machine translation (Birch et al, 2007; Hassan et
al., 2007) to explore rich syntactic information con-
tained in them.
Generally speaking, supertags are lexical tem-
plates extracted from a grammar. These templates
encode possible syntactic behavior of a word. Al-
though the number of supertags is far larger than the
45 POS tags defined in Penn Treebank, sequence la-
beling techniques are still effective for supertagging.
Previous research (Clark, 2002) showed that a POS
sequence is very informative for supertagging, and
some extent of local syntactic information can be
captured by the context of surrounding words and
POS tags. However, since the context window
length is limited for the computational cost reasons,
there are still long-range dependencies which are not
easily captured in sequential models (Zhang et al,
2009). In practice, the multi-tagging technique pro-
posed by Clark (2002) assigned more than one su-
pertag to each word and let the ambiguous supertags
be selected by the parser. As for other NLP applica-
tions which use supertags, resolving more supertag
ambiguities in supertagging stage is preferred. With
this consideration, we focus on supertagging and
aim to make it as accurate as possible.
In this paper, we incorporated long-distance in-
formation into supertagging. First, we used depen-
dency parser formalism to model long-distance re-
lationships between the input words, which is hard
to model in sequence labeling models. Then, we
combined the dependency information with local
context in a simple point-wise model. The experi-
ments showed that dependency information is very
informative for supertagging and we got a compet-
itive 93.70% on supertagging accuracy (fed golden
POS). In addition, we also evaluated the improved
supertagger in the HPSG parser.
2 HPSG Supertagging and Dependency
2.1 HPSG Supertags
HPSG (Pollard and Sag, 1994) is a lexicalist gram-
mar framework. In HPSG, a large number of
lexical entries is used to describe word-specific
syntactic characteristics, while only a small num-
ber of schemas is used to explain general con-
struction rules. These lexical entries are called
?HPSG supertags?. For example, one possi-
ble supertag for the word ?like? is written like
?[NP.nom<V.bse>NP.acc] lxm?, which indicates
645
the head syntactic category of ?like? is verb in base
form. It has a NP subject and a NP complement.
With such fine-grained grammatical type distinc-
tions, the number of supertags is much larger than
the number of tags used in other sequence labeling
tasks. The HPSG grammar used in our experiment
includes 2,308 supertags. This increases computa-
tional cost of sequence labeling models.
2.2 Why Use Dependency in Supertagging
By analyzing the internal structure of the supertags,
we found that subject and complements are two im-
portant syntactic properties for each supertag. If
we could predict subject and complements of the
word well, supertagging would be an easier job to
do. However, current widely used sequence labeling
models have the limited ability to catch these long-
distance syntactic relations. In supertagging stage,
tree structures are still not constructed. Dependency
formalism is an alternative way to describe these two
syntactic properties. Based on this observation, we
think dependency information could assist supertag
prediction.
Figure 1: Model structure of incorporating dependency
information into the supertagging stage. Dotted arrows
describe the augmented long distance dependency infor-
mation provided for supertag prediction.
3 Our Method
3.1 Modeling Dependency for Supertags
First of all, we need to characterize the dependency
between words for supertagging. Since exact de-
pendency locations are not encoded in supertags, to
make use of state-of-the-art dependency parser, we
recover HPSG supertag dependencies with the aid
of HPSG treebanks. The dependencies are extracted
from each branch in the HPSG trees by regarding
the non-head daughter as the modifier of the head-
daughter. HPSG schemas are expressed in depen-
dency arcs.
To model the dependency, we follow mainstream
dependency parsing formalism. Two representa-
tive methods for dependency parsing are transition-
based model like MaltParser (Nivre, 2003) and
graph-based model like MSTParser1 (McDonald et
al., 2005). Previous research (Nivre and McDon-
ald, 2008) showed that MSTParser is more accurate
than MaltParser for long dependencies. Since our
motivation is to capture long-distance dependency
as a complement for local supertagging models, we
use the projective MSTParser formalism to model
dependencies.
{(pi ? pj)&sj |(j, i) ? E}
MOD-IN {(pi ? wj)&sj|(j, i) ? E}
{(wi ? pj)&sj|(j, i) ? E}
{(wi ? wj)&sj |(j, i) ? E}
{(pi ? pj)&si|(i, j) ? E}
MOD-OUT {(pi ? wj)&si|(i, j) ? E}
{(wi ? pj)&si|(i, j) ? E}
{(wi ? wj)&si|(i, j) ? E}
Table 1: Non-local feature templates used for super-
tagging. Here, p, w and s represent POS, word
and schema respectively. Direction (Left/Right) from
MODIN/MODOUTword to the current word is also con-
sidered in the feature templates.
3.2 Integrating Dependency into Supertagging
There are several ways to combine long-distance
dependency into supertagging. Integrating depen-
dency information into training process would be
more intuitive. Here, we use feature-based integra-
tion. The base model is a point-wise averaged per-
ceptron (PW-AP) which has been shown very ef-
fective (Zhang et al, 2009). The improved model
structure is described in Figure 1. The long-distance
information is formalized as first-order dependency.
For the word being predicted, we extract its modi-
fiers (MODIN) and its head (MODOUT) (Table 1)
based on first-order dependency arcs. Then MODIN
and MODOUT relations are combined as features
with local context for supertag prediction. To com-
pare with previous work, the basic local context fea-
tures are the same as in Matsuzaki et al (2007).
1http://sourceforge.net/projects/mstparser/
646
4 Experiments
We evaluated dependency-informed supertagger
(PW-DEP) both by supertag accuracy 2 and by a
HPSG parser. The experiments were conducted on
WSJ-HPSG treebank (Miyao, 2006). Sections 02-
21 were used to train the dependency parser, the
dependency-informed supertagger and the HPSG
parser. Section 23 was used as the testing set. The
evaluation metric for HPSG parser is the accuracy
of predicate-argument relations in the parser?s out-
put, as in previous work (Sagae et al, 2007).
Model Dep Acc%? Acc%
PW-AP / 91.14
PW-DEP 90.98 92.18
PW-AP (gold POS) / 92.48
PW-DEP (gold POS) 92.05 93.70
100 97.43
Table 2: Supertagging accuracy on section 23. (?)
Dependencies are given by MSTParser evaluated with
labeled accuracy. PW-AP is the baseline point-wise
averaged perceptron model. PW-DEP is point-wise
dependency-informed model. The automatically tagged
POS tags were given by a maximum entropy tagger with
97.39% accuracy.
4.1 Results on Supertagging
We first evaluated the upper-bound of dependency-
informed supertagging model, given gold standard
first-order dependencies. As shown in Table 2,
with such long-distance information supertagging
accuracy can reach 97.43%. Comparing to point-
wise model (PW-AP) which only used local con-
text (92.48%), this absolute 4.95% gain indicated
that dependency information is really informative
for supertagging. When automatically predicted de-
pendency relations were given, there still were ab-
solute 1.04% (auto POS) and 1.22% (gold POS) im-
provements from baseline PW-AP model.
We also compared supertagging results with pre-
vious works (reported on section 22). Here we
mainly compared the dependency-informed point-
wise models with perceptron-based Bayes point ma-
chine (BPM) plus CFG-filter (Zhang et al, 2009).
To the best of our knowledge, these are the state-of-
the-art results on the same dataset with gold POS
2?UNK? supertags are ignored in evaluation as previous.
Figure 2: HPSG Parser F-score on section 23, given au-
tomatically tagged POS.
tags. CFG-filtering can be considered as an al-
ternative way of incorporating long-distance con-
straints on supertagging results. Although our base-
line system was slightly behind (PW-AP: 92.16%
vs. BPM:92.53%), the final accuracies of grammati-
cally constrained models were very close (PW-DEP:
93.53% vs. BPM-CFG: 93.60%); They were not sta-
tistically significantly different (P-value is 0.26). As
the result of oracle PW-DEP indicated, supertagging
accuracy can be further improved with better depen-
dency modeling (e.g., with a semi-supervised de-
pendency parser), which makes it more extensible
and attractive than using CFG-filter after the super-
tagging process.
4.2 HPSG parsing results
We also evaluated the dependency-informed su-
pertagger in a HPSG parser. Considering the effi-
ciency, we use the HPSG parser3 described by Ma-
tsuzaki et al (2007).
In practice, several supertag candidates are re-
served for each word to avoid parsing failure. To
evaluate the quality of the two supertaggers, we re-
stricted the number of each word?s supertag candi-
dates fed to the HPSG parser. As shown in Figure 2,
for the case when only one supertag was predicted
for each word, F-score of the HPSG parser using
dependency-informed supertagger is 5.06% higher
than the parser using the baseline supertagger mod-
ule. As the candidate number increased, the gap nar-
rowed: when all candidates were given, the gains
gradually came down to 0.2%. This indicated that
3Enju v2.3.1, http://www-tsujii.is.s.u-tokyo.ac.jp/enju.
647
improved supertagger can optimize the search space
of the deep parser, which may contribute to more ac-
curate and fast deep parsing. From another aspect,
supertagging can be viewed as an interface to com-
bine different types of parsers.
As for the overall parsing time, we didn?t opti-
mize for speed in current setting. The parsing time4
saved by using the improved supertagger (around
6.0 ms/sen, 21.5% time reduction) can not compen-
sate for the extra cost of MSTParser (around 73.8
ms/sen) now. But there is much room to improve the
final speed (e.g., optimizing the dependency parser
for speed or reusing acquired dependencies for ef-
fective pruning). In addition, small beam-size can be
?safely? used with improved supertagger for speed.
Using shallow dependencies in deep HPSG pars-
ing has been previously explored by Sagae et al
(2007), who used dependency constraints in schema
application stage to guide HPSG tree construction
(F-score was improved from 87.2% to 87.9% with
a single shift-reduce dependency parser). Since the
baseline parser is different, we didn?t make a direct
comparison here. However, it would be interesting
to compare these two different ways of incorporat-
ing the dependency parser into HPSG parsing. We
left it as further work.
5 Conclusions
In this paper, focusing on improving the accu-
racy of supertagging, we proposed a simple but
effective way to incorporate long-distance depen-
dency relations into supertagging. The experiments
mainly showed that these long-distance dependen-
cies, which are not easy to model in traditional se-
quence labeling models, are very informative for su-
pertag predictions. Although these were preliminary
results, the method shows its potential strength for
related applications. Not limited to HPSG, it can be
extended to other lexicalized grammar supertaggers.
Acknowledgments
Thanks to the anonymous reviewers for valuable
comments. We also thank Goran Topic for his self-
less help. The first author was supported by The
University of Tokyo Fellowship (UT-Fellowship).
4Tested on section 23 (2291 sentences) using an AMD
Opteron 2.4GHz server, given all supertag candidates.
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Super-
tagging: An approach to almost parsing. Computa-
tional Linguistics, 25:237?265.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003.
Stephen Clark. 2002. Supertagging for combinatory cat-
egorial grammar. In Proceedings of the 6th Interna-
tional Workshop on Tree Adjoining Grammars and Re-
lated Frameworks (TAG+ 6), pages 19?24.
Hany Hassan, Mary Hearne, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of ACL 2007, pages 288?295.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient hpsg parsing with supertagging and
cfg-filtering. In Proceedings of IJCAI-07.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL-05.
Yusuke Miyao. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Develop-
ment and Feature Forest Model. Ph.D. Dissertation,
The University of Tokyo.
Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Matsu-
zaki, and Yusuke Miyao. 2006. Extremely lexicalized
models for accurate and fast hpsg parsing. In Proceed-
ings of EMNLP-2006, pages 155?163.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of IWPT-03, pages
149?160. Citeseer.
Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase
Structure Grammar. University of Chicago / CSLI.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
Hpsg parsing with shallow dependency constraints. In
Proceedings of ACL-07.
Libin Shen and Aravind K. Joshi. 2003. A snow based
supertagger with application to np chunking. In Pro-
ceedings of ACL 2003, pages 505?512.
Yao-zhong Zhang, Takuya Matsuzaki, and Jun?ichi Tsu-
jii. 2009. Hpsg supertagging: A sequence labeling
view. In Proceedings of IWPT-09, Paris, France.
648
