Term Contributed Boundary Tagging by Conditional Random 
Fields for SIGHAN 2010 Chinese Word Segmentation Bakeoff 
Tian-Jian Jiang?? Shih-Hung Liu*? Cheng-Lung Sung*? Wen-Lian Hsu?? 
?Department of 
Computer Science 
National Tsing-Hua University 
*Department of 
Electrical Engineering 
National Taiwan University 
?Institute of 
Information Science 
Academia Sinica 
{tmjiang,journey,clsung,hsu}@iis.sinica.edu.tw 
 
Abstract 
This paper presents a Chinese word 
segmentation system submitted to the 
closed training evaluations of CIPS-
SIGHAN-2010 bakeoff. The system uses 
a conditional random field model with 
one simple feature called term contri-
buted boundaries (TCB) in addition to 
the ?BI? character-based tagging ap-
proach. TCB can be extracted from unla-
beled corpora automatically, and seg-
mentation variations of different do-
mains are expected to be reflected impli-
citly. The experiment result shows that 
TCB does improve ?BI? tagging domain-
independently about 1% of the F1 meas-
ure score. 
1 Introduction 
The CIPS-SIGHAN-2010 bakeoff task of Chi-
nese word segmentation is focused on cross-
domain texts. The design of data set is challeng-
ing particularly. The domain-specific training 
corpora remain unlabeled, and two of the test 
corpora keep domains unknown before releasing, 
therefore it is not easy to apply ordinary machine 
learning approaches, especially for the closed 
training evaluations. 
2 Methodology 
2.1 The ?BI? Character-Based Tagging of 
Conditional Random Field as Baseline 
The character-based ?OBI? tagging of 
Conditional Random Field (Lafferty et al, 2001) 
has been widely used in Chinese word 
segmentation recently (Xue and Shen, 2003; 
Peng and McCallum, 2004; Tseng et al, 2005). 
Under the scheme, each character of a word is 
labeled as ?B? if it is the first character of a 
multiple-character word, or ?I? otherwise. If the 
character is a single-character word itself, ?O? 
will be its label. As Table 1 shows, the lost of 
performance is about 1% by replacing ?O? with 
?B? for character-based CRF tagging on the 
dataset of CIPS-SIGHAN-2010 bakeoff task of 
Chinese word segmentation, thus we choose 
?BI? as our baseline for simplicity, with this 1% 
lost bearing in mind. In tables of this paper, SC 
stands for Simplified Chinese and TC represents 
for Traditional Chinese. Test corpora of SC and 
TC are divided into four domains, where suffix 
A, B, C and D attached, for texts of literature, 
computer, medicine and finance, respectively. 
  R P F OOV 
SC-A OBI 0.906 0.916 0.911 0.539 
 BI 0.896 0.907 0.901 0.508 
SC-B OBI 0.868 0.797 0.831 0.410 
 BI 0.850 0.763 0.805 0.327 
SC-C OBI 0.897 0.897 0.897 0.590 
 BI 0.888 0.886 0.887 0.551 
SC-D OBI 0.900 0.903 0.901 0.472 
 BI 0.888 0.891 0.890 0.419 
TC-A OBI 0.873 0.898 0.886 0.727 
 BI 0.856 0.884 0.870 0.674 
TC-B OBI 0.906 0.932 0.919 0.578 
 BI 0.894 0.920 0.907 0.551 
TC-C OBI 0.902 0.923 0.913 0.722 
 BI 0.891 0.914 0.902 0.674 
TC-D OBI 0.924 0.934 0.929 0.765 
 BI 0.908 0.922 0.915 0.722 
Table 1. OBI vs. BI; where the lost of F > 1%, 
such as SC-B, is caused by incorrect English 
segments that will be discussed in the section 4. 
2.2 Term Contributed Boundary 
The word boundary and the word frequency are 
the standard notions of frequency in corpus-
based natural language processing, but they lack 
the correct information about the actual boun-
dary and frequency of a phrase?s occurrence. 
The distortion of phrase boundaries and frequen-
cies was first observed in the Vodis Corpus 
when the bigram ?RAIL ENQUIRIES? and tri-
gram ?BRITISH RAIL ENQUIRIES? were ex-
amined and reported by O'Boyle (1993). Both of 
them occur 73 times, which is a large number for 
such a small corpus. ?ENQUIRIES? follows 
?RAIL? with a very high probability when it is 
preceded by ?BRITISH.? However, when 
?RAIL? is preceded by words other than ?BRIT-
ISH,? ?ENQUIRIES? does not occur, but words 
like ?TICKET? or ?JOURNEY? may. Thus, the 
bigram ?RAIL ENQUIRIES? gives a misleading 
probability that ?RAIL? is followed by ?EN-
QUIRIES? irrespective of what precedes it. This 
problem happens not only with word-token cor-
pora but also with corpora in which all the com-
pounds are tagged as units since overlapping N-
grams still appear, therefore corresponding solu-
tions such as those of Zhang et al (2006) were 
proposed. 
We uses suffix array algorithm to calculate ex-
act boundaries of phrase and their frequencies 
(Sung et al, 2008), called term contributed 
boundaries (TCB) and term contributed fre-
quencies (TCF), respectively, to analogize simi-
larities and differences with the term frequencies 
(TF). For example, in Vodis Corpus, the original 
TF of the term ?RAIL ENQUIRIES? is 73. 
However, the actual TCF of ?RAIL ENQUI-
RIES? is 0, since all of the frequency values are 
contributed by the term ?BRITISH RAIL EN 
QUIRIES?. In this case, we can see that ?BRIT-
ISH RAIL ENQUIRIES? is really a more fre-
quent term in the corpus, where ?RAIL EN-
QUIRIES? is not. Hence the TCB of ?BRITISH 
RAIL ENQUIRIES? is ready for CRF tagging as 
?BRITISH/TB RAIL/TB ENQUIRIES/TI,? for 
example. 
3 Experiments 
Besides submitted results, there are several 
different experiments that we have done. The 
configuration is about the trade-off between data 
sparseness and domain fitness. For the sake of 
OOV issue, TCBs from all the training and test 
corpora are included in the configuration of 
submitted results. For potentially better consis-
tency to different types of text, TCBs from the 
training corpora and/or test corpora are grouped 
by corresponding domains of test corpora. Table 
2 and Table 3 provide the details, where the 
baseline is the character-based ?BI? tagging, and 
others are ?BI? with additional different TCB 
configurations: TCBall stands for the submitted 
results; TCBa, TCBb, TCBta, TCBtb, TCBtc, 
TCBtd represents TCB extracted from the train-
ing corpus A, B, and the test corpus A, B, C, D, 
respectively. Table 2 indicates that F1 measure 
scores can be improved by TCB about 1%, do-
main-independently. Table 3 gives a hint of the 
major contribution of performance is from TCB 
of each test corpus. 
Table 2. Baseline vs. Submitted Results 
 
 
 
 
 
 
  R P F OOV 
SC-A BI 0.896 0.907 0.901 0.508 
 TCBall 0.917 0.921 0.919 0.699 
SC-B BI 0.850 0.763 0.805 0.327 
 TCBall 0.876 0.799 0.836 0.456 
SC-C BI 0.888 0.886 0.887 0.551 
 TCBall 0.900 0.896 0.898 0.699 
SC-D BI 0.888 0.891 0.890 0.419 
 TCBall 0.910 0.906 0.908 0.562 
TC-A BI 0.856 0.884 0.870 0.674 
 TCBall 0.871 0.891 0.881 0.670 
TC-B BI 0.894 0.920 0.907 0.551 
 TCBall 0.913 0.917 0.915 0.663 
TC-C BI 0.891 0.914 0.902 0.674 
 TCBall 0.900 0.915 0.908 0.668 
TC-D BI 0.908 0.922 0.915 0.722 
 TCBall 0.929 0.922 0.925 0.732 
  F OOV 
SC-A TCBta 0.918 0.690 
 TCBa 0.917 0.679 
 TCBta + TCBa 0.917 0.690 
 TCBall 0.919 0.699 
SC-B TCBtb 0.832 0.465 
 TCBb 0.828 0.453 
 TCBtb + TCBb 0.830 0.459 
 TCBall 0.836 0.456 
SC-C TCBtc 0.897 0.618 
 TCBall 0.898 0.699 
SC-D  TCBtd 0.905 0.557 
 TCBall 0.910 0.562 
Table 3a. Simplified Chinese Domain-specific 
TCB vs. TCBall 
  F OOV 
TC-A TCBta 0.889 0.706 
 TCBa 0.888 0.690 
 TCBta + TCBa 0.889 0.710 
 TCBall 0.881 0.670 
TC-B TCBtb 0.911 0.636 
 TCBb 0.921 0.696 
 TCBtb + TCBb 0.912 0.641 
 TCBall 0.915 0.663 
TC-C TCBtc 0.918 0.705 
 TCBall 0.908 0.668 
TC-D TCBtd 0.927 0.717 
 TCBall 0.925 0.732 
Table 3b. Traditional Chinese Domain-specific 
TCB vs. TCBall 
 
4 Error Analysis 
The most significant type of error in our results 
is unintentionally segmented English words. Ra-
ther than developing another set of tag for Eng-
lish alphabets, we applies post-processing to fix 
this problem under the restriction of closed train-
ing by using only alphanumeric character infor-
mation. Table 4 compares F1 measure score of 
the Simplified Chinese experiment results before 
and after the post-processing. 
 
 
 F1 measure score 
before after 
SC-A OBI 0.911 0.918 
 BI 0.901 0.908 
 TCBta 0.918 0.920 
 TCBta + TCBa 0.917 0.920 
 TCBall 0.919 0.921 
SC-B OBI 0.831 0.920 
 BI 0.805 0.910 
 TCBtb 0.832 0.917 
 TCBtb + TCBb 0.830 0.916 
 TCBall 0.836 0.916 
SC-C OBI 0.897 0.904 
 BI 0.887 0.896 
 TCBtc 0.897 0.901 
 TCBall 0.898 0.902 
SC-D OBI 0.901 0.919 
 BI 0.890 0.908 
 TCBtd 0.905 0.915 
 TCBall 0.908 0.918 
Table 4. F1 measure scores before and after 
English Problem Fixed 
The major difference between gold standards 
of the Simplified Chinese corpora and the Tradi-
tional Chinese corpora is about non-Chinese 
characters. All of the alphanumeric and the 
punctuation sequences are separated from Chi-
nese sequences in the Simplified Chinese corpo-
ra, but can be part of the Chinese word segments 
in the Traditional Chinese corpora. For example, 
a phrase ??? / simvastatin / ? / statins? / ? / ? /
? / ?? (?/? represents the word boundary) from 
the domain C of the test data cannot be either 
recognized by ?BI? and/or TCB tagging ap-
proaches, or post-processed. This is the reason 
why Table 4 does not come along with Tradi-
tional Chinese experiment results. 
Some errors are due to inconsistencies in the 
gold standard of non-Chinese character, For ex-
ample, in the Traditional Chinese corpora, some 
percentage digits are separated from their per-
centage signs, meanwhile those percentage signs 
are connected to parentheses right next to them. 
5 Conclusion 
This paper introduces a simple CRF feature 
called term contributed boundaries (TCB) for 
Chinese word segmentation. The experiment 
result shows that it can improve the basic ?BI? 
tagging scheme about 1% of the F1 measure 
score, domain-independently. 
Further tagging scheme for non-Chinese cha-
racters are desired for recognizing some sophis-
ticated gold standard of Chinese word segmenta-
tion that concatenates alphanumeric characters 
to Chinese characters. 
Acknowledgement 
The CRF model used in this paper is developed based 
on CRF++, http://crfpp.sourceforge.net/ 
Term Contributed Boundaries used in this paper are 
extracted by YASA, http://yasa.newzilla.org/ 
References 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: proba-
bilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference of Machine Learning, 591?598. 
Peter O'Boyle. 1993. A Study of an N-Gram Lan-
guage Model for Speech Recognition. PhD thesis. 
Queen's University Belfast. 
Fuchun Peng and Andrew McCallum. 2004. Chinese 
segmentation and new word detection using condi-
tional random fields. In Proceedings of Interna-
tional Conference of Computational linguistics, 
562?568, Geneva, Switzerland. 
Cheng-Lung Sung, Hsu-Chun Yen, and Wen-Lian 
Hsu. 2008. Compute the Term Contributed Fre-
quency. In Proceedings of the 2008 Eighth Inter-
national Conference on Intelligent Systems Design 
and Applications, 325-328, Washington, D.C., 
USA. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Da-
niel Jurafsky, and Christopher Manning. 2005. A 
conditional random field word segmenter for Sig-
han bakeoff 2005. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language 
Processing, Jeju, Korea. 
Nianwen Xue and Libin Shen. 2003. Chinese word-
segmentation as LMR tagging. In Proceedings of 
the Second SIGHAN Workshop on Chinese Lan-
guage Processing. 
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumi-
ta. 2006. Subword-based tagging by conditional 
random fields for Chinese word segmentation. In 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 193-
196, New York, USA. 
 
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 76?80,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cost-benefit Analysis of Two-Stage Conditional Random Fields based 
English-to-Chinese Machine Transliteration 
 
Chan-Hung Kuoa  Shih-Hung Liuab  MikeTian-Jian Jiangac  
Cheng-Wei Leea  Wen-Lian Hsua 
aInstitute of Information Science, Academia Sinica 
bDepartment of Electrical Engineering, National Taiwan University 
cDepartment of Computer Science, National Tsing Hua University 
{laybow, journey, tmjiang, aska, hsu}@iis.sinica.edu.tw 
 
 
Abstract 
This work presents an English-to-Chinese 
(E2C) machine transliteration system based 
on two-stage conditional random fields 
(CRF) models with accessor variety (AV) 
as an additional feature to approximate 
local context of the source language. 
Experiment results show that two-stage 
CRF method outperforms the one-stage 
opponent since the former costs less to 
encode more features and finer grained 
labels than the latter. 
1 Introduction 
Machine transliteration is the phonetic 
transcription of names across languages and is 
essential in numerous natural language processing 
applications, such as machine translation, cross-
language information retrieval/extraction, and 
automatic lexicon acquisition (Li et al, 2009). It 
can be either phoneme-based, grapheme-based, or 
a hybrid of the above. The phoneme-based 
approach transforms source and target names into 
comparable phonemes for an intuitive phonetic 
similarity measurement between two names 
(Knight and Graehl, 1998; Virga and Khudanpur, 
2003). The grapheme-based approach, which treats 
transliteration as statistical machine translation 
problem under monotonic constraint, aims to 
obtain a direct orthographical mapping (DOM) to 
reduce possible errors introduced in multiple 
conversions (Li et al, 2004). The hybrid approach 
attempts to utilize both phoneme and grapheme 
information (Oh and Choi, 2006). Phoneme-based 
approaches are usually not good enough, because 
name entities have various etymological origins 
and transliterations are not always decided by 
pronunciations (Li et al, 2004). The state-of-the-
art of transliteration approach is bilingual DOMs 
without intermediate phonetic projections (Yang et 
al., 2010). 
Due to the success of CRF on sequential 
labeling problem (Lafferty et al, 2001), numerous 
machine transliteration systems applied it. Some of 
them treat transliteration as a two-stage sequential 
labeling problem: the first stage predicts syllable 
boundaries of source names, and the second stage 
uses those boundaries to get corresponding 
characters of target names (Yang et al, 2010; Qin 
and Chen, 2011). Dramatically de-creasing the cost 
of training with complex features is the major 
advantage of two-stage methods, but their 
downside is, compared to one-stage methods, 
features of target language are not directly applied 
in the first stage. 
Richer context generally gains better results of 
sequential labeling, but squeezed performance 
always comes with a price of computational 
complexity. To balance cost and benefit for 
English-to-Chinese (E2C) transliteration, this work 
compares the one-stage method with the two-stage 
one, using additional features of AV (Feng et al, 
2004) and M2M-aligner as an initial alignment  
(Jiampojamarn et al, 2007), to explore where the 
best investment reward is. 
The remainder of this paper is organized as 
follows. Section 2 briefly introduces related works, 
including two-stage methods and AV. The 
machine transliteration system using M2M-aligner, 
CRF models, and AV features in this work is 
explained in Section 3. Section 4 describes 
76
experiment results along with a discussion in 
Section 5. Finally, Section 6 draws a conclusion. 
2 Related Works 
Reddy and Waxmonsky (2009) presented a phrase-
based transliteration system that groups characters 
into substrings mapping onto target names, to 
demonstrate how a substring representation can be 
incorporated into CRF models with local context 
and phonemic information. Shishtla et al (2009) 
adopted a statistical transliteration technique that 
consists of alignment models of GIZA++ (Och and 
Ney, 2003) and CRF models. Jiang et al (2011) 
used M2M-aligner instead of GIZA++ and applied 
source grapheme?s AV in a CRF-based 
transliteration. 
A two-stage CRF-based transliteration was first 
designed to pipeline two independent processes 
(Yang et al, 2009). To recover from error 
propagations of the pipeline, a joint optimization of 
two-stage CRF method is then proposed to utilize 
n-best candidates of source name segmentations 
(Yang et al 2010). Another approach to resist 
errors from the first stage is split training data into 
pools to lessen computation cost of sophisticated 
CRF models for the second stage (Qin and Chen, 
2011). 
3 System Description  
3.1 EM for Initial Alignments 
M2M-aligner first maximizes the probability of 
observed source-target pairs using EM algorithm 
and subsequently sets alignments via maximum a 
posteriori estimation. To obtain initial alignments 
as good as possible, this work empirically sets the 
parameter ?maxX? of M2M-aligner for the 
maximum size of sub-alignments in the source side 
to 8, and sets the parameter ?maxY? for the 
maximum size of sub-alignments in the target side 
to 1 (denoted as X8Y1 in short), since one of the 
well-known a priori of Chinese is that almost all 
Chinese characters are monosyllabic. 
3.2 Format of Electronic Manuscript 
The two-stage CRF method consists of syllable 
segmentation and Chinese character conversion 
CRF models, namely Stage-1 and Stage-2, 
respectively. Stage-1 CRF model is trained with 
source name segmentations initially aligned by 
M2M-aligner to predict syllable boundaries as 
accurate as possible. According to the 
discriminative power of CRF, some syllable 
boundary errors from preliminary alignments could 
be counterbalanced. Stage-2 CRF model then sees 
predicted syllable boundaries as input to produce 
optimal target names. For CRF modeling, this 
work uses Wapiti (Lavergne et al, 2010). 
Using ?BULLOUGH? as an example, labeling 
schemes below are for Stage-1 training. 
? B/B U/B L/I L/I O/I U/I G/I H/E 
? B/S U/B L/1 L/2 O/3 U/4 G/5 H/E 
The first one is the common three-tag set ?BIE?. 
The last one is the eight-tag set ?B8?, including B, 
1-5, E and S: tag B indicates the beginning 
character of a syllable segment, tag E means the 
ending character, tag I or 1-5 stand for characters 
in-between, and tag S represents a single character 
segment. The expectation of the eight-tag set is the 
finer grained tags we used, the better segmentation 
accuracy we would gain. 
For Stage-2, two labeling schemes are listed in 
the following. 
? B/? ULLOUGH/? 
? B/? U/? L/I L/I O/I U/I G/I H/I 
The former as substring-based labeling scheme are 
commonly used in two-stage CRF-based 
transliteration. Syllable segments in a source word 
are composed from Stage-1 results and then are 
associated with corresponding Chinese characters 
(Yang et al 2009; Yang et al 2010; Qin and Chen, 
2011). The latter is a character-based labeling 
scheme where tags B or S from Stage-1 will be 
labeled with a Chinese character and others will be 
labeled as I. The merit of character-based method 
is to retrench the duration of the training, while 
substring-based method takes too much time to be 
included in this work for NEWS shared task. 
Section 5 will discuss more about pros and cons 
between substring and character based labeling 
schemes. 
This work tests numerous CRF feature 
combinations, for example: 
? C-3, C-2, C-1, C0, C1 , C2, C3 and 
? C-3C-2, C-2C-1, C-1C0, C0C1, C1C2, C2C3, where local context is ranging from -3 to 3, and Ci 
denotes the characters bound individually to the 
prediction label at its current position i. 
77
3.3 CRF with AV  
AV was for unsupervised Chinese word 
segmentation (Feng et al, 2004). Jiang et al, 
(2011) showed that using AV of source grapheme 
as CRF features could improve transliteration. In 
our two-stage system, Source AV is used in Stage-
1 in hope for better syllable segmentations, but not 
in Stage-2 since it may be redundant and surely 
increase training cost of Stage-2. 
4 Experiment Results 
4.1 Results of Standard Runs 
Four standard runs are submitted to NEWS12 E2C 
shared task. Their configurations are listed in Table 
1, where ?U? and ?B? denote observation 
combinations of unigram and bigram, respectively. 
A digit in front of a ?UB?, for example, ?2?, 
indicates local context ranging from -2 to 2. PBIE 
stands for ?BIE? tag set and PB8 is for ?B8? tag set. 
To summarize, the 4th (i.e. the primary) standard 
run exceeds 0.3 in terms of top-1 accuracy (ACC), 
and other ACCs of standard runs are approximate 
to 0.3. The 3rd standard run uses the one-stage CRF 
method to compare with the two-stage CRF 
method. Experiment results show that the two-
stage CRF method can excel the one-stage 
opponent, while AV and richer context also 
improve performance.  
4.2 Results of Inside Tests 
Numerous pilot tests have been conducted by 
training with both the training and development 
sets, and then testing on the development set, as 
?inside? tests. Three of them are shown in Table 2, 
where configurations I and II use the two-stage 
method, and configuration III is in one-stage. 
Table 2 suggests a trend that the one-stage CRF 
method performs better than the two-stage one on 
inside tests, but Table 1 votes the opposite. Since 
the development set includes semi-semantic 
transliterations that are unseen in both the training 
and the test sets (Jiang et al, 2011), models of 
inside tests are probably over-fitted to these noises. 
Table 3 further indicates that the number of 
features in the one-stage CRF method is doubled 
than that in the two-stage one. By putting these 
observations together, the two-stage CRF method 
is believed to be more effective and efficient than 
the one-stage CRF method. 
5 Discussions  
There are at least two major differences of two-
stage CRF-based transliteration between our 
approach and others. One is that we enrich the 
local context as much as possible, such as using 
eight-tag set in Stage-1. The other is using a 
character-based labeling method instead of a 
substring-based one in Stage-2. 
Reasonable alignments can cause CRF models 
troubles when a single source grapheme is mapped 
onto multiple phones. For instance, the alignment 
between ?HAX? and ????? generating by 
M2M-aligner. 
 HA ? ? 
 X ? ?? 
In this case, a single grapheme <X> pronounced as 
/ks/ in English therefore is associated with two 
Chinese characters ????, and won?t be an easy 
case to common character-based linear-chain CRF. 
Although for the sake of efficiency, this work 
adopts character-based CRF models, only a few of 
such single grapheme for consonant blends or 
diphthongs appeared in training and test data, and 
then the decline of accuracy would be moderate. 
One may want to know how high the price is for 
using a substring-based method to solve this 
problem. We explore the number of features 
between substring-based and character-based 
ID Configuration ACC Mean 
F-score
1 Two-stage, 2UB, PBIE 0.295 0.652 2 Two-stage, 2UB, PBIE, AV 0.299 0.659 3 One-stage, 3UB, PBIE, AV 0.291 0.654 4 Two-stage, 3UB, PB8, AV 0.311  0.662 
Table 1. Selected E2C standard runs 
 
ID Configuration ACC Mean F-score
I Two-stage, 2UB, PBIE, AV 0.363 0.707 II Two-stage, 3UB, PB8, AV 0.397 0.727III One-stage, 3UB, PBIE, AV 0.558 0.834 
Table 2. Selected E2C inside tests 
 
ID Number of Features  Numbers of Label 
II Stage-1: 60,496 Stage-1: 8 Stage-2: 2,567,618 Stage-2: 547 
III 4,439,896 548 
Table 3. Cost of selected E2C inside tests 
78
methods in Stage-2 with the same configuration II, 
as shown in Table 4. Features of substring-based 
method are tremendously more than character-
based one. Qin (2011) also reported similar 
observations. 
However, there is another issue in our character-
based method: only the starting position of a 
source syllable segment will be labeled as Chinese 
character, others are labeled as I. Base on this 
labeling strategy, the local context of the target 
graphemes is missing. 
6 Conclusions and Future Works  
This work analyzes cost-benefit trade-offs between 
two-stage and one-stage CRF-based methods for 
E2C transliteration. Experiment results indicate 
that the two-stage method can outperform its one-
stage opponent since the former costs less to 
encode more features and finer grained labels than 
the latter. Recommended future investigations 
would be encoding more features of target 
graphemes and utilizing n-best lattices from the 
outcome of Stage-1. 
Acknowledgments 
This research was supported in part by the National 
Science Council under grant NSC 100-2631-S-
001-001, and the research center for Humanities 
and Social Sciences under grant IIS-50-23. The 
authors would like to thank anonymous reviewers 
for their constructive criticisms. 
References  
Haodi Feng, Kang Chen, Xiaotie Deng, and Wiemin 
Zheng. 2004. Accessor Variety Criteria for Chinese 
Word Extraction. Computational Linguistics, 
30(1):75-93. 
Zellig Sabbetai Harris. 1970. Morpheme boundaries 
within words. Papers in Structural and 
Transformational Linguistics, 68-77. 
Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek 
Sherif. 2007. Applying Many-to-Many Alignments 
and Hidden Markov Models to Letter-to-Phoneme 
Conversion. Proceedings of NAACL 2007, 372-379. 
Mike Tian-Jian Jiang, Chan-Hung Kuo and Wen-Lian 
Hsu. 2011. English-to-Chinese Machine 
Transliteration using Accessor Variety Features of 
Source Graphemes. Proceedings of the 2011 Named 
Entities Workshop. 86-90. 
K. Knight and J. Graehl. 1998. Machine Transliteration. 
Computational Linguistics, 24(4):599-612. 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. Conditional Random Fields Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
Proceedings of ICML, 591-598. 
Thomas Lavergne, Oliver Capp? and Fran?ois Yvon. 
2010. Practical Very Large Scale CRF. Proceedings 
the 48th ACL, 504-513. 
Haizhou Li, Min Zhang and Jian Su. 2004. A Joint 
Source Channel Model for Machine Transliteration. 
Proceedings of the 42nd ACL, 159-166. 
Haizhou Li, A Kumaran, Min Zhang and Vladimir 
Pervouchine. 2009. Report of NEWS 2009 
Transliteration Generation Shared Task. Proceedings 
of the 2009 Named Entities Workshop. 1-18. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1):19-51. 
J. H. Oh and K. S. Choi. 2006. An Ensemble of 
Transliteration Models for Information Retrieval. 
Information Processing and Management, 42:980-
1002. 
Ying Qin. 2011. Phoneme strings based machine 
transliteration. Proceedings of the 7th IEEE 
International Conference on Natural Language 
Processing and Knowledge Engineering. 304-309. 
Ying Qin and Guohua Chen. 2011. Forward-backward 
Machine Transliteration between English and 
Chinese Base on Combined CRF. Proceedings of the 
2011 Named Entities Workshop. 82-85. 
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning 
String Edit Distance. IEEE Transactions on Pattern 
Recognition and Machine Intelligence, 20(5):522-
532. 
Sravana Reddy and Sonjia Waxmonsky. 2009. 
Substring-based transliteration with conditional 
random fields. Proceedings of the 2009 Named 
Entities Workshop, 92-95. 
Praneeth Shishtla, V. Surya Ganesh, Sethuramalingam 
Subramaniam and Vasudeva Varma. 2009. A 
language-independent transliteration schema using 
character aligned models at NEWS 2009. 
Proceedings of the 2009 Named Entities Workshop, 
40-43. 
ID Substring-based Character-Based 
II 106,070,874 2,567,618 
Table 4. Number of features between substring 
and character based method in Stage-2 
79
P. Virga and S. Khudanpur. 2003. Transliteration of 
Proper Names in Cross-lingual Information Retrieval. 
In the Proceedings of the ACL Workshop on Multi-
lingual Named Entity Recognition. 
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku 
Oonishi, Masanobu Nakamura, Sadaoki Furui. 2009. 
Combining a two-step conditional random field 
model and a joint source channel model for machine 
transliteration. Proceedings of the 2009 Named 
Entities Workshop, 72-75. 
Dong Yang, Paul Dixon and Sadaoki Furui. 2010. 
Jointly optimizing a two-step conditional random 
field model for machine transliteration and its fast 
decoding algorithm. Proceedings of the ACL 2010. 
Conference Short Papers, 275-280 
Hai Zhao and Chunyu Kit. 2008. Unsupervised 
Segmentation Helps Supervised Learning of 
Character Tagging for Word Segmentation and 
Named Entity Recognition. Proceedings of the Sixth 
SIGHAN Workshop on Chinese Language 
Processing. 
 
80
