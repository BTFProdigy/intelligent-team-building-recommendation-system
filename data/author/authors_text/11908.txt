Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 315?323,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Classifier Combination for Contextual Idiom Detection
Without Labelled Data
Linlin Li and Caroline Sporleder
Saarland University
Postfach 15 11 50
66041 Saarbr?ucken
Germany
{linlin,csporled}@coli.uni-saarland.de
Abstract
We propose a novel unsupervised approach
for distinguishing literal and non-literal use
of idiomatic expressions. Our model com-
bines an unsupervised and a supervised
classifier. The former bases its decision
on the cohesive structure of the context and
labels training data for the latter, which can
then take a larger feature space into account.
We show that a combination of both classi-
fiers leads to significant improvements over
using the unsupervised classifier alone.
1 Introduction
Idiomatic expressions are abundant in natural lan-
guage. They also often behave idiosyncratically
and are therefore a significant challenge for natural
language processing systems. For example, idioms
can violate selectional restrictions (as in push one?s
luck), disobey typical subcategorisation constraints
(e.g., in line without a determiner before line), or
change the default assignments of semantic roles
to syntactic categories (e.g., in break sth with X the
argument X would typically be an instrument but
for the idiom break the ice it is more likely to fill a
patient role, as in break the ice with Russia).
In order to deal with such idiosyncracies and as-
sign the correct analyses, NLP systems need to be
able to recognise idiomatic expressions. Much pre-
vious research on idioms has been concerned with
type-based classification, i.e., dividing expressions
into ?idiom? or ?not idiom? irrespective of their ac-
tual use in a given context. However, while some
expressions, such as by and large, always have an
idiomatic meaning, several other expressions, such
as break the ice or spill the beans, can be used liter-
ally as well as idiomatically (see examples (1) and
(2), respectively). Sometimes the literal usage can
even dominate in a domain, as for drop the ball,
which occurs fairly frequently in a literal sense in
the sports section of news texts.
(1) Dad had to break the ice on the chicken troughs so
that they could get water.
(2) Somehow I always end up spilling the beans all
over the floor and looking foolish when the clerk
comes to sweep them up.
Hence, whether a particular occurrence of a po-
tentially ambiguous expression has literal or non-
literal meaning has to be inferred from the context
(token-based idiom classification). Recently, there
has been increasing interest in this classification
task and both supervised and unsupervised tech-
niques have been proposed. The work we present
here builds on previous research by Sporleder and
Li (2009), who describe an unsupervised method
that exploits the presence or absence of cohesive
ties between the component words of a potential
idiom and its context to distinguish between literal
and non-literal use. If strong ties can be found
the expression is classified as literal otherwise as
non-literal. While this approach often works fairly
well, it has the disadvantage that it focuses exclu-
sively on lexical cohesion, other linguistic cues
that might influence the classification decision are
disregarded.
We show that it is possible to improve on
Sporleder and Li?s (2009) results by employing
a two-level strategy, in which a cohesion-based
unsupervised classifier is combined with a super-
vised classifier. We use the unsupervised classifier
to label a sub-set of the test data with high confi-
dence. This sub-set is then passed on as training
data to the supervised classifier, which then labels
the remainder of the data set. Compared to a fully
unsupervised approach, this two-stage method has
the advantage that a larger feature set can be ex-
ploited. This is beneficial for examples, in which
the cohesive ties are relatively weak but which con-
tain other linguistic cues for literal or non-literal
use.
315
2 Related Work
Most studies on idiom classification focus on type-
based classification; few researchers have worked
on token-based approaches (i.e., classification of an
expression in a given context). Type-based meth-
ods frequently exploit the fact that idioms have a
number of properties which differentiate them from
other expressions. For example, they often exhibit
a degree of syntactic and lexical fixedness. Some
idioms, for instance, do not allow internal modi-
fiers (*shoot the long breeze) or passivisation (*the
bucket was kicked). They also typically only al-
low very limited lexical variation (*kick the vessel,
*strike the bucket).
Many approaches for identifying idioms focus
on one of these two aspects. For instance, measures
that compute the association strength between the
elements of an expression have been employed
to determine its degree of compositionality (Lin,
1999; Fazly and Stevenson, 2006) (see also Villav-
icencio et al (2007) for an overview and a com-
parison of different measures). Other approaches
use Latent Semantic Analysis (LSA) to determine
the similarity between a potential idiom and its
components (Baldwin et al, 2003). Low similar-
ity is supposed to indicate low compositionality.
Bannard (2007) looks at the syntactic fixedness
of idiomatic expressions, i.e., how likely they are
to take modifiers or be passivised, and compares
this to what would be expected based on the ob-
served behaviour of the component words. Fazly
and Stevenson (2006) combine information about
syntactic and lexical fixedness (i.e., estimated de-
gree of compositionality) into one measure.
The few token-based approaches include a study
by Katz and Giesbrecht (2006), who devise a super-
vised method in which they compute the meaning
vectors for the literal and non-literal usages of a
given expression in the training data. An unseen
test instance of the same expression is then labelled
by performing a nearest neighbour classification.
Birke and Sarkar (2006) model literal vs. non-
literal classification as a word sense disambiguation
task and use a clustering algorithm which compares
test instances to two automatically constructed seed
sets (one with literal and one with non-literal ex-
pressions), assigning the label of the closest set.
While the seed sets are created without immediate
human intervention they do rely on manually cre-
ated resources such as databases of known idioms.
Cook et al (2007) and Fazly et al (2009) pro-
pose an alternative method which crucially relies
on the concept of canonical form, which is a fixed
form (or a small set of those) corresponding to the
syntactic pattern(s) in which the idiom normally
occurs (Riehemann, 2001).1 The canonical form
allows for inflectional variation of the head verb but
not for other variations (such as nominal inflection,
choice of determiner etc.). It has been observed that
if an expression is used idiomatically, it typically
occurs in its canonical form. For example, Riehe-
mann (2001, p. 34) found that for decomposable
idioms 75% of the occurrences are in canonical
form, rising to 97% for non-decomposable idioms.2
Cook et al exploit this behaviour and propose an
unsupervised method which classifies an expres-
sion as idiomatic if it occurs in canonical form and
literal otherwise.
Finally, in earlier work, we proposed an unsu-
pervised method which detects the presence or ab-
sence of cohesive links between the component
words of the idiom and the surrounding discourse
(Sporleder and Li, 2009). If such links can be found
the expression is classified as ?literal? otherwise as
?non-literal?. In this paper we show that the per-
formance of such a classifier can be significantly
improved by complementing it with a second-stage
supervised classifier.
3 First Stage: Unsupervised Classifier
As our first-stage classifier, we use the unsuper-
vised model proposed by Sporleder and Li (2009).
This model exploits the fact that words in a co-
herent discourse exhibit lexical cohesion (Halliday
and Hasan, 1976), i.e. concepts referred to in sen-
tences are typically related to other concepts men-
tioned elsewhere in the discourse. Given a suitable
measure of semantic relatedness, it is possible to
compute the strength of such cohesive ties between
pairs of words. While the component words of
literally used expressions tend to exhibit lexical co-
hesion with their context, the words of non-literally
used expressions do not. For example, in (3) the ex-
pression play with fire is used literally and the word
fire is related to surrounding words like grilling,
dry-heat, cooking, and coals. In (4), however play
with fire is used non-literally and cohesive ties be-
1This is also the form in which an idiom is usually listed
in a dictionary.
2Decomposable idioms are expressions such as spill the
beans which have a composite meaning whose parts can be
mapped to the words of the expression (e.g., spill??reveal?,
beans??secret?).
316
tween play or fire and the context are absent.
(3) Grilling outdoors is much more than just another
dry-heat cooking method. It?s the chance to
play with fire, satisfying a primal urge to stir around
in coals .
(4) And PLO chairman Yasser Arafat has accused Israel
of playing with fire by supporting HAMAS in its
infancy.
To determine the strength of cohesive links, the
unsupervised model builds a graph structure (called
cohesion graph) in which all pairs of content words
in the context are connected by an edge which is
weighted by the pair?s semantic relatedness. Then
the connectivity of the graph is computed, defined
as the average edge weight. If the connectivity
increases when the component words of the idiom
are removed, then there are no strong cohesive ties
between the expression and the context and the
example is labelled as ?non-literal?, otherwise it is
labelled as ?literal?.
To model semantic distance, we use the Nor-
malized Google Distance (NGD, see Cilibrasi and
Vitanyi (2007)), which computes relatedness on the
basis of page counts returned by a search engine.
3
It is defined as follows:
NGD(x, y) =
max{log f(x), log f(y)} ? log f(x, y)
log M ?min{log f(x), log f(y)}
(5)
where x and y are the two words whose association
strength is computed (e.g., fire and coal), f(x) is
the page count returned by the search engine for x
(and likewise for f(y) and y), f(x, y) is the page
count returned when querying for ?x AND y?, and
M is the number of web pages indexed by the
search engine. The basic idea is that the more often
two terms occur together, relative to their overall
occurrence, the more closely they are related.
We hypothesise that the unsupervised classifier
will give us relatively good results for some exam-
ples. For instance, in (3) there are several strong
cues which suggest that play with fire is used liter-
ally. However, because the unsupervised classifier
only looks at lexical cohesion, it misses many other
clues which could help distinguish literal and non-
literal usages. For example, if break the ice is
followed by the prepositions between or over as in
example (6), it is more likely to be used idiomati-
cally (at least in the news domain).
(6) ?Gujral will meet Sharif on Monday and discuss
bilateral relations,? the Press Trust of India added.
3We employ Yahoo! rather than Google since we found
that it returns more stable counts.
The minister said Sharif and Gujral would be able
to break the ice over Kashmir.
Furthermore, idiomatic usages also exhibit co-
hesion with their context but the cohesive ties are
with the non-literal meaning of the expression. For
example, in news texts, break the ice in its figu-
rative meaning often co-occurs with discuss, rela-
tions, talks or diplomacy (see (6)). At the moment
we do not have any way to model these cohesive
links, as we do not know the non-literal meaning
of the idiom.4 However if we had labelled data we
could train a supervised classifier to learn these and
other contextual clues. The trained classifier might
then be able to correctly classify examples which
were misclassified by the unsupervised classifier,
i.e., examples in which the cohesive ties are weak
but where other clues exist which indicate how the
expression is used.
For example, in (7) there is weak cohesive evi-
dence for a literal use of break the ice, due to the
semantic relatedness between ice and water. How-
ever, there are stronger cues for non-literal usage,
such as the preposition between and the presence
of words like diplomats and talks, which are in-
dicative of idiomatic usage. Examples like this
are likely to be misclassified by the unsupervised
model; a supervised classifier, on the other hand,
has a better chance to pick up on such additional
cues and predict the correct label.
(7) Next week the two diplomats will meet in an attempt
to break the ice between the two nations. A crucial
issue in the talks will be the long-running water
dispute.
4 Second Stage: Supervised Classifier
For the supervised classifier, we used Support Vec-
tor Machines as implemented by the LIBSVM
package.5 We implemented four types of features,
which encode both cohesive information and word
co-occurrence more generally.
6
4It might be possible to compute the Normalized Google
Distance between the whole expression and the words in the
context, assuming that whenever the whole expression occurs
it is much more likely to be used figuratively than literally.
For expressions in canonical form this is indeed often the
case (Riehemann, 2001), however there are exceptions (see
Section 6.1) for which such an approach would not work.
5Available from: http://www.csie.ntu.edu.tw/
?cjlin/libsvm/ We used the default parameters.
6
We also experimented with linguistically more informed
features, such as the presence of named entities in the local
context of the expression, and properties of the subject or
co-ordinated verbs, but we found that these features did not
lead to a better performance of the supervised classifier. This
is probably partly due to data sparseness.
317
Salient Words (salW) This feature aims to iden-
tify words which are particularly salient for literal
usage. We used a frequency-based definition of
salience and computed the literal saliency score for
each word in a five-paragraph context around the
target expression:
sal
lit
(w) =
log f
lit
(w)? i
lit
(w)
log f
nonlit
(w)? i
nonlit
(w)
(8)
where sal
lit
(w) is the saliency score of the word w
for the class lit; f
lit
(w) is the token frequency of
the word w for literally used expressions; i
lit
(w) is
the number of instances of the target expressions
classified as lit which co-occur with word w (and
mutatis mutandis nonlit for target expressions la-
belled as non-literal).
7
Words with a high sal
lit
occur much more fre-
quently with literal usages than with non-literal
ones. Conversely, words with a low sal
lit
should
be more indicative of the non-literal class. How-
ever, we found that, in practice, the measure is
better at picking out indicative words for the literal
class; non-literal usages tend to co-occur with a
wide range of words. For example, among the high-
est scoring words for break the ice we find thick,
bucket, cold, water, reservoir etc. While we do find
words like relations, diplomacy, discussions among
the lowest scoring terms (i.e., terms indicative of
the non-literal class), we also find a lot of noise
(ask, month). The effect is even more pronounced
for other expressions (like drop the ball) which
tend to be used idiomatically in a wider variety
of situations (drop the ball on a ban of chemical
weapons, drop the ball on debt reduction etc.).
We implement the saliency score in our model by
encoding for the 300 highest scoring words whether
the word is present in the context of a given exam-
ple and how frequently it occurs.8 Note that this
feature (as well as the next one) can be computed in
a per-idiom or a generic fashion. In the former case,
we would encode the top 300 words separately for
each idiom in the training set, in the latter across all
idioms (with the consequence that more frequent
7Our definition of sal
lit
bears similarities with the well
known tf.idf score. We include both the term frequencies
(f
lit
) and the instance frequencies (i
lit
) in the formula because
we believe both are important. However, the instance fre-
quency is more informative and less sensitive to noise because
it indicates that expression classified as ?literal? consistently
co-occurs with the word in question. Therefore we weight
down the effect of the term frequency by taking its log.
8We also experimented with different feature dimensions
besides 300 but did not find a big difference in performance.
idioms in the training set contribute to more po-
sitions in the feature vector). We found that, in
practice, it does not make a big difference which
variant is used. Moreover, in our bootstrapping
scenario, we cannot ensure that we have sufficient
examples of each idiom in the training set to train
separate classifiers, so we opted for generic models
throughout all experiments.
Related Words (relW) This feature set is a vari-
ant of the previous one. Here we score the words
not based on their saliency but we determine the
semantic relatedness between the noun in the id-
iomatic expression and each word in the global
context, using the Normalized Google Distance
mentioned in Section 3. Again we encode the 300
top-scoring words.
While the related words feature is less prone to
overestimation of accidental co-occurrence than the
saliency feature, it has the disadvantage of conflat-
ing different word senses. For example, among the
highest scoring words for ice are cold, melt, snow,
skate, hockey but also cream, vanilla, dessert.
Relatedness Score (relS) The fourth feature set
implements the relatedness score which encodes
the scores for the 100 most highly weighted edges
in the cohesion graph of an instance.9 If these
scores are high, there are many cohesive ties with
the surrounding discourse and the target expression
is likely to be used literally.
Discourse Connectivity (connect.) Finally, we
implemented two features which look at the cohe-
sion graph of an instance. We encode the connec-
tivity of the graph (i) when the target expression
is included and (ii) when it is excluded. The un-
supervised classifier uses the difference between
these two values to make its prediction. By encod-
ing the absolute connectivity values as features we
enable the supervised classifier to make use of this
information as well.
5 Combining the Classifiers
As mentioned before, we use the unsupervised clas-
sifier to label an initial training set for the super-
vised one. To ensure that the training set does
not contain too much noise, we only add those ex-
amples about which the unsupervised classifier is
9We only used the 100 highest ranked edges because we
are looking at a specific context here rather than the contexts
of the literal or non-literal class overall. Since the contexts we
use are only five paragraphs long, recording the 100 strongest
edges seems sufficient.
318
most confident. We thus need to address two ques-
tions: (i) how to define a confidence function for
the unsupervised classifier, and (ii) how to set the
confidence threshold governing what proportion of
the data set is used for training the second classifier.
The first question is relatively easy to answer:
as the unsupervised classifier bases its decision on
the difference in connectivity between including or
excluding the component words of the idiom in the
cohesion graph, an obvious choice for a confidence
function is the difference in connectivity; i.e., the
higher the difference, the higher the confidence of
the classifier in the predicted label.
The confidence threshold could be selected on
the basis of the unsupervised classifier?s perfor-
mance on a development set. Note that when choos-
ing such a threshold there is usually a trade-off be-
tween the size of the training set and the amount of
noise in it: the lower the threshold, the larger and
the noisier the training set. Ideally we would like
a reasonably-sized training set which is also rela-
tively noise-free, i.e., does not contain too many
wrongly labelled examples. One way to achieve
this is to start with a relatively small training set
and then expand it gradually.
A potential problem for the supervised classifier
is that our data set is relatively imbalanced, with
the non-literal class being four times as frequent
as the literal class. Supervised classifiers often
have problems with imbalanced data and tend to be
overly biased towards the majority class (see, e.g.,
Japkowicz and Stephen (2002)). To overcome this
problem, we experimented with boosting the literal
class with additional examples.
10
We describe our
methods for training set enlargement and boosting
the literal class in the remainder of this section.
Iteratively Enlarging the Training Set A typi-
cal method for increasing the training set is to go
through several iterations of enlargement and re-
training.11 We adopt a conservative enlargement
strategy: we only consider instances on whose la-
bels both classifiers agree and we use the confi-
dence function of the unsupervised classifier to
determine which of these examples to add to the
training set. The motivation for this is that we hy-
pothesise that the supervised classifier will not have
10Throughout this paper, we use the term ?boosting? in a
non-technical sense.
11In our case re-training also involves re-computing the
ranked lists of salient and related words. As the process goes
on the classifier will be able to discover more and more useful
cue words and encode them in the feature vector.
a very good performance initially, as it is trained
on a very small data set. As a consequence its con-
fidence function may also not be very accurate. On
the other hand, we know from Sporleder and Li
(2009) that the unsupervised classifier has a rea-
sonably good performance. So while we give the
supervised classifier a veto-right, we do not allow
it to select new training data by itself or overturn
classifications made by the unsupervised classifier.
A similar strategy was employed by Ng and
Cardie (2003) in a self-training set-up. However,
while they use an ensemble of supervised classi-
fiers, which they re-train after each iteration, we
can only re-train the second classifier; the first one,
being unsupervised, will never change its predic-
tion. Hence it does not make sense to go through
a large number of iterations; the more iterations
we go through, the closer the performance of the
combined classifier will be to that of the unsuper-
vised one because that classifier will label a larger
and larger proportion of the data. However, going
through one or two iterations allows us to slowly
enlarge the training set and thereby gradually im-
prove the performance of the supervised classifier.
In each iteration, we select 10% of the remain-
ing examples to be added to the training set.12
We could simply add those 10% of the data about
which the unsupervised classifier is most confident,
but if the classifier was more confident about one
class than about the other, we would risk obtain-
ing a severely imbalanced training set. Hence, we
decided to separate examples classified as ?literal?
from those classified as ?non-literal? and add the
top 10% from each set. Provided the automatic
classification is reasonably accurate, this will en-
sure that the distribution of classes in the training
set is roughly similar to that in the overall data set
at least at the early stages of the bootstrapping.
Boosting the Literal Class As the process goes
on, we are still likely to introduce more and more
imbalance in the training set. This is due to the
fact that the supervised classifier is likely to have
some bias towards the majority class (and our ex-
periments in Section 6.2 suggest that this is indeed
the case). Hence, as the bootstrapping process goes
on, potentially more and more examples will be
labelled as ?non-literal? and if we always select the
top 10% of these, our training set will gradually
12Since we do not have a separate development set, we
chose the value of 10% intuitively as it seemed a reasonably
good threshold.
319
become more imbalanced. This is a well-known
problem for bootstrapping approaches (Blum and
Mitchell, 1998; Le et al, 2006). We could coun-
teract this by selecting a higher proportion of ex-
amples labelled as ?literal?. However given that
the number of literal examples in our data set is
relatively small, we would soon deplete our literal
instance pool and moreover, because we would be
forced to add less confidently labelled examples
for the literal class, we are likely to introduce more
noise in the training set.
A better option is to boost the literal class with
external examples. To do this we exploit the
fact that non-canonical forms of idioms are highly
likely to be used literally. Given that our data set
only contains canonical forms (see Section 6.1), we
automatically extract non-canonical form variants
and label them as ?literal?. To generate possible
variants, we either (i) change the number of the
noun (e.g., rock the boat becomes rock the boats),
(ii) change the determiner (e.g., rock a boat), or (iii)
replace the verb or noun by one of its synonyms,
hypernyms, or siblings from WordNet (e.g., rock
the ship). While this strategy does not give us addi-
tional literal examples for all idioms, for example
we were not able to find non-canonical form occur-
rences of sweep under the carpet in the Gigaword
corpus, for most idioms we were able to gener-
ate additional examples. Note that this data set is
potentially noisy as not all non-canonical form ex-
amples are used literally. However, when checking
a small sample manually, we found that only very
small percentage (<< 1%) was mis-labelled.
To reduce the classifier bias when enlarging the
training set, we add additional literal examples dur-
ing each iteration to ensure that the class distri-
bution does not deviate too much from the dis-
tribution originally predicted by the unsupervised
classifier.13 The examples to be added are selected
randomly but we try to ensure that each idiom is
represented. When reporting the results, we disre-
gard these additional external examples.
6 Experiments and Results
We carried out a number of different experiments.
In Section 6.2 we investigate the performance of
the different features of the supervised classifier
and in Section 6.3 we look more closely at the
13We are assuming that the true distribution is not known
and use the predictions of the unsupervised classifier to ap-
proximate the true distribution.
behaviour of the combined classifier. We start by
describing the data set.
6.1 Data
We used the data from Sporleder and Li (2009),
which consist of 17 idioms that can be used both
literally and non-literally (see Table 1). For each
expression, all canonical form occurrences were
extracted from the Gigaword corpus together with
five paragraphs of context and labelled as ?literal?
or ?non-literal?.14 The inter-annotator agreement
on a small sample of doubly annotated examples
was 97% and the kappa score 0.7 (Cohen, 1960).
non-
expression literal literal all
back the wrong horse 0 25 25
bite off more than one can chew 2 142 144
bite one?s tongue 16 150 166
blow one?s own trumpet 0 9 9
bounce off the wall* 39 7 46
break the ice 20 521 541
drop the ball* 688 215 903
get one?s feet wet 17 140 157
pass the buck 7 255 262
play with fire 34 532 566
pull the trigger* 11 4 15
rock the boat 8 470 478
set in stone 9 272 281
spill the beans 3 172 175
sweep under the carpet 0 9 9
swim against the tide 1 125 126
tear one?s hair out 7 54 61
all 862 3102 3964
Table 1: Idiom statistics (* indicates expressions
for which the literal usage is more common than
the non-literal one)
6.2 Feature Analysis for the Supervised
Classifier
In a first experiment, we tested the contribution of
the different features (Table 2). For each set, we
trained a separate classifier and tested it in 10-fold
cross-validation mode. We also tested the perfor-
mance of the first three features combined (salient
and related words and relatedness score) as we
wanted to know whether their combination leads
to performance gains over the individual classifiers.
Moreover, testing these three features in combi-
nation allows us to assess the contribution of the
connectivity feature, which is most closely related
to the unsupervised classifier. We report the accu-
racy, and because our data are fairly imbalanced,
14The restriction to canonical forms was motivated by the
fact that for the mostly non-decomposable idioms in the set,
the vast majority (97%) of non-canonical form occurrences
will be used literally (see Section 2).
320
also the F-Score for the minority class (?literal?).
Avg. literal (%) Avg. (%)
Feature Prec. Rec. F-Score Acc.
salW 77.10 56.10 65.00 86.83
relW 78.00 43.20 55.60 84.99
relS 74.90 37.50 50.00 83.68
connectivity 78.30 2.10 4.10 78.58
salW+relW+relS 82.90 63.50 71.90 89.20
all 85.80 66.60 75.00 90.34
Table 2: Performance of different feature sets, 10-
fold cross-validation
It can be seen that the salient words (salW) fea-
ture has the highest performance of the individual
features, both in terms of accuracy and in terms of
literal F-Score, followed by related words (relW),
and relatedness score (relS). Intuitively, it is plausi-
ble that the saliency feature performs quite well as
it can also pick up on linguistic indicators of idiom
usage that do not have anything to do with lexical
cohesion. However, a combination of the first three
features leads to an even better performance, sug-
gesting that the features do indeed model somewhat
different aspects of the data.
The performance of the connectivity feature is
also interesting: while it does not perform very well
on its own, as it over-predicts the non-literal class, it
noticeably increases the performance of the model
when combined with the other features, suggesting
that it picks up on complementary information.
6.3 Testing the Combined Classifier
We experimented with different variants of the
combined classifier. The results are shown in Ta-
ble 3. In particular, we looked at: (i) combining the
two classifiers without training set enlargement or
boosting of the literal class (combined), (ii) boost-
ing the literal class with 200 automatically labelled
non-canonical form examples (combined+boost),
(iii) enlarging the training set by iteration (com-
bined+it), and (iv) enlarging the training set by
iteration and boosting the literal class after each
iteration (combined+boost+it). The table shows
the literal precision, recall and F-Score of the com-
bined model (both classifiers) on the complete data
set (excluding the extra literal examples). Note that
the results for the set-ups involving iterative train-
ing set enlargement are optimistic: since we do not
have a separate development set, we report the op-
timal performance achieved during the first seven
iterations. In a real set-up, when the optimal num-
ber of iterations is chosen on the basis of a separate
data set, the results may be lower. The table also
shows the majority class baseline (Base
maj
), and
the overall performance of the unsupervised model
(unsup) and the supervised model when trained in
10-fold cross-validation mode (super 10CV).
Model Prec
l
Rec
l
F-Score
l
Acc.
Base
maj
- - - 78.25
unsup. 50.04 69.72 58.26 78.38
combined 83.86 45.82 59.26 86.30
combined+boost 70.26 62.76 66.30 86.13
combined+it
?
85.68 46.52 60.30 86.68
combined+boost+it
?
71.86 66.36 69.00 87.03
super. 10CV 85.80 66.60 75.00 90.34
Table 3: Results for different classifiers; ? indicates
best performance (optimistic)
It can be seen that the combined classifier is 8%
more accurate than both the majority baseline and
the unsupervised classifier. This amounts to an
error reduction of over 35% (the difference is sta-
tistically significant, ?2 test, p << 0.01). While
the F-Score of the unboosted combined classifier is
comparable to that of the unsupervised one, boost-
ing the literal class leads to a 7% increase, due
to a significantly increased recall, with no signif-
icant drop in accuracy. These results show that
complementing the unsupervised classifier with a
supervised one, can lead to tangible performance
gains. Note that the accuracy of the combined clas-
sifier, which uses no manually labelled training
data, is only 4% below that of a fully supervised
classifier; in other words, we do not lose much by
starting with an automatically labelled data set. It-
erative enlargement of the training set can lead to
further improvements, especially when combined
with boosting to reduce the classifier bias.
To get a better idea of the effect of training set
enlargement, we plotted the accuracy and F-Score
of the combined classifier for a given number of
iterations with boosting (Figure 1) and without (Fig-
ure 2). It can be seen that enlargement has a notice-
able positive effect if combined with boosting. If
the literal class is not boosted, the increasing bias
of the classifier seems to outweigh most of the pos-
itive effects from the enlarged training set. Figure 1
also shows that the best performance is obtained af-
ter a relatively small number of iterations (namely
two), as expected.15 With more iterations the per-
formance decreases again. However, it decays rel-
15Note that this also depends on the confidence threshold.
For example, if a threshold of 5% is chosen, more iterations
may be required for optimal performance.
321
atively gracefully and even after seven iterations,
when more than 40% of the data are classified by
the unsupervised classifier, the combined classifier
still achieves an overall performance that is sig-
nificantly above that of the unsupervised classifier
(84.28% accuracy compared to 78.38%, significant
at p << 0.01). Hence, the combined classifier
seems not to be very sensitive to the exact number
of iterations and performs reasonably well even if
the number of iterations is sub-optimal.
1
2
3
4
5
6
7
Iterat
ions
50
50
55
55
60
60
65
65
70
70
75
75
80
80
85
85
90
90
95
95
Performance
Acc. 
comb
ined
Acc. 
unsu
pervi
sed
F-Sco
re co
mbin
ed
F-Sco
re un
super
vised
Figure 1: Accuracy and literal F-Score on complete
data set after different iterations with boosting of
the literal class
1
2
3
4
5
6
7
Iterat
ion
50
50
55
55
60
60
65
65
70
70
75
75
80
80
85
85
90
90
95
95
Performance
2
4
6
Acc. 
comb
ined
Acc. 
unsu
pervi
sed
F-Sco
re co
mbin
ed
F-Sco
re un
super
vised
Figure 2: Accuracy and literal F-Score on complete
data set after different iterations without boosting
of the literal class
Figure 3 shows how the training set increases
as the process goes on16 and how the number of
mis-classifications in the training set develops. In-
terestingly, when going from the first to the second
iteration the training set nearly doubles (from 396
to 669 instances), while the proportion of errors is
also reduced by a third (from 7% to 5%). Hence,
the training set does not only grow but the pro-
portion of noise in it decreases, too. This shows
16
Again, we disregard the extra literal examples here.
that our conservative enlargement strategy is fairly
successful in selecting correctly labelled examples.
Only at later stages, when the classifier bias takes
over, does the proportion of noise increase again.
400
600
800
1000
1200
1400
1600
Items
 in Tr
ainin
g Set
44.555.566.577.58
Classification Error
traini
ng se
t erro
r rate
Figure 3: Training set size and error in training set
at different iterations
7 Conclusion
We presented a two-stage classification approach
for distinguishing literal and non-literal use of id-
iomatic expressions. Our approach complements
an unsupervised classifier, which exploits informa-
tion about the cohesive structure of the discourse,
with a supervised classifier. The latter can make
use of a range of features and therefore base its
classification decision on additional properties of
the discourse, besides lexical cohesion. We showed
that such a combined classifier can lead to a sig-
nificant reduction of classification errors. Its per-
formance can be improved further by iteratively
increasing the training set in a bootstrapping loop
and by adding additional examples of the literal
class, which is typically the minority class. We
found that such examples can be obtained automat-
ically by extracting non-canonical variants of the
target idioms from an unlabelled corpus.
Future work should look at improving the su-
pervised classifier, which so far has an accuracy
of 90%. While this is already pretty good, a more
sophisticated model might lead to further improve-
ments. For example, one could experiment with
linguistically more informed features. While our
initial studies in this direction were negative, care-
ful feature engineering might lead to better results.
Acknowledgements
This work was funded by the Cluster of Excellence ?Multi-
modal Computing and Interaction?.
322
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL 2003 Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment.
Colin Bannard. 2007. A measure of syntactic flex-
ibility for automatically identifying multiword ex-
pressions in corpora. In Proceedings of the ACL-07
Workshop on A Broader Perspective on Multiword
Expressions.
Julia Birke and Anoop Sarkar. 2006. A clustering
approach for the nearly unsupervised recognition of
nonliteral language. In Proceedings of EACL-06.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of COLT-98.
Rudi L. Cilibrasi and Paul M.B. Vitanyi. 2007. The
Google similarity distance. IEEE Trans. Knowledge
and Data Engineering, 19(3):370?383.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurements, 20:37?46.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic
forms for the automatic identification of idiomatic
expressions in context. In Proceedings of the ACL-
07 Workshop on A Broader Perspective on Multi-
word Expressions.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In Proceedings of EACL-06.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Computational Linguistics,
35(1):61?103.
M.A.K. Halliday and R. Hasan. 1976. Cohesion in
English. Longman House, New York.
Nathalie Japkowicz and Shaju Stephen. 2002. The
class imbalance problem: A systematic study. In-
telligent Data Analysis Journal, 6(5):429?450.
Graham Katz and Eugenie Giesbrecht. 2006. Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the ACL/COLING-06 Workshop on
Multiword Expressions: Identifying and Exploiting
Underlying Properties.
Anh-Cuong Le, Akira Shimazu, and Le-Minh Nguyen.
2006. Investigating problems of semi-supervised
learning for word sense disambiguation. In Proc.
ICCPOL-06.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-99,
pages 317?324.
Vincent Ng and Claire Cardie. 2003. Weakly super-
vised natural language learning without redundant
views. In Proc. of HLT-NAACL-03.
Susanne Riehemann. 2001. A Constructional Ap-
proach to Idioms and Word Formation. Ph.D. thesis,
Stanford University.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. In Proceedings of EACL-09.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In Proceedings
of EMNLP-07.
323
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 754?762,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Unsupervised Recognition of Literal and Non-Literal Use
of Idiomatic Expressions
Caroline Sporleder and Linlin Li
Saarland University
Postfach 15 11 50
66041 Saarbru?cken, Germany
{csporled,linlin}@coli.uni-saarland.de
Abstract
We propose an unsupervised method for
distinguishing literal and non-literal us-
ages of idiomatic expressions. Our
method determines how well a literal inter-
pretation is linked to the overall cohesive
structure of the discourse. If strong links
can be found, the expression is classified
as literal, otherwise as idiomatic. We show
that this method can help to tell apart lit-
eral and non-literal usages, even for id-
ioms which occur in canonical form.
1 Introduction
Texts frequently contain expressions whose mean-
ing is not strictly literal, such as metaphors or id-
ioms. Non-literal expressions pose a major chal-
lenge to natural language processing as they often
exhibit lexical and syntactic idiosyncrasies. For
example, idioms can violate selectional restric-
tions (as in push one?s luck under the assumption
that only concrete things can normally be pushed),
disobey typical subcategorisation constraints (e.g.,
in line without a determiner before line), or change
the default assignments of semantic roles to syn-
tactic categories (e.g., in break sth with X the ar-
gument X would typically be an instrument but for
the idiom break the ice it is more likely to fill a
patient role, as in break the ice with Russia).
To avoid erroneous analyses, a natural language
processing system should recognise if an expres-
sion is used non-literally. While there has been a
lot of work on recognising idioms (see Section 2),
most previous approaches have focused on a type-
based classification, dividing expressions into ?id-
iom? or ?not an idiom? irrespective of their actual
use in a discourse context. However, while some
expressions, such as by and large, always have a
non-compositional, idiomatic meaning, many id-
ioms, such as break the ice or spill the beans, share
their linguistic form with perfectly literal expres-
sions (see examples (1) and (2), respectively). For
some expressions, such as drop the ball, the lit-
eral usage can even dominate in some domains.
Hence, whether a potentially ambiguous expres-
sion has literal or non-literal meaning has to be
inferred from the discourse context.
(1) Dad had to break the ice on the chicken troughs so
that they could get water.
(2) Somehow I always end up spilling the beans all
over the floor and looking foolish when the clerk
comes to sweep them up.
Type-based idiom classification thus only ad-
dresses part of the problem. While it can au-
tomatically compile lists of potentially idiomatic
expressions, it does not say anything about the
idiomaticity of an expression in a particular
context. In this paper, we propose a novel,
cohesion-based approach for detecting non-literal
usages (token-based idiom classification). Our
approach is unsupervised and similar in spirit to
Hirst and St-Onge?s (1998) method for detecting
malapropisms. Like them, we rely on the presence
or absence of cohesive links between the words in
a text. However, unlike Hirst and St-Onge we do
not require a hand-crafted resource like WordNet
or Roget?s Thesaurus; our approach is knowledge-
lean.
2 Related Work
Most studies on idiom classification focus on type-
based classification; few researchers have worked
on token-based approaches. Type-based meth-
ods frequently exploit the fact that idioms have
754
a number of properties which differentiate them
from other expressions. Apart from not having a
(strictly) compositional meaning, they also exhibit
some degree of syntactic and lexical fixedness. For
example, some idioms do not allow internal modi-
fiers (*shoot the long breeze) or passivisation (*the
bucket was kicked). They also typically only al-
low very limited lexical variation (*kick the vessel,
*strike the bucket).
Many approaches for identifying idioms focus
on one of these two aspects. For instance, mea-
sures that compute the association strength be-
tween the elements of an expression have been
employed to determine its degree of composition-
ality (Lin, 1999; Fazly and Stevenson, 2006) (see
also Villavicencio et al (2007) for an overview
and a comparison of different measures). Other
approaches use Latent Semantic Analysis (LSA)
to determine the similarity between a potential id-
iom and its components (Baldwin et al, 2003).
Low similarity is supposed to indicate low com-
positionality. Bannard (2007) proposes to iden-
tify idiomatic expressions by looking at their syn-
tactic fixedness, i.e., how likely they are to take
modifiers or be passivised, and comparing this to
what would be expected based on the observed
behaviour of the component words. Fazly and
Stevenson (2006) combine information about syn-
tactic and lexical fixedness (i.e., estimated degree
of compositionality) into one measure.
The few token-based approaches include a
study by Katz and Giesbrecht (2006), who devise
a supervised method in which they compute the
meaning vectors for the literal and non-literal us-
ages of a given expression in the training data. An
unseen test instance of the same expression is then
labelled by performing a nearest neighbour classi-
fication. They report an average accuracy of 72%,
though their evaluation is fairly small scale, using
only one expression and 67 instances. Birke and
Sarkar (2006) model literal vs. non-literal classi-
fication as a word sense disambiguation task and
use a clustering algorithm which compares test in-
stances to two automatically constructed seed sets
(one with literal and one with non-literal expres-
sions), assigning the label of the closest set. While
the seed sets are created without immediate human
intervention they do rely on manually created re-
sources such as databases of known idioms.
Cook et al (2007) and Fazly et al (To appear)
propose an alternative method which crucially re-
lies on the concept of canonical form (CForm).
It is assumed that for each idiom there is a fixed
form (or a small set of those) corresponding to
the syntactic pattern(s) in which the idiom nor-
mally occurs (Riehemann, 2001).1 The canoni-
cal form allows for inflectional variation of the
head verb but not for other variations (such as
nominal inflection, choice of determiner etc.). It
has been observed that if an expression is used
idiomatically, it typically occurs in its canonical
form. For example, Riehemann (2001, p. 34)
found that for decomposable idioms 75% of the
occurrences are in canonical form, rising to 97%
for non-decomposable idioms.2 Cook et al ex-
ploit this behaviour and propose an unsupervised
method in which an expression is classified as id-
iomatic if it occurs in canonical form and literal
otherwise. Canonical forms are determined auto-
matically using a statistical, frequency-based mea-
sure. The authors report an average accuracy of
72% for their classifier.
3 Using Lexical Cohesion to Identify
Idiomatic Expressions
3.1 Lexical Cohesion
In this paper we exploit lexical cohesion to detect
idiomatic expressions. Lexical cohesion is a prop-
erty exhibited by coherent texts: concepts referred
to in individual sentences are typically related to
other concepts mentioned elsewhere (Halliday and
Hasan, 1976). Such sequences of semantically re-
lated concepts are called lexical chains. Given
a suitable measure of semantic relatedness, such
chains can be computed automatically and have
been used successfully in a number of NLP appli-
cations, starting with Hirst and St-Onge?s (1998)
seminal work on detecting real-word spelling er-
rors. Their approach is based on the insight that
misspelled words do not ?fit? their context, i.e.,
they do not normally participate in lexical chains.
Content words which do not belong to any lexi-
cal chain but which are orthographically close to
words which do, are therefore good candidates for
spelling errors.
Idioms behave similarly to spelling errors in
that they typically also do not exhibit a high de-
1This is also the form in which an idiom is usually listed
in a dictionary.
2Decomposable idioms are expressions such as spill the
beans which have a composite meaning whose parts can be
mapped to the words of the expression (e.g., spill??reveal?,
beans??secret?).
755
gree of lexical cohesion with their context, at least
not if one assumes a literal meaning for their com-
ponent words. Hence if the component words of a
potentially idiomatic expression do not participate
in any lexical chain, it is likely that the expression
is indeed used idiomatically, otherwise it is prob-
ably used literally. For instance, in example (3),
where the expression play with fire is used in a lit-
eral sense, the word fire does participate in a chain
(shown in bold face) that also includes the words
grilling, dry-heat, cooking, and coals, while for
the non-literal usage in example (4) there are no
chains which include fire.3
(3) Grilling outdoors is much more than just an-
other dry-heat cooking method. It?s the chance
to play with fire, satisfying a primal urge to stir
around in coals .
(4) And PLO chairman Yasser Arafat has accused Is-
rael of playing with fire by supporting HAMAS in
its infancy.
Unfortunately, there are also a few cases in
which a cohesion-based approach fails. Some-
times an expression is used literally but does not
feature prominently enough in the discourse to
participate in a chain, as in example (5) where the
main focus of the discourse is on the use of mor-
phine and not on children playing with fire.4 The
opposite case also exists: sometimes idiomatic us-
ages do exhibit lexical cohesion on the component
word level. This situation is often a consequence
of a deliberate ?play with words?, e.g. the use of
several related idioms or metaphors (see example
(6)). However, we found that both cases are rel-
atively rare. For instance, in a study of 75 literal
usages of various expressions, we only discovered
seven instances in which no relevant chain could
be found, including some cases where the context
was too short to establish the cohesive structure
(e.g., because the expression occurred in a head-
line).
(5) Chinamasa compared McGown?s attitude to mor-
phine to a child?s attitude to playing with fire ? a
lack of concern over the risks involved.
(6) Saying that the Americans were
?playing with fire? the official press specu-
lated that the ?gunpowder barrel? which is Taiwan
might well ?explode? if Washington and Taipei do
not put a stop to their ?incendiary gesticulations.?
3Idioms may, of course, link to the surrounding discourse
with their idiomatic meaning, i.e., for play with fire one may
expect other words in the discourse which are related to the
concept ?danger?.
4Though one could argue that there is a chain linking child
and play which points to the literal usage here.
3.2 Modelling Semantic Relatedness
While a cohesion-based approach to token-based
idiom classification should be intuitively success-
ful, its practical usefulness depends crucially on
the availability of a suitable method for computing
semantic relatedness. This is currently an area of
active research. There are two main approaches.
Methods based on manually built lexical knowl-
edge bases, such as WordNet, model semantic re-
latedness by computing the shortest path between
two concepts in the knowledge base and/or by
looking at word overlap in the glosses (see Budan-
itsky and Hirst (2006) for an overview). Distribu-
tional approaches, on the other hand, rely on text
corpora, and model relatedness by comparing the
contexts in which two words occur, assuming that
related words occur in similar context (e.g., Hindle
(1990), Lin (1998), Mohammad and Hirst (2006)).
More recently, there has also been research on us-
ing Wikipedia and related resources for modelling
semantic relatedness (Ponzetto and Strube, 2007;
Zesch et al, 2008).
All approaches have advantages and disadvan-
tages. WordNet-based approaches, for instance,
typically have a low coverage and only work
for so-called ?classical relations? like hypernymy,
antonymy etc. Distributional approaches usually
conflate different word senses and may therefore
lead to unintuitive results. For our task, we need to
model a wide range of semantic relations (Morris
and Hirst, 2004), for example, relations based on
some kind of functional or situational association,
as between fire and coal in (3) or between ice and
water in example (1). Likewise we also need to
model relations between non-nouns, for instance
between spill and sweep up in example (2). Some
relations also require world-knowledge, as in ex-
ample (7), where the literal usage of drop the
ball is not only indicated by the presence of goal-
keeper but also by knowing that Wayne Rooney
and Kevin Campbell are both football players.
(7) When Rooney collided with the goalkeeper, caus-
ing him to drop the ball, Kevin Campbell fol-
lowed in.
We thus decided against a WordNet-based mea-
sure of semantic relatedness, opting instead for a
distributional approach, Normalized Google Dis-
tance (NGD, see Cilibrasi and Vitanyi (2007)),
which computes relatedness on the basis of page
counts returned by a search engine. NGD is a mea-
sure of association that quantifies the strength of a
756
relationship between two words. It is defined as
follows:
NGD(x, y) =
max{log f(x), log f(y)} ? log f(x, y)
log M ?min{log f(x), log f(y)}
(8)
where x and y are the two words whose asso-
ciation strength is computed (e.g., fire and coal),
f(x) is the page count returned by the search en-
gine for the term x (and likewise for f(y) and y),
f(x, y) is the page count returned when querying
for ?x AND y? (i.e., the number of pages that con-
tain both, x and y), and M is the number of web
pages indexed by the search engine. The basic idea
is that the more often two terms occur together rel-
ative to their overall occurrence the more closely
they are related. For most pairs of search terms
the NGD falls between 0 and 1, though in a small
number of cases NGD can exceed 1 (see Cilibrasi
and Vitanyi (2007) for a detailed discussion of the
mathematical properties of NGD).
Using web counts rather than bi-gram counts
from a corpus as the basis for computing semantic
relatedness was motivated by the fact that the web
is a significantly larger database than any com-
piled corpus, which makes it much more likely
that we can find information about the concepts we
are looking for (thus alleviating data sparseness).
The information is also more up-to-date, which is
important for modelling the kind of world knowl-
edge about named entities we need to resolve ex-
amples like (7). Furthermore, it has been shown
that web counts can be used as reliable proxies for
corpus-based counts and often lead to better sta-
tistical models (Zhu and Rosenfeld, 2001; Lapata
and Keller, 2005).
To obtain the web counts we used Yahoo rather
than Google because we found Yahoo gave us
more stable counts over time. Both the Yahoo
and the Google API seemed to have problems with
very high frequency words, so we excluded those
cases. Effectively, this amounted to filtering out
function words. As it is difficult to obtain reli-
able figures for the number of pages indexed by a
search engine, we approximated this number (M
in formula (8) above) by setting it to the number
of hits obtained for the word the, assuming that
this word occurs in virtually all English language
pages (Lapata and Keller, 2005). When generat-
ing the queries we made sure that we queried for
all combinations of inflected forms (for example
?fire AND coal? would be expanded to ?fire AND
coal?, ?fires AND coal?, ?fire AND coals?, and
?fires AND coals?). The inflected forms were gen-
erated by the morph tools developed at the Univer-
sity of Sussex (Minnen et al, 2001).5
3.3 Cohesion-based Classifiers
We implemented two cohesion-based classifiers:
the first one computes the lexical chains for the
input text and classifies an expression as literal or
non-literal depending on whether its component
words participate in any of the chains, the second
classifier builds a cohesion graph and determines
how this graph changes when the expression is in-
serted or left out.
Chain-based classifier Various methods for
building lexical chains have been proposed in the
literature (Hirst and St-Onge, 1998; Barzilay and
Elhadad, 1997; Silber and McCoy, 2002) but the
basic idea is as follows: the content words of the
text are considered in sequence and for each word
it is determined whether it is similar enough to (the
words in) one of the existing chains to be placed
in that chain, if not it is placed in a chain of its
own. Depending on the chain building algorithm
used, a word is placed in a chain if it is related to
one other word in the chain or to all of them. The
latter strategy is more conservative and tends to
lead to shorter but more reliable chains and it is the
method we adopted here.6 Note that the chaining
algorithm has a free parameter, namely a threshold
which has to be surpassed to consider two words
related (relatedness threshold).
On the basis of the computed chains, the classi-
fier has to decide whether the target expression is
used literally or not. A simple strategy would clas-
sify an expression as literal whenever one or more
of its component words participates in any chain.
However, as the chains are potentially noisy, this
may not be the best strategy. We therefore also
evaluate the strength of the chain(s) in which the
expression participates. If a component word of
the expression participates in a long chain (and is
related to all words in the chain, as we require)
5The tools are available at: http://www.
informatics.susx.ac.uk/research/groups/
nlp/carroll/morph.html.
6If a WordNet-based relatedness measure is used, the
chaining algorithm has to perform word sense disambigua-
tion as well. As we use a distributional relatedness measure
which conflates different senses anyway, we do not have to
disambiguate here.
757
then this is good evidence that the expression is
indeed used in a literal sense. For instance, in
(3) the word fire belongs to the relatively long
chain grilling ? dry-heat ? cooking ? fire ? coals,
providing strong evidence of literal usage of play
with fire. To determine the strength of the evi-
dence in favour of a literal interpretation, we take
the longest chain in which any of the component
words of the idiom participate7 and check whether
this is above a predefined threshold (the classifi-
cation threshold). Both the relatedness threshold
and the classification threshold are set empirically
by optimising on a manually annotated develop-
ment set (see Section 4.2).
Graph-based classifier The chain-based clas-
sifier has two parameters which need to be op-
timised on labelled data, making this method
weakly supervised. To overcome this drawback,
we designed a second classifier which does not
have free parameters and is thus fully unsuper-
vised. This classifier relies on cohesion graphs.
The vertices of such a cohesion graph correspond
to the (content) word tokens in the text, each pair
of vertices is connected by an edge and the edges
are weighted by the semantic relatedness (i.e., the
inverse NGD) between the two words. The co-
hesion graph for example (1) is shown in Figure 1
(for expository reasons, edge weights are excluded
from the figure). Once we have built the cohe-
sion graph we compute its connectivity (defined
as the average edge weight) and compare it to the
connectivity of the graph that results from remov-
ing the (component words of the) target expres-
sion. For instance in Figure 1, we would com-
pare the connectivity of the graph as it is shown
to the connectivity that results from removing the
dashed edges. If removing the idiom words from
the graph leads to a higher connectivity, we as-
sume that the idiom is used non-literally, other-
wise we assume it is used literally. In Figure 1,
for example, most edges would have a relatively
low weight, indicating a weak relation between the
words they link. The edge between ice and water,
however, would have a higher weight. Removing
ice from the graph would therefore lead to a de-
creased connectivity and the classifier would pre-
dict that break the ice is used in the literal sense
in example (1). Effectively, we replace the ex-
7Note, that it is not only the noun that can participate in a
chain. In example (2), the word spill can be linked to sweep
up to provide evidence of literal usage.
break ice
water
troughschicken
Dad
Figure 1: Cohesion graph for example (1)
plicit thresholds of the lexical chain method by
an implicit threshold (i.e., change in connectivity),
which does not have to be optimised.
4 Evaluating the Cohesion-Based
Approach
We tested our two cohesion-based classifiers as
well as a supervised classifier on a manually an-
notated data set. Section 4.2 gives details of the
experiments and results. We start, however, by de-
scribing the data used in the experiments.
4.1 Data
We chose 17 idioms from the Oxford Dictionary
of Idiomatic English (Cowie et al, 1997) and other
idiom lists found on the internet. The idioms were
more or less selected randomly, subject to two
constraints: First, because the focus of the present
study is on distinguishing literal and non-literal us-
age, we chose expressions for which we assumed
that the literal meaning was not too infrequent. We
thus disregarded expressions like play the second
fiddle or sail under false colours. Second, in line
with many previous approaches to idiom classifi-
cation (Fazly et al, To appear; Cook et al, 2007;
Katz and Giesbrecht, 2006), we focused mainly on
expressions of the form V+NP or V+PP as this is
a fairly large group and many of these expressions
can be used literally as well, making them an ideal
test set for our purpose. However, our approach
also works for expressions which match a differ-
ent syntactic pattern and to test the generality of
our method we included a couple of these in the
data set (e.g., get one?s feet wet). For the same rea-
son, we also included some expressions for which
we could not find a literal use in the corpus (e.g.,
back the wrong horse).
For each of the 17 expressions shown in Ta-
ble 1, we extracted all occurrences found in the
Gigaword corpus that were in canonical form (the
forms listed in the table plus inflectional varia-
758
tions of the head verb).8 Hence, for rock the boat
we would extract rocked the boat and rocking the
boat but not rock a boat, rock the boats or rock
the ship. The motivation for this was two-fold.
First, as was discussed in Section 2, the vast ma-
jority of idiomatic usages are in canonical form.
This is especially true for non-decomposable id-
ioms (most of our 17 idioms), where only around
3% of the idiomatic usages are not in canonical
form. Second, we wanted to test whether our ap-
proach would be able to detect literal usages in the
set of canonical form expressions as this is pre-
cisely the set of expressions that would be classi-
fied as idiomatic by the unsupervised CForm clas-
sifier (Cook et al (2007), Fazly et al (To appear)).
While expressions in the canonical form are more
likely to be used idiomatically, it is still possible
to find literal usages as in examples (1) and (2).
For some expressions, such as drop the ball the
literal usage even outweighs the non-literal usage.
These literal usages would be mis-classified by the
CForm classifier.
In principle, though, our approach is very gen-
eral and would also work on expressions that are
not in canonical form and expressions whose id-
iomatic status is unclear, i.e., we do not necessar-
ily require a predefined set of idioms but could run
the classifiers on any V+NP or V+PP chunk.
For each extracted example, we included five
paragraphs of context (the current paragraph plus
the two preceding and following ones).9 This was
the context used by the classifiers. The examples
were then labelled as ?literal? or ?non-literal? by
an experienced annotator. If the distinction could
not be made reliably, e.g., because the context
was not long enough to disambiguate, the anno-
tator was allowed to annotate ???. These cases
were excluded from the data sets. To estimate
the reliability of our annotation, a randomly se-
lected sample (300 instances) was annotated inde-
pendently by a second annotator. The annotations
deviated in eight cases from the original, amount-
ing to an inter-annotator agreement of over 97%
and a kappa score of 0.7 (Cohen, 1960). All de-
viations were cases in which one of the annotators
chose ???, often because there was not sufficient
context and the annotation decision had to be made
on the basis of world knowledge.
8The extraction was done via manually built regular ex-
pressions.
9Note that paragraphs tend to be rather short in newswire.
For other genres it may be sufficient to extract one paragraph.
expression literal non-literal all
back the wrong horse 0 25 25
bite off more than one can chew 2 142 144
bite one?s tongue 16 150 166
blow one?s own trumpet 0 9 9
bounce off the wall* 39 7 46
break the ice 20 521 541
drop the ball* 688 215 903
get one?s feet wet 17 140 157
pass the buck 7 255 262
play with fire 34 532 566
pull the trigger* 11 4 15
rock the boat 8 470 478
set in stone 9 272 281
spill the beans 3 172 175
sweep under the carpet 0 9 9
swim against the tide 1 125 126
tear one?s hair out 7 54 61
all 862 3102 3964
Table 1: Idiom statistics (* indicates expressions
for which the literal usage is more common than
the non-literal one)
4.2 Experimental Set-Up and Results
For the lexical chain classifier we ran two experi-
ments. In the first, we used the data for one expres-
sion (break the ice) as a development set for opti-
mising the two parameters (the relatedness thresh-
old and the classification threshold). To find good
thresholds, a simple hill-climbing search was im-
plemented during which we increased the relat-
edness threshold in steps of 0.02 and the classi-
fication threshold (governing the minimum chain
length needed) in steps of 1. We optimised the F-
Score for the literal class, though we found that the
selected parameters varied only minimally when
optimising for accuracy. We then used the param-
eter values determined in this way and applied the
classifier to the remainder of the data.
The results obtained in this way depend to some
extent on the data set used for the parameter set-
ting.10 To control this factor, we also ran another
experiment in which we used an oracle to set the
parameters (i.e., the parameters were optimised for
the complete set). While this is not a realistic sce-
nario as it assumes that the labels of the test data
are known during parameter setting, it does pro-
vide an upper bound for the lexical chain method.
For comparison, we also implemented an in-
formed baseline classifier, which employs a sim-
ple model of cohesion, classifying expressions as
10We also ran the experiment for different development
sets and found that there was a relatively high degree of vari-
ation in the parameters selected and in the results obtained
with those settings.
759
literal if the noun inside the expression (e.g., ice
for break the ice) is repeated elsewhere in the con-
text, and non-literal otherwise. One would expect
this classifier to have a high precision for literal
expressions but a low recall.
Finally, we implemented a supervised classi-
fier. Supervised classifiers have been used be-
fore for this task, notably by Katz and Giesbrecht
(2006). Our approach is slightly different: in-
stead of creating meaning vectors we look at the
word overlap11 of a test instance with the literal
and non-literal instances in the training set (for the
same expression) and then assign the label of the
closest set.
That such an approach might be promising be-
comes clear when one looks at some examples of
literal and non-literal usage. For instance, non-
literal examples of break the ice occur frequently
with words such as diplomacy, relations, dialogue
etc. Effectively these words form lexical chains
with the idiomatic meaning of break the ice. They
are absent for literal usages. A supervised classi-
fier can learn which terms are indicative of which
usage. Note that this information is expression-
specific, i.e., it is not possible to train a classifier
for play with fire on labelled examples for break
the ice. This makes the supervised approach quite
expensive in terms of annotation effort as data has
to be labelled for each expression. Nonetheless, it
is instructive to see how well one could do with
this approach. In the experiments, we ran the su-
pervised classifier in leave-one-out mode on each
expression for which we had literal examples.
Table 2 shows the results for the five classi-
fiers discussed above: the informed baseline clas-
sifier (Rep), the cohesion graph (Graph), the lexi-
cal chain classifier with the parameters optimised
on break the ice (LC), the lexical chain classifier
with the parameters set by an oracle (LC-O), and
the supervised classifier (Super). The table also
shows the accuracy that would be obtained by a
CForm classifier (Cook et al, 2007; Fazly et al,
To appear) with gold standard canonical forms.
This classifier would label all examples in our data
set as ?non-literal? (it is thus equivalent to a ma-
jority class baseline). Since the majority of ex-
amples is indeed used idiomatically, this classifier
achieves a relatively high accuracy. However, ac-
curacy is not the best evaluation measure here be-
11We used the Dice coefficient as implemented in Ted Ped-
ersen?s Text::Similarity module: http://www.d.umn.
edu/?tpederse/text-similarity.html.
CForm Rep Graph LC LC-O Super
Acc 78.25 79.06 79.61 80.50 80.42 95.69
Pl - 70.00 52.21 62.26 53.89 84.62
Rl - 5.96 67.87 26.21 69.03 96.45
Fl - 10.98 59.02 36.90 60.53 90.15
Table 2: Accuracy, literal precision (Pl), recall
(Rl), and F-Score (Fl) for the classifiers
cause we are interested in detecting literal usages
among the canonical forms. Therefore, we also
computed the precision (Pl), recall (Rl), and F-
score (Fl) for the literal class.
It can be seen that all classifiers obtain a rela-
tively high accuracy but vary in precision, recall
and F-Score. For the CForm classifier, precision,
recall, and F-Score are undefined as it does not
label any examples as ?literal?. As expected the
baseline classifier, which looks for repetitions of
the component words of the target expression, has
a relatively high precision, showing that the ex-
pression is typically used in the literal sense if part
of it is repeated in the context. The recall, though,
is very low, indicating that lexical repetition is not
a sufficient signal for literal usage.
The graph-based classifier and the globally op-
timised lexical chain classifier (LC-O) outperform
the other two unsupervised classifiers (CForm and
Rep), with an F-Score of around 60%. For both
classifiers recall is higher than precision. Note,
however, that this is an upper bound for the lexical
chain classifier that would not be obtained in a re-
alistic scenario. An example of the values that can
be expected in a realistic setting (with parameter
optimisation on a development set that is separate
from the test set) is shown in column five (LC).
Here the F-Score is much lower due to lower re-
call. This classifier is too conservative when cre-
ating the chains and deciding how to interpret the
chain structure; it thus only rarely outputs the lit-
eral class. The reason for this conservatism may
be that literal usages of break the ice (the develop-
ment data) tend to have very strong chains, hence
when optimising the parameters for this data set, it
pays to be conservative. It is positive to note that
the (unsupervised) graph-based classifier performs
just as well as the (weakly supervised) chain-based
classifier does under optimal circumstances. This
means that one can by-pass the parameter setting
and the need to label development data by employ-
ing the graph-based method.
Finally, as expected, the supervised classifier
760
outperforms all other classifiers. It does so by a
large margin, which is surprising given that it is
based on relatively simplistic model. This shows
that the context in which an expression occurs
can really provide vital cues about its idiomatic-
ity. Note that our results are noticeably higher than
those reported by Cook et al (2007), Fazly et al
(To appear) and Katz and Giesbrecht (2006) for
similar supervised classifiers. We believe that this
may be partly explained by the size of our data set
which is significantly larger than the ones used in
these studies.
To assess how well our cohesion-based ap-
proach works for different idioms, we also com-
puted the accuracy of the graph-based classifier for
each expression individually (Table 3). We report
accuracy here rather than literal F-Score as the lat-
ter is often undefined for the individual data sets
(either because all examples of an expression are
non-literal or because the classifier only predicts
non-literal usages). It can be seen that the perfor-
mance of the classifier is generally relatively sta-
ble, with accuracies above 50% for most idioms.12
In particular, the classifier performs well on both,
expressions with a dominant non-literal meaning
and those with a dominant literal meaning; it is not
biased towards the non-literal class. For expres-
sions with a dominant literal meaning like drop the
ball, it correctly classifies more items as ?literal?
(530 items, 472 of which are correct) than as ?non-
literal? (373 items, 157 correct).
5 Conclusion
In this paper, we described a novel method for
token-based idiom classification. Our approach is
based on the observation that literally used expres-
sions typically exhibit cohesive ties with the sur-
rounding discourse, while idiomatic expressions
do not. Hence idiomatic expressions can be de-
tected by the absence of such ties. We propose two
methods that exploit this behaviour, one based on
lexical chains, the other based on cohesion graphs.
We showed that a cohesion-based approach is
well suited for distinguishing literal and non-
literal usages, even for expressions in canonical
form which tend to be largely idiomatic and would
all be classified as non-literal by the previously
proposed CForm classifier. Moreover, our find-
12Note that the data set for the worst performing idiom,
blow one?s own trumpet only contained 9 instances. Hence,
the low performance for this idiom may well be accidental.
expression Accuracy
back the wrong horse 68.00
bite off more than one can chew 79.17
bite one?s tongue 37.35
blow one?s own trumpet 11.11
bounce off the wall* 47.82
break the ice 85.03
drop the ball* 69.66
get one?s feet wet 64.33
pass the buck 82.44
play with fire 82.33
pull the trigger* 60.00
rock the boat 98.95
set in stone 85.41
spill the beans 83.43
sweep under the carpet 88.89
swim against the tide 93.65
tear one?s hair out 49.18
Table 3: Accuracies of the graph-based classifier
on each of the expressions (* indicates a dominant
literal usage)
ings suggest that the graph-based method per-
forms nearly as well as the best performance to be
expected for the chain-based method. This means
that the task can be addressed in a completely un-
supervised way.
While our results are encouraging they are still
below the results obtained by a basic supervised
classifier. In future work we would like to explore
whether better performance can be achieved by
adopting a bootstrapping strategy, in which we use
the examples about which the unsupervised clas-
sifier is most confident (i.e., those with the largest
difference in connectivity in either direction) as in-
put for a second stage supervised classifier.
Another potential improvement has to do with
the way in which the cohesion graph is computed.
Currently the graph includes all content words in
the context. This means that the graph is rela-
tively big and removing the potential idiom often
does not have a big effect on the connectivity; all
changes in connectivity are fairly close to zero.
In future, we want to explore intelligent strategies
for pruning the graph (e.g., by including a smaller
context). We believe that this might result in more
reliable classifications.
Acknowledgments
This work was funded by the German Research
Foundation DFG (under grant PI 154/9-3 and the
MMCI Cluster of Excellence). Thanks to Anna
Mu?ndelein for her help with preparing the data and
to Marco Pennacchiotti and Josef Ruppenhofer,
for feedback and comments.
761
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 workshop on Multiword
expressions: analysis, acquisition and treatment,
pages 89?96.
Colin Bannard. 2007. A measure of syntactic flex-
ibility for automatically identifying multiword ex-
pressions in corpora. In Proceedings of the ACL-07
Workshop on A Broader Perspective on Multiword
Expressions, pages 1?8.
Regina Barzilay and Michael Elhadad. 1997. Using
lexical chains for text summarization. In Proceed-
ings of the ACL-97 Intelligent Scalable Text Summa-
rization Workshop (ISTS-97).
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for the nearly unsupervised recognition of
nonliteral language. In Proceedings of EACL-06,
pages 329?336.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic dis-
tance. Computational Linguistics, 32(1):13?47.
Rudi L. Cilibrasi and Paul M.B. Vitanyi. 2007. The
Google similarity distance. IEEE Trans. Knowledge
and Data Engineering, 19(3):370?383.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurements, 20:37?46.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic
forms for the automatic identification of idiomatic
expressions in context. In Proceedings of the ACL-
07 Workshop on A Broader Perspective on Multi-
word Expressions, pages 41?48.
A.P. Cowie, R. Mackin, and I.R. McCaig. 1997. Ox-
ford dictionary of English idioms. Oxford Univer-
sity Press.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In Proceedings of EACL-06.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson. To
appear. Unsupervised type and token identification
of idiomatic expressions. Computational Linguis-
tics.
M.A.K. Halliday and R. Hasan. 1976. Cohesion in
English. Longman House, New York.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proceedings of
ACL-90, pages 268?275.
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detec-
tion and correction of malapropisms. In Christiane
Fellbaum, editor, WordNet: An electronic lexical
database, pages 305?332. The MIT Press.
Graham Katz and Eugenie Giesbrecht. 2006. Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the ACL/COLING-06 Workshop on
Multiword Expressions: Identifying and Exploiting
Underlying Properties, pages 12?19.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Transactions on Speech and Language Processing,
2:1?31.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL-98, pages
768?774.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-99,
pages 317?324.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Saif Mohammad and Graeme Hirst. 2006. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of EMNLP-06.
Jane Morris and Graeme Hirst. 2004. Non-classical
lexical semantic relations. In HLT-NAACL-04 Work-
shop on Computational Lexical Semantics, pages
46?51.
Simone Paolo Ponzetto and Michael Strube. 2007.
Knowledge derived from Wikipedia for computing
semantic relatedness. Journal of Artificial Intelli-
gence Research, 30:181?212.
Susanne Riehemann. 2001. A Constructional Ap-
proach to Idioms and Word Formation. Ph.D. thesis,
Stanford University.
H. Gregory Silber and Kathleen F. McCoy. 2002. Ef-
ficiently computed lexical chains as an intermedi-
ate representation for automatic text summarization.
Computational Linguistics, 28(4):487?496.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In Proceedings
of EMNLP-07, pages 1034?1043.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Using wiktionary for computing semantic re-
latedness. In Proceedings of AAAI-08, pages 861?
867.
Xiaojin Zhu and Ronald Rosenfeld. 2001. Improving
trigram language modeling with the world wide web.
In Proceedings of ICASSP-01.
762
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 75?83,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
A Cohesion Graph Based Approach for Unsupervised Recognition of
Literal and Non-literal Use of Multiword Expressions
Linlin Li and Caroline Sporleder
Saarland University
Postfach 15 11 50
66041 Saarbr?ucken
Germany
{linlin,csporled}@coli.uni-saarland.de
Abstract
We present a graph-based model for rep-
resenting the lexical cohesion of a dis-
course. In the graph structure, vertices cor-
respond to the content words of a text and
edges connecting pairs of words encode
how closely the words are related semanti-
cally. We show that such a structure can be
used to distinguish literal and non-literal
usages of multi-word expressions.
1 Introduction
Multiword expressions (MWEs) are defined as
?idiosyncratic interpretations that cross word
boundaries or spaces? (Sag et al, 2001). Such
expressions are pervasive in natural language;
they are estimated to be equivalent in number
to simplex words in mental lexicon (Jackendoff,
1997). MWEs exhibit a number of lexical, syn-
tactic, semantic, pragmatic and statistical idiosyn-
crasies: syntactic peculiarities (e.g., by and large,
ad hoc), semantic non-compositionality (e.g., as
in kick the bucket (die) and red tape (bureau-
cracy)), pragmatic idiosyncrasies (the expression
is sometimes associated with a fixed pragmatic
point, e.g., good morning, good night), variation
in syntactic flexibility (e.g., I handed in my thesis
= I handed my thesis in vs. Kim kicked the bucket
6= *the bucket was kicked by Kim), variation in
productivity (there are various levels of productiv-
ity for different MWEs, e.g., kick/*beat/*hit the
bucket, call/ring/phone/*telephone up).
These idiosyncrasies pose challenges for NLP
systems, which have to recognize that an expres-
sion is anMWE to deal with it properly. Recogniz-
ing MWEs has been shown to be useful for a num-
ber of applications such as information retrieval
(Lewis and Croft, 1990; Rila Mandala and Tanaka,
2000; Wacholder and Song, 2003) and POS tag-
ging (Piao et al, 2003). It has also been shown
that MWEs account for 8% of parsing errors with
precision grammars (Baldwin et al, 2004). Fur-
thermore, MWE detection is used in information
extraction (Lin, 1998b) and an integral component
of symbolic MT systems (Gerber and Yang, 1997;
Bond and Shirai, 1997).
However, the special properties of MWEs can
also be exploited to recognize MWEs automati-
cally. There have been many studies on MWEs:
identification (determining whether multiple sim-
plex words form a MWE in a given token context,
e.g. put the sweater on vs. put the sweater on
the table), extraction (recognizing MWEs as word
units at the type level), detecting or measuring
compositionality of MWEs, semantic interpreta-
tion (interpreting the semantic association among
components in MWEs).
To extract MWEs, various methods have been
proposed that exploit the syntactic and lexical
fixedness exhibited by MWEs, or apply various
statistical measures across all co-occurrence vec-
tors between the whole expression and its com-
ponent parts (see Section 2). These methods can
be used to automatically identify potentially id-
iomatic expressions at a type level, but they do not
say anything about the idiomaticity of an expres-
sion in a particular context. While some idioms
(e.g., ad hoc) are always used idiomatically, there
are numerous others that can be used both idiomat-
ically (see Example 1) and non-idiomatically (see
Example 2).
(1) When the members of De la Guarda aren?t
hanging around, they?re yelling and
bouncing off the wall.
(2) Blinded by the sun, Erstad leaped at the
wall, but the ball bounced off the wall well
below his glove.
Our work aims to distinguish the literal and
non-literal usages of idiomatic expressions in a
75
discourse context (so-called token based classifi-
cation). It is therefore different from type-based
approaches which aim to detect the general id-
iomaticity of an expression rather than its actual
usage in a particular context.
We utilize the cohesive structure of a discourse
(Halliday and Hasan, 1976) to distinguish literal or
non-literal usage of MWEs. The basic idea is that
the component words of an MWE contribute to the
cohesion of the discourse in the literal case, while
in the non-literal case they do not. For instance, in
the literal use of break the ice in Example 3, the
content word ice contributes to the overall seman-
tic connectivity of the whole sentence by the fact
that ice is semantically related to water. In con-
trast, in the non-literal example in 4, the word ice
does not contribute to the overall cohesion as it is
poorly connected to all the other (content) words
in this specific context (play, party, games).
(3) The water would break the ice into floes
with its accumulated energy.
(4) We played a couple of party games to
break the ice.
Our approach bears similarities to Hirst and St-
Onge?s (1998) method for detecting malapropisms
based on their non-participation in cohesive
chains. However, computing such chains requires
a pre-defined similarity threshold which governs
whether a word is placed in a particular chain. Set-
ting this threshold typically requires a manually la-
beled development set, which makes this method
weakly supervised. We propose an alternative,
parameter-free method in which we model the co-
hesive structure of a discourse as a graph structure
(called cohesion graph), where the vertices of the
graph correspond to the content words of the text
and the edges encode the semantic relatedness be-
tween pairs of words. To distinguish between lit-
eral and non-literal use of MWEs, we look at how
the average relatedness of the graph changes when
the component words of the MWE are excluded or
included in the graph (see Section 3).
1
We first introduced the cohesion graph method
in Sporleder and Li (2009). In the present paper,
1
By modeling lexical cohesion as a graph structure, we
follow earlier approaches in information retrieval, notably by
Salton and colleagues (Salton et al, 1994). The difference is
that these works aim at representing similarity between larger
text segments (e.g., paragraphs) in a so-called ?text? or ?para-
graph relation map?, whose vertices correspond to a text seg-
ment and whose edges represent the similarity between the
segments (modeled as weighted term overlap).
we provide a formalization of the graph and ex-
periment with different vertex and edge weight-
ing schemes. We also report on experiments with
varying the size of the input context and also with
pruning the graph structure automatically.
2 Related Work
Type-based MWE classification aims to extract
multiword expression types in text from observa-
tions of the token distribution. It aims to pick
up on word combinations which occur with com-
paratively high frequencies when compared to the
frequencies of the individual words (Evert and
Krenn, 2001; Smadja, 19993). The lexical and
syntactic fixedness property can also be utilized to
automatically extract MWEs (Baldwin and Villav-
icencio, 2002).
The study of semantic compositionality of
MWEs focuses on the degree to which the seman-
tics of the parts of an MWE contribute towards the
meaning of the whole. The aim is a binary classi-
fication of the MWEs as idiosyncratically decom-
posable (e.g. spill the beans) or non-decomposable
(e.g. kick the bucket). Several approaches have
been proposed. Lin (1999) uses the substitution
test
2
and mutual information (MI) to determine
the compositionality of the phrase. An obvious
change of the MI value of the phrase in the sub-
stitution test is taken as the evidence of the MWEs
being non-compositional. Bannard et al (2003)
assume that compositional MWEs occur in sim-
ilar lexical context as their component parts. The
co-occurrence vector representations of verb parti-
cle construction (VPC) and the component words
are utilized to determine the compositionality of
the MWE.
There have also been a few token-based classi-
fication approaches, aimed at classifying individ-
ual instances of a potential idiom as literal or non-
literal. Katz and Giesbrecht (2006) make use of
latent semantic analysis (LSA) to explore the local
linguistic context that can serve to identify multi-
word expressions that have non-compositional
meaning. They measure the cosine vector similar-
ity between the vectors associated with an MWE
as a whole and the vectors associated with its con-
stituent parts and interpret it as the degree to which
the MWE is compositional. They report an av-
2
The substitution test aims to replace part of the idiom?s
component words with semantically similar words, and test
how the co-occurrence frequency changes.
76
erage accuracy of 72%, but the data set used in
their evaluation is small. Birke and Sarkar (2006)
use literal and non-literal seed sets acquired with-
out human supervision to perform bootstrapping
learning. The new instances of potential idioms
are always labeled according to the closest set.
While their approach is unsupervised clustering,
they do rely on some resources such as databases
of idioms. Cook et al (2007) and Fazly et al
(2009) rely crucially on the concept of canonical
form (CForm). It is assumed that for each idiom
there is a fixed form (or a small set of those) cor-
responding to the syntactic pattern(s) in which the
idiom normally occurs. The canonical form al-
lows for inflection variation of the heard verb but
not for other variations (such as nominal inflec-
tion, choice of determiner etc.). It has been ob-
served that if an expression is used idiomatically
it typically occurs in its canonical form (Riehe-
mann, 2001). Fazly and her colleagues exploit this
behavior and propose an unsupervised method for
token-based idiom classification in which an ex-
pression is classified as idiomatic if it occurs in
canonical form and literal otherwise. The canon-
ical forms are determined automatically using a
statistical, frequency-based measure. They also
developed statistical measures to measure the lex-
ical and syntactic fixedness of a given expression,
which is used to automatically recognize expres-
sion types, as well as their token identification in
context. They report an average accuracy of 72%
for their canonical form (CForm) classifier.
3 Cohesion Graph
In this section, we first give a formal definition of
the cohesion graph that is used for modeling dis-
course connectivity, then we define the discourse
connectivity. Finally, we introduced our graph-
based classifier for distinguishing literal and non-
literal use of MWEs.
3.1 Cohesion Graph Structure
A cohesion graph (CG) is an undirected complete
graph
3
G = (V,E), where
V : is a set of nodes {v
1
, v
2
, ..., v
n
}, where each
node v
i
= (t
i
, id
i
) represents a unique token in the
discourse. t
i
is the string form of the token, and id
i
denotes the position of the token in the context.
3
In the mathematical field of graph theory, a complete
graph is a simple graph in which every pair of distinct vertices
is connected by an edge. The complete graph on n vertices
has n(n? 1)/2 edges.
E: is a set of edges {e
12
, e
13
, ..., e
(n)(n?1)
},
such that each edge e
ij
connects a pair of nodes
(v
i
, v
j
). n is the total number of tokens in the dis-
course that the graph models. The value of e
ij
rep-
resents the semantic relatedness of the two tokens
t
i
, t
j
that e
ij
connects:
e
ij
= h(t
i
, t
j
) (5)
where h is a semantic relatedness assignment
function. The explicit form of h will be discussed
in the next section.
e
i
is the average semantic relatedness of the to-
ken t
i
in the discourse. It represents the average
relatedness score of a certain token to its surround-
ing context:
e
i
=
n
?
j=1,j 6=i
?
ij
? e
ij
(6)
where ?
ij
is the weight of the edge e
ij
, with the
constraint,
n
?
j=1,j 6=i
?
ij
= 1.
The edge weight function ?
ij
allows us to
weight the relatedness between two tokens, for ex-
ample based on their distance in the text. The mo-
tivation for this is that the closer two tokens occur
together, the more likely it is that their relatedness
is not accidental. For instance, the idiom break
the ice in Example 7 could be misclassified as lit-
eral due to there being a high relatedness score be-
tween ice and snow. The weight function is in-
troduced so that relatedness with tokens that are
closer to MWE component words counts more.
(7) The train was canceled because of the wind
and snow. All the people in the small village
train station felt upset. Suddenly, one guy
broke the ice and proposed to play a game.
The weight function ?
ij
is defined in terms of
the inverse of the distance ? between the two token
positions id
i
and id
j
:
?
ij
=
?(id
i
, id
j
)
?
j
?(id
i
, id
j
)
(8)
As the semantic relatedness among the MWE
component words does not contain any informa-
tion of how these component words are seman-
tically involved in the context, we do not count
the edges between the MWE component words
77
(as e
45
in Figure 1). We set al the weights
for connecting MWE component words to be 0,
?(id
mwe
?
i
, id
mwe
j
) = 0.
c(G): is defined as the discourse connectivity
of the cohesion graph. It represents the semantic
relatedness score of the discourse.
c(G) =
n
?
i=1
(?
i
? e
i
) (9)
where n is the total number of tokens in the
discourse, ?
i
is the weight of the average seman-
tic relatedness of the token t
i
with the constraint
?
i
?
i
= 1. It represents the importance of the
relatedness contribution of a specific token t
i
in
the discourse. For instance, the word Monday in
Example 12 should be assigned less weight than
the word bilateral as it is not part of the central
theme(s) of the discourse. This is often the case
for time expressions. ?
i
is defined as:
?
i
=
salience(t
i
)
?
j
salience(t
j
)
(10)
To model the salience of a token for the se-
mantic context of the text we use a tf.idf -based
weighting scheme. Since we represent word to-
kens rather than word types in the cohesion graph,
we do not need to model the term frequency tf
separately, instead we set salience to the log value
of the inverse document frequency idf :
salience(t
i
) = log
|D|
|{d : t
i
? d}|
(11)
whereD is the total number of documents in our
data set and |{d : t
i
? d}| is the number of docu-
ments in which t
i
occurs. Terms which are related
to the sub-topics of a document will typically only
occur in a few texts in the collection, hence their
idf (and often also their tf ) is high and they will
thus be given more weight in the graph. Terms
which are not related to the central themes of a
text, such as temporal expressions, will be given
a lower weight. A complication arises for compo-
nent words of the MWE: these occur in all of our
examples and thus will receive a very low idf. This
is an artifact of the data and not what we want as
it means that the average connectivity of the graph
virtually always increases if the MWE is excluded,
causing the classifier to over-predict ?non-literal?.
To counteract this effect, we set |{d : t
i
? d}| of
these words uniformly to 1.
(12) ?Gujral will meet Sharif on Monday and
discuss bilateral relations,? the Press Trust
of India added. The minister said Sharif and
Gujral would be able to ?break the ice? over
Kashmir.
3.2 Graph-based Classifier
The cohesion graph based classifier compares the
cohesion graph connectivity of the discourse in-
cluding the MWE component words with the con-
nectivity of the discourse excluding the MWE
component words to check how well the MWE
component words are semantically connected to
the context. If the cohesion graph connectivity
increases by including MWE component words,
the MWE is thought to be semantically well re-
lated to its discourse. It is classified as literal (oth-
erwise as non-literal). In other words, the cohe-
sion graph based algorithm detects the strength of
relatedness between the MWE component words
and their context by calculating the discourse con-
nectivity gain, and classifies instances as literal or
non-literal based on this gain. This process is de-
scribed as Formula 13 (if ?c > 0, it is literal;
otherwise it is non-literal):
?c = c(G)? c(G
?
) (13)
where, c(G) is the discourse connectivity of the
context with MWE component words (as shown
with the complete graph in Figure 1 ); c(G
?
) is
the discourse connectivity of the context without
MWE component words (as shown with the sub-
graph {v
1
, v
2
, v
3
} in Figure 1).
Figure 1: Cohesion Graph for identifying literal or
non-literal usage of MWEs
78
4 Modeling Semantic Relatedness
In Section 3.1, we did not define how we model
the semantic relatedness between two tokens
(h(t
i
, t
j
)). Modeling semantic relatedness be-
tween two terms is currently an area of active re-
search. There are two main approaches. Methods
based on manually built lexical knowledge bases,
such as WordNet, compute the shortest path be-
tween two concepts in the knowledge base and/or
look at word overlap in the glosses (see Budan-
itsky and Hirst (2006) for an overview). Distri-
butional approaches, on the other hand, rely on
text corpora, and model relatedness by comparing
the contexts in which two words occur, assuming
that related words occur in similar context (e.g.,
Hindle (1990), Lin (1998a), Mohammad and Hirst
(2006)). More recently, there has also been re-
search on using Wikipedia and related resources
for modeling semantic relatedness (Ponzetto and
Strube, 2007; Zesch et al, 2008).
WordNet-based approaches are unsuitable for
our purposes as they only model so-called ?classi-
cal relations? like hypernymy, antonymy etc. For
our task, we need to model a wide range of re-
lations, e.g., between ice and water. Hence we
opted for a distributional approach. We experi-
mented with two different approaches, one (DV )
based on syntactic co-occurrences in a large text
corpus and the other (NGD) based on search en-
gine page counts.
Dependency Vectors (DV) is a distributional
approach which does not look simply at word co-
occurrences in a fixed-size window but takes into
account syntactic (dependency) relations between
words (Pad?o and Lapata, 2007). Each target word
is represented by a co-occurrence vector where di-
mension represents a chosen term and the vector
contains the co-occurrence information between
that word and the chosen terms in a corpus (we
used the BNC in our experiments). A variety of
distance measures can be used to compute the sim-
ilarity of two vectors; here we use the cosine sim-
ilarity which is defined as:
sim
cos
(
??
x ,
??
y ) =
n
X
i=1
x
i
y
i
v
u
u
t
n
X
i=1
x
2
i
v
u
u
t
n
X
i=1
y
2
i
(14)
Normalized Google Distance (NGD) uses the
page counts returned by a search engine as prox-
ies for word co-occurrence and thereby quantifies
the strength of a relationship between two words
(see Cilibrasi and Vitanyi (2007)). The basic idea
is that the more often two terms occur together rel-
ative to their overall occurrence the more closely
they are related. NGD is defined as follows:
NGD(x, y) =
max{log f(x), log f(y)} ? log f(x, y)
log M ?min{log f(x), log f(y)}
(15)
where x and y are the two words whose associ-
ation strength is computed, f(x) is the page count
returned by the search engine for the term x (and
likewise for f(y) and y), f(x, y) is the page count
returned when querying for ?x AND y? (i.e., the
number of pages that contain both, x and y), and
M is the number of web pages indexed by the
search engine. When querying for a term we query
for a disjunction of all its inflected forms.
4
As it
is difficult to obtain a specific and reliable number
for the number of pages indexed by a search en-
gine, we approximated it by setting it to the num-
ber of hits obtained for the word the. The assump-
tion is that the word the occurs in all English lan-
guage web pages (Lapata and Keller, 2005).
Using web counts rather than bi-gram counts
from a corpus as the basis for computing semantic
relatedness has the advantage that the web is a sig-
nificantly larger database than any compiled cor-
pus, which makes it much more likely that we can
find information about the concepts we are look-
ing for (thus alleviating data sparseness). How-
ever, search engine counts are notoriously unre-
liable (Kilgariff, 2007; Matsuo et al, 2007) and
while previous studies have shown that web counts
can be used as reliable proxies for corpus-based
counts for some applications (Zhu and Rosenfeld,
2001; Lapata and Keller, 2005) it is not clear that
this also applies when modeling semantic related-
ness. We thus carried out a number of experiments
testing the reliability of page counts (Section 4.1)
and comparing the NGD measure to a standard
distributional approach (Section 4.2).
4
The inflected forms were generated by apply-
ing the morph tools developed at the University of
Sussex (Minnen et al, 2001) which are available
at: http://www.informatics.susx.ac.uk/
research/groups/nlp/carroll/morph.html
79
4.1 Search Engine Stability
We first carried out some experiments to test the
stability of the page counts returned by two of the
most widely-used search engines, Google and Ya-
hoo. For both search engines, we found a number
of problems.
5
Total number of pages indexed The total num-
ber of the web pages indexed by a search engine
varies across time and the numbers provided are
somewhat unreliable. This is a potential problem
for NGD because we need to fix the value of M in
Formula 15. As an approximative solution, we set
it to the number of hits obtained for the word the,
assuming that it will occur in all English language
pages (Lapata and Keller, 2005).
Page count variation The number of page hits
for a given term also varies across time (see exam-
ple (4.1) for two queries for Jim at different times
t1 and t2). However, we found that the variance in
the number of pages tends to be relatively stable
over short time spans, hence we can address this
problem by carrying out all queries in one quick
session without much delay. However, this means
we cannot store page counts in a database and re-
use them at a later stage; for each new example
which we want to classify at a later stage, we have
to re-compute all relevant counts.
(16) Hits(Jim, t1) = 763,000,000
Hits(Jim, t2) = 757,000,000
Problems with conjunction and disjunction
The search engines? AND and OR operators are
problematic and can return counter-intuitive re-
sults (see Table 1). This is a potential problem
for us because we have to query for conjunctions
of terms and disjunctions of inflected forms. For
the time being we ignored this problem as it is not
straightforward to solve.
OPT = AND OPT = OR
car 3,590,000,000
car OPT car 4,670,000,000 3,550,000,000
car OPT car OPT car 3,490,000,000 3,530,000,000
Table 1: Operator test for Yahoo
Problems with high-frequency terms We also
found that both the Google and Yahoo API seem
to have problems with high frequency words, with
the Google SOAP API throwing an exception and
5
See also the discussions in Jean V?eronis blog: http://
aixtal.blogspot.com and the comments in Kilgariff
(2007).
the Yahoo API returning the same 10-digit num-
ber for every high frequency word. This might be
a data overflow problem. We addressed this prob-
lem by excluding high frequency words.
When comparing Yahoo and Google we found
that Yahoo?s page counts tend to be more consis-
tent than Google?s. We therefore opted for Yahoo
in our further experiments.
4.2 NGD vs. Co-occurrence Vectors
In principle, we believe that the web-based ap-
proach for computing relatedness is more suitable
for our task since it gives us access to more data
and allows us to also model relations based on (up-
to-date) world knowledge. However, the question
arises whether the stability problems observed in
the previous section have a negative effect on the
performance of the NGD measure. To test this, we
conducted a small study in which we compared
the relatedness scores obtained by NGD and the
semantic vector space model to the human ratings
compiled by Finkelstein et al (2002).
6
We used Spearman?s correlation test (Spear-
man, 1904) to compare the ranked human ratings
to the ranked ratings obtained by NGD and the
vector space method. The (human) inter-annotator
agreement varies a lot for different pairs of annota-
tors (between 0.41 and 0.82 by Spearman?s corre-
lation test), suggesting that deciding on the seman-
tic relatedness between arbitrary pairs of words
is not an easy task even for humans. In gen-
eral, the NGD-human agreement is comparable to
the human-human agreement. The agreement be-
tween the NGD and average human agreement is
higher than some human-human agreements. Fur-
thermore, we found that NGD actually outper-
forms the dependency vector method on this data
set.
7
Hence, we decided to use NGD in the fol-
lowing experiments.
5 Experiments
We tested our graph-based classifiers on a manu-
ally annotated data set, which we describe in Sec-
6
The data sets are available at: http://www.cs.
technion.ac.il/?gabr/resources/data/
wordsim353/
7
There may be several reasons for this. Apart from the
fact that NGD has access to a larger data set, it may also be
that syntactic co-occurrence information is not ideal for mod-
eling this type of relatedness; co-occurrence information in a
fixed window might be more useful. Furthermore, we did not
spend much time on finding an optimal parameter setting for
the dependency vector method.
80
tion 5.1. We report on our experiments and results
in Section 5.2.
5.1 Data
Throughout the experiments we used the data set
from Sporleder and Li (2009). The data consist of
17 potentially idiomatic expressions from the En-
glish Gigaword corpus, which were extracted with
five paragraphs of context and manually annotated
as ?literal? or ?non-literal? (see Table 2). The inter-
annotator agreement on a doubly annotated sam-
ple of the data was 97% and the kappa score 0.7
(Cohen, 1960).
expression literal non-lit. all
back the wrong horse 0 25 25
bite off more than one can chew 2 142 144
bite one?s tongue 16 150 166
blow one?s own trumpet 0 9 9
bounce off the wall* 39 7 46
break the ice 20 521 541
drop the ball* 688 215 903
get one?s feet wet 17 140 157
pass the buck 7 255 262
play with fire 34 532 566
pull the trigger* 11 4 15
rock the boat 8 470 478
set in stone 9 272 281
spill the beans 3 172 175
sweep under the carpet 0 9 9
swim against the tide 1 125 126
tear one?s hair out 7 54 61
all 862 3102 3964
Table 2: Idiom statistics (* indicates expressions
for which the literal usage is more common than
the non-literal one)
5.2 The Influence of Context Size and
Weighting Scheme
To gain some insights into the performance of the
graph-based classifier, we experimented with dif-
ferent context sizes and weighting schemes. In ad-
dition to the basic cohesion graph approach with
five paragraphs of context (CGA), we tested a
variant which only uses the current paragraph as
context (CGA
para
) to determine how sensitive the
classifier is to the context size. We also experi-
mented with three weighting schemes. The ba-
sic classifier (CGA) uses uniform edge and node
weights. CGA
ew
uses edge weights based on the
inverse distance between the tokens. CGA
nw
uses
node weights based on idf . Finally, CGA
ew+nw
uses both edge and node weights.
We also carried out a pruning experiment in
which we removed nodes from the graph that are
only weakly connected to the context (called weak
cohesion nodes). We hypothesize that these do
not contribute much to the overall connectivity but
may add noise. Pruning can thus be seen as a
more gentle version of node weighting, in which
we only remove the top n outliers rather than re-
weight all nodes. For comparison we also imple-
mented a baseline (BASE), which always assigns
the majority class (?non-literal?).
Table 3 shows the results for the classifiers dis-
cussed above. In addition to accuracy, which is
not very informative as the class distribution in our
data set is quite skewed, we show the precision,
recall, and F-score for the minority class (literal).
All classifiers obtain a relatively high accuracy but
vary in the precision, recall and F-Score values.
Method LPrec. LRec. LF
?=1
Acc.
Base ? ? ? 0.78
CGA 0.50 0.69 0.58 0.79
CGA
para
0.42 0.67 0.51 0.71
CGA
prun
0.49 0.72 0.58 0.78
CGA
ew
0.51 0.63 0.57 0.79
CGA
nw
0.48 0.68 0.56 0.77
CGA
ew+nw
0.49 0.61 0.54 0.78
Table 3: Accuracy (Acc.), literal precision
(LPrec.), recall (LRec.), and F-Score (LF
?=1
) for
the classifier
It can be seen that the basic cohesion graph
classifier (CGA) outperforms the baseline on ac-
curacy. Moreover, it is reasonably good at iden-
tifying literal usages among the majority of non-
literal occurrences, as witnessed by an F-score of
58%. To obtain a better idea of the behavior of
this classifier, we plotted the distribution of the
MWE instances in the classifier?s feature space,
where the first dimension represents the discourse
connectivity of the context with MWE component
words (c(G)) and the second represents the dis-
course connectivity of the context without MWE
component words (c(G
?
)). The graph-based clas-
sifier, which calculates the connectivity gain (see
Equation 13), is a simple linear classifier in which
the line y = x is chosen as the decision boundary.
Examples above that line are classified as ?literal?,
examples below as ?non-literal?. Figure 2 shows
the true distribution of literal and non-literal exam-
ples in our data set. It can be seen that most non-
literal examples are indeed below the line while
most literal ones are above it (though a certain
number of literal examples can also be found be-
81
low the line). So, in general we would expect our
classifier to have a reasonable performance.
Figure 2: Decision boundaries of the cohesion
graph
Returning to the results in Table 3, we find
that a smaller context worsens the performance
of the classifier (CGA
para
). Pruning the 3 least
connected nodes (CGA
prun
) does not lead to a
significant change in performance. Edge weight-
ing (CGA
ew
), node weighting (CGA
nw
) and their
combination (CGA
ew+nw
), on the other hand,
seem to have a somewhat negative influence on
the literal recall and F-score. It seems that the
weighting scheme scales down the influence of
MWE component words. As a result, the prod-
uct of the weight and the relatedness value for the
idiom component words are lower than the aver-
age, which leads to the negative contribution of
the idiom words to the cohesion graph (over pre-
dicting non-literal usage). We need to investigate
more sophisticated weighting schemes to assign
better weights to idiom component words in the
future. The negative performance of the weight-
ing scheme may be also due to the fact that we
used a relatively small context of five paragraphs.
8
Both the idf and the distance weighting should
probably be defined on larger contexts. For ex-
ample, the distance between two tokens within a
paragraph probably has not such a large effect on
whether their relatedness score is reliable or ac-
cidental. Hence it might be better to model the
edge weight as the distance in terms of paragraphs
rather than words. The idf scores, too, might be
more reliable if more context was used.
8
Note that we used news texts which typically have very
short paragraphs.
6 Conclusion
In this paper, we described an approach for token-
based idiom classification. Our approach is based
on the observation that literally used expressions
typically exhibit strong cohesive ties with the sur-
rounding discourse, while idiomatic expressions
do not. Hence, idiomatic use of MWEs can be
detected by the absence of such ties.
We propose a graph-based method which ex-
ploits this behavior to classify MWEs as literal
or non-literal. The method compares how the
MWE component words contribute the overall se-
mantic connectivity of the graph. We provided a
formalization of the graph and experimented with
varying the context size and weighting scheme for
nodes and edges. We found that the method gener-
ally works better for larger contexts; the weighting
schemes proved somewhat unsuccessful, at least
for our current context size. In the future, we plan
to experiment with larger context sizes and more
sophisticated weighting schemes.
Acknowledgments
This work was funded by the Cluster of Excellence ?Multi-
modal Computing and Interaction?.
References
T. Baldwin, A. Villavicencio. 2002. Extracting the un-
extractable: a case study on verb-particles. In Proc.
of CoNLL-02.
T. Baldwin, E. M. Bender, D. Flickinger, A. Kim,
S. Oepen. 2004. Road-testing the english resource
grammar over the british national corpus. In Proc.
LREC-04, 2047?2050.
C. Bannard, T. Baldwin, A. Lascarides. 2003. A sta-
tistical approach to the semantics of verb-particles.
In Proc. ACL 2003 Workshop on Multiword Expres-
sions.
J. Birke, A. Sarkar. 2006. A clustering approach
for the nearly unsupervised recognition of nonliteral
language. In Proceedings of EACL-06.
F. Bond, S. Shirai. 1997. Practical and efficient or-
ganization of a large valency dictionary. In Work-
shop on Multilingual Information Processing Natu-
ral Language Processing Pacific Rim Symposium.
A. Budanitsky, G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computa-
tional Linguistics, 32(1):13?47.
R. L. Cilibrasi, P. M. Vitanyi. 2007. The Google sim-
ilarity distance. IEEE Trans. Knowledge and Data
Engineering, 19(3):370?383.
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surements, 20:37?46.
82
P. Cook, A. Fazly, S. Stevenson. 2007. Pulling their
weight: Exploiting syntactic forms for the automatic
identification of idiomatic expressions in context. In
Proceedings of the ACL-07 Workshop on A Broader
Perspective on Multiword Expressions.
S. Evert, B. Krenn. 2001. Methods for the qualitative
evaluation of lexical association measures. In Proc.
ACL-01.
A. Fazly, P. Cook, S. Stevenson. 2009. Unsupervised
type and token identification of idiomatic expres-
sions. Computational Linguistics, 35(1):61?103.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116?
131.
L. Gerber, J. Yang. 1997. Systran mt dictionary devel-
opment. In Proc. Fifth Machine Translation Summit.
M. Halliday, R. Hasan. 1976. Cohesion in English.
Longman House, New York.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90,
268?275.
G. Hirst, D. St-Onge. 1998. Lexical chains as rep-
resentations of context for the detection and correc-
tion of malapropisms. In C. Fellbaum, ed., Word-
Net: An electronic lexical database, 305?332. The
MIT Press.
R. Jackendoff. 1997. The Architecture of the Language
Faculty. MIT Press.
G. Katz, E. Giesbrecht. 2006. Automatic identifi-
cation of non-compositional multi-word expressions
using latent semantic analysis. In Proceedings of the
ACL/COLING-06 Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties.
A. Kilgariff. 2007. Googleology is bad science. Com-
putational Linguistics, 33(1):147?151.
M. Lapata, F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2:1?31.
D. D. Lewis, W. B. Croft. 1990. Term clustering of
syntactic phrase. In Proceedings of SIGIR-90, 13th
ACM International Conference on Research and De-
velopment in Information Retrieval.
D. Lin. 1998a. Automatic retrieval and clustering of
similar words. In Proceedings of ACL-98.
D. Lin. 1998b. Using collocation statistics in informa-
tion extraction. In Proc. MUC-7.
D. Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-99,
317?324.
Y. Matsuo, H. Tomobe, T. Nishimura. 2007. Robust
estimation of google counts for social network ex-
traction. In AAAI-07.
G. Minnen, J. Carroll, D. Pearce. 2001. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
S. Mohammad, G. Hirst. 2006. Distributional mea-
sures of concept-distance: A task-oriented evalua-
tion. In Proceedings of EMNLP-06.
S. Pad?o, M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
S. S. L. Piao, P. Rayson, D. Archer, A. Wilson,
T. McEnery. 2003. Extracting multiword expres-
sions with a semantic tagger. In Proc. of the ACL
2003 Workshop on Multiword Expressions, 49?56.
S. P. Ponzetto, M. Strube. 2007. Knowledge derived
from Wikipedia for computing semantic relatedness.
Journal of Artificial Intelligence Research, 30:181?
212.
S. Riehemann. 2001. A Constructional Approach to
Idioms and Word Formation. Ph.D. thesis, Stanford
University.
T. T. Rila Mandala, H. Tanaka. 2000. Query expansion
using heterogeneous thesauri. Inf. Process. Man-
age., 36(3).
I. A. Sag, T. Baldwin, F. Bond, A. Copestake,
D. Flickinger. 2001. Multiword expressions: a pain
in the neck for NLP. In Lecture Notes in Computer
Science.
G. Salton, J. Allan, C. Buckley, A. Singhal. 1994.
Automatic analysis, theme generation and sum-
marization of machine-readable texts. Science,
264(3):1421?1426.
F. Smadja. 19993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?177.
C. Spearman. 1904. The proof and measurement of
association between two things. Amer. J. Psychol,
72?101.
C. Sporleder, L. Li. 2009. Unsupervised recognition of
literal and non-literal use of idiomatic expressions.
In Proceedings of EACL-09.
N. Wacholder, P. Song. 2003. Toward a task-based
gold standard for evaluation of NP chunks and tech-
nical terms. In Proc HLT-NAACL.
T. Zesch, C. M?uller, I. Gurevych. 2008. Using wik-
tionary for computing semantic relatedness. In Pro-
ceedings of AAAI-08, 861?867.
X. Zhu, R. Rosenfeld. 2001. Improving trigram lan-
guage modeling with the world wide web. In Pro-
ceedings of ICASSP-01.
83
Coling 2010: Poster Volume, pages 683?691,
Beijing, August 2010
Linguistic Cues for Distinguishing Literal and Non-Literal Usages
Linlin Li and Caroline Sporleder
Department of Computational Linguistics
Saarland University
{linlin, csporled}@coli.uni-saarland.de
Abstract
We investigate the effectiveness of differ-
ent linguistic cues for distinguishing lit-
eral and non-literal usages of potentially
idiomatic expressions. We focus specif-
ically on features that generalize across
different target expressions. While id-
ioms on the whole are frequent, instances
of each particular expression can be rela-
tively infrequent and it will often not be
feasible to extract and annotate a suffi-
cient number of examples for each expres-
sion one might want to disambiguate. We
experimented with a number of different
features and found that features encoding
lexical cohesion as well as some syntac-
tic features can generalize well across id-
ioms.
1 Introduction
Nonliteral expressions are a major challenge in
NLP because they are (i) fairly frequent and (ii)
often behave idiosyncratically. Apart from typi-
cally being semantically more or less opaque, they
can also disobey grammatical constraints (e.g., by
and large, lie in wait). Hence, idiomatic expres-
sions are not only a problem for semantic anal-
ysis but can also have a negative effect on other
NLP applications (Sag et al, 2001), such as pars-
ing (Baldwin et al, 2004).
To process non-literal language correctly, NLP
systems need to recognise such expressions au-
tomatically. While there has been a significant
body of work on idiom (and more generally multi-
word expression) detection (see Section 2), un-
til recently most approaches have focused on
a type-based classification, dividing expressions
into ?idiomatic? or ?not idiomatic? irrespective of
their actual use in a discourse context. However,
while some expressions, such as by and large, al-
ways have a non-compositional, idiomatic mean-
ing, many other expressions, such as break the ice
or spill the beans, can be used literally as well as
idiomatically and for some expressions, such as
drop the ball, the literal usage can even dominate
in some domains. Consequently, those expres-
sions have to be disambiguated in context (token-
based classification).
We investigate how well models for distin-
guishing literal and non-literal use can be learned
from annotated examples. We explore different
types of features, such as the local and global con-
text, syntactic properties of the local context, the
form of the expression itself and properties re-
lating to the cohesive structure of the discourse.
We show that several feature types work well for
this task. However, some features can generalize
across specific idioms, for instance features which
compute how well an idiom ?fits? its surrounding
context under a literal or non-literal interpretation.
This property is an advantage because such fea-
tures are not restricted to training data for a spe-
cific target expression but can also benefit from
data for other idioms. This is important because,
while idioms as a general linguistic class are rela-
tively frequent, instances of each particular idiom
are much more difficult to find in sufficient num-
bers. The situation is exacerbated by the fact the
distributions of literal vs. non-literal usage tend
to be highly skewed, with one usage (often the
non-literal one) being much more frequent than
the other. Finding sufficient examples of the mi-
nority class can then be difficult, even if instances
are extracted from large corpora. Furthermore, for
highly skewed distributions, many more majority
class examples have to be annotated to obtain an
acceptable number of minority class instances.
We show that it is possible to circumvent this
problem by employing a generic feature space that
683
looks at the cohesive ties between the potential id-
iom and its surrounding discourse. Such features
generalize well across different expressions and
lead to acceptable performance even on expres-
sions unseen in the training set.
2 Related Work
Until recently, most studies on idiom classifi-
cation focus on type-based classification; sofar
there are only comparably few studies on token-
based classification. Among the earliest studies
on token-based classification were the ones by
Hashimoto et al (2006) on Japanese and Katz
and Giesbrecht (2006) on German. Hashimoto et
al. (2006) present a rule-based system in which
lexico-syntactic features of different idioms are
hard-coded in a lexicon and then used to distin-
guish literal and non-literal usages. The features
encode information about the passivisation, argu-
ment movement, and the ability of the target ex-
pression to be negated or modified. Katz and
Giesbrecht (2006) compute meaning vectors for
literal and non-literal examples in the training set
and then classify test instances based on the close-
ness of their meaning vectors to those of the train-
ing examples. This approach was later extended
by Diab and Krishna (2009), who take a larger
context into account when computing the feature
vectors (e.g., the whole paragraph) and who also
include prepositions and determiners in addition
to content words.
Cook et al (2007) and Fazly et al (2009) take a
different approach, which crucially relies on the
concept of canonical form (CForm). It is as-
sumed that for each idiom there is a fixed form
(or a small set of those) corresponding to the syn-
tactic pattern(s) in which the idiom normally oc-
curs (Riehemann, 2001).The canonical form al-
lows for inflectional variation of the head verb but
not for other variations (such as nominal inflec-
tion, choice of determiner etc.). It has been ob-
served that if an expression is used idiomatically,
it typically occurs in its canonical form (Riehe-
mann, 2001). Cook et al exploit this behaviour
and propose an unsupervised method in which an
expression is classified as idiomatic if it occurs in
canonical form and literal otherwise. Canonical
forms are determined automatically using a statis-
tical, frequency-based measure.
Birke and Sarkar (2006) model literal vs. non-
literal classification as a word sense disambigua-
tion task and use a clustering algorithm which
compares test instances to two seed sets (one with
literal and one with non-literal expressions), as-
signing the label of the closest set.
Sporleder and Li (2009) propose another un-
supervised method which detects the presence or
absence of cohesive links between the component
words of the idiom and the surrounding discourse.
If such links can be found the expression is clas-
sified as literal otherwise as non-literal. Li and
Sporleder (2009) later extended this work by com-
bining the unsupervised classifier with a second-
stage supervised classifier.
Hashimoto and Kawahara (2008) present a su-
pervised approach to token-based idiom distinc-
tion for Japanese, in which they implement several
features, such as features known from other word
sense disambiguation tasks (e.g., collocations)
and idiom-specific features taken from Hashimoto
et al (2006). Finally, Boukobza and Rappoport
(2009) also experimented with a supervised clas-
sifier, which takes into account various surface
features.
In the present work, we also investigate super-
vised models for token-based idiom detection. We
are specifically interested in which types of fea-
tures (e.g., local context, global context, syntac-
tic properties) perform best on this task and more
specifically which features generalize across id-
ioms.
3 Data
We used the data set created by Sporleder and Li
(2009), which consists of 13 English expressions
(mainly V+PP or V+NP) that can be used both
literally and idiomatically, such as break the ice
or play with fire.1 To create the data set al in-
stances of the target expressions were extracted
from the Gigaword corpus together with five para-
graphs of context and then labelled manually as
?literal? or ?non-literal?. Overall the data set con-
sists of just under 4,000 instances. For most ex-
1We excluded four expressions from the original data set
because their number of literal examples was very small (<
2).
684
pressions the distribution is heavily skewed to-
wards the idiomatic interpretation, however for
some, like drop the ball, the literal reading is more
frequent. The number of instances varies, rang-
ing from 15 for pull the trigger to 903 for drop
the ball. While the instances were extracted from
a news corpus, none of them are domain-specific
and all expressions also occur in the BNC, which
is a balanced, multi-domain corpus.
To compute the features which we extract in
the next section, all instances in our data sets
were part-of-speech tagged by the MXPOST tag-
ger (Ratnaparkhi, 1996), parsed with the Malt-
Parser2, and named entity tagged with the Stan-
ford NE tagger (Finkel et al, 2005). The lemma-
tization was done by RASP (Briscoe and Carroll,
2006).
4 Indicators of Idiomatic and Literal
Usage
In this study we are particularly interested in
which linguistic indicators work well for the task
of distinguishing literal and idiomatic language
use. The few previous studies have mainly looked
at the lexical context in which and expression
occurs (Katz and Giesbrecht, 2006; Birke and
Sarkar, 2006). However, other properties of the
linguistic context might also be useful. We dis-
tinguish these features into different groups and
discuss them in the following sections.
4.1 Global Lexical Context (glc)
That the lexical context might be a good indica-
tor for the usage of an expression is obvious when
one looks at examples as in (1) and (2), which sug-
gest that literal and non-literal usages of a specific
idiom co-occur with different sets of words. Non-
literal uses of break the ice (1), for instance, tend
to occur with words like discuss, bilateral or re-
lations, while literal usages (2) predictably occur
with, among others, frozen, cold or water. What
we are looking at here is the global lexical context
of an expression, i.e., taking into account previ-
ous and following sentences. We are specifically
looking for words which are either semantically
related (in a wide sense) to the literal or the non-
2http://maltparser.org/index.html
literal sense of the target expression. The presence
or absence of such words can be a good indicator
of how the expression is used in a context.
(1) ?Gujral will meet Sharif on Monday and dis-
cuss bilateral relations,? the Press Trust of India
added. The minister said Sharif and Gujral would
be able to ?break the ice? over Kashmir.
(2) Meanwhile in Germany, the cold penetrated
Cologne cathedral, where worshippers had to
break the ice on the frozen holy water in the font.
We implemented two sets of features which en-
code the global lexical context: salient words and
related words as described in Li and Sporleder
(2009). The former feature uses a variant of
tf.idf to identify words that are particulary salient
for different usages. The latter feature identifies
words which are most strongly related to the com-
ponent words of the idiom.
We notice that sometimes several idioms co-
occur within the same instance. This is to say that
nonliteral usages may be indicators of each other
since authors may put them in a same context to
convey a specific opinion (e.g., irony). Due to this,
global lexical context features may also generalize
across idioms to some extend.
4.2 Local Lexical Context (locCont)
In addition to the global context, the local lex-
ical context, i.e., the words preceding and fol-
lowing the target expression, might also provide
important information. One obvious local clue
are words like literally or metaphorically speak-
ing, which when preceding or following an ex-
pression might indicate its usage. Unfortunately,
such clues are not only very rare (we only found
a handful in nearly 4,000 annotated examples) but
also not always reliable. For instance, it is not
difficult to find examples like (3) and (4) where
the word literally is used even though the idiom
clearly has a non-literal meaning.
(3) In the documentary the producer literally
spills the beans on the real deal behind the movie
production.
(4) The new philosophy is blatantly permissive and lit-
erally passes the buck to the House?s other com-
mittees.
685
However, there are other local cues. For exam-
ple, we found that the word just before get ones
feet wet tends to indicate non-literal usage as in
(5). Non-literal usage can also be indicated by the
occurrence of the prepositions over or between af-
ter break the ice as in (1) and (6). While such
cues are not perfect they often make one usage
more likely than the other. Unlike the semanti-
cally based global cues, many local clues are more
rooted in syntax, i.e., local cues work because spe-
cific constructions tend to be more frequent for
one or the other usage.
(5) The wiki includes a page of tasks suitable for those
just getting their feet wet.
(6) Would the visit of the minister help break the ice
between India and Pakistan?
Another type of local cues involves selectional
preferences. For example, idiomatic usage is
probable if the subject of play with fire is a coun-
try as in (7) or if break the ice is followed by a
with-PP whose NP refers to a person (8).
(7) Dudayev repeated his frequent warnings that Rus-
sia was playing with fire.
(8) Edwards usually manages to break the ice with the
taciturn monarch.
Based on those observations, we encode which
words occur in a ten word window around the tar-
get expression, five pre-target words and five post-
target words, as the locCont features.
4.3 Discourse Cohesion (dc)
We implemented two features, related score and
discourse connectivity, which take into account
the cohesive structure of an expression in its con-
text as described by Li and Sporleder (2009).
In addition, we also included the prediction of
the cohesion graph proposed by Sporleder and Li
(2009) as an additional feature. These features
look at the lexical cohesion between an expression
and the surrounding discourse, so they are more
likely to generalize across different idioms.
4.4 Syntactic Structure (allSyn)
To capture syntactic effects, we encoded infor-
mation of the head node (heaSyn) of the tar-
get expression in the dependency tree (e.g., break
may:ROOT
visit:SUB
the:NMOD of:NMOD
minister:PMOD
the:NMOD
break:VMOD
ice:OBJ
the:NMOD between
...
Figure 1: Dependency tree for a nonliteral exam-
ple of break the ice (The visit of the minister may
break the ice between India and Pakistan.)
in the dependency tree in Figure 1). The syn-
tactic features we encoded are the parent node
(parSyn), sibling nodes (sibSyn) and children
nodes (chiSyn) of the head node. These nodes in-
clude the following type of syntactic information:
Dependency Relation of the Verb Phrase The
whole idiomatic expression used as an object of
a preposition can be an indicative factor of id-
iomatic usage (see Example 9). This property is
captured by the heaSyn feature.
(9) Ross headed back last week to Washington to brief
president Bill Clinton on the Hebron talks after
achieving a breakthrough in breaking the ice in the
Hebron talks by arranging an Arafat-Netanyahu
summit .
Modal Verbs usually appear in the parent posi-
tion of the head verb (parSyn). Modals can be an
indicator of idiomatic usage such as may in Figure
1. In contrast, the modal had to is indicative that
the same phrase is used literally (Example 10).
(10) Dad had to break the ice on the chicken troughs.
Subjects can also provide clues about the usage
of an expression, e.g., if selectional preferences
are disobeyed. For instance, visit as a subject of
the verb phrase break the ice is an indicator of id-
iomatic usage (see Figure 1). Subjects typically
appear in the children position of the head verb
(chiSyn), but sometimes may appear in the sibling
position (sibSyn) as in Figure 1 .
Verb Subcat We also encode the arguments of
the head verb of the target expression. These ar-
guments can be, for example, additional PPs. This
feature encodes syntactic constraints and attempts
686
to model selectional restrictions. The likelihood
of subcategorisation frames may differ for the two
usages of an expression, e.g., non-literal expres-
sions often tend to have a shorter argument list.
For instance, the subcat frame <PP-on, PP-for>
intuitively seems more likely for literal usages of
the expression drop the ball (see Example 11)
than for non-literal ones, for which <PP-on> is
more likely (12). To capture subcategorisation be-
haviour, we encode the children nodes of the head
node (chiSyn).
(11) US defender Alexi Lalas twice went close to forc-
ing an equaliser , first with a glancing equaliser
from a Paul Caligiuri free kick and then from a
Wynalda corner when Prunea dropped the ball [on
the ground] only [for Tibor Selyme to kick fran-
tically clear] .
(12) ?Clinton dropped the ball [on this],? said John
Parachini.
Modifiers of the verb can also be indicative of
the usage of the target expression. For example,
in 13, the fact that the phrase get one?s feet wet is
modified by the adverb just suggest that it is used
idiomatically. Similar to verb subcat, modifiers
are often appear in the children position (chiSyn).
(13) The wiki includes a page of tasks suitable for those
just getting their feet wet.
Coordinated Verb Which verbs are coordi-
nated with the target expression, if any, can also
provide cues for the intended interpretation. For
example, in (14), the fact that break the ice is co-
ordinated with fall suggest that it is used literally.
The coordinated verb can appear at the sibling po-
sition, children position, or some other position of
the head verb depending on the parser. The Malt-
parser tends to put the coordinated verbs in the
children position (chiSyn).
(14) They may break the ice and fall through.
4.5 Other Features
Named Entities (ne) can also indicate the us-
age of an expression. For instance, a country
name in the subject position of the target expres-
sion break the ice is a strong indicator of this
phrase being used idiomatically (see Example 7).
Diab and Bhutada (2009) find that NE-features
perform best. They used a commercial NE-tagger
with 19 classes. We used the Stanford NE tag-
ger (Finkel et al, 2005), and encoded three named
entity classes (?person?, ?location?, ?organisza-
tion?) in the feature vector.
Indicative Terms (iTerm) Some words such as
literally, proverbially are also indicative of literal
or idiomatic usages. We encoded the frequencies
of those indicative terms as features.
Scare Quotes (quote) This feature encodes
whether the idiom is marked off by scare quotes,
which often indicates non-literal usage (15).
(15) Do consider ?getting your feet wet? online, using
some of the technology that is now available to us.
5 Experiments
In the previous section we discussed different lin-
guistic cues for idiom usage. To determine which
of these cues work best for the task and which
ones generalize across different idioms, we car-
ried out three experiments. In the first one (Sec-
tion 5.1) we trained one model for each idiom (see
Section 3) and tested the predictiveness of each
feature type individually as well as all features to-
gether. In the second experiment (Section 5.2), we
trained one generic model for all idioms and deter-
mined how the performance of this model differs
from the idiom-specific models. Specifically we
wanted to know whether the model would bene-
fit from the additional training data available by
combining information from several idioms. Fi-
nally (Section 5.3), we tested the generic model on
unseen idioms to determine whether these could
be classified based on generic properties even if
training data for the target expressions had not
been seen.
5.1 Idiom Specific Models
The first question we wanted to answer was how
difficult token-based idiom classification is and
which of the features we defined in the previous
section work well for this task. We implemented
a specific classifier for each of the idioms in the
data set. We trained one model for all features
in combination and one for each individual fea-
ture. Because the data set is not very big we de-
cided to run these experiments in 10-fold stratified
687
cross-validation mode. We used the SVM classi-
fier (SMO) from Weka.3
Table 1 shows the results. We report the pre-
cision (Prec.), recall (Rec.) and F-Score for the
literal class, as well as the accuracy. Note that due
to the imbalance in the data set, accuracy is not a
very informative measure here; a classifier always
predicting the majority class would already obtain
a relatively high accuracy. The literal F-Score ob-
tained for individual idioms varies from 38.10%
for bite one?s tongue to 96.10% for bounce of the
wall. However, the data sets for the different id-
ioms are relatively small and it is impossible to
say whether performance differences on individ-
ual idioms are accidental, or due to differences
in training set size or due to some inherent dif-
ficulty of the individual idiom. Thus we chose not
to report the performance of our models on indi-
vidual idioms but on the whole data set for which
the numbers are much more reliable. The final
performance confusion matrix is the sum over all
individual idiom confusion matrices.
Avg. literal Avg.
feature Prec. Rec. F-Score Acc.
all 89.84 77.06 82.96 93.36
glc+dc 90.42 76.44 82.85 93.36
allSyn 76.30 86.13 80.92 91.48
heaSyn 76.64 85.77 80.95 91.53
parSyn 76.43 88.34 81.96 91.84
chiSyn 76.49 88.22 81.94 91.84
sibSyn 76.27 88.34 81.86 91.78
locCont 76.51 88.34 82.00 91.86
ne 76.49 88.22 81.94 91.84
iTerm 76.51 88.34 82.00 91.86
quote 76.51 88.34 82.00 91.86
Basemaj 76.71 88.34 82.00 91.86
Table 1: Performance of idiom-specific models
(averaged over different idioms), 10-fold stratified
cross-validation.
The Baseline (Base) is built based on predict-
ing the majority class for each expression. This
means predicting literal for the expressions which
consist of more literal examples and nonliteral for
the expressions consisting of more nonliteral ex-
3http://www.cs.waikato.ac.nz/ml/weka/
amples. We notice the baseline gets a fairly high
performance (Acc.=91.86%).
The results show that the expressions can be
classified relatively reliably by the proposed fea-
tures. The performance beats the majority base-
line statistically significantly (p = 0.01, ?2 test).
We noticed that parSyn, chiSyn, locCont, iTerm
and quote features are too sparse. These indi-
vidual features cannot guide the classifier. As
a result, the classifier only predicts the majority
class which results in a performance similar to
the baseline. Some of the syntactic features are
less sparse and they get different results from the
baseline classifier, however, the performances of
these features are actually worse than the baseline.
This may be due to the relatively small training
size in each idiom specific model. When adding
those features together with statistical-based fea-
tures (glc+dc), the performance of the literal class
can be improved slightly. However, we did not ob-
serve any performance increase on the accuracy.
5.2 Generic Models
Having verified that literal and idiomatic usages
can be distinguished with some success by train-
ing expression-specific models, we carried out
a second experiment in which we merged the
data sets for different expressions and trained one
generic model. We wanted to see whether a
generic model, which has access to more training
data, performs better and whether some features,
e.g., the cohesion features profit more from this.
The experiment was again run in 10-fold stratified
cross-validation mode (using 10% from each id-
iom in the test set in each fold).
Table 2 shows the results. The baseline classi-
fier always predict the majority class ?nonliteral?.
Note that the result of this baseline is different
from the majority baseline in the idiom specific
model. In the idiom specific model, there are three
expressions 4 for which the majority class is ?lit-
eral?.
Unsurprisingly, the F-Score and accuracy of the
combined feature set drops a bit. However, the
performance still statistically significantly beats
the majority baseline classifier (p << 0.01,
?2 test). Similar to previous observation, the
4I.e., bounce off the wall, drop the ball, pull the trigger
688
Avg. literal Avg.
feature Prec. Rec. F-Score Acc.
all 89.59 65.77 73.22 89.90
glc+dc 82.53 60.86 70.06 89.08
allSyn 50.83 59.88 54.99 79.42
heaSyn 50.57 59.88 54.83 79.29
sibSyn 33.33 0.86 1.67 78.83
ne 62.45 20.00 30.30 80.69
iTerm 40.00 0.25 0.49 78.99
Basemaj ? ? ? 79.01
Table 2: Performance of the generic model (av-
eraged over different idioms), 10-fold stratified
cross-validation.
statistical-based features (glc+dc) work the best,
while the syntactic features are also helpful. How-
ever, the local context, iTerm, quote features are
very sparse and, as in the idiom-specific experi-
ments, the performances of these features are sim-
ilar to the majority baseline classifier. We ex-
cluded them from the Table 2.
The numbers show that the syntactic features
help more in this model compared with the idiom-
specific model. When including these features, lit-
eral F-Score increases by 3.16% while accuracy
increases by 0.9%. It seems that the syntactic
features benefit from the increased training set.
This is evidence that these features can generalize
across idioms. For instance, the phrase ?The US?
on the subject position may be not only indicative
of the idiomatic usage of break the ice, but also of
idiomatic usage of drop the ball.
We found that the indicative terms are rare in
our corpus. This is the reason why the recall rate
of the indicative terms is very low (0.25%). The
indicative terms are not very predictive of literal or
non-literal usage, since the precision rate is also
relatively low (40%), which means those words
can be used in both literal and nonliteral cases.
5.3 Unseen Idioms
In our final experiment, we tested whether a
generic model can also be applied to completely
new expressions, i.e., expressions for which no
instances have been seen in the data set. Such a
behaviour would be desireable for practical pur-
poses as it is unrealistic to label training data for
each idiom the model might possibly encounter in
a text. To test whether the generic model does in-
deed generalize to unseen expressions, we test it
on all instances of a given expression while train-
ing on the rest of the expressions in the dataset.
That is, we used a modified cross-validation set-
ting, in which each fold contains instances from
one expression in the test set. Since our dataset
contains 13 expressions, we run a 13-fold cross
validation. The final confusion matrix is the sum
over each confusion matrix in each round.
Avg. literal Avg.
feature Prec. Rec. F-Score Acc.
all 96.70 81.65 88.54 95.41
glc+dc 96.93 77.00 85.83 94.48
allSyn 52.54 58.77 55.48 79.52
heaSyn 51.35 59.47 55.11 78.96
sibSyn 55.56 2.32 4.46 78.38
ne 61.89 19.05 29.13 79.87
iTerm 66.67 0.7 1.38 78.36
Basemaj ? ? ? 79.01
Table 3: Performance of the generic model on un-
seen idioms (cross validation, instances from each
idiom are chosen as test set for each fold)
The results are shown in Table 3. Similar to the
generic model, we found that the cohesion fea-
tures and syntactic features do generalize across
expressions. Statistical features (glc+dc) perform
well in this experiment. When including more
linguistically orientated features, the performance
can be further increased by almost 1%. In line
with former observations, the sparse features men-
tioned in the former two experiments also do not
work for this experiments. We also excluded them
from the table.
One interesting finding about this experiment of
this model is that the F-Score is higher than for the
?generic model?. This is counter-intuitive, since
in the generic model, each idiom in the testing set
has examples in the training set, thus, we might
expect the performance to be better due to the fact
that instances from the same expression appear-
ing in the training set are more informative com-
pared with instances from different idioms. Fur-
ther analysis revealed that there are some expres-
sions for which it may actually be beneficial to
689
train on other expressions, as the evidence of some
features may be misleading.
literal F-S. Acc.
feature Spe. Gen. Spe. Gen.
all 86.85 91.79 80.67 88.37
glc+dc 86.75 88.84 80.67 84.61
allSyn 85.71 71.94 75.28 61.13
heaSyn 85.79 71.94 75.39 61.13
Table 4: Comparing the performance of the idiom
drop the ball on the idiom specific model (Spe.)
and generic model (Gen.)
Table 4 shows the comparison of the perfor-
mance of drop the ball on the idiom specific
model and the generic model on unseen idioms.
It can be seen that the statistical features (glc+dc)
work better for the model that is trained on the in-
stances from other idioms than the model which
is trained on the instances of the target expression
itself. We found this is due to the fact that drop the
ball is especially difficult to classify with the dis-
course cohesion features (dc). The literal cases are
often found in a context containing words, such
as fault, mistake, fail, and miss, which are used
to describe a scenario in a baseball game,5 while,
on the other hand, those context words are also
closely semantically related to the idiomatic read-
ing of drop the ball. This means the classifier can
be mislead by the cohesion features of the literal
instances of this idiom in the training set, since
they exhibit strong idiomatic cohesive links with
the target expression. When excluding drop the
ball from the training set, the cohesive links in
the training data are less noisy. Thus, the perfor-
mance increases. Unsurprisingly, the performance
of syntactic features works better for the idiom
specific model compared with the unseen idiom
model.
6 Conclusion
Idioms on the whole are frequent but instances of
each particular idiom can be relatively infrequent
(even for common idioms like ?spill the beans?).
The classes can also be fairly imbalanced, with
one class (typically the nonliteral interpretation)
5The corpus contains many sports news text
being much more frequent than the other. This
causes problems for training data generation. For
idiom specific classifiers, it is difficult to obtain
large data sets even when extracting from large
corpora and it is even more difficult to find suf-
ficient examples of the minority class. In order
to address this problem, we looked for features
which can generalize across idioms.
We found that statistical features (glc+dc) work
best for distinguishing literal and nonliteral read-
ings. Certain linguistically motivated features can
further boost the performance. However, those
linguistic features are more likely to suffer from
data sparseness, as a result, they often only predict
the majority class if used on their own. We also
found that some of the features that we designed
generalize well across idioms. The cohesion fea-
tures have the best generalization ability, while
syntactic features can also generalize to some ex-
tent.
Acknowledgments
This work was funded by the DFG within the
Cluster of Excellence MMCI.
References
Baldwin, Timothy, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephen Oepen. 2004. Road-testing
the English resource grammar over the British Na-
tional Corpus. In Proc. LREC-04, pages 2047?
2050.
Birke, Julia and Anoop Sarkar. 2006. A clustering
approach for the nearly unsupervised recognition of
nonliteral language. In Proceedings of EACL-06.
Boukobza, Ram and Ari Rappoport. 2009. Multi-
word expression identification using sentence sur-
face features. In Proceedings of EMNLP-09.
Briscoe, Ted and John Carroll. 2006. Evaluating
the accuracy of an unlexicalized statistical parser
on the PARC DepBank. In Proceedings of the
COLING/ACL on Main conference poster sessions,
pages 41?48.
Cook, Paul, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic
forms for the automatic identification of idiomatic
expressions in context. In Proceedings of the ACL-
07 Workshop on A Broader Perspective on Multi-
word Expressions.
690
Diab, Mona and Pravin Bhutada. 2009. Verb noun
construction mwe token classification. In Proceed-
ings of the Workshop on Multiword Expressions:
Identification, Interpretation, Disambiguation and
Applications, pages 17?22.
Diab, Mona T. and Madhav Krishna. 2009. Unsuper-
vised classification of verb noun multi-word expres-
sion tokens. In CICLing 2009, pages 98?110.
Fazly, Afsaneh, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Computational Linguistics,
35(1):61?103.
Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL-05, pages 363?
370.
Hashimoto, Chikara and Daisuke Kawahara. 2008.
Construction of an idiom corpus and its application
to idiom identification based on WSD incorporating
idiom-specific features. In Proceedings of EMNLP-
08, pages 992?1001.
Hashimoto, Chikara, Satoshi Sato, and Takehito Ut-
suro. 2006. Japanese idiom recognition: Drawing
a line between literal and idiomatic meanings. In
Proceedings of COLING/ACL-06, pages 353?360.
Katz, Graham and Eugenie Giesbrecht. 2006. Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the ACL/COLING-06 Workshop on
Multiword Expressions: Identifying and Exploiting
Underlying Properties.
Li, Linlin and Caroline Sporleder. 2009. Contextual
idiom detection without labelled data. In Proceed-
ings of EMNLP-09.
Ratnaparkhi, Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of EMNLP-
96.
Riehemann, Susanne. 2001. A Constructional Ap-
proach to Idioms and Word Formation. Ph.D. thesis,
Stanford University.
Sag, Ivan A., Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2001. Multiword
expressions: a pain in the neck for NLP. In Lecture
Notes in Computer Science.
Sporleder, Caroline and Linlin Li. 2009. Unsuper-
vised recognition of literal and non-literal use of id-
iomatic expressions. In Proceedings of EACL-09.
691
Improved Estimation of Entropy for
Evaluation of Word Sense Induction
Linlin Li?
Microsoft Development Center Norway
Ivan Titov??
University of Amsterdam
Caroline Sporleder?
Trier University
Information-theoretic measures are among the most standard techniques for evaluation of
clustering methods including word sense induction (WSI) systems. Such measures rely on
sample-based estimates of the entropy. However, the standard maximum likelihood estimates
of the entropy are heavily biased with the bias dependent on, among other things, the number of
clusters and the sample size. This makes the measures unreliable and unfair when the number
of clusters produced by different systems vary and the sample size is not exceedingly large. This
corresponds exactly to the setting of WSI evaluation where a ground-truth cluster sense number
arguably does not exist and the standard evaluation scenarios use a small number of instances of
each word to compute the score. We describe more accurate entropy estimators and analyze their
performance both in simulations and on evaluation of WSI systems.
1. Introduction
The task of word sense induction (WSI) has grown in popularity recently. WSI has the
advantage of not assuming a predefined inventory of senses. Rather, senses are induced
in an unsupervised fashion on the basis of corpus evidence (Schu?tze 1998; Purandare
and Pedersen 2004). WSI systems can therefore better adapt to different target domains
that may require sense inventories of different granularities. However, the fact that WSI
systems do not rely on fixed inventories also makes it notoriously difficult to evaluate
and compare their performance. WSI evaluation is a type of cluster evaluation problem.
Although cluster evaluation has received much attention (see, e.g., Dom 2001; Strehl
and Gosh 2002; Meila 2007), it is still not a solved problem. Finding a good way to
score partially incorrect clusters is particularly difficult. Several solutions have been
? Microsoft Development Center Norway. E-mail: linlin@coli.uni-saarland.de.
?? Institute for Logic, Language and Computation. E-mail: titov@uva.nl.
? Computational Linguistics and Digital Humanities, Trier University, 54286 Trier, Germany.
E-mail: sporledc@uni-trier.de.
Submission received: 14 March 2013; revised version received: 13 September 2013; accepted for publication:
20 November 2013
doi:10.1162/COLI a 00196
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
proposed but information theoretic measures have been among the most successful
and widely used techniques. One example is the normalized mutual information, also
known as V-measure (Strehl and Gosh 2002; Rosenberg and Hirschberg 2007), which
has, for example, been adopted in the SemEval 2010 WSI task (Manandhar et al. 2010).
All information theoretic measures of cluster quality essentially rely on sample-
based estimates of entropy. For instance, the mutual information I(c, k) between a gold
standard class c and an output cluster k can be written H(c) + H(k) ? H(k, c), where
H(c) and H(k) are the marginal entropies of c and k, respectively, and H(k, c) is their joint
entropy. The most standard estimator is the maximum-likelihood (ML) estimator, which
substitutes the probability of each event (cluster, classes, or cluster-class pair occurrence)
with its normalized empirical frequency.
Entropy estimators, even though consistent, are biased. This means that the expected
estimate of the entropy on a finite sample set is different from the true value. It is
also different from an expected estimate on a larger test set generated from the same
distribution, as the bias depends on the size of the sample. This discrepancy negatively
affects entropy-based evaluation measures, such as the V-measure. This is different from
supervised classification evaluation, where the classification accuracy on a finite test set
is expected to be equal to the error rate (for the independent and identically distributed,
i.i.d.) case, though it can be different due to variance (due to choice of the test set). As
long as the number of samples is large with respect to the number of classes and clusters,
the estimate is sufficiently close to the true entropy. Otherwise, the quality of entropy
estimators matters and the bias of the estimator can be large. This problem is especially
prominent for the ML estimator (Miller 1955).
In WSI, we are faced with exactly those conditions that negatively affect the entropy
estimators. In a typical setting, the number of examples per word is small?for example,
less than 100 on average for the SemEval 2010 WSI task. The number of clusters, on
the other hand, can be fairly high, with some systems outputting more than 10 sense
clusters per word on average. Because the bias of an entropy estimator is dependent
on, among other things, the number of clusters, the ranking of different WSI systems is
partly affected by the number of clusters they produce. Even worse, the ranking is also
affected by the size of the test set. The problem is exacerbated when computing the joint
entropy between clusters and classes, H(k, c), because this requires estimating the joint
probability of cluster-class pairs for which the statistics are even more sparse.
The bias problem of entropy estimators has long been known in the information
theory community and many studies have addressed this issue (e.g., Miller 1955; Batu
et al. 2002; Grasberger and Schu?rmann 1996). In this article, we compare different
estimators and their influence on the computed evaluation scores. We run simulations
using a Zipfian distribution where we know the true entropy. We also compare different
estimators against the SemEval 2010 WSI benchmark. Our results strongly suggest that
there are estimators, namely, the best-upper-bound (BUB) estimator (Paninski 2003)
and jackknifed (Tukey 1958; Quenouille 1956) estimators, which are clearly preferable
to the commonly used ML estimators.
2. Clustering Evaluation
2.1 Information-Theoretic Measures
The main challenge in evaluating clustering methods is that successful measures should
be able to compare solutions found at different levels of granularity. In other words,
one cannot assume that there exists one-to-one mapping between the predicted clusters
672
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
and the gold-standard classes. A natural approach would be to consider arbitrary
statistical dependencies between the cluster assignment k and the class assignment c.
The standard measure of statistical dependence of two random variables is the Shannon
mutual information I(k, c) (MI). MI is 0 if two variables are independent, and it is equal
to the entropy of a variable if another variable is deterministically dependent on it.
Clearly, such measure would favor clusterings with higher entropy, and, consequently,
normalized versions of MI are normally used to evaluate clusterings. One instance of
normalized MI actively used in the context of WSI evaluation is the V-measure, or
symmetric uncertainty (Witte and Frank 2005; Rosenberg and Hirschberg 2007):
V(k, c) =
2I(k, c)
H(k) + H(c)
=
2(H(k) + H(c) ? H(k, c))
H(k) + H(c)
though other forms of MI normalization have also been explored (Strehl and Gosh 2002).
Because the true marginal and the joint entropies are not known, the standard
maximum likelihood estimators (also called plug-in estimators of entropy) are normally
used instead. The ML estimates H? have the analytical form of an entropy with the
normalized empirical frequency substituted instead of the unknown true membership
probabilities, for example:
H?(c) =
m
?
i=1
?
ni
N log
ni
N (1)
where ni is the number of times cluster i appears in the set, m is the number of clusters,
and N is the size of the set (i.e., the sample size).
The ML estimators of entropy are consistent but heavily negatively biased (see
Section 3 for details). In other words, the expectation of H? is lower than the true entropy,
and this discrepancy increases with the number of clusters m and decreases with the
sample size N. When m is comparable to N, the ML estimator is known to be very
inaccurate (Paninski 2004).
Note that for V-measure estimation the main source of the estimation error is the
joint entropy H(k, c),1 as the number of possible pairs (c, k) for most systems would be
large whereas the total number of occurrences will remain the same as for the estimation
of H(c) and H(k). Therefore, the absolute value of the bias for H?(c, k) will exceed the
aggregate bias of the estimators of marginal entropy, H?(c) and H?(k). As a result, the
V-measure will be positively biased, and this bias would be especially high for systems
predicting a large number of clusters.
This phenomenon has been previously noticed (Manandhar et al. 2010) but no satis-
factory explanation has been given. The shortcomings of the ML estimator are especially
easy to see on the example of a baseline system that assigns every instance in the testing
set to an individual cluster. This baseline, when averaged over the 100 target words, out-
performs all the participants? systems of the SemEval-2010 task on the standard testing
set (Manandhar and Klapaftis 2009). Though we cannot compute the true bias for any
real system, the computation is trivial for this baseline. The true V-measure is equal to 0,
1 V-measure can be expressed via entropies in a number of different ways, although, for ML estimation
they are all equivalent. For some more complex estimators, including some of the ones considered here,
the resulting estimates will be somewhat different depending on the decomposition. We will focus on the
symmetric form presented here.
673
Computational Linguistics Volume 40, Number 3
as the baseline can be regarded as a limiting case of a stochastic system that picks up one
of the m clusters under the uniform distribution with m ? ?; the mutual information
between any class labels and clustering produced by such model equals 0 for every m.
However, the ML estimate for the V-measure is V?(k, c) = 2H?(c)/(log N + H?(c)). For the
testing set of SemEval 2010, this estimate, averaged over all the words, yields 31.7%,
which by far exceeds the best result of any system (16.2%). On an infinite (or sufficiently
large) set, however, its performance would change to the worst. This is a problem not
only for the baseline but for any system which outputs a large number of classes: The
error measures computed on the small test set are far from their expectations on the
new data. We will see in our quantitative analyses (Section 5) that using more accurate
estimators will have the most significant effect on both the V-measure and on the ranks
of systems which output richer clustering, agreeing with this argument.
Though in this analysis we focused on the V-measure, other information theoretic
measures have also been proposed. Examples of such measures include the variation
of information measure (Meila 2007) VI(c, k) = H(c|k) + H(k|c) and Q0 measure (Dom
2001) H(c|k). This argument applies to these evaluation measures as well, and they can
all be potentially improved by using more accurate estimators.
2.2 Alternative Measures
Not only information-theoretic measures have been proposed for clustering evaluation.
An alternative evaluation strategy is to attempt to find the best possible mapping
between the predicted clusters and the gold-standard classes and then apply standard
measures like precision, recall, and F-score. However, if the best mapping is selected on
the test set the result can be overoptimistic, especially for rich clusterings. Consequently,
such methods constrain the set of permissible mappings to a restricted family. For exam-
ple, for the F-score, one considers only mappings from each class to a single predicted
cluster (Zhao and Karypis 2005; Agirre and Soroa 2007). This restriction is generally too
strong for many clustering problems (Meila 2007; Rosenberg and Hirschberg 2007), and
especially inappropriate for the WSI evaluation setting, as it penalizes sense induction
systems that induce more fine-grained senses than the ones present in the gold-standard
sense set.
The Paired F-score (Manandhar et al. 2010) is somewhat less restrictive than the
F-score measures in that it defines precision and recall in terms of pairs of instances (i.e.,
effectively evaluating systems based on the proportion of correct links). However, the
Paired F-score has the undesirable property that it ranks those systems highest which
put all instances in one cluster, thereby obtaining perfect recall.
As an alternative, the supervised evaluation measure has been proposed (Agirre
et al. 2006). This approach in addition to the testing set uses an auxiliary mapping set.
First the mapping is induced on the mapping set, then the quality of the mapping is
evaluated on the testing set. One problem with this evaluation scenario is that the size
of the mapping set has an effect on the results and there is no obvious criterion for
selecting the right size of the mapping set. For the WSI task, the importance of the set
size was empirically confirmed when the evaluation set was split in proportions 80:20
(80% for the mapping sets, and 20% for testing) instead of the original 60:40 split: The
scores of all top 10 systems improved and the ranking changed as well (Manandhar
et al. 2010) (see also Table 1 later in this article).
Further cluster evaluation measures have been proposed for other language pro-
cessing tasks, such as B3 (Bagga and Baldwin 1998) or CEAF (Luo 2005) for coreference
resolution evaluation. In this article, we are concerned with entropy-based measures.
674
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
For a more general assessment of measures for clustering evaluation see Amigo et al.
(2009) and Klapaftis and Manandhar (2013).
3. Entropy Estimation
Given the influence that information theory has had on many fields, including signal
processing, neurophysiology, and psychology, to name a few, it is not surprising that the
topic of entropy estimation has received considerable attention over the last 50 years.2
However, much of the work has focused on settings where the number of classes is
significantly lower than the size of the sample. More recently a set-up where the sample
size N is comparable to the number of classes m has started to receive attention (Paninski
2003, 2004).
In this section, we start by discussing the intuition for why the ML estimator is
heavily biased in this setting. Though unbiased estimators of entropy do not exist,3
various techniques have been proposed to reduce the bias while controlling the vari-
ance (Grasberger and Schu?rmann 1996; Batu et al. 2002). We will discuss widely used
bias-corrected estimators, the ML estimator with Miller-Madow bias correction (Miller
1955) and the jackknifed estimator (Strong et al. 1998). Then we turn to the more
recent technique proposed specifically for the N ? m setting, the best-upper-bound
(BUB) estimator (Paninski 2003). We will conclude this section by explaining how these
estimators can be computed using stochastic (weighted) output of WSI systems.
3.1 Standard Estimators of Entropy
As we discussed earlier, the ML estimator (1) is negatively biased. For a fixed distri-
bution p, a little algebra can be used to show that the bias of the maximum likelihood
estimator can be written as
H ? Ep(H?) = Ep(D(p? ? p))
where Ep denotes an expectation under p, D is the Kullback-Leibler (KL) divergence,
and p? is the empirical distribution in the sample of N elements drawn i.i.d. from p.
Because the KL-divergence is always non-negative, it follows that the bias is always
non-positive. It also follows that the expected divergence is larger if the size of the
sample is small. In fact, this expression can be used to obtain the asymptotic bias rate
(N ? ?) (Miller 1955). The bias rate derived in this way would suggest a form of
correction to the ML estimator, called Miller-Madow bias correction H?MM = H? +
m??1
N ,
where m? is an estimate of m, as the true size of support m may not be known. In our
experiments, we use a basic estimator m? which is just the number of different clusters
(classes or cluster-class pairs depending on the considered entropy) appearing in the
sample. We will call the estimator H?MM the Miller-Madow (MM) estimator. As the MM
estimator is motivated by the asymptotic behavior of the bias, it is not very appropriate
for N ? m.
2 For a relatively recent overview of progress in entropy estimation research see, for example, the
proceedings of the NIPS 2003 workshop on entropy estimation.
3 The expectation of any estimate from i.i.d. samples is a polynomial function of class probabilities.
The entropy is non-polynomial and therefore unbiased estimators do not exist.
675
Computational Linguistics Volume 40, Number 3
The bias of the ML estimator decreases with the size of the sample. Intuitively, an
estimate of the discrepancy in estimates produced from samples of different sizes can
be used to correct the ML estimator: If an estimate based on N ? 1 samples significantly
exceeds the estimate from N samples, then the bias of the estimator is still large.
Roughly, this intuition is encoded in the jackknifed (JK) estimator (Strong et al. 1998):
H?JK = NH? ?
N ? 1
N
N
?
j=1
H?
?j
where H?
?j is the ML estimator based on the original sample excluding the example j.
3.2 BUB Estimator
We can observe that all the previous estimators can be expressed in the form of a linear
function of the ordered histogram statistics
H?(a) =
N
?
j=0
aj,Nhj (2)
where hj is the number of classes which appear j times in the sample:
hj =
m
?
i=1
[[ni = j]] (3)
where [[ ]] denotes the indicator function. The coefficients aj,N for the ML, MM, and JK
estimators are equal to:
aML,j,N = ?
j
N log
j
N
aMM,j,N = ?
j
N log
j
N +
1 ? jN
N
aJK,j,N = NaML,j,N ?
N ? 1
N ((N ? j)aML,j,N?1 + jaML,j?1,N?1)
This observation suggests that it makes sense to study an estimator of the general
form H?(a) as defined in Equation (2). Upper bounds on the bias and variance of such
estimators4 have been stated in Paninski (2003). These bounds imply an upper bound
on the standard measure of estimator performance, mean squared error (MSE, the
sum of the variance and squared bias). The worst-case estimator is then obtained
by selecting a to minimize the upper bound on MSE, and, therefore, it is called the
best-upper-bound estimator. This optimization problem5 corresponds to a regularized
4 We argued that variance is not particularly important for the ML estimator with N ? m. However,
for an arbitrary estimator of the form of Equation (2) this may not be true, as the coefficients aj,N
may be oscillating, resulting in an estimator with a large variance (Antos and Kontoyiannis 2001).
5 More formally, its modification where the L2 norm is optimized instead of the original L?
optimization set-up.
676
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
least-squares problem and can be solved analytically (see Appendix A and Paninski
[2003] for technical details).
This technique is fairly general, and can potentially be used to minimize the bound
for a particular type of distribution. This direction can be promising, as the types of
distributions observed in WSI are normally fairly skewed (arguably Zipfian) and tighter
bounds on MSE may be possible. In this work, we use the universal worst-case bounds
advocated in Paninski (2003).
3.3 Estimation with Stochastic Predictions
As many WSI systems maintain a distribution over predicted clusters, in SemEval 2010
the participants were encouraged to provide a weighted prediction (i.e., a distribution
over potential clusters for each example) instead of predicting just a single most-likely
cluster.
We interpret the weighted output of a system on an example l as a categorical
trial with the probabilities of outcomes p?(l)i provided by the model where i is an index
of the cluster. Therefore a stochastic output of a system on the test set represents
a distribution over samples generated from these trials; the estimator can be com-
puted as an expectation under this distribution. For estimators of the form of Equa-
tion (2), we can exploit the linearity of expectations and write the expected value of the
estimator as
Ep?
[
H?(a)
]
=
N
?
j=0
aj,NEp?
[
hj
]
where Ep?
[
hj
]
is the expected number of classes with j counts. We can rewrite it using the
linearity property again, this time for expression (3):
Ep?
[
H?(a)
]
=
N
?
j=0
aj,N
m
?
i=1
P?(ni = j, N) (4)
where P?i(ni = j, N) is the distribution over the number of counts for non-identical
Bernoulli trials p?(l)i and 1 ? p?
(l)
i (l = 1, . . . , N), known as the Poisson binomial distribu-
tion, a generalization of the standard binomial distribution. The probabilities can be
efficiently computed using one of alternative recursive formulas (Wang 1993). One
of the simplest schemes, with good numerical stability properties, is the recursive
computation:
P?i(ni = j, t) = P?i(ni = j ? 1, t ? 1)p?
(j)
i + P?i(ni = j, t ? 1)(1 ? p?
(j)
i )
where j = 1, . . . , N.
4. Simulations
Because the true entropy (and V-measure) is not known on the WSI task, we start with
simulations where we generated samples from a known distribution and can compare
677
Computational Linguistics Volume 40, Number 3
0 5 10 15 20 25 30 35 40 45 50
0
0.5
1
1.5
2
2.5
Sample Size (N)
En
tro
py
 
 
H
BUB
JK
MM
ML
Figure 1
The estimated and true entropy for uniform distribution.
the estimates (and their biases) with the true entropy. In all our experiments, we set the
number of clusters m to 10 and varied the sample size N (Figure 1). Each point on the
graph is the result of averaging over 1,000 sampling experiments.6
The distribution of senses for a given word is normally skewed: For most words
the vast majority of occurrences correspond to one or two most common senses even
though the total number of senses can be quite large (Kilgarriff 2004). This type of long-
tail distribution can be modeled with Zipf?s law. Consequently, most of our experiments
consider Zipfian distributions. For Zipf?s law, the probability of choosing an element
with rank k is proportional to 1ks , where s is a shape parameter. Small values of the
parameter s correspond to flatter distributions; the distributions with a larger s are
increasingly more skewed. The estimators? prediction for Zipfian distributions with
a different s are shown in Figure 2. For s = 4, over 90% of the probability mass is
concentrated on a single class. For every distribution we plot the true entropy (H)
and the estimated values; compare results with a uniform distribution as seen in
Figure 1.
In all figures, we observe that over the entire range of sample sizes, the bias
for the bias-corrected estimates is indeed reduced substantially with respect to that
of the ML estimator. This difference is particularly large for smaller N?the realistic
setting for the computation of H(c, k) for the WSI task. For the uniform distribution
and flatter Zipf distributions (s = 1 and 2), the JK estimator seems preferable for all
but the smallest sample sizes (N > 3). The BUB estimator outperforms the JK estimator
with very skewed distributions (s = 3 and s = 4) and in most cases provides the least
biased estimates with very small N. However, these results with very small sample sizes
(N ? 2) may not have much practical relevance as any estimator is highly inaccurate in
this mode. The MM bias correction, as expected, is not sufficient for small N. Although
it outperforms the ML estimates, its error is consistently larger than those of other bias-
correction strategies.
Overall, the simulations suggest that the ML estimators are not very appropriate for
entropy estimation with the types of distributions which are likely to be observed in the
6 In this way we study only the bias of estimators.
678
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
0 5 10 15 20 25 30 35 40 45 50
0
0.5
1
1.5
2
2.5
Sample Size (N)
En
tro
py
 
 
H
BUB
JK
MM
ML
0 5 10 15 20 25 30 35 40 45 50
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Sample Size (N)
En
tro
py
 
 
H
BUB
JK
MM
ML
(a) s = 1 (b) s = 2
0 5 10 15 20 25 30 35 40 45 50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Sample Size (N)
En
tro
py
 
 
H
BUB
JK
MM
ML
0 5 10 15 20 25 30 35 40 45 50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Sample Size (N)
En
tro
py
 
 
H
BUB
JK
MM
ML
(c) s = 3 (d) s = 4
Figure 2
The estimated and true entropy of Zipf?s law.
WSI tasks. Both the JK and BUB estimators are considerably less biased alternatives to
the ML estimations.
5. Effects on WSI Evaluation
To gauge the effect of the bias problem on WSI evaluation, we computed how the
ranking of the SemEval 2010 systems (Manandhar et al. 2010) were affected by different
estimators. The SemEval 2010 organizers supplied a test set containing 8,915 manually
annotated examples covering 100 polysemous lemmas.
The average number of gold standard senses per lemma was 3.79. Overall,
27 systems participated and were ranked according to their performance on the test
set, applying the V-measure evaluation as well as paired F-score and a supervised
evaluation scheme. The systems were also compared against three baselines. For the
Most Frequent Sense (MFS) baseline all test instances of a given target lemma are
grouped into one cluster, that is, there is exactly one cluster per lemma. The second
baseline, Random, assigns each instance randomly to one of four clusters. The last
baseline, proposed in Manandhar and Klapaftis (2009), 1-cluster-per-instance (1ClI),
produces as many clusters as there are instances in the test set.
Table 1 gives an overview of the different systems and the three baselines (shown in
italics). The systems are presented in the order in which they were given in the official
679
Computational Linguistics Volume 40, Number 3
Table 1
V-measure computed with different estimators. Supervised recall is shown for comparison
(80:20 and 60:40 splits for mapping/evaluation, numbers as provided by Manandhar et al. 2010).
The corresponding ranks are shown in parentheses.
System C# ML MM JK BUB Supervised Recall
80:20 60:40
1ClI 89.1 31.6 (1) 29.5 (1) 27.4 (1) ?3.6 (29) ? ?
Hermit 10.8 16.2 (2) 13.1 (4) 10.7 (4) 11.0 (2) 58.3 (17) 57.3 (18)
UoY 11.5 15.7 (4) 14.3 (2) 13.1 (2) 11.4 (1) 62.4 (1) 62.0 (1)
KSU KDD 17.5 15.7 (3) 13.2 (3) 11.0 (3) 7.6 (3) 52.2 (24) 50.4 (25)
Duluth-WSI 4.1 9.0 (5) 6.9 (5) 5.7 (5) 5.6 (5) 60.5 (2) 59.5 (5)
Duluth-WSI-SVD 4.1 9.0 (6) 6.9 (6) 5.7 (6) 5.6 (6) 60.5 (3) 59.5 (4)
Duluth-R-110 9.7 8.6 (7) 4.7 (16) 1.9 (20) 3 (17) 54.8 (23) 53.6 (23)
Duluth-WSI-Co 2.5 7.9 (8) 6.4 (7) 5.7 (7) 5.7 (4) 60.8 (4) 60.1 (2)
KCDC-PCGD 2.9 7.8 (9) 6.3 (8) 5.5 (8) 5.2 (7) 59.5 (9) 59.1 (7)
KCDC-PC 2.9 7.5 (10) 6.2 (9) 5.4 (9) 5.0 (8) 59.7 (8) 58.9 (9)
KCDC-PC-2 2.9 7.1 (11) 5.7 (12) 4.9 (12) 4.5 (13) 59.8 (7) 58.9 (8)
Duluth-Mix-Narrow-Gap 2.4 6.9 (15) 5.5 (14) 4.8 (14) 4.8 (9) 56.6 (21) 56.2 (21)
KCDC-GD-2 2.8 6.9 (14) 5.7 (11) 4.9 (11) 4.6 (12) 58.7 (13) 57.9 (15)
KCDC-GD 2.8 6.9 (12) 5.8 (10) 5.0 (10) 4.6 (11) 59.0 (11) 58.3 (11)
Duluth-Mix-Narrow-PK2 2.7 6.8 (16) 5.4 (15) 4.6 (15) 4.6 (10) 56.1 (22) 55.7 (22)
Duluth-MIX-PK2 2.7 5.6 (17) 4.3 (17) 3.5 (17) 3.5 (16) 51.6 (25) 50.5 (24)
Duluth-R-15 5.0 5.3 (18) 2.4 (20) 0.7 (24) 1.3 (22) 56.8 (20) 56.5 (19)
Duluth-WSI-Co-Gap 1.6 4.8 (19) 4.1 (18) 3.8 (16) 4.1 (15) 60.3 (5) 59.5 (3)
Random 4.0 4.4 (20) 1.9 (22) 0.5 (25) 0.8 (24) 57.3 (19) 56.5 (20)
Duluth-R-13 3.0 3.6 (21) 1.5 (25) 0.5 (26) 0.7 (25) 58.0 (18) 57.6 (17)
Duluth-WSI-Gap 1.4 3.1 (22) 2.6 (19) 2.5 (18) 2.7 (18) 59.8 (6) 59.3 (6)
Duluth-Mix-Gap 1.6 3.0 (23) 2.3 (21) 1.9 (19) 2.0 (19) 50.6 (26) 49.8 (26)
Duluth-Mix-Uni-PK2 2.0 2.4 (24) 1.8 (23) 1.5 (21) 1.4 (21) 19.3 (27) 19.1 (27)
Duluth-R-12 2.0 2.3 (25) 0.8 (27) 0.2 (27) 0.3 (28) 58.5 (16) 57.7 (16)
KCDC-PT 1.5 1.9 (26) 1.6 (24) 1.4 (22) 1.5 (20) 58.9 (12) 58.3 (13)
Duluth-Mix-Uni-Gap 1.4 1.4 (27) 1.0 (26) 0.8 (23) 1.0 (23) 18.7 (28) 18.9 (28)
KCDC-GDC 2.8 6.9 (13) 5.7 (13) 4.8 (13) 4.5 (14) 59.1 (10) 58.3 (10)
MFS 1.0 0 (29) 0.0 (29) 0.0 (28) 0.5 (27) 58.7 (15) 58.3 (12)
Duluth-WSI-SVD-Gap 1.0 0.0 (28) 0.0 (28) 0.0 (29) 0.5 (26) 58.7 (14) 58.2 (14)
KCDC-PC-2* 2.9 5.7 7.2 2.3 2.2 ? ?
UoY* 11.5 25.1 22.8 17.8 5.0 ? ?
SemEval 2010 results table (Table 4 in Manandhar et al. (2010), p. 66). Table 1 shows
the average number of clusters per word (C#), the V-measure computed with different
estimators (ML, MM, JK, and BUB), and the rankings it produces (in brackets).7 For
comparison, the results of a supervised evaluation are also shown. The bottom two rows
(KCDC-PC-2? and UoY?) show the scores computed from the stochastic (weighted)
output (Section 3.3) for systems KCDC-PC-2 and UoY, respectively. Other systems did
not produce weighted output.
7 The ranking produced by the ML estimator should mirror that of the official results. In some cases it
does not?for example, system UoY was placed before KSU in the official results, whereas the ML
estimator would predict the reverse order. As the difference in V-measure is small, we attribute this
discrepancy to rounding errors. The system KCDC-GDC seems to be misplaced in the official results
list; according to V-measure it should be ranked higher. Our ranking was computed before rounding,
and there were no ties.
680
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
0 2 4 6 8 10 12 14 16 18
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Average Cluster Number
M
L?
JK
0 2 4 6 8 10 12 14 16 18
?0.01
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
Average Cluster Number
M
L?
BU
B
(a) H? - H?JK (b) H? - H?BUB
Figure 3
Discrepancy in estimates as a function of the predicted number of classes.
The 27 systems vary widely in the number of average clusters they output
per lemma, ranging from 1.02 (Duluth-WSI-SVD) to 17.5 (KSU-KDD). To assess the
influence of the cluster granularity on the entropy estimates, we compared the estimates
given by the ML estimator against those given by JK and BUB for different numbers of
clusters. Figure 3 plots the cluster numbers output by the systems against the estimate
difference for ML vs. JK (Figure 3a) and ML vs. BUB (Figure 3b). If two estimators
agree perfectly (i.e., produce the same estimate), their difference should always be
zero, independent of the number of clusters. As can be seen, this is not the case. As
expected, the difference is larger for systems with larger numbers of clusters, such as
KSU-KDD. This trend will result in unfair preference towards systems producing richer
clusterings.
Figure 4 shows the effect that the discrepancy in estimates has on the rankings
produced by using either of the three estimators. Figure 4a plots the ranking of the
ML estimator against JK, and Figure 4b plots the ranking of ML against BUB. Dots that
lie on the diagonal line indicate systems whose rank has not changed. It can be seen that
this only applies to a minority of the systems. In general, there are significant differences
between the rankings produced by ML and those by JK or BUB. We have seen that the
ML estimator can lead to counterintuitive and undesirable results, such as ranking the
0 5 10 15 20 25 30
0
5
10
15
20
25
30
ML Ranking (SemEval10)
JK
 R
an
kin
g
0 5 10 15 20 25 30
0
5
10
15
20
25
30
ML Ranking (SemEval10)
BU
B 
Ra
nk
in
g
(a) H? vs. H?JK (b) H? vs. H?BUB
Figure 4
Discrepancy in rankings as a function of the predicted number of classes.
681
Computational Linguistics Volume 40, Number 3
1-cluster-per-instance baseline highest. The BUB estimator corrects this and assigns the
last rank to this baseline.8
The estimate for the V-measure is based on the estimates of the marginal and joint
entropies. To confirm our intuition that joint entropies are more significantly corrected,
we looked into the differences between estimates of each entropy for five systems with
the largest number of clusters (excluding the 1C1I baseline). The average differences
in estimation of H(k) and H(k, c) between JK and ML estimators are 0.08 and 0.16,
respectively, confirming our hypothesis. Analogous discrepancies for the pair BUB vs.
ML are 0.15 and 0.06, respectively. The differences in the entropy of the gold standard
clustering, H(c), is less significant (< 0.02 for both methods) as the gold standard is less
fine-grained than the clusters proposed by these five systems.
For the stochastic version evaluation, we can observe that the score for the KCDC-
PC-2 system is mostly decreased with respect to the ?deterministic? evaluation (except
for the MM estimator). Conversely, the score of UoY is mostly improved except to
the prediction of the BUB estimator. These differences are somewhat surprising: The
stochastic version resulted in significantly larger disagreement between the estimators
than the deterministic version. We do not yet have a satisfactory explanation for this
phenomenon.
It is important to notice that for the vast majority of the systems there is agreement
between the scores of the JK and BUB estimator, wheres the ML estimator significantly
overestimates the V-measure for most of the systems. This observation, coupled with
the observed behavior of the JK and BUB estimators in the simulations, suggest that
their predictions are considerably more reliable than predictions of the plug-in ML
estimator.
Comparing the V-measure (BUB) rankings to those obtained by supervised evalu-
ation (last two columns in Table 1) shows noticeable differences. Several systems that
rank highly according to the V-measure occupy the lower end of the scale when evalu-
ated according to supervised recall (Hermit, KSU KDD, Duluth-Mix-Narrow-Gap).
6. Conclusions
In this work, we analyzed the shortcomings of information-theoretic measures in the
context of WSI evaluation and argued that main drawbacks of these approaches, such
as the preference for the systems predicting richer clusterings or assigning the top score
to the 1-cluster-per-instance baseline, are caused by the bias of the underlying sample-
based estimates of entropy. We studied alternative estimators, including one specifically
designed to deal with cases where the number of examples is comparable with the num-
ber of clusters. Two of the considered estimators, the jackknifed estimator and the best-
upper-bound estimator, achieve consistently and significantly less biased results than
the standard ML estimator when evaluated in simulations with Zipfian distributions.
The corresponding estimates in the WSI evaluation context can result in significant
changes in scores and relative rankings, with systems producing richer clusterings more
severely affected. We believe that these results strongly suggest that more accurate
estimates of entropy should be used in future evaluations of sense induction systems.
Other unsupervised tasks in natural language processing, such as word clustering or
8 Note that the V-measure is actually negative here. Though this is not possible for the true V-measure,
the estimated V-measure expresses a difference between the estimated joint entropy and the marginal
entropies and can be negative.
682
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
named entity disambiguation, may also benefit from using information-theoretic scores
based on more accurate estimators.
Appendix A: Derivation for the BUB Estimator
We provide a brief derivation for the BUB estimator and refer the reader to Paninski
(2003) for details and discussion. The BUB estimator is obtained by minimizing an upper
bound on MSE for estimators H(a) (see Equation (2)). First, MSE is bounded from above
by maximizing the variance and the bias independently:
max
p
Bp(H?(a))
2
+ Vp(H?(a)) ? max
p
Bp(H?(a))
2
+ max
p
Vp(H?(a)) (A.1)
where p = (p1, . . . , pm) is an underlying discrete measure; Bp and Vp are the bias and the
variance of the estimator given p. Then individual bounds both for the squared bias and
the variance can be constructed.
We start by deriving a bound for the bias. Using linearity of expectation, the expec-
tation of H?(a) can be written as
Ep(H?(a)) =
N
?
j=0
aj,NEp(hj) =
N
?
j=0
aj,N
m
?
i=1
Bj,N(pi)
where Bj,N(x) is the binomial polynomial
(
N
j
)
xj(1 ? x)N?j. Then, with simple algebra,
we have
Bp(H?(a)) =
m
?
i=1
?
?
N
?
j=0
aj,NBj,N(pi) ? H(pi)
?
?
where H(x) = ?x log x, the entropy function. A uniform upper bound can be obtained
by bounding each term in the sum:
|Bp(H?(a))| ? m sup
x
|
N
?
j=0
aj,NBj,N(x) ? H(x)|
However, this bound is not too tight as it would overemphasize importance of the
approximation quality for components i with pi close to 1. Intuitively, the behavior near
0 is more important, as there can be more components pi close to 0. Paninski (2003)
generalizes this bound by considering a weighted version
|Bp(H?(a))| ? 2 sup
x
f (x)|
N
?
j=0
aj,NBj,N(x) ? H(x)| (A.2)
with the function f chosen to emphasize smaller components
f (x) =
{
m, x < 1m
1/x, x ? 1m
683
Computational Linguistics Volume 40, Number 3
As shown in Antos and Kontoyiannis (2001) and in Paninski (2003), bounds on
the variance of the estimator H?(a) can be derived using either McDiarmid or Steele
bounds (Steele 1986). For the Steele bound, it has the form
Vp(H?(a)) < N max
0?j<N
(aj+1 ? aj)
2 (A.3)
Finally, MSE can be bounded by substituting Equations (A.2) and (A.3) into the
inequality (A.1). For computational reasons, instead of choosing a to minimize the
bound, the L2 relaxation of the L? loss is used, resulting in a regularized least-squares
problem.
Acknowledgments
The research was carried out when the
authors were at Saarland University. It was
funded by the German Research Foundation
DFG (Cluster of Excellence on Multimodal
Computing and Interaction, Saarland
University, Germany). We would like to
thank the anonymous reviewers for their
valuable feedback.
References
Agirre, Eneko, David Mart??nez, Oier Lo?pez
de Lacalle, and Aitor Soroa. 2006. Evaluating
and optimizing the parameters of an
unsupervised graph-based WSD algorithm.
In Workshop on TextGraphs, at HLT-NAACL
2006, pages 89?96, New York, NY.
Agirre, Eneko and Aitor Soroa. 2007.
Semeval-2007 task 02: Evaluating word
sense induction and discrimination
systems. In Proceedings of the 4th
International Workshop on Semantic
Evaluation, pages 7?12, Prague.
Amigo, Enrique, Julio Gonzalo, Javier
Artiles, and Felisa Verdejo. 2009. A
comparison of extrinsic clustering
evaluation metrics based on formal
constraints. Information Retrieval,
12(4):461?486.
Antos, A. and I. Kontoyiannis. 2001.
Convergence properties of functional
estimates of discrete distributions. Random
Structures and Algorithms, 19:163?193.
Bagga, Amit and Breck Baldwin. 1998.
Algorithms for scoring coreference chains.
In The First International Conference on
Language Resources and Evaluation Workshop
on Linguistics Coreference, pages 563?566,
Granada.
Batu, Tugkan, Sanjoy Dasgupta, Ravi Kumar,
and Ronitt Rubinfeld. 2002. The complexity
of approximating entropy. In Symposium
on the Theory of Computing (STOC),
pages 678?687, Montreal.
Dom, Byron E. 2001. An information-
theoretic external cluster-validity measure.
Technical Report No. RJ10219, IBM.
Grasberger, P. and T. Schu?rmann. 1996.
Entropy estimation of symbol sequences.
CHAOS, 6(3):414?427.
Kilgarriff, Adam. 2004. How dominant
is the commonest sense of a word? In
Sojka, Kopecek, and Pala, editors, Text,
Speech, Dialogue, volume 3206 of Lecture
Notes in Artificial Intelligence. Springer,
pages 103?112.
Klapaftis, Ioannis P. and Suresh Manandhar.
2013. Evaluating word sense induction
and disambiguation methods. Language
Resources and Evaluation, 47(3):1?27.
Luo, Xiaoqiang. 2005. On coreference
resolution performance metrics. In
Proceedings of the Conference on Human
Language Technology and Empirical Methods
in Natural Language Processing (HLT-05),
pages 25?32, Vancouver.
Manandhar, Suresh and Ioannis Klapaftis.
2009. Semeval-2010 task 14: Evaluation
setting for word sense induction &
disambiguation systems. In Proceedings of
the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions
(SEW-2009), pages 117?122, Boulder, CO.
Manandhar, Suresh, Ioannis Klapaftis,
Dmitriy Dligach, and Sameer Pradhan.
2010. Semeval-2010 task 14: Word sense
induction & disambiguation. In Proceedings
of the 5th International Workshop on Semantic
Evaluation, pages 63?68, Uppsala.
Meila, Marina. 2007. Comparing
clusterings?an information based
distance. Journal of Multivariate Analysis,
98:873?895.
Miller, G. 1955. Note on the bias of
information estimates. Information Theory
in Psychology II-B, pages 95?100.
Paninski, Liam. 2003. Estimation of entropy
and mutual information. Neural
Computation, 15:1,191?1,253.
684
Li, Titov, and Sporleder Improved Estimation of Entropy for Word Sense Induction
Paninski, Liam. 2004. Estimating entropy
of m bins given fewer than m samples.
IEEE Transactions on Information Theory,
50(9):2,200?2,203.
Purandare, Amruta and Ted Pedersen. 2004.
Word sense discrimination by clustering
contexts in vector and similarity spaces.
In Proceedings of the CoNLL, pages 41?48,
Boston, MA.
Quenouille, M. 1956. Notes on bias and
estimation. Biometrika, 43:353?360.
Rosenberg, Andrew and Julia Hirschberg.
2007. V-measure: A conditional entropy-
based external cluster evaluation
measure. In Proceedings of the 2007
EMNLP-CoNll Joint Conference,
pages 410?420, Prague.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?123.
Steele, J. Michael. 1986. An Efron-Stein
inequality for nonsymmetric statistics.
Annals of Statistics, 14:753?758.
Strehl, Alexander and Joydeep Gosh. 2002.
Cluster ensembles: A knowledge reuse
framework for combining multiple
partitions. Journal of Machine Learning
Research, 3:583?617.
Strong, S., R. Koberle, S. R. van de Ruyter,
and W. Bialek. 1998. Entropy and
information in neural spike trains.
Physical Review Letters, 80:197?202.
Tukey, J. 1958. Bias and confidence in not
quite large samples. Annals of Mathematical
Statistics, 29:614.
Wang, Y. H. 1993. On the number of
successes in independent trials. Statistica
Sinica 3, 2:295?312.
Witte, Ian and Eibe Frank. 2005. Data
Mining: Practical Machine Learning Tools
and Techniques. Morgan Kaufmann,
Amsterdam.
Zhao, Y. and G. Karypis. 2005. Hierarchical
clustering algorithms for document
datasets. Data Mining and Knowledge
Discovery, 10(2):141?168.
685

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 297?300,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Gaussian Mixture Models to Detect Figurative Language in Context
Linlin Li and Caroline Sporleder
Saarland University, Postfach 15 11 50
66041 Saarbru?cken, Germany
{linlin, csporled}@coli.uni-saarland.de
Abstract
We present a Gaussian Mixture model for de-
tecting different types of figurative language
in context. We show that this model performs
well when the parameters are estimated in an
unsupervised fashion using EM. Performance
can be improved further by estimating the pa-
rameters from a small annotated data set.
1 Introduction
Figurative language employs words in a way that de-
viates from their normal meaning. It includes id-
iomatic usage, metaphor, metonymy or other types
of creative language. Being able to detect figurative
language is important for a number of NLP applica-
tions, e.g., machine translation.
Simply checking the input against an idiom dic-
tionary does not solve the problem. While some
expressions (e.g., trip the light fantastic) are al-
ways used idiomatically, many expressions (e.g.,
spill the beans), can take on a literal meaning as
well. Whether such expression is used idiomatically
or not has to be inferred from the discourse context.
Likewise, simple dictionary look-up would not work
for truly creative, one-off usages; these can neither
be found in a dictionary nor can they be detected
by standard idiom extraction methods, which apply
statistical measures to accumulated corpus evidence
for an expression to assess its ?idiomaticity?. An ex-
ample of a fairly creative usage can be found in (1),
which is a variation of the idiom put a sock in.
(1) Take the sock out of your mouth and create a
brand-new relationship with your mom.
We propose a method for detecting figurative lan-
guage in context. Because we use context informa-
tion rather than corpus statistics, our approach works
also for truly creative usages.
2 Related Work
Most studies on the detection of idioms and other
types of figurative language focus on one of three
aspects: type-based extraction (detect idioms on the
type level), token-based classification (given a po-
tentially idiomatic phrase in context, decide whether
it is used idiomatically), token-based detection (de-
tect figurative expressions in running text).
Type-based extractions exploit the fact that idioms
have many properties which differentiate them from
other expressions, e.g., they often exhibit a degree of
syntactic and lexical fixedness. These properties can
be used to identify potential idioms, for instance, by
employing measures of association strength between
the elements of an expression (Lin, 1999).
Type-based approaches are unsuitable for expres-
sions which can be used both figuratively and lit-
erally. These have to be disambiguated in context.
Token-based classification aims to do this. A num-
ber of token-based approaches have been proposed:
supervised (Katz and Giesbrecht, 2006), weakly su-
pervised (Birke and Sarkar, 2006), and unsupervised
(Fazly et al, 2009; Sporleder and Li, 2009).
Finally, token-based detection can be viewed
as a two stage task which is the combination of
type-based extraction and token-based classifica-
tion. There has been relatively little work on this so
far. One exception are Fazly et al (2009) who detect
idiom types by using statistical methods that model
the general idiomaticity of an expression and then
combine this with a simple second-stage process that
detects whether the target expression is used figura-
tively in a given context, based on whether the ex-
pression occurs in canonical form or not.
However, modeling token-based detection as a
297
combination of type-based extraction and token-
based classification has some drawbacks. First,
type-based approaches typically compute statistics
from multiple occurrences of a target expression,
hence they cannot be applied to novel usages. Sec-
ond, these methods were developed to detect figu-
ratively used multi-word expressions (MWEs) and
do not work for figuratively used individual words,
like sparrow in example (2). Ideally, one would like
to have a generic model that can detect any type of
figurative usage in a given context. The model we
propose in this paper is one step in this direction.
(2) During the Iraq war, he was a sparrow; he
didn?t condone the bloodshed but wasn?t both-
ered enough to go out and protest.
3 Using Gaussian Mixture Model to Detect
Figurative Language
We address the problem by using Gaussian Mix-
ture Models (GMMs). We assume that the literal
(l) and non-literal (n) data are generated by two dif-
ferent Gaussians (literal and nonliteral Gaussian).
The token-based detection task is done by compar-
ing which Gaussian has the higher probability of
generating a specific instance.
The Gaussian mixture model is defined as:
p(x) =
?
c?{l,n}
wc ?N(x|?c,?c)
Where, c is the category of the Gaussian, ?c is the
mean, ?c is the covariance matrix, and wc is the
Gaussian weight.
Our method is based on the insight that figura-
tive language exhibits less semantic cohesive ties
with the context than literal language (Sporleder and
Li, 2009). We use Normalized Google Distance to
model semantic relatedness (Cilibrasi and Vitanyi,
2007) and represent the data by five types of seman-
tic relatedness features x = (x1, x2, x3, x4, x5):
x1 is the average relatedness between the target
expression and context words,
x1 =
2
|T | ? |C|
?
(wi,cj)?T?C
relatedness(wi, cj)
where wi is a component word of the target expres-
sion (T); cj is one of the context words (C); |T | is
the total number of words in the target expression,
and |C| is the total number of words in the context.
The term 2|T |?|C| is the normalization factor, which
is the total number of relatedness pairs between tar-
get component words and context words.
x2 is the average semantic relatedness in the con-
text of the target expression,
x2 =
1
(
|C|
2
)
?
(ci,cj)?C?C,i6=j
relatedness(ci, cj)
x3 is the difference between the average seman-
tic relatedness between the target expression and the
context words and the average semantic relatedness
of the context (i.e., x3 = x1? x2). It is an indicator
of how strongly the target expression is semantically
related to the discourse context.
x4 is the feature used by Sporleder and Li (2009)
for predicting literal or idiomatic use in the cohesion
graph based method,
x4 =
{
1 if x3 < 0
0 else
x5 is a high dimensional vector which represents
the top relatedness scores between the component
words of the target expression and the context,
x5(k) = max
(wi,cj)?T?C
(k, {relatedness(wi, cj)})
where the function max(k,A) is defined to choose
the kth highest element from the set A.1
The detection task is done by a Bayes decision
rule, which chooses the category by maximizing the
probability of fitting the data into the different Gaus-
sian components:
c(x) = arg max
i?{l,n}
{wi ?N(x|?i,?i)}
4 Evaluating the GMM Approach
4.1 Data
We evaluate our method on two data sets. The
first set (idiom set) is taken from Sporleder and Li
(2009) and consists of 3964 idiom occurrences (17
idiom types) which were manually labeled as ?lit-
eral? or ?figurative?. The second data set (V+NP
set), consists of a randomly selected sample of
500 V+NP constructions from the Gigaword corpus,
which were manually labeled.
To determine how well our model deals with dif-
ferent types of figurative usage, we distinguish four
phenomena: Phrase-level figurative means that the
1We set k to be 100 in our experiment.
298
whole phrase is used figuratively. We further divide
this class into expressions which are potentially am-
biguous between literal and figurative usage (nsa),
e.g., spill the beans, and those that are unambigu-
ously figurative irrespective of the context (nsu),
e.g., trip the light fantastic. The latter can, theoreti-
cally, be detected by dictionary look-up, the former
cannot. The label token-level figurative (nw) is used
when part of the phrase is used figuratively (e.g.,
sparrow in (2)). Often it is difficult to determine
whether a word is still used in a ?literal? sense or
whether it is already used figuratively. Since we are
interested in improving the performance of NLP ap-
plications such as MT, we take a pragmatic approach
and classify usages as ?figurative? if they are not lex-
icalized, i.e., if the specific sense is not listed in a
dictionary.2 For example, we would classify summit
in the ?meeting? sense as ?literal? (l). In our data set,
7.3% of the instances were annotated as ?nsa?, 1.9%
as ?nsu?, 9.2% as ?nw? and 81.5% as ?l?. A randomly
selected sample (100 instances) was annotated inde-
pendently by a second annotator. The kappa score
(Cohen, 1960) is 0.84, which suggest that the anno-
tations are reliable.
4.2 GMM Estimated by EM
We used the MatLab package provided by Cali-
non (2009) for estimating the GMM model. The
GMM is trained by the EM algorithm. The pri-
ors of Gaussian components, means and covariance
of each components, are initialized by the k-means
clustering algorithm (Hartigan, 1975).
To determine whether the GMM is able to per-
form token-based idiom classification, we applied
it to the idiom data set. The results (see Table 1)
show that the GMM can distinguish usages quite
well and gains equally good results as Sporleder and
Li?s cohesion graph method (Co-Graph). In addi-
tion, this method can deal with unobserved occur-
rences of non-literal language.
Table 2 shows the results on the second data set.
The baseline predicts ?idiomatic? and ?literal? ac-
cording to a biased probability which is based on the
true distribution in the annotated set. GMM shows
the performance on the whole V+NP set. We also
split the test set into three different subsets to de-
2We used http://www.askoxford.com.
Model C Pre. Rec. F-S. Acc.
Co-Graph
n 90.55 80.66 85.32
78.38
l 50.04 69.72 58.26
GMM
n 90.69 80.66 85.38
78.39
l 50.17 70.15 58.50
Table 1: Results on the idiom data set, n(on-literal) is the
union of the predefined three sub-classes (nsu, nsa, nw),
l(iteral), Acc(uracy), Pre(cision), Rec(all), F-S(core)
Model C Pre. Rec. F-S. Acc.
Baseline
n 21.79 22.67 22.22
71.87
l 83.19 82.47 82.83
Co-Graph
n 37.29 84.62 51.76
70.92
l 95.12 67.83 79.19
GMM
n 40.71 73.08 52.29
75.41
l 92.58 75.94 83.44
GMM{nsu,l}
n 8.79 1.00 16.16
76.49
l 1.00 75.94 86.33
GMM{nsa,l}
n 22.43 77.42 34.78
76.06
l 97.40 75.94 85.34
GMM{nw,l}
n 23.15 64.10 34.01
74.74
l 94.93 75.94 84.38
Table 2: Results on the V+NP data set, Gaussian compo-
nent parameters estimated by EM
termine how the GMM performs on distinguishing
literal usage from the different types of figurative us-
age: GMM{nsu, l}, GMM{nsa, l}, GMM{nw, l}.
The unsupervised GMM model beats the base-
line and achieves good results on the V+NP data set.
It also outperforms the Co-Graph approach, which
suggests that the statistical model, GMM, is more
likely to boost the performance by capturing statisti-
cal properties of the data for more difficult cases (id-
ioms vs. general figurative usages), compared with
the Co-Graph approach.
In conclusion, the model is not only able to clas-
sify idiomatic expressions but also to detect new fig-
urative expressions. However, the performance on
the second data set is worse compared with run-
ning the same model on the idiom data set. This
is because the V+NP data set contains more diffi-
cult examples, e.g., expressions which are only par-
tially figurative (e.g., (2)). One would expect the
literal part of the expression to exhibit cohesive ties
with the context, hence the cohesion based features
may fail to detect this type of figurative usage. Con-
sequently the performance of the GMM is lower
for figuratively used words (?nw?) than for idioms
(?nsa?, ?nsu?). However, even for ?nw? cases the
model still obtains a relatively high accuracy.
299
4.3 GMM estimated from Annotated Data
In a second experiment, we tested how well the
GMM performs when utilizing the annotated idiom
data set to estimate the two Gaussian components in-
stead of using EM. We give equal weights to the two
Gaussian components and predict the label on the
V+NP data set by fixing the mixture model which
is estimated from the training set (GMM+f). This
method further improves the performance compared
to the unsupervised approach (Table 3).
We also experimented with setting a threshold and
abstaining from making a prediction when the prob-
ability of an instance belonging to the Gaussian is
below the threshold (GMM+f+s). Table 3 shows
the performance when only evaluating on the subset
for which a classification was made. It can be seen
that the accuracy and the overall performance on the
literal class improve, but the precision for the non-
literal class remains relatively low, i.e., many literal
instances are still misclassified as ?non-literal?. One
reason for this may be that there are a few instances
containing named entities, which exhibit weak co-
hesive ties with the context even if though they are
used literally. Using a named-entity tagger before
applying the GMM might solve the problem.
Model C Pre. Rec. F-S. Acc.
GMM+f
n 42.22 73.08 53.52
76.60
l 92.71 77.39 84.36
GMM+f+s
n 41.38 54.55 47.06
83.44
l 92.54 87.94 90.18
Table 3: Results on the V+NP data set, Gaussian compo-
nent parameters estimated by annotated data
Finally, Table 4 shows the result when using dif-
ferent idioms to generate the nonliteral Gaussian.
The literal Gaussian can be generated from the au-
tomatically obtained nonliteral examples by Li and
Sporleder (2009). We found the estimation of the
GMM is not sensitive to idioms; our model is robust
and can use any existing idiom data to discover new
figurative expressions. Furthermore, Table 4 also
shows that the GMM does not need a large amount
of annotated data for parameter estimation. A few
hundred instances are sufficient.
5 Conclusion
We described a GMM based approach for detecting
figurative expressions. This method not only works
Train (size) C Pre. Rec. F-S. Acc.
bite one?s tongue n 40.79 79.49 53.91
74.94
(166) l 94.10 73.91 82.79
break the ice n 39.05 52.56 44.81
76.12
(541) l 88.36 81.45 84.77
Table 4: Results on the V+NP dataset, Gaussian compo-
nent parameters estimated on different idioms
for distinguishing literal and non-literal usages of a
potential idiomatic expression in a discourse con-
text, but also discovers new figurative expressions.
The components of the GMM can be effectively
estimated using the EM algorithm. The performance
can be further improved when employing an anno-
tated data set for parameter estimation. Our results
show that the estimation of Gaussian components
are not idiom-dependent. Furthermore, a small an-
notated data set is enough to obtain good results.
Acknowledgments
This work was funded by the DFG within the Cluster
of Excellence ?Multimodal Computing and Interaction?.
Thanks to Benjamin Roth for discussions and comments.
References
J. Birke, A. Sarkar. 2006. A clustering approach for
the nearly unsupervised recognition of nonliteral lan-
guage. In Proceedings of EACL-06.
S. Calinon. 2009. Robot Programming by Demonstra-
tion: A Probabilistic Approach. EPFL/CRC Press.
R. L. Cilibrasi, P. M. B. Vitanyi. 2007. The Google simi-
larity distance. IEEE Trans. on Knowl. and Data Eng.,
19(3):370?383.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurements,
20:37?46.
A. Fazly, P. Cook, S. Stevenson. 2009. Unsupervised
type and token identification of idiomatic expressions.
Computational Linguistics, 35(1):61?103.
J. A. Hartigan. 1975. Clustering Algorithm. Wiley.
G. Katz, E. Giesbrecht. 2006. Automatic identification
of non-compositional multi-word expressions using la-
tent semantic analysis. In Proceedings of the ACL06
Workshop on Multiword Expressions: Identifying and
Exploiting Underlying Properties.
L. Li, C. Sporleder. 2009. Contextual idiom detection
without labelled data. In Proceedings of EMNLP-09.
D. Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-99.
C. Sporleder, L. Li. 2009. Unsupervised recognition of
literal and non-literal use of idiomatic expressions. In
Proceedings of EACL-09.
300
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1138?1147,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Topic Models for Word Sense Disambiguation and
Token-based Idiom Detection
Linlin Li, Benjamin Roth, and Caroline Sporleder
Saarland University, Postfach 15 11 50
66041 Saarbru?cken, Germany
{linlin, beroth, csporled}@coli.uni-saarland.de
Abstract
This paper presents a probabilistic model
for sense disambiguation which chooses
the best sense based on the conditional
probability of sense paraphrases given a
context. We use a topic model to decom-
pose this conditional probability into two
conditional probabilities with latent vari-
ables. We propose three different instanti-
ations of the model for solving sense dis-
ambiguation problems with different de-
grees of resource availability. The pro-
posed models are tested on three different
tasks: coarse-grained word sense disam-
biguation, fine-grained word sense disam-
biguation, and detection of literal vs. non-
literal usages of potentially idiomatic ex-
pressions. In all three cases, we outper-
form state-of-the-art systems either quan-
titatively or statistically significantly.
1 Introduction
Word sense disambiguation (WSD) is the task of
automatically determining the correct sense for a
target word given the context in which it occurs.
WSD is an important problem in NLP and an es-
sential preprocessing step for many applications,
including machine translation, question answering
and information extraction. However, WSD is a
difficult task, and despite the fact that it has been
the focus of much research over the years, state-
of-the-art systems are still often not good enough
for real-world applications. One major factor that
makes WSD difficult is a relative lack of manu-
ally annotated corpora, which hampers the perfor-
mance of supervised systems.
To address this problem, there has been a
significant amount of work on unsupervised
WSD that does not require manually sense-
disambiguated training data (see McCarthy (2009)
for an overview). Recently, several researchers
have experimented with topic models (Brody and
Lapata, 2009; Boyd-Graber et al, 2007; Boyd-
Graber and Blei, 2007; Cai et al, 2007) for sense
disambiguation and induction. Topic models are
generative probabilistic models of text corpora in
which each document is modelled as a mixture
over (latent) topics, which are in turn represented
by a distribution over words.
Previous approaches using topic models for
sense disambiguation either embed topic features
in a supervised model (Cai et al, 2007) or rely
heavily on the structure of hierarchical lexicons
such as WordNet (Boyd-Graber et al, 2007). In
this paper, we propose a novel framework which
is fairly resource-poor in that it requires only 1)
a large unlabelled corpus from which to estimate
the topics distributions, and 2) paraphrases for the
possible target senses. The paraphrases can be
user-supplied or can be taken from existing re-
sources.
We approach the sense disambiguation task by
choosing the best sense based on the conditional
probability of sense paraphrases given a context.
We propose three models which are suitable for
different situations: Model I requires knowledge
of the prior distribution over senses and directly
maximizes the conditional probability of a sense
given the context; Model II maximizes this condi-
tional probability by maximizing the cosine value
of two topic-document vectors (one for the sense
and one for the context). We apply these models
to coarse- and fine-grained WSD and find that they
outperform comparable systems for both tasks.
We also test our framework on the related task
of idiom detection, which involves distinguishing
literal and nonliteral usages of potentially ambigu-
ous expressions such as rock the boat. For this
task, we propose a third model. Model III cal-
culates the probability of a sense given a context
according to the component words of the sense
1138
paraphrase. Specifically, it chooses the sense type
which maximizes the probability (given the con-
text) of the paraphrase component word with the
highest likelihood of occurring in that context.
This model also outperforms state-of-the-art sys-
tems.
2 Related Work
There is a large body of work on WSD, cover-
ing supervised, unsupervised (word sense induc-
tion) and knowledge-based approaches (see Mc-
Carthy (2009) for an overview). While most su-
pervised approaches treat the task as a classifica-
tion task and use hand-labelled corpora as train-
ing data, most unsupervised systems automatically
group word tokens into similar groups using clus-
tering algorithms, and then assign labels to each
sense cluster. Knowledge-based approaches ex-
ploit information contained in existing resources.
They can be combined with supervised machine-
learning models to assemble semi-supervised ap-
proaches.
Recently, a number of systems have been pro-
posed that make use of topic models for sense
disambiguation. Cai et al (2007), for example,
use LDA to capture global context. They com-
pute topic models from a large unlabelled corpus
and include them as features in a supervised sys-
tem. Boyd-Graber and Blei (2007) propose an un-
supervised approach that integrates McCarthy et
al.?s (2004) method for finding predominant word
senses into a topic modelling framework. In ad-
dition to generating a topic from the document?s
topic distribution and sampling a word from that
topic, the enhanced model also generates a distri-
butional neighbour for the chosen word and then
assigns a sense based on the word, its neighbour
and the topic. Boyd-Graber and Blei (2007) test
their method on WSD and information retrieval
tasks and find that it can lead to modest improve-
ments over state-of-the-art results.
In another unsupervised system, Boyd-Graber
et al (2007) enhance the basic LDA algorithm by
incorporating WordNet senses as an additional la-
tent variable. Instead of generating words directly
from a topic, each topic is associated with a ran-
dom walk through the WordNet hierarchy which
generates the observed word. Topics and synsets
are then inferred together. While Boyd-Graber
et al (2007) show that this method can lead to
improvements in accuracy, they also find that id-
iosyncracies in the hierarchical structure of Word-
Net can harm performance. This is a general prob-
lem for methods which use hierarchical lexicons
to model semantic distance (Budanitsky and Hirst,
2006). In our approach, we circumvent this prob-
lem by exploiting paraphrase information for the
target senses rather than relying on the structure
of WordNet as a whole.
Topic models have also been applied to the re-
lated task of word sense induction. Brody and
Lapata (2009) propose a method that integrates a
number of different linguistic features into a single
generative model.
Topic models have been previously consid-
ered for metaphor extraction and estimating the
frequency of metaphors (Klebanov et al, 2009;
Bethard et al, 2009). However, we have a differ-
ent focus in this paper, which aims to distinguish
literal and nonliteral usages of potential idiomatic
expressions. A number of methods have been ap-
plied to this task. Katz and Giesbrecht (2006)
devise a supervised method in which they com-
pute the meaning vectors for the literal and non-
literal usages of a given expression in the trainning
data. Birke and Sarkar (2006) use a clustering al-
gorithm which compares test instances to two au-
tomatically constructed seed sets (one literal and
one nonliteral), assigning the label of the closest
set. An unsupervised method that computes co-
hesive links between the component words of the
target expression and its context have been pro-
posed (Sporleder and Li, 2009; Li and Sporleder,
2009). Their system predicts literal usages when
strong links can be found.
3 The Sense Disambiguation Model
3.1 Topic Model
As pointed out by Hofmann (1999), the starting
point of topic models is to decompose the con-
ditional word-document probability distribution
p(w|d) into two different distributions: the word-
topic distribution p(w|z), and the topic-document
distribution p(z|d) (see Equation 1). This allows
each semantic topic z to be represented as a multi-
nominal distribution of words p(w|z), and each
document d to be represented as a multinominal
distribution of semantic topics p(z|d). The model
introduces a conditional independence assumption
that document d and word w are independent con-
1139
ditioned on the hidden variable, topic z.
p(w|d) =
?
z
p(z|d)p(w|z) (1)
LDA is a Bayesian version of this framework with
Dirichlet hyper-parameters (Blei et al, 2003).
The inference of the two distributions given an
observed corpus can be done through Gibbs Sam-
pling (Geman and Geman, 1987; Griffiths and
Steyvers, 2004). For each turn of the sampling,
each word in each document is assigned a seman-
tic topic based on the current word-topic distribu-
tion and topic-document distribution. The result-
ing topic assignments are then used to re-estimate
a new word-topic distribution and topic-document
distribution for the next turn. This process re-
peats until convergence. To avoid statistical co-
incidence, the final estimation of the distributions
is made by the average of all the turns after con-
vergence.
3.2 The Sense Disambiguation Model
Assigning the correct sense s to a target word w
occurring in a context c involves finding the sense
which maximizes the conditional probability of
senses given a context:
s = argmax
si
p(si|c) (2)
In our model, we represent a sense (si) as a col-
lection of ?paraphrases? that capture (some aspect
of) the meaning of the sense. These paraphrases
can be taken from an existing resource such as
WordNet (Miller, 1995) or supplied by the user
(see Section 4).
This conditional probability is decomposed by
incorporating a hidden variable, topic z, intro-
duced by the topic model. We propose three varia-
tions of the basic model, depending on how much
background information is available, i.e., knowl-
edge of the prior sense distribution available and
type of sense paraphrases used. In Model I and
Model II, the sense paraphrases are obtained from
WordNet, and both the context and the sense para-
phrases are treated as documents, c = dc and
s = ds.
WordNet is a fairly rich resource which pro-
vides detailed information about word senses
(glosses, example sentences, synsets, semantic re-
lations between senses, etc.). Sometimes such de-
tailed information may not be available, for in-
stance for languages for which such a resource
does not exist or for expressions that are not
very well covered in WordNet, such as idioms.
For those situations, we propose another model,
Model III, in which contexts are treated as docu-
ments while sense paraphrases are treated as se-
quences of independent words.1
Model I directly maximizes the conditional
probability of the sense given the context, where
the sense is modeled as a ?paraphrase document?
ds and the context as a ?context document? dc.
The conditional probability of sense given context
p(ds|dc) can be rewritten as a joint probability di-
vided by a normalization factor:
p(ds|dc) =
p(ds, dc)
p(dc)
(3)
This joint probability can be rewritten as a gen-
erative process by introducing a hidden variable z.
We make the conditional independence assump-
tion that, conditioned on the topic z, a paraphrase
document ds is generated independently of the
specific context document dc:
p(ds, dc) =
?
z
p(ds)p(z|ds)p(dc|z) (4)
We apply the same process to the conditional
probability p(dc|z). It can be rewritten as:
p(dc|z) =
p(dc)p(z|dc)
p(z)
(5)
Now, the disambiguation model p(ds|dc) can be
rewritten as a prior p(ds) times a topic function
f(z):
p(ds|dc) = p(ds)
?
z
p(z|dc)p(z|ds)
p(z)
(6)
As p(z) is a uniform distribution according to
the uniform Dirichlet priors assumption, Equation
6 can be rewritten as:
p(ds|dc) ? p(ds)
?
z
p(z|dc)p(z|ds) (7)
Model I:
argmax
dsi
p(dsi)
?
z
p(z|dc)p(z|dsi) (8)
Model I has the disadvantage that it requires
information about the prior distribution of senses
1The idea is that these key words capture the meaning of
the idioms.
1140
p(ds), which is not always available. We use sense
frequency information from WordNet to estimate
the prior sense distribution, although it must be
kept in mind that, depending on the genre of the
texts, it is possible that the distribution of senses
in the testing corpus may diverge greatly from the
WordNet-based estimation. If there is no means
for estimating the prior sense distribution of an
experimental corpus, generally a uniform distri-
bution must be assumed. However, this assump-
tion does not hold, as the true distribution of word
senses is often highly skewed (McCarthy, 2009).
To overcome this problem, we propose Model
II, which indirectly maximizes the sense-context
probability by maximizing the cosine value of two
document vectors that encode the document-topic
frequencies from sampling, v(z|dc) and v(z|ds).
The document vectors are represented by topics,
with each dimension representing the number of
times that the tokens in this document are assigned
to a certain topic.
Model II:
argmax
dsi
cos(v(z|dc), v(z|dsi)) (9)
If the prior distribution of senses is known, Model
I is the best choice. However, Model II has to be
chosen instead when this knowledge is not avail-
able. In our experiments, we test the performance
of both models (see Section 5).
If the sense paraphrases are very short, it is diffi-
cult to reliably estimate p(z|ds). In order to solve
this problem, we treat the sense paraphrase ds as
a ?query?, a concept which is used in information
retrieval. One model from information retrieval
takes the conditional probability of the query given
the document as a product of all the conditional
probabilities of words in the query given the doc-
ument. The assumption is that the query is gener-
ated by a collection of conditionally independent
words (Song and Croft, 1999).
We make the same assumption here. How-
ever, instead of taking the product of all the condi-
tional probabilities of words given the document,
we take the maximum. There are two reasons for
this: (i) taking the product may penalize longer
paraphrases since the product of probabilities de-
creases as there are more words; (ii) we do not
want to model the probability of generating spe-
cific paraphrases, but rather the probability of gen-
erating a sense, which might only be represented
by one or two words in the paraphrases (e.g., the
potentially idiomatic phrase ?rock the boat? can be
paraphrased as ?break the norm? or ?cause trou-
ble?. A similar topic distribution to that of the
individual words ?norm? or ?trouble? would be
strong supporting evidence of the corresponding
idiomatic reading.). We propose Model III:
argmax
qsi
max
wi?qs
?
z
p(wi|z)p(z|dc) (10)
where qs is a collection of words contained in the
sense paraphrases.
3.3 Inference
One possible inference approach is to combine the
context documents and sense paraphrases into a
corpus and run Gibbs sampling on this corpus.
The problem with this approach is that the test set
and sense paraphrase set are relatively small, and
topic models running on a small corpus are less
likely to capture rich semantic topics. One sim-
ple explanation for this is that a small corpus usu-
ally has a relatively small vocabulary, which is less
representative of topics, i.e., p(w|z) cannot be es-
timated reliably.
In order to overcome this problem, we infer the
word-topic distribution from a very large corpus
(Wikipedia dump, see Section 4). All the follow-
ing inference experiments on the test corpus are
based on the assumption that the word-topic dis-
tribution p(w|z) is the same as the one estimated
from the Wikipedia dump. Inference of topic-
document distributions for context and sense para-
phrases is done by fixing the word-topic distribu-
tion as a constant.
4 Experimental Setup
We evaluate our models on three different tasks:
coarse-grained WSD, fine-grained WSD and lit-
eral vs. nonliteral sense detection. In this section
we discuss our experimental set-up. We start by
describing the three datasets for evaluation and an-
other dataset for probability estimation. We also
discuss how we choose sense paraphrases and in-
stance contexts.
Data We use three datasets for evaluation. The
coarse-grained task is evaluated on the Semeval-
2007 Task-07 benchmark dataset released by Nav-
igli et al (2009). The dataset consists of 5377
words of running text from five different articles:
the first three were obtained from the WSJ cor-
pus, the fourth was the Wikipedia entry for com-
puter programming, and the fifth was an excerpt of
1141
Amy Steedman?s Knights of the Art, biographies
of Italian painters. The proportion of the non news
text, the last two articles, constitutes 51.87% of the
whole testing set. It consists of 1108 nouns, 591
verbs, 362 adjectives, and 208 adverbs. The data
were annotated with coarse-grained senses which
were obtained by clustering senses from the Word-
Net 2.1 sense inventory based on the procedure
proposed by Navigli (2006).
To determine whether our model is also suitable
for fine-grained WSD, we test on the data provided
by Pradhan et al (2009) for the Semeval-2007
Task-17 (English fine-grained all-words task).
This dataset is a subset of the set from Task-07. It
comprises the three WSJ articles from Navigli et
al. (2009). A total of 465 lemmas were selected as
instances from about 3500 words of text. There are
10 instances marked as ?U? (undecided sense tag).
Of the remaining 455 instances, 159 are nouns and
296 are verbs. The sense inventory is from Word-
Net 2.1.
Finally, we test our model on the related sense
disambiguation task of distinguishing literal and
nonliteral usages of potentially ambiguous expres-
sions such as break the ice. For this, we use the
dataset from Sporleder and Li (2009) as a test set.
This dataset consists of 3964 instances of 17 po-
tential English idioms which were manually anno-
tated as literal or nonliteral.
A Wikipedia dump2 is used to estimate the
multinomial word-topic distribution. This dataset,
which consists of 320,000 articles,3 is significantly
larger than SemCor, which is the dataset used by
Boyd-Graber et al (2007). All markup from the
Wikipedia dump was stripped off using the same
filter as the ESA implementation (Sorg and Cimi-
ano, 2008), and stopwords were filtered out using
the Snowball (Porter, October 2001) stopword list.
In addition, words with a Wikipedia document fre-
quency of 1 were filtered out. The lemmatized
version of the corpus consists of 299,825 lexical
units.
The test sets were POS-tagged and lemmatized
using RASP (Briscoe and Carroll, 2006). The in-
ference processes are run on the lemmatized ver-
sion of the corpus. For the Semeval-2007 Task 17
English all-words, the organizers do not supply the
part-of-speech and lemma information of the tar-
get instances. In order to avoid the wrong predic-
2We use the English snapshot of 2009-07-13
3All articles of fewer than 100 words were discarded.
tions caused by tagging or lemmatization errors,
we manually corrected any bad tags and lemmas
for the target instances.4
Sense Paraphrases For word sense disam-
biguation tasks, the paraphrases of the sense keys
are represented by information from WordNet 2.1.
(Miller, 1995). To obtain the paraphrases, we use
the word forms, glosses and example sentences
of the synset itself and a set of selected reference
synsets (i.e., synsets linked to the target synset by
specific semantic relations, see Table 1). We ex-
cluded the ?hypernym reference synsets?, since in-
formation common to all of the child synsets may
confuse the disambiguation process.
For the literal vs. nonliteral sense detection
task, we selected the paraphrases of the nonlit-
eral meaning from several online idiom dictionar-
ies. For the literal senses, we used 2-3 manu-
ally selected words with which we tried to cap-
ture (aspects of) the literal meaning of the expres-
sion.5 For instance, the literal ?paraphrases? that
we chose for ?break the ice? were ice, water and
snow. The paraphrases are shorter for the idiom
task than for the WSD task, because the mean-
ing descriptions from the idiom dictionaries are
shorter than what we get from WordNet. In the
latter case, each sense can be represented by its
synset as well as its reference synsets.
Instance Context We experimented with differ-
ent context sizes for the disambiguation task. The
five different context settings that we used for the
WSD tasks are: collocations (1w), ?5-word win-
dow (5w), ?10-word window (10w), current sen-
tence, and whole text. Because the idiom corpus
also includes explicitly marked paragraph bound-
aries, we included ?paragraph? as a sixth type of
context size for the idiom sense detection task.
5 Experiments
As mentioned above, we test our proposed sense
disambiguation framework on three tasks. We
start by describing the sampling experiments for
4This was done by comparing the predicted sense keys
and the gold standard sense keys. We only checked instances
for which the POS-tags in the predicted sense keys are not
consistent with those in the gold standard. This was the case
for around 20 instances.
5Note that we use the word ?paraphrase? in a fairly wide
sense in this paper. Sometimes it is not possible to obtain ex-
act paraphrases. This applies especially to the task of distin-
guishing literal from nonliteral senses of multi-word expres-
sions. In this case we take as paraphrases some key words
which capture salient aspects of the meaning.
1142
POS Paraphrase reference synsets
N hyponyms, instance hyponyms, member holonyms, substance holonyms, part holonyms,
member meronyms, part meronyms, substance meronyms, attributes, topic members,
region members, usage members, topics, regions, usages
V Troponyms, entailments, outcomes, phrases, verb groups, topics, regions, usages, sentence frames
A similar, pertainym, attributes, related, topics, regions, usages
R pertainyms, topics, regions, usages
Table 1: Selected reference synsets from WordNet that were used for different parts-of-speech to obtain
word sense paraphrase. N(noun), V(verb), A(adj), R(adv).
estimating the word-topic distribution from the
Wikipedia dump. We used the package provided
by Wang et al (2009) with the suggested Dirich-
let hyper-parameters 6. In order to avoid statistical
instability, the final result is averaged over the last
50 iterations. We did four rounds of sampling with
1000, 500, 250, and 125 topics respectively. The
final word-topic distribution is a normalized con-
catenate of the four distributions estimated in each
round. In average, the sampling program run on
the Wikipedia dump consumed 20G memory, and
each round took about one week on a single AMD
Dual-Core 1000MHZ processor.
5.1 Coarse-Grained WSD
In this section we first describe the landscape of
similar systems against which we compare our
models, then present the results of the comparison.
The systems that participated in the SemEval-2007
coarse-grained WSD task (Task-07) can be di-
vided into three categories, depending on whether
training data is needed and whether other types
of background knowledge are required: What we
call Type I includes all the systems that need an-
notated training data. All the participating sys-
tems that have the mark TR fall into this cate-
gory (see Navigli et al (2009) for the evaluation
for all the participating systems). Type II con-
sists of systems that do not need training data but
require prior knowledge of the sense distribution
(estimated sense frequency). All the participating
systems that have the mark MFS belong to this cat-
egory. Systems that need neither training data nor
prior sense distribution knowledge are categorized
as Type III.
We make this distinction based on two princi-
ples: (i) the cost of building a system; (ii) the
portability of the established resource. Type III
is the cheapest system to build, while Type I and
6They were set as: ? = 50#topics and ? = 0.01.
Type II both need extra resources. Type II has
an advantage over Type I since the prior knowl-
edge of the sense distribution can be estimated
from annotated corpora (e.g.: SemCor, Senseval).
In contrast, training data in Type I may be sys-
tem specific (e.g.: different input format, different
annotation guidelines). McCarthy (2009) also ad-
dresses the issue of performance and cost by com-
paring supervised word sense disambiguation sys-
tems with unsupervised ones.
We exclude the system provided by one of
the organizers (UoR-SSI) from our categorization.
The reason is that although this system is claimed
to be unsupervised, and it performs better than
all the participating systems (including the super-
vised systems) in the SemEval-2007 shared task, it
still needs to incorporate a lot of prior knowledge,
specifically information about co-occurrences be-
tween different word senses, which was obtained
from a number of resources (SSI+LKB) includ-
ing: (i) SemCor (manually annotated); (ii) LDC-
DSO (partly manually annotated); (iii) collocation
dictionaries which are then disambiguated semi-
automatically. Even though the system is not
?trained?, it needs a lot of information which is
largely dependent on manually annotated data, so
it does not fit neatly into the categories Type II or
Type III either.
Table 2 lists the best participating systems of
each type in the SemEval-2007 task (Type I:
NUS-PT (Chan et al, 2007); Type II: UPV-WSD
(Buscaldi and Rosso, 2007); Type III: TKB-UO
(Anaya-Sa?nchez et al, 2007)). Our Model I be-
longs to Type II, and our Model II belongs to Type
III.
Table 2 compares the performance of our mod-
els with the Semeval-2007 participating systems.
We only compare the F-score, since all the com-
pared systems have an attempted rate7 of 1.0,
7Attempted rate is defined as the total number of disam-
biguated output instances divided by the total number of input
1143
which makes both the precision and recall rates the
same as the F-score. We focus on comparisons be-
tween our models and the best SemEval-2007 par-
ticipating systems within the same type. Model I is
compared with UPV-WSD, and Model II is com-
pared with TKB-UO. In addition, we also compare
our system with the most frequent sense baseline
which was not outperformed by any of the systems
of Type II and Type III in the SemEval-2007 task.
Comparison on Type III is marked with ?, while
comparison on Type II is marked with ?. We find
that Model II performs statistically significantly
better than the best participating system of the
same type TKB-UO (p<<0.01, ?2 test). When
encoded with the prior knowledge of sense distri-
bution, Model I outperforms by 1.36% the best
Type II system UPV-WSD, although the differ-
ence is not statistically significant. Furthermore,
Model I also quantitatively outperforms the most
frequent sense baseline BLmfs, which, as men-
tioned above, was not beat by any participating
systems that do not use training data.
We also find that our model works best for
nouns. The unsupervised Type III model Model
II achieves better results than the most frequent
sense baseline on nouns, but not on other parts-
of-speech. This is in line with results obtained
by previous systems (Griffiths et al, 2005; Boyd-
Graber and Blei, 2008; Cai et al, 2007). While the
performance on verbs can be increased to outper-
form the most frequent sense baseline by including
the prior sense probability, the performance on ad-
jectives and adverbs remains below the most fre-
quent sense baseline. We think that there are three
reasons for this: first, adjectives and adverbs have
fewer reference synsets for paraphrases compared
with nouns and verbs (see Table 1); second, adjec-
tives and adverbs tend to convey less key semantic
content in the document, so they are more difficult
to capture by the topic model; and third, adjectives
and adverbs are a small portion of the test set, so
their performances are statistically unstable. For
example, if ?already? appears 10 times out of 20
adverb instances, a system may get bad result on
adverbs only because of its failure to disambiguate
the word ?already?.
Paraphrase analysis Table 2 also shows the
effect of different ways of choosing sense para-
phrases. MII+ref is the result of including the ref-
erence synsets, while MII-ref excludes the refer-
instances.
System Noun Verb Adj Adv All
UoR-SSI 84.12 78.34 85.36 88.46 83.21
NUS-PT 82.31 78.51 85.64 89.42 82.50
UPV-WSD 79.33 72.76 84.53 81.52 78.63?
TKB-UO 70.76 62.61 78.73 74.04 70.21?
MII?ref 78.16 70.39 79.56 81.25 76.64
MII+ref 80.05 70.73 82.04 82.21 78.14?
MI+ref 79.96 75.47 83.98 86.06 79.99?
BLmfs 77.44 75.30 84.25 87.50 78.99?
Table 2: Model performance (F-score) on the
coarse-grained dataset (context=sentence). Para-
phrases with/without reference synsets (+ref/-ref).
Context Ate. Pre. Rec. F1
?1w 91.67 75.05 68.80 71.79
?5w 99.29 77.14 76.60 76.87
?10w 100 77.92 77.92 77.92
text 100 76.86 76.86 76.86
sent. 100 78.14 78.14 78.14
Table 3: Model II performance on different con-
text size. attempted rate (Ate.), precision (Pre.),
recall (Rec.), F-score (F1).
ence synsets. As can be seen from the table, in-
cluding all reference synsets in sense paraphrases
increases performance. Longer paraphrases con-
tain more information, and they are statistically
more stable for inference.
We find that nouns get the greatest perfor-
mance boost from including reference synsets, as
they have the largest number of different types of
synsets. We also find the ?similar? reference synset
for adjectives to be very useful. Performance on
adjectives increases by 2.75% when including this
reference synset.
Context analysis In order to study how the con-
text influences the performance, we experiment
with Model II on different context sizes (see Ta-
ble 3). We find sentence context is the best size for
this disambiguation task. Using a smaller context
not only reduces the precision, but also reduces the
recall rate, which is caused by the all-zero topic as-
signment by the topic model for documents only
containing words that are not in the vocabulary.
As a result, the model is unable to disambiguate.
The context based on the whole text (article) does
not perform well either, possibly because using the
full text folds in too much noisy information.
1144
System F-score
RACAI 52.7 ?4.5
BLmfs 55.91?4.5
MI+ref 56.99?4.5
Table 4: Model performance (F-score) for the fine-
grained word sense disambiguation task.
5.2 Fine-grained WSD
We saw in the previous section that our frame-
work performs well on coarse-grained WSD. Fine-
grained WSD, however, is a more difficult task. To
determine whether our framework is also able to
detect subtler sense distinctions, we tested Model I
on the English all-words subtask of SemEval-2007
Task-17 (see Table 4).
We find that Model I performs better than both
the best unsupervised system, RACAI (Ion and
Tufis?, 2007) and the most frequent sense baseline
(BLmfs), although these differences are not sta-
tistically significant due to the small size of the
available test data (465).
5.3 Idiom Sense Disambiguation
In the previous section, we provided the results
of applying our framework to coarse- and fine-
grained word sense disambiguation tasks. For
both tasks, our models outperform the state-of-
the-art systems of the same type either quantita-
tively or statistically significantly. In this section,
we apply Model III to another sense disambigua-
tion task, namely distinguishing literal and nonlit-
eral senses of ambiguous expressions.
WordNet has a relatively low coverage for id-
iomatic expressions. In order to represent non-
literal senses, we replace the paraphrases obtained
automatically from WordNet by words selected
manually from online idiom dictionaries (for the
nonliteral sense) and by linguistic introspection
(for the literal sense). We then compare the topic
distributions of literal and nonliteral senses.
As the paraphrases obtained from the idiom dic-
tionary are very short, we treat the paraphrase
as a sequence of independent words instead of
as a document and apply Model III (see Sec-
tion 3). Table 5 shows the results of our pro-
posed model compared with state-of-the-art sys-
tems. We find that the system significantly out-
performs the majority baseline (p<<0.01, ?2 test)
and the cohesion-graph based approach proposed
by Sporleder and Li (2009) (p<<0.01, ?2 test).
The system also outperforms the bootstrapping
System Precl Recl Fl Acc.
Basemaj - - - 78.25
co-graph 50.04 69.72 58.26 78.38
boot. 71.86 66.36 69.00 87.03
Model III 67.05 81.07 73.40 87.24
Table 5: Performance on the literal or nonliteral
sense disambiguation task on idioms. literal pre-
cision (Precl), literal recall (Recl), literal F-score
(Fl), accuracy(Acc.).
system by Li and Sporleder (2009), although not
statistically significantly. This shows how a lim-
ited amount of human knowledge (e.g., para-
phrases) can be added to an unsupervised system
for a strong boost in performance ( Model III com-
pared with the cohesion-graph and the bootstrap-
ping approaches).
For obvious reasons, this approach is sensitive
to the quality of the paraphrases. The paraphrases
chosen to characterise (aspects of) the meaning of
a sense should be non-ambiguous between the lit-
eral or idiomatic meaning. For instance, ?fire? is
not a good choice for a paraphrase of the literal
reading of ?play with fire?, since this word can
be interpreted literally as ?fire? or metaphorically
as ?something dangerous?. The verb component
word ?play? is a better literal paraphrase.
For the same reason, this approach works well
for expressions where the literal and nonliteral
readings are well separated (i.e., occur in different
contexts), while the performance drops for expres-
sions whose literal and idiomatic readings can ap-
pear in a similar context. We test the performance
on individual idioms on the five most frequent id-
ioms in our corpus8 (see Table 6). We find that
?drop the ball? is a difficult case. The words ?fault?,
?mistake?, ?fail? or ?miss? can be used as the nonlit-
eral paraphrases. However, it is also highly likely
that these words are used to describe a scenario in
a baseball game, in which ?drop the ball? is used
literally. In contrast, the performance on ?rock the
boat? is much better, since the nonliteral reading
of the phrases ?break the norm? or ?cause trouble?
are less likely to be linked with the literal reading
?boat?. This may also be because ?boat? is not of-
ten used metaphorically in the corpus.
As the topic distribution of nouns and verbs
exhibit different properties, topic comparisons
across parts-of-speech do not make sense. We
8We tested only on the most frequent idioms in order to
avoid statistically unreliable observations.
1145
Idiom Acc.
drop the ball 75.86
play with fire 91.17
break the ice 87.43
rock the boat 95.82
set in stone 89.39
Table 6: Performance on individual idioms.
make the topic distributions comparable by mak-
ing sure each type of paraphrase contains the same
sets of parts-of-speech. For instance, we do not
permit combinations of literal paraphrases which
only consist of nouns and nonliteral paraphrases
which only consist of verbs.
6 Conclusion
We propose three models for sense disambigua-
tion on words and multi-word expressions. The
basic idea of these models is to compare the topic
distribution of a target instance with the candidate
sense paraphrases and choose the most probable
one. While Model I and Model III model the
problem in a probabilistic way, Model II uses a
vector space model by comparing the cosine val-
ues of two topic vectors. Model II and Model III
are completely unsupervised, while Model I needs
the prior sense distribution. Model I and Model
II treat the sense paraphrases as documents, while
Model III treats the sense paraphrases as a collec-
tion of independent words.
We test the proposed models on three tasks. We
apply Model I and Model II to the WSD tasks due
to the availability of more paraphrase information.
Model III is applied to the idiom detection task
since the paraphrases from the idiom dictionary
are smaller. We find that all models outperform
comparable state-of-the-art systems either quanti-
tatively or statistically significantly.
By testing our framework on three different
sense disambiguation tasks, we show that the
framework can be used flexibly in different ap-
plication tasks. The system also points out a
promising way of solving the granularity problem
of word sense disambiguation, as new application
tasks which need different sense granularities can
utilize this framework when new paraphrases of
sense clusters are available. In addition, this sys-
tem can also be used in a larger context such as
figurative language identification (literal or figu-
rative) and sentiment detection (positive or nega-
tive).
Acknowledgments
This work was funded by the DFG within the
Cluster of Excellence ?Multimodal Computing
and Interaction?.
References
H. Anaya-Sa?nchez, A. Pons-Porrata, R. Berlanga-
Llavori. 2007. TKB-UO: using sense clustering for
WSD. In SemEval ?07: Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, 322?
325.
S. Bethard, V. T. Lai, J. H. Martin. 2009. Topic model
analysis of metaphor frequency for psycholinguistic
stimuli. In CALC ?09: Proceedings of the Workshop
on Computational Approaches to Linguistic Creativ-
ity, 9?16, Morristown, NJ, USA. Association for
Computational Linguistics.
J. Birke, A. Sarkar. 2006. A clustering approach
for the nearly unsupervised recognition of nonliteral
language. In Proceedings of EACL-06.
D. M. Blei, A. Y. Ng, M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Reseach, 3:993?1022.
J. Boyd-Graber, D. Blei. 2007. PUTOP: turning
predominant senses into a topic model for word
sense disambiguation. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), 277?281.
J. Boyd-Graber, D. Blei. 2008. Syntactic topic models.
Computational Linguistics.
J. Boyd-Graber, D. Blei, X. Zhu. 2007. A topic
model for word sense disambiguation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), 1024?1033.
T. Briscoe, J. Carroll. 2006. Evaluating the accuracy
of an unlexicalized statistical parser on the PARC
DepBank. In Proceedings of the COLING/ACL on
Main conference poster sessions, 41?48.
S. Brody, M. Lapata. 2009. Bayesian word sense
induction. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
103?111.
A. Budanitsky, G. Hirst. 2006. Evaluating wordnet-
based measures of lexical semantic relatedness.
Computational Linguistics, 32(1):13?47.
D. Buscaldi, P. Rosso. 2007. UPV-WSD: Combining
different WSD methods by means of Fuzzy Borda
Voting. In SemEval ?07: Proceedings of the 4th
International Workshop on Semantic Evaluations,
434?437.
J. Cai, W. S. Lee, Y. W. Teh. 2007. Improving word
sense disambiguation using topic features. In Pro-
ceedings of the 2007 Joint Conference on Empirical
1146
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), 1015?1023.
Y. S. Chan, H. T. Ng, Z. Zhong. 2007. NUS-PT: ex-
ploiting parallel texts for word sense disambiguation
in the English all-words tasks. In SemEval ?07: Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, 253?256.
S. Geman, D. Geman. 1987. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration
of images. In Readings in computer vision: is-
sues, problems, principles, and paradigms, 564?
584. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
T. L. Griffiths, M. Steyvers. 2004. Finding scientific
topics. Proceedings of the National Academy of Sci-
ences, 101(Suppl. 1):5228?5235.
T. L. Griffiths, M. Steyvers, D. M. Blei, J. B. Tenen-
baum. 2005. Integrating topics and syntax. In In
Advances in Neural Information Processing Systems
17, 537?544. MIT Press.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In SIGIR ?99: Proceedings of the 22nd an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
50?57.
R. Ion, D. Tufis?. 2007. Racai: meaning affinity mod-
els. In SemEval ?07: Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, 282?
287, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
G. Katz, E. Giesbrecht. 2006. Automatic identifi-
cation of non-compositional multi-word expressions
using latent semantic analysis. In Proceedings of the
ACL/COLING-06 Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties.
B. B. Klebanov, E. Beigman, D. Diermeier. 2009. Dis-
course topics and metaphors. In CALC ?09: Pro-
ceedings of the Workshop on Computational Ap-
proaches to Linguistic Creativity, 1?8, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
L. Li, C. Sporleder. 2009. Contextual idiom detection
without labelled data. In Proceedings of EMNLP-
09.
D. McCarthy, R. Koeling, J. Weeds, J. Carroll. 2004.
Finding predominant word senses in untagged text.
In Proceedings of the 42nd Meeting of the Associa-
tion for Computational Linguistics (ACL?04), Main
Volume, 279?286.
D. McCarthy. 2009. Word sense disambiguation:
An overview. Language and Linguistics Compass,
3(2):537?558.
G. A. Miller. 1995. WordNet: a lexical database for
english. Commun. ACM, 38(11):39?41.
R. Navigli, K. C. Litkowski, O. Hargraves. 2009.
SemEval-2007 Task 07: Coarse-grained English all-
words task. In Proceedings of the 4th International
Workshop on Semantic Evaluation (SemEval-2007).
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In Proceedings of the 44th Annual Meeting
of the Association for Computational Liguistics joint
with the 21st International Conference on Computa-
tional Liguistics (COLING-ACL 2006).
M. Porter. October 2001. Snowball: A lan-
guage for stemming algorithms. http:
//snowball.tartarus.org/texts/
introduction.html.
S. S. Pradhan, E. Loper, D. Dligach, M. Palmer. 2009.
SemEval-2007 Task 07: Coarse-grained English all-
words task. In Proceedings of the 4th International
Workshop on Semantic Evaluation (SemEval-2007).
F. Song, W. B. Croft. 1999. A general language model
for information retrieval (poster abstract). In Re-
search and Development in Information Retrieval,
279?280.
P. Sorg, P. Cimiano. 2008. Cross-lingual information
retrieval with explicit semantic analysis. In In Work-
ing Notes for the CLEF 2008 Workshop.
C. Sporleder, L. Li. 2009. Unsupervised recognition of
literal and non-literal use of idiomatic expressions.
In Proceedings of EACL-09.
Y. Wang, H. Bai, M. Stanton, W.-Y. Chen, E. Y. Chang.
2009. Plda: Parallel latent dirichlet alocation for
large-scale applications. In Proc. of 5th Interna-
tional Conference on Algorithmic Aspects in Infor-
mation and Management. Software available at
http://code.google.com/p/plda.
1147
