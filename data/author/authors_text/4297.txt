Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 141?150, Prague, June 2007. c?2007 Association for Computational Linguistics
Structured Prediction Models via the Matrix-Tree Theorem
Terry Koo, Amir Globerson, Xavier Carreras and Michael Collins
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,gamir,carreras,mcollins}@csail.mit.edu
Abstract
This paper provides an algorithmic frame-
work for learning statistical models involv-
ing directed spanning trees, or equivalently
non-projective dependency structures. We
show how partition functions and marginals
for directed spanning trees can be computed
by an adaptation of Kirchhoff?s Matrix-Tree
Theorem. To demonstrate an application of
the method, we perform experiments which
use the algorithm in training both log-linear
and max-margin dependency parsers. The
new training methods give improvements in
accuracy over perceptron-trained models.
1 Introduction
Learning with structured data typically involves
searching or summing over a set with an exponen-
tial number of structured elements, for example the
set of all parse trees for a given sentence. Meth-
ods for summing over such structures include the
inside-outside algorithm for probabilistic context-
free grammars (Baker, 1979), the forward-backward
algorithm for hidden Markov models (Baum et
al., 1970), and the belief-propagation algorithm for
graphical models (Pearl, 1988). These algorithms
compute marginal probabilities and partition func-
tions, quantities which are central to many meth-
ods for the statistical modeling of complex struc-
tures (e.g., the EM algorithm (Baker, 1979; Baum
et al, 1970), contrastive estimation (Smith and Eis-
ner, 2005), training algorithms for CRFs (Lafferty et
al., 2001), and training algorithms for max-margin
models (Bartlett et al, 2004; Taskar et al, 2004a)).
This paper describes inside-outside-style algo-
rithms for the case of directed spanning trees. These
structures are equivalent to non-projective depen-
dency parses (McDonald et al, 2005b), and more
generally could be relevant to any task that involves
learning a mapping from a graph to an underlying
spanning tree. Unlike the case for projective depen-
dency structures, partition functions and marginals
for non-projective trees cannot be computed using
dynamic-programming methods such as the inside-
outside algorithm. In this paper we describe how
these quantities can be computed by adapting a well-
known result in graph theory: Kirchhoff?s Matrix-
Tree Theorem (Tutte, 1984). A na??ve application of
the theorem yields O(n4) and O(n6) algorithms for
computation of the partition function and marginals,
respectively. However, our adaptation finds the par-
tition function and marginals in O(n3) time using
simple matrix determinant and inversion operations.
We demonstrate an application of the new infer-
ence algorithm to non-projective dependency pars-
ing. Specifically, we show how to implement
two popular supervised learning approaches for this
task: globally-normalized log-linear models and
max-margin models. Log-linear estimation criti-
cally depends on the calculation of partition func-
tions and marginals, which can be computed by
our algorithms. For max-margin models, Bartlett
et al (2004) have provided a simple training al-
gorithm, based on exponentiated-gradient (EG) up-
dates, that requires computation of marginals and
can thus be implemented within our framework.
Both of these methods explicitly minimize the loss
incurred when parsing the entire training set. This
contrasts with the online learning algorithms used in
previous work with spanning-tree models (McDon-
ald et al, 2005b).
We applied the above two marginal-based train-
ing algorithms to six languages with varying de-
grees of non-projectivity, using datasets obtained
from the CoNLL-X shared task (Buchholz and
Marsi, 2006). Our experimental framework com-
pared three training approaches: log-linear models,
max-margin models, and the averaged perceptron.
Each of these was applied to both projective and
non-projective parsing. Our results demonstrate that
marginal-based training yields models which out-
141
perform those trained using the averaged perceptron.
In summary, the contributions of this paper are:
1. We introduce algorithms for inside-outside-
style calculations for directed spanning trees, or
equivalently non-projective dependency struc-
tures. These algorithms should have wide
applicability in learning problems involving
spanning-tree structures.
2. We illustrate the utility of these algorithms in
log-linear training of dependency parsing mod-
els, and show improvements in accuracy when
compared to averaged-perceptron training.
3. We also train max-margin models for depen-
dency parsing via an EG algorithm (Bartlett
et al, 2004). The experiments presented here
constitute the first application of this algorithm
to a large-scale problem. We again show im-
proved performance over the perceptron.
The goal of our experiments is to give a rigorous
comparative study of the marginal-based training al-
gorithms and a highly-competitive baseline, the av-
eraged perceptron, using the same feature sets for
all approaches. We stress, however, that the purpose
of this work is not to give competitive performance
on the CoNLL data sets; this would require further
engineering of the approach.
Similar adaptations of the Matrix-Tree Theorem
have been developed independently and simultane-
ously by Smith and Smith (2007) andMcDonald and
Satta (2007); see Section 5 for more discussion.
2 Background
2.1 Discriminative Dependency Parsing
Dependency parsing is the task of mapping a sen-
tence x to a dependency structure y. Given a sen-
tence x with n words, a dependency for that sen-
tence is a tuple (h,m) where h ? [0 . . . n] is the
index of the head word in the sentence, and m ?
[1 . . . n] is the index of a modifier word. The value
h = 0 is a special root-symbol that may only ap-
pear as the head of a dependency. We use D(x) to
refer to all possible dependencies for a sentence x:
D(x) = {(h,m) : h ? [0 . . . n],m ? [1 . . . n]}.
A dependency parse is a set of dependencies
that forms a directed tree, with the sentence?s root-
symbol as its root. We will consider both projective
Projective Non-projective
Single
Root 1 30 2Heroot saw her 1 30 2Heroot saw her
Multi
Root 1 30 2Heroot saw her 1 30 2Heroot saw her
Figure 1: Examples of the four types of dependency struc-
tures. We draw dependency arcs from head to modifier.
trees, where dependencies are not allowed to cross,
and non-projective trees, where crossing dependen-
cies are allowed. Dependency annotations for some
languages, for example Czech, can exhibit a signifi-
cant number of crossing dependencies. In addition,
we consider both single-root and multi-root trees. In
a single-root tree y, the root-symbol has exactly one
child, while in a multi-root tree, the root-symbol has
one or more children. This distinction is relevant
as our training sets include both single-root corpora
(in which all trees are single-root structures) and
multi-root corpora (in which some trees are multi-
root structures).
The two distinctions described above are orthog-
onal, yielding four classes of dependency structures;
see Figure 1 for examples of each kind of structure.
We use T sp (x) to denote the set of all possible pro-
jective single-root dependency structures for a sen-
tence x, and T snp(x) to denote the set of single-root
non-projective structures for x. The sets T mp (x) and
T mnp (x) are defined analogously for multi-root struc-
tures. In contexts where any class of dependency
structures may be used, we use the notation T (x) as
a placeholder that may be defined as T sp (x), T
s
np(x),
T mp (x) or T
m
np (x).
Following McDonald et al (2005a), we use a dis-
criminative model for dependency parsing. Fea-
tures in the model are defined through a function
f(x, h,m) which maps a sentence x together with
a dependency (h,m) to a feature vector in Rd. A
feature vector can be sensitive to any properties of
the triple (x, h,m). Given a parameter vector w,
the optimal dependency structure for a sentence x is
y?(x;w) = argmax
y?T (x)
?
(h,m)?y
w ? f(x, h,m) (1)
where the set T (x) can be defined as T sp (x), T
s
np(x),
T mp (x) or T
m
np (x), depending on the type of parsing.
142
The parameters w will be learned from a train-
ing set {(xi, yi)}Ni=1 where each xi is a sentence and
each yi is a dependency structure. Much of the pre-
vious work on learningw has focused on training lo-
cal models (see Section 5). McDonald et al (2005a;
2005b) trained global models using online algo-
rithms such as the perceptron algorithm or MIRA.
In this paper we consider training algorithms based
on work in conditional random fields (CRFs) (Laf-
ferty et al, 2001) and max-margin methods (Taskar
et al, 2004a).
2.2 Three Inference Problems
This section highlights three inference problems
which arise in training and decoding discriminative
dependency parsers, and which are central to the ap-
proaches described in this paper.
Assume that we have a vector ? with values
?h,m ? R for all (h,m) ? D(x); these values cor-
respond to weights on the different dependencies in
D(x). Define a conditional distribution over all de-
pendency structures y ? T (x) as follows:
P (y |x;?) =
exp
{?
(h,m)?y ?h,m
}
Z(x;?)
(2)
Z(x;?) =
?
y?T (x)
exp
?
?
?
?
(h,m)?y
?h,m
?
?
?
(3)
The function Z(x;?) is commonly referred to as the
partition function.
Given the distribution P (y |x;?), we can define
the marginal probability of a dependency (h,m) as
?h,m(x;?) =
?
y?T (x) : (h,m)?y
P (y |x;?)
The inference problems are then as follows:
Problem 1: Decoding:
Find argmaxy?T (x)
?
(h,m)?y ?h,m
Problem 2: Computation of the Partition Func-
tion: Calculate Z(x;?).
Problem 3: Computation of the Marginals:
For all (h,m) ? D(x), calculate ?h,m(x;?).
Note that all three problems require a maximiza-
tion or summation over the set T (x), which is ex-
ponential in size. There is a clear motivation for
being able to solve Problem 1: by setting ?h,m =
w ? f(x, h,m), the optimal dependency structure
y?(x;w) (see Eq. 1) can be computed. In this paper
the motivation for solving Problems 2 and 3 arises
from training algorithms for discriminative models.
As we will describe in Section 4, both log-linear and
max-margin models can be trained via methods that
make direct use of algorithms for Problems 2 and 3.
In the case of projective dependency structures
(i.e., T (x) defined as T sp (x) or T
m
p (x)), there are
well-known algorithms for all three inference prob-
lems. Decoding can be carried out using Viterbi-
style dynamic-programming algorithms, for exam-
ple the O(n3) algorithm of Eisner (1996). Com-
putation of the marginals and partition function can
also be achieved in O(n3) time, using a variant of
the inside-outside algorithm (Baker, 1979) applied
to the Eisner (1996) data structures (Paskin, 2001).
In the non-projective case (i.e., T (x) defined as
T snp(x) or T
m
np (x)), McDonald et al (2005b) de-
scribe how the CLE algorithm (Chu and Liu, 1965;
Edmonds, 1967) can be used for decoding. How-
ever, it is not possible to compute the marginals
and partition function using the inside-outside algo-
rithm. We next describe a method for computing
these quantities in O(n3) time using matrix inverse
and determinant operations.
3 Spanning-tree inference using the
Matrix-Tree Theorem
In this section we present algorithms for computing
the partition function and marginals, as defined in
Section 2.2, for non-projective parsing. We first re-
iterate the observation of McDonald et al (2005a)
that non-projective parses correspond to directed
spanning trees on a complete directed graph of n
nodes, where n is the length of the sentence. The
above inference problems thus involve summation
over the set of all directed spanning trees. Note that
this set is exponentially large, and there is no obvi-
ous method for decomposing the sum into dynamic-
programming-like subproblems. This section de-
scribes how a variant of Kirchhoff?s Matrix-Tree
Theorem (Tutte, 1984) can be used to evaluate the
partition function and marginals efficiently.
In what follows, we consider the single-root set-
ting (i.e., T (x) = T snp(x)), leaving the multi-root
143
case (i.e., T (x) = T mnp (x)) to Section 3.3. For a
sentence x with n words, define a complete directed
graph G on n nodes, where each node corresponds
to a word in x, and each edge corresponds to a de-
pendency between two words in x. Note thatG does
not include the root-symbol h = 0, nor does it ac-
count for any dependencies (0,m) headed by the
root-symbol. We assign non-negative weights to the
edges of this graph, yielding the following weighted
adjacency matrix A(?) ? Rn?n, for h,m = 1 . . . n:
Ah,m(?) =
{
0, if h = m
exp {?h,m} , otherwise
To account for the dependencies (0,m) headed by
the root-symbol, we define a vector of root-selection
scores r(?) ? Rn, form = 1 . . . n:
rm(?) = exp {?0,m}
Let the weight of a dependency structure y ? T snp(x)
be defined as:
?(y;?) = rroot(y)(?)
?
(h,m)?y : h 6=0
Ah,m(?)
Here, root(y) = m : (0,m) ? y is the child of the
root-symbol; there is exactly one such child, since
y ? T snp(x). Eq. 2 and 3 can be rephrased as:
P (y |x;?) =
?(y;?)
Z(x;?)
(4)
Z(x;?) =
?
y?T snp(x)
?(y;?) (5)
In the remainder of this section, we drop the nota-
tional dependence on x for brevity.
The original Matrix-Tree Theorem addressed the
problem of counting the number of undirected span-
ning trees in an undirected graph. For the models
we study here, we require a sum of weighted and
directed spanning trees. Tutte (1984) extended the
Matrix-Tree Theorem to this case. We briefly sum-
marize his method below.
First, define the Laplacian matrix L(?) ? Rn?n
of G, for h,m = 1 . . . n:
Lh,m(?) =
{ ?n
h?=1Ah?,m(?) if h = m
?Ah,m(?) otherwise
Second, for a matrix X , let X(h,m) be the minor of
X with respect to row h and column m; i.e., the
determinant of the matrix formed by deleting row h
and columnm fromX . Finally, define the weight of
any directed spanning tree of G to be the product of
the weights Ah,m(?) for the edges in that tree.
Theorem 1 (Tutte, 1984, p. 140). Let L(?) be the
Laplacian matrix of G. Then L(m,m)(?) is equal to
the sum of the weights of all directed spanning trees
of G which are rooted at m. Furthermore, the mi-
nors vary only in sign when traversing the columns
of the Laplacian (Tutte, 1984, p. 150):
?h,m : (?1)h+mL(h,m)(?) = L(m,m)(?) (6)
3.1 Partition functions via matrix determinants
From Theorem 1, it directly follows that
L(m,m)(?) =
?
y?U(m)
?
(h,m)?y : h 6=0
Ah,m(?)
where U(m) = {y ? T snp : root(y) = m}. A
na??ve method for computing the partition function is
therefore to evaluate
Z(?) =
n?
m=1
rm(?)L(m,m)(?)
The above would require calculating n determinants,
resulting in O(n4) complexity. However, as we
show below Z(?) may be obtained in O(n3) time
using a single determinant evaluation.
Define a newmatrix L?(?) to beL(?) with the first
row replaced by the root-selection scores:
L?h,m(?) =
{
rm(?) h = 1
Lh,m(?) h > 1
This matrix allows direct computation of the parti-
tion function, as the following proposition shows.
Proposition 1 The partition function in Eq. 5 is
given by Z(?) = |L?(?)|.
Proof: Consider the row expansion of |L?(?)| with
respect to row 1:
|L?(?)| =
n?
m=1
(?1)1+mL?1,m(?)L?(1,m)(?)
=
n?
m=1
(?1)1+mrm(?)L(1,m)(?)
=
n?
m=1
rm(?)L(m,m)(?) = Z(?)
The second line follows from the construction of
L?(?), and the third line follows from Eq. 6.
144
3.2 Marginals via matrix inversion
The marginals we require are given by
?h,m(?) =
1
Z(?)
?
y?T snp : (h,m)?y
?(y;?)
To calculate these marginals efficiently for all values
of (h,m) we use a well-known identity relating the
log partition-function to marginals
?h,m(?) =
? logZ(?)
??h,m
Since the partition function in this case has a closed-
form expression (i.e., the determinant of a matrix
constructed from ?), the marginals can also obtained
in closed form. Using the chain rule, the derivative
of the log partition-function in Proposition 1 is
?h,m(?) =
? log |L?(?)|
??h,m
=
n?
h?=1
n?
m?=1
? log |L?(?)|
?L?h?,m?(?)
?L?h?,m?(?)
??h,m
To perform the derivative, we use the identity
? log |X|
?X
=
(
X?1
)T
and the fact that ?L?h?,m?(?)/??h,m is nonzero for
only a few h?,m?. Specifically, when h = 0, the
marginals are given by
?0,m(?) = rm(?)
[
L??1(?)
]
m,1
and for h > 0, the marginals are given by
?h,m(?) = (1 ? ?1,m)Ah,m(?)
[
L??1(?)
]
m,m
?
(1 ? ?h,1)Ah,m(?)
[
L??1(?)
]
m,h
where ?h,m is the Kronecker delta. Thus, the com-
plexity of evaluating all the relevant marginals is
dominated by the matrix inversion, and the total
complexity is therefore O(n3).
3.3 Multiple Roots
In the case of multiple roots, we can still compute
the partition function and marginals efficiently. In
fact, the derivation of this case is simpler than for
single-root structures. Create an extended graph G?
which augmentsG with a dummy root node that has
edges pointing to all of the existing nodes, weighted
by the appropriate root-selection scores. Note that
there is a bijection between directed spanning trees
ofG? rooted at the dummy root and multi-root struc-
tures y ? T mnp (x). Thus, Theorem 1 can be used to
compute the partition function directly: construct a
Laplacian matrix L(?) for G? and compute the mi-
nor L(0,0)(?). Since this minor is also a determi-
nant, the marginals can be obtained analogously to
the single-root case. More concretely, this technique
corresponds to defining the matrix L?(?) as
L?(?) = L(?) + diag(r(?))
where diag(v) is the diagonal matrix with the vector
v on its diagonal.
3.4 Labeled Trees
The techniques above extend easily to the case
where dependencies are labeled. For a model with
L different labels, it suffices to define the edge
and root scores as Ah,m(?) =
?L
`=1 exp {?h,m,`}
and rm(?) =
?L
`=1 exp {?0,m,`}. The partition
function over labeled trees is obtained by operat-
ing on these values as described previously, and
the marginals are given by an application of the
chain rule. Both inference problems are solvable in
O(n3 + Ln2) time.
4 Training Algorithms
This section describes two methods for parameter
estimation that rely explicitly on the computation of
the partition function and marginals.
4.1 Log-Linear Estimation
In conditional log-linear models (Johnson et al,
1999; Lafferty et al, 2001), a distribution over parse
trees for a sentence x is defined as follows:
P (y |x;w) =
exp
{?
(h,m)?y w ? f(x, h,m)
}
Z(x;w)
(7)
where Z(x;w) is the partition function, a sum over
T sp (x), T
s
np(x), T
m
p (x) or T
m
np (x).
We train the model using the approach described
by Sha and Pereira (2003). Assume that we have a
training set {(xi, yi)}Ni=1. The optimal parameters
145
are taken to be w? = argminw L(w) where
L(w) = ?C
N?
i=1
logP (yi |xi;w) +
1
2
||w||2
The parameterC > 0 is a constant dictating the level
of regularization in the model.
Since L(w) is a convex function, gradient de-
scent methods can be used to search for the global
minimum. Such methods typically involve repeated
computation of the loss L(w) and gradient ?L(w)?w ,
requiring efficient implementations of both func-
tions. Note that the log-probability of a parse is
logP (y |x;w) =
?
(h,m)?y
w ? f(x, h,m)? logZ(x;w)
so that the main issue in calculating the loss func-
tion L(w) is the evaluation of the partition functions
Z(xi;w). The gradient of the loss is given by
?L(w)
?w
= w ? C
N?
i=1
?
(h,m)?yi
f(xi, h,m)
+ C
N?
i=1
?
(h,m)?D(xi)
?h,m(xi;w)f(xi, h,m)
where
?h,m(x;w) =
?
y?T (x) : (h,m)?y
P (y |x;w)
is the marginal probability of a dependency (h,m).
Thus, the main issue in the evaluation of the gradient
is the computation of the marginals ?h,m(xi;w).
Note that Eq. 7 forms a special case of the log-
linear distribution defined in Eq. 2 in Section 2.2.
If we set ?h,m = w ? f(x, h,m) then we have
P (y |x;w) = P (y |x;?), Z(x;w) = Z(x;?), and
?h,m(x;w) = ?h,m(x;?). Thus in the projective
case the inside-outside algorithm can be used to cal-
culate the partition function and marginals, thereby
enabling training of a log-linear model; in the non-
projective case the algorithms in Section 3 can be
used for this purpose.
4.2 Max-Margin Estimation
The second learning algorithm we consider is
the large-margin approach for structured prediction
(Taskar et al, 2004a; Taskar et al, 2004b). Learning
in this framework again involves minimization of a
convex function L(w). Let the margin for parse tree
y on the i?th training example be defined as
mi,y(w) =
?
(h,m)?yi
w?f(xi, h,m) ?
?
(h,m)?y
w?f(xi, h,m)
The loss function is then defined as
L(w) = C
N?
i=1
max
y?T (xi)
(Ei,y ?mi,y(w)) +
1
2
||w||2
where Ei,y is a measure of the loss?or number of
errors?for parse y on the i?th training sentence. In
this paper we take Ei,y to be the number of incorrect
dependencies in the parse tree y when compared to
the gold-standard parse tree yi.
The definition of L(w) makes use of the expres-
sion maxy?T (xi) (Ei,y ?mi,y(w)) for the i?th train-
ing example, which is commonly referred to as the
hinge loss. Note that Ei,yi = 0, and also that
mi,yi(w) = 0, so that the hinge loss is always non-
negative. In addition, the hinge loss is 0 if and only
ifmi,y(w) ? Ei,y for all y ? T (xi). Thus the hinge
loss directly penalizes margins mi,y(w) which are
less than their corresponding losses Ei,y.
Figure 2 shows an algorithm for minimizing
L(w) that is based on the exponentiated-gradient al-
gorithm for large-margin optimization described by
Bartlett et al (2004). The algorithm maintains a set
of weights ?i,h,m for i = 1 . . . N, (h,m) ? D(xi),
which are updated example-by-example. The algo-
rithm relies on the repeated computation of marginal
values ?i,h,m, which are defined as follows:1
?i,h,m =
?
y?T (xi) : (h,m)?y
P (y |xi) (8)
P (y |xi) =
exp
{?
(h,m)?y ?i,h,m
}
?
y??T (xi) exp
{?
(h,m)?y? ?i,h,m
}
A similar definition is used to derive marginal val-
ues ??i,h,m from the values ?
?
i,h,m. Computation of
the ? and ?? values is again inference of the form
described in Problem 3 in Section 2.2, and can be
1Bartlett et al (2004) write P (y |xi) as ?i,y . The ?i,y vari-
ables are dual variables that appear in the dual objective func-
tion, i.e., the convex dual of L(w). Analysis of the algorithm
shows that as the ?i,h,m variables are updated, the dual vari-
ables converge to the optimal point of the dual objective, and
the parameters w converge to the minimum of L(w).
146
Inputs: Training examples {(xi, yi)}Ni=1.
Parameters: Regularization constant C, starting point ?,
number of passes over training set T .
Data Structures: Real values ?i,h,m and li,h,m for i =
1 . . . N, (h,m) ? D(xi). Learning rate ?.
Initialization: Set learning rate ? = 1C . Set ?i,h,m = ? for
(h,m) ? yi, and ?i,h,m = 0 for (h,m) /? yi. Set li,h,m = 0
for (h,m) ? yi, and li,h,m = 1 for (h,m) /? yi. Calculate
initial parameters as
w = C
?
i
?
(h,m)?D(xi)
?i,h,mf(xi, h,m)
where ?i,h,m = (1? li,h,m ??i,h,m) and the ?i,h,m values
are calculated from the ?i,h,m values as described in Eq. 8.
Algorithm: Repeat T passes over the training set, where
each pass is as follows:
Set obj = 0
For i = 1 . . . N
? For all (h,m) ? D(xi):
??i,h,m = ?i,h,m + ?C (li,h,m +w ? f(xi, h,m))
? For example i, calculate marginals ?i,h,m
from ?i,h,m values, and marginals ??i,h,m
from ??i,h,m values (see Eq. 8).
? Update the parameters:
w = w + C
?
(h,m)?D(xi)
?i,h,mf(xi, h,m)
where ?i,h,m = ?i,h,m ? ??i,h,m,
? For all (h,m) ? D(xi), set ?i,h,m = ??i,h,m
? Set obj = obj + C
?
(h,m)?D(xi)
li,h,m??i,h,m
Set obj = obj ? ||w||
2
2 . If obj has decreased
compared to last iteration, set ? = ?2 .
Output: Parameter values w.
Figure 2: The EG Algorithm for Max-Margin Estimation.
The learning rate ? is halved each time the dual objective func-
tion (see (Bartlett et al, 2004)) fails to increase. In our experi-
ments we chose ? = 9, which was found to work well during
development of the algorithm.
achieved using the inside-outside algorithm for pro-
jective structures, and the algorithms described in
Section 3 for non-projective structures.
5 Related Work
Global log-linear training has been used in the con-
text of PCFG parsing (Johnson, 2001). Riezler et al
(2004) explore a similar application of log-linear
models to LFG parsing. Max-margin learning
has been applied to PCFG parsing by Taskar et al
(2004b). They show that this problem has a QP
dual of polynomial size, where the dual variables
correspond to marginal probabilities of CFG rules.
A similar QP dual may be obtained for max-margin
projective dependency parsing. However, for non-
projective parsing, the dual QP would require an ex-
ponential number of constraints on the dependency
marginals (Chopra, 1989). Nevertheless, alternative
optimization methods like that of Tsochantaridis et
al. (2004), or the EGmethod presented here, can still
be applied.
The majority of previous work on dependency
parsing has focused on local (i.e., classification of
individual edges) discriminative training methods
(Yamada and Matsumoto, 2003; Nivre et al, 2004;
Y. Cheng, 2005). Non-local (i.e., classification of
entire trees) training methods were used by McDon-
ald et al (2005a), who employed online learning.
Dependency parsing accuracy can be improved
by allowing second-order features, which consider
more than one dependency simultaneously. McDon-
ald and Pereira (2006) define a second-order depen-
dency parsing model in which interactions between
adjacent siblings are allowed, and Carreras (2007)
defines a second-order model that allows grandpar-
ent and sibling interactions. Both authors give poly-
time algorithms for exact projective parsing. By
adapting the inside-outside algorithm to these mod-
els, partition functions and marginals can be com-
puted for second-order projective structures, allow-
ing log-linear and max-margin training to be ap-
plied via the framework developed in this paper.
For higher-order non-projective parsing, however,
computational complexity results (McDonald and
Pereira, 2006; McDonald and Satta, 2007) indicate
that exact solutions to the three inference problems
of Section 2.2 will be intractable. Exploration of ap-
proximate second-order non-projective inference is
a natural avenue for future research.
Two other groups of authors have independently
and simultaneously proposed adaptations of the
Matrix-Tree Theorem for structured inference on di-
rected spanning trees (McDonald and Satta, 2007;
Smith and Smith, 2007). There are some algorithmic
differences between these papers and ours. First, we
define both multi-root and single-root algorithms,
whereas the other papers only consider multi-root
147
parsing. This distinction can be important as one
often expects a dependency structure to have ex-
actly one child attached to the root-symbol, as is the
case in a single-root structure. Second, McDonald
and Satta (2007) propose an O(n5) algorithm for
computing the marginals, as opposed to the O(n3)
matrix-inversion approach used by Smith and Smith
(2007) and ourselves.
In addition to the algorithmic differences, both
groups of authors consider applications of the
Matrix-Tree Theorem which we have not discussed.
For example, both papers propose minimum-risk
decoding, and McDonald and Satta (2007) dis-
cuss unsupervised learning and language model-
ing, while Smith and Smith (2007) define hidden-
variable models based on spanning trees.
In this paper we used EG training methods only
for max-margin models (Bartlett et al, 2004). How-
ever, Globerson et al (2007) have recently shown
how EG updates can be applied to efficient training
of log-linear models.
6 Experiments on Dependency Parsing
In this section, we present experimental results
applying our inference algorithms for dependency
parsing models. Our primary purpose is to estab-
lish comparisons along two relevant dimensions:
projective training vs. non-projective training, and
marginal-based training algorithms vs. the averaged
perceptron. The feature representation and other rel-
evant dimensions are kept fixed in the experiments.
6.1 Data Sets and Features
We used data from the CoNLL-X shared task
on multilingual dependency parsing (Buchholz and
Marsi, 2006). In our experiments, we used a subset
consisting of six languages; Table 1 gives details of
the data sets used.2 For each language we created
a validation set that was a subset of the CoNLL-X
2Our subset includes the two languages with the lowest ac-
curacy in the CoNLL-X evaluations (Turkish and Arabic), the
language with the highest accuracy (Japanese), the most non-
projective language (Dutch), a moderately non-projective lan-
guage (Slovene), and a highly projective language (Spanish).
All languages but Spanish have multi-root parses in their data.
We are grateful to the providers of the treebanks that constituted
the data of our experiments (Hajic? et al, 2004; van der Beek et
al., 2002; Kawata and Bartels, 2000; Dz?eroski et al, 2006; Civit
and Mart??, 2002; Oflazer et al, 2003).
language %cd train val. test
Arabic 0.34 49,064 5,315 5,373
Dutch 4.93 178,861 16,208 5,585
Japanese 0.70 141,966 9,495 5,711
Slovene 1.59 22,949 5,801 6,390
Spanish 0.06 78,310 11,024 5,694
Turkish 1.26 51,827 5,683 7,547
Table 1: Information for the languages in our experiments.
The 2nd column (%cd) is the percentage of crossing dependen-
cies in the training and validation sets. The last three columns
report the size in tokens of the training, validation and test sets.
training set for that language. The remainder of each
training set was used to train the models for the dif-
ferent languages. The validation sets were used to
tune the meta-parameters (e.g., the value of the reg-
ularization constantC) of the different training algo-
rithms. We used the official test sets and evaluation
script from the CoNLL-X task. All of the results that
we report are for unlabeled dependency parsing.3
The non-projective models were trained on the
CoNLL-X data in its original form. Since the pro-
jective models assume that the dependencies in the
data are non-crossing, we created a second train-
ing set for each language where non-projective de-
pendency structures were automatically transformed
into projective structures. All projective models
were trained on these new training sets.4 Our feature
space is based on that of McDonald et al (2005a).5
6.2 Results
We performed experiments using three training al-
gorithms: the averaged perceptron (Collins, 2002),
log-linear training (via conjugate gradient descent),
and max-margin training (via the EG algorithm).
Each of these algorithms was trained using pro-
jective and non-projective methods, yielding six
training settings per language. The different
training algorithms have various meta-parameters,
which we optimized on the validation set for
each language/training-setting combination. The
3Our algorithms also support labeled parsing (see Section
3.4). Initial experiments with labeled models showed the same
trend that we report here for unlabeled parsing, so for simplicity
we conducted extensive experiments only for unlabeled parsing.
4The transformations were performed by running the pro-
jective parser with score +1 on correct dependencies and -1 oth-
erwise: the resulting trees are guaranteed to be projective and to
have a minimum loss with respect to the correct tree. Note that
only the training sets were transformed.
5It should be noted that McDonald et al (2006) use a richer
feature set that is incomparable to our features.
148
Perceptron Max-Margin Log-Linear
p np p np p np
Ara 71.74 71.84 71.74 72.99 73.11 73.67
Dut 77.17 78.83 76.53 79.69 76.23 79.55
Jap 91.90 91.78 92.10 92.18 91.68 91.49
Slo 78.02 78.66 79.78 80.10 78.24 79.66
Spa 81.19 80.02 81.71 81.93 81.75 81.57
Tur 71.22 71.70 72.83 72.02 72.26 72.62
Table 2: Test data results. The p and np columns show results
with projective and non-projective training respectively.
Ara Dut Jap Slo Spa Tur AV
P 71.74 78.83 91.78 78.66 81.19 71.70 79.05
E 72.99 79.69 92.18 80.10 81.93 72.02 79.82
L 73.67 79.55 91.49 79.66 81.57 72.26 79.71
Table 3: Results for the three training algorithms on the differ-
ent languages (P = perceptron, E = EG, L = log-linear models).
AV is an average across the results for the different languages.
averaged perceptron has a single meta-parameter,
namely the number of iterations over the training set.
The log-linear models have two meta-parameters:
the regularization constant C and the number of
gradient steps T taken by the conjugate-gradient
optimizer. The EG approach also has two meta-
parameters: the regularization constant C and the
number of iterations, T .6 For models trained using
non-projective algorithms, both projective and non-
projective parsing was tested on the validation set,
and the highest scoring of these two approaches was
then used to decode test data sentences.
Table 2 reports test results for the six training sce-
narios. These results show that for Dutch, which is
the language in our data that has the highest num-
ber of crossing dependencies, non-projective train-
ing gives significant gains over projective training
for all three training methods. For the other lan-
guages, non-projective training gives similar or even
improved performance over projective training.
Table 3 gives an additional set of results, which
were calculated as follows. For each of the three
training methods, we used the validation set results
to choose between projective and non-projective
training. This allows us to make a direct com-
parison of the three training algorithms. Table 3
6We trained the perceptron for 100 iterations, and chose the
iteration which led to the best score on the validation set. Note
that in all of our experiments, the best perceptron results were
actually obtained with 30 or fewer iterations. For the log-linear
and EG algorithms we tested a number of values for C, and for
each value of C ran 100 gradient steps or EG iterations, finally
choosing the best combination of C and T found in validation.
shows the results of this comparison.7 The results
show that log-linear and max-margin models both
give a higher average accuracy than the perceptron.
For some languages (e.g., Japanese), the differences
from the perceptron are small; however for other
languages (e.g., Arabic, Dutch or Slovene) the im-
provements seen are quite substantial.
7 Conclusions
This paper describes inference algorithms for
spanning-tree distributions, focusing on the funda-
mental problems of computing partition functions
and marginals. Although we concentrate on log-
linear and max-margin estimation, the inference al-
gorithms we present can serve as black-boxes in
many other statistical modeling techniques.
Our experiments suggest that marginal-based
training produces more accurate models than per-
ceptron learning. Notably, this is the first large-scale
application of the EG algorithm, and shows that it is
a promising approach for structured learning.
In line with McDonald et al (2005b), we confirm
that spanning-tree models are well-suited to depen-
dency parsing, especially for highly non-projective
languages such as Dutch. Moreover, spanning-tree
models should be useful for a variety of other prob-
lems involving structured data.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their constructive comments. In addi-
tion, the authors gratefully acknowledge the follow-
ing sources of support. Terry Koo was funded by
a grant from the NSF (DMS-0434222) and a grant
from NTT, Agmt. Dtd. 6/21/1998. Amir Glober-
son was supported by a fellowship from the Roth-
schild Foundation - Yad Hanadiv. Xavier Carreras
was supported by the Catalan Ministry of Innova-
tion, Universities and Enterprise, and a grant from
NTT, Agmt. Dtd. 6/21/1998. Michael Collins was
funded by NSF grants 0347631 and DMS-0434222.
7We ran the sign test at the sentence level to measure the
statistical significance of the results aggregated across the six
languages. Out of 2,472 sentences total, log-linear models gave
improved parses over the perceptron on 448 sentences, and
worse parses on 343 sentences. The max-margin method gave
improved/worse parses for 500/383 sentences. Both results are
significant with p ? 0.001.
149
References
J. Baker. 1979. Trainable grammars for speech recognition. In
97th meeting of the Acoustical Society of America.
P. Bartlett, M. Collins, B. Taskar, and D. McAllester. 2004. Ex-
ponentiated gradient algorithms for large?margin structured
classification. In NIPS.
L.E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970. A max-
imization technique occurring in the statistical analysis of
probabilistic functions of markov chains. Annals of Mathe-
matical Statistics, 41:164?171.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In Proc. CoNLL-X.
X. Carreras. 2007. Experiments with a higher-order projective
dependency parser. In Proc. EMNLP-CoNLL.
S. Chopra. 1989. On the spanning tree polyhedron. Oper. Res.
Lett., pages 25?29.
Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a
directed graph. Science Sinica, 14:1396?1400.
M. Civit and Ma A. Mart??. 2002. Design principles for a Span-
ish treebank. In Proc. of the First Workshop on Treebanks
and Linguistic Theories (TLT).
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In Proc. EMNLP.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Z?abokrtsky, and
A. Z?ele. 2006. Towards a Slovene dependency treebank. In
Proc. of the Fifth Intern. Conf. on Language Resources and
Evaluation (LREC).
J. Edmonds. 1967. Optimum branchings. Journal of Research
of the National Bureau of Standards, 71B:233?240.
J. Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proc. COLING.
A. Globerson, T. Koo, X. Carreras, and M. Collins. 2007. Ex-
ponentiated gradient algorithms for log-linear structured pre-
diction. In Proc. ICML.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka. 2004.
Prague Arabic dependency treebank: Development in data
and tools. In Proc. of the NEMLAR Intern. Conf. on Arabic
Language Resources and Tools, pages 110?117.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999.
Estimators for stochastic unification-based grammars. In
Proc. ACL.
M. Johnson. 2001. Joint and conditional estimation of tagging
and parsing models. In Proc. ACL.
Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese
treebank in VERBMOBIL. Verbmobil-Report 240, Seminar
fu?r Sprachwissenschaft, Universita?t Tu?bingen.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditonal ran-
dom fields: Probabilistic models for segmenting and labeling
sequence data. In Proc. ICML.
R. McDonald and F. Pereira. 2006. Online learning of approx-
imate dependency parsing algorithms. In Proc. EACL.
R. McDonald and G. Satta. 2007. On the complexity of non-
projective data-driven dependency parsing. In Proc. IWPT.
R. McDonald, K. Crammer, and F. Pereira. 2005a. Online
large-margin training of dependency parsers. In Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proc. HLT-EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual
dependency parsing with a two-stage discriminative parser.
In Proc. CoNLL-X.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proc. CoNLL.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In A. Abeille?, editor, Tree-
banks: Building and Using Parsed Corpora, chapter 15.
Kluwer Academic Publishers.
M.A. Paskin. 2001. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical Report
UCB/CSD-01-1148, University of California, Berkeley.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent Sys-
tems: Networks of Plausible Inference (2nd edition). Mor-
gan Kaufmann Publishers.
S. Riezler, R. Kaplan, T. King, J. Maxwell, A. Vasserman, and
R. Crouch. 2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proc. HLT-NAACL.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL.
N.A. Smith and J. Eisner. 2005. Contrastive estimation: Train-
ing log-linear models on unlabeled data. In Proc. ACL.
D.A. Smith and N.A. Smith. 2007. Probabilistic models of
nonprojective dependency trees. In Proc. EMNLP-CoNLL.
B. Taskar, C. Guestrin, and D. Koller. 2004a. Max-margin
markov networks. In NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004b. Max-margin parsing. In Proc. EMNLP.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdependent
and structured output spaces. In Proc. ICML.
W. Tutte. 1984. Graph Theory. Addison-Wesley.
L. van der Beek, G. Bouma, R. Malouf, and G. van Noord.
2002. The Alpino dependency treebank. In Computational
Linguistics in the Netherlands (CLIN).
Y. Matsumoto Y. Cheng, M. Asahara. 2005. Machine learning-
based dependency analyzer for chinese. In Proc. ICCC.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. IWPT.
150
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 507?514, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Hidden?Variable Models for Discriminative Reranking
Terry Koo
MIT CSAIL
maestro@mit.edu
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Abstract
We describe a new method for the repre-
sentation of NLP structures within rerank-
ing approaches. We make use of a condi-
tional log?linear model, with hidden vari-
ables representing the assignment of lexi-
cal items to word clusters or word senses.
The model learns to automatically make
these assignments based on a discrimina-
tive training criterion. Training and de-
coding with the model requires summing
over an exponential number of hidden?
variable assignments: the required sum-
mations can be computed efficiently and
exactly using dynamic programming. As
a case study, we apply the model to
parse reranking. The model gives an F?
measure improvement of ? 1.25% be-
yond the base parser, and an ? 0.25%
improvement beyond the Collins (2000)
reranker. Although our experiments are
focused on parsing, the techniques de-
scribed generalize naturally to NLP struc-
tures other than parse trees.
1 Introduction
A number of recent approaches in statistical NLP
have focused on reranking algorithms. In rerank-
ing methods, a baseline model is used to generate a
set of candidate output structures for each input in
training or test data. A second model, which typi-
cally makes use of more complex features than the
baseline model, is then used to rerank the candidates
proposed by the baseline. Reranking approaches
have given improvements in accuracy on a number
of NLP problems including parsing (Collins, 2000;
Charniak and Johnson, 2005), machine translation
(Och and Ney, 2002; Shen et al, 2004), informa-
tion extraction (Collins, 2002), and natural language
generation (Walker et al, 2001).
The success of reranking approaches depends
critically on the choice of representation used by the
reranking model. Typically, each candidate struc-
ture (e.g., each parse tree in the case of parsing) is
mapped to a feature?vector representation. Previous
work has generally relied on two approaches to rep-
resentation: explicitly hand?crafted features (e.g., in
Charniak and Johnson (2005)) or features defined
through kernels (e.g., see Collins and Duffy (2002)).
This paper describes a new method for the rep-
resentation of NLP structures within reranking ap-
proaches. We build on the intuition that lexical items
in natural language often fall into word clusters (for
example, president and chairman might belong to
the same cluster) or fall into distinct word senses
(e.g., bank might have two distinct senses). Our
method involves a hidden?variable model, where
the hidden variables correspond to an assignment
of words to either clusters or word?senses. Lexical
items are automatically assigned their hidden values
using unsupervised learning within a discriminative
reranking approach.
We make use of a conditional log?linear model
for our task. Formally, hidden variables within
the log?linear model consist of global assignments,
where a global assignment entails an assignment of
every word in the sentence to some hidden cluster
or sense value. The number of such global assign-
ments grows exponentially fast with the length of
the sentence being processed. Training and decod-
ing with the model requires summing over the ex-
ponential number of possible global assignments, a
major technical challenge in our model. We show
that the required summations can be computed ef-
ficiently and exactly using dynamic?programming
methods (i.e., the belief propagation algorithm for
Markov random fields (Yedidia et al, 2003)) under
certain restrictions on features in the model.
Previous work on reranking has made heavy use
of lexical statistics, but has treated lexical items as
atoms. The motivation for our method comes from
the observation that statistics based on lexical items
are critical, but that these statistics suffer consid-
erably from problems of data sparsity and word?
507
sense polysemy. Our model has the ability to allevi-
ate data sparsity issues by learning to assign words
to word clusters, and can mitigate problems with
word?sense polysemy by learning to assign lexical
items to underlying word senses based upon con-
textual information. A critical difference between
our method and previous work on unsupervised ap-
proaches to word?clustering or word?sense discov-
ery is that our model is trained using a discriminative
criterion, where the assignment of words to clusters
or senses is driven by the reranking task in question.
As a case study, in this paper we focus on syn-
tactic parse reranking. We describe three model
types that can be captured by our approach. The
first method emulates a clustering operation, where
the aim is to place similar words (e.g., president and
chairman) into the same cluster. The second method
emulates a refinement operation, where the aim is to
recover distinct senses underlying a single word (for
example, distinct senses underlying the noun bank).
The third definition makes use of an existing ontol-
ogy (i.e., WordNet (Miller et al, 1993)). In this case
the set of possible hidden values for each word cor-
responds to possible WordNet senses for the word.
In experimental results on the Penn Wall Street
Journal treebank parsing domain, the hidden?
variable model gives an F?measure improvement of
? 1.25% beyond a baseline model (the parser de-
scribed in Collins (1999)), and gives an ? 0.25%
improvement beyond the reranking approach de-
scribed in Collins (2000). Although the experiments
in this paper are focused on parsing, the techniques
we describe generalize naturally to other NLP struc-
tures such as strings or labeled sequences. We dis-
cuss this point further in Section 6.1.
2 Related Work
Various machine?learning methods have been used
within reranking tasks, including conditional log?
linear models (Ratnaparkhi et al, 1994; Johnson et
al., 1999), boosting methods (Collins, 2000), vari-
ants of the perceptron algorithm (Collins, 2002;
Shen et al, 2004), and generalizations of support?
vector machines (Shen and Joshi, 2003). There have
been several previous approaches to parsing using
log?linear models and hidden variables. Riezler
et al (2002) describe a discriminative LFG pars-
ing model that is trained on standard (syntax only)
treebank annotations by treating each tree as a full
LFG analysis with an observed c-structure and hid-
den f -structure. Clark and Curran (2004) present an
alternative CCG parsing approach that divides each
CCG parse into a dependency structure (observed)
and a derivation (hidden). More recently, Matsuzaki
et al (2005) introduce a probabilistic CFG aug-
mented with hidden information at each nontermi-
nal, which gives their model the ability to tailor it-
self to the task at hand. The form of our model is
closely related to that of Quattoni et al (2005), who
describe a hidden?variable model for object recog-
nition in computer vision.
The approaches of Riezler et al, Clark and Cur-
ran, and Matsuzaki et al are similar to our own
work in that the hidden variables are exponential
in number and must be handled with dynamic?
programming techniques. However, they differ from
our approach in the definition of the hidden variables
(the Matsuzaki et al model is the most similar). In
addition, these three approaches don?t use rerank-
ing, so their features must be restricted to local scope
in order to allow dynamic?programming approaches
to training. Finally, these approaches use Viterbi
or other approximations during decoding, something
our model can avoid (see section 6.2).
In some instantiations, our model effectively clus-
ters words into categories. Our approach differs
from standard word clustering in that the cluster-
ing criteria is directly linked to the reranking objec-
tive, whereas previous word?clustering approaches
(e.g. Brown et al (1992) or Pereira et al (1993))
have typically leveraged distributional similarity. In
other instantiations, our model establishes word?
sense distinctions. Bikel (2000) has done previous
work on incorporating the WordNet hierarchy into
a generative parsing model; however, this approach
requires data with word?sense annotations whereas
our model deals with word?sense ambiguity through
unsupervised discriminative training.
3 The Hidden?Variable Model
In this section we describe a hidden?variable model
based on conditional log?linear models. Each sen-
tence si for i = 1 . . . n in our training data has a
set of ni candidate parse trees ti,1, . . . , ti,ni , which
are the output of an N?best baseline parser. Each
candidate parse has an associated F?measure score,
508
indicating its similarity to the gold?standard parse.
Without loss of generality, we define ti,1 to be the
parse with the highest F?measure for sentence si.
Given a candidate parse tree ti,j , the hidden?
variable model assigns a domain of hidden val-
ues to each word in the tree. For example, the
hidden?value domain for the word bank could be
{bank1, bank2, bank3} or {NN1,NN2,NN3}. De-
tailed descriptions of the domains we used are given
in Section 4.1. Formally, if ti,j spans m words then
the hidden?value domains for each word are the sets
H1(ti,j), . . . ,Hm(ti,j). A global hidden?value as-
signment, which attaches a hidden value to every
word in ti,j , is written h = (h1, . . . , hm) ? H(ti,j),
where H(ti,j) = H1(ti,j)? . . .?Hm(ti,j) is the set
of all possible global assignments for ti,j .
We define a feature?based representation ? such
that ?(ti,j ,h) ? Rd is a vector of feature occur-
rence counts that describes candidate parse ti,j with
global assignment h ? H(ti,j). We write ?k for
k = 1 . . . d to denote the kth component of the vec-
tor ?. Each component of the feature vector is the
count of some substructure within (ti,j ,h). For ex-
ample, ?12 and ?101 could be defined as follows:
?12(ti,j ,h) =
Number of times the word the
occurs with hidden value the3
and part of speech tag DT in
(ti,j ,h).
?101(ti,j ,h) =
Number of times CEO1 ap-
pears as the subject of owns2
in (ti,j ,h)
(1)
We use a parameter vector ? ? Rd to define a
log?linear distribution over candidate trees together
with global hidden?value assignments:
p(ti,j ,h | si,?) =
e?(ti,j ,h)??
?
j?,h??H(ti,j? )
e?(ti,j? ,h
?)??
By marginalizing out the global assignments, we ob-
tain a distribution over the candidate parses alone:
p(ti,j | si,?) =
?
h?H(ti,j)
p(ti,j ,h | si,?) (2)
Later in this paper we will describe how to train
the parameters of the model by minimizing the fol-
lowing loss function?which is the negative log?
likelihood of the training data?with respect to ?:
L(?) = ?
?
i
log p(ti,1 | si,?)
= ?
?
i
log
?
h?H(ti,1) p(ti,1,h | si,?)
(3)
saw with
PP(with)VP(saw)
S(saw)
a telescope
NP(telescope)
the boy
NP(boy)
The man
NP(man)
saw
man
The the
withboy telescope
a
Figure 1: A sample parse tree and its dependency tree.
3.1 Local Feature Vectors
Note that the number of possible global assignments
(i.e., |H(ti,j)|) grows exponentially fast with respect
to the number of words spanned by ti,j . This poses
a problem when training the model, or when calcu-
lating the probability of a parse tree through Eq. 2.
This section describes how to address this difficulty
by restricting features to sufficiently local scope. In
Section 3.2 we show that this restriction allows effi-
cient training and decoding of the model.
The restriction to local feature?vectors makes use
of the dependency structure underlying the parse
tree ti,j . Formally, for tree ti,j , we define the cor-
responding dependency tree D(ti,j) to be a set of
edges between words in ti,j , where (u, v) ? D(ti,j)
if and only if there is a head?modifier dependency
between words u and v. See Figure 1 for an exam-
ple dependency tree. We restrict the definition of
? in the following way1. If w, u and v are word
indices, we introduce single?variable local feature
vectors ?(ti,j , w, hw) ? Rd and pairwise local fea-
ture vectors ?(ti,j , u, v, hu, hv) ? Rd. The global
feature vector ?(ti,j ,h) is then decomposed into a
sum over the local feature vectors:
?(ti,j ,h) =
?
1?w?m
?(ti,j , w, hw) +
?
(u,v)?D(ti,j)
?(ti,j , u, v, hu, hv)
(4)
Notice that the second sum, over pairwise local
feature vectors, respects the dependency structure
D(ti,j). Section 3.2 describes how this decompo-
sition of ? leads to an efficient and exact dynamic?
programming approach that, during training, allows
us to calculate the gradient ?L?? and, during testing,
allows us to find the most probable candidate parse.
In our implementation, each dimension of the lo-
cal feature vectors is an indicator function signaling
the presence of a feature, so that a sum over local
feature vectors in a tree gives the occurrence count
1Note that the restriction on local feature vectors only con-
cerns the inclusion of hidden values; features may still observe
arbitrary structure within the underlying parse tree ti,j .
509
of features in that tree. For instance, define
?12(ti,j , w, hw) =
r
hw = the3 and tree ti,j assigns word
w to part?of?speech DT
z
?101(ti,j , u, v, hu, hv) =
s
(hu, hv) = (CEO1, owns2)
and tree ti,j places (u, v) in
a subject?verb relationship
{
where the notation JPK signifies a 0/1 indicator of
predicate P . When summed over the tree, these defi-
nitions of ?12 and ?101 yield global features ?12 and
?101 as given in the previous example (see Eq. 1).
3.2 Training the Model
We now describe how the loss function in Eq. 3 can
be optimized using gradient descent. The gradient
of the loss function is given by:
?L
?? = ?
?
i
F (ti,1,?) +
?
i,j
p(ti,j | si,?)F (ti,j ,?)
where F (ti,j ,?) =
?
h?H(ti,j)
p(ti,j ,h | si,?)
p(ti,j | si,?)
?(ti,j ,h)
is the expected value of the feature vector produced
by parse tree ti,j . As we remarked earlier, |H(ti,j)|
is exponential in size so direct calculation of either
p(ti,j | si,?) or F (ti,j ,?) is impractical. However,
using the feature?vector decomposition in Eq. 4, we
can rewrite the key functions of ? as follows:
p(ti,j | si,?) =
Zi,j?
j?
Zi,j?
F (ti,j ,?) =?
1 ? w ? m
hw ? Hw(ti,j)
p(ti,j , w, hw)?(ti,j , w, hw) +
?
(u, v) ? D(ti,j)
hu ? Hu(ti,j)
hv ? Hv(ti,j)
p(ti,j , u, v, hu, hv)?(ti,j , u, v, hu, hv)
where p(ti,j , w, hw) and p(ti,j , u, v, hu, hv) are
marginalized probabilities and Zi,j is the associated
normalization constant:
Zi,j =
?
h?H(ti,j)
e?(ti,j ,h)??
p(ti,j , w, x) =
?
h |hw=x
p(ti,j ,h | si,?)
p(ti,j , u, v, x, y) =
?
h |hu=x,hv=y
p(ti,j ,h | si,?)
The three quantities above can be computed with be-
lief propagation (Yedidia et al, 2003), a dynamic?
programming technique that is efficient2 and exact
2The running time of belief propagation varies linearly with
the number of nodes in D(ti,j) and quadratically with the car-
dinality of the largest hidden?value domain.
when the graph D(ti,j) is a tree, which is the case
in our parse reranking model. Having calculated
the gradient in this way, we minimize the loss using
stochastic gradient descent3 (LeCun et al, 1998).
4 Features for Parse Reranking
The previous section described hidden?variable
models for discriminative reranking. We now de-
scribe features for the parse reranking problem. We
focus on the definition of hidden?value domains and
local feature vectors in the reranking model.
4.1 Hidden?Value Domains and Local Features
Each word in a parse tree is given a domain of pos-
sible hidden values by the hidden?variable model.
Models with widely varying behavior can be created
by changing the way these domains are defined. In
particular, in this section we will see how different
definitions of the domains give rise to the three main
model types: clustering, refinement, and mapping
into a pre?built ontology such as WordNet.
As illustration, consider a simple approach that
splits each word into a domain of three word?sense
hidden values (e.g., the word bank would yield the
domain {bank1, bank2, bank3}). In this approach,
each word receives a domain of hidden values that
is not shared with any other word. The model is
then able to distinguish several different usages for
each word, emulating a refinement operation. An
alternative approach is to split each word?s part?of?
speech tag into several sub?tags (e.g., bank would
yield {NN1,NN2,NN3}). This approach assigns the
same domain to many words; for instance, singular
nouns such as bond, market, and bank all receive the
same domain. The behavior of the model then emu-
lates a clustering operation.
Figure 2 shows the single?variable and pairwise
features used in our experiments. The features
are shown with hidden variables corresponding to
word?specific hidden values, such as shares1 or
bought3. In our experiments, we made use of fea-
tures such as those in Figure 2 in combination with
the following four definitions of the hidden?value
3We also performed some experiments using the conjugate
gradient descent algorithm (Johnson et al, 1999). However, we
did not find a significant difference between the performance of
either method. Since stochastic gradient descent was faster and
required less memory, our final experiments used the stochastic
gradient method.
510
bought yesterday1.5m shares in heavy trading
VBD(bought) PP(in) NP(yesterday)NP(shares)
S(bought)
VP(bought)
PSfrag replacements
Pairwise features generated for (shares1, bought3) =
(shares1, bought3, Dependency: VBD, NP, VP, Right, +Adj, -CC)
(shares1, bought3, Rule: VP? VBDhead, NPmod, PP, NP)
(shares1, bought3, Gpar Rule: S ? VP ? VBDhead, NPmod, PP, NP)
Single?variable features generated for (shares1) =
(shares1)
(shares1, Word: shares)
(shares1, POS: NN)
(shares1, Word: shares, POS: NN)
(shares1, Highest NT: NP)
(shares1, Word: shares, Highest NT: NP)
(shares1, POS: NN, Highest NT: NP)
(shares1, Word: shares, POS: NN, Highest NT: NP)
(shares1, Up Path: NP, VP, S)
(shares1, Word: shares, Up Path: NP, VP, S)
(shares1, Down Path: NP, NN)
(shares1, Word: shares, Down Path: NP, NN)
(shares1, Head Rule: NP ? CD, CD, NNShead)
(shares1, Word: shares, Head Rule: NP? CD, CD, NNShead)
(shares1, Mod Rule: VP ? VBDhead, NPmod, PP, NP)
(shares1, Word: shares, Mod Rule: VP? VBDhead, NPmod, PP, NP)
(shares1, Head Gpar Rule: VP ? NP? CD, CD, NNShead)
(shares1, Word: shares, Head Gpar Rule: VP? NP? CD, CD, NNShead)
(shares1, Mod Gpar Rule: S ? VP? VBDhead, NPmod, PP, NP)
(shares1, Word: shares, Mod Gpar Rule: S ? VP ? VBDhead, NPmod, PP, NP)
Figure 2: The features used in our model. We
show the single?variable features produced for hidden value
shares1 and the pairwise features produced for hidden values
(shares1, bought3), in the context of the given parse fragment.
Highest NT = highest nonterminal, Up Path = sequence of ances-
tor nonterminals, Down Path = sequence of headed nonterminals,
Head Rule = rules headed by the word, Mod Rule = rule in which
word acts as modifier, Head/Mod Gpar Rule = Head/Mod Rule plus
grandparent nonterminal.
domains (in each case we give the model type that
results from the definition?clustering, refinement,
or pre?built ontology?in parentheses):
Lexical (Refinement) Each word is split into
three sub?values. See Figure 2 for an example of
features generated for this choice of domain.
Part?of?Speech (Clustering) The part?of?
speech tag of each word is split into five sub?values.
In Figure 2, the word shares would be assigned
the domain {NNS1, . . . ,NNS5}, and the word bought
would have the domain {VBD1, . . . , VBD5}.
Highest Nonterminal (Clustering) The high-
est nonterminal to which each word propagates as
a headword is split into five sub?values. In Figure 2
the word bought yields domain {S1, . . . , S5}, while
in yields {PP1, . . . , PP5}.
Supersense (Pre?Built Ontology) We borrow
the idea of using WordNet lexicographer filenames
as broad ?supersenses? from Ciaramita and John-
son (2003). For each word, we split each of its
supersenses into three sub?supersenses. If no su-
persenses are available, we fall back to splitting
the part?of?speech into five sub?values. For ex-
ample, shares has the supersenses noun.possession,
noun.act and noun.artifact, which yield the do-
main {noun.possession1, noun.act1, noun.artifact1, . . .
noun.possession3, noun.act3, noun.artifact3}. On the
other hand, in does not have any WordNet super-
senses, so it is assigned the domain {IN1, . . . , IN5}.
4.2 The Final Feature Sets
We created eight feature sets by combining the
four hidden?value domains above with two alterna-
tive definitions of dependency structures: standard
head?modifier dependencies and ?sibling dependen-
cies.? When using sibling dependencies, connec-
tions are established between the headwords of ad-
jacent siblings. For instance, the head?modifier
dependencies produced by the tree fragment in
Figure 2 are (bought, shares), (bought, in), and
(bought, yesterday), while the corresponding sibling
dependencies are (bought, shares), (shares, in), and
(in, yesterday).
4.3 Mixed Models
The different hidden?variable models display vary-
ing strengths and weaknesses. We created mixtures
of different models using a weighted average:
log p(ti,j |si) =
M?
m=1
?m log pm(ti,j |si,?m)?Z(si)
where Z(si) is a normalization constant that can be
ignored, as it does not affect the ranking of parses.
The ?m weights are determined through optimiza-
tion of parsing scores on a development set.
5 Experimental Results
We trained and tested the model on data from the
Penn Treebank (Marcus et al, 1994). Candidate
parses were produced by an N?best version of the
Collins (1999) parser. Our training data consists of
Treebank Sections 2?21, divided into a training cor-
pus of 35,540 sentences and a development corpus
of 3,676 sentences. In later experiments, we made
use of a secondary development corpus of 1,914 sen-
tences from Section 0. Sections 22?24, containing
5,455 sentences, were held out as the test set.
For each of the eight feature sets described in
Section 4.2, we used the stochastic gradient descent
511
Section 22 Section 23 Section 24 Total
LR LP LR LP LR LP LR LP
C99 89.12 89.20 88.14 88.56 87.17 87.97 88.19 88.60
MIX 90.43 90.61 89.25 89.69 88.46 89.29 89.41 89.87
C2K 90.27 90.62 89.43 89.97 88.56 89.58 89.46 90.07
MIX+ 90.57 90.79 89.80 90.27 88.78 89.73 89.78 90.29
Table 1: The results on Sections 22?24. LR = Labeled Recall,
LP = Labeled Precision.
method to optimize the parameters of the model. We
created various mixtures of the eight models using
the weighted?average technique described in Sec-
tion 4.3, testing the accuracy of each mixture on the
secondary development set. Our final model was a
mixture of three of the eight possible models: super-
sense hidden values with sibling trees, lexical hid-
den values with sibling trees, and highest nontermi-
nal hidden values with normal head?modifier trees.
Our final tests evaluated four models. The two
baseline models are the Collins (1999) base parser,
C99, and the Collins (2000) reranker, C2K. The first
new model is the MIX model, which is a combina-
tion of the C99 base model with the three models
described above. The second new model, MIX+, is
created by augmenting MIX with features from the
method in C2K. Table 1 shows the results. The
MIX model obtains an F?measure improvement of
? 1.25% over the C99 baseline, an improvement that
is comparable to the C2K reranker. The MIX+ model
yields an improvement of ? 0.25% beyond C2K.
We tested the significance of 8 comparisons cor-
responding to the results in Table 1 using the sign
test4: we tested MIX vs. C99 on Sections 22, 23, and
24 individually, as well as on Sections 22?24 taken
as a whole; we also tested MIX+ vs. C2K on these 4
test sets. Of the 8 comparisons, all showed signif-
icant improvements at the level p ? 0.01 with the
exception of one test, MIX+ vs. C2K on Section 24.
6 Discussion
6.1 Applying the Model to Other NLP Tasks
In this section, we discuss how hidden?variable
models might be applied to other NLP problems, and
in particular to structures other than parse trees. To
4The input to the sign test is a set of sentences with judge-
ments for each sentence indicating whether model 1 gives a
better parse than model 2, model 2 gives a better parse than
model 1, or models 1 and 2 give equal quality parses. When
using the sign test, for each sentence in question we calculate
the F?measure at the sentence level for the two models being
compared, deriving the required judgement from these scores.
summarize the model, the major components of the
approach are as follows:
? We assume some set of candidate structures ti,j ,
which are to be reranked by the model. Each struc-
ture ti,j has ni,j words w1, . . . , wni,j , and each word
wk has a set Hk(ti,j) of possible hidden values.
? We assume a graph D(ti,j) for each ti,j that de-
fines possible interactions between hidden variables
in the model. We assume some definition of local
feature vectors, which consider either single hidden
variables, or pairs of hidden variables that are con-
nected by an edge in D(ti,j).
The approach can be instantiated in several ways
when applying the model to other NLP tasks. We
have already seen that by changing the definition
of the hidden?value domains Hk(ti,j), we can de-
rive models with widely varying behavior. In ad-
dition, there is no requirement that the hidden vari-
ables only be associated with words in the structure;
the hidden variables could be associated with other
units. For example, in speech recognition hidden
variables could be associated with phonemes rather
than words, and in Chinese word segmentation, hid-
den variables could be associated with individual
characters rather than words.
NLP tasks other than parsing involve structures
ti,j that are not necessarily parse trees. For example,
in speech recognition candidates are simply strings
(utterances); in tagging tasks candidates are labeled
sequences (e.g., sentences labeled with part?of?
speech tag sequences); in machine translation can-
didate structures may be source?language/target?
language pairs, along with alignment structures
specifying the correspondence between words in the
two languages. Sentences and labeled sequences are
in a sense simplifications of the parsing case, where
a natural choice for the underlying graph D(ti,j)
would be an N th order Markov structure, where each
word depends on the previous N words. Machine
translation alignments are a more interesting type of
structure, where the choice ofD(ti,j) might actually
depend on the alignment between the two sentences.
As a final note, there is some flexibility in the
choice of D(ti,j). In the case that D(ti,j) is a tree
belief propagation is exact. In the more general case
where D(ti,j) contains cycles, there are alternative
algorithms that are either exact (Cowell et al, 1999)
or approximate (Yedidia et al, 2003).
512
6.2 Packed Representations and Locality
One natural extension of our reranker is to adapt it to
candidate parses represented as a packed parse for-
est, so that it can operate on the base parser?s full
output instead of a limited N -best list. However,
as we described in Section 3.1, our features are lo-
cally scoped with respect to hidden?variable interac-
tions but unrestricted regarding information derived
from the underlying candidate parses, which poses
a problem for the use of packed representations.
For instance, the Up/Down Path features (see Figure
2) enumerate the vertical sequences of nontermi-
nals that extend above and below a given headword.
We could restrict the features to local scope on the
candidate parses, allowing dynamic?programming
to be used to train the model with a packed rep-
resentation. However, even with these restrictions,
finding argmaxt
?
h p(t,h | s,?) is NP?hard, and
the Viterbi approximation argmaxt,h p(t,h | s,?)
? or other approximations ? would have to be used
(see Matsuzaki et al (2005)).
6.3 Empirical Analysis of the Hidden Values
Our model makes no assumptions about the interpre-
tation of the hidden values assigned to words: dur-
ing training, the model simply learns a distribution
over global hidden?value assignments that is useful
in improving the log?likelihood of the training data.
Intuitively, however, we expect that the model will
learn to make hidden?value assignments that are rea-
sonable from a linguistic standpoint. In this section
we describe some empirical observations concern-
ing hidden values assigned by the model.
We established a corpus of parse trees with
hidden?value annotations, as follows. First, we find
the optimal parameters ?? on the training set. For
every sentence si in the training set, we then use
?? to find t?i , the most probable candidate parse un-
der the model. Finally, we use ?? to decode h?i ,
the most probable global assignment of hidden val-
ues, for each parse tree t?i . We created a corpus of
(t?i ,h
?
i ) pairs for the feature set defined by part?of?
speech hidden?value domains and standard depen-
dency structures. The remainder of this section de-
scribes trends for several of the most common part?
of?speech categories in the corpus.
As a first example, consider the hidden values
for the part?of?speech VB (infinitival verb). In the
majority of cases, words tagged VB either modify a
modal verb tagged MD (e.g., in the new rate will/MD
be/VB payable) or the infinitival marker to (e.g., in in
an effort to streamline/VB bureaucracy). The statis-
tics of our corpus reflect this distinction. In 11,546
cases of the VB1 hidden value, 10,652 cases mod-
ified to, and 81 cases modified modals tagged MD.
In contrast, in 11,042 cases of the VB2 value, the
numbers were 8,354 and 599 for modification of
modals and to respectively, showing the opposite
preference. This polarization of hidden values al-
lows modifiers to the VB (e.g., payable in the new
rate will be payable) to be sensitive to whether the
verb is modifying a modal or to.
In a related case, the hidden values for the part?
of?speech TO (corresponding to the word to) also
show that the model is learning useful structure.
Consider cases where to heads a clause which may
or may not have a subject (e.g., in it expects ?its sales
to remain steady? vs. a proposal ?to ease reporting
requirements?). We find that for hidden values TO1
and TO5 together, 946 out of 976 cases have a sub-
ject. In contrast, for the hidden value TO4, only 29
out of 10,148 cases have a subject. This splitting
of the TO part?of?speech allows modifiers to to, or
words modified by to, to be sensitive to the presence
or absence of a subject in the clause headed by to.
Finally, consider the hidden values for the part?
of?speech NNS (plural noun). In this case, the model
distinguishes contexts where a plural noun acting as
the head of a noun?phrase is or isn?t modified by a
post?modifier (such as a prepositional phrase or rel-
ative clause). For hidden value NNS3, 12,003 out
of the 12,664 instances in our corpus have a post?
modifier, but for hidden value NNS5, only 4,099 of
the 39,763 occurrences have a post?modifier. Sim-
ilar contextual effects were observed for other noun
categories such as singular or proper nouns.
7 Conclusions and Future Research
The hidden?variable model is a novel method for
representing NLP structures in the reranking frame-
work. We can obtain versatile behavior from the
model simply by manipulating the definition of the
hidden?value domains, and we have experimented
with models that emulate word clustering, word re-
finement, and mappings from words into an existing
ontology. In the case of the parse reranking task,
513
the hidden?variable model achieves reranking per-
formance comparable to the reranking approach de-
scribed by Collins (2000), and the two rerankers can
be combined to yield an additive improvement.
Future work may consider the use of hidden?
value domains with mixed contents, such as a do-
main that contains 3 refinement?oriented lexical val-
ues and 3 clustering?oriented part?of?speech val-
ues. These mixed values would allow the hidden?
variable model to exploit interactions between clus-
tering and refinement at the level of words and de-
pendencies. Another area for future research is to
investigate the use of unlabeled data within the ap-
proach, for example by making use of clusters de-
rived from large amounts of unlabeled data (e.g., see
Miller et al (2004)). Finally, future work may apply
the models to NLP tasks other than parsing.
Acknowledgements
We would like to thank Regina Barzilay, Igor
Malioutov, and Luke Zettlemoyer for their many
comments on the paper. We gratefully acknowl-
edge the support of the National Science Founda-
tion (under grants 0347631 and 0434222) and the
DARPA/SRI CALO project (through subcontract
No. 03-000215).
References
Daniel Bikel. 2000. A statistical model for parsing and word?
sense disambiguation. In Proceedings of EMNLP.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jen-
nifer C. Lai, and Robert L. Mercer. 1992. Class?based n?
gram models of natural language. Computational Linguis-
tics, 18(4):467?479.
Eugene Charniak and Mark Johnson. 2005. Coarse?to?fine n?
best parsing and maxent discriminative reranking. In Pro-
ceedings of the 43rd ACL.
Massimiliano Ciaramita and Mark Johnson. 2003. Supersense
tagging of unknown nouns in wordnet. In EMNLP 2003.
Stephen Clark and James R. Curran. 2004. Parsing the wsj
using ccg and log?linear models. In ACL, pages 103?110.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL 2002.
Michael Collins. 1999. Head?Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of the 17th ICML.
Michael Collins. 2002. Ranking algorithms for named entity
extraction: Boosting and the voted perceptron. In ACL 2002.
Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, and
David J. Spiegelhalter. 1999. Probabilistic Networks and
Expert Systems. Springer.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ?unification?
based? grammars. In Proceedings of the 37th ACL.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient?based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278?2324, November.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated corpus
of english: The penn treebank. Computational Linguistics,
19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2005.
Probabilistic cfg with latent annotations. In ACL.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller. 1993. Five papers on
wordnet. Technical report, Stanford University.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative train-
ing. In HLT?NAACL, pages 337?342.
Franz Josef Och and Hermann Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical machine
translation. In Proceedings of the 40th ACL, pages 295?302.
Fernando C. N. Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In Proceedings
of the 31st ACL, pages 183?190.
Ariadna Quattoni, Michael Collins, and Trevor Darrell. 2005.
Conditional random fields for object recognition. In NIPS
17. MIT Press.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994.
A maximum entropy model for parsing. In ICSLP 1994.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard S.
Crouch, John T. Maxwell III, and Mark Johnson. 2002.
Parsing the wall street journal using a lexical?functional
grammar and discriminative estimation techniques. In ACL.
Libin Shen and Aravind K. Joshi. 2003. An svm?based vot-
ing algorithm with application to parse reranking. In Walter
Daelemans and Miles Osborne, editors, Proceedings of the
7th CoNLL, pages 9?16. Edmonton, Canada.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Dis-
criminative reranking for machine translation. In HLT?
NAACL, pages 177?184.
Marilyn A. Walker, Owen Rambow, and Monica Rogati. 2001.
Spot: A trainable sentence planner. In NAACL.
Jonathan S. Yedidia, William T. Freeman, and Yair Weiss,
2003. Exploring Artificial Intelligence in the New Millen-
nium, chapter 8: ?Understanding Belief Propagation and Its
Generalizations?, pages 239?236. Science & Technology
Books.
514
Articles
Discriminative Reranking for Natural
Language Parsing
Michael Collins
Massachusetts Institute of Technology
Terry Koo
Massachusetts Institute of Technology
This article considers approaches which rerank the output of an existing probabilistic parser.
The base parser produces a set of candidate parses for each input sentence, with associated
probabilities that define an initial ranking of these parses. A second model then attempts to
improve upon this initial ranking, using additional features of the tree as evidence. The strength
of our approach is that it allows a tree to be represented as an arbitrary set of features, without
concerns about how these features interact or overlap and without the need to define a
derivation or a generative model which takes these features into account. We introduce a new
method for the reranking task, based on the boosting approach to ranking problems described in
Freund et al (1998). We apply the boosting method to parsing the Wall Street Journal treebank.
The method combined the log-likelihood under a baseline model (that of Collins [1999]) with
evidence from an additional 500,000 features over parse trees that were not included in the
original model. The new model achieved 89.75% F-measure, a 13% relative decrease in F-
measure error over the baseline model?s score of 88.2%. The article also introduces a new
algorithm for the boosting approach which takes advantage of the sparsity of the feature space in
the parsing data. Experiments show significant efficiency gains for the new algorithm over the
obvious implementation of the boosting approach. We argue that the method is an appealing
alternative?in terms of both simplicity and efficiency?to work on feature selection methods
within log-linear (maximum-entropy) models. Although the experiments in this article are on
natural language parsing (NLP), the approach should be applicable to many other NLP
problems which are naturally framed as ranking tasks, for example, speech recognition, machine
translation, or natural language generation.
1. Introduction
Machine-learning approaches to natural language parsing have recently shown some
success in complex domains such as news wire text. Many of these methods fall into
the general category of history-based models, in which a parse tree is represented as a
derivation (sequence of decisions) and the probability of the tree is then calculated as a
product of decision probabilities. While these approaches have many advantages, it
can be awkward to encode some constraints within this framework. In the ideal case,
the designer of a statistical parser would be able to easily add features to the model
* 2005 Association for Computational Linguistics
 MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), the Stata Center, Building 32,
32 Vassar Street, Cambridge, MA 02139. Email: mcollins@csail.mit.edu, maestro@mit.edu.
Submission received: 15th October 2003; Accepted for publication: 29th April 2004
that are believed to be useful in discriminating among candidate trees for a sentence.
In practice, however, adding new features to a generative or history-based model can
be awkward: The derivation in the model must be altered to take the new features into
account, and this can be an intricate task.
This article considers approaches which rerank the output of an existing
probabilistic parser. The base parser produces a set of candidate parses for each
input sentence, with associated probabilities that define an initial ranking of these
parses. A second model then attempts to improve upon this initial ranking, using
additional features of the tree as evidence. The strength of our approach is that it
allows a tree to be represented as an arbitrary set of features, without concerns about
how these features interact or overlap and without the need to define a derivation
which takes these features into account.
We introduce a new method for the reranking task, based on the boosting
approach to ranking problems described in Freund et al (1998). The algorithm can be
viewed as a feature selection method, optimizing a particular loss function (the
exponential loss function) that has been studied in the boosting literature. We applied
the boosting method to parsing the Wall Street Journal (WSJ) treebank (Marcus,
Santorini, and Marcinkiewicz 1993). The method combines the log-likelihood under a
baseline model (that of Collins [1999]) with evidence from an additional 500,000
features over parse trees that were not included in the original model. The baseline
model achieved 88.2% F-measure on this task. The new model achieves 89.75% F-
measure, a 13% relative decrease in F-measure error.
Although the experiments in this article are on natural language parsing, the
approach should be applicable to many other natural language processing (NLP)
problems which are naturally framed as ranking tasks, for example, speech
recognition, machine translation, or natural language generation. See Collins (2002a)
for an application of the boosting approach to named entity recognition, and Walker,
Rambow, and Rogati (2001) for the application of boosting techniques for ranking in
the context of natural language generation.
The article also introduces a new, more efficient algorithm for the boosting
approach which takes advantage of the sparse nature of the feature space in the
parsing data. Other NLP tasks are likely to have similar characteristics in terms of
sparsity. Experiments show an efficiency gain of a factor of 2,600 for the new algorithm
over the obvious implementation of the boosting approach. Efficiency issues are
important, because the parsing task is a fairly large problem, involving around one
million parse trees and over 500,000 features. The improved algorithm can perform
100,000 rounds of feature selection on our task in a few hours with current processing
speeds. The 100,000 rounds of feature selection require computation equivalent to
around 40 passes over the entire training set (as opposed to 100,000 passes for the
??naive?? implementation).
The problems with history-based models and the desire to be able to specify
features as arbitrary predicates of the entire tree have been noted before. In particular,
previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della
Pietra, and Lafferty 1997; Johnson et al 1999; Riezler et al 2002) has investigated the
use of Markov random fields (MRFs) or log-linear models as probabilistic models with
global features for parsing and other NLP tasks. (Log-linear models are often referred
to as maximum-entropy models in the NLP literature.) Similar methods have also been
proposed for machine translation (Och and Ney 2002) and language understanding in
dialogue systems (Papineni, Roukos, and Ward 1997, 1998). Previous work (Friedman,
Hastie, and Tibshirani 1998) has drawn connections between log-linear models and
26
Computational Linguistics Volume 31, Number 1
27
boosting for classification problems. One contribution of our research is to draw
similar connections between the two approaches to ranking problems.
We argue that the efficient boosting algorithm introduced in this article is an
attractive alternative to maximum-entropy models, in particular, feature selection
methods that have been proposed in the literature on maximum-entropy models. The
earlier methods for maximum-entropy feature selection methods (Ratnaparkhi,
Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra,
Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require
several full passes over the training set for each round of feature selection, suggesting
that at least for the parsing data, the improved boosting algorithm is several orders of
magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to
these earlier methods for feature selection, as well as the more recent work of
McCallum (2003); Zhou et al (2003); and Riezler and Vasserman (2004).
The remainder of this article is structured as follows. Section 2 reviews history-
based models for NLP and highlights the perceived shortcomings of history-based
models which motivate the reranking approaches described in the remainder of the
article. Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000;
Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty
2001; Collins, Schapire, and Singer 2002) that derives connections between boosting
and maximum-entropy models for the simpler case of classification problems; this
work forms the basis for the reranking methods. Section 4 describes how these
approaches can be generalized to ranking problems. We introduce loss functions for
boosting and MRF approaches and discuss optimization methods. We also derive the
efficient algorithm for boosting in this section. Section 5 gives experimental results,
investigating the performance improvements on parsing, efficiency issues, and the
effect of various parameters of the boosting algorithm. Section 6 discusses related work
in more detail. Finally, section 7 gives conclusions.
The reranking models in this article were originally introduced in Collins (2000). In
this article we give considerably more detail in terms of the algorithms involved, their
justification, and their performance in experiments on natural language parsing.
2. History-Based Models
Before discussing the reranking approaches, we describe history-based models (Black
et al 1992). They are important for a few reasons. First, several of the best-performing
parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997,
1999; Henderson 2003) are cases of history-based models. Many systems applied to
part-of-speech tagging, speech recognition, and other language or speech tasks also fall
into this class of model. Second, a particular history-based model (that of Collins
[1999]) is used as the initial model for our approach. Finally, it is important to describe
history-based models?and to explain their limitations?to motivate our departure
from them.
Parsing can be framed as a supervised learning task, to induce a function f : XYY
given training examples ?xi, yi?, where xi Z X , yi Z Y. We define GEN?x??Y to be the
set of candidates for a given input x. In the parsing problem x is a sentence, and
1 Note, however, that log-linear models which employ regularization methods instead of feature
selection?see, for example, Johnson et al (1999) and Lafferty, McCallum, and Pereira (2001)?are likely
to be comparable in terms of efficiency to our feature selection approach. See section 6.3 for more
discussion.
Collins and Koo Discriminative Reranking for NLP
GEN?x? is a set of candidate trees for that sentence. A particular characteristic of the
problem is the complexity of GEN?x? : GEN?x? can be very large, and each member of
GEN?x? has a rich internal structure. This contrasts with ??typical?? classification prob-
lems in which GEN?x? is a fixed, small set, for example, f1;?1g in binary
classification problems.
In probabilistic approaches, a model is defined which assigns a probability P?x, y?
to each ?x, y? pair.2 The most likely parse for each sentence x is then arg maxyZGEN(x)
P?x, y?. This leaves the question of how to define P?x, y?. In history-based approaches,
a one-to-one mapping is defined between each pair ?x, y? and a decision sequence
bd1::: dn?. The sequence bd1::: dn? can be thought of as the sequence of moves that build
?x, y? in some canonical order. Given this mapping, the probability of a tree can be
written as
P?x, y? ?
Y
i?1:::n
P?di)F?d1::: di1??
Here, ?d1::: di1? is the history for the ith decision. F is a function which groups histo-
ries into equivalence classes, thereby making independence assumptions in the model.
Probabilistic context-free grammars (PCFGs) are one example of a history-based
model. The decision sequence bd1::: dn? is defined as the sequence of rule expansions in
a top-down, leftmost derivation of the tree. The history is equivalent to a partially built
tree, and F picks out the nonterminal being expanded (i.e., the leftmost nonterminal in
the fringe of this tree), making the assumption that P?dijd1::: di1? depends only on
the nonterminal being expanded. In the resulting model a tree with rule expansions
bAiYbi? is assigned a probability
Q
i?1
n P?bijAi?.
Our base model, that of Collins (1999), is also a history-based model. It can be con-
sidered to be a type of PCFG, where the rules are lexicalized. An example rule would be
VP?saw? > VBD?saw? NPC?her? NP?today?
Lexicalization leads to a very large number of rules; to make the number of parameters
manageable, the generation of the right-hand side of a rule is broken down into a number
of decisions, as follows:
 First the head nonterminal (VBD in the above example) is chosen.
 Next, left and right subcategorization frames are chosen
({} and {NP-C}).
 Nonterminal sequences to the left and right of the VBD are chosen
(an empty sequence to the left, bNP-C, NP? to the right).
 Finally, the lexical heads of the modifiers are chosen (her and today).
28
2 To be more precise, generative probabilistic models assign joint probabilities P?x, y? to each ?x, y? pair.
Similar arguments apply to conditional history-based models, which define conditional probabilities
P?y j x? through a definition
P?y j x? ?
Y
i?1:::n
P?di j F?d1::: di1, x??
where d1 . . . dn are again the decisions made in building a parse, and F is a function that groups histories
into equivalence classes. Note that x is added to the domain of F (the context on which decisions are
conditioned). See Ratnaparkhi (1997) for one example of a method using this approach.
Computational Linguistics Volume 31, Number 1
29
Figure 1 illustrates this process. Each of the above decisions has an associated
probability conditioned on the left-hand side of the rule (VP(saw)) and other infor-
mation in some cases.
History-based approaches lead to models in which the log-probability of a parse
tree can be written as a linear sum of parameters ak multiplied by features hk. Each
feature hk?x, y? is the count of a different ??event?? or fragment within the tree. As an
example, consider a PCFG with rules bAkYbk? for 1  k  m. If hk?x, y? is the number
of times bAkYbk? is seen in the tree, and ak ? log P?bkjAk? is the parameter associated
with that rule, then
log P?x, y? ?
Xm
k?1
akhk?x, y?
All models considered in this article take this form, although in the boosting models
the score for a parse is not a log-probability. The features hk define an m-dimensional
vector of counts which represent the tree. The parameters ak represent the influence of
each feature on the score of a tree.
A drawback of history-based models is that the choice of derivation has a
profound influence on the parameterization of the model. (Similar observations have
been made in the related cases of belief networks [Pearl 1988], and language models
for speech recognition [Rosenfeld 1997].) When designing a model, it would be
desirable to have a framework in which features can be easily added to the model.
Unfortunately, with history-based models adding new features often requires a
modification of the underlying derivations in the model. Modifying the derivation to
include a new feature type can be a laborious task. In an ideal situation we would be
able to encode arbitrary features hk, without having to worry about formulating a
derivation that included these features.
To take a concrete example, consider part-of-speech tagging using a hidden
Markov model (HMM). We might have the intuition that almost every sentence has at
least one verb and therefore that sequences including at least one verb should have
increased scores under the model. Encoding this constraint in a compact way in an
HMM takes some ingenuity. The obvious approach?to add to each state the
information about whether or not a verb has been generated in the history?doubles
Figure 1
The sequence of decisions involved in generating the right-hand side of a lexical rule.
Collins and Koo Discriminative Reranking for NLP
the number of states (and parameters) in the model. In contrast, it would be trivial to
implement a feature hk?x, y? which is 1 if y contains a verb, 0 otherwise.
3. Logistic Regression and Boosting
We now turn to machine-learning methods for the ranking task. In this section we
review two methods for binary classification problems: logistic regression (or
maximum-entropy) models and boosting. These methods form the basis for the
reranking approaches described in later sections of the article. Maximum-entropy
models are a very popular method within the computational linguistics community;
see, for example, Berger, Della Pietra, and Della Pietra (1996) for an early article which
introduces the models and motivates them. Boosting approaches to classification have
received considerable attention in the machine-learning community since the intro-
duction of AdaBoost by Freund and Schapire (1997).
Boosting algorithms, and in particular the relationship between boosting
algorithms and maximum-entropy models, are perhaps not familiar topics in the
NLP literature. However there has recently been much work drawing connections
between the two methods (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy
and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001;
Collins, Schapire, and Singer 2002); in this section we review this work. Much of this
work has focused on binary classification problems, and this section is also restricted
to problems of this type. Later in the article we show how several of the ideas can be
carried across to reranking problems.
3.1 Binary Classification Problems
The general setup for binary classification problems is as follows:
 The ??input domain?? (set of possible inputs) is X .
 The ??output domain?? (set of possible labels) is simply a set of two
labels, Y = {1, +1}.3
 The training set is an array of n labeled examples,
b?x1, y1?, ?x2; y2?, : : : , ?xn, yn??, where each xi ZX , yi ZY.
 Input examples are represented through m ??features,?? which are
functions hk : XY < for k = 1, . . . , m. It is also sometimes convenient
to think of an example x as being represented by an m-dimensional
??feature vector?? 7?x? ? bh1?x?, h2?x?, : : : , hm?x??.
 Finally, there is a parameter vector, a ? ba1, : : : ,am?,
where each ak Z <, hence a? is an m-dimensional real-valued vector.
We show that both logistic regression and boosting implement a linear, or hyperplane,
classifier. This means that given an input example x and parameter values a?, the
output from the classifier is
sign ?F?x, a??? ?1?
30
3 It turns out to be convenient to define Y = {1, +1} rather than Y = {0, +1}, for example.
Computational Linguistics Volume 31, Number 1
31
where
F?x, a?? ?
Xm
k?1
akhk?x? ? a? I 7?x? ?2?
Here a? I 7?x? is the inner or dot product between the vectors a? and 7?x?, and sign(z) =
1 if z  0, sign(z) = 1 otherwise. Geometrically, the examples x are represented as
vectors 7(x) in some m-dimensional vector space, and the parameters a? define a
hyperplane which passes through the origin4 of the space and has a? as its normal.
Points lying on one side of this hyperplane are classified as +1; points on the other side
are classified as 1. The central question in learning is how to set the parameters a?,
given the training examples b?x1, y1?, ?x2, y2?, : : : ,?xn, yn??. Logistic regression and
boosting involve different algorithms and criteria for training the parameters a?, but
recent work (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and
Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins,
Schapire, and Singer 2002) has shown that the methods have strong similarities. The
next section describes parameter estimation methods.
3.2 Loss Functions for Logistic Regression and Boosting
A central idea in both logistic regression and boosting is that of a loss function, which
drives the parameter estimation methods of the two approaches. This section describes
loss functions for binary classification. Later in the article, we introduce loss functions
for reranking tasks which are closely related to the loss functions for classification tasks.
First, consider a logistic regression model. The parameters of the model a? are used
to define a conditional probability
P?y j x, a?? ? e
yF?x, a??
1 ? eyF?x, a??
?3?
where F?x, a?? is as defined in equation (2). Some form of maximum-likelihood
estimation is often used for parameter estimation. The parameters are chosen to
maximize the log-likelihood of the training set; equivalently: we talk (to emphasize the
similarities to the boosting approach) about minimizing the negative log-likelihood.
The negative log-likelihood, LogLoss(a?), is defined as
LogLoss ?a?? ? 
Xn
i?1
log P?yi j xi, a??? 
Xn
i?1
log
eyiF?xi, a??
1 ? eyiF?xi, a??
 
?
Xn
i?1
log 1 ? eyiF?xi, a??
 
?4?
There are many methods in the literature for minimizing LogLoss(a?) with respect to
a?, for example, generalized or improved iterative scaling (Berger, Della Pietra, and
4 It might seem to be a restriction to have the hyperplane passing through the origin of the space. However
if a constant ??bias?? feature hm?1?x? ? 1 for all x is added to the representation, a hyperplane passing
through the origin in this new space is equivalent to a hyperplane in general position in the original
m-dimensional space.
Collins and Koo Discriminative Reranking for NLP
Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), or conjugate gradient
methods (Malouf 2002). In the next section we describe feature selection methods, as
described in Berger, Della Pietra, and Della Pietra (1996) and Della Pietra, Della Pietra,
and Lafferty (1997).
Once the parameters a? are estimated on training examples, the output for an
example x is the most likely label under the model,
arg max
yZY
P?y j x, a?? ? arg max
yZf1,?1g
yF?x, a?? ? sign?F?x, a??? ?5?
where as before, sign ?z? ? 1 if z  0, sign ?z? ? 1 otherwise. Thus we see that the
logistic regression model implements a hyperplane classifier.
In boosting, a different loss function is used, namely, ExpLoss(a?), which is defined
as
ExpLoss?a?? ?
Xn
i?1
eyiF?xi, a?? ?6?
This loss function is minimized using a feature selection method, which we describe in
the next section.
There are strong similarities between LogLoss (equation (4)) and ExpLoss
(equation (6)). In making connections between the two functions, it is useful to
consider a third function of the parameters and training examples,
Error?a?? ?
Xn
i?1
gyiF?xi, a??  0? ?7?
where gp? is one if p is true, zero otherwise. Error(a?) is the number of incorrectly
classified training examples under parameter values a?.
Finally, it will be useful to define the margin on the ith training example, given
parameter values a?, as
Mi?a?? ? yiF?xi, a?? ?8?
With these definitions, the three loss functions can be written in the following form:
LogLoss?a?? ?
Xn
i?1
f ?Mi?a???, where f ?z? ? log?1 ? ez?
ExpLoss?a?? ?
Xn
i?1
f ?Mi?a???, where f ?z? ? ez
Error?a?? ?
Xn
i?1
f ?Mi?a???, where f ?z? ? gz  0?
The three loss functions differ only in their choice of an underlying ??potential
function?? of the margins, f(z). This function is f(z) = log (1 + ez), f(z) = ez, or
32
Computational Linguistics Volume 31, Number 1
33
f ?z? ? gz  0? for LogLoss, ExpLoss, and Error, respectively. The f(z) functions penalize
nonpositive margins on training examples. The simplest function, f ?z? ? gz  0?, gives
a cost of one if a margin is negative (an error is made), zero otherwise. ExpLoss and
LogLoss involve definitions for f?z? which quickly tend to zero as z Y V but heavily
penalize increasingly negative margins.
Figure 2 shows plots for the three definitions of f ?z?. The functions f ?z? ? ez and
f ?z? ? log ?1 ? ez? are both upper bounds on the error function, so that mini-
mizing either LogLoss or ExpLoss can be seen as minimizing an upper bound on
the number of training errors. (Note that minimizing Error(a?) itself is known to be
at least NP-hard if no parameter settings can achieve zero errors on the training
set; see, for example, Hoffgen, van Horn, and Simon [1995].) As zYV, the func-
tions f ?z? ? ez and f ?z? ? log?1 ? ez? become increasingly similar, because log
?1 ? ez?Y ez as ez Y 0. For negative z, the two functions behave quite differently.
f ?z? ? ez shows an exponentially growing cost function as zY V. In contrast, as
zYV it can be seen that log?1 ? ez?Y log?ez? ? z, so this function shows
asymptotically linear growth for negative z. As a final remark, note that both f ?z? ?
ez and f ?z? ? log?1 ? ez? are convex in z, with the result that LogLoss?a?? and
ExpLoss?a?? are convex in the parameters a?. This means that there are no problems
with local minima when optimizing these two loss functions.
3.3 Feature Selection Methods
In this article we concentrate on feature selection methods: algorithms which aim to
make progress in minimizing the loss functions LogLoss?a?? and ExpLoss?a?? while
using a small number of features (equivalently, ensuring that most parameter values in
Figure 2
Potential functions underlying ExpLoss, LogLoss, and Error. The graph labeled ExpLoss is a plot
of f ?z? ? ez for z ? ?1:5 : : :1:5; LogLoss shows a similar plot for f ?z? ? log?1 ? ez?; Error is a
plot of f ?z? ? gz  0?.
Collins and Koo Discriminative Reranking for NLP
a? are zero). Roughly speaking, the motivation for using a small number of features is
the hope that this will prevent overfitting in the models.
Feature selection methods have been proposed in the maximum-entropy literature
by several authors (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and
Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and
Ward 1997, 1998; McCallum 2003; Zhou et al 2003; Riezler and Vasserman 2004). The
most basic approach?for example see Ratnaparkhi, Roukos, and Ward (1994) and
Berger, Della Pietra, and Della Pietra (1996)?involves selection of a single feature at
each iteration, followed by an update to the entire model, as follows:
Step 1: Throughout the algorithm, maintain a set of active features. Initialize this set to
be empty.
Step 2: Choose a feature from outside of the set of active features which has the largest
estimated impact in terms of reducing the loss function LogLoss, and add this to the
active feature set.
Step 3: Minimize LogLoss?a?? with respect to the set of active features; that is, allow
only the active features to take nonzero parameter values when minimizing LogLoss.
Return to Step 2.
Methods in the boosting literature (see, for example, Schapire and Singer [1999]) can
be considered to be feature selection methods of the following form:
Step 1: Start with all parameter values set to zero.
Step 2: Choose a feature which has largest estimated impact in terms of reducing the
loss function ExpLoss.
Step 3: Update the parameter for the feature chosen at Step 2 in such a way as to
minimize ExpLoss?a?? with respect to this one parameter. All other parameter values
are left fixed. Return to Step 2.
The difference with this latter ??boosting?? approach is that in Step 3, only one
parameter value is adjusted, namely, the parameter corresponding to the newly chosen
feature. Note that in this framework, the same feature may be chosen at more than one
iteration.5 The maximum-entropy feature selection method can be quite inefficient,
as the entire model is updated at each step. For example, Ratnaparkhi (1998) quotes
times of around 30 hours for 500 rounds of feature selection on a prepositional-
phrase attachment task. These experiments were performed in 1998, when pro-
cessors were no doubt considerably slower than those available today. However,
the PP attachment task is much smaller than the parsing task that we are address-
ing: Our task involves around 1,000,000 examples, with perhaps a few hundred features
per example, and 100,000 rounds of feature selection; this compares to 20,000 exam-
ples, 16 features per example, and 500 rounds of feature selection for the PP attach-
ment task in Ratnaparkhi (1998). As an estimate, assuming that computational
complexity scales linearly in these factors,6 our task is 1,000,00020,000  32016 
100,000
500 ? 200,000
34
5 That is, the feature may be repeatedly updated, although the same feature will never be chosen
in consecutive iterations, because after an update the model is minimized with respect to the
selected feature.
6 We believe this is a realistic assumption, as each round of feature selection takes O?nf ? time, where n
is the number of training examples, and f is the number of active features on each example.
Computational Linguistics Volume 31, Number 1
35
as large as the PP attachment task. These figures suggest that the maximum-entropy
feature selection approach may be infeasible for large-scale tasks such as the one in this
article.
The fact that the boosting approach does not update the entire model at each
round of feature selection may be a disadvantage in terms of the number of features or
the test data accuracy of the final model. There is reason for concern that Step 2 will at
some iterations mistakenly choose features which are apparently useful in reducing
the loss function, but which would have little utility if the entire model had been
optimized at the previous iteration of Step 3. However, previous empirical results for
boosting have shown that it is a highly effective learning method, suggesting that this
is not in fact a problem for the approach. Given the previous strong results for the
boosting approach, and for reasons of computational efficiency, we pursue the
boosting approach to feature selection in this article.
3.4 Statistical Justification for the Methods
Minimization of LogLoss is most often justified as a parametric, maximum-likelihood
(ML) approach to estimation. Thus this approach benefits from the usual guarantees
for ML estimation: If the distribution generating examples is within the class of
distributions specified by the log-linear form, then in the limit as the sample size goes
to infinity, the model will be optimal in the sense of convergence to the true underlying
distribution generating examples. As far as we are aware, behavior of the models for
finite sample sizes is less well understood. In particular, while feature selection
methods have often been proposed for maximum-entropy models, little theoretical
justification (in terms of guarantees about generalization) has been given for them. It
seems intuitive that a model with a smaller number of parameters will require fewer
samples for convergence, but this is not necessarily the case, and at present this
intuition lacks a theoretical basis. Feature selection methods can probably be
motivated either from a Bayesian perspective (through a prior favoring models with
a smaller number of nonzero parameters) or from a frequentist/goodness-of-fit
perspective (models with fewer parameters are less likely to fit the data by chance), but
this requires additional research.
The statistical justification for boosting approaches is quite different. Boosting
algorithms were originally developed within the PAC framework (Valiant 1984) for
machine learning, specifically to address questions regarding the equivalence of weak
and strong learning. Freund and Schapire (1997) originally introduced AdaBoost and
gave a first set of statistical guarantees for the algorithm. Schapire et al (1998) gave a
second set of guarantees based on the analysis of margins on training examples. Both
papers assume that a fixed distribution D(x, y) is generating both training and test
examples and that the goal is to find a hypothesis with a small number of expected
errors with respect to this distribution. The form of the distribution is not assumed to
be known, and in this sense the guarantees are nonparametric, or ??distribution free.??
Freund and Schapire (1997) show that if the weak learning assumption holds (i.e.,
roughly speaking, a feature with error rate better than chance can be found for any
distribution over the sample space X  {1, +1}), then the training error for the
ExpLoss method decreases rapidly enough for there to be good generalization to test
examples. Schapire et al (1998) show that under the same assumption, minimization of
ExpLoss using the feature selection method ensures that the distribution of margins on
training data develops in such a way that good generalization performance on test
examples is guaranteed.
Collins and Koo Discriminative Reranking for NLP
3.5 Boosting with Complex Feature Spaces
Thus far in this article we have presented boosting as a feature selection approach. In
this section, we note that there is an alternative view of boosting in which it is
described as a method for combining multiple models, for example, as a method for
forming a linear combination of decision trees. We consider only the simpler, feature
selection view of boosting in this article. This section is included for completeness and
because the more general view of boosting may be relevant to future work on boosting
approaches for parse reranking (note, however, that the discussion in this section is not
essential to the rest of the article, so the reader may safely skip this section if she or he
wishes to do so).
In feature selection approaches, as described in this article, the set of possible
features hk?x? for k = 1, . . . , m is taken to be a fixed set of relatively simple functions. In
particular, we have assumed that m is relatively small (for example, small enough for
algorithms that require O(m) time or space to be feasible). More generally, however,
boosting can be applied in more complex settings. For example, a common use of
boosting is to form a linear combination of decision trees. In this case each example x is
represented as a number of attribute-value pairs, and each ??feature?? hk(x) is a complete
decision tree built on predicates over the attribute values in x. In this case the number
of features m is huge: There are as many features as there are decision trees over the
given set of attributes, thus m grows exponentially quickly with the number of
attributes that are used to represent an example x. Boosting may even be applied in
situations in which the number of features is infinite. For example, it may be used to
form a linear combination of neural networks. In this case each feature hk(x)
corresponds to a different parameter setting within the (infinite) set of possible
parameter settings for the neural network.
In more complex settings such as boosting of decision trees or neural networks, it
is generally not feasible to perform an exhaustive search (with O(m) time complexity)
for the feature which has the greatest impact on the exponential7 loss function. Instead,
an approximate search is performed. In boosting approaches, this approximate search
is achieved through a protocol in which at each round of boosting, a ??distribution??
over the training examples is maintained. The distribution can be interpreted as
assigning an importance weight to each training example, most importantly giving
higher weight to examples which are incorrectly classified. At each round of boosting
the distribution is passed to an algorithm such as a decision tree or neural network
learning method, which attempts to return a feature (a decision tree, or a neural
network parameter setting) which has a relatively low error rate with respect to the
distribution. The feature that is returned is then incorporated into the linear
combination of features. The algorithm which generates a classifier given a
distribution over the examples (for example, the decision tree induction method) is
usually referred to as ??the weak learner.?? The weak learner generally uses an
approximate (for example, greedy) method to find a function with a low error rate
with respect to the distribution. Freund and Schapire (1997) show that provided that at
each round of boosting the weak learner returns a feature with greater than (50 + &) %
accuracy for some fixed &, the number of training errors falls exponentially quickly
with the number of rounds of boosting. This fast drop in training errors translates to
statistical bounds on generalization performance (Freund and Schapire 1997).
36
7 Note that it is also possible to apply these methods to the LogLoss function; see, for example, Friedman
et al (2000) and Duffy and Helmbold (1999).
Computational Linguistics Volume 31, Number 1
37
Under this view of boosting, the feature selection methods in this article are a
particularly simple case in which the weak learner can afford to exhaustively search
through the space of possible features. Future work on reranking approaches might
consider other approaches?such as boosting of decision trees?which can effectively
consider more complex features.
4. Reranking Approaches
This section describes how the ideas from classification problems can be extended to
reranking tasks. A baseline statistical parser is used to generate N-best output both for
its training set and for test data sentences. Each candidate parse for a sentence is
represented as a feature vector which includes the log-likelihood under the baseline
model, as well as a large number of additional features. The additional features can in
principle be any predicates over sentence/tree pairs. Evidence from the initial log-
likelihood and the additional features is combined using a linear model. Parameter
estimation becomes a problem of learning how to combine these different sources of
information. The boosting algorithm we use is related to the generalization of boosting
methods to ranking problems in Freund et al (1998); we also introduce an approach
related to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward (1994),
Papineni, Roukos, and Ward (1997, 1998), Johnson et al (1999), Riezler et al (2002), and
Och and Ney (2002).
Section 4.1 gives a formal definition of the reranking problem. Section 4.2
introduces loss functions for reranking that are analogous to the LogLoss and ExpLoss
functions in section 3.2. Section 4.3 describes a general approach to feature selection
methods with these loss functions. Section 4.4 describes a first algorithm for the
exponential loss (ExpLoss) function; section 4.5 introduces a more efficient algorithm
for the case of ExpLoss. Finally, section 4.6 describes issues in feature selection
algorithms for the LogLoss function.
4.1 Problem Definition
We use the following notation in the rest of this article:
 si is the ith sentence in the training set. There are n sentences in training
data, so that 1  i  n.
 xi, j is the jth parse of the ith sentence. There are ni parses for the ith
sentence, so that 1  i  n and 1  j  ni. Each xi, j contains both the
tree and the underlying sentence (i.e., each xi, j is a pair bsi, ti, j?,
where si is the ith sentence in the training data, and ti, j is the jth tree
for this sentence). We assume that the parses are distinct, that is, that
xi, j m xi, j? for j m j?.
 Score?xi, j? is the ??score?? for parse xi, j, a measure of the similarity of
xi, j to the gold-standard parse. For example, Score?xi, j? might be the
F-measure accuracy of parse xi, j compared to the gold-standard parse
for si.
 Q?xi, j) is the probability that the base parsing model assigns to parse xi, j.
L?xi, j? ? log Q?xi, j? is the log-probability.
Collins and Koo Discriminative Reranking for NLP
 Without loss of generality, we assume xi,1 to be the highest-scoring
parse for the ith sentence.8 More precisely, for all i, 2  j  ni,
Score?xi,1? > Score?xi, j?. Note that xi,1 may not be identical to the
gold-standard parse; in some cases the parser may fail to propose
the correct parse anywhere in its list of candidates.9
Thus our training data consist of a set of parses, fxi, j : i ? 1, : : : , n, j ? 1, : : : , nig,
together with scores Score?xi, j? and log-probabilities L?xi, j?.
We represent candidate parse trees through m features, hk for k ? 1, : : : , m: Each hk
is an indicator function, for example,
hk?x? ? 1 if x contains the rule bS Y NP VP?
0 otherwise
We show that the restriction to binary-valued features is important for the simplicity
and efficiency of the algorithms.10 We also assume a vector of m + 1 parameters, a? =
{a0, a1, . . . , am}. Each ai can take any value in the reals. The ranking function for a
parse tree x implied by a parameter vector a? is defined as
F?x, a?? ? a0L?x? ?
Xm
k?1
akhk?x?
Given a new test sentence s, with parses xj for j = 1, : : : , N, the output of the model is
the highest-scoring tree under the ranking function
arg max
xZfx1 : : : xNg
F?x, a??
Thus F?x, a?? can be interpreted as a measure of how plausible a parse x is, with higher
scores meaning that x is more plausible. Competing parses for the same sentence are
ranked in order of plausibility by this function. We can recover the base ranking
function?the log-likelihood L?x??by setting a0 to a positive constant and setting all
other parameter values to be zero. Our intention is to use the training examples to pick
parameter values which improve upon this initial ranking.
We now discuss how to set these parameters. First we discuss loss functions
Loss?a?? which can be used to drive the training process. We then go on to describe
feature selection methods for the different loss functions.
38
8 In the event that multiple parses get the (same) highest score, the parse with the highest value of log-
likelihood L under the baseline model is taken as xi, 1. In the event that two parses have the same score
and the same log-likelihood?which occurred rarely if ever in our experiments?we make a random
choice between the two parses.
9 This is not necessarily a significant issue if an application using the output from the parser is sensitive to
improvements in evaluation measures such as precision and recall that give credit for partial matches
between the parser?s output and the correct parse. In this case, it is important only that the precision/
recall for xi, 1 is significantly higher than that of the baseline parser, that is, that there is some ??head room??
for the reranking module in terms of precision and recall.
10 In particular, this restriction allows closed-form parameter updates for the models based on ExpLoss that
we consider. Note that features tracking the counts of different rules can be simulated through several
features which take value one if a rule is seen  1 time,  2 times  3 times, and so on.
Computational Linguistics Volume 31, Number 1
39
4.2 Loss Functions for Ranking Problems
4.2.1 Ranking Errors and Margins. The loss functions we consider are all related to
the number of ranking errors a function F makes on the training set. The ranking error
rate is the number of times a lower-scoring parse is (incorrectly) ranked above the best
parse:
Error ?a?? ?
X
i
Xni
j?2
gF?xi,1, a??  F?xi,j, a??? ?
X
i
Xni
j?2
gF?xi,1, a??  F?xi,j, a??  0?
where again, gp? is one if p is true, zero otherwise. In the ranking problem we define the
margin for each example xi,j such that i = 1, : : : , n, j = 2, : : : , ni, as
Mij?a?? ? F?xi,1, a??  F?xi,j, a??
Thus Mij?a?? is the difference in ranking score between the correct parse of a sentence
and a competing parse xi,j. It follows that
Error?a?? ?
X
i
Xni
j?2
gMij?a??  0?
The ranking error is zero if all margins are positive. The loss functions we discuss all
turn out to be direct functions of the margins on training examples.
4.2.2 Log-Likelihood. The first loss function is that suggested by Markov random
fields. As suggested by Ratnaparkhi, Roukos, and Ward (1994) and Johnson et al
(1999), the conditional probability of xi,q being the correct parse for the ith sentence is
defined as
P?xi,q j si, a?? ?
eF?xi,q, a??
Pni
j?1
eF?xi,j, a??
Given a new test sentence s, with parses xj for j = 1, : : : , N, the most likely tree is
arg max
xj
eF?xj,a??
PN
q?1
eF?xq,a??
? arg max
xj
F?xj, a??
Hence once the parameters are trained, the ranking function is used to order candidate
trees for test examples.
The log-likelihood of the training data is
X
i
log P?xi,1 j si, a?? ?
X
i
log
eF?xi,1, a??P
j?1
ni eF?xi,j, a??
Under maximum-likelihood estimation, the parameters a? would be set to maxi-
mize the log-likelihood. Equivalently, we again talk about minimizing the negative
Collins and Koo Discriminative Reranking for NLP
log-likelihood. Some manipulation shows that the negative log-likelihood is a function
of the margins on training data:
LogLoss ?a?? ?
X
i
log e
F?xi,1, a??
P
j?1
ni eF?xi,j, a??
?
X
i
log 1P
j?1
ni e?F?xi,1, a??F?xi,j, a???
?
X
i
log?1 ?Xni
j?2
e?F?xi,1, a??F?xi,j, a???? ?X
i
log?1 ?Xni
j?2
eMi,j?a??? ?9?
Note the similarity of equation (9) to the LogLoss function for classification in equa-
tion (4).
4.2.3 Exponential Loss. The next loss function is based on the boosting method
described in Schapire and Singer (1999). It is a special case of the general ranking
methods described in Freund et al (1998), with the ranking ??feedback?? being a simple
binary distinction between the highest-scoring parse and the other parses. Again, the
loss function is a function of the margins on training data:
ExpLoss?a?? ?
X
i
Xni
j?2
e?F?xi,1, a??F?xi,j, a??? ?
X
i
Xni
j?2
eMi,j?a?? ?10?
Note the similarity of equation (10) to the ExpLoss function for classification
in equation (6). It can be shown that ExpLoss?a??  Error?a??, so that minimizing
ExpLoss?a?? is closely related to minimizing the number of ranking errors.11 This
follows from the fact that for any x, ex  gx < 0?, and therefore that
X
i
Xni
j?2
eMi,j?a?? 
X
i
Xni
j?2
gMi,j?a??  0?
We generalize the ExpLoss function slightly, by allowing a weight for each example
xi,j, for i = 1, . . . , n, j = 2, . . . , ni. We use Si,j to refer to this weight. In particular, in some
experiments in this article, we use the following definition:
Si,j ? Score?xi,1?  Score?xi,j? ?11?
40
11 Note that LogLoss is not a direct upper bound on the number of ranking errors, although it can be shown
that it is a (relatively loose) upper bound on the number of times the correct parse is not the highest-
ranked parse on the model. The latter observation follows from the property that the correct parse must
be highest ranked if its probability is greater than 0.5.
Computational Linguistics Volume 31, Number 1
41
where, as defined in section 4.1, Score?xi, j? is some measure of the ??goodness?? of a
parse, such as the F-measure (see section 5 for the exact definition of Score used in our
experiments). The definition for ExpLoss is modified to be
ExpLoss?a?? ?
X
i
Xni
j?2
Si,jeMi,j?a??
This definition now takes into account the importance, Si,j, of each example. It is an
upper bound on the following quantity:
X
i
Xni
j?2
Si,jgMi,j?a??  0?
which is the number of errors weighted by the factors Si,j. The original definition of
ExpLoss in equation (10) can be recovered by setting Si,j = 1 for all i, j (i.e., by giving
equal weight to all examples). In our experiments we found that a definition of Si,j such
as that in equation (11) gave improved performance on development data, presumably
because it takes into account the relative cost of different ranking errors in training-
data examples.
4.3 A General Approach to Feature Selection
At this point we have definitions for ExpLoss and LogLoss which are analogous to the
definitions in section 3.2 for binary classification tasks. Section 3.3 introduced the idea
of feature selection methods; the current section gives a more concrete description of
the methods used in our experiments.
The goal of feature selection methods is to find a small subset of the features that
contribute most to reducing the loss function. The methods we consider are greedy, at
each iteration picking the feature hk with additive weight d which has the most impact
on the loss function. In general, a separate set of instances is used in cross-validation to
choose the stopping point, that is, to decide on the number of features in the model.
At this point we introduce some notation concerning feature selection methods.
We define Upd?a?, k, d? to be an updated parameter vector, with the same parameter
values as a? with the exception of ak, which is incremented by d:
Upd?a?, k, d? ? fa0,a1, : : : ,ak ? d, : : : ,amg
The d parameter can potentially take any value in the reals. The loss for the updated
model is Loss?Upd?a?, k, d??. Assuming we greedily pick a single feature with some
weight to update the model, and given that the current parameter settings are a?, the
optimal feature/weight pair ?k, d? is
?k, d? ? arg min
k, d
Loss?Upd?a?, k, d??
Collins and Koo Discriminative Reranking for NLP
The feature selection algorithms we consider take the following form (a?t is the
parameter vector at the tth iteration):
Step 1: Initialize a?0 to some value. (This will generally involve values of zero for
a1, . . . , am and a nonzero value for a0, for example, a?0 = {1, 0, 0, . . .}.)
Step 2: For t = 1 to N (the number of iterations N will be chosen by cross-
validation)
a: Find ?k, d? = arg mink,d Loss?Upd ?a?t1, k, d??
b: Set a?t = Upd?a?t1, k, d?
Note that this is essentially the idea behind the ??boosting?? approach to feature se-
lection introduced in section 3.3. In contrast, the feature selection method of Berger,
Della Pietra, and Della Pietra (1996), also described in section 3.3, would involve
updating parameter values for all selected features at step 2b.
The main computation for both loss functions involves searching for the optimal
feature/weight pair ?k, d?. In both cases we take a two-step approach to solving this
problem. In the first step the optimal update for each feature hk is calculated. We
define BestWt?k, a?? as the optimal update for the kth feature (it must be calculated for
all features k = 1, . . . , m):
BestWt?k, a?? ? arg min
d
Loss?Upd?a?, k, d??
The next step is to calculate the Loss for each feature with its optimal update, which
we will call
BestLoss?k, a?? ? min
d
Loss?Upd?a?, k, d?? ? Loss?Upd?a?, k, BestWt?k, a????
BestWt and BestLoss for each feature having been computed, the optimal feature/
weight pair can be found:
k ? arg min
k
BestLoss?k, a??, d ? BestWt?k, a??
The next sections describe how BestWt and BestLoss can be computed for the two loss
functions.
4.4 Feature Selection for ExpLoss
At the first iteration, a0 is set to optimize ExpLoss (recall that L?xi,j? is the log-
likelihood for parse xi,j under the base parsing model):
a0 ? arg mina
X
i
Xni
j?2
Si,je?a?L?xi,1?L?xi,j?? ?12?
In initial experiments we found that this step was crucial to the performance of the
method (as opposed to simply setting a0 ? 1, for example). It ensures that the
42
Computational Linguistics Volume 31, Number 1
43
contribution of the log-likelihood feature is well-calibrated with respect to the
exponential loss function. In our implementation a0 was optimized using simple brute-
force search. All values of a0 between 0.001 and 10 at increments of 0.001 were tested,
and the value which minimized the function in equation (12) was chosen.12
Feature selection then proceeds to search for values of the remaining param-
eters, a1, . . . , am. (Note that it might be preferable to also allow a0 to be adjusted as
features are added; we leave this to future work.) This requires calculation of the
terms BestWt?k, a?? and BestLoss?k, a?? for each feature. For binary-valued features
these values have closed-form solutions, which is computationally very convenient.
We now describe the form of these updates. See appendix A for how the updates
can be derived (the derivation is essentially the same as that in Schapire and Singer
[1999]).
First, we note that for any feature, ?hk?xi,1?  hk?xi,j? can take on three values: +1,
1, or 0 (this follows from our assumption of binary-valued feature values). For each
k we define the following sets:
A?k ? f?i,j? : ?hk?xi,1?  hk?xi,j? ? 1g
Ak ? f?i,j? : ?hk?xi,1?  hk?xi,j? ? 1g
Thus A+k is the set of training examples in which the kth feature is seen in the correct
parse but not in the competing parse; Ak is the set in which the kth feature is seen in
the incorrect but not the correct parse.
Based on these definitions, we next define W?k and W

k as follows:
W?k ?
X
?i,j?ZA?k
Si,jeMi,j?a?? ?13?
Wk ?
X
?i,j?ZAk
si,jeMi,j?a?? ?14?
Given these definitions, it can be shown (see appendix A) that
BestWt?k, a?? ? 1
2
log
W?k
Wk
?15?
and
BestLoss?k, a?? ? Z 
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q 2
?16?
12 A more precise approach, for example, binary search, could also be used to solve this optimization
problem. We used the methods that searches through a set of fixed values for simplicity, implicitly
assuming that a precision of 0.001 was sufficient for our problem.
Collins and Koo Discriminative Reranking for NLP
where Z ?
P
i
Pni
j?2 Si,je
Mi,j?a?? ? ExpLoss?a?? is a constant (for fixed a?) which appears
in the BestLoss for all features and therefore does not affect their ranking.
As Schapire and Singer (1999) point out, the updates in equation (15) can be
problematic, as they are undefined (infinite) when either W?k or W

k is zero. Following
Schapire and Singer (1999), we introduce smoothing through a parameter & and the
following new definition of BestWt:
BestWt?k, a?? ? 1
2
log
W?k ? &Z
Wk ? &Z
?17?
The smoothing parameter & is chosen through optimization on a development set.
See Figure 3 for a direct implementation of the feature selection method for
ExpLoss. We use an array of values
Gk ? j
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q
j
to indicate the gain of each feature (i.e., the impact that choosing this feature will have
on the ExpLoss function). The features are ranked by this quantity. It can be seen that
almost all of the computation involves the calculation of Z and W?k and W

k for each
feature hk. Once these values have been computed, the optimal feature and its update
can be chosen.
4.5 A New, More Efficient Algorithm for ExpLoss
This section presents a new algorithm which is equivalent to the ExpLoss algo-
rithm in Figure 3, but can be vastly more efficient for problems with sparse fea-
ture spaces. In the experimental section of this article we show that it is almost
2,700 times more efficient for our task than the algorithm in Figure 3. The efficiency
of the different algorithms is important in the parsing problem. The training data we
eventually used contained around 36,000 sentences, with an average of 27 parses per
sentence, giving around 1,000,000 parse trees in total. There were over 500,000 dif-
ferent features.
The new algorithm is also applicable, with minor modifications, to boosting
approaches for classification problems in which the representation also involves sparse
binary features (for example, the text classification problems in Schapire and Singer
[2000]). As far as we are aware, the new algorithm has not appeared elsewhere in the
boosting literature.
Figure 4 shows the improved boosting algorithm. Inspection of the algorithm in
Figure 3 shows that only margins on examples in the sets A?k and A

k are modified
when a feature k is selected. The feature space in many NLP problems is very sparse
(most features only appear on relatively few training examples, or equivalently, most
training examples will have only a few nonzero features). It follows that in many cases,
the sets A?k and A

k will be much smaller than the overall size of the training set.
Therefore when updating the model from a? to Upd?a?, k, d?, the values W?k and Wk
remain unchanged for many features and do not need to be recalculated. In fact, only
44
Computational Linguistics Volume 31, Number 1
45
features which co-occur with k* on some example must be updated. The algorithm in
Figure 4 recalculates the values of A?k and A

k only for those features which co-occur
with the selected feature k*.
To achieve this, the algorithm relies on a second pair of indices. For all i, 2  j  ni,
we define
B?i,j ? fk : ?hk?xi,1?  hk?xi,j? ? 1 g
Bi,j ? fk : ?hk?xi,1?  hk?xi,j? ? 1g ?18?
Figure 3
A naive algorithm for the boosting loss function.
Collins and Koo Discriminative Reranking for NLP
46
Figure 4
An improved algorithm for the boosting loss function.
Computational Linguistics Volume 31, Number 1
47
So Bi,j
+ and Bi,j
 are indices from training examples to features. With the algorithm in
Figure 4, updating the values of W?k and W

k for the features which co-occur with k*
involves the following number of steps:
C ?
X
?i,j?ZA?
k
?jB?i,j j ? jBi,j j? ?
X
?i,j?ZAk
?jB?i,j j ? jBi,j j? ?19?
In contrast, the naive algorithm requires a pass over the entire training set, which
requires the following number of steps:
T ?
Xn
i?1
Xni
j?2
?jB?i,j j ? jBi,j j? ?20?
The relative efficiency of the two algorithms depends on the value of C/T at each
iteration. In the worst case, when every feature chosen appears on every training
example, then C/T = 1, and the two algorithms essentially have the same running time.
However in sparse feature spaces there is reason to believe that C/T will be small for
most iterations. In section 5.4.3 we show that this is the case for our experiments.
4.6 Feature Selection for LogLoss
We now describe an approach that was implemented for LogLoss. At the first iteration,
a0 is set to one. Feature selection then searches for values of the remaining parameters,
a1, : : : ,am. We now describe how to calculate the optimal update for a feature k with
the LogLoss function. First we recap the definition of the probability of a particular
parse xi,q given parameter settings a?:
P?xi,q j si, a?? ?
eF?xi,q,a??P
j?1
ni eF?xi,j,a??
Recall that the log-loss is
LogLoss?a?? ?
X
i
 log P?xi,1 j si, a??
Unfortunately, unlike the case of ExpLoss, in general an analytic solution for BestWt
does not exist. However, we can define an iterative solution using techniques from
iterative scaling (Della Pietra, Della Pietra, and Lafferty 1997). We first define h?k, the
number of times that feature k is seen in the best parse, and p?k?a??, the expected number
of times under the model that feature k is seen:
h?k ?
X
i
hk?xi,1?, p?k?a?? ?
X
i
Xni
j?1
hk?xi,j?P?xi,j j si, a??
Collins and Koo Discriminative Reranking for NLP
Iterative scaling then defines the following update d?
d? ? log h?k
p?k?a??
While in general it is not true that d? ? BestWt?k, a??, it can be shown that this update
leads to an improvement in the LogLoss (i.e., that LogLoss?Upd?a?, k, d???  LogLoss a??),
with equality holding only when a?k is already at the optimal value, in other words,
when arg mind LogLoss?Upd?a?, k, d?? = 0. This suggests the following iterative method
for finding BestWt?k, a??:
Step 1: Initialization: Set d = 0 and a?? ? a?, calculate h?k.
Step 2: Repeat until convergence of d:
a: Calculate p?k(a??).
b: d@ d? log h?kp?k?a??? .
c : a??@Upd?a?, k, d?.
Step 3: Return BestWt?k, a?? ? d.
Given this method for calculating BestWt?k, a??, BestLoss?k, a?? can be calculated as
Loss?k, BestWt?k, a???. Note that this is only one of a number of methods for finding
BestWt?k, a??: Given that this is a one-parameter, convex optimization problem, it is a
fairly simple task, and there are many methods which could be used.
Unfortunately there does not appear to be an efficient algorithm for LogLoss that is
analogous to the ExpLoss algorithm in Figure 4 (at least if the feature selection method
is required to pick the feature with highest impact on the loss function at each
iteration). A similar observation for LogLoss can be made, in that when the model is
updated with a feature/weight pair ?k, d?, many features will have their values for
BestWt and BestLoss unchanged. Only those features which co-occur with k on some
example will need to have their values of BestWt and BestLoss updated. However, this
observation does not lead to an efficient algorithm: Updating these values is much
more expensive than in the ExpLoss case. The procedure for finding the optimal value
BestWt?k, a?? must be applied for each feature which co-occurs with the chosen feature
k. For example, the iterative scaling procedure described above must be applied for a
number of features. For each feature, this will involve recalculation of the distribution
fP?xi, 1 j si?, P?xi,2 j si?, : : : , P?xi,ni j si?g for each example i on which the feature occurs.
13 It
takes only one feature that is seen on all training examples for the algorithm to involve
recalculation of P?xi,j j si? for the entire training set. This contrasts with the simple
updates in the improved boosting algorithm ?W?k ? W
?
k ? D and Wk ? Wk ? D?. In
fact in the parsing experiments, we were forced to give up on the LogLoss feature
selection methods because of their inefficiency (see section 6.4 for more discussion about
efficiency).
48
13 This is not a failure of iterative scaling alone: Given that in the general case, closed-form solutions for
BestWt and BestLoss do not exist, it is hard to imagine a method that computes these values exactly
without some kind of iterative method which requires repeatedly visiting the examples on which a
feature is seen.
Computational Linguistics Volume 31, Number 1
49
Note, however, that approximate methods for finding the best feature and
updating its weight may lead to efficient algorithms. Appendix B gives a sketch of one
such approach, which is based on results from Collins, Schapire, and Singer (2002). We
did not test this method; we leave this to future work.
5. Experimental Evaluation
5.1 Generation of Parsing Data Sets
We used the Penn Wall Street Journal treebank (Marcus, Santorini, and
Marcinkiewicz 1993) as training and test data. Sections 2?21 inclusive (around
40,000 sentences) were used as training data, section 23 was used as the final test set.
Of the 40,000 training sentences, the first 36,000 were used as the main training set.
The remaining 4,000 sentences were used as development data and to cross-validate
the number of rounds (features) in the model. Model 2 of Collins (1999) was used to
parse both the training and test data, producing multiple hypotheses for each
sentence. We achieved this by disabling dynamic programming in the parser and
choosing a relatively narrow beam width of 1,000. The resulting parser returns all
parses that fall within the beam. The number of such parses varies sentence by
sentence.
In order to gain a representative set of training data, the 36,000 training sentences
were parsed in 2,000 sentence chunks, each chunk being parsed with a model trained
on the remaining 34,000 sentences (this prevented the initial model from being
unrealistically ??good?? on the training sentences). The 4,000 development sentences
were parsed with a model trained on the 36,000 training sentences. Section 23 was
parsed with a model trained on all 40,000 sentences.
In the experiments we used the following definition for the Score of the parse:
Score?xi,j? ?
Fmeasure?xi,j?
100
 Size?xi,j?
where F-measure(xi,j) is the F1 score
14 of the parse when compared to the gold-
standard parse (a value between 0 and 100), and Size(xi,j) is the number of constituents
in the gold-standard parse for the ith sentence. Hence the Score function is sensitive to
both the accuracy of the parse, and also the number of constituents in the gold-
standard parse.
5.2 Features
The following types of features were included in the model. We will use the rule VP Y
PP VBD NP NP SBAR with head VBD as an example. Note that the output of our
baseline parser produces syntactic trees with headword annotations (see Collins
[1999]) for a description of the rules used to find headwords).
14 Note that in the rare cases in which the baseline parser produces no constituents, the precision is
undefined; in these cases we defined the F-measure to be 0.
Collins and Koo Discriminative Reranking for NLP
Rules. These include all context-free rules in the
tree, for example, VP Y PP VBD NP NP SBAR.
Bigrams. These are adjacent pairs of nonterminals
to the left and right of the head. As shown, the
example rule would contribute the bigrams
(Right,VP,NP,NP), (Right,VP,NP,SBAR),
(Right,VP,SBAR,STOP) to the right of the head
and (Left,VP,PP,STOP) to the left of the head.
Grandparent rules. Same as Rules, but also
including the nonterminal above the rule.
Grandparent bigrams. Same as Bigrams,
but also including the nonterminal above
the bigrams.
Lexical bigrams.
Same as Bigrams,
but with the lexical
heads of the two
nonterminals also
included.
Two-level rules. Same as Rules, but also
including the entire rule above the rule.
Two-level bigrams. Same as Bigrams, but
also including the entire rule above the rule.
Trigrams. All trigrams within the rule. The
example rule would contribute the trigrams
(VP, STOP, PP, VBD!), (VP, PP, VBD!, NP),
(VP, VBD!, NP, NP), (VP, NP, NP, SBAR),
and (VP,NP, SBAR, STOP) (! is used to mark
the head of the rule).
50
Computational Linguistics Volume 31, Number 1
51
Head Modifiers. All head-modifier pairs, with
the grandparent nonterminal also included.
An adj flag is also included, which is one if
the modifier is adjacent to the head, zero
otherwise. As an example, say the nonterminal
dominating the example rule is S. The
example rule would contribute (Left, S, VP,
VBD, PP, adj = 1), (Right, S, VP, VBD, NP,
adj = 1), (Right, S, VP, VBD, NP, adj = 0),
and (Right, S, VP, VBD, SBAR, adj = 0).
PPs. Lexical trigrams involving the heads
of arguments of prepositional phrases.
The example shown at right would
contribute the trigram (NP, NP, PP, NP,
president, of, U.S.), in addition to the
relation (NP, NP, PP, NP, of, U.S.), which
ignores the headword of the constituent
being modified by the PP. The three
nonterminals (for example, NP, NP, PP)
identify the parent of the entire phrase,
the nonterminal of the head of the phrase,
and the nonterminal label for the PP.
Distance head modifiers. Features involving the distance between
headwords. For example, assume dist is the number of words between
the headwords of the VBD and SBAR in the (VP, VBD, SBAR) head-modifier
relation in the above rule. This relation would then generate features
(VP, VBD, SBAR, = dist), and (VP, VBD, SBAR,  x) for all dist  x  9 and
(VP, VBD, SBAR,  x) for all 1  x  dist.
Further lexicalization. In order to generate more features, a second pass
was made in which all nonterminals were augmented with their lexical
heads when these headwords were closed-class words. All features
apart from head modifiers, PPs, and distance head modifiers were then
generated with these augmented nonterminals.
All of these features were initially generated, but only features seen on at least
one parse for at least five different sentences were included in the final model (this
count cutoff was implemented to keep the number of features down to a tractable
number).
5.3 Applying the Reranking Methods
The ExpLoss method was trained with several values for the smoothing parameter &:
{0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075}. For each value of &, the
method was run for 100,000 rounds on the training data. The implementation was
such that the feature updates for all 100,000 rounds for each training run were
recorded in a file. This made it simple to test the model on development data for all
values of N between 0 and 100,000.
Collins and Koo Discriminative Reranking for NLP
The different values of & and N were compared on development data through the
following criterion:
X
i
Score?zi? ?21?
where Score is as defined above, and zi is the output of the model on the ith
development set example. The &, N values which maximized this quantity were used
to define the final model applied to the test data (section 23 of the treebank). The
optimal values were & ? 0.0025 and N ? 90,386, at which point 11,673 features had
nonzero values (note that the feature selection techniques may result in a given feature
being updated more than once). The computation took roughly 3?4 hours on a
machine with a 1.6 GHz pentium processor and around 2 GB of memory.
Table 1 shows results for the method. The model of Collins (1999) was the base
model; the ExpLoss model gave a 1.5% absolute improvement over this method. The
method gives very similar accuracy to the model of Charniak (2000), which also uses a
rich set of initial features in addition to Charniak?s (1997) original model.
The LogLoss method was too inefficient to run on the full data set. Instead we
made some tests on a smaller subset of the data (5,934 sentences, giving 200,000 parse
trees) and 52,294 features.15 On an older machine (an order of magnitude or more
slower than the machine used for the final tests) the boosting method took 40 minutes
for 10,000 rounds on this data set. The LogLoss method took 20 hours to complete
3,500 rounds (a factor of about 85 times slower). This was in spite of various heuristics
that were implemented in an attempt to speed up LogLoss: for example, selecting
multiple features at each round or recalculating the statistics for only the best K
features for some small K at the previous round of feature selection. In initial
experiments we found ExpLoss to give similar, perhaps slightly better, accuracy than
LogLoss.
5.4 Further Experiments
This section describes further experiments investigating various aspects of the
boosting algorithm: the effect of the & and N parameters, learning curves, the choice
of the Si,j weights, and efficiency issues.
5.4.1 The Effect of the & and N Parameters. Figure 5 shows the learning curve on
development data for the optimal value of & (0.0025). The accuracy shown is the
performance relative to the baseline method of using the probability from the
generative model alone in ranking parses, where the measure in equation (21) is used
to measure performance. For example, a score of 101.5 indicates a 1.5% increase in this
score. The learning curve is initially steep, eventually flattening off, but reaching its
peak value after a large number (90,386) of rounds of feature selection.
Table 2 indicates how the peak performance varies with the smoothing parameter
&. Figure 6 shows learning curves for various values of &. It can be seen that values
other than & ? 0.0025 can lead to undertraining or overtraining of the model.
52
15 All features described above except distance head modifiers and further lexicalization were included.
Computational Linguistics Volume 31, Number 1
53
Figure 5
Learning curve on development data for the optimal value for & (0.0025). The y-axis is the level of
accuracy (100 is the baseline score), and the x-axis is the number of rounds of boosting.
Table 1
Results on section 23 of the WSJ Treebank. ??LR?? is labeled recall; ??LP?? is labeled precision;
??CBs?? is the average number of crossing brackets per sentence; ??0 CBs?? is the percentage of
sentences with 0 crossing brackets; ??2 CBs?? is the percentage of sentences with two or more
crossing brackets. All the results in this table are for models trained and tested on the same data,
using the same evaluation metric. Note that the ExpLoss results are very slightly different from
the original results published in Collins (2000). We recently reimplemented the boosting code
and reran the experiments, and minor differences in the code and & values tested on
development data led to minor improvements in the results.
Model  40 Words (2,245 sentences)
LR LP CBs 0 CBs 2 CBs
Charniak 1997 87.5% 87.4% 1.00 62.1% 86.1%
Collins 1999 88.5% 88.7% 0.92 66.7% 87.1%
Charniak 2000 90.1% 90.1% 0.74 70.1% 89.6%
ExpLoss 90.2% 90.4% 0.73 71.2% 90.2%
Model  100 Words (2,416 sentences)
LR LP CBs 0 CBs 2 CBs
Charniak 1997 86.7% 86.6% 1.20 59.5% 83.2%
Ratnaparkhi 1998 86.3% 87.5% 1.21 60.2% ?
Collins 1999 88.1% 88.3% 1.06 64.0% 85.1%
Charniak 2000 89.6% 89.5% 0.88 67.6% 87.7%
ExpLoss 89.6% 89.9% 0.86 68.7% 88.3%
Collins and Koo Discriminative Reranking for NLP
5.4.2 The Effect of the Si,j Weights on Examples. In section 4.2.3 we introduced the
idea of weights Si,j representing the importance of examples. Thus far, in the
experiments in this article, we have used the definition
Si,j ? Score?xi,1?  Score?xi,j? ?22?
thereby weighting examples in proportion to their difference in score from the correct
parse for the sentence in question. In this section we compare this approach to a
default definition of Si,j, namely,
Si,j ? 1 ?23?
Using this definition, we trained the ExpLoss method on the same training set for
several values of the smoothing parameter & and evaluated the performance on
development data. Table 3 compares the peak performance achieved under the two
definitions of Si,j on the development set. It can be seen that the definition in equation
(22) outperforms the simpler method in equation (23). Figure 7 shows the learning
curves for the optimal values of & for the two methods. It can be seen that the learning
curve for the definition of Si,j in equation (22) consistently dominates the curve for the
simpler definition.
5.4.3 Efficiency Gains. Section 4.5 introduced an efficient algorithm for optimizing
ExpLoss. In this section we explore the empirical gains in efficiency seen on the
parsing data sets in this article.
We first define the quantity T as follows:
T ?
X
i
Xni
j?2
?jB?i,j j ? jBi,j j?
54
Table 2
Peak performance achieved for various values of &. ??Best N?? refers to the number of
rounds at which peak development set accuracy was reached. ??Best score?? indicates the
relative performance, compared to the baseline method, at the optimal value for N.
& Best N Best score
0.0001 29,471 101.743
0.00025 22,468 101.849
0.0005 48,795 101.845
0.00075 43,386 101.809
0.001 43,975 101.849
0.0025 90,386 101.864
0.005 66,378 101.824
0.0075 80,746 101.722
Computational Linguistics Volume 31, Number 1
55
Figure 6
Learning curves on development data for various values of &. In each case the y-axis is the
level of accuracy (100 is the baseline score), and the x-axis is the number of rounds of
boosting. The three graphs compare the curve for & = 0.0025 (the optimal value) to (from top
to bottom) & = 0.0001, & = 0.0075, and & = 0.001. The top graph shows that & = 0.0001 leads to
undersmoothing (overtraining). Initially the graph is higher than that for & = 0.0025, but on later
rounds the performance starts to decrease. The middle graph shows that & = 0.0075 leads to
oversmoothing (undertraining). The graph shows consistently lower performance than that for
& = 0.0025. The bottom graph shows that there is little difference in performance for & = 0.001
versus & = 0.0025.
Collins and Koo Discriminative Reranking for NLP
This is a measure of the number of updates to the W?k and W

k variables required in
making a pass over the entire training set. Thus it is a measure of the amount of
computation that the naive algorithm for ExpLoss, presented in Figure 3, requires for
each round of feature selection.
Next, say the improved algorithm in Figure 4 selects feature k* on the t th round of
feature selection. Then we define the following quantity:
Ct ?
X
?i,j?&A?
k
?jB?i,j j ? jBi,j j? ?
X
?i,j?&Ak
?jB?i,j j ? jBi,j j?
This is a measure of the number of summations required by the improved algorithm in
Figure 4 at the t th round of feature selection.
56
Figure 7
Performance versus number of rounds of boosting for Si,j ? Score?xi,1?  Score?xi, j?
(curve labeled ??Weighted??) and Si,j ? 1 (curve labeled ??Not Weighted??).
Table 3
Peak performance achieved for various values of & for Si, j ? Score?xi,1?  Score?xi, j? (column
labeled ??weighted??) and Si,j ? 1 (column labeled ??unweighted??).
& Best score (weighted) Best score (unweighted)
0.0001 101.743 101.744
0.00025 101.849 101.754
0.0005 101.845 101.778
0.00075 101.809 101.762
0.001 101.849 101.778
0.0025 101.864 101.699
0.005 101.824 101.610
0.0075 101.722 101.604
Computational Linguistics Volume 31, Number 1
57
We are now in a position to compare the running times of the two algorithms. We
define the following quantities:
Work?n? ?
Xn
t?1
Ct
T
?24?
Savings?n? ? nTPn
t?1
Ct
?25?
Savings?a, b? ? ?1 ? b  a?TPb
t?a
Ct
?26?
Here, Work?n? is the computation required for n rounds of feature selection, where
a single unit of computation corresponds to a pass over the entire training set.
Savings?n? tracks the relative efficiency of the two algorithms as a function of the
number of features, n. For example, if Savings?100? ? 1,200, this signifies that for the
first 100 rounds of feature selection, the improved algorithm is 1,200 times as efficient
as the naive algorithm. Finally, Savings?a, b? indicates the relative efficiency between
rounds a and b, inclusive, of feature selection. For example, Savings?11, 100? ? 83
signifies that between rounds 11 and 100 inclusive of the algorithm, the improved
algorithm was 83 times as efficient.
Figures 8 and 9 show graphs of Work?n? and Savings?n? versus n. The savings
from the improved algorithm are dramatic. In 100,000 rounds of feature selection,
the improved algorithm requires total computation that is equivalent to a mere 37.1
passes over the training set. This is a saving of a factor of 2,692 over the naive algorithm.
Table 4 shows the value of Savings?a,b? for various values of ?a,b?. It can be
seen that the performance gains are significantly larger in later rounds of feature
selection, presumably because in later stages relatively infrequent features are being
selected. Even so, there are still savings of a factor of almost 50 in the early stages of
the method.
6. Related Work
6.1 History-Based Models with Complex Features
Charniak (2000) describes a parser which incorporates additional features into a
previously developed parser, that of Charniak (1997). The method gives substantial
improvements over the original parser and results which are very close to the results
of the boosting method we have described in this article (see section 5 for experimental
results comparing the two methods). Our features are in many ways similar to those of
Charniak (2000). The model in Charniak (2000) is quite different, however. The
additional features are incorporated using a method inspired by maximum-entropy
models (e.g., the model of Ratnaparkhi [1997]).
Ratnaparkhi (1997) describes the use of maximum-entropy techniques ap-
plied to parsing. Log-linear models are used to estimate the conditional probabilities
P?di j F ?d1, : : : , di1?? in a history-based parser. As a result the model can take into
account quite a rich set of features in the history.
Collins and Koo Discriminative Reranking for NLP
58
Figure 8
Work?n??yaxis? versus n ?xaxis?.
Figure 9
Savings?n??y-axis? versus n?x-axis?.
Computational Linguistics Volume 31, Number 1
59
Both approaches still rely on decomposing a parse tree into a sequence of
decisions, and we would argue that the techniques described in this article have more
flexibility in terms of the features that can be included in the model.
6.2 Joint Log-Linear Models
Abney (1997) describes the application of log-linear models to stochastic head-
driven phrase structure grammars (HPSGs). Della Pietra, Della Pietra, and Lafferty
(1997) describe feature selection methods for log-linear models, and Rosenfeld
(1997) describes application of these methods to language modeling for speech
recognition. These methods all emphasize models which define a joint proba-
bility over the space of all parse trees (or structures in question): For this reason
we describe these approaches as ??Joint log-linear models.?? The probability of a tree
xi,j is
P?xi,j? ?
eF?xi,j?P
xZZ
eF?x?
?27?
Here Z is the (infinite) set of possible trees, and the denominator cannot be
calculated explicitly. This is a problem for parameter estimation, in which an estimate
of the denominator is required, and Monte Carlo methods have been proposed
(Della Pietra, Della Pietra, and Lafferty 1997; Abney 1997; Rosenfeld 1997) as a
technique for estimation of this value. Our sense is that these methods can be
computationally expensive. Notice that the joint likelihood in equation (27) is not a
direct function of the margins on training examples, and its relation to error
rate is therefore not so clear as in the discriminative approaches described in this
article.
6.3 Conditional Log-Linear Models
Ratnaparkhi, Roukos, and Ward (1994), Johnson et al (1999), and Riezler et al (2002)
suggest training log-linear models (i.e., the LogLoss function in equation (9)) for
parsing problems. Ratnaparkhi, Roukos, and Ward (1994) use feature selection
techniques for the task. Johnson et al (1999) and Riezler et al (2002) do not use a
feature selection technique, employing instead an objective function which includes a
Table 4
Values of Savings (a, b) for various values of a, b.
a?b Savings (a, b)
1?100,000 2,692.7
1?10 48.6
11?100 83.5
101?1,000 280.0
1,001?10,000 1,263.9
10,001?50,000 2,920.2
50,001?100,000 4,229.8
Collins and Koo Discriminative Reranking for NLP
Gaussian prior on the parameter values, thereby penalizing parameter values which
become too large:
a? ? arg min
a ?LogLoss?a?? ? X
k?0 : : :m
a2k
7
2
k
? ?28?
Closed-form updates under iterative scaling are not possible with this objective
function; instead, optimization algorithms such as gradient descent or conjugate
gradient methods are used to estimate parameter values.
In more recent work, Lafferty, McCallum, and Pereira (2001) describe the use of
conditional Markov random fields (CRFs) for tagging tasks such as named entity
recognition or part-of-speech tagging (hidden Markov models are a common method
applied to these tasks). CRFs employ the objective function in equation (28). A key
insight of Lafferty, McCallum, and Pereira (2001) is that when features are of a
significantly local nature, the gradient of the function in equation (28) can be calculated
efficiently using dynamic programming, even in cases in which the set of candidates
involves all possible tagged sequences and is therefore exponential in size. See also Sha
and Pereira (2003) for more recent work on CRFs.
Optimizing a log-linear model with a Gaussian prior (i.e., choosing parameter
values which achieve the global minimum of the objective function in equation (28)) is
a plausible alternative to the feature selection approaches described in the current
article or to the feature selection methods previously applied to log-linear models. The
Gaussian prior (i.e., the
P
k a2k=7
2
k penalty) has been found in practice to be very
effective in combating overfitting of the parameters to the training data (Chen and
Rosenfeld 1999; Johnson et al 1999; Lafferty, McCallum, and Pereira 2001; Riezler et al
2002). The function in equation (28) can be optimized using variants of gradient
descent, which in practice require tens or at most hundreds of passes over the training
data (see, e.g., Sha and Pereira 2003). Thus log-linear models with a Gaussian prior are
likely to be comparable in terms of efficiency to the feature selection approach
described in this article (in the experimental section, we showed that for the parse-
reranking task, the efficient boosting algorithm requires computation that is equivalent
to around 40 passes over the training data).
Note, however, that the two methods will differ considerably in terms of the
sparsity of the resulting reranker. Whereas the feature selection approach leads to
around 11,000 (2%) of the features in our model having nonzero parameter values,
log-linear models with Gaussian priors typically have very few nonzero parameters
(see, e.g., Riezler and Vasserman 2004). This may be important in some domains, for
example, those in which there are a very large number of features and this large
number leads to difficulties in terms of memory requirements or computation time.
6.4 Feature Selection Methods
A number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi
1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al 2003;
Riezler and Vasserman 2004) describe feature selection approaches for log-linear
models applied to NLP problems. Earlier work (Berger, Della Pietra, and Della Pietra
1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997) suggested
methods that added a feature at a time to the model and updated all parameters in the
current model at each step (for more detail, see section 3.3).
60
Computational Linguistics Volume 31, Number 1
61
Assuming that selection of a feature takes one pass over the training set and that
fitting a model takes p passes over the training set, these methods require f  ?p + 1?
passes over the training set, where f is the number of features selected. In our
experiments, f , 10,000. It is difficult to estimate the value for p, but assuming (very
conservatively) that p = 2, selecting 10,000 features would require 30,000 passes over
the training set. This is around 1,000 times as much computation as that required for
the efficient boosting algorithm applied to our data, suggesting that the feature
selection methods in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998),
and Della Pietra, Della Pietra, and Lafferty (1997) are not sufficiently efficient for the
parsing task.
More recent work (McCallum 2003; Zhou et al 2003; Riezler and Vasserman 2004)
has considered methods for speeding up the feature selection methods described in
Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della
Pietra, and Lafferty (1997). McCallum (2003) and Riezler and Vasserman (2004)
describe approaches that add k features at each step, where k is some constant greater
than one. The running time for these methods is therefore O? f  ?p ? 1?=k?. Riezler
and Vasserman (2004) test a variety of values for k, finding that k = 100 gives optimal
performance. McCallum (2003) uses a value of k = 1,000. Zhou et al (2003) use a
different heuristic that avoids having to recompute the gain for every feature at every
iteration.
We would argue that the alternative feature selection methods in the current
article may be preferable on the grounds of both efficiency and simplicity. Even with
large values of k in the approach of McCallum (2003) and Riezler and Vasserman
(2004) (e.g., k = 1,000), the approach we describe is likely to be at least as efficient as
these alternative approaches. In terms of simplicity, the methods in McCallum (2003)
and Riezler and Vasserman (2004) require selection of a number of free parameters
governing the behavior of the algorithm: the value for k, the value for a regularizer
constant (used in both McCallum [2003] and Riezler and Vasserman [2004]), and the
precision with which the model is optimized at each stage of feature selection
(McCallum [2003] describes using ??just a few BFGS iterations?? at each stage). In
contrast, our method requires a single parameter to be chosen (the value for the &
smoothing parameter) and makes a single approximation (that only a single feature is
updated at each round of feature selection). The latter approximation is particularly
important, as it leads to the efficient algorithm in Figure 4, which avoids a pass over
the training set at each iteration of feature selection (note that in sparse feature spaces, f
rounds of feature selection in our approach can take considerably fewer than f passes
over the training set, in contrast to other work on feature selection within log-linear
models).
Note that there are other important differences among the approaches. Both Della
Pietra, Della Pietra, and Lafferty (1997) and McCallum (2003) describe methods that
induce conjunctions of ??base?? features, in a way similar to decision tree learners. Thus
a relatively small number of base features can lead to a very large number of possible
conjoined features. In future work it might be interesting to consider these kinds of
approaches for the parsing problem. Another difference is that both McCallum, and
Riezler and Vasserman, describe approaches that use a regularizer in addition to
feature selection: McCallum uses a two-norm regularizer; Riezler and Vasserman use a
one-norm regularizer.
Finally, note that other feature selection methods have been proposed within the
machine-learning community: for example, ??filter?? methods, in which feature
selection is performed as a preprocessing step before applying a learning method,
Collins and Koo Discriminative Reranking for NLP
and backward selection methods (Koller and Sahami 1996), in which initially all
features are added to the model and features are then incrementally removed from the
model.
6.5 Boosting, Perceptron, and Support Vector Machine Approaches for Ranking
Problems
Freund et al (1998) introduced a formulation of boosting for ranking problems. The
problem we have considered is a special case of the problem in Freund et al (1998), in
that we have considered a binary distinction between candidates (i.e., the best parse
vs. other parses), whereas Freund et al consider learning full or partial orderings over
candidates. The improved algorithm that we introduced in Figure 4 is, however, a new
algorithm that could perhaps be generalized to the full problem of Freund et al (1998);
we leave this to future research.
Altun, Hofmann, and Johnson (2003) and Altun, Johnson, and Hofmann (2003)
describe experiments on tagging tasks using the ExpLoss function, in contrast to the
LogLoss function used in Lafferty, McCallum, and Pereira (2001). Altun, Hofmann,
and Johnson (2003) describe how dynamic programming methods can be used to
calculate gradients of the ExpLoss function even in cases in which the set of candidates
again includes all possible tagged sequences, a set which grows exponentially in size
with the length of the sentence being tagged. Results in Altun, Johnson, and Hofmann
(2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact
on accuracy for the tagging task in question.
Perceptron-based algorithms, or the voted perceptron approach of Freund and
Schapire (1999), are another alternative to boosting and LogLoss methods. See Collins
(2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron
algorithm. Collins (2002b) gives convergence proofs for the methods; Collins (2002a)
directly compares the boosting and perceptron approaches on a named entity task;
and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which
allow representations of parse trees or labeled sequences in very-high-dimensional
spaces.
Shen, Sarkar, and Joshi (2003) describe support vector machine approaches to
ranking problems and apply support vector machines (SVMs) using tree-adjoining
grammar (Joshi, Levy, and Takahashi 1975) features to the parsing data sets we have
described in this article, with good empirical results.
See Collins (2004) for a discussion of many of these methods, including an
overview of statistical bounds for the boosting, perceptron, and SVM methods, as well
as a discussion of the computational issues involved in the different algorithms.
7. Conclusions
This article has introduced a new algorithm, based on boosting approaches in machine
learning, for ranking problems in natural language processing. The approach gives a
13% relative reduction in error on parsing Wall Street Journal data. While in this article
the experimental focus has been on parsing, many other problems in natural language
processing or speech recognition can also be framed as reranking problems, so the
methods described should be quite broadly applicable. The boosting approach to
ranking has been applied to named entity segmentation (Collins 2002a) and natural
language generation (Walker, Rambow, and Rogati 2001). The key characteristics of
the approach are the use of global features and of a training criterion (optimization
62
Computational Linguistics Volume 31, Number 1
63
problem) that is discriminative and closely related to the task at hand (i.e., parse
accuracy).
In addition, the article introduced a new algorithm for the boosting approach
which takes advantage of the sparse nature of the feature space in the parsing data that
we use. Other NLP tasks are likely to have similar characteristics in terms of sparsity.
Experiments show an efficiency gain of a factor of over 2,600 on the parsing data for
the new algorithm over the obvious implementation of the boosting approach. We
would argue that the improved boosting algorithm is a natural alternative to
maximum-entropy or (conditional) log-linear models. The article has drawn connec-
tions between boosting and maximum-entropy models in terms of the optimization
problems that they involve, the algorithms used, their relative efficiency, and their
performance in empirical tests.
Appendix A: Derivation of Updates for ExpLoss
This appendix gives a derivation of the optimal updates for ExpLoss. The derivation is
very close to that in Schapire and Singer (1999). Recall that for parameter values a?, we
need to compute BestWt?k, a?? and BestLoss?k, a?? for k ? 1, . . . , m, where
BestWt?k, a?? ? arg min
d
ExpLoss?Upd?a?, k, d??
and
BestLoss?k, a?? ? ExpLoss?Upd?a?, k, BestWt?k, a????
The first thing to note is that an update in parameters from a? to Upd?a?, k, d??
results in a simple additive update to the ranking function F:
F?xi,j, Upd?a?, k, d?? ? F?xi,j,a? ? dhk?xi,j?
It follows that the margin on example ?i, j? also has a simple update:
Mi,j?Upd?a?, k, d?? ? F?xi,1, Upd?a?, k, d??  F?xi,j, Upd?a?, k, d??
? F?xi,1, a??  F?xi,j, a?? ? d?hk?xi,1?  hk?xi,j?
? Mi,j?a?? ? d?hk?xi,1?  hk?xi,j?
The updated ExpLoss function can then be written as
ExpLoss ?Upd?a?,k, d?? ?
X
i
Xni
j?2
Si,jeMi,j?Upd?a?, k, d??
?
X
i
Xni
j?2
Si,jeMi,j?a?d?hk?xi,1?hk?xi,j?
Next, we note that ?hk?xi,1?  hk?xi,j? can take on three values: +1, 1, or 0. We split the
training sample into three sets depending on this value:
A?k ? f?i,j? : ?hk?xi,1?  hk?xi,j? ? 1g
Collins and Koo Discriminative Reranking for NLP
Ak ? f?i,j? : ?hk?xi,1?  hk?xi,j? ? 1g
A0k ? f?i,j? : ?hk?xi,1?  hk?xi,j? ? 0g
Given these definitions, we define Wk
+, Wk
, and Wk
0 as
W?k ?
X
?i,j?&A?k
Si,jeMi,j?a??
Wk ?
X
?i,j?&Ak
Si,jeMi,j?a??
W0k ?
X
?i,j?&A0k
Si,jeMi,j?a??
ExpLoss is now rewritten in terms of these quantities:
ExpLoss?Upd?a?, k, d?? ?
X
?i,j?&A?k
Si,jeMi,j?a??d ?
X
?i,j?&Ak
Si,jeMi,j?a???d ?
X
?i,j?&A0k
Si,jeMi,j?a??
? edW?k ? e
dWk ? W0k ?A:1?
To find the value of d that minimizes this loss, we set the derivative of (A.1) with
respect to d to zero, giving the following solution:
BestWt?k, a?? ? 1
2
log
W?k
Wk
Plugging this value of d back into (A.1) gives the best loss:
BestLoss?k, a?? ? 2
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
W?k W

k
q
? W0k
? 2
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
W?k W

k
q
? Z  W?k  Wk
? Z 
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q 2
?A:2?
where Z ? ExpLoss?a?? ?
P
i
Pni
j?2 Si,je
Mi,j?a?? is a constant (for constant a?) which
appears in the BestLoss for all features and therefore does not affect their ranking.
Appendix B: An Alternative Method for LogLoss
In this appendix we sketch an alternative approach for feature selection in LogLoss
that is potentially an efficient method, at the cost of introducing an approximation
64
Computational Linguistics Volume 31, Number 1
65
in the feature selection method. Until now, we have defined BestLoss?k, a?? to be the
minimum of the loss given that the kth feature is updated an optimal amount:
BestLoss?k, a?? ? min
d
LogLoss?Upd?a?,k, d??
In this section we sketch a different approach, based on results from Collins, Schapire,
and Singer (2002), which leads to an algorithm very similar to that for ExpLoss in
Figures 3 and 4. Take the following definitions (note the similarity to the definitions in
equations (13), (14), (15), and (16), with only the definitions for Wk
+ and Wk
 being
altered):
W?k ?
X
?i,j?&A?k
qi,j, Wk ?
X
?i,j?&Ak
qi,j, where qi,j ?
eMi,j?a??
1 ?
Pni
q?2 e
Mi,q?a?? ?B:1?
BestWt?k, a?? ? 1
2
log
W?k
Wk
?B:2?
BestLoss?k, a?? ? LogLoss?a?? 
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q 2
?B:3?
Note that the ExpLoss computations can be recovered by replacing qi,j in equation
(B.1) with qi,j ? eMi,j?a??. This is the only essential difference between the new
algorithm and the ExpLoss method.
Results from Collins, Schapire and Singer (2002) show that under these definitions
the following guarantee holds:
LogLoss?Upd?a?,k, BestWt?k, a????  BestLoss?k, a??
So it can be seen that the update from a? to Upd?a?, k, BestWt?k, a??? is guaranteed to
decrease LogLoss by at least
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q 2. From these results, the algorithms in Figures 3
and 4 could be altered to take the revised definitions of W?k and W

k into account.
Selecting the feature with the minimum value of BestLoss?k, a?? at each iteration leads
to the largest guaranteed decrease in LogLoss. Note that this is now an approximation,
in that BestLoss?k, a?) is an upper bound on the log-likelihood which may or may not be
tight. There are convergence guarantees for the method, however, in that as the
number of rounds of feature selection goes to infinity, the LogLoss approaches its
minimum value.
The algorithms in Figures 3 and 4 could be modified to take the alternative
definitions of W?k and W

k into account, thereby being modified to optimize LogLoss
instead of ExpLoss. The denominator terms in the qi,j definitions in equation (B.1) may
complicate the algorithms somewhat, but it should still be possible to derive relatively
efficient algorithms using the technique.
For a full derivation of the modified updates and for quite technical convergence
proofs, see Collins, Schapire and Singer (2002). We give a sketch of the argument here.
First, we show that
LogLoss?Upd?a?; k; d??  LogLoss?a? W?k  W

k ? W?k e
d ? Wk ed? ?B:4?
Collins and Koo Discriminative Reranking for NLP
This can be derived as follows (in this derivation we use gk?xi,j? ? hk?xi,1?  hk?xi,j??:
LogLoss?Upd?a?,k, d??
? LogLoss?a?? ? LogLoss?Upd?a?, k, d??  LogLoss?a??
? LogLoss?a?? ?
X
i
log
1 ?
Pni
j?2 e
Mi,j?a??dgk?xi,j?
1 ?
Pni
j?2 e
Mi,j?a??
 !
? LogLoss?a?? ?
X
i
log
1
1 ?
Pni
j?2 e
Mi,j?a?? ?
Pni
j?2 e
Mi,j?a??dgk?xi,j?
1 ?
Pni
j?2 e
Mi,j?a??
 !
? LogLoss?a?? ?
X
i
log 1 
Xni
j?2
qi,j ?
Xni
j?2
qi,jedgk?xi,j?
0
@
1
A
?B:5?
 LogLoss ?a?? 
X
i
Xni
j?2
qi,j ?
X
i
Xni
j?2
qi,jedgk?xi,j?
? LogLoss?a??  ?W0k ? W?k ? W

k ? ? W0k ? W?k e
d ? Wk ed
? LogLoss?a??  W?k  W

k ? W?k e
d ? Wk ed
?B:6?
Equation (B.6) can be derived from equation (B.5) through the bound log?1 + x?  x
for all x.
The second step is to minimize the right-hand side of the bound in equation (B.4)
with respect to d. It can be verified that the minimum is found at
d ? 1
2
log
W?k
Wk
at which value the right-hand side of equation (B.4) is equal to
LogLoss?d? 
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q 2
66
Acknowledgments
Thanks to Rob Schapire and Yoram Singer for
useful discussions on boosting algorithms and
to Mark Johnson for useful discussions about
linear models for parse ranking. Steve Abney
and Fernando Pereira gave useful feedback on
earlier drafts of this work. Finally, thanks to
the anonymous reviewers for several useful
comments.
References
Abney, Steven. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597?618.
Altun, Yasemin, Thomas Hofmann, and
Mark Johnson. 2003. Discriminative
learning for label sequences via boosting.
In Advances in Neural Information Processing
Systems (NIPS 15), Vancouver.
Altun, Yasemin, Mark Johnson, and
Thomas Hofmann. 2003. Loss functions
and optimization methods for
discriminative learning of label sequences.
In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP 2003),
Sapporo, Japan.
Berger, Adam L., Stephen A. Della Pietra, and
Vincent J. Della Pietra. 1996. A maximum
entropy approach to natural language
Computational Linguistics Volume 31, Number 1
67
processing. Computational Linguistics,
22(1):39?71.
Black, Ezra, Frederick Jelinek, John Lafferty,
David Magerman, Robert Mercer, and Salim
Roukos. 1992. Towards history-based
grammars: Using richer models for
probabilistic parsing. In Proceedings of the
Fifth DARPA Speech and Natural Language
Workshop, Harriman, NY.
Charniak, Eugene. 1997. Statistical parsing
with a context-free grammar and word
statistics. Proceedings of the 14th National
Conference on Artificial Intelligence, Menlo
Park, CA. AAAI Press/MIT Press.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of NAACL-2000, Seattle.
Chen, Stanley F., and Ronald Rosenfeld.
1999. A gaussian prior for smoothing
maximum entropy models. Technical
Report CMU-CS-99-108, Computer
Science Department, Carnegie Mellon
University.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics
and Eighth Conference of the European Chapter
of the Association for Computational
Linguistics, pages 16?23, Madrid.
Collins, Michael. 1999. Head-Driven
Statistical Models for Natural Language
Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
In Proceedings of the 17th International
Conference on Machine Learning (ICML 2000),
Stanford, CA. Morgan Kaufmann,
San Francisco.
Collins, Michael. 2002a. Ranking algorithms
for named-entity extraction: Boosting and
the voted perceptron. In ACL 2002:
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
Philadelphia.
Collins, Michael. 2002b. Discriminative
training methods for hidden Markov
models: Theory and experiments with the
perceptron algorithm. In Proceedings of
EMNLP 2002, Philadelphia.
Collins, Michael. 2004. Parameter estimation
for statistical parsing models: Theory and
practice of distribution-free methods. In
Harry Bunt, John Carroll, and Giorgio Satta,
editors, New Developments in Parsing
Technology. Kluwer.
Collins, Michael and Nigel Duffy. 2001.
Convolution kernels for natural language.
In Advances in Neural Information Processing
Systems (NIPS 14), Vancouver.
Collins, Michael, and Nigel Duffy. 2002.
New ranking algorithms for parsing and
tagging: Kernels over discrete structures,
and the voted perceptron. In ACL 2002:
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
Philadelphia.
Collins, Michael, Robert E. Schapire, and
Yoram Singer. 2002. Logistic regression,
AdaBoost and Bregman distances. Machine
Learning, 48(1/2/3):253?285.
Della Pietra, Stephen, Vincent Della Pietra,
and John Lafferty. 1997. Inducing features
of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
19(4):380?393.
Duffy, Nigel and David Helmbold. 1999.
Potential boosters? In Advances in
Neural Information Processing Systems
(NIPS 12), Denver.
Freund, Yoav, Raj Iyer, Robert E. Schapire,
and Yoram Singer. 1998. An efficient
boosting algorithm for combining
preferences. In Machine Learning:
Proceedings of the 15th International
Conference, Madison, WI.
Freund, Yoav and Robert E. Schapire. 1997. A
decision-theoretic generalization of on-line
learning and an application to boosting.
Journal of Computer and System Sciences,
55(1):119?139.
Freund, Yoav and Robert E. Schapire. 1999.
Large margin classification using the
perceptron algorithm. Machine Learning,
37(3):277?296.
Friedman, Jerome H., Trevor Hastie, and
Robert Tibshirani. 2000. Additive logistic
regression: A statistical view of boosting.
Annals of Statistics, 38(2):337?374.
Henderson, James. 2003. Inducing history
representations for broad coverage
statistical parsing. In Proceedings of the Joint
Meeting of the North American Chapter of the
Association for Computational Linguistics and
the Human Language Technology Conference
(HLT-NAACL 2003), pages 103?110,
Edmonton, Alberta, Canada.
Hoffgen, Klauss U., Kevin S. van Horn, and
Hans U. Simon. 1995. Robust trainability of
single neurons. Journal of Computer and
System Sciences, 50:114?125.
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic
??unification-based?? grammars. In
Proceedings of ACL 1999, College
Park, MD.
Collins and Koo Discriminative Reranking for NLP
68
Joshi, Aravind K., Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars.
Journal of Computer and System Science 10,
no. 1:136?163.
Koller, Daphne, and Mehran Sahami. 1996.
Toward optimal feature selection. In
Proceedings of the 13th International
Conference on Machine Learning (ICML),
pages 284?292, Bari, Italy, July.
Lafferty, John. 1999. Additive models,
boosting, and inference for generalized
divergences. In Proceedings of the 12th
Annual Conference on Computational Learning
Theory (COLT?99), Santa Cruz, CA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of ICML 2001,
Williamstown, MA.
Lebanon, Guy and John Lafferty. 2001.
Boosting and maximum likelihood for
exponential models. In Advances in
Neural Information Processing Systems
(NIPS 14), Vancouver.
Malouf, Robert. 2002. A comparison of
algorithms for maximum entropy
parameter estimation. In Proceedings of the
Sixth Conference on Natural Language
Learning (CoNNL-2002), Taipei, Taiwan.
Marcus, Mitchell, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19(2):313?330.
Mason, Llew, Peter L. Bartlett, and Jonathan
Baxter. 1999. Direct optimization of margins
improves generalization in combined
classifiers. In Advances in Neural Information
Processing Systems (NIPS 12), Denver.
McCallum, Andrew. 2003. Efficiently
inducing features of conditional random
fields. In Proceedings of the Conference on
Uncertainty in Artificial Intelligence (UAI
2003), Acapulco.
Och, Franz Josef, and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In ACL 2002: Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 295?302,
Philadelphia.
Papineni, Kishore A., Salim Roukos, and R. T.
Ward. 1997. Feature-based language
understanding. In Proceedings of
EuroSpeech?97, vol. 3, pages 1435?1438,
Rhodes, Greece.
Papineni, Kishore A., Salim Roukos, and
R. T. Ward. 1998. Maximum likelihood
and discriminative training of direct
translation models. In Proceedings of the
1998 IEEE International Conference on
Acoustics, Speech and Signal Processing,
vol. 1, pages 189?192, Seattle.
Pearl, Judea. 1988. Probabilistic Reasoning in
Intelligent Systems. Morgan Kaufmann, San
Mateo, CA.
Ratnaparkhi, Adwait. 1997. A linear observed
time statistical parser based on maximum
entropy models. In Proceedings of the Second
Conference on Empirical Methods in Natural
Language Processing, Brown University,
Providence, RI.
Ratnaparkhi, Adwait. 1998. Maximum Entropy
Models for Natural Language Ambiguity
Resolution. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
Ratnaparkhi, Adwait, Salim Roukos, and
R. T. Ward. 1994. A maximum entropy
model for parsing. In Proceedings of the
International Conference on Spoken Language
Processing, pages 803?806, Yokohama,
Japan.
Riezler, Stefan, Tracy H. King,
Ronald M. Kaplan, Richard Crouch,
John T. Maxwell III, and Mark Johnson.
2002. Parsing the Wall Street Journal
using a lexical-functional grammar
and discriminative estimation
techniques. In ACL 2002: Proceedings
of the 40th Annual Meeting of the
Association for Computational Linguistics,
Philadelphia.
Riezler, Stefan and Alexander Vasserman.
2004. Incremental feature selection and
l1 regularization for relaxed
maximum-entropy modeling. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP?04),
Barcelona, Spain.
Rosenfeld, Ronald. 1997. A whole sentence
maximum entropy language model. In
Proceedings of the IEEE Workshop on
Speech Recognition and Understanding,
Santa Barbara, CA, December.
Schapire, Robert E., Yoav Freund, Peter
Bartlett, and W. S. Lee. 1998. Boosting the
margin: A new explanation for the
effectiveness of voting methods. Annals of
Statistics, 26(5):1651?1686.
Schapire, Robert E. and Yoram Singer. 1999.
Improved boosting algorithms using
confidence-rated predictions. Machine
Learning, 37(3):297?336.
Schapire, Robert E. and Yoram Singer. 2000.
BoosTexter: A boosting-based system for
text categorization. Machine Learning,
39(2/3):135?168.
Computational Linguistics Volume 31, Number 1
69
Sha, Fei and Fernando Pereira. 2003. Shallow
parsing with conditional random fields. In
Proceedings of HLT-NAACL 2003,
Edmonton, Alberta, Canada.
Shen, Libin, Anoop Sarkar, and Aravind K.
Joshi. 2003. Using LTAG based features in
parse reranking. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2003),
Sapporo, Japan.
Valiant, Leslie G. 1984. A theory of the
learnable. Communications of the ACM,
27(11):1134?1142.
Walker, Marilyn, Owen Rambow, and
Monica Rogati. 2001. SPoT: A trainable
sentence planner. In Proceedings of the Second
Meeting of the North American Chapter of the
Association for Computational
Linguistics (NAACL 2001), Pittsburgh.
Zhou, Yaqian, Fuliang Weng, Lide Wu, and
Hauke Schmidt. 2003. A fast algorithm for
feature selection in conditional maximum
entropy modeling. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2003),
Sapporo, Japan.
Collins and Koo Discriminative Reranking for NLP

Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Simple Semi-supervised Dependency Parsing
Terry Koo, Xavier Carreras, and Michael Collins
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,carreras,mcollins}@csail.mit.edu
Abstract
We present a simple and effective semi-
supervised method for training dependency
parsers. We focus on the problem of lex-
ical representation, introducing features that
incorporate word clusters derived from a large
unannotated corpus. We demonstrate the ef-
fectiveness of the approach in a series of de-
pendency parsing experiments on the Penn
Treebank and Prague Dependency Treebank,
and we show that the cluster-based features
yield substantial gains in performance across
a wide range of conditions. For example, in
the case of English unlabeled second-order
parsing, we improve from a baseline accu-
racy of 92.02% to 93.16%, and in the case
of Czech unlabeled second-order parsing, we
improve from a baseline accuracy of 86.13%
to 87.13%. In addition, we demonstrate that
our method also improves performance when
small amounts of training data are available,
and can roughly halve the amount of super-
vised data required to reach a desired level of
performance.
1 Introduction
In natural language parsing, lexical information is
seen as crucial to resolving ambiguous relationships,
yet lexicalized statistics are sparse and difficult to es-
timate directly. It is therefore attractive to consider
intermediate entities which exist at a coarser level
than the words themselves, yet capture the informa-
tion necessary to resolve the relevant ambiguities.
In this paper, we introduce lexical intermediaries
via a simple two-stage semi-supervised approach.
First, we use a large unannotated corpus to define
word clusters, and then we use that clustering to
construct a new cluster-based feature mapping for
a discriminative learner. We are thus relying on the
ability of discriminative learning methods to identify
and exploit informative features while remaining ag-
nostic as to the origin of such features. To demon-
strate the effectiveness of our approach, we conduct
experiments in dependency parsing, which has been
the focus of much recent research?e.g., see work
in the CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007).
The idea of combining word clusters with dis-
criminative learning has been previously explored
by Miller et al (2004), in the context of named-
entity recognition, and their work directly inspired
our research. However, our target task of depen-
dency parsing involves more complex structured re-
lationships than named-entity tagging; moreover, it
is not at all clear that word clusters should have any
relevance to syntactic structure. Nevertheless, our
experiments demonstrate that word clusters can be
quite effective in dependency parsing applications.
In general, semi-supervised learning can be mo-
tivated by two concerns: first, given a fixed amount
of supervised data, we might wish to leverage ad-
ditional unlabeled data to facilitate the utilization of
the supervised corpus, increasing the performance of
the model in absolute terms. Second, given a fixed
target performance level, we might wish to use un-
labeled data to reduce the amount of annotated data
necessary to reach this target.
We show that our semi-supervised approach
yields improvements for fixed datasets by perform-
ing parsing experiments on the Penn Treebank (Mar-
cus et al, 1993) and Prague Dependency Treebank
(Hajic?, 1998; Hajic? et al, 2001) (see Sections 4.1
and 4.3). By conducting experiments on datasets of
varying sizes, we demonstrate that for fixed levels of
performance, the cluster-based approach can reduce
the need for supervised data by roughly half, which
is a substantial savings in data-annotation costs (see
Sections 4.2 and 4.4).
The remainder of this paper is divided as follows:
595
Ms. Haag plays Elianti .*
obj
proot
nmod sbj
Figure 1: An example of a labeled dependency tree. The
tree contains a special token ?*? which is always the root
of the tree. Each arc is directed from head to modifier and
has a label describing the function of the attachment.
Section 2 gives background on dependency parsing
and clustering, Section 3 describes the cluster-based
features, Section 4 presents our experimental results,
Section 5 discusses related work, and Section 6 con-
cludes with ideas for future research.
2 Background
2.1 Dependency parsing
Recent work (Buchholz and Marsi, 2006; Nivre
et al, 2007) has focused on dependency parsing.
Dependency syntax represents syntactic informa-
tion as a network of head-modifier dependency arcs,
typically restricted to be a directed tree (see Fig-
ure 1 for an example). Dependency parsing depends
critically on predicting head-modifier relationships,
which can be difficult due to the statistical sparsity
of these word-to-word interactions. Bilexical depen-
dencies are thus ideal candidates for the application
of coarse word proxies such as word clusters.
In this paper, we take a part-factored structured
classification approach to dependency parsing. For a
given sentence x, let Y(x) denote the set of possible
dependency structures spanning x, where each y ?
Y(x) decomposes into a set of ?parts? r ? y. In the
simplest case, these parts are the dependency arcs
themselves, yielding a first-order or ?edge-factored?
dependency parsing model. In higher-order parsing
models, the parts can consist of interactions between
more than two words. For example, the parser of
McDonald and Pereira (2006) defines parts for sib-
ling interactions, such as the trio ?plays?, ?Elianti?,
and ?.? in Figure 1. The Carreras (2007) parser
has parts for both sibling interactions and grandpar-
ent interactions, such as the trio ?*?, ?plays?, and
?Haag? in Figure 1. These kinds of higher-order
factorizations allow dependency parsers to obtain a
limited form of context-sensitivity.
Given a factorization of dependency structures
into parts, we restate dependency parsing as the fol-
apple pear Apple IBM bought run of in
01
100 101 110 111000 001 010 011
00
0
10
1
11
Figure 2: An example of a Brown word-cluster hierarchy.
Each node in the tree is labeled with a bit-string indicat-
ing the path from the root node to that node, where 0
indicates a left branch and 1 indicates a right branch.
lowing maximization:
PARSE(x;w) = argmax
y?Y(x)
?
r?y
w ? f(x, r)
Above, we have assumed that each part is scored
by a linear model with parameters w and feature-
mapping f(?). For many different part factoriza-
tions and structure domains Y(?), it is possible to
solve the above maximization efficiently, and several
recent efforts have concentrated on designing new
maximization algorithms with increased context-
sensitivity (Eisner, 2000; McDonald et al, 2005b;
McDonald and Pereira, 2006; Carreras, 2007).
2.2 Brown clustering algorithm
In order to provide word clusters for our exper-
iments, we used the Brown clustering algorithm
(Brown et al, 1992). We chose to work with the
Brown algorithm due to its simplicity and prior suc-
cess in other NLP applications (Miller et al, 2004;
Liang, 2005). However, we expect that our approach
can function with other clustering algorithms (as in,
e.g., Li and McCallum (2005)). We briefly describe
the Brown algorithm below.
The input to the algorithm is a vocabulary of
words to be clustered and a corpus of text containing
these words. Initially, each word in the vocabulary
is considered to be in its own distinct cluster. The al-
gorithm then repeatedly merges the pair of clusters
which causes the smallest decrease in the likelihood
of the text corpus, according to a class-based bigram
language model defined on the word clusters. By
tracing the pairwise merge operations, one obtains
a hierarchical clustering of the words, which can be
represented as a binary tree as in Figure 2.
Within this tree, each word is uniquely identified
by its path from the root, and this path can be com-
pactly represented with a bit string, as in Figure 2.
In order to obtain a clustering of the words, we se-
lect all nodes at a certain depth from the root of the
596
hierarchy. For example, in Figure 2 we might select
the four nodes at depth 2 from the root, yielding the
clusters {apple,pear}, {Apple,IBM}, {bought,run},
and {of,in}. Note that the same clustering can be ob-
tained by truncating each word?s bit-string to a 2-bit
prefix. By using prefixes of various lengths, we can
produce clusterings of different granularities (Miller
et al, 2004).
For all of the experiments in this paper, we used
the Liang (2005) implementation of the Brown algo-
rithm to obtain the necessary word clusters.
3 Feature design
Key to the success of our approach is the use of fea-
tures which allow word-cluster-based information to
assist the parser. The feature sets we used are simi-
lar to other feature sets in the literature (McDonald
et al, 2005a; Carreras, 2007), so we will not attempt
to give a exhaustive description of the features in
this section. Rather, we describe our features at a
high level and concentrate on our methodology and
motivations. In our experiments, we employed two
different feature sets: a baseline feature set which
draws upon ?normal? information sources such as
word forms and parts of speech, and a cluster-based
feature set that also uses information derived from
the Brown cluster hierarchy.
3.1 Baseline features
Our first-order baseline feature set is similar to the
feature set of McDonald et al (2005a), and consists
of indicator functions for combinations of words and
parts of speech for the head and modifier of each
dependency, as well as certain contextual tokens.1
Our second-order baseline features are the same as
those of Carreras (2007) and include indicators for
triples of part of speech tags for sibling interactions
and grandparent interactions, as well as additional
bigram features based on pairs of words involved
these higher-order interactions. Examples of base-
line features are provided in Table 1.
1We augment the McDonald et al (2005a) feature set with
backed-off versions of the ?Surrounding Word POS Features?
that include only one neighboring POS tag. We also add binned
distance features which indicate whether the number of tokens
between the head and modifier of a dependency is greater than
2, 5, 10, 20, 30, or 40 tokens.
Baseline Cluster-based
ht,mt hc4,mc4
hw,mw hc6,mc6
hw,ht,mt hc*,mc*
hw,ht,mw hc4,mt
ht,mw,mt ht,mc4
hw,mw,mt hc6,mt
hw,ht,mw,mt ht,mc6
? ? ? hc4,mw
hw,mc4
? ? ?
ht,mt,st hc4,mc4,sc4
ht,mt,gt hc6,mc6,sc6
? ? ? ht,mc4,sc4
hc4,mc4,gc4
? ? ?
Table 1: Examples of baseline and cluster-based feature
templates. Each entry represents a class of indicators for
tuples of information. For example, ?ht,mt? represents
a class of indicator features with one feature for each pos-
sible combination of head POS-tag and modifier POS-
tag. Abbreviations: ht = head POS, hw = head word,
hc4 = 4-bit prefix of head, hc6 = 6-bit prefix of head,
hc* = full bit string of head; mt,mw,mc4,mc6,mc* =
likewise for modifier; st,gt,sc4,gc4,. . . = likewise
for sibling and grandchild.
3.2 Cluster-based features
The first- and second-order cluster-based feature sets
are supersets of the baseline feature sets: they in-
clude all of the baseline feature templates, and add
an additional layer of features that incorporate word
clusters. Following Miller et al (2004), we use pre-
fixes of the Brown cluster hierarchy to produce clus-
terings of varying granularity. We found that it was
nontrivial to select the proper prefix lengths for the
dependency parsing task; in particular, the prefix
lengths used in the Miller et al (2004) work (be-
tween 12 and 20 bits) performed poorly in depen-
dency parsing.2 After experimenting with many dif-
ferent feature configurations, we eventually settled
on a simple but effective methodology.
First, we found that it was helpful to employ two
different types of word clusters:
1. Short bit-string prefixes (e.g., 4?6 bits), which
we used as replacements for parts of speech.
2One possible explanation is that the kinds of distinctions
required in a named-entity recognition task (e.g., ?Alice? versus
?Intel?) are much finer-grained than the kinds of distinctions
relevant to syntax (e.g., ?apple? versus ?eat?).
597
2. Full bit strings,3 which we used as substitutes
for word forms.
Using these two types of clusters, we generated new
features by mimicking the template structure of the
original baseline features. For example, the baseline
feature set includes indicators for word-to-word and
tag-to-tag interactions between the head and mod-
ifier of a dependency. In the cluster-based feature
set, we correspondingly introduce new indicators for
interactions between pairs of short bit-string pre-
fixes and pairs of full bit strings. Some examples
of cluster-based features are given in Table 1.
Second, we found it useful to concentrate on
?hybrid? features involving, e.g., one bit-string and
one part of speech. In our initial attempts, we fo-
cused on features that used cluster information ex-
clusively. While these cluster-only features provided
some benefit, we found that adding hybrid features
resulted in even greater improvements. One possible
explanation is that the clusterings generated by the
Brown algorithm can be noisy or only weakly rele-
vant to syntax; thus, the clusters are best exploited
when ?anchored? to words or parts of speech.
Finally, we found it useful to impose a form of
vocabulary restriction on the cluster-based features.
Specifically, for any feature that is predicated on a
word form, we eliminate this feature if the word
in question is not one of the top-N most frequent
words in the corpus. When N is between roughly
100 and 1,000, there is little effect on the perfor-
mance of the cluster-based feature sets.4 In addition,
the vocabulary restriction reduces the size of the fea-
ture sets to managable proportions.
4 Experiments
In order to evaluate the effectiveness of the cluster-
based feature sets, we conducted dependency pars-
ing experiments in English and Czech. We test the
features in a wide range of parsing configurations,
including first-order and second-order parsers, and
labeled and unlabeled parsers.5
3As in Brown et al (1992), we limit the clustering algorithm
so that it recovers at most 1,000 distinct bit-strings; thus full bit
strings are not equivalent to word forms.
4We used N = 800 for all experiments in this paper.
5In an ?unlabeled? parser, we simply ignore dependency la-
bel information, which is a common simplification.
The English experiments were performed on the
Penn Treebank (Marcus et al, 1993), using a stan-
dard set of head-selection rules (Yamada and Mat-
sumoto, 2003) to convert the phrase structure syn-
tax of the Treebank to a dependency tree represen-
tation.6 We split the Treebank into a training set
(Sections 2?21), a development set (Section 22), and
several test sets (Sections 0,7 1, 23, and 24). The
data partition and head rules were chosen to match
previous work (Yamada and Matsumoto, 2003; Mc-
Donald et al, 2005a; McDonald and Pereira, 2006).
The part of speech tags for the development and test
data were automatically assigned by MXPOST (Rat-
naparkhi, 1996), where the tagger was trained on
the entire training corpus; to generate part of speech
tags for the training data, we used 10-way jackknif-
ing.8 English word clusters were derived from the
BLLIP corpus (Charniak et al, 2000), which con-
tains roughly 43 million words of Wall Street Jour-
nal text.9
The Czech experiments were performed on the
Prague Dependency Treebank 1.0 (Hajic?, 1998;
Hajic? et al, 2001), which is directly annotated
with dependency structures. To facilitate compar-
isons with previous work (McDonald et al, 2005b;
McDonald and Pereira, 2006), we used the train-
ing/development/test partition defined in the corpus
and we also used the automatically-assigned part of
speech tags provided in the corpus.10 Czech word
clusters were derived from the raw text section of
the PDT 1.0, which contains about 39 million words
of newswire text.11
We trained the parsers using the averaged percep-
tron (Freund and Schapire, 1999; Collins, 2002),
which represents a balance between strong perfor-
mance and fast training times. To select the number
6We used Joakim Nivre?s ?Penn2Malt? conversion tool
(http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html). Depen-
dency labels were obtained via the ?Malt? hard-coded setting.
7For computational reasons, we removed a single 249-word
sentence from Section 0.
8That is, we tagged each fold with the tagger trained on the
other 9 folds.
9We ensured that the sentences of the Penn Treebank were
excluded from the text used for the clustering.
10Following Collins et al (1999), we used a coarsened ver-
sion of the Czech part of speech tags; this choice also matches
the conditions of previous work (McDonald et al, 2005b; Mc-
Donald and Pereira, 2006).
11This text was disjoint from the training and test corpora.
598
Sec dep1 dep1c MD1 dep2 dep2c MD2 dep1-L dep1c-L dep2-L dep2c-L
00 90.48 91.57 (+1.09) ? 91.76 92.77 (+1.01) ? 90.29 91.03 (+0.74) 91.33 92.09 (+0.76)
01 91.31 92.43 (+1.12) ? 92.46 93.34 (+0.88) ? 90.84 91.73 (+0.89) 91.94 92.65 (+0.71)
23 90.84 92.23 (+1.39) 90.9 92.02 93.16 (+1.14) 91.5 90.32 91.24 (+0.92) 91.38 92.14 (+0.76)
24 89.67 91.30 (+1.63) ? 90.92 91.85 (+0.93) ? 89.55 90.06 (+0.51) 90.42 91.18 (+0.76)
Table 2: Parent-prediction accuracies on Sections 0, 1, 23, and 24. Abbreviations: dep1/dep1c = first-order parser with
baseline/cluster-based features; dep2/dep2c = second-order parser with baseline/cluster-based features; MD1 = Mc-
Donald et al (2005a); MD2 = McDonald and Pereira (2006); suffix -L = labeled parser. Unlabeled parsers are scored
using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions. Improvements of
cluster-based features over baseline features are shown in parentheses.
of iterations of perceptron training, we performed up
to 30 iterations and chose the iteration which opti-
mized accuracy on the development set. Our feature
mappings are quite high-dimensional, so we elimi-
nated all features which occur only once in the train-
ing data. The resulting models still had very high
dimensionality, ranging from tens of millions to as
many as a billion features.12
All results presented in this section are given
in terms of parent-prediction accuracy, which mea-
sures the percentage of tokens that are attached to
the correct head token. For labeled dependency
structures, both the head token and dependency label
must be correctly predicted. In addition, in English
parsing we ignore the parent-predictions of punc-
tuation tokens,13 and in Czech parsing we retain
the punctuation tokens; this matches previous work
(Yamada and Matsumoto, 2003; McDonald et al,
2005a; McDonald and Pereira, 2006).
4.1 English main results
In our English experiments, we tested eight differ-
ent parsing configurations, representing all possi-
ble choices between baseline or cluster-based fea-
ture sets, first-order (Eisner, 2000) or second-order
(Carreras, 2007) factorizations, and labeled or unla-
beled parsing.
Table 2 compiles our final test results and also
includes two results from previous work by Mc-
Donald et al (2005a) and McDonald and Pereira
(2006), for the purposes of comparison. We note
a few small differences between our parsers and the
12Due to the sparsity of the perceptron updates, however,
only a small fraction of the possible features were active in our
trained models.
13A punctuation token is any token whose gold-standard part
of speech tag is one of {?? ?? : , .}.
parsers evaluated in this previous work. First, the
MD1 and MD2 parsers were trained via the MIRA
algorithm (Crammer and Singer, 2003; Crammer et
al., 2004), while we use the averaged perceptron. In
addition, the MD2 model uses only sibling interac-
tions, whereas the dep2/dep2c parsers include both
sibling and grandparent interactions.
There are some clear trends in the results of Ta-
ble 2. First, performance increases with the order of
the parser: edge-factored models (dep1 and MD1)
have the lowest performance, adding sibling rela-
tionships (MD2) increases performance, and adding
grandparent relationships (dep2) yields even better
accuracies. Similar observations regarding the ef-
fect of model order have also been made by Carreras
(2007).
Second, note that the parsers using cluster-based
feature sets consistently outperform the models us-
ing the baseline features, regardless of model order
or label usage. Some of these improvements can be
quite large; for example, a first-order model using
cluster-based features generally performs as well as
a second-order model using baseline features. More-
over, the benefits of cluster-based feature sets com-
bine additively with the gains of increasing model
order. For example, consider the unlabeled parsers
in Table 2: on Section 23, increasing the model or-
der from dep1 to dep2 results in a relative reduction
in error of roughly 13%, while introducing cluster-
based features from dep2 to dep2c yields an addi-
tional relative error reduction of roughly 14%. As a
final note, all 16 comparisons between cluster-based
features and baseline features shown in Table 2 are
statistically significant.14
14We used the sign test at the sentence level. The comparison
between dep1-L and dep1c-L is significant at p < 0.05, and all
other comparisons are significant at p < 0.0005.
599
Tagger always trained on full Treebank Tagger trained on reduced dataset
Size dep1 dep1c ? dep2 dep2c ?
1k 84.54 85.90 1.36 86.29 87.47 1.18
2k 86.20 87.65 1.45 87.67 88.88 1.21
4k 87.79 89.15 1.36 89.22 90.46 1.24
8k 88.92 90.22 1.30 90.62 91.55 0.93
16k 90.00 91.27 1.27 91.27 92.39 1.12
32k 90.74 92.18 1.44 92.05 93.36 1.31
All 90.89 92.33 1.44 92.42 93.30 0.88
Size dep1 dep1c ? dep2 dep2c ?
1k 80.49 84.06 3.57 81.95 85.33 3.38
2k 83.47 86.04 2.57 85.02 87.54 2.52
4k 86.53 88.39 1.86 87.88 89.67 1.79
8k 88.25 89.94 1.69 89.71 91.37 1.66
16k 89.66 91.03 1.37 91.14 92.22 1.08
32k 90.78 92.12 1.34 92.09 93.21 1.12
All 90.89 92.33 1.44 92.42 93.30 0.88
Table 3: Parent-prediction accuracies of unlabeled English parsers on Section 22. Abbreviations: Size = #sentences in
training corpus; ? = difference between cluster-based and baseline features; other abbreviations are as in Table 2.
4.2 English learning curves
We performed additional experiments to evaluate the
effect of the cluster-based features as the amount
of training data is varied. Note that the depen-
dency parsers we use require the input to be tagged
with parts of speech; thus the quality of the part-of-
speech tagger can have a strong effect on the per-
formance of the parser. In these experiments, we
consider two possible scenarios:
1. The tagger has a large training corpus, while
the parser has a smaller training corpus. This
scenario can arise when tagged data is cheaper
to obtain than syntactically-annotated data.
2. The same amount of labeled data is available
for training both tagger and parser.
Table 3 displays the accuracy of first- and second-
order models when trained on smaller portions of
the Treebank, in both scenarios described above.
Note that the cluster-based features obtain consistent
gains regardless of the size of the training set. When
the tagger is trained on the reduced-size datasets,
the gains of cluster-based features are more pro-
nounced, but substantial improvements are obtained
even when the tagger is accurate.
It is interesting to consider the amount by which
cluster-based features reduce the need for supervised
data, given a desired level of accuracy. Based on
Table 3, we can extrapolate that cluster-based fea-
tures reduce the need for supervised data by roughly
a factor of 2. For example, the performance of the
dep1c and dep2c models trained on 1k sentences is
roughly the same as the performance of the dep1
and dep2 models, respectively, trained on 2k sen-
tences. This approximate data-halving effect can be
observed throughout the results in Table 3.
When combining the effects of model order and
cluster-based features, the reductions in the amount
of supervised data required are even larger. For ex-
ample, in scenario 1 the dep2c model trained on 1k
sentences is close in performance to the dep1 model
trained on 4k sentences, and the dep2c model trained
on 4k sentences is close to the dep1 model trained on
the entire training set (roughly 40k sentences).
4.3 Czech main results
In our Czech experiments, we considered only unla-
beled parsing,15 leaving four different parsing con-
figurations: baseline or cluster-based features and
first-order or second-order parsing. Note that our
feature sets were originally tuned for English pars-
ing, and except for the use of Czech clusters, we
made no attempt to retune our features for Czech.
Czech dependency structures may contain non-
projective edges, so we employ a maximum directed
spanning tree algorithm (Chu and Liu, 1965; Ed-
monds, 1967; McDonald et al, 2005b) as our first-
order parser for Czech. For the second-order pars-
ing experiments, we used the Carreras (2007) parser.
Since this parser only considers projective depen-
dency structures, we ?projectivized? the PDT 1.0
training set by finding, for each sentence, the pro-
jective tree which retains the most correct dependen-
cies; our second-order parsers were then trained with
respect to these projective trees. The development
and test sets were not projectivized, so our second-
order parser is guaranteed to make errors in test sen-
tences containing non-projective dependencies. To
overcome this, McDonald and Pereira (2006) use a
15We leave labeled parsing experiments to future work.
600
dep1 dep1c dep2 dep2c
84.49 86.07 (+1.58) 86.13 87.13 (+1.00)
Table 4: Parent-prediction accuracies of unlabeled Czech
parsers on the PDT 1.0 test set, for baseline features and
cluster-based features. Abbreviations are as in Table 2.
Parser Accuracy
Nivre and Nilsson (2005) 80.1
McDonald et al (2005b) 84.4
Hall and Nova?k (2005) 85.1
McDonald and Pereira (2006) 85.2
dep1c 86.07
dep2c 87.13
Table 5: Unlabeled parent-prediction accuracies of Czech
parsers on the PDT 1.0 test set, for our models and for
previous work.
Size dep1 dep1c ? dep2 dep2c ?
1k 72.79 73.66 0.87 74.35 74.63 0.28
2k 74.92 76.23 1.31 76.63 77.60 0.97
4k 76.87 78.14 1.27 78.34 79.34 1.00
8k 78.17 79.83 1.66 79.82 80.98 1.16
16k 80.60 82.44 1.84 82.53 83.69 1.16
32k 82.85 84.65 1.80 84.66 85.81 1.15
64k 84.20 85.98 1.78 86.01 87.11 1.10
All 84.36 86.09 1.73 86.09 87.26 1.17
Table 6: Parent-prediction accuracies of unlabeled Czech
parsers on the PDT 1.0 development set. Abbreviations
are as in Table 3.
two-stage approximate decoding process in which
the output of their second-order parser is ?deprojec-
tivized? via greedy search. For simplicity, we did
not implement a deprojectivization stage on top of
our second-order parser, but we conjecture that such
techniques may yield some additional performance
gains; we leave this to future work.
Table 4 gives accuracy results on the PDT 1.0
test set for our unlabeled parsers. As in the En-
glish experiments, there are clear trends in the re-
sults: parsers using cluster-based features outper-
form parsers using baseline features, and second-
order parsers outperform first-order parsers. Both of
the comparisons between cluster-based and baseline
features in Table 4 are statistically significant.16 Ta-
ble 5 compares accuracy results on the PDT 1.0 test
set for our parsers and several other recent papers.
16We used the sign test at the sentence level; both compar-
isons are significant at p < 0.0005.
N dep1 dep1c dep2 dep2c
100 89.19 92.25 90.61 93.14
200 90.03 92.26 91.35 93.18
400 90.31 92.32 91.72 93.20
800 90.62 92.33 91.89 93.30
1600 90.87 ? 92.20 ?
All 90.89 ? 92.42 ?
Table 7: Parent-prediction accuracies of unlabeled En-
glish parsers on Section 22. Abbreviations: N = thresh-
old value; other abbreviations are as in Table 2. We
did not train cluster-based parsers using threshold values
larger than 800 due to computational limitations.
dep1-P dep1c-P dep1 dep2-P dep2c-P dep2
77.19 90.69 90.89 86.73 91.84 92.42
Table 8: Parent-prediction accuracies of unlabeled En-
glish parsers on Section 22. Abbreviations: suffix -P =
model without POS; other abbreviations are as in Table 2.
4.4 Czech learning curves
As in our English experiments, we performed addi-
tional experiments on reduced sections of the PDT;
the results are shown in Table 6. For simplicity, we
did not retrain a tagger for each reduced dataset,
so we always use the (automatically-assigned) part
of speech tags provided in the corpus. Note that
the cluster-based features obtain improvements at all
training set sizes, with data-reduction factors simi-
lar to those observed in English. For example, the
dep1c model trained on 4k sentences is roughly as
good as the dep1 model trained on 8k sentences.
4.5 Additional results
Here, we present two additional results which fur-
ther explore the behavior of the cluster-based fea-
ture sets. In Table 7, we show the development-set
performance of second-order parsers as the thresh-
old for lexical feature elimination (see Section 3.2)
is varied. Note that the performance of cluster-based
features is fairly insensitive to the threshold value,
whereas the performance of baseline features clearly
degrades as the vocabulary size is reduced.
In Table 8, we show the development-set perfor-
mance of the first- and second-order parsers when
features containing part-of-speech-based informa-
tion are eliminated. Note that the performance ob-
tained by using clusters without parts of speech is
close to the performance of the baseline features.
601
5 Related Work
As mentioned earlier, our approach was inspired by
the success of Miller et al (2004), who demon-
strated the effectiveness of using word clusters as
features in a discriminative learning approach. Our
research, however, applies this technique to depen-
dency parsing rather than named-entity recognition.
In this paper, we have focused on developing new
representations for lexical information. Previous re-
search in this area includes several models which in-
corporate hidden variables (Matsuzaki et al, 2005;
Koo and Collins, 2005; Petrov et al, 2006; Titov
and Henderson, 2007). These approaches have the
advantage that the model is able to learn different
usages for the hidden variables, depending on the
target problem at hand. Crucially, however, these
methods do not exploit unlabeled data when learn-
ing their representations.
Wang et al (2005) used distributional similarity
scores to smooth a generative probability model for
dependency parsing and obtained improvements in
a Chinese parsing task. Our approach is similar to
theirs in that the Brown algorithm produces clusters
based on distributional similarity, and the cluster-
based features can be viewed as being a kind of
?backed-off? version of the baseline features. How-
ever, our work is focused on discriminative learning
as opposed to generative models.
Semi-supervised phrase structure parsing has
been previously explored by McClosky et al (2006),
who applied a reranked parser to a large unsuper-
vised corpus in order to obtain additional train-
ing data for the parser; this self-training appraoch
was shown to be quite effective in practice. How-
ever, their approach depends on the usage of a
high-quality parse reranker, whereas the method de-
scribed here simply augments the features of an ex-
isting parser. Note that our two approaches are com-
patible in that we could also design a reranker and
apply self-training techniques on top of the cluster-
based features.
6 Conclusions
In this paper, we have presented a simple but effec-
tive semi-supervised learning approach and demon-
strated that it achieves substantial improvement over
a competitive baseline in two broad-coverage depen-
dency parsing tasks. Despite this success, there are
several ways in which our approach might be im-
proved.
To begin, recall that the Brown clustering algo-
rithm is based on a bigram language model. Intu-
itively, there is a ?mismatch? between the kind of
lexical information that is captured by the Brown
clusters and the kind of lexical information that is
modeled in dependency parsing. A natural avenue
for further research would be the development of
clustering algorithms that reflect the syntactic be-
havior of words; e.g., an algorithm that attempts to
maximize the likelihood of a treebank, according to
a probabilistic dependency model. Alternately, one
could design clustering algorithms that cluster entire
head-modifier arcs rather than individual words.
Another idea would be to integrate the cluster-
ing algorithm into the training algorithm in a limited
fashion. For example, after training an initial parser,
one could parse a large amount of unlabeled text and
use those parses to improve the quality of the clus-
ters. These improved clusters can then be used to
retrain an improved parser, resulting in an overall
algorithm similar to that of McClosky et al (2006).
Setting aside the development of new clustering
algorithms, a final area for future work is the exten-
sion of our method to new domains, such as con-
versational text or other languages, and new NLP
problems, such as machine translation.
Acknowledgments
The authors thank the anonymous reviewers for
their insightful comments. Many thanks also to
Percy Liang for providing his implementation of
the Brown algorithm, and Ryan McDonald for his
assistance with the experimental setup. The au-
thors gratefully acknowledge the following sources
of support. Terry Koo was funded by NSF grant
DMS-0434222 and a grant from NTT, Agmt. Dtd.
6/21/1998. Xavier Carreras was supported by the
Catalan Ministry of Innovation, Universities and
Enterprise, and a grant from NTT, Agmt. Dtd.
6/21/1998. Michael Collins was funded by NSF
grants 0347631 and DMS-0434222.
602
References
P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai,
and R.L. Mercer. 1992. Class-Based n-gram Mod-
els of Natural Language. Computational Linguistics,
18(4):467?479.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task
on Multilingual Dependency Parsing. In Proceedings
of CoNLL, pages 149?164.
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proceedings of
EMNLP-CoNLL, pages 957?961.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.
2000. BLLIP 1987?89 WSJ Corpus Release 1, LDC
No. LDC2000T43. Linguistic Data Consortium.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A Statistical Parser for Czech. In Proceedings of ACL,
pages 505?512.
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP, pages 1?8.
K. Crammer and Y. Singer. 2003. Ultraconservative On-
line Algorithms for Multiclass Problems. Journal of
Machine Learning Research, 3:951?991.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2004. Online Passive-Aggressive Algorithms. In
S. Thrun, L. Saul, and B. Scho?lkopf, editors, NIPS 16,
pages 1229?1236.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Eisner. 2000. Bilexical Grammars and Their Cubic-
Time Parsing Algorithms. In H. Bunt and A. Nijholt,
editors, Advances in Probabilistic and Other Parsing
Technologies, pages 29?62. Kluwer Academic Pub-
lishers.
Y. Freund and R. Schapire. 1999. Large Margin Clas-
sification Using the Perceptron Algorithm. Machine
Learning, 37(3):277?296.
J. Hajic?, E. Hajic?ova?, P. Pajas, J. Panevova, and P. Sgall.
2001. The Prague Dependency Treebank 1.0, LDC
No. LDC2001T10. Linguistics Data Consortium.
J. Hajic?. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In
E. Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?19.
K. Hall and V. Nova?k. 2005. Corrective Modeling for
Non-Projective Dependency Parsing. In Proceedings
of IWPT, pages 42?52.
T. Koo and M. Collins. 2005. Hidden-Variable Models
for Discriminative Reranking. In Proceedings of HLT-
EMNLP, pages 507?514.
W. Li and A. McCallum. 2005. Semi-Supervised Se-
quence Modeling with Syntactic Topic Models. In
Proceedings of AAAI, pages 813?818.
P. Liang. 2005. Semi-Supervised Learning for Natural
Language. Master?s thesis, Massachusetts Institute of
Technology.
M.P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with Latent Annotations. In Proceedings of
ACL, pages 75?82.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective Self-Training for Parsing. In Proceedings of
HLT-NAACL, pages 152?159.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proceedings of EACL, pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line Large-Margin Training of Dependency Parsers. In
Proceedings of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-Projective Dependency Parsing using Spanning
Tree Algorithms. In Proceedings of HLT-EMNLP,
pages 523?530.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
Tagging with Word Clusters and Discriminative Train-
ing. In Proceedings of HLT-NAACL, pages 337?342.
J. Nivre and J. Nilsson. 2005. Pseudo-Projective Depen-
dency Parsing. In Proceedings of ACL, pages 99?106.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. In Proceedings
of EMNLP-CoNLL 2007, pages 915?932.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings of COLING-ACL, pages
433?440.
A. Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-Of-Speech Tagging. In Proceedings of EMNLP,
pages 133?142.
I. Titov and J. Henderson. 2007. Constituent Parsing
with Incremental Sigmoid Belief Networks. In Pro-
ceedings of ACL, pages 632?639.
Q.I. Wang, D. Schuurmans, and D. Lin. 2005. Strictly
Lexical Dependency Parsing. In Proceedings of IWPT,
pages 152?159.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis With Support Vector Machines. In
Proceedings of IWPT, pages 195?206.
603
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1288?1298,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Dual Decomposition for Parsing with Non-Projective Head Automata
Terry Koo Alexander M. Rush Michael Collins Tommi Jaakkola David Sontag
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,srush,mcollins,tommi,dsontag}@csail.mit.edu
Abstract
This paper introduces algorithms for non-
projective parsing based on dual decomposi-
tion. We focus on parsing algorithms for non-
projective head automata, a generalization of
head-automata models to non-projective struc-
tures. The dual decomposition algorithms are
simple and efficient, relying on standard dy-
namic programming and minimum spanning
tree algorithms. They provably solve an LP
relaxation of the non-projective parsing prob-
lem. Empirically the LP relaxation is very of-
ten tight: for many languages, exact solutions
are achieved on over 98% of test sentences.
The accuracy of our models is higher than pre-
vious work on a broad range of datasets.
1 Introduction
Non-projective dependency parsing is useful for
many languages that exhibit non-projective syntactic
structures. Unfortunately, the non-projective parsing
problem is known to be NP-hard for all but the sim-
plest models (McDonald and Satta, 2007). There has
been a long history in combinatorial optimization of
methods that exploit structure in complex problems,
using methods such as dual decomposition or La-
grangian relaxation (Lemare?chal, 2001). Thus far,
however, these methods are not widely used in NLP.
This paper introduces algorithms for non-
projective parsing based on dual decomposition. We
focus on parsing algorithms for non-projective head
automata, a generalization of the head-automata
models of Eisner (2000) and Alshawi (1996) to non-
projective structures. These models include non-
projective dependency parsing models with higher-
order (e.g., sibling and/or grandparent) dependency
relations as a special case. Although decoding of full
parse structures with non-projective head automata
is intractable, we leverage the observation that key
components of the decoding can be efficiently com-
puted using combinatorial algorithms. In particular,
1. Decoding for individual head-words can be ac-
complished using dynamic programming.
2. Decoding for arc-factored models can be ac-
complished using directed minimum-weight
spanning tree (MST) algorithms.
The resulting parsing algorithms have the following
properties:
? They are efficient and easy to implement, relying
on standard dynamic programming and MST al-
gorithms.
? They provably solve a linear programming (LP)
relaxation of the original decoding problem.
? Empirically the algorithms very often give an ex-
act solution to the decoding problem, in which
case they also provide a certificate of optimality.
In this paper we first give the definition for non-
projective head automata, and describe the parsing
algorithm. The algorithm can be viewed as an in-
stance of Lagrangian relaxation; we describe this
connection, and give convergence guarantees for the
method. We describe a generalization to models
that include grandparent dependencies. We then in-
troduce a perceptron-driven training algorithm that
makes use of point 1 above.
We describe experiments on non-projective pars-
ing for a number of languages, and in particu-
lar compare the dual decomposition algorithm to
approaches based on general-purpose linear pro-
gramming (LP) or integer linear programming (ILP)
solvers (Martins et al, 2009). The accuracy of our
models is higher than previous work on a broad
range of datasets. The method gives exact solutions
to the decoding problem, together with a certificate
of optimality, on over 98% of test examples for many
of the test languages, with parsing times ranging be-
tween 0.021 seconds/sentence for the most simple
languages/models, to 0.295 seconds/sentence for the
1288
most complex settings. The method compares favor-
ably to previous work using LP/ILP formulations,
both in terms of efficiency, and also in terms of the
percentage of exact solutions returned.
While the focus of the current paper is on non-
projective dependency parsing, the approach opens
up new ways of thinking about parsing algorithms
for lexicalized formalisms such as TAG (Joshi and
Schabes, 1997), CCG (Steedman, 2000), and pro-
jective head automata.
2 Related Work
McDonald et al (2005) describe MST-based parsing
for non-projective dependency parsing models with
arc-factored decompositions; McDonald and Pereira
(2006) make use of an approximate (hill-climbing)
algorithm for parsing with more complex models.
McDonald and Pereira (2006) and McDonald and
Satta (2007) describe complexity results for non-
projective parsing, showing that parsing for a variety
of models is NP-hard. Riedel and Clarke (2006) de-
scribe ILP methods for the problem; Martins et al
(2009) recently introduced alternative LP and ILP
formulations. Our algorithm differs in that we do not
use general-purpose LP or ILP solvers, instead using
an MST solver in combination with dynamic pro-
gramming; thus we leverage the underlying struc-
ture of the problem, thereby deriving more efficient
decoding algorithms.
Both dual decomposition and Lagrangian relax-
ation have a long history in combinatorial optimiza-
tion. Our work was originally inspired by recent
work on dual decomposition for inference in graph-
ical models (Wainwright et al, 2005; Komodakis
et al, 2007). However, the non-projective parsing
problem has a very different structure from these
models, and the decomposition we use is very dif-
ferent in nature from those used in graphical mod-
els. Other work has made extensive use of de-
composition approaches for efficiently solving LP
relaxations for graphical models (e.g., Sontag et
al. (2008)). Methods that incorporate combinato-
rial solvers within loopy belief propagation (LBP)
(Duchi et al, 2007; Smith and Eisner, 2008) are
also closely related to our approach. Unlike LBP,
our method has strong theoretical guarantees, such
as guaranteed convergence and the possibility of a
certificate of optimality.
Finally, in other recent work, Rush et al (2010)
describe dual decomposition approaches for other
NLP problems.
3 Sibling Models
This section describes a particular class of models,
sibling models; the next section describes a dual-
decomposition algorithm for decoding these models.
Consider the dependency parsing problem for a
sentence with n words. We define the index set
for dependency parsing to be I = {(i, j) : i ?
{0 . . . n}, j ? {1 . . . n}, i 6= j}. A dependency
parse is a vector y = {y(i, j) : (i, j) ? I}, where
y(i, j) = 1 if a dependency with head word i and
modifier j is in the parse, 0 otherwise. We use i = 0
for the root symbol. We define Y to be the set of all
well-formed non-projective dependency parses (i.e.,
the set of directed spanning trees rooted at node 0).
Given a function f : Y 7? R that assigns scores to
parse trees, the optimal parse is
y? = argmax
y?Y
f(y) (1)
A particularly simple definition of f(y) is f(y) =
?
(i,j)?I y(i, j)?(i, j) where ?(i, j) is the score for
dependency (i, j). Models with this form are often
referred to as arc-factored models. In this case the
optimal parse tree y? can be found efficiently using
MST algorithms (McDonald et al, 2005).
This paper describes algorithms that compute y?
for more complex definitions of f(y); in this sec-
tion, we focus on algorithms for models that capture
interactions between sibling dependencies. To this
end, we will find it convenient to define the follow-
ing notation. Given a vector y, define
y|i = {y(i, j) : j = 1 . . . n, j 6= i}
Hence y|i specifies the set of modifiers to word i;
note that the vectors y|i for i = 0 . . . n form a parti-
tion of the full set of variables.
We then assume that f(y) takes the form
f(y) =
n?
i=0
fi(y|i) (2)
Thus f(y) decomposes into a sum of terms, where
each fi considers modifiers to the i?th word alone.
In the general case, finding y? =
argmaxy?Y f(y) under this definition of f(y)
is an NP-hard problem. However for certain
1289
definitions of fi, it is possible to efficiently compute
argmaxy|i?Zi fi(y|i) for any value of i, typically
using dynamic programming. (Here we use Zi to
refer to the set of all possible values for y|i: specifi-
cally, Z0 = {0, 1}n and for i 6= 0, Zi = {0, 1}n?1.)
In these cases we can efficiently compute
z? = argmax
z?Z
f(z) = argmax
z?Z
?
i
fi(z|i) (3)
where Z = {z : z|i ? Zi for i = 0 . . . n} by
simply computing z?|i = argmaxz|i?Zi fi(z|i) for
i = 0 . . . n. Eq. 3 can be considered to be an approx-
imation to Eq. 1, where we have replaced Y with
Z . We will make direct use of this approximation
in the dual decomposition parsing algorithm. Note
that Y ? Z , and in all but trivial cases, Y is a strict
subset of Z . For example, a structure z ? Z could
have z(i, j) = z(j, i) = 1 for some (i, j); it could
contain longer cycles; or it could contain words that
do not modify exactly one head. Nevertheless, with
suitably powerful functions fi?for example func-
tions based on discriminative models?z? may be a
good approximation to y?. Later we will see that
dual decomposition can effectively use MST infer-
ence to rule out ill-formed structures.
We now give the main assumption underlying sib-
ling models:
Assumption 1 (Sibling Decompositions) A model
f(y) satisfies the sibling-decomposition assumption
if: 1) f(y) =
?n
i=0 fi(y|i) for some set of functions
f0 . . . fn. 2) For any i ? {0 . . . n}, for any value
of the variables u(i, j) ? R for j = 1 . . . n, it is
possible to compute
argmax
y|i?Zi
?
?fi(y|i)?
?
j
u(i, j)y(i, j)
?
?
in polynomial time.
The second condition includes additional terms in-
volving u(i, j) variables that modify the scores of
individual dependencies. These terms are benign for
most definitions of fi, in that they do not alter de-
coding complexity. They will be of direct use in the
dual decomposition parsing algorithm.
Example 1: Bigram Sibling Models. Recall that
y|i is a binary vector specifying which words are
modifiers to the head-word i. Define l1 . . . lp to be
the sequence of left modifiers to word i under y|i,
and r1 . . . rq to be the set of right modifiers (e.g.,
consider the case where n = 5, i = 3, and we have
y(3, 1) = y(3, 5) = 0, and y(3, 2) = y(3, 4) = 1:
in this case p = 1, l1 = 2, and q = 1, r1 = 4). In
bigram sibling models, we have
fi(y|i) =
p+1?
k=1
gL(i, lk?1, lk) +
q+1?
k=1
gR(i, rk?1, rk)
where l0 = r0 = START is the initial state, and
lp+1 = rq+1 = END is the end state. The functions
gL and gR assign scores to bigram dependencies to
the left and right of the head. Under this model cal-
culating argmaxy|i?Zi
(
fi(y|i)?
?
j u(i, j)y(i, j)
)
takes O(n2) time using dynamic programming,
hence the model satisfies Assumption 1.
Example 2: Head Automata Head-automata
models constitute a second important model type
that satisfy the sibling-decomposition assumption
(bigram sibling models are a special case of head
automata). These models make use of functions
gR(i, s, s?, r) where s ? S, s? ? S are variables in a
set of possible states S, and r is an index of a word
in the sentence such that i < r ? n. The function
gR returns a cost for taking word r as the next depen-
dency, and transitioning from state s to s?. A similar
function gL is defined for left modifiers. We define
fi(y|i, s0 . . . sq, t0 . . . tp) =
q?
k=1
gR(i, sk?1, sk, rk) +
p?
k=1
gL(i, tk?1, tk, ll)
to be the joint score for dependencies y|i, and left
and right state sequences s0 . . . sq and t0 . . . tp. We
specify that s0 = t0 = START and sq = tp = END.
In this case we define
fi(y|i) = maxs0...sq ,t0...tp
fi(y|i, s0 . . . sq, t0 . . . tp)
and it follows that argmaxy|i?Zi fi(y|i) can be com-
puted inO(n|S|2) time using a variant of the Viterbi
algorithm, hence the model satisfies the sibling-
decomposition assumption.
4 The Parsing Algorithm
We now describe the dual decomposition parsing al-
gorithm for models that satisfy Assumption 1. Con-
sider the following generalization of the decoding
1290
Set u(1)(i, j)? 0 for all (i, j) ? I
for k = 1 to K do
y(k) ? argmax
y?Y
?
(i,j)?I
(
?(i, j) + u(k)(i, j)
)
y(i, j)
for i ? {0 . . . n},
z(k)|i ? argmax
z|i?Zi
(fi(z|i)?
?
j
u(k)(i, j)z(i, j))
if y(k)(i, j) = z(k)(i, j) for all (i, j) ? I then
return (y(k), z(k))
for all (i, j) ? I,
u(k+1)(i, j)? u(k)(i, j)+?k(z(k)(i, j)?y(k)(i, j))
return (y(K), z(K))
Figure 1: The parsing algorithm for sibling decompos-
able models. ?k ? 0 for k = 1 . . .K are step sizes, see
Appendix A for details.
problem from Eq. 1, where f(y) =
?
i fi(y|i),
h(y) =
?
(i,j)?I ?(i, j)y(i, j), and ?(i, j) ? R for
all (i, j):1
argmax
z?Z,y?Y
f(z) + h(y) (4)
such that z(i, j) = y(i, j) for all (i, j) ? I (5)
Although the maximization w.r.t. z is taken over the
set Z , the constraints in Eq. 5 ensure that z = y for
some y ? Y , and hence that z ? Y .
Without the z(i, j) = y(i, j) constraints, the
objective would decompose into the separate max-
imizations z? = argmaxz?Z f(z), and y
? =
argmaxy?Y h(y), which can be easily solved us-
ing dynamic programming and MST, respectively.
Thus, it is these constraints that complicate the op-
timization. Our approach gets around this difficulty
by introducing new variables, u(i, j), that serve to
enforce agreement between the y(i, j) and z(i, j)
variables. In the next section we will show that these
u(i, j) variables are actually Lagrange multipliers
for the z(i, j) = y(i, j) constraints.
Our parsing algorithm is shown in Figure 1. At
each iteration k, the algorithm finds y(k) ? Y us-
ing an MST algorithm, and z(k) ? Z through sep-
arate decoding of the (n + 1) sibling models. The
u(k) variables are updated if y(k)(i, j) 6= z(k)(i, j)
1This is equivalent to Eq. 1 when ?(i, j) = 0 for all (i, j).
In some cases, however, it is convenient to have a model with
non-zero values for the ? variables; see the Appendix. Note that
this definition of h(y) allows argmaxy?Y h(y) to be calculated
efficiently, using MST inference.
for some (i, j); these updates modify the objective
functions for the two decoding steps, and intuitively
encourage the y(k) and z(k) variables to be equal.
4.1 Lagrangian Relaxation
Recall that the main difficulty in solving Eq. 4 was
the z = y constraints. We deal with these con-
straints using Lagrangian relaxation (Lemare?chal,
2001). We first introduce Lagrange multipliers u =
{u(i, j) : (i, j) ? I}, and define the Lagrangian
L(u, y, z) = (6)
f(z) + h(y) +
?
(i,j)?I
u(i, j)
(
y(i, j)? z(i, j)
)
If L? is the optimal value of Eq. 4 subject to the
constraints in Eq. 5, then for any value of u,
L? = max
z?Z,y?Y,y=z
L(u, y, z) (7)
This follows because if y = z, the right term in Eq. 6
is zero for any value of u. The dual objective L(u)
is obtained by omitting the y = z constraint:
L(u) = max
z?Z,y?Y
L(u, y, z)
= max
z?Z
(
f(z)?
?
i,j
u(i, j)z(i, j)
)
+max
y?Y
(
h(y) +
?
i,j
u(i, j)y(i, j)
)
.
Since L(u) maximizes over a larger space (y may
not equal z), we have that L? ? L(u) (compare this
to Eq. 7). The dual problem, which our algorithm
optimizes, is to obtain the tightest such upper bound,
(Dual problem) min
u?R|I|
L(u). (8)
The dual objective L(u) is convex, but not differen-
tiable. However, we can use a subgradient method
to derive an algorithm that is similar to gradient de-
scent, and which minimizes L(u). A subgradient of
a convex function L(u) at u is a vector du such that
for all v ? R|I|, L(v) ? L(u) + du ? (v ? u). By
standard results,
du(k) = y
(k) ? z(k)
is a subgradient for L(u) at u = u(k), where z(k) =
argmaxz?Z f(z)?
?
i,j u
(k)(i, j)z(i, j) and y(k) =
1291
argmaxy?Y h(y) +
?
i,j u
(k)(i, j)y(i, j). Subgra-
dient optimization methods are iterative algorithms
with updates that are similar to gradient descent:
u(k+1) = u(k) ? ?kdu(k) = u
(k) ? ?k(y
(k) ? z(k)),
where ?k is a step size. It is easily verified that the
algorithm in Figure 1 uses precisely these updates.
4.2 Formal Guarantees
With an appropriate choice of the step sizes ?k, the
subgradient method can be shown to solve the dual
problem, i.e.
lim
k??
L(u(k)) = min
u
L(u).
See Korte and Vygen (2008), page 120, for details.
As mentioned before, the dual provides an up-
per bound on the optimum of the primal problem
(Eq. 4),
max
z?Z,y?Y,y=z
f(z) + h(y) ? min
u?R|I|
L(u). (9)
However, we do not necessarily have strong
duality?i.e., equality in the above equation?
because the sets Z and Y are discrete sets. That
said, for some functions h(y) and f(z) strong du-
ality does hold, as stated in the following:
Theorem 1 If for some k ? {1 . . .K} in the al-
gorithm in Figure 1, y(k)(i, j) = z(k)(i, j) for all
(i, j) ? I, then (y(k), z(k)) is a solution to the max-
imization problem in Eq. 4.
Proof. We have that f(z(k)) + h(y(k)) =
L(u(k), z(k), y(k)) = L(u(k)), where the last equal-
ity is because y(k), z(k) are defined as the respective
argmax?s. Thus, the inequality in Eq. 9 is tight, and
(y(k), z(k)) and u(k) are primal and dual optimal.
Although the algorithm is not guaranteed to sat-
isfy y(k) = z(k) for some k, by Theorem 1 if it does
reach such a state, then we have the guarantee of an
exact solution to Eq. 4, with the dual solution u pro-
viding a certificate of optimality. We show in the
experiments that this occurs very frequently, in spite
of the parsing problem being NP-hard.
It can be shown that Eq. 8 is the dual of an LP
relaxation of the original problem. When the con-
ditions of Theorem 1 are satisfied, it means that the
LP relaxation is tight for this instance. For brevity
we omit the details, except to note that when the LP
relaxation is not tight, the optimal primal solution to
the LP relaxation could be recovered by averaging
methods (Nedic? and Ozdaglar, 2009).
5 Grandparent Dependency Models
In this section we extend the approach to consider
grandparent relations. In grandparent models each
parse tree y is represented as a vector
y = {y(i, j) : (i, j) ? I} ? {y?(i, j) : (i, j) ? I}
where we have added a second set of duplicate vari-
ables, y?(i, j) for all (i, j) ? I. The set of all valid
parse trees is then defined as
Y = {y : y(i, j) variables form a directed tree,
y?(i, j) = y(i, j) for all (i, j) ? I}
We again partition the variables into n + 1 subsets,
y|0 . . . y|n, by (re)defining
y|i = {y(i, j) : j = 1 . . . n, j 6= i}
?{y?(k, i) : k = 0 . . . n, k 6= i}
So as before y|i contains variables y(i, j) which in-
dicate which words modify the i?th word. In addi-
tion, y|i includes y?(k, i) variables that indicate the
word that word i itself modifies.
The set of all possible values of y|i is now
Zi = {y|i : y(i, j) ? {0, 1} for j = 1 . . . n, j 6= i;
y?(k, i) ? {0, 1} for k = 0 . . . n, k 6= i;
?
k
y?(k, i) = 1}
Hence the y(i, j) variables can take any values, but
only one of the y?(k, i) variables can be equal to 1
(as only one word can be a parent of word i). As be-
fore, we define Z = {y : y|i ? Zi for i = 0 . . . n}.
We introduce the following assumption:
Assumption 2 (GS Decompositions)
A model f(y) satisfies the grandparent/sibling-
decomposition (GSD) assumption if: 1) f(z) =
?n
i=0 fi(z|i) for some set of functions f0 . . . fn. 2)
For any i ? {0 . . . n}, for any value of the variables
u(i, j) ? R for j = 1 . . . n, and v(k, i) ? R for
k = 0 . . . n, it is possible to compute
argmax
z|i?Zi
(fi(z|i)?
?
j
u(i, j)z(i, j)?
?
k
v(k, i)z?(k, i))
in polynomial time.
1292
Again, it follows that we can approxi-
mate y? = argmaxy?Y
?n
i=0 fi(y|i) by
z? = argmaxz?Z
?n
i=0 fi(z|i), by defining
z?|i = argmaxz|i?Zi fi(z|i) for i = 0 . . . n. The
resulting vector z? may be deficient in two respects.
First, the variables z?(i, j) may not form a well-
formed directed spanning tree. Second, we may
have z??(i, j) 6= z
?(i, j) for some values of (i, j).
Example 3: Grandparent/Sibling Models An
important class of models that satisfy Assumption 2
are defined as follows. Again, for a vector y|i de-
fine l1 . . . lp to be the sequence of left modifiers to
word i under y|i, and r1 . . . rq to be the set of right
modifiers. Define k? to the value for k such that
y?(k, i) = 1. Then the model is defined as follows:
fi(y|i) =
p+1?
j=1
gL(i, k
?, lj?1, lj)+
q+1?
j=1
gR(i, k
?, rj?1, rj)
This is very similar to the bigram-sibling model, but
with the modification that the gL and gR functions
depend in addition on the value for k?. This al-
lows these functions to model grandparent depen-
dencies such as (k?, i, lj) and sibling dependencies
such as (i, lj?1, lj). Finding z?|i under the definition
can be accomplished inO(n3) time, by decoding the
model using dynamic programming separately for
each of the O(n) possible values of k?, and pick-
ing the value for k? that gives the maximum value
under these decodings.
A dual-decomposition algorithm for models that
satisfy the GSD assumption is shown in Figure 2.
The algorithm can be justified as an instance of La-
grangian relaxation applied to the problem
argmax
z?Z,y?Y
f(z) + h(y) (10)
with constraints
z(i, j) = y(i, j) for all (i, j) ? I (11)
z?(i, j) = y(i, j) for all (i, j) ? I (12)
The algorithm employs two sets of Lagrange mul-
tipliers, u(i, j) and v(i, j), corresponding to con-
straints in Eqs. 11 and 12. As in Theorem 1, if at any
point in the algorithm z(k) = y(k), then (z(k), y(k))
is an exact solution to the problem in Eq. 10.
Set u(1)(i, j)? 0, v(1)(i, j)? 0 for all (i, j) ? I
for k = 1 to K do
y(k) ? argmax
y?Y
?
(i,j)?I
y(i, j)?(i, j)
where ?(i, j) = ?(i, j) + u(k)(i, j) + v(k)(i, j).
for i ? {0 . . . n},
z(k)|i ? argmax
z|i?Zi
(fi(z|i) ?
?
j
u(k)(i, j)z(i, j)
?
?
j
v(k)(j, i)z?(j, i))
if y(k)(i, j) = z(k)(i, j) = z(k)? (i, j) for all (i, j) ? I
then
return (y(k), z(k))
for all (i, j) ? I,
u(k+1)(i, j)? u(k)(i, j)+?k(z(k)(i, j)?y(k)(i, j))
v(k+1)(i, j)? v(k)(i, j)+?k(z
(k)
? (i, j)?y
(k)(i, j))
return (y(K), z(K))
Figure 2: The parsing algorithm for grandparent/sibling-
decomposable models.
6 The Training Algorithm
In our experiments we make use of discriminative
linear models, where for an input sentence x, the
score for a parse y is f(y) = w ? ?(x, y) where
w ? Rd is a parameter vector, and ?(x, y) ? Rd
is a feature-vector representing parse tree y in con-
junction with sentence x. We will assume that the
features decompose in the same way as the sibling-
decomposable or grandparent/sibling-decomposable
models, that is ?(x, y) =
?n
i=0 ?(x, y|i) for some
feature vector definition ?(x, y|i). In the bigram sib-
ling models in our experiments, we assume that
?(x, y|i) =
p+1?
k=1
?L(x, i, lk?1, lk) +
q+1?
k=1
?R(x, i, rk?1, rk)
where as before l1 . . . lp and r1 . . . rq are left and
right modifiers under y|i, and where ?L and ?R
are feature vector definitions. In the grandparent
models in our experiments, we use a similar defi-
nition with feature vectors ?L(x, i, k?, lk?1, lk) and
?R(x, i, k?, rk?1, rk), where k? is the parent for
word i under y|i.
We train the model using the averaged perceptron
for structured problems (Collins, 2002). Given the
i?th example in the training set, (x(i), y(i)), the per-
ceptron updates are as follows:
? z? = argmaxy?Z w ? ?(x
(i), y)
? If z? 6= y(i), w = w+?(x(i), y(i))??(x(i), z?)
1293
The first step involves inference over the set Z ,
rather than Y as would be standard in the percep-
tron. Thus, decoding during training can be achieved
by dynamic programming over head automata alone,
which is very efficient.
Our training approach is closely related to local
training methods (Punyakanok et al, 2005). We
have found this method to be effective, very likely
because Z is a superset of Y . Our training algo-
rithm is also related to recent work on training using
outer bounds (see, e.g., (Taskar et al, 2003; Fin-
ley and Joachims, 2008; Kulesza and Pereira, 2008;
Martins et al, 2009)). Note, however, that the LP re-
laxation optimized by dual decomposition is signifi-
cantly tighter than Z . Thus, an alternative approach
would be to use the dual decomposition algorithm
for inference during training.
7 Experiments
We report results on a number of data sets. For
comparison to Martins et al (2009), we perform ex-
periments for Danish, Dutch, Portuguese, Slovene,
Swedish and Turkish data from the CoNLL-X
shared task (Buchholz and Marsi, 2006), and En-
glish data from the CoNLL-2008 shared task (Sur-
deanu et al, 2008). We use the official training/test
splits for these data sets, and the same evaluation
methodology as Martins et al (2009). For com-
parison to Smith and Eisner (2008), we also re-
port results on Danish and Dutch using their alter-
nate training/test split. Finally, we report results on
the English WSJ treebank, and the Prague treebank.
We use feature sets that are very similar to those
described in Carreras (2007). We use marginal-
based pruning, using marginals calculated from an
arc-factored spanning tree model using the matrix-
tree theorem (McDonald and Satta, 2007; Smith and
Smith, 2007; Koo et al, 2007).
In all of our experiments we set the value K, the
maximum number of iterations of dual decompo-
sition in Figures 1 and 2, to be 5,000. If the al-
gorithm does not terminate?i.e., it does not return
(y(k), z(k)) within 5,000 iterations?we simply take
the parse y(k) with the maximum value of f(y(k)) as
the output from the algorithm. At first sight 5,000
might appear to be a large number, but decoding is
still fast?see Sections 7.3 and 7.4 for discussion.2
2Note also that the feature vectors ? and inner productsw ??
The strategy for choosing step sizes ?k is described
in Appendix A, along with other details.
We first discuss performance in terms of accu-
racy, success in recovering an exact solution, and
parsing speed. We then describe additional experi-
ments examining various aspects of the algorithm.
7.1 Accuracy
Table 1 shows results for previous work on the var-
ious data sets, and results for an arc-factored model
with pure MST decoding with our features. (We use
the acronym UAS (unlabeled attachment score) for
dependency accuracy.) We also show results for the
bigram-sibling and grandparent/sibling (G+S) mod-
els under dual decomposition. Both the bigram-
sibling and G+S models show large improvements
over the arc-factored approach; they also compare
favorably to previous work?for example the G+S
model gives better results than all results reported in
the CoNLL-X shared task, on all languages. Note
that we use different feature sets from both Martins
et al (2009) and Smith and Eisner (2008).
7.2 Success in Recovering Exact Solutions
Next, we consider how often our algorithms return
an exact solution to the original optimization prob-
lem, with a certificate?i.e., how often the algo-
rithms in Figures 1 and 2 terminate with y(k) = z(k)
for some value of k < 5000 (and are thus optimal,
by Theorem 1). The CertS and CertG columns in Ta-
ble 1 give the results for the sibling and G+S models
respectively. For all but one setting3 over 95% of the
test sentences are decoded exactly, with 99% exact-
ness in many cases.
For comparison, we also ran both the single-
commodity flow and multiple-commodity flow LP
relaxations of Martins et al (2009) with our mod-
els and features. We measure how often these re-
laxations terminate with an exact solution. The re-
sults in Table 2 show that our method gives exact
solutions more often than both of these relaxations.4
In computing the accuracy figures for Martins et al
only need to be computed once, thus saving computation.
3The exception is Slovene, which has the smallest training
set at only 1534 sentences.
4Note, however, that it is possible that the Martins et al re-
laxations would have given a higher proportion of integral solu-
tions if their relaxation was used during training.
1294
Ma09 MST Sib G+S Best CertS CertG TimeS TimeG TrainS TrainG
Dan 91.18 89.74 91.08 91.78 91.54 99.07 98.45 0.053 0.169 0.051 0.109
Dut 85.57 82.33 84.81 85.81 85.57 98.19 97.93 0.035 0.120 0.046 0.048
Por 92.11 90.68 92.57 93.03 92.11 99.65 99.31 0.047 0.257 0.077 0.103
Slo 85.61 82.39 84.89 86.21 85.61 90.55 95.27 0.158 0.295 0.054 0.130
Swe 90.60 88.79 90.10 91.36 90.60 98.71 98.97 0.035 0.141 0.036 0.055
Tur 76.34 75.66 77.14 77.55 76.36 98.72 99.04 0.021 0.047 0.016 0.036
Eng1 91.16 89.20 91.18 91.59 ? 98.65 99.18 0.082 0.200 0.032 0.076
Eng2 ? 90.29 92.03 92.57 ? 98.96 99.12 0.081 0.168 0.032 0.076
Sm08 MST Sib G+S ? CertS CertG TimeS TimeG TrainS TrainG
Dan 86.5 87.89 89.58 91.00 ? 98.50 98.50 0.043 0.120 0.053 0.065
Dut 88.5 88.86 90.87 91.76 ? 98.00 99.50 0.036 0.046 0.050 0.054
Mc06 MST Sib G+S ? CertS CertG TimeS TimeG TrainS TrainG
PTB 91.5 90.10 91.96 92.46 ? 98.89 98.63 0.062 0.210 0.028 0.078
PDT 85.2 84.36 86.44 87.32 ? 96.67 96.43 0.063 0.221 0.019 0.051
Table 1: A comparison of non-projective automaton-based parsers with results from previous work. MST: Our first-
order baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via
dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al (2009). Sm08:
The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald
and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task
(Buchholz and Marsi, 2006) or in any column of Martins et al (2009, Table 1); note that the latter includes McDonald
and Pereira (2006), Nivre and McDonald (2008), and Martins et al (2008). CertS/CertG: Percent of test examples
for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for
test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing,
test decoding was carried out on identical machines with zero additional load; however, training was conducted on
machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng1: UAS when testing on
the CoNLL-08 validation set, following Martins et al (2009). Eng2: UAS when testing on the CoNLL-08 test set.
(2009), we project fractional solutions to a well-
formed spanning tree, as described in that paper.
Finally, to better compare the tightness of our
LP relaxation to that of earlier work, we consider
randomly-generated instances. Table 2 gives results
for our model and the LP relaxations of Martins et al
(2009) with randomly generated scores on automata
transitions. We again recover exact solutions more
often than the Martins et al relaxations. Note that
with random parameters the percentage of exact so-
lutions is significantly lower, suggesting that the ex-
actness of decoding of the trained models is a special
case. We speculate that this is due to the high perfor-
mance of approximate decoding with Z in place of
Y under the trained models for fi; the training algo-
rithm described in section 6 may have the tendency
to make the LP relaxation tight.
7.3 Speed
Table 1, columns TimeS and TimeG, shows decod-
ing times for the dual decomposition algorithms.
Table 2 gives speed comparisons to Martins et al
(2009). Our method gives significant speed-ups over
 0
 5
 10
 15
 20
 25
 30
 0  1000  2000  3000  4000  5000%
 of 
He
ad 
Au
tom
ata
 Re
com
put
ed
Iterations of Dual Decomposition
% recomputed, g+s% recomputed, sib
Figure 3: The average percentage of head automata that
must be recomputed on each iteration of dual decompo-
sition on the PTB validation set.
the Martins et al (2009) method, presumably be-
cause it leverages the underlying structure of the
problem, rather than using a generic solver.
7.4 Lazy Decoding
Here we describe an important optimization in the
dual decomposition algorithms. Consider the algo-
rithm in Figure 1. At each iteration we must find
z(k)|i = argmax
z|i?Zi
(fi(z|i)?
?
j
u(k)(i, j)z(i, j))
1295
Sib Acc Int Time Rand
LP(S) 92.14 88.29 0.14 11.7
LP(M) 92.17 93.18 0.58 30.6
ILP 92.19 100.0 1.44 100.0
DD-5000 92.19 98.82 0.08 35.6
DD-250 92.23 89.29 0.03 10.2
G+S Acc Int Time Rand
LP(S) 92.60 91.64 0.23 0.0
LP(M) 92.58 94.41 0.75 0.0
ILP 92.70 100.0 1.79 100.0
DD-5000 92.71 98.76 0.23 6.8
DD-250 92.66 85.47 0.12 0.0
Table 2: A comparison of dual decomposition with lin-
ear programs described by Martins et al (2009). LP(S):
Linear Program relaxation based on single-commodity
flow. LP(M): Linear Program relaxation based on
multi-commodity flow. ILP: Exact Integer Linear Pro-
gram. DD-5000/DD-250: Dual decomposition with non-
projective head automata, with K = 5000/250. Upper
results are for the sibling model, lower results are G+S.
Columns give scores for UAS accuracy, percentage of so-
lutions which are integral, and solution speed in seconds
per sentence. These results are for Section 22 of the PTB.
The last column is the percentage of integral solutions on
a random problem of length 10 words. The (I)LP experi-
ments were carried out using Gurobi, a high-performance
commercial-grade solver.
for i = 0 . . . n. However, if for some i, u(k)(i, j) =
u(k?1)(i, j) for all j, then z(k)|i = z
(k?1)
|i . In
lazy decoding we immediately set z(k)|i = z
(k?1)
|i if
u(k)(i, j) = u(k?1)(i, j) for all j; this check takes
O(n) time, and saves us from decoding with the i?th
automaton. In practice, the updates to u are very
sparse, and this condition occurs very often in prac-
tice. Figure 3 demonstrates the utility of this method
for both sibling automata and G+S automata.
7.5 Early Stopping
We also ran experiments varying the value of K?
the maximum number of iterations?in the dual de-
composition algorithms. As before, if we do not find
y(k) = z(k) for some value of k ? K, we choose
the y(k) with optimal value for f(y(k)) as the final
solution. Figure 4 shows three graphs: 1) the accu-
racy of the parser on PTB validation data versus the
value for K; 2) the percentage of examples where
y(k) = z(k) at some point during the algorithm,
hence the algorithm returns a certificate of optimal-
ity; 3) the percentage of examples where the solution
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
Pe
rce
nta
ge
Maximum Number of Dual Decomposition Iterations
% validation UAS% certificates% match K=5000
Figure 4: The behavior of the dual-decomposition parser
with sibling automata as the value of K is varied.
Sib P-Sib G+S P-G+S
PTB 92.19 92.34 92.71 92.70
PDT 86.41 85.67 87.40 86.43
Table 3: UAS of projective and non-projective decoding
for the English (PTB) and Czech (PDT) validation sets.
Sib/G+S: as in Table 1. P-Sib/P-G+S: Projective versions
of Sib/G+S, where the MST component has been re-
placed with the Eisner (2000) first-order projective parser.
returned is the same as the solution for the algorithm
with K = 5000 (our original setting). It can be seen
for K as small as 250 we get very similar accuracy
to K = 5000 (see Table 2). In fact, for this set-
ting the algorithm returns the same solution as for
K = 5000 on 99.59% of the examples. However
only 89.29% of these solutions are produced with a
certificate of optimality (y(k) = z(k)).
7.6 How Good is the Approximation z??
We ran experiments measuring the quality of z? =
argmaxz?Z f(z), where f(z) is given by the
perceptron-trained bigram-sibling model. Because
z? may not be a well-formed tree with n dependen-
cies, we report precision and recall rather than con-
ventional dependency accuracy. Results on the PTB
validation set were 91.11%/88.95% precision/recall,
which is accurate considering the unconstrained na-
ture of the predictions. Thus the z? approximation is
clearly a good one; we suspect that this is one reason
for the good convergence results for the method.
7.7 Importance of Non-Projective Decoding
It is simple to adapt the dual-decomposition algo-
rithms in figures 1 and 2 to give projective depen-
dency structures: the set Y is redefined to be the set
1296
of all projective structures, with the argmax over Y
being calculated using a projective first-order parser
(Eisner, 2000). Table 3 shows results for projec-
tive and non-projective parsing using the dual de-
composition approach. For Czech data, where non-
projective structures are common, non-projective
decoding has clear benefits. In contrast, there is little
difference in accuracy between projective and non-
projective decoding on English.
8 Conclusions
We have described dual decomposition algorithms
for non-projective parsing, which leverage existing
dynamic programming and MST algorithms. There
are a number of possible areas for future work. As
described in section 7.7, the algorithms can be easily
modified to consider projective structures by replac-
ing Y with the set of projective trees, and then using
first-order dependency parsing algorithms in place
of MST decoding. This method could be used to
derive parsing algorithms that include higher-order
features, as an alternative to specialized dynamic
programming algorithms. Eisner (2000) describes
extensions of head automata to include word senses;
we have not discussed this issue in the current pa-
per, but it is simple to develop dual decomposition
algorithms for this case, using similar methods to
those used for the grandparent models. The gen-
eral approach should be applicable to other lexical-
ized syntactic formalisms, and potentially also to de-
coding in syntax-driven translation. In addition, our
dual decomposition approach is well-suited to paral-
lelization. For example, each of the head-automata
could be optimized independently in a multi-core or
GPU architecture. Finally, our approach could be
used with other structured learning algorithms, e.g.
Meshi et al (2010).
A Implementation Details
This appendix describes details of the algorithm,
specifically choice of the step sizes ?k, and use of
the ?(i, j) parameters.
A.1 Choice of Step Sizes
We have found the following method to be effec-
tive. First, define ? = f(z(1)) ? f(y(1)), where
(z(1), y(1)) is the output of the algorithm on the first
iteration (note that we always have ? ? 0 since
f(z(1)) = L(u(1))). Then define ?k = ?/(1 + ?k),
where ?k is the number of times that L(u(k
?)) >
L(u(k
??1)) for k? ? k. Hence the learning rate drops
at a rate of 1/(1+ t), where t is the number of times
that the dual increases from one iteration to the next.
A.2 Use of the ?(i, j) Parameters
The parsing algorithms both consider a general-
ized problem that includes ?(i, j) parameters. We
now describe how these can be useful. Re-
call that the optimization problem is to solve
argmaxz?Z,y?Y f(z) + h(y), subject to a set of
agreement constraints. In our models, f(z) can
be written as f ?(z) +
?
i,j ?(i, j)z(i, j) where
f ?(z) includes only terms depending on higher-
order (non arc-factored features), and ?(i, j) are
weights that consider the dependency between i
and j alone. For any value of 0 ? ? ?
1, the problem argmaxz?Z,y?Y f2(z) + h2(y) is
equivalent to the original problem, if f2(z) =
f ?(z) + (1 ? ?)
?
i,j ?(i, j)z(i, j) and h2(y) =
?
?
i,j ?(i, j)y(i, j). We have simply shifted the
?(i, j) weights from one model to the other. While
the optimization problem remains the same, the al-
gorithms in Figure 1 and 2 will converge at differ-
ent rates depending on the value for ?. In our ex-
periments we set ? = 0.001, which puts almost
all the weight in the head-automata models, but al-
lows weights on spanning tree edges to break ties in
MST inference in a sensible way. We suspect this is
important in early iterations of the algorithm, when
many values for u(i, j) or v(i, j) will be zero, and
where with ? = 0 many spanning tree solutions y(k)
would be essentially random, leading to very noisy
updates to the u(i, j) and v(i, j) values. We have
not tested other values for ?.
Acknowledgments MIT gratefully acknowledges the
support of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL, or the US
government. A. Rush was supported by the GALE program of
the DARPA, Contract No. HR0011-06-C-0022. D. Sontag was
supported by a Google PhD Fellowship.
1297
References
H. Alshawi. 1996. Head Automata and Bilingual Tiling:
Translation with Minimal Representations. In Proc.
ACL, pages 167?176.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
CoNLL, pages 149?164.
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. EMNLP-
CoNLL, pages 957?961.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proc. EMNLP, pages
1?8.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007. Us-
ing Combinatorial Optimization within Max-Product
Belief Propagation. In NIPS, pages 369?376.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. Advances in Probabilistic
and Other Parsing Technologies, pages 29?62.
T. Finley and T. Joachims. 2008. Training structural
svms when exact inference is intractable. In ICML,
pages 304?311.
A.K. Joshi and Y. Schabes. 1997. Tree-Adjoining
Grammars. Handbook of Formal Languages: Beyond
Words, 3:69?123.
N. Komodakis, N. Paragios, and G. Tziritas. 2007. MRF
Optimization via Dual Decomposition: Message-
Passing Revisited. In Proc. ICCV.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured Prediction Models via the Matrix-Tree The-
orem. In Proc. EMNLP-CoNLL, pages 141?150.
B.H. Korte and J. Vygen. 2008. Combinatorial Opti-
mization: Theory and Algorithms. Springer Verlag.
A. Kulesza and F. Pereira. 2008. Structured learning
with approximate inference. In NIPS.
C. Lemare?chal. 2001. Lagrangian Relaxation. In Com-
putational Combinatorial Optimization, Optimal or
Provably Near-Optimal Solutions [based on a Spring
School], pages 112?156, London, UK. Springer-
Verlag.
A.F.T. Martins, D. Das, N.A. Smith, and E.P. Xing. 2008.
Stacking Dependency Parsers. In Proc. EMNLP,
pages 157?166.
A.F.T. Martins, N.A. Smith., and E.P. Xing. 2009. Con-
cise Integer Linear Programming Formulations for De-
pendency Parsing. In Proc. ACL, pages 342?350.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proc. EACL, pages 81?88.
R. McDonald and G. Satta. 2007. On the Complexity of
Non-Projective Data-Driven Dependency Parsing. In
Proc. IWPT.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-Projective Dependency Parsing using Spanning
Tree Algorithms. In Proc. HLT-EMNLP, pages 523?
530.
O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson.
2010. Learning Efficiently with Approximate Infer-
ence via Dual Losses. In Proc. ICML.
A. Nedic? and A. Ozdaglar. 2009. Approximate
Primal Solutions and Rate Analysis for Dual Sub-
gradient Methods. SIAM Journal on Optimization,
19(4):1757?1780.
J. Nivre and R. McDonald. 2008. Integrating Graph-
Based and Transition-Based Dependency Parsers. In
Proc. ACL, pages 950?958.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005.
Learning and Inference over Constrained Output. In
Proc. IJCAI, pages 1124?1129.
S. Riedel and J. Clarke. 2006. Incremental Integer Linear
Programming for Non-projective Dependency Parsing.
In Proc. EMNLP, pages 129?137.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On Dual Decomposition and Linear Program-
ming Relaxations for Natural Language Processing. In
Proc. EMNLP.
D.A. Smith and J. Eisner. 2008. Dependency Parsing by
Belief Propagation. In Proc. EMNLP, pages 145?156.
D.A. Smith and N.A. Smith. 2007. Probabilistic Mod-
els of Nonprojective Dependency Trees. In Proc.
EMNLP-CoNLL, pages 132?140.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP Relaxations for MAP
using Message Passing. In Proc. UAI.
M. Steedman. 2000. The Syntactic Process. MIT Press.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 Shared Task on
Joint Parsing of Syntactic and Semantic Dependencies.
In Proc. CoNLL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP
estimation via agreement on trees: message-passing
and linear programming. In IEEE Transactions on In-
formation Theory, volume 51, pages 3697?3717.
1298
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1?11,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Third-order Dependency Parsers
Terry Koo and Michael Collins
MIT CSAIL, Cambridge, MA, 02139, USA
{maestro,mcollins}@csail.mit.edu
Abstract
We present algorithms for higher-order de-
pendency parsing that are ?third-order?
in the sense that they can evaluate sub-
structures containing three dependencies,
and ?efficient? in the sense that they re-
quire only O(n4) time. Importantly, our
new parsers can utilize both sibling-style
and grandchild-style interactions. We
evaluate our parsers on the Penn Tree-
bank and Prague Dependency Treebank,
achieving unlabeled attachment scores of
93.04% and 87.38%, respectively.
1 Introduction
Dependency grammar has proven to be a very use-
ful syntactic formalism, due in no small part to the
development of efficient parsing algorithms (Eis-
ner, 2000; McDonald et al, 2005b; McDonald
and Pereira, 2006; Carreras, 2007), which can be
leveraged for a wide variety of learning methods,
such as feature-rich discriminative models (Laf-
ferty et al, 2001; Collins, 2002; Taskar et al,
2003). These parsing algorithms share an impor-
tant characteristic: they factor dependency trees
into sets of parts that have limited interactions. By
exploiting the additional constraints arising from
the factorization, maximizations or summations
over the set of possible dependency trees can be
performed efficiently and exactly.
A crucial limitation of factored parsing algo-
rithms is that the associated parts are typically
quite small, losing much of the contextual in-
formation within the dependency tree. For the
purposes of improving parsing performance, it is
desirable to increase the size and variety of the
parts used by the factorization.1 At the same
time, the need for more expressive factorizations
1For examples of how performance varies with the degree
of the parser?s factorization see, e.g., McDonald and Pereira
(2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al
(2008, Tables 2 and 4), or Suzuki et al (2009, Tables 3?6).
must be balanced against any resulting increase in
the computational cost of the parsing algorithm.
Consequently, recent work in dependency pars-
ing has been restricted to applications of second-
order parsers, the most powerful of which (Car-
reras, 2007) requires O(n4) time and O(n3) space,
while being limited to second-order parts.
In this paper, we present new third-order pars-
ing algorithms that increase both the size and vari-
ety of the parts participating in the factorization,
while simultaneously maintaining computational
requirements of O(n4) time and O(n3) space. We
evaluate our parsers on the Penn WSJ Treebank
(Marcus et al, 1993) and Prague Dependency
Treebank (Hajic? et al, 2001), achieving unlabeled
attachment scores of 93.04% and 87.38%. In sum-
mary, we make three main contributions:
1. Efficient new third-order parsing algorithms.
2. Empirical evaluations of these parsers.
3. A free distribution of our implementation.2
The remainder of this paper is divided as follows:
Sections 2 and 3 give background, Sections 4 and
5 describe our new parsing algorithms, Section 6
discusses related work, Section 7 presents our ex-
perimental results, and Section 8 concludes.
2 Dependency parsing
In dependency grammar, syntactic relationships
are represented as head-modifier dependencies:
directed arcs between a head, which is the more
?essential? word in the relationship, and a modi-
fier, which supplements the meaning of the head.
For example, Figure 1 contains a dependency be-
tween the verb ?report? (the head) and its object
?sales? (the modifier). A complete analysis of a
sentence is given by a dependency tree: a set of de-
pendencies that forms a rooted, directed tree span-
ning the words of the sentence. Every dependency
tree is rooted at a special ?*? token, allowing the
2http://groups.csail.mit.edu/nlp/dpo3/
1
Insiders must report purchases and immediatelysales
 * 
Figure 1: An example dependency structure.
selection of the sentential head to be modeled as if
it were a dependency.
For a sentence x, we define dependency parsing
as a search for the highest-scoring analysis of x:
y
?
(x) = argmax
y?Y(x)
SCORE(x, y) (1)
Here, Y(x) is the set of all trees compatible with
x and SCORE(x, y) evaluates the event that tree y
is the analysis of sentence x. Since the cardinal-
ity of Y(x) grows exponentially with the length of
the sentence, directly solving Eq. 1 is impractical.
A common strategy, and one which forms the fo-
cus of this paper, is to factor each dependency tree
into small parts, which can be scored in isolation.
Factored parsing can be formalized as follows:
SCORE(x, y) =
?
p?y
SCOREPART(x, p)
That is, we treat the dependency tree y as a set
of parts p, each of which makes a separate contri-
bution to the score of y. For certain factorizations,
efficient parsing algorithms exist for solving Eq. 1.
We define the order of a part according to the
number of dependencies it contains, with analo-
gous terminology for factorizations and parsing al-
gorithms. In the remainder of this paper, we focus
on factorizations utilizing the following parts:
g
g
hh
h h h
mm
m mm
ss
s
t
dependency sibling grandchild
tri-siblinggrand-sibling
Specifically, Sections 4.1, 4.2, and 4.3 describe
parsers that, respectively, factor trees into grand-
child parts, grand-sibling parts, and a mixture of
grand-sibling and tri-sibling parts.
3 Existing parsing algorithms
Our new third-order dependency parsers build on
ideas from existing parsing algorithms. In this
section, we provide background on two relevant
parsers from previous work.
(a) +=
h h mm ee
(b) +=
h h mm r r+1
Figure 2: The dynamic-programming structures
and derivations of the Eisner (2000) algorithm.
Complete spans are depicted as triangles and in-
complete spans as trapezoids. For brevity, we elide
the symmetric right-headed versions.
3.1 First-order factorization
The first type of parser we describe uses a ?first-
order? factorization, which decomposes a depen-
dency tree into its individual dependencies. Eis-
ner (2000) introduced a widely-used dynamic-
programming algorithm for first-order parsing; as
it is the basis for many parsers, including our new
algorithms, we summarize its design here.
The Eisner (2000) algorithm is based on two
interrelated types of dynamic-programming struc-
tures: complete spans, which consist of a head-
word and its descendents on one side, and incom-
plete spans, which consist of a dependency and the
region between the head and modifier.
Formally, we denote a complete span as Ch,e
where h and e are the indices of the span?s head-
word and endpoint. An incomplete span is de-
noted as Ih,m where h and m are the index of the
head and modifier of a dependency. Intuitively,
a complete span represents a ?half-constituent?
headed by h, whereas an incomplete span is only
a partial half-constituent, since the constituent can
be extended by adding more modifiers to m.
Each type of span is created by recursively
combining two smaller, adjacent spans; the con-
structions are specified graphically in Figure 2.
An incomplete span is constructed from a pair
of complete spans, indicating the division of the
range [h,m] into constituents headed by h and
m. A complete span is created by ?complet-
ing? an incomplete span with the other half of
m?s constituent. The point of concatenation in
each construction?m in Figure 2(a) or r in Fig-
ure 2(b)?is the split point, a free index that must
be enumerated to find the optimal construction.
In order to parse a sentence x, it suffices to
find optimal constructions for all complete and
incomplete spans defined on x. This can be
2
(a) +=
h h mm ee
(b) +=
h h mm ss
(c) +=
mms s r r+1
Figure 3: The dynamic-programming structures
and derivations of the second-order sibling parser;
sibling spans are depicted as boxes. For brevity,
we elide the right-headed versions.
accomplished by adapting standard chart-parsing
techniques (Cocke and Schwartz, 1970; Younger,
1967; Kasami, 1965) to the recursive derivations
defined in Figure 2. Since each derivation is de-
fined by two fixed indices (the boundaries of the
span) and a third free index (the split point), the
parsing algorithm requires O(n3) time and O(n2)
space (Eisner, 1996; McAllester, 1999).
3.2 Second-order sibling factorization
As remarked by Eisner (1996) and McDonald
and Pereira (2006), it is possible to rearrange the
dynamic-programming structures to conform to an
improved factorization that decomposes each tree
into sibling parts?pairs of dependencies with a
shared head. Specifically, a sibling part consists
of a triple of indices (h,m, s) where (h,m) and
(h, s) are dependencies, and where s and m are
successive modifiers to the same side of h.
In order to parse this factorization, the second-
order parser introduces a third type of dynamic-
programming structure: sibling spans, which rep-
resent the region between successive modifiers of
some head. Formally, we denote a sibling span
as Ss,m where s and m are a pair of modifiers in-
volved in a sibling relationship. Modified versions
of sibling spans will play an important role in the
new parsing algorithms described in Section 4.
Figure 3 provides a graphical specification of
the second-order parsing algorithm. Note that in-
complete spans are constructed in a new way: the
second-order parser combines a smaller incom-
plete span, representing the next-innermost depen-
dency, with a sibling span that covers the region
between the two modifiers. Sibling parts (h,m, s)
can thus be obtained from Figure 3(b). Despite
the use of second-order parts, each derivation is
(a) = +
gg hhh mm ee
(b) = +
g gh h hm mr r+1
(c) = +
gg hh hm me e
(d) = +
gg hh hm mr r+1
Figure 4: The dynamic-programming structures
and derivations of Model 0. For brevity, we elide
the right-headed versions. Note that (c) and (d)
differ from (a) and (b) only in the position of g.
still defined by a span and split point, so the parser
requires O(n3) time and O(n2) space.
4 New third-order parsing algorithms
In this section we describe our new third-order de-
pendency parsing algorithms. Our overall method
is characterized by the augmentation of each span
with a ?grandparent? index: an index external to
the span whose role will be made clear below. This
section presents three parsing algorithms based on
this idea: Model 0, a second-order parser, and
Models 1 and 2, which are third-order parsers.
4.1 Model 0: all grandchildren
The first parser, Model 0, factors each dependency
tree into a set of grandchild parts?pairs of de-
pendencies connected head-to-tail. Specifically,
a grandchild part is a triple of indices (g, h,m)
where (g, h) and (h,m) are dependencies.3
In order to parse this factorization, we augment
both complete and incomplete spans with grand-
parent indices; for brevity, we refer to these aug-
mented structures as g-spans. Formally, we denote
a complete g-span as Cgh,e, where Ch,e is a normal
complete span and g is an index lying outside the
range [h, e], with the implication that (g, h) is a
dependency. Incomplete g-spans are defined anal-
ogously and are denoted as Igh,m.
Figure 4 depicts complete and incomplete g-
spans and provides a graphical specification of the
3The Carreras (2007) parser also uses grandchild parts but
only in restricted cases; see Section 6 for details.
3
OPTIMIZEALLSPANS(x)
1. ? g, i Cgi,i = 0 / base case
2. for w = 1 . . . (n? 1) / span width
3. for i = 1 . . . (n? w) / span start index
4. j = i + w / span end index
5. for g < i or g > j / grandparent index
6. Igi,j = max i?r<j {C
g
i,r + C
i
j,r+1} +
SCOREG(x, g, i, j)
7. Igj,i = max i?r<j {C
g
j,r+1 + C
j
i,r} +
SCOREG(x, g, j, i)
8. Cgi,j = max i<m?j {I
g
i,m + C
i
m,j}
9. Cgj,i = max i?m<j {I
g
j,m + C
j
m,i}
10. endfor
11. endfor
12. endfor
Figure 5: A bottom-up chart parser for Model 0.
SCOREG is the scoring function for grandchild
parts. We use the g-span identities as shorthand
for their chart entries (e.g., Igi,j refers to the entry
containing the maximum score of that g-span).
Model 0 dynamic-programming algorithm. The
algorithm resembles the first-order parser, except
that every recursive construction must also set the
grandparent indices of the smaller g-spans; for-
tunately, this can be done deterministically in all
cases. For example, Figure 4(a) depicts the de-
composition of Cgh,e into an incomplete half and
a complete half. The grandparent of the incom-
plete half is copied from Cgh,e while the grandpar-
ent of the complete half is set to h, the head of m
as defined by the construction. Clearly, grandchild
parts (g, h,m) can be read off of the incomplete
g-spans in Figure 4(b,d). Moreover, since each
derivation copies the grandparent index g into suc-
cessively smaller g-spans, grandchild parts will be
produced for all grandchildren of g.
Model 0 can be parsed by adapting standard
top-down or bottom-up chart parsing techniques.
For concreteness, Figure 5 provides a pseudocode
sketch of a bottom-up chart parser for Model 0;
although the sketch omits many details, it suf-
fices for the purposes of illustration. The algo-
rithm progresses from small widths to large in
the usual manner, but after defining the endpoints
(i, j) there is an additional loop that enumerates
all possible grandparents. Since each derivation is
defined by three fixed indices (the g-span) and one
free index (the split point), the complexity of the
algorithm is O(n4) time and O(n3) space.
Note that the grandparent indices cause each g-
(a) = +
gg hhh mm ee
(b) = +
g gh h hm mss
(c) = +
hh hm mss r r+1
Figure 6: The dynamic-programming structures
and derivations of Model 1. Right-headed and
right-grandparented versions are omitted.
span to have non-contiguous structure. For ex-
ample, in Figure 4(a) the words between g and h
will be controlled by some other g-span. Due to
these discontinuities, the correctness of the Model
0 dynamic-programming algorithm may not be
immediately obvious. While a full proof of cor-
rectness is beyond the scope of this paper, we note
that each structure on the right-hand side of Fig-
ure 4 lies completely within the structure on the
left-hand side. This nesting of structures implies,
in turn, that the usual properties required to ensure
the correctness of dynamic programming hold.
4.2 Model 1: all grand-siblings
We now describe our first third-order parsing al-
gorithm. Model 1 decomposes each tree into a
set of grand-sibling parts?combinations of sib-
ling parts and grandchild parts. Specifically, a
grand-sibling is a 4-tuple of indices (g, h,m, s)
where (h,m, s) is a sibling part and (g, h,m) and
(g, h, s) are grandchild parts. For example, in Fig-
ure 1, the words ?must,? ?report,? ?sales,? and
?immediately? form a grand-sibling part.
In order to parse this factorization, we intro-
duce sibling g-spans Shm,s, which are composed of
a normal sibling span Sm,s and an external index
h, with the implication that (h,m, s) forms a valid
sibling part. Figure 6 provides a graphical specifi-
cation of the dynamic-programming algorithm for
Model 1. The overall structure of the algorithm re-
sembles the second-order sibling parser, with the
addition of grandparent indices; as in Model 0, the
grandparent indices can be set deterministically in
all cases. Note that the sibling g-spans are crucial:
they allow grand-sibling parts (g, h,m, s) to be
read off of Figure 6(b), while simultaneously prop-
agating grandparent indices to smaller g-spans.
4
(a) = +
gg hhh mm ee
(b) =
g hh mm s
(c) = +
hh hm ms sst
(d) = +
hh hm mss r r+1
Figure 7: The dynamic-programming structures
and derivations of Model 2. Right-headed and
right-grandparented versions are omitted.
Like Model 0, Model 1 can be parsed via adap-
tations of standard chart-parsing techniques; we
omit the details for brevity. Despite the move to
third-order parts, each derivation is still defined by
a g-span and a split point, so that parsing requires
only O(n4) time and O(n3) space.
4.3 Model 2: grand-siblings and tri-siblings
Higher-order parsing algorithms have been pro-
posed which extend the second-order sibling fac-
torization to parts containing multiple siblings
(McDonald and Pereira, 2006, also see Section 6
for discussion). In this section, we show how our
g-span-based techniques can be combined with a
third-order sibling parser, resulting in a parser that
captures both grand-sibling parts and tri-sibling
parts?4-tuples of indices (h,m, s, t) such that
both (h,m, s) and (h, s, t) are sibling parts.
In order to parse this factorization, we intro-
duce a new type of dynamic-programming struc-
ture: sibling-augmented spans, or s-spans. For-
mally, we denote an incomplete s-span as Ih,m,s
where Ih,m is a normal incomplete span and s is an
index lying in the strict interior of the range [h,m],
such that (h,m, s) forms a valid sibling part.
Figure 7 provides a graphical specification of
the Model 2 parsing algorithm. An incomplete
s-span is constructed by combining a smaller in-
complete s-span, representing the next-innermost
pair of modifiers, with a sibling g-span, covering
the region between the outer two modifiers. As
in Model 1, sibling g-spans are crucial for propa-
gating grandparent indices, while allowing the re-
covery of tri-sibling parts (h,m, s, t). Figure 7(b)
shows how an incomplete s-span can be converted
into an incomplete g-span by exchanging the in-
ternal sibling index for an external grandparent in-
dex; in the process, grand-sibling parts (g, h,m, s)
are enumerated. Since every derivation is defined
by an augmented span and a split point, Model 2
can be parsed in O(n4) time and O(n3) space.
It should be noted that unlike Model 1, Model
2 produces grand-sibling parts only for the outer-
most pair of grandchildren,4 similar to the behav-
ior of the Carreras (2007) parser. In fact, the re-
semblance is more than passing, as Model 2 can
emulate the Carreras (2007) algorithm by ?demot-
ing? each third-order part into a second-order part:
SCOREGS(x, g, h,m, s) = SCOREG(x, g, h,m)
SCORETS(x, h,m, s, t) = SCORES(x, h,m, s)
where SCOREG, SCORES, SCOREGS and
SCORETS are the scoring functions for grand-
children, siblings, grand-siblings and tri-siblings,
respectively. The emulated version has the same
computational complexity as the original, so there
is no practical reason to prefer it over the original.
Nevertheless, the relationship illustrated above
highlights the efficiency of our approach: we
are able to recover third-order parts in place of
second-order parts, at no additional cost.
4.4 Discussion
The technique of grandparent-index augmentation
has proven fruitful, as it allows us to parse ex-
pressive third-order factorizations while retaining
an efficient O(n4) runtime. In fact, our third-
order parsing algorithms are ?optimally? efficient
in an asymptotic sense. Since each third-order part
is composed of four separate indices, there are
?(n
4
) distinct parts. Any third-order parsing al-
gorithm must at least consider the score of each
part, hence third-order parsing is ?(n4) and it fol-
lows that the asymptotic complexity of Models 1
and 2 cannot be improved.
The key to the efficiency of our approach is a
fundamental asymmetry in the structure of a di-
rected tree: a head can have any number of mod-
ifiers, while a modifier always has exactly one
head. Factorizations like that of Carreras (2007)
obtain grandchild parts by augmenting spans with
the indices of modifiers, leading to limitations on
4The reason for the restriction is that in Model 2, grand-
siblings can only be derived via Figure 7(b), which does not
recursively copy the grandparent index for reuse in smaller
g-spans as Model 1 does in Figure 6(b).
5
the grandchildren that can participate in the fac-
torization. Our method, by ?inverting? the modi-
fier indices into grandparent indices, exploits the
structural asymmetry.
As a final note, the parsing algorithms described
in this section fall into the category of projective
dependency parsers, which forbid crossing depen-
dencies. If crossing dependencies are allowed, it
is possible to parse a first-order factorization by
finding the maximum directed spanning tree (Chu
and Liu, 1965; Edmonds, 1967; McDonald et al,
2005b). Unfortunately, designing efficient higher-
order non-projective parsers is likely to be chal-
lenging, based on recent hardness results (McDon-
ald and Pereira, 2006; McDonald and Satta, 2007).
5 Extensions
We briefly outline a few extensions to our algo-
rithms; we hope to explore these in future work.
5.1 Probabilistic inference
Many statistical modeling techniques are based on
partition functions and marginals?summations
over the set of possible trees Y(x). Straightfor-
ward adaptations of the inside-outside algorithm
(Baker, 1979) to our dynamic-programming struc-
tures would suffice to compute these quantities.
5.2 Labeled parsing
Our parsers are easily extended to labeled depen-
dencies. Direct integration of labels into Models 1
and 2 would result in third-order parts composed
of three labeled dependencies, at the cost of in-
creasing the time and space complexities by fac-
tors of O(L3) and O(L2), respectively, where L
bounds the number of labels per dependency.
5.3 Word senses
If each word in x has a set of possible ?senses,?
our parsers can be modified to recover the best
joint assignment of syntax and senses for x, by
adapting methods in Eisner (2000). Complex-
ity would increase by factors of O(S4) time and
O(S
3
) space, where S bounds the number of
senses per word.
5.4 Increased context
If more vertical context is desired, the dynamic-
programming structures can be extended with ad-
ditional ancestor indices, resulting in a ?spine? of
ancestors above each span. Each additional an-
cestor lengthens the vertical scope of the factor-
ization (e.g., from grand-siblings to ?great-grand-
siblings?), while increasing complexity by a factor
of O(n). Horizontal context can also be increased
by adding internal sibling indices; each additional
sibling widens the scope of the factorization (e.g.,
from grand-siblings to ?grand-tri-siblings?), while
increasing complexity by a factor of O(n).
6 Related work
Our method augments each span with the index
of the head that governs that span, in a manner
superficially similar to parent annotation in CFGs
(Johnson, 1998). However, parent annotation is
a grammar transformation that is independent of
any particular sentence, whereas our method an-
notates spans with indices into the current sen-
tence. These indices allow the use of arbitrary fea-
tures predicated on the position of the grandparent
(e.g., word identity, POS tag, contextual POS tags)
without affecting the asymptotic complexity of the
parsing algorithm. Efficiently encoding this kind
of information into a sentence-independent gram-
mar transformation would be challenging at best.
Eisner (2000) defines dependency parsing mod-
els where each word has a set of possible ?senses?
and the parser recovers the best joint assignment
of syntax and senses. Our new parsing algorithms
could be implemented by defining the ?sense? of
each word as the index of its head. However, when
parsing with senses, the complexity of the Eisner
(2000) parser increases by factors of O(S3) time
and O(S2) space (ibid., Section 4.2). Since each
word has n potential heads, a direct application
of the word-sense parser leads to time and space
complexities of O(n6) and O(n4), respectively, in
contrast to our O(n4) and O(n3).5
Eisner (2000) also uses head automata to score
or recognize the dependents of each head. An in-
teresting question is whether these automata could
be coerced into modeling the grandparent indices
used in our parsing algorithms. However, note
that the head automata are defined in a sentence-
independent manner, with two automata per word
in the vocabulary (ibid., Section 2). The automata
are thus analogous to the rules of a CFG and at-
5In brief, the reason for the inefficiency is that the word-
sense parser is unable to exploit certain constraints, such as
the fact that the endpoints of a sibling g-span must have the
same head. The word-sense parser would needlessly enumer-
ate all possible pairs of heads in this case.
6
tempts to use them to model grandparent indices
would face difficulties similar to those already de-
scribed for grammar transformations in CFGs.
It should be noted that third-order parsers
have previously been proposed by McDonald and
Pereira (2006), who remarked that their second-
order sibling parser (see Figure 3) could easily
be extended to capture m > 1 successive modi-
fiers in O(nm+1) time (ibid., Section 2.2). To our
knowledge, however, Models 1 and 2 are the first
third-order parsing algorithms capable of model-
ing grandchild parts. In our experiments, we find
that grandchild interactions make important con-
tributions to parsing performance (see Table 3).
Carreras (2007) presents a second-order parser
that can score both sibling and grandchild parts,
with complexities of O(n4) time and O(n3) space.
An important limitation of the parser?s factoriza-
tion is that it only defines grandchild parts for
outermost grandchildren: (g, h,m) is scored only
when m is the outermost modifier of h in some di-
rection. Note that Models 1 and 2 have the same
complexity as Carreras (2007), but strictly greater
expressiveness: for each sibling or grandchild part
used in the Carreras (2007) factorization, Model 1
defines an enclosing grand-sibling, while Model 2
defines an enclosing tri-sibling or grand-sibling.
The factored parsing approach we focus on is
sometimes referred to as ?graph-based? parsing;
a popular alternative is ?transition-based? parsing,
in which trees are constructed by making a se-
ries of incremental decisions (Yamada and Mat-
sumoto, 2003; Attardi, 2006; Nivre et al, 2006;
McDonald and Nivre, 2007). Transition-based
parsers do not impose factorizations, so they can
define arbitrary features on the tree as it is being
built. As a result, however, they rely on greedy or
approximate search algorithms to solve Eq. 1.
7 Parsing experiments
In order to evaluate the effectiveness of our parsers
in practice, we apply them to the Penn WSJ Tree-
bank (Marcus et al, 1993) and the Prague De-
pendency Treebank (Hajic? et al, 2001; Hajic?,
1998).6 We use standard training, validation, and
test splits7 to facilitate comparisons. Accuracy is
6For English, we extracted dependencies using Joakim
Nivre?s Penn2Malt tool with standard head rules (Yamada
and Matsumoto, 2003); for Czech, we ?projectivized? the
training data by finding best-match projective trees.
7For Czech, the PDT has a predefined split; for English,
we split the Sections as: 2?21 training, 22 validation, 23 test.
measured with unlabeled attachment score (UAS):
the percentage of words with the correct head.8
7.1 Features for third-order parsing
Our parsing algorithms can be applied to scores
originating from any source, but in our experi-
ments we chose to use the framework of structured
linear models, deriving our scores as:
SCOREPART(x, p) = w ? f(x, p)
Here, f is a feature-vector mapping and w is a
vector of associated parameters. Following stan-
dard practice for higher-order dependency parsing
(McDonald and Pereira, 2006; Carreras, 2007),
Models 1 and 2 evaluate not only the relevant
third-order parts, but also the lower-order parts
that are implicit in their third-order factoriza-
tions. For example, Model 1 defines feature map-
pings for dependencies, siblings, grandchildren,
and grand-siblings, so that the score of a depen-
dency parse is given by:
MODEL1SCORE(x, y) =
?
(h,m)?y
wdep ? fdep(x, h,m)
?
(h,m,s)?y
wsib ? fsib(x, h,m, s)
?
(g,h,m)?y
wgch ? fgch(x, g, h,m)
?
(g,h,m,s)?y
wgsib ? fgsib(x, g, h,m, s)
Above, y is simultaneously decomposed into sev-
eral different types of parts; trivial modifications
to the Model 1 parser allow it to evaluate all of
the necessary parts in an interleaved fashion. A
similar treatment of Model 2 yields five feature
mappings: the four above plus ftsib(x, h,m, s, t),
which represents tri-sibling parts.
The lower-order feature mappings fdep, fsib, and
fgch are based on feature sets from previous work
(McDonald et al, 2005a; McDonald and Pereira,
2006; Carreras, 2007), to which we added lexical-
ized versions of several features. For example, fdep
contains lexicalized ?in-between? features that de-
pend on the head and modifier words as well as a
word lying in between the two; in contrast, pre-
vious work has generally defined in-between fea-
tures for POS tags only. As another example, our
8As in previous work, English evaluation ignores any to-
ken whose gold-standard POS tag is one of {?? ?? : , .}.
7
second-order mappings fsib and fgch define lexical
trigram features, while previous work has gener-
ally used POS trigrams only.
Our third-order feature mappings fgsib and ftsib
consist of four types of features. First, we define
4-gram features that characterize the four relevant
indices using words and POS tags; examples in-
clude POS 4-grams and mixed 4-grams with one
word and three POS tags. Second, we define 4-
gram context features consisting of POS 4-grams
augmented with adjacent POS tags: for exam-
ple, fgsib(x, g, h,m, s) includes POS 7-grams for
the tags at positions (g, h,m, s, g+1, h+1,m+1).
Third, we define backed-off features that track bi-
gram and trigram interactions which are absent
in the lower-order feature mappings: for exam-
ple, ftsib(x, h,m, s, t) contains features predicated
on the trigram (m, s, t) and the bigram (m, t),
neither of which exist in any lower-order part.
Fourth, noting that coordinations are typically an-
notated as grand-siblings (e.g., ?report purchases
and sales? in Figure 1), we define coordination
features for certain grand-sibling parts. For exam-
ple, fgsib(x, g, h,m, s) contains features examin-
ing the implicit head-modifier relationship (g,m)
that are only activated when the POS tag of s is a
coordinating conjunction.
Finally, we make two brief remarks regarding
the use of POS tags. First, we assume that input
sentences have been automatically tagged in a pre-
processing step.9 Second, for any feature that de-
pends on POS tags, we include two copies of the
feature: one using normal POS tags and another
using coarsened versions10 of the POS tags.
7.2 Averaged perceptron training
There are a wide variety of parameter estima-
tion methods for structured linear models, such
as log-linear models (Lafferty et al, 2001) and
max-margin models (Taskar et al, 2003). We
chose the averaged structured perceptron (Freund
and Schapire, 1999; Collins, 2002) as it combines
highly competitive performance with fast training
times, typically converging in 5?10 iterations. We
train each parser for 10 iterations and select pa-
9For Czech, the PDT provides automatic tags; for English,
we used MXPOST (Ratnaparkhi, 1996) to tag validation and
test data, with 10-fold cross-validation on the training set.
Note that the reliance on POS-tagged input can be relaxed
slightly by treating POS tags as word senses; see Section 5.3
and McDonald (2006, Table 6.1).
10For Czech, we used the first character of the tag; for En-
glish, we used the first two characters, except PRP and PRP$.
Beam Pass Orac Acc1 Acc2 Time1 Time2
0.0001 26.5 99.92 93.49 93.49 49.6m 73.5m
0.001 16.7 99.72 93.37 93.29 25.9m 24.2m
0.01 9.1 99.19 93.26 93.16 6.7m 7.9m
Table 1: Effect of the marginal-probability beam
on English parsing. For each beam value, parsers
were trained on the English training set and evalu-
ated on the English validation set; the same beam
value was applied to both training and validation
data. Pass = %dependencies surviving the beam in
training data, Orac = maximum achievable UAS
on validation data, Acc1/Acc2 = UAS of Models
1/2 on validation data, and Time1/Time2 = min-
utes per perceptron training iteration for Models
1/2, averaged over all 10 iterations. For perspec-
tive, the English training set has a total of 39,832
sentences and 950,028 words. A beam of 0.0001
was used in all experiments outside this table.
rameters from the iteration that achieves the best
score on the validation set.
7.3 Coarse-to-fine pruning
In order to decrease training times, we follow
Carreras et al (2008) and eliminate unlikely de-
pendencies using a form of coarse-to-fine pruning
(Charniak and Johnson, 2005; Petrov and Klein,
2007). In brief, we train a log-linear first-order
parser11 and for every sentence x in training, val-
idation, and test data we compute the marginal
probability P (h,m |x) of each dependency. Our
parsers are then modified to ignore any depen-
dency (h,m) whose marginal probability is below
0.0001?maxh? P (h?,m |x). Table 1 provides in-
formation on the behavior of the pruning method.
7.4 Main results
Table 2 lists the accuracy of Models 1 and 2 on the
English and Czech test sets, together with some
relevant results from related work.12 The mod-
els marked ??? are not directly comparable to our
work as they depend on additional sources of in-
formation that our models are trained without?
unlabeled data in the case of Koo et al (2008) and
11For English, we generate marginals using a projective
parser (Baker, 1979; Eisner, 2000); for Czech, we generate
marginals using a non-projective parser (Smith and Smith,
2007; McDonald and Satta, 2007; Koo et al, 2007). Param-
eters for these models are obtained by running exponentiated
gradient training for 10 iterations (Collins et al, 2008).
12Model 0 was not tested as its factorization is a strict sub-
set of the factorization of Model 1.
8
Parser Eng Cze
McDonald et al (2005a,2005b) 90.9 84.4
McDonald and Pereira (2006) 91.5 85.2
Koo et al (2008), standard 92.02 86.13
Model 1 93.04 87.38
Model 2 92.93 87.37
Koo et al (2008), semi-sup? 93.16 87.13
Suzuki et al (2009)? 93.79 88.05
Carreras et al (2008)? 93.5
Table 2: UAS of Models 1 and 2 on test data, with
relevant results from related work. Note that Koo
et al (2008) is listed with standard features and
semi-supervised features. ?: see main text.
Suzuki et al (2009) and phrase-structure annota-
tions in the case of Carreras et al (2008). All three
of the ??? models are based on versions of the Car-
reras (2007) parser, so modifying these methods to
work with our new third-order parsing algorithms
would be an interesting topic for future research.
For example, Models 1 and 2 obtain results com-
parable to the semi-supervised parsers of Koo et
al. (2008), and additive gains might be realized by
applying their cluster-based feature sets to our en-
riched factorizations.
7.5 Ablation studies
In order to better understand the contributions of
the various feature types, we ran additional abla-
tion experiments; the results are listed in Table 3,
in addition to the scores of Model 0 and the emu-
lated Carreras (2007) parser (see Section 4.3). In-
terestingly, grandchild interactions appear to pro-
vide important information: for example, when
Model 2 is used without grandchild-based features
(?Model 2, no-G? in Table 3), its accuracy suffers
noticeably. In addition, it seems that grandchild
interactions are particularly useful in Czech, while
sibling interactions are less important: consider
that Model 0, a second-order grandchild parser
with no sibling-based features, can easily outper-
form ?Model 2, no-G,? a third-order sibling parser
with no grandchild-based features.
8 Conclusion
We have presented new parsing algorithms that are
capable of efficiently parsing third-order factoriza-
tions, including both grandchild and sibling inter-
actions. Due to space restrictions, we have been
necessarily brief at some points in this paper; some
additional details can be found in Koo (2010).
Parser Eng Cze
Model 0 93.07 87.39
Carreras (2007) emulation 93.14 87.25
Model 1 93.49 87.64
Model 1, no-3rd 93.17 87.57
Model 2 93.49 87.46
Model 2, no-3rd 93.20 87.43
Model 2, no-G 92.92 86.76
Table 3: UAS for modified versions of our parsers
on validation data. The term no-3rd indicates a
parser that was trained and tested with the third-
order feature mappings fgsib and ftsib deactivated,
though lower-order features were retained; note
that ?Model 2, no-3rd? is not identical to the Car-
reras (2007) parser as it defines grandchild parts
for the pair of grandchildren. The term no-G indi-
cates a parser that was trained and tested with the
grandchild-based feature mappings fgch and fgsib
deactivated; note that ?Model 2, no-G? emulates
the third-order sibling parser proposed by McDon-
ald and Pereira (2006).
There are several possibilities for further re-
search involving our third-order parsing algo-
rithms. One idea would be to consider extensions
and modifications of our parsers, some of which
have been suggested in Sections 5 and 7.4. A sec-
ond area for future work lies in applications of de-
pendency parsing. While we have evaluated our
new algorithms on standard parsing benchmarks,
there are a wide variety of tasks that may bene-
fit from the extended context offered by our third-
order factorizations; for example, the 4-gram sub-
structures enabled by our approach may be useful
for dependency-based language modeling in ma-
chine translation (Shen et al, 2008). Finally, in
the hopes that others in the NLP community may
find our parsers useful, we provide a free distribu-
tion of our implementation.2
Acknowledgments
We would like to thank the anonymous review-
ers for their helpful comments and suggestions.
We also thank Regina Barzilay and Alexander
Rush for their much-appreciated input during the
writing process. The authors gratefully acknowl-
edge the following sources of support: Terry
Koo and Michael Collins were both funded by
a DARPA subcontract under SRI (#27-001343),
and Michael Collins was additionally supported
by NTT (Agmt. dtd. 06/21/98).
9
References
Giuseppe Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Pro-
ceedings of the 10th CoNLL, pages 166?170. Asso-
ciation for Computational Linguistics.
James Baker. 1979. Trainable Grammars for Speech
Recognition. In Proceedings of the 97th meeting of
the Acoustical Society of America.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, Dynamic Programming, and the Per-
ceptron for Efficient, Feature-rich Parsing. In Pro-
ceedings of the 12th CoNLL, pages 9?16. Associa-
tion for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL, pages 957?961. Association for Computa-
tional Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine N -best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd ACL.
Y.J. Chu and T.H. Liu. 1965. On the Shortest Ar-
borescence of a Directed Graph. Science Sinica,
14:1396?1400.
John Cocke and Jacob T. Schwartz. 1970. Program-
ming Languages and Their Compilers: Preliminary
Notes. Technical report, New York University.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponenti-
ated Gradient Algorithms for Conditional Random
Fields and Max-Margin Markov Networks. Journal
of Machine Learning Research, 9:1775?1822, Aug.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the 7th EMNLP, pages 1?8. Association for Com-
putational Linguistics.
Jack R. Edmonds. 1967. Optimum Branchings. Jour-
nal of Research of the National Bureau of Standards,
71B:233?240.
Jason Eisner. 1996. Three New Probabilistic Models
for Dependency Parsing: An Exploration. In Pro-
ceedings of the 16th COLING, pages 340?345. As-
sociation for Computational Linguistics.
Jason Eisner. 2000. Bilexical Grammars and Their
Cubic-Time Parsing Algorithms. In Harry Bunt
and Anton Nijholt, editors, Advances in Probabilis-
tic and Other Parsing Technologies, pages 29?62.
Kluwer Academic Publishers.
Yoav Freund and Robert E. Schapire. 1999. Large
Margin Classification Using the Perceptron Algo-
rithm. Machine Learning, 37(3):277?296.
Jan Hajic?, Eva Hajic?ova?, Petr Pajas, Jarmila Panevova,
and Petr Sgall. 2001. The Prague Dependency Tree-
bank 1.0, LDC No. LDC2001T10. Linguistics Data
Consortium.
Jan Hajic?. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Eva
Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?19.
Mark Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24(4):613?632.
Tadao Kasami. 1965. An Efficient Recognition and
Syntax-analysis Algorithm for Context-free Lan-
guages. Technical Report AFCRL-65-758, Air
Force Cambridge Research Lab.
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured Prediction Mod-
els via the Matrix-Tree Theorem. In Proceedings
of EMNLP-CoNLL, pages 141?150. Association for
Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Pars-
ing. In Proceedings of the 46th ACL, pages 595?603.
Association for Computational Linguistics.
Terry Koo. 2010. Advances in Discriminative Depen-
dency Parsing. Ph.D. thesis, Massachusetts Institute
of Technology, Cambridge, MA, USA, June.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of the 18th ICML,
pages 282?289. Morgan Kaufmann.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David A. McAllester. 1999. On the Complexity
Analysis of Static Analyses. In Proceedings of
the 6th Static Analysis Symposium, pages 312?329.
Springer-Verlag.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the Errors of Data-Driven Dependency Parsers.
In Proceedings of EMNLP-CoNLL, pages 122?131.
Association for Computational Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Al-
gorithms. In Proceedings of the 11th EACL, pages
81?88. Association for Computational Linguistics.
Ryan McDonald and Giorgio Satta. 2007. On the
Complexity of Non-Projective Data-Driven Depen-
dency Parsing. In Proceedings of IWPT.
10
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online Large-Margin Training of
Dependency Parsers. In Proceedings of the 43rd
ACL, pages 91?98. Association for Computational
Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-Projective Dependency
Parsing using Spanning Tree Algorithms. In Pro-
ceedings of HLT-EMNLP, pages 523?530. Associa-
tion for Computational Linguistics.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA, July.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en
Eryig?it, and Svetoslav Marinov. 2006. Labeled
Pseudo-Projective Dependency Parsing with Sup-
port Vector Machines. In Proceedings of the 10th
CoNLL, pages 221?225. Association for Computa-
tional Linguistics.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of HLT-
NAACL, pages 404?411. Association for Computa-
tional Linguistics.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Proceedings
of the 1st EMNLP, pages 133?142. Association for
Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proceedings of the 46th ACL, pages 577?
585. Association for Computational Linguistics.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic Models of Nonprojective Dependency Trees.
In Proceedings of EMNLP-CoNLL, pages 132?140.
Association for Computational Linguistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An Empirical Study of
Semi-supervised Structured Conditional Models for
Dependency Parsing. In Proceedings of EMNLP,
pages 551?560. Association for Computational Lin-
guistics.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max margin markov networks. In Sebastian Thrun,
Lawrence K. Saul, and Bernhard Scho?lkopf, editors,
NIPS. MIT Press.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector Ma-
chines. In Proceedings of the 8th IWPT, pages 195?
206. Association for Computational Linguistics.
David H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
11
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 9?16
Manchester, August 2008
TAG, Dynamic Programming, and the Perceptron
for Efficient, Feature-rich Parsing
Xavier Carreras Michael Collins Terry Koo
MIT CSAIL, Cambridge, MA 02139, USA
{carreras,mcollins,maestro}@csail.mit.edu
Abstract
We describe a parsing approach that makes use
of the perceptron algorithm, in conjunction with
dynamic programming methods, to recover full
constituent-based parse trees. The formalism allows
a rich set of parse-tree features, including PCFG-
based features, bigram and trigram dependency fea-
tures, and surface features. A severe challenge in
applying such an approach to full syntactic pars-
ing is the efficiency of the parsing algorithms in-
volved. We show that efficient training is feasi-
ble, using a Tree Adjoining Grammar (TAG) based
parsing formalism. A lower-order dependency pars-
ing model is used to restrict the search space of the
full model, thereby making it efficient. Experiments
on the Penn WSJ treebank show that the model
achieves state-of-the-art performance, for both con-
stituent and dependency accuracy.
1 Introduction
In global linear models (GLMs) for structured pre-
diction, (e.g., (Johnson et al, 1999; Lafferty et al,
2001; Collins, 2002; Altun et al, 2003; Taskar et
al., 2004)), the optimal label y? for an input x is
y
?
= arg max
y?Y(x)
w ? f(x, y) (1)
where Y(x) is the set of possible labels for the in-
put x; f(x, y) ? Rd is a feature vector that rep-
resents the pair (x, y); and w is a parameter vec-
tor. This paper describes a GLM for natural lan-
guage parsing, trained using the averaged percep-
tron. The parser we describe recovers full syntac-
tic representations, similar to those derived by a
probabilistic context-free grammar (PCFG). A key
motivation for the use of GLMs in parsing is that
they allow a great deal of flexibility in the features
which can be included in the definition of f(x, y).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
A critical problem when training a GLM for
parsing is the computational complexity of the
inference problem. The averaged perceptron re-
quires the training set to be repeatedly decoded
under the model; under even a simple PCFG rep-
resentation, finding the argmax in Eq. 1 requires
O(n
3
G) time, where n is the length of the sen-
tence, and G is a grammar constant. The average
sentence length in the data set we use (the Penn
WSJ treebank) is over 23 words; the grammar con-
stant G can easily take a value of 1000 or greater.
These factors make exact inference algorithms vir-
tually intractable for training or decoding GLMs
for full syntactic parsing.
As a result, in spite of the potential advantages
of these methods, there has been very little previ-
ous work on applying GLMs for full parsing with-
out the use of fairly severe restrictions or approxi-
mations. For example, the model in (Taskar et al,
2004) is trained on only sentences of 15 words or
less; reranking models (Collins, 2000; Charniak
and Johnson, 2005) restrict Y(x) to be a small set
of parses from a first-pass parser; see section 1.1
for discussion of other related work.
The following ideas are central to our approach:
(1) A TAG-based, splittable grammar. We
describe a novel, TAG-based parsing formalism
that allows full constituent-based trees to be recov-
ered. A driving motivation for our approach comes
from the flexibility of the feature-vector represen-
tations f(x, y) that can be used in the model. The
formalism that we describe allows the incorpora-
tion of: (1) basic PCFG-style features; (2) the
use of features that are sensitive to bigram depen-
dencies between pairs of words; and (3) features
that are sensitive to trigram dependencies. Any
of these feature types can be combined with sur-
face features of the sentence x, in a similar way
9
to the use of surface features in conditional ran-
dom fields (Lafferty et al, 2001). Crucially, in
spite of these relatively rich representations, the
formalism can be parsed efficiently (in O(n4G)
time) using dynamic-programming algorithms de-
scribed by Eisner (2000) (unlike many other TAG-
related approaches, our formalism is ?splittable?
in the sense described by Eisner, leading to more
efficient parsing algorithms).
(2) Use of a lower-order model for pruning.
The O(n4G) running time of the TAG parser is
still too expensive for efficient training with the
perceptron. We describe a method that leverages
a simple, first-order dependency parser to restrict
the search space of the TAG parser in training and
testing. The lower-order parser runs in O(n3H)
time where H ? G; experiments show that it is
remarkably effective in pruning the search space
of the full TAG parser.
Experiments on the Penn WSJ treebank show
that the model recovers constituent structures with
higher accuracy than the approaches of (Charniak,
2000; Collins, 2000; Petrov and Klein, 2007),
and with a similar level of performance to the
reranking parser of (Charniak and Johnson, 2005).
The model also recovers dependencies with sig-
nificantly higher accuracy than state-of-the-art de-
pendency parsers such as (Koo et al, 2008; Mc-
Donald and Pereira, 2006).
1.1 Related Work
Previous work has made use of various restrictions
or approximations that allow efficient training of
GLMs for parsing. This section describes the rela-
tionship between our work and this previous work.
In reranking approaches, a first-pass parser
is used to enumerate a small set of candidate
parses for an input sentence; the reranking model,
which is a GLM, is used to select between these
parses (e.g., (Ratnaparkhi et al, 1994; Johnson et
al., 1999; Collins, 2000; Charniak and Johnson,
2005)). A crucial advantage of our approach is that
it considers a very large set of alternatives in Y(x),
and can thereby avoid search errors that may be
made in the first-pass parser.1
Another approach that allows efficient training
of GLMs is to use simpler syntactic representa-
tions, in particular dependency structures (McDon-
1Some features used within reranking approaches may be
difficult to incorporate within dynamic programming, but it
is nevertheless useful to make use of GLMs in the dynamic-
programming stage of parsing. Our parser could, of course,
be used as the first-stage parser in a reranking approach.
ald et al, 2005). Dependency parsing can be
implemented in O(n3) time using the algorithms
of Eisner (2000). In this case there is no gram-
mar constant, and parsing is therefore efficient. A
disadvantage of these approaches is that they do
not recover full, constituent-based syntactic struc-
tures; the increased linguistic detail in full syntac-
tic structures may be useful in NLP applications,
or may improve dependency parsing accuracy, as
is the case in our experiments.2
There has been some previous work on GLM
approaches for full syntactic parsing that make use
of dynamic programming. Taskar et al (2004)
describe a max-margin approach; however, in this
work training sentences were limited to be of 15
words or less. Clark and Curran (2004) describe
a log-linear GLM for CCG parsing, trained on the
Penn treebank. This method makes use of paral-
lelization across an 18 node cluster, together with
up to 25GB of memory used for storage of dy-
namic programming structures for training data.
Clark and Curran (2007) describe a perceptron-
based approach for CCG parsing which is consid-
erably more efficient, and makes use of a super-
tagging model to prune the search space of the full
parsing model. Recent work (Petrov et al, 2007;
Finkel et al, 2008) describes log-linear GLMs ap-
plied to PCFG representations, but does not make
use of dependency features.
2 The TAG-Based Parsing Model
2.1 Derivations
This section describes the idea of derivations in
our parsing formalism. As in context-free gram-
mars or TAGs, a derivation in our approach is a
data structure that specifies the sequence of opera-
tions used in combining basic (elementary) struc-
tures in a grammar, to form a full parse tree. The
parsing formalism we use is related to the tree ad-
joining grammar (TAG) formalisms described in
(Chiang, 2003; Shen and Joshi, 2005). However,
an important difference of our work from this pre-
vious work is that our formalism is defined to be
?splittable?, allowing use of the efficient parsing
algorithms of Eisner (2000).
A derivation in our model is a pair ?E,D? where
E is a set of spines, and D is a set of dependencies
2Note however that the lower-order parser that we use to
restrict the search space of the TAG-based parser is based on
the work of McDonald et al (2005). See also (Sagae et al,
2007) for a method that uses a dependency parser to restrict
the search space of a more complex HPSG parser.
10
(a) S
VP
VBD
ate
NP
NN
cake
(b) S
VP
VP
VBD
ate
NP
NN
cake
Figure 1: Two example trees.
specifying how the spines are combined to form
a parse tree. The spines are similar to elementary
trees in TAG. Some examples are as follows:
NP
NNP
John
S
VP
VBD
ate
NP
NN
cake
ADVP
RB
quickly
ADVP
RB
luckily
These structures do not have substitution nodes, as
is common in TAGs.3 Instead, the spines consist
of a lexical anchor together with a series of unary
projections, which usually correspond to different
X-bar levels associated with the anchor.
The operations used to combine spines are sim-
ilar to the TAG operations of adjunction and sis-
ter adjunction. We will call these operations regu-
lar adjunction (r-adjunction) and sister adjunction
(s-adjunction). As one example, the cake spine
shown above can be s-adjoined into the VP node of
the ate spine, to form the tree shown in figure 1(a).
In contrast, if we use the r-adjunction operation to
adjoin the cake tree into the VP node, we get a dif-
ferent structure, which has an additional VP level
created by the r-adjunction operation: the resulting
tree is shown in figure 1(b). The r-adjunction op-
eration is similar to the usual adjunction operation
in TAGs, but has some differences that allow our
grammars to be splittable; see section 2.3 for more
discussion.
We now give formal definitions of the sets E and
D. Take x to be a sentence consisting of n + 1
words, x
0
. . . x
n
, where x
0
is a special root sym-
bol, which we will denote as ?. A derivation for the
input sentence x consists of a pair ?E,D?, where:
? E is a set of (n + 1) tuples of the form ?i, ??,
where i ? {0 . . . n} is an index of a word in the
sentence, and ? is the spine associated with the
word x
i
. The set E specifies one spine for each
of the (n + 1) words in the sentence. Where it is
3It would be straightforward to extend the approach to in-
clude substitution nodes, and a substitution operation.
clear from context, we will use ?
i
to refer to the
spine in E corresponding to the i?th word.
? D is a set of n dependencies. Each depen-
dency is a tuple ?h,m, l?. Here h is the index of
the head-word of the dependency, corresponding
to the spine ?
h
which contains a node that is being
adjoined into. m is the index of the modifier-word
of the dependency, corresponding to the spine ?
m
which is being adjoined into ?
h
. l is a label.
The label l is a tuple ?POS, A, ?
h
, ?
m
, L?. ?
h
and
?
m
are the head and modifier spines that are be-
ing combined. POS specifies which node in ?
h
is
being adjoined into. A is a binary flag specifying
whether the combination operation being used is s-
adjunction or r-adjunction. L is a binary flag spec-
ifying whether or not any ?previous? modifier has
been r-adjoined into the position POS in ?
h
. By a
previous modifier, we mean a modifier m? that was
adjoined from the same direction as m (i.e., such
that h < m? < m or m < m? < h).
It would be sufficient to define l to be the pair
?POS, A??the inclusion of ?
h
, ?
m
and L adds re-
dundant information that can be recovered from
the set E, and other dependencies in D?but it
will be convenient to include this information in
the label. In particular, it is important that given
this definition of l, it is possible to define a func-
tion GRM(l) that maps a label l to a triple of non-
terminals that represents the grammatical relation
between m and h in the dependency structure. For
example, in the tree shown in figure 1(a), the gram-
matical relation between cake and ate is the triple
GRM(l) = ?VP VBD NP?. In the tree shown in
figure 1(b), the grammatical relation between cake
and ate is the triple GRM(l) = ?VP VP NP?.
The conditions under which a pair ?E,D? forms
a valid derivation for a sentence x are similar to
those in conventional LTAGs. Each ?i, ?? ? E
must be such that ? is an elementary tree whose
anchor is the word x
i
. The dependencies D must
form a directed, projective tree spanning words
0 . . . n, with ? at the root of this tree, as is also
the case in previous work on discriminative ap-
proches to dependency parsing (McDonald et al,
2005). We allow any modifier tree ?
m
to adjoin
into any position in any head tree ?
h
, but the de-
pendencies D must nevertheless be coherent?for
example they must be consistent with the spines in
E, and they must be nested correctly.4 We will al-
4For example, closer modifiers to a particular head must
adjoin in at the same or a lower spine position than modifiers
11
low multiple modifier spines to s-adjoin or r-adjoin
into the same node in a head spine; see section 2.3
for more details.
2.2 A Global Linear Model
The model used for parsing with this approach is
a global linear model. For a given sentence x, we
define Y(x) to be the set of valid derivations for x,
where each y ? Y(x) is a pair ?E,D? as described
in the previous section. A function f maps (x, y)
pairs to feature-vectors f(x, y) ? Rd. The param-
eter vector w is also a vector in Rd. Given these
definitions, the optimal derivation for an input sen-
tence x is y? = argmax
y?Y(x)
w ? f(x, y).
We now come to how the feature-vector f(x, y)
is defined in our approach. A simple ?first-order?
model would define
f(x, y) =
?
?i,???E(y)
e(x, ?i, ??) +
?
?h,m,l??D(y)
d(x, ?h,m, l?) (2)
Here we use E(y) and D(y) to respectively refer
to the set of spines and dependencies in y. The
function e maps a sentence x paired with a spine
?i, ?? to a feature vector. The function d maps de-
pendencies within y to feature vectors. This de-
composition is similar to the first-order model of
McDonald et al (2005), but with the addition of
the e features.
We will extend our model to include higher-
order features, in particular features based on sib-
ling dependencies (McDonald and Pereira, 2006),
and grandparent dependencies, as in (Carreras,
2007). If y = ?E,D? is a derivation, then:
? S(y) is a set of sibling dependencies. Each
sibling dependency is a tuple ?h,m, l, s?. For each
?h,m, l, s? ? S the tuple ?h,m, l? is an element of
D; there is one member of S for each member of
D. The index s is the index of the word that was
adjoined to the spine for h immediately before m
(or the NULL symbol if no previous adjunction has
taken place).
? G(y) is a set of grandparent dependencies of
type 1. Each type 1 grandparent dependency is a
tuple ?h,m, l, g?. There is one member of G for
every member of D. The additional information,
the index g, is the index of the word that is the first
modifier to the right of the spine for m.
that are further from the head.
(a) S
VP
VP
VP
VBD
ate
NP
NN
cake
NP
NN
today
ADVP
RB
quickly
(b) S
NP
NNP
John
VP
ADVP
RB
luckily
VP
VBD
ate
Figure 2: Two Example Trees
S
NP
NNP
John
VP
VP
ADVP
RB
luckily
VP
VBD
ate
NP
NN
cake
NP
NN
today
ADVP
RB
quickly
Figure 3: An example tree, formed by a combina-
tion of the two structures in figure 2.
? Q(y) is an additional set of grandparent de-
pendencies, of type 2. Each of these dependencies
is a tuple ?h,m, l, q?. Again, there is one member
of Q for every member of D. The additional infor-
mation, the index q, is the index of the word that is
the first modifier to the left of the spine for m.
The feature-vector definition then becomes:
f(x, y) =
X
?i,???E(y)
e(x, ?i, ??) +
X
?h,m,l??D(y)
d(x, ?h,m, l?) +
X
?h,m,l,s??S(y)
s(x, ?h,m, l, s?) +
X
?h,m,l,g??G(y)
g(x, ?h,m, l, g?) +
X
?h,m,l,q??Q(y)
q(x, ?h,m, l, q?)
(3)
where s, g and q are feature vectors corresponding
to the new, higher-order elements.5
2.3 Recovering Parse Trees from Derivations
As in TAG approaches, there is a mapping from
derivations ?E,D? to parse trees (i.e., the type of
trees generated by a context-free grammar). In our
case, we map a spine and its dependencies to a con-
stituent structure by first handling the dependen-
5We also added constituent-boundary features to the
model, which is a simple change that led to small improve-
ments on validation data; for brevity we omit the details.
12
cies on each side separately and then combining
the left and right sides.
First, it is straightforward to build the con-
stituent structure resulting from multiple adjunc-
tions on the same side of a spine. As one exam-
ple, the structure in figure 2(a) is formed by first
s-adjoining the spine with anchor cake into the VP
node of the spine for ate, then r-adjoining spines
anchored by today and quickly into the same node,
where all three modifier words are to the right of
the head word. Notice that each r-adjunction op-
eration creates a new VP level in the tree, whereas
s-adjunctions do not create a new level. Now con-
sider a tree formed by first r-adjoining a spine for
luckily into the VP node for ate, followed by s-
adjoining the spine for John into the S node, in
both cases where the modifiers are to the left of
the head. In this case the structure that would be
formed is shown in figure 2(b).
Next, consider combining the left and right
structures of a spine. The main issue is how to
handle multiple r-adjunctions or s-adjunctions on
both sides of a node in a spine, because our deriva-
tions do not specify how adjunctions from different
sides embed with each other. In our approach, the
combination operation preserves the height of the
different modifiers from the left and right direc-
tions. To illustrate this, figure 3 shows the result
of combining the two structures in figure 2. The
combination of the left and right modifier struc-
tures has led to flat structures, for example the rule
VP? ADVP VP NP in the above tree.
Note that our r-adjunction operation is different
from the usual adjunction operation in TAGs, in
that ?wrapping? adjunctions are not possible, and
r-adjunctions from the left and right directions are
independent from each other; because of this our
grammars are splittable.
3 Parsing Algorithms
3.1 Use of Eisner?s Algorithms
This section describes the algorithm for finding
y
?
= argmax
y?Y(x)
w ? f(x, y) where f(x, y) is
defined through either the first-order model (Eq. 2)
or the second-order model (Eq. 3).
For the first-order model, the methods described
in (Eisner, 2000) can be used for the parsing algo-
rithm. In Eisner?s algorithms for dependency pars-
ing each word in the input has left and right finite-
state (weighted) automata, which generate the left
and right modifiers of the word in question. We
make use of this idea of automata, and also make
direct use of the method described in section 4.2 of
(Eisner, 2000) that allows a set of possible senses
for each word in the input string. In our use of
the algorithm, each possible sense for a word cor-
responds to a different possible spine that can be
associated with that word. The left and right au-
tomata are used to keep track of the last position
in the spine that was adjoined into on the left/right
of the head respectively. We can make use of sep-
arate left and right automata?i.e., the grammar is
splittable?because left and right modifiers are ad-
joined independently of each other in the tree. The
extension of Eisner?s algorithm to the second-order
model is similar to the algorithm described in (Car-
reras, 2007), but again with explicit use of word
senses and left/right automata. The resulting algo-
rithms run in O(Gn3) and O(Hn4) time for the
first-order and second-order models respectively,
where G and H are grammar constants.
3.2 Efficient Parsing
The efficiency of the parsing algorithm is impor-
tant in applying the parsing model to test sen-
tences, and also when training the model using dis-
criminative methods. The grammar constants G
and H introduced in the previous section are poly-
nomial in factors such as the number of possible
spines in the model, and the number of possible
states in the finite-state automata implicit in the
parsing algorithm. These constants are large, mak-
ing exhaustive parsing very expensive.
To deal with this problem, we use a simple ini-
tial model to prune the search space of the more
complex model. The first-stage model we use
is a first-order dependency model, with labeled
dependencies, as described in (McDonald et al,
2005). As described shortly, we will use this model
to compute marginal scores for dependencies in
both training and test sentences. A marginal score
?(x, h,m, l) is a value between 0 and 1 that re-
flects the plausibility of a dependency for sentence
x with head-word x
h
, modifier word x
m
, and la-
bel l. In the first-stage pruning model the labels l
are triples of non-terminals representing grammat-
ical relations, as described in section 2.1 of this
paper?for example, one possible label would be
?VP VBD NP?, and in general any triple of non-
terminals is possible.
Given a sentence x, and an index m of a word
in that sentence, we define DMAX(x,m) to be the
13
highest scoring dependency with m as a modifier:
DMAX(x,m) = max
h,l
?(x, h,m, l)
For a sentence x, we then define the set of allow-
able dependencies to be
pi(x) = {?h,m, l? : ?(x, h,m, l) ? ?DMAX(x,m)}
where ? is a constant dictating the beam size that
is used (in our experiments we used ? = 10?6).
The set pi(x) is used to restrict the set of pos-
sible parses under the full TAG-based model. In
section 2.1 we described how the TAG model has
dependency labels of the form ?POS, A, ?
h
, ?
m
, L?,
and that there is a function GRM that maps labels
of this form to triples of non-terminals. The ba-
sic idea of the pruned search is to only allow de-
pendencies of the form ?h,m, ?POS, A, ?
h
, ?
m
, L??
if the tuple ?h,m, GRM(?POS, A, ?
h
, ?
m
, L?)? is a
member of pi(x), thus reducing the search space
for the parser.
We now turn to how the marginals ?(x, h,m, l)
are defined and computed. A simple approach
would be to use a conditional log-linear model
(Lafferty et al, 2001), with features as defined by
McDonald et al (2005), to define a distribution
P (y|x) where the parse structures y are depen-
dency structures with labels that are triples of non-
terminals. In this case we could define
?(x, h,m, l) =
?
y:(h,m,l)?y
P (y|x)
which can be computed with inside-outside style
algorithms, applied to the data structures from
(Eisner, 2000). The complexity of training and ap-
plying such a model is again O(Gn3), where G is
the number of possible labels, and the number of
possible labels (triples of non-terminals) is around
G = 1000 in the case of treebank parsing; this
value for G is still too large for the method to be ef-
ficient. Instead, we train three separate models ?
1
,
?
2
, and ?
3
for the three different positions in the
non-terminal triples. We then take ?(x, h,m, l) to
be a product of these three models, for example we
would calculate
?(x, h,m, ?VP VBD NP?) =
?
1
(x, h,m, ?VP?)? ?
2
(x, h,m, ?VBD?)
??
3
(x, h,m, ?NP?)
Training the three models, and calculating the
marginals, now has a grammar constant equal
to the number of non-terminals in the grammar,
which is far more manageable. We use the algo-
rithm described in (Globerson et al, 2007) to train
the conditional log-linear model; this method was
found to converge to a good model after 10 itera-
tions over the training data.
4 Implementation Details
4.1 Features
Section 2.2 described the use of feature vectors
associated with spines used in a derivation, to-
gether with first-order, sibling, and grandparent
dependencies. The dependency features used in
our experiments are closely related to the features
described in (Carreras, 2007), which are an ex-
tension of the McDonald and Pereira (2006) fea-
tures to cover grandparent dependencies in addi-
tion to first-order and sibling dependencies. The
features take into account the identity of the la-
bels l used in the derivations. The features could
potentially look at any information in the la-
bels, which are of the form ?POS, A, ?
h
, ?
m
, L?,
but in our experiments, we map labels to a pair
(GRM(?POS, A, ?
h
, ?
m
, L?), A). Thus the label fea-
tures are sensitive only to the triple of non-
terminals corresponding to the grammatical rela-
tion involved in an adjunction, and a binary flag
specifiying whether the operation is s-adjunction
or r-adjunction.
For the spine features e(x, ?i, ??), we use fea-
ture templates that are sensitive to the identity of
the spine ?, together with contextual features of
the string x. These features consider the iden-
tity of the words and part-of-speech tags in a win-
dow that is centered on x
i
and spans the range
x
(i?2)
. . . x
(i+2)
.
4.2 Extracting Derivations from Parse Trees
In the experiments in this paper, the following
three-step process was used: (1) derivations were
extracted from a training set drawn from the Penn
WSJ treebank, and then used to train a parsing
model; (2) the test data was parsed using the re-
sulting model, giving a derivation for each test
data sentence; (3) the resulting test-data deriva-
tions were mapped back to Penn-treebank style
trees, using the method described in section 2.1.
To achieve step (1), we first apply a set of head-
finding rules which are similar to those described
in (Collins, 1997). Once the head-finding rules
have been applied, it is straightforward to extract
14
precision recall F
1
PPK07 ? ? 88.3
FKM08 88.2 87.8 88.0
CH2000 89.5 89.6 89.6
CO2000 89.9 89.6 89.8
PK07 90.2 89.9 90.1
this paper 91.4 90.7 91.1
CJ05 ? ? 91.4
H08 ? ? 91.7
CO2000(s24) 89.6 88.6 89.1
this paper (s24) 91.1 89.9 90.5
Table 1: Results for different methods. PPK07, FKM08,
CH2000, CO2000, PK07, CJ05 and H08 are results on section
23 of the Penn WSJ treebank, for the models of Petrov et al
(2007), Finkel et al (2008), Charniak (2000), Collins (2000),
Petrov and Klein (2007), Charniak and Johnson (2005), and
Huang (2008). (CJ05 is the performance of an updated
model at http://www.cog.brown.edu/mj/software.htm.) ?s24?
denotes results on section 24 of the treebank.
s23 s24
KCC08 unlabeled 92.0 91.0
KCC08 labeled 92.5 91.7
this paper 93.5 92.5
Table 2: Table showing unlabeled dependency accuracy for
sections 23 and 24 of the treebank, using the method of (Ya-
mada and Matsumoto, 2003) to extract dependencies from
parse trees from our model. KCC08 unlabeled is from (Koo
et al, 2008), a model that has previously been shown to have
higher accuracy than (McDonald and Pereira, 2006). KCC08
labeled is the labeled dependency parser from (Koo et al,
2008); here we only evaluate the unlabeled accuracy.
derivations from the Penn treebank trees.
Note that the mapping from parse trees to
derivations is many-to-one: for example, the ex-
ample trees in section 2.3 have structures that are
as ?flat? (have as few levels) as is possible, given
the set D that is involved. Other similar trees,
but with more VP levels, will give the same set
D. However, this issue appears to be benign in the
Penn WSJ treebank. For example, on section 22 of
the treebank, if derivations are first extracted using
the method described in this section, then mapped
back to parse trees using the method described in
section 2.3, the resulting parse trees score 100%
precision and 99.81% recall in labeled constituent
accuracy, indicating that very little information is
lost in this process.
4.3 Part-of-Speech Tags, and Spines
Sentences in training, test, and development data
are assumed to have part-of-speech (POS) tags.
POS tags are used for two purposes: (1) in the
features described above; and (2) to limit the set
of allowable spines for each word during parsing.
Specifically, for each POS tag we create a separate
1st stage 2nd stage
? active coverage oracle F
1
speed F
1
10
?4 0.07 97.7 97.0 5:15 91.1
10
?5 0.16 98.5 97.9 11:45 91.6
10
?6 0.34 99.0 98.5 21:50 92.0
Table 3: Effect of the beam size, controlled by ?, on the
performance of the parser on the development set (1,699 sen-
tences). In each case ? refers to the beam size used in both
training and testing the model. ?active?: percentage of de-
pendencies that remain in the beam out of the total number of
labeled dependencies (1,000 triple labels times 1,138,167 un-
labeled dependencies); ?coverage?: percentage of correct de-
pendencies in the beam out of the total number of correct de-
pendencies. ?oracle F
1
?: maximum achievable score of con-
stituents, given the beam. ?speed?: parsing time in min:sec
for the TAG-based model (this figure does not include the time
taken to calculate the marginals using the lower-order model);
?F
1
?: score of predicted constituents.
dictionary listing the spines that have been seen
with this POS tag in training data; during parsing
we only allow spines that are compatible with this
dictionary. (For test or development data, we used
the part-of-speech tags generated by the parser of
(Collins, 1997). Future work should consider in-
corporating the tagging step within the model; it is
not challenging to extend the model in this way.)
5 Experiments
Sections 2-21 of the Penn Wall Street Journal tree-
bank were used as training data in our experiments,
and section 22 was used as a development set. Sec-
tions 23 and 24 were used as test sets. The model
was trained for 20 epochs with the averaged per-
ceptron algorithm, with the development data per-
formance being used to choose the best epoch. Ta-
ble 1 shows the results for the method.
Our experiments show an improvement in per-
formance over the results in (Collins, 2000; Char-
niak, 2000). We would argue that the Collins
(2000) method is considerably more complex than
ours, requiring a first-stage generative model, to-
gether with a reranking approach. The Char-
niak (2000) model is also arguably more com-
plex, again using a carefully constructed genera-
tive model. The accuracy of our approach also
shows some improvement over results in (Petrov
and Klein, 2007). This work makes use of a
PCFG with latent variables that is trained using
a split/merge procedure together with the EM al-
gorithm. This work is in many ways comple-
mentary to ours?for example, it does not make
use of GLMs, dependency features, or of repre-
sentations that go beyond PCFG productions?and
15
some combination of the two methods may give
further gains.
Charniak and Johnson (2005), and Huang
(2008), describe approaches that make use of non-
local features in conjunction with the Charniak
(2000) model; future work may consider extend-
ing our approach to include non-local features.
Finally, other recent work (Petrov et al, 2007;
Finkel et al, 2008) has had a similar goal of scal-
ing GLMs to full syntactic parsing. These mod-
els make use of PCFG representations, but do not
explicitly model bigram or trigram dependencies.
The results in this work (88.3%/88.0% F
1
) are
lower than our F
1
score of 91.1%; this is evidence
of the benefits of the richer representations enabled
by our approach.
Table 2 shows the accuracy of the model in
recovering unlabeled dependencies. The method
shows improvements over the method described
in (Koo et al, 2008), which is a state-of-the-art
second-order dependency parser similar to that of
(McDonald and Pereira, 2006), suggesting that the
incorporation of constituent structure can improve
dependency accuracy.
Table 3 shows the effect of the beam-size on the
accuracy and speed of the parser on the develop-
ment set. With the beam setting used in our exper-
iments (? = 10?6), only 0.34% of possible depen-
dencies are considered by the TAG-based model,
but 99% of all correct dependencies are included.
At this beam size the best possible F
1
constituent
score is 98.5. Tighter beams lead to faster parsing
times, with slight drops in accuracy.
6 Conclusions
We have described an efficient and accurate parser
for constituent parsing. A key to the approach has
been to use a splittable grammar that allows effi-
cient dynamic programming algorithms, in com-
bination with pruning using a lower-order model.
The method allows relatively easy incorporation of
features; future work should leverage this in pro-
ducing more accurate parsers, and in applying the
parser to different languages or domains.
Acknowledgments X. Carreras was supported by the
Catalan Ministry of Innovation, Universities and Enterprise,
by the GALE program of DARPA, Contract No. HR0011-06-
C-0022, and by a grant from NTT, Agmt. Dtd. 6/21/1998.
T. Koo was funded by NSF grant IIS-0415030. M. Collins
was funded by NSF grant IIS-0347631 and DARPA contract
No. HR0011-06-C-0022. Thanks to Jenny Rose Finkel for
suggesting that we evaluate dependency parsing accuracies.
References
Altun, Y., I. Tsochantaridis, and T. Hofmann. 2003. Hidden
markov support vector machines. In ICML.
Carreras, X. 2007. Experiments with a higher-order projec-
tive dependency parser. In Proc. EMNLP-CoNLL Shared
Task.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
ACL.
Charniak, E. 2000. A maximum-entropy-inspired parser. In
Proc. NAACL.
Chiang, D. 2003. Statistical parsing with an automatically
extracted tree adjoining grammar. In Bod, R., R. Scha, and
K. Sima?an, editors, Data Oriented Parsing, pages 299?
316. CSLI Publications.
Clark, S. and J. R. Curran. 2004. Parsing the wsj using ccg
and log-linear models. In Proc. ACL.
Clark, Stephen and James R. Curran. 2007. Perceptron train-
ing for a wide-coverage lexicalized-grammar parser. In
Proc. ACL Workshop on Deep Linguistic Processing.
Collins, M. 1997. Three generative, lexicalised models for
statistical parsing. In Proc. ACL.
Collins, M. 2000. Discriminative reranking for natural lan-
guage parsing. In Proc. ICML.
Collins, M. 2002. Discriminative training methods for hid-
den markov models: Theory and experiments with percep-
tron algorithms. In Proc. EMNLP.
Eisner, J. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In Bunt, H. C. and A. Nijholt, editors,
New Developments in Natural Language Parsing, pages
29?62. Kluwer Academic Publishers.
Finkel, J. R., A. Kleeman, and C. D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing. In
Proc. ACL/HLT.
Globerson, A., T. Koo, X. Carreras, and M. Collins. 2007.
Exponentiated gradient algorithms for log-linear struc-
tured prediction. In Proc. ICML.
Huang, L. 2008. Forest reranking: Discriminative parsing
with non-local features. In Proc. ACL/HLT.
Johnson, M., S. Geman, S. Canon, Z. Chi, and S. Riezler.
1999. Estimators for stochastic unification-based gram-
mars. In Proc. ACL.
Koo, Terry, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
ACL/HLT.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditonal
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. ICML.
McDonald, R. and F. Pereira. 2006. Online learning of ap-
proximate dependency parsing algorithms. In Proc. EACL.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In Proc.
ACL.
Petrov, S. and D. Klein. 2007. Improved inference for unlex-
icalized parsing. In Proc. of HLT-NAACL.
Petrov, S., A. Pauls, and D. Klein. 2007. Discriminative log-
linear grammars with latent variables. In Proc. NIPS.
Ratnaparkhi, A., S. Roukos, and R. Ward. 1994. A maximum
entropy model for parsing. In Proc. ICSLP.
Sagae, Kenji, Yusuke Miyao, and Jun?ichi Tsujii. 2007. Hpsg
parsing with shallow dependency constraints. In Proc.
ACL, pages 624?631.
Shen, L. and A.K. Joshi. 2005. Incremental ltag parsing. In
Proc HLT-EMNLP.
Taskar, B., D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of the
EMNLP-2004.
Yamada, H. and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. IWPT.
16
