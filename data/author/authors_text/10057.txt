Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 72?75,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Speaking without knowing what to say? or when to end 
 
Anna Hjalmarsson 
Centre for Speech Technology  
KTH 
SE-10044, Stockholm, Sweden 
annah@speech.kth.se 
 
 
 
 
 
 
Abstract 
Humans produce speech incrementally and 
on-line as the dialogue progresses using in-
formation from several different sources in 
parallel. A dialogue system that generates 
output in a stepwise manner and not in pre-
planned syntactically correct sentences needs 
to signal how new dialogue contributions re-
late to previous discourse. This paper de-
scribes a data collection which is the 
foundation for an effort towards more human-
like language generation in DEAL, a spoken 
dialogue system developed at KTH. Two an-
notators labelled cue phrases in the corpus 
with high inter-annotator agreement (kappa 
coefficient 0.82). 
1 Introduction 
This paper describes a data collection with the goal 
of modelling more human-like language generation 
in DEAL, a spoken dialogue system developed at 
KTH. The DEAL objectives are to build a system 
which is fun, human-like, and engaging to talk to, 
and which gives second language learners of 
Swedish conversation training (as described in 
Hjalmarsson et al, 2007). The scene of DEAL is 
set at a flea market where a talking animated agent 
is the owner of a shop selling used objects. The 
student is given a mission: to buy items from the 
shop-keeper at the best possible price by bargain-
ing. From a language learning perspective and to 
keep the students motivated, the agent?s language 
is crucial. The agent needs to behave human-like in 
a way which allows the users to suspend some of 
their disbeliefs and talk to DEAL as if talking to 
another human being. In an experimental study 
(Hjalmarsson & Edlund, in press), where a spoken 
dialogue system with human behaviour was simu-
lated, two different systems were compared: a rep-
lica of human behaviour and a constrained version 
with less variability. The version based on human 
behaviour was rated as more human-like, polite 
and intelligent. 
1.1 Human language production 
Humans produce speech incrementally and on-line 
as the dialogue progresses using information from 
several different sources in parallel (Brennan, 
2000; Aist et al, 2006). We anticipate what the 
other person is about to say in advance and start 
planning our next move while this person is still 
speaking. When starting to speak, we typically do 
not have a complete plan of how to say something 
or even what to say. Yet, we manage to rapidly 
integrate information from different sources in par-
allel and simultaneously plan and realize new dia-
logue contributions. Pauses, corrections and 
repetitions are used to stepwise refine, alter and 
revise our plans as we speak (Clark & Wasow, 
1998). These human behaviours bring valuable 
information that contains more than the literal 
meanings of the words (Arnold et al, 2003).  
In order to generate output incrementally in 
DEAL we need extended knowledge on how to 
signal relations between different segments of 
speech. In this paper we report on a data collection 
of human-human dialogue aiming at extending the 
knowledge of human interaction and in particular 
to distinguish different types of cue phrases used in 
the DEAL domain. 
72
2 The DEAL corpus collection 
The dialogue data recorded was informal, human-
human, face-to-face conversation. The task and the 
recording environment were set up to mimic the 
DEAL domain and role play.  
2.1 Data collection 
The data collection was made with 6 subjects (4 
male and 2 female), 2 posing as shop keepers and 4 
as potential buyers. Each customer interacted with 
the same shop-keeper twice, in two different sce-
narios. The shop-keepers and customers were in-
structed separately. The customers were given a 
mission: to buy items at a flea market at the best 
possible price from the shop-keeper. The task was 
to buy 3 objects for a specific purpose (e.g. to buy 
tools to repair a house). The customers were given 
a certain amount of toy money, however not 
enough to buy what they were instructed to buy 
without bargaining. The shop-keeper sat behind a 
desk with images of different objects pinned to the 
wall behind him. Some of the object had obvious 
flaws, for example a puzzle with a missing piece, 
to open up for interesting negotiation. None of the 
shop-keepers had any professional experience of 
bargaining, which was appropriate since we were 
more interested in capturing na?ve conceptual 
metaphors of bargaining rather than real life price 
negotiation. Each dialogue was about 15 minutes 
long, so about 2 hours of speech were collected 
altogether. The shop-keepers used an average of 
13.4 words per speaker turn while the buyers? turns 
were generally shorter, 8.5 words per turn (in this 
paper turn always refers to speaker turns). In total 
16357 words were collected. 
3 Annotation  
All dialogues were first transcribed orthographi-
cally including non-lexical entities such as laughter 
and hawks. Filled pauses, repetitions, corrections 
and restarts were also labelled manually. 
3.1 Cue phrases 
Linguistic devices used to signal relations between 
different segments of speech are often referred to 
as cue phrases. Other frequently used terms are 
discourse markers, pragmatic markers or discourse 
particles. Typical cue phrases in English are: oh, 
well, now, then, however, you know, I mean, be-
cause, and, but and or. Much research within dis-
course analysis, communicative analysis and 
psycholinguistics has been concerned with these 
connectives and what kind of relations they hold 
(for an overview see Schourup, 1999). Our defini-
tion of cue phrases is broad and all types of lin-
guistic entities that the speakers use to hold the 
dialogue together at different communicative lev-
els are included. A rule of thumb is that cue 
phrases are words or chunks of words that have 
little lexical impact at the local speech segment 
level but serve significant pragmatic function. To 
give an exact definition of what cue phrases are is 
difficult, as these entities often are ambiguous. Ac-
cording to the definition used here, cue phrases can 
be a single word or larger units, occupy various 
positions, belong to different syntactic classes, and 
be realized with different prosodic contours.  
The first dialogue was analyzed and used 
to decide which classes to use in the annotation 
scheme. Nine of the classes were a subset of the 
functional classification scheme of discourse 
markers presented in Lindstr?m (2008). A tenth 
class, referring, was added. There were 3 different 
classes for connectives, 3 classes for responsives 
and 4 remaining classes. The classes are presented 
in Table 1; the first row contains an example in its 
context from data, the word(s) in bold are the la-
belled cue phrase, and the second row presents fre-
quently used instances of that class. 
 
Additive Connectives (CAD) 
och gr?nt ?r ju fint 
[and green is nice] 
och, allts?, s? 
[and, therefore, so] 
Contrastive Connectives (CC) 
men den ?r ganska antik  
[but it is pretty antique] 
men, fast, allts? 
[but, although, thus] 
Alternative Connectives (CAL) 
som jag kan titta p? ist?llet  
[which I can look at instead] 
eller, ist?llet [or, instead] 
Responsive (R) 
ja jag tycker ju det  
[yeah I actually think so] 
ja, mm, jaha, ok  
[yes, mm, yeah, ok] 
Responsive New Information (RNI) 
jaha har du n?gra s?dana  
[right do you have any of those] 
jaha, ok, ja, mm 
 [right, ok, yes, mm] 
73
Responsive Disprefrence (RD) 
ja men det ?r klart dom funkar  
[yeah but of course they work] 
ja, mm, jo [yes, mm, sure] 
Response Eliciting (RE) 
vad ska du ha f?r den d? 
[how much do you want for that one then] 
d?, eller hur [then, right] 
Repair Correction (RC) 
nej nu sa jag fel 
 [no now I said wrong] 
nej, jag menade [no, I meant] 
Modifying (MOD) 
ja jag tycker ju det  
[yeah I actually think so] 
ju, liksom, jag tycker ju det [of course, so to speak, I like] 
Referring (REF) 
fyra hundra kronor sa vi  
[four hundred crowns we said] 
sa vi, sa vi inte det [we said, wasn?t that what we said] 
 
Table 1: The DEAL annotation scheme 
 
The labelling of cue phrases included a two-fold 
task, both to decide if a word was a cue phrase or 
not ? a binary task ? but also to classify which 
functional class it belongs to according to the an-
notation scheme. The annotators could both see the 
transcriptions and listen to the recordings while 
labelling. 81% of the speaker turns contained at 
least one cue phrase and 21% of all words were 
labelled as cue phrases. Table 2 presents the distri-
bution of cue phrases over the different classes.  
0%
15%
30%
MOD R CAD CC RD RNI RE REF RC CAL
 
Table 2: Cue phrase distribution over the different classes  
 
Two of the eight dialogues were annotated by two 
different annotators. A kappa coefficient was cal-
culated on word level. The kappa coefficient for 
the binary task, to classify if a word was a cue 
phrase or not, was 0.87 (p=0.05). The kappa coef-
ficient for the classification task was 0.82 (p=0.05). 
Three of the classes, referring, connective alterna-
tive and repair correction, had very few instances. 
The agreement in percentage distributed over the 
different classes is presented in Table 3. 
0%
20%
40%
60%
80%
100%
M
O
D R
CA
D CC RD RN
I
RE RE
F
RC CA
L
 
Table 3: % agreement for the different classes  
4 Data analysis 
To separate cue phrases from other lexical entities 
and to determine what they signal is a complex 
task. The DEAL corpus is rich in disfluencies and 
cue phrases; 86% of the speaker turns contained at 
least one cue phrase or disfluency. The annotators 
had access to the context and were allowed to lis-
ten to the recordings while labelling. The respon-
sives were generally single words or non lexical 
units (e.g. ?mm?) and appeared in similar dialogue 
contexts (i.e. as responses to assertions). The clas-
sification is likely based on their prosodic realiza-
tion. Acoustic analysis is needed in order to see if 
and how they differ in prosodic contour. In 
Hirschberg & Litman (1993) prosodic analysis is 
used to distinguish between discourse and senten-
tial use of cue phrases. Table 4 presents how the 
different cue phrases were distributed over speaker 
turns, at initial, middle or end position. 
0%
20%
40%
60%
80%
100%
M
O
D R
CA
D
CC RD RN
I
RE R
EF R
C
CA
L Al
l
end
middle
initial
 
Table 4: Turn position distribution  
5 Generation in DEAL 
The collected and labelled data is a valuable re-
source of information for what cue phrases signal 
in the DEAL domain as well as how they are lexi-
cally and prosodically realized. To keep the re-
74
sponse times constant and without unnaturally long 
delays, DEAL needs to be capable of grabbing the 
turn, hold it while the system is producing the rest 
of the message, and release it after completion. 
DEAL is implemented using components from the 
Higgins project (Skantze et al, 2006) an off-the-
shelf ASR system and a GUI with an embodied 
conversational agent (ECA) (Beskow, 2003). A 
current research challenge is to redesign the mod-
ules and architecture for incremental processing, to 
allow generation of conversational speech. Deep 
generation in DEAL ? the decision of what to say 
on an abstract semantic level ? is distributed over 
three different modules; (1) the action manger, (2) 
the agent manager and the (3) communicative 
manager. The action manger is responsible for ac-
tions related to user input and previous discourse1. 
The agent manager represents the agents? personal 
motivations and personality. DEAL uses mixed 
initiative and the agent manager takes initiatives. It 
may for example try to promote certain objects or 
suggest prices of objects in focus. It also generates 
emotional facial gestures related to events in the 
dialogue. The communicative manager generates 
responses on a communicative level based on shal-
low analysis of input. For example, it initiates re-
quests for confirmations if speech recognition 
confidence scores are low. This module initiates 
utterances when the user yields the floor, regard-
less of whether the system has a complete plan of 
what to say or not. Using similar strategies as the 
subjects recorded here, the dialogue system can 
grab the turn and start to say something before 
having completed processing input. Many cue 
phrases were used in combination, signalling func-
tion on different discourse levels; first a simple 
responsive, saying that the previous message was 
perceived, and then some type of connective to 
signal how the new contribution relates.  
6 Final remarks 
Since DEAL focuses on generation in role play, we 
are less interested in the ambiguous cue phrases 
and more concerned with the instances where the 
annotators agreed. The DEAL users are second 
language learners with poor knowledge in Swed-
ish, and it may even be advisable that the agent?s 
behaviour is exaggerated. 
                                                          
1
 For more details on the discourse modeller see Skantze et al 
2006.  
Acknowledgments 
This research was carried out at Centre for Speech 
Technology, KTH. The research is also supported 
by the Swedish research council project #2007-
6431, GENDIAL and the Graduate School for Lan-
guage Technology (GSLT). Many thanks to Jenny 
Klarenfjord for help on data collection and annota-
tion and thanks to Rolf Carlson, Preben Wik and 
Jens Edlund for valuable comments.  
References  
G. Aist, J. Allen, E. Campana, L. Galescu, C. A. G?mez 
Gallo, S. Stoness, M. Swift, and M. Tanenhaus. 
2006. Software Architectures for Incremental Under-
standing of Human Speech. In Proc. of Interspeech. 
J. Arnold, M. Fagano, and M. Tanenhaus. 2003. Disflu-
encies signal theee, um, new information. Journal of 
Psycholinguistic Research, 32, 25-36. 
J. Beskow. 2003. Talking heads - Models and applica-
tions for multimodal speech synthesis. Doctoral dis-
sertation, KTH. 
S. Brennan. 2000. Processes that shape conversation and 
their implications for computational. In Proc. of the 
38th Annual Meeting of the ACL.  
H. Clark, and T. Wasow. 1998. Repeating words in 
spontaneous speech. Cognitive Psychology, 37(3), 
201-242. 
J. Hirschberg, and D. Litman. 1993. Empirical studies 
on the disambiguation of cue phrases. Computational 
Linguistics, 19(3), 501-530. 
A. Hjalmarsson, and J. Edlund. In press. Human-
likeness in utterance generation: effects of variability. 
To be published in Proc. of the 4th IEEE Workshop 
on Perception and Interactive Technologies for 
Speech-Based Systems. Kloster Irsee, Germany. 
A. Hjalmarsson, P. Wik, and J. Brusk. 2007. Dealing 
with DEAL: a dialogue system for conversation 
training. In Proc. of SigDial. Antwerp, Belgium. 
J. Lindstr?m. 2008. Diskursmark?rer. In Tur och 
ordning; introduktion till svensk samtalsgrammatik 
(pp. 56-104). Norstedts Akademiska F?rlag. Stock-
holm, Sweden. 
L. Schourup. 1999. Discourse markers. Lingua, 107(3-
4), 227-265. 
G. Skantze, J. Edlund, and R. Carlson. 2006. Talking 
with Higgins: Research challenges in a spoken dia-
logue system. In Perception and Interactive Tech-
nologies (pp. 193-196). Berlin/Heidelberg: Springer. 
75
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 1?8,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Towards Incremental Speech Generation in Dialogue Systems 
 
Gabriel Skantze 
Dept. of Speech Music and Hearing 
KTH, Stockholm, Sweden 
gabriel@speech.kth.se 
Anna Hjalmarsson 
Dept. of Speech Music and Hearing 
KTH, Stockholm, Sweden 
annah@speech.kth.se 
 
 
Abstract 
We present a first step towards a model of 
speech generation for incremental dialogue 
systems. The model allows a dialogue system 
to incrementally interpret spoken input, while 
simultaneously planning, realising and self-
monitoring the system response. The model 
has been implemented in a general dialogue 
system framework. Using this framework, we 
have implemented a specific application and 
tested it in a Wizard-of-Oz setting, comparing 
it with a non-incremental version of the same 
system. The results show that the incremental 
version, while producing longer utterances, 
has a shorter response time and is perceived 
as more efficient by the users. 
1 Introduction 
Speakers in dialogue produce speech in a piece-
meal fashion and on-line as the dialogue pro-
gresses. When starting to speak, dialogue partici-
pants typically do not have a complete plan of 
how to say something or even what to say. Yet, 
they manage to rapidly integrate information 
from different sources in parallel and simultane-
ously plan and realize new dialogue contribu-
tions. Moreover, interlocutors continuously self-
monitor the actual production processes in order 
to facilitate self-corrections (Levelt, 1989). Con-
trary to this, most spoken dialogue systems use a 
silence threshold to determine when the user has 
stopped speaking. The user utterance is then 
processed by one module at a time, after which a 
complete system utterance is produced and real-
ised by a speech synthesizer.  
This paper has two purposes. First, to present 
an initial step towards a model of speech genera-
tion that allows a dialogue system to incremen-
tally interpret spoken input, while simultaneously 
planning, realising and self-monitoring the sys-
tem response. The model has been implemented 
in a general dialogue system framework. This is 
described in Section 2 and 3. The second purpose 
is to evaluate the usefulness of incremental 
speech generation in a Wizard-of-Oz setting, us-
ing the proposed model. This is described in Sec-
tion 4. 
1.1 Motivation 
A non-incremental dialogue system waits until 
the user has stopped speaking (using a silence 
threshold to determine this) before starting to 
process the utterance and then produce a system 
response. If processing takes time, for example 
because an external resource is being accessed, 
this may result in a confusing response delay. An 
incremental system may instead continuously 
build a tentative plan of what to say as the user is 
speaking. When it detects that the user?s utter-
ance has ended, it may start to asynchronously 
realise this plan while processing continues, with 
the possibility to revise the plan if needed.  
There are many potential reasons for why dia-
logue systems may need additional time for 
processing. For example, it has been assumed 
that ASR processing has to be done in real-time, 
in order to avoid long and confusing response 
delays. Yet, if we allow the system to start 
speaking before input is complete, we can allow 
more accurate (and time-consuming) ASR proc-
essing (for example by broadening the beam). In 
this paper, we will explore incremental speech 
generation in a Wizard-of-oz setting. A common 
problem in such settings is the time it takes for 
the Wizard to interpret the user?s utterance 
and/or decide on the next system action, resulting 
in unacceptable response delays (Fraser & Gil-
bert, 1991). Thus, it would be useful if the sys-
tem could start to speak as soon as the user has 
finished speaking, based on the Wizard?s actions 
so far. 
1
1.2 Related work 
Incremental speech generation has been studied 
from different perspectives. From a psycholin-
guistic perspective, Levelt (1989) and others 
have studied how speakers incrementally pro-
duce utterances while self-monitoring the output, 
both overtly (listening to oneself speaking) and 
covertly (mentally monitoring what is about to 
be said). As deviations from the desired output is 
detected, the speaker may initiate  self-repairs. If 
the item to be repaired has already been spoken, 
an overt repair is needed (for example by using 
an editing term, such as ?sorry?). If not, the ut-
terance plan may be altered to accommodate the 
repair, a so-called covert repair. Central to the 
concept of incremental speech generation is that 
the realization of overt speech can be initiated 
before the speaker has a complete plan of what to 
say. An option for a speaker who does not know 
what to say (but wants to claim the floor) is to 
use hesitation phenomena such as filled pauses 
(?eh?) or cue phrases such as ?let?s see?.  
A dialogue system may not need to self-
monitor its output for the same reasons as hu-
mans do. For example, there is no risk of articu-
latory errors (with current speech synthesis tech-
nology). However, a dialogue system may utilize 
the same mechanisms of self-repair and hesita-
tion phenomena to simultaneously plan and real-
ise the spoken output, as there is always a risk 
for revision in the input to an incremental mod-
ule (as described in Section 2.1).  
There is also another aspect of self-monitoring 
that is important for dialogue systems. In a sys-
tem with modules operating asynchronously, the 
dialogue manager cannot know whether the in-
tended output is actually realized, as the user 
may interrupt the system. Also, the timing of the 
synthesized speech is important, as the user may 
give feedback in the middle of a system utter-
ance. Thus, an incremental, asynchronous system 
somehow needs to self-monitor its own output.   
From a syntactic perspective, Kempen & 
Hoenkamp (1987) and Kilger & Finkler (1995) 
have studied how to syntactically formulate sen-
tences incrementally under time constraints. 
Dohsaka & Shimazu (1997) describes a system 
architecture for incremental speech generation. 
However, there is no account for revision of the 
input (as discussed in Section 2.1) and there is no 
evaluation with users. Skantze & Schlangen 
(2009) describe an incremental system that partly 
supports incremental output and that is evaluated 
with users, but the domain is limited to number 
dictation. 
In this study, the focus is not on syntactic con-
struction of utterances, but on how to build prac-
tical incremental dialogue systems within limited 
domains that can handle revisions and produce 
convincing, flexible and varied speech output in 
on-line interaction with users.  
2 The Jindigo framework 
The proposed model has been implemented in 
Jindigo ? a Java-based open source framework 
for implementing and experimenting with incre-
mental dialogue systems (www.jindigo.net). We 
will here briefly describe this framework and the 
model of incremental dialogue processing that it 
is based on. 
2.1 Incremental units 
Schlangen & Skantze (2009) describes a general, 
abstract model of incremental dialogue process-
ing, which Jindigo is based on. In this model, a 
system consists of a network of processing mod-
ules. Each module has a left buffer, a processor, 
and a right buffer, where the normal mode of 
processing is to receive input from the left 
buffer, process it, and provide output in the right 
buffer, from where it is forwarded to the next 
module?s left buffer. An example is shown in 
Figure 1. Modules exchange incremental units 
(IUs), which are the smallest ?chunks? of infor-
mation that can trigger connected modules into 
action (such as words, phrases, communicative 
acts, etc). IUs are typically part of larger units: 
individual words are parts of an utterance; con-
cepts are part of the representation of an utter-
ance meaning. This relation of being part of the 
same larger unit is recorded through same-level 
links. In the example below, IU2 has a same-level 
link to IU1 of type PREDECESSOR, meaning that 
they are linearly ordered. The information that 
was used in creating a given IU is linked to it via 
grounded-in links. In the example, IU3 is 
grounded in IU1 and IU2, while IU4 is grounded 
in IU3. 
 
IU1 IU2
IU1 IU2
IU3 IU3
IU3
IU4
IU4
Module A
Module B
left buffer processor right buffer
left buffer processor right buffer
 
Figure 1: Two connected modules. 
2
A challenge for incremental systems is to han-
dle revisions. For example, as the first part of the 
word ?forty? is recognised, the best hypothesis 
might be ?four?. As the speech recogniser re-
ceives more input, it might need to revise its pre-
vious output, which might cause a chain of revi-
sions in all subsequent modules. To cope with 
this, modules have to be able to react to three 
basic situations: that IUs are added to a buffer, 
which triggers processing; that IUs that were er-
roneously hypothesized by an earlier module are 
revoked, which may trigger a revision of a mod-
ule?s own output; and that modules signal that 
they commit to an IU, that is, won?t revoke it 
anymore. 
Jindigo implements an efficient model for 
communicating these updates. In this model, IUs 
are associated with edges in a graph, as shown in 
Table 1. The graph may be incrementally 
amended without actually removing edges or 
vertices, even if revision occurs. At each time-
step, a new update message is sent to the con-
suming module. The update message contains a 
pair of pointers [C, A]: (C) the vertex from which 
the currently committed hypothesis can be con-
structed, and (A) the vertex from which the cur-
rently best tentative hypothesis can be con-
structed. In Jindigo, all modules run as threads 
within a single Java process, and therefore have 
access to the same memory space.  
2.2 A typical architecture 
A typical Jindigo system architecture is shown in 
Figure 2. The word buffer from the Recognizer 
module is parsed by the Interpreter module 
which tries to find an optimal sequence of top 
phrases and their semantic representations. These 
phrases are then interpreted in light of the current 
dialogue context by the Contextualizer module 
and are packaged as Communicative Acts (CAs). 
As can be seen in Figure 2, the Contextualizer 
also self-monitors Concepts from the system as 
they are spoken by the Vocalizer, which makes it 
possible to contextually interpret user responses 
to system utterances. This also makes it possible 
for the system to know whether an intended ut-
terance actually was produced, or if it was inter-
rupted. The current context is sent to the Action 
Manager, which generates a SpeechPlan that is 
sent to the Vocalizer. This is described in detail 
in the next section.  
Figure 2: A typical Jindigo system architecture. 
 
String Right buffer Update 
message 
t1: one w1 one w2
 
[w1, w2] 
t2: one five w1 one w2 five w3
 
[w1, w3] 
t3: one w1 one w2 five w3
 
[w1, w2] 
t4: one four five w1 one w2 five w3
five w5four w4
 
[w1, w5] 
t5: [commit] w1 one w2 five w3
five w5four w4
 
[w5,w5] 
Table 1: The right buffer of an ASR module, and up-
date messages at different time-steps. 
 
Figure 3: Incremental Units at different levels of processing. Some grounded-in relations are shown with dotted 
lines. W=Word, SS=SpeechSegment, SU=SpeechUnit, CA=Communicative Act. 
Interpreter
VAD
ASR
Action Manager
Vocalizer
Contextualizer
SpeechPlan
Speech
Segment
SU SU SU SU SU SU
Self
Delay
Other
Delay
W W W W W W W W W
P
CA
SS
Concept
CA
Response
To
SS
C Phrase Concept
Utterance Utterance
Utterance
Segment
US US
User System
SS
User
Vocalizer
Speech
Speech
Interpreter
Word
ContextualizerActionManager
Utterance
Segment
ASR
SpeechPlan
Context
Phrase
Concept
3
3 Incremental speech generation 
3.1 Incremental units of speech 
In order for user and system utterances to be in-
terpreted and produced incrementally, they need 
to be decomposed into smaller units of process-
ing (IUs). This decomposition is shown in Figure 
3. Using a standard voice activity detector 
(VAD) in the ASR, the user?s speech is chunked 
into Utterance-units. The Utterance bounda-
ries determine when the ASR hypothesis is 
committed. However, for the system to be able to 
respond quickly, the end silence threshold of 
these Utterances are typically too long. Therefore 
smaller units of the type UtteranceSegment 
(US) are detected, using a much shorter silence 
threshold of about 50ms. Such short silence 
thresholds allow the system to give very fast re-
sponses (such as backchannels). Information 
about US boundaries is sent directly from the 
ASR to the Vocalizer. As Figure 3 illustrates, the 
grounded-in links can be followed to derive the 
timing of IUs at different levels of processing.  
The system output is also modelled using IUs 
at different processing levels. The widest-
spanning IU on the output side is the 
SpeechPlan. The rendering of a SpeechPlan 
will result in a sequence of SpeechSegment?s, 
where each SpeechSegment represents a con-
tinuous audio rendering of speech, either as a 
synthesised string or a pre-recorded audio file. 
For example, the plan may be to say ?okay, a red 
doll, here is a nice doll?, consisting of three seg-
ments. Now, there are two requirements that we 
need to meet. First, the output should be varied: 
the system should not give exactly the same re-
sponse every time to the same request. But, as 
we will see, the output in an incremental system 
must also be flexible, as speech plans are incre-
mentally produced and amended. In order to re-
lieve the Action Manager of the burden of vary-
ing the output and making time-critical adjust-
ments, we model the SpeechPlan as a directed 
graph, where each edge is associated with a 
SpeechSegment, as shown in Figure 4. Thus, the 
Action Manager may asynchronously plan (a set 
of possible) responses, while the Vocalizer se-
lects the rendering path in the graph and takes 
care of time-critical synchronization. To control 
the rendering, each SpeechSegment has the 
properties optional, committing, selfDelay 
and otherDelay, as described in the next sec-
tion. It must also be possible for an incremental 
system to interrupt and make self-repairs in the 
middle of a SpeechSegment. Therefore, each 
SpeechSegment may also be decomposed into an 
array of SpeechUnit?s, where each SpeechUnit 
contains pointers to the audio rendering in the 
SpeechSegment. 
3.2 Producing and consuming SpeechPlans 
The SpeechPlan does not need to be complete 
before the system starts to speak. An example of 
this is shown in Figure 4. As more words are 
recognised by the ASR, the Action Manager may 
add more SpeechSegment?s to the graph. Thus, 
the system may start to say ?it costs? before it 
knows which object is being talked about.  
 
w1 how w2 much w3 is w4 the w5 doll w6
eh
well
s1
you can have it for
it costs
let?s say s3 s640 crowns
 
Figure 4: The right buffer of an ASR (top) and the 
SpeechPlan that is incrementally produced (bottom). 
Vertex s1 is associated with w1, s3 with w3, etc. Op-
tional, non-committing SpeechSegment?s are marked 
with dashed outline. 
The SpeechPlan has a pointer called 
finalVertex. When the Vocalizer reaches the 
finalVertex, the SpeechPlan is completely 
realised. If finalVertex is not set, it means that 
the SpeechPlan is not yet completely con-
structed. The SpeechSegment property 
optional tells whether the segment needs to be 
realised or if it could be skipped if the 
finalVertex is in sight. This makes it possible 
to insert floor-keeping SpeechSegment?s (such 
as ?eh?) in the graph, which are only realised if 
needed. The Vocalizer also keeps track of which 
SpeechSegment?s it has realised before, so that it 
can look ahead in the graph and realise a more 
varied output. Each SpeechSegment may carry a 
semantic representation of the segment (a 
Concept). This is sent by the Vocalizer to the 
Contextualizer as soon as the segment has been 
realised. 
The SpeechSegment properties selfDelay 
and otherDelay regulate the timing of the out-
put (as illustrated in Figure 3). They specify the 
number of milliseconds that should pass before 
the Vocalizer starts to play the segment, depend-
ing on the previous speaker. By setting the 
otherDelay of a segment, the Action Manager 
may delay the response depending on how cer-
tain it is that it is appropriate to speak, for exam-
ple by considering pitch and semantic complete-
ness. (See Raux & Eskenazi (2008) for a study 
4
on how such dynamic delays can be derived us-
ing machine learning.)  
If the user starts to speak (i.e., a new 
UtteranceSegment is initiated) as the system is 
speaking, the Vocalizer pauses (at a SpeechUnit 
boundary) and waits until it has received a new 
response from the Action Manager. The Action 
Manager may then choose to generate a new re-
sponse or simply ignore the last input, in which 
case the Vocalizer continues from the point of 
interruption. This may happen if, for example, 
the UtteranceSegment was identified as a back-
channel, cough, or similar. 
3.3 Self-repairs  
As Figure 3 shows, a SpeechPlan may be 
grounded in a user CA (i.e., it is a response to 
this CA). If this CA is revoked, or if the 
SpeechPlan is revised, the Vocalizer may initial-
ize a self-repair. The Vocalizer keeps a list of the 
SpeechSegment?s it has realised so far. If the 
SpeechPlan is revised when it has been partly 
realised, the Vocalizer compares the history with 
the new graph and chooses one of the different 
repair strategies shown in Table 2. In the best 
case, it may smoothly switch to the new plan 
without the user noticing it (covert repair). In 
case of a unit repair, the Vocalizer searches for a 
zero-crossing point in the audio segment, close to 
the boundary pointed out by the SpeechUnit.  
 
covert 
segment 
repair 
you are right it is blue
you are right they are blue
 
overt 
segment 
repair 
you are right it is blue
you are wrong it is redsorry
 
covert 
unit 
repair 
you are right it is blue
you are wrong it is red
 
overt 
unit 
repair 
you are right it is blue
you are wrong it is red
sorry
 
Table 2: Different types of self-repairs. The shaded 
boxes show which SpeechUnit?s have been realised, 
or are about to be realised, at the point of revision. 
The SpeechSegment property committing 
tells whether it needs to be repaired if the 
SpeechPlan is revised. For example, a filled 
pause such as ?eh? is not committing (there is no 
need to insert an editing term after it), while a 
request or an assertion usually is. If (parts of) a 
committing segment has already been realised 
and it cannot be part of the new plan, an overt 
repaired is made with the help of an editing term 
(e.g., ?sorry?). When comparing the history with 
the new graph, the Vocalizer searches the graph 
and tries to find a path so that it may avoid mak-
ing an overt repair. For example if the graph in 
Figure 4 is replaced with a corresponding one 
that ends with ?60 crowns?, and it has so far 
partly realised ?it costs?, it may choose the cor-
responding path in the new SpeechPlan, making 
a covert repair. 
4 A Wizard-of-Oz experiment 
A Wizard-of-Oz experiment was conducted to 
test the usefulness of the model outlined above. 
All modules in the system were fully functional, 
except for the ASR, since not enough data had 
been collected to build language models. Thus, 
instead of using ASR, the users? speech was 
transcribed by a Wizard. As discussed in section 
1.1, a common problem is the time it takes for 
the Wizard to transcribe incoming utterances, 
and thus for the system to respond. Therefore, 
this is an interesting test-case for our model. In 
order to let the system respond as soon as the 
user finished speaking, even if the Wizard hasn?t 
completed the transcription yet, a VAD is used. 
The setting is shown in Figure 5 (compare with 
Figure 2). The Wizard may start to type as soon 
as the user starts to speak and may alter whatever 
he has typed until the return key is pressed and 
the hypothesis is committed. The word buffer is 
updated in exactly the same manner as if it had 
been the output of an ASR.  
User VAD
Vocalizer
Speech
Speech
Interpreter
Word
ContextualizerActionManager
Utterance
Segment
Wizard
 
Figure 5: The system architecture used in the Wizard-
of-Oz experiment. 
For comparison, we also configured a non-
incremental version of the same system, where 
nothing was sent from the Wizard until he com-
5
mitted by pressing the return key. Since we did 
not have mature models for the Interpreter either, 
the Wizard was allowed to adapt the transcrip-
tion of the utterances to match the models, while 
preserving the semantic content. 
4.1 The DEAL domain 
The system that was used in the experiment was 
a spoken dialogue system for second language 
learners of Swedish under development at KTH, 
called DEAL (Hjalmarsson et al, 2007). The 
scene of DEAL is set at a flea market where a 
talking agent is the owner of a shop selling used 
goods. The student is given a mission to buy 
items at the flea market getting the best possible 
price from the shop-keeper. The shop-keeper can 
talk about the properties of goods for sale and 
negotiate about the price. The price can be re-
duced if the user points out a flaw of an object, 
argues that something is too expensive, or offers 
lower bids. However, if the user is too persistent 
haggling, the agent gets frustrated and closes the 
shop. Then the user has failed to complete the 
task.  
For the experiment, DEAL was re-
implemented using the Jindigo framework. Fig-
ure 6 shows the GUI that was shown to the user. 
 
 
Figure 6: The user interface in DEAL. The object on 
the table is the one currently in focus. Example ob-
jects are shown on the shelf. Current game score, 
money and bought objects are shown on the right. 
4.2 Speech segments in DEAL 
In a previous data collection of human-human 
interaction in the DEAL domain (Hjalmarsson, 
2008) it was noted that about 40% of the speaker 
turns were initiated with standardized lexical ex-
pressions (cue phrases) or filled pauses. Such 
speech segments commit very little semantically 
to the rest of the utterance and are therefore very 
useful as initiations of utterances, since such 
speech segments can be produced immediately 
after the user has stopped speaking, allowing the 
Wizard to exploit the additional time to tran-
scribe the rest of the utterance.  
The DEAL corpus was used to create utter-
ance initial speech segments for the experiment. 
The motivation to use speech segments derived 
from human recordings was to make the system 
sound convincing in terms of both lexical choice 
and intonation. In particular, we wanted a reper-
toire of different types of filled pauses and feed-
back expression such as ?eh? and ?mm? in order 
to avoid a system that sounds monotone and re-
petitive. First, a number of feedback expression 
such as ?ja?, ?a?, ?mm? (Eng: ?yes?), filled 
pauses such as ?eh?, ?ehm? and expressions used 
to initiate different domain specific speech acts 
(for example ?it costs? and ?let me see?) were 
extracted.  The segments were re-synthesized 
using Expros, a tool for experimentation with 
prosody in diphone voices (Gustafson & Edlund, 
2008). Based on manual transcriptions and sound 
files, Expros automatically extracts pitch, dura-
tion and intensity from the human voice and cre-
ates a synthetic version using these parameters. 
In the speech plan, these canned segments were 
mixed with generated text segments (for example 
references to objects, prices, etc) that were syn-
thesized and generated on-line with the same 
diphone voice. 
An example interaction with the incremental 
version of the system is shown in Table 3. S.11 
exemplifies a self-correction, where the system 
prepares to present another bid, but then realizes 
that the user?s bid is too low to even consider. A 
video (with subtitles) showing an interaction 
with one of the users can be seen at 
http://www.youtube.com/watch?v=cQQmgItIMvs. 
 
S.1 [welcome] [how may I help you] 
U.2 I want to buy a doll 
S.3 [eh] [here is] [a doll] 
U.4 how much is it? 
S.5 [eh] [it costs] [120 crowns] 
U.6 that is too expensive 
how much is the teddy bear? 
S.7 [well] [you can have it for] [let?s see] 
[40 crowns] 
U.8 I can give you 30 crowns 
S.9 [you could have it for] [37 crowns] 
U.10 I can give you 10 crowns 
S.11 [let?s say] [or, I mean] [that is way too 
little] 
Table 3: An example DEAL dialogue (translated from 
Swedish). Speech segments are marked in brackets. 
6
4.3 Experimental setup 
In order to compare the incremental and non-
incremental versions of the system, we con-
ducted an experiment with 10 participants, 4 
male and 6 female. The participants were given a 
mission: to buy three items (with certain charac-
teristics) in DEAL at the best possible price from 
the shop-keeper. The participants were further 
instructed to evaluate two different versions of 
the system, System A and System B. However, 
they were not informed how the versions dif-
fered. The participants were lead to believe that 
they were interacting with a fully working dia-
logue system and were not aware of the Wizard-
of-Oz set up. Each participant interacted with the 
system four times, first two times with each ver-
sion of the system, after which a questionnaire 
was completed. Then they interacted with the 
two versions again, after which they filled out a 
second questionnaire with the same questions. 
The order of the versions was balanced between 
subjects.  
The mid-experiment questionnaire was used to 
collect the participants? first opinions of the two 
versions and to make them aware of what type of 
characteristics they should consider when inter-
acting with the system the second time. When 
filling out the second questionnaire, the partici-
pants were asked to base their ratings on their 
overall experience with the two system versions. 
Thus, the analysis of the results is based on the 
second questionnaire. In the questionnaires, they 
were requested to rate which one of the two ver-
sions was most prominent according to 8 differ-
ent dimensions: which version they preferred; 
which was more human-like, polite, efficient, and 
intelligent; which gave a faster response and bet-
ter feedback; and with which version it was eas-
ier to know when to speak. All ratings were done 
on a continuous horizontal line with System A on 
the left end and System B on the right end. The 
centre of the line was labelled with ?no differ-
ence?.  
The participants were recorded during their in-
teraction with the system, and all messages in the 
system were logged.  
4.4 Results 
Figure 7 shows the difference in response time 
between the two versions. As expected, the in-
cremental version started to speak more quickly 
(M=0.58s, SD=1.20) than the non-incremental 
version (M=2.84s, SD=1.17), while producing 
longer utterances. It was harder to anticipate 
whether it would take more or less time for the 
incremental version to finish utterances. Both 
versions received the final input at the same 
time. On the one hand, the incremental version 
initiates utterances with speech segments that 
contain little or no semantic information. Thus, if 
the system is in the middle of such a segment 
when receiving the complete input from the 
Wizard, the system may need to complete this 
segment before producing the rest of the utter-
ance. Moreover, if an utterance is initiated and 
the Wizard alters the input, the incremental ver-
sion needs to make a repair which takes addi-
tional time. On the other hand, it may also start 
to produce speech segments that are semantically 
relevant, based on the incremental input, which 
allows it to finish the utterance more quickly. As 
the figure shows, it turns out that the average 
response completion time for the incremental 
version (M=5.02s, SD=1.54) is about 600ms 
faster than the average for non-incremental ver-
sion (M=5.66s, SD=1.50), (t(704)=5.56, 
p<0.001).  
 
0,00
1,00
2,00
3,00
4,00
5,00
6,00
start end length
Se
co
nd
s
inc
non
 
Figure 7: The first two column pairs show the average 
time from the end of the user?s utterance to the start 
of the system?s response, and from the end of the 
user?s utterance to the end of the system?s response. 
The third column pair shows the average total system 
utterance length (end minus start).  
In general, subjects reported that the system 
worked very well. After the first interaction with 
the two versions, the participants found it hard to 
point out the difference, as they were focused on 
solving the task. The marks on the horizontal 
continuous lines on the questionnaire were 
measured with a ruler based on their distance 
from the midpoint (labelled with ?no difference?) 
and normalized to a scale from -1 to 1, each ex-
treme representing one system version. A Wil-
coxon Signed Ranks Test was carried out, using 
these rankings as differences. The results are 
shown in Table 4. As the table shows, the two 
versions differed significantly in three dimen-
sions, all in favour of the incremental version. 
7
Hence, the incremental version was rated as 
more polite, more efficient, and better at indicat-
ing when to speak. 
 
 diff z-value p-value 
preferred 0.23 -1.24 0.214 
human-like 0.15 -0.76 0.445 
polite 0.40 -2.19 0.028* 
efficient 0.29 -2.08 0.038* 
intelligent 0.11 -0.70 0.484 
faster response 0.26 -1.66 0.097 
feedback 0.08 -0.84 0.400 
when to speak 0.35 -2.38 0.017* 
Table 4: The results from the second questionnaire. 
All differences are positive, meaning that they are in 
favour of the incremental version. 
A well known phenomena in dialogue is that 
of entrainment (or adaptation or alignment), that 
is, speakers (in both human-human and human-
computer dialogue) tend to adapt the conversa-
tional behaviour to their interlocutor (e.g., Bell, 
2003). In order to examine whether the different 
versions affected the user?s behaviour, we ana-
lyzed both the user utterance length and user re-
sponse time, but found no significant differences 
between the interactions with the two versions. 
5 Conclusions & Future work 
This paper has presented a first step towards in-
cremental speech generation in dialogue systems. 
The results are promising: when there are delays 
in the processing of the dialogue, it is possible to 
incrementally produce utterances that make the 
interaction more efficient and pleasant for the 
user.  
As this is a first step, there are several ways to 
improve the model. First, the edges in the 
SpeechPlan could have probabilities, to guide 
the path planning. Second, when the user has 
finished speaking, it should (in some cases) be 
possible to anticipate how long it will take until 
the processing is completed and thereby choose a 
more optimal path (by taking the length of the 
SpeechSegment?s into consideration). Third, a 
lot of work could be done on the dynamic gen-
eration of SpeechSegment?s, considering syntac-
tic and pragmatic constraints, although this 
would require a speech synthesizer that was bet-
ter at convincingly produce conversational 
speech. 
The experiment also shows that it is possible 
to achieve fast turn-taking and convincing re-
sponses in a Wizard-of-Oz setting. We think that 
this opens up new possibilities for the Wizard-of-
Oz paradigm, and thereby for practical develop-
ment of dialogue systems in general.  
6 Acknowledgements 
This research was funded by the Swedish research 
council project GENDIAL (VR #2007-6431). 
References 
Bell, L. (2003). Linguistic adaptations in spoken hu-
man-computer dialogues. Empirical studies of user 
behavior. Doctoral dissertation, Department of 
Speech, Music and Hearing, KTH, Stockholm. 
Dohsaka, K., & Shimazu, A. (1997). System architec-
ture for spoken utterance production in collaborative 
dialogue. In Working Notes of IJCAI 1997 Work-
shop on Collaboration, Cooperation and Conflict in 
Dialogue Systems.  
Fraser, N. M., & Gilbert, G. N. (1991). Simulating 
speech systems. Computer Speech and Language, 
5(1), 81-99. 
Gustafson, J., & Edlund, J. (2008). expros: a toolkit 
for exploratory experimentation with prosody in 
customized diphone voices. In Proceedings of Per-
ception and Interactive Technologies for Speech-
Based Systems (PIT 2008) (pp. 293-296). Ber-
lin/Heidelberg: Springer. 
Hjalmarsson, A., Wik, P., & Brusk, J. (2007). Dealing 
with DEAL: a dialogue system for conversation 
training. In Proceedings of SigDial (pp. 132-135). 
Antwerp, Belgium. 
Hjalmarsson, A. (2008). Speaking without knowing 
what to say... or when to end. In Proceedings of 
SIGDial 2008. Columbus, Ohio, USA. 
Kempen, G., & Hoenkamp, E. (1987). An incremental 
procedural grammar for sentence formulation. Cog-
nitive Science, 11(2), 201-258. 
Kilger, A., & Finkler, W. (1995). Incremental Gen-
eration for Real-Time Applications. Technical Re-
port RR-95-11, German Research Center for Artifi-
cial Intelligence. 
Levelt, W. J. M. (1989). Speaking: From Intention to 
Articulation. Cambridge, Mass., USA: MIT Press. 
Raux, A., & Eskenazi, M. (2008). Optimizing end-
pointing thresholds using dialogue features in a 
spoken dialogue system. In Proceedings of SIGdial 
2008. Columbus, OH, USA. 
Schlangen, D., & Skantze, G. (2009). A general, ab-
stract model of incremental dialogue processing. In 
Proceedings of EACL-09. Athens, Greece. 
Skantze, G., & Schlangen, D. (2009). Incremental 
dialogue processing in a micro-domain. In Proceed-
ings of EACL-09. Athens, Greece. 
 
8
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 225?228,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
The vocal intensity of turn-initial cue phrases in dialogue 
 
 
Anna Hjalmarsson 
Department of Speech Music and Hearing, KTH 
Stockholm, Sweden 
annah@speech.kth.se 
 
  
 
Abstract 
The present study explores the vocal intensity 
of turn-initial cue phrases in a corpus of dia-
logues in Swedish. Cue phrases convey rela-
tively little propositional content, but have 
several important pragmatic functions. The 
majority of these entities are frequently occur-
ring monosyllabic words such as ?eh?, ?mm?, 
?ja?. Prosodic analysis shows that these words 
are produced with higher intensity than other 
turn-initial words are. In light of these results, 
it is suggested that speakers produce these ex-
pressions with high intensity in order to claim 
the floor. It is further shown that the difference 
in intensity can be measured as a dynamic in-
ter-speaker relation over the course of a dia-
logue using the end of the interlocutor?s previ-
ous turn as a reference point. 
1 Introduction 
In dialogue, interlocutors produce speech incre-
mentally and on-line as the dialogue progresses. 
Articulation can be initiated before the speaker 
has a complete plan of what to say (Pechmann, 
1989). When speaking, processes at all levels 
(e.g. semantic, syntactic, phonologic and articu-
latory) work in parallel to render the utterance. 
This processing strategy is efficient, since the 
speaker may employ the time devoted to articu-
lating an early part of an utterance to plan the 
rest. 
Speakers often initiate new turns with cue 
phrases ? standardized lexical or non-lexical ex-
pressions such as ?ehm? ?okay?, ?yeah?, and 
?but? (c.f. Gravano, 2009). Cue phrases (or dis-
course markers) are linguistic devices used to 
signal relations between different segments of 
speech (for an overview see Fraser, 1996). These 
devices convey relatively little propositional con-
tent, but have several important pragmatic func-
tions. For example, these words provide feed-
back and signal how the upcoming utterance re-
lates to previous context. Another important 
function is to claim the conversational floor (c.f. 
Levinson, 1983). 
With these fundamental properties of language 
production in mind, it is proposed that turn-initial 
cue phrases can be used in spoken dialogue sys-
tems to initiate new turns, allowing the system 
additional time to generate a complete response. 
This approach was recently explored in a user 
study with a dialogue system that generates turn-
initial cue phrases incrementally (Skantze & 
Hjalmarsson, in press). Results from this experi-
ment show that an incremental version that used 
turn-initial cue phrases had shorter response 
times and was rated as more efficient, more po-
lite and better at indicating when to speak than a 
non-incremental implementation of the same sys-
tem. The present study carries on this research, 
investigating acoustic parameters of turn-initial 
cue phrases in order to build a dialogue system 
that sounds convincing intonation wise. 
Another aim of this study was to explore if the 
vocal intensity of the other speaker?s immedi-
ately preceding speech can be used as a reference 
point in order to measure intensity as an inter-
speaker relation over the course of a dialogue. 
Thus, in addition to measuring overall differ-
ences in intensity, the relative difference between 
the first token of a new turn and the last token of 
the immediately preceding turn was measured. 
This dynamic approach, if proven feasible, al-
lows spoken dialogue system designers to adjust 
the system?s vocal intensity on-line in order to 
accommodate variations in the surrounding 
acoustic environment.  
2 Related work 
There are a few examples of research that have 
manipulated intensity to signal pragmatic func-
tions. For example, Str?m & Seneff (2000) in-
creases intensity in order to signal that user 
225
barge-ins are disallowed in particular dialogue 
states. Theoretical support for such manipula-
tions is provided by an early line of research on 
interruptions in dialogue (Meltzer et al, 1971). 
Meltzer et al (1971) propose that the outcome of 
speech overlaps is affected by prosodic charac-
teristics and show that the greater the increase in 
amplitude, the greater the likelihood of ?interrup-
tion success?. Moreover, it is show that the suc-
cess of interruptions, that is who retains the floor, 
is based on how much higher the intensity of the 
interruption is compared to the previous 
speaker?s intensity or compared  to the speaker?s 
own intensity at the end of that speaker?s previ-
ous speaker turn.  
Measuring inter-speaker relative intensity is 
further motivated by research that suggests that 
speakers adjust their vocal intensity online over 
the course of a dialogue in order to accommodate 
the surrounding acoustic context. For example, 
speakers tend to raise their voice unintentionally 
when background noise increases to enhance 
their audibility; this is the so-called Lombard 
effect (Pick et al, 1989). Moreover, speakers 
adjust intensity based on their conversational 
partners (Natale, 1975) and the distance to their 
listeners (Healey et al, 1997).  
3 Method 
3.1 Data: The DEAL corpus 
DEAL is a dialogue system that is currently be-
ing developed at the department of Speech, Mu-
sic and Hearing, KTH (Wik & Hjalmarsson, 
2009). The aim of the DEAL dialogue system is 
to provide conversation training for second lan-
guage learners of Swedish. The scene of DEAL 
is set at a flea market where a talking animated 
persona is the owner of a shop selling used 
goods. 
The dialogue data used as a basis for the data 
analyzes presented in this paper were human-
human dialogues, collected in a recording envi-
ronment set up to mimic the interaction in the 
DEAL domain. The dialogue collected were in-
formal, human-human, face-to-face conversation 
in Swedish. The recordings were made with 
close talk microphones with six subjects (four 
male and two female). In total, eight dialogues 
were collected. Each dialogue was about 15 min-
utes, making for about two hours of speech in 
total in the corpus. The dialogues were tran-
scribed orthographically and annotated for enti-
ties such as laughter, lip-smacks, breathing and 
hemming. The transcripts from the dialogues 
were time-aligned with the speech signal. This 
was done using forced alignment with subse-
quent manual verification of the timings. The 
dialogues were also segmented into speaker 
turns. A speaker turn here is a segment of speech 
of arbitrary length surrounded by another 
speaker?s vocalization. All together, the dia-
logues contained 2036 speaker turns. 
The corpus was also annotated for cue phrases 
using 11 functional categories. The definition of 
cue phrases used for annotation of the DEAL 
corpus was broad and all types of vocalizations 
that the speakers use to hold the dialogue to-
gether at different communicative levels were 
included. Cue phrase annotation was designed as 
a two-fold task: (i) to decide if a word was a cue 
phrase or not ? a binary task, and (ii) to select its 
functional class according to the annotation 
scheme. The annotators could see the transcrip-
tions and listen to the recordings while labelling. 
The kappa coefficient for task (i) was 0.87 
(p<.05). The kappa coefficient for (ii) was 0.82 
(p<.05). For a detailed description of the cue 
phrase categories and their annotation, see 
(Hjalmarsson, 2008).  
3.2 Data analysis 
The first word in each turn was extracted and 
analyzed. Here, a word is all annotated tokens in 
the corpus except breathing, lip-smacks, and 
laughter, which are all relevant, but outside the 
scope of this study. 1137 (57%) words were an-
notated as some type of cue phrase, and 903 (43 
%) were other words. The turn-initial cue phrases 
were annotated with different cue phrase catego-
ries. 587 (28%) turn-initial words were annotated 
as either RESPONSIVE, RESPONSIVE DISPREFER-
ENCE or RESPONSIVE NEW INFORMATION. The 
annotation of these was based on the interpreta-
tion of the speakers? attitudes, expressing either 
neutral feedback (RESPONSIVE), non-agreement 
(RESPONSIVE DISPREFERENCE) or surprise (RE-
SPONSIVE NEW INFORMATION). The RESPON-
SIVES were most frequently realized as either 
?ja?, ?a?, and ?mm? (Eng: ?yeah?, ?mm?).  
Furthermore, 189 (9%) of all turn-initial words 
were annotated as CONNECTIVES. The connective 
cue phrase categories indicate how the new ut-
terance relates to previous context. For example, 
these signal whether the upcoming speaker turn 
is additive, contrastive or alternative to previous 
context. Examples of these categories are ?och? 
(Eng: ?and?), ?men? (Eng: ?but?) and ?eller? 
(Eng: ?or?), respectively. 
226
A third category of cue phrases in a turn-initial 
position was filled pauses (57, 3%). Whereas 
filled pause may not typically be considered as 
cue phrases, these elements have similar charac-
teristics. For example, fillers provide important 
pragmatic information that listeners attend and 
adjust their behaviour according to. For example, 
a corpus study of Dutch fillers showed that these 
tokens highlight discourse structure (Swertz, 
1998). Frequently occurring filler words in the 
corpus were ?eh? and ?ehm?. 
The majority of the turn-initial cue phrases 
were high frequency monosyllabic words, which 
are typically not associated with stress, although 
on listening, they give the impression of being 
louder than other turn-initial vocalizations. To 
verify this observation, the intensity in decibel of 
the first word of each turn was extracted using 
Snack (www.speech.kth.se/snack). In order to 
explore the vocal intensity as an inter-speaker 
relation over the course of the dialogue, the aver-
age intensity of the last word of all turns was 
extracted. The motivation of this approach is to 
use the previous speaker?s voice intensity as a 
reference point. Thus, in order to avoid the need 
for global analysis over speakers and dialogues, 
only the (un-normalized) difference in intensity 
between the last word of the immediately preced-
ing turn and the first word of a new turn was cal-
culated.  
All turns following a one word only turn from 
the other speaker were excluded as an approxi-
mation to avoid speech following backchannel 
responses. 300 (33%) of the speaker changes 
contained overlapping speech. These overlaps  
were excluded from the data analysis since the 
recordings were not completely channel-
separated and crosstalk could conceivably inter-
fere with the results.  
Since the distance between the lips and the 
microphone was not controlled for during the 
recordings, the values were first normalized per 
speaker and dialogue (each value was shifted by 
the mean value per speaker and dialogue). 
4 Results 
Figure 1 presents the average normalized inten-
sity for turns initiated with cue phrases and other 
words.  
An independent samples t-test was conducted 
between the intensity of turns initiated with cue 
phrases and other turn-initial words. There was a 
significant difference in intensity between turns 
initiated with cue phrases (M=3.20 dB, SD=6.99) 
and turns initiated with other words (M=-4.20 
dB, SD=9.98), t(597)=10.55, p<.000. This shows 
that, on average, turns initiated with cue phrases 
were significantly louder (on average 6 dB) than 
turns initiated with other words. 
 
-6
-4
-2
0
2
4
6
Other words Cue phrases
Av
era
ge
 no
rm
ali
ze
d i
nte
ns
ity
 in
 d
 Figure 1 : Average normalized vocal intensity in dB 
for turn-initial words. Error bars represents the 
standard error. 
In order to explore the vocal intensity as an in-
ter-speaker relation the difference in voice inten-
sity between a new turn and the end of the im-
mediately preceding turn was extracted. The in-
ter-speaker differences in intensity for turn-initial 
cue phrases and other words are presented in 
Figure 2.  
 
-8
-6
-4
-2
0
2
4
6
8
Other words Cue phrases
Av
er
ag
e d
iff
er
en
ce
 in
 in
te
ns
ity
 in
 d
B
 Figure 2 Average difference in intensity (in dB) for 
turn-initial words. Error bars represents the stan-
dard error. 
An independent samples t-test was conducted to 
explore the difference in voice intensity as an 
inter-speaker relation. There was a significant 
difference in intensity between turns initiated 
with cue phrases (M=6.14 dB, SD=11.86) and 
turns initiated with other words (M=-1.52 dB, 
SD=13.07); t(595)=7.48, p<.000. This shows that 
the increase in intensity was significantly larger 
for turns initiated with cue phrases (about 7 dB) 
than for turns initiated with other words. 
5 Discussion 
This paper presents analyses of the intensity of 
turn-initial words. It shown that turns are fre-
quently initiated with cue phrases (about 55% of 
the turns in the DEAL corpus). The majority of 
227
these consist of high frequency monosyllabic 
words such as ?yes?, ?mm? and ?okay?. The 
most frequent turn-initial words that were not 
annotated as cue phrases were ?den? (Eng: ?it?), 
?vad? (Eng: ?what?), and ?jag? (Eng: ?I?). Thus, 
similar to turn-initial cue phrases, this category 
contains high-frequency monosyllabic words, 
items that are not typically associated with pro-
sodic stress. Yet, the results show that turn-initial 
cue phrases are produced with higher intensity 
than other turn-initial words are. In the light of 
previous research, which suggests that increased 
intensity have turn-claiming functions, one can 
speculate that speakers produce talkspurt-initial 
cue phrases with increased intensity in order to 
claim the floor convincingly before having for-
mulated a complete utterance. 
It is further argued that turn-initial cue phrases 
can be used in dialogue systems capable of in-
cremental speech production. Such words can be 
used to initiate turns once the user has stopped 
speaking, allowing the system more time to 
process input without response delays.  
Finally, it is suggested that intensity may be 
better modelled relative to the intensity of the 
immediately preceding speech rather than in ab-
solute of speaker-normalized terms. Speakers 
adjust their intensity to the current acoustical 
environment, and such a dynamic inter-speaker 
relative model may accommodate the current 
acoustic context over the course of a dialogue. In 
support of this approach, the present study shows 
that the increase in intensity can be calculated 
dynamically over the dialogue using the end of 
the previous speaker?s turn as a reference point. 
Inter-speaker relative measures are also moti-
vated practically. Extracting objective measures 
of intensity is problematic since contextual fac-
tors such as the distance between the microphone 
and the lips are difficult to control between dia-
logues and speakers, but the effects are mitigated 
by dynamic and relative measures. This is not to 
say that measuring intensity over the course of a 
single dialogue is trivial. Variation due to for 
example unforeseen alterations of the distance 
between the lips and the microphone during the 
dialogue are still problematic, but it is less of a 
problem within a session than between different 
sessions. 
 
 
 
Acknowledgments 
This research was carried out at Centre for 
Speech Technology, KTH. Funding was pro-
vided by Riksbankens Jubileumsfond (RJ) pro-
ject P09-0064:1-E Prosody in conversation and 
the Graduate School for Language Technology 
(GSLT). Many thanks to Rolf Carlson, Jens Ed-
lund and Joakim Gustafson for valuable com-
ments. 
References  
Fraser, B. (1996). Pragmatic markers. Pragmatics, 
6(2), 167-190. 
Gravano, A. (2009). Turn-Taking and Affirmative Cue 
Words in Task-Oriented Dialogue. Doctoral disser-
tation, Columbia University. 
Healey, C., Jones, R., & Berky, R. (1997). Effects of 
perceived listeners on speakers'vocal intensity. 
Journal of Voice, 11(1), 67-73. 
Hjalmarsson, A. (2008). Speaking without knowing 
what to say... or when to end. In Proceedings of 
SIGDial 2008. Columbus, Ohio, USA. 
Levinson, S. C. (1983). Pragmatics. Cambridge: 
Cambridge University press. 
Meltzer, L., Hayes, D., & Morris, M. (1971). Interrup-
tion Outcomes and Vocal Amplitude: Explorations 
in Social Psychophysics. Journal of Personality 
and Social Psychology, 18(3), 392-402. 
Natale, M. (1975). Convergence of mean vocal inten-
sity in dyadic communication as a function of so-
cial desirability. Personality and Social Psychol-
ogy, 32(5), 790-804. 
Pechmann, T. (1989). Incremental speech production 
and referential overspecification. Linguistics, 27, 
89-110. 
Pick, H. L. J., Siegel, G. M., Fox, P. W., Garber, S. 
R., & Kearney, J. K. (1989). Inhibiting the 
Lombard effect. JASA, 85(2), 894-900. 
Skantze, G., & Hjalmarsson, A. (in press). Towards 
Incremental Speech Generation in Dialogue Sys-
tems. To be published in Proceedings of SigDial. 
Tokyo, Japan. 
Str?m, N., & Seneff, S. (2000). Intelligent barge-in in 
conversational systems. In Procedings of ICSLP-
00.  
Swertz, M. (1998). Filled pauses as markers of dis-
course structure. Journal of Pragmatics, 30(4), 
485-496. 
Wik, P., & Hjalmarsson, A. (2009). Embodied con-
versational agents in computer assisted language 
learning. Speech communication, 51(10), 1024-
1037. 
228
Proceedings of the SIGDIAL 2013 Conference, pages 163?172,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Exploring the effects of gaze and pauses  
in situated human-robot interaction  
 
Gabriel Skantze, Anna Hjalmarsson, Catharine Oertel 
KTH Speech, Music and Hearing 
Stockholm, Sweden 
gabriel@speech.kth.se, annah@speech.kth.se, catha@kth.se 
 
  
 
Abstract 
In this paper, we present a user study where a 
robot instructs a human on how to draw a 
route on a map, similar to a Map Task. This 
setup has allowed us to study user reactions to 
the robot?s conversational behaviour in order 
to get a better understanding of how to gener-
ate utterances in incremental dialogue systems. 
We have analysed the participants' subjective 
rating, task completion, verbal responses, gaze 
behaviour, drawing activity, and cognitive 
load. The results show that users utilise the ro-
bot?s gaze in order to disambiguate referring 
expressions and manage the flow of the inter-
action. Furthermore, we show that the user?s 
behaviour is affected by how pauses are real-
ised in the robot?s speech.  
1 Introduction 
Dialogue systems have traditionally relied on 
several simplifying assumptions. When it comes 
to temporal resolution, the interaction has been 
assumed to take place with a strict turn-taking 
protocol, where each speaker takes discrete turns 
with noticeable gaps in between. While this as-
sumption simplifies processing, it fails to model 
many aspects of human-human interaction such 
as turn-taking with very short gaps or brief over-
laps and backchannels in the middle of utteranc-
es (Heldner & Edlund, 2010). Recently, re-
searchers have turned to more incremental mod-
els, where the dialogue is processed in smaller 
units (Schlangen & Skantze, 2011). On the out-
put side, this allows dialogue systems to start 
speaking before processing is complete, generat-
ing and synthesizing the response segment by 
segment, until the complete response is realised. 
If a segment is delayed, there will be a pause in 
the middle of the system?s speech. While previ-
ous studies have clearly shown the potential ben-
efits of incremental speech generation (Skantze 
& Hjalmarsson, 2012; Dethlefs et al, 2012; 
Buschmeier et al, 2012), there are few studies on 
how users react to pauses in the middle of the 
system?s speech.  
Apart from the real-time nature of spoken in-
teraction, spoken dialog technology has for a 
long time also neglected the physical space in 
which the interaction takes place. In application 
scenarios which involve situated interaction, 
such as human-robot interaction, there might be 
several users talking to the system at the same 
time (Bohus & Horvitz, 2010), and there might 
be physical objects in the surroundings that the 
user and the system refer to during the interac-
tion (Boucher et al, 2012). In such settings, gaze 
plays a very important role in the coordination of 
joint attention and turn-taking. However, it is not 
clear to what extent humans are able to utilize 
the gaze of a robot and respond to these cues.  
Here, we present a user study where a robot 
instructs a human on how to draw a route on a 
map, similar to a Map Task. The nature of this 
setting allows us to study the two phenomena 
outlined above. First, we want to understand how 
a face-to-face setting facilitates coordination of 
actions between a robot and a user, and how well 
humans can utilize the robot's gaze to disambig-
uate referring expressions in situated interaction. 
The second purpose of this study is to investigate 
how the system can either inhibit or encourage 
different types of user reactions while pausing by 
using filled pauses, gaze and syntactic complete-
ness.   
2 Background 
2.1 Gaze in situated interaction 
Gaze is one of the most studied visual cues in 
face-to-face interaction, and it has been associat-
ed with a variety of functions, such as managing 
attention (Vertegaal et al, 2001), expressing in-
timacy and exercising social control (Kleinke, 
163
1986), highlighting the information structure of 
the propositional content of speech (Cassell, 
1999) as well as coordinating turn-taking 
(Duncan, 1972). One of the most influential pub-
lications on this subject (Kendon, 1967) shows 
that speakers gaze away when initiating a new 
turn. At the end of a turn, in contrast, speakers 
shift their gaze towards their interlocutors as to 
indicate that the conversational floor is about to 
become available. Furthermore, it has been 
shown that gaze plays an important role in col-
laborative tasks. In a map task study by Boyle et 
al. (1994), it was shown that speakers in a face-
to-face setting interrupt each other less and use 
fewer turns, words, and backchannels per dia-
logue than speakers who can not see each other. 
A lot of research has also been done on how 
gaze can be used to facilitate turn-taking with 
robots (Mutlu et al, 2006; Al Moubayed et al, 
2013) and embodied conversational agents 
(Torres et al, 1997). Several studies have also 
explored situated human-robot interaction, where 
the interlocutors sit around a table with objects 
that can be referred to, thus constituting a shared 
space of attention (Yoshikawa et al, 2006; John-
son-Roberson et al, 2011). However, there are 
very few studies on how the robot?s gaze at ob-
jects in the shared visual scene may improve task 
completion in an interactive setting. One excep-
tion is a controlled experiment presented by 
Boucher et al (2012), where the iCub robot in-
teracted with human subjects. While the study 
showed that humans could utilize the robot?s 
gaze, the interaction was not that of a free con-
tinuous dialogue.  
Similarly to the study presented here, Nakano 
et al (2003) presented a system that describes a 
route to a user in a face-to-face setting. Based on 
studies of human-human interaction, they im-
plemented a model of face-to-face grounding. 
However, they did not provide a detailed analysis 
of the users? behaviour when interacting with 
this system. 
Even if we successfully manage to model hu-
man-like behaviour in a system, it is not certain 
to what extent humans react to these signals 
when interacting with a robot. In the current 
work, we investigate to what extent the robot?s 
gaze can be used to: (1) help the user disambigu-
ate referring expressions to objects in the shared 
visual scene, and (2) to either inhibit or encour-
age different types of user reactions while the 
system pauses or at turn endings. 
2.2 Pauses in the system's speech 
Speakers in dialogue produce speech piece by 
piece as the dialogue progresses. When starting 
to speak, dialogue participants typically do not 
have a complete plan of how to say something or 
even what to say. Yet, they manage to rapidly 
integrate information from different sources in 
parallel and simultaneously plan and realize new 
dialogue contributions (Levelt, 1989). Still, 
pauses occur frequently within utterances and it 
has been shown that these play a significant role 
in human-human dialogue (for an overview, see 
Rochester, 1973). For example, the timing and 
duration of pauses have important structural 
functions (Goldman-Eisler, 1972), pauses (filled 
and silent) are associated with high cognitive 
load and planning difficulties (Brennan & Wil-
liams, 1995), and whether a pause is detected or 
not does not only depend on duration but also on 
its linguistic context (Boomer & Dittmann, 
1962). 
Recently, several studies have looked into the 
possibilities of replicating the incremental behav-
iour of humans in human-machine interaction. 
Work on incremental speech generation has fo-
cused on the underlying system architecture 
(Schlangen & Skantze, 2011), how to incremen-
tally react to events that occur while realizing an 
utterance (Dohsaka & Shimazu, 1997, 
Buschmeier et al, 2012), and how to make the 
incremental processes more efficient in order to 
reduce the system?s response time (e.g. Dethlefs 
et al, 2012). In a recent study, we implemented a 
model of incremental speech generation in a dia-
logue system (Skantze & Hjalmarsson, 2012). By 
allowing the system to generate and synthesize 
the response segment by segment, the system 
could start to speak before the processing of the 
input was complete. However, if a system seg-
ment was delayed for some reason, the system 
generated a response based on the information 
obtained so far or by generating a pause (filled or 
unfilled). The system also employed self-repairs 
when the system needed to revise an already re-
alised speech segment. Despite these disfluencies 
(filled pauses and self-repairs), an evaluation of 
the system showed that in comparison to a non-
incremental version, the incremental version had 
a shorter response time and was perceived as 
more efficient by the users. 
However, pauses do not only have to be a 
side-effect of processing delays. Pauses could 
also be used wisely to chunk longer instructions 
into shorter segments, giving the user enough 
164
time to process the information. In this case, the 
system should instead invite user reactions dur-
ing the course of its utterance. In the current 
work, we investigate to what extent the system 
can use filled pauses, syntactic completeness and 
gaze as cues to either inhibit or encourage the 
user to react when the system pauses.  
3 Human-robot Map Task data 
Map Task is a well establish experimental para-
digm for collecting data on human-human dia-
logue [30]. Typically, an instruction-giver has a 
map with landmarks and a route, and is given the 
task of describing this route to an instruction-
follower, who has a similar map but without the 
route drawn on it. In a previous study, (Skantze, 
2012) we used this paradigm for collecting data 
on how humans elicit feedback in human-
computer dialogue. In that study, the human was 
the instruction-giver. In the current study, we use 
the same paradigm for a human-robot dialogue, 
but here the robot is the instruction-giver and the 
human is the instruction-follower. This has re-
sulted in a rich multi-modal corpus of various 
types of user reactions to the robot?s instructions, 
which vary across conditions.  
 
Figure 1: The experimental setup. 
3.1 A Map Task dialogue system 
The experimental setup is shown in Figure 1. 
The user is seated opposite to the robot head 
Furhat (Al Moubayed et al, 2013), developed at 
KTH. Furhat uses a facial animation model that 
is back-projected on a static mask. The head is 
mounted on a neck (with 3 degrees of freedom), 
which allows the robot to direct its gaze using 
both eye and head movements. The dialogue sys-
tem was implemented using the IrisTK frame-
work developed at KTH (Skantze & Al Mou-
bayed, 2012), which provides a set of modules 
for input and output, including control of Furhat 
(facial gestures, eye and head movements), as 
well as a statechart-based authoring language for 
controlling the flow of the interaction. For 
speech synthesis, we used the CereVoice unit 
selection synthesizer developed by CereProc 
(www.cereproc.com). 
Between the user and the robot lies a large 
map printed on paper. In addition, the user has a 
digital version of the map presented on a screen 
and is given the task to draw the route that the 
robot describes with a digital pen. However, the 
landmarks on the user?s screen are blurred and 
therefore the user also needs to look at the large 
map in order to identify the landmarks. This map 
thereby constitutes a target for joint attention. 
While the robot is describing the route, its gaze is 
directed at the landmarks under discussion (on 
the large map), which should help the user to 
disambiguate between landmarks. In a previous 
study, we have shown that human subjects can 
identify the target of Furhat's gaze with an accu-
racy that is very close to that of observing a hu-
man (Al Moubayed et al, 2013). At certain plac-
es in the route descriptions, the robot also looks 
up at the user. A typical interaction between the 
robot and a user is shown in Table 1. As the ex-
ample illustrates, each instruction is divided into 
two parts with a pause in between, which results 
in four phases per instruction: Part I, Pause, Part 
II and Release. Whereas user responses are not 
mandatory in the Pause phase (the system will 
continue anyway after a short silence threshold, 
as in U.2), the Release requires a verbal re-
sponse, after which the system will continue. We 
have explored three different realisations of 
pauses, which were systematically varied in the 
experiment: 
COMPLETE: Pauses preceded by a syntactically 
complete  phrase (R.5). 
INCOMPLETE: Pauses preceded by a syntactical-
ly incomplete phrase (R.9).  
FILLED: Pauses preceded by a filled pause (R.1). 
The phrase before the filled pause was some-
times incomplete and sometimes complete. 
To make the conditions comparable, the amount 
of information given before the pauses was bal-
anced between conditions. Thus, the incomplete 
phrases still contained an important piece of in-
formation and the pause was inserted in the be-
ginning of the following phrase (as in R.9).  
  
165
Table 1: An example interaction. 
Turn Activity Phase 
R.1 [gazing at map] continue towards the 
lights, ehm... 
Part I 
U.2 [drawing] Pause 
R.3 until you stand south of the stop 
lights [gazing at user] 
Part II 
U.4 [drawing] alright [gazing at robot] Release 
R.5 [gaze at map] continue and pass east 
of the lights... 
Part I 
U.6 okay [drawing] Pause 
R.7 ...on your way towards the tower 
[gaze at user] 
Part II 
U.8 Could you take that again? Release 
R.9 [gaze at map] Continue to the large 
tower, you pass... 
Part I 
U.10 [drawing] Pause 
R.11 ...east of the stop lights [gaze at user] Part II 
U.12 [drawing] okay, I am at the tower Release 
 
 
Figure 2: An example map. 
Given the current limitations of conversational 
speech recognition, and lack of data relevant for 
this task, we needed to employ some trick to be 
able to build a system that could engage in this 
task in a convincing way in order to evoke natu-
ral reactions from the user. One possibility would 
be to use a Wizard-of-Oz setup, but that was 
deemed to be infeasible for the time-critical be-
haviour that is under investigation here. Instead, 
we employed a trick similar to the one used in 
(Skantze, 2012). Although the users are told that 
the robot cannot see their drawing behaviour, the 
drawing on the digital map, together with a voice 
activity detector that detects the user?s verbal 
responses, is actually used by the system to se-
lect the next action. An example of a map can be 
seen in Figure 2. On the intended route (which 
obviously is not shown on the user?s screen), a 
number of hidden ?spots? were defined ? posi-
tions relative to some landmark (e.g. ?east of the 
field?). Each instruction from the system was 
intended to guide the user to the next hidden 
spot. Each map also contained an ambiguous 
landmark reference (as ?the tower? in the exam-
ple). 
Pilot studies showed that there were three 
basic kinds of verbal reactions from the user: (1) 
an acknowledgement of some sort, encouraging 
the system to continue, (2) a request for repeti-
tion, or (3) a statement that some misunderstand-
ing had occurred. By combining the length of the 
utterance with the information about the progres-
sion of the drawing, these could be distinguished 
in a fairly robust manner. How this was done is 
shown in Table 2. Notice that this scheme allows 
for both short and long acknowledgements (U.4, 
U.6 and U.12 in the example above), as well as 
clarification requests (U.8). It also allows us to 
explore misunderstandings, i.e. cases where the 
user thinks that she is at the right location and 
makes a short acknowledgement, while she is in 
fact moving in the wrong direction. Such prob-
lems are usually detected and repaired in the fol-
lowing turns, when the system continues with the 
instruction from the intended spot and the user 
objects with a longer response. This triggers the 
system to either RESTART the instruction from a 
previous spot where the user is known to have 
been ("I think that we lost each other, could we 
start again from where you were at the bus 
stop?"), or to explicitly CHECK whether the user 
is at the intended location ("Are you at the bus 
stop?"), which helps the user to correct the path.  
Table 2: The system?s action selection based on 
the user?s voice activity and drawing. 
User  
response 
Drawing Action 
Short/Long Continues to the 
next spot 
CONTINUE 
Short/Long Still at the same 
spot 
REPHRASE 
Short (<1s.) At the wrong spot CONTINUE (with  
misunderstanding) 
Long (>1s.) At the wrong spot RESTART or CHECK 
No resp. Any CHECK 
3.2 Experimental conditions  
In addition to the utterance-level conditions 
(concerning completeness) described above, 
three dialogue-level conditions were implement-
ed:  
CONSISTENT gaze (FACE): The robot gazes at 
the landmark that is currently being described 
during the phases Part I, Pause and Part II. In 
166
accordance with the findings in for example 
Kendon (1967), the robot looks up at the end 
of phase Part II, seeking mutual gaze with the 
user during the Release phase. 
RANDOM gaze (FACE): A random gaze behav-
iour, where the robot randomly shifts between 
looking at the map (at no particular landmark) 
and looking at the user, with an interval of 5-
10 seconds. 
NOFACE: The robot head was hidden behind a 
paper board so that the user could not see it, 
only hear the voice. 
3.3 Data collection and analysis 
We collected a corpus of 24 subjects interacting 
with the system, 20 males and 4 females between 
the ages of 21-47. Although none of them were 
native speakers, all of them had a high proficien-
cy in English. First, each subject completed a 
training dialogue and then six dialogues that 
were used for the analysis. For each dialogue, 
different maps were used. The subjects were di-
vided into three groups with 8 subjects in each:  
Group A: Three maps with the CONSISTENT 
(FACE) version and three maps with the 
NOFACE version. All pauses were 1.5 s. long. 
Group B: Three maps with the RANDOM (FACE) 
version and three maps with the NOFACE ver-
sion. All pauses were 1.5 s. long. 
Group C: Three maps with the CONSISTENT ver-
sion and three maps with the NOFACE ver-
sion. All pauses were 2-4 s. long (varied ran-
domly with a uniform distribution).  
For all groups, the order between the FACE 
and the NOFACE condition was varied and bal-
anced. Group A and Group B allow us to explore 
differences between the CONSISTENT and RAN-
DOM versions. This is important, since it is not 
evident to what extent the mere presence of a 
face affects the interaction and to what extent 
differences are due to a consistent gazing behav-
iour. Group C was added to the data collection 
since we wanted to be able to study users' behav-
iour during pauses in more detail. Thus, Group C 
will only be used to study within-group effects of 
different pause types and will not be compared 
against the other groups.  
After the subjects had interacted with the sys-
tem, they filled out a questionnaire. First, they 
were requested to rate with which version (FACE 
or NOFACE) it was easier to complete the task. 
Second, the participants were requested to rate 
whether the robot?s gaze was helpful or confus-
ing when it came to task completion, landmark 
identification and the timing of feedback. All 
ratings were done on a continuous horizontal line 
with either FACE or ?the gaze was helpful? on 
the left end and NOFACE or ?the gaze was con-
fusing? on the right end. The centre of the line 
was labelled with ?no difference?. 
During the experiments, the users? speech and 
face were recorded and all events in the system 
and the drawing activity were automatically 
logged. Afterwards, the users' voice activity that 
had been automatically detected online was 
manually corrected and transcribed. Using the 
video recordings, the users? gaze was also manu-
ally annotated, depending on whether the user 
was looking at the map, the screen or at the ro-
bot.  
 In this study, we also wanted to explore the 
possibility of measuring cognitive load in hu-
man-robot interaction using EDA (electrodermal 
activity). Hence, in an explorative manner, we 
investigated how the realisation of the system?s 
pauses and the presence of the face affected the 
cognitive costs of processing the system?s in-
structions. For measuring this, we used a weara-
ble EDA device, which exerts a direct current on 
the skin of the subject in order to measure skin 
conductance responses. For these measurements 
as well as the logging of the data the Q-Sensor 
developed by Affectiva 1  was used. The meas-
urements were taken from the fingertips of the 
subjects. The sampling rate was 8 Hz. All post 
processing was carried out in Ledalab2. We first 
applied the Butterworth filter and then carried 
out a Continuous Decomposition Analysis. All 
skin conductance responses (SCR) with a mini-
mum amplitude of 0.01 muS and a minimal dis-
tance of 700ms were used for further analysis. 
Due to problems with the EDA device, we only 
have data for six subjects in Group A, six in 
Group B and none in Group C.  
4 Results 
Analyses of the different measures used here re-
vealed that they were not normally distributed. 
We have therefore consistently used non-
parametric tests. All tests of significance are 
done using two-tailed tests at the .05 level.  
                                                 
1
 http://www.affectiva.com/ 
2
 http://www.ledalab.de/ 
167
4.1 Subjective ratings 
The questionnaire was used to analyse differ-
ences in subjective ratings between Group A and 
B. The marks on the horizontal continuous lines 
in the questionnaire were measured with a ruler 
based on their distance from the midpoint (la-
belled with ?no difference?) and normalized to a 
scale between 0 and 1. A Wilcoxon Signed 
Ranks Test was carried out, using these rankings 
as differences. The results show that the Con-
sistent version differed significantly from the 
midpoint (?no difference?) in four dimensions 
whereas there were no significant differences 
from the midpoint for RANDOM version. More 
specifically, Group A (CONSISTENT) (n=8) found 
it easier to complete the task in the face condi-
tion than in the no face condition (Mdn=0.88, 
Z=-2.54, p=.012). The same group thought that 
the robot?s gaze was helpful rather than confus-
ing when it came to task completion (Mdn=0.84, 
Z=-2.38, p=.017), landmark identification 
(Mdn=0.83, Z=-2.52, p=.012) and to decide 
when to give feedback (Mdn=0.66, Z=-1.99, 
p=.046). The results of the questionnaire are pre-
sented in Figure 3. 
 
Figure 3: The results from the questionnaire. The 
bars show the median rating for Group A (con-
sistent) and Group B (random). 
4.2 Task completion 
Apart from the subjective ratings, we also want-
ed to see whether the face-to-face setting affect-
ed task completion. In order to explore this, we 
analysed the time and number of utterances it 
took for the users to complete the maps. On av-
erage, the dialogues in Group A (CONSISTENT) 
were 2.5 system utterances shorter and 8.9 sec-
onds faster in the FACE condition than in the 
NOFACE condition. For Group B (RANDOM), the 
dialogues were instead 2.3 system utterances and 
17.3 seconds longer in the FACE condition 
(Mann-Whitney U-test, p<.05). Thus, it seems 
like the face facilitates the solving of the task, 
and that this is not just due to the mere presence 
of a face, but that the intelligent gaze behaviour 
actually contributes. In fact, the RANDOM gaze 
worsens the performance, possibly because sub-
jects spent time on trying to make sense of sig-
nals that did not provide any useful information. 
Looking at more local phenomena, it seems 
like there was also a noticeable difference when 
it comes to miscommunication. The dialogues in 
the RANDOM/FACE condition had a total of 18 
system utterances of the type RESTART (vs. 7 in 
CONSISTENT), and a total of 33 CHECK utteranc-
es (vs. 15 in CONSISTENT). A chi-square test 
shows that the differences are statistically signif-
icant (?2(1, N=25) = 4.8, p =.028; ?2(1, N=48) = 
6.75, p=.009). This indicates that the users that 
did not get the CONSISTENT gaze to a larger ex-
tent did not manage to follow the system?s in-
structions, most likely because they did not get 
guidance from the robot?s gaze in disambiguat-
ing referring expressions. 
4.3 Gaze behaviour 
In order to analyse the users? direction of atten-
tion during the dialogues, the manual annotation 
of the participants? gaze was analysed. First, we 
explored how the completion type of the robot's 
utterance affected the users? gaze. In this analy-
sis, FILLED and INCOMPLETE have been merged 
(since there was no difference in the users? gaze 
between these conditions). The percentage of 
gaze at the robot over the four different utterance 
phases for complete and incomplete utterances is 
plotted in Figure A in the Appendix. Note that 
the different phases actually are of different 
lengths depending on the actual content of the 
utterance and the length of the pause. However, 
these lengths have been normalized in order to 
make it possible to analyse the average user be-
haviour. For each phase, a Mann-Whitney U-test 
was conducted. The results show that the per-
centage of gaze at Furhat during the mid-
utterance pause is higher when the first part of 
the utterance is incomplete than when it is com-
plete (U=7573.0, p<.001). There were, however, 
no significant differences in gaze direction be-
tween complete and incomplete utterance during 
the other three phases (p>.05). This indicates that 
users gaze at the robot to elicit a continuation of 
the instruction when it is incomplete. 
Second, we wanted to explore if gaze direction 
can be used as a cue of whether the user will 
provide a verbal response in the pause or not. 
The percentage of gaze at the robot over the four 
utterance phases for system utterances with and 
0 0.5 1
RANDOM CONSISTENT
Did the robot?s gaze help you to 
understand which landmark he was 
talking about? (0=confusing, 1=helpful)
Did the robot?s gaze help you to 
complete the task?(0=confusing, 
1=helpful)
Did the robot?s gaze affect your 
decisions of when to give feedback?
(0=confusing, 1=helpful)
When was it easier to complete
the task? (noFace=0, face=1)
?No difference?
168
without user response in the pause is plotted in 
Figure B in the Appendix. For each phase, a 
Mann-Whitney U-test was conducted. The re-
sults show that the percentage of gaze at Furhat 
during the mid-utterance pause (U=1945.5, 
p=.008) and Part II (U=2090.0, p=.008) of the 
utterance is lower when the user gives a verbal 
response compared to when there is no response. 
There were however no significant differences in 
gaze direction between complete and incomplete 
utterance during the other two phases (p>.05). 
4.4 Verbal feedback behaviour 
Apart from the user?s gaze behaviour, we also 
wanted to see whether syntactic completeness 
before pauses had an effect on whether the users 
gave verbal responses in the pause. Figure 4 
shows the extent to which users gave feedback 
within pauses, depending on pause type and 
FACE/NOFACE condition. As can be seen, COM-
PLETE triggers more feedback, FILLED less feed-
back and INCOMPLETE even less. Interestingly, 
this difference is more distinct in the FACE con-
dition (?2(2, N=157) = 10.32, p<.01). In fact, the 
difference is not significant in the NOFACE con-
dition (p >.05).  
 
Figure 4: Presence of feedback depending on 
pause type (Group C). 
In Skantze et al (2013), we have also done a 
more thorough analysis of the verbal acknowl-
edgements from the users. The analysis shows 
that the prosody and lexical choice in these 
acknowledgements ("okay", "yes", "yeah", 
"mm", "mhm", "ah", "alright" and "oh") to some 
extent signal whether the drawing activity is 
about to be initiated or has been completed. The 
analysis also shows how these parameters are 
correlated to the perception of uncertainty. 
4.5 Drawing behaviour 
Whereas gaze and verbal responses can be re-
garded as communicative signals, the users were 
told that the robot could not observe their draw-
ing activity. However, the drawing of the route 
can be regarded as the purpose of the interaction 
and it is therefore important to understand how 
this is affected by the system?s behaviour under 
different conditions. First, we wanted to see how 
the completeness of the robot's utterance in com-
bination with the presence of the face affected 
the drawing activity. In this analysis, FILLED and 
INCOMPLETE have been merged (since there was 
no clear difference). The mean drawing activity 
over the four phases of the descriptions is plotted 
in Figure C in the Appendix. For each phase, a 
Kruskal-Wallis test was conducted showing that 
there is a significant difference between the con-
ditions in the Pause phase (H(3) = 28.8, p<.001). 
Post-hoc tests showed that FACE/INCOMPLETE 
has a lower drawing activity than the other con-
ditions, and that NOFACE/INCOMPLETE has a 
lower drawing activity than the COMPLETE con-
dition. Thus, INCOMPLETE phrases before pauses 
seem to have an inhibiting effect on the user?s 
drawing activity in general, but this effect ap-
pears to be much larger in the FACE condition. 
Second, we aimed to investigate to what ex-
tent the robot?s gaze at landmarks during ambig-
uous references helps users to discriminate be-
tween landmarks.  The mean drawing activity 
over the four phases of the descriptions of am-
biguous landmarks is plotted in Figure D in the 
Appendix. For each phase, a Kruskal-Wallis test 
was conducted showing that there is a significant 
difference between the conditions in the Part II 
phase (H(2)=10.2, p=.006). Post-hoc tests 
showed that CONSISTENT has a higher drawing 
activity than the RANDOM and NOFACE condi-
tions. However, there is no such difference when 
looking at non-ambiguous descriptions. This 
shows that robot?s gaze at the target landmark 
during ambiguous references makes it possible 
for the subjects to start to draw quicker.  
4.6 Cognitive load 
As mentioned above, we also wanted to study the 
cognitive costs of processing the system?s in-
structions, as measured with a wearable EDA 
device. For each system utterance part (Part I and 
Part II), we calculated the sum of the amplitudes 
of the skin conductance responses (SoSCR) dur-
ing the following three seconds. The SoSCR dur-
ing the pause, depending on pause type are 
shown in Figure 5. A Kruskal-Wallis test re-
vealed that there is an overall effect (H(2)=8.7, 
p=.13), and post-hoc tests showed that there is a 
significant difference between utterances which 
are incomplete and those with filled pauses, indi-
169
cating that the syntactic incompleteness without 
a filled pause leads to a higher cognitive load.  
We have no good explanation for this, and we do 
not know whether this is due to how the syntacti-
cally incomplete segments were realised by the 
synthesizer, or whether the same effect would 
appear in human-human interaction. 
 
Figure 5: EDA at different pause types (Group A 
and B). 
A similar analysis was done after both Part I and 
Part II to see if there is any difference in SoSCR 
between ambiguous and non-ambiguous refer-
ences in the different conditions, as shown in in 
Figure 6. No such differences were found for 
Group B, but for Group A, ambiguous references 
were followed by a higher SoSCR in the 
NOFACE condition, indicating that the robot?s 
gaze helps in disambiguating the referring ex-
pressions and reduces cognitive load (Mann-
Whitney U-test; U = 6585, p = .001).  
 
Figure 6: EDA for Group A (CONSISTENT). 
5 Conclusions and Discussion 
In this study, we have investigated to what extent 
the robot?s gaze can be used to: (1) help the user 
disambiguate referring expressions to objects in 
the shared visual scene, and (2) to either inhibit 
or encourage different types of user reactions 
while the system pauses. The  results show  that  
the robot?s gaze behaviour  was  rated  as  help-
ful  rather  than  confusing for  task completion,  
landmark  identification and feedback timing. 
These effects were not present when the robot 
used a random gaze behaviour. The efficiency of 
the gaze was further supported by the time it 
took to complete the task and the number of mis-
understandings. These results in combination 
with a faster drawing activity and lower cogni-
tive load when system?s reference was ambigu-
ous, suggest that the users indeed utilized the 
system?s gaze to discriminate between land-
marks.  
The second purpose of this study was to inves-
tigate to what extent filled pauses, syntactic 
completeness and gaze can be used as cues to 
either inhibit or encourage the user to react in 
pauses. First, the results show that pauses pre-
ceded by incomplete syntactic segments or filled 
pauses appear to inhibit user activity. Thus, our 
analyses of gaze and drawing activity show that 
users give less feedback, draw less and look at 
the robot to a larger extent when the preceding 
system utterance segment is incomplete than 
when it is complete. An interesting observation is 
that the inhibiting effect on drawing activity ap-
pears to be more pronounced in the face-to-face 
condition, which indicates that gaze also plays an 
important role here (since the robot looked down 
at the map during the pauses). Additionally, there 
is less cognitive load when the silence is preced-
ed by a filled pause. These results suggest that 
incomplete system utterances prevent further 
user processing; instead the user waits for more 
input from the system before starting to carry out 
the system?s instruction. After complete utter-
ance segments, however, there is more drawing 
activity and the user looks less at the robot, sug-
gesting that the user has already started to carry 
out the system?s instruction. 
The results presented in this study have impli-
cations for generating multimodal behaviours 
incrementally in dialogue systems for human-
robot interaction. Such a system should be able 
to generate speech and gaze intelligently in order 
to inhibit or encourage the user to act, depending 
on the state of the system's processing. In future 
studies, we plan to extend our previous model of 
incremental speech generation (Skantze & 
Hjalmarsson, 2012) with such capabilities.   
Acknowledgments 
Gabriel Skantze is supported by the Swedish research 
council (VR) project Incremental processing in mul-
timodal conversational systems (2011-6237). Anna 
Hjalmarsson is supported by the Swedish Research 
Council (VR) project Classifying and deploying paus-
es for flow control in conversational systems (2011-
6152). Catharine Oertel is supported by GetHomeSafe 
(EU 7th Framework STREP 288667).  
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
Complete Incomplete Filled
So
SC
R 
(m
uS
)
170
References  
Al Moubayed, S., Skantze, G., & Beskow, J. (2013). 
The Furhat Back-Projected Humanoid Head - Lip 
reading, Gaze and Multiparty Interaction. Interna-
tional Journal of Humanoid Robotics, 10(1). 
Anderson, A., Bader, M., Bard, E., Boyle, E., 
Doherty, G., Garrod, S., Isard, S., Kowtko, J., 
McAllister, J., Miller, J., Sotillo, C., Thompson, H., 
& Weinert, R. (1991). The HCRC Map Task corpus. 
Language and Speech, 34(4), 351-366. 
Bohus, D., & Horvitz, E. (2010). Facilitating multi-
party dialog with gaze, gesture, and speech. In 
Proc ICMI?10. Beijing, China. 
Boomer, D. S., & Dittmann, A. T. (1962). Hesitation 
pauses and juncture pauses in speech. Language 
and Speech, 5, 215-222. 
Boucher, J. D., Pattacini, U., Lelong, A., Bailly, G., 
Elisei, F., Fagel, S., Dominey, P. F., & Ventre-
Dominey, J. (2012). I reach faster when I see you 
look: gaze effects in human-human and human-
robot face-to-face cooperation. Frontiers in neuro-
robotics, 6. 
Boyle, E., Anderson, A., & Newlands, A. (1994). The 
effects of visibility on dialogue and performance 
in a cooperative problem solving task. Language 
and speech, 37(1), 1-20. 
Brennan, S., & Williams, M. (1995). The Feeling of 
Another's knowing: Prosody and Filled Pauses as 
Cues to Listeners about the Metacognitive States 
of Speakers. Journal of Memory and Language, 
34, 383-398. 
Buschmeier, H., Baumann, T., Dosch, B., Kopp, S., & 
Schlangen, D. (2012). Combining incremental 
language generation and incremental speech syn-
thesis for adaptive information presentation. In 
Proceedings of SigDial (pp. 295?303). Seoul, 
South Korea. 
Cassell, J. (1999). Nudge, nudge, wink, wink: Ele-
ments of face-toface conversation for embodied 
conversational agents. In Cassell, J., Suillivan, J., 
Prevost, S., & Churchill, E. (Eds.), Embodied 
Conversational Agents. Cambridge, MA: MIT 
Press. 
Dethlefs, N., Hastie, H., Rieser, V., & Lemon, O. 
(2012). Optimising Incremental Dialogue Deci-
sions Using Information Density for Interactive 
Systems. In Proceedings of the Conference on 
Empirical Methods in Natural Language Pro-
cessing (EMNLP) (pp. 82-93). Jeju, South Korea. 
Dohsaka, K., & Shimazu, A. (1997). System architec-
ture for spoken utterance production in collabora-
tive dialogue. In Working Notes of IJCAI 1997 
Workshop on Collaboration, Cooperation and 
Conflict in Dialogue Systems.  
Duncan, S. (1972). Some Signals and Rules for Tak-
ing Speaking Turns in Conversations. Journal of 
Personality and Social Psychology, 23(2), 283-
292. 
Goldman-Eisler, F. (1972). Pauses, clauses, sentences. 
Language and Speech, 15, 103-113. 
Heldner, M., & Edlund, J. (2010). Pauses, gaps and 
overlaps in conversations. Journal of Phonetics, 
38, 555-568. 
Johnson-Roberson, M., Bohg, J., Skantze, G., Gus-
tafson, J., Carlson, R., Rasolzadeh, B., & Kragic, 
D. (2011). Enhanced Visual Scene Understanding 
through Human-Robot Dialog. In IEEE/RSJ Inter-
national Conference on Intelligent Robots and 
Systems.  
Kendon, A. (1967). Some functions of gaze direction 
in social interaction. Acta Psychologica, 26, 22-
63. 
Kleinke, C. L. (1986). Gaze and eye contact: a re-
search review. Psychological Bulletin, 100, 78-
100. 
Mutlu, B., Forlizzi, J., & Hodgins, J. (2006). A story-
telling robot: Modeling and evaluation of human-
like gaze behavior. In Proceedings of 6th IEEE-
RAS International Conference on Humanoid Ro-
bots (pp. 518-523).  
Nakano, Y., Reinstein, G., Stocky, T., & Cassell, J. 
(2003). Towards a model of face-to-face ground-
ing. In Proceedings of the Annual Meeting of the 
Association for Computational Linguistics (ACL 
2003) (pp. 553-561).  
Rochester, S. R. (1973). The significance of Pauses in 
Spontaneous Speech. Journal of Psycholinguistic 
Research, 2(1). 
Schlangen, D., & Skantze, G. (2011). A General, Ab-
stract Model of Incremental Dialogue Processing. 
Dialogue & Discourse, 2(1), 83-111. 
Skantze, G., & Al Moubayed, S. (2012). IrisTK: a 
statechart-based toolkit for multi-party face-to-
face interaction. In Proceedings of ICMI. Santa 
Monica, CA. 
Skantze, G., & Hjalmarsson, A. (2012). Towards In-
cremental Speech Generation in Conversational 
Systems. Computer Speech & Language, 27(1), 
243-262. 
Skantze, G., Oertel, C., & Hjalmarsson, A. (2013). 
User feedback in human-robot interaction: Proso-
dy, gaze and timing. In Proceedings of Inter-
speech.  
Skantze, G. (2012). A Testbed for Examining the 
Timing of Feedback using a Map Task. In Pro-
ceedings of the Interdisciplinary Workshop on 
Feedback Behaviors in Dialog. Portland, OR. 
Torres, O., Cassell, J., & prevost, S. (1997). Modeling 
gaze behavior as a function of discourse structure. 
Proc. of the First International Workshop on Hu-
man-Computer Conversation. 
Vertegaal, R., Slagter, R., van der Veer, G., & Nijholt, 
A. (2001). Eye gaze patterns in conversations: 
there is more to conversational agents than meets 
the eyes. In Proceedings of ACM Conf. on Human 
Factors in Computing Systems.  
Yoshikawa, Y., Shinozawa, K., Ishiguro, H., Hagita, 
N., & Miyamoto, T. (2006). Responsive robot 
gaze to interaction partner. In Proceedings of ro-
botics: Science and systems.  
171
Appendix 
 
 Part I Pause Part II Release 
Figure A: Average user gaze depending on pause type (Group C). 
 
 Part I Pause Part II Release 
Figure B: Average user gaze depending whether the user responds in the pause (Group A and B). 
 
 Part I Pause Part II Release 
Figure C: Average drawing activity depending on pause type and the presence of the face (Group C). 
 
 Part I Pause Part II Release 
Figure D: Average drawing activity during ambiguous references depending on condition (Group A and B). 
172
