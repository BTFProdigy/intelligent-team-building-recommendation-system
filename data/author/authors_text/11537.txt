Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 19?24,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Iterative Approach for Joint  
Dependency Parsing and Semantic Role Labeling 
 
 
Qifeng Dai 
Department of Computer Sci-
ence, University of Science and 
Technology of China, Hefei, 
China 
daiqifeng001@126.com  
Enhong Chen 
Department of Computer Sci-
ence, University of Science and 
Technology of China, Hefei, 
China 
cheneh@ustc.edu.cn 
Liu Shi 
Department of Computer Sci-
ence, University of Science and 
Technology of China, Hefei, 
China 
shiliu@ustc.edu 
 
 
 
Abstract 
We propose a system to carry out the joint pars-
ing of syntactic and semantic dependencies in 
multiple languages for our participation in the 
shared task of CoNLL-2009. We present an it-
erative approach for dependency parsing and 
semantic role labeling. We have participated in 
the closed challenge, and our system achieves 
73.98% on labeled macro F1 for the complete 
problem, 77.11% on labeled attachment score 
for syntactic dependencies, and 70.78% on la-
beled F1 for semantic dependencies. The cur-
rent experimental results  show that our method 
effectively improves system performance. 
1 Introduction 
In this paper we describe the system submitted to 
the closed challenge of the CoNLL-2009 shared 
task on joint parsing of syntactic and semantic de-
pendencies in multiple languages.  
Give a sentence, the task of dependency parsing 
is to identify the syntactic head of each word in the 
sentence and classify the relation between the de-
pendent and its head. The task of semantic role 
labeling is to label the senses of predicates in the 
sentence and labeling the semantic role of each 
word in the sentence relative to each predicate. 
The difficulty of this shared task is to perform 
joint task on dependency parsing and semantic role 
labeling. We split the shared task into four sub-
problems: syntactic dependency parsing, syntactic 
dependency label classification, word sense disam-
biguation, and semantic role labeling. And we pro-
pose a novel iterative approach to perform the joint 
task. In the first step, the system performs depend-
ency parsing and semantic role labeling in a pipe-
lined manner and the four sub-problems extract 
features based on the known information. In the 
iterative step, the system performs the four tasks in 
a pipelined manner but uses features extracted 
from the previous parsing result. 
The remainder of the paper is structured as fol-
lows. Section 2 presents the technical details of our 
system. Section 3 presents experimental results and 
the performance analysis. Section 4 looks into a 
few issues concerning our forthcoming work for 
this shared task, and concludes the paper. 
2 System description 
This section briefly describes the main components 
of our system: a) system flow; b) syntactic parsing; 
c) semantic role labeling; d) an iterative approach 
to perform joint syntactic-semantic parsing. 
2.1 System flow 
As many systems did in CoNLL Shared Task 2008, 
the most direct way for such task is pipeline ap-
proach. First, Split the system into four subtasks: 
syntactic dependency parsing, syntactic depend-
ency relation labeling, predicate sense labeling and 
semantic role labeling. Then, execute them one by 
one. In our system, we extend this pipeline system 
to an iterative system so that it can do a joint label-
ing to improve the performance. 
Our iterative system is based on the pipeline 
system. For the first iteration (original step), we 
use the pipeline system to parse and label the 
19
whole sentence. For the rest iterations (iterative 
step), we use another pipeline system to parse and 
label it. The structure of this pipeline is the same as 
the original one, but each subtask can have much 
more features than the original subtask. Because 
the whole sentence has been labeled in the original 
step, all information is available for every subtask. 
For example, when doing syntactic dependency 
relation labeling, we can add some features about 
sense and semantic role. It seems like using syntac-
tic results to do semantic labeling, then using se-
mantic results to improve syntactic labeling. This 
is the core idea of our joint system. Figure 1 shows 
the main flow of our system. 
 
 
Figure 1. The main flow of iteration system 
 
 
 
 
2.2 Dependency Parsing 
In the dependency parsing step, we split the task 
into two sub-problems: syntactic dependency pars-
ing and syntactic dependency relation labeling. 
In the syntactic dependency parsing stage, 
MSTParser1, a dependency parser that searches for 
maximum spanning trees over directed graphs, is 
applied. Due to the differences between the seven 
languages, we use different parameters to train a 
parsing model. Specifically, as Czech and German 
languages are none-projective and the others are 
projective, we train Czech and German languages 
with parameter ?none-projective? and the others 
with ?projective?. 
On the syntactic dependency label classification 
step, we used the max-entropy classification algo-
rithm to train the model. This step contains two 
processes. In the first process the sub-problem 
trains the model with the following basic features: 
Start 
End 
Syntactic dependency 
parsing 
Syntactic dependency 
relation labeling 
Set count = iterate times 
Set isIterStep = false 
Predicate sense label-
ing 
Semantic role labeling 
count -- 
isIterStep = true 
count = 0 
Y
 
Get fea-
tures: 
this step 
return the 
feature of 
system 
judge by 
the type of 
sub task  
and the 
parameter 
isIterStep. 
N
? FORM1: FORM of the head. 
? LEMMA1: LEMMA of the head. 
? STEM1 (English only): STEM of the head. 
? POS1: POS of the head. 
? IS_PRED1: the value of FILLPRED of the 
head. 
? FEAT1: FEAT of the head. 
? LM_STEM1 (English only): the left-most 
modifier?s STEM of head. 
? LM_POS1: the left-most modifier?s POS 
of head. 
? L_NUM1: number of the head?s left modi-
fiers. 
? RM_STEM1 (English only): the right-
most modifier?s STEM of head. 
? RM_POS1: the right-most modifier?s POS 
of head. 
? M_NUM1: number of modifiers of the 
head. 
? SUFFIX1 (English only): suffix of the 
head. 
? FORM2: FORM of the dependent. 
? LEMMA2: LEMMA of the dependent. 
? STEM2 (English only): STEM of the de-
pendent. 
? POS2: POS of the dependent. 
? IS_PRED2: the value of FILLPRED of the 
dependent. 
                                                          
1 http://sourceforge.net/projects/mstparser 
20
? FEAT2: FEAT of the dependent. 
? LM_STEM2 (English only): the left-most 
modifier?s STEM of dependent. 
? LM_POS2: the left-most modifier?s POS 
of dependent. 
? L_NUM2:  number of the dependent?s left 
modifiers. 
? RM_STEM2 (English only): the right-
most modifier?s STEM of dependent. 
? RM_POS2: the right-most modifier?s POS 
of dependent. 
? M_NUM2: number of modifiers of the de-
pendent. 
? SUFFIX2 (English only): suffix of the de-
pendent. 
? DEP_PATH_ROOT_POS2: POS list from 
dependent to tree?s root through the syn-
tactic dependency path. 
? DEP_PATH_ROOT_LEN2: length from 
dependent to tree?s root through the syn-
tactic dependency path.  
? POSITION: The position of the word with 
respect to its predicate. It has three values, 
?before?, ?is? and ?after?, for the predicate. 
In the iterative step, in addition to the features 
mentioned above, the sub-task trains the model 
with the following features: 
? DEP_PATH_ROOT_POS1: POS list from 
head to tree?s root through the syntactic 
dependency path. 
? DEP_PATH_ROOT_REL1: length from 
dependent to tree?s root through the syn-
tactic dependency path. 
? PRED_POS: POS list of all predicates in 
the sentence. 
? FORM2 + DEP_PATH_REL: component 
of FORM2 and the POS list from head to 
the dependent through the syntactic de-
pendency path. 
? POSITION + FORM2 
? STEM1 + FORM2 (English only) 
? STEM1 + STEM2 (English only) 
? POSITION + POS2 
? ROLE_LIST2: list of APRED when the 
dependent is a predicate. 
? ROLE: list of APRED and PRED when 
the head is predicate. 
? L_ROLE: the nearest semantic role in its 
left side when head is a predicate. 
? R_ROLE: the nearest semantic role in its 
right side when head is a predicate. 
? IS_ROLE1: whether dependent is a se-
mantic role of head when head is a predi-
cate. 
2.3 Semantic role labeling 
Unlike CoNLL-2008 shared task, this shared task 
does not need to identify predicates. So the main 
task of this step is to label the sense of each predi-
cate and label the semantic role for each predicate. 
When labeling the sense of each predicate, we 
build a classification model for each predicate. As 
the senses of different predicates are usually unre-
lated even if they have the same sense label, this 
makes it difficult for us to use only one classifier to 
label them. But this approach leads to another issue. 
The set of predicates in the training set cannot 
cover all predicates. For new predicates in the test 
set, no classification model can be found for them, 
and we build a most common sense for them. The 
features we used are as follow: 
? DEPREL1: DEPREL of the predicate. 
? STEM1 
? POS1 
? RM_STEM1 (English only) 
? RM_POS1 
? FORM2 
? POS2 
? SUFFIX2 
? VOICE (English only): VOICE of predi-
cate. 
? POSITION + POS2 
? L_POS1 + POS1 + R_POS1: component 
of left word?s POS and predicate POS and 
right word?s POS. 
? FORM2 + DEP_PATH_REL 
? DEP_PATH_ROOT_POS1 
? DEP_PATH_ROOT_REL1 
When labeling the semantic role, we use a simi-
lar approach as we did in CoNLL Shared Task 
2008. However, as the frames information is not 
supplied for all languages, we do not use it in this 
task. The features we use are as follows: 
? DEPREL1 
? STEM1 (English only) 
? POS1 
? RM_STEM1 (English only) 
? RM_POS1 
21
? FORM2 
? POS2 
? SUFFIX2 
? VOICE2 (English only) 
? POSITION 
? DEP_PATH_REL 
? DEP_PATH_POS 
? SENSE2 
? SENSE2 + VOICE2 
? POSITION +  VOICE2 
? DEP_PATH_LEN 
? DEP_PATH_ROOT_REL1 
Moreover, we build an iterative model in this 
shared task. When doing an iterative labeling, the 
previous labeling results are known. So we can 
design some new features for checking the previ-
ous results in a global view. The features we add 
for the iterative model are as follows: 
? SENSE1: SENSE of the predicate. 
? SENSE1 + VOICE1: component of the 
SENSE + VOICE of predicate. 
? VOICE1 + FORM1: component of VOICE 
and FORM. 
? ROLE_LIST1: list of APRED of predicate. 
2.4 Iterative Approach 
As described above, some subtasks have two 
groups of features. One is for the pipeline model, 
and the other is for the iterative model. The usage 
of these two types of model is the same. The only 
difference is that they use different features. The 
iterative model can get more information, so they 
can use more features. These additional features 
can contain some joint and global (like frame and 
global structure) information. The performance 
may be improved because the viewer is extended. 
Some structural error and semantic conflict can be 
fixed. 
Although the usage of the two types of model is 
the same, there are some differences when building 
the models. 
In the iterative step, all information is available 
for doing parsing and labeling. For example, when 
doing syntactic dependency relation labeling in the 
iterative step, the fields ?HEAD?, ?DEPREL?, 
?PRED? and ?APREDs? are filled by the pervious 
iteration. So all these information can be used in 
the iterative step. This will cause one issue: use 
?HEAD1? to label ?HEAD2?. When training the 
model, ?HEAD1? is golden. The classifier will 
build a model directly and let ?HEAD2? equal to 
?HEAD1?. However, in the iterative step, 
?HEAD1? is not golden, but such model makes it 
impossible to change the results.. The iterative step 
will be useless. 
We design a simple method to avoid this issue.  
? Firstly, split the training set into N (N>1) 
subsets.  
? Secondly, for each subset, use the left N-1 
subsets to build an original sub-model (use 
features in the pipeline step). 
? Thirdly, use each sub-model to label the 
corresponding subset. 
? Lastly, use these labeled N subsets to ex-
tract samples (use features in the iterative 
step) for building the iterative model. 
In this way, the ?HEAD1? is not golden any 
more. And for each sub-task, we can use the simi-
lar method to build the original model and the it-
erative model.  
Moreover, in our system, we only build the it-
erative models for syntactic dependency relation 
labeling and semantic role labeling. For syntactic 
dependency parsing, we use an approach with very 
high time and space complexity, so it is not added 
to the iterative step. Thus, its results will not be 
changed in the iterative step. For sense labeling, 
we build classification models for every predicate. 
There are too many models and each model con-
tains only a few classes. We think they are not 
suitable for building the iterative model. But, as its 
previous sub-task (syntactic dependency relation 
labeling) is added to the iterative step, it is useful 
to add it to the iterative step. Though we do not 
build an iterative model for sense labeling, we can 
directly use its pipeline model. This is another ad-
vantage of our iterative model: if one subtask is not 
suitable for doing iterative labeling/parsing, we can 
use its pipeline model instead. 
3 Experiments and Results 
We have tested our system with the test set and 
obtained official results as shown in Table 1. We 
have tried to find how the iterative step influences 
syntactic dependency parsing and semantic role 
labeling. For syntactic dependency parsing and 
semantic role labeling, we do experiments on the 
test set. 
  
22
 Macro F1 Score 
Average 73.98 
Catalan 72.09 
Chinese 72.72 
Czech 67.14 
English 81.89 
German 75.00 
Japanese 80.89 
Spanish 68.14 
Table 1. The Macro F1 Score of every languages and 
the average value. 
3.1 Syntactic Dependency Parsing 
Dependency Parsing can be split into two sub-
problems: syntactic dependency parsing and syn-
tactic dependency label classification. We use the 
iterative method on syntactic dependency label 
classification. We do experiments on the test set.  
On the test set, we do two group experiments. In 
the first group, we build a subtest to test this sub-
task only. All other information is given, and we 
just label the dependency relation. The results are 
shown in Table 2. The row of ?Initial step? shows 
the results of this sub task in the original step. The 
left two rows show the results in the iterative step 
with iterating once and twice. The table shows that 
the iterative approach improves the performance. 
Especially for Catalan, the performance increases 
by 2.89%. 
Certainly, in the whole system, this subtask can-
not get golden information about sense and seman-
tic roles. So we test it in the whole system (joint 
test) on the test set in the second group of experi-
ments. As shown in Table 3, the iterative step is 
not as good as previous test. But it is still useful for 
some languages. The reason that some languages 
have no improvements on the iterative step is that 
the result of the initial step is not so good. 
3.2 Semantic Role Labeling 
Like syntactic dependency parsing, we do two tests 
on Semantic Role Labeling. This result is not con-
sistent with the official data because we have add-
ed some features of the subtask. The results of 
subtest can be found in Table 4. And Table 5 
shows the results of the joint test. These two 
groups of results show that the advantage of the 
iterative step is not as good as that of syntactic de-
pendency labeling in subtest. But it improves the 
performance for most languages. The iterative step 
improves the performance in both two tests.  
3.3 Analysis of Results  
From the experimental results, we can see that the 
effect of each part of the iterative step depends on 
the overall labeling result of the previous step. And 
the labeling effect varies with different languages. 
Iterative approach can improve the performance of 
the system but it strongly depends on the initial 
labeling result.  
4 Conclusion and Future Work  
This paper has presented a simple discriminative 
system submitted to the CoNLL-2009 shared task 
to address the learning task of syntactic and seman-
tic dependencies. The paper first describes how to 
carry out syntactic dependency parsing and seman-
tic role labeling, and then a new iterative approach 
is presented for joint parsing. The experimental 
results show that the iterative process can improve 
the labeling accuracy on syntactic and semantic 
analysis. However, this approach probably depends 
on the accuracy of the initial labeling results. The 
results of the initial labeling results will affect the 
effect of the iterative process.  
Because of time constraints and inadequate ex-
perimental environment, our first results do not 
meet our expectation, and the effect of the iterative 
step is not so clear. Next, we will strive to refine 
our approach to produce good results for the syn-
tactic dependency parsing, since it has a great im-
pact on the final parsing results. 
Acknowledgments 
The authors would like to thank the reviewers for 
their helpful comments. This work was supported 
by National Natural Science Foundation of China 
(No.60573077, No.60775037) and the National 
High Technology Research and Development Pro-
gram of China (863 Program) (grant no. 
2009AA01Z123). We also thank the High-
Performance Center of USTC for providing us 
with the experimental platform. 
 
 
 
 
 
 
 
 
23
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 93.64 95.66 95.01 88.10 88.10 96.79 92.98 96.41 89.71* 98.17 95.48
Iteration 1 94.60 98.56* 96.08* 88.59 88.29 97.31* 94.57* 96.63* 89.31 98.34 98.30
Iteration 2 94.65 98.55 96.08* 88.68* 88.45* 97.29 94.56 96.63* 89.53 98.35* 98.33*
Table 2. The subtest result of Labeled Syntactic Accuracy of each language and the average performance value 
on test set. (* denotes the best score for the system) 
 
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 74.02 77.75 73.81 58.69* 55.50* 84.75 78.85 82.45 66.27* 90.45* 71.64
Iteration 1 73.90 77.82 73.86* 58.17 54.95 84.81 78.95 82.51* 65.78 90.43 71.68
Iteration 2 73.94 77.85* 73.86* 58.31 55.13 84.82* 79.02* 82.46 65.85 90.45* 71.69*
Table 3. The joint test result of Labeled Syntactic Accuracy of each language and the average performance value 
on test set. (* denotes the best score for the system) 
 
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 83.83 88.56 85.86 88.08 86.20* 86.23 82.09 80.98 78.82 74.32* 87.45
Iteration 1 84.34 89.02* 87.14* 87.88 86.09 86.66 82.07 83.66* 79.28* 74.06 87.59
Iteration 2 84.36 89.02* 87.01 88.10* 86.17 86.78* 82.34* 83.15 79.18 74.06 87.81*
Table 4. The sub test result of Semantic Labeled F1 of each language and the average performance value on test 
set. (* denotes the best score for the system) 
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 70.01 66.87 71.63 75.50 75.71 78.97 69.87 67.50 58.47 70.91* 64.64
Iteration 1 70.15 67.12 71.98 75.54 75.68 79.40 70.17* 68.08* 58.55* 70.69 64.32
Iteration 2 70.20 67.33* 71.99* 75.65* 75.90* 79.47* 69.98 67.98 58.33 70.70 64.65*
Table 5. The joint test result of Semantic Labeled F1 of each language and the average performance value on test 
set. (* denotes the best score for the system) 
References  
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?s 
M?rquez, Adam Meyers, Joakim Nivre, Sebastian 
Pad?, Jan ?t?p?nek, Pavel Stra??k, Mihai Surdeanu, 
Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009 
Shared Task: Syntactic and Semantic Dependencies 
in Multiple Languages. Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2009). Boulder, Colorado, USA. June 4-
5. pp. 3-22. 
Mariona Taul?, Maria Ant?nia Mart? and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora 
for Catalan and Spanish. Proceedings of the 6th In-
ternational Conference on Language Resources and 
Evaluation (LREC-2008). Marrakech, Morocco. 
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank.  Natural Language 
Engineering, 15(1):143-172. 
Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall, 
Petr Pajas, Jan ?t?p?nek, Ji?? Havelka, Marie Miku-
lov? and Zden?k ?abokrtsk?. 2006. The Prague De-
pendency Treebank 2.0. CD-ROM. Linguistic Data 
Consortium, Philadelphia, Pennsylvania, USA. ISBN 
1-58563-370-4. LDC Cat. No. LDC2006T01. URL: 
http://ldc.upenn.edu.  
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings of 
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).  
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea 
Kowalski, Sebastian Pad? and Manfred Pinkal. 2006. 
The SALSA Corpus: a German Corpus Resource for 
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and 
Evaluation (LREC-2006). Genoa, Italy. 
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 
2002. Construction of a Japanese Relevance-tagged 
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. pp. 2008-2013. 
McDonald, Ryan. 2006. Discriminative learning and 
Spanning Tree Algorithms for Dependency    parsing. 
Ph.D. thesis, University of Pennyslvania. 
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus- 
sian prior for smoothing maximum entropy models. 
Technical. Report CMU-CS-99-108. 
24
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 151?156,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Hedge Classification with Syntactic Dependency Features based on an 
Ensemble Classifier 
Yi Zheng, Qifeng Dai, Qiming Luo, Enhong Chen 
Department of Computer Science,  
University of Science and Technology of China, Hefei, China. 
{xiaoe, dqf2008}@mail.ustc.edu.cn 
{luoq, cheneh}@ustc.edu.cn 
 
Abstract 
We present our CoNLL-2010 Shared Task 
system in the paper. The system operates in 
three steps: sequence labeling, syntactic de-
pendency parsing, and classification. We have 
participated in the Shared Task 1. Our experi-
mental results measured by the in-domain and 
cross-domain F-scores on the biological do-
main are 81.11% and 67.99%, and on the 
Wikipedia domain 55.48% and 55.41%. 
1 Introduction 
The goals of the Shared Task (Farkas et al, 2010) 
are: (1) learning to detect sentences containing 
uncertainty and (2) learning to resolve the in-
sentence scope of hedge cues. We have partici-
pated in the in-domain and cross-domain chal-
lenges of Task 1. Specifically, the aim of Task 1 
is to identify sentences in texts that contain unre-
liable or uncertain information, and it is formu-
lated as a binary classification problem. 
Similar to Morante et al (2009), we use the 
BIO-cue labels for all tokens in a sentence to 
predict whether a token is the first one of a hedge 
cue (B-cue), inside a hedge cue (I-cue), or out-
side of a hedge cue (O-cue). Thus we formulate 
the problem at the token level, and our task is to 
label tokens in every sentence with BIO-cue. Fi-
nally, sentences that contain at least one B-cue or 
I-cue are considered as uncertain.  
Our system operates in three steps: sequence 
labeling, syntactic dependency parsing, and clas-
sification. Sequence labeling is a preprocessing 
step for splitting sentence into tokens and obtain-
ing features of tokens. Then a syntactic depend-
ency parser is applied to obtain the dependency 
information of tokens. Finally, we employ an 
ensemble classifier based on combining CRF 
(conditional random field) and MaxEnt (maxi-
mum entropy) classifiers to label each token with 
the BIO-cue. 
Our experiments are conducted on two train-
ing data sets: one is the abstracts and full articles 
from BioScope (biomedical domain) corpus 
(Vincze et al, 2008)1, the other one is paragraphs 
from Wikipedia possibly containing weasel in-
formation. Both training data sets have been an-
notated manually for hedge/weasel cues. The 
annotation of weasel/hedge cues is carried out at 
the phrase level. Sentences containing at least 
one hedge/weasel cue are considered as uncertain, 
while sentences with no hedge/weasel cues are 
considered as factual. The results show that em-
ploying the ensemble classifier outperforms the 
single classifier system on the Wikipedia data set, 
and using the syntactic dependency information 
in the feature set outperform the system without 
syntactic dependency information on the biologi-
cal data set (in-domain). 
In related work, Szarvas (2008) extended the 
methodology of Medlock and Briscoe (2007), 
and presented a hedge detection method in bio-
medical texts with a weakly supervised selection 
of keywords. Ganter and Strube (2009) proposed 
an approach for automatic detection of sentences 
containing linguistic hedges using Wikipedia 
weasel tags and syntactic patterns. 
The remainder of this paper is organized as 
follows. Section 2 presents the technical details 
of our system. Section 3 presents experimental 
results and performance analysis. Section 4 pre-
sents our discussion of the experiments. Section 
5 concludes the paper and proposes future work. 
2 System Description 
This section describes the implementation of our 
system. 
2.1 Information Flow of Our System 
Common classification systems consist of two 
steps: feature set construction and classification. 
The feature set construction process of our sys-
                                                 
1
 http://www.inf.u-szeged.hu/rgai/bioscope 
151
tem consists of sequence labeling and syntactic 
dependency parsing. Figure 1 shows the main 
information flow of our system. 
 
Figure 1: The main information flow of our sys-
tem 
2.2 Sequence labeling 
The sequence labeling step consists of the fol-
lowing consecutive stages: (1) tokenizing, (2) 
chunking, (3) POS-tagging, (4) lemmatizing. 
Firstly, the PTBTokenizer2 is employed to split 
sentence into tokens. Then, tokens are labeled 
with BIO-tags by the OpenNLP 3  chunker. Fi-
nally, Stanford Parser4 is used to obtain the POS 
and lemma of tokens. 
2.3 Syntactic Dependency Parsing 
In the syntactic dependency parsing stage, we 
use the Stanford Parser again to obtain depend-
ency information of tokens. Based on the Stan-
ford typed dependencies manual (Marneffe and 
Manning 2008), we have decided to obtain the 
tree dependencies structure. During the process 
of parsing, we found that the parser may fail due 
                                                 
2
 a tokenizer from Stanford Parser. 
3
 http://www.opennlp.org/ 
4
 http://nlp.stanford.edu/software/lex-parser.shtml 
to either empty sentences or very long sentences. 
To deal with very long sentences, we decided to 
allocate more memory. To deal with empty sen-
tences, we decided to simply label them as cer-
tain ones because there are only a few empty 
sentences in the training and test data sets and we 
could ignore their influence. 
2.4 Features 
After sequence labeling and syntactic depend-
ency parsing, we obtain candidate features. In 
our system, all the features belong to the follow-
ing five categories: (1) token features, (2) de-
pendency features, (3) neighbor features, (4) data 
features, (5) bigram and trigram features. 
Token features of the current token are listed 
below: 
? token: the current token. 
? index: index of the current token in the sen-
tence 
? pos: POS of the current token. 
? lemma: lemma of the current token. 
? chunk: BIO-chunk tags of the current token. 
Dependency features of the current token are 
listed below: 
? parent_index: the index of the parent token 
of the current token. 
? parent_token: the parent token of the current 
token. 
? parent_lemma: the lemma of the parent token 
of the current token. 
? parent_pos: the POS of the parent token of 
the current token. 
? parent_relation: the dependency relation of 
the current token and its parent token. 
Neighbor features of the current token include 
token, lemma, pos, chunk tag of three tokens to 
the right and three to the left. 
Data features of current token are listed below: 
? type: indicating documentPart 5  type of the 
sentence which contains the current token, 
such as Text, SectionTitle and so on.  
? domain: distinguishing the Wikipedia and 
biological domain. 
? abstract_article: indicating document type of 
the sentence which contains the current token, 
abstract or article. 
                                                 
5
 documentPart, SectionTitle, Text and so on are tags 
in the training and test data sets. 
CRF  MaxEnt 
Start 
Merging 
End 
Tokenizing 
Chunking 
POS-tagging 
Lemmatizing 
Syntactic 
Dependency 
Parsing 
152
We empirically selected some bigram features 
and trigram features as listed below: 
? left_token_2+left_token_1 
? left_token_1+token 
? token+right_token_1 
? right_token_1+right_token_2 
? left_token_2+left_token_1+token 
? left_token_1+token+right_token_1 
? token+right_token_1+right_token_2 
These are the complete set of features for our 
system. If the value of a feature is empty, we set 
it to a default value. In the ensemble classifier, 
we have selected different features for each indi-
vidual classifier. Details of this are described in 
the next subsection. 
2.5 Classification 
In our system, we have combined CRF++6 and 
OpenNLP MaxEnt7 classifiers into an ensemble 
classifier. The set of features for each classifier 
are shown in the column named ?system? of Ta-
ble 6. And the two classifiers are used in training 
and prediction separately, based on their individ-
ual set of features. Then we merge the results in 
this way: for each token, if the two predictions 
for it are both O-cue, then we label the token 
with an O-cue; otherwise, we label the token 
with a B-cue (one of the predictions is B-cue) or 
an I-cue (no B-cue in the predictions). The moti-
vation of the ensemble classifier approach is 
based on the observation of our internal experi-
ments using 10-fold cross validation, which we 
describe in Section 3. In addition, the parameters 
of OpenNLP MaxEnt classifier are all set to de-
fault values (number of iterations is 100, cutoff is 
0 and without smoothing). For CRF++, we only 
set the option ?-f? as 3 and the option ?-c? as 1.5, 
and the others are set to default values. 
3 Experimental Results 
We have participated in four subtasks, biological 
in-domain challenge (Bio-in-domain), biological 
cross-domain challenge (Bio-cross-domain), 
Wikipedia in-domain challenge (Wiki-in-
domain) and Wikipedia cross-domain challenge 
(Wiki-cross-domain). In all the experiments, TP, 
FP, FN and F-Score for the uncertainty class are 
used as the performance measures. We have 
                                                 
6
 http://crfpp.sourceforge.net/ 
7
 http://maxent.sourceforge.net/ 
tested our system with the test data set and ob-
tained official results as shown in Table 1. In 
addition, we have performed several internal ex-
periments on the training data set and several 
experiments on the test data set, which we de-
scribe in the next two subsections. The feature 
sets used for each subtask in our system are 
shown in Table 6, where each column denotes a 
feature set named after the title of the column 
(?System?, ?dep?, ?).  Actually, for different 
subtasks, we make use of the same feature set 
named ?system?. 
 
SubTask TP FP FN F-Score 
Bio-in-domain 717 261 73 81.11 
Bio-cross-domain 566 309 224 67.99 
Wiki-in-domain 974 303 1260 55.48 
Wiki-cross-domain 991 352 1243 55.41 
 
Table 1: Official results of our system. 
3.1 Internal Experiments 
Initially we only used a single classifier instead 
of an ensemble classifier. We performed 10-fold 
cross validation experiments on the training data 
set at the sentence level with different feature 
sets. The results of these experiments are shown 
in Table 2 and Table 3. 
In internal experiments, we mainly focus on 
the results of different models and different fea-
ture sets. In Table 2 and Table 3, CRF and ME 
(MaxEnt) indicate the two classifiers; ENSMB 
stands for the ensemble classifier obtained by 
combining CRF and MaxEnt classifiers; the three 
words ?dep?, ?neighbor? and ?together? indicate 
the feature sets for different experiments shown 
in Table 6, and ?together? is the union set of 
?dep? and ?neighbor?. 
The results of ME and CRF experiments (third 
column of Table 2 and Table 3) show that the 
individual classifier wrongly predicts many un-
certain sentences as certain ones. The number of 
such errors is much greater than the number of 
errors of predicting certain ones as uncertain. In 
other words, FN is greater than FP in our ex-
periments and the recall ratio is very low, espe-
cially for the Wikipedia data set. 
153
 Biological in-domain Biological cross-domain Experiment TP FP FN F-Score TP FP FN F-Score 
ME-dep 244 28 34 88.73 220 24 58 84.29 
CRF-dep 244 20 34 90.04 230 19 48 87.29 
ENSMB-dep 248 32 30 88.89 235 28 43 86.88 
ME-neighbor 229 14 49 87.91 211 12 67 84.23 
CRF-neighbor 244 16 34 90.71 228 21 50 86.53 
ENSMB-neighbor 247 22 31 90.31 241 26 37 88.44 
ME-together 234 11 44 89.48 205 12 73 82.83 
CRF-together 247 13 31 91.82 234 21 44 87.80 
ENSMB-together 253 17 25 92.36 242 26 36 88.64 
 
Table 2: Results of internal experiments on the biological training data set. 
 
Wikipedia in-domain Wikipedia cross-domain Experiment TP FP FN F-Score TP FP FN F-Score 
ME-dep 131 91 117 55.74 145 108 103 57.88 
CRF-dep 108 51 140 53.07 115 60 133 54.37 
ENSMB-dep 148 103 100 59.32 153 119 95 58.85 
ME-neighbor 106 52 142 52.22 130 77 118 57.14 
CRF-neighbor 123 44 125 59.28 123 72 125 55.53 
ENSMB-neighbor 145 71 103 62.50 154 116 94 59.46 
ME-together 100 57 148 49.38 117 69 131 53.92 
CRF-together 125 54 123 58.55 127 67 121 57.47 
ENSMB-together 141 83 107 59.75 146 104 102 58.63 
 
Table 3: Results of internal experiments on the Wikipedia training data set. 
 
Biological in-domain Biological cross-domain Experiment TP FP FN F-Score TP FP FN F-Score 
System-ME 650 159 140 81.30 518 265 272 65.86 
System-CRF 700 197 90 82.99 464 97 326 68.69 
System-ENSMB 717 261 73 81.11 566 309 224 67.99 
 
Table 4: Results of additional experiment of biological test data set. 
 
Wikipedia in-domain Wikipedia cross-domain Experiment TP FP FN F-Score TP FP FN F-Score 
System-ME 794 235 1440 48.67 798 284 1436 48.13 
System-CRF 721 112 1513 47.02 747 153 1487 47.67 
System-ENSMB 974 303 1260 55.48 991 352 1243 55.41 
 
Table 5: Results of additional experiment of Wikipedia test data set. 
 
Based on this analysis, we propose an ensem-
ble classifier approach to decrease FN in order to 
improve the recall ratio. The results of the en-
semble classifier show that: along with the de-
creasing of FN, FP and TP are both increasing. 
Although the recall ratio increases, the precision 
ratio decreases at the same time. Therefore, the 
ensemble classifier approach is a trade-off be-
tween precision and recall. For data sets with low 
recall ratio, such as Wikipedia, the ensemble 
classifier outperforms each single classifier in 
terms of F-score, just as the ME, CRF and 
ENSMB experiments show in Table 2 and Table 
3. 
In addition, we have performed simple feature 
selection in the internal experiments. The com-
parison of ?dep?, ?neighbor? and ?together? ex-
periments shown in Table 2 demonstrates that 
the dependency and neighbor features are both 
beneficial only for the biological in-domain ex-
periment. This may be because that sentences of 
the biological data are more regular than those of 
the Wikipedia data. 
3.2 Additional experiments on test data set 
We have also performed experiments on the test 
data set, and the results are shown in Table 4 and 
Table 5. With the same set of features of our sys-
154
tem as shown in Table 6, we have performed 
three experiments: System-ME (ME denotes 
MaxEnt classifier), System-CRF (CRF denotes 
CRF classifier) and System-ENSMB (ENSMB 
denotes ensemble classifier), where ?System? 
denotes the feature set in Table 6. The meanings 
of these words are similar to internal experiments. 
As Table 4 and Table 5 show, for the Wikipe-
dia test data set, the ensemble classifier outper-
forms each single classifier in terms of F-score 
by improving the recall ratio with a larger extent 
than the extent of the decreasing of the precision 
ratio. For the biological test data set, the ensem-
ble classifier outperforms System-ME but under-
performs System-CRF. This may be due to the 
relatively high values of the precision and recall 
ratios already obtained by each single classifier. 
4 Discussion 
The features in our experiments are selected em-
pirically, and the performance of our system 
could be improved with more elaborate feature 
selection. From the experimental results, we ob-
serve that there are still many uncertain sen-
tences predicted as certain ones. This indicates 
that the ability of learning uncertain information 
with the current classifiers and feature sets needs 
to be improved. We had the plan of exploring the 
ensemble classifier by combining CRF, MaxEnt 
and SVM (Support Vector Machine), but it was 
given up due to limited time. In addition, we 
were not able to complete experiments with 
MaxEnt classifier based on bigram and trigram 
features due to limited time. Actually only two 
labels I and O are needed for Task 1. We have 
not done the experiments with only I and O la-
bels, and we plan to do it in the future. 
According to our observation, the low F-score 
on the Wikipedia data set is due to many uncer-
tain phrases. By contrast, for the biological data 
set, the uncertain information consists of mostly 
single words rather than phrases. It is difficult for 
a classifier to learn uncertain information con-
sisting of 3 words or more. As we have observed, 
these uncertain phrases follow several patterns. 
A hybrid approach based on rule-based and sta-
tistical approaches to recognize them seems to be 
a promising. 
5 Conclusion and Future Work 
Our CoNLL-2010 Shared Task system operates 
in three steps: sequence labeling, syntactic de-
pendency parsing, and classification. The results 
show that employing the ensemble classifier out-
performs each single classifier for the Wikipedia 
data set, and using the syntactic dependency in-
formation in the feature set outperform the sys-
tem without syntactic dependency information 
for the biological data set (in-domain). Our final 
system achieves promising results. Due to lim-
ited time, we have only performed simple feature 
selection empirically. In the future, we plan to 
explore more elaborate feature selection and ex-
plore ensemble classifier by combining more 
classifiers. 
 
Acknowledgments 
The work was supported by the National Natural 
Science Foundation of China (No.60775037), the 
National High Technology Research and Devel-
opment Program of China (863 Program, 
No.2009 AA01Z123), and Specialized Research 
Fund for the Doctoral Program of Higher Educa-
tion (No.20093402110017). 
References 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, 
J?nos Csirik, and Gy?rgy Szarvas. 2010. The 
CoNLL-2010 Shared Task: Learning to Detect 
Hedges and their Scope in Natural Language Text. 
In Proceedings of CoNLL-2010: Shared Task, 
pages 1?12. 
Viola Ganter, and Michael Strube. 2009. Finding 
hedges by chasing weasels: Hedge detection using 
Wikipedia tags and shallow linguistic features. In 
Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, pages 173?176. 
Marie-Catherine de Marneffe, and Christopher D. 
Manning. 2008. Stanford typed dependencies 
manual, September 2008. 
Ben Medlock, and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific 
literature. In Proc. of ACL 2007, pages 992?999. 
Roser Morante, and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In 
Proc. of the BioNLP 2009 Workshop, pages 
28?36. 
Gy?rgy Szarvas. 2008. Hedge classification in bio-
medical texts with a weakly supervised selection of 
keywords. In Proc. of ACL 2008, pages 281?289. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for un-
certainty, negation and their scopes. BMC Bioin-
formatics, 9(Suppl 11):S9. 
 
155
 Feature System Dep Neighbor Together 
token mc mc mc mc 
index m m m m 
pos mc mc mc mc 
lemma mc mc mc mc 
chunk 
 mc mc mc 
parent_index mc mc  mc 
parent_token 
 mc  mc 
parent_lemma mc mc  mc 
parent_relation mc mc  mc 
parent_pos mc mc  mc 
left_token_1 c  c c 
left_lemma_1 mc  mc mc 
left_pos_1 mc  mc mc 
left_chunk_1 
  mc mc 
left_token_2 c  c c 
left_lemma_2 c  mc mc 
left_pos_2 mc  mc mc 
left_chunk_2 
  mc mc 
left_token_3 
    
left_lemma_3 mc  m m 
left_pos_3 mc  m m 
left_chunk_3 
  m m 
right_token_1 c  c c 
right_lemma_1 mc  mc mc 
right _pos_1 mc  mc mc 
right _chunk_1 
  mc mc 
right_token_2 c  c c 
right _lemma_2 mc  mc mc 
right _pos_2 c  mc mc 
right _chunk_2 
  mc mc 
right_token_3 
    
right _lemma_3 c  m m 
right _pos_3 mc  m m 
right _chunk_3 
  m m 
type m mc mc mc 
domain m mc mc mc 
abstract_article m mc mc mc 
left_token_2+left_token_1 c  c c 
left_token_1+token c  c c 
token+right_token_1 c  c c 
right_token_1+right_token_2 c  c c 
left_token_2+left_token_1+token c  c c 
left_token_1+token+right_token_1 c  c c 
token+right_token_1+right_token_2 c  c c 
 
Table 6: Features selected for different experiments. The symbol m indicates MaxEnt classifier and c indicates 
CRF classifier.  
156
