Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1377?1381,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Statistical Machine Translation with Word Class Models
Joern Wuebker, Stephan Peitz, Felix Rietig and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University
Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
Automatically clustering words from a mono-
lingual or bilingual training corpus into
classes is a widely used technique in statisti-
cal natural language processing. We present
a very simple and easy to implement method
for using these word classes to improve trans-
lation quality. It can be applied across differ-
ent machine translation paradigms and with
arbitrary types of models. We show its ef-
ficacy on a small German?English and a
larger French?German translation task with
both standard phrase-based and hierarchical
phrase-based translation systems for a com-
mon set of models. Our results show that with
word class models, the baseline can be im-
proved by up to 1.4% BLEU and 1.0% TER
on the French?German task and 0.3% BLEU
and 1.1% TER on the German?English task.
1 Introduction
Data sparsity is one of the major problems for statis-
tical learning methods in natural language process-
ing (NLP) today. Even with the huge training data
sets available in some tasks, for many phenomena
that need to be modeled only few training instances
can be observed. This is partly due to the large vo-
cabularies of natural languages. One possiblity to
reduce the sparsity for model estimation is to re-
duce the vocabulary size. By clustering the vocab-
ulary into a fixed number of word classes, it is pos-
sible to train models that are less prone to sparsity
issues. This work investigates the performance of
standard models used in statistical machine transla-
tion when they are trained on automatically learned
word classes rather than the actual word identities.
In the popular tooklit GIZA++ (Och and Ney,
2003), word classes are an essential ingredient to
model alignment probabilities with the HMM or
IBM translation models. It contains the mkcls tool
(Och, 1999), which can automatically cluster the vo-
cabulary into classes.
Using this tool, we propose to re-parameterize the
standard models used in statistical machine transla-
tion (SMT), which are usually conditioned on word
identities rather than word classes. The idea is that
this should lead to a smoother distribution, which
is more reliable due to less sparsity. Here, we fo-
cus on the phrase-based and lexical channel models
in both directions, simple count models identifying
frequency thresholds, lexicalized reordering models
and an n-gram language model. Although our re-
sults show that it is not a good idea to replace the
original models, we argue that adding them to the
log-linear feature combination can improve transla-
tion quality. They can easily be computed for dif-
ferent translation paradigms and arbitrary models.
Training and decoding is possible without or with
only little change to the code base.
Our experiments are conducted on a medium-
sized French?German task and a small
German?English task and with both phrase-
based and hierarchical phrase-based translation
decoders. By using word class models, we can
improve our respective baselines by 1.4% BLEU and
1.0% TER on the French?German task and 0.3%
BLEU and 1.1% TER on the German?English task.
Training an additional language model for trans-
1377
lation based on word classes has been proposed in
(Wuebker et al, 2012; Mediani et al, 2012; Koehn
and Hoang, 2007). In addition to the reduced spar-
sity, an advantage of the smaller vocabulary is that
longer n-gram context can be modeled efficiently.
Mathematically, our idea is equivalent to a special
case of the Factored Translation Models proposed
by Koehn and Hoang (2007). We will go into more
detail in Section 4. Also related to our work, Cherry
(2013) proposes to parameterize a hierarchical re-
ordering model with sparse features that are condi-
tioned on word classes trained with mkcls. How-
ever, the features are trained with MIRA rather than
estimated by relative frequencies.
2 Word Class Models
2.1 Standard Models
The translation model of most phrase-based and hi-
erarchical phrase-based SMT systems is parameter-
ized by two phrasal and two lexical channel models
(Koehn et al, 2003) which are estimated as relative
frequencies. Their counts are extracted heuristically
from a word aligned bilingual training corpus.
In addition to the four channel models, our base-
line contains binary count features that fire, if the
extraction count of the corresponding phrase pair is
greater or equal to a given threshold ? . We use the
thresholds ? = {2, 3, 4}.
Our phrase-based baseline contains the hierarchi-
cal reordering model (HRM) described by Galley
and Manning (2008). Similar to (Cherry et al,
2012), we apply it in both translation directions
with separate scaling factors for the three orientation
classes, leading to a total of six feature weights.
An n-gram language model (LM) is another im-
portant feature of our translation systems. The
baselines apply 4-gram LMs trained by the SRILM
toolkit (Stolcke, 2002) with interpolated modified
Kneser-Ney smoothing (Chen and Goodman, 1998).
The smaller vocabulary size allows us to efficiently
model larger context, so in addition to the 4-gram
LM, we also train a 7-gram LM based on word
classes. In contrast to an LM of the same size trained
on word identities, the increase in computational re-
sources needed for translation is negligible for the
7-gram word class LM (wcLM).
2.2 Training
By replacing the words on both source and target
side of the training data with their respective word
classes and keeping the word alignment unchanged,
all of the above models can easily be trained con-
ditioned on word classes by using the same training
procedure as usual. We end up with two separate
model files, usually in the form of large tables, one
with word identities and one with classes. Next, we
sort both tables by their word classes. By walking
through both sorted tables simultaneously, we can
then efficiently augment the standard model file with
an additonal feature (or additional features) based on
word classes. The word class LM is directly passed
on to the decoder.
2.3 Decoding
The decoder searches for the best translation given
a set of models hm(eI1, s
K
1 , f
J
1 ) by maximizing the
log-linear feature score (Och and Ney, 2004):
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
, (1)
where fJ1 = f1 . . . fJ is the source sentence, e
I
1 =
e1 . . . eI the target sentence and sK1 = s1 . . . sK the
hidden alignment or derivation.
All the above mentioned models can easily be in-
tegrated into this framework as additional features
hm. The feature weights ?m are tuned with mini-
mum error rate training (MERT) (Och, 2003).
3 Experiments
3.1 Data
Our experiments are performed on a
French?German task. In addition to some
project-internal data, we train the system on the data
provided for the WMT 2012 shared task1. Both the
dev and the test set are composed of a mixture
of broadcast news and broadcast conversations
crawled from the web and have two references.
Table 1 shows the data statistics.
To confirm our results we also run experiments
on the German?English task of the IWSLT 2012
evaluation campaign2.
1http://www.statmt.org/wmt12/
2http://hltc.cs.ust.hk/iwslt/
1378
French German
train Sentences 1.9M
Running Words 57M 50M
dev Sentences 1900
Running Words 61K 55K
test Sentences 2037
Running Words 60K 54K
Table 1: Corpus statistics for the French?German task.
The running word counts for the German side of dev and
test are averaged over both references.
3.2 Setup
In the French?German task, our baseline is a stan-
dard phrase-based system augmented with the hier-
archical reordering model (HRM) described in Sec-
tion 2.1. The language model is a 4-gram LM
trained on all German monolingual sources provided
for WMT 2012. For the class-based models, we
run mkcls on the source and target side of the
bilingual training data to cluster the vocabulary into
100 classes each. This clustering is used to train
the models described above for word classes on the
same training data as their counterparts based on
word identity. This also holds for the wcLM, which
is a 4-gram LM trained on the same data as the base-
line LM. Further, the smaller vocabulary allows us
to build an additional wcLM with a 7-gram context
length. On this task we also run additional experi-
ments with 200 and 500 classes.
On the German?English task, we evaluate our
method for both a standard phrase-based and the hi-
erarchical phrase-based baseline. Again, the phrase-
based baseline contains the HRM model. As bilin-
gual training data we use the TED talks, which we
cluster into 100 classes on both source and target
side. The 4-gram LM is trained on the TED, Eu-
roparl and news-commentary corpora. On this data
set, we directly use a 7-gram wcLM.
In all setups, the feature weights are optimized
with MERT. Results are reported in BLEU (Pap-
ineni et al, 2002) and TER (Snover et al, 2006),
confidence level computation is based on (Koehn,
2004). Our experiments are conducted with the open
source toolkit Jane (Wuebker et al, 2012; Vilar et
al., 2010).
dev test
BLEU TER BLEU TER
[%] [%] [%] [%]
-TM +wcTM 21.2 64.2 24.7 59.5
-LM +wcLM 22.2 62.9 25.9 58.9
-HRM +wcHRM 24.6 61.9 27.5 58.1
phrase-based 24.6 61.8 27.8 57.6
+ wcTM 24.7 61.4 28.1 57.1
+ wcLM 24.9 61.2 28.4 57.1
+ wcHRM 25.4? 60.9? 28.9? 56.9?
+ wcLM7 25.5? 60.7? 29.2? 56.6?
+ wcModels200 25.5? 60.8? 29.3? 56.4?
+ wcModels500 25.2? 60.8? 29.0? 56.6?
Table 2: BLEU and TER results on the French?German
task. Results marked with ? are statistically significant
with 95% confidence, results marked with ? with 90%
confidence. -X +wcX denote the systems, where the
model X in the baseline is replaced by its word class
counterpart. The 7-gram word class LM is denoted
as wcLM7. wcModelsX denotes all word class models
trained on X classes.
3.3 Results
Results for the French?German task are given in
Table 2. In a first set of experiments we replaced one
of the standard TM, LM and HRM models by the
same model based on word classes. Unsurprisingly,
this degrades performance with different levels of
severity. The strongest degradation can be seen
when replacing the TM, while replacing the HRM
only leads to a small drop in performance. However,
when the word class models are added as additional
features to the baseline, we observe improvements.
The wcTM yields 0.3% BLEU and 0.5% TER on
test. By adding the 4-gram wcLM, we get another
0.3% BLEU and the wcHRM shows further improve-
ments of 0.5% BLEU and 0.2% TER. Extending the
context length of the wcLM to 7-grams gives an ad-
ditional boost, reaching a total gain over the baseline
of 1.4% BLEU and 1.0% TER. Using 200 classes
instead of 100 seems to perform slightly better on
test, but with 500 classes, translation quality de-
grades again.
On the German?English task, the results shown
in Table 3 are similar in TER, but less pronounced
in BLEU. Here we are able to improve over the
phrase-based baseline by 0.3% BLEU and 1.1% TER
1379
dev test
BLEU TER BLEU TER
[%] [%] [%] [%]
phrase-based 30.2 49.6 28.6 51.6
+ wcTM 30.2 49.2 28.9 51.3
+ wcLM7 30.5 48.3? 29.0 50.6?
+ wcHRM 30.8 48.3? 28.9 50.5?
hiero 29.6 50.3 27.9 52.5
+ wcTM 29.8 50.3 28.1 52.3
+ wcLM7 30.0 49.8 28.2 51.7
Table 3: BLEU and TER results on the German?English
task. Results marked with ? are statistically significant
with 95% confidence, results marked with ? with 90%
confidence.
by adding the wcTM, the 7-gram wcLM and the
wcHRM. With the hierarchical decoder we gain
0.3% BLEU and 0.8% TER by adding the wcTM and
the 7-gram wcLM.
4 Equivalence to Factored Translation
Koehn and Hoang (2007) propose to integrate differ-
ent levels of annotation (e.g. morphologial analysis)
as factors into the translation process. Here, the sur-
face form of the source word is analyzed to produce
the factors, which are then translated and finally the
surface form of the target word is generated from the
target factors. Although the translations of the fac-
tors operate on the same phrase segmentation, they
are assumed to be independent. In practice this is
done by phrase expansion, which generates a joint
phrase table as the cross product from the phrase ta-
bles of the individual factors.
In contrast, in this work each word is mapped to
a single class, which means that when we have se-
lected a translation option for the surface form, the
target side on the word class level is predetermined.
Thus, no phrase expansion or generation steps are
necessary to incorporate the word class information.
The phrase table can simply be extended with addi-
tional scores, keeping the set of phrases constant.
Although the implementation is simpler, our ap-
proach is mathematically equivalent to a special
case of the factored translation framework, which is
shown in Figure 1. The generation step from target
word e to its target class c(e) assigns all probability
Input Output
word f word e
class c(f) class c(e)
analysis
translation
translation
generation   
Figure 1: The factored translation model equivalent to
our approach. The generation step assigns all probability
mass to a single event: pgen(c(e)|e) = 1.
mass to a single event:
pgen(c|e) =
{
1, if c = c(e)
0, else
(2)
5 Conclusion
We have presented a simple and very easy to im-
plement method to make use of word clusters for
improving machine translation quality. It is appli-
cable across different paradigms and for arbitrary
types of models. Depending on the model type,
it requires little or no change to the training and
decoding software. We have shown the efficacy
of this method on two translation tasks and with
both the standard phrase-based and the hierarchi-
cal phrase-based translation paradigm. It was ap-
plied to relative frequency translation probabilities,
the n-gram language model and a hierarchical re-
ordering model. In our experiments, the baseline
is improved by 1.4% BLEU and 1.0% TER on the
French?German task and by 0.3% BLEU and 1.1%
TER on the German?English task.
In future work we plan to apply our method to a
wider range of languages. Intuitively, it should be
most effective for morphologically rich languages,
which naturally have stronger sparsity problems.
Acknowledgments
This work was partially realized as part of the
Quaero Programme, funded by OSEO, French State
agency for innovation. The research leading to these
results has also received funding from the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement no 287658.
1380
References
Stanley F. Chen and Joshuo Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. Technical Report TR-10-98, Computer
Science Group, Harvard University, Cambridge, MA,
August.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On Hierarchical Re-ordering and Permutation Parsing
for Phrase-based Decoding. In Proceedings of the 7th
Workshop on Statistical Machine Translation, WMT
?12, pages 200?209, Montral, Canada.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In The 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT 2013), pages 22?
31, Atlanta, Georgia, USA, June.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 847?855, Honolulu, Hawaii, USA, October.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 868?876, Prague, Czech Republic, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In Proceedings of the 2003
Meeting of the North American chapter of the Associa-
tion for Computational Linguistics (NAACL-03), pages
127?133, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of the Conf.
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 388?395, Barcelona, Spain, July.
Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan
Niehues, Eunah Cho, Teresa Herrmann, and Alex
Waibel. 2012. The kit translation systems for iwslt
2012. In Proceedings of the International Work-
shop for Spoken Language Translation (IWSLT 2012),
Hong Kong.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449, De-
cember.
F. J. Och. 1999. An efficient method for determining
bilingual word classes. In Proc. of the Ninth Conf.
of the Europ. Chapter of the Association of Compu-
tational Linguistics, pages 71?76, Bergen, Norway,
June.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, Penn-
sylvania, USA, July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA, Au-
gust.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf. on
Speech and Language Processing (ICSLP), volume 2,
pages 901?904, Denver, CO, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open source hierarchical transla-
tion, extended with reordering and lexicon models. In
ACL 2010 Joint Fifth Workshop on Statistical Machine
Translation and Metrics MATR, pages 262?270, Upp-
sala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Man-
sour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mumbai,
India, December.
1381
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 14?25,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Translation Modeling with Bidirectional Recurrent Neural Networks
Martin Sundermeyer
1
, Tamer Alkhouli
1
, Joern Wuebker
1
, and Hermann Ney
1,2
1
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Aachen, Germany
2
Spoken Language Processing Group
Univ. Paris-Sud, France and LIMSI/CNRS, Orsay, France
{surname}@cs.rwth-aachen.de
Abstract
This work presents two different trans-
lation models using recurrent neural net-
works. The first one is a word-based ap-
proach using word alignments. Second,
we present phrase-based translation mod-
els that are more consistent with phrase-
based decoding. Moreover, we introduce
bidirectional recurrent neural models to
the problem of machine translation, allow-
ing us to use the full source sentence in our
models, which is also of theoretical inter-
est. We demonstrate that our translation
models are capable of improving strong
baselines already including recurrent neu-
ral language models on three tasks:
IWSLT 2013 German?English, BOLT
Arabic?English and Chinese?English.
We obtain gains up to 1.6% BLEU
and 1.7% TER by rescoring 1000-best
lists.
1 Introduction
Neural network models have recently experienced
unprecedented attention in research on statistical
machine translation (SMT). Several groups have
reported strong improvements over state-of-the-art
baselines using feedforward neural network-based
language models (Schwenk et al., 2006; Vaswani
et al., 2013), as well as translation models (Le et
al., 2012; Schwenk, 2012; Devlin et al., 2014).
Different from the feedforward design, recurrent
neural networks (RNNs) have the advantage of be-
ing able to take into account an unbounded his-
tory of previous observations. In theory, this en-
ables them to model long-distance dependencies
of arbitrary length. However, while previous work
on translation modeling with recurrent neural net-
works shows its effectiveness on standard base-
lines, so far no notable gains have been presented
on top of recurrent language models (Auli et al.,
2013; Kalchbrenner and Blunsom, 2013; Hu et al.,
2014).
In this work, we present two novel approaches
to recurrent neural translation modeling: word-
based and phrase-based. The word-based ap-
proach assumes one-to-one aligned source and
target sentences. We evaluate different ways of
resolving alignment ambiguities to obtain such
alignments. The phrase-based RNN approach is
more closely tied to the underlying translation
paradigm. It models actual phrasal translation
probabilities while avoiding sparsity issues by us-
ing single words as input and output units. Fur-
thermore, in addition to the unidirectional formu-
lation, we are the first to propose a bidirectional
architecture which can take the full source sen-
tence into account for all predictions. Our ex-
periments show that these models can improve
state-of-the-art baselines containing a recurrent
language model on three tasks. For our compet-
itive IWSLT 2013 German?English system, we
observe gains of up to 1.6% BLEU and 1.7% TER.
Improvements are also demonstrated on top of our
evaluation systems for BOLT Arabic?English
and Chinese?English, which also include recur-
rent neural language models.
The rest of this paper is structured as follows. In
Section 2 we review related work and in Section 3
an overview of long short-term memory (LSTM)
neural networks, a special type of recurrent neural
networks we make use of in this work, is given.
Section 4 describes our novel translation models.
Finally, experiments are presented in Section 5
and we conclude with Section 6.
14
2 Related Work
In this Section we contrast previous work to ours,
where we design RNNs to model bilingual depen-
dencies, which are applied to rerank n-best lists
after decoding.
To the best of our knowledge, the earliest at-
tempts to apply neural networks in machine trans-
lation (MT) are presented in (Casta?no et al.,
1997; Casta?no and Casacuberta, 1997; Casta?no
and Casacuberta, 1999), where they were used for
example-based MT.
Recently, Le et al. (2012) presented translation
models using an output layer with classes and
a shortlist for rescoring using feedforward net-
works. They compare between word-factored and
tuple-factored n-gram models, obtaining their best
results using the word-factored approach, which is
less amenable to data sparsity issues. Both of our
word-based and phrase-based models eventually
work on the word level. Kalchbrenner and Blun-
som (2013) use recurrent neural networks with
full source sentence representations. The continu-
ous representations are obtained by applying a se-
quence of convolutions, and the result is fed into
the hidden layer of a recurrent language model.
Rescoring results indicate no improvements over
the state of the art. Auli et al. (2013) also in-
clude source sentence representations built either
using Latent Semantic Analysis or by concatenat-
ing word embeddings. This approach produced
no notable gain over systems using a recurrent
language model. On the other hand, our pro-
posed bidirectional models include the full source
sentence relying on recurrency, yielding improve-
ments over competitive baselines already includ-
ing a recurrent language model.
RNNs were also used with minimum translation
units (Hu et al., 2014), which are phrase pairs un-
dergoing certain constraints. At the input layer,
each of the source and target phrases are mod-
eled as a bag of words, while the output phrase
is predicted word-by-word assuming conditional
independence. The approach seeks to alleviate
data sparsity problems that would arise if phrases
were to be uniquely distinguished. Our proposed
phrase-based models maintain word order within
phrases, but the phrases are processed in a word-
pair manner, while the phrase boundaries remain
implicitly encoded in the way the words are pre-
sented to the network. Schwenk (2012) proposed
a feedforward network that predicts phrases of a
fixed maximum length, such that all phrase words
are predicted at once. The prediction is condi-
tioned on the source phrase. Since our phrase-
based model predicts one word at a time, it does
not assume any phrase length. Moreover, our
model?s predictions go beyond phrase boundaries
and cover unbounded history and future contexts.
Using neural networks during decoding re-
quires tackling the costly output normalization
step. Vaswani et al. (2013) avoid this step by
training feedforward neural language models us-
ing noise contrastive estimation, while Devlin et
al. (2014) augment the training objective function
to produce approximately normalized scores di-
rectly. The latter work makes use of translation
and joint models, and pre-computes the first hid-
den layer beforehand, resulting in large speedups.
They report major improvements over strong base-
lines. The speedups achieved by both works al-
lowed to integrate feedforward neural networks
into the decoder.
3 LSTM Recurrent Neural Networks
Our work is based on recurrent neural networks.
In related fields like e. g. language modeling, this
type of neural network has been shown to perform
considerably better than standard feedforward ar-
chitectures (Mikolov et al., 2011; Arisoy et al.,
2012; Sundermeyer et al., 2013; Liu et al., 2014).
Most commonly, recurrent neural networks are
trained with stochastic gradient descent (SGD),
where the gradient of the training criterion is com-
puted with the backpropagation through time al-
gorithm (Rumelhart et al., 1986; Werbos, 1990;
Williams and Zipser, 1995). However, the combi-
nation of RNN networks with conventional back-
propagation training leads to conceptual difficul-
ties which are known as the vanishing (or explod-
ing) gradient problem, described e. g. in (Bengio
et al., 1994). To remedy this problem, in (Hochre-
iter and Schmidhuber, 1997) it was suggested to
modify the architecture of a standard RNN in such
a way that vanishing and exploding gradients are
avoided during backpropagation. In particular, no
modification of the training algorithm is necessary.
The resulting architecture is referred to as long
short-term memory (LSTM) neural network.
Bidirectional recurrent neural networks
(BRNNs) were first proposed in (Schuster and
Paliwal, 1997) and applied to speech recognition
tasks. They have been since applied to different
15
Surfers
,
for
example
,
know
this
incredibly
.
S
u
r
f
e
r
z
u
m
B
e
i
s
p
i
e
l
k
e
n
n
e
n
d
a
s
z
u
r
G
e
n
?
u
g
e
.
(a) Original
Surfers
,
for
example
,
know
this

unaligned
incredibly
.
S
u
r
f
e
r

a
l
i
g
n
e
d
z
u
m
B
e
i
s
p
i
e
l

u
n
a
l
i
g
n
e
d
k
e
n
n
e
n
d
a
s
z
u
r
G
e
n
?
u
g
e
.
(b) One-to-one alignment
Figure 1: Example sentence from the German?English IWSLT data. The one-to-one alignment is
created by introducing 
aligned
and 
unaligned
tokens.
tasks like parsing (Henderson, 2004) and spoken
language understanding (Mesnil et al., 2013).
Bidirectional long short-term memory (BLSTM)
networks are BRNNs using LSTM hidden layers
(Graves and Schmidhuber, 2005). This work
introduces BLSTMs to the problem of machine
translation, allowing powerful models that employ
unlimited history and future information to make
predictions.
While the proposed models do not make any as-
sumptions about the type of RNN used, all of our
experiments make use of recurrent LSTM neural
networks, where we include later LSTM exten-
sions proposed in (Gers et al., 2000; Gers et al.,
2003). The cross-entropy error criterion is used
for training. Further details on LSTM neural net-
works can be found in (Graves and Schmidhuber,
2005; Sundermeyer et al., 2012).
4 Translation Modeling with RNNs
In the following we describe our word- and
phrase-based translation models in detail. We also
show how bidirectional RNNs can enable such
models to include full source information.
4.1 Resolving Alignment Ambiguities
Our word-based recurrent models are only de-
fined for one-to-one-aligned source-target sen-
tence pairs. In this work, we always evaluate the
model in the order of the target sentence. How-
ever, we experiment with several different ways
to resolve ambiguities due to unaligned or mul-
tiply aligned words. To that end, we introduce
two additional tokens, 
aligned
and 
unaligned
. Un-
dev test
BLEU TER BLEU TER
baseline 33.5 45.8 30.9 48.4
w/o  34.2 45.3 31.8 47.7
w/o 
unaligned
34.4 44.8 31.7 47.4
source identity 34.5 45.0 31.9 47.5
target identity 34.5 44.6 31.9 47.0
all  34.6 44.5 32.0 47.1
Table 1: Comparison of including different sets
of  tokens into the one-to-one alignment on the
IWSLT 2013 German?English task using the uni-
directional RNN translation model.
aligned words are either removed or aligned to an
extra 
unaligned
token on the opposite side. If an

unaligned
is introduced on the target side, its posi-
tion is determined by the aligned source word that
is closest to the unaligned source word in question,
preferring left to right. To resolve one-to-many
alignments, we use an IBM-1 translation table to
decide for one of the alignment connections to be
kept. The remaining words are also either deleted
or aligned to additionally introduced 
aligned
to-
kens on the opposite side. Fig. 1 shows an ex-
ample sentence from the IWSLT data, where all 
tokens are introduced.
In a short experiment, we evaluated 5 differ-
ent setups with our unidirectional RNN translation
model (cf. next Section): without any  tokens,
without 
unaligned
, source identity, target identity
and using all  tokens. Source identity means we
16
introduce no  tokens on source side, but all on
target side. Target identity is defined analogously.
The results can be found in Tab. 1. We use the
setup with all  tokens in all following experi-
ments, which showed the best BLEU performance.
4.2 Word-based RNN Models
Given a pair of source sequence f
I
1
= f
1
. . . f
I
and target sequence e
I
1
= e
1
. . . e
I
, where we as-
sume a direct correspondence between f
i
and e
i
,
we define the posterior translation probability by
factorizing on the target words:
p(e
I
1
|f
I
1
) =
I
?
i=1
p(e
i
|e
i?1
1
, f
I
1
) (1)
?
I
?
i=1
p(e
i
|e
i?1
1
, f
i+d
1
) (2)
?
I
?
i=1
p(e
i
|f
i+d
1
). (3)
We denote the formulation (1) as the bidirectional
joint model (BJM). This model can be simplified
by several independence assumptions. First, we
drop the dependency on the future source infor-
mation, receiving what we denote as the unidirec-
tional joint model (JM) in (2). Here, d ? N
0
is
a delay parameter, which is set to d = 0 for all
experiments, except for the comparative results re-
ported in Fig. 7. Finally, assuming conditional in-
dependence from the previous target sequence, we
receive the unidirectional translation model (TM)
in (3). Analogously, we can define a bidirectional
translation model (BTM) by keeping the depen-
dency on the full source sentence f
I
1
, but dropping
the previous target sequence e
i?1
1
:
p(e
I
1
|f
I
1
) ?
I
?
i=1
p(e
i
|f
I
1
). (4)
Fig. 2 shows the dependencies of the word-
based neural translation and joint models. The
alignment points are traversed in target order and
at each time step one target word is predicted.
The pure translation model (TM) takes only source
words as input, while the joint model (JM) takes
the preceding target words as an additional input.
A delay of d > 0 is implemented by shifting the
target sequence by d time steps and filling the first
d target positions and the last d source positions
with a dedicated 
padding
symbol. The RNN archi-
tecture for the unidirectional word-based models
j
o
i
n
t
m
o
d
e
l
all models bidirectional
Surfers
,
for
example
,
know
this

unaligned
incredibly
.
S
u
r
f
e
r

a
l
i
g
n
e
d
z
u
m
B
e
i
s
p
i
e
l

u
n
a
l
i
g
n
e
d
k
e
n
n
e
n
d
a
s
z
u
r
G
e
n
?
u
g
e
.
Figure 2: Dependencies modeled within the word-
based RNN models when predicting the target
word ?know?. Directly processed information is
depicted with solid rectangles, and information
available through recurrent connections is marked
with dashed rectangles.
is illustrated in Fig. 3, which corresponds to the
following set of equations:
y
i
= A
1
?
f
i
+A
2
e?
i?1
z
i
= ?(y
i
;A
3
, y
i?1
1
)
p
(
c(e
i
)|e
i?1
1
, f
i
1
)
= ?
c(e
i
)
(A
4
z
i
)
p
(
e
i
|c(e
i
), e
i?1
1
, f
i
1
)
= ?
e
i
(A
c(e
i
)
z
i
)
p(e
i
|e
i?1
1
, f
i
1
) = p
(
e
i
|c(e
i
), e
i?1
1
, f
i
1
)
?
p
(
c(e
i
)|e
i?1
1
, f
i
1
)
Here, by
?
f
i
and e?
i?1
we denote the one-hot en-
coded vector representations of the source and
target words f
i
and e
i?1
. The outgoing activa-
tion values of the projection layer and the LSTM
layer are y
i
and z
i
, respectively. The matrices A
j
contain the weights of the neural network layers.
By ?(? ;A
3
, y
i?1
1
) we denote the LSTM formalism
that we plug in at the third layer. As the LSTM
layer is recurrent, we explicitly include the de-
pendence on the previous layer activations y
i?1
1
.
Finally, ? is the widely-used softmax function to
obtain normalized probabilities, and c denotes a
word class mapping from any target word to its
unique word class. For the bidirectional model,
the equations can be defined analogously.
Due to the use of word classes, the output
layer consists of two parts. The class probabil-
ity p
(
c(e
i
)|e
i?1
1
, f
i
1
)
is computed first, and then
17
p(
c(e
i
)|ei?11 , f i+d1
)
p
(
e
i
|c(e
i
), ei?11 , f i+d1
)
class layer output layer
LSTM layer
projection layer
input layer
e
i?1f
i+d)
Figure 3: Architecture of a recurrent unidirec-
tional translation model. By including the dashed
parts, a joint model is obtained.
the word probability p
(
e
i
|c(e
i
), e
i?1
1
, f
i
1
)
is ob-
tained given the word class. This trick helps avoid-
ing the otherwise computationally expensive nor-
malization sum, which would be carried out over
all words in the target vocabulary. In a class-
factorized output layer where each word belongs
to a single class, the normalization is carried out
over all classes, whose number is typically much
less than the vocabulary size. The other normal-
ization sum needed to produce the word probabil-
ity is limited to the words belonging to the same
class (Goodman, 2001; Morin and Bengio, 2005).
4.3 Phrase-based RNN Models
One of the conceptual disadvantages of word-
based modeling as introduced in the previous sec-
tion is that there is a mismatch between train-
ing and testing conditions: During neural network
training, the vocabulary has to be extended by ad-
ditional  tokens, and a one-to-one alignment is
used which does not reflect the situation in decod-
ing. In phrase-based machine translation, more
complex alignments in terms of multiple words
on both the source and the target sides are used,
which allow the decoder to make use of richer
short-distance dependencies and are crucial for the
performance of the resulting system.
From this perspective, it seems interesting to
standardize the alignments used in decoding, and
in training the neural network. However, it is dif-
ficult to use the phrases themselves as the vocab-
ulary of the RNN. Usually, the huge number of
potential phrases in comparison to the relatively
small amount of training data makes the learn-
ing of continuous phrase representations difficult
Surfer
Surfers
zum Beispiel
, for example ,
kennen
know
zur Gen?ge
incredibly
.
.
das
this
Figure 4: Example phrase alignment for a sen-
tence from the IWSLT training data.
due to data sparsity. This is confirmed by results
presented in (Le et al., 2012), which show that a
word-factored translation model outperforms the
phrase-factored version. Therefore, in this work
we continue relying on source and target word vo-
cabularies for building our phrase representations.
However, we no longer use a direct correspon-
dence between a source and a target word, as en-
forced in our word-based models.
Fig. 4 shows an example phrase alignment,
where a sequence of source words
?
f
i
is directly
mapped to a sequence of target words e?
i
for 1 ?
i ?
?
I . By
?
I , we denote the number of phrases in
the alignment. We decompose the target sentence
posterior probability in the following way:
p(e
I
1
|f
J
1
) =
?
I
?
i=1
p(e?
i
|e?
i?1
1
,
?
f
?
I
1
) (5)
?
?
I
?
i=1
p(e?
i
|e?
i?1
1
,
?
f
i
1
) (6)
where the joint model in Eq. 5 would correspond
to a bidirectional RNN, and Eq. 6 only requires a
unidirectional RNN. By leaving out the condition-
ing on the target side, we obtain a phrase-based
translation model.
As there is no one-to-one correspondence be-
tween the words within a phrase, the basic idea of
our phrase-based approach is to let the neural net-
work learn the dependencies itself, and present the
full source side of the phrase to the network be-
fore letting it predict target side words. Then the
probability for the target side of a phrase can be
computed, in case of Eq. 6, by:
p(e?
i
|e?
i?1
1
,
?
f
?
I
1
) =
|e?
i
|
?
j=1
p
(
(e?
i
)
j
|(e?
i
)
j?1
1
, e?
i?1
1
,
?
f
i
1
)
,
and analogously for the case of Eq. 5. Here, (e?
i
)
j
denotes the j-th word of the i-th aligned target
phrase.
We feed the source side of a phrase into the neu-
ral network one word at a time. Only when the
18
output layer
LSTM layer
projection layer
input layer
Surfers ,, for example know this incredibly .??
?s? Surfers
,, for example
know
this
incredibly
.Surfer
zum
B
eispiel
kennen
das zur Genu?ge
?? ???
Figure 5: A recurrent phrase-based joint translation model, unfolded over time. Source words are printed
in normal face, while target words are printed in bold face. Dashed lines indicate phrases from the
example sentence. For brevity, we omit the precise handling of sentence begin and end tokens.
presentation of the source side is finished we start
estimating probabilities for the target side. There-
fore, we do not let the neural network learn a target
distribution until the very last source word is con-
sidered. In this way, we break up the conventional
RNN training scheme where an input sample is di-
rectly followed by its corresponding teacher sig-
nal. Similarly, the presentation of the source side
of the next phrase only starts after the prediction
of the current target side is completed.
To this end, we introduce a no-operation token,
denoted by ?, which is not part of the vocabulary
(which means it cannot be input to or predicted by
the RNN). When the ? token occurs as input, it in-
dicates that no input needs to be processed by the
RNN. When the ? token occurs as a teacher signal
for the RNN, the output layer distribution is ig-
nored, and does not even have to be computed. In
both cases, all the other layers are still processed
during forward and backward passes such that the
RNN state can be advanced even without addi-
tional input or output.
Fig. 5 depicts the evaluation of a phrase-based
joint model for the example alignment from Fig. 4.
For a source phrase
?
f
i
, we include (|e?
i
|?1) many ?
symbols at the end of the phrase. Conversely, for
a target phrase e?
i
, we include (|
?
f
i
| ? 1) many ?
symbols at the beginning of the phrase.
E. g., in the figure, the second dashed rectan-
gle from the left depicts the training of the English
phrase ?, for example ,? and its German transla-
tion ?zum Beispiel?. At the input layer, we feed in
the source words one at a time, while we present
? tokens at the target side input layer and the out-
put layer (with the exception of the very first time
step, where we still have the last target word from
the previous phrase as input instead of ?). With
the last word of the source phrase ?Beispiel? being
presented to the network, the full source phrase is
stored in the hidden layer, and the neural network
is then trained to predict the target phrase words
at the output layer. Subsequently, the source input
is ?, and the target input is the most recent target
side history word.
To obtain a phrase-aligned training sequence for
the phrase-based RNN models, we force-align the
training data with the application of leave-one-out
as described in (Wuebker et al., 2010).
4.4 Bidirectional RNN Architecture
While the unidirectional RNNs include an un-
bounded sentence history, they are still limited in
the number of future source words they include.
Bidirectional models provide a flexible means to
also include an unbounded future context, which,
unlike the delayed unidirectional models, require
no tuning to determine the amount of delay.
Fig. 6 illustrates the bidirectional model archi-
tecture, which is an extension of the unidirectional
model of Fig. 3. First, an additional recurrent
hidden layer is added in parallel to the existing
one. This layer will be referred to as the back-
ward layer, since it processes information in back-
ward time direction. This hidden layer receives
source word input only, while target words in the
case of a joint model are fed to the forward layer
as in the unidirectional case. Due to the backward
recurrency, the backward layer will make the in-
formation f
I
i
available when predicting the target
word e
i
, while the forward layer takes care of the
source history f
i
1
. Jointly, the forward and back-
ward branches include the full source sentence f
I
1
,
as indicated in Fig. 2. Fig. 6 shows the ?deep?
variant of the bidirectional model, where the for-
19
p(
c(e
i
)|ei?11 , fI1
)
p
(
e
i
|c(e
i
), ei?11 , fI1
)
class layer output layer
2nd LSTM layer
1st LSTM layer
projection layer
input layer
e
i?1fi
(+)
(+)
(?)
Figure 6: Architecture of a recurrent bidirectional
translation model. By (+) and (?), we indicate
a processing in forward and backward time direc-
tions, respectively. The inclusion of the dashed
parts leads to a bidirectional joint model. One
source projection matrix is used for the forward
and backward branches.
ward and backward layers converge into a hidden
layer. A shallow variant can be obtained if the
parallel layers converge into the output layer di-
rectly
1
.
Due to the full dependence on the source se-
quence, evaluating bidirectional networks requires
computing the forward pass of the forward and
backward layers for the full sequence, before be-
ing able to evaluate the next layers. In the back-
ward pass of backpropagation, the forward and
backward recurrent layers are processed in de-
creasing and increasing time order, respectively.
5 Experiments
5.1 Setup
All translation experiments are performed with the
Jane toolkit (Vilar et al., 2010; Wuebker et al.,
2012). The largest part of our experiments is car-
ried out on the IWSLT 2013 German?English
shared translation task.
2
The baseline system is
trained on all available bilingual data, 4.3M sen-
tence pairs in total, and uses a 4-gram LM with
modified Kneser-Ney smoothing (Kneser and Ney,
1995; Chen and Goodman, 1998), trained with
the SRILM toolkit (Stolcke, 2002). As additional
1
In our implementation, the forward and backward layers
converge into an intermediate identity layer, and the aggre-
gate is weighted and fed to the next layer.
2
http://www.iwslt2013.org
data sources for the LM we selected parts of the
Shuffled News and LDC English Gigaword cor-
pora based on cross-entropy difference (Moore
and Lewis, 2010), resulting in a total of 1.7 bil-
lion running words for LM training. The state-of-
the-art baseline is a standard phrase-based SMT
system (Koehn et al., 2003) tuned with MERT
(Och, 2003). It contains a hierarchical reorder-
ing model (Galley and Manning, 2008) and a 7-
gram word cluster language model (Wuebker et
al., 2013). Here, we also compare against a feed-
forward joint model as described by Devlin et al.
(2014), with a source window of 11 words and a
target history of three words, which we denote as
BBN-JM. Instead of POS tags, we predict word
classes trained with mkcls. We use a shortlist
of size 16K and 1000 classes for the remaining
words. All neural networks are trained on the TED
portion of the data (138K segments) and are ap-
plied in a rescoring step on 1000-best lists.
To confirm our results, we run additional
experiments on the Arabic?English and
Chinese?English tasks of the DARPA BOLT
project. In both cases, the neural network models
are added on top of our most competitive eval-
uation system. On Chinese?English, we use a
hierarchical phrase-based system trained on 3.7M
segments with 22 dense features, including an ad-
vanced orientation model (Huck et al., 2013). For
the neural network training, we selected a subset
of 9M running words. The Arabic?English
system is a standard phrase-based decoder trained
on 6.6M segments, using 17 dense features. The
neural network training was performed using a
selection amounting to 15.5M running words.
For both tasks we apply the neural networks by
rescoring 1000-best lists and evaluate results on
two data sets from the ?discussion forum? domain,
test1 and test2. The sizes of the data sets
for the Arabic?English system are: 1219 (dev),
1510 (test1), and 1137 (test2) segments, and
for the Chinese?English system are: 5074 (dev),
1844 (test1), and 1124 (test2) segments. All
results are measured in case-insensitive BLEU [%]
(Papineni et al., 2002) and TER [%] (Snover et al.,
2006) on a single reference.
5.2 Results
Our results on the IWSLT German?English task
are summarized in Tab. 2. At this point, we
do not include a recurrent neural network lan-
20
dev test
BLEU TER BLEU TER
baseline 33.5 45.8 30.9 48.4
TM 34.6 44.5 32.0 47.1
JM 34.7 44.7 31.8 47.4
BTM 34.7 44.9 32.3 47.0
BTM (deep) 34.8 44.3 32.5 46.7
BJM 34.7 44.5 32.1 47.0
BJM (deep) 34.9 44.1 32.2 46.6
PTM 34.3 44.9 32.1 47.5
PJM 34.3 45.0 32.0 47.5
PJM (10-best) 34.4 44.8 32.0 47.3
PJM (deep) 34.6 44.7 32.0 47.6
PBJM (deep) 34.8 44.9 31.9 47.5
BBN-JM 34.4 44.9 31.9 47.6
Table 2: Results for the IWSLT 2013
German?English task with different RNN
models. T: translation, J: joint, B: bidirectional,
P: phrase-based.
guage model yet. Here, the delay parameter d
from Equations 2 and 3 is set to zero. We ob-
serve that for all recurrent translation models, we
achieve substantial improvements over the base-
line on the test data, ranging from 0.9 BLEU
up to 1.6 BLEU. These results are also consistent
with the improvements in terms of TER, where we
achieve reductions by 0.8 TER up to 1.8 TER.
These numbers can be directly compared to the
case of feedforward neural network-based transla-
tion modeling as proposed in (Devlin et al., 2014)
which we include in the very last row of the table.
Nearly all of our recurrent models outperform the
feedforward approach, where the RNN model per-
forming best on the dev data is better on test
by 0.3 BLEU and 1.0 TER.
Interestingly, for the recurrent word-based mod-
els, on the test data it can be seen that TMs per-
form better than JMs, even though TMs do not
take advantage of the target side history words.
However, exploiting this extra information does
not always need to result in a better model, as the
target side words are only derived from the given
source side, which is available to both TMs and
JMs. On the other hand, including future source
words in a bidirectional model clearly improves
the performance further. By adding another LSTM
 
31
 
31
.5
 
32
 
32
.5
 
33
0
1
2
3
4
BLEU[%]
De
layRN
N-
TM
RN
N-
BT
M
Figure 7: BLEU scores on the IWSLT test set
with different delays for the unidirectional RNN-
TM and the bidirectional RNN-BTM.
layer that combines forward and backward time
directions (indicated as ?deep? in the table), we ob-
tain our overall best model.
In Fig. 7 we compare the word-based bidirec-
tional TM with a unidirectional TM that uses dif-
ferent time delays d = 0, . . . , 4. For a delay d =
2, the same performance is obtained as with the
bidirectional model, but this comes at the price of
tuning the delay parameter.
In comparison to the unidirectional word-based
models, phrase-based models perform similarly.
In the tables, we include those phrase-based vari-
ants which perform best on the dev data, where
phrase-based JMs always are at least as good or
better than the corresponding TMs in terms of
BLEU. Therefore, we mainly report JM results
for the phrase-based networks. A phrase-based
model can also be trained on multiple variants for
the phrase alignment. For our experiments, we
tested 10-best alignments against the single best
alignment, which resulted in a small improvement
of 0.2 TER on both dev and test. We did not ob-
serve consistent gains by using an additional hid-
den layer or bidirectional models. To some ex-
tent, future information is already considered in
unidirectional phrase-based models by feeding the
complete source side before predicting the target
side.
Tab. 3 shows different model combination re-
sults for the IWSLT task, where a recurrent lan-
guage model is included in the baseline. Adding
a deep bidirectional TM or JM to the recur-
rent language model improves the RNN-LM base-
line by 1.2 BLEU or 1.1 BLEU, respectively. A
phrase-based model substantially improves over
21
dev eval11 test
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
baseline (w/ RNN-LM) 34.3 44.8 36.4 42.9 31.5 47.8
BTM (deep) 34.9 43.7 37.6 41.5 32.7 46.1
BJM (deep) 35.0 44.4 37.4 41.9 32.6 46.5
PBJM (deep) 34.8 44.6 36.9 42.6 32.3 47.2
4 RNN models 35.2 43.4 38.0 41.2 32.7 46.0
Table 3: Results for the IWSLT 2013 German?English task with different RNN models. All results
include a recurrent language model. T: translation, J: joint, B: bidirectional, P: phrase-based.
the RNN-LM baseline, but performs not as good
as its word-based counterparts. By adding four
different translation models, including models in
reverse word order and reverse translation direc-
tion, we are able to improve these numbers even
further. However, especially on the test data, the
gains from model combination saturate quickly.
Apart from the IWSLT track, we also ana-
lyze the performance of our translation models on
the BOLT Chinese?English and Arabic?English
translation tasks. Due to the large amount of train-
ing data, we concentrate on models of high perfor-
mance in the IWSLT experiments. The results can
be found in Tab. 4 and 5. In both cases, we see
consistent improvements over the recurrent neural
network language model baseline, improving the
Arabic?English system by 0.6 BLEU and 0.5 TER
on test1. This can be compared to the rescoring
results for the same task reported by (Devlin et al.,
2014), where they achieved 0.3 BLEU, despite the
fact that they used multiple references for scoring,
whereas in our experiments we rely on a single
reference only. The models are also able to im-
prove the Chinese?English system by 0.5 BLEU
and 0.5 TER on test2.
5.3 Analysis
To investigate whether bidirectional models ben-
efit from future source information, we compare
the single-best output of a system reranked with a
unidirectional model to the output reranked with
a bidirectional model. We choose the models
to be translation models in both cases, as they
predict target words independent of previous
predictions, given the source information (cf. Eqs.
(3, 4)). This makes it easier to detect the effect
of including future source information or the lack
thereof. The examples are taken from the IWSLT
test1 test2
BLEU TER BLEU TER
baseline 25.2 57.4 26.8 57.3
BTM (deep) 25.6 56.6 26.8 56.7
BJM (deep) 25.9 56.9 27.4 56.7
RNN-LM 25.6 57.1 27.5 56.7
+ BTM (deep) 25.9 56.7 27.3 56.8
+ BJM (deep) 26.2 56.6 27.9 56.5
Table 4: Results for the BOLT Arabic?English
task with different RNN models. The ?+? sign in
the last two rows indicates that either of the corre-
sponding deep models (BTM and BJM) are added
to the baseline including the recurrent language
model (i.e. they are not applied at the same time).
T: translation, J: joint, B: bidirectional.
task, where we include the one-to-one source
information, reordered according to the target
side.
source: nicht so wie ich
reference: not like me
Hypothesis 1:
1-to-1 source: so ich  nicht wie
1-to-1 target: so I do n?t like
Hypothesis 2:
1-to-1 source: nicht so wie ich
1-to-1 target: not  like me
In this example, the German phrase ?so wie?
translates to ?like? in English. The bidirectional
model prefers hypothesis 2, making use of the
future word ?wie? when translating the German
word ?so? to , because it has future insight that
this move will pay off later when translating
22
BLEU TER BLEU TER
baseline 18.3 63.6 16.7 63.0
BTM (deep) 18.7 63.3 17.1 62.6
BJM (deep) 18.5 63.1 17.2 62.3
RNN-LM 18.8 63.3 17.2 62.8
+ BTM (deep) 18.9 63.1 17.7 62.3
+ BJM (deep) 18.8 63.3 17.5 62.5
Table 5: Results for the BOLT Chinese?English
task with different RNN models. The ?+? sign in
the last two rows indicates that either of the corre-
sponding deep models (BTM and BJM) are added
to the baseline including the recurrent language
model (i.e. they are not applied at the same time).
T: translation, B: bidirectional.
the rest of the sentence. This information is
not available to the unidirectional model, which
prefers hypothesis 1 instead.
source: das taten wir dann auch und verschafften uns
so eine Zeit lang einen Wettbewerbs Vorteil .
reference: and we actually did that and it gave us a
competitive advantage for a while .
Hypothesis 1:
1-to-1 source: das    wir dann auch taten und
verschafften uns so eine Zeit lang einen Wettbewerbs
Vorteil .
1-to-1 target: that ?s just what we   did and gave us 
a time , a competitive advantage .
Hypothesis 2:
1-to-1 source: das    wir dann auch taten und
verschafften uns so einen Wettbewerbs Vorteil  eine
Zeit lang .
1-to-1 target: that ?s just what we   did and gave us 
a competitive advantage for a  while .
Here, the German phrase ?eine Zeit lang? trans-
lates to ?for a while? in English. Bidirectional
scoring favors hypothesis 2, while unidirectional
scoring favors hypothesis 1. It seems that the uni-
directional model translates ?Zeit? to ?time? as the
object of the verb ?give? in hypothesis 1, being
blind to the remaining part ?lang? of the phrase
which changes the meaning. The bidirectional
model, to its advantage, has the full source infor-
mation, allowing it to make the correct prediction.
6 Conclusion
We developed word- and phrase-based RNN trans-
lation models. The former is simple and performs
well in practice, while the latter is more consistent
with the phrase-based paradigm. The approach in-
herently evades data sparsity problems as it works
on words in its lowest level of processing. Our
experiments show the models are able to achieve
notable improvements over baselines containing a
recurrent LM.
In addition, and for the first time in statistical
machine translation, we proposed a bidirectional
neural architecture that allows modeling past and
future dependencies of any length. Besides its
good performance in practice, the bidirectional ar-
chitecture is of theoretical interest as it allows the
exact modeling of posterior probabilities.
Acknowledgments
This material is partially based upon work sup-
ported by the DARPA BOLT project under Con-
tract No. HR0011- 12-C-0015. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
The research leading to these results has also re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreements n
o
287658 and n
o
287755.
Experiments were performed with computing re-
sources granted by JARA-HPC from RWTH
Aachen University under project ?jara0085?. We
would like to thank Jan-Thorsten Peter for provid-
ing the BBN-JM system.
References
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep neural network
language models. In Proceedings of the NAACL-
HLT 2012 Workshop: Will We Ever Really Replace
the N-gram Model? On the Future of Language
Modeling for HLT, pages 20?28. Association for
Computational Linguistics.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 1044?1054, Seattle, USA, Octo-
ber.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
23
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157?166.
Maria Asunci?on Casta?no and Francisco Casacuberta.
1997. A connectionist approach to machine trans-
lation. In 5th International Conference on Speech
Communication and Technology (EUROSPEECH-
97), Rhodes, Greece.
Maria Asunci?on Casta?no and Francisco Casacuberta.
1999. Text-to-text machine translation using the
RECONTRA connectionist model. In Lecture Notes
in Computer Science (IWANN 99), volume 1607,
pages 683?692, Alicante, Spain.
Maria Asunci?on Casta?no, Francisco Casacuberta, and
Enrique Vidal. 1997. Machine translation using
neural networks and finite-state models. In 7th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation. TMI?97,
pages 160?167, Santa Fe, USA.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, August.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics, page to appear, Baltimore, MD, USA, June.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 848?856, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Felix A. Gers, J?urgen Schmidhuber, and Fred Cum-
mins. 2000. Learning to forget: Contin-
ual prediction with LSTM. Neural computation,
12(10):2451?2471.
Felix A. Gers, Nicol N. Schraudolph, and J?urgen
Schmidhuber. 2003. Learning precise timing with
lstm recurrent networks. The Journal of Machine
Learning Research, 3:115?143.
Joshua Goodman. 2001. Classes for fast maximum
entropy training. In Acoustics, Speech, and Signal
Processing, 2001. Proceedings.(ICASSP?01). 2001
IEEE International Conference on, volume 1, pages
561?564. IEEE.
Alex Graves and J?urgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5):602?610.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 95. Association for Com-
putational Linguistics.
Sepp Hochreiter and J?urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735?1780.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20?29,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A phrase orientation model for hi-
erarchical machine translation. In ACL 2013 Eighth
Workshop on Statistical Machine Translation, pages
452?463, Sofia, Bulgaria, August.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700?1709, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processingw, volume 1,
pages 181?184, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Meeting of the North Ameri-
can chapter of the Association for Computational
Linguistics (NAACL-03), pages 127?133, Edmon-
ton, Alberta.
Hai Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
39?48, Montreal, Canada, June.
Xunying Liu, Yongqiang Wang, Xie Chen, Mark J. F.
Gales, and Phil C. Woodland. 2014. Efficient lattice
rescoring using recurrent neural network language
models. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on,
pages 4941?4945. IEEE.
Gr?egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In Interspeech, pages
3771?3775.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
24
(ICASSP), 2011 IEEE International Conference on,
pages 5528?5531. IEEE.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In ACL
(Short Papers), pages 220?224, Uppsala, Sweden,
July.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246?252.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In: J. L. McClelland, D. E.
Rumelhart, and The PDP Research Group: ?Paral-
lel Distributed Processing, Volume 1: Foundations?.
The MIT Press.
Mike Schuster and Kuldip K. Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673?2681.
Holger Schwenk, Daniel D?echelotte, and Jean-Luc
Gauvain. 2006. Continuous Space Language Mod-
els for Statistical Machine Translation. In Proceed-
ings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 723?730, Sydney, Australia,
July.
Holger Schwenk. 2012. Continuous Space Translation
Models for Phrase-Based Statistical Machine Trans-
lation. In 25th International Conference on Compu-
tational Linguistics (COLING), pages 1071?1080,
Mumbai, India, December.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, September.
Martin Sundermeyer, Ralf Schl?uter, and Hermann Ney.
2012. LSTM neural networks for language model-
ing. In Interspeech, Portland, OR, USA, September.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl?uter, and Hermann Ney.
2013. Comparison of feedforward and recurrent
neural network language models. In IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing, pages 8430?8434, Vancouver, Canada,
May.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387?1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Paul J. Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10):1550?1560.
Ronald J. Williams and David Zipser. 1995. Gradient-
Based Learning Algorithms for Recurrent Net-
works and Their Computational Complexity. In:
Yves Chauvain and David E. Rumelhart: ?Back-
Propagation: Theory, Architectures and Applica-
tions?. Lawrence Erlbaum Publishers.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475?484, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving statistical machine
translation with word class models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, USA, October.
25
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 475?484,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Training Phrase Translation Models with Leaving-One-Out
Joern Wuebker and Arne Mauser and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Germany
<surname>@cs.rwth-aachen.de
Abstract
Several attempts have been made to learn
phrase translation probabilities for phrase-
based statistical machine translation that
go beyond pure counting of phrases
in word-aligned training data. Most
approaches report problems with over-
fitting. We describe a novel leaving-
one-out approach to prevent over-fitting
that allows us to train phrase models that
show improved translation performance
on the WMT08 Europarl German-English
task. In contrast to most previous work
where phrase models were trained sepa-
rately from other models used in transla-
tion, we include all components such as
single word lexica and reordering mod-
els in training. Using this consistent
training of phrase models we are able to
achieve improvements of up to 1.4 points
in BLEU. As a side effect, the phrase table
size is reduced by more than 80%.
1 Introduction
A phrase-based SMT system takes a source sen-
tence and produces a translation by segmenting the
sentence into phrases and translating those phrases
separately (Koehn et al, 2003). The phrase trans-
lation table, which contains the bilingual phrase
pairs and the corresponding translation probabil-
ities, is one of the main components of an SMT
system. The most common method for obtain-
ing the phrase table is heuristic extraction from
automatically word-aligned bilingual training data
(Och et al, 1999). In this method, all phrases of
the sentence pair that match constraints given by
the alignment are extracted. This includes over-
lapping phrases. At extraction time it does not
matter, whether the phrases are extracted from a
highly probable phrase alignment or from an un-
likely one.
Phrase model probabilities are typically defined
as relative frequencies of phrases extracted from
word-aligned parallel training data. The joint
counts C(f? , e?) of the source phrase f? and the tar-
get phrase e? in the entire training data are normal-
ized by the marginal counts of source and target
phrase to obtain a conditional probability
pH(f? |e?) =
C(f? , e?)
C(e?)
. (1)
The translation process is implemented as a
weighted log-linear combination of several mod-
els hm(eI1, s
K
1 , f
J
1 ) including the logarithm of the
phrase probability in source-to-target as well as in
target-to-source direction. The phrase model is
combined with a language model, word lexicon
models, word and phrase penalty, and many oth-
ers. (Och and Ney, 2004) The best translation e?I?1
as defined by the models then can be written as
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
(2)
In this work, we propose to directly train our
phrase models by applying a forced alignment pro-
cedure where we use the decoder to find a phrase
alignment between source and target sentences of
the training data and then updating phrase transla-
tion probabilities based on this alignment. In con-
trast to heuristic extraction, the proposed method
provides a way of consistently training and using
phrase models in translation. We use a modified
version of a phrase-based decoder to perform the
forced alignment. This way we ensure that all
models used in training are identical to the ones
used at decoding time. An illustration of the basic
475
Figure 1: Illustration of phrase training with
forced alignment.
idea can be seen in Figure 1. In the literature this
method by itself has been shown to be problem-
atic because it suffers from over-fitting (DeNero
et al, 2006), (Liang et al, 2006). Since our ini-
tial phrases are extracted from the same training
data, that we want to align, very long phrases can
be found for segmentation. As these long phrases
tend to occur in only a few training sentences, the
EM algorithm generally overestimates their prob-
ability and neglects shorter phrases, which better
generalize to unseen data and thus are more useful
for translation. In order to counteract these effects,
our training procedure applies leaving-one-out on
the sentence level. Our results show, that this leads
to a better translation quality.
Ideally, we would produce all possible segmen-
tations and alignments during training. However,
this has been shown to be infeasible for real-world
data (DeNero and Klein, 2008). As training uses
a modified version of the translation decoder, it is
straightforward to apply pruning as in regular de-
coding. Additionally, we consider three ways of
approximating the full search space:
1. the single-best Viterbi alignment,
2. the n-best alignments,
3. all alignments remaining in the search space
after pruning.
The performance of the different approaches is
measured and compared on the German-English
Europarl task from the ACL 2008 Workshop on
Statistical Machine Translation (WMT08). Our
results show that the proposed phrase model train-
ing improves translation quality on the test set by
0.9 BLEU points over our baseline. We find that
by interpolation with the heuristically extracted
phrases translation performance can reach up to
1.4 BLEU improvement over the baseline on the
test set.
After reviewing the related work in the fol-
lowing section, we give a detailed description
of phrasal alignment and leaving-one-out in Sec-
tion 3. Section 4 explains the estimation of phrase
models. The empirical evaluation of the different
approaches is done in Section 5.
2 Related Work
It has been pointed out in literature, that training
phrase models poses some difficulties. For a gen-
erative model, (DeNero et al, 2006) gave a de-
tailed analysis of the challenges and arising prob-
lems. They introduce a model similar to the one
we propose in Section 4.2 and train it with the EM
algorithm. Their results show that it can not reach
a performance competitive to extracting a phrase
table from word alignment by heuristics (Och et
al., 1999).
Several reasons are revealed in (DeNero et al,
2006). When given a bilingual sentence pair, we
can usually assume there are a number of equally
correct phrase segmentations and corresponding
alignments. For example, it may be possible to
transform one valid segmentation into another by
splitting some of its phrases into sub-phrases or by
shifting phrase boundaries. This is different from
word-based translation models, where a typical as-
sumption is that each target word corresponds to
only one source word. As a result of this am-
biguity, different segmentations are recruited for
different examples during training. That in turn
leads to over-fitting which shows in overly deter-
minized estimates of the phrase translation prob-
abilities. In addition, (DeNero et al, 2006) found
that the trained phrase table shows a highly peaked
distribution in opposition to the more flat distribu-
tion resulting from heuristic extraction, leaving the
decoder only few translation options at decoding
time.
Our work differs from (DeNero et al, 2006)
in a number of ways, addressing those problems.
476
To limit the effects of over-fitting, we apply the
leaving-one-out and cross-validation methods in
training. In addition, we do not restrict the train-
ing to phrases consistent with the word alignment,
as was done in (DeNero et al, 2006). This allows
us to recover from flawed word alignments.
In (Liang et al, 2006) a discriminative transla-
tion system is described. For training of the pa-
rameters for the discriminative features they pro-
pose a strategy they call bold updating. It is simi-
lar to our forced alignment training procedure de-
scribed in Section 3.
For the hierarchical phrase-based approach,
(Blunsom et al, 2008) present a discriminative
rule model and show the difference between using
only the viterbi alignment in training and using the
full sum over all possible derivations.
Forced alignment can also be utilized to train a
phrase segmentation model, as is shown in (Shen
et al, 2008). They report small but consistent
improvements by incorporating this segmentation
model, which works as an additional prior proba-
bility on the monolingual target phrase.
In (Ferrer and Juan, 2009), phrase models are
trained by a semi-hidden Markov model. They
train a conditional ?inverse? phrase model of the
target phrase given the source phrase. Addition-
ally to the phrases, they model the segmentation
sequence that is used to produce a phrase align-
ment between the source and the target sentence.
They used a phrase length limit of 4 words with
longer phrases not resulting in further improve-
ments. To counteract over-fitting, they interpolate
the phrase model with IBM Model 1 probabilities
that are computed on the phrase level. We also in-
clude these word lexica, as they are standard com-
ponents of the phrase-based system.
It is shown in (Ferrer and Juan, 2009), that
Viterbi training produces almost the same results
as full Baum-Welch training. They report im-
provements over a phrase-based model that uses
an inverse phrase model and a language model.
Experiments are carried out on a custom subset of
the English-Spanish Europarl corpus.
Our approach is similar to the one presented in
(Ferrer and Juan, 2009) in that we compare Viterbi
and a training method based on the Forward-
Backward algorithm. But instead of focusing on
the statistical model and relaxing the translation
task by using monotone translation only, we use a
full and competitive translation system as starting
point with reordering and all models included.
In (Marcu and Wong, 2002), a joint probability
phrase model is presented. The learned phrases
are restricted to the most frequent n-grams up to
length 6 and all unigrams. Monolingual phrases
have to occur at least 5 times to be considered
in training. Smoothing is applied to the learned
models so that probabilities for rare phrases are
non-zero. In training, they use a greedy algorithm
to produce the Viterbi phrase alignment and then
apply a hill-climbing technique that modifies the
Viterbi alignment by merge, move, split, and swap
operations to find an alignment with a better prob-
ability in each iteration. The model shows im-
provements in translation quality over the single-
word-based IBM Model 4 (Brown et al, 1993) on
a subset of the Canadian Hansards corpus.
The joint model by (Marcu and Wong, 2002)
is refined by (Birch et al, 2006) who use
high-confidence word alignments to constrain the
search space in training. They observe that due to
several constraints and pruning steps, the trained
phrase table is much smaller than the heuristically
extracted one, while preserving translation quality.
The work by (DeNero et al, 2008) describes
a method to train the joint model described in
(Marcu and Wong, 2002) with a Gibbs sampler.
They show that by applying a prior distribution
over the phrase translation probabilities they can
prevent over-fitting. The prior is composed of
IBM1 lexical probabilities and a geometric distri-
bution over phrase lengths which penalizes long
phrases. The two approaches differ in that we ap-
ply the leaving-one-out procedure to avoid over-
fitting, as opposed to explicitly defining a prior
distribution.
3 Alignment
The training process is divided into three parts.
First we obtain all models needed for a normal
translations system. We perform minimum error
rate training with the downhill simplex algorithm
(Nelder and Mead, 1965) on the development data
to obtain a set of scaling factors that achieve a
good BLEU score. We then use these models and
scaling factors to do a forced alignment, where
we compute a phrase alignment for the training
data. From this alignment we then estimate new
phrase models, while keeping all other models un-
477
changed. In this section we describe our forced
alignment procedure that is the basic training pro-
cedure for the models proposed here.
3.1 Forced Alignment
The idea of forced alignment is to perform a
phrase segmentation and alignment of each sen-
tence pair of the training data using the full transla-
tion system as in decoding. What we call segmen-
tation and alignment here corresponds to the ?con-
cepts? used by (Marcu and Wong, 2002). We ap-
ply our normal phrase-based decoder on the source
side of the training data and constrain the transla-
tions to the corresponding target sentences from
the training data.
Given a source sentence fJ1 and target sentence
eI1, we search for the best phrase segmentation and
alignment that covers both sentences. A segmen-
tation of a sentence into K phrase is defined by
k ? sk := (ik, bk, jk), for k = 1, . . . ,K
where for each segment ik is last position of kth
target phrase, and (bk, jk) are the start and end
positions of the source phrase aligned to the kth
target phrase. Consequently, we can modify Equa-
tion 2 to define the best segmentation of a sentence
pair as:
s?K?1 = argmax
K,sK1
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
(3)
The identical models as in search are used: condi-
tional phrase probabilities p(f?k|e?k) and p(e?k|f?k),
within-phrase lexical probabilities, distance-based
reordering model as well as word and phrase
penalty. A language model is not used in this case,
as the system is constrained to the given target sen-
tence and thus the language model score has no
effect on the alignment.
In addition to the phrase matching on the source
sentence, we also discard all phrase translation
candidates, that do not match any sequence in the
given target sentence.
Sentences for which the decoder can not find
an alignment are discarded for the phrase model
training. In our experiments, this is the case for
roughly 5% of the training sentences.
3.2 Leaving-one-out
As was mentioned in Section 2, previous ap-
proaches found over-fitting to be a problem in
phrase model training. In this section, we de-
scribe a leaving-one-out method that can improve
the phrase alignment in situations, where the prob-
ability of rare phrases and alignments might be
overestimated. The training data that consists ofN
parallel sentence pairs fn and en for n = 1, . . . , N
is used for both the initialization of the transla-
tion model p(f? |e?) and the phrase model training.
While this way we can make full use of the avail-
able data and avoid unknown words during train-
ing, it has the drawback that it can lead to over-
fitting. All phrases extracted from a specific sen-
tence pair fn, en can be used for the alignment of
this sentence pair. This includes longer phrases,
which only match in very few sentences in the
data. Therefore those long phrases are trained to
fit only a few sentence pairs, strongly overesti-
mating their translation probabilities and failing to
generalize. In the extreme case, whole sentences
will be learned as phrasal translations. The aver-
age length of the used phrases is an indicator of
this kind of over-fitting, as the number of match-
ing training sentences decreases with increasing
phrase length. We can see an example in Figure
2. Without leaving-one-out the sentence is seg-
mented into a few long phrases, which are unlikely
to occur in data to be translated. Phrase boundaries
seem to be unintuitive and based on some hidden
structures. With leaving-one-out the phrases are
shorter and therefore better suited for generaliza-
tion to unseen data.
Previous attempts have dealt with the over-
fitting problem by limiting the maximum phrase
length (DeNero et al, 2006; Marcu and Wong,
2002) and by smoothing the phrase probabilities
by lexical models on the phrase level (Ferrer and
Juan, 2009). However, (DeNero et al, 2006) expe-
rienced similar over-fitting with short phrases due
to the fact that the same word sequence can be seg-
mented in different ways, leading to specific seg-
mentations being learned for specific training sen-
tence pairs. Our results confirm these findings. To
deal with this problem, instead of simple phrase
length restriction, we propose to apply the leaving-
one-out method, which is also used for language
modeling techniques (Kneser and Ney, 1995).
When using leaving-one-out, we modify the
phrase translation probabilities for each sentence
pair. For a training example fn, en, we have to
remove all phrases Cn(f? , e?) that were extracted
from this sentence pair from the phrase counts that
478
Figure 2: Segmentation example from forced alignment. Top: without leaving-one-out. Bottom: with
leaving-one-out.
we used to construct our phrase translation table.
The same holds for the marginal counts Cn(e?) and
Cn(f?). Starting from Equation 1, the leaving-one-
out phrase probability for training sentence pair n
is
pl1o,n(f? |e?) =
C(f? , e?)? Cn(f? , e?)
C(e?)? Cn(e?)
(4)
To be able to perform the re-computation in an
efficient way, we store the source and target phrase
marginal counts for each phrase in the phrase ta-
ble. A phrase extraction is performed for each
training sentence pair separately using the same
word alignment as for the initialization. It is then
straightforward to compute the phrase counts after
leaving-one-out using the phrase probabilities and
marginal counts stored in the phrase table.
While this works well for more frequent obser-
vations, singleton phrases are assigned a probabil-
ity of zero. We refer to singleton phrases as phrase
pairs that occur only in one sentence. For these
sentences, the decoder needs the singleton phrase
pairs to produce an alignment. Therefore we retain
those phrases by assigning them a positive proba-
bility close to zero. We evaluated with two differ-
ent strategies for this, which we call standard and
length-based leaving-one-out. Standard leaving-
one-out assigns a fixed probability ? to singleton
phrase pairs. This way the decoder will prefer us-
ing more frequent phrases for the alignment, but is
able to resort to singletons if necessary. However,
we found that with this method longer singleton
phrases are preferred over shorter ones, because
fewer of them are needed to produce the target sen-
tence. In order to better generalize to unseen data,
we would like to give the preference to shorter
phrases. This is done by length-based leaving-
one-out, where singleton phrases are assigned the
probability ?(|f? |+|e?|) with the source and target
Table 1: Avg. source phrase lengths in forced
alignment without leaving-one-out and with stan-
dard and length-based leaving-one-out.
avg. phrase length
without l1o 2.5
standard l1o 1.9
length-based l1o 1.6
phrase lengths |f? | and |e?| and fixed ? < 1. In our
experiments we set ? = e?20 and ? = e?5. Ta-
ble 1 shows the decrease in average source phrase
length by application of leaving-one-out.
3.3 Cross-validation
For the first iteration of the phrase training,
leaving-one-out can be implemented efficiently as
described in Section 3.2. For higher iterations,
phrase counts obtained in the previous iterations
would have to be stored on disk separately for each
sentence and accessed during the forced alignment
process. To simplify this procedure, we propose
a cross-validation strategy on larger batches of
data. Instead of recomputing the phrase counts for
each sentence individually, this is done for a whole
batch of sentences at a time. In our experiments,
we set this batch-size to 10000 sentences.
3.4 Parallelization
To cope with the runtime and memory require-
ments of phrase model training that was pointed
out by previous work (Marcu and Wong, 2002;
Birch et al, 2006), we parallelized the forced
alignment by splitting the training corpus into
blocks of 10k sentence pairs. From the initial
phrase table, each of these blocks only loads the
phrases that are required for alignment. The align-
479
ment and the counting of phrases are done sep-
arately for each block and then accumulated to
build the updated phrase model.
4 Phrase Model Training
The produced phrase alignment can be given as a
single best alignment, as the n-best alignments or
as an alignment graph representing all alignments
considered by the decoder. We have developed
two different models for phrase translation proba-
bilities which make use of the force-aligned train-
ing data. Additionally we consider smoothing by
different kinds of interpolation of the generative
model with the state-of-the-art heuristics.
4.1 Viterbi
The simplest of our generative phrase models esti-
mates phrase translation probabilities by their rel-
ative frequencies in the Viterbi alignment of the
data, similar to the heuristic model but with counts
from the phrase-aligned data produced in training
rather than computed on the basis of a word align-
ment. The translation probability of a phrase pair
(f? , e?) is estimated as
pFA(f? |e?) =
CFA(f? , e?)
?
f? ?
CFA(f?
?, e?)
(5)
where CFA(f? , e?) is the count of the phrase pair
(f? , e?) in the phrase-aligned training data. This can
be applied to either the Viterbi phrase alignment
or an n-best list. For the simplest model, each
hypothesis in the n-best list is weighted equally.
We will refer to this model as the count model as
we simply count the number of occurrences of a
phrase pair. We also experimented with weight-
ing the counts with the estimated likelihood of the
corresponding entry in the the n-best list. The sum
of the likelihoods of all entries in an n-best list is
normalized to 1. We will refer to this model as the
weighted count model.
4.2 Forward-backward
Ideally, the training procedure would consider all
possible alignment and segmentation hypotheses.
When alternatives are weighted by their posterior
probability. As discussed earlier, the run-time re-
quirements for computing all possible alignments
is prohibitive for large data tasks. However, we
can approximate the space of all possible hypothe-
ses by the search space that was used for the align-
ment. While this might not cover all phrase trans-
lation probabilities, it allows the search space and
translation times to be feasible and still contains
the most probable alignments. This search space
can be represented as a graph of partial hypothe-
ses (Ueffing et al, 2002) on which we can com-
pute expectations using the Forward-Backward al-
gorithm. We will refer to this alignment as the full
alignment. In contrast to the method described in
Section 4.1, phrases are weighted by their poste-
rior probability in the word graph. As suggested in
work on minimum Bayes-risk decoding for SMT
(Tromble et al, 2008; Ehling et al, 2007), we use
a global factor to scale the posterior probabilities.
4.3 Phrase Table Interpolation
As (DeNero et al, 2006) have reported improve-
ments in translation quality by interpolation of
phrase tables produced by the generative and the
heuristic model, we adopt this method and also re-
port results using log-linear interpolation of the es-
timated model with the original model.
The log-linear interpolations pint(f? |e?) of the
phrase translation probabilities are estimated as
pint(f? |e?) =
(
pH(f? |e?)
)1??
?
(
pgen(f? |e?)
)(?)
(6)
where ? is the interpolation weight, pH the
heuristically estimated phrase model and pgen the
count model. The interpolation weight ? is ad-
justed on the development corpus. When inter-
polating phrase tables containing different sets of
phrase pairs, we retain the intersection of the two.
As a generalization of the fixed interpolation of
the two phrase tables we also experimented with
adding the two trained phrase probabilities as ad-
ditional features to the log-linear framework. This
way we allow different interpolation weights for
the two translation directions and can optimize
them automatically along with the other feature
weights. We will refer to this method as feature-
wise combination. Again, we retain the intersec-
tion of the two phrase tables. With good log-
linear feature weights, feature-wise combination
should perform at least as well as fixed interpo-
lation. However, the results presented in Table 5
480
Table 2: Statistics for the Europarl German-
English data
German English
TRAIN Sentences 1 311 815
Run. Words 34 398 651 36 090 085
Vocabulary 336 347 118 112
Singletons 168 686 47 507
DEV Sentences 2 000
Run. Words 55 118 58 761
Vocabulary 9 211 6 549
OOVs 284 77
TEST Sentences 2 000
Run. Words 56 635 60 188
Vocabulary 9 254 6 497
OOVs 266 89
show a slightly lower performance. This illustrates
that a higher number of features results in a less
reliable optimization of the log-linear parameters.
5 Experimental Evaluation
5.1 Experimental Setup
We conducted our experiments on the German-
English data published for the ACL 2008
Workshop on Statistical Machine Translation
(WMT08). Statistics for the Europarl data are
given in Table 2.
We are given the three data sets TRAIN ,DEV
and TEST . For the heuristic phrase model, we
first use GIZA++ (Och and Ney, 2003) to compute
the word alignment on TRAIN . Next we obtain
a phrase table by extraction of phrases from the
word alignment. The scaling factors of the trans-
lation models have been optimized for BLEU on
the DEV data.
The phrase table obtained by heuristic extraction
is also used to initialize the training. The forced
alignment is run on the training data TRAIN
from which we obtain the phrase alignments.
Those are used to build a phrase table according
to the proposed generative phrase models. After-
ward, the scaling factors are trained on DEV for
the new phrase table. By feeding back the new
phrase table into forced alignment we can reiterate
the training procedure. When training is finished
the resulting phrase model is evaluated on DEV
Table 3: Comparison of different training setups
for the count model on DEV .
leaving-one-out max phr.len. BLEU TER
baseline 6 25.7 61.1
none 2 25.2 61.3
3 25.7 61.3
4 25.5 61.4
5 25.5 61.4
6 25.4 61.7
standard 6 26.4 60.9
length-based 6 26.5 60.6
and TEST . Additionally, we can apply smooth-
ing by interpolation of the new phrase table with
the original one estimated heuristically, retrain the
scaling factors and evaluate afterwards.
The baseline system is a standard phrase-based
SMT system with eight features: phrase transla-
tion and word lexicon probabilities in both transla-
tion directions, phrase penalty, word penalty, lan-
guage model score and a simple distance-based re-
ordering model. The features are combined in a
log-linear way. To investigate the generative mod-
els, we replace the two phrase translation prob-
abilities and keep the other features identical to
the baseline. For the feature-wise combination
the two generative phrase probabilities are added
to the features, resulting in a total of 10 features.
We used a 4-gram language model with modified
Kneser-Ney discounting for all experiments. The
metrics used for evaluation are the case-sensitive
BLEU (Papineni et al, 2002) score and the trans-
lation edit rate (TER) (Snover et al, 2006) with
one reference translation.
5.2 Results
In this section, we investigate the different as-
pects of the models and methods presented be-
fore. We will focus on the proposed leaving-one-
out technique and show that it helps in finding
good phrasal alignments on the training data that
lead to improved translation models. Our final
results show an improvement of 1.4 BLEU over
the heuristically extracted phrase model on the test
data set.
In Section 3.2 we have discussed several meth-
ods which aim to overcome the over-fitting prob-
481
Figure 3: Performance on DEV in BLEU of the
count model plotted against size n of n-best list
on a logarithmic scale.
lems described in (DeNero et al, 2006). Table 3
shows translation scores of the count model on the
development data after the first training iteration
for both leaving-one-out strategies we have in-
troduced and for training without leaving-one-out
with different restrictions on phrase length. We
can see that by restricting the source phrase length
to a maximum of 3 words, the trained model is
close to the performance of the heuristic phrase
model. With the application of leaving-one-out,
the trained model is superior to the baseline, the
length-based strategy performing slightly better
than standard leaving-one-out. For these experi-
ments the count model was estimated with a 100-
best list.
The count model we describe in Section 4.1 esti-
mates phrase translation probabilities using counts
from the n-best phrase alignments. For smaller n
the resulting phrase table contains fewer phrases
and is more deterministic. For higher values of
n more competing alignments are taken into ac-
count, resulting in a bigger phrase table and a
smoother distribution. We can see in Figure 3
that translation performance improves by moving
from the Viterbi alignment to n-best alignments.
The variations in performance with sizes between
n = 10 and n = 10000 are less than 0.2 BLEU.
The maximum is reached for n = 100, which we
used in all subsequent experiments. An additional
benefit of the count model is the smaller phrase
table size compared to the heuristic phrase extrac-
tion. This is consistent with the findings of (Birch
et al, 2006). Table 4 shows the phrase table sizes
for different n. With n = 100 we retain only 17%
of the original phrases. Even for the full model, we
Table 4: Phrase table size of the count model for
different n-best list sizes, the full model and for
heuristic phrase extraction.
N # phrases % of full table
1 4.9M 5.3
10 8.4M 9.1
100 15.9M 17.2
1000 27.1M 29.2
10000 40.1M 43.2
full 59.6M 64.2
heuristic 92.7M 100.0
do not retain all phrase table entries. Due to prun-
ing in the forced alignment step, not all translation
options are considered. As a result experiments
can be done more rapidly and with less resources
than with the heuristically extracted phrase table.
Also, our experiments show that the increased per-
formance of the count model is partly derived from
the smaller phrase table size. In Table 5 we can see
that the performance of the heuristic phrase model
can be increased by 0.6 BLEU on TEST by fil-
tering the phrase table to contain the same phrases
as the count model and reoptimizing the log-linear
model weights. The experiments on the number of
different alignments taken into account were done
with standard leaving-one-out.
The final results are given in Table 5. We can
see that the count model outperforms the base-
line by 0.8 BLEU on DEV and 0.9 BLEU on
TEST after the first training iteration. The perfor-
mance of the filtered baseline phrase table shows
that part of that improvement derives from the
smaller phrase table size. Application of cross-
validation (cv) in the first iteration yields a perfor-
mance close to training with leaving-one-out (l1o),
which indicates that cross-validation can be safely
applied to higher training iterations as an alterna-
tive to leaving-one-out. The weighted count model
clearly under-performs the simpler count model.
A second iteration of the training algorithm shows
nearly no changes in BLEU score, but a small im-
provement in TER. Here, we used the phrase table
trained with leaving-one-out in the first iteration
and applied cross-validation in the second itera-
tion. Log-linear interpolation of the count model
with the heuristic yields a further increase, show-
ing an improvement of 1.3 BLEU onDEV and 1.4
BLEU on TEST over the baseline. The interpo-
482
Table 5: Final results for the heuristic phrase table
filtered to contain the same phrases as the count
model (baseline filt.), the count model trained with
leaving-one-out (l1o) and cross-validation (cv),
the weighted count model and the full model. Fur-
ther, scores for fixed log-linear interpolation of the
count model trained with leaving-one-out with the
heuristic as well as a feature-wise combination are
shown. The results of the second training iteration
are given in the bottom row.
DEV TEST
BLEU TER BLEU TER
baseline 25.7 61.1 26.3 60.9
baseline filt. 26.0 61.6 26.9 61.2
count (l1o) 26.5 60.6 27.2 60.5
count (cv) 26.4 60.7 27.0 60.7
weight. count 25.9 61.4 26.4 61.3
full 26.3 60.0 27.0 60.2
fixed interpol. 27.0 59.4 27.7 59.2
feat. comb. 26.8 60.1 27.6 59.9
count, iter. 2 26.4 60.3 27.2 60.0
lation weight is adjusted on the development set
and was set to ? = 0.6. Integrating both models
into the log-linear framework (feat. comb.) yields
a BLEU score slightly lower than with fixed inter-
polation on both DEV and TEST . This might
be attributed to deficiencies in the tuning proce-
dure. The full model, where we extract all phrases
from the search graph, weighted with their poste-
rior probability, performs comparable to the count
model with a slightly worse BLEU and a slightly
better TER.
6 Conclusion
We have shown that training phrase models can
improve translation performance on a state-of-
the-art phrase-based translation model. This is
achieved by training phrase translation probabil-
ities in a way that they are consistent with their
use in translation. A crucial aspect here is the use
of leaving-one-out to avoid over-fitting. We have
shown that the technique is superior to limiting
phrase lengths and smoothing with lexical prob-
abilities alone.
While models trained from Viterbi alignments
already lead to good results, we have demonstrated
that considering the 100-best alignments allows to
better model the ambiguities in phrase segmenta-
tion.
The proposed techniques are shown to be supe-
rior to previous approaches that only used lexical
probabilities to smooth phrase tables or imposed
limits on the phrase lengths. On the WMT08 Eu-
roparl task we show improvements of 0.9 BLEU
points with the trained phrase table and 1.4 BLEU
points when interpolating the newly trained model
with the original, heuristically extracted phrase ta-
ble. In TER, improvements are 0.4 and 1.7 points.
In addition to the improved performance, the
trained models are smaller leading to faster and
smaller translation systems.
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation, and also partly based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. HR001-06-C-0023. Any opinions,
ndings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reect the views of the DARPA.
References
Alexandra Birch, Chris Callison-Burch, Miles Os-
borne, and Philipp Koehn. 2006. Constraining the
phrase-based, joint probability statistical translation
model. In smt2006, pages 154?157, Jun.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL-08:
HLT, pages 200?208, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?312, June.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 25?28, Morristown, NJ,
USA. Association for Computational Linguistics.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why Generative Phrase Models Un-
derperform Surface Heuristics. In Proceedings of the
483
Workshop on Statistical Machine Translation, pages
31?38, New York City, June.
John DeNero, Alexandre Buchard-Co?te?, and Dan
Klein. 2008. Sampling Alignment Structure under
a Bayesian Translation Model. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 314?323, Honolulu,
October.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007.
Minimum bayes risk decoding for bleu. In ACL ?07:
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 101?104, Morristown, NJ, USA. Association
for Computational Linguistics.
Jesu?s-Andre?s Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-markov approach to machine
translation. In Procedings of European Association
for Machine Translation (EAMT), Barcelona, Spain,
May. European Association for Machine Translation.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-Off for M-gram Language Modelling. In
IEEE Int. Conf. on Acoustics, Speech and Signal
Processing (ICASSP), pages 181?184, Detroit, MI,
May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, pages 48?54, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Percy Liang, Alexandre Buchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An End-to-End Discriminative
Approach to Machine Translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, pages 761?
768, Sydney, Australia.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2002), July.
J.A. Nelder and R. Mead. 1965. A Simplex Method
for Function Minimization. The Computer Journal),
7:308?313.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449,
December.
F.J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint SIGDAT Conf. on Empirical
Methods in Natural Language Processing and Very
Large Corpora (EMNLP99), pages 20?28, Univer-
sity of Maryland, College Park, MD, USA, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Wade Shen, Brian Delaney, Tim Anderson, and Ray
Slyh. 2008. The MIT-LL/AFRL IWSLT-2008 MT
System. In Proceedings of IWSLT 2008, pages 69?
76, Hawaii, U.S.A., October.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of AMTA, pages 223?231, Aug.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
620?629, Honolulu, Hawaii, October. Association
for Computational Linguistics.
N. Ueffing, F.J. Och, and H. Ney. 2002. Genera-
tion of word graphs in statistical machine translation.
In Proc. of the Conference on Empirical Methods
for Natural Language Processing, pages 156?163,
Philadelphia, PA, USA, July.
484
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 28?32,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fast and Scalable Decoding with Language Model Look-Ahead
for Phrase-based Statistical Machine Translation
Joern Wuebker, Hermann Ney
Human Language Technology
and Pattern Recognition Group
Computer Science Department
RWTH Aachen University, Germany
surname@cs.rwth-aachen.de
Richard Zens*
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043
zens@google.com
Abstract
In this work we present two extensions to
the well-known dynamic programming beam
search in phrase-based statistical machine
translation (SMT), aiming at increased effi-
ciency of decoding by minimizing the number
of language model computations and hypothe-
sis expansions. Our results show that language
model based pre-sorting yields a small im-
provement in translation quality and a speedup
by a factor of 2. Two look-ahead methods are
shown to further increase translation speed by
a factor of 2 without changing the search space
and a factor of 4 with the side-effect of some
additional search errors. We compare our ap-
proach with Moses and observe the same per-
formance, but a substantially better trade-off
between translation quality and speed. At a
speed of roughly 70 words per second, Moses
reaches 17.2% BLEU, whereas our approach
yields 20.0% with identical models.
1 Introduction
Research efforts to increase search efficiency for
phrase-based MT (Koehn et al, 2003) have ex-
plored several directions, ranging from generalizing
the stack decoding algorithm (Ortiz et al, 2006) to
additional early pruning techniques (Delaney et al,
2006), (Moore and Quirk, 2007) and more efficient
language model (LM) querying (Heafield, 2011).
This work extends the approach by (Zens and
Ney, 2008) with two techniques to increase trans-
lation speed and scalability. We show that taking
a heuristic LM score estimate for pre-sorting the
phrase translation candidates has a positive effect on
both translation quality and speed. Further, we intro-
duce two novel LM look-ahead methods. The idea
of LM look-ahead is to incorporate the LM proba-
bilities into the pruning process of the beam search
as early as possible. In speech recognition it has
been used for many years (Steinbiss et al, 1994;
Ortmanns et al, 1998). First-word LM look-ahead
exploits the search structure to use the LM costs of
the first word of a new phrase as a lower bound for
the full LM costs of the phrase. Phrase-only LM
look-ahead makes use of a pre-computed estimate
of the full LM costs for each phrase. We detail the
implementation of these methods and analyze their
effect with respect to the number of LM computa-
tions and hypothesis expansions as well as on trans-
lation speed and quality. We also run comparisons
with the Moses decoder (Koehn et al, 2007), which
yields the same performance in BLEU, but is outper-
formed significantly in terms of scalability for faster
translation. Our implementation is available under
a non-commercial open source licence?.
2 Search Algorithm Extensions
We apply the decoding algorithm described in (Zens
and Ney, 2008). Hypotheses are scored by a
weighted log-linear combination of models. A beam
search strategy is used to find the best hypothesis.
During search we perform pruning controlled by the
parameters coverage histogram size? Nc and lexical
?Richard Zens?s contribution was during his time at RWTH.
?www-i6.informatik.rwth-aachen.de/jane
?number of hypothesized coverage vectors per cardinality
28
histogram size? Nl .
2.1 Phrase candidate pre-sorting
In addition to the source sentence f J1 , the beam
search algorithm takes a matrix E(?, ?) as input,
where for each contiguous phrase f? = f j . . . f j?
within the source sentence, E( j, j?) contains a list of
all candidate translations for f? . The candidate lists
are sorted according to their model score, which was
observed to speed up translation by Delaney et al
(2006). In addition to sorting according to the purely
phrase-internal scores, which is common practice,
we compute an estimate qLME(e?) for the LM score
of each target phrase e?. qLME(e?) is the weighted
LM score we receive by assuming e? to be a com-
plete sentence without using sentence start and end
markers. We limit the number of translation options
per source phrase to the No top scoring candidates
(observation histogram pruning).
The pre-sorting during phrase matching has two
effects on the search algorithm. Firstly, it defines
the order in which the hypothesis expansions take
place. As higher scoring phrases are considered first,
it is less likely that already created partial hypothe-
ses will have to be replaced, thus effectively reduc-
ing the expected number of hypothesis expansions.
Secondly, due to the observation pruning the sorting
affects the considered phrase candidates and conse-
quently the search space. A better pre-selection can
be expected to improve translation quality.
2.2 Language Model Look-Ahead
LM score computations are among the most expen-
sive in decoding. Delaney et al (2006) report signif-
icant improvements in runtime by removing unnec-
essary LM lookups via early pruning. Here we de-
scribe an LM look-ahead technique, which is aimed
at further reducing the number of LM computations.
The innermost loop of the search algorithm iter-
ates over all translation options for a single source
phrase to consider them for expanding the current
hypothesis. We introduce an LM look-ahead score
qLMLA(e?|e??), which is computed for each of the
translation options. This score is added to the over-
all hypothesis score, and if the pruning threshold is
?number of lexical hypotheses per coverage vector
exceeded, we discard the expansion without com-
puting the full LM score.
First-word LM look-ahead pruning defines the
LM look-ahead score qLMLA(e?|e??) = qLM(e?1|e??) to
be the LM score of the first word of target phrase e?
given history e??. As qLM(e?1|e??) is an upper bound for
the full LM score, the technique does not introduce
additional seach errors. The score can be reused, if
the LM score of the full phrase e? needs to be com-
puted afterwards.
We can exploit the structure of the search to speed
up the LM lookups for the first word. The LM prob-
abilities are stored in a trie, where each node cor-
responds to a specific LM history. Usually, each
LM lookup consists of first traversing the trie to find
the node corresponding to the current LM history
and then retrieving the probability for the next word.
If the n-gram is not present, we have to repeat this
procedure with the next lower-order history, until a
probability is found. However, the LM history for
the first words of all phrases within the innermost
loop of the search algorithm is identical. Just be-
fore the loop we can therefore traverse the trie once
for the current history and each of its lower order n-
grams and store the pointers to the resulting nodes.
To retrieve the LM look-ahead scores, we can then
directly access the nodes without the need to traverse
the trie again. This implementational detail was con-
firmed to increase translation speed by roughly 20%
in a short experiment.
Phrase-only LM look-ahead pruning defines the
look-ahead score qLMLA(e?|e??) = qLME(e?) to be the
LM score of phrase e?, assuming e? to be the full sen-
tence. It was already used for sorting the phrases,
is therefore pre-computed and does not require ad-
ditional LM lookups. As it is not a lower bound for
the real LM score, this pruning technique can intro-
duce additional search errors. Our results show that
it radically reduces the number of LM lookups.
3 Experimental Evaluation
3.1 Setup
The experiments are carried out on the
German?English task provided for WMT 2011?.
?http://www.statmt.org/wmt11
29
system BLEU[%] #HYP #LM w/s
No = ?
baseline 20.1 3.0K 322K 2.2
+pre-sort 20.1 2.5K 183K 3.6
No = 100
baseline 19.9 2.3K 119K 7.1
+pre-sort 20.1 1.9K 52K 15.8
+first-word 20.1 1.9K 40K 31.4
+phrase-only 19.8 1.6K 6K 69.2
Table 1: Comparison of the number of hypothesis expan-
sions per source word (#HYP) and LM computations per
source word (#LM) with respect to LM pre-sorting, first-
word LM look-ahead and phrase-only LM look-ahead on
newstest2009. Speed is given in words per second.
Results are given with (No = 100) and without (No = ?)
observation pruning.
The English language model is a 4-gram LM
created with the SRILM toolkit (Stolcke, 2002) on
all bilingual and parts of the provided monolingual
data. newstest2008 is used for parameter
optimization, newstest2009 as a blind test
set. To confirm our results, we run the final set of
experiments also on the English?French task of
IWSLT 2011?. We evaluate with BLEU (Papineni et
al., 2002) and TER (Snover et al, 2006).
We use identical phrase tables and scaling fac-
tors for Moses and our decoder. The phrase table
is pruned to a maximum of 400 target candidates per
source phrase before decoding. The phrase table and
LM are loaded into memory before translating and
loading time is eliminated for speed measurements.
3.2 Methodological analysis
To observe the effect of the proposed search al-
gorithm extensions, we ran experiments with fixed
pruning parameters, keeping track of the number of
hypothesis expansions and LM computations. The
LM score pre-sorting affects both the set of phrase
candidates due to observation histogram pruning and
the order in which they are considered. To sepa-
rate these effects, experiments were run both with
histogram pruning (No = 100) and without. From
Table 1 we can see that in terms of efficiency both
cases show similar improvements over the baseline,
?http://iwslt2011.org
 16
 17
 18
 19
 20
 1  4  16  64  256  1024  4096
B
L
E
U
[
%
]
words/sec
Mosesbaseline
+pre-sort
+first-word
+phrase-only
Figure 1: Translation performance in BLEU [%] on the
newstest2009 set vs. speed on a logarithmic scale.
We compare Moses with our approach without LM look-
ahead and LM score pre-sorting (baseline), with added
LM pre-sorting and with either first-word or phrase-only
LM look-ahead on top of +pre-sort. Observation his-
togram size is fixed to No = 100 for both decoders.
which performs pre-sorting with respect to the trans-
lation model scores only. The number of hypothesis
expansions is reduced by ?20% and the number of
LM lookups by ?50%. When observation pruning
is applied, we additionally observe a small increase
by 0.2% in BLEU.
Application of first-word LM look-ahead further
reduces the number of LM lookups by 23%, result-
ing in doubled translation speed, part of which de-
rives from fewer trie node searches. The heuristic
phrase-only LM look-ahead method introduces ad-
ditional search errors, resulting in a BLEU drop by
0.3%, but yields another 85% reduction in LM com-
putations and increases throughput by a factor of 2.2.
3.3 Performance evaluation
In this section we evaluate the proposed extensions
to the original beam search algorithm in terms of
scalability and their usefulness for different appli-
cation constraints. We compare Moses and four dif-
ferent setups of our decoder: LM score pre-sorting
switched on or off without LM look-ahead and both
LM look-ahead methods with LM score pre-sorting.
We translated the test set with the beam sizes set to
Nc = Nl = {1,2,4,8,16,24,32,48,64}. For Moses
we used the beam sizes 2i, i ? {1, . . . ,9}. Transla-
30
setup system WMT 2011 German?English IWSLT 2011 English?French
beam size speed BLEU TER beam size speed BLEU TER
(Nc,Nl) w/s [%] [%] (Nc,Nl) w/s [%] [%]
best Moses 256 0.7 20.2 63.2 16 10 29.5 52.8
this work: first-word (48,48) 1.1 20.2 63.3 (8,8) 23 29.5 52.9
phrase-only (64,64) 1.4 20.1 63.2 (16,16) 18 29.5 52.8
BLEU: Moses 16 12 19.6 63.7 4 40 29.1 53.2
? -1% this work: first-word (4,4) 67 20.0 63.2 (2,2) 165 29.1 53.1
phrase-only (8,8) 69 19.8 63.0 (4,4) 258 29.3 52.9
BLEU: Moses 8 25 19.1 64.2 2 66 28.1 54.3
? -2% this work: first-word (2,2) 233 19.5 63.4 (1,1) 525 28.4 53.9
phrase-only (4,4) 280 19.3 63.0 (2,2) 771 28.5 53.2
fastest Moses 1 126 15.6 68.3 1 116 26.7 55.9
this work: first-word (1,1) 444 18.4 64.6 (1,1) 525 28.4 53.9
phrase-only (1,1) 2.8K 16.8 64.4 (1,1) 2.2K 26.4 54.7
Table 2: Comparison of Moses with this work. Either first-word or phrase-only LM look-ahead is applied. We consider
both the best and the fastest possible translation, as well as the fastest settings resulting in no more than 1% and 2%
BLEU loss on the development set. Results are given on the test set (newstest2009).
tion performance in BLEU is plotted against speed
in Figure 1. Without the proposed extensions, Moses
slightly outperforms our decoder in terms of BLEU.
However, the latter already scales better for higher
speed. With LM score pre-sorting, the best BLEU
value is similar to Moses while further accelerat-
ing translation, yielding identical performance at 16
words/sec as Moses at 1.8 words/sec. Application
of first-word LM look-ahead shifts the graph to the
right, now reaching the same performance at 31
words/sec. At a fixed translation speed of roughly
70 words/sec, our approach yields 20.0% BLEU,
whereas Moses reaches 17.2%. For phrase-only LM
look-ahead the graph is somewhat flatter. It yields
nearly the same top performance with an even better
trade-off between translation quality and speed.
The final set of experiments is performed on both
the WMT and the IWSLT task. We directly com-
pare our decoder with the two LM look-ahead meth-
ods with Moses in four scenarios: the best possi-
ble translation, the fastest possible translation with-
out performance constraint and the fastest possible
translation with no more than 1% and 2% loss in
BLEU on the dev set compared to the best value.
Table 2 shows that on the WMT data, the top per-
formance is similar for both decoders. However, if
we allow for a small degradation in translation per-
formance, our approaches clearly outperform Moses
in terms of translation speed. With phrase-only LM
look-ahead, our decoder is faster by a factor of 6
for no more than 1% BLEU loss, a factor of 11 for
2% BLEU loss and a factor of 22 in the fastest set-
ting. The results on the IWSLT data are very similar.
Here, the speed difference reaches a factor of 19 in
the fastest setting.
4 Conclusions
This work introduces two extensions to the well-
known beam search algorithm for phrase-based ma-
chine translation. Both pre-sorting the phrase trans-
lation candidates with an LM score estimate and LM
look-ahead during search are shown to have a pos-
itive effect on translation speed. We compare our
decoder to Moses, reaching a similar highest BLEU
score, but clearly outperforming it in terms of scal-
ability with respect to the trade-off ratio between
translation quality and speed. In our experiments,
the fastest settings of our decoder and Moses differ
in translation speed by a factor of 22 on the WMT
data and a factor of 19 on the IWSLT data. Our soft-
ware is part of the open source toolkit Jane.
Acknowledgments
This work was partially realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for innovation.
31
References
[Delaney et al2006] Brian Delaney, Wade Shen, and
Timothy Anderson. 2006. An efficient graph search
decoder for phrase-based statistical machine transla-
tion. In International Workshop on Spoken Language
Translation, Kyoto, Japan, November.
[Heafield2011] Kenneth Heafield. 2011. KenLM: Faster
and Smaller Language Model Queries. In Proceedings
of the 6th Workshop on Statistical Machine Transla-
tion, pages 187?197, Edinburgh, Scotland, UK, July.
[Koehn et al2003] P. Koehn, F. J. Och, and D. Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-03), pages 127?133, Edmonton, Alberta.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondr?ej Bo-
jar, Alexandra Constantine, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Annual Meeting of the Association for
Computational Linguistics (ACL), demonstration ses-
sion, pages 177?180, Prague, Czech Republic, June.
[Moore and Quirk2007] Robert C. Moore and Chris
Quirk. 2007. Faster beam-search decoding for phrasal
statistical machine translation. In Proceedings of MT
Summit XI.
[Ortiz et al2006] Daniel Ortiz, Ismael Garcia-Varea, and
Francisco Casacuberta. 2006. Generalized stack de-
coding algorithms for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation, pages 64?71, New York City, June.
[Ortmanns et al1998] S. Ortmanns, H. Ney, and A. Ei-
den. 1998. Language-model look-ahead for large vo-
cabulary speech recognition. In International Confer-
ence on Spoken Language Processing, pages 2095?
2098, Sydney, Australia, October.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method
for Automatic Evaluation of Machine Translation. In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
[Snover et al2006] Matthew Snover, Bonnie Dorr,
Richard Schwartz, Linnea Micciulla, and John
Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings
of the 7th Conference of the Association for Ma-
chine Translation in the Americas, pages 223?231,
Cambridge, Massachusetts, USA, August.
[Steinbiss et al1994] V. Steinbiss, B. Tran, and Hermann
Ney. 1994. Improvements in Beam Search. In Proc.
of the Int. Conf. on Spoken Language Processing (IC-
SLP?94), pages 2143?2146, September.
[Stolcke2002] Andreas Stolcke. 2002. SRILM ? An
Extensible Language Modeling Toolkit. In Proceed-
ings of the Seventh International Conference on Spoken
Language Processing, pages 901?904. ISCA, Septem-
ber.
[Zens and Ney2008] Richard Zens and Hermann Ney.
2008. Improvements in Dynamic Programming Beam
Search for Phrase-based Statistical Machine Transla-
tion. In International Workshop on Spoken Language
Translation, pages 195?205, Honolulu, Hawaii, Octo-
ber.
32
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 93?97,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2010
Carmen Heger, Joern Wuebker, Matthias Huck, Gregor Leusch,
Saab Mansour, Daniel Stein and Hermann Ney
RWTH Aachen University
Aachen, Germany
surname@cs.rwth-aachen.de
Abstract
In this paper we describe the statisti-
cal machine translation system of the
RWTH Aachen University developed for
the translation task of the Fifth Workshop
on Statistical Machine Translation. State-
of-the-art phrase-based and hierarchical
statistical MT systems are augmented
with appropriate morpho-syntactic en-
hancements, as well as alternative phrase
training methods and extended lexicon
models. For some tasks, a system combi-
nation of the best systems was used to gen-
erate a final hypothesis. We participated
in the constrained condition of German-
English and French-English in each trans-
lation direction.
1 Introduction
This paper describes the statistical MT system
used for our participation in the WMT 2010 shared
translation task. We used it as an opportunity to in-
corporate novel methods which have been investi-
gated at RWTH over the last year and which have
proven to be successful in other evaluations.
For all tasks we used standard alignment and
training tools as well as our in-house phrase-
based and hierarchical statistical MT decoders.
When German was involved, morpho-syntactic
preprocessing was applied. An alternative phrase-
training method and additional models were tested
and investigated with respect to their effect for the
different language pairs. For two of the language
pairs we could improve performance by system
combination.
An overview of the systems and models will fol-
low in Section 2 and 3, which describe the base-
line architecture, followed by descriptions of the
additional system components. Morpho-syntactic
analysis and other preprocessing issues are cov-
ered by Section 4. Finally, translation results for
the different languages and system variants are
presented in Section 5.
2 Translation Systems
For the WMT 2010 Evaluation we used stan-
dard phrase-based and hierarchical translation sys-
tems. Alignments were trained with a variant of
GIZA++. Target language models are 4-gram lan-
guage models trained with the SRI toolkit, using
Kneser-Ney discounting with interpolation.
2.1 Phrase-Based System
Our phrase-based translation system is similar to
the one described in (Zens and Ney, 2008). Phrase
pairs are extracted from a word-aligned bilingual
corpus and their translation probability in both di-
rections is estimated by relative frequencies. Ad-
ditional models include a standard n-gram lan-
guage model, phrase-level IBM1, word-, phrase-
and distortion-penalties and a discriminative re-
ordering model as described in (Zens and Ney,
2006).
2.2 Hierarchical System
Our hierarchical phrase-based system is similar to
the one described in (Chiang, 2007). It allows for
gaps in the phrases by employing a context-free
grammar and a CYK-like parsing during the de-
coding step. It has similar features as the phrase-
based system mentioned above. For some sys-
tems, we only allowed the non-terminals in hierar-
chical phrases to be substituted with initial phrases
as in (Iglesias et al, 2009), which gave better re-
sults on some language pairs. We will refer to this
as ?shallow rules?.
2.3 System Combination
The RWTH approach to MT system combination
of the French?English systems as well as the
German?English systems is a refined version of
the ROVER approach in ASR (Fiscus, 1997) with
93
German?English French?English English?French
BLEU # Phrases BLEU # Phrases BLEU # Phrases
Standard 19.7 128M 25.5 225M 23.7 261M
FA 20.0 12M 25.9 35M 24.0 33M
Table 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA). For
German?English and English?French phrase table interpolation was applied.
additional steps to cope with reordering between
different hypotheses, and to use true casing infor-
mation from the input hypotheses. The basic con-
cept of the approach has been described by Ma-
tusov et al (2006). Several improvements have
been added later (Matusov et al, 2008). This ap-
proach includes an enhanced alignment and re-
ordering framework. Alignments between the sys-
tems are learned by GIZA++, a one-to-one align-
ment is generated from the learned state occupa-
tion probabilities.
From these alignments, a confusion network
(CN) is then built using one of the hypotheses as
?skeleton? or ?primary? hypothesis. We do not
make a hard decision on which of the hypothe-
ses to use for that, but instead combine all pos-
sible CNs into a single lattice. Majority voting on
the generated lattice is performed using the prior
probabilities for each system as well as other sta-
tistical models such as a special trigram language
model. This language model is also learned on
the input hypotheses. The intention is to favor
longer phrases contained in individual hypotheses.
The translation with the best total score within this
lattice is selected as consensus translation. Scal-
ing factors of these models are optimized similar
to MERT using the Downhill Simplex algorithm.
As the objective function for this optimization, we
selected a linear combination of BLEU and TER
with a weight of 2 on the former; a combination
that has proven to deliver stable results on sev-
eral MT evaluation measures in preceding experi-
ments.
In contrast to previous years, we now include a
separate consensus true casing step to exploit the
true casing capabilities of some of the input sys-
tems: After generating a (lower cased) consensus
translation from the CN, we sum up the counts of
different casing variants of each word in a sen-
tence over the input hypotheses, and use the ma-
jority casing over those. In previous experiments,
this showed to work significantly better than us-
ing a fixed non-consensus true caser, and main-
tains flexibility on the input systems.
3 New Additional Models
3.1 Forced Alignment
For the German?English, French?English and
English?French language tasks we applied a
forced alignment procedure to train the phrase
translation model with the EM algorithm, sim-
ilar to the one described in (DeNero et al,
2006). Here, the phrase translation probabil-
ities are estimated from their relative frequen-
cies in the phrase-aligned training data. The
phrase alignment is produced by a modified
version of the translation decoder. In addi-
tion to providing a statistically well-founded
phrase model, this has the benefit of produc-
ing smaller phrase tables and thus allowing
more rapid experiments. For the language pairs
German?English and English?French the best
results were achieved by log-linear interpolation
of the standard phrase table with the generative
model. For French?English we directly used the
model trained by forced alignment. A detailed
description of the training procedure is given in
(Wuebker et al, 2010). Table 1 shows the system
performances and phrase table sizes with the stan-
dard phrase table and the one trained with forced
alignment after the first EM iteration. We can see
that the generative model reduces the phrase table
size by 85-90% while increasing performance by
0.3% to 0.4% BLEU.
3.2 Extended Lexicon Models
In previous work, RWTH was able to show the
positive impact of extended lexicon models that
cope with lexical context beyond the limited hori-
zon of phrase pairs and n-gram language models.
Mauser et al (2009) report improvements of
up to +1% in BLEU on large-scale systems for
Chinese?English and Arabic?English by incor-
porating discriminative and trigger-based lexicon
models into a state-of-the-art phrase-based de-
coder. They discuss how the two types of lexicon
94
models help to select content words by capturing
long-distance effects.
The triplet model is a straightforward extension
of the IBM model 1 with a second trigger, and like
the former is trained iteratively using the EM al-
gorithm. In search, the triggers are usually on the
source side, i.e., p(e|f, f ?) is modeled. The path-
constrained triplet model restricts the first source
trigger to the aligned target word, whereas the sec-
ond trigger can move along the whole source sen-
tence. See (Hasan et al, 2008) for a detailed de-
scription and variants of the model and its training.
For the WMT 2010 evaluation, triplets mod-
eling p(e|f, f ?) were trained and applied di-
rectly in search for all relevant language pairs.
Path-constrained models were trained on the in-
domain news-commentary data only and on the
news-commentary plus the Europarl data. Al-
though experience from similar setups indicates
that triplet lexicon models can be beneficial for
machine translation between the languages En-
glish, French, and German, on this year?s WMT
translation tasks slight improvements on the devel-
opment sets did not or only partially carry over to
the held-out test sets. Nevertheless, systems with
triplets were used for system combination, as ex-
tended lexicon models often help to predict con-
tent words and to capture long-range dependen-
cies. Thus they can help to find a strong consensus
hypothesis.
3.3 Unsupervised Training
Due to the small size of the English?German re-
sources available for language modeling as well as
for lexicon extraction, we decided to apply the un-
supervised adaptation suggested in (Schwenk and
Senellart, 2009). We use a baseline SMT system to
translate in-domain monolingual source data, fil-
ter the translations according to a decoder score
normalized by sentence length, add this synthetic
bilingual data to the original one and rebuild the
SMT system from scratch.
The motivation behind the method is that the
phrase table will adapt to the genre, and thus
let phrases which are domain related have higher
probabilities. Two phenomena are observed from
phrase tables and the corresponding translations:
? Phrase translation probabilities are changed,
making the system choose better phrase
translation candidates.
Running Words
English German
Bilingual 44.3M 43.4M
Dict. 1.4M 1.2M
AFP 610.7M
AFP unsup. 152.0M 157.3M
Table 2: Overview on data for unsupervised train-
ing.
BLEU
Dev Test
baseline 15.0 14.7
+dict. 15.1 14.6
+unsup.+dict 15.4 14.9
Table 3: Results for unsupervised training method.
? Phrases which appear repeatedly in the do-
main get higher probabilities, so that the de-
coder can better segment the sentence.
To implement this idea, we translate the AFP part
of the English LDC Gigaword v4.0 and obtain the
synthetic data.
To decrease the number of OOV words, we use
dictionaries from the stardict directory as addi-
tional bilingual data to translate the AFP corpus.
We filter sentences with OOV words and sentences
longer than 100 tokens. A summary of the addi-
tional data used is shown in Table 2.
We tried to use the best 10%, 20% and 40% of
the synthetic data, where the 40% option worked
best. A summary of the results is given in Table 3.
Although this is our best result for the
English?German task, it was not submitted, be-
cause the use of the dictionary is not allowed in
the constrained track.
4 Preprocessing
4.1 Large Parallel Data
In addition to the provided parallel Europarl and
news-commentary corpora, also the large French-
English news corpus (about 22.5 Mio. sentence
pairs) and the French-English UN corpus (about
7.2 Mio. sentence pairs) were available. Since
model training and tuning with such large cor-
pora takes a very long time, we extracted about
2 Mio. sentence pairs of both of these corpora. We
filter sentences with the following properties:
95
? Only sentences of minimum length of 4 to-
kens were considered.
? At least 92% of the vocabulary of each sen-
tence occur in the development set.
? The ratio of the vocabulary size of a sen-
tence and the number of its tokens is mini-
mum 80%.
4.2 Morpho-Syntactic Analysis
German, as a flexible and morphologically rich
language, raises a couple of problems in machine
translation. We picked two major problems and
tackled them with morpho-syntactic pre- and post-
processing: compound splitting and long-range
verb reordering.
For the translation from German into English,
German compound words were split using the
frequency-based method described in (Koehn and
Knight, 2003). Thereby, we forbid certain words
and syllables to be split. For the other trans-
lation direction, the English text was first trans-
lated into the modified German language with
split compounds. The generated output was then
postprocessed by re-merging the previously gen-
erated components using the method described in
(Popovic? et al, 2006).
Additionally, for the German?English phrase-
based system, the long-range POS-based reorder-
ing rules described in (Popovic? and Ney, 2006)
were applied on the training and test corpora as a
preprocessing step. Thereby, German verbs which
occur at the end of a clause, like infinitives and
past participles, are moved towards the beginning
of that clause. With this, we improved our baseline
phrase-based system by 0.6% BLEU.
5 Experimental Results
For all translation directions, we used the provided
parallel corpora (Europarl, news) to train the trans-
lation models and the monolingual corpora to train
BLEU
Dev Test
phrase-based baseline 19.9 19.2
phrase-based (+POS+mero+giga) 21.0 20.3
hierarchical baseline 20.2 19.6
hierarchical (+giga) 20.5 20.1
system combination 21.4 20.4
Table 4: Results for the German?English task.
the language models. We improved the French-
English systems by enriching the data with parts of
the large addional data, extracted with the method
described in Section 4.1. Depending on the sys-
tem this gave an improvement of 0.2-0.7% BLEU.
We also made use of the large giga-news as well
as the LDC Gigaword corpora for the French and
English language models. All systems were opti-
mized for BLEU score on the development data,
newstest2008. The newstest2009 data is
used as a blind test set.
In the following, we will give the BLEU scores
for all language tasks of the baseline system and
the best setup for both, the phrase-based and the
hierarchical system. We will use the following
notations to indicate the several methods we used:
(+POS) POS-based verb reordering
(+mero) maximum entropy reordering
(+giga) including giga-news and
LDC Gigaword in LM
(fa) trained by forced alignment
(shallow) allow only shallow rules
We applied system combination of up to 6 sys-
tems with several setups. The submitted systems
are marked in tables 4-7.
6 Conclusion
For the participation in the WMT 2010 shared
translation task, RWTH used state-of-the-art
phrase-based and hierarchical translation systems.
To deal with the rich morphology and word or-
der differences in German, compound splitting
and long range verb reordering were applied in a
preprocessing step. For the French-English lan-
guage pairs, RWTH extracted parts of the large
news corpus and the UN corpus as additional
training data. Further, training the phrase trans-
lation model with forced alignment yielded im-
provements in BLEU. To obtain the final hypothe-
sis for the French?English and German?English
BLEU
Dev Test
phrase-based baseline 14.8 14.5
phrase-based (+mero) 15.0 14.7
hierarchical baseline 14.2 13.9
hierarchical (shallow) 14.5 14.3
Table 5: Results for the English?German task.
96
BLEU
Dev Test
phrase-based baseline 21.8 25.1
phrase-based (fa+giga) 23.0 26.1
hierarchical baseline 21.9 25.0
hierarchical (shallow+giga) 22.7 25.6
system combination 23.1 26.1
Table 6: Results for the French?English task.
BLEU
Dev Test
phrase-based baseline 20.9 23.2
phrase-based (fa+mero+giga) 23.0 24.6
hierarchical baseline 20.6 22.5
hierarchical (shallow,+giga) 22.4 24.3
Table 7: Results for the English?French task.
language pairs, RWTH applied system combina-
tion. Altogether, by application of these meth-
ods RWTH was able to increase performance in
BLEU by 0.8% for German?English, 0.2% for
English?German, 1.0% for French?English and
1.4% for English?French on the test set over the
respective baseline systems.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Sur-
face Heuristics. In Proceedings of the Workshop on
Statistical Machine Translation, pages 31?38.
J.G. Fiscus. 1997. A Post-Processing System to Yield
Reduced Word Error Rates: Recognizer Output Vot-
ing Error Reduction (ROVER). In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-
Ferrer. 2008. Triplet Lexicon Models for Statisti-
cal Machine Translation. In Proceedings of Emperi-
cal Methods of Natural Language Processing, pages
372?381.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Rule Filtering by Pattern for Efficient Hierar-
chical Translation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 380?388.
P. Koehn and K. Knight. 2003. Empirical Methods for
Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
A. Mauser, S. Hasan, and H. Ney. 2009. Extend-
ing Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 210?217.
M. Popovic? and H. Ney. 2006. POS-based Word Re-
orderings for Statistical Machine Translation. In In-
ternational Conference on Language Resources and
Evaluation, pages 1278?1283.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
H. Schwenk and J. Senellart. 2009. Translation Model
Adaptation for an Arabic/French News Translation
System by Lightly-Supervised Training. In MT
Summit XII.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics. To ap-
pear.
R. Zens and H. Ney. 2006. Discriminative Reorder-
ing Models for Statistical Machine Translation. In
Workshop on Statistical Machine Translation, pages
55?63.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statis-
tical Machine Translation. In International Work-
shop on Spoken Language Translation.
97
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358?364,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joint WMT Submission of the QUAERO Project
?Markus Freitag, ?Gregor Leusch, ?Joern Wuebker, ?Stephan Peitz, ?Hermann Ney,
?Teresa Herrmann, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Gilles Adda,?Josep Maria Crego,
?Bianka Buschbeck, ?Tonio Wandmacher, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2011 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems. Then RWTH system combination
combines these translations to a better one. In
this paper, we describe the single systems of
each group. Before we present the results of
the system combination, we give a short de-
scription of the RWTH Aachen system com-
bination approach.
1 Overview
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
1.1 Data Sets
For WMT 2011 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned on
the newstest2009 dev set. The newstest2008 dev set
was used to train the system combination parame-
ters. Finally the newstest2010 dev set was used to
compare the results of the different system combi-
nation approaches and settings.
2 Translation Systems
2.1 RWTH Aachen Single Systems
For the WMT 2011 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
bilingual corpus, the translation probabilities are es-
timated by relative frequencies. The standard feature
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. Parameters are optimized with the Downhill-
Simplex algorithm (Nelder and Mead, 1965) on the
word graph.
358
2.1.2 Hierarchical System
For the hierarchical setups described in this pa-
per, the open source Jane toolkit (Vilar et al, 2010)
is employed. Jane has been developed at RWTH
and implements the hierarchical approach as intro-
duced by Chiang (2007) with some state-of-the-art
extensions. In hierarchical phrase-based translation,
a weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The model weights are optimized
with standard MERT (Och, 2003) on 100-best lists.
2.1.3 Phrase Model Training
For some PBT systems a forced alignment pro-
cedure was applied to train the phrase translation
model as described in Wuebker et al (2010). A
modified version of the translation decoder is used
to produce a phrase alignment on the bilingual train-
ing data. The phrase translation probabilities are es-
timated from their relative frequencies in the phrase-
aligned training data. In addition to providing a sta-
tistically well-founded phrase model, this has the
benefit of producing smaller phrase tables and thus
allowing more rapid and less memory consuming
experiments with a better translation quality.
2.1.4 Final Systems
For the German?English task, RWTH conducted
experiments comparing the standard phrase extrac-
tion with the phrase training technique described in
Section 2.1.3. Further experiments included the use
of additional language model training data, rerank-
ing of n-best lists generated by the phrase-based sys-
tem, and different optimization criteria.
A considerable increase in translation quality can
be achieved by application of German compound
splitting (Koehn and Knight, 2003). In comparison
to standard heuristic phrase extraction techniques,
performing force alignment phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006),
sentence length model, a 6-gram LM and IBM-1 lex-
icon models in both normal and inverse direction.
These models are combined in a log-linear fashion
and the scaling factors are tuned in the same man-
ner as the baseline system (using TER?4BLEU on
newstest2009).
The final table includes two identical Jane sys-
tems which are optimized on different criteria. The
one optimized on TER?BLEU yields a much lower
TER.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogeneous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation. Op-
timization with regard to the BLEU score is done
using Minimum Error Rate Training as described
by Venugopal et al (2005). The translation model
is trained on the Europarl and News Commentary
Corpus and the phrase table is based on a GIZA++
Word Alignment. We use two 4-gram SRI language
models, one trained on the News Shuffle corpus and
one trained on the Gigaword corpus. Reordering is
performed based on continuous and non-continuous
POS rules to cover short and long-range reorder-
ings. The long-range reordering rules were also ap-
plied to the training corpus and phrase extraction
was performed on the resulting reordering lattices.
Part-of-speech tags are obtained using the TreeTag-
1http://hunspell.sourceforge.net/
359
ger (Schmid, 1994). In addition, the system applies
a bilingual language model to extend the context of
source language words available for translation. The
individual models are described briefly in the fol-
lowing.
2.2.3 POS-based Reordering Model
We use a reordering model that is based on parts-
of-speech (POS) and learn probabilistic rules from
the POS tags of the words in the training corpus and
the alignment information. In addition to continu-
ous reordering rules that model short-range reorder-
ing (Rottmann and Vogel, 2007), we apply non-
continuous rules to address long-range reorderings
as typical for German-English translation (Niehues
and Kolss, 2009). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are encoded
in a word lattice which is used as input to the de-
coder.
2.2.4 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract also phrase pairs for origi-
nally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths. To limit the number of extracted
phrase pairs, we extract a source phrase only once
per sentence, even if it is found in different paths and
we only use long-range reordering rules to generate
the lattices for the training corpus.
2.2.5 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder during the search pro-
cess. This segmentation into phrases leads to the
loss of context information at the phrase boundaries.
The language model can make use of more target
side context. To make also source language context
available we use a bilingual language model, an ad-
ditional language model in the phrase-based system
in which each token consist of a target word and all
source words it is aligned to. The bilingual tokens
enter the translation process as an additional target
factor.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
The LIMSI system is built with n-code2, an open
source statistical machine translation system based
on bilingual n-grams.
2.3.2 n-code Overview
In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training this
model requires to reorder source sentences so as to
match the target word order. This is performed by a
stochastic finite-state reordering model, which uses
part-of-speech information3 to generalize reordering
patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a weak
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
use in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 data as development
set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2http://www.limsi.fr/Individu/jmcrego/n-code
3Part-of-speech information for English and German is com-
puted using the TreeTagger.
360
2.3.3 Data Preprocessing
Based on previous experiments which have
demonstrated that better normalization tools provide
better BLEU scores (K. Papineni and Zhu, 2002),
all the English texts are tokenized and detokenized
with in-house text processing tools (De?chelotte et
al., 2008). For German, the standard tokenizer sup-
plied by evaluation organizers is used.
2.3.4 Target n-gram Language Models
The English language model is trained assuming
that the test set consists in a selection of news texts
dating from the end of 2010 to the beginning of
2011. This assumption is based on what was done
for the 2010 evaluation. Thus, a development cor-
pus is built in order to create a vocabulary and to
optimize the target language model.
Development Set and Vocabulary In order to
cover different period, two development sets are
used. The first one is newstest2008. However, this
corpus is two years older than the targeted time pe-
riod. Thus a second development corpus is gath-
ered by randomly sampling bunches of 5 consecu-
tive sentences from the provided news data of 2010
and 2011.
To estimate a LM, the English vocabulary is first
defined by including all tokens observed in the Eu-
roparl and news-commentary corpora. This vocabu-
lary is then expanded with all words that occur more
that 5 times in the French-English giga-corpus, and
with the most frequent proper names taken from the
monolingual news data of 2010 and 2011. This pro-
cedure results in a vocabulary around 500k words.
Language Model Training All the training data
allowed in the constrained task are divided into 9
sets based on dates on genres. On each set, a
standard 4-gram LM is estimated from the 500k
word vocabulary with in-house tools using abso-
lute discounting interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 are first linearly interpolated.
The associated coefficients are estimated so as to
minimize the perplexity evaluated on the dev2010-
2011. The resulting LM and the 2010-2011 LM are
finally interpolated with newstest2008 as develop-
ment data. This two steps interpolation aims to avoid
an overestimate of the weight associated to the 2010-
2011 LM.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
the SYSTRAN baseline system in combination with
a statistical post editing (SPE) component.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k ?
800k entries per language pair).
The basic setup of the SPE component is identi-
cal to the one described in (L. Dugast and Koehn,
2007). A statistical translation model is trained on
the rule-based translation of the source and the tar-
get side of the parallel corpus. This is done sepa-
rately for each parallel corpus. Language models are
trained on each target half of the parallel corpora and
also on additional in-domain corpora. Moreover, the
following measures ? limiting unwanted statistical
effects ? were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is signif-
icantly reduced. In addition, entity translation
is handled more reliably by the rule-based en-
gine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus (whose target is identical
to the source). This was added to the parallel
text in order to improve word alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
361
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained 15M
phrases from the news/europarl corpora, provided
as training data for WMT 2011. Weights for these
separate models were tuned by the MERT algorithm
provided in the Moses toolkit (P. Koehn et al, 2007),
using the provided news development set.
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical mod-
els is selected as consensus translation. A deeper
description will be also given in the WMT11 sys-
tem combination paper of RWTH Aachen Univer-
sity. For this task only the A2L framework has been
used.
4 Experiments
We tried different system combinations with differ-
ent sets of single systems and different optimiza-
tion criteria. As RWTH has two different transla-
tion systems, we put the output of both systems into
system combination. Although both systems have
the same preprocessing, their hypotheses differ. Fi-
nally, we added for both RWTH systems two addi-
tional hypotheses to the system combination. The
two hypotheses of Jane were optimized on differ-
ent criteria. The first hypothesis was optimized on
BLEU and the second one on TER?BLEU. The first
RWTH phrase-based hypothesis was generated with
force alignment, the second RWTH phrase-based
hypothesis is a reranked version of the first one as
described in 2.1.4. Compared to the other systems,
the system by SYSTRAN has a completely different
approach (see section 2.4). It is mainly based on a
rule-based system. For the German?English pair,
SYSTRAN achieves a lower BLEU score in each
test set compared to the other groups. But since the
SYSTRAN system is very different to the others, we
still obtain an improvement when we add it also to
system combination.
We obtain the best result from system combina-
tion of all seven systems, optimizing the parameters
on BLEU. This system was the system we submitted
to the WMT 2011 evaluation.
For each dev set we obtain an improvement com-
pared to the best single systems. For newstest2008
and newstest2009 we get an improvement of 0.5
points in BLEU and 1.8 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. For newstest2010 we get an improvement
of 1.8 points in BLEU and 2.7 points in TER com-
pared to the best single system of RWTH. The sys-
tem combination weights optimized for the best run
are listed in Table 2. We see that although the single
system of SYSTRAN has the lowest BLEU scores,
it gets the second highest system weight. This high
value shows the influence of a completely different
system. On the other hand, all RWTH systems are
very similar, because of their same preprocessing
and their small variations. Therefor the system com-
bination parameter of all four systems by themselves
are relatively small. The summarized ?RWTH ap-
proach? system weight, though, is again on par with
the other systems.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU
compared to each single system. If the system
combination implementation can handle enough sin-
gle systems we would recommend to add all single
systems to the system combination. Although the
single system of SYSTRAN has the lowest BLEU
scores and the RWTH single systems are similar we
achieved the best result in using all single systems.
362
newstest2008 newstest2009 newstest2010 description
BLEU TER BLEU TER BLEU TER
22.73 60.73 22.50 59.82 25.26 57.37 sc (all systems) BLEU opt
22.61 60.60 22.28 59.39 25.07 56.95 sc (all systems - (1)) TER?BLEU opt
22.50 60.41 22.52 59.61 25.23 57.40 sc (all systems) TER?BLEU opt
22.19 60.09 22.05 59.31 24.74 56.89 sc (all systems - (4)) TER?BLEU opt
22.21 60.71 21.89 59.95 24.72 57.58 sc (all systems - (4,7)) TER?BLEU opt
22.22 60.45 21.79 59.72 24.32 57.59 sc (all systems - (3,4)) TER?BLEU opt
22.27 60.60 21.75 59.92 24.35 57.64 sc (all systems - (3,4)) BLEU opt
22.10 62.59 22.01 61.64 23.34 60.35 (1) Karlsruhe Institute of Technology
21.41 62.77 21.12 61.91 23.44 60.06 (2) RWTH PBT (FA) rerank +GW
21.11 62.96 21.06 62.16 23.29 60.26 (3) RWTH PBT (FA)
21.47 63.89 21.00 63.33 22.93 61.71 (4) RWTH jane + GW BLEU opt
20.89 61.05 20.36 60.47 23.42 58.31 (5) RWTH jane + GW TER?BLEU opt
20.33 64.50 19.79 64.91 21.97 61.44 (6) Limsi-CNRS
17.06 69.48 17.52 67.34 18.68 66.37 (7) SYSTRAN Software
Table 1: All systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results are in
percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model. sc denotes
system combination.
system weight
Karlsruhe Institute of Technology 0.350
RWTH PBT (FA) rerank +GW 0.001
RWTH PBT (FA) 0.046
RWTH jane + GW BLEU opt 0.023
RWTH jane + GW TER?BLEU opt 0.034
Limsi-CNRS 0.219
SYSTRAN Software 0.328
Table 2: Optimized systems weights for each system of the best system combination result.
Acknowledgments
This work was achieved as part of the QUAERO
Programme, funded by OSEO, French State agency
for innovation.
References
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
S.F. Chen and J.T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
T. Ward K. Papineni, S. Roukos and W. Zhu. 2002. Bleu:
363
a method for automatic evaluation of machine transla-
tion. In ACL ?02: Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, pages
311?318. Association for Computational Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
364
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 405?412,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2011
Matthias Huck, Joern Wuebker, Christoph Schmidt, Markus Freitag, Stephan Peitz,
Daniel Stein, Arnaud Dagnelies, Saab Mansour, Gregor Leusch and Hermann Ney
RWTH Aachen University
Aachen, Germany
surname@cs.rwth-aachen.de
Abstract
This paper describes the statistical machine
translation (SMT) systems developed by
RWTH Aachen University for the translation
task of the EMNLP 2011 Sixth Workshop on
Statistical Machine Translation. Both phrase-
based and hierarchical SMT systems were
trained for the constrained German-English
and French-English tasks in all directions. Ex-
periments were conducted to compare differ-
ent training data sets, training methods and op-
timization criteria, as well as additional mod-
els on dependency structure and phrase re-
ordering. Further, we applied a system com-
bination technique to create a consensus hy-
pothesis from several different systems.
1 Overview
We sketch the baseline architecture of RWTH?s se-
tups for the WMT 2011 shared translation task by
providing an overview of our translation systems in
Section 2. In addition to the baseline features, we
adopted several novel methods, which will be pre-
sented in Section 3. Details on the respective se-
tups and translation results for the French-English
and German-English language pairs (in both trans-
lation directions) are given in Sections 4 and 5. We
finally conclude the paper in Section 6.
2 Translation Systems
For the WMT 2011 evaluation we utilized RWTH?s
state-of-the-art phrase-based and hierarchical trans-
lation systems as well as our in-house system com-
bination framework. GIZA++ (Och and Ney, 2003)
was employed to train word alignments, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
2.1 Phrase-Based System
We applied a phrase-based translation (PBT) system
similar to the one described in (Zens and Ney, 2008).
Phrase pairs are extracted from a word-aligned bilin-
gual corpus and their translation probability in both
directions is estimated by relative frequencies. The
standard feature set moreover includes an n-gram
language model, phrase-level single-word lexicons
and word-, phrase- and distortion-penalties. To lexi-
calize reordering, a discriminative reordering model
(Zens and Ney, 2006a) is used. Parameters are opti-
mized with the Downhill-Simplex algorithm (Nelder
and Mead, 1965) on the word graph.
2.2 Hierarchical System
For the hierarchical setups described in this paper,
the open source Jane toolkit (Vilar et al, 2010) was
employed. Jane has been developed at RWTH and
implements the hierarchical approach as introduced
by Chiang (2007) with some state-of-the-art exten-
sions. In hierarchical phrase-based translation, a
weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The standard models integrated into
our Jane systems are: phrase translation probabil-
ities and lexical translation probabilities on phrase
level, each for both translation directions, length
405
penalties on word and phrase level, three binary fea-
tures marking hierarchical phrases, glue rule, and
rules with non-terminals at the boundaries, source-
to-target and target-to-source phrase length ratios,
four binary count features and an n-gram language
model. The model weights are optimized with stan-
dard MERT (Och, 2003) on 100-best lists.
2.3 System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (Matusov et al, 2006;
Matusov et al, 2008). This approach includes an
enhanced alignment and reordering framework. A
lattice is built from the input hypotheses. The trans-
lation with the best score within the lattice according
to a couple of statistical models is selected as con-
sensus translation.
3 Translation Modeling
We incorporated several novel methods into our sys-
tems for the WMT 2011 evaluation. This section
provides a short survey of three of the methods
which we suppose to be of particular interest.
3.1 Language Model Data Selection
For the English and German language models,
we applied the data selection method proposed in
(Moore and Lewis, 2010). Each sentence is scored
by the difference in cross-entropy between a lan-
guage model trained from in-domain data and a lan-
guage model trained from a similar-sized sample of
the out-of-domain data. As in-domain data we used
the news-commentary corpus. The out-of-domain
data from which the data was selected are the news
crawl corpus for both languages and for English the
109 corpus and the LDC Gigaword data. We used a
3-gram trained with the SRI toolkit to compute the
cross-entropy. For the news crawl corpus, only 1/8
of the sentences were discarded. Of the 109 corpus
we retained 1/2 and of the LDC Gigaword data we
retained 1/4 of the sentences to train the language
models.
3.2 Phrase Model Training
For the German?English and French?English
translation tasks we applied a forced alignment pro-
cedure to train the phrase translation model with the
EM algorithm, similar to the one described in (DeN-
ero et al, 2006). Here, the phrase translation prob-
abilities are estimated from their relative frequen-
cies in the phrase-aligned training data. The phrase
alignment is produced by a modified version of the
translation decoder. In addition to providing a statis-
tically well-founded phrase model, this has the ben-
efit of producing smaller phrase tables and thus al-
lowing more rapid experiments. A detailed descrip-
tion of the training procedure is given in (Wuebker
et al, 2010).
3.3 Soft String-to-Dependency
Given a dependency tree of the target language,
we are able to introduce language models that span
over longer distances than the usual n-grams, as in
(Shen et al, 2008). To obtain dependency structures,
we apply the Stanford parser (Klein and Manning,
2003) on the target side of the training material.
RWTH?s open source hierarchical translation toolkit
Jane has been extended to include dependency infor-
mation in the phrase table and to build dependency
trees on the output hypotheses at decoding time from
this information.
Shen et al (2008) use only phrases that meet cer-
tain restrictions. The first possibility is what the au-
thors call a fixed dependency structure. With the
exception of one word within this phrase, called
the head, no outside word may have a dependency
within this phrase. Also, all inner words may only
depend on each other or on the head. For a second
structure, called a floating dependency structure, the
head dependency word may also exist outside the
phrase. If the dependency structure of a phrase con-
forms to these restrictions, it is denoted as valid.
In our phrase table, we mark those phrases that
possess a valid dependency structure with a binary
feature, but all phrases are retained as translation op-
tions. In addition to storing the dependency informa-
tion, we also memorize for all hierarchical phrases
if the content of gaps has been dependent on the left
or on the right side. We utilize the dependency in-
formation during the search process by adding three
406
French English
Sentences 3 710 985
Running Words 98 352 916 87 689 253
Vocabulary 179 548 216 765
Table 1: Corpus statistics of the preprocessed high-
quality training data (Europarl, news-commentary, and
selected parts of the 109 and UN corpora) for the
RWTH systems for the WMT 2011 French?English and
English?French translation tasks. Numerical quantities
are replaced by a single category symbol.
features to the log-linear model: merging errors to
the left, merging errors to the right, and the ratio of
valid vs. non-valid dependency structures. The de-
coder computes the corresponding costs when it tries
to construct a dependency tree of a (partial) hypothe-
sis on-the-fly by merging the dependency structures
of the used phrase pairs.
In an n-best reranking step, we compute depen-
dency language model scores on the dependencies
which were assembled on the hypotheses by the
search procedure. We apply one language model
for left-side dependencies and one for right-side de-
pendencies. For head structures, we also compute
their scores by exploiting a simple unigram language
model. We furthermore include a language count
feature that is incremented each time we compute
a dependency language model score. As trees with
few dependencies have less individual costs to be
computed, they tend to obtain lower overall costs
than trees with more complex structures in other
sentences. The intention behind this feature is thus
comparable to the word penalty in combination with
a normal n-gram language model.
4 French-English Setups
We set up both hierarchical and standard phrase-
based systems for the constrained condition of the
WMT 2011 French?English and English?French
translation tasks. The English?French RWTH pri-
mary submission was produced with a single hierar-
chical system, while a system combination of three
systems was used to generate a final hypothesis for
the French?English primary submission.
Besides the Europarl and news-commentary cor-
pora, the provided parallel data also comprehends
French English
Sentences 29 996 228
Running Words 916 347 538 778 544 843
Vocabulary 1 568 089 1 585 093
Table 2: Corpus statistics of the preprocessed full training
data for the RWTH primary system for the WMT 2011
English?French translation task. Numerical quantities
are replaced by a single category symbol.
the large French-English 109 corpus and the French-
English UN corpus. Since model training with
such a huge amount of data requires a consider-
able computational effort, RWTH decided to select
a high-quality part of altogether about 2 Mio. sen-
tence pairs from the latter two corpora. The selec-
tion of parallel sentences was carried out according
to three criteria: (1) Only sentences of minimum
length of 4 tokens are considered, (2) at least 92%
of the vocabulary of each sentence occurs in new-
stest2008, and (3) the ratio of the vocabulary size
of a sentence and the number of its tokens is mini-
mum 80%. Word alignments in both directions were
trained with GIZA++ and symmetrized according to
the refined method that was proposed in (Och and
Ney, 2003). The phrase tables of the translation
systems are extracted from the Europarl and news-
commentary parallel training data as well as the se-
lected high-quality parts the 109 and UN corpora
only. The only exception is the hierarchical system
used for the English?French RWTH primary sub-
mission which comprehends a second phrase table
with lexical (i.e. non-hierarchical) phrases extracted
from the full parallel data (approximately 30 Mio.
sentence pairs).
Detailed statistics of the high-quality parallel
training data (Europarl, news-commentary, and the
selected parts of the 109 and UN corpora) are given
in Table 1, the corpus statistics of the full parallel
data from which the second phrase table with lexi-
cal phrases for the English?French RWTH primary
system was created are presented in Table 2.
The translation systems use large 4-gram lan-
guage models with modified Kneser-Ney smooth-
ing. The French language model was trained on
most of the provided French data including the
monolingual LDC Gigaword corpora, the English
407
newstest2009 newstest2010
French?English BLEU TER BLEU TER
System combination of ? systems (primary) 26.7 56.0 27.4 54.9
PBT with triplet lexicon, no forced alignment (contrastive) ? 26.2 56.7 27.2 55.3
Jane as below + improved LM (contrastive) 26.3 57.4 26.7 56.2
Jane with parse match + syntactic labels + dependency ? 26.2 57.5 26.5 56.4
PBT with forced alignment phrase training ? 26.0 57.1 26.3 56.0
Table 3: RWTH systems for the WMT 2011 French?English translation task (truecase). BLEU and TER results are
in percentage.
newstest2009 newstest2010
English?French BLEU TER BLEU TER
Jane shallow + in-domain TM + lexical phrases from full data 25.3 60.1 27.1 57.2
Jane shallow + in-domain TM + triplets + DWL + parse match 24.8 60.5 26.6 57.5
PBT with triplets, DWL, sentence-level word lexicon, discrim. reord. 24.8 60.1 26.5 57.3
Table 4: RWTH systems for the WMT 2011 English?French translation task (truecase). BLEU and TER results are
in percentage.
language model was trained on automatically se-
lected English data (cf. Section 3.1) from the pro-
vided resources including the 109 corpus and LDC
Gigaword.
The scaling factors of the log-linear model com-
bination are optimized towards BLEU on new-
stest2009, newstest2010 is used as an unseen test set.
4.1 Experimental Results French?English
The results for the French?English task are given in
Table 3. RWTH?s three submissions ? one primary
and two contrastive ? are labeled accordingly in the
table. The first contrastive submission is a phrase-
based system with a standard feature set plus an ad-
ditional triplet lexicon model (Mauser et al, 2009).
The triplet lexicon model was trained on in-domain
news commentary data only. The second contrastive
submission is a hierarchical Jane system with three
syntax-based extensions: A parse match model (Vi-
lar et al, 2008), soft syntactic labels (Stein et al,
2010), and the soft string-to-dependency extension
as described in Section 3.3. The primary submis-
sion combines the phrase-based contrastive system,
a hierarchical system that is very similar to the Jane
contrastive submission but with a slightly worse lan-
guage model, and an additional PBT system that has
been trained with forced alignment (Wuebker et al,
2010) on WMT 2010 data only.
4.2 Experimental Results English?French
The results for the English?French task are given
in Table 4. We likewise submitted two contrastive
systems for this translation direction. The first con-
trastive submission is a phrase-based system, en-
hanced with a triplet lexicon model and a discrim-
inative word lexicon model (Mauser et al, 2009) ?
both trained on in-domain news commentary data
only ? as well as a sentence-level single-word lex-
icon model and a discriminative reordering model
(Zens and Ney, 2006a). The second contrastive sub-
mission is a hierarchical Jane system with shallow
rules (Iglesias et al, 2009), a triplet lexicon model, a
discriminative word lexicon, the parse match model,
and a second phrase table extracted from in-domain
data only. Our primary submission is very similar
to the latter Jane setup. It does not comprise the ex-
tended lexicon models and the parse match exten-
sion, but instead includes lexical phrases from the
full 30 Mio. sentence corpus as described above.
5 German-English Setups
We trained phrase-based and hierarchical transla-
tion systems for both translation directions of the
German-English language pair. The corpus statis-
408
German English
Sentences 1 857 745
Running Words 48 449 977 50 559 217
Vocabulary 387 593 123 470
Table 5: Corpus statistics of the preprocessed train-
ing data for the WMT 2011 German?English and
English?German translation tasks. Numerical quantities
are replaced by a single category symbol.
tics can be found in Table 5. Word alignments were
generated with GIZA++ and symmetrized as for the
French-English setups.
The language models are 4-grams trained on the
bilingual data as well as the provided News crawl
corpus. For the English language model the 109
French-English and LDC Gigaword corpora were
used additionally. For the 109 French-English and
LDC Gigaword corpora RWTH applied the data se-
lection technique described in Section 3.1. We ex-
amined two different language models, one with
LDC data and one without.
Systems were optimized on the newstest2009 data
set, newstest2008 was used as test set. The scores
for newstest2010 are included for completeness.
5.1 Morpho-Syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation, the source side
was preprocessed by splitting German compound
words with the frequency-based method described
in (Koehn and Knight, 2003). To further reduce
translation complexity, we performed the long-range
part-of-speech based reordering rules proposed by
(Popovic? et al, 2006). For additional experiments
we used the TreeTagger (Schmid, 1995) to produce
a lemmatized version of the German source.
5.2 Optimization Criterion
We studied the impact of different optimization cri-
teria on tranlsation performance. The usual prac-
tice is to optimize the scaling factors to maximize
BLEU. We also experimented with two different
combinations of BLEU and Translation Edit Rate
(TER): TER?BLEU and TER?4BLEU. The first
denotes the equally weighted combination, while for
the latter BLEU is weighted 4 times as strong as
TER.
5.3 Experimental Results German?English
For the German?English task we conducted ex-
periments comparing the standard phrase extraction
with the phrase training technique described in Sec-
tion 3.2. For the latter we applied log-linear phrase-
table interpolation as proposed in (Wuebker et al,
2010). Further experiments included the use of addi-
tional language model training data, reranking of n-
best lists generated by the phrase-based system, and
different optimization criteria. We also carried out
a system combination of several systems, including
phrase-based systems on lemmatized German and
on source data without compound splitting and two
hierarchical systems optimized for different criteria.
The results are given in Table 6.
A considerable increase in translation quality can
be achieved by application of German compound
splitting. The system that operates on German
surface forms without compound splitting (SUR)
clearly underperforms the baseline system with mor-
phological preprocessing. The system on lemma-
tized German (LEM) is at about the same level as
the system on surface forms.
In comparison to the standard heuristic phrase ex-
traction technique, performing phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006b),
sentence length model, a 6-gram LM and single-
word lexicon models in both normal and inverse di-
rection. These models are combined in a log-linear
fashion and the scaling factors are tuned in the same
manner as the baseline system (using TER?4BLEU
on newstest2009).
The table includes three identical Jane systems
which are optimized for different criteria. The one
optimized for TER?4BLEU offers the best balance
between BLEU and TER, but was not finished in
time for submission. As primary submission we
chose the reranked PBT system, as secondary the
system combination.
409
newstest2008 newstest2009 newstest2010
German?English opt criterion BLEU TER BLEU TER BLEU TER
Syscombi of ? (secondary) TER?BLEU 21.1 62.1 20.8 61.2 23.7 59.2
Jane +GW ? BLEU 21.5 63.9 21.0 63.3 22.9 61.7
Jane +GW TER?4BLEU 21.4 62.6 21.1 62.0 23.5 60.3
PBT (FA) rerank +GW (primary) ? TER?4BLEU 21.4 62.8 21.1 61.9 23.4 60.1
PBT (FA) +GW ? TER?4BLEU 21.1 63.0 21.1 62.2 23.3 60.3
Jane +GW ? TER?BLEU 20.9 61.1 20.4 60.5 23.4 58.3
PBT (FA) TER?4BLEU 21.1 63.2 20.6 62.4 23.2 60.4
PBT TER?4BLEU 20.6 62.7 20.3 61.9 23.3 59.7
PBT (SUR) ? TER?4BLEU 19.5 66.5 18.9 65.8 21.0 64.9
PBT (LEM) ? TER?4BLEU 19.2 66.1 18.9 65.4 21.0 63.5
Table 6: RWTH systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results
are in percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model.
SUR and LEM denote the systems without compound splitting and on the lemmatized source, respectively. The three
hierarchical Jane systems are identical, but used different parameter optimization criterea.
newstest2008 newstest2009 newstest2010
English?German opt criterion BLEU TER BLEU TER BLEU TER
PBT + discrim. reord. (primary) TER?4BLEU 15.3 70.2 15.1 69.8 16.2 65.6
PBT + discrim. reord. BLEU 15.2 70.6 15.2 70.1 16.2 66.0
PBT TER?4BLEU 15.2 70.7 15.2 70.2 16.2 66.1
Jane BLEU 15.1 72.1 15.4 71.2 16.4 67.4
Jane TER?4BLEU 15.1 68.4 14.6 69.5 14.6 65.9
Table 7: RWTH systems for the WMT 2011 English?German translation task (truecase). BLEU and TER results are
in percentage.
5.4 Experimental Results English?German
We likewise studied the effect of using BLEU only
versus using TER?4BLEU as optimization crite-
rion in the English?German translation direction.
Moreover, we tested the impact of the discriminative
reordering model (Zens and Ney, 2006a). The re-
sults can be found in Table 7. For the phrase-based
system, optimizing towards TER?4BLEU leads to
slightly better results both in BLEU and TER than
optimizing towards BLEU. Using the discriminative
reordering model yields some improvements both on
newstest2008 and newstest2010. In the case of the
hierarchical system, the effect of the optimization
criterion is more pronounced than for the phrase-
based system. However, in this case it clearly leads
to a tradeoff between BLEU and TER, as the choice
of TER?4BLEU harms the translation results of
test2010 with respect to BLEU.
6 Conclusion
For the participation in the WMT 2011 shared trans-
lation task, RWTH experimented with both phrase-
based and hierarchical translation systems. We used
all bilingual and monolingual data provided for the
constrained track. To limit the size of the lan-
guage model, a data selection technique was applied.
Several techniques yielded improvements over the
baseline, including three syntactic models, extended
lexicon models, a discriminative reordering model,
forced alignment training, reranking methods and
different optimization criteria.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
410
References
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Surface
Heuristics. In Proceedings of the Workshop on Statis-
tical Machine Translation, pages 31?38.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Rule Filtering by Pattern for Efficient Hierar-
chical Translation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 380?388.
D. Klein and C.D. Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics
- Volume 1, ACL ?03, pages 423?430.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
A. Mauser, S. Hasan, and H. Ney. 2009. Extending Sta-
tistical Machine Translation with Discriminative and
Trigger-Based Lexicon Models. In Conference on
Empirical Methods in Natural Language Processing,
pages 210?217.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short Pa-
pers), pages 220?224, Uppsala, Sweden, July.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Natural
Language Processing, Springer Verlag, LNCS, pages
616?624.
H. Schmid. 1995. Improvements in Part-of-Speech Tag-
ging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50, Dublin,
Ireland, March.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In Proceedings
of ACL-08: HLT. Association for Computational Lin-
guistics, pages 577?585, June.
D. Stein, S. Peitz, D. Vilar, and H. Ney. 2010. A Cocktail
of Deep Syntactic Features for Hierarchical Machine
Translation. In Conference of the Association for Ma-
chine Translation in the Americas 2010, page 9, Den-
ver, USA, October.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901 ? 904, Denver, Col-
orado, USA, September.
D. Vilar, D. Stein, and H. Ney. 2008. Analysing Soft
Syntax Features and Heuristics for Hierarchical Phrase
Based Machine Translation. In Proc. of the Int. Work-
shop on Spoken Language Translation (IWSLT), pages
190?197, Waikiki, Hawaii, October.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006a. Discriminative Reordering
Models for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
R. Zens and H. Ney. 2006b. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
411
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
412
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 450?459,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Phrase Model Training for Statistical Machine Translation with
Word Lattices of Preprocessing Alternatives
Joern Wuebker and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University
Aachen, Germany
{wuebker,ney}@cs.rwth-aachen.de
Abstract
In statistical machine translation, word lattices
are used to represent the ambiguities in the
preprocessing of the source sentence, such as
word segmentation for Chinese or morpholog-
ical analysis for German. Several approaches
have been proposed to define the probability
of different paths through the lattice with ex-
ternal tools like word segmenters, or by apply-
ing indicator features. We introduce a novel
lattice design, which explicitly distinguishes
between different preprocessing alternatives
for the source sentence. It allows us to make
use of specific features for each preprocess-
ing type and to lexicalize the choice of lattice
path directly in the phrase translation model.
We argue that forced alignment training can
be used to learn lattice path and phrase trans-
lation model simultaneously. On the news-
commentary portion of the German?English
WMT 2011 task we can show moderate im-
provements of up to 0.6% BLEU over a state-
of-the-art baseline system.
1 Introduction
The application of statistical machine translation
(SMT) to word lattice input was first introduced for
the translation of speech recognition output. Rather
than translating the single-best transcription, the
speech recognition system encodes all possible tran-
scriptions and their probabilities within a word lat-
tice, which is then used as input for the machine
translation system (Ney, 1999; Matusov et al, 2005;
Bertoldi et al, 2007).
Since then, several groups have adapted this ap-
proach to model ambiguities in representing the
source language with lattices and were able to re-
port improvements over their respective baselines.
The probabilities for different paths through the lat-
tice are usually modeled by assigning probabilities
to arcs as a byproduct of the lattice generation or
by defining binary indicator features. Applying the
first method only makes sense if the lattice construc-
tion is based on a single, comprehensive probabilis-
tic method, like a Chinese word segmentation model
as is used by Xu et al (2005). In applications like
the one described by Dyer et al (2008), where sev-
eral different segmenters for Chinese are combined
to create the lattice, this is not possible. Also, our
intuition suggests that simply defining indicator fea-
tures for each of the segmenters may not be ideal, if
we assume that there is not a single best segmenter,
but rather that for different data instances a different
one works best.
In this paper, we propose to model the lattice
path implicitly within the phrase translation model.
We introduce a novel lattice design, which explic-
itly distinguishes between different ways of prepro-
cessing the source sentence. It enables us to define
specific binary features for each preprocessing type
and to learn lexicalized lattice path probabilities and
the phrase translation model simultaneously with a
forced alignment training procedure.
To train the phrase translation model, most state-
of-the-art SMT systems rely on heuristic phrase ex-
traction from a word-aligned training corpus. Us-
ing a modified version of the translation decoder to
450
force-align the training data provides a more consis-
tent way of training. Wuebker et al (2010) intro-
duce a leave-one-out method which can overcome
the over-fitting effects inherent to this training pro-
cedure (DeNero et al, 2006). The authors report this
to yield both a significantly smaller phrase table and
higher translation quality than the heuristic phrase
extraction.
We argue that applying forced alignment train-
ing helps to exploit the full potential of word lattice
translation. The effects of the training on lattice in-
put are analyzed on the news-commentary portion of
the German?English WMT 2011 task. Our results
show moderate improvements of up to 0.6% BLEU
over the baseline.
This paper is organized as follows: We will re-
view related work in Section 2, describe the decoder
in Section 3 and present our novel lattice design in
Section 4. The phrase training algorithm is intro-
duced in Section 5, and Section 6 gives a detailed
account of the experimental setup and discusses the
results. Finally, our findings are summarized in Sec-
tion 7.
2 Related work
Word lattices have been used for machine transla-
tion of text in a variety of ways. Dyer et al (2008)
use it to encode different Chinese word segmenta-
tions or Arabic morphological analyses. For the
phrase-based model, they report improvements of
up to 0.9% BLEU for Chinese?English and 1.6%
BLEU for Arabic?English over the respective sin-
gle best word segmented and morphologically ana-
lyzed source. These results are achieved without an
explicit way of modeling probabilities for different
paths within the lattice. The training of the phrase
model is done by generating one version of the train-
ing data for each segmentation method or morpho-
logical analysis. The word alignments are trained
separately, and are then concatenated for phrase ex-
traction. Our work differs from (Dyer et al, 2008) in
that we explicitly distinguish the various preprocess-
ing types in the lattice so that we can define specific
path features and lexicalize the lattice path probabil-
ities within the phrase model.
In (Xu et al, 2005) the probability of a segmen-
tation, as given by the Chinese word segmentation
model, and the translation model are combined into
a global decision rule. This is done by weighting
the lattice edges with a source language model. The
authors report an improvement of 1.5% BLEU over
translation of the single best segmentation with a
phrase-based SMT system.
Dyer (2009) introduces a maximum entropy
model for compound word splitting, which he
uses to create word lattices for translation in-
put. He shows improvements in German-English,
Hungarian-English and Turkish-English over state-
of-the-art baselines.
For the German?English WMT 2010 task, Hard-
meier et al (2010) encode the morphological re-
duction and decompounding of the German surface
form as alternative paths in a word lattice. They
show improvements of roughly 0.5% BLEU over the
baseline. A binary indicator feature is added to the
log-linear framework for the alternative edges. Ad-
ditionally, they integrate long-range reorderings of
the source sentence into the lattice, in order to match
the word order of the English language, which yields
another improvement of up to 0.5% BLEU.
Niehues and Kolss (2009) also use lattices to en-
code different alternative reorderings of the source
sentence which results in an improvement of
2.0% BLEU over the baseline on the WMT 2008
German?English task.
Onishi et al (2010) propose a method of modeling
paraphrases in a lattice. They perform experiments
on the English?Japanese and English?Chinese
IWSLT 2007 tasks, and report improvements of
1.1% and 0.9% BLEU over a paraphrase-augmented
baseline.
Schroeder et al (2009) generalize usage of lattices
to combine input from multiple source languages.
Factored translation models (Koehn and Hoang,
2007) approach the idea of integrating annotation
into translation from the opposite direction. Where
lattices allow the decoder to choose a single level of
annotation as translation source, factored models are
designed to jointly translate several annotation lev-
els (factors). Thus, they are more suited to integrate
low-level annotation that by itself does not provide
sufficient information for accurate translation, like
451
part-of-speech tags, gender, etc. On the other hand,
they require a one-to-one correspondence between
the factors, which makes them unsuitable to model
word segmentation or decompounding.
The problem of performing real training for the
phrase translation model has been approached in a
number of different ways in the past. The first one,
to the best of our knowledge, was the joint proba-
bility phrase model presented by Marcu and Wong
(2002). It is shown to perform slightly inferior to
the standard heuristic phrase extraction from word
alignments by Koehn et al (2003).
A detailed analysis of the inherent over-fitting
problems when training a generative phrase model
with the EM algorithm is given in (DeNero et al,
2006). These findings are in principle confirmed by
Moore and Quirk (2007) who, however, can show
that their model is less sensitive to reducing compu-
tational resources than the state-of-the-art heuristic.
Birch et al (2006) and DeNero et al (2008)
present alternative training procedures for the joint
model introduced by Marcu and Wong (2002),
which are shown to improve its performance.
In (Mylonakis and Sima?an, 2008) a phrase model
is described, whose training procedure is designed
to counteract the inherent over-fitting problem by in-
cluding prior probabilities based on Inversion Trans-
duction Grammar and smoothing as learning objec-
tive. It yields a small improvement over a standard
phrase-based baseline.
Ferrer and Juan (2009) present an approach,
where the phrase model is trained by a semi-hidden
Markov model.
In this work we apply the phrase training method
introduced by Wuebker et al (2010), where the
phrase translation model of a fully competitive SMT
system is trained in a generative way. The key to
avoiding the over-fitting effects described by DeN-
ero et al (2006) is their novel leave-one-out proce-
dure.
3 Decoding
3.1 Phrase-based translation
We use a standard phrase-based decoder which
searches for the best translation e?I?1 for a given input
sentence fJ1 by maximizing the posterior probability
e?I?1 = argmax
I,eI1
Pr(eI1|f
J
1 ). (1)
Generalizing the noisy channel approach (Brown
et al, 1990) and making use of the maximum ap-
proximation (Viterbi), the decoder directly mod-
els the posterior probability by a log-linear combi-
nation of several feature functions hm(eI1, s
K
1 , f
J
1 )
weighted with scaling factors ?m, which results in
the decision rule (Och and Ney, 2004)
e?I?1 = argmax
I,eI1,K,s
K
1
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
. (2)
Here, sK1 denotes the segmentation of e
I
1 and f
J
1
into K phrase-pairs and their alignment. The fea-
tures used are the language model, phrase translation
and lexical smoothing models in both directions,
word and phrase penalty and a simple distance-
based reordering penalty.
3.2 Lattice translation
For lattice input we generalize Equation 2 to also
maximize over the set of sentences F(L) encoded
by a given source word lattice L:
e?I?1 =
argmax
I,eI1,K,s
K
1 ,f
J
1 ?F(L)
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
(3)
Note that in this formulation there are no prob-
abilities assigned to the arcs of L. We define ad-
ditional binary indicator features hm and lexical-
ize path probabilities by encoding the path into the
word identities. To translate lattice input, we adapt
the standard phrase-based decoding algorithm as de-
scribed in (Matusov et al, 2008). The decoder keeps
track of the covered slots, which represent the topo-
logical order of the nodes, rather than the covered
words. When expanding a hypothesis, it has to be
verified that there is no overlap between the covered
nodes and that a path exists from start to goal node,
452
Pakistans Streitkr?fte - wiederholt Ziel von Selbstmordattent?tern - sind demoralisiert .
Pakistan Streit Kraft
Streit Kr?fte
selbst Mord Attentat
Selbst Mord Attent?tern
sein demoralisierenLemma
Surface
Compound
0 1
2
2
3 4 5 6 7
8 9
8 9
10 11 12 13 14
Pakistans Streitkr?fte - wiederholt Ziel von Selbstmordattent?tern - sind demoralisiert .
Pakistan Streit Kraft
Streit Kr?fte
selbst Mord Attentat
Selbst Mord Attent?tern
sein demoralisierenLemma
Surface
Compound
0 1
2
2
3 4 5 6 7
8 9
8 9
10 11 12 13 14
Pakistans
-
wiederholt Ziel von
- 
sind demoralisiert
.
- - .
wiederholt Ziel von
Figure 1: Top: Slim lattice. Bottom: Full lattice. The sentence is taken from the training data. The three layers
Surface, Compound and Lemma are separated with dashed lines. Nodes are labeled with slot information. Slots are
ordered horizontally, layers vertically.
which passes through all covered nodes. In prac-
tice, when considering a possible expansion cover-
ing slots j?, ..., j?? with start and end states n? and
n??, we make sure that the following two conditions
hold:
? n? is reachable from the lattice node that cor-
responds to the nearest already covered slot to
the left of j?.
? The node that corresponds to the nearest al-
ready covered slot to the right of j?? is reachable
from n??.
It was noted by Dyer et al (2008) that the stan-
dard distance-based reordering model needs to be
redefined for lattice input. We define the distortion
penalty as the difference in slot number. Using the
shortest path within the lattice is reported to have
better performance in (Dyer et al, 2008), however
we did not implement it due to time constraints.
4 Lattice design
We construct lattices from three different prepro-
cessing variants of the German source side of the
data. The surface form is the standard tokenization
of the source sentence. The word compounds are
produced by the frequency-based compound split-
ting method described in (Koehn and Knight, 2003),
applied to the tokenized sentence. From the com-
pound split sentence we produce the lemma of the
German words by applying the TreeTagger toolkit
(Schmid, 1995). Each of the different preprocess-
ing variants is assigned a separate layer within the
lattice. For the phrase model, word identities are de-
fined by both the word and its layer. In this way, the
phrase model can assign different scores to phrases
in different layers, allowing it to guide the search to-
wards a specific layer for each word. In practice, this
is done by annotating words with a unique identifier
for each layer. For example, the word sein from the
lemmatized layer will be written as LEM.sein within
both the data and the phrase table. If sein appears in
the surface form layer, it will be written as SUR.sein
and is treated as a different word. SUR is the identi-
fier for the compound layer.
We experiment with two different lattice designs.
In the full lattice, all three layers are included for
each source word in surface form. The slim lattice
only includes arcs for the lemma layer if it differs
from the surface form, and only includes arcs for the
compound layer if it differs from both surface form
and lemma. Figure 1 shows a slim and a full lattice
for the same training data sentence.
For each layer, we add two indicator features to
the phrase table: One binary feature which is set
to 1 if the phrase is taken from this layer, and one
feature which is equal to the number of words from
this layer. This results in six additional feature func-
tions, whose weights are optimized jointly with the
standard features described in Section 3.1. We will
453
denote them as layer features.
5 Phrase translation model training
To train the phrase model, we use a modified version
of the translation decoder to force-align the training
data. We apply the method described in (Wuebker et
al., 2010), but with word lattices on the source side.
To avoid over-fitting, we use their cross-validation
technique, which is described as a low-cost alterna-
tive to leave-one-out. For cross-validation we seg-
ment the training data into batches containing 5000
sentences. For each batch, the phrase table is up-
dated by reducing the phrase counts by the local
counts produced by the current batch in the previ-
ous training iteration. For the first iteration, we per-
form the standard phrase extraction separately for
each batch to produce the local counts. Singleton
phrases are assigned the probability ?(|f? |+|e?|) with
the source and target phrase lengths |f? | and |e?| and
fixed ? = e?5 (length-based leave-one-out). Sen-
tences for which the decoder is not able to find an
alignment are discarded (about 4% for our experi-
ments). To estimate the probabilities of the phrase
model, we count all phrase pairs used in training
within an n-best list (equally weighted). The trans-
lation probability for a phrase pair (f? , e?) is estimated
as
pFA(e?|f?) =
CFA(f? , e?)
Cmon(f?)
, (4)
where CFA(f? , e?) is the count of the phrase pair
(f? , e?) in the force-aligned training data. In order to
learn the lattice path along with the phrase transla-
tion probabilities, we make the following modifica-
tion to the original formulation in (Wuebker et al,
2010). The denominator Cmon(f?) is the count of
f? in the target side of the training data, rather than
using the real marginal counts. This means that it
is independent of the training procedure, and can be
computed by ignoring one side of the training data
and performing a simple n-gram count on the other.
In this way the model learns to prefer lattice paths
which are taken more often in training. For exam-
ple, if the phrase (LEM.Streit LEM.Kraft) is used
to align the sentence from Figure 1, Cmon(f?) will
be increased for f? = (SUR.Streitkr?fte) and f? =
(SPL.Streit SPL.Kr?fte) without affecting their joint
counts. This leads to a lower probability for these
phrases, which is not the case if marginal counts
are used. Note that on the source side we have one
training corpus for each lattice layer, which are con-
catenated to compute Cmon(f?). The size of the n-
best lists used in this work is fixed to 20000. Using
smaller n-best lists was tested, but seems to have dis-
advantages for the application to lattices. After re-
estimation of the phrase model, the feature weights
are optimized again.
In order to achieve a good coverage of the train-
ing data, we allow the decoder to generate backoff
phrases. If a source phrase consisting of a single
word does not have any translation candidates left
after the bilingual phrase matching, one phrase pair
is added to the translation candidates for each word
in the target sentence. The backoff phrases are as-
signed a fixed probability ? = e?12. Note that this
is smaller than the probability the phrase would be
assigned according to the length-based leave-one-
out heuristic, leading to a preference of singleton
phrases over backoff phrases. The lexical smooth-
ing models are applied in the usual way to both sin-
gleton and backoff phrases. After each sentence, the
backoff phrases are discarded. However, in the ex-
periments for this work, introducing backoff phrases
only increases the coverage from 95.8% to 96.2% of
the sentences.
6 Experimental evaluation
6.1 Experimental setup
Our experiments are carried out on the news-
commentary portion of the German?English data
provided for the EMNLP 2011 Sixth Workshop
on Statistical Machine Translation (WMT 2011).?
We use newstest2008 as development set and
newstest2009 and newstest2010 as unseen
test sets. The word alignments are produced with
GIZA++ (Och and Ney, 2003). To optimize the log-
linear parameters, the Downhill-Simplex algorithm
(Nelder and Mead, 1965) is applied with BLEU (Pa-
pineni et al, 2002) as optimization criterion. The
?http://www.statmt.org/wmt11
454
German English
Surface Compound Lemma
Train Sentences 136K
Running Words 3.4M 3.5M 3.3M
Vocabulary Size 118K 81K 52K 57K
newstest2008 Sentences 2051
Running Words 48K 50K 50K
Vocabulary Size 10.3K 9.7K 7.3K 8.1K
OOVs (Running Words) 3041 2092 1742 2070
newstest2009 Sentences 2525
Running Words 63K 66K 66K
Vocabulary Size 12.2K 11.4K 8.4K 9.4K
OOVs (Running Words) 4058 2885 2400 2729
newstest2010 Sentences 2489
Running Words 62K 65K 62K
Vocabulary Size 12.3K 11.4K 8.5K 9.2K
OOVs (Running Words) 4357 2952 2565 2742
Table 1: Corpus Statistics for the WMT 2011 news-commentary data, the development set (newstest2008) and
the two test sets (newstest2009, newstest2010). For the source side, three different preprocessing alternatives
are included: Surface, Compound and Lemma.
language model is a standard 4-gram LM with mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998) produced with the SRILM toolkit (Stolcke,
2002). It is trained on the full bilingual data and
parts of the monolingual News crawl corpus pro-
vided for WMT 2011. Numbers are replaced with
a single category symbol in a separate preprocess-
ing step and we apply the long-range part-of-speech
based reordering rules proposed by (Popovic? and
Ney, 2006).
Table 1 shows statistics for the bilingual training
data and the development and test corpora for the
three different German preprocessing alternatives.
It can be seen that both compound splitting and
lemmatization reduce the vocabulary size and num-
ber of out-of-vocabulary (OOV) words. Results are
measured in BLEU and TER (Snover et al, 2006),
which are computed case-insensitively with a single
reference.
6.2 Baseline experiments
To get an overview over the effects of the different
preprocessing alternatives for the German source,
we built three baseline systems, one for each prepro-
cessing type. The phrase tables are extracted heuris-
tically in the standard way from the word-aligned
training data. Additionally, we performed phrase
training for the compound split version of the data.
The results are shown in Table 2. When moving
from the Surface to the Compound layer, we observe
improvements of up to 1.0% in BLEU and 1.1% in
TER. Reducing the morphological richness further
(Lemma) leads to a clear performance drop. Appli-
cation of phrase training on the compound split data
yields a small degradation in TER on all data sets and
in BLEU on newstest2010. We assume that this
is due to the small size of the training data and its
heterogeneity, which makes it hard for the decoder
to find good phrase alignments.
6.3 Lattice experiments: Heuristic extraction
We generated both slim and full lattices for all data
sets. Similar to (Dyer et al, 2008), we concate-
nate the three training data sets and their word align-
ments to extract the phrases. Note that this only pro-
duces single-layer phrases. It can be seen in Table
2 that without the application of layer features the
slim lattice slightly outperforms the full lattice. In-
455
newstest2008 newstest2009 newstest2010
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
Baseline Surface 19.5 64.6 18.6 64.4 20.6 62.8
Compounds 20.5 63.5 19.1 63.5 21.1 61.9
FA Compounds 20.5 63.9 19.1 63.8 20.9 62.3
Lemma 19.2 65.4 18.2 65.2 19.9 63.9
Slim Lattice without layer feat. 19.9 64.4 18.9 64.1 20.8 62.6
(heuristic) with layer feat. 20.5 63.8 19.4 63.9 21.0 62.4
Full Lattice without layer feat. 19.8 64.6 18.7 64.2 20.6 62.8
(heuristic) with layer feat. 20.4 64.0 19.5 63.8 21.3 62.3
Full Lattice without layer feat. 20.0 64.3 19.3 64.1 20.8 62.6
(FA w/o layer feat.) with layer feat. 20.2 64.3 19.1 64.2 20.7 62.8
Full Lattice without layer feat. 20.5 63.7 19.5 63.6 21.3 62.1
(FA w/ layer feat.) with layer feat. 20.7 63.6 19.7 63.4 21.4 61.8
Table 2: Results on the German-English WMT 2011 data. Scores are computed case-insensitively for BLEU [%]
and TER [%]. We evaluate performance of the baseline systems, one for each of the three different encodings, with
both slim and full lattices using heuristic phrase extraction and with full lattices using forced alignment phrase model
training (FA). All lattice systems are evaluated with and without layer features. The best scores in each column are in
boldface, statistically significant improvement over the Compounds baseline is marked with blue color.
troducing layer features boosts the performance for
both lattice types. However, the performance in-
crease is considerably larger for the full lattice sys-
tems, which now outperform the slim lattice systems
on newstest2009 and newstest2010. Com-
pared to the Compounds baseline, the full lattice
system with layer features shows a small improve-
ment of up to 0.4% BLEU on newstest2009 and
newstest2010, but a degradation in TER.
6.4 Lattice experiments: Phrase training
The experiments on phrase training are setup as fol-
lows. The phrase table is initialized with the stan-
dard extraction and is identical to the one used for
the experiments in Section 6.3. The log-linear scal-
ing factors used in training are the optimized param-
eters on the corresponding lattice, also taken from
the experiments described in Section 6.3. The forced
alignment procedure was run for one iteration. Fur-
ther iterations were tested, but did not give any im-
provements.
The phrase training was performed on the full lat-
tice design. The reason for this is that we want the
system to learn all possible phrases. Even if there is
no difference in wording between the layers in train-
ing, the additional phrases could be useful for un-
seen test data. The training was performed both with
and without layer features. The resulting systems
were also optimized with and without layer features,
resulting in four different setups.
From the results in Table 2 it is clear that phrase
training without layer features does not have the
desired effect. Even if we apply layer features to
the system trained without them, we do not reach
the performance of the best standard lattice system.
We conclude that, without these indicator features,
the standard lattice system does not produce good
phrase alignments.
When the layer features are applied for both train-
ing and translation, we observe improvements of up
to 0.2% in BLEU and 0.5% in TER over the corre-
sponding standard lattice system. The gap between
the systems with and without layer features is much
smaller than for the heuristically trained lattices.
This indicates that our goal of encoding the best lat-
tice path directly in the phrase model was at least
partially achieved. However, in order to exceed the
performance of our state-of-the-art baseline on both
measures, the layer features are still needed within
the phrase training procedure and for translation. Al-
456
source Das Warten hat gedauert mehr als NUM Minuten, was im Fall einer Stra?e, wo
werden erwartet NUM Menschen, ist unverst?ndlich.
reference The wait lasted more than NUM minutes, something incomprehensible for a race
where you expect more than NUM people.
lattice (heuristic) The wait has taken more than NUM minutes, which in the case of a street, where
NUM people are expected to be, can?t understand it.
lattice (FA) The wait has taken more than NUM minutes, which in the case of a street, where
expected NUM people, is incomprehensible.
Figure 2: Example sentence from the newstest2009 data set. The faulty phrase in the heuristic lattice translation
is marked in boldface.
together, our phrase trained lattice approach outper-
forms the state-of-the-art baseline on all three data
sets by up to 0.6% BLEU. On newstest2009,
this result is statistically significant with 95% confi-
dence according to the bootstrap resampling method
described by Koehn (2004).
For a direct comparison between the heuristic and
phrase-trained full lattice systems, we manually in-
spected the optimized log-linear parameter values
for the layer features. We observe that for the stan-
dard lattices, paths through the lemmatized layer are
heavily penalized. In the phrase trained lattice setup,
the penalty is much smaller. As a result, the num-
ber of words from the Lemma layer used for transla-
tion of the newstest2009 data set is increased by
49% from 1828 to 2715 words. However, a manual
inspection of the translations reveals that the main
improvement seems to come from a better choice
of phrases from the Compound layer. More specif-
ically, the used phrases tend to be shorter ? the av-
erage phrase length of Compound layer phrases is
1.5 words for both the baseline and the heuristic lat-
tice system. In the phrase trained lattice system, it
is 1.3 words. An example is given in Figure 2. We
focus on the end of the sentence, where the heuris-
tic system uses the rather disfluent phrase (ist unver-
st?ndlich. # can?t understand it.), whereas the forced
alignment trained system applies the three phrases
(ist # is), (unverst?ndlich # incomprehensible) and
(. # .).
This effect can be explained by the leave-one-out
procedure. As lemmatized phrases usually map to
several phrases in the other layers, their count is gen-
erally higher. Application of leave-one-out, which
reduces the counts of all phrases extracted from the
current sentence by a fixed value, therefore has a
stronger penalizing effect on Surface and Compound
layer phrases. In the extreme case, phrases which are
singletons in the Compound layer are unlikely to be
used at all in training, if the corresponding phrase
in the Lemma layer has a higher count. While this
rarely leads to the competing lemmatized phrases
being used in free translation, it allows for shorter,
more general phrases from the more expressive lay-
ers to be applied. Indeed, the ?bad? phrase (ist unver-
st?ndlich. # can?t understand it.) from the example
in Figure 2 is a singleton.
7 Conclusion and future work
In this work we apply a forced alignment phrase
training technique to input word lattices in SMT for
the first time. The goal of encoding better lattice
path probabilities directly into the phrase model was
at least partially successful. The proposed method
outperforms our baseline by up to 0.6% BLEU. To
achieve this, we presented a novel lattice design,
which distinguishes between different layers, for
which we can define separate indicator features. Al-
though these layer features are still necessary for the
final system to improve over state-of-the-art perfor-
mance, they are less important than in the heuristi-
cally trained setup.
One advantage of our approach is its adaptability
to a variety of scenarios. In future work, we plan
to apply it to additional language pairs. Arabic and
Chinese on the source side, where the layers could
represent different word segmentations, seem a nat-
ural choice. We also hope to be able to leverage
larger training data sets. As a natural extension we
plan to allow learning of cross-layer phrases. Fur-
457
ther, applying this framework to lattices modeling
different reorderings could be an interesting direc-
tion.
Acknowledgments
This work was partially realized as part of the
Quaero Programme, funded by OSEO, French State
agency for innovation, and also partially funded by
the European Union under the FP7 project T4ME
Net, Contract No. 249119.
References
N. Bertoldi, R. Zens, and M. Federico. 2007. Speech
translation by confusion network decoding. In Pro-
ceedings of ICASSP 2007, pages 1297?1300, Hon-
olulu, Hawaii, April.
Alexandra Birch, Chris Callison-Burch, Miles Osborne,
and Philipp Koehn. 2006. Constraining the phrase-
based, joint probability statistical translation model. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 154?157, Jun.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16:79?85, June.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Computer Science
Group, Harvard University, Aug.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why Generative Phrase Models Underperform
Surface Heuristics. In Proceedings of the Workshop
on Statistical Machine Translation, pages 31?38, New
York City, June.
John DeNero, Alexandre Buchard-C?t?, and Dan Klein.
2008. Sampling Alignment Structure under a Bayesian
Translation Model. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 314?323, Honolulu, October.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generaliz-
ing word lattice translation. In Proceedings of Annual
Meeting of the Association for Computational Linguis-
tics, pages 1012?1020, Columbus, Ohio, June.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceedings
of the 2009 Annual Conference of the North Ameri-
can Chapter of the ACL, pages 406?414, Boulder, Col-
orado, June.
Jes?s-Andr?s Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-markov approach to machine trans-
lation. In Proceedings of European Association for
Machine Translation (EAMT), Barcelona, Spain, May.
European Association for Machine Translation.
C. Hardmeier, A. Bisazza, and M. Federico. 2010. FBK
at WMT 2010: Word Lattices for Morphological Re-
duction and Chunk-based Reordering. In Proceedings
of the Joint 5th Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 88?92, Uppsala, Swe-
den, July.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology - Volume 1, pages
48?54, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. pages 388?395,
Barcelona, Spain, July.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2002), July.
E. Matusov, H. Ney, and R. Schl?ter. 2005. Phrase-
based translation of speech recognizer word lattices us-
ing loglinear model combination. In Proceedings of
the IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU), pages 110?115, San Juan,
Puerto Rico.
Evgeny Matusov, Bj?rn Hoffmeister, and Hermann Ney.
2008. ASR Word Lattice Translation with Exhaustive
Reordering is Possible. In Interspeech, pages 2342?
2345, Brisbane, Australia, September.
Robert C. Moore and Chris Quirk. 2007. An iteratively-
trained segmentation-free phrase translation model for
statistical machine translation. In Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 112?119, Prague, June.
Markos Mylonakis and Khalil Sima?an. 2008. Phrase
Translation Probabilities with ITG Priors and Smooth-
ing as Learning Objective. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 630?639, Honolulu, October.
458
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
H. Ney. 1999. Speech translation: Coupling of recog-
nition and translation. In Proceedings of IEEE Inter-
national Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pages 517?520, Phoenix, Ari-
zona, USA, March.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece,
March.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449, De-
cember.
T. Onishi, M. Utiyama, and E. Sumita. 2010. Paraphrase
lattice for statistical machine translation. In Proceed-
ings of the ACL 2010 Conference Short Papers, pages
1?5, Uppsala, Sweden, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting on Association for Computational Lin-
guistics, pages 311?318, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
M. Popovic? and H. Ney. 2006. POS-based Word Re-
orderings for Statistical Machine Translation. In Inter-
national Conference on Language Resources and Eval-
uation, pages 1278?1283.
H. Schmid. 1995. Improvements in Part-of-Speech Tag-
ging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50, Dublin,
Ireland, March.
Josh Schroeder, Trevor Cohn, and Philipp Koehn. 2009.
Word lattices for multi-source translation. In Proceed-
ings of the 12th Conference of the European Chapter
of the ACL, pages 719?727, Athens, Greece.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proc. of AMTA, pages 223?231, Aug.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901 ? 904, Denver, Col-
orado, USA, September.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of the
Assoc. for Computational Linguistics, pages 475?484,
Uppsala, Sweden, July.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated chinese word segmentation in
statistical machine translation. In International Work-
shop on Spoken Language Translation, pages 141?147,
Pittsburgh, PA, USA, October.
459
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 460?467,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Leave-One-Out Phrase Model Training for Large-Scale Deployment
Joern Wuebker
Human Language Technology
and Pattern Recognition Group
RWTH Aachen University, Germany
wuebker@cs.rwth-aachen.de
Mei-Yuh Hwang, Chris Quirk
Microsoft Corporation
Redmond, WA, USA
{mehwang,chrisq}@microsoft.com
Abstract
Training the phrase table by force-aligning
(FA) the training data with the reference trans-
lation has been shown to improve the phrasal
translation quality while significantly reduc-
ing the phrase table size on medium sized
tasks. We apply this procedure to several
large-scale tasks, with the primary goal of re-
ducing model sizes without sacrificing transla-
tion quality. To deal with the noise in the auto-
matically crawled parallel training data, we in-
troduce on-demand word deletions, insertions,
and backoffs to achieve over 99% successful
alignment rate. We also add heuristics to avoid
any increase in OOV rates. We are able to re-
duce already heavily pruned baseline phrase
tables by more than 50% with little to no
degradation in quality and occasionally slight
improvement, without any increase in OOVs.
We further introduce two global scaling fac-
tors for re-estimation of the phrase table via
posterior phrase alignment probabilities and
a modified absolute discounting method that
can be applied to fractional counts.
Index Terms: phrasal machine translation, phrase
training, phrase table pruning
1 Introduction
Extracting phrases from large amounts of noisy
word-aligned training data for statistical machine
translation (SMT) generally has the disadvantage of
producing many unnecessary phrases (Johnson et
al., 2007). These can include poor quality phrases,
composite phrases that are concatenations of shorter
ones, or phrases that are assigned very low proba-
bilities, so that they have no realistic chance when
competing against higher scoring phrase pairs. The
goal of this work is two-fold: (i) investigating forced
alignment training as a phrase table pruning method
for large-scale commercial SMT systems and (ii)
proposing several extensions to the training proce-
dure to deal with practical issues and stimulate fur-
ther research.
Generative phrase translation models have the in-
herent problem of over-fitting to the training data
(Koehn et al, 2003; DeNero et al, 2006). (Wue-
bker et al, 2010) introduce a leave-one-out proce-
dure which is shown to counteract over-fitting ef-
fects. The authors report significant improvements
on the German-English Europarl data with the ad-
ditional benefit of a severely reduced phrase table
size. This paper investigates its impact on a num-
ber of commercial large-scale systems and presents
several extensions.
The first extension is to deal with the highly noisy
training data, which is automatically crawled and
sentence aligned. The noise and the baseline prun-
ing of the phrase table lead to low success rates
when aligning the source sentence with the target
sentence. We introduce on-demand word deletions,
insertions, and backoff phrases to increase the suc-
cess rate so that we can cover essentially the en-
tire training data. Secondly, phrase table pruning
makes out-of-vocabulary (OOV) issues even more
pronounced. To avoid an increased OOV rate, we
retrieve single-word translations from the baseline
phrase table. Lastly, we propose two global scaling
460
factors to allow fine-tuning of the phrase counts in
an attempt to re-estimate the translation probabili-
ties and a modification of absolute discounting that
can be applied to fractional counts.
Our main contribution is applying forced-
alignment on the training data to prune the phrase
table. The rationale behind this is that by decoding
the training data, we can identify the phrases that are
actually used by the decoder. Further, we present
preliminary experiments on re-estimating the chan-
nel models in the phrase table based on counts ex-
tracted from the force-aligned data.
This work is organized as follows. We discuss re-
lated work in Section 2, describe our decoder and
training procedure in Section 3 and the experiments
in Section 4. A conclusion and discussion of future
work is given in Section 5.
2 Related Work
Force-aligning bilingual data has been explored as
a means of model training in previous work. Liang
et al (2006) use it for their bold updating strategy
to update discriminative feature weights. Utilizing
force-aligned data to train a unigram phrase segmen-
tation model is proposed by Shen et al (2008). Wue-
bker et al (2010) apply forced alignment to train the
phrase table in an EM-like fashion. They report a
significant reduction in phrase table size.
In this work we apply forced alignment training
as a pure phrase table pruning technique. Johnson
et al (2007) successfully investigate a number of
pruning methods for the phrase inventory based on
significance testing. While their approach is more
straightforward and less elaborate, we argue that our
method is directly tailored to the decoding process
and works on top of an already heavily pruned base-
line phrase table.
We further experiment with applying the (scaled)
phrase alignment posteriors to train the phrase ta-
ble. A similar idea has been addressed in previous
work, e.g. (Venugopal et al, 2003; de Gispert et al,
2010), where word alignment posterior probabilities
are leveraged for grammar extraction.
Finally, a number of papers describe extending
real phrase training to the hierarchical machine
translation paradigm (Blunsom et al, 2008; Cme-
jrek et al, 2009; Mylonakis and Sima?an, 2010).
3 Phrase Training
3.1 Decoder
Our translation decoder is similar to the open-source
toolkit Moses (Koehn et al, 2007). It models trans-
lation as a log-linear combination of two phrasal
and two lexical channel models, an n-gram language
model (LM), phrase, word and distortion penalties
and a lexicalized reordering model. The decoding
can be summarized as finding the best scoring target
sentence T ? given a source sentence S:
T ? = argmax
T
?
i
?i log gi(S,T ) (1)
where each gi represents one feature (the channel
models, n-gram, phrase count, etc.). The model
weights ?i are usually discriminatively learned on a
development data set via minimum error rate train-
ing (MERT) (Och, 2003).
Constraining the decoder to a fixed target sentence
is straightforward. Each partial hypothesis is com-
pared to the reference and discarded if it does not
match. The language model feature can be dropped
since all hypotheses lead to the same target sentence.
The training data is divided into subsets for parallel
alignment. A bilingual phrase matching is applied to
the phrase table to extract only the subset of entries
that are pertinent to each subset of training data, for
memory efficiency. For forced alignment training,
we set the distortion limit ? to be larger than in reg-
ular translation decoding. As unlimited distortion
leads to very long training times, we compromise on
the following heuristic. The distortion limit is set
to be the maximum of 10, twice that of the baseline
setting, and 1.5 times the maximum phrase length:
? = max{10,
2? (baseline distortion),
1.5? (max phrase length)} (2)
To avoid over-fitting, we employ the same leave-
one-out procedure as (Wuebker et al, 2010) for
training. Here, it is applied on top of the Good-
Turing (GT) smoothed phrase table (Foster et al,
461
2006). Our phrase table stores the channel proba-
bilites and marginal counts for each phrase pair, but
not the discounts applied. Therefore, for each sen-
tence, if the phrase pair (s, t) has a joint count c(s, t)
computed from the entire training data, and occurs
c1(s, t) times in the current sentence, the leave-one-
out probability p?(t|s) for the current sentence will
be:
p?(t|s) =
c?(s, t)?d
c?(s)
=
c(s, t)? c1(s, t)?d
c(s)? c1(s)
=
p(t|s)c(s)? c1(s, t)
c(s)? c1(s)
(3)
since p(t|s)c(s) = c(s, t)?d, where d is the GT dis-
count value. In the case where c(s, t) = c1(s, t) (i.e.
(s, t) occurs exclusively in one sentence pair), we
use a very low probability as the floor value. We
apply leave-one-out discounting to the forward and
backward translation models only, not to the lexical
channel models.
Our baseline phrase extraction applies some
heuristic-based pruning strategies. For example,
it prunes offensive translations and many-words to
many-words singletons (i.e. a joint count of 1 and
both source phrase and target phrase contain mul-
tiple words)?. Finally the forward and backward
translation probabilities are smoothed with Good-
Turing discounting.
3.2 Weak Lambda Training with High
Distortion
Our leave-one-out training flowchart can be illus-
trated in Figure 1. To force-align the training data
with good quality, we need a set of trained lambda
weights, as shown in Equation 1. We can use the
lambda weights learned from the baseline system for
that purpose. However, ideally we want the lambda
values to be learned under a similar configuration as
the forced alignment. Therefore, for this purpose we
run MERT with the larger distortion limit given in
Equation 2.
?The pruned entries are nevertheless used in computing joint
counts and marginal counts.
 Parallel training data 
with word-level alignments 
Phrase extraction with 
heuristic pruning 
Weak lambda training 
Phrase table 
Leave-one-out 
forced alignment 
? 1 = {?} 
Normal lambda training 
Intersection + 
OOV Recovery 
Selected phrases 
Selected phrases+ 
Large ? 
2-grams 
5-grams 
Small ? 
? 2 = {?} 
 {uniform ?} 
 {baseline ?} 
Figure 1: Flowchart of forced-alignment phrase training.
Additionally, since forced alignment does not use
the language model, we propose to use a weaker lan-
guage model for training the lambdas (?1) to be used
in the forced alignment decoding.
Using a weaker language model also speeds up the
lambda training process, especially when we are us-
ing a distortion limit ? at least twice as high as in
the baseline system. In our experiments, the base-
line system uses an English 5-gram language model
trained on a large amount of monolingual data. The
lambda values used for forced alignment are learned
using the bigram LM trained on the target side of the
462
parallel corpus for each system.
We compared a number of systems using differ-
ent degrees of weak models and found out the im-
pact on the final system was minimal. However, us-
ing a small bigram LM with large distortion yielded
a stable performance in terms of BLEU, and was
25% faster than using a large 5-gram with the base-
line distortion. Because of the speed improvement
and its stability, this paper adopts the weak bigram
lambda training.
3.3 On-demand Word Insertions and Deletions
For many training sentences the translation decoder
is not able to find a phrasal alignment. We identified
the following main reasons for failed alignments:
? Incorrect sentence alignment or sentence seg-
mentation by the data crawler,
? OOVs due to initial pruning in the phrase ex-
traction phase,
? Faulty word alignments,
? Strongly reordered sentence structure. That is,
the distortion limit during forced alignment is
too restrictive.
For some of these cases, discarding the sentence
pairs can be seen as implicit data cleaning. For
others, there do exist valid sub-sentences that are
aligned properly. We would like to be able to lever-
age those sub-sentences, effectively allowing us to
do partial sentence removal. Therefore, we in-
troduce on-demand word insertions and deletions.
Whenever a partial hypothesis can not be expanded
to the next target word t j, with the given phrase ta-
ble, we allow the decoder to artificially introduce a
phrase pair (null, t j) to insert the target word into
the hypothesis without consuming any source word.
These artificial phrase pairs are introduced with a
high penalty and are ignored when creating the out-
put phrase table. We can also introduce backoff
phrase pairs (si, t j) for all source words si that are
not covered so far, also with a fixed penalty.
After we reach the end of the target sentence, if
there are any uncovered source words si, we arti-
ficially add the deletion phrase pairs (si,null) with
a high penalty. Introducing on-demand word inser-
tions and deletions increases the data coverage to
at least 99% of the training sentences on all tasks
we have worked on. Due to the success of inser-
tion/deletion phrases, we have not conducted exper-
iments using backoff phrases within the scope of this
work, but leave this to future work.
3.4 Phrase Training as Pruning
This work concentrates on practical issues with large
and noisy training data. Our main goal is to ap-
ply phrase training to reduce phrase table size with-
out sacrificing quality. We do this by dumping n-
best alignments of the training data, where n ranges
from 100-200. We prune the baseline phrase table to
only contain phrases that appear in any of the n-best
phrase alignments, leaving the channel probabilities
unchanged. That is, the model scores are still esti-
mated from the original counts. We can control the
size of the final phrase table by adjusting the size
of the n-best list. Based on the amount of memory
we can afford, we can thus keep the most important
entries in the phrase table.
3.5 OOV retrieval
When performing phrase table pruning as de-
scribed in Section 3.4, OOV rates tend to increase.
This effect is even more pronounced when dele-
tion/insertion phrases are not used, due to the low
alignment success rate. For commercial applica-
tions, untranslated words are a major concern for
end users, although it rarely has any impact on BLEU
scores. Therefore, for the final phrase table after
forced alignment training, we check the translations
for single words in the baseline phrase table. If any
single word has no translation in the new table, we
recover the top x translations from the baseline table.
In practice, we set x = 3.
3.6 Fractional Counts and Model
Re-estimation
As mentioned in Section 3.4, for each training sen-
tence pair we produce the n-best phrasal alignments.
If we interpret the model score of an alignment as
its log likelihood, we can weight the count for each
phrase by its posterior probability. However, as the
463
log-linear model weights are trained in a discrim-
inative fashion, they do not directly correspond to
probabilities. In order to leverage the model scores,
we introduce two scaling factors ? and ? that al-
low us to shape the count distribution according to
our needs. For one sentence pair, the count for the
phrase pair (s, t) is defined as
c(s, t)=
?
?
?
?
?
n
?
i=1
c(s, t|hi) ?
exp(? ??(hi))
n
?
j=1
exp(? ??(h j))
?
?
?
?
?
?
, (4)
where hi is the i-th hypothesis of the n-best list,
?(hi) the log-linear model score of the alignment
hypothesis hi and c(s, t|hi) the count of (s, t) within
hi. If ? = 0, all alignments within the n-best list
are weighted equally. Setting ? = 0 means that all
phrases that are used anywhere in the n-best list re-
ceive a count of 1.
Absolute discounting is a popular smoothing
method for relative frequencies (Foster et al, 2006).
Its application, however, is somewhat difficult, if
counts are not required to be integer numbers and
can in fact reach arbitrarily small values. We pro-
pose a minor modification, where the discount pa-
rameter d is added to the denominator, rather than
subtracting it from the numerator. The discounted
relative frequency for a phrase pair (s, t) is computed
as
p(s|t) =
c(s, t)
d+?
s?
c(s?, t)
(5)
3.7 Round-Two Lambda Training
After the phrase table is pruned with forced align-
ment (either re-estimating the channel probabilities
or not), we recommend a few more iterations of
lambda training to ensure our lambda values are ro-
bust with respect to the new phrase table. In our
experiments, we start from the baseline lambdas and
train at most 5 more iterations using the baseline dis-
tortion and the 5-gram English language model. The
settings have to be consistent with the final decod-
ing; therefore we are not using weak lambda training
here.
system parallel corpus Dev Test1 WMT
(sent. pairs)
it-en 13.0M 2000 5000 3027
pt-en 16.9M 2448 5000 1000
nl-en 15.0M 499 4996 1000
et-en 3.5M 1317 1500 995
Table 1: Data sizes of the four systems Italian, Por-
tuguese, Dutch and Estonian to English. All numbers
refer to sentence pairs.
Empirically we found the final lambdas (?2) made
a very small improvement over the baseline lamb-
das. However, we decided to keep this second round
of lambda training to guarantee its stability across
all language pairs.
4 Experiments
In this section, we describe our experiments on
large-scale training data. First, we prune the orig-
inal phrase table without re-estimation of the mod-
els. We conducted experiments on many language
pairs. But due to the limited space here, we chose to
present two high traffic systems and the two worst
systems so that readers can set the correct expecta-
tion with the worst-case scenario. The four systems
are: Italian (it), Portuguese (pt), Dutch (nl) and Es-
tonian (et), all translating to English (en).
4.1 Corpora
The amount of data for the four systems is shown in
Table 1. There are two test sets: Test1 and WMT.
Test1 is our internal data set, containing web page
translations among others. WMT is sampled from
the English side of the benchmark test sets of the
Workshop on Statistical Machine Translation?. The
sampled English sentences are then manually trans-
lated into other languages, as the input to test X-to-
English translation. WMT tends to contain news-
like and longer sentences. The development set (for
learning lambdas) is from our internal data set. We
make sure that there is no overlap among the devel-
opment set, test sets, and the training set.
?www.statmt.org/wmt09
464
baseline FA w/ del. FA w/o del.
it-en
suc.rate ? 99.5% 61.2%
Test1 42.27 42.05 42.31
WMT 30.16 30.19 30.19
pt-en
suc.rate ? 99.5% 66.9%
Test1 47.55 47.47 47.24
WMT 40.74 41.36 41.01
nl-en
suc.rate ? 99.6% 79.9%
Test1 32.39 31.87 31.18
WMT 43.37 43.06 43.38
et-en
suc.rate ? 99.1% 73.1%
Test1 46.14 46.35 45.77
WMT 20.08 19.60 19.83
Table 2: BLEU scores of forced-alignment-based phrase-
table pruning using weak lambda training. n-best size is
100 except for nl-en, where it is 160. We contrast forced
alignment with and without on-demand insertion/deletion
phrases. With the on-demand artificial phrases, FA suc-
cess rate is over 99%.
4.2 Insertion/Deletion Phrases
Unless explicitly stated, all experiments here used
the weak bigram LMs to obtain the lambdas used for
forced alignment, and on-demand insertion/deletion
phrases are applied. For the size of n-best, we use
n = 100. The only exception is the nl-en language
pair, for which we set n = 160 because its phrase
distortion setting is higher than the others and for its
higher number of morphological variations. Table 2
shows the BLEU performance of the four systems, in
the baseline setting and in the forced-alignment set-
ting with insertion/deletion phrases and without in-
sertion/deletion phrases. Whether partial sentences
should be kept or not (via insertion/deletion phrases)
depends on the quality of the training data. One
would have to run both settings to decide which is
better for each system. In all cases, there is little
or no degradation in quality after the table is suffi-
ciently pruned.
Table 3 shows that our main goal of reducing the
phrase table size is achieved. On all four language
pairs, we are able to prune over 50% of the phrase
PT size reduction
w/o del. w/ del.
it-en 65.4% 54.0%
pt-en 68.5% 61.3%
nl-en 64.1% 56.9%
et-en 63.6% 58.5%
Table 3: % Phrase table size reduction compared with the
baseline phrase table
table. Without on-demand insertions/deletions, the
size reduction is even stronger. Notice the size re-
duction here is relative to the already heavily pruned
baseline phrase table.
With such a successful size cut, we expected a
significant increase in decoding speed in the final
system. In practice we experienced 3% to 12% of
speedup across all the systems we tested. Both our
baseline and the reduced systems use a tight beam
width of 20 hypotheses per stack. We assume that
with a wider beam, the speed improvement would
be more pronounced.
We also did human evaluation on all 8 system out-
puts (four language pairs, with two test sets per lan-
guage pair) and all came back positive (more im-
provements than regressions), even on those that had
minor BLEU degradation. We conclude that the size
cut in the phrase table is indeed harmless, and there-
fore we declare our initial goal of phrase table prun-
ing without sacrificing quality is achieved.
In (Wuebker et al, 2010) it was observed, that
phrase training reduces the average phrase length.
The longer phrases, which are unlikely to gener-
alize, are dropped. We can confirm this obersva-
tion for the it-en and pt-en language pairs in Ta-
ble 4. However, for nl-en and et-en the aver-
age source phrase length is not significantly af-
fected by phrase training, especially with the inser-
tion/deletion phrases. When these artificial phrases
are added during forced alignment, they tend to en-
courage long target phrases as uncovered single tar-
get words can be consumed by the insertion phrases.
However, these insertion phrases are not dumped
into the final phrase table and hence cannot help
in reducing the average phrase length of the final
phrase table.
465
avg. src phrase length
baseline w/o del. w/ del.
it-en 3.1 2.4 2.4
pt-en 3.7 3.0 3.0
nl-en 3.1 3.0 3.0
et-en 2.9 2.8 3.0
Table 4: Comparison of average source phrase length in
the phrase table.
nl-en Test1 WMT PT size reduction
baseline 32.29 43.37 ?
n=100 31.45 42.90 66.0%
n=160 31.87 43.06 64.1%
et-en Test1 WMT PT size reduction
baseline 46.14 20.08 ?
n=100 46.35 19.60 63.6%
n=200 46.34 19.88 58.4%
Table 5: BLEU scores of different n-best sizes for the
highly inflected Dutch system and the noisy Estonian sys-
tem.
Table 5 illustrates how the n-best size affects
BLEU scores and model sizes for the nl-en and et-
en systems.
4.3 Phrase Model Re-estimation
This section conducts a preliminary evaluation of
the techniques introduced in Section 3.6. For fast
turnaround, these experiments were conducted on
approximately 1/3 of the Italian-English training
data. Training is performed with and without inser-
tion/deletion phrases and both with (FaTrain) and
without (FaPrune) re-training of the forward and
backward phrase translation probabilities. Table 6
shows the BLEU scores with different settings of the
global scaling factor ? and the inverse discount d.
The second global scaling factor is fixed to ? = 0.
The preliminary results seem to be invariant of the
settings. We conclude that using forced alignment
posteriors as a feature training method seems to be
less effective than using competing hypotheses from
free decoding as in (He and Deng, 2012).
BLEU
ins/del ? d Test1 WMT
baseline - - - 40.6 28.9
FaPrune no - - 40.7 29.1
FaTrain no 0 0 40.4 28.9
0.5 0 40.2 28.9
FaPrune yes - - 40.6 28.9
FaTrain yes 0 0 40.1 28.6
0.5 0 40.5 29.1
0.5 0.2 40.5 29.0
0.5 0.4 40.5 29.0
Table 6: Phrase pruning (FaPrune) vs. further model
re-estimation after pruning (FaTrain) on 1/3 it-en train-
ing data, both with and without on-demand inser-
tions/deletions.
5 Conclusion and Outlook
We applied forced alignment on parallel training
data with leave-one-out on four large-scale commer-
cial systems. In this way, we were able to reduce the
size of our already heavily pruned phrase tables by
at least 54%, with almost no loss in translation qual-
ity, and with a small improvement in speed perfor-
mance. We show that for language pairs with strong
reordering, the n-best list size needs to be increased
to account for the larger search space.
We introduced several extensions to the training
procedure. On-demand word insertions and dele-
tions can increase the data coverage to nearly 100%.
We plan to extend our work to use backoff transla-
tions (the target word that can not be extended given
the input phrase table will be aligned to any uncov-
ered single source word) to provide more alignment
varieties, and hence hopefully to be able to keep
more good phrase pairs. To avoid higher OOV rates
after pruning, we retrieved single-word translations
from the baseline phrase table.
We would like to emphasize that this leave-one-
out pruning technique is not restricted to phrasal
translators, even though all experiments presented
in this paper are on phrasal translators. It is possible
to extend the principle of forced alignment guided
pruning to hierarchical decoders, treelet decoders, or
syntax-based decoders, to prune redundant or use-
less phrase mappings or translation rules.
466
Re-estimating phrase translation probabilities us-
ing forced alignment posterior scores did not yield
any noticable BLEU improvement so far. Instead, we
propose to apply discriminative training similar to
(He and Deng, 2012) after forced-alignment-based
pruning as future work.
References
[Blunsom et al2008] Phil Blunsom, Trevor Cohn, and
Miles Osborne. 2008. A discriminative latent vari-
able model for statistical machine translation. In Pro-
ceedings of the 46th Annual Conference of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-08:HLT), pages 200?208, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
[Cmejrek et al2009] Martin Cmejrek, Bowen Zhou, and
Bing Xiang. 2009. Enriching SCFG Rules Directly
From Efficient Bilingual Chart Parsing. In Proc. of the
International Workshop on Spoken Language Transla-
tion, pages 136?143, Tokyo, Japan.
[de Gispert et al2010] Adria? de Gispert, Juan Pino, and
William Byrne. 2010. Hierarchical Phrase-based
Translation Grammars Extracted from Alignment Pos-
terior Probabilities. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 545?554, MIT, Massachusetts,
U.S.A., October.
[DeNero et al2006] John DeNero, Dan Gillick, James
Zhang, and Dan Klein. 2006. Why Generative Phrase
Models Underperform Surface Heuristics. In Proceed-
ings of the Workshop on Statistical Machine Transla-
tion, pages 31?38, New York City, June.
[Foster et al2006] George Foster, Roland Kuhn, and
Howard Johnson. 2006. Phrasetable Smoothing for
Statistical Machine Translation. In Proc. of the Conf.
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 53?61, Sydney, Australia, July.
[He and Deng2012] Xiaodong He and Li Deng. 2012.
Maximum Expected BLEU Training of Phrase and
Lexicon Translation Models. In Proceedings of the
50th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), page to appear, Jeju, Republic
of Korea, Jul.
[Johnson et al2007] J Howard Johnson, Joel Martin,
George Foster, and Roland Kuhn. 2007. Improv-
ing Translation Quality by Discarding Most of the
Phrasetable. In Proceedings of 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 967?975, Prague, June.
[Koehn et al2003] P. Koehn, F. J. Och, and D. Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-03), pages 127?133, Edmonton, Alberta.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondr?ej Bo-
jar, Alexandra Constantine, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Annual Meeting of the Association for
Computational Linguistics (ACL), demonstration ses-
sion, pages 177?180, Prague, Czech Republic, June.
[Liang et al2006] Percy Liang, Alexandre Buchard-Co?te?,
Dan Klein, and Ben Taskar. 2006. An End-to-End
Discriminative Approach to Machine Translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
pages 761?768, Sydney, Australia.
[Mylonakis and Sima?an2010] Markos Mylonakis and
Khalil Sima?an. 2010. Learning Probabilistic Syn-
chronous CFGs for Phrase-based Translation. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning, pages 117?, Up-
psala,Sweden, July.
[Och2003] Franz Josef Och. 2003. Minimum Error Rate
Training in Statistical Machine Translation. In Proc. of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
[Shen et al2008] Wade Shen, Brian Delaney, Tim Ander-
son, and Ray Slyh. 2008. The MIT-LL/AFRL IWSLT-
2008 MT System. In Proceedings of IWSLT 2008,
pages 69?76, Hawaii, U.S.A., October.
[Venugopal et al2003] Ashish Venugopal, Stephan Vo-
gel, and Alex Waibel. 2003. Effective Phrase Transla-
tion Extraction from Alignment Models. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics, pages 319?326, Sapporo,
Japan, July.
[Wuebker et al2010] Joern Wuebker, Arne Mauser, and
Hermann Ney. 2010. Training phrase translation mod-
els with leaving-one-out. In Proceedings of the 48th
Annual Meeting of the Assoc. for Computational Lin-
guistics, pages 475?484, Uppsala, Sweden, July.
467
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 193?199,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2013
Stephan Peitz, Saab Mansour, Jan-Thorsten Peter, Christoph Schmidt,
Joern Wuebker, Matthias Huck, Markus Freitag and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
This paper describes the statistical ma-
chine translation (SMT) systems devel-
oped at RWTH Aachen University for
the translation task of the ACL 2013
Eighth Workshop on Statistical Machine
Translation (WMT 2013). We partici-
pated in the evaluation campaign for the
French-English and German-English lan-
guage pairs in both translation directions.
Both hierarchical and phrase-based SMT
systems are applied. A number of dif-
ferent techniques are evaluated, including
hierarchical phrase reordering, translation
model interpolation, domain adaptation
techniques, weighted phrase extraction,
word class language model, continuous
space language model and system combi-
nation. By application of these methods
we achieve considerable improvements
over the respective baseline systems.
1 Introduction
For the WMT 2013 shared translation task1
RWTH utilized state-of-the-art phrase-based and
hierarchical translation systems as well as an in-
house system combination framework. We give
a survey of these systems and the basic meth-
ods they implement in Section 2. For both
the French-English (Section 3) and the German-
English (Section 4) language pair, we investigate
several different advanced techniques. We con-
centrate on specific research directions for each
of the translation tasks and present the respec-
tive techniques along with the empirical results
they yield: For the French?English task (Sec-
tion 3.2), we apply a standard phrase-based sys-
tem with up to five language models including a
1http://www.statmt.org/wmt13/
translation-task.html
word class language model. In addition, we em-
ploy translation model interpolation and hierarchi-
cal phrase reordering. For the English?French
task (Section 3.1), we train translation mod-
els on different training data sets and augment
the phrase-based system with a hierarchical re-
ordering model, a word class language model,
a discriminative word lexicon and a insertion
and deletion model. For the German?English
(Section 4.3) and English?German (Section 4.4)
tasks, we utilize morpho-syntactic analysis to pre-
process the data (Section 4.1), domain-adaptation
(Section 4.2) and a hierarchical reordering model.
For the German?English task, an augmented hi-
erarchical phrase-based system is set up and we
rescore the phrase-based baseline with a continu-
ous space language model. Finally, we perform a
system combination.
2 Translation Systems
In this evaluation, we employ phrase-based trans-
lation and hierarchical phrase-based translation.
Both approaches are implemented in Jane (Vilar et
al., 2012; Wuebker et al, 2012), a statistical ma-
chine translation toolkit which has been developed
at RWTH Aachen University and is freely avail-
able for non-commercial use.2
2.1 Phrase-based System
In the phrase-based decoder (source cardinality
synchronous search, SCSS), we use the standard
set of models with phrase translation probabilities
and lexical smoothing in both directions, word and
phrase penalty, distance-based distortion model,
an n-gram target language model and three bi-
nary count features. Optional additional models
used in this evaluation are the hierarchical reorder-
ing model (HRM) (Galley and Manning, 2008), a
word class language model (WCLM) (Wuebker et
2http://www.hltpr.rwth-aachen.de/jane/
193
al., 2012), a discriminative word lexicon (DWL)
(Mauser et al, 2009), and insertion and deletion
models (IDM) (Huck and Ney, 2012). The param-
eter weights are optimized with minimum error
rate training (MERT) (Och, 2003). The optimiza-
tion criterion is BLEU.
2.2 Hierarchical Phrase-based System
In hierarchical phrase-based translation (Chiang,
2007), a weighted synchronous context-free gram-
mar is induced from parallel text. In addition to
continuous lexical phrases, hierarchical phrases
with up to two gaps are extracted. The search is
carried out with a parsing-based procedure. The
standard models integrated into our Jane hierar-
chical systems (Vilar et al, 2010; Huck et al,
2012c) are: phrase translation probabilities and
lexical smoothing probabilities in both translation
directions, word and phrase penalty, binary fea-
tures marking hierarchical phrases, glue rule, and
rules with non-terminals at the boundaries, four
binary count features, and an n-gram language
model. Optional additional models comprise IBM
model 1 (Brown et al, 1993), discriminative word
lexicon and triplet lexicon models (Mauser et al,
2009; Huck et al, 2011), discriminative reordering
extensions (Huck et al, 2012a), insertion and dele-
tion models (Huck and Ney, 2012), and several
syntactic enhancements like preference grammars
(Stein et al, 2010) and soft string-to-dependency
features (Peter et al, 2011). We utilize the cube
pruning algorithm for decoding (Huck et al, 2013)
and optimize the model weights with MERT. The
optimization criterion is BLEU.
2.3 System Combination
System combination is used to produce consensus
translations from multiple hypotheses generated
with different translation engines. First, a word
to word alignment for the given single system hy-
potheses is produced. In a second step a confusion
network is constructed. Then, the hypothesis with
the highest probability is extracted from this con-
fusion network. For the alignment procedure, one
of the given single system hypotheses is chosen as
primary system. To this primary system all other
hypotheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1.
The model weights of the system combination
are optimized with standard MERT on 100-best
lists. For each single system, a factor is added to
the log-linear framework of the system combina-
tion. Moreover, this log-linear model includes a
word penalty, a language model trained on the in-
put hypotheses, a binary feature which penalizes
word deletions in the confusion network and a pri-
mary feature which marks the system which pro-
vides the word order. The optimization criterion is
4BLEU-TER.
2.4 Other Tools and Techniques
We employ GIZA++ (Och and Ney, 2003) to train
word alignments. The two trained alignments are
heuristically merged to obtain a symmetrized word
alignment for phrase extraction. All language
models (LMs) are created with the SRILM toolkit
(Stolcke, 2002) and are standard 4-gram LMs
with interpolated modified Kneser-Ney smooth-
ing (Kneser and Ney, 1995; Chen and Goodman,
1998). The Stanford Parser (Klein and Manning,
2003) is used to obtain parses of the training data
for the syntactic extensions of the hierarchical sys-
tem. We evaluate in truecase with BLEU (Papineni
et al, 2002) and TER (Snover et al, 2006).
2.5 Filtering of the Common Crawl Corpus
The new Common Crawl corpora contain a large
number of sentences that are not in the labelled
language. To clean these corpora, we first ex-
tracted a vocabulary from the other provided cor-
pora. Then, only sentences containing at least
70% word from the known vocabulary were kept.
In addition, we discarded sentences that contain
more words from target vocabulary than source
vocabulary on the source side. These heuristics
reduced the French-English Common Crawl cor-
pus by 5,1%. This filtering technique was also ap-
plied on the German-English version of the Com-
mon Crawl corpus.
3 French?English Setups
We trained phrase-based translation systems for
French?English and for English?French. Cor-
pus statistics for the French-English parallel data
are given in Table 1. The LMs are 4-grams trained
on the provided resources for the respective lan-
guage (Europarl, News Commentary, UN, 109,
Common Crawl, and monolingual News Crawl
194
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
Table 1: Corpus statistics of the preprocessed
French-English parallel training data. EPPS de-
notes Europarl, NC denotes News Commentary,
CC denotes Common Crawl. In the data, numeri-
cal quantities have been replaced by a single cate-
gory symbol.
French English
EPPS Sentences 2.2M
+ NC Running Words 64.7M 59.7M
Vocabulary 153.4K 132.2K
CC Sentences 3.2M
Running Words 88.1M 80.9.0M
Vocabulary 954.8K 908.0K
UN Sentences 12.9M
Running Words 413.3M 362.3M
Vocabulary 487.1K 508.3K
109 Sentences 22.5M
Running Words 771.7M 661.1M
Vocabulary 1 974.0K 1 947.2K
All Sentences 40.8M
Running Words 1 337.7M 1 163.9M
Vocabulary 2 749.8K 2 730.1K
language model training data).3
3.1 Experimental Results English?French
For the English?French task, separate translation
models (TMs) were trained for each of the five
data sets and fed to the decoder. Four additional
indicator features are introduced to distinguish the
different TMs. Further, we applied the hierar-
chical reordering model, the word class language
model, the discriminative word lexicon, and the
insertion and deletion model. Table 2 shows the
results of our experiments.
As a development set for MERT, we use new-
stest2010 in all setups.
3.2 Experimental Results French?English
For the French?English task, a translation model
(TM) was trained on all available parallel data.
For the baseline, we interpolated this TM with
3The parallel 109 corpus is often also referred to as WMT
Giga French-English release 2.
an in-domain TM trained on EPPS+NC and em-
ployed the hierarchical reordering model. More-
over, three language models were used: The first
language model was trained on the English side
of all available parallel data, the second one on
EPPS and NC and the third LM on the News Shuf-
fled data. The baseline was improved by adding a
fourth LM trained on the Gigaword corpus (Ver-
sion 5) and a 5-gram word class language model
trained on News Shuffled data. For the WCLM,
we used 50 word classes clustered with the tool
mkcls (Och, 2000). All results are presented in Ta-
ble 3.
4 German?English Setups
For both translation directions of the German-
English language pair, we trained phrase-based
translation systems. Corpus statistics for German-
English can be found in Table 4. The language
models are 4-grams trained on the respective tar-
get side of the bilingual data as well as on the
provided News Crawl corpus. For the English
language model the 109 French-English, UN and
LDC Gigaword Fifth Edition corpora are used ad-
ditionally.
4.1 Morpho-syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation, the German text
is preprocessed by splitting German compound
words with the frequency-based method described
in (Koehn and Knight, 2003). To further reduce
translation complexity, we employ the long-range
part-of-speech based reordering rules proposed by
Popovic? and Ney (2006).
4.2 Domain Adaptation
This year, we experimented with filtering and
weighting for domain-adaptation for the German-
English task. To perform adaptation, we define a
general-domain (GD) corpus composed from the
news-commentary, europarl and Common Crawl
corpora, and an in-domain (ID) corpus using
a concatenation of the test sets (newstest{2008,
2009, 2010, 2011, 2012}) with the correspond-
ing references. We use the test sets as in-domain
195
Table 2: Results for the English?French task (truecase). newstest2010 is used as development set.
BLEU and TER are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011 newstest2012
English?French BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
TM:EPPS + HRM 22.9 63.0 25.0 60.0 27.8 56.7 28.9 54.4 27.2 57.1
TM:UN + HRM 22.7 63.4 25.0 60.0 28.3 56.4 29.5 54.2 27.3 57.1
TM:109 + HRM 23.5 62.3 26.0 59.2 29.6 55.2 30.3 53.3 28.0 56.4
TM:CC + HRM 23.5 62.3 26.2 58.8 29.2 55.3 30.3 53.3 28.2 56.0
TM:NC 21.0 64.8 22.3 61.6 25.6 58.7 26.9 56.6 25.7 58.5
+ HRM 21.5 64.3 22.6 61.2 26.1 58.4 27.3 56.1 26.0 58.2
+ TM:EPPS,CC,UN 23.9 61.8 26.4 58.6 29.9 54.7 31.0 52.7 28.6 55.6
+ TM:109 24.0 61.5 26.5 58.4 30.2 54.2 31.1 52.3 28.7 55.3
+ WCLM, DWL, IDM 24.0 61.6 26.5 58.3 30.4 54.0 31.4 52.1 28.8 55.2
Table 3: Results for the French?English task (truecase). newstest2010 is used as development set.
BLEU and TER are given in percentage.
newstest2010 newstest2011 newstest2012
French?English BLEU TER BLEU TER BLEU TER
SCSS baseline 28.1 54.6 29.1 53.3 - -
+ GigaWord.v5 LM 28.6 54.2 29.6 52.9 29.6 53.3
+ WCLM 29.1 53.8 30.1 52.5 29.8 53.1
(newswire) as the other corpora are coming from
differing domains (news commentary, parliamen-
tary discussions and various web sources), and on
initial experiments, the other corpora did not per-
form well when used as an in-domain representa-
tive for adaptation. To check whether over-fitting
occurs, we measure the results of the adapted
systems on the evaluation set of this year (new-
stest2013) which was not used as part of the in-
domain set.
The filtering experiments are done similarly to
(Mansour et al, 2011), where we compare filtering
using LM and a combined LM and IBM Model 1
(LM+M1) based scores. The scores for each sen-
tence pair in the general-domain corpus are based
on the bilingual cross-entropy difference of the
in-domain and general-domain models. Denoting
HLM (x) as the cross entropy of sentence x ac-
cording to LM , then the cross entropy difference
DHLM (x) can be written as:
DHLM (x) = HLMID(x)?HLMGD(x)
The bilingual cross entropy difference for a sen-
tence pair (s, t) in the GD corpus is then defined
by:
DHLM (s) + DHLM (t)
For IBM Model 1 (M1), the cross-entropy
HM1(s|t) is defined similarly to the LM cross-
entropy, and the resulting bilingual cross-entropy
difference will be of the form:
DHM1(s|t) + DHM1(t|s)
The combined LM+M1 score is obtained by
summing the LM and M1 bilingual cross-entropy
difference scores. To perform filtering, the GD
corpus sentence pairs are scored by the appropri-
ate method, sorted by the score, and the n-best sen-
tences are then used to build an adapted system.
In addition to adaptation using filtering, we ex-
periment with weighted phrase extraction similar
to (Mansour and Ney, 2012). We differ from their
work by using a combined LM+M1 weight to per-
form the phrase extraction instead of an LM based
weight. We use a combined LM+M1 weight as
this worked best in the filtering experiments, mak-
ing scoring with LM+M1 more reliable than LM
scores only.
4.3 Experimental Results German?English
For the German?English task, the baseline is
trained on all available parallel data and includes
the hierarchical reordering model. The results of
the various filtering and weighting experiments are
summarized in Table 5.
196
Table 5: German-English results (truecase). BLEU and TER are given in percentage. Corresponding
development set is marked with *. ? labels the single systems selected for the system combination.
newstest2009 newstest2010 newstest2011 newstest2012 newstest2013
German?English BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
SCSS baseline 21.7 61.1 24.8* 58.9* 22.0 61.1 23.4 60.0 26.1 56.4
LM 800K-best 21.6 60.5 24.7* 58.3* 22.0 60.5 23.6 59.7 - -
LM+M1 800K-best 21.4 60.5 24.7* 58.1* 22.0 60.4 23.7 59.2 - -
(LM+M1)*TM 22.1 60.2 25.4* 57.8* 22.5 60.1 24.0 59.1 - -
(LM+M1)*TM+GW 22.8 59.5 25.7* 57.2* 23.1 59.5 24.4 58.6 26.6 55.5
(LM+M1)*TM+GW? 22.9* 61.1* 25.2 59.3 22.8 61.5 23.7 60.8 26.4 57.1
SCSS baseline 22.6* 61.6* 24.1 60.1 22.1 62.0 23.1 61.2 - -
CSLM rescoring? 22.0 60.4 25.1* 58.3* 22.4 60.2 23.9 59.3 26.0 56.0
HPBT? 21.9 60.4 24.9* 58.2* 22.3 60.3 23.6 59.6 25.9 56.3
system combination - - - - 23.4* 59.3* 24.7 58.5 27.1 55.3
Table 6: English-German results (truecase). newstest2009 was used as development set. BLEU and TER
are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011 newstest2012
English?German BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
SCSS baseline 14.9 70.9 14.9 70.4 16.0 66.3 15.4 69.5 15.7 67.5
LM 800K-best 15.1 70.9 15.1 70.3 16.2 66.3 15.6 69.4 15.9 67.4
(LM+M1) 800K-best 15.8 70.8 15.4 70.0 16.2 66.2 16.0 69.3 16.1 67.4
(LM+M1) ifelse 16.1 70.6 15.7 69.9 16.5 66.0 16.2 69.2 16.3 67.2
Table 4: Corpus statistics of the preprocessed
German-English parallel training data (Europarl,
News Commentary and Common Crawl). In the
data, numerical quantities have been replaced by a
single category symbol.
German English
Sentences 4.1M
Running Words 104M 104M
Vocabulary 717K 750K
For filtering, we use the 800K best sentences
from the whole training corpora, as this se-
lection performed best on the dev set among
100K,200K,400K,800K,1600K setups. Filtering
seems to mainly improve on the TER scores, BLEU
scores are virtually unchanged in comparison to
the baseline. LM+M1 filtering improves further
on TER in comparison to LM-based filtering.
The weighted phrase extraction performs best
in our experiments, where the weights from the
LM+M1 scoring method are used. Improvements
in both BLEU and TER are achieved, with BLEU
improvements ranging from +0.4% up-to +0.6%
and TER improvements from -0.9% and up-to -
1.1%.
As a final step, we added the English Gigaword
corpus to the LM (+GW). This resulted in further
improvements of the systems.
In addition, the system as described above was
tuned on newstest2009. Using this development
set results in worse translation quality.
Furthermore, we rescored the SCSS baseline
tuned on newstest2009 with a continuous space
language model (CSLM) as described in (Schwenk
et al, 2012). The CSLM was trained on the eu-
roparl and news-commentary corpora. For rescor-
ing, we used the newstest2011 set as tuning set and
re-optimized the parameters with MERT on 1000-
best lists. This results in an improvement of up to
0.8 points in BLEU compared to the baseline.
We compared the phrase-based setups with a
hierarchical translation system, which was aug-
mented with preference grammars, soft string-
to-dependency features, discriminative reordering
extensions, DWL, IDM, and discriminative re-
197
ordering extensions. The phrase table of the hier-
archical setup has been extracted from News Com-
mentary and Europarl parallel data only (not from
Common Crawl).
Finally, three setups were joined in a system
combination and we gained an improvement of up
to 0.5 points in BLEU compared to the best single
system.
4.4 Experimental Results English?German
The results for the English?German task are
shown in Table 6. While the LM-based filter-
ing led to almost no improvement over the base-
line, the LM+M1 filtering brought some improve-
ments in BLEU. In addition to the sentence fil-
tering, we tried to combine the translation model
trained on NC+EPPS with a TM trained on Com-
mon Crawl using the ifelse combination (Mansour
and Ney, 2012). This combination scheme con-
catenates both TMs and assigns the probabilities
of the in-domain TM if it contains the phrase,
else it uses the probabilities of the out-of-domain
TM. Appling this method, we achieved further im-
provements.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, RWTH experimented with both
phrase-based and hierarchical translation systems.
Several different techniques were evaluated and
yielded considerable improvements over the re-
spective baseline systems as well as over our last
year?s setups (Huck et al, 2012b). Among these
techniques are a hierarchical phrase reordering
model, translation model interpolation, domain
adaptation techniques, weighted phrase extraction,
a word class language model, a continuous space
language model and system combination.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311, June.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, Massachusetts, USA, August.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201?
228.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, Hawaii, USA,
October.
Matthias Huck and Hermann Ney. 2012. Insertion and
Deletion Models for Statistical Machine Translation.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics - Human
Language Technologies conference, pages 347?351,
Montre?al, Canada, June.
Matthias Huck, Saab Mansour, Simon Wiesler, and
Hermann Ney. 2011. Lexicon Models for Hierar-
chical Phrase-Based Machine Translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 191?198, San
Francisco, California, USA, December.
Matthias Huck, Stephan Peitz, Markus Freitag, and
Hermann Ney. 2012a. Discriminative Reordering
Extensions for Hierarchical Phrase-Based Machine
Translation. In 16th Annual Conference of the Eu-
ropean Association for Machine Translation, pages
313?320, Trento, Italy, May.
Matthias Huck, Stephan Peitz, Markus Freitag, Malte
Nuhn, and Hermann Ney. 2012b. The RWTH
Aachen Machine Translation System for WMT
2012. In NAACL 2012 Seventh Workshop on
Statistical Machine Translation, pages 304?311,
Montre?al, Canada, June.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012c. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37?
50, October.
Matthias Huck, David Vilar, Markus Freitag, and
Hermann Ney. 2013. A Performance Study of
Cube Pruning for Large-Scale Hierarchical Machine
Translation. In Proceedings of the NAACL 7thWork-
shop on Syntax, Semantics and Structure in Statis-
tical Translation, pages 29?38, Atlanta, Georgia,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proc. of the 41th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 423?430, Sapporo, Japan,
July.
198
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-gram Language Model-
ing. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 181?184, May.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings of
European Chapter of the ACL (EACL 2009), pages
187?194.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
ACL 2007 Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic, June.
Saab Mansour and Hermann Ney. 2012. A Simple and
Effective Weighted Phrase Extraction for Machine
Translation Adaptation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 193?200, Hong Kong, December.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, California, USA, December.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Proc. of the Conf. on Empirical Methods for Natu-
ral Language Processing (EMNLP), pages 210?218,
Singapore, August.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2000. mkcls: Training
of word classes for language modeling.
http://www.hltpr.rwth-aachen.de/
web/Software/mkcls.html.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
Jan-Thorsten Peter, Matthias Huck, Hermann Ney, and
Daniel Stein. 2011. Soft String-to-Dependency
Hierarchical Machine Translation. In International
Workshop on Spoken Language Translation, pages
246?253, San Francisco, California, USA, Decem-
ber.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Transla-
tion. In International Conference on Language Re-
sources and Evaluation, pages 1278?1283, Genoa,
Italy, May.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space
Language Models on a GPU for Statistical Machine
Translation. In NAACL-HLT 2012 Workshop: Will
We Ever Really Replace the N-gram Model? On the
Future of Language Modeling for HLT, pages 11?
19, Montre?al, Canada, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Daniel Stein, Stephan Peitz, David Vilar, and Hermann
Ney. 2010. A Cocktail of Deep Syntactic Fea-
tures for Hierarchical Machine Translation. In Conf.
of the Association for Machine Translation in the
Americas (AMTA), Denver, Colorado, USA, Octo-
ber/November.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, Colorado, USA,
September.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216, September.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statis-
tical Machine Translation. In International Confer-
ence on Computational Linguistics, pages 483?491,
Mumbai, India, December.
199
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 309?319,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Length-incremental Phrase Training for SMT
Joern Wuebker and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University
Aachen, Germany
{wuebker,ney}@cs.rwth-aachen.de
Abstract
We present an iterative technique to gener-
ate phrase tables for SMT, which is based
on force-aligning the training data with
a modified translation decoder. Differ-
ent from previous work, we completely
avoid the use of a word alignment or
phrase extraction heuristics, moving to-
wards a more principled phrase generation
and probability estimation. During train-
ing, we allow the decoder to generate new
phrases on-the-fly and increment the max-
imum phrase length in each iteration. Ex-
periments are carried out on the IWSLT
2011 Arabic-English task, where we are
able to reach moderate improvements on a
state-of-the-art baseline with our training
method. The resulting phrase table shows
only a small overlap with the heuristically
extracted one, which demonstrates the re-
strictiveness of limiting phrase selection
by a word alignment or heuristics. By
interpolating the heuristic and the trained
phrase table, we can improve over the
baseline by 0.5% BLEU and 0.5% TER.
1 Introduction
Most state-of-the-art SMT systems get the statis-
tics from which the different component models
are estimated via heuristics using a Viterbi word
alignment. The word alignment is usually gener-
ated with tools like GIZA++ (Och and Ney, 2003),
that apply the EM algorithm to estimate the align-
ment with the HMM or IBM-4 translation mod-
els. This is also the case for the phrases or rules
which serve as translation units for the decoder.
All phrases that do not violate the word alignment
are extracted and their probabilities are estimated
as relative frequencies (Koehn et al, 2003).
A number of different approaches have tried to
do away with the heuristics and close this gap be-
tween the phrase table generation and translation
decoding. However, most of these approaches ei-
ther fail to achieve state-of-the-art performance or
still make use of the word alignment or the ex-
traction heuristics, e.g. as a prior in discriminative
training or to initialize a generative or generatively
inspired training procedure and are thus biased by
their weaknesses. Here, we aim at moving towards
the ideal situation, where a unified framework in-
duces the phrases based on the same models as in
decoding.
We train the phrase table without using a word
alignment or the extraction heuristics. Different
from previous work, we are able to generate all
possible phrase pairs on-the-fly during the train-
ing procedure. A further advantage of our pro-
posed algorithm is that we use basically the same
beam search as in translation. This makes it easy
to re-implement by modifying any translation de-
coder, and makes sure that training and translation
are consistent. In principle, we apply the forced
decoding approach described in (Wuebker et al,
2010) with cross-validation to prevent over-fitting,
but we initialize the phrase table with IBM-1 lex-
ical probabilities (Brown et al, 1993) instead of
heuristically extracted relative frequencies. The
algorithm is extended with the concept of back-
off phrases, so that new phrase pairs can be gener-
ated at training time. The size of the newly gener-
ated phrases is incremented over the training iter-
ations. By introducing fallback decoding runs, we
are able to successfully align the complete training
data. Local language models are used for better
phrase pair pre-selection.
309
The experiments are carried out on the IWSLT
2011 Arabic-English shared task. We are able to
show that it is possible and feasible to reach state-
of-the-art performance without the need to word-
align the bilingual training data. The small over-
lap of 18.5% between the trained and the heuristi-
cally extracted phrase table demonstrates the limi-
tations of previous work, where training is initial-
ized by the baseline phrase table or phrase selec-
tion is restricted by a word alignment. With a lin-
ear interpolation of phrase tables an improvement
of 0.5% BLEU and 0.5% TER over the baseline
can be achieved. The result in BLEU is statisti-
cally significant on the test set with 90% confi-
dence. Further, we can confirm the observation
of previous work, that phrases with near-zero en-
tropies seem to be a disadvantage for translation
quality. Although we use a phrase-based decoder
here, the principles of our work can be applied to
any statistical machine translation paradigm. The
software used for our experiments is available un-
der a non-commercial open source licence.
The paper is organized as follows. We review
related work in Section 2. The decoder and its
features are described in Section 3 and we give
an overview of the training procedure in Section
4. The complete algorithm is described in Section
5 and experiments are presented in Section 6. We
conclude with Section 7.
2 Related Work
Marcu and Wong (2002) present a joint probabil-
ity model, which is trained with a hill-climbing
technique based on break, merge, swap and move
operations. Due to the computational complexity
they are only able to consider phrases, which ap-
pear at least five times in the data. The model is
shown to slightly underperform heuristic extrac-
tion in (Koehn et al, 2003). For higher efficiency,
it is constrained by a word alignment in (Birch et
al., 2006). DeNero et al (2008) introduce a differ-
ent training procedure for this model based on a
Gibbs sampler. They make use of the word align-
ment for initialization.
A generative phrase model trained with the
Expectation-Maximization (EM) algorithm is
shown in (DeNero et al, 2006). It also does not
reach the same top performance as heuristic ex-
traction. The authors identify the hidden segmen-
tation variable, which results in over-fitting, as the
main problem.
Liang et al (2006) present a discriminative
translation system. One of the proposed strategies
for training, which the authors call bold updating,
is similar to our training scheme. They use heuris-
tically extracted phrase translation probabilities as
blanket features in all setups.
Another iteratively-trained phrase model is de-
scribed by Moore and Quirk (2007). Their model
is segmentation-free and, confirming the findings
in (DeNero et al, 2006), can close the gap to
phrase tables induced from surface heuristics. It
relies on word alignment for phrase selection.
Mylonakis and Sima?an (2008) present a phrase
model, whose training procedure uses prior prob-
abilities based on Inversion Transduction Gram-
mar and smoothing as learning objective to pre-
vent over-fitting. They also rely on the word align-
ment to select phrase pairs.
Blunsom et al (2009) perform inference over
latent synchronous derivation trees under a non-
parametric Bayesian model with a Gibbs sampler.
Training is also initialized by extracting rules from
a word alignment, but the authors let the sampler
diverge from the initial value for 1000 passes over
the data, before the samples are used. However,
as the model is to weak for actual translation, the
usual extraction heuristics are applied on the hier-
archical alignments to infer a distribution over rule
tables.
Wuebker et al (2010) use a forced decoding
training procedure, which applies a leave-one-out
technique to prevent over-fitting. They are able to
show improvements over a heuristically extracted
phrase table, which is used for initialization of the
training.
In (Saers and Wu, 2011), the EM algorithm is
applied for principled induction of bilexica based
on linear inversion transduction grammar. The
model itself underperforms the baseline, but the
authors show moderate improvements by combin-
ing it with the baseline phrase table, which is sim-
ilar to our results.
(Neubig et al, 2011) also propose a probabilis-
tic model based on inversion transduction gram-
mar, which allows for direct phrase table extrac-
tion from unaligned data. They show results simi-
lar to the heuristic baseline on several tasks.
A number of different models that can be
trained from forced derivation trees are shown in
(Duan et al, 2012), including a re-estimated trans-
lation model, two reordering models and a rule se-
310
quence model. For inference, they optimize their
parameters towards alignment F-score. The forced
derivations are initialized with the standard heuris-
tic extraction scheme.
He and Deng (2012) describe a discriminative
phrase training procedure, where n-best transla-
tions are produced by the decoder on the whole
training data. The heuristically extracted relative
frequencies serve as a prior, and the probabili-
ties are updated with a maximum BLEU criterion
based on the n-best lists.
3 Translation Model
We use the standard phrase-based translation de-
coder from the open source toolkit Jane 2 (Wue-
bker et al, 2012a) for both the training proce-
dure and the translation experiments. It makes use
of the usual features: Translation channel mod-
els in both directions, lexical smoothing models in
both directions, an n-gram language model (LM),
phrase and word penalty and a jump-distance-
based distortion model. Formally, the best trans-
lation e?I?1 as defined by the models hm(eI1, sK1 , fJ1 )
can be written as (Och and Ney, 2004)
e?I?1 = argmax
I,eI1
{ M?
m=1
?mhm(eI1, sK1 , fJ1 )
}
, (1)
where fJ1 = f1 . . . fJ is the source sentence,
eI1 = e1 . . . eI the target sentence and sK1 =
s1 . . . sK their phrase segmentation and align-
ment. We define sk := (ik, bk, jk), where ik is
the last position of kth target phrase, and (bk, jk)
are the start and end positions of the source phrase
aligned to the kth target phrase. Different from
many standard systems, the lexical smoothing
scores are not estimated by extracting counts from
a word alignment, but with IBM-1 model scores
trained on the bilingual data with GIZA++. They
are computed as (Zens, 2008)
hlex(eI1, sK1 , fJ1 ) =
K?
k=1
jk?
j=bk
log
?
?p(fj |e0) +
ik?
i=ik?1+1
p(fj |ei)
?
?
(2)
Here, e0 denotes the empty target word. The
lexical smoothing model for the inverse direc-
tion is computed analogously. The log-linear fea-
ture weights ?m are optimized on a development
data set with minimum error rate training (MERT)
(Och, 2003). As optimization criterion we use
BLEU (Papineni et al, 2001).
4 Training
4.1 Overview
In this work we employ a training procedure in-
spired by the Expectation-Maximization (EM) al-
gorithm.
The E-step corresponds to force-aligning the
training data with a modified translation decoder,
which yields a distribution over possible phrasal
segmentations and their alignment. Different from
original EM, we make use of not only the two
translation channel models that are being learned,
but the full log-linear combination of models as in
translation decoding. Formally, we are searching
for the best phrase segmentation and alignment for
the given sentence pair, which is defined by
s?K?1 = argmax
K,sK1
{ M?
m=1
?mhm(eI1, sK1 , fJ1 )
}
(3)
To force-align the training data, the translation
decoder is constrained to the given target sentence.
The translation candidates applicable for each sen-
tence pair are selected through a bilingual phrase
matching before the actual search.
In the M-step, we re-estimate the phrase table
from the phrase alignments. The translation prob-
ability of a phrase pair (f? , e?) is estimated as
pFA(f? |e?) =
CFA(f? , e?)?
f? ?
CFA(f? ?, e?)
(4)
where CFA(f? , e?) is the count of the phrase pair
(f? , e?) in the phrase-aligned training data.
In contrast to original EM, this is done by tak-
ing the phrase counts from a uniformly weighted
n-best list. The limitation to n phrase alignments
helps keeping the number of considered phrases
reasonably small. Because the log-linear feature
weights have been tuned in a discriminative fash-
ion to optimize the ranking of translation hypothe-
ses, rather than their probability distribution, pos-
terior probabilities received by exponentiation and
renormalization need to be scaled similar to (Wue-
bker et al, 2012b). Uniform weights can alle-
viate this mismatch between the discriminatively
311
trained log-linear feature weights and the actual
probability distribution, without having to resort
to an arbitrarily chosen global scaling factor. This
corresponds to the count model in (Wuebker et al,
2010) and was shown by the authors to perform
similar or better than using actual posterior proba-
bilities. In our experiments, we set the size of the
n-best list to n = 1000.
The first iteration of phrase training is initialized
with an empty phrase table. We use the notion of
backoff phrases to generate new phrase pairs on-
the-fly. To avoid over-fitting, we apply the cross-
validation technique presented in (Wuebker et al,
2010) with a batch-size of 2000 sentences. This
means that for each batch the phrase and marginal
counts from the full phrase table are reduced by
the statistics taken from the same batch in the pre-
vious iteration. The phrase translation probabili-
ties are then estimated from these updated counts.
Phrase pairs only appearing in a single batch are
assigned a fixed penalty.
4.2 Backoff Phrases
Backoff phrases are phrase pairs that are generated
on-the-fly by the decoder at training time. When
aligning a sentence pair, for a given maximum
phrase length m, the decoder inserts all combi-
nations of source ms-grams and target mt-grams
into the translation options, that are present in the
sentence pair and with ms,mt ? m. Formally,
for the sentence pair (fJ1 , eI1), fJ1 = f1 . . . fJ ,
eI1 = e1 . . . eI , and maximum length m, we gen-
erate all phrase pairs (f? , e?) where
?ms,mt, j, i :
1 ? ms,mt ? m ? 1 ? j ? J ?ms + 1
? 1 ? i ? I ?mt + 1
? f? = f (j+ms?1)j ? e? = e
(i+mt?1)
i . (5)
These generated phrase pairs are given a fixed
penalty penp per phrase, pens per source word and
pent per target word, which are summed up and
substituted for the two channel models. The lex-
ical smoothing scores are computed in the usual
way based on an IBM-1 table. Note that this table
is not extracted from a word alignment, but con-
tains the real probabilities trained with the IBM-1
model by GIZA++.
We use backoff phrases in two different con-
texts. In the first mmax = 6 iterations, they are
applied as a means to generate new phrase pairs on
the fly. We increase the maximum phrase length
m in each iteration and always generate all possi-
ble backoff phrases before aligning each sentence.
Later, when a sufficient number of phrases have
been generated in the previous iterations, they are
used as a last resort in order to avoid alignment
failures.
At the later stage of the length-incremental
training, we also make use of a modified version,
where we only allow new phrase pairs (f? , e?) to be
generated, if no translation candidates exist for f?
after the bilingual phrase matching. However, in
this case, backoff phrases are only used if a first
decoding run fails and we have to resort to fallback
runs, which are described in the next Section.
4.3 Fallback Decoding Runs
To maximize the number of successfully aligned
sentences, we allow for fallback decoding runs
with slightly altered parameterization, whenever
constrained decoding fails. In this work, we
only change the parameterization of the backoff
phrases. After mmax = 6 iterations, we no longer
generate any backoff phrases in the first decoding
run. If it fails, a second run is performed, where
we allow to generate backoff phrases for all source
phrases, which have no target candidates after the
bilingual phrase matching. Finally, if this one also
fails, all possible phrases are generated in the third
run. Here, the maximum backoff phrase length is
fixed to m = 1. We denote the number of fallback
runs with nfb = 2. In our experiments, the two
fallback runs enable us to align every sentence pair
of the training data after the sixth iteration.
4.4 Local Language Models
To make the training procedure feasible, it is par-
allelized by splitting the training data into batches
of 2000 sentences. The batches are aligned inde-
pendently. For each batch, we produce a local
language model, which is a unigram LM trained
on the target side of the current batch. We pre-
sort the phrases before search by their log-linear
model score, which uses the phrase-internal uni-
gram LM costs as one feature function. One ef-
fect of this is that the order in which phrase candi-
dates are considered is adjusted to the local part of
the data, which has a positive effect on decoding
speed. Secondly, we limit the number of transla-
tion candidates for each source phrase to the best
scoring 500 before the bilingual phrase matching.
312
 23
 24
 25
 26
 27
 28
 2  3  4  5  6
 80
 90
 100
B
L
E
U
[
%
]
w
o
r
d
 
c
o
v
e
r
a
g
e
 
[
%
]
Iteration
wp=-0.1
wp=-0.3
wp=-0.5
wp=-0.7
Figure 1: BLEU scores and word coverages on
dev over the first 6 training iterations with dif-
ferent word penalties (wp).
Using the local LM for this means that the pre-
selection better suits the current data batch. As a
result, the number of phrases remaining after the
phrase matching is increased as compared to the
same setup without a local language model.
4.5 Parameterization
The training procedure has a number of hyper pa-
rameters, most of which do not seem to have a
strong impact on the results. This section de-
scribes the parameters that have to be chosen care-
fully. To successfully align a sentence pair, our
decoder is required to fully cover the source sen-
tence. However, in order to achieve a good suc-
cess rate in terms of number of aligned sentence
pairs, we allow for incompletely aligned target
sentences. We denote the percentage of success-
fully aligned sentence pairs as sentence coverage.
Note that we count a sentence pair as successfully
aligned, even if the target sentence is not fully
covered. the word penalty (wp) feature weight
?wp needs to be adjusted carefully. A high value
leads to a high sentence coverage, but many of
their target sides may be incompletely aligned. A
 24
 25
 26
 27
 2  3  4  5  6
 0
 20
 40
 60
 80
 100
B
L
E
U
[
%
]
s
u
r
p
l
u
s
 
p
h
r
a
s
e
s
 
[
%
]
Iteration
pen0=4pen0=3pen0=2pen0=1pen0=0.5
Figure 2: BLEU scores and percentage of surplus
phrases on dev over the first 6 training iterations
with different backoff phrase penalties pen0.
low word penalty can decrease the sentence cover-
age, while aligning larger parts of the target sen-
tences. We denote the total percentage of suc-
cessfully aligned target words as word coverage.
Please note the distincton to the sentence cover-
age, which is defined above. Figure 1 shows the
word coverages and BLEU scores for training iter-
ations 2 through 6 with different word penalties. In
the first iteration, the results are identical, as only
one-to-one phrases are allowed and the number of
aligned target words is therefore predetermined.
For ?wp = ?0.1, the word coverages are continu-
ously decreasing with each iteration, although not
by much. For ?wp = ?0.3 to ?wp = ?0.7 the
word coverage slightly increases from iteration 2
to 3 and then decreases again. In terms of BLEU
score, ?wp = ?0.3 has a slight advantage over the
other values and we decided to continue using this
value in all subsequent experiments.
The backoff phrase penalties directly affect
the learning rate of the training procedure. With
low penalties, only few, very good phrases get
an advantage over the ones generated on-the-fly,
which corresponds to a slow learning rate. In-
313
1. Initialize with empty phrase table
2. Set backoff phrase penalties to
pen0 = 3 and m = 1
3. Until m = mmax, iterate:
? If iteration > 1: set
m = m+ 1
?s2t = ?s2t + ?
?t2s = ?t2s + ?
? Force-align training data and
re-estimate phrase table
4. Set m = 1 and nfb = 2
5. Iterate:
? Force-align training data and
re-estimate phrase table
Figure 3: The complete training algorithm.
creasing the penalties means that a larger per-
centage of the phrase pairs generated in the pre-
vious iterations will be favored over new back-
off phrases, which corresponds to a faster learn-
ing rate. We denote phrase pairs that are more
expensive than their backoff phrase counterparts
as surplus phrases. Figure 2 shows the behavior
over the training iterations 2 through 6 with differ-
ent penalties pen0 in terms of percentage of sur-
plus phrase pairs and BLEU score. Here we set
pens = pent = pen0 and penp = 5pen0. We can
see that pen0 = 4 yields less than 0.1% surplus
phrases through all iterations, whereas pen0 = 0.5
starts off with 98.2% surplus phrases and goes
down to 55.9% in iteration 6. In terms of BLEU, a
fast learning rate seems to be preferable. The best
results are achieved with pen0 = 3, where the rate
of surplus phrases starts at 6.8% and decreases to
1.7% until iteration 6. In all subsequent experi-
ments, we set pen0 = 3.
5 Length-incremental Training
In this section we describe the complete training
algorithm. The first training iteration is initial-
ized with an empty phrase table. The phrases used
in alignment are backoff phrases, which are gen-
erated on-the-fly. The maximum backoff phrase
length is set to m = 1. Then the forced alignment
is iterated, increasing m by 1 in each iteration, up
to a maximum of mmax = 6.
After mmax = 6 iterations, we have created a
sufficient number of phrase pairs and continue it-
erating the training procedure with new parame-
Arabic English
train Sentences 305K
Running Words 6.5M 6.5M
Vocabulary 104K 74K
dev Sentences 934
Run. Words 19K 20K
Vocabulary 4293 3182
OOVs (run. words) 445 182
test Sentences 1664
Run. Words 31K 32K
Vocabulary 5415 3650
OOVs (run. words) 658 159
Table 1: Statistics for the IWSLT 2011 Arabic-
English data. The out-of-vocabulary words are de-
noted as OOVs.
ters. Now, we do not allow usage of any back-
off phrases in the first decoding run. If the first
run fails, we allow a fallback decoding run, where
backoff phrases are generated only for source
phrases without translation candidates. If this
one also fails, in a final fallback run all possible
phrases are generated. Here we allow a maximum
backoff phrase length of m = 1.
The log-linear feature weights ?i used for train-
ing are mostly standard values. Only ?wp for
the word penalty is adjusted as described in Sec-
tion 4.5, and ?s2t,?t2s for the two phrasal channel
models are incremented with each iteration. We
start off with ?s2t = ?t2s = 0 and increment the
weights by ? = 0.02 in each iteration, until the
standard value ?s2t = ?t2s = 0.1 is reached in
iteration 6, after which the values are kept fixed.
MERT is not part of the training procedure, but
only used afterwards for evaluation. The full algo-
rithm is illustrated in Figure 3.
6 Experiments
6.1 Data
We carry out our experiments on the IWSLT 2011
Arabic-English shared task1. It focuses on the
translation of TED talks, a collection of lectures
on a variety of topics ranging from science to cul-
ture. Our bilingual training data is composed of all
available in-domain (TED) data and a selection of
the out-of-domain MultiUN data provided for the
evaluation campaign. The bilingual data selection
1www.iwslt2011.org
314
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 2  4  6  8  10  12  14  16  18  20
B
L
E
U
[
%
]
Iteration
Length-incremental Training (dev)Baseline (dev)Length-incremental Training (test)Baseline (test)
Figure 4: BLEU scores on dev and test over 20
training iterations.
is based on (Axelrod et al, 2011). Data statistics
are given in Table 1. The language model is a 4-
gram LM trained on all provided in-domain mono-
lingual data and a selection based on (Moore and
Lewis, 2010) of the out-of-domain corpora. To ac-
count for statistical variation, all reported results
are average scores over three independent MERT
runs.
6.2 Results
To build the baseline phrase table, we perform
the standard phrase extraction from a symmetrized
word alignment created with the IBM-4 model by
GIZA++. The length of the extracted phrases is
limited to a maximum of six words. The lexical
smoothing scores are computed from IBM-1 prob-
abilities. We run MERT on the development set
(dev) and evaluate on the test set (test). A sec-
ond baseline is the technique described in (Wue-
bker et al, 2010), which we denote as leave-one-
out. It is initialized with the heuristically extracted
table and run for one iteration, which the authors
have shown to be sufficient.
Length-incremental training is performed as de-
scribed in Section 5. After each iteration, we run
MERT on dev using the resulting phrase table and
evaluate. The set of models used here is identical
to the baseline.
The results in BLEU are plotted in Figure 4. We
can see that the performance increases up to it-
eration 5, after which only small changes can be
observed. The performance on dev is similar to
dev test
BLEU TER BLEU TER
[%] [%] [%] [%]
baseline 27.4 54.0 24.6 57.8
leave-one-out 27.3 54.2 24.6 57.7
length-increm. 27.5 53.8 24.9 57.4
lin. interp. 27.9 53.5 25.1? 57.3
Table 2: BLEU and TER scores of the baseline,
phrase training with leave-one-out and length-
incremental training after 12 iterations, as well as
a linear interpolation of the baseline with length-
incremental phrase table. Results marked with ?
are statistically significant with 90% confidence.
the baseline, on test the trained phrase tables
are consistently slightly above the baseline. The
optimum on dev is reached in iteration 12. Ex-
act BLEU and TER (Snover et al, 2006) scores of
the optimum on dev and the baseline are given
in Table 2. The phrase table trained with leave-
one-out (Wuebker et al, 2010) performs simlar to
the heuristic baseline. Length-incremental train-
ing is slightly superior to the baseline, yielding
an improvement of 0.3% BLEU and 0.4% TER
on test. Similar to results observed in (DeN-
ero et al, 2006) and (Wuebker et al, 2010), a lin-
ear interpolation with the baseline containing all
phrase pairs from either of the two tables yields a
moderate improvement of 0.5% BLEU and 0.5%
TER both data sets. The BLEU improvement on
test is statistically significant with 90% confi-
dence based on bootstrap resampling as described
by Koehn (2004).
6.3 Analysis
In Figure 5 we plot the number of phrase pairs
present in the phrase tables after each iteration.
In the first 6 iterations, we keep generating new
phrase pairs via backoff phrases. The maximum
of 14.4M phrase pairs is reached after three itera-
tions. For comparison, the size of the heuristically
extracted table is 19M phrase pairs. Afterwards,
backoff phrases are only used in fallback decoding
runs, which leads to drop in the number of phrase
pairs that are being used. It levels out at 10.4M
phrases.
When we take a look at the phrase length distri-
butions in both the baseline and the trained phrase
table shown in Figure 6, we can see that in the lat-
ter the phrases are generally shorter, which con-
315
5.0 M
10.0 M
15.0 M
 2  4  6  8  10  12  14  16  18  20
#
 
p
h
r
a
s
e
 
p
a
i
r
s
Iteration
Figure 5: Number of generated phrase pairs over
20 training iterations.
firms observations from previous work. In the
trained phrase table, phrases of length one and two
make up 47% of all phrases. In the heuristically
extracted table it is only 32%. This is even more
pronounced in the intersection of the two tables,
where 68% of the phrases are of length one and
two.
Interestingly, the total overlap between the two
phrase tables is rather small. Only about 18.5%
of the phrases from the trained table also appear in
the heuristically extracted one. This shows that, by
generating phrases on-the-fly without restrictions
based on a word alignment or a bias from intializa-
tion, our training procedure strongly diverges from
the baseline phrase table. We conclude that most
previous work in this area, which adhered to the
above mentioned restrictions, was only able to ex-
plore a fraction of the full potential of real phrase
training.
Following (DeNero et al, 2006), we compute
the entropy of the distributions within the phrase
tables to quantify the ?smoothness? of the distri-
bution. For a given source phrase f? , it is defined
as
H(f?) =
?
e?
p(e?|f?)log(p(e?|f?)). (6)
A flat distribution with a high level of uncer-
tainty yields a high entropy, whereas a peaked dis-
tribution with little uncertainty produces a low en-
tropy. We analyze the phrase tables filtered to-
wards the dev and test sets. The average en-
 0
 5
 10
 15
 20
 25
 30
 35
 40
1 2 3 4 5 6
#
 
p
h
r
a
s
e
 
p
a
i
r
s
 
[
%
]
source phrase length
BaselineLength-incremental trainingIntersection
Figure 6: Histogram of the phrase lengths present
in the phrase tables.
tropy, weighted by frequency, is 3.1 for the ta-
ble learned with length-incremental training, com-
pared to 2.7 for the heuristically extracted one.
However, the interpolated table, which has the best
performance, lies in between with an average en-
tropy of 2.9. When we consider the histogram of
entropies for the phrase tables in Figure 7, we can
see that in the baseline phrase table 3.8% of the
phrases haven an entropy below 0.5, compared to
0.90% for length-incremental training and 0.16%
for the linear interpolation. Therefore, we can
confirm the observation in (DeNero et al, 2006),
that phrases with a near-zero entropy are undesir-
able for decoding. The distribution of the higher
entropies, however, does not seem to matter for
translation quality. This also gives us a handle for
understanding, why phrase table interpolation of-
ten improves results: It largely seems to eliminate
near-zero entropies from either table.
6.4 Training time
The training was not run under controlled condi-
tions, so we can only give a rough estimate of
how the training times between the different meth-
ods compare. Also, some of the steps were par-
allelized while others are not. To account for
the computational resources needed, we report the
trainig times on a single machine by summing the
times for all parallel and sequential processes.
Heuristc phrase extraction from the word align-
ment took us about 1.7 hours. A single itera-
tion of standard phrase training (leave-one-out)
needs about 24 hours. The first iteration of length-
316
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
0-0.5 0.5-1 1-2 2-3 3-4 >4
#
 
p
h
r
a
s
e
 
p
a
i
r
s
 
[
%
]
entropy
BaselineLength-incrementalLinear interpolation
Figure 7: Histogram of entropies present in the
phrase tables.
incremental training as well as all iterations after
the sixth also took roughly 24 hours. The itera-
tions two through six of length-incremental train-
ing are considerably more expensive due to the
larger size of backoff phrases. Iteration six, with
a maximum backoff phrase size of six words on
source and target side, was the slowest with around
740 hours.
7 Conclusion
In this work we presented a training procedure for
phrase or rule tables in statistical machine trans-
lation. It is based on force-aligning the training
data with a modified version of the translation de-
coder. Different from previous work, we com-
pletely avoid the use of a word alignment on the
bilingual training corpus. Instead, we initialize the
procedure with an empty phrase table and gener-
ate all possible phrases on-the-fly through the con-
cept of backoff phrases. Starting with a maximum
phrase length of m = 1, we increment m in each
iteration, until we reach mmax. Then, we con-
tinue training in a more conventional fashion, al-
lowing creation of new phrases only in fallback
runs. As additional extensions to previous work
we introduce fallback decoding runs for higher
coverage of the data and local language models
for better pre-selection of phrases. The effects
of the most important hyper parameters of our
procedure are discussed and we show how they
were selected in our setup. The experiments are
carried out with a phrase-based decoder on the
IWSLT 2011 Arabic-English shared task. The
trained phrase table slightly outperforms our state-
of-the-art baseline and a linear interpolation yields
an improvement of 0.5% BLEU and 0.5% TER.
The BLEU improvement on test is statistically
significant with 90% confidence. The small over-
lap of 18.5% between the trained and the heuris-
tically extracted phrase table shows how initial-
ization or restrictions based on word alignments
would have biased the training procedure. We also
analyzed the distribution of entropies within the
phrase tables, confirming the previous observation
that fewer near-zero entropy phrases are advanta-
geous for decoding. We also showed that, in our
setup, near-zero entropies are largely eliminated
by phrase table interpolation.
In future work we plan to apply this technique
as a more principled way to train a wider range of
models similar to (Duan et al, 2012). But even
for the phrase models, we have only scratched the
surface of its potential. We hope that by finding
a meaningful way to set the hyper parameters of
our training procedure, better and smaller phrase
tables can be created.
Acknowledgments
This work was partially realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. The material is
also partially based upon work supported by
the DARPA BOLT project under Contract No.
HR0011- 12-C-0015. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Alexandra Birch, Chris Callison-Burch, Miles Os-
borne, and Philipp Koehn. 2006. Constraining the
Phrase-Based, Joint Probability Statistical Transla-
tion Model. In Human Language Technology Conf.
(HLT-NAACL): Proc. Workshop on Statistical Ma-
chine Translation, pages 154?157, New York City,
NY, June.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009. A gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of
317
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP:
Volume 2 - Volume 2, ACL ?09, pages 782?790,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311, June.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why Generative Phrase Models Un-
derperform Surface Heuristics. In Proceedings of
the Workshop on Statistical Machine Translation,
pages 31?38, New York City, June.
John DeNero, Alexandre Buchard-Co?te?, and Dan
Klein. 2008. Sampling Alignment Structure under
a Bayesian Translation Model. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 314?323, Honolulu,
October.
Nan Duan, Mu Li, and Ming Zhou. 2012. Forced
derivation tree based model training to statistical
machine translation. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 445?454, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 292?301, Jeju, Republic of Korea, Jul.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal Phrase-Based Translation. In Proceedings of the
2003 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
03), pages 127?133, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proc. of the
Conf. on Empirical Methods for Natural Language
Processing (EMNLP), pages 388?395, Barcelona,
Spain, July.
Percy Liang, Alexandre Buchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An End-to-End Discrimina-
tive Approach to Machine Translation. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 761?768, Sydney, Australia.
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proc. of the Conf. on Em-
pirical Methods for Natural Language Processing
(EMNLP), pages 133?139, Philadelphia, PA, July.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Robert C. Moore and Chris Quirk. 2007. An
Iteratively-Trained Segmentation-Free Phrase
Translation Model for Statistical Machine Transla-
tion. In Proceedings of the Second Workshop on
Statistical Machine Translation, pages 112?119,
Prague, June.
Markos Mylonakis and Khalil Sima?an. 2008.
Phrase Translation Probabilities with ITG Priors and
Smoothing as Learning Objective. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 630?639, Hon-
olulu, October.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ?11, pages 632?641, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449,
December.
Franz J. Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. IBM Research
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, P.O. Box
218, Yorktown Heights, NY 10598, September.
Markus Saers and Dekai Wu. 2011. Principled induc-
tion of phrasal bilexica. In Proceedings of the 15th
International Conference of the European Associa-
tion for Machine Translation, pages 313?320, May.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
318
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475?484, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012a. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
Joern Wuebker, Mei-Yuh Hwang, and Chris Quirk.
2012b. Leave-One-Out Phrase Model Training
for Large-Scale Deployment. In Proceedings of
the NAACL 2012 Seventh Workshop on Statisti-
cal Machine Translation, pages 460?467, Montreal,
Canada, June.
Richard Zens. 2008. Phrase-based Statistical Machine
Translation: Models, Search, Training. Ph.D. the-
sis, Computer Science Department, RWTH Aachen
? University of Technology, Germany, February.
319
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 452?463,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
A Phrase Orientation Model for Hierarchical Machine Translation
Matthias Huck and Joern Wuebker and Felix Rietig and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{huck,wuebker,rietig,ney}@i6.informatik.rwth-aachen.de
Abstract
We introduce a lexicalized reordering
model for hierarchical phrase-based ma-
chine translation. The model scores mono-
tone, swap, and discontinuous phrase ori-
entations in the manner of the one pre-
sented by Tillmann (2004). While this
type of lexicalized reordering model is a
valuable and widely-used component of
standard phrase-based statistical machine
translation systems (Koehn et al, 2007), it
is however commonly not employed in hi-
erarchical decoders.
We describe how phrase orientation prob-
abilities can be extracted from word-
aligned training data for use with hierar-
chical phrase inventories, and show how
orientations can be scored in hierarchi-
cal decoding. The model is empirically
evaluated on the NIST Chinese?English
translation task. We achieve a signifi-
cant improvement of +1.2 %BLEU over
a typical hierarchical baseline setup and
an improvement of +0.7 %BLEU over a
syntax-augmented hierarchical setup. On
a French?German translation task, we
obtain a gain of up to +0.4 %BLEU.
1 Introduction
In hierarchical phrase-based translation (Chiang,
2005), a probabilistic synchronous context-free
grammar (SCFG) is induced from bilingual train-
ing corpora. In addition to continuous lexical
phrases as in standard phrase-based translation,
hierarchical phrases with usually up to two non-
terminals are extracted from the word-aligned par-
allel training data.
Hierarchical decoding is typically carried out
with a parsing-based procedure. The parsing al-
gorithm is extended to handle translation candi-
dates and to incorporate language model scores
via cube pruning (Chiang, 2007). During decod-
ing, a hierarchical translation rule implicitly spec-
ifies the placement of the target part of a sub-
derivation which is substituting one of its non-
terminals in a partial hypothesis. The hierarchical
phrase-based model thus provides an integrated re-
ordering mechanism. The reorderings which are
being conducted by the hierarchical decoder are
a result of the application of SCFG rules, which
generally means that there must have been some
evidence in the training data for each reordering
operation. At first glance one might be tempted to
believe that any additional designated phrase ori-
entation modeling would be futile in hierarchical
translation as a consequence of this. We argue
that such a conclusion is false, and we will pro-
vide empirical evidence in this work that lexical-
ized phrase orientation scoring can be highly ben-
eficial not only in standard phrase-based systems,
but also in hierarchical ones.
The purpose of a phrase orientation model is
to assess the adequacy of phrase reordering dur-
ing search. In standard phrase-based translation
with continuous phrases only and left-to-right hy-
pothesis generation (Koehn et al, 2003; Zens and
Ney, 2008), phrase reordering is implemented by
jumps within the input sentence. The choice of the
best order for the target sequence is made based
on the language model score of this sequence and
a distortion cost that is computed from the source-
side jump distances. Though the space of admis-
sible reorderings is in most cases contrained by a
maximum jump width or coverage-based restric-
tions (Zens et al, 2004) for efficiency reasons,
the basic approach of arbitrarily jumping to un-
covered positions on source side is still very per-
missive. Lexicalized reordering models assist the
decoder in taking a good decision. Phrase-based
decoding allows for a straightforward integration
of lexicalized reordering models which assign
452
different scores depending on how a currently
translated phrase has been reordered with respect
to its context. Popular lexicalized reordering mod-
els for phrase-based translation distinguish three
orientation classes: monotone, swap, and discon-
tinuous (Tillmann, 2004; Koehn et al, 2007; Gal-
ley and Manning, 2008). To obtain such a model,
scores for the three classes are calculated from the
counts of the respective orientation occurrences in
the word-aligned training data for each extracted
phrase. The left-to-right orientation of phrases
during phrase-based search can be easily deter-
mined from the start and end positions of con-
tinuous phrases. Approximations may need to be
adopted for the right-to-left scoring direction.
The utility of phrase orientation models in stan-
dard phrase-based translation is plausible and has
been empirically established in practice. In hierar-
chical phrase-based translation, some other types
of lexicalized reordering models have been inves-
tigated recently (He et al, 2010a; He et al, 2010b;
Hayashi et al, 2010; Huck et al, 2012a), but
in none of them are the orientation scores condi-
tioned on the lexical identity of each phrase in-
dividually. These models are rather word-based
and applied on block boundaries. Experimental
results obtained with these other types of lexical-
ized reordering models have been very encourag-
ing, though.
There are certain reasons why assessing the ad-
equacy of phrase reordering should be useful in
hierarchical search:
? Albeit phrase reorderings are always a result
of the application of SCFG rules, the decoder
is still able to choose from many different
parses of the input sentence.
? The decoder can furthermore choose from
many translation options for each given
parse, which result in different reorderings
and different phrases being embedded in the
reordering non-terminals.
? All other models only weakly connect an em-
bedded phrase with the hierarchical phrase it
is placed into, in particular as the set of non-
terminals of the hierarchical grammar only
contains two generic non-terminal symbols.
We therefore investigate phrase orientation mod-
eling for hierarchical translation in this work.
2 Outline
The remainder of the paper is structured as fol-
lows: We briefly outline important related pub-
lications in the following section. We subse-
quently give a summary of some essential aspects
of the hierarchical phrase-based translation ap-
proach (Section 4). Phrase orientation modeling
and a way in which a phrase orientation model can
be trained for hierarchical phrase inventories are
explained in Section 5. In Section 6 we introduce
an extension of hierarchical search which enables
the decoder to score phrase orientations. Empiri-
cal results are presented in Section 7. We conclude
the paper in Section 8.
3 Related Work
Hierarchical phrase-based translation was pro-
posed by Chiang (2005). He et al (2010a) inte-
grated a maximum entropy based lexicalized re-
ordering model with word- and class-based fea-
tures. Different classifiers for different rule pat-
terns are trained for their model (He et al,
2010b). A comparable discriminatively trained
model which applies a single classifier for all types
of rules was developed by Huck et al (2012a).
Hayashi et al (2010) explored the word-based re-
ordering model by Tromble and Eisner (2009) in
hierarchical translation.
For standard phrase-based translation, Galley
and Manning (2008) introduced a hierarchical
phrase orientation model. Similar to previous ap-
proaches (Tillmann, 2004; Koehn et al, 2007), it
distinguishes the three orientation classes mono-
tone, swap, and discontinuous. However, it differs
in that it is not limited to model local reordering
phenomena, but allows for phrases to be hierarchi-
cally combined into blocks in order to determine
the orientation class. This has the advantage that
probability mass is shifted from the rather uninfor-
mative default category discontinuous to the other
two orientation classes, which model the location
of a phrase more specifically. In this work, we
transfer this concept to a hierarchical phrase-based
machine translation system.
4 Hierarchical Phrase-Based Translation
The non-terminal set of a standard hierarchical
grammar comprises two symbols which are shared
by source and target: the initial symbol S and one
generic non-terminal symbol X . The generic non-
terminal X is used as a placeholder for the gaps
453
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(a) Monotone phrase orientation.
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(b) Swap phrase orientation.
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(c) Discontinuous phrase orientation.
Figure 1: Extraction of the orientation classes monotone, swap, and discontinuous from word-aligned
training samples. The examples show the left-to-right orientation of the shaded phrases. The dashed
rectangles indicate how the predecessor words are merged into blocks with regard to their word align-
ment.
within the right-hand side of hierarchical transla-
tion rules as well as on all left-hand sides of the
translation rules that are extracted from the paral-
lel training corpus.
Extracted rules of a standard hierarchical gram-
mar are of the form X ? ??, ?,? ? where ??, ??
is a bilingual phrase pair that may contain X , i.e.
? ? ({X } ? VF )+ and ? ? ({X } ? VE)+, where
VF and VE are the source and target vocabulary,
respectively. The non-terminals on the source side
and on the target side of hierarchical rules are
linked in a one-to-one correspondence. The ? re-
lation defines this one-to-one correspondence. In
addition to the extracted rules, a non-lexicalized
initial rule
S ? ?X?0, X?0? (1)
is engrafted into the hierarchical grammar, as well
as a special glue rule
S ? ?S?0X?1, S?0X?1? (2)
that the system can use for serial concatenation
of phrases as in monotonic phrase-based transla-
tion. The initial symbol S is the start symbol of
the grammar.
Hierarchical search is conducted with a cus-
tomized version of the CYK+ parsing algo-
rithm (Chappelier and Rajman, 1998) and cube
pruning (Chiang, 2007). A hypergraph which rep-
resents the whole parsing space is built employing
CYK+. Cube pruning operates in bottom-up topo-
logical order on this hypergraph and expands at
most k derivations at each hypernode.
5 Modeling Phrase Orientation for
Hierarchical Machine Translation
The phrase orientation model we are using was
introduced by Galley and Manning (2008). To
model the sequential order of phrases within the
global translation context, the three orientation
classes monotone (M), swap (S) and discontinu-
ous (D) are distinguished, each in both left-to-
right and right-to-left direction. In order to cap-
ture the global rather than the local context, previ-
ous phrases can be merged into blocks if they are
consistent with respect to the word alignment. A
phrase is in monotone orientation if a consistent
monotone predecessor block exists, and in swap
orientation if a consistent swap predecessor block
exists. Otherwise it is in discontinuous orientation.
Given a sequence of source words fJ1 and a se-
quence of target words eI1, a block ?f j2j1 , ei2i1? (with
1 ? j1 ? j2 ? J and 1 ? i1 ? i2 ? I)
is consistent with respect to the word alignment
A ? {1, ..., I} ? {1, ..., J} iff
?(i, j) ? A : i1 ? i ? i2 ? j1 ? j ? j2
? ?(i, j) ? A : i1 ? i ? i2 ? j1 ? j ? j2.
(3)
Consistency is based upon two conditions in this
definition: (1.) At least one source and target po-
sition within the block must be aligned, and (2.)
words from inside the source interval may only
be aligned to words from inside the target inter-
val and vice versa. These are the same condi-
tions as those that are applied for the extraction of
454
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(a) A monotone orientation.
Left-to-right orientation counts:
N(M |f2X?0f4, e2X?0e4) = 1
N(S|f2X?0f4, e2X?0e4) = 0
N(D|f2X?0f4, e2X?0e4) = 0
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(b) Another monotone orientation.
Left-to-right orientation counts:
N(M |f2X?0f4, e2X?0e4) = 2
N(S|f2X?0f4, e2X?0e4) = 0
N(D|f2X?0f4, e2X?0e4) = 0
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(c) A swap orientation.
Left-to-right orientation counts:
N(M |f2X?0f4, e2X?0e4) = 2
N(S|f2X?0f4, e2X?0e4) = 1
N(D|f2X?0f4, e2X?0e4) = 0
Figure 2: Accumulation of orientation counts for hierarchical phrases during extraction. The hierarchical
phrase ?f2X?0f4, e2X?0e4? (dark shaded) can be extracted from all the three training samples. Its
orientation is identical to the orientation of the continuous phrase (lightly shaded) which the sub-phrase
is cut out of, respectively. Note that the actual lexical content of the sub-phrase may differ. For instance,
the sub-phrase ?f3, e3? is being cut out in Fig. 2a, and the sub-phrase ?f6, e6? is being cut out in Fig. 2b.
standard continuous phrases. The only difference
is that length constraints are applied to phrases, but
not to blocks.
Figure 1 illustrates the extraction of monotone,
swap, and discontinuous orientation classes in
left-to-right direction from word-aligned bilingual
training samples. The right-to-left direction works
analogously.
We found that this concept can be neatly
plugged into the hierarchical phrase-based trans-
lation paradigm, without having to resort to ap-
proximations in decoding, which is necessary to
determine the right-to-left orientation in a standard
phrase-based system (Cherry et al, 2012). To train
the orientations, the extraction procedure from the
standard phrase-based version of the reordering
model can be used with a minor extension. The
model is trained on the same word-aligned data
from which the translation rules are extracted. For
each training sentence, we extract all phrases of
unlimited length that are consistent with the word
alignment, and store their corners in a matrix. The
corners are distinguished by their location: top-
left, top-right, bottom-left, and bottom-right. For
each bilingual phrase, we determine its left-to-
right and right-to-left orientation by checking for
adjacent corners.
The lexicalized orientation probability for the
orientation O ? {M,S,D} and the phrase pair
??, ?? is estimated as its smoothed relative fre-
quency:
p(O) = N(O)?
O??{M,S,D}N(O?)
(4)
p(O|?, ?) = ? ? p(O) +N(O|?, ?)
? +
?
O??{M,S,D}N(O?|f? , e?)
.
(5)
Here, N(O) denotes the global count and
N(O|?, ?) the lexicalized count for the orienta-
tion O. ? is a smoothing constant.
To determine the orientation frequency for a hi-
erarchical phrase with non-terminal symbols, the
orientation counts of all those phrases are accu-
mulated from which a sub-phrase is cut out and
replaced by a non-terminal symbol to obtain this
hierarchical phrase. Figure 2 gives an example.
Negative logarithms of the values are used as
costs in the log-linear model combination (Och
and Ney, 2002). Cost 0 for all orientations is as-
signed to the special rules which are not extracted
from the training data (initial and glue rule).
455
f1
f2
f3
45e
f
	1 	2 	3 45e	
target
so
ur
ce
(a) Monotone non-terminal orientation.
f1
234
f5
fe
f
	1 	5 	e 234	
target
so
ur
ce
(b) Swap non-terminal orientation.
f1
f2
345
fe
f
	1 	2 345 	
target
so
ur
ce
	e
(c) Discontinuous non-terminal orienta-
tion.
Figure 3: Scoring with the orientation classes monotone, swap, and discontinuous. Each picture shows
exactly one hierarchical phrase. The block which replaces the non-terminalX during decoding is embed-
ded with the orientation of this non-terminal X within the hierarchical phrase. The examples show the
left-to-right orientation of the non-terminal. The left-to-right orientation can be detected from the word
alignment of the hierarchical phrase, except for cases where the non-terminal is in boundary position on
target side.
6 Phrase Orientation Scoring in
Hierarchical Decoding
Our implementation of phrase orientation scoring
in hierarchical decoding is based on the observa-
tion that hierarchical rule applications, i.e. the us-
age of rules with non-terminals within their right-
hand sides, settle the target sequence order. Mono-
tone, swap, or discontinuous orientations of blocks
are each due to monotone, swap, or discontinuous
placement of non-terminals which are being sub-
stituted by these blocks.
The problem of phrase orientation scoring can
thus be mostly reduced to three steps which need
to be carried out whenever a hierarchical rule is
applied:
1. Determining the orientations of the non-
terminals in the rule.
2. Retrieving the proper orientation cost of the
topmost rule application in the sub-derivation
which corresponds to the embedded block for
the respective non-terminal.
3. Applying the orientation cost to the log-linear
model combination for the current derivation.
The orientation of a non-terminal in a hierarchi-
cal rule is dependent on the word alignments in
its context. Figure 3 depicts three examples.1 We
however need to deal with special cases where a
non-terminal orientation cannot be established at
the moment when the hierarchical rule is consid-
ered. We first describe the non-degenerate case
(Section 6.1). Afterwards we briefly discuss our
strategy in the special situation of boundary non-
terminals where the non-terminal orientation can-
not be determined from information which is in-
herent to the hierarchical rule under consideration
(Section 6.3).
We focus on left-to-right orientation scoring;
right-to-left scoring is symmetric.
6.1 Determining Orientations
In order to determine the orientation class of a
non-terminal, we rely on the word alignments
within the phrases. With each phrase, we store
the alignment matrix that has been seen most fre-
quently during phrase extraction. Non-terminal
symbols on target side are assumed to be aligned
to the respective non-terminal symbols on source
1Note that even maximal consecutive lexical intervals (ei-
ther on source or target side) are not necessarily aligned in
a way which makes them consistent bilingual blocks. In
Fig. 3a, e4 is for instance aligned both below and above
the non-terminal. In Fig. 3c, neither ?f1f2, e1e2? nor
?f1f2, e3e4? would be valid continuous phrases (the same
holds for ?f3f4, e1e2? and ?f3f4, e3e4?). We actually need
the generalization of the phrase orientation model to hierar-
chical phrases as described in Section 5 for this reason. Oth-
erwise we would be able to just score neighboring consistent
sub-blocks with a model that does not account for hierarchi-
cal phrases with non-terminals.
456
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(a) Last previous aligned target position.
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(b) Initial box.
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(c) Expansion of the initial box.
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(d) The final box is a consistent left-to-right mono-
tone predecessor block of the non-terminal.
Figure 4: Determining the orientation class during decoding. Starting from the last previous aligned
target position, a box is spanned across the relevant alignment links onto the corner of the non-terminal.
The box is then checked for consistency.
side according to the ? relation. In the alignment
matrix, the rows and columns of non-terminals can
obviously contain only exactly this one alignment
link.
Starting from the last previous aligned target po-
sition to the left of the non-terminal, the algorithm
expands a box that spans across the other rele-
vant alignment links onto the corner of the non-
terminal. Afterwards it checks whether the areas
on the opposite sides of the non-terminal position
are non-aligned in the source and target intervals
of this box. The non-terminal is in discontinu-
ous orientation if the box is not a consistent block.
If the box is a consistent block, the non-terminal
is in monotone orientation if its source-side posi-
tion is larger than the maximum of the source-side
interval of the box, and in swap orientation if its
source-side position is smaller than the minimum
of the source-side interval of the box.
Figure 4 illustrates how the procedure operates.
In left-to-right direction, an initial box is spanned
from the last previous aligned target position to
the lower (monotone) or upper (swap) left cor-
ner of the non-terminal. In the example, starting
from ?f3, e5? (Fig. 4a), this initial box is spanned
to the lower left corner by iterating from f3 to
f4 and expanding its target interval to the mini-
mum aligned target position within these two rows
of the alignment matrix. The initial box cov-
ers ?f3f4, e3e4e5? (Fig. 4b). The procedure then
repeatedly checks whether the box needs to be
expanded?alternating to the bottom (monotone)
or top (swap) and to the left?until no alignment
links below or to the left of the box break the
consistency. Two box expansion are conducted
in the example: the first one expands the ini-
tial box below, resulting in a larger box which
covers ?f1f2f3f4, e3e4e5? (Fig. 4c); the second
457
f1
f2
345
e1345
target
so
ur
ce
(a) Left boundary non-
terminal that can be placed
in left-to-right monotone or
discontinuous orientation
when the phrase is embedded
into another one.
f1
f2
345
e1345
target
so
ur
ce
(b) Left boundary non-
terminal that can be placed
in left-to-right discontinuous
or swap orientation when
the phrase is embedded into
another one.
f1
f2345
e1 345
target
so
ur
ce
(c) Left boundary non-
terminal that can be placed in
left-to-right monotone, swap,
or discontinuous orientation
when the phrase is embedded
into another one.
f1
f2345
e1345
target
so
ur
ce
(d) Left boundary non-
terminal that can only be
placed in left-to-right dis-
continuous orientation when
the phrase is embedded into
another one.
Figure 5: Left boundary non-terminal symbols. Orientations the non-terminal can eventually turn out to
get placed in differ depending on existing alignment links in the rest of the phrase. Delayed left-to-right
scoring is not required in cases as in Fig. 5d. Fractional costs for the possible orientations are temporarily
applied in the other cases and recursively corrected as soon as an orientation is constituted in an upper
hypernode.
one expands this new box to the left, resulting in
a final box which covers ?f1f2f3f4, e1e2e3e4e5?
(Fig. 4d) and does not need to be expanded to-
wards the lower left corner any more. Afterwards
the procedure examines whether the final box is
a consistent block by inspecting whether the ar-
eas on the opposite side of the non-terminal po-
sition are non-aligned in the intervals of the box
(areas with waved lines in the Fig. 4d). These ar-
eas do not contain alignment links in the example:
the orientation class of the non-terminal is mono-
tone as it has a consistent left-to-right monotone
predecessor block. (Suppose an alignment link
?f5, e2? would break the consistency: the orienta-
tion class would then be discontinuous as the final
box would not be a consistent block.)
Orientations of non-terminals could basically be
precomputed and stored in the translation table.
We however compute them on demand during de-
coding. The computational overhead did not seem
to be too severe in our experiments.
6.2 Scoring Orientations
Once the orientation is determined, the proper ori-
entation cost of the embedded block needs to be
retrieved. We access the topmost rule application
in the sub-derivation which corresponds to the em-
bedded block for the respective non-terminal and
read the orientation model costs for this rule. The
special case of delayed scoring for boundary non-
terminals as described in the subsequent section is
recursively processed if necessary. The retrieved
orientation costs of the embedded blocks of all
non-terminals are finally added to the log-linear
model combination for the current derivation.
6.3 Boundary Non-Terminals
Cases where a non-terminal orientation cannot be
established at the moment when the hierarchi-
cal rule is considered arise when a non-terminal
symbol is in a boundary position on target side.
We define a non-terminal to be in (left or right)
boundary position iff no symbols are aligned be-
tween the phrase-internal target-side index of the
non-terminal and the (left or right) phrase bound-
ary. Left boundary positions of non-terminals
are critical for left-to-right orientation scoring,
right boundary positions for right-to-left orienta-
tion scoring. We denote non-terminals in bound-
ary position as boundary non-terminals.
The procedure as described in Section 6.1 is not
applicable to boundary non-terminals because a
last previous aligned target position does not ex-
ist. If it is impossible to determine the final non-
terminal orientation in the hypothesis from infor-
mation which is inherent to the phrase, we are
forced to delay the orientation scoring of the em-
bedded block. Our solution in these cases is to
heuristically add fractional costs of all orientations
the non-terminal can still eventually turn out to get
placed in (cf. Figure 5). We do so because not
adding an orientation cost to the derivation would
give it an unjustified advantage over other ones.
As soon as an orientation is constituted in an up-
458
per hypernode, any heuristic and actual orientation
costs can be collected by means of a recursive call.
Note that monotone or swap orientations in upper
hypernodes can top-down transition into discon-
tinuous orientations for boundary non-terminals,
depending on existing phrase-internal alignment
links in the context of the respective boundary
non-terminal. In the derivation at the upper hyper-
node, the heuristic costs are subtracted and the cor-
rect actual costs added. Delayed scoring can lead
to search errors; in order to keep them confined,
the delayed scoring needs to be done separately
for all derivations, not just for the first-best sub-
derivations along the incoming hyperedges.
7 Experiments
We evaluate the effect of phrase orienta-
tion scoring in hierarchical translation on the
Chinese?English 2008 NIST task2 and on the
French?German language pair using the standard
WMT3 newstest sets for development and testing.
7.1 Experimental Setup
We work with a Chinese?English parallel train-
ing corpus of 3.0 M sentence pairs (77.5 M Chi-
nese / 81.0 M English running words). To train the
German?French baseline system, we use 2.0 M
sentence pairs (53.1 M French / 45.8 M German
running words) that are partly taken from the
Europarl corpus (Koehn, 2005) and have partly
been collected within the Quaero project.4
Word alignments are created by aligning the
data in both directions with GIZA++5 and sym-
metrizing the two trained alignments (Och and
Ney, 2003). When extracting phrases, we ap-
ply several restrictions, in particular a maximum
length of ten on source and target side for lexi-
cal phrases, a length limit of five on source and
ten on target side for hierarchical phrases (includ-
ing non-terminal symbols), and no more than two
non-terminals per phrase.
A standard set of models is used in the base-
lines, comprising phrase translation probabilities
and lexical translation probabilities in both direc-
tions, word and phrase penalty, binary features
marking hierarchical rules, glue rule, and rules
2http://www.itl.nist.gov/iad/mig/
tests/mt/2008/
3http://www.statmt.org/wmt13/
translation-task.html
4http://www.quaero.org
5http://code.google.com/p/giza-pp/
with non-terminals at the boundaries, three sim-
ple count-based binary features, phrase length ra-
tios, and a language model. The language models
are 4-grams with modified Kneser-Ney smooth-
ing (Kneser and Ney, 1995; Chen and Goodman,
1998) which have been trained with the SRILM
toolkit (Stolcke, 2002).
Model weights are optimized against BLEU (Pa-
pineni et al, 2002) with MERT (Och, 2003) on
100-best lists. For Chinese?English we employ
MT06 as development set, MT08 is used as unseen
test set. For German?French we employ news-
test2009 as development set, newstest2008, news-
test2010, and newstest2011 are used as unseen test
sets. During decoding, a maximum length con-
straint of ten is applied to all non-terminals except
the initial symbol S . Translation quality is mea-
sured in truecase with BLEU and TER (Snover et
al., 2006). The results on MT08 are checked for
statistical significance over the baseline. Confi-
dence intervals have been computed using boot-
strapping for BLEU and Cochran?s approximate
ratio variance for TER (Leusch and Ney, 2009).
7.2 Chinese?English Experimental Results
Table 1 comprises all results of our empirical eval-
uation on the Chinese?English task.
We first compare the performance of the phrase
orientation model in left-to-right direction only
with the performance of the phrase orientation
model in left-to-right and right-to-left direction
(bidirectional). In all experiments, monotone,
swap, and discontinuous orientation costs are
treated as being from different feature functions
in the log-linear model combination: we assign
a separate scaling factor to each of the orienta-
tions. We have three more scaling factors than in
the baseline for left-to-right direction only, and six
more scaling factors for bidirectional phrase ori-
entation scoring. As can be seen from the results
table, the left-to-right model already yields a gain
of 1.1 %BLEU over the baseline on the unseen test
set (MT08). The bidirectional model performs just
slightly better (+1.2 %BLEU over the baseline).
With both models, the TER is reduced significantly
as well (-1.1 / -1.3 compared to the baseline). We
adopted the discriminative lexicalized reordering
model (discrim. RO) that has been suggested by
Huck et al (2012a) for comparison purposes. The
phrase orientation model provides clearly better
translation quality in our experiments.
459
MT06 (Dev) MT08 (Test)
NIST Chinese?English BLEU [%] TER [%] BLEU [%] TER [%]
HPBT Baseline 32.6 61.2 25.2 66.6
+ discrim. RO 33.0 61.3 25.8 66.0
+ phrase orientation (left-to-right) 33.3 60.7 26.3 65.5
+ phrase orientation (bidirectional) 33.2 60.6 26.4 65.3
+ swap rule 32.8 61.7 25.8 66.6
+ discrim. RO 33.1 61.2 26.0 66.1
+ phrase orientation (bidirectional) 33.3 60.7 26.5 65.3
+ binary swap feature 33.2 61.0 25.9 66.2
+ discrim. RO 33.2 61.3 26.2 66.1
+ phrase orientation (bidirectional) 33.6 60.5 26.6 65.1
+ soft syntactic labels 33.4 60.8 26.1 66.4
+ phrase orientation (bidirectional) 33.7 60.1 26.8 65.1
+ phrase-level s2t+t2s DWL + triplets 34.3 60.1 27.7 65.0
+ discrim. RO 34.8 59.8 27.7 64.7
+ phrase orientation (bidirectional) 35.3 59.0 28.4 63.7
Table 1: Experimental results for the NIST Chinese?English translation task (truecase). On the test set,
bold font indicates results that are significantly better than the baseline (p < .05).
As a next experiment, we bring in more re-
ordering capabilities by augmenting the hierarchi-
cal grammar with a single swap rule
X ? ?X?0X?1,X?1X?0? (6)
supplementary to the initial rule and glue rule.
The swap rule allows adjacent phrases to be trans-
posed. The setup with swap rule and bidirectional
phrase orientation model is about as good as the
setup with just the bidirectional phrase orienta-
tion model and no swap rule. If we furthermore
mark the swap rule with a binary feature (binary
swap feature), we end up at an improvement of
+1.4 %BLEU over the baseline. The phrase ori-
entation model again provides higher translation
quality than the discriminative reordering model.
In a third experiment, we investigate whether
the phrase orientation model also has a positive in-
fluence when integrated into a syntax-augmented
hierarchical system. We configured a hierarchi-
cal setup with soft syntactic labels (Stein et al,
2010), a syntactic enhancement in the manner of
preference grammars (Venugopal et al, 2009). On
MT08, the syntax-augmented system performs 0.9
%BLEU above the baseline setup. We achieve an
additional improvement of +0.7 %BLEU and -1.3
TER by including the bidirectional phrase orien-
tation model. Interestingly, the translation quality
of the setup with soft syntactic labels (but with-
out phrase orientation model) is worse than of the
setup with phrase orientation model (but without
soft syntactic labels) on MT08. The combination
of both extensions provides the best result, though.
In a last experiment, we finally took a very
strong setup which improves over the baseline by
2.5 %BLEU through the integration of phrase-level
discriminative word lexicon (DWL) models and
triplet lexicon models in source-to-target (s2t) and
target-to-source (t2s) direction. The models have
been presented by Hasan et al (2008), Bangalore
et al (2007), and Mauser et al (2009). We apply
them in a similar manner as proposed by Huck et
al. (2011). In this strong setup, the discriminative
reordering model gives gains on the development
set which barely carry over to the test set. Adding
the bidirectional phrase orientation model, in con-
trast, results in a nice gain of +0.7 %BLEU and a
reduction of 1.3 points in TER on the test set, even
on top of the DWL and triplet lexicon models.
7.3 French?German Experimental Results
Table 2 comprises the results of our empirical eval-
uation on the French?German task.
The left-to-right phrase orientation model
boosts the translation quality by up to 0.3 %BLEU.
The reduction in TER is in a similar order of
magnitude. The bidirectional model performs a
bit better again, with an advancement of up to
0.4 %BLEU and a maximal reduction in TER of
0.6 points.
460
newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER
French?German [%] [%] [%] [%] [%] [%] [%] [%]
HPBT Baseline 15.2 71.7 15.0 71.7 15.7 69.5 14.2 72.2
+ phrase orientation (left-to-right) 15.1 71.4 15.3 71.4 15.9 69.2 14.5 71.8
+ phrase orientation (bidirectional) 15.4 71.1 15.4 71.3 15.9 69.1 14.6 71.6
Table 2: Experimental results for the French?German translation task (truecase). newstest2009 is used
as development set.
8 Conclusion
In this paper, we introduced a phrase orientation
model for hierarchical machine translation. The
training of a lexicalized reordering model which
assigns probabilities for monotone, swap, and dis-
continuous orientation of phrases was generalized
from standard continuous phrases to hierarchical
phrases. We explained how phrase orientation
scoring can be implemented in hierarchical decod-
ing and conducted a number of experiments on a
Chinese?English and a French?German transla-
tion task. The results indicate that phrase orienta-
tion modeling is a very suitable enhancement of
the hierarchical paradigm.
Our implementation will be released as part of
Jane (Vilar et al, 2010; Vilar et al, 2012; Huck
et al, 2012b), the RWTH Aachen University open
source statistical machine translation toolkit.6
Acknowledgments
This work was partly achieved as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This material is also
partly based upon work supported by the DARPA
BOLT project under Contract No. HR0011-12-
C-0015. Any opinions, findings and conclu-
sions or recommendations expressed in this ma-
terial are those of the authors and do not neces-
sarily reflect the views of the DARPA. The re-
search leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no 287658.
References
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical Machine Translation through
6http://www.hltpr.rwth-aachen.de/jane/
Global Lexical Selection and Sentence Reconstruc-
tion. In Proc. of the Annual Meeting of the Assoc. for
Computational Linguistics (ACL), pages 152?159,
Prague, Czech Republic, June.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochas-
tic CFG. In Proc. of the First Workshop on Tab-
ulation in Parsing and Deduction, pages 133?137,
Paris, France, April.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, USA, August.
Colin Cherry, Robert C. Moore, and Chris Quirk.
2012. On Hierarchical Re-ordering and Permuta-
tion Parsing for Phrase-based Decoding. In Proc. of
the Workshop on Statistical Machine Translation
(WMT), pages 200?209, Montre?al, Canada, June.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
of the Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 263?270, Ann Ar-
bor, MI, USA, June.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201?
228, June.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of the Conf. on Empirical Meth-
ods for Natural Language Processing (EMNLP),
pages 847?855, Honolulu, HI, USA, October.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, and
Jesu?s Andre?s-Ferrer. 2008. Triplet Lexicon Mod-
els for Statistical Machine Translation. In Proc. of
the Conf. on Empirical Methods for Natural Lan-
guage Processing (EMNLP), pages 372?381, Hon-
olulu, HI, USA, October.
Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Sudoh,
Kevin Duh, and Seiichi Yamamoto. 2010. Hi-
erarchical Phrase-based Machine Translation with
Word-based Reordering Model. In Proc. of the
Int. Conf. on Computational Linguistics (COLING),
pages 439?446, Beijing, China, August.
461
Zhongjun He, Yao Meng, and Hao Yu. 2010a. Extend-
ing the Hierarchical Phrase Based Model with Max-
imum Entropy Based BTG. In Proc. of the Conf. of
the Assoc. for Machine Translation in the Americas
(AMTA), Denver, CO, USA, October/November.
Zhongjun He, Yao Meng, and Hao Yu. 2010b. Max-
imum Entropy Based Phrase Reordering for Hier-
archical Phrase-based Translation. In Proc. of the
Conf. on Empirical Methods for Natural Language
Processing (EMNLP), pages 555?563, Cambridge,
MA, USA, October.
Matthias Huck, Saab Mansour, Simon Wiesler, and
Hermann Ney. 2011. Lexicon Models for Hierar-
chical Phrase-Based Machine Translation. In Proc.
of the Int. Workshop on Spoken Language Transla-
tion (IWSLT), pages 191?198, San Francisco, CA,
USA, December.
Matthias Huck, Stephan Peitz, Markus Freitag, and
Hermann Ney. 2012a. Discriminative Reordering
Extensions for Hierarchical Phrase-Based Machine
Translation. In Proc. of the Annual Conf. of the
European Assoc. for Machine Translation (EAMT),
pages 313?320, Trento, Italy, May.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012b. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37?
50, October.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-Off for M-gram Language Modeling. In
Proc. of the Int. Conf. on Acoustics, Speech, and Sig-
nal Processing (ICASSP), volume 1, pages 181?184,
Detroit, MI, USA, May.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the Human Language Technology Conf. / North
American Chapter of the Assoc. for Computational
Linguistics (HLT-NAACL), pages 127?133, Edmon-
ton, Canada, May/June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of the Annual Meeting of the Assoc. for
Computational Linguistics (ACL), Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proc. of the MT
Summit X, Phuket, Thailand, September.
Gregor Leusch and Hermann Ney. 2009. Edit dis-
tances with block movements and error rate confi-
dence estimates. Machine Translation, 23(2):129?
140, December.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Proc. of the Conf. on Empirical Methods for Natu-
ral Language Processing (EMNLP), pages 210?218,
Singapore, August.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive Training and Maximum Entropy Models for Sta-
tistical Machine Translation. In Proc. of the Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 295?302, Philadelphia, PA, USA, July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Proc. of
the Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 160?167, Sapporo, Japan,
July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
USA, July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proc. of the Conf. of the Assoc. for
Machine Translation in the Americas (AMTA), pages
223?231, Cambridge, MA, USA, August.
Daniel Stein, Stephan Peitz, David Vilar, and Hermann
Ney. 2010. A Cocktail of Deep Syntactic Fea-
tures for Hierarchical Machine Translation. In Proc.
of the Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), Denver, CO, USA, Octo-
ber/November.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 2,
pages 901?904, Denver, CO, USA, September.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04, pages 101?104, Boston, MA,
USA.
Roy Tromble and Jason Eisner. 2009. Learning Linear
Ordering Problems for Better Translation. In Proc.
of the Conf. on Empirical Methods for Natural Lan-
guage Processing (EMNLP), pages 1007?1016, Sin-
gapore, August.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference Grammars:
462
Softening Syntactic Constraints to Improve Statis-
tical Machine Translation. In Proc. of the Hu-
man Language Technology Conf. / North American
Chapter of the Assoc. for Computational Linguistics
(HLT-NAACL), pages 236?244, Boulder, CO, USA,
June.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchical
Translation, Extended with Reordering and Lexicon
Models. In Proc. of the Workshop on Statistical Ma-
chine Translation (WMT), pages 262?270, Uppsala,
Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216, September.
Richard Zens and Hermann Ney. 2008. Improvements
in Dynamic Programming Beam Search for Phrase-
Based Statistical Machine Translation. In Proc. of
the Int. Workshop on Spoken Language Translation
(IWSLT), pages 195?205, Waikiki, HI, USA, Octo-
ber.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering Constraints for
Phrase-Based Statistical Machine Translation. In
Proc. of the Int. Conf. on Computational Linguis-
tics (COLING), pages 205?211, Geneva, Switzer-
land, August.
463
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105?113,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
EU-BRIDGE MT: Combined Machine Translation
?
Markus Freitag,
?
Stephan Peitz,
?
Joern Wuebker,
?
Hermann Ney,
?
Matthias Huck,
?
Rico Sennrich,
?
Nadir Durrani,
?
Maria Nadejde,
?
Philip Williams,
?
Philipp Koehn,
?
Teresa Herrmann,
?
Eunah Cho,
?
Alex Waibel
?
RWTH Aachen University, Aachen, Germany
?
University of Edinburgh, Edinburgh, Scotland
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de
?
{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk
?
v1rsennr@staffmail.ed.ac.uk
?
maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk
?
{teresa.herrmann,eunah.cho,alex.waibel}@kit.edu
Abstract
This paper describes one of the col-
laborative efforts within EU-BRIDGE to
further advance the state of the art in
machine translation between two Euro-
pean language pairs, German?English
and English?German. Three research
institutes involved in the EU-BRIDGE
project combined their individual machine
translation systems and participated with a
joint setup in the shared translation task of
the evaluation campaign at the ACL 2014
Eighth Workshop on Statistical Machine
Translation (WMT 2014).
We combined up to nine different machine
translation engines via system combina-
tion. RWTH Aachen University, the Uni-
versity of Edinburgh, and Karlsruhe In-
stitute of Technology developed several
individual systems which serve as sys-
tem combination input. We devoted spe-
cial attention to building syntax-based sys-
tems and combining them with the phrase-
based ones. The joint setups yield em-
pirical gains of up to 1.6 points in BLEU
and 1.0 points in TER on the WMT news-
test2013 test set compared to the best sin-
gle systems.
1 Introduction
EU-BRIDGE
1
is a European research project
which is aimed at developing innovative speech
translation technology. This paper describes a
1
http://www.eu-bridge.eu
joint WMT submission of three EU-BRIDGE
project partners. RWTH Aachen University
(RWTH), the University of Edinburgh (UEDIN)
and Karlsruhe Institute of Technology (KIT) all
provided several individual systems which were
combined by means of the RWTH Aachen system
combination approach (Freitag et al., 2014). As
distinguished from our EU-BRIDGE joint submis-
sion to the IWSLT 2013 evaluation campaign (Fre-
itag et al., 2013), we particularly focused on trans-
lation of news text (instead of talks) for WMT. Be-
sides, we put an emphasis on engineering syntax-
based systems in order to combine them with our
more established phrase-based engines. We built
combined system setups for translation from Ger-
man to English as well as from English to Ger-
man. This paper gives some insight into the tech-
nology behind the system combination framework
and the combined engines which have been used
to produce the joint EU-BRIDGE submission to
the WMT 2014 translation task.
The remainder of the paper is structured as fol-
lows: We first describe the individual systems by
RWTH Aachen University (Section 2), the Uni-
versity of Edinburgh (Section 3), and Karlsruhe
Institute of Technology (Section 4). We then
present the techniques for machine translation sys-
tem combination in Section 5. Experimental re-
sults are given in Section 6. We finally conclude
the paper with Section 7.
2 RWTH Aachen University
RWTH (Peitz et al., 2014) employs both the
phrase-based (RWTH scss) and the hierarchical
(RWTH hiero) decoder implemented in RWTH?s
publicly available translation toolkit Jane (Vilar
105
et al., 2010; Wuebker et al., 2012). The model
weights of all systems have been tuned with stan-
dard Minimum Error Rate Training (Och, 2003)
on a concatenation of the newstest2011 and news-
test2012 sets. RWTH used BLEU as optimiza-
tion objective. Both for language model estima-
tion and querying at decoding, the KenLM toolkit
(Heafield et al., 2013) is used. All RWTH sys-
tems include the standard set of models provided
by Jane. Both systems have been augmented with
a hierarchical orientation model (Galley and Man-
ning, 2008; Huck et al., 2013) and a cluster lan-
guage model (Wuebker et al., 2013). The phrase-
based system (RWTH scss) has been further im-
proved by maximum expected BLEU training sim-
ilar to (He and Deng, 2012). The latter has been
performed on a selection from the News Commen-
tary, Europarl and Common Crawl corpora based
on language and translation model cross-entropies
(Mansour et al., 2011).
3 University of Edinburgh
UEDIN contributed phrase-based and syntax-
based systems to both the German?English and
the English?German joint submission.
3.1 Phrase-based Systems
UEDIN?s phrase-based systems (Durrani et al.,
2014) have been trained using the Moses toolkit
(Koehn et al., 2007), replicating the settings de-
scribed in (Durrani et al., 2013b). The features
include: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA
++
align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, a lexically-driven 5-gram
operation sequence model (OSM) (Durrani et al.,
2013a), msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al., 2012), a distortion limit of 6, a maxi-
mum phrase length of 5, 100-best translation op-
tions, Minimum Bayes Risk decoding (Kumar and
Byrne, 2004), cube pruning (Huang and Chiang,
2007), with a stack size of 1000 during tuning and
5000 during testing and the no-reordering-over-
punctuation heuristic. UEDIN uses POS and mor-
phological target sequence models built on the in-
domain subset of the parallel corpus using Kneser-
Ney smoothed 7-gram models as additional factors
in phrase translation models (Koehn and Hoang,
2007). UEDIN has furthermore built OSM mod-
els over POS and morph sequences following
Durrani et al. (2013c). The English?German
system additionally comprises a target-side LM
over automatically built word classes (Birch et
al., 2013). UEDIN has applied syntactic pre-
reordering (Collins et al., 2005) and compound
splitting (Koehn and Knight, 2003) of the source
side for the German?English system. The sys-
tems have been tuned on a very large tuning set
consisting of the test sets from 2008-2012, with
a total of 13,071 sentences. UEDIN used news-
test2013 as held-out test set. On top of UEDIN
phrase-based 1 system, UEDIN phrase-based 2
augments word classes as additional factor and
learns an interpolated target sequence model over
cluster IDs. Furthermore, it learns OSM models
over POS, morph and word classes.
3.2 Syntax-based Systems
UEDIN?s syntax-based systems (Williams et al.,
2014) follow the GHKM syntax approach as pro-
posed by Galley, Hopkins, Knight, and Marcu
(Galley et al., 2004). The open source Moses
implementation has been employed to extract
GHKM rules (Williams and Koehn, 2012). Com-
posed rules (Galley et al., 2006) are extracted in
addition to minimal rules, but only up to the fol-
lowing limits: at most twenty tree nodes per rule,
a maximum depth of five, and a maximum size of
five. Singleton hierarchical rules are dropped.
The features for the syntax-based systems com-
prise Good-Turing-smoothed phrase translation
probabilities, lexical translation probabilities in
both directions, word and phrase penalty, a rule
rareness penalty, a monolingual PCFG probability,
and a 5-gram language model. UEDIN has used
the SRILM toolkit (Stolcke, 2002) to train the lan-
guage model and relies on KenLM for language
model scoring during decoding. Model weights
are optimized to maximize BLEU. 2000 sentences
from the newstest2008-2012 sets have been se-
lected as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and each contain less than 30 words
for more rapid tuning. Decoding for the syntax-
based systems is carried out with cube pruning
using Moses? hierarchical decoder (Hoang et al.,
2009).
UEDIN?s German?English syntax-based setup
is a string-to-tree system with compound splitting
106
on the German source-language side and syntactic
annotation from the Berkeley Parser (Petrov et al.,
2006) on the English target-language side.
For English?German, UEDIN has trained var-
ious string-to-tree GHKM syntax systems which
differ with respect to the syntactic annotation. A
tree-to-string system and a string-to-string system
(with rules that are not syntactically decorated)
have been trained as well. The English?German
UEDIN GHKM system names in Table 3 denote:
UEDIN GHKM S2T (ParZu): A string-to-tree
system trained with target-side syntactic an-
notation obtained with ParZu (Sennrich et
al., 2013). It uses a modified syntactic label
set, target-side compound splitting, and addi-
tional syntactic constraints.
UEDIN GHKM S2T (BitPar): A string-to-tree
system trained with target-side syntactic
annotation obtained with BitPar (Schmid,
2004).
UEDIN GHKM S2T (Stanford): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Stan-
ford Parser (Rafferty and Manning, 2008a).
UEDIN GHKM S2T (Berkeley): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Berke-
ley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
UEDIN GHKM T2S (Berkeley): A tree-to-
string system trained with source-side syn-
tactic annotation obtained with the English
Berkeley Parser (Petrov et al., 2006).
UEDIN GHKM S2S (Berkeley): A string-to-
string system. The extraction is GHKM-
based with syntactic target-side annotation
from the German Berkeley Parser, but we
strip off the syntactic labels. The final gram-
mar contains rules with a single generic non-
terminal instead of syntactic ones, plus rules
that have been added from plain phrase-based
extraction (Huck et al., 2014).
4 Karlsruhe Institute of Technology
The KIT translations (Herrmann et al., 2014) are
generated by an in-house phrase-based transla-
tions system (Vogel, 2003). The provided News
Commentary, Europarl, and Common Crawl par-
allel corpora are used for training the translation
model. The monolingual part of those parallel
corpora, the News Shuffle corpus for both direc-
tions and additionally the Gigaword corpus for
German?English are used as monolingual train-
ing data for the different language models. Opti-
mization is done with Minimum Error Rate Train-
ing as described in (Venugopal et al., 2005), using
newstest2012 and newstest2013 as development
and test data respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side of the corpus for
German?English translation before training. In
order to improve the quality of the web-crawled
Common Crawl corpus, noisy sentence pairs are
filtered out using an SVM classifier as described
by Mediani et al. (2011).
The word alignment for German?English is
generated using the GIZA
++
toolkit (Och and Ney,
2003). For English?German, KIT uses discrimi-
native word alignment (Niehues and Vogel, 2008).
Phrase extraction and scoring is done using the
Moses toolkit (Koehn et al., 2007). Phrase pair
probabilities are computed using modified Kneser-
Ney smoothing as in (Foster et al., 2006).
In both systems KIT applies short-range re-
orderings (Rottmann and Vogel, 2007) and long-
range reorderings (Niehues and Kolss, 2009)
based on POS tags (Schmid, 1994) to perform
source sentence reordering according to the target
language word order. The long-range reordering
rules are applied to the training corpus to create
reordering lattices to extract the phrases for the
translation model. In addition, a tree-based re-
ordering model (Herrmann et al., 2013) trained
on syntactic parse trees (Rafferty and Manning,
2008b; Klein and Manning, 2003) as well as a lex-
icalized reordering model (Koehn et al., 2005) are
applied.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) and use modified Kneser-
Ney smoothing. Both systems utilize a lan-
guage model based on automatically learned
word classes using the MKCLS algorithm (Och,
1999). The English?German system comprises
language models based on fine-grained part-of-
speech tags (Schmid and Laws, 2008). In addi-
tion, a bilingual language model (Niehues et al.,
2011) is used as well as a discriminative word lex-
icon (Mauser et al., 2009) using source context to
guide the word choices in the target sentence.
107
In total, the English?German system uses the
following language models: two 4-gram word-
based language models trained on the parallel data
and the filtered Common Crawl data separately,
two 5-gram POS-based language models trained
on the same data as the word-based language mod-
els, and a 4-gram cluster-based language model
trained on 1,000 MKCLS word classes.
The German?English system uses a 4-gram
word-based language model trained on all mono-
lingual data and an additional language model
trained on automatically selected data (Moore and
Lewis, 2010). Again, a 4-gram cluster-based
language model trained on 1000 MKCLS word
classes is applied.
5 System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses which
are outputs of different translation engines. The
consensus translations can be better in terms of
translation quality than any of the individual hy-
potheses. To combine the engines of the project
partners for the EU-BRIDGE joint setups, we ap-
ply a system combination implementation that has
been developed at RWTH Aachen University.
The implementation of RWTH?s approach to
machine translation system combination is de-
scribed in (Freitag et al., 2014). This approach
includes an enhanced alignment and reordering
framework. Alignments between the system out-
puts are learned using METEOR (Banerjee and
Lavie, 2005). A confusion network is then built
using one of the hypotheses as ?primary? hypoth-
esis. We do not make a hard decision on which
of the hypotheses to use for that, but instead com-
bine all possible confusion networks into a single
lattice. Majority voting on the generated lattice
is performed using the prior probabilities for each
system as well as other statistical models, e.g. a
special n-gram language model which is learned
on the input hypotheses. Scaling factors of the
models are optimized using the Minimum Error
Rate Training algorithm. The translation with the
best total score within the lattice is selected as con-
sensus translation.
6 Results
In this section, we present our experimental results
on the two translation tasks, German?English
and English?German. The weights of the in-
dividual system engines have been optimized on
different test sets which partially or fully include
newstest2011 or newstest2012. System combina-
tion weights are either optimized on newstest2011
or newstest2012. We kept newstest2013 as an un-
seen test set which has not been used for tuning
the system combination or any of the individual
systems.
6.1 German?English
The automatic scores of all individual systems
as well as of our final system combination sub-
mission are given in Table 1. KIT, UEDIN and
RWTH are each providing one individual phrase-
based system output. RWTH (hiero) and UEDIN
(GHKM) are providing additional systems based
on the hierarchical translation model and a string-
to-tree syntax model. The pairwise difference
of the single system performances is up to 1.3
points in BLEU and 2.5 points in TER. For
German?English, our system combination pa-
rameters are optimized on newstest2012. System
combination gives us a gain of 1.6 points in BLEU
and 1.0 points in TER for newstest2013 compared
to the best single system.
In Table 2 the pairwise BLEU scores for all in-
dividual systems as well as for the system combi-
nation output are given. The pairwise BLEU score
of both RWTH systems (taking one as hypothesis
and the other one as reference) is the highest for all
pairs of individual system outputs. A high BLEU
score means similar hypotheses. The syntax-based
system of UEDIN and RWTH scss differ mostly,
which can be observed from the fact of the low-
est pairwise BLEU score. Furthermore, we can
see that better performing individual systems have
higher BLEU scores when evaluating against the
system combination output.
In Figure 1 system combination output is com-
pared to the best single system KIT. We distribute
the sentence-level BLEU scores of all sentences of
newstest2013. To allow for sentence-wise evalu-
ation, all bi-, tri-, and four-gram counts are ini-
tialized with 1 instead of 0. Many sentences have
been improved by system combination. Neverthe-
less, some sentences fall off in quality compared
to the individual system output of KIT.
6.2 English?German
The results of all English?German system setups
are given in Table 3. For the English?German
translation task, only UEDIN and KIT are con-
108
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
KIT 25.0 57.6 25.2 57.4 27.5 54.4
UEDIN 23.9 59.2 24.7 58.3 27.4 55.0
RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0
RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9
UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9
syscom 25.6 57.1 26.4 56.5 29.1 53.4
Table 1: Results for the German?English translation task. The system combination is tuned on news-
test2012, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly better than the best single system
with p < 0.05.
KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscom
KIT 59.07 57.60 57.91 55.62 77.68
UEDIN 59.17 56.96 57.84 59.89 72.89
RWTH scss 57.64 56.90 64.94 53.10 71.16
RWTH hiero 57.98 57.80 64.97 55.73 70.87
UEDIN S2T 55.75 59.95 53.19 55.82 65.35
syscom 77.76 72.83 71.17 70.85 65.24
Table 2: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as hypothesis and the other one as reference.)
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
UEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7
UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3
UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8
UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9
UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2
UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8
UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8
UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3
KIT 17.1 67.0 17.8 64.8 20.2 62.2
syscom 18.4 65.0 18.7 63.4 21.3 60.6
Table 3: Results for the English?German translation task. The system combination is tuned on news-
test2011, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than
the best single system with p< 0.05. Italic font indicates system combination results that are significantly
better than the best single system with p < 0.1.
tributing individual systems. KIT is providing a
phrase-based system output, UEDIN is providing
two phrase-based system outputs and six syntax-
based ones (GHKM). For English?German, our
system combination parameters are optimized on
newstest2011. Combining all nine different sys-
tem outputs yields an improvement of 0.5 points
in BLEU and 1.7 points in TER over the best sin-
gle system performance.
In Table 4 the cross BLEU scores for all
English?German systems are given. The individ-
ual system of KIT and the syntax-based ParZu sys-
tem of UEDIN have the lowest BLEU score when
scored against each other. Both approaches are
quite different and both are coming from differ-
ent institutes. In contrast, both phrase-based sys-
tems pbt 1 and pbt 2 from UEDIN are very sim-
ilar and hence have a high pairwise BLEU score.
109
pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscom
pbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12
pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75
ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39
BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08
Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51
S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81
T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13
S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46
KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33
syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27
Table 4: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as reference and the other one as hypothesis.)
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 1: Sentence distribution for the
German?English newstest2013 test set compar-
ing system combination output against the best
individual system.
As for the German?English translation direction,
the best performing individual system outputs are
also having the highest BLEU scores when evalu-
ated against the final system combination output.
In Figure 2 system combination output is com-
pared to the best single system pbt 2. We distribute
the sentence-level BLEU scores of all sentences
of newstest2013. Many sentences have been im-
proved by system combination. But there is still
room for improvement as some sentences are still
better in terms of sentence-level BLEU in the indi-
vidual best system pbt 2.
7 Conclusion
We achieved significantly better translation perfor-
mance with gains of up to +1.6 points in BLEU
and -1.0 points in TER by combining up to nine
different machine translation systems. Three dif-
ferent research institutes (RWTH Aachen Univer-
sity, University of Edinburgh, Karlsruhe Institute
of Technology) provided machine translation en-
gines based on different approaches like phrase-
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 2: Sentence distribution for the
English?German newstest2013 test set compar-
ing system combination output against the best
individual system.
based, hierarchical phrase-based, and syntax-
based. For English?German, we included six
different syntax-based systems, which were com-
bined to our final combined translation. The au-
tomatic scores of all submitted system outputs for
the actual 2014 evaluation set are presented on the
WMT submission page.
2
Our joint submission is
the best submission in terms of BLEU and TER for
both translation directions German?English and
English?German without adding any new data.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658.
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1 148717.
2
http://matrix.statmt.org/
110
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 65?72, Ann Arbor, MI, USA, June.
Alexandra Birch, Nadir Durrani, and Philipp Koehn.
2013. Edinburgh SLT and MT System Description
for the IWSLT 2013 Evaluation. In Proceedings
of the 10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg, Ger-
many, December.
Maximilian Bisani and Hermann Ney. 2004. Bootstrap
Estimates for Confidence Intervals in ASR Perfor-
mance Evaluation. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
volume 1, pages 409?412, Montr?eal, Canada, May.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Ma-
chine Translation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Arbor,
Michigan, June.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013a. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013b. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richard Farkas. 2013c. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburgh?s Phrase-based
Machine Translation Systems for WMT-14. In Pro-
ceedings of the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, MD, USA,
June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In EMNLP, pages 53?61.
M. Freitag, S. Peitz, J. Wuebker, H. Ney, N. Dur-
rani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues,
M. Mediani, T. Herrmann, A. Waibel, N. Bertoldi,
M. Cettolo, and M. Federico. 2013. EU-BRIDGE
MT: Text Translation of Talks in the EU-BRIDGE
Project. In International Workshop on Spoken Lan-
guage Translation, Heidelberg, Germany, Decem-
ber.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open Source Machine Translation Sys-
tem Combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, HI, USA, Octo-
ber.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of the Human Language Technology Conf.
/ North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273?280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st International Conf. on Computa-
tional Linguistics and 44th Annual Meeting of the
Assoc. for Computational Linguistics, pages 961?
968, Sydney, Australia, July.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 292?301, Jeju, Republic of Korea,
July.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, UK, July.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Atlanta, GA, USA, June.
111
Teresa Herrmann, Mohammed Mediani, Eunah Cho,
Thanh-Le Ha, Jan Niehues, Isabel Slawik, Yuqi
Zhang, and Alex Waibel. 2014. The Karlsruhe In-
stitute of Technology Translation Systems for the
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. pages 152?159, Tokyo, Japan, December.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A Phrase Orientation Model
for Hierarchical Machine Translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452?463, Sofia, Bulgaria, August.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Augmenting String-to-Tree and Tree-to-
String Translation with Non-Syntactic Phrases. In
Proceedings of the ACL 2014 Ninth Workshop on
Statistical Machine Translation, Baltimore, MD,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In EMNLP-CoNLL, pages 868?876,
Prague, Czech Republic, June.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, pages 177?180,
Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In Proc. Human Language Technol-
ogy Conf. / North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 169?176, Boston, MA, USA,
May.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, CA, USA, December.
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 210?217, Singapore, Au-
gust.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA, USA.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of Third ACL Workshop on Statisti-
cal Machine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Stephan Peitz, Joern Wuebker, Markus Freitag, and
Hermann Ney. 2014. The RWTH Aachen German-
English Machine Translation System for WMT
2014. In Proceedings of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation, Baltimore,
MD, USA, June.
112
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 433?440, Sydney,
Australia, July.
Anna N. Rafferty and Christopher D. Manning. 2008a.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Anna N. Rafferty and Christopher D. Manning. 2008b.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, UK.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, USA, Septem-
ber.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation (WMT), pages 388?394,
Montr?eal, Canada, June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh?s Syntax-Based Systems at
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statisti-
cal Machine Translation. In COLING ?12: The 24th
Int. Conf. on Computational Linguistics, pages 483?
491, Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving Statistical Machine
Translation with Word Class Models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, WA, USA, Oc-
tober.
113
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 157?162,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The RWTH Aachen German-English Machine Translation System for
WMT 2014
Stephan Peitz, Joern Wuebker, Markus Freitag and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
This paper describes the statistical ma-
chine translation (SMT) systems devel-
oped at RWTH Aachen University for the
German?English translation task of the
ACL 2014 Eighth Workshop on Statisti-
cal Machine Translation (WMT 2014).
Both hierarchical and phrase-based SMT
systems are applied employing hierarchi-
cal phrase reordering and word class lan-
guage models. For the phrase-based sys-
tem, we run discriminative phrase training.
In addition, we describe our preprocessing
pipeline for German?English.
1 Introduction
For the WMT 2014 shared translation task
1
RWTH utilized state-of-the-art phrase-based and
hierarchical translation systems. First, we describe
our preprocessing pipeline for the language pair
German?English in Section 2. Furthermore, we
utilize morpho-syntactic analysis to preprocess the
data (Section 2.3). In Section 3, we give a survey
of the employed systems and the basic methods
they implement. More details are given about the
discriminative phrase training (Section 3.4) and
the hierarchical reordering model for hierarchical
machine translation (Section 3.5). Experimental
results are discussed in Section 4.
2 Preprocessing
In this section we will describe the modification of
our preprocessing pipeline compared to our 2013
WMT German?English setup.
2.1 Categorization
We put some effort in building better categories for
digits and written numbers. All written numbers
1
http://www.statmt.org/wmt14/
translation-task.html
were categorized. In 2013 they were just handled
as normal words which leads to a higher number of
out-of-vocabulary words. For German?English,
in most cases for numbers like ?3,000? or ?2.34?
the decimal mark ?,? and the thousands separator
?.? has to be inverted. As the training data and also
the test sets contain several errors for numbers in
the source as well as in the target part, we put more
effort into producing correct English numbers.
2.2 Remove Foreign Languages
The WMT German?English corpus contains
some bilingual sentence pairs with non-German
source or/and non-English target sentences. For
this WMT translation task, we filtered all non-
matching language pairs (in terms of source lan-
guage German and target language English) from
our bilingual training set.
First, we filtered languages which contain non-
ascii characters. For example Chinese, Arabic or
Russian can be easily filtered when deleting sen-
tences which contain more than 70 percent non-
ascii words. The first examples of Table 1 was
filtered due to the fact, that the source sentence
contains too many non-ascii characters.
In a second step, we filtered European lan-
guages containing ascii characters. We used the
WMT monolingual corpora in Czech, French,
Spanish, English and German to filter these lan-
guages from our bilingual data. We could both
delete a sentence pair if it contains a wrong source
language or a wrong target language. That is the
reason why we even search for English sentences
in the source part and for German sentences in
the target part. For each language, we built a
word count of all words in the monolingual data
for each language separately. We removed punc-
tuation which are no indicator of a language. In
our experiments, we only considered words with
frequency higher than 20 (e.g. to ignore names).
Given the word frequency, we removed a bilingual
157
Table 1: Examples of sentences removed in preprocessing.
Example
remove non-ascii symbols ????????? .
zum Bericht A?noveros Tr??as de Bes
remove wrong languages from target Honni soit qui mal y pense !
as you yourself have said : travailler plus pour gagner plus
remove wrong languages from source je d?eclare interrompue la session du Parlement europ?een .
Quelle der Tabelle : ? what Does the European Union do ? ?
sentence pair from our training data if more than
70 percent of the words had a higher count in a
different language then the one we expected. In
Table 1 some example sentences, which were re-
moved, are illustrated.
In Table 2 the amount of sentences and the cor-
responding vocabulary sizes of partial and totally
cleaned data sets are given. Further we provide the
number of out-of-vocabulary words (OOVs) for
newstest2012. The vocabulary size could be re-
duced by ?130k words for both source and target
side of our bilingual training data while the OOV
rate kept the same. Our experiments showed, that
the translation quality is the same with or with-
out removing wrong sentences. Nevertheless, we
reduced the training data size and also the vocabu-
lary size without any degradation in terms of trans-
lation quality.
2.3 Morpho-syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation further, the Ger-
man text is preprocessed by splitting German com-
pound words with the frequency-based method de-
scribed in (Koehn and Knight, 2003). To reduce
translation complexity, we employ the long-range
part-of-speech based reordering rules proposed by
Popovi?c and Ney (2006).
3 Translation Systems
In this evaluation, we employ phrase-based trans-
lation and hierarchical phrase-based translation.
Both approaches are implemented in Jane (Vilar et
al., 2012; Wuebker et al., 2012), a statistical ma-
chine translation toolkit which has been developed
at RWTH Aachen University and is freely avail-
able for non-commercial use.
2
In the newest inter-
nal version, we use the KenLM Language Model
Interface provided by (Heafield, 2011) for both de-
coders.
2
http://www.hltpr.rwth-aachen.de/jane/
3.1 Phrase-based System
In the phrase-based decoder (source cardinality
synchronous search, SCSS, Wuebker et al. (2012)),
we use the standard set of models with phrase
translation probabilities and lexical smoothing in
both directions, word and phrase penalty, distance-
based distortion model, an n-gram target language
model and three binary count features. Additional
models used in this evaluation are the hierarchical
reordering model (HRM) (Galley and Manning,
2008) and a word class language model (wcLM)
(Wuebker et al., 2013). The parameter weights
are optimized with minimum error rate training
(MERT) (Och, 2003). The optimization criterion
is BLEU (Papineni et al., 2002).
3.2 Hierarchical Phrase-based System
In hierarchical phrase-based translation (Chiang,
2007), a weighted synchronous context-free gram-
mar is induced from parallel text. In addition to
contiguous lexical phrases, hierarchical phrases
with up to two gaps are extracted. The search is
carried out with a parsing-based procedure. The
standard models integrated into our Jane hierar-
chical systems (Vilar et al., 2010; Huck et al.,
2012) are: Phrase translation probabilities and lex-
ical smoothing probabilities in both translation di-
rections, word and phrase penalty, binary features
marking hierarchical phrases, glue rule, and rules
with non-terminals at the boundaries, three binary
count features, and an n-gram language model.
We utilize the cube pruning algorithm for decod-
ing (Huck et al., 2013a) and optimize the model
weights with MERT. The optimization criterion is
BLEU.
3.3 Other Tools and Techniques
We employ GIZA
++
(Och and Ney, 2003) to train
word alignments. The two trained alignments
are heuristically merged to obtain a symmetrized
word alignment for phrase extraction. All lan-
158
Table 2: Corpus statistics after each filtering step and compound splitting.
Vocabulary OOVs
Sentences German English newstest2012
Preprocessing 2013 4.19M 1.43M 784K 1019
Preprocessing 2014 4.19M 1.42M 773K 1018
+ remove non-ascii symbols 4.17M 1.36M 713K 1021
+ remove wrong languages from target 4.15M 1.34M 675K 1027
+ remove wrong languages from source 4.08M 1.30M 655K 1039
+ compound splitting 4.08M 652K 655K 441
guage models (LMs) are created with the SRILM
toolkit (Stolcke, 2002) or with the KenLM lan-
guage model toolkit (Heafield et al., 2013) and are
standard 4-gram LMs with interpolated modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998). We evaluate in true-
case with BLEU and TER (Snover et al., 2006).
3.4 Discriminative Phrase Training
In our baseline translation systems the phrase ta-
bles are created by a heuristic extraction from
word alignments and the probabilities are esti-
mated as relative frequencies, which is still the
state-of-the-art for many standard SMT systems.
Here, we applied a more sophisticated discrimi-
native phrase training method for the WMT 2014
German?English task. Similar to (He and Deng,
2012), a gradient-based method is used to opti-
mize a maximum expected BLEU objective, for
which we define BLEU on the sentence level with
smoothed 3-gram and 4-gram precisions. To that
end, the training data is decoded to generate 100-
best lists. We apply a leave-one-out heuristic
(Wuebker et al., 2010) to make better use of the
training data. Using these n-best lists, we itera-
tively perform updates on the phrasal translation
scores of the phrase table. After each iteration,
we run MERT, evaluate on the development set
and select the best performing iteration. In this
work, we perform two rounds of discriminative
training on two separate data sets. In the first
round, training is performed on the concatenation
of newstest2008 through newstest2010 and an au-
tomatic selection from the News-commentary, Eu-
roparl and Common Crawl corpora. The selec-
tion is based on cross-entropy difference of lan-
guage models and IBM-1 models as described by
Mansour et al. (2011) and contains 258K sentence
pairs. The training took 4.5 hours for 30 iterations.
On top of the final phrase-based systems, a second
round of discriminative training is run on the full
news-commentary corpus concatenated with new-
stest2008 through newstest2010.
3.5 A Phrase Orientation Model for
Hierarchical Machine Translation
In Huck et al. (2013b) a lexicalized reorder-
ing model for hierarchical phrase-based machine
translation was introduced. The model scores
monotone, swap, and discontinuous phrase ori-
entations in the manner of the one presented by
(Tillmann, 2004). Since improvements were re-
ported on a Chinese?English translation task, we
investigate the impact of this model on a European
language pair. As in German the word order is
more flexible compared with the target language
English, we expect that an additional reordering
model could improve the translation quality. In
our experiments we use the same settings which
worked best in (Huck et al., 2013b).
4 Setup
We trained the phrase-based and the hierarchical
translation system on all available bilingual train-
ing data. Corpus statistics can be found in the
last row of Table 2. The language model are
4-grams trained on the respective target side of
the bilingual data,
1
2
of the Shuffled News Crawl
corpus,
1
4
of the 10
9
French-English corpus and
1
2
of the LDC Gigaword Fifth Edition corpus.
The monolingual data selection is based on cross-
entropy difference as described in (Moore and
Lewis, 2010). For the baseline language model,
we trained separate models for each corpus, which
were then interpolated. For our final experiments,
we also trained a single unpruned language model
on the concatenation of all monolingual data with
KenLM.
159
Table 3: Results (truecase) for the German?English translation task. BLEU and TER are given in
percentage. All HPBT setups are tuned on the concatenation of newstest2012 and newstest2013. The
very first SCSS setups are optimized on newstest2012 only.
newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
SCSS +HRM 22.4 60.1 23.7 59.0 25.9 55.7
+wcLM 22.8 59.6 24.0 58.6 26.3 55.4
+1st round discr. 23.0 59.5 24.2 58.2 26.8 55.1
+tune11+12. 23.4 59.5 24.2 58.6 26.8 55.2
+unprunedLM 23.6 59.5 24.2 58.6 27.1 55.0
+2nd round discr. 23.7 59.5 24.4 58.5 27.2 55.0
HPBT baseline 23.3 59.9 24.2 58.9 26.7 55.6
+wcLM 23.4 59.8 24.1 58.9 26.8 55.6
+HRM 23.3 60.0 24.2 58.9 26.9 55.5
+HRM +wcLM 23.3 59.9 24.1 59.1 26.7 55.9
4.1 Experimental Results
The results of the phrase-based system (SCSS)
as well as the hierarchical phrase-based system
(HPBT) are summarized in Table 3.
The phrase-based baseline system, which in-
cludes the hierarchical reordering model by (Gal-
ley and Manning, 2008) and is tuned on new-
stest2012, reaches a performance of 25.9% BLEU
on newstest2013. Adding the word class language
model improves performance by 0.4% BLEU ab-
solute and the first round of discriminative phrase
training by 0.5% BLEU absolute. Next, we
switched to tuning on a concatenation of new-
stest2011 and newstest2012, which we expect to
be more reliable with respect to unseen data. Al-
though the BLEU score does not improve and TER
goes up slightly, we kept this tuning set in the sub-
sequent setups, as it yielded longer translations,
which in our experience will usually be preferred
by human evaluators. Switching from the inter-
polated language model to the unpruned language
model trained with KenLM on the full concate-
nated monolingual training data in a single pass
gained us another 0.3% BLEU. For the final sys-
tem, we ran a second round of discriminative train-
ing on different training data (cf. Section 3.4),
which increased performance by 0.1% BLEU to
the final score 27.2.
For the phrase-based system, we also exper-
imented with weighted phrase extraction (Man-
sour and Ney, 2012), but did not observe improve-
ments.
The hierarchical phrase-based baseline without
any additional model is on the same level as the
phrase-based system including the word class lan-
guage model, hierarchical reordering model and
discriminative phrase training in terms of BLEU.
However, extending the system with a word class
language model or the additional reordering mod-
els does not seem to help. Even the combination
of both models does not improve the translation
quality. Note, that the hierarchical system was
tuned on the concatenation newstest2011 and new-
stest2012. The final system employs both word
class language model and hierarchical reordering
model.
Both phrase-based and hierarchical phrase-
based final systems are used in the EU-Bridge sys-
tem combination (Freitag et al., 2014).
5 Conclusion
For the participation in the WMT 2014 shared
translation task, RWTH experimented with both
phrase-based and hierarchical translation systems.
For both approaches, we applied a hierarchical
phrase reordering model and a word class lan-
guage model. For the phrase-based system we em-
ployed discriminative phrase training. Addition-
ally, improvements of our preprocessing pipeline
compared to our WMT 2013 setup were described.
New introduced categories lead to a lower amount
of out-of-vocabulary words. Filtering the corpus
for wrong languages gives us lower vocabulary
sizes for source and target without loosing any per-
formance.
160
Acknowledgments
The research leading to these results has partially
received funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n
o
287658.
Furthermore, this material is partially based
upon work supported by the DARPA BOLT
project under Contract No. HR0011- 12-C-0015.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of DARPA.
References
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, Massachusetts, USA, August.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201?
228.
Markus Freitag, Stephan Peitz, Joern Wuebker, Her-
mann Ney, Matthias Huck, Rico Sennrich, Nadir
Durrani, Maria Nadejde, Philip Williams, Philipp
Koehn, Teresa Herrmann, Eunah Cho, and Alex
Waibel. 2014. EU-BRIDGE MT: Combined Ma-
chine Translation. In Proceedings of the ACL 2014
Ninth Workshop on Statistical Machine Translation,
Baltimore, MD, USA, June.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, Hawaii, USA,
October.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 292?301, Jeju, Republic of Korea, Jul.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37?
50, October.
Matthias Huck, David Vilar, Markus Freitag, and Her-
mann Ney. 2013a. A Performance Study of
Cube Pruning for Large-Scale Hierarchical Machine
Translation. In Proceedings of the NAACL 7thWork-
shop on Syntax, Semantics and Structure in Statis-
tical Translation, pages 29?38, Atlanta, Georgia,
USA, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013b. A phrase orientation model
for hierarchical machine translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452?463, Sofia, Bulgaria, August.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-gram Language Model-
ing. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 181?184, May.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings of
European Chapter of the ACL (EACL 2009), pages
187?194.
Saab Mansour and Hermann Ney. 2012. A Simple and
Effective Weighted Phrase Extraction for Machine
Translation Adaptation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 193?200, Hong Kong, December.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, California, USA, December.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In ACL
(Short Papers), pages 220?224, Uppsala, Sweden,
July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
161
Maja Popovi?c and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Transla-
tion. In International Conference on Language Re-
sources and Evaluation, pages 1278?1283, Genoa,
Italy, May.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, Colorado, USA,
September.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04, pages 101?104, Boston, MA,
USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216, September.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475?484, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statis-
tical Machine Translation. In International Confer-
ence on Computational Linguistics, pages 483?491,
Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving statistical machine
translation with word class models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, USA, October.
162
