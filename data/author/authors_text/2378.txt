Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 600?609,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Mention Detection Crossing the Language Barrier
Imed Zitouni and Radu Florian
IBM T.J. Watson Research Center
1101 Kitchawan Rd, Yorktown Heights, NY 10598
{izitouni, raduf}@us.ibm.com
Abstract
While significant effort has been put into an-
notating linguistic resources for several lan-
guages, there are still many left that have
only small amounts of such resources. This
paper investigates a method of propagat-
ing information (specifically mention detec-
tion information) into such low resource
languages from richer ones. Experiments
run on three language pairs (Arabic-English,
Chinese-English, and Spanish-English) show
that one can achieve relatively decent perfor-
mance by propagating information from a lan-
guage with richer resources such as English
into a foreign language alone (no resources
or models in the foreign language). Fur-
thermore, while examining the performance
using various degrees of linguistic informa-
tion in a statistical framework, results show
that propagated features from English help
improve the source-language system perfor-
mance even when used in conjunction with all
feature types built from the source language.
The experiments also show that using propa-
gated features in conjunction with lexically-
derived features only (as can be obtained di-
rectly from a mention annotated corpus) yields
similar performance to using feature types de-
rived from many linguistic resources.
1 Introduction
Information extraction is a crucial step toward un-
derstanding a text, as it identifies the important con-
ceptual objects and relations between them in a dis-
course. It includes classification, filtering, and se-
lection based on the language content of the source
data, i.e., based on the meaning conveyed by the
data. It is a crucial step for several applications,
such as summarization, information retrieval, data
mining, question answering, language understand-
ing, etc. This paper addresses an important and basic
task of information extraction: mention detection1 :
the identification and classification of textual refer-
ences to objects/abstractions mentions, which can be
either named (e.g. John Smith), nominal (the presi-
dent) or pronominal (e.g. he, she). For instance, in
the sentence
President John Smith said he has no
comments.
there are three mentions: President, John Smith and
he. This is similar to the named entity recognition
(NER) task with the additional twist of also identi-
fying nominal and pronominal mentions.
A few languages have received a lot of attention
in terms of natural language resources that were cre-
ated ? for instance, in English one has access to la-
beled part-of-speech data, word sense information,
parse tree structure, discourse, semantic role labeles,
named entity data, to name just a few (our apologies
if we missed your favorite resource). There are a few
other languages that also have annotated resources
(such as Arabic, Chinese, German, French, Spanish,
etc), but also a very large number of languages with
few resources. It would be very useful if one could
make use of the resources in the former languages
to help bootstrapping (or just the projection) of re-
source in any resource-challenged language.
Information transfer from a language to another
can be very useful when the ?donor? language has
more resources than the receiving one. As resources
grow in quantity and quality in the receiving lan-
guage, it becomes less and less likely that there will
be a gain in performance by transfering information,
as there are several sources of noise involved in the
1We adopt here the ACE (NIST, 2007) nomenclature
600
process - such as the translation (machine generated
or not) and the inherent imperfection of the mention
detection in the donor language. To test this hypoth-
esis, we conducted experiments on systems build
with a varied amount of resources in the receiv-
ing language, starting with the case where there are
none2 (all information is transferred through transla-
tion alignment), and ending with the case where we
used all the resources we could gather for that lan-
guage. The experiments will show that the gain in
performance decreases with the amount of resources
used in the source language, but, still, even when all
resources were used, a statistically significant gain
was still observed.
Similarly to classical NLP tasks such as text
chunking (Ramshaw and Marcus, 1995) and named
entity recognition (Tjong Kim Sang, 2002), we for-
mulate mention detection as a sequence classifica-
tion problem, by assigning a label to each token in
the text, indicating whether it starts a specific men-
tion, is inside a specific mention, or is outside any
mentions. The classification is performed with a sta-
tistical approach, built around the maximum entropy
(MaxEnt) principle (Berger et al, 1996), that has the
advantage of combining arbitrary types of informa-
tion in making a classification decision.
2 Previous Work
There are several investigations in literature that
explore using parallel corpora to transfer informa-
tion content from one language (most of the time
English) to another. The earliest investigations of
the subject have been performed, on word sense
disambiguation (Dagan et al, 1991; P.F.Brown et
al., 1991; Gale et al, 1992) (perhaps unsurpris-
ingly given its close connection to machine trans-
lation) ? all propose and (lightly) evaluate methods
to use word sense information extracted from the
target language to help the sense resolution in the
source language and machine translation. (Dagan
and Itai, 1994) explicitly suggests performing word
sense disambiguation in the target language (English
in the article) with the goal of resolving ambiguity in
the source language (Hebrew), and show moderate
2While applying this method in the case where the source
language has absolutely no resources might be an interesting
test case, we don?t see it as being realistic. Resources are build
nowadays in a large variety of languages, and not making use
of them is rather foolish (a certain big bird and sand comes to
mind).
improvement on a small data set3. More recently,
(Diab and Resnik, 2001) presents a method for per-
forming word sense tagging in both the source and
target texts of parallel bilingual corpora with the En-
glish WordNet sense inventory, by using translation
correspondences.
On more general cross-language information
transfer, (Yarowsky et al, 2001) proposed and eval-
uated a method of propagating POS tagging, named
mention, base noun phrase, and morphological in-
formation from English into a foreign language,
which is very similar to the one presented in this
article (experiments were run on French, Chinese,
Czech, and Spanish ? on human-generated transla-
tions). Their results show a significant improvement
in performance while building an automatic classi-
fier on the projected annotations over the same au-
tomatic classifier trained on a small amount of an-
notated data in the source language. (Riloff et al,
2002) extends the ideas in (Yarowsky et al, 2001),
by showing how it can be used, in conjunction with
an automatically trained information extraction sys-
tem on the source language, to bootstrap the annota-
tion of resources in the target language. They show
that they can obtain 48 F-measure on a information
extraction task identifying locations, vehicles and
victims in plane crashes. (Hwa et al, 2002) proposes
a framework that enables the acquisition of syntactic
dependency trees for low-resource languages by im-
porting linguistic annotation from rich-resource lan-
guages (English). The authors run a large-scale ex-
periment in which Chinese dependency parses were
induced from English, and show that a parser trained
on the resulting trees outperformed simple baselines.
(Cabezas et al, 2001) investigates a similar method
of propagating syntactic treebank-like annotations
from English to Spanish.
Finally, a large body of research has been done
on cross-language information retrieval, where the
goal is to find information in one language (e.g. Chi-
nese newswire) corresponding to a query in a differ-
ent language (e.g. English) ? although the list of rel-
evant papers is too long to be mentioned here (see,
for instance, (Grefenstette, 1998)).
The work presented here differs from the infor-
mation extraction investigations presented above in
two aspects:
? it handles unrestricted text and a full set of
3Very small by ?modern? standards - 137 examples. Prob-
ably because at the time the article was written, there were no
large publicly annotated databases, such as Semcor.
601
mention types (the ACE entity types) during the
information transfer
? it investigates whether using a resource-rich
language (English) can improve on the perfor-
mance obtained by using various degrees of ex-
istent resources in the source language (Arabic,
Chinese, Spanish)
? the information transfer is performed over ma-
chine generated translations and alignments.
3 Mention Detection
As mentioned in the introduction, the mention detec-
tion problem is formulated as a classification prob-
lem, by assigning to each token in the text a label,
indicating whether it starts a specific mention, is in-
side a specific mention, or is outside any mentions.
Good performance in many natural language pro-
cessing tasks has been shown to depend heavily on
integrating many sources of information (Florian et
al., 2004).4 Given this observation, we are interested
in algorithms that can easily integrate and make ef-
fective use of diverse input types. We select a ex-
ponential classifier, the Maximum Entropy (MaxEnt
henceforth) classifier that integrates arbitrary types
of information and makes a classification decision
by aggregating all information available for a given
classification. But the reader can replace it with her
favorite feature-based classifier throughout the pa-
per.
To help with the presentation, we introduce some
notations: let Y = {y1, . . . , yn} be the set of pre-
dicted classes, X be the example space and F =
{0, 1}m be a feature space. Each example x ? X
has associated a vector of m binary features f (x) =
(f1 (x) , . . . , fm (x)). The goal of the training pro-
cess is to associate examples x ? X with either
a probability distribution over the labels from Y ,
P (?|x)(if we are interested in soft classification) or
associate one label y ? Y (if we are interested in
hard classification).
The MaxEnt algorithm associates a set of weights
{?ij}i=1...nj=1...m with the features (fj)i, and computes
the probability distribution as
P (yi|x) =
1
Z(x)
m
?
j=1
?fj(x,yi)ij , (1)
Z(x) =
?
i
?
j
?fj(x,yi)ij
4In fact, the feature set used for classification has a much
larger impact on the performance of the resulting system than
the classifier method itself.
where Z(x) is a normalization factor. The
{?ij}j=1...m weights are estimated during the train-
ing phase to maximize the likelihood of the
data (Berger et al, 1996). In this paper, the Max-
Ent model is trained using the sequential condi-
tional generalized iterative scaling (SCGIS) tech-
nique (Goodman, 2002), and it uses a Gaussian
prior for regularization (Chen and Rosenfeld, 2000).
Now take xN1 = (x1, x2, . . . xN ), a sequence of
contiguous tokens (i.e., a sentence or a document) in
the source language. The goal of mention detection
system is to find the most likely sequence of labels
yN1 = (y1, y2 . . . yN ) that best matches the input xN1 .
In the mention detection case, each token xi in xN1
is tagged with a label yi as follows:5
? if it?s not part of any entity, yi = O (O for ?out-
side any mentions?)
? if it is part of an entity, it is composed of a sub-
tag specifying whether it starts a mention (B-)
or is inside a mention (I-), and a sub-type cor-
responding to mention type (e.g. B-PERSON).
In ACE, there are seven possible types: person,
organization, location, facility, geopolitical en-
tity (GPE), weapon, and vehicle.
To compute the best sequence yN1 , we use
yN1 = arg max
y?N1
P
(
y?N1 |xN1
)
= arg max
y?
?
P
(
y?j |xN1 , y?j?11
)
= arg max
y?
?
j
P
(
y?j |xN1 , yj?1j?k
)
where P
(
y?j|xN1 , yj?1j?k
)
has an exponential form of
the type (2). We also used the standard Markov as-
sumption that the probability P
(
y?j|xN1 , y?
j?1
1
)
only
depends on the previous k classifications. This
model is similar to the MEMM model (McCallum
et al, 2000), but it does not separate the probability
into generation probabilities and transition probabil-
ities, and, crucially, has access to ?future? observed
features (i.e. it can examine the entire xN1 sequence,
though in practice it will only examine some small
part of it) ? which is one way of eliminating label
5The mention encoding is the IOB2 encoding presented in
(Tjong Kim Sang and Veenstra, 1999) and introduced by
(Ramshaw and Marcus, 1994) for base noun phrase chunking.
602
bias observed by (Lafferty et al, 2001).6
The experiments are run on four languages, part
of the ACE-2007 evaluation (NIST, 2007): Arabic,
Chinese, English and Spanish.7 Systems across the
languages use a large range of features, including
lexical (words and morphs in a 3-word window, pre-
fixes and suffixes of length up to 4 characters, Word-
Net (Miller, 1995) for English), syntactic (POS tags,
text chunks), and the output of other information ex-
traction models. These features were described in
(Florian et al, 2004), and are not discussed here. In
this paper we focus on the examining the benefit of
cross-language mention propagation information in
improving mention detection systems.
Besides generic types of features, we also have
implemented language-specific features:
? In Arabic, blank-delimited words are com-
posed of zero or more prefixes, followed by a
stem and zero or more suffixes. Each prefix,
stem or suffix is a token; any contiguous se-
quence of tokens can represent a mention. Sim-
ilar to the approaches described in (Florian et
al., 2004) and (Zitouni et al, 2005), we decided
to ?condition? the output of the system on the
segmented data: the text is segmented first into
tokens and classification is then performed on
tokens. The segmentation model is similar to
the one presented by (Lee et al, 2003) and ob-
tains an accuracy of 98%.
? In Chinese text, unlike in Indo-European lan-
guages, words neither are white-space delim-
ited nor do they have capitalization markers.
Instead of a word-based model, we build a
character-based one, since word segmentation
errors can lead to irrecoverable mention detec-
tion errors; Jing et al (2003) also observes that
character-based models are better performing
than word-based ones. Word segmentation in-
formation is still useful and is integrated as an
additional feature stream.
? In English and in Spanish mention detection
systems are similar to those described in (Flo-
rian et al, 2004) where words are the tokens to
classify.
6In fact their example of label bias can be trivially solved
by allowing the classifier to examine features for subsequent
words.
7The ACE data has the nice property of being consistent in
annotations across these languages.
4 Cross-Language Mention Propagation
The approach proposed in this article requires a
mention detection system build in a resource-rich
language, and a translation from the source lan-
guage to the resource-rich language, together with
word alignment. This assumption is realistic: while
truly parallel data (humanly created) might be in
short supply or harder to acquire, adapting statis-
tical machine translation (SMT) systems from one
language-pair to another is not as challenging as it
used to be (Al-Onaizan and Papineni, 2006). We
also find that there is a large number of parallel
corpora available these days which cover many lan-
guage pairs. For example, for the European Union?s
23 official languages we find 253 language pairs;
each document in one language might have to be
translated in all other 22 languages. This is in ad-
dition to parallel corpora one could get from books,
including religious texts such as the Bible, that are
translated to a large number of languages. On the
other hand, even though mention detection system
is important for many natural language processing
applications, we still find lack of mention-annotated
corpora in many languages. In the approach we pro-
pose below, the annotated corpus used to train the
mention detection classifier does not have to be part
of a parallel corpus.
To start the process, we first use a SMT system
to translate the source unit (document or sentence)
xN1 into the resource-rich language, yielding the se-
quence ?M1 = (?1, ?2, . . . ?M ). Taking the sequence
of tokens ?M1 as input, the MaxEnt classifier assigns
a mention label to each token, building the label se-
quence ?M1 = (?1, ?2 . . . ?M ). Using the SMT-
produced word alignment between source text xN1
and translated text ?M1 (Koehn, 2004),we propagate
the target labels ?M1 to the source language build-
ing the label sequence y?N1 = (y?1, y?2 . . . y?N ).8 As
an example, if a sequence of tokens in the resource-
rich language ?i?i+1?i+2 is aligned to xjxj+1 in the
source language and if ?i?i+1?i+2 is tagged as a lo-
cation mention, then the sequence xjxj+1 can be la-
beled as a location mention: B-LOC, I-LOC. Hence,
each token xi in xN1 is tagged with a corresponding
propagated label y?i in y?N1 , y?i = ?
(
i, A, ?M1
)
, where
A is the alignment between the source and resource-
rich languages. In cases when the alignment is 1-
to-1 the function becomes the identity, but one can
imagine different scenarios which can be used in
8Or by using Giza++ if your favorite engine does not give
you word alignment.
603
 El soldado nepal?s fue baleado              por ex soldados haitianos cuando patrullaba la zona central de Haiti , inform? Minustah .
The Nepalese soldier was gunned down by former Haitian soldiers when patrullaba  the central area of Haiti , reported minustah .
GPELOCPERGPE
GPEPER ORGGPELOCGPEPER
PERGPE
Figure 1: Word alignment for a Spanish sentence and its English machine-translation. The mention labels shown are
the gold-standard ones for Spanish and the automatically detected ones for English. If mentions were to be propagated
from English to Spanish, the last mention would be a miss, due to the fact that the English mention detection failed to
identify ?minustah? as an organization.
many-to-many alignment cases. The alignement we
use in this paper is 1-to-many ({1...n}) from the
source language (eg., Arabic) to the resource-rich
language (e.g., English). Once we use SMT word
alignment to propagate label sequence ?M1 of ?M1 to
the corresponding text xN1 in the target language, we
end up with a sequence of labels y?N1 where for each
token xi in xN1 we attach its label y?i in y?N1 . Hence,
we label te entire span and if the strategy results in
two mentions where one contains the other, we elim-
inate the inner one.
Figure 1 displays the alignment between a Span-
ish sentence and its English automatic translation. It
also shows a good match between the gold-standard
tags in Spanish and the automatically extracted tags
in English.
There are three ways in which we propose using
these propagated labels:
1. Consider y?N1 as the result of propagating the
detected mentions in the original text xN1 , basi-
cally selecting yN1 = y?N1 . This situation corre-
sponds to a case where no resources (annotated
data) are available/needed on the source side,
where the propagated labels are the output of
the system.
2. Use the label sequence y?N1 as an additional fea-
ture in the MaxEnt framework when predicting
P
(
yj|xN1 , yj?1j?k
)
, together with other features
built from resources available on the source
language. We will call this model CDP (Con-
text Dependent Propagation).
3. Starting with a large corpus (possibly including
the training data), translate it into the resource-
rich language and run mention detection. Then
select the word sequences in the source lan-
guage associated with the found mentions in
the translation and add them to a machine-
generated gazetteer G9. This gazetteer G is then
used to construct features for classification. We
will call this model CIP (Context Independent
Propagation).
From a runtime point of view, the CIP method has
the advantage that there is no need to perform ma-
chine translation, and it can incorporate data from a
very large amount of text. The CDP method, on the
other hand, has the advantage that features are com-
puted in context, and will not fire unless the corre-
sponding mentions were found in the translated ver-
sion (hence the name). Of course, the CDP method
can incorporate features generated in the dictionary
G. The experimental section analyzes the impact of
each of these techniques on mention detection task
performance.
5 Resources
Experiments are conducted on the ACE 2007 data
sets10, in four languages: Arabic, Chinese, English,
and Spanish. This data is selected from a variety
of sources (broadcast news, broadcast conversations,
newswire, web log, newswire, conversational tele-
phony) and is labeled with 7 types: person, organi-
zation, location, facility, GPE (geo-political entity),
vehicle and weapon. Besides mention level informa-
tion, also labeled are coreference between the men-
tions, relations, events, and time resolution.
Since the evaluation tests set are not publicly
available, we have split the publicly available train-
ing corpus into an 85%/15% data split. To facilitate
future comparisons with work presented here, and
to simulate a realistic scenario, the splits are created
based on article dates: the test data is selected as the
latest 15% of the data in chronological order, in each
of the covered genres. This way, the documents in
9This is in fact a way to automatically construct a source-
side mention dictionary.
10Same data as for ACE 2008.
604
Language Training Test
Arabic 323 56
Chinese 538 95
English 499 100
Spanish 467 52
Table 1: Datasets size (number of documents)
the training and test data sets do not overlap in time,
and the content of the test data is more recent than
the training data. Table 1 presents the number of
documents in the training/test datasets for each of
the four languages.
While performance on the ACE data is usually
evaluated using a special-purpose measure - the
ACE value metric (NIST, 2007), given that we are
interested in the mention detection task only, we
decided to use the more intuitive and popular (un-
weighted) F-measure, the harmonic mean of preci-
sion and recall.
6 Resource-Rich Languages
From the set of four languages in ACE 2007, we
will unsurprisingly select English as the resource-
rich language. Table 2 shows the performance of
mention detection systems in all 4 languages one
can obtain by using all available resources in that
language, including lexical (words and morphs in a
3-word window, prefixes and suffixes of length up
to 4, WordNet (Miller, 1995) for English), syntac-
tic (POS tags, text chunks), and the output of other
information extraction models.
N P R F
Arabic 3566 83.6 76.8 80.0
Chinese 4791 81.1 71.3 75.8
English 8170 84.6 80.8 82.7
Spanish 2487 79.1 73.5 76.2
Table 2: Performance of Arabic, Chinese, English and
Spanish mention detection systems. Performance is pre-
sented in terms of Precision (P), Recall (R), and F-
measure (F). The column (N) displays the number of
mentions in the test set.
Results show that the English mention detection
system has a better performance when compared to
systems dealing with other languages such as Ara-
bic, Chinese and Spanish. These results are not un-
expected since the English model has access to a
larger training data and uses richer set of informa-
tion such as WordNet (Miller, 1995) and the output
Language Pair BLEU Score
Arabic-English 0.55
Chinese-English 0.32
Spanish-English 0.55
Table 3: BLEU performance of the SMT systems on the
3 language pairs
of a larger set of information extraction models.
7 Experiments
To show the effectiveness of cross-language mention
propagation information in improving mention de-
tection system performance in Arabic, Chinese and
Spanish, we use three SMT systems with very com-
petitive performance in terms of BLEU11 (Papineni
et al, 2002).
To give an idea of the SMT performance, Table 3
shows the performance of the translation systems on
the three language pairs, computed on standard test
sets. The Arabic to English SMT system is similar to
the one described in (Huang and Papineni, 2007); it
has 0.55 BLEU score on NIST 2003 Arabic-English
machine translation evaluation test set. The Chi-
nese to English SMT system has similar architecture
to the one described in (Al-Onaizan and Papineni,
2006). This system obtains a score of 0.32 cased
BLUE on NIST 2003 Arabic-English machine trans-
lation evaluation test set. The Spanish to English
SMT system is similar to the one described in (Lee et
al., 2006); it has a 0.55 BLEU score on the final text
edition of the European Parliament Plenary Speech
corpus in TC-STAR 2006 evaluation. As mentioned
earlier, these three SMT systems have very compet-
itive performance and are ranked among top 2 sys-
tems participating to NIST or TC-STAR evaluations.
Also, the English mention detection system used for
experiments has an F-measure of 82.7 and that has
very competitive results among systems participat-
ing in the ACE 2007 evaluation.
Experiments are conducted under several con-
ditions in order to investigate the effectiveness of
our approach in improving mention detection sys-
tem performance on languages with different levels
of resource availability (from simple to more com-
plex):
1. the system does not have access to any train-
ing data in the source language (no resources
11BLEU is an automatic measure for the translation quality
which makes good use of multiple reference translations.
605
needed besides the MT system);
2. the system has access to only lexical informa-
tion (information that can be directly derived
exclusively from mention-labeled text);
3. the system has access to lexical and syntactic
(e.g., POS tags, text chunks) information (re-
quires mention-labeled text, and models to pre-
dict POS tags, etc);
4. the system that has access to lexical, syntactic,
and semantic information (requires even more
models and labeled data).
The rest of this section examines in detail these four
cases.
To measure whether the improvement in per-
formance of a particular system over another
one is statistically significant or not, we use
the stratified bootstrap re-sampling significance
test (Noreen, 1989). This approach was used in the
named entity recognition shared task of CoNNL-
2002 (http://www.cnts.ua.ac.be/conll2002/ner/,
2002). In the following tables, we add a dagger sign
? to results that are not statistically significant when
compared to the baseline results.
7.1 No Source Language Training Data
In this first case, as described in Section 4, the men-
tion labels in the source language are obtained di-
rectly through the alignment from the mentions in
the translated text. This is a very simple scenario,
which can be implemented with ease, and, as we will
see, yields reasonable performance out-of-the-box.
N P R F
Arabic 3566 52.7 49.6 51.1
Chinese 4791 66.4 52.2 58.5
Spanish 2487 63.4 63.6 63.5
Table 4: Performance of the cross-language propagation
from English mention detection system onto Arabic, Chi-
nese and Spanish texts. Performance is presented in terms
of Precision (P), Recall (R), and F-measure (F). The col-
umn (N) shows the number of mentions in the test set.
Experimental results presented in Table 4 show
the performance of applying this information trans-
fer approach. For each source language (Arabic,
Chinese, or Arabic), we show the performance of
propagating mentions from the English text. Even
though no training data to build a source language
mention classifier is available, we still can detect
mentions with reasonably high accuracy. We con-
sider the obtained accuracy as reasonably good be-
cause, as an example, the performance of a sys-
tem that attaches to every word its most frequent
label (unigram) is around 25% F-measure on Ara-
bic. Results in Table 4 also show that even though
the Chinese-to-English SMT system is lower in term
of BLEU than the Arbic-to-English SMT system
(0.32 vs. 0.55), performance of the cross-language
propagation from English mention detection system
onto Chinese is better than the performance of the
propagation from English mention detection system
onto Arabic. One reason for this is that we notice
that Chinese-to-English SMT system translates and
aligns ACE categories better than Arabic-to-English
SMT system.
7.2 Lexical Resources
In this section, we consider the case when we have
available training data in the source language to be
able to train a statistical classifier. We also consider
that the classifier has access to lexical information
only. Our goal here is to study the effectiveness of
adding cross-language mention propagation infor-
mation to improve mention detection performance
on languages with limited resources.
Table 5 shows the performance of the 3 languages
with and without cross-language mention propaga-
tion information from English, with the 3 propa-
gation methods described in Section 4. One can
see that propagating mention propagation informa-
tion results in system performance increase12. When
systems use the CIP method, no improvement can
be observed on Arabic and Chinese, while a small
improvement of 0.5F point is obtained on Spanish
(74.5 vs. 75.0). In contrast, when systems use the
CDP method an improvement is obtained in recall
? which is to be expected, given the method ? lead-
ing to systems with better performance in terms of
F-measure: 1.6F points improvement for Arabic,
1.5F points improvement for Chinese and almost 3F
points improvement for Spanish. The results for all
the CDP transfers and the CIP for Spanish are statis-
tically significant.
7.3 Lexical and Syntactic Resources
We represent in Table 6 mention detection system
performance when syntactic resources are available
in the source language, in addition to lexical re-
12Only systems? performance marked with ? is not statisti-
cally significantly better.
606
Baseline CIP CDP
N P R F P R F P R F
Arabic: 3566 81.8 71.7 76.4 82.2 71.3 76.4? 82.6 73.9 78.0
Chinese: 4791 79.3 70.2 74.5 79.4 70.5 74.7? 79.8 72.5 76.0
Spanish: 2478 79.1 70.4 74.5 79.7 70.8 75.0 80.4 74.6 77.4
Table 5: Performance of Arabic, Chinese and Spanish mention detection using lexical features (?Baseline? column).
Columns ?CIP? stands for systems that add cross-language context independent mention propagation information and
column ?CDP? is for systems that add cross-language context dependent mention propagation information.
Baseline CIP CDP
N P R F P R F P R F
Arabic: 3566 82.2 72.6 77.1 82.7 72.9 77.5 83.2 74.5 78.6
Chinese: 4791 80.0 71.3 75.5 79.9 71.5 75.5? 81.0 72.4 76.5
Spanish: 2487 79.1 71.2 74.9 79.9 71.9 75.7 80.7 74.6 77.5
Table 6: Performance of Arabic, Chinese and Spanish mention detection using lexical and syntactic features (POS
tags, chunk information, etc).
sources available in the previous Subsection. This
experiment is important because it tests the effec-
tiveness of the propagation approach in improving
performance on languages with a typical level of re-
sources.
Results show that even in this situation, the use
of cross language mention propagation informa-
tion still lead to considerable improvement: using
the CDP transfer method yields improvements from
1.1F in Chinese to 2.6F in Spanish. Similar to the
previous section, the use of CIP information did not
improve performance significantly on Arabic (77.5
vs. 77.1) and Chinese (75.5 vs. 75.5) systems, but
we notice an improvement in Spanish13.
7.4 Lexical, Syntactic and Semantic Resources
This final section investigates whether the access
to cross-language mention propagation information
can still improve the performance of existing com-
petitive mention detection systems trained on lan-
guages with large resources. In this case, systems
have access to a full array of lexical, syntax, seman-
tic information, including the output from other in-
formation extraction models. Table 7 presents the
performance of mention detection systems on the
three languages, in the familiar 3 propagation meth-
ods: again, results show that better performance
is obtained when cross language mention informa-
tion is used. Under CIP, almost no change in terms
of performance is obtained for Arabic and Span-
13The dagger sign ? marks the systems that are not statisti-
cally significantly better.
ish, though a slight improvement can be observed
for Chinese (76.9F vs. 75.8F). When CDP is used
the performance of mention detection systems is im-
proved by 0.9F for Arabic (80.9 vs. 80.0), 2.3F
for Chinese (78.1F vs. 75.8F) and 1.9F for Span-
ish (78.1 vs. 76.2F). Once again, the results prove
that the use of cross language mention propagation
information, especially through CDP, is effective in
improving the performance even in this case.
By comparing results across tables, one can note
that systems having access to only lexical and cross
language mention propagation information are as ef-
fective as systems having access to large set of in-
formation. For Chinese, we obtain a performance of
75.8F when the system has access to lexical, syntac-
tic and output of other information extraction mod-
els. On the other hand, the same system has a
slightly better performance of 76.0 when it has ac-
cess to lexical and cross language mention propa-
gation information. The same behavior is observed
for Spanish, we obtain a performance of 76.2F when
the system has access to lexical, syntactic and output
of other information extraction models; compared to
77.4F when lexical and cross language mention in-
formation are used. This is not true for Arabic where
having access to larger set of information led to bet-
ter performance when compared to systems having
access to lexical information and CDP information
(80.0F vs. 78.0). We attribute this difference to
the fact that in Arabic we use the output of larger
number of information extraction models, and con-
sequently a richer set of information.
607
Baseline CIP CDP
N P R F P R F P R F
Arabic: 3566 83.6 76.8 80.0 83.9 77.0 80.2? 84.2 77.8 80.9
Chinese: 4791 81.1 71.3 75.8 81.4 73.0 76.9 81.7 74.8 78.1
Spanish: 2487 79.1 73.5 76.2 79.3 73.4 76.2? 80.1 76.2 78.1
Table 7: Performance of Arabic, Chinese and Spanish mention detection using lexical, syntactic and output of other
information extraction models: full-blown systems.
The other observation that is worth making is that
the improvement in performance has a decreasing
tendency as more resources are available. The per-
formance gain for CDP in Arabic goes from 1.6 to
1.5 to 0.9, and the one on Spanish goes from 2.9 to
2.6 to 1.9. The one on Chinese follows part of this
trend, as it goes from 1.4 to 1.1 to 2.3. While the
evidence here is not definitive, one can indeed note
the reduced effectiveness of the method as more re-
sources are available, which was indeed what we ex-
pected.
Results obtained by all these experiments help
answer an important question: when trying to im-
prove mention detection systems in a resource-poor
language, should we invest in building resources or
should we use propagation from a resource-rich lan-
guage to (at least) bootstrap the process? The answer
seems to be the latter.
8 Conclusion
This paper presents a new approach to mention de-
tection in low, medium or high-resource languages,
which benefits from projecting the output from a
resource-rich language such as English. We show
that even when no training data is available in one
source language, we can still build a decently per-
forming baseline mention detection system by only
using resources from English. This approach re-
quires a mention detection system on a resource-
rich language and an SMT system that translate text
from the source to the resource-rich language, both
of which can be attained.
In cases when large resources are available in the
source language, our cross language mention propa-
gation technique is still able to further improve men-
tion detection system performance. Experiments
performed on the four languages of ACE 2007, with
English chosen as the resource-rich language, show
consistent and significant improvements across con-
ditions and levels of linguistic sophistication. The
experiments are conducted on clearly specified par-
titions of the ACE 2007 data set, so future compar-
isons against the presented work can be correctly
and accurately made. We also note that systems
that have access to lexical and cross language men-
tion propagation information are as accurate as those
that have access to lexical, syntactic and output of
other information extraction models in the source
language (but no cross-language resources). As fu-
ture work, we plan to extend this work to use semi-
supervised and unsupervised approaches that can
make use of cross-language information propaga-
tion.
We believe that it is important for the research
community to continue to invest in building better
resources in ?source? languages, as it looks the most
promising approach. However, using a propagation
approach can definitely help bootstrap the process.
Acknowledgments
This work was supported by DARPA/IPTO Contract
No. HR0011-06-2-0001 under the GALE program.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 529?536, Sydney, Australia, July. Association
for Computational Linguistics.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
C. Cabezas, B. Dorr, and P. Resnik. 2001. Spanish lan-
guage processing at university of maryland: Building
infrastructure for multilingual applications. In Pro-
ceedings of the 2nd International Workshop on Span-
ish Language Processing and Language Technologies.
Stanley Chen and Ronald Rosenfeld. 2000. A survey of
smoothing techniques for me models. IEEE Trans. on
Speech and Audio Processing.
I. Dagan and A. Itai. 1994. Word sense disambiguation
using a second language monolingual corpus. Compu-
tational Linguistics, 20(4):563?596.
608
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Meet-
ing of the Association for Computational Linguistics,
pages 130?137.
Mona Diab and Philip Resnik. 2001. An unsupervised
method for word sense tagging using parallel corpora.
In ACL ?02: Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
255?262, Morristown, NJ, USA. Association for Com-
putational Linguistics.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics:
HLT-NAACL 2004, pages 1?8.
W. Gale, K. Church, and D. Yarowsky. 1992. A method
for disambiguating word senses in a large corpus.
Computers and the Humanities, 26:415?439.
Joshua Goodman. 2002. Sequential conditional general-
ized iterative scaling. In Proceedings of ACL?02.
Gregory Grefenstette. 1998. Cross-Language Informa-
tion Retrieval, volume 079238122X. Kluwer Aca-
demic Publishers.
http://www.cnts.ua.ac.be/conll2002/ner/. 2002.
Fei Huang and Kishore Papineni. 2007. Hierarchi-
cal system combination for machine translation. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 277?286.
Rebecca Hwa, Philip Resnik, and Amy Weinberg. 2002.
Breaking the resource bottleneck for multilingual pars-
ing. In Proceedings of the Workshop on Linguis-
tic Knowledge Acquisition and Representation: Boot-
strapping Annotated Language Data.
H. Jing, R. Florian, X. Luo, T. Zhang, and A. Itty-
cheriah. 2003. HowtogetaChineseName(Entity): Seg-
mentation and combination issues. In Proceedings of
EMNLP?03, pages 200?207.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings of AMTA?04, Washington
DC, September-October.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word seg-
mentation. In Proceedings of the ACL?03, pages 399?
406.
Young-Suk Lee, Yaser Al-Onaizan, Kishore Papineni,
and Salim Roukos. 2006. Ibm spoken language trans-
lation system. In TC-STAR Workshop on Speech-to-
Speech Translation, pages 13?18, Barcelona, Spain,
June.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models for
information extraction and segmentation. In ICML.
G. A. Miller. 1995. WordNet: A lexical database. Com-
munications of the ACM, 38(11).
NIST. 2007. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley Sons.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318.
P.F.Brown, S.A.Della Pietra, V.J. Della Pietra, and
R.L.Mercer. 1991. Word-sense disambiguation using
statistical methods. In Proceedings of ACL?91.
L. Ramshaw and M. Marcus. 1994. Exploring the sta-
tistical derivation of transformational rule sequences
for part-of-speech tagging. In Proceedings of the ACL
Workshop on Combining Symbolic and Statistical Ap-
proaches to Language, pages 128?135.
L. Ramshaw and M. Marcus. 1995. Text chunking us-
ing transformation-based learning. In David Yarowsky
and Kenneth Church, editors, Proceedings of the Third
Workshop on Very Large Corpora, pages 82?94, Som-
erset, New Jersey. Association for Computational Lin-
guistics.
E. Riloff, C. Schafer, and D. Yarowsky. 2002. Inducing
information extraction systems for new languages via
cross-language projection. In Proceedings of Coling
2002, Taipei, Taiwan.
E. F. Tjong Kim Sang and J. Veenstra. 1999. Represent-
ing text chunks. In Proceedings of EACL?99.
E. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independentnamed entity
recognition. In Proceedings of CoNLL-2002, pages
155?158.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In-
ducing multilingual text analysis tools via robust pro-
jection across aligned corpora. In Proceedings of HLT
2001, San Diego, California, USA.
Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, and Radu
Florian. 2005. The impact of morphological stem-
ming on Arabic mention detection and coreference res-
olution. In Proceedings of the ACL Workshop on Com-
putational Approaches to Semitic Languages, pages
63?70, Ann Arbor, June.
609
TIPS: A Translingual Information Processing System
Y. Al-Onaizan, R. Florian, M. Franz, H. Hassan, Y. S. Lee, S. McCarley, K.
Papineni, S. Roukos, J. Sorensen, C. Tillmann, T. Ward, F. Xia
IBM T. J. Watson Research Center
Yorktown Heights
Abstract
Searching online information is
increasingly a daily activity for many
people. The multilinguality of online
content is also increasing (e.g. the
proportion of English web users, which
has been decreasing as a fraction the
increasing population of web users, dipped
below 50% in the summer of 2001). To
improve the ability of an English speaker
to search mutlilingual content, we built a
system that supports cross-lingual search
of an Arabic newswire collection and
provides on demand translation of Arabic
web pages into English. The cross-lingual
search engine supports a fast search
capability (sub-second response for typical
queries) and achieves state-of-the-art
performance in the high precision region
of the result list. The on demand statistical
machine translation uses the Direct
Translation model along with a novel
statistical Arabic Morphological Analyzer
to yield state-of-the-art translation quality.
The on demand SMT uses an efficient
dynamic programming decoder that
achieves reasonable speed for translating
web documents.
Overview
Morphologically rich languages like Arabic
(Beesley, K. 1996) present significant challenges
to many natural language processing applications
as the one described above because a word often
conveys complex meanings decomposable into
several morphemes (i.e. prefix, stem, suffix). By
segmenting words into morphemes, we can
improve the performance of natural language
systems including machine translation (Brown et
al. 1993) and information retrieval (Franz, M.
and McCarley, S. 2002). In this paper, we
present a cross-lingual English-Arabic search
engine combined with an on demand Arabic-
English statistical machine translation system
that relies on source language analysis for both
improved search and translation. We developed
novel statistical learning algorithms for
performing Arabic word segmentation (Lee, Y.
et al2003) into morphemes and morphological
source language (Arabic) analysis (Lee, Y. et al
2003b). These components improve both mono-
lingual (Arabic) search and cross-lingual
(English-Arabic) search and machine
translation. In addition, the system supports
either document translation or convolutional
models for cross-lingual search (Franz, M. and
McCarley, S. 2002).
The overall demonstration has the following
major components:
1. Mono-lingual search: uses Arabic word
segmentation and an okapi-like search
engine for document ranking.
2. Cross-lingual search: uses Arabic word
segmentation and morphological
analysis along with a statistical
morpheme translation matrix in a
convolutional model for document
ranking. The search can also use
document translation into English to
rank the Arabic documents. Both
approaches achieve similar precision in
the high precision region of retrieval.
The English query is also
morphologically analyzed to improve
performance.
3. OnDemand statistical machine
translation: this component uses both
analysis components along with a direct
channel translation model with a fast
dynamic programming decoder
(Tillmann, C. 2003). This system
                                                               Edmonton, May-June 2003
                                                              Demonstrations , pp. 1-2
                                                         Proceedings of HLT-NAACL 2003
achieves state-of-the-art Arabic-English
translation quality.
4. Arabic named entity detection and
translation: we have 31 categories of
Named Entities (Person, Organization,
etc.) that we detect and highlight in
Arabic text and provide the translation
of these entities into English. The
highlighted named entities help the user
to quickly assess the relevance of a
document.
All of the above functionality is available
through a web browser. We indexed the Arabic
AFP corpus about 330k documents for the
demonstration. The resulting search engine
supports sub-second query response. We also
provide an html detagging capability that allows
the translation of Arabic web pages while trying
to preserve the original layout as much as
possible in the on demand SMT component. The
Arabic Name Entity Tagger is currently run as an
offline process but we expect to have it online by
the demonstration time. We aslo include two
screen shots of the demonstration system.
Acknowledgments
This work was partially supported by the
Defense Advanced Research Projects Agency
and monitored by SPAWAR under contract No.
N66001-99-2-8916. The views and findings
contained in this material are those of the authors
and do not necessarily reflect the position of
policy of the Government and no official
endorsement should be inferred.
References
Beesley, K. 1996. Arabic Finite-State
Morphological Analysis and Generation.
Proceedings of COLING-96, pages 89? 94.
Brown, P., Della Pietra, S., Della Pietra, V., and
Mercer, R. 1993. The mathematics of statistical
machine translation: Parameter Estimation.
Computational Linguistics, 19(2): 263?311.
Franz, M. and McCarley, S. 2002. Arabic
Information Retrieval at IBM. Proceedings
of TREC 2002, pages 402?405.
Lee, Y., Papineni, K., Roukos, S.,
Emam, O., and Hassan, H. 2003. Language
Model Based Arabic Word Segmentation.
Submitted for publication.
Lee, Y., Papineni, K., Roukos, S., Emam,
O., and Hassan, H. 2003b. Automatic
Induction of Morphological Analysis for
Statistical Machine Translation. Manuscript in
preparation.
Tillmann, C., 2003. Word Reordering and a
DP Beam Search Algorithm for Statistical
Machine Translation. Computational
Linguistics, 29(1): 97-133.
A Statistical Model for Multilingual Entity Detection and Tracking
R. Florian, H. Hassan   , A. Ittycheriah, H. Jing
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos
I.B.M. T.J. Watson Research Center
Yorktown Heights, NY 10598
{raduf,abei,hjing,nanda,xiaoluo, nicolas,roukos}@us.ibm.com

hanyh@eg.ibm.com
Abstract
Entity detection and tracking is a relatively new
addition to the repertoire of natural language
tasks. In this paper, we present a statistical
language-independent framework for identify-
ing and tracking named, nominal and pronom-
inal references to entities within unrestricted
text documents, and chaining them into clusters
corresponding to each logical entity present in
the text. Both the mention detection model
and the novel entity tracking model can use
arbitrary feature types, being able to integrate
a wide array of lexical, syntactic and seman-
tic features. In addition, the mention detec-
tion model crucially uses feature streams de-
rived from different named entity classifiers.
The proposed framework is evaluated with sev-
eral experiments run in Arabic, Chinese and
English texts; a system based on the approach
described here and submitted to the latest Au-
tomatic Content Extraction (ACE) evaluation
achieved top-tier results in all three evaluation
languages.
1 Introduction
Detecting entities, whether named, nominal or pronom-
inal, in unrestricted text is a crucial step toward under-
standing the text, as it identifies the important concep-
tual objects in a discourse. It is also a necessary step for
identifying the relations present in the text and populating
a knowledge database. This task has applications in in-
formation extraction and summarization, information re-
trieval (one can get al hits for Washington/person and not
the ones for Washington/state or Washington/city), data
mining and question answering.
The Entity Detection and Tracking task (EDT hence-
forth) has close ties to the named entity recognition
(NER) and coreference resolution tasks, which have been
the focus of attention of much investigation in the recent
past (Bikel et al, 1997; Borthwick et al, 1998; Mikheev
et al, 1999; Miller et al, 1998; Aberdeen et al, 1995;
Ng and Cardie, 2002; Soon et al, 2001), and have been
at the center of several evaluations: MUC-6, MUC-7,
CoNLL?02 and CoNLL?03 shared tasks. Usually, in com-
putational linguistic literature, a named entity represents
an instance of a name, either a location, a person, an or-
ganization, and the NER task consists of identifying each
individual occurrence of such an entity. We will instead
adopt the nomenclature of the Automatic Content Extrac-
tion program1 (NIST, 2003a): we will call the instances
of textual references to objects or abstractions mentions,
which can be either named (e.g. John Mayor), nominal
(e.g. the president) or pronominal (e.g. she, it). An entity
consists of all the mentions (of any level) which refer to
one conceptual entity. For instance, in the sentence
President John Smith said he has no comments.
there are two mentions: John Smith and he (in the order
of appearance, their levels are named and pronominal),
but one entity, formed by the set {John Smith, he}.
In this paper, we present a general statistical frame-
work for entity detection and tracking in unrestricted text.
The framework is not language specific, as proved by ap-
plying it to three radically different languages: Arabic,
Chinese and English. We separate the EDT task into a
mention detection part ? the task of finding all mentions
in the text ? and an entity tracking part ? the task of com-
bining the detected mentions into groups of references to
the same object.
The work presented here is motivated by the ACE eval-
uation framework, which has the more general goal of
building multilingual systems which detect not only enti-
ties, but also relations among them and, more recently,
events in which they participate. The EDT task is ar-
guably harder than traditional named entity recognition,
because of the additional complexity involved in extract-
ing non-named mentions (nominals and pronouns) and
the requirement of grouping mentions into entities.
We present and evaluate empirically statistical mod-
els for both mention detection and entity tracking prob-
lems. For mention detection we use approaches based on
Maximum Entropy (MaxEnt henceforth) (Berger et al,
1996) and Robust Risk Minimization (RRM henceforth)
1For a description of the ACE program see
http://www.nist.gov/speech/tests/ace/.
(Zhang et al, 2002). The task is transformed into a se-
quence classification problem. We investigate a wide ar-
ray of lexical, syntactic and semantic features to perform
the mention detection and classification task including,
for all three languages, features based on pre-existing sta-
tistical semantic taggers, even though these taggers have
been trained on different corpora and use different seman-
tic categories. Moreover, the presented approach implic-
itly learns the correlation between these different seman-
tic types and the desired output types.
We propose a novel MaxEnt-based model for predict-
ing whether a mention should or should not be linked to
an existing entity, and show how this model can be used
to build entity chains. The effectiveness of the approach
is tested by applying it on data from the above mentioned
languages ? Arabic, Chinese, English.
The framework presented in this paper is language-
universal ? the classification method does not make any
assumption about the type of input. Most of the fea-
ture types are shared across the languages, but there are a
small number of useful feature types which are language-
specific, especially for the mention detection task.
The paper is organized as follows: Section 2 describes
the algorithms and feature types used for mention detec-
tion. Section 3 presents our approach to entity tracking.
Section 4 describes the experimental framework and the
systems? results for Arabic, Chinese and English on the
data from the latest ACE evaluation (September 2003), an
investigation of the effect of using different feature types,
as well as a discussion of the results.
2 Mention Detection
The mention detection system identifies the named, nom-
inal and pronominal mentions introduced in the previous
section. Similarly to classical NLP tasks such as base
noun phrase chunking (Ramshaw and Marcus, 1994), text
chunking (Ramshaw and Marcus, 1995) or named entity
recognition (Tjong Kim Sang, 2002), we formulate the
mention detection problem as a classification problem,
by assigning to each token in the text a label, indicating
whether it starts a specific mention, is inside a specific
mention, or is outside any mentions.
2.1 The Statistical Classifiers
Good performance in many natural language process-
ing tasks, such as part-of-speech tagging, shallow pars-
ing and named entity recognition, has been shown to de-
pend heavily on integrating many sources of information
(Zhang et al, 2002; Jing et al, 2003; Ittycheriah et al,
2003). Given the stated focus of integrating many feature
types, we are interested in algorithms that can easily in-
tegrate and make effective use of diverse input types. We
selected two methods which satisfy these criteria: a linear
classifier ? the Robust Risk Minimization classifier ? and
a log-linear classifier ? the Maximum Entropy classifier.
Both methods can integrate arbitrary types of informa-
tion and make a classification decision by aggregating all
information available for a given classification.
Before formally describing the methods2, we introduce
some notations: let
 	



be the set of pre-
dicted classes,  be the example space and 

be the feature space. Each example Proceedings of NAACL HLT 2009: Short Papers, pages 201?204,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving Coreference Resolution by Using Conversational Metadata
Xiaoqiang Luo and Radu Florian and Todd Ward
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
{xiaoluo,raduf,toddward}@us.ibm.com
Abstract
In this paper, we propose the use of metadata
contained in documents to improve corefer-
ence resolution. Specifically, we quantify the
impact of speaker and turn information on the
performance of our coreference system, and
show that the metadata can be effectively en-
coded as features of a statistical resolution sys-
tem, which leads to a statistically significant
improvement in performance.
1 Introduction
Coreference resolution aims to find the set of lin-
guistic expressions that refer to a common entity. It
is a discourse-level task given that the ambiguity of
many referential relationships among linguistic ex-
pressions can only be correctly resolved by examin-
ing information extracted from the entire document.
In this paper, we focus on exploiting the struc-
tural information (e.g., speaker and turn in conversa-
tional documents) represented in the metadata of an
input document. Such metadata often coincides with
the discourse structure, and is presumably useful to
coreference resolution. The goal of this study is to
quantify the effect metadata. To this end, informa-
tion contained in metadata is encoded as features in
our coreference resolution system, and statistically
significant improvement is observed.
The rest of the paper is organized as follows.
In Section 2 we describe the data set on which
this study is based. In Section 3 we first show
how to incorporate information carried by metadata
into a statistical coreference resolution system. We
also quantify the impact of metadata when they are
treated as extraneous data. Results and discussions
of the results are also presented in that section.
2 Data Set
This study uses the 2007 ACE data. In the ACE
program, a mention is textual reference to an
object of interest while the set of mentions in a
document referring to the same object is called
entity. Each mention is of one of 7 entity
types: FAC(cility), GPE (Geo-Political Entity),
LOC(ation), ORG(anization), PER(son), VEH(icle),
and WEA(pon). Every entity type has a prede-
fined set of subtypes. For example, ORG sub-
types include commercial,governmental and
educational etc, which reflect different sub-
groups of organizations. Mentions referring to the
same entity share the same type and subtype. A
mention can also be assigned with one of 3 men-
tion types: either NAM(e), NOM(inal), or PRO(noun).
Accordingly, entities have ?levels:? if an entity con-
tains at least one NAM mention, its level is NAM; or
if it does not contain any NAM mention, but contains
at least one NOM mention, then the entity is of level
NOM; if an entity has only PRO mention(s), then its
level is PRO. More information about ACE entity
annotation can be found in the official annotation
guideline (Linguistic Data Consortium, 2008).
The ACE 2007 documents come from a variety of
sources, namely newswire, broadcast conversation,
broadcast news, Usenet, web log and telephone con-
versation. Some of them contain rich metadata, as
illustrated in the following excerpt of one broadcast
conversation document:
<DOC>
<DOCID>CNN_CF_20030303.1900.00</DOCID>
<TEXT>
<TURN>
<SPEAKER> Begala </SPEAKER>
Well, we?ll debate that later on in the
show. We?ll have a couple of experts
come out, ...
201
</TURN>
<TURN>
<SPEAKER> Novak </SPEAKER>
Paul, as I understand your definition
of a political -- of a professional
politician based on that is somebody
who is elected to public office. ...
</TURN>
...
</TEXT>
</DOC>
In this example, SPEAKER and TURN informa-
tion are marked by their corresponding SGML tags.
Such metadata provides structural information: for
instance, the metadata implies that Begala is the
speaker of the utterance ?Well, we?ll debate ..., ?
and Novak the speaker of the utterance ?Paul, as
I understand your definition ...? Intuitively, knowing
the speakers of the previous and current turn would
make it a lot easier to find the right antecedent of
pronominal mentions I and your in the sentence:
?Paul, as I understand your definition ...?
Documents in non-conversational genres (e.g.
newswire documents) also contain speaker and quo-
tation, which resemble conversational utterance, but
they are not annotated. For these documents, we
use heuristics (e.g., existence of double or single
quote, a short list of communication verb lemmas
such as ?say,? ?tell? and ?speak? etc) to determine
the speaker of a direct quotation if necessary.
3 Impact of Metadata
In this section we describe how metadata is used to
improve our statistical coreference resolution sys-
tem.
3.1 Resolution System
The coreference system used in our study is a data-
driven, machine-learning-based system. Mentions
in a document are processed sequentially by men-
tion type: NAM mentions are processed first, fol-
lowed by NOM mentions and then PRO mentions.
The first mention is used to create an initial entity
with a deterministic score 1. The second mention
can be either linked to the first entity, or used to cre-
ate a new entity, and the two actions are assigned a
score computed from a log linear model. This pro-
cess is repeated until all mentions in a document are
processed. During training time, the process is ap-
plied to the training data and training instances (both
positive and negative) are generated. At testing time,
the same process is applied to an input document
and the hypothesis with the highest score is selected
as the final coreference result. At the core of the
coreference system is a conditional log linear model
P (l|e,m) which measures how likely a mention m
is or is not coreferential with an existing entity e.
The modeling framework provides us with the flexi-
bility to integrate metadata information by encoding
it as features.
The coreference resolution system employs a va-
riety of lexical, semantic, distance and syntactic
features(Luo et al, 2004; Luo and Zitouni, 2005).
The full-blown system achieves an 56.2% ACE-
value score on the official 2007 ACE test data,
which is about the same as the best-performing sys-
tem in the Entity Detection and Recognition (EDR)
task (NIST, 2007). So we believe that the resolution
system is fairly solid.
The aforementioned 56.2% score includes men-
tion detection (i.e., finding mention boundaries and
predicting mention attributes) and coreference res-
olution. Since this study is about coreference res-
olution only, the subsequent experiments, are thus
performed on gold-standard mentions. We split the
ACE 2007 data into a training set consisting of 499
documents, and a test set of 100 documents. The
training and test split ratio is roughly the same across
genres. The performance numbers reported in the
subsequent subsections are on the 100-document de-
velopment test set.
3.2 Metadata Features
For conversational documents with speaker and turn
information, we compute a group of binary features
for a candidate referent r and the current mention
m. Feature values are 1 if the conditions described
below hold:
? if r is a speaker, m is a pronominal mention and
r utters the sentence containing m.
? if r is a speaker, m is pronoun and r utters the
sentence one turn before the one containing m.
? if mention r and mention m are seen in the
same turn.
? if mention r and mention m are in two consec-
utive turns.
Note that the first feature is not subsumed by the
third one since a turn may contain multiple sen-
tences. For the same reason, the last feature does not
subsume the second one. For the sample document
in Section 2, the first feature fires if r = Novak and
m = I; the second features fires if r = Begala
202
and m = I; the third feature fires if r = Paul
and m = I; and lastly, the fourth feature fires if
r = We and m = I. For ACE documents that
do not carry turn and speaker information such as
newswire, we use heuristic rules to empirically de-
termine the speaker and the corresponding quota-
tions before computing these features.
To test the effect of the feature group, we trained
two models: a baseline system without speaker and
turn features, and a contrast system by adding the
speaker and turn features to the baseline system. The
contrast results are tabulated in Table 1. We observe
an overall 0.7 point ACE-value improvement. We
also compute the ACE-values at document level for
the two systems, and a paired Wilcoxon (Wilcoxon,
1945) rank-sum test is conducted, which indicates
that the difference between the two systems is statis-
tically significant at level p ? 0.002.
Note that the features often help link pronouns
with their antecedents in conversational documents.
But ACE-value is a weighted metric which heav-
ily discounts pronominal mentions and entities. We
suspect that the effect of speaker and turn informa-
tion could be larger if we weigh all mention types
equally. This is confirmed when we looked at the un-
weighted B3 (Bagga and Baldwin, 1998) numbers
reported by the official ACE08 scorer (column B3
in Table 1): the overall B3 score is improved from
73.8% to 76.4% ? a 2.6 point improvement, which
is almost 4 times as large as the ACE-value change.
System ACE-Value B3
baseline 78.7 73.8
+ Spkr/Turn 79.4 76.4
Table 1: Coreference performance: baseline vs. system
with speaker and turn features.
3.3 Metadata: To Use Or Not to Use?
In the ACE evaluations prior to 2008, mentions in-
side metadata (such as speaker and poster) are anno-
tated and scored as normal mentions, although such
metadata is not part of the actual content of a doc-
ument. An interesting question is: how large an ef-
fect do mentions inside metadata have on the system
performance? If metadata are not annotated as men-
tions, is it still useful to look into them? To answer
this question, we remove speaker mentions in con-
versational documents (i.e., broadcast conversation
and telephone conversation) from both the training
and test data. Then we train two systems:
? System A: the system totally disregards meta-
data.
? System B: the system first recovers speaker
metadata using a very simple rule: all to-
kens within the <SPEAKER> tags are treated
as one PER mention. This rule recovers most
speaker mentions, but it can occasionally re-
sult in errors. For instance, the speaker ?CNN
correspondent John Smith? includes affilia-
tion and profession information and ought to
be tagged as three mentions: ?CNN? as an
ORG(anization) mention, ?correspondent? and
?John Smith? as two PER mentions. With re-
covered speaker mentions, we train a model
and resolve coreference as normal.
After mentions in the test data are chained in Sys-
tem B, speaker mentions are then removed from sys-
tem output so that the coreference result is directly
comparable with that of System A.
The ACE-value comparison between System A
and System B is shown in Table 2. As can be
seen, System B works much better than System A,
which ignores SPEAKER tags. For telephone con-
versations (cts), ACE-value improves as much as 4.6
points. A paired Wilcoxon test on document-level
ACE-values indicates that the difference is statisti-
cally significant at p < 0.016.
System bc cts
A 75.2 66.8
B 76.6 71.4
Abs. Change 1.4 4.6
Table 2: Metadata improves the ACE-value for broadcast
conversation (bc) and telephone conversation (cts) docu-
ments.
The reason why metadata helps is that speaker
mention can be used to localize the coreference pro-
cess and therefore improves the performance. For
example, in the sentences uttered by ?Novak? (cf.
the sample document in Section 2), it is intuitively
straightforward to link mention I with Novak, and
your with Begala ? when speaker mentions are
made present in the coreference system B. On the
other hand, in System A, ?I? is likely to be linked
with ?Paul? because of its proximity of ?Paul? in the
absence of speaker information.
The result of this experiment suggests that, unsur-
prisingly, speaker and turn metadata carry structural
203
information helpful for coreference resolution. Even
if speaker mentions are not annotated (as in System
A), it is still beneficial to make use of it, e.g., by first
identifying them automatically as in System B.
4 Related Work
There is a large body of literature for coreference
resolution based on machine learning (Kehler, 1997;
Soon et al, 2001; Ng and Cardie, 2002; Yang et al,
2008; Luo et al, 2004) approach. Strube and Muller
(2003) presented a machine-learning based pronoun
resolution system for spoken dialogue (Switchboard
corpus). The document genre in their study is simi-
lar to the ACE telephony conversation documents,
and they did include some dialogue-specific fea-
tures, such as an anaphora?s preference for S, VP
or NP, in their system, but they did not use speaker
or turn information. Gupta et al (2007) presents
an algorithm disambiguating generic and referential
?you.?
Cristea et al (1999) attempted to improve coref-
erence resolution by first analyzing the discourse
structure of a document with rhetoric structure the-
ory (RST) (Mann and Thompson, 1987) and then
using the resulted discourse structure in coreference
resolution. Since obtaining reliably the discourse
structure itself is a challenge, they got mixed results
compared with a linear structure baseline.
Our work presented in this paper concentrates on
the structural information represented in metadata,
such as turn or speaker information. Such metadata
provides reliable discourse structure, especially for
conversational documents, which is proven benefi-
cial for enhancing the performance of our corefer-
ence resolution system.
Acknowledgments
This work is partially supported by DARPA GALE
program under the contract number HR0011-06-02-
0001. We?d also like to thank 3 reviewers for their
helpful comments.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at The First Interna-
tional Conference on Language Resources and Eval-
uation (LREC?98), pages 563?566.
Dan Cristea, Nancy lde, Daniel Marcu, Valentin Tablan-
livia Polanyi, and Martin van den Berg. 1999. Dis-
course structure and co-reference: An empirical study.
In Proceedings of ACL Workshop ?The Relation of
Discourse/Dialogue Structure and Reference?. Asso-
ciation for Computational Linguistics.
Surabhi Gupta, Matthew Purver, and Dan Jurafsky. 2007.
Disambiguating between generic and referential ?you?
in dialog. In Proceedings of the 45th ACL(the Demo
and Poster Sessions), pages 105?108, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Andrew Kehler. 1997. Probabilistic coreference in infor-
mation extraction. In Proc. of EMNLP.
Linguistic Data Consortium. 2008. ACE (Automatic
Content Extraction) English annotation guidelines
for entities. http://projects.ldc.upenn.edu/ace/docs/
English-Entities-Guidelines v6.5.pdf.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
lingual coreference resolution with syntactic fea-
tures. In Proc. of Human Language Technology
(HLT)/Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proc. of ACL.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical structure theory: A theory of text organiza-
tion. Technical Report RS-87-190, USC/Information
Sciences Institute.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proc. of ACL, pages 104?111.
NIST. 2007. 2007 automatic con-
tent extraction evaluation official results.
http://www.nist.gov/speech/tests/ace/2007/doc/
ace07 eval official results 20070402.html.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Michael Strube and Christoph Muller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics.
Frank Wilcoxon. 1945. Individual comparisons by rank-
ing methods. Biometrics, I:80?83.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In Proceedings of ACL-08: HLT, pages 843?
851, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
204
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 473?480,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Factorizing Complex Models: A Case Study in Mention
Detection
Radu Florian, Hongyan Jing, Nanda Kambhatla and Imed Zitouni
IBM TJ Watson Research Center
Yorktown Heights, NY 10598
{raduf,hjing,nanda,izitouni}@us.ibm.com
Abstract
As natural language understanding re-
search advances towards deeper knowledge
modeling, the tasks become more and more
complex: we are interested in more nu-
anced word characteristics, more linguistic
properties, deeper semantic and syntactic
features. One such example, explored in
this article, is the mention detection and
recognition task in the Automatic Content
Extraction project, with the goal of iden-
tifying named, nominal or pronominal ref-
erences to real-world entities?mentions?
and labeling them with three types of in-
formation: entity type, entity subtype and
mention type. In this article, we investi-
gate three methods of assigning these re-
lated tags and compare them on several
data sets. A system based on the methods
presented in this article participated and
ranked very competitively in the ACE?04
evaluation.
1 Introduction
Information extraction is a crucial step toward un-
derstanding and processing natural language data,
its goal being to identify and categorize impor-
tant information conveyed in a discourse. Exam-
ples of information extraction tasks are identifi-
cation of the actors and the objects in written
text, the detection and classification of the rela-
tions among them, and the events they participate
in. These tasks have applications in, among other
fields, summarization, information retrieval, data
mining, question answering, and language under-
standing.
One of the basic tasks of information extraction
is the mention detection task. This task is very
similar to named entity recognition (NER), as the
objects of interest represent very similar concepts.
The main difference is that the latter will identify,
however, only named references, while mention de-
tection seeks named, nominal and pronominal ref-
erences. In this paper, we will call the identified
references mentions ? using the ACE (NIST, 2003)
nomenclature ? to differentiate them from entities
which are the real-world objects (the actual person,
location, etc) to which the mentions are referring
to1.
Historically, the goal of the NER task was to find
named references to entities and quantity refer-
ences ? time, money (MUC-6, 1995; MUC-7, 1997).
In recent years, Automatic Content Extraction
evaluation (NIST, 2003; NIST, 2004) expanded the
task to also identify nominal and pronominal refer-
ences, and to group the mentions into sets referring
to the same entity, making the task more compli-
cated, as it requires a co-reference module. The set
of identified properties has also been extended to
include the mention type of a reference (whether it
is named, nominal or pronominal), its subtype (a
more specific type dependent on the main entity
type), and its genericity (whether the entity points
to a specific entity, or a generic one2), besides the
customary main entity type. To our knowledge,
little research has been done in the natural lan-
guage processing context or otherwise on investi-
gating the specific problem of how such multiple la-
bels are best assigned. This article compares three
methods for such an assignment.
The simplest model which can be considered for
the task is to create an atomic tag by ?gluing? to-
gether the sub-task labels and considering the new
label atomic. This method transforms the prob-
lem into a regular sequence classification task, sim-
ilar to part-of-speech tagging, text chunking, and
named entity recognition tasks. We call this model
the all-in-one model. The immediate drawback
of this model is that it creates a large classifica-
tion space (the cross-product of the sub-task clas-
sification spaces) and that, during decoding, par-
tially similar classifications will compete instead of
cooperate - more details are presented in Section
3.1. Despite (or maybe due to) its relative sim-
plicity, this model obtained good results in several
instances in the past, for POS tagging in morpho-
logically rich languages (Hajic and Hladka?, 1998)
1In a pragmatic sense, entities are sets of mentions
which co-refer.
2This last attribute, genericity, depends only loosely
on local context. As such, it should be assigned while
examining all mentions in an entity, and for this reason
is beyond the scope of this article.
473
and mention detection (Jing et al, 2003; Florian
et al, 2004).
At the opposite end of classification methodol-
ogy space, one can use a cascade model, which per-
forms the sub-tasks sequentially in a predefined or-
der. Under such a model, described in Section 3.3,
the user will build separate models for each sub-
task. For instance, it could first identify the men-
tion boundaries, then assign the entity type, sub-
type, and mention level information. Such a model
has the immediate advantage of having smaller
classification spaces, with the drawback that it re-
quires a specific model invocation path.
In between the two extremes, one can use a joint
model, which models the classification space in the
same way as the all-in-one model, but where the
classifications are not atomic. This system incor-
porates information about sub-model parts, such
as whether the current word starts an entity (of
any type), or whether the word is part of a nomi-
nal mention.
The paper presents a novel contrastive analysis
of these three models, comparing them on several
datasets in three languages selected from the ACE
2003 and 2004 evaluations. The methods described
here are independent of the underlying classifiers,
and can be used with any sequence classifiers. All
experiments in this article use our in-house imple-
mentation of a maximum entropy classifier (Flo-
rian et al, 2004), which we selected because of its
flexibility of integrating arbitrary types of features.
While we agree that the particular choice of classi-
fier will undoubtedly introduce some classifier bias,
we want to point out that the described procedures
have more to do with the organization of the search
space, and will have an impact, one way or another,
on most sequence classifiers, including conditional
random field classifiers.3
The paper is organized as follows: Section 2 de-
scribes the multi-task classification problem and
prior work, Section 3.3 presents and contrasts the
three meta-classification models. Section 4 outlines
the experimental setup and the obtained results,
and Section 5 concludes the paper.
2 Multi-Task Classification
Many tasks in Natural Language Processing in-
volve labeling a word or sequence of words with
a specific property; classic examples are part-of-
speech tagging, text chunking, word sense disam-
biguation and sentiment classification. Most of the
time, the word labels are atomic labels, containing
a very specific piece of information (e.g. the word
3While not wishing to delve too deep into the issue
of label bias, we would also like to point out (as it
was done, for instance, in (Klein, 2003)) that the label
bias of MEMM classifiers can be significantly reduced
by allowing them to examine the right context of the
classification point - as we have done with our model.
is noun plural, or starts a noun phrase, etc). There
are cases, though, where the labels consist of sev-
eral related, but not entirely correlated, properties;
examples include mention detection?the task we
are interested in?, syntactic parsing with func-
tional tag assignment (besides identifying the syn-
tactic parse, also label the constituent nodes with
their functional category, as defined in the Penn
Treebank (Marcus et al, 1993)), and, to a lesser
extent, part-of-speech tagging in highly inflected
languages.4
The particular type of mention detection that we
are examining in this paper follows the ACE gen-
eral definition: each mention in the text (a refer-
ence to a real-world entity) is assigned three types
of information:5
? An entity type, describing the type of the en-
tity it points to (e.g. person, location, organi-
zation, etc)
? An entity subtype, further detailing the type
(e.g. organizations can be commercial, gov-
ernmental and non-profit, while locations can
be a nation, population center, or an interna-
tional region)
? A mention type, specifying the way the en-
tity is realized ? a mention can be named
(e.g. John Smith), nominal (e.g. professor),
or pronominal (e.g. she).
Such a problem ? where the classification consists
of several subtasks or attributes ? presents addi-
tional challenges, when compared to a standard
sequence classification task. Specifically, there are
inter-dependencies between the subtasks that need
to be modeled explicitly; predicting the tags inde-
pendently of each other will likely result in incon-
sistent classifications. For instance, in our running
example of mention detection, the subtype task is
dependent on the entity type; one could not have a
person with the subtype non-profit. On the other
hand, the mention type is relatively independent of
the entity type and/or subtype: each entity type
could be realized under any mention type and vice-
versa.
The multi-task classification problem has been
subject to investigation in the past. Caruana
et al (1997) analyzed the multi-task learning
4The goal there is to also identify word properties
such as gender, number, and case (for nouns), mood
and tense (for verbs), etc, besides the main POS tag.
The task is slightly different, though, as these proper-
ties tend to have a stronger dependency on the lexical
form of the classified word.
5There is a fourth assigned type ? a flag specifying
whether a mention is specific (i.e. it refers at a clear
entity), generic (refers to a generic type, e.g. ?the sci-
entists believe ..?), unspecified (cannot be determined
from the text), or negative (e.g. ?no person would do
this?). The classification of this type is beyond the
goal of this paper.
474
(MTL) paradigm, where individual related tasks
are trained together by sharing a common rep-
resentation of knowledge, and demonstrated that
this strategy yields better results than one-task-at-
a-time learning strategy. The authors used a back-
propagation neural network, and the paradigm was
tested on several machine learning tasks. It also
contains an excellent discussion on how and why
the MTL paradigm is superior to single-task learn-
ing. Florian and Ngai (2001) used the same multi-
task learning strategy with a transformation-based
learner to show that usually disjointly handled
tasks perform slightly better under a joint model;
the experiments there were run on POS tagging
and text chunking, Chinese word segmentation and
POS tagging. Sutton et al (2004) investigated
the multitask classification problem and used a dy-
namic conditional random fields method, a gener-
alization of linear-chain conditional random fields,
which can be viewed as a probabilistic generaliza-
tion of cascaded, weighted finite-state transducers.
The subtasks were represented in a single graphi-
cal model that explicitly modeled the sub-task de-
pendence and the uncertainty between them. The
system, evaluated on POS tagging and base-noun
phrase segmentation, improved on the sequential
learning strategy.
In a similar spirit to the approach presented in
this article, Florian (2002) considers the task of
named entity recognition as a two-step process:
the first is the identification of mention boundaries
and the second is the classification of the identified
chunks, therefore considering a label for each word
being formed from two sub-labels: one that spec-
ifies the position of the current word relative in a
mention (outside any mentions, starts a mention, is
inside a mention) and a label specifying the men-
tion type . Experiments on the CoNLL?02 data
show that the two-process model yields consider-
ably higher performance.
Hacioglu et al (2005) explore the same task, in-
vestigating the performance of the AIO and the
cascade model, and find that the two models have
similar performance, with the AIO model having a
slight advantage. We expand their study by adding
the hybrid joint model to the mix, and further in-
vestigate different scenarios, showing that the cas-
cade model leads to superior performance most of
the time, with a few ties, and show that the cas-
cade model is especially beneficial in cases where
partially-labeled data (only some of the component
labels are given) is available. It turns out though,
(Hacioglu, 2005) that the cascade model in (Ha-
cioglu et al, 2005) did not change to a ?mention
view? sequence classification6 (as we did in Section
3.3) in the tasks following the entity detection, to
allow the system to use longer range features.
6As opposed to a ?word view?.
3 Classification Models
This section presents the three multi-task classifi-
cation models, which we will experimentally con-
trast in Section 4. We are interested in performing
sequence classification (e.g. assigning a label to
each word in a sentence, otherwise known as tag-
ging). Let X denote the space of sequence elements
(words) and Y denote the space of classifications
(labels), both of them being finite spaces. Our goal
is to build a classifier
h : X+ ? Y+
which has the property that |h (x?)| = |x?| ,?x? ? X+
(i.e. the size of the input sequence is preserved).
This classifier will select the a posteriori most likely
label sequence y? = argmaxy?? p
(y??|x?); in our case
p (y?|x?) is computed through the standard Markov
assumption:
p (y1,m| x?) =
?
i
p (yi|x?, yi?n+1,i?1) (1)
where yi,j denotes the sequence of labels yi..yj .
Furthermore, we will assume that each label y
is composed of a number of sub-labels y =(y1y2 . . . yk)7; in other words, we will assume the
factorization of the label space into k subspaces
Y = Y1 ? Y2 ? . . .? Yk.
The classifier we used in the experimental sec-
tion is a maximum entropy classifier (similar to
(McCallum et al, 2000))?which can integrate sev-
eral sources of information in a rigorous manner.
It is our empirical observation that, from a perfor-
mance point of view, being able to use a diverse
and abundant feature set is more important than
classifier choice, and the maximum entropy frame-
work provides such a utility.
3.1 The All-In-One Model
As the simplest model among those presented here,
the all-in-one model ignores the natural factoriza-
tion of the output space and considers all labels as
atomic, and then performs regular sequence clas-
sification. One way to look at this process is the
following: the classification space Y = Y1 ? Y2 ?
. . . ? Yk is first mapped onto a same-dimensional
space Z through a one-to-one mapping o : Y ? Z;
then the features of the system are defined on the
space X+ ?Z, instead of X+ ? Y.
While having the advantage of being simple, it
suffers from some theoretical disadvantages:
? The classification space can be very large, be-
ing the product of the dimensions of sub-task
spaces. In the case of the 2004 ACE data
there are 7 entity types, 4 mention types and
many subtypes; the observed number of actual
7We can assume, without any loss of generality, that
all labels have the same number of sub-labels.
475
All-In-One Model Joint Model
B-PER
B-LOC
B-ORG B-
B-MISC
Table 1: Features predicting start of an entity in
the all-in-one and joint models
sub-label combinations on the training data is
401. Since the dynamic programing (Viterbi)
search?s runtime dependency on the classifica-
tion space is O (|Z|n) (n is the Markov depen-
dency size), using larger spaces will negatively
impact the decoding run time.8
? The probabilities p (zi|x?, zi?n,i?1) require
large data sets to be computed properly. If
the training data is limited, the probabilities
might be poorly estimated.
? The model is not friendly to partial evaluation
or weighted sub-task evaluation: different, but
partially similar, labels will compete against
each other (because the system will return a
probability distribution over the classification
space), sometimes resulting in wrong partial
classification.9
? The model cannot directly use data that is
only partially labeled (i.e. not all sub-labels
are specified).
Despite the above disadvantages, this model has
performed well in practice: Hajic and Hladka?
(1998) applied it successfully to find POS se-
quences for Czech and Florian et al (2004) re-
ports good results on the 2003 ACE task. Most
systems that participated in the CoNLL 2002 and
2003 shared tasks on named entity recognition
(Tjong Kim Sang, 2002; Tjong Kim Sang and
De Meulder, 2003) applied this model, as they
modeled the identification of mention boundaries
and the assignment of mention type at the same
time.
3.2 The Joint Model
The joint model differs from the all-in-one model
in the fact that the labels are no longer atomic: the
features of the system can inspect the constituent
sub-labels. This change helps alleviate the data
8From a practical point of view, it might not be very
important, as the search is pruned in most cases to only
a few hypotheses (beam-search); in our case, pruning
the beam only introduced an insignificant model search
error (0.1 F-measure).
9To exemplify, consider that the system outputs the
following classifications and probabilities: O (0.2), B-
PER-NAM (0.15), B-PER-NOM (0.15); even the latter
2 suggest that the word is the start of a person mention,
the O label will win because the two labels competed
against each other.
Detect Boundaries   & Entity Types
Assemble full tag
Detect Entity Subtype Detect Mention Type
Figure 1: Cascade flow example for mention detec-
tion.
sparsity encountered by the previous model by al-
lowing sub-label modeling. The joint model the-
oretically compares favorably with the all-in-one
model:
? The probabilities p (yi|x?, yi?n,i?1) =
p
((y1i , . . . , yki
) |x?,
(
yji?n,i?1
)
j=1,k
)
might
require less training data to be properly
estimated, as different sub-labels can be
modeled separately.
? The joint model can use features that predict
just one or a subset of the sub-labels. Ta-
ble 1 presents the set of basic features that
predict the start of a mention for the CoNLL
shared tasks for the two models. While the
joint model can encode the start of a mention
in one feature, the all-in-one model needs to
use four features, resulting in fewer counts per
feature and, therefore, yielding less reliably es-
timated features (or, conversely, it needs more
data for the same estimation confidence).
? The model can predict some of the sub-tags
ahead of the others (i.e. create a dependency
structure on the sub-labels). The model used
in the experimental section predicts the sub-
labels by using only sub-labels for the previous
words, though.
? It is possible, though computationally expen-
sive, for the model to use additional data
that is only partially labeled, with the model
change presented later in Section 3.4.
3.3 The Cascade Model
For some tasks, there might already exist a natural
hierarchy among the sub-labels: some sub-labels
could benefit from knowing the value of other,
primitive, sub-labels. For example,
? For mention detection, identifying the men-
tion boundaries can be considered as a primi-
tive task. Then, knowing the mention bound-
aries, one can assign an entity type, subtype,
and mention type to each mention.
? In the case of parsing with functional tags, one
can perform syntactic parsing, then assign the
functional tags to the internal constituents.
476
Words Since Donna Karan International went public in 1996 ...
Labels O B-ORG I-ORG I-ORG O O O O ...
Figure 2: Sequence tagging for mention detection: the case for a cascade model.
? For POS tagging, one can detect the main
POS first, then detect the other specific prop-
erties, making use of the fact that one knows
the main tag.
The cascade model is essentially a factorization
of individual classifiers for the sub-tasks; in this
framework, we will assume that there is a more
or less natural dependency structure among sub-
tasks, and that models for each of the subtasks
will be built and applied in the order defined by
the dependency structure. For example, as shown
in Figure 1, one can detect mention boundaries and
entity type (at the same time), then detect mention
type and subtype in ?parallel? (i.e. no dependency
exists between these last 2 sub-tags).
A very important advantage of the cascade
model is apparent in classification cases where
identifying chunks is involved (as is the case with
mention detection), similar to advantages that
rescoring hypotheses models have: in the second
stage, the chunk classification stage, it can switch
to a mention view, where the classification units
are entire mentions and words outside of mentions.
This allows the system to make use of aggregate
features over the mention words (e.g. all the words
are capitalized), and to also effectively use a larger
Markov window (instead of 2-3 words, it will use 2-
3 chunks/words around the word of interest). Fig-
ure 2 contains an example of such a case: the cas-
cade model will have to predict the type of the
entire phrase Donna Karan International, in the
context ?Since <chunk> went public in ..?, which
will give it a better opportunity to classify it as an
organization. In contrast, because the joint model
and AIO have a word view of the sentence, will lack
the benefit of examining the larger region, and will
not have access at features that involve partial fu-
ture classifications (such as the fact that another
mention of a particular type follows).
Compared with the other two models, this clas-
sification method has the following advantages:
? The classification spaces for each subtask are
considerably smaller; this fact enables the cre-
ation of better estimated models
? The problem of partially-agreeing competing
labels is completely eliminated
? One can easily use different/additional data to
train any of the sub-task models.
3.4 Adding Partially Labeled Data
Annotated data can be sometimes expensive to
come by, especially if the label set is complex. But
not all sub-tasks were created equal: some of them
might be easier to predict than others and, there-
fore, require less data to train effectively in a cas-
cade setup. Additionally, in realistic situations,
some sub-tasks might be considered to have more
informational content than others, and have prece-
dence in evaluation. In such a scenario, one might
decide to invest resources in annotating additional
data only for the particularly interesting sub-task,
which could reduce this effort significantly.
To test this hypothesis, we annotated additional
data with the entity type only. The cascade model
can incorporate this data easily: it just adds it
to the training data for the entity type classifier
model. While it is not immediately apparent how
to incorporate this new data into the all-in-one and
joint models, in order to maintain fairness in com-
paring the models, we modified the procedures to
allow for the inclusion. Let T denote the original
training data, and T ? denote the additional train-
ing data.
For the all-in-one model, the additional training
data cannot be incorporated directly; this is an in-
herent deficiency of the AIO model. To facilitate a
fair comparison, we will incorporate it in an indi-
rect way: we train a classifier C on the additional
training data T ?, which we then use to classify the
original training data T . Then we train the all-
in-one classifier on the original training data T ,
adding the features defined on the output of ap-
plying the classifier C on T .
The situation is better for the joint model: the
new training data T ? can be incorporated directly
into the training data T .10 The maximum entropy
model estimates the model parameters by maxi-
mizing the data log-likelihood
L =
?
(x,y)
p? (x, y) log q? (y|x)
where p? (x, y) is the observed probability dis-
tribution of the pair (x, y) and q? (y|x) =
1
Z
?
j exp (?j ? fj (x, y)) is the conditional ME
probability distribution as computed by the model.
In the case where some of the data is partially an-
notated, the log-likelihood becomes
L =
?
(x,y)?T ?T ?
p? (x, y) log q? (y|x)
10The solution we present here is particular for
MEMM models (though similar solutions may exist for
other models as well). We also assume the reader is fa-
miliar with the normal MaxEnt training procedure; we
present here only the differences to the standard algo-
rithm. See (Manning and Schu?tze, 1999) for a good
description.
477
=
?
(x,y)?T
p? (x, y) log q? (y|x)
+
?
(x,y)?T ?
p? (x, y) log q? (y|x) (2)
The only technical problem that we are faced with
here is that we cannot directly estimate the ob-
served probability p? (x, y) for examples in T ?, since
they are only partially labeled. Borrowing the
idea from the expectation-maximization algorithm
(Dempster et al, 1977), we can replace this proba-
bility by the re-normalized system proposed prob-
ability: for (x, yx) ? T ?, we define
q? (x, y) = p? (x) ? (y ? yx) q? (y|x)?
y??yx q? (y?|x)? ?? ?
=q??(y|x)
where yx is the subset of labels from Y which are
consistent with the partial classification of x in T ?.
? (y ? yx) is 1 if and only if y is consistent with
the partial classification yx.11 The log-likelihood
computation in Equation (2) becomes
L =
?
(x,y)?T
p? (x, y) log q? (y|x)
+
?
(x,y)?T ?
q? (x, y) log q? (y|x)
To further simplify the evaluation, the quantities
q? (x, y) are recomputed every few steps, and are
considered constant as far as finding the optimum
? values is concerned (the partial derivative com-
putations and numerical updates otherwise become
quite complicated, and the solution is no longer
unique). Given this new evaluation function, the
training algorithm will proceed exactly the same
way as in the normal case where all the data is
fully labeled.
4 Experiments
All the experiments in this section are run on the
ACE 2003 and 2004 data sets, in all the three
languages covered: Arabic, Chinese, and English.
Since the evaluation test set is not publicly avail-
able, we have split the publicly available data into
a 80%/20% data split. To facilitate future compar-
isons with work presented here, and to simulate a
realistic scenario, the splits are created based on
article dates: the test data is selected as the last
20% of the data in chronological order. This way,
the documents in the training and test data sets
do not overlap in time, and the ones in the test
data are posterior to the ones in the training data.
Table 2 presents the number of documents in the
training/test datasets for the three languages.
11For instance, the full label B-PER is consistent
with the partial label B, but not with O or I.
Language Training Test
Arabic 511 178
Chinese 480 166
English 2003 658 139
English 2004 337 114
Table 2: Datasets size (number of documents)
Each word in the training data is labeled with
one of the following properties:12
? if it is not part of any entity, it?s labeled as O
? if it is part of an entity, it contains a tag spec-
ifying whether it starts a mention (B -) or is
inside a mention (I -). It is also labeled with
the entity type of the mention (seven possible
types: person, organization, location, facility,
geo-political entity, weapon, and vehicle), the
mention type (named, nominal, pronominal,
or premodifier13), and the entity subtype (de-
pends on the main entity type).
The underlying classifier used to run the experi-
ments in this article is a maximum entropy model
with a Gaussian prior (Chen and Rosenfeld, 1999),
making use of a large range of features, includ-
ing lexical (words and morphs in a 3-word win-
dow, prefixes and suffixes of length up to 4, Word-
Net (Miller, 1995) for English), syntactic (POS
tags, text chunks), gazetteers, and the output of
other information extraction models. These fea-
tures were described in (Florian et al, 2004), and
are not discussed here. All three methods (AIO,
joint, and cascade) instantiate classifiers based on
the same feature types whenever possible. In terms
of language-specific processing, the Arabic system
uses as input morphological segments, while the
Chinese system is a character-based model (the in-
put elements x ? X are characters), but it has
access to word segments as features.
Performance in the ACE task is officially eval-
uated using a special-purpose measure, the ACE
value metric (NIST, 2003; NIST, 2004). This
metric assigns a score based on the similarity be-
tween the system?s output and the gold-standard
at both mention and entity level, and assigns dif-
ferent weights to different entity types (e.g. the
person entity weights considerably more than a fa-
cility entity, at least in the 2003 and 2004 evalu-
ations). Since this article focuses on the mention
detection task, we decided to use the more intu-
itive (unweighted) F-measure: the harmonic mean
of precision and recall.
12The mention encoding is the IOB2 encoding pre-
sented in (Tjong Kim Sang and Veenstra, 1999) and
introduced by (Ramshaw and Marcus, 1994) for the
task of base noun phrase chunking.
13This is a special class, used for mentions that mod-
ify other labeled mentions; e.g. French in ?French
wine?. This tag is specific only to ACE?04.
478
For the cascade model, the sub-task flow is pre-
sented in Figure 1. In the first step, we identify
the mention boundaries together with their entity
type (e.g. person, organization, etc). In prelimi-
nary experiments, we tried to ?cascade? this task.
The performance was similar on both strategies;
the separated model would yield higher recall at
the expense of precision, while the combined model
would have higher precision, but lower recall. We
decided to use in the system with higher precision.
Once the mentions are identified and classified with
the entity type property, the data is passed, in par-
allel, to the mention type detector and the subtype
detector.
For English and Arabic, we spent three person-
weeks to annotate additional data labeled with
only the entity type information: 550k words for
English and 200k words for Arabic. As mentioned
earlier, adding this data to the cascade model is a
trivial task: the data just gets added to the train-
ing data, and the model is retrained. For the AIO
model, we have build another mention classifier on
the additional training data, and labeled the orig-
inal ACE training data with it. It is important
to note here that the ACE training data (called
T in Section 3.4) is consistent with the additional
training data T ?: the annotation guidelines for T ?
are the same as for the original ACE data, but we
only labeled entity type information. The result-
ing classifications are then used as features in the
final AIO classifier. The joint model uses the addi-
tional partially-labeled data in the way described
in Section 3.4; the probabilities q? (x, y) are updated
every 5 iterations.
Table 3 presents the results: overall, the cascade
model performs significantly better than the all-
in-one model in four out the six tested cases - the
numbers presented in bold reflect that the differ-
ence in performance to the AIO model is statisti-
cally significant.14 The joint model, while manag-
ing to recover some ground, falls in between the
AIO and the cascade models.
When additional partially-labeled data was
available, the cascade and joint models receive a
statistically significant boost in performance, while
the all-in-one model?s performance barely changes.
This fact can be explained by the fact that the en-
tity type-only model is in itself errorful; measuring
the performance of the model on the training data
yields a performance of 82 F-measure;15 therefore
the AIO model will only access partially-correct
14To assert the statistical significance of the results,
we ran a paired Wilcoxon test over the series obtained
by computing F-measure on each document in the test
set. The results are significant at a level of at least
0.009.
15Since the additional training data is consistent in
the labeling of the entity type, such a comparison is in-
deed possible. The above mentioned score is on entity
types only.
Language Data+ A-I-O Joint Cascade
Arabic?04 no 59.2 59.1 59.7
yes 59.4 60.0 60.7
English?04 no 72.1 72.3 73.7
yes 72.5 74.1 75.2
Chinese?04 no 71.2 71.7 71.7
English ?03 no 79.5 79.5 79.7
Table 3: Experimental results: F-measure on the
full label
Language Data+ A-I-O Joint Cascade
Arabic?04 no 66.3 66.5 67.5
yes 66.4 67.9 68.9
English?04 no 77.9 78.1 79.2
yes 78.3 80.5 82.6
Chinese?04 no 75.4 76.1 76.8
English ?03 no 80.4 80.4 81.1
Table 4: F-measure results on entity type only
data, and is unable to make effective use of it.
In contrast, the training data for the entity type
in the cascade model effectively triples, and this
change is reflected positively in the 1.5 increase in
F-measure.
Not all properties are equally valuable: the en-
tity type is arguably more interesting than the
other properties. If we restrict ourselves to eval-
uating the entity type output only (by projecting
the output label to the entity type only), the differ-
ence in performance between the all-in-one model
and cascade is even more pronounced, as shown in
Table 4. The cascade model outperforms here both
the all-in-one and joint models in all cases except
English?03, where the difference is not statistically
significant.
As far as run-time speed is concerned, the AIO
and cascade models behave similarly: our imple-
mentation tags approximately 500 tokens per sec-
ond (averaged over the three languages, on a Pen-
tium 3, 1.2Ghz, 2Gb of memory). Since a MaxEnt
implementation is mostly dependent on the num-
ber of features that fire on average on a example,
and not on the total number of features, the joint
model runs twice as slow: the average number of
features firing on a particular example is consider-
ably higher. On average, the joint system can tag
approximately 240 words per second. The train
time is also considerably longer; it takes 15 times as
long to train the joint model as it takes to train the
all-in-one model (60 mins/iteration compared to
4 mins/iteration); the cascade model trains faster
than the AIO model.
One last important fact that is worth mention-
ing is that a system based on the cascade model
participated in the ACE?04 competition, yielding
very competitive results in all three languages.
479
5 Conclusion
As natural language processing becomes more so-
phisticated and powerful, we start focus our at-
tention on more and more properties associated
with the objects we are seeking, as they allow for
a deeper and more complex representation of the
real world. With this focus comes the question of
how this goal should be accomplished ? either de-
tect all properties at once, one at a time through
a pipeline, or a hybrid model. This paper presents
three methods through which multi-label sequence
classification can be achieved, and evaluates and
contrasts them on the Automatic Content Extrac-
tion task. On the ACE mention detection task,
the cascade model which predicts first the mention
boundaries and entity types, followed by mention
type and entity subtype outperforms the simple all-
in-one model in most cases, and the joint model in
a few cases.
Among the proposed models, the cascade ap-
proach has the definite advantage that it can easily
and productively incorporate additional partially-
labeled data. We also presented a novel modifica-
tion of the joint system training that allows for the
direct incorporation of additional data, which in-
creased the system performance significantly. The
all-in-one model can only incorporate additional
data in an indirect way, resulting in little to no
overall improvement.
Finally, the performance obtained by the cas-
cade model is very competitive: when paired with a
coreference module, it ranked very well in the ?En-
tity Detection and Tracking? task in the ACE?04
evaluation.
References
R. Caruana, L. Pratt, and S. Thrun. 1997. Multitask
learning. Machine Learning, 28:41.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, Computer Sci-
ence Department, Carnegie Mellon University.
A. P. Dempster, N. M. Laird, , and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal statistical Soci-
ety, 39(1):1?38.
R. Florian and G. Ngai. 2001. Multidimensional
transformation-based learning. In Proceedings of
CoNLL?01, pages 1?8.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N Nicolov, and S Roukos.
2004. A statistical model for multilingual entity de-
tection and tracking. In Proceedings of the Human
Language Technology Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004, pages 1?8.
R. Florian. 2002. Named entity recognition as a
house of cards: Classifier stacking. In Proceedings
of CoNLL-2002, pages 175?178.
Kadri Hacioglu, Benjamin Douglas, and Ying Chen.
2005. Detection of entity mentions occuring in en-
glish and chinese text. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 379?386, Vancouver, British Columbia,
Canada, October. Association for Computational
Linguistics.
Kadri Hacioglu. 2005. Private communication.
J. Hajic and Hladka?. 1998. Tagging inflective lan-
guages: Prediction of morphological categories for a
rich, structured tagset. In Proceedings of the 36th
Annual Meeting of the ACL and the 17th ICCL,
pages 483?490, Montre?al, Canada.
H. Jing, R. Florian, X. Luo, T. Zhang, and A. It-
tycheriah. 2003. HowtogetaChineseName(Entity):
Segmentation and combination issues. In Proceed-
ings of EMNLP?03, pages 200?207.
Dan Klein. 2003. Maxent models, conditional estima-
tion, and optimization, without the magic. Tutorial
presented at NAACL-03 and ACL-03.
C. D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19:313?330.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models
for information extraction and segmentation. In Pro-
ceedings of ICML-2000.
G. A. Miller. 1995. WordNet: A lexical database.
Communications of the ACM, 38(11).
MUC-6. 1995. The sixth mes-
sage understanding conference.
www.cs.nyu.edu/cs/faculty/grishman/muc6.html.
MUC-7. 1997. The seventh mes-
sage understanding conference.
www.itl.nist.gov/iad/894.02/related projects/
muc/proceedings/muc 7 toc.html.
NIST. 2003. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
NIST. 2004. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
L. Ramshaw and M. Marcus. 1994. Exploring the sta-
tistical derivation of transformational rule sequences
for part-of-speech tagging. In Proceedings of the
ACL Workshop on Combining Symbolic and Statis-
tical Approaches to Language, pages 128?135.
C. Sutton, K. Rohanimanesh, and A. McCallum.
2004. Dynamic conditional random fields: Factor-
ized probabilistic models for labeling and segment-
ing sequence data. In In Proceedings of the Twenty-
First International Conference on Machine Learning
(ICML-2004).
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142?147. Edmonton,
Canada.
E. F. Tjong Kim Sang and J. Veenstra. 1999. Repre-
senting text chunks. In Proceedings of EACL?99.
E. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independent named en-
tity recognition. In Proceedings of CoNLL-2002,
pages 155?158.
480
Named Entity Recognition through Classifier Combination
Radu Florian and Abe Ittycheriah and Hongyan Jing and Tong Zhang
IBM T.J. Watson Research Center
1101 Kitchawan Rd, Yorktown Heights, NY 10598, USA
{raduf,abei,hjing,tzhang}@us.ibm.com
Abstract
This paper presents a classifier-combination
experimental framework for named entity
recognition in which four diverse classi-
fiers (robust linear classifier, maximum en-
tropy, transformation-based learning, and hid-
den Markov model) are combined under differ-
ent conditions. When no gazetteer or other ad-
ditional training resources are used, the com-
bined system attains a performance of 91.6F
on the English development data; integrat-
ing name, location and person gazetteers, and
named entity systems trained on additional,
more general, data reduces the F-measure error
by a factor of 15 to 21% on the English data.
1 Introduction
This paper investigates the combination of a set of di-
verse statistical named entity classifiers, including a
rule-based classifier ? the transformation-based learning
classifier (Brill, 1995; Florian and Ngai, 2001, hence-
forth fnTBL) with the forward-backward extension de-
scribed in Florian (2002a), a hidden Markov model clas-
sifier (henceforth HMM), similar to the one described
in Bikel et al (1999), a robust risk minimization classi-
fier, based on a regularized winnow method (Zhang et al,
2002) (henceforth RRM) and a maximum entropy clas-
sifier (Darroch and Ratcliff, 1972; Berger et al, 1996;
Borthwick, 1999) (henceforth MaxEnt). This particular
set of classifiers is diverse across multiple dimensions,
making it suitable for combination:
? fnTBL is a discriminant classifier ? it bases its clas-
sification decision only on the few most discriminant
features active on an example ? while HMM, RRM
and MaxEnt are agglomerative classifiers ? their de-
cision is based on the combination of all features ac-
tive for the particular example.
? In dealing with the data sparseness problem, fnTBL,
MaxEnt and RRM investigate and integrate in their
decision arbitrary feature types, while HMM is de-
pendent on a prespecified back-off path.
? The search methods employed by each classifier are
different: the HMM, MaxEnt and RRM classifiers
construct a model for each example and then rely
on a sequence search such as the Viterbi algorithm
(Viterbi, 1967) to identify the best overall sequence,
while fnTBL starts with most frequent classification
(usually per token), and then dynamically models
the interaction between classifications, effectively
performing the search at training time.
? The classifiers also differ in their output: fnTBL
and RRM return a single classification per exam-
ple1, while the MaxEnt and HMM classifiers return
a probability distribution.
The remainder of the paper is organized as follows: Sec-
tion 2 describes the features used by the classifiers, Sec-
tion 3 briefly describes the algorithms used by each clas-
sifier, and Section 4 analyzes in detail the results obtained
by each classifier and their combination.
2 The Classification Method and Features
Used
All algorithms described in this paper identify the named
entities in the text by labeling each word with a tag
corresponding to its position relative to a named entity:
whether it starts/continues/ends a specific named entity,
or does not belong to any entity. RRM, MaxEnt, and
fnTBL treat the problem entirely as a tagging task, while
the HMM algorithm used here is constraining the transi-
tions between the various phases, similar to the method
described in (Bikel et al, 1999).
Feature design and integration is of utmost importance
in the overall classifier design ? a rich feature space is the
key to good performance. Often, high performing classi-
fiers operating in an impoverished space are surpassed by
a lower performing classifier when the latter has access
to enhanced feature spaces (Zhang et al, 2002; Florian,
1 However, both classifiers? algorithms can be modified such
that a class probability distribution is returned instead.
2002a). In accordance with this observation, the clas-
sifiers used in this research can access a diverse set of
features when examining a word in context, including:
? words and their lemmas in a 5-word-window sur-
rounding the current word
? the part-of-speech tags of the current and surround-
ing words
? the text chunks in a -1..1 window
? the prefixes and suffixes of length up to 4 of the cur-
rent and the surrounding words
? a word feature flag for each word, similar to the flag
described in (Bikel et al, 1999); examples of such
assigned flags are firstCap, 2digit and allCaps.
? gazetteer information, in the form of a list of 50,000
cities, 80,000 proper names and 3500 organizations
? the output of other two named entity classifiers,
trained on a richer tagset data (32 named categories),
used in the IBM question answering system (Itty-
cheriah et al, 2001)
In addition, a ngram-based capitalization restoration al-
gorithm has been applied on the sentences that appear in
all caps2, for the English task.
3 The Algorithms
This section describes only briefly the classifiers used in
combination in Section 4; a full description of the algo-
rithms and their properties is beyond the scope of this pa-
per ? the reader is instead referred to the original articles.
3.1 The Robust Risk Minimization Classifier
This classifier is described in detail in (Zhang and John-
son, 2003, this volume), along with a comprehensive
evaluation of its performance, and therefore is not pre-
sented here.
3.2 The Maximum Entropy Classifier
The MaxEnt classifier computes the posterior class prob-
ability of an example by evaluating the normalized prod-
uct of the weights active for the particular example. The
model weights are trained using the improved iterative
scaling algorithm (Berger et al, 1996). To avoid running
in severe over-training problems, a feature cutoff of 4 is
applied before the model weights are learned. At decod-
ing time, the best sequence of classifications is identified
with the Viterbi algorithm.
3.3 The Transformation-Based Learning Classifier
Transformation-based learning is an error-driven algo-
rithm which has two major steps: it starts by assigning
some classification to each example, and then automat-
ically proposing, evaluating and selecting the classifica-
tion changes that maximally decrease the number of er-
rors.
2 Usually, document titles, but also table headers, etc.
English German
(a) (b) (a) (b)
HMM 82.0 74.6 - -
TBL 88.1 81.2 69.5 68.6
MaxEnt 90.8 85.6 68.0 67.3
RRM 92.1 85.5 70.7 71.3
Tab. 1: Individual classifier results on the two test sets.
TBL has some attractive qualities that make it suitable
for the language-related tasks: it can automatically in-
tegrate heterogeneous types of knowledge, without the
need for explicit modeling, it is error?driven, and has an
inherently dynamic behavior.
The particular setup in which fnTBL is used in this
work is described in Florian (2002a): in a first phase,
TBL is used to identify the entity boundaries, followed by
a sequence classification stage, where the entities identi-
fied at the first step are classified using internal and exter-
nal clues3.
3.4 The Hidden Markov Model Classifier
The HMM classifier used in the experiments in Section
4 follows the system description in (Bikel et al, 1999),
and it performs sequence classification by assigning each
word either one of the named entity types or the label
NOT-A-NAME to represent "not a named entity". The
states in the HMM are organized into regions, one re-
gion for each type of named entity plus one for NOT-
A-NAME. Within each of the regions, a statistical bi-
gram language model is used to compute the likelihood of
words occurring within that region (named entity type).
The transition probabilities are computed by deleted in-
terpolation (Jelinek, 1997), and the decoding is done
through the Viterbi algorithm. The particular implemen-
tation we used underperformed consistently all the other
classifiers on German, and is not included.
4 Combination Methodology and
Experimental Results
The results obtained by each individual classifier, bro-
ken down by entity type, are presented in Table 1. Out
of the four classifiers, the MaxEnt and RRM classifiers
are the best performers, followed by the modified fnTBL
classifier and the HMM classifier. The error-based clas-
sifiers (RRM and fnTBL) tend to obtain balanced preci-
sion/recall numbers, while the other two tend to be more
precise at the expense of recall. To facilitate comparison
with other classifiers for this task, most reported results
3 The method of retaining only the boundaries and reclas-
sifying the entities was shown to improve the performance of
11 of the 12 systems participating in the CoNLL-2002 shared
tasks, in both languages (Florian, 2002b).
are obtained by using features exclusively extracted from
the training data.
In general, given n classifiers, one can interpret the
classifier combination framework as combining probabil-
ity distributions:
P (C|w,Cn1 ) = f ((Pi (C|w,C
n
1 ))i=1...n) (1)
where Ci is the classifier i?s classification output, f is
a combination function. A widely used combination
scheme is through linear interpolation of the classifiers?
class probability distribution
P (C|w,Cn1 ) =
n?
i=1
P (C|w, i, Ci) ? P (i|w)
=
n?
i=1
Pi (C|w,Ci) ? ?i (w) (2)
The weights ?i (w) encode the importance given to clas-
sifier i in combination, for the context of word w, and
Pi (C|w,Ci) is an estimation of the probability that the
correct classification is C, given that the output of the
classifier i on word w is Ci.
To estimate the parameters in Equation (2), the pro-
vided training data was split into 5 equal parts, and each
classifier was trained, in a round-robin fashion, on 4 fifths
of the data and applied on the remaining fifth. This
way, the entire training data can be used to estimate the
weight parameters ?i (w) and Pi (C|w,Ci) but, at de-
coding time, the individual classifier outputs Ci are com-
puted by using the entire training data.
Table 2 presents the combination results, for differ-
ent ways of estimating the interpolation parameters. A
simple combination method is the equal voting method
(van Halteren et al, 2001; Tjong Kim Sang et al, 2000),
where the parameters are computed as ?i (w) = 1n and
Pi (C|w,Ci) = ? (C,Ci), where ? is the Kronecker op-
erator (? (x, y) := (x = y?1 : 0)) ? each of the classi-
fiers votes with equal weight for the class that is most
likely under its model, and the class receiving the largest
number of votes wins. However, this procedure may lead
to ties, where some classes receive the same number of
votes ? one usually resorts to randomly selecting one of
the tied candidates in this case ? Table 2 presents the av-
erage results obtained by this method, together with the
variance obtained over 30 trials. To make the decision de-
terministically, the weights associated with the classifiers
can be chosen as ?i (w) = Pi (error). In this method,
presented in Table 2 as weighted voting, better perform-
ing classifiers will have a higher impact in the final clas-
sification.
In the voting methods, each classifier gave its entire
vote to one class ? its own output. However, Equation
(2) allows for classifiers to give partial credit to alterna-
tive classifications, through the probability Pi (C|w,Ci).
Method Precision Recall Fmeasure
Best Classifier 91.37% 88.56% 89.94
Equal voting 91.5?0.13 91.0?0.06 91.23?0.08
Weighted voting 92.13% 91.00% 91.56
Model 1 90.99% 90.81% 90.9
Model 2 92.43% 90.86% 91.64
RRM (Combo) 92.01% 91.25% 91.63
Tab. 2: Classifier combination results on English devset
data (no gazetteers of any kind)
Development Test
Language Unique Corpus Unique Corpus
English 33.4% 8.0% 40.3% 11.7%
German 52% 16.2% 48.6% 14.2%
Tab. 3: Word statistics (percent unknown words)
In our experiments, this value is computed through 5-
fold cross-validation on the training data. The space
of possible choices for C, w and Ci is large enough
to make the estimation unreliable, so we use two ap-
proximations, named Model 1 and Model 2 in Table 2:
Pi (C|w,Ci) = Pi (C|w)and Pi (C|w,Ci) = Pi (C|Ci),
respectively. On the development data, the former esti-
mation type obtains a lower performance than the latter.
In a last experiment using only features extracted from
the training data, we use the RRM method to compute
the function f in Equation (1), allowing the system to
select a good performing combination of features. At
training time, the system was fed the output of each clas-
sifier on the cross-classified data, the part-of-speech and
chunk boundary tags. At test time, the system was fed the
classifications of each system trained on the entire train-
ing data, and the corresponding POS and chunk bound-
ary tags. The result obtained rivals the one obtained by
model 2, both displaying a 17% reduction in F-measure
error4, indicating that maybe all sources of information
have been explored and incorporated.
The RRM method is showing its combining power
when additional information sources are used. Specifi-
cally, the system was fed additional feature streams from
a list of gazetteers and the output of two other named en-
tity systems trained on 1.7M words annotated with 32
name categories. The RRM system alone obtains an F-
measure of 92.1, and can effectively integrate these in-
formation streams with the output of the four classifiers,
gazetteers and the two additional classifiers into obtaining
93.9 F-measure, as detailed in Table 4, a 21% reduction
in F-measure error. In contrast, combination model 2 ob-
tains only a performance of 92.4, showing its limitations
4 Measured as 100? F .
in combining diverse sources of information.
German poses a completely different problem for
named entity recognition: the data is considerably
sparser. Table 3 shows the relative distribution of un-
known words in the development and test corpora. We
note that the numbers are roughly twice as large for the
development data in German as they are for English.
Since the unknown words are classed by most classifiers,
this results in few data points to estimate classifier com-
binations. Also, specifically for the German data, tradi-
tional approaches which utilize capitalization do not work
as well as in English, because all nouns are capitalized in
German.
For German, in addition to the entity lists provided, we
also used a small gazetteer of names (4500 first and last
names, 4800 locations in Germany and 190 countries),
which was collected by browsing web pages in about two
person-hours. The average classifier performance gain by
using these features is about 1.5F for the testa data and
about .6F for the testb data.
5 Conclusion
In conclusion, we have shown results on a set of both
well-established and novel classifier techniques which
improve the overall performance, when compared with
the best performing classifier, by 17-21% on the English
task. For the German task, the improvement yielded by
classifier combination is smaller. As a machine learning
method, the RRM algorithm seems especially suited to
handle additional feature streams, and therefore is a good
candidate for classifier combination.
References
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maxi-
mum entropy approach to natural language processing. Com-
putational Linguistics, 22(1):39?71.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s in a
name. Machine Learning, 34(1-3):211?231.
A. Borthwick. 1999. A Maximum Entropy Approach to Named
Entity Recognition. Ph.D. thesis, New York University.
E. Brill. 1995. Transformation-based error-driven learning and
natural language processing: A case study in part of speech
tagging. Computational Linguistics, 21(4):543?565.
J. N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathematical
Statistics, 43(5):1470?1480.
R. Florian and G. Ngai, 2001. Fast Transformation-
Based Learning Toolkit. Johns Hopkins University,
http://nlp.cs.jhu.edu/?rflorian/fntbl/documentation.html.
R. Florian. 2002a. Named entity recognition as a house of
cards: Classifier stacking. In Proceedings of CoNLL-2002,
pages 175?178.
R. Florian. 2002b. Transformation Based Learning and Data-
Driven Lexical Disambiguation: Syntactic and Semantic
Ambiguity Resolution. Ph.D. thesis, Johns Hopkins Univer-
sity. Chapter 5.3, pages 135?142.
English devel. Precision Recall F?=1
LOC 96.59% 95.65% 96.12
MISC 90.77% 87.42% 89.06
ORG 90.85% 89.63% 90.24
PER 96.08% 97.12% 96.60
overall 94.26% 93.47% 93.87
English test Precision Recall F?=1
LOC 90.59% 91.73% 91.15
MISC 83.46% 77.64% 80.44
ORG 85.93% 83.44% 84.67
PER 92.49% 95.24% 93.85
overall 88.99% 88.54% 88.76
German devel. Precision Recall F?=1
LOC 83.19% 72.90% 77.71
MISC 83.20% 42.18% 55.98
ORG 83.64% 61.80% 71.08
PER 87.43% 67.02% 75.88
overall 84.60% 61.93% 71.51
German test Precision Recall F?=1
LOC 80.19% 71.59% 75.65
MISC 77.87% 41.49% 54.14
ORG 79.43% 54.46% 64.62
PER 91.93% 75.31% 82.80
overall 83.87% 63.71% 72.41
Tab. 4: Results on the development and test sets in En-
glish and German
Abraham Ittycheriah, Martin Franz, and Salim Roukos. 2001.
IBM?s statistical question answering system ? trec-10.
TREC-10 Proceedings, pages 258?264.
F. Jelinek. 1997. Statistical Methods for Speech Recognition.
MIT Press.
E. F. Tjong Kim Sang, W. Daelemans, H. Dejean, R. Koeling,
Y. Krymolowsky, V. Punyakanok, and D. Roth. 2000. Ap-
plying system combination to base noun phrase identifica-
tion. In Proceedings of COLING 2000, pages 857?863.
H. van Halteren, J. Zavrel, and W. Daelemans. 2001. Improv-
ing accuracy in word class tagging through the combination
fo machine learning systems. Computational Linguistics,
27(2):199?230.
A. J. Viterbi. 1967. Error bounds for convolutional codes and an
asymptotically optimum decoding algorithm. IEEE Transac-
tions on Information Theory, IT-13:260?267.
T. Zhang and D. Johnson. 2003. A robust risk minimization
based named entity recognition system. In Proceedings of
CoNLL-2003.
T. Zhang, F. Damerau, and D. Johnson. 2002. Text chunking
based on a generalization of winnow. Journal of Machine
Learning Research, 2:615?637, March.
HowtogetaChineseName(Entity): Segmentation and Combination Issues
Hongyan Jing Radu Florian Xiaoqiang Luo
Tong Zhang Abraham Ittycheriah
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
 
hjing,raduf,xiaoluo,tzhang,abei  @us.ibm.com
Abstract
When building a Chinese named entity
recognition system, one must deal with
certain language-specific issues such as
whether the model should be based on
characters or words. While there is no
unique answer to this question, we discuss
in detail advantages and disadvantages of
each model, identify problems in segmen-
tation and suggest possible solutions, pre-
senting our observations, analysis, and
experimental results. The second topic
of this paper is classifier combination.
We present and describe four classifiers
for Chinese named entity recognition and
describe various methods for combining
their outputs. The results demonstrate that
classifier combination is an effective tech-
nique of improving system performance:
experiments over a large annotated corpus
of fine-grained entity types exhibit a 10%
relative reduction in F-measure error.
1 Introduction
Named entity (NE) recognition has drawn much at-
tention in recent years. It was a designated task
in a number of conferences, including the Mes-
sage Understanding Conferences (MUC-6, 1995;
MUC-7, 1997), the Information Retrieval and Ex-
traction Conference (IREX, 1999), the Conferences
on Natural Language Learning (Tjong Kim Sang,
2002; Tjong Kim Sang and De Meulder, 2003),
and the recent Automatic Content Extraction Con-
ference (ACE, 2002).
A variety of algorithms have been proposed for
NE recognition. Many of these algorithms are, in
principle, language-independent. However, when
applying these algorithms to languages such as
Chinese and Japanese, we must deal with cer-
tain language-specific issues: for example, should
we build a character-based model or a word-based
model? how do word segmentation errors affect NE
recognition? how should word segmentation and NE
recognition interact with each other? Besides word
segmentation related issues, Chinese does not have
capitalization, which is a very useful feature in iden-
tifying NEs in languages such as English, Spanish,
or Dutch. How does the lack of features such as cap-
italization affect the performance?
In the first part of this paper, we discuss these
language-specific issues in Chinese NE recogni-
tion. In particular, we use a hidden Markov model
(HMM) system as an example, and discuss various
issues related to applying the HMM classifier to Chi-
nese. The HMM classifier is similar to the one de-
scribed in (Bikel et al, 1999).
In the second part of this paper, we investigate
the combination of a set of diverse NE recognition
classifiers. Four statistical classifiers are combined
in the experiments, including the above-mentioned
hidden Markov model classifier, a transformation-
based learning classifier (Brill, 1995; Florian and
Ngai, 2001), a maximum entropy classifier (Ratna-
parkhi, 1999), and a robust risk minimization classi-
fier (Zhang et al, 2002).
The remainder of this paper is organized as fol-
lows: Section 2 describes the experiment data, Sec-
tion 3 discusses specific issues related to Chinese
NE recognition, Section 4 presents the four classi-
fiers and approaches to combining these classifiers.
2 Data
We used three annotated Chinese corpora in our ex-
periments.
The IBM-FBIS Corpus
The Foreign Broadcast Information Service (FBIS)
offers an extensive collection of translations and
transcriptions of open source information monitored
worldwide on diverse topics such as military af-
fairs, politics, economics, and science and technol-
ogy. The IBM-FBIS corpus consists of approxi-
mately 3,000 Chinese articles obtained from FBIS
(about 3.2 million Chinese characters in total). This
corpus was tagged by a native Chinese speaker with
32 NE categories, such as person, location, organi-
zation, country, people, date, time, percentage, car-
dinal, ordinal, product, substance, and salutation.
There are approximately 300,000 NEs in the entire
corpus, 16% of which are labeled as person, 16% as
organization, and 11% as location.
The IBM-CT Corpus
The Chinese Treebank (Xia et al, 2000), avail-
able from Linguistic Data Consortium, consists of
a 100,000 word (approximately 160,000 characters)
corpus annotated with word segmentation, part-of-
speech tags, and syntactic bracketing. It includes
325 articles from Xinhua newswire between 1994
and 1998. The same Chinese annotator who worked
on the above IBM-FBIS data also annotated the Chi-
nese Treebank data with NE information, henceforth
the IBM-CT corpus, using the same 32 categories as
mentioned above.
The IEER data
The National Institute of Standard and Technol-
ogy organized the Information Extraction ? Entity
Recognition (IEER) evaluation, which involves en-
tity recognition from textual information sources in
both English and Mandarin. The Mandarin training
data consists of approximately 10 hours of broad-
cast news transcripts comprised of approximately
390 stories. The test data also contains transcripts of
broadcast news1. The training data includes approx-
imately 170,000 characters and the test data includes
approximately 6,500 characters. Ten categories of
NEs were annotated, such as person, location, orga-
nization, date, duration, and measure.
1Other types of test data were also used in IEER evaluation,
including newswire text and real automatic speech recognition
transcripts, but we did not use them in our experiments.
3 Language-Specific Issues in Chinese NE
Recognition
Chinese does not have delimiters between words,
so a key design issue in Chinese NE recognition
is whether to build a character-based model or a
word-based model. In this section, we use a hid-
den Markov model NE recognition system as an ex-
ample to discuss language-specific issues in Chinese
NE recognition.
3.1 The Hidden Markov Model Classifier
NE recognition can be formulated as a classification
task, where the goal is to label each token with a
tag indicating whether it belongs to a specific NE
or is not part of any NE. The HMM classifier used
in our experiments follows the algorithm described
in (Bikel et al, 1999). It performs sequence clas-
sification by assigning each token either one of the
NE types or the label ?O? to represent ?outside any
NE?. The states in the HMM are organized into re-
gions, one region for each type of NE plus one for
?O?. Within each of the regions, a statistical lan-
guage model is used to compute the likelihood of
words occurring within that region. The transition
probabilities are smoothed by deleted interpolation,
and the decoding is performed using the Viterbi al-
gorithm.
3.2 Character-Based, Word-Based, and
Class-Based Models
To build a model for identifying Chinese NEs, we
need to determine the basic unit of the model: char-
acter or word. On one hand, the word-based model
is attractive since it allows the system to inspect a
larger window of text, which may lead to more in-
formative decisions. On the other hand, a word seg-
menter is not error-prone and these errors may prop-
agate and result in errors in NE recognition.
Two systems, a character-based HMM model and
a word-based HMM model, were built for compar-
ison. The word segmenter used in our experiments
relies on dictionaries and surrounding words in lo-
cal context to determine the word boundaries. Dur-
ing training, the NE boundaries were provided to the
word segmenter; the latter is restricted to enforce
word boundaries at each entity boundary. Therefore,
at training time, the word boundaries are consistent
with the entity boundaries. At test time, however,
the segmenter could create words which do not agree
with the gold-standard entity boundaries.
Corpus Model Prec Rec 
Character 74.36% 80.24% 77.19
IBM- Word 72.46% 75.97% 74.17
FBIS Class 72.74% 76.20% 74.43
Character 74.57% 78.01% 76.25
IEER Word 77.51% 65.22% 70.83
Class 77.21% 64.36% 70.20
Table 1: Performance of the character-based HMM
model, the word-based HMM model, and the class-
based HMM model. (The precision, recall, and F-
measure presented in this table and throughout this
paper are based on correct identification of all the
attributes of an NE, including boundary, content,
and type.)
The performance of the character-based model
and the word-based model are shown in Table 1. The
two corpora used in the evaluation, the IBM-FBIS
corpus and the IEER corpus, differ greatly in data
size and the number of NE types. The IBM-FBIS
training data consists of 3.1 million characters and
the corresponding test data has 270,000 characters.
As we can see from the table, for both corpora, the
character-based model outperforms the word-based
model, with a lead of 3 to 5.5 in F-measure. The per-
formance gap between two models is larger for the
IEER data than for the IBM-FBIS data.
We also built a class-based NE model. After word
segmentation, class tags such as number, chinese-
name, foreign-name, date, and percent are used to
replace words belonging to these classes. Whether
a word belongs to a specific class is identified by
a rule-based normalizer. The performance of the
class-based HMM model is also shown in Table 1.
For the IBM-FBIS corpus, the class-based model
outperforms the word-based model; for the IEER
corpus, the class-based model is worse than the
word-based model. In both cases, the performance
difference between the word-based model and the
class-based model is very small. The character-
based model outperforms the class-based model in
both tests.
A more careful analysis indicates that although
the word-based model performs worse than the
character-based model overall in our evaluation, it
performs better for certain NE types. For instance,
the word-based model has a better performance
for the organization category than the character-
based model in both tests. While the character-
based model has an F-measure of 65.07 (IBM-FBIS)
and 64.76 (IEER) for the organization category,
the word-based model achieves F-measure scores of
69.14 (IBM-FBIS) and 72.38 (IEER) respectively.
One reason may be that organization names tend to
contain many characters, and since the word-based
model allows the system to analyze a larger window
of text, it is more likely to make a correct guess.
We can integrate the character-based model and the
word-based model by combining the decisions from
the two models. For instance, if we use the de-
cisions of the word-based model for the organiza-
tion category, but use the decisions of the character-
based model for all the other categories, the over-
all F-measure goes up to 76.91 for the IEER data,
higher than using either the character-based or word-
based model alone. Another way to integrate the
two models is to use a hybrid model ? starting with
a word-based model and backing off to character-
based model if the word is unknown.
3.3 Granularity of Word Segmentation
We believe that one main reason for the lower per-
formance of the word-based model is that the word
granularity defined by the word segmenter is not
suitable for the HMM model to perform the NE
recognition task. What exactly constitutes a Chinese
word has been a topic of major debate. We are in-
terested in what is the best word granularity for our
particular task.
To illustrate the word granularity problem for NE
tagging, we take person names as an example. Our
word segmenter marks a person?s name as one word,
consistent with the convention used by the Chinese
treebank and many other word segmentation sys-
tems. While this may be useful in other applications,
it is certainly not a good choice for our NE model.
Chinese names typically contain two or three char-
acters, with family name preceding first name. Only
a limited set of characters are used as family names,
while the first name can be any character(s). There-
fore, the family name is a very important and use-
ful feature in identifying an NE in the person cate-
gory. By combining the family name and the first
name into one word, this important feature is lost to
the word-based model. In our tests, the word-based
model performs much worse for the person category
than the character-based model. We believe that, for
the purpose of NE recognition, it is better to separate
the family name from the first name in word segmen-
tation, although this is not the convention used in the
Chinese treebank.
Other examples include the segmentation of
words indicating dates, countries, locations, percent-
ages, measures, and ordinals. For instance, ?July
4th? is expressed by four characters ?7th month 4th
day? in Chinese. The word segmenter marks the
four characters as a single word; however, the sec-
ond and the last character are actually good features
for indicating date, since the dates are usually ex-
pressed using the same structure (e.g., ?March 25th?
is expressed by ?3rd month 25th day? in Chinese).
For reasons similar to the above, we believe that it
is better to separate characters representing ?month?
and ?day?, rather than combining the four charac-
ters into one word. A similar problem can be ob-
served in English with tokens such as ?61-year-old
man? if one is interested in identifying a person?s
age, in which case ?year? and ?old? are good features
for predication.
The above analysis suggests that a better way to
apply a word segmenter in an NE system is to first
adapt the segmenter so that the segmentation granu-
larity is more appropriate to the particular task and
model. As a guideline, characters that are good fea-
tures for identifying NEs should not be combined
with other characters into word. Additional ex-
amples include characters expressing ?percent? and
characters representing monetary measures .
3.4 The Effect of Segmentation Errors
Word segmentation errors can lead to mistakes in
NE recognition. Suppose an NE consists of four
characters 	
  
  
  
 , if the word segmen-
tation merges 
  with a character preceding it,
then this NE cannot be correctly identified by the
word-based model since the boundary will be incor-
rect. Besides inducing NE boundary errors, incor-
rect word segmentation also leads to wrong match-
ings between training examples and testing exam-
ples, which may result in mistakes in identifying en-
tities.
We computed the upper bound for the word-based
model for the IBM-FBIS test presented in Table 1.
The upper bound of performance is computed by
dividing the total number of NEs whose bound-
aries are also recognized as boundaries by the word
segmenter by the total number of NEs in the cor-
pus, which is the precision, recall, and also the F-
measure. For the IBM-FBIS test data in Table 1,
the upper bound of the word-based model is 95.7 F-
measure.
We also did the following experiment to measure
the effect of word segmentation errors: we gave
the boundaries of NEs in the test data to the word
segmenter and forced it to mark entity boundaries
as word boundaries. This eliminates the word seg-
mentation errors that inevitably result in NE bound-
ary errors. For the IBM-FBIS data, the word-based
HMM model achieves 76.60 F-measure when the
entity boundaries in the test data are given, and the
class-based model achieves 77.77 F-measure, higher
than the 77.19 F-measure by the character-based
model in Table 1. For the IEER data, the F-measure
of the word-based model improves from 70.83 to
73.74 when the entity boundaries are given, and the
class-based model improves from 70.20 to 72.47.
This suggests that with the improvement in Chi-
nese word segmentation, the word-based model may
achieve comparable or better performance than the
character-based model.
3.5 Lexical Features
Capitalization in English gives good evidence of
names. Our HMM classifier for English uses a set
of word-features to indicate whether a word con-
tains all capitalized letters, only digits, or capital-
ized letters and period, as described in (Bikel et al,
1999). However, Chinese does not have capitaliza-
tion. When we applied the HMM system to Chinese,
we retained such features since Chinese text also in-
clude digits and roman words (such as in product
or company names). In an attempt to investigate the
usefulness of such features for Chinese, we removed
them from the system and observed very little dif-
ference in overall performance (0.4 difference in F-
measure).
3.6 Sensitivity to Corpus and Training Size
Variation
To test the robustness of the model, we trained the
system on the 100,000 word IBM-CT data and tested
on the same IBM-FBIS data. The character-based
model achieves 61.36 F-measure and the word-
based model achieves 58.40 F-measure, compared
to 77.19 and 74.17, respectively, using the 20 times
larger IBM-FBIS training set. This represents an ap-
proximately 20% relative reduction in performance
when trained on a related yet different and consider-
ably smaller training set. We plan to investigate fur-
ther the relation between corpus type and size and
performance.
4 Classifier Combination
This section investigates the combination of a set of
classifiers for NE recognition. We first introduce the
classifiers used in our experiments and then describe
the combination methods.
4.1 The Classifiers
Besides the HMM classifier mentioned in the previ-
ous section, the following three classifiers were used
in the experiments.
4.1.1 The Transformation-Based Learning
(fnTBL) Classifier
Transformation-based learning is an error-driven
algorithm which has two major steps: it starts by
assigning some classification to each example, and
then automatically proposing, evaluating and select-
ing the classification changes that maximally de-
crease the number of errors.
TBL has some attractive qualities that make it
suitable for the language-related tasks: it can au-
tomatically integrate heterogeneous types of knowl-
edge, without the need for explicit modeling (similar
to Snow (Dagan et al, 1997), Maximum Entropy,
decision trees, etc); it is error?driven, thus directly
minimizes the ultimate evaluation measure: the er-
ror rate. The TBL toolkit used in this experiment is
described in (Florian and Ngai, 2001).
4.1.2 The Maximum Entropy Classifier
(MaxEnt)
The model used here is based on the maxi-
mum entropy model used for shallow parsing (Rat-
naparkhi, 1999). A sentence with NE tags is
converted into a shallow tree: tokens not in any
NE are assigned an ?O? tag, while tokens within
an NE are represented as constituents whose la-
bel is the same as the NE type. For exam-
ple, the annotated sentence ?I will fly to (LO-
CATION New York) (DATEREF tomorrow)? is
represented as a tree ?(S I/O will/O fly/O to/O
(LOCATION New/LOCATION York/LOCATION)
(DATEREF tomorrow/DATEREF) )?. Once an NE
is represented as a shallow tree, NE recognition can
be realized by performing shallow parsing.
We use the tagging and chunking model described
in (Ratnaparkhi, 1999) for shallow parsing. In the
tagging model, the context consists of a window of
five tokens (including the token being tagged and
two tokens to its left and two tokens to its right) and
two tags to the left of the current token. Five groups
of feature templates are used: token unigram, token
bigram, token trigram, tag unigram and tag bigram
(all within the context window). In the chunking
model, the context is limited to a window of three
subtrees: the previous, current and next subtree. Un-
igram and bigram chunk (or tag) labels are used as
features.
4.1.3 The Robust Risk Minimization (RRM)
Classifier
This system is a variant of the text chunking
system described in Zhang et al (2002), where the
NE recognition problem is regarded as a sequential
token-based tagging problem. We denote by Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 63?70,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Impact of Morphological Stemming on Arabic Mention
Detection and Coreference Resolution
Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, Radu Florian
{izitouni, sorenj, xiaoluo, raduf}@watson.ibm.com
IBM T.J. Watson Research Center
1101 Kitchawan Rd, Yorktown Heights, NY 10598, USA
Abstract
Arabic presents an interesting challenge to
natural language processing, being a highly
inflected and agglutinative language. In
particular, this paper presents an in-depth
investigation of the entity detection and
recognition (EDR) task for Arabic. We
start by highlighting why segmentation is
a necessary prerequisite for EDR, continue
by presenting a finite-state statistical seg-
menter, and then examine how the result-
ing segments can be better included into
a mention detection system and an entity
recognition system; both systems are statis-
tical, build around the maximum entropy
principle. Experiments on a clearly stated
partition of the ACE 2004 data show that
stem-based features can significantly im-
prove the performance of the EDT system
by 2 absolute F-measure points. The sys-
tem presented here had a competitive per-
formance in the ACE 2004 evaluation.
1 Introduction
Information extraction is a crucial step toward un-
derstanding and processing language. One goal of
information extraction tasks is to identify important
conceptual information in a discourse. These tasks
have applications in summarization, information re-
trieval (one can get al hits for Washington/person
and not the ones for Washington/state or Washing-
ton/city), data mining, question answering, language
understanding, etc.
In this paper we focus on the Entity Detection and
Recognition task (EDR) for Arabic as described in
ACE 2004 framework (ACE, 2004). The EDR has
close ties to the named entity recognition (NER) and
coreference resolution tasks, which have been the fo-
cus of several recent investigations (Bikel et al, 1997;
Miller et al, 1998; Borthwick, 1999; Mikheev et al,
1999; Soon et al, 2001; Ng and Cardie, 2002; Florian
et al, 2004), and have been at the center of evalu-
ations such as: MUC-6, MUC-7, and the CoNLL?02
and CoNLL?03 shared tasks. Usually, in computa-
tional linguistics literature, a named entity is an in-
stance of a location, a person, or an organization, and
the NER task consists of identifying each of these
occurrences. Instead, we will adopt the nomencla-
ture of the Automatic Content Extraction program
(NIST, 2004): we will call the instances of textual
references to objects/abstractions mentions, which
can be either named (e.g. John Mayor), nominal
(the president) or pronominal (she, it). An entity is
the aggregate of all the mentions (of any level) which
refer to one conceptual entity. For instance, in the
sentence
President John Smith said he has no com-
ments
there are two mentions (named and pronomial) but
only one entity, formed by the set {John Smith, he}.
We separate the EDR task into two parts: a men-
tion detection step, which identifies and classifies all
the mentions in a text ? and a coreference resolution
step, which combinines the detected mentions into
groups that refer to the same object. In its entirety,
the EDR task is arguably harder than traditional
named entity recognition, because of the additional
complexity involved in extracting non-named men-
tions (nominal and pronominal) and the requirement
of grouping mentions into entities. This is particu-
larly true for Arabic where nominals and pronouns
are also attached to the word they modify. In fact,
most Arabic words are morphologically derived from
a list of base forms or stems, to which prefixes and
suffixes can be attached to form Arabic surface forms
(blank-delimited words). In addition to the differ-
ent forms of the Arabic word that result from the
63
derivational and inflectional process, most preposi-
tions, conjunctions, pronouns, and possessive forms
are attached to the Arabic surface word. It is these
orthographic variations and complex morphological
structure that make Arabic language processing chal-
lenging (Xu et al, 2001; Xu et al, 2002).
Both tasks are performed with a statistical frame-
work: the mention detection system is similar to
the one presented in (Florian et al, 2004) and
the coreference resolution system is similar to the
one described in (Luo et al, 2004). Both systems
are built around from the maximum-entropy tech-
nique (Berger et al, 1996). We formulate the men-
tion detection task as a sequence classification prob-
lem. While this approach is language independent,
it must be modified to accomodate the particulars of
the Arabic language. The Arabic words may be com-
posed of zero or more prefixes, followed by a stem and
zero or more suffixes. We begin with a segmentation
of the written text before starting the classification.
This segmentation process consists of separating the
normal whitespace delimited words into (hypothe-
sized) prefixes, stems, and suffixes, which become the
subject of analysis (tokens). The resulting granular-
ity of breaking words into prefixes and suffixes allows
different mention type labels beyond the stem label
(for instance, in the case of nominal and pronominal
mentions). Additionally, because the prefixes and
suffixes are quite frequent, directly processing unseg-
mented words results in significant data sparseness.
We present in Section 2 the relevant particularities
of the Arabic language for natural language process-
ing, especially for the EDR task. We then describe
the segmentation system we employed for this task in
Section 3. Section 4 briefly describes our mention de-
tection system, explaining the different feature types
we use. We focus in particular on the stem n-gram,
prefix n-gram, and suffix n-gram features that are
specific to a morphologically rich language such as
Arabic. We describe in Section 5 our coreference
resolution system where we also describe the advan-
tage of using stem based features. Section 6 shows
and discusses the different experimental results and
Section 7 concludes the paper.
2 Why is Arabic Information
Extraction difficult?
The Arabic language, which is the mother tongue of
more than 300 million people (Center, 2000), present
significant challenges to many natural language pro-
cessing applications. Arabic is a highly inflected and
derived language. In Arabic morphology, most mor-
phemes are comprised of a basic word form (the root
or stem), to which many affixes can be attached to
form Arabic words. The Arabic alphabet consists
of 28 letters that can be extended to ninety by ad-
ditional shapes, marks, and vowels (Tayli and Al-
Salamah, 1990). Unlike Latin-based alphabets, the
orientation of writing in Arabic is from right to left.
In written Arabic, short vowels are often omitted.
Also, because variety in expression is appreciated
as part of a good writing style, the synonyms are
widespread. Arabic nouns encode information about
gender, number, and grammatical cases. There are
two genders (masculine and feminine), three num-
bers (singular, dual, and plural), and three gram-
matical cases (nominative, genitive, and accusative).
A noun has a nominative case when it is a subject,
accusative case when it is the object of a verb, and
genitive case when it is the object of a preposition.
The form of an Arabic noun is consequently deter-
mined by its gender, number, and grammatical case.
The definitive nouns are formed by attaching the
Arabic article ?

@ to the immediate front of the
nouns, such as in the word ??Q???

@ (the company).
Also, prepositions such as H. (by), and ? (to) can beattached as a prefix as in ??Q???? (to the company).
A noun may carry a possessive pronoun as a suffix,
such as in ?? D?Q?? (their company). For the EDR task,
in this previous example, the Arabic blank-delimited
word ?? D?Q?? should be split into two tokens: ??Q?? and
??. The first token ??Q?? is a mention that refers to
an organization, whereas the second token ?? is also
a mention, but one that may refer to a person. Also,
the prepositions (i.e., H. and ?) not be considered a
part of the mention.
Arabic has two kinds of plurals: broken plurals and
sound plurals (Wightwick and Gaafar, 1998; Chen
and Gey, 2002). The formation of broken plurals is
common, more complex and often irregular. As an
example, the plural form of the noun ?g. P (man) is
?A g. P (men), which is formed by inserting the infix
@. The plural form of the noun H. A
J? (book) is I. J?
(books), which is formed by deleting the infix @. The
plural form and the singular form may also be com-
pletely different (e.g. ?

@Q?@ for woman, but ZA
?	 for
women). The sound plurals are formed by adding
plural suffixes to singular nouns (e.g., IkAK. meaning
researcher): the plural suffix is H@ for feminine nouns
in grammatical cases (e.g., HA
JkAK.), 	?? for masculine
nouns in the nominative case (e.g., 	??JkAK.), and 	?K

for masculine nouns in the genitive and accusative
cases (e.g., 	?
JkAK.). The dual suffix is 	?@ for the nom-
inative case (e.g., 	?A
JkAK.), and 	?K
 for the genitive or
accusative (e.g., 	?
JkAK.).
Because we consider pronouns and nominals as men-
tions, it is essential to segment Arabic words into
these subword tokens. We also believe that the in-
64
formation denoted by these affixes can help with the
coreference resolution task1.
Arabic verbs have perfect and imperfect tenses (Ab-
bou and McCarus, 1983). Perfect tense denotes com-
pleted actions, while imperfect denotes ongoing ac-
tions. Arabic verbs in the perfect tense consist of a
stem followed by a subject marker, denoted as a suf-
fix. The subject marker indicates the person, gender,
and number of the subject. As an example, the verb
?K. A
? (to meet) has a perfect tense I?K. A
? for the third
person feminine singular, and @?

?K. A
? for the third per-
son masculine plural. We notice also that a verb with
a subject marker and a pronoun suffix can be by itself
a complete sentence, such us in the word ?? D?K. A
?: it
has a third-person feminine singular subject-markerH (she) and a pronoun suffix ?? (them). It is also
a complete sentence meaning ?she met them.? The
subject markers are often suffixes, but we may find
a subject marker as a combination of a prefix and a
suffix as in ???K. A
?K (she meets them). In this example,
the EDR system should be able to separate ???K. A
?K,
to create two mentions ( H and ??). Because the
two mentions belong to different entities, the EDR
system should not chain them together. An Arabic
word can potentially have a large number of vari-
ants, and some of the variants can be quite complex.
As an example, consider the word A ?D
JkAJ. ?? (and to
her researchers) which contains two prefixes and one
suffix ( A ? + ?

?kAK. + ? + ?).
3 Arabic Segmentation
Lee et al (2003) demonstrates a technique for seg-
menting Arabic text and uses it as a morphological
processing step in machine translation. A trigram
language model was used to score and select among
hypothesized segmentations determined by a set of
prefix and suffix expansion rules.
In our latest implementation of this algorithm, we
have recast this segmentation strategy as the com-
position of three distinct finite state machines. The
first machine, illustrated in Figure 1 encodes the pre-
fix and suffix expansion rules, producing a lattice of
possible segmentations. The second machine is a dic-
tionary that accepts characters and produces identi-
fiers corresponding to dictionary entries. The final
machine is a trigram language model, specifically a
Kneser-Ney (Chen and Goodman, 1998) based back-
off language model. Differing from (Lee et al, 2003),
we have also introduced an explicit model for un-
1As an example, we do not chain mentions with dif-
ferent gender, number, etc.
known words based upon a character unigram model,
although this model is dominated by an empirically
chosen unknown word penalty. Using 0.5M words
from the combined Arabic Treebanks 1V2, 2V2 and
3V1, the dictionary based segmenter achieves a exact
word match 97.8% correct segmentation.
SEP/epsilon
a/A#
epsilon/#
a/epsilon
a/epsilon
b/epsilon
b/B
UNK/epsilon
c/C
b/epsilon
c/BC
e/+E
epsilon/+
d/epsilon
d/epsilon
epsilon/epsilon
b/AB#
b/A#B#
e/+DE
c/epsilon d/BCD e/+D+E
Figure 1: Illustration of dictionary based segmenta-
tion finite state transducer
3.1 Bootstrapping
In addition to the model based upon a dictionary of
stems and words, we also experimented with models
based upon character n-grams, similar to those used
for Chinese segmentation (Sproat et al, 1996). For
these models, both arabic characters and spaces, and
the inserted prefix and suffix markers appear on the
arcs of the finite state machine. Here, the language
model is conditioned to insert prefix and suffix mark-
ers based upon the frequency of their appearance in
n-gram character contexts that appear in the train-
ing data. The character based model alone achieves
a 94.5% exact match segmentation accuracy, consid-
erably less accurate then the dictionary based model.
However, an analysis of the errors indicated that the
character based model is more effective at segment-
ing words that do not appear in the training data.
We seeked to exploit this ability to generalize to im-
prove the dictionary based model. As in (Lee et al,
2003), we used unsupervised training data which is
automatically segmented to discover previously un-
seen stems. In our case, the character n-gram model
is used to segment a portion of the Arabic Giga-
word corpus. From this, we create a vocabulary of
stems and affixes by requiring that tokens appear
more than twice in the supervised training data or
more than ten times in the unsupervised, segmented
corpus.
The resulting vocabulary, predominately of word
stems, is 53K words, or about six times the vo-
cabulary observed in the supervised training data.
This represents about only 18% of the total num-
ber of unique tokens observed in the aggregate
training data. With the addition of the automat-
ically acquired vocabulary, the segmentation accu-
racy achieves 98.1% exact match.
65
3.2 Preprocessing of Arabic Treebank Data
Because the Arabic treebank and the gigaword cor-
pora are based upon news data, we apply some
small amount of regular expression based preprocess-
ing. Arabic specific processing include removal of
the characters tatweel (), and vowels. Also, the fol-
lowing characters are treated as an equivalence class
during all lookups and processing: (1) ? ,?
 , and
(2)

@ , @ ,

@ ,

@. We define a token and introduce whites-
pace boundaries between every span of one or more
alphabetic or numeric characters. Each punctuation
symbol is considered a separate token. Character
classes, such as punctuation, are defined according
to the Unicode Standard (Aliprand et al, 2004).
4 Mention Detection
The mention detection task we investigate identifies,
for each mention, four pieces of information:
1. the mention type: person (PER), organiza-
tion (ORG), location (LOC), geopolitical en-
tity (GPE), facility (FAC), vehicle (VEH), and
weapon (WEA)
2. the mention level (named, nominal, pronominal,
or premodifier)
3. the mention class (generic, specific, negatively
quantified, etc.)
4. the mention sub-type, which is a sub-category
of the mention type (ACE, 2004) (e.g. OrgGov-
ernmental, FacilityPath, etc.).
4.1 System Description
We formulate the mention detection problem as a
classification problem, which takes as input seg-
mented Arabic text. We assign to each token in the
text a label indicating whether it starts a specific
mention, is inside a specific mention, or is outside
any mentions. We use a maximum entropy Markov
model (MEMM) classifier. The principle of maxi-
mum entropy states that when one searches among
probability distributions that model the observed
data (evidence), the preferred one is the one that
maximizes the entropy (a measure of the uncertainty
of the model) (Berger et al, 1996). One big advan-
tage of this approach is that it can combine arbitrary
and diverse types of information in making a classi-
fication decision.
Our mention detection system predicts the four la-
bels types associated with a mention through a cas-
cade approach. It first predicts the boundary and
the main entity type for each mention. Then, it uses
the information regarding the type and boundary in
different second-stage classifiers to predict the sub-
type, the mention level, and the mention class. Af-
ter the first stage, when the boundary (starting, in-
side, or outside a mention) has been determined, the
other classifiers can use this information to analyze
a larger context, capturing the patterns around the
entire mentions, rather than words. As an example,
the token sequence that refers to a mention will be-
come a single recognized unit and, consequently, lex-
ical and syntactic features occuring inside or outside
of the entire mention span can be used in prediction.
In the first stage (entity type detection and classifica-
tion), Arabic blank-delimited words, after segment-
ing, become a series of tokens representing prefixes,
stems, and suffixes (cf. section 2). We allow any
contiguous sequence of tokens can represent a men-
tion. Thus, prefixes and suffixes can be, and often
are, labeled with a different mention type than the
stem of the word that contains them as constituents.
4.2 Stem n-gram Features
We use a large set of features to improve the predic-
tion of mentions. This set can be partitioned into
4 categories: lexical, syntactic, gazetteer-based, and
those obtained by running other named-entity clas-
sifiers (with different tag sets). We use features such
as the shallow parsing information associated with
the tokens in a window of 3 tokens, POS, etc.
The context of a current token ti is clearly one of
the most important features in predicting whether ti
is a mention or not (Florian et al, 2004). We de-
note these features as backward token tri-grams and
forward token tri-grams for the previous and next
context of ti respectively. For a token ti, the back-
ward token n-gram feature will contains the previous
n ? 1 tokens in the history (ti?n+1, . . . ti?1) and the
forward token n-gram feature will contains the next
n ? 1 tokens (ti+1, . . . ti+n?1).
Because we are segmenting arabic words into
multiple tokens, there is some concern that tri-
gram contexts will no longer convey as much
contextual information. Consider the following
sentence extracted from the development set:
H. 	Qj?? ?
??A
J
??@ I. J?
??? Q
??? @ ?J??
 @
	Y? (transla-
tion ?This represents the location for Political
Party Office?). The ?Political Party Office? is
tagged as an organization and, as a word-for-word
translation, is expressed as ?to the Office of the
political to the party?. It is clear in this example
that the word Q?? (location for) contains crucial
information in distinguishing between a location
and an organization when tagging the token I. J?
?
66
(office). After segmentation, the sentence becomes:
+ I. J?
? + ?

@ + ? + Q?? + ?

@ + ?J? + ?
 + @
	Y?
.H. 	Qk + ?

@ + ? + ?
??A
J
? + ?

@
When predicting if the token I. J?
? (office) is the
beginning of an organization or not, backward and
forward token n-gram features contain only ?

@ + ?
(for the) and ?
??A
J
? + ?

@ (the political). This is
most likely not enough context, and addressing the
problem by increasing the size of the n-gram context
quickly leads to a data sparseness problem.
We propose in this paper the stem n-gram features as
additional features to the lexical set. If the current
token ti is a stem, the backward stem n-gram feature
contains the previous n ? 1 stems and the forward
stem n-gram feature will contain the following n? 1
stems. We proceed similarly for prefixes and suffixes:
if ti is a prefix (or suffix, respectively) we take the
previous and following prefixes (or suffixes)2. In the
sentence shown above, when the system is predict-
ing if the token I. J?
? (office) is the beginning of an
organization or not, the backward and forward stem
n-gram features contain Q?? ?J? (represent location
of) and H. 	Qk ?
??A
J
? (political office). The stem fea-
tures contain enough information in this example to
make a decision that I. J?
? (office) is the beginning of
an organization. In our experiments, n is 3, therefore
we use stem trigram features.
5 Coreference Resolution
Coreference resolution (or entity recognition) is de-
fined as grouping together mentions referring to the
same object or entity. For example, in the following
text,
(I) ?John believes Mary to be the best student?
three mentions ?John?, ?Mary?, ?student? are un-
derlined. ?Mary? and ?student? are in the same en-
tity since both refer to the same person.
The coreference system system is similar to the Bell
tree algorithm as described by (Luo et al, 2004).
In our implementation, the link model between a
candidate entity e and the current mention m is com-
puted as
PL(L = 1|e, m) ? maxmk?e P?L(L = 1|e, mk, m), (1)
2Thus, the difference to token n-grams is that the to-
kens of different type are removed from the streams, be-
fore the features are created.
where mk is one mention in entity e, and the basic
model building block P?L(L = 1|e, mk, m) is an ex-
ponential or maximum entropy model (Berger et al,
1996).
For the start model, we use the following approxima-
tion:
PS(S = 1|e1, e2, ? ? ? , et, m) ?
1 ? max
1?i?t
PL(L = 1|ei, m) (2)
The start model (cf. equation 2) says that the prob-
ability of starting a new entity, given the current
mention m and the previous entities e1, e2, ? ? ? , et, is
simply 1 minus the maximum link probability be-
tween the current mention and one of the previous
entities.
The maximum-entropy model provides us with a
flexible framework to encode features into the the
system. Our Arabic entity recognition system uses
many language-indepedent features such as strict
and partial string match, and distance features (Luo
et al, 2004). In this paper, however, we focus on the
addition of Arabic stem-based features.
5.1 Arabic Stem Match Feature
Features using the word context (left and right to-
kens) have been shown to be very helpful in corefer-
ence resolution (Luo et al, 2004). For Arabic, since
words are morphologically derived from a list of roots
(stems), we expected that a feature based on the
right and left stems would lead to improvement in
system accuracy.
Let m1 and m2 be two candidate mentions where
a mention is a string of tokens (prefixes, stems,
and suffixes) extracted from the segmented text.
In order to make a decision in either linking the
two mentions or not we use additional features
such as: do the stems in m1 and m2 match, do
stems in m1 match all stems in m2, do stems
in m1 partially match stems in m2. We proceed
similarly for prefixes and suffixes. Since prefixes and
suffixes can belong to different mention types, we
build a parse tree on the segmented text and we can
explore features dealing with the gender and number
of the token. In the following example, between
parentheses we make a word-for-word translations in
order to better explain our stemming feature. Let us
take the two mentions H. 	Qj?? ?
??A
J
??@ I. J?
???
(to-the-office the-politic to-the-party) and
?
G.
	Qm?'@ I. J?
? (office the-party?s) segmented as
H. 	Qk + ?

@ + ? + ?
??A
J
? + ?

@ + I. J?
? + ?

@ + ?
and ?
 + H. 	Qk + ?

@ + I. J?
? respectively. In our
67
development corpus, these two mentions are chained
to the same entity. The stemming match feature
in this case will contain information such us all
stems of m2 match, which is a strong indicator
that these mentions should be chained together.
Features based on the words alone would not help
this specific example, because the two strings m1
and m2 do not match.
6 Experiments
6.1 Data
The system is trained on the Arabic ACE 2003 and
part of the 2004 data. We introduce here a clearly
defined and replicable split of the ACE 2004 data,
so that future investigations can accurately and cor-
rectly compare against the results presented here.
There are 689 Arabic documents in LDC?s 2004 re-
lease (version 1.4) of ACE data from three sources:
the Arabic Treebank, a subset of the broadcast
(bnews) and newswire (nwire) TDT-4 documents.
The 178-document devtest is created by taking
the last (in chronological order) 25% of docu-
ments in each of three sources: 38 Arabic tree-
bank documents dating from ?20000715? (i.e., July
15, 2000) to ?20000815,? 76 bnews documents from
?20001205.1100.0489? (i.e., Dec. 05 of 2000 from
11:00pm to 04:89am) to ?20001230.1100.1216,? and
64 nwire documents from ?20001206.1000.0050? to
?20001230.0700.0061.? The time span of the test
set is intentionally non-overlapping with that of the
training set within each data source, as this models
how the system will perform in the real world.
6.2 Mention Detection
We want to investigate the usefulness of stem n-
gram features in the mention detection system. As
stated before, the experiments are run in the ACE?04
framework (NIST, 2004) where the system will iden-
tify mentions and will label them (cf. Section 4)
with a type (person, organization, etc), a sub-type
(OrgCommercial, OrgGovernmental, etc), a mention
level (named, nominal, etc), and a class (specific,
generic, etc). Detecting the mention boundaries (set
of consecutive tokens) and their main type is one of
the important steps of our mention detection sys-
tem. The score that the ACE community uses (ACE
value) attributes a higher importance (outlined by
its weight) to the main type compared to other sub-
tasks, such as the mention level and the class. Hence,
to build our mention detection system we spent a lot
of effort in improving the first step: detecting the
mention boundary and their main type. In this pa-
per, we report the results in terms of precision, recall,
and F-measure3.
Lexical features
Precision Recall F-measure
(%) (%) (%)
Total 73.3 58.0 64.7
FAC 76.0 24.0 36.5
GPE 79.4 65.6 71.8
LOC 57.7 29.9 39.4
ORG 63.1 46.6 53.6
PER 73.2 63.5 68.0
VEH 83.5 29.7 43.8
WEA 77.3 25.4 38.2
Lexical features + Stem
Precision Recall F-measure
(%) (%) (%)
Total 73.6 59.4 65.8
FAC 72.7 29.0 41.4
GPE 79.9 67.2 73.0
LOC 58.6 31.9 41.4
ORG 62.6 47.2 53.8
PER 73.8 64.6 68.9
VEH 81.7 35.9 49.9
WEA 78.4 29.9 43.2
Table 1: Performance of the mention detection sys-
tem using lexical features only.
To assess the impact of stemming n-gram features
on the system under different conditions, we consider
two cases: one where the system only has access to
lexical features (the tokens and direct derivatives in-
cluding standard n-gram features), and one where
the system has access to a richer set of information,
including lexical features, POS tags, text chunks,
parse tree, and gazetteer information. The former
framework has the advantage of being fast (making
it more appropriate for deployment in commercial
systems). The number of parameters to optimize in
the MaxEnt framework we use when only lexical fea-
tures are explored is around 280K parameters. This
number increases to 443K approximately when all in-
formation is used except the stemming feature. The
number of parameters introduced by the use of stem-
ming is around 130K parameters. Table 1 reports
experimental results using lexical features only; we
observe that the stemming n-gram features boost the
performance by one point (64.7 vs. 65.8). It is im-
portant to notice the stemming n-gram features im-
proved the performance of each category of the main
type.
In the second case, the systems have access to a large
amount of feature types, including lexical, syntac-
tic, gazetteer, and those obtained by running other
3The ACE value is an important factor for us, but its
relative complexity, due to different weights associated
with the subparts, makes for a hard comparison, while
the F-measure is relatively easy to interpret.
68
AllFeatures
Precision Recall F-measure
(%) (%) (%)
Total 74.3 64.0 68.8
FAC 72.3 36.8 48.8
GPE 80.5 70.8 75.4
LOC 61.1 35.4 44.8
ORG 61.4 50.3 55.3
PER 75.3 70.2 72.7
VEH 83.2 38.1 52.3
WEA 69.0 36.6 47.8
All-Features + Stem
Precision Recall F-measure
(%) (%) (%)
Total 74.4 64.6 69.2
FAC 68.8 38.5 49.4
GPE 80.8 71.9 76.1
LOC 60.2 36.8 45.7
ORG 62.2 51.0 56.1
PER 75.3 70.2 72.7
VEH 81.4 41.8 55.2
WEA 70.3 38.8 50.0
Table 2: Performance of the mention detection sys-
tem using lexical, syntactic, gazetteer features as well
as features obtained by running other named-entity
classifiers
named-entity classifiers (with different semantic tag
sets). Features are also extracted from the shal-
low parsing information associated with the tokens
in window of 3, POS, etc. The All-features system
incorporates all the features except for the stem n-
grams. Table 2 shows the experimental results with
and without the stem n-grams features. Again, Ta-
ble 2 shows that using stem n-grams features gave
a small boost to the whole main-type classification
system4. This is true for all types. It is interesting to
note that the increase in performance in both cases
(Tables 1 and 2) is obtained from increased recall,
with little change in precision. When the prefix and
suffix n-gram features are removed from the feature
set, we notice in both cases (Tables 1 and 2) a in-
significant decrease of the overall performance, which
is expected: what should a feature of preceeding (or
following) prepositions or finite articles captures?
As stated in Section 4.1, the mention detection sys-
tem uses a cascade approach. However, we were curi-
ous to see if the gain we obtained at the first level was
successfully transfered into the overall performance
of the mention detection system. Table 3 presents
the performance in terms of precision, recall, and F-
measure of the whole system. Despite the fact that
the improvement was small in terms of F-measure
(59.4 vs. 59.7), the stemming n-gram features gave
4The difference in performance is not statistically sig-
nificant
interesting improvement in terms of ACE value to
the hole EDR system as showed in section 6.3.
Precision Recall F-measure
(%) (%) (%)
All-Features 64.2 55.3 59.4
All-Features+Stem 64.4 55.7 59.7
Lexical 64.4 50.8 56.8
Lexical+Stem 64.6 52.0 57.6
Table 3: Performance of the mention detection sys-
tem including all ACE?04 subtasks
6.3 Coreference Resolution
In this section, we present the coreference results on
the devtest defined earlier. First, to see the effect of
stem matching features, we compare two coreference
systems: one with the stem features, the other with-
out. We test the two systems on both ?true? and
system mentions of the devtest set. ?True? men-
tions mean that input to the coreference system are
mentions marked by human, while system mentions
are output from the mention detection system. We
report results with two metrics: ECM-F and ACE-
Value. ECM-F is an entity-constrained mention F-
measure (cf. (Luo et al, 2004) for how ECM-F is
computed), and ACE-Value is the official ACE eval-
uation metric. The result is shown in Table 4: the
baseline numbers without stem features are listed un-
der ?Base,? and the results of the coreference system
with stem features are listed under ?Base+Stem.?
On true mention, the stem matching features im-
prove ECM-F from 77.7% to 80.0%, and ACE-value
from 86.9% to 88.2%. The similar improvement is
also observed on system mentions.The overall ECM-
F improves from 62.3% to 64.2% and the ACE value
improves from 61.9 to 63.1%. Note that the increase
on the ACE value is smaller than ECM-F. This is
because ACE-value is a weighted metric which em-
phasizes on NAME mentions and heavily discounts
PRONOUN mentions. Overall the stem features give
rise to consistent gain to the coreference system.
7 Conclusion
In this paper, we present a fully fledged Entity Detec-
tion and Tracking system for Arabic. At its base, the
system fundamentally depends on a finite state seg-
menter and makes good use of the relationships that
occur between word stems, by introducing features
which take into account the type of each segment.
In mention detection, the features are represented as
stem n-grams, while in coreference resolution they
are captured through stem-tailored match features.
69
Base Base+Stem
ECM-F ACEVal ECM-F ACEVal
Truth 77.7 86.9 80.0 88.2
System 62.3 61.9 64.2 63.1
Table 4: Effect of Arabic stemming features on coref-
erence resolution. The row marked with ?Truth?
represents the results with ?true? mentions while the
row marked with ?System? represents that mentions
are detected by the system. Numbers under ?ECM-
F? are Entity-Constrained-Mention F-measure and
numbers under ?ACE-Val? are ACE-values.
These types of features result in an improvement in
both the mention detection and coreference resolu-
tion performance, as shown through experiments on
the ACE 2004 Arabic data. The experiments are per-
formed on a clearly specified partition of the data, so
comparisons against the presented work can be cor-
rectly and accurately made in the future. In addi-
tion, we also report results on the official test data.
The presented system has obtained competitive re-
sults in the ACE 2004 evaluation, being ranked
amongst the top competitors.
8 Acknowledgements
This work was partially supported by the Defense
Advanced Research Projects Agency and monitored
by SPAWAR under contract No. N66001-99-2-8916.
The views and findings contained in this material are
those of the authors and do not necessarily reflect
the position of policy of the U.S. government and no
official endorsement should be inferred.
References
Peter F. Abbou and Ernest N. McCarus, editors. 1983.
Elementary modern standard Arabic. Cambridge Univer-
sity Press.
ACE. 2004. Automatic content extraction.
http://www.ldc.upenn.edu/Projects/ACE/.
Joan Aliprand, Julie Allen, Joe Becker, Mark Davis,
Michael Everson, Asmus Freytag, John Jenkins, Mike
Ksar, Rick McGowan, Eric Muller, Lisa Moore, Michel
Suignard, and Ken Whistler. 2004. The unicode stan-
dard. http://www.unicode.org/.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language process-
ing. Computational Linguistics, 22(1):39?71.
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-finder.
In Proceedings of ANLP-97, pages 194?201.
A. Borthwick. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, New York Uni-
versity.
Egyptian Demographic Center. 2000.
http://www.frcu.eun.eg/www/homepage/cdc/cdc.htm.
Aitao Chen and Fredic Gey. 2002. Building an arabic
stemmer for information retrieval. In Proceedings of the
Eleventh Text REtrieval Conference (TREC 2002), Na-
tional Institute of Standards and Technology, November.
S. F. Chen and J. Goodman. 1998. An empirical study
of smoothing techinques for language modeling. Techni-
cal Report TR-10-98, Center for Research in Comput-
ing Technology, Harvard University, Cambridge, Mas-
sachusettes, August.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A statisti-
cal model for multilingual entity detection and tracking.
In Proceedings of HLT-NAACL 2004, pages 1?8.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word segmen-
tation. In Proceedings of the ACL?03, pages 399?406.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based on
the bell tree. In Proc. of ACL?04.
A. Mikheev, M. Moens, and C. Grover. 1999. Named
entity recognition without gazetteers. In Proceedings of
EACL?99.
S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwarz,
R. Stone, and R. Weischedel. 1998. Bbn: Description of
the SIFT system as used for MUC-7. In MUC-7.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings of
the ACL?02, pages 104?111.
NIST. 2004. Proceedings of ace evaluation and pi meet-
ing 2004 workshop. Alexandria, VA, September. NIST.
W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
R. Sproat, C. Shih, W. Gale, and N. Chang. 1996. A
stochastic finite-state word-segmentation algorithm for
Chinese. Computational Linguistics, 22(3).
M. Tayli and A. Al-Salamah. 1990. Building bilingual
microcomputer systems. Communications of the ACM,
33(5):495?505.
J. Wightwick and M. Gaafar. 1998. Arabic Verbs and
Essentials of Grammar. Passport Books.
J. Xu, A. Fraser, and R. Weischedel. 2001. Trec2001
cross-lingual retrieval at bbn. In TREC 2001, Gaithers-
burg: NIST.
J. Xu, A. Fraser, and R. Weischedel. 2002. Empirical
studies in strategies for arabic information retrieval. In
SIGIR 2002, Tampere, Finland.
70
Transformation-Based Learning in the Fast Lane
Grace Ngai
y;z
and Radu Florian
y
{gyn,rorian}@cs.jhu.edu
y
Johns Hopkins University
Baltimore, MD 21218, USA
z
Weniwen Technologies
Hong Kong
Abstract
Transformation-based learning has been successfully
employed to solve many natural language process-
ing problems. It achieves state-of-the-art perfor-
mance on many natural language processing tasks
and does not overtrain easily. However, it does have
a serious drawback: the training time is often in-
torelably long, especially on the large corpora which
are often used in NLP. In this paper, we present a
novel and realistic method for speeding up the train-
ing time of a transformation-based learner without
sacricing performance. The paper compares and
contrasts the training time needed and performance
achieved by our modied learner with two other
systems: a standard transformation-based learner,
and the ICA system (Hepple, 2000). The results of
these experiments show that our system is able to
achieve a signicant improvement in training time
while still achieving the same performance as a stan-
dard transformation-based learner. This is a valu-
able contribution to systems and algorithms which
utilize transformation-based learning at any part of
the execution.
1 Introduction
Much research in natural language processing has
gone into the development of rule-based machine
learning algorithms. These algorithms are attractive
because they often capture the linguistic features of
a corpus in a small and concise set of rules.
Transformation-based learning (TBL) (Brill,
1995) is one of the most successful rule-based ma-
chine learning algorithms. It is a exible method
which is easily extended to various tasks and do-
mains, and it has been applied to a wide variety of
NLP tasks, including part of speech tagging (Brill,
1995), noun phrase chunking (Ramshaw and Mar-
cus, 1999), parsing (Brill, 1996), phrase chunking
(Florian et al, 2000), spelling correction (Mangu
and Brill, 1997), prepositional phrase attachment
(Brill and Resnik, 1994), dialog act tagging (Samuel
et al, 1998), segmentation and message understand-
ing (Day et al, 1997). Furthermore, transformation-
based learning achieves state-of-the-art performance
on several tasks, and is fairly resistant to overtrain-
ing (Ramshaw and Marcus, 1994).
Despite its attractive features as a machine learn-
ing algorithm, TBL does have a serious draw-
back in its lengthy training time, especially on the
larger-sized corpora often used in NLP tasks. For
example, a well-implemented transformation-based
part-of-speech tagger will typically take over 38
hours to nish training on a 1 million word cor-
pus. This disadvantage is further exacerbated when
the transformation-based learner is used as the base
learner in learning algorithms such as boosting or
active learning, both of which require multiple it-
erations of estimation and application of the base
learner. In this paper, we present a novel method
which enables a transformation-based learner to re-
duce its training time dramatically while still retain-
ing all of its learning power. In addition, we will
show that our method scales better with training
data size.
2 Transformation-based Learning
The central idea of transformation-based learning
(TBL) is to learn an ordered list of rules which
progressively improve upon the current state of the
training set. An initial assignment is made based on
simple statistics, and then rules are greedily learned
to correct the mistakes, until no net improvement
can be made.
The following denitions and notations will be
used throughout the paper:
 The sample space is denoted by S;
 C denotes the set of possible classications of
the samples;
 C[s] denotes the classication associated with a
sample s, and T [s] denotes the true classica-
tion of s;
 p will usually denote a predicate dened on S;
 A rule r is dened as a predicate - class label
pair, (p; t), where t 2 C is called the target of r;
 R denotes the set of all rules;
 If r = (p; t), p
r
will denote p and t
r
will denote
t;
 A rule r = (p
r
; t
r
) applies to a sample s if
p
r
(s) = true and t
r
6= C[s]; the resulting sam-
ple is denoted by r(s).
Using the TBL framework to solve a problem as-
sumes the existence of:
 An initial class assignment. This can be as sim-
ple as the most common class label in the train-
ing set, or it can be the output of another clas-
sier.
 A set of allowable templates for rules. These
templates determine the types of predicates the
rules will test; they have the largest impact on
the behavior of the system.
 An objective function f for learning. Unlike
in many other learning algorithms, the objec-
tive function for TBL will directly optimize the
evaluation function. A typical example is the
dierence in performance resulting from apply-
ing the rule:
f (r) = good (r)   bad (r)
where
good (r) = jfsjC [s] 6= T [s] ^ C [r (s)] = T [s]gj
bad (r) = jfsjC [s] = T [s] ^ C [r (s)] 6= T [s]gj
Since we are not interested in rules that have a nega-
tive objective function value, only the rules that have
a positive good (r) need be examined. This leads to
the following approach:
1. Generate the rules (using the rule template set)
that correct at least an error (i.e. good (r) > 0),
by examining all the incorrect samples (s s.t.
C [s] 6= T [s]);
2. Compute the values bad () for each rule r such
that good(r) > f(b) , storing at each point in
time the rule b that has the highest score; while
computing bad(r), skip to the next rule when
f (r) < f (b)
The system thus learns a list of rules in a greedy
fashion, according to the objective function. When
no rule that improves the current state of the train-
ing set beyond a pre-set threshold can be found, the
training phase ends. During the application phase,
the evaluation set is initialized with the initial class
assignment. The rules are then applied sequentially
to the evaluation set in the order they were learned.
The nal classication is the one attained when all
rules have been applied.
2.1 Previous Work
As was described in the introductory section, the
long training time of TBL poses a serious prob-
lem. Various methods have been investigated to-
wards ameliorating this problem, and the following
subsections detail two of the approaches.
2.1.1 The Ramshaw & Marcus Approach
One of the most time-consuming steps in
transformation-based learning is the updating
step. The iterative nature of the algorithm requires
that each newly selected rule be applied to the
corpus, and the current state of the corpus updated
before the next rule is learned.
Ramshaw & Marcus (1994) attempted to reduce
the training time of the algorithm by making the up-
date process more ecient. Their method requires
each rule to store a list of pointers to samples that
it applies to, and for each sample to keep a list of
pointers to rules that apply to it. Given these two
sets of lists, the system can then easily:
1. identify the positions where the best rule applies
in the corpus; and
2. update the scores of all the rules which are af-
fected by a state change in the corpus.
These two processes are performed multiple times
during the update process, and the modication re-
sults in a signicant reduction in running time.
The disadvantage of this method consists in the
system having an unrealistically high memory re-
quirement. For example, a transformation-based
text chunker training upon a modestly-sized corpus
of 200,000 words has approximately 2 million rules
active at each iteration. The additional memory
space required to store the lists of pointers associ-
ated with these rules is about 450 MB, which is a
rather large requirement to add to a system.
1
2.1.2 The ICA Approach
The ICA system (Hepple, 2000) aims to reduce the
training time by introducing independence assump-
tions on the training samples that dramatically re-
duce the training time with the possible downside of
sacricing performance.
To achieve the speedup, the ICA system disallows
any interaction between the learned rules, by enforc-
ing the following two assumptions:
 Sample Independence  a state change in a
sample (e.g. a change in the current part-
of-speech tag of a word) does not change the
context of surrounding samples. This is cer-
tainly the case in tasks such as prepositional
phrase attachment, where samples are mutually
independent. Even for tasks such as part-of-
speech tagging where intuition suggests it does
not hold, it may still be a reasonable assump-
tion to make if the rules apply infrequently and
sparsely enough.
1
We need to note that the 200k-word corpus used in this
experiment is considered small by NLP standards. Many of
the available corpora contain over 1 million words. As the
size of the corpus increases, so does the number of rules and
the additional memory space required.
 Rule Commitment  there will be at most one
state change per sample. In other words, at
most one rule is allowed to apply to each sample.
This mode of application is similar to that of a
decision list (Rivest, 1987), where an sample is
modied by the rst rule that applies to it, and
not modied again thereafter. In general, this
assumption will hold for problems which have
high initial accuracy and where state changes
are infrequent.
The ICA system was designed and tested on the
task of part-of-speech tagging, achieving an impres-
sive reduction in training time while suering only
a small decrease in accuracy. The experiments pre-
sented in Section 4 include ICA in the training time
and performance comparisons
2
.
2.1.3 Other Approaches
Samuel (1998) proposed a Monte Carlo approach
to transformation-based learning, in which only a
fraction of the possible rules are randomly selected
for estimation at each iteration. The -TBL sys-
tem described in Lager (1999) attempts to cut down
on training time with a more ecient Prolog imple-
mentation and an implementation of lazy learning.
The application of a transformation-based learning
can be considerably sped-up if the rules are compiled
in a nite-state transducer, as described in Roche
and Schabes (1995).
3 The Algorithm
The approach presented here builds on the same
foundation as the one in (Ramshaw and Marcus,
1994): instead of regenerating the rules each time,
they are stored into memory, together with the two
values good (r) and bad (r).
The following notations will be used throughout
this section:
 G (r) = fs 2 Sjp
r
(s) = true and C[s] 6=
t
r
and t
r
= T [s]g  the samples on which the
rule applies and changes them to the correct
classication; therefore, good(r) = jG(r)j.
 B (r) = fs 2 Sjp
r
(s) = true and C[s] 6=
t
r
and C[s] = T [s]g  the samples on which
the rule applies and changes the classication
from correct to incorrect; similarly, bad(r) =
jB(r)j.
Given a newly learned rule b that is to be applied
to S, the goal is to identify the rules r for which at
least one of the sets G (r) ; B (r) is modied by the
application of rule b. Obviously, if both sets are not
modied when applying rule b, then the value of the
objective function for rule r remains unchanged.
2
The algorithm was implemented by the the authors, fol-
lowing the description in Hepple (2000).
The presentation is complicated by the fact that,
in many NLP tasks, the samples are not indepen-
dent. For instance, in POS tagging, a sample is de-
pendent on the classication of the preceding and
succeeding 2 samples (this assumes that there ex-
ists a natural ordering of the samples in S). Let
V (s) denote the vicinity of a sample  the set of
samples on whose classication the sample s might
depend on (for consistency, s 2 V (s)); if samples are
independent, then V (s) = fsg.
3.1 Generating the Rules
Let s be a sample on which the best rule b applies
(i.e. [b (s)] 6= C [s]). We need to identify the rules
r that are inuenced by the change s ! b (s). Let
r be such a rule. f (r) needs to be updated if and
only if there exists at least one sample s
0
such that
s
0
2 G (r) and b (s
0
) =2 G (r) or (1)
s
0
2 B (r) and b (s
0
) =2 B (r) or (2)
s
0
=2 G (r) and b (s
0
) 2 G (r) or (3)
s
0
=2 B (r) and b (s
0
) 2 B (r) (4)
Each of the above conditions corresponds to a spe-
cic update of the good (r) or bad (r) counts. We
will discuss how rules which should get their good or
bad counts decremented (subcases (1) and (2)) can
be generated, the other two being derived in a very
similar fashion.
The key observation behind the proposed algo-
rithm is: when investigating the eects of applying
the rule b to sample s, only samples s
0
in the set
V (s) need to be checked. Any sample s
0
that is not
in the set
[
fsjb changes sg
V (s)
can be ignored since s
0
= b(s
0
).
Let s
0
2 V (s) be a sample in the vicinity of s.
There are 2 cases to be examined  one in which b
applies to s
0
and one in which b does not:
Case I: c (s
0
) = c (b (s
0
)) (b does not modify the
classication of sample s
0
). We note that the
condition
s
0
2 G (r) and b (s
0
) =2 G (r)
is equivalent to
p
r
(s
0
) = true ^ C [s
0
] 6= t
r
^
t
r
= T [s
0
] ^ p
r
(b (s
0
)) = false
(5)
and the formula
s
0
2 B (r) and b (s
0
) =2 B (r)
is equivalent to
p
r
(s
0
) = true ^ C [s
0
] 6= t
r
^
C [s
0
] = T [s
0
] ^ p
r
(b (s
0
)) = false
(6)
(for the full details of the derivation, inferred from
the denition of G (r) and B (r), please refer to
Florian and Ngai (2001)).
These formulae oer us a method of generating
the rules r which are inuenced by the modication
s
0
! b (s
0
):
1. Generate all predicates p (using the predicate
templates) that are true on the sample s
0
.
2. If C [s
0
] 6= T [s
0
] then
(a) If p (b (s
0
)) = false then decrease good (r),
where r is the rule created with predicate
p s.t. target T [s
0
];
3. Else
(a) If p (b (s
0
)) = false then for all the rules
r whose predicate is p
3
and t
r
6= C [s
0
] de-
crease bad (r);
The algorithm for generating the rules r that need
their good counts (formula (3)) or bad counts (for-
mula (4)) increased can be obtained from the formu-
lae (1) (respectively (2)), by switching the states s
0
and b (s
0
), and making sure to add all the new pos-
sible rules that might be generated (only for (3)).
Case II: C [s
0
] 6= C [b (s
0
)] (b does change the clas-
sication of sample s
0
). In this case, the formula (5)
is transformed into:
p
r
(s
0
) = true ^ C [s
0
] 6= t
r
^ t
r
= T [s
0
] ^
(p
r
(b (s
0
)) = false _ t
r
= C [b (s
0
)])
(7)
(again, the full derivation is presented in Florian and
Ngai (2001)). The case of (2), however, is much
simpler. It is easy to notice that C [s
0
] 6= C [b (s
0
)]
and s
0
2 B (r) implies that b (s
0
) =2 B (r); indeed,
a necessary condition for a sample s
0
to be in a set
B (r) is that s
0
is classied correctly, C [s
0
] = T [s
0
].
Since T [s
0
] 6= C [b (s
0
)], results C [b (s
0
)] 6= T [s
0
] and
therefore b (s
0
) =2 B (r). Condition (3) is, therefore,
equivalent to
p
r
(s
0
) = true ^ C [s
0
] 6= t
r
^ C [s
0
] = T [s
0
]
(8)
The algorithm is modied by replacing the test
p (b (s
0
)) = false with the test p
r
(b (s
0
)) = false _
C [b (s)] = t
r
in formula (1) and removing the test
altogether for case of (2). The formulae used to gen-
erate rules r that might have their counts increased
(equations (3) and (4)) are obtained in the same
fashion as in Case I.
3.2 The Full Picture
At every point in the algorithm, we assumed that all
the rules that have at least some positive outcome
(good (r) > 0) are stored, and their score computed.
3
This can be done eciently with an appropriate data
structure - for example, using a double hash.
For all samples s that satisfy C [s] 6= T [s], generate all rules
r that correct the classication of s; increase good (r).
For all samples s that satisfy C [s] = T [s] generate all pred-
icates p s.t. p (s) = true; for each rule r s.t. p
r
= p and
t
r
6= C [s] increase bad (r).
1: Find the rule b = argmax
r2R
f (r).
If (f (b) < Threshold or corpus learned to completion) then
quit.
For each predicate p, let R (p) be the rules whose predicate
is p (p
r
= r).
For each samples s; s
0
s.t. C [s] 6= C [b (s)] and s
0
2 V (s):
If C [s
0
] = C [b (s
0
)] then
 for each predicate p s.t. p (s
0
) = true
 If C [s
0
] 6= T [s
0
] then
 If p (b (s
0
)) = false then decrease good (r),
where r = [p; T [s
0
]], the rule created with
predicate p and target T [s
0
];
Else
 If p (b (s
0
)) = false then for all the rules
r 2 R (p) s.t. t
r
6= C [s
0
] decrease bad (r);
 for each predicate p s.t. p (b (s
0
)) = true
 If C [b (s
0
)] 6= T [s
0
] then
 If p (s
0
) = false then increase good (r),
where r = [p; T [s
0
]];
Else
 If p (s
0
) = false then for all rules r 2 R (p)
s.t. t
r
6= C [b (s
0
)] increase bad (r);
Else
 for each predicate p s.t. p (s
0
) = true
 If C [s
0
] 6= T [s
0
] then
 If p (b (s
0
)) = false _ C [b (s
0
)] = t
r
then
decrease good (r), where r = [p; T [s
0
]];
Else
For all the rules r 2 R(p) s.t. t
r
6= C [s
0
]
decrease bad (r);
 for each predicate p s.t. p (b (s
0
)) = true
 If C [b (s
0
)] 6= T [s
0
] then
 If p (s
0
) = false _ C [s
0
] = t
r
then increase
good (r), where r = [p; T [s
0
]];
Else
For all rules r 2 R (p) s.t. t
r
6= C [b (s
0
)]
increase bad (r);
Repeat from step 1:
Figure 1: FastTBL Algorithm
Therefore, at the beginning of the algorithm, all the
rules that correct at least one wrong classication
need to be generated. The bad counts for these rules
are then computed by generation as well: in every
position that has the correct classication, the rules
that change the classication are generated, as in
Case 4, and their bad counts are incremented. The
entire FastTBL algorithm is presented in Figure 1.
Note that, when the bad counts are computed, only
rules that already have positive good counts are se-
lected for evaluation. This prevents the generation
of useless rules and saves computational time.
The number of examined rules is kept close to the
minimum. Because of the way the rules are gen-
erated, most of them need to modify either one of
their counts. Some additional space (besides the one
needed to represent the rules) is necessary for repre-
senting the rules in a predicate hash  in order to
have a straightforward access to all rules that have a
given predicate; this amount is considerably smaller
than the one used to represent the rules. For exam-
ple, in the case of text chunking task described in
section 4, only approximately 30Mb additional mem-
ory is required, while the approach of Ramshaw and
Marcus (1994) would require approximately 450Mb.
3.3 Behavior of the Algorithm
As mentioned before, the original algorithm has a
number of deciencies that cause it to run slowly.
Among them is the drastic slowdown in rule learning
as the scores of the rules decrease. When the best
rule has a high score, which places it outside the tail
of the score distribution, the rules in the tail will be
skipped when the bad counts are calculated, since
their good counts are small enough to cause them
to be discarded. However, when the best rule is in
the tail, many other rules with similar scores can no
longer be discarded and their bad counts need to be
computed, leading to a progressively longer running
time per iteration.
Our algorithm does not suer from the same prob-
lem, because the counts are updated (rather than
recomputed) at each iteration, and only for the sam-
ples that were aected by the application of the lat-
est rule learned. Since the number of aected sam-
ples decreases as learning progresses, our algorithm
actually speeds up considerably towards the end of
the training phase. Considering that the number
of low-score rules is a considerably higher than the
number of high-score rules, this leads to a dramatic
reduction in the overall running time.
This has repercussions on the scalability of the al-
gorithm relative to training data size. Since enlarg-
ing the training data size results in a longer score dis-
tribution tail, our algorithm is expected to achieve
an even more substantial relative running time im-
provement over the original algorithm. Section 4
presents experimental results that validate the su-
perior scalability of the FastTBL algorithm.
4 Experiments
Since the goal of this paper is to compare and con-
trast system training time and performance, extra
measures were taken to ensure fairness in the com-
parisons. To minimize implementation dierences,
all the code was written in C++ and classes were
shared among the systems whenever possible. For
each task, the same training set was provided to each
system, and the set of possible rule templates was
kept the same. Furthermore, extra care was taken
to run all comparable experiments on the same ma-
chine and under the same memory and processor
load conditions.
To provide a broad comparison between the sys-
tems, three NLP tasks with dierent properties
were chosen as the experimental domains. The
rst task, part-of-speech tagging, is one where the
commitment assumption seems intuitively valid and
the samples are not independent. The second
task, prepositional phrase attachment, has examples
which are independent from each other. The last
task is text chunking, where both independence and
commitment assumptions do not seem to be valid.
A more detailed description of each task, data and
the system parameters are presented in the following
subsections.
Four algorithms are compared during the follow-
ing experiments:
 The regular TBL, as described in section 2;
 An improved version of TBL, which makes ex-
tensive use of indexes to speed up the rules' up-
date;
 The FastTBL algorithm;
 The ICA algorithm (Hepple, 2000).
4.1 Part-of-Speech Tagging
The goal of this task is to assign to each word
in the given sentence a tag corresponding to its
part of speech. A multitude of approaches have
been proposed to solve this problem, including
transformation-based learning, Maximum Entropy
models, Hidden Markov models and memory-based
approaches.
The data used in the experiment was selected from
the Penn Treebank Wall Street Journal, and is the
same used by Brill and Wu (1998). The training set
contained approximately 1M words and the test set
approximately 200k words.
Table 1 presents the results of the experiment
4
.
All the algorithms were trained until a rule with
a score of 2 was reached. The FastTBL algorithm
performs very similarly to the regular TBL, while
running in an order of magnitude faster. The two
assumptions made by the ICA algorithm result in
considerably less training time, but the performance
is also degraded (the dierence in performance is sta-
tistically signicant, as determined by a signed test,
at a signicance level of 0:001). Also present in Ta-
ble 1 are the results of training Brill's tagger on the
same data. The results of this tagger are presented
to provide a performance comparison with a widely
used tagger. Also worth mentioning is that the tag-
ger achieved an accuracy of 96:76% when trained on
the entire data
5
; a Maximum Entropy tagger (Rat-
naparkhi, 1996) achieves 96:83% accuracy with the
same training data/test data.
4
The time shown is the combined running time for both
the lexical tagger and the contextual tagger.
5
We followed the setup from Brill's tagger: the contextual
tagger is trained only on half of the training data. The train-
ing time on the entire data was approximately 51 minutes.
Brill's tagger Regular TBL Indexed TBL FastTBL ICA (Hepple)
Accuracy 96:61% 96:61% 96:61% 96:61% 96:23%
Running time 5879 mins, 46 secs 2286 mins, 21 secs 420 mins, 7 secs 17 mins, 21 secs 6 mins, 13 secs
Time ratio 0:4 1:0 5:4 131:7 367:8
Table 1: POS tagging: Evaluation and Running Times
Regular TBL Indexed TBL Fast TBL ICA (Hepple)
Accuracy 81:0% 81:0% 81:0% 77:8%
Running time 190 mins, 19 secs 65 mins, 50 secs 14 mins, 38 secs 4 mins, 1 sec
Time Ratio 1:0 2:9 13 47:4
Table 2: PP Attachment:Evaluation and Running Times
4.2 Prepositional Phrase Attachment
Prepositional phrase attachment is the task of decid-
ing the point of attachment for a given prepositional
phrase (PP). As an example, consider the following
two sentences:
1. I washed the shirt with soap and water.
2. I washed the shirt with pockets.
In Sentence 1, the PP with soap and water de-
scribes the act of washing the shirt. In Sentence 2,
however, the PP with pockets is a description for
the shirt that was washed.
Most previous work has concentrated on situa-
tions which are of the form VP NP1 P NP2. The
problem is cast as a classication task, and the sen-
tence is reduced to a 4-tuple containing the preposi-
tion and the non-inected base forms of the head
words of the verb phrase VP and the two noun
phrases NP1 and NP2. For example, the tuple cor-
responding to the two above sentences would be:
1. wash shirt with soap
2. wash shirt with pocket
Many approaches to solving this this problem have
been proposed, most of them using standard ma-
chine learning techniques, including transformation-
based learning, decision trees, maximum entropy
and backo estimation. The transformation-based
learning system was originally developed by Brill
and Resnik (1994).
The data used in the experiment consists of ap-
proximately 13,000 quadruples (VP NP1 P NP2 )
extracted from Penn Treebank parses. The set is
split into a test set of 500 samples and a training set
of 12,500 samples. The templates used to generate
rules are similar to the ones used by Brill and Resnik
(1994) and some include WordNet features. All the
systems were trained until no more rules could be
learned.
Table 2 shows the results of the experiments.
Again, the ICA algorithm learns the rules very fast,
but has a slightly lower performance than the other
two TBL systems. Since the samples are inherently
independent, there is no performance loss because
of the independence assumption; therefore the per-
formance penalty has to come from the commitment
assumption. The Fast TBL algorithm runs, again,
in a order of magnitude faster than the original TBL
while preserving the performance; the time ratio is
only 13 in this case due to the small training size
(only 13000 samples).
4.3 Text Chunking
Text chunking is a subproblem of syntactic pars-
ing, or sentence diagramming. Syntactic parsing at-
tempts to construct a parse tree from a sentence by
identifying all phrasal constituents and their attach-
ment points. Text chunking simplies the task by
dividing the sentence into non-overlapping phrases,
where each word belongs to the lowest phrasal con-
stituent that dominates it. The following exam-
ple shows a sentence with text chunks and part-of-
speech tags:
[NP A.P.
NNP
Green
NNP
] [ADVP
currently
RB
] [VP has ] [NP 2,664,098
CD
shares
NNS
] [ADJP outstanding
JJ
] .
The problem can be transformed into a classication
task. Following Ramshaw & Marcus' (1999) work in
base noun phrase chunking, each word is assigned
a chunk tag corresponding to the phrase to which
it belongs . The following table shows the above
sentence with the assigned chunk tags:
Word POS tag Chunk Tag
A.P. NNP B-NP
Green NNP I-NP
currently RB B-ADVP
has VBZ B-VP
2,664,098 CD B-NP
shares NNS I-NP
outstanding JJ B-ADJP
. . O
The data used in this experiment is the CoNLL-
2000 phrase chunking corpus (Tjong Kim Sang and
Buchholz, 2000). The training corpus consists of
sections 15-18 of the Penn Treebank (Marcus et al,
1993); section 20 was used as the test set. The chunk
tags are derived from the parse tree constituents,
Regular TBL Indexed TBL Fast TBL ICA (Hepple)
F-measure 92.30 92.30 92.30 86.20
Running Time 19211 mins, 40 secs 2056 mins, 4secs 137 mins, 57 secs 12 mins, 40 secs
Time Ratio 1:0 9:3 139:2 1516:7
Table 3: Text Chunking: Evaluation and Running Times
and the part-of-speech tags were generated by Brill's
tagger (Brill, 1995). All the systems are trained to
completion (until all the rules are learned).
Table 3 shows the results of the text chunking ex-
periments. The performance of the FastTBL algo-
rithm is the same as of regular TBL's, and runs in an
order of magnitude faster. The ICA algorithm again
runs considerably faster, but at a cost of a signi-
cant performance hit. There are at least 2 reasons
that contribute to this behavior:
1. The initial state has a lower performance than
the one in tagging; therefore the independence
assumption might not hold. 25% of the samples
are changed by at least one rule, as opposed to
POS tagging, where only 2.5% of the samples
are changed by a rule.
2. The commitment assumption might also not
hold. For this task, 20% of the samples that
were modied by a rule are also changed again
by another one.
4.4 Training Data Size Scalability
A question usually asked about a machine learning
algorithm is how well it adapts to larger amounts
of training data. Since the performance of the Fast
TBL algorithm is identical to that of regular TBL,
the issue of interest is the dependency between the
running time of the algorithm and the amount of
training data.
The experiment was performed with the part-of-
speech data set. The four algorithms were trained
on training sets of dierent sizes; training times were
recorded and averaged over 4 trials. The results are
presented in Figure 2(a). It is obvious that the Fast
TBL algorithm is much more scalable than the reg-
ular TBL  displaying a linear dependency on the
amount of training data, while the regular TBL has
an almost quadratic dependency. The explanation
for this behavior has been given in Section 3.3.
Figure 2(b) shows the time spent at each iteration
versus the iteration number, for the original TBL
and fast TBL systems. It can be observed that the
time taken per iteration increases dramatically with
the iteration number for the regular TBL, while for
the FastTBL, the situation is reversed. The con-
sequence is that, once a certain threshold has been
reached, the incremental time needed to train the
FastTBL system to completion is negligible.
5 Conclusions
We have presented in this paper a new and im-
proved method of computing the objective function
for transformation-based learning. This method al-
lows a transformation-based algorithm to train an
observed 13 to 139 times faster than the original
one, while preserving the nal performance of the
algorithm. The method was tested in three dier-
ent domains, each one having dierent characteris-
tics: part-of-speech tagging, prepositional phrase at-
tachment and text chunking. The results obtained
indicate that the algorithmic improvement gener-
ated by our method is not linked to a particular
task, but extends to any classication task where
transformation-based learning can be applied. Fur-
thermore, our algorithm scales better with training
data size; therefore the relative speed-up obtained
will increase when more samples are available for
training, making the procedure a good candidate for
large corpora tasks.
The increased speed of the Fast TBL algorithm
also enables its usage in higher level machine learn-
ing algorithms, such as adaptive boosting, model
combination and active learning. Recent work (Flo-
rian et al, 2000) has shown how a TBL frame-
work can be adapted to generate condences on the
output, and our algorithm is compatible with that
framework. The stability, resistance to overtraining,
the existence of probability estimates and, now, rea-
sonable speed make TBL an excellent candidate for
solving classication tasks in general.
6 Acknowledgements
The authors would like to thank David Yarowsky
for his advice and guidance, Eric Brill and John
C. Henderson for discussions on the initial ideas of
the material presented in the paper, and the anony-
mous reviewers for useful suggestions, observations
and connections with other published material. The
work presented here was supported by NSF grants
IRI-9502312, IRI-9618874 and IIS-9985033.
References
E. Brill and P. Resnik. 1994. A rule-based ap-
proach to prepositional phrase attachment disam-
biguation. In Proceedings of the Fifteenth Interna-
tional Conference on Computational Linguistics
(COLING-1994), pages 11981204, Kyoto.
E. Brill and J. Wu. 1998. Classier combination for
05000
10000
15000
20000
25000
100000 150000 200000 250000 300000 350000 400000 450000 500000 550000
Ru
nn
in
g 
Ti
m
e (
mi
nu
tes
)
Training Set Size (words)
ICA FastTBL
Indexed TBL
Regular TBL
(a) Running Time versus Training Data Size
Ru
nn
in
g 
Ti
m
e (
sec
on
ds
)
Iteration Number
Regular TBL
FastTBL
Indexed TBL
0
500
1000
1500
2000
0 200 400 600 800 1000
(b) Running Time versus Iteration Number
Figure 2: Algorithm Scalability
improved lexical disambiguation. Proceedings of
COLING-ACL'98, pages 191195, August.
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational
Linguistics, 21(4):543565.
E. Brill, 1996. Recent Advances in Parsing Technol-
ogy, chapter Learning to Parse with Transforma-
tions. Kluwer.
D. Day, J. Aberdeen, L. Hirschman, R. Kozierok,
P. Robinson, and M. Vilain. 1997. Mixed-
initiative development of language processing sys-
tems. In Fifth Conference on Applied Natural
Language Processing, pages 348355. Association
for Computational Linguistics, March.
R. Florian and G. Ngai. 2001. Transformation-
based learning in the fast lane. Technical report,
Johns Hopkins University, Computer Science De-
partment.
R. Florian, J.C. Henderson, and G. Ngai. 2000.
Coaxing condence from an old friend: Probabilis-
tic classications from transformation rule lists.
In Proceedings of SIGDAT-EMNLP 2000, pages
2643, Hong Kong, October.
M. Hepple. 2000. Independence and commitment:
Assumptions for rapid training and execution of
rule-based pos taggers. In Proceedings of the 38th
Annual Meeting of the ACL, pages 278285, Hong
Kong, October.
T. Lager. 1999. The -tbl system: Logic pro-
gramming tools for transformation-based learn-
ing. In Proceedings of the 3rd International Work-
shop on Computational Natural Language Learn-
ing, Bergen.
L. Mangu and E. Brill. 1997. Automatic rule acqui-
sition for spelling correction. In Proceedings of the
Fourteenth International Conference on Machine
Learning, pages 734741, Nashville, Tennessee.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large an-
notated corpus of english: The Penn Treebank.
Computational Linguistics, 19(2):313330.
L. Ramshaw and M. Marcus. 1994. Exploring the
statistical derivation of transformational rule se-
quences for part-of-speech tagging. In The Bal-
ancing Act: Proceedings of the ACL Workshop on
Combining Symbolic and Statistical Approaches to
Language, pages 128135, New Mexico State Uni-
versity, July.
L. Ramshaw and M. Marcus, 1999. Natural Lan-
guage Processing Using Very Large Corpora, chap-
ter Text Chunking Using Transformation-based
Learning, pages 157176. Kluwer.
A. Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the First Con-
ference on Empirical Methods in NLP, pages 133
142, Philadelphia, PA.
R. Rivest. 1987. Learning decision lists. Machine
Learning, 2(3):229246.
E. Roche and Y. Schabes. 1995. Computational
linguistics. Deterministic Part of Speech Tagging
with Finite State Transducers, 21(2):227253.
K. Samuel, S. Carberry, and K. Vijay-Shanker.
1998. Dialogue act tagging with transformation-
based learning. In Proceedings of the 17th Interna-
tional Conference on Computational Linguistics
and the 36th Annual Meeting of the Association
for Computational Linguistics, pages 11501156,
Montreal, Quebec, Canada.
K. Samuel. 1998. Lazy transformation-based
learning. In Proceedings of the 11th Intera-
tional Florida AI Research Symposium Confer-
ence, pages 235239, Florida, USA.
E. Tjong Kim Sang and S. Buchholz. 2000. In-
troduction to the conll-2000 shared task: Chunk-
ing. In Proceedings of CoNLL-2000 and LLL-
2000, pages 127132, Lisbon, Portugal.
Coaxing Confidences from an Old Friend: 
Probabilistic Classifications from Transformation Rule Lists 
Radu F lo r ian*  John  C.  Henderson  t Grace  Nga i*  
*Department of Computer  Science 
Johns Hopkins University 
Baltimore, MD 21218, USA 
{rf lorian,gyn}@cs.jhu.edu 
tThe MITRE Corporat ion  
202 Bur l ington Road 
Bedford,  MA 01730, USA 
jhndrsn@mitre .org 
Abst rac t  
Transformation-based l arning has been success- 
fully employed to solve many natural language 
processing problems. It has many positive fea- 
tures, but one drawback is that it does not provide 
estimates of class membership probabilities. 
In this paper, we present a novel method for 
obtaining class membership robabilities from a 
transformation-based rule list classifier. Three ex- 
periments are presented which measure the model- 
ing accuracy and cross-entropy ofthe probabilistic 
classifier on unseen data and the degree to which 
the output probabilities from the classifier can be 
used to estimate confidences in its classification 
decisions. 
The results of these experiments show that, for 
the task of text chunking 1, the estimates produced 
by this technique are more informative than those 
generated by a state-of-the-art decision tree. 
1 In t roduct ion  
In natural language processing, a great amount of 
work has gone into the development of machine 
learning algorithms which extract useful linguistic 
information from resources such as dictionaries, 
newswire feeds, manually annotated corpora and 
web pages. Most of the effective methods can 
be roughly divided into rule-based and proba- 
bilistic algorithms. In general, the rule-based 
methods have the advantage of capturing the 
necessary information in a small and concise set 
of rules. In part-of-speech tagging, for exam- 
ple, rule-based and probabilistic methods achieve 
comparable accuracies, but rule-based methods 
capture the knowledge in a hundred or so simple 
rules, while the probabilistic methods have a 
very high--dimensional parameter space (millions 
of parameters). 
One of the main advantages of probabilistic 
methods, on the other hand, is that they include a 
measure of uncertainty in their output. This can 
take the form of a probability distribution over 
potential outputs, or it may be a ranked list of 
IA11 the experiments are performed on text chnnklng. 
The technique presented is general-purpose, however, and 
can be applied to many tasks for which transformation- 
based learning performs well, without changing the inter- 
rials of the learner. 
candidate outputs. These uncertainty measures 
are useful in situations where both the classifi- 
cation of an sample and the system's confidence 
in that classification are needed. An example of 
this is a situation in an ensemble system where 
ensemble members disagree and a decision must 
be made about how to resolve the disagreement. 
A similar situation arises in pipeline systems, such 
as a system which performs parsing on the output 
of a probabilistic part-of-speech tagging. 
Transformation-based learning (TBL) (Brill, 
1995) is a successful rule-based machine learning 
algorithm in natural language processing. It has 
been applied to a wide variety of tasks, including 
part of speech tagging (Roche and Schabes, 1995; 
Brill, 1995), noun phrase chvnklng (Ramshaw and 
Marcus, 1999), parsing (Brill, 1996; Vilain and 
Day, 1996), spelling correction (Mangu and Brill, 
1997), prepositional phrase attachment (Brill and 
Resnik, 1994), dialog act tagging (Samuel et 
al., 1998), segmentation and message understand- 
ing (Day et al, 1997), often achieving state- 
of-the-art performance with a small and easily- 
understandable list of rules. 
In this paper, we describe a novel method 
which enables a transformation-based classifier to 
generate a probability distribution on the class 
labels. Application of the method allows the 
transformation rule list to retain the robustness of 
the transformation-based algorithms, while bene- 
fitting from the advantages ofa probabilistic clas- 
sifter. The usefulness of the resulting probabilities 
is demonstrated bycomparison with another state- 
of-the-art classifier, the C4.5 decision tree (Quin- 
lan, 1993). The performance of our algorithm 
compares favorably across many dimensions: it 
obtains better perplexity and cross-entropy; an 
active learning algorithm using our system outper- 
forms a similar algorithm using decision trees; and 
finally, our algorithm has better rejection curves 
than a similar decision tree. Section 2 presents the 
transformation based learning paradigm; Section 
3 describes the algorithm for construction of the 
decision tree associated with the transformation 
based list; Section 4 describes the experiments 
in detail and Section 5 concludes the paper and 
outlines the future work. 
26 
2 Trans format ion  ru le  l i s t s  
The central idea of transformation-based l arn- 
ing is to learn an ordered list of rules which 
progressively improve upon the current state of 
the training set. An initial assignment is made 
based on simple statistics, and then rules are 
greedily learned to correct he mistakes, until no 
net improvement can be made. 
These definitions and notation will be used 
throughout the paper: 
? X denotes the sample space; 
? C denotes the set of possible classifications of 
the samples; 
? The state space is defined as 8 = X x C. 
? 7r will usually denote a predicate defined on 
X; 
? A rule r is defined as a predicate - class label 
- time tuple, (~r,c,t), c E C,t E N, where t is 
the learning iteration in which when the rule 
was learned, its position in the list. 
? A rule r = (~r, c, t) applies to a state (z, y) if 
7r(z) = true and c # y. 
Using a TBL framework to solve a problem as- 
sumes the existence of: 
? An initial class assignment (mapping from X 
to ,.9). This can be as simple as the most 
common class label in the training set, or it 
can be the output from another classifier. 
? A set of allowable templates for rules. These 
templates determine the predicates the rules 
will test, and they have the biggest influence 
over the behavior of the system. 
? An objective function for learning. Unlike in 
many other learning algorithms, the objective 
function for TBL will typically optimize the 
evaluation function. An often-used method is 
the difference in performance resulting from 
applying the rule. 
At the beginning of the learning phase, the 
training set is first given an initial class assign- 
ment. The system then iteratively executes the 
following steps: 
1. Generate all productive rules. 
2. For each rule: 
(a) Apply to a copy of the most recent state 
of the training set. 
(b) Score the result using the objective func- 
tion. 
3. Select he rule with the best score. 
4. Apply the rule to the current state of the 
training set, updating it to reflect his change. 
5. Stop if the score is smaller than some pre-set 
threshold T. 
6. Repeat from Step 1. 
The system thus learns a list of rules in a greedy 
fashion, according to the objective function. When 
no rule that improves the current state of the 
training set beyond the pre-set threshold can 
be found, the training phase ends. During the 
evaluation phase, the evaluation set is initialized 
with the same initial class assignment. Each rule 
is then applied, in the order it was learned, to the 
evaluation set. The final classification is the one 
attained when all rules have been applied. 
3 Probab i l i ty  es t imat ion  w i th  
t rans format ion  ru le  l i s t s  
Rule lists are infamous for making hard decisions, 
decisions which adhere entirely to one possibility, 
excluding all others. These hard decisions are 
often accurate and outperform other types of 
classifiers in terms of exact-match accuracy, but 
because they do not have an associated proba- 
bility, they give no hint as to when they might 
fail. In contrast, probabilistic systems make soft 
decisions by assigning a probability distribution 
over all possible classes. 
There are many applications where soft deci- 
sions prove useful. In situations such as active 
learning, where a small number of samples are 
selected for annotation, the probabilities can be 
used to determine which examples the classifier 
was most unsure of, and hence should provide the 
most extra information. A probabilistic system 
can also act as a filter for a more expensive 
system or a human expert when it is permitted 
to reject samples. Soft decision-making is also 
useful when the system is one of the components 
in a larger decision-malting process, as is the case 
in speech recognition systems (Bald et al, 1989), 
or in an ensemble system like AdaBoost (Freund 
and Schapire, 1997). There are many other 
applications in which a probabilistic lassifier is 
necessary, and a non-probabHistic classifier cannot 
be used instead. 
3.1 Estimation via conversion to decision 
tree 
The method we propose to obtain probabilis- 
tic classifications from a transformation rule list 
involves dividing the samples into equivalence 
classes and computing distributions over each 
equivalence class. At any given point in time i, 
each sample z in the training set has an associated 
state si(z) = (z,~l). Let R (z )  to be the set of rules 
r~ that applies to the state el(z), 
R(z) = {ri ~ 7~Ir~ applies to si(z)} 
An  equivalence class consists of all the samples 
z that have the same R(z). Class probability 
assignments are then estimated using statistics 
computed on the equivalence classes. 
27 
An illustration of the conversion from a rule 
list to a decision tree is shown below. Table 1 
shows an example transformation rule list. It is 
straightforward to convert this rule list into a de- 
cision pylon (Bahl et al, 1989)~. which can be used 
to represent all the possible sequences of labels 
assigned to a sample during the application of the 
TBL  algorithm. The decision pylon associated 
with this particular rule list is displayed on the left 
side of Figure 1. The decision tree shown on the 
right side of Figure 1 is constructed such that the 
samples stored in any leaf have the same class label 
sequence as in the displayed decision pylon. In 
the decision pylon, "no" answers go straight down; 
in the decision tree, "yes" answers take the right 
branch. Note that a one rule in the transformation 
rule list can often correspond to more than one 
node in the decision tree. 
Initial label = A 
I f  Q1 and label=A then  label+-B 
I f  Q2 and label=A then  labele-B 
I f  Q3 and label=B then  label~A 
Table I: Example of a Transformation Rule List. 
Figure 1: Converting the transformation rule list 
from Table 1 to a decision tree, 
The conversion from a transformation rule list 
to a decision tree is presented as a recursive 
procedure. The set of samples in the training set 
is transformed to a set of states by applying the 
initial class assignments. A node n is created for 
each of the initial class label assignments c and all 
states labeled c are assigned to n. 
The following recursive procedure is invoked 
with an initial "root" node, the complete set of 
states (from the corpus) and the whole sequence 
of rules learned uring training: 
A lgor i thm:  Ru leL is tToDec is ionTree  
(RLTDT)  
Input :  
* A set/3 of N states ((Zl, Yl) --- (ZN, YN)) with 
labels Yi E C; 
? A set 7~ of M rules (ro,rl . . .rM) where ri = 
Do:  
1. If 7~ is empty, the end of the rule list has been 
reached. Create a leaf node, n, and estimate 
the probability class distribution based on the 
true classifications of the states in 13. Return 
n.  
2. Let rj  = (Ir j ,yj , j )  be the lowest-indexed rule 
in 7~. Remove it from 7~. 
3. Split the data in/3 using the predicate 7rj and 
the current hypothesis uch that samples on 
which 7rj returns true are on the right of the 
split: 
BL = {x E BlTrj(x ) = false} 
/3R = {x E/31 j(x) = true} 
4. If IBLI > K and IBRI > K,  the split is 
acceptable: 
(a) Create a new internal node, n; 
(b) Set the question: q(n) = 7rj; 
(c) Create the left child of n using a recursive 
call to RLTDT(BL, 7~); 
(d) Create the right child of n using a recur- 
sive call to RLTDT(BR, 7~); 
(e) Return node n. 
Otherwise, no split is performed using rj.  
Repeat from Step 1. 
The parameter K is a constant that determines the 
minimum weight that a leaf is permitted to have, 
effectively pruning the tree during construction. 
In all the experiments, K was set to 5. 
3.2 Fur ther  growth  o f  the  decis ion t ree  
When a rule list is converted into a decision tree, 
there are often leaves that are inordinately heavy 
because they contain a large number of samples. 
Examples of such leaves are those containing 
samples which were never transformed by any 
of the rules in the rule list. These populations 
exist either because they could not be split up 
during the rule list learning without incurring a 
net penalty, or because any rule that acts on them 
has an objective function score of less than the 
threshold T. This is sub-optimal for estimation 
because when a large portion of the corpus falls 
into the same equivalence class, the distribution 
assigned to it reflects only the mean of those 
samples. The undesirable consequence is that all 
of those samples are given the same probability 
distribution. 
To ameliorate this problem, those samples are 
partitioned into smaller equivalence classes by 
further growing the decision tree. Since a decision 
tree does not place all the samples with the same 
current label into a single equivalence class, it does 
not get stuck in the same situation as a rule list 
m in which no change in the current state of 
corpus can be made without incurring a net loss 
in performance. 
28 
Continuing to grow the decision tree that was 
converted from a rule list can be viewed from 
another angle. A highly accurate prefix tree 
for the final decision tree is created by tying 
questions together during the first phase of the 
growth process (TBL). Unlike traditional decision 
trees which select splitting questions for a node 
by looking only at the samples contained in the 
local node, this decision tree selects questions by 
looking at samples contained in all nodes on the 
frontier whose paths have a suM< in common. An 
illustration of this phenomenon can be seen in 
Figure 1, where the choice to split on Question 
3 was made from samples which tested false 
on the predicate of Question 1, together with 
samples which tested false on the predicate of 
Question 2. The result of this is that questions 
are chosen based on a much larger population than 
in standard decision tree growth, and therefore 
have a much greater chance of being useful and 
generalizable. This alleviates the problem of over- 
partitioning of data, which is a widely-recognized 
concern during decision tree growth. 
The decision tree obtained from this conversion 
can be grown further. When the rule list 7~ is 
exhausted at Step 1, instead of creating a leaf 
node, continue splitting the samples contained in 
the node with a decision tree induction algorithm. 
The splitting criterion used in the experiments is 
the information gain measure. 
4 Exper iments  
Three experiments that demonstrate the effec- 
tiveness and appropriateness of our probability 
estimates are presented in this section. The 
experiments are performed on text chunking, a 
subproblem ofsyntactic parsing. Unlike full pars- 
ing, the sentences are divided into non-overlapping 
phrases, where each word belongs to the lowest 
parse constituent that dominates it. 
The data used in all of these experiments i  
the CoNLL-2000 phrase chunking corpus (CoNLL, 
2000). The corpus consists of sections 15-18 and 
section 20 of the Penn Treebank (Marcus et al, 
1993), and is pre-divided into a 8936-sentence 
(211727 tokens) training set and a 2012-sentence 
(47377 tokens) test set. The chunk tags are 
derived from the parse tree constituents, and the 
part-of-speech tags were generated by the Brill 
tagger (Brill, 1995). 
As was noted by Ramshaw & Marcus (1999), 
text chunking can be mapped to a tagging task, 
where each word is tagged with a chunk tag 
representing the phrase that it belongs to. An 
example sentence from the corpus is shown in 
Table 4. As a contrasting system, our results 
are compared with those produced by a C4.5 
decision tree system (henceforth C4.5). The 
reason for using C4.5 is twofold: firstly, it is a 
widely-used algorithm which achieves state-.of-the- 
art performance on a broad variety of tasks; and 
Word 
A.P. 
Green 
currently 
has 
2,664,098 
shares 
outstanding 
POS tag 
NNP 
NNP 
RB 
VBZ 
CD 
NNS 
JJ 
Chunk Tag 
B-NP 
I-NP 
B-ADVP 
B-VP 
B-NP 
I-NP 
B-ADJP 
O 
Table 2: Example of a sentence with chunk tags 
secondly, it belongs to the same class of classifiers 
as our converted transformation-based rule list 
(henceforth TBLDT). 
To perform a fair evaluation, extra care was 
taken to ensure that both C4.5 and TBLDT 
explore as similar a sample space as possible. The 
systems were allowed to consult the word, the 
part-of-speech, and the chunk tag of all examples 
within a window of 5 positions (2 words on either 
side) of each target example. 2 Since multiple 
features covering the entire vocabulary of the 
training set would be too large a space for C4.5 
to deal with, in all of experiments where TBLDT 
is directly compared with C4.5, the word types 
that both systems can include in their predicates 
are restricted to the most "ambiguous" 100 words 
in the training set, as measured by the number of 
chunk tag types that are assigned to them. The 
initial prediction was made for both systems using 
a class assignment based solely on the part-of- 
speech tag of the word. 
Considering chunk tags within a contextual win- 
dow of the target word raises a problem with C4.5. 
A decision tree generally trains on independent 
samples and does not take into account changes 
of any features in the context. In our case, the 
samples are dependent; the classification ofsample 
i is a feature for sample i + 1, which means that 
changing the classification for sample i affects 
the context of sample i + 1. To address this 
problem, the C4.5 systems are trained with the 
correct chlmk~ in the left context. When the 
system is used for classification, input is processed 
in a left-to-right manner;and the output of the 
system is fed forward to be used as features 
in the left context of following samples. Since 
C4.5 generates probabilities for each classification 
decision, they can be redirected into the input for 
the next position. Providing the decision treewith 
this confidence information effectively allows it to 
perform a limited search over the entire sentence. 
C4.5 does have one advantage over TBLDT, 
however. A decision tree can be trained using the 
subsetting feature, where questions asked are of 
the form: "does feature f belong to the set FT'. 
This is not something that a TBL can do readily, 
2The TBL templates are similar to those used in 
l~am.~haw and Marcus (1999). 
29 
but since the objective is in comparing TBLDT to 
another state-of-the-art system, this feature was 
enabled. 
4.1 Evaluation Measures 
The most commonly used measure for evaluating 
tagging tasks is tag accuracy, lit is defined as 
Accuracy = # of correctly tagged examples 
of examples 
In syntactic parsing, though, since the task is 
to identify the phrasal components, it is more 
appropriate o measure the precision and recall: 
# of correct proposed phrases 
Precision = 
# of proposed phrases 
# of correct proposed phrases 
Recall = # of correct phrases 
To facilitate the comparison of systems with dif- 
ferent precision and recall, the F-measure metric 
is computed as a weighted harmonic mean of 
precision and recall: 
(82 + 1) ? Precision x Recall 
= 
82 x Precision + Recall 
The ~ parameter is used to give more weight to 
precision or recall, as the task at hand requires. 
In all our experiments, ~ is set to 1, giving equal 
weight o precision and recall. 
The reported performances are all measured 
with the evaluation tool provided with the CoNLL 
corpus (CoNLL, 2000). 
4.2 Active Learning 
To demonstrate the usefulness of obtaining proba- 
bilities from a transformation rule list, this section 
describes an application which utilizes these prob- 
abilities, and compare the resulting performance 
of the system with that achieved by C4.5. 
Natural language processing has traditionally 
required large amounts of annotated ata from 
which to extract linguistic properties. However, 
not all data is created equal: a normal distribu- 
tion of aunotated ata contains much redundant 
information. Seung et al (1992) and Freund et 
al. (1997) proposed a theoretical ctive learning 
approach, where samples are intelligently selected 
for annotation. By eliminating redundant infor- 
mation, the same performance can be achieved 
while using fewer resources. Empirically, active 
learning has been applied to various NLP tasks 
such as text categorization (Lewis and Gale, 1994; 
Lewis and Catlett, 1994; Liere and Tadepalli, 
1997), part-of-speech tagging (Dagan and Engel- 
son, 1995; Engelson and Dagan, 1996), and base 
noun phrase chunbiug (Ngai and Yarowsky, 2000), 
resulting in significantly large reductions in the 
quantity of data needed to achieve comparable 
performance. 
This section presents two experimental results 
which show the effectiveness of the probabilities 
generated by the TBLDT. The first experiment 
compares the performance achieved by the active 
learning algorithm using TBLDT with the perfor- 
mance obtained by selecting samples equentially 
from the training set. The second experiment 
compares the performances achieved by TBLDT 
and C4.5 training on samples elected by active 
learning. 
The following describes the active learning algo- 
rithm used in the experiments: 
1. Label an initial T1 sentences ofthe corpus; 
2. Use the machine learning algorithm (G4.5 or 
TBLDT) to obtain chunk probabilities on the 
rest of the training data; 
3. Choose T2 samples from the rest of the train- 
ing set, specifically the samples that optimize 
an evaluation function f ,  based on the class 
distribution probability of each sample; 
4. Add the samples, including their "true" classi- 
fication 3 to the training pool and retrain the 
system; 
5. If a desired number of samples is reached, 
stop, otherwise repeat from Step 2. 
The evaluation function f that was used in our 
experiments is:
where H(UIS, i ) is the entropy of the chllnk 
probability distribution associated with the word 
index i in sentence S. 
Figure 2 displays the performance (F-measure 
and chllnk accuracy) of a TBLDT system trained 
on samples elected by active learning and the 
same system trained on samples elected sequen- 
tially from the corpus versus the number of words 
in the annotated tralniug set. At each step of 
the iteration, the active learning-trained TBLDT 
system achieves a higher accuracy/F-measure, or, 
conversely, is able to obtain the same performance 
level with less training data. Overall, our system 
can yield the same performance as the sequential 
system with 45% less data, a significant reduction 
in the annotation effort. 
Figure 3 shows a comparison between two active 
learning experiments: one using TBLDT and the 
other using C4.5. 4 For completeness, a sequential 
run using C4.5 is also presented. Even though 
C4.5 examines a larger space than TBLDT by 
SThe true (reference or gold standard) classification is 
available in this experiment. In an annotation situation, 
the samples are sent o human annotators for labeling. 
4As mentioned arlier, both the TBLDT and C4.5 were 
limited to the same 100 most ambiguous words in the 
corpus to ensure comparability. 
3O 
84 
AL?TBLDT ' ~ ' ' ' 
i i i I i 
(a) F-measure vs. number of words in trrdniug set 
Oil 
AL* 'mI .~ ' - -  . . . .  
i I 
(b) Chunk Accuracy vs. number of words in training 
set 
Figure 2: Performance of the TBLDT system versus sequential choice. 
87 
86 
| -  
81 
...=2- ' 
, . r .~ . . . r  - - I~ ' ' ' '~ ' '~"  
\ [~"  
i 
I i L i 
(a) F-measure vs. number of words in tr~inln s set 
31 
AL?'nBL (I0? ~ )  ~ '  
J i i ~ i i 
(b) Accuracy vs. number of words in training set 
Figure 3: Performance of the TBLDT system versus the DT system 
utilizing the feature subset predicates, TBLDT 
still performs better. The difference in accuracy at 
26200 words (at the end of the active learning run 
for TBLDT) is statistically significant at a 0.0003 
level. 
As a final remark on this experiment, note that 
at an annotation level of 19000 words, the fully 
lexicalized TBLDT outperformed the C4.5 system 
by making 15% fewer errors. 
4.3 Re jec t ion  curves 
It is often very useful for a classifier to be able 
to offer confidence scores associated with its deci- 
sions. Confidence scores are associated with the 
probability P(C(z) correct\[z) where C(z) is the 
classification of sample z. These scores can be 
used in real-life problems to reject samples that 
the the classifier is not sure about, in which case 
a better observation, or a human decision, might 
be requested. The performance of the classifier 
is then evaluated on the samples that were not 
rejected. This experiment framework is well- 
established in machine learning and optimization 
research (Dietterich and Bakiri, 1995; Priebe et 
al., 1999). 
Since non-probabilistic classifiers do not offer 
any insights into how sure they are about a 
particular classification, it is not easy to obtain 
confidence scores from them. A probabilistic 
classifier, in contrast, offers information about the 
class probability distribution of a given sample. 
Two measures that can be used in generating 
confidence scores are proposed in this section. 
The first measure, the entropy H of the class 
probability distribution of a sample z, C(z) = 
{p(CllZ),p(c2\[z)...p(cklZ)}, i s  a measure  of the 
uncertainty in the distribution: 
k 
HCCCz)) = - I=) log2 pC Iz) 
i=I 
The higher the entropy of the distribution of 
class probability estimates, the more uncertain the 
0.99 
0.98 
0.97 
g~ 
0.~ 
~0.95  
0.94 
0.93 
///.._.-.--f" 
/ / C4.5 (hard d=fisions)__ i 
I / / / . -  ..... ~ % _ .. ~.; 
':" \] 
I I I I I I I I I 
0J O2 O3 0.4 0.5 O.6 0,7 O.8 O.9 Z 
l~c~t of rej~xaed ~
(a)  Subcorpus  (batch)  re jec t ion  
0~ 1 i i 1 
0.985 " 
0~8 
O.975 
097 
0.965 
0.96 - TBL-DT 
0.955 
095 - C4_5 (soft decisi 
0.945 ..- .... 
0.94 .-.:.-.-.r.-. r.~---.':.'.':.'. "
0.935 \[ 
0 0.2 0.4 0.6 ~8 1 
Probability of th~ most lflmly tag 
(b)  Thresho ld  (on l ine)  re jec t ion  
Figure 4: Rejection curves. 
classifier is of its classification. The samples e- 
lected for rejection are chosen by sorting the data 
using the entropies of the estimated probabilities, 
and then selecting the ones with highest entropies. 
The resulting curve is a measure of the correlation 
between the true probability distribution and the 
one given by the classifier. 
Figure 4(a) shows the rejection curves for the 
TBLDT system and two C4.5 decision trees - one 
which receives a probability distribution as input 
("soft" decisions on the left context) , and one 
which receives classifications ("hard" decisions on 
all fields). At the left of the curve, no samples 
are rejected; at the right side, only the samples 
about which the classifiers were most certain are 
kept (the samples with minimum entropy). Note 
that the y-values on the right side of the curve are 
based on less data, effectively introducing wider 
variance in the curve as it moves right. 
As shown in Figure 4(a), the C4.5 classifier 
that has access to the left context chunk tag 
probability distributions behaves better than the 
other C4.5 system, because this information about 
the surrounding context allows it to effectively 
perform a shallow search of the classification 
space. The TBLDT system, which also receives 
a probability distribution on the chunk tags in 
the left context, clearly outperforms both C4.5 
systems at all rejection levels. 
The second proposed measure is based on the 
probability of the most likely tag. The assumption 
here is that this probability is representative of 
how certain the system is about the classifica- 
tion. The samples are put in bins based on 
the probability of the most likely chnnk tag, and 
accuracies are computed for each bin (these bins 
are cumulative, meaning that a sample will be 
included in all the bins that have a lower threshold 
than the probability of its most likely chnnl? 
tag). At each accuracy level, a sample will be 
rejected if the probability of its most likely chnn~ 
Cross Entropy 
TBLDT 1.2944 0.2580 
DT+probs 1.4150 0.3471 
DT 1.4568 0.3763 
Table 3: Cross entropy and perplexities for two 
C4.5 systems and the TBLDT system 
is below the accuracy level. The resulting curve 
is a measure of the correlation between the true 
distribution probability and the probability of the 
most likely chunk tag, i.e. how appropriate those 
probabilities are as confidence measures. Unlike 
the first measure mentioned before, a threshold 
obtained using this measure can be used in an 
online manner to identify the samples of whose 
classification the system is confident. 
Figure 4(b) displays the rejection curve for 
the second measure and the same three systems. 
TBLDT again outperforms both C4.5 systems, at 
all levels of confidence. 
In summary, the TBLDT system outperforms 
both C4.5 systems presented, resulting in fewer re- 
jections for the same performance, or, conversely, 
better performance at the same rejection rate. 
4.4 Perp lex i ty  and  Cross Ent ropy  
Cross entropy is a goodness measure for probabil- 
ity estimates that takes into account he accuracy 
of the estimates as well as the classification accu- 
racy of the system. It measures the performance 
of a system trained on a set of samples distributed 
according to the probability distribution p when 
tested on a set following a probability distribution 
q. More specifically, we utilize conditional cross 
entropy, which is defined as 
n (C lX)  = - q (=) -  q(cl=) ? log2 pC@:) 
zEX ?EC 
where X is the set of examples and C is the set of 
chnnlr tags, q is the probabil i ty distribution on the 
32 
Chunk  
Type  
A c c u r a c y  
(%) 
Precisionl Recall 
(%) I (%) 
Overall 95.23 92.02 92.50 
ADJP  - 75.69 68.95 
ADVP - 80.88 78.64 
CONJP  - 40.00 44.44 
INTJ - 50.00 50.00 
LST - 0.00 0.00 
NP  - 92.18 92.72 
PP  95.89 97.90 
PRT  - 67.80 75.47 
SBAR 88.71 82.24 
VP 92.00 92.87 
Fi 
92.26! 
72.16 
79.74 
42.11 
50.00 
0.00 
92.45 
96.88 
71.43 
85.35 
92.44 
Table 4: Performance of TBLDT on the CoNLL 
Test Set 
test document and p is the probability distribution 
on the train corpus. 
The cross entropy metric fails if any outcome is 
given zero probability by the estimator. To avoid 
this problem, estimators are "smoothed", ensuring 
that novel events receive non-zero probabilities. 
A very simple smoothing technique (interpolation 
with a constant) was used for all of these systems. 
A closely related measure is perplexity, defined 
as 
P = 2~(cl x) 
The cross entropy and perplexity results for the 
various estimation schemes are presented in Table 
? 3. The TBLDT outperforms both C4.5 systems, 
obtaining better cross-entropy and chunk tag per- 
plexity. This shows that the overall probability 
distribution obtained from the TBLDT system 
better matches the true probability distribution. 
This strongly suggests hat probabilities generated 
this way can be used successfully in system com- 
bination techniques such as voting or boosting. 
4.5 Chunk ing  per formance 
It is worth noting that the transformation-based 
system used in the comparative graphs in Figure 
3 was not r, uning at full potential. As described 
earlier, the TBLDT system was only allowed to 
consider words that C4.5 had access to. However, 
a comparison between the corresponding TBLDT 
curves in Figures 2 (where the system is given 
access to all the words) and 3 show that a 
transformation-based system given access to all 
the words performs better than the one with a 
restricted lexicon, which in turn outperforms the 
best C4.5 decision tree system both in terms of 
accuracy and F-measure. 
Table 4 shows the performance of the TBLDT 
system on the full CoNLL  test set, broken down 
by chunk type. Even though the TBLDT results 
could not be compared with other published re- 
sults on the same task and data (CoNLL  will 
not take place until September 2000), our system 
significantly outperforms a similar system trained 
with a C4.5 decision tree, shown in Table 5, both 
in chunk accuracy and F-measure. 
Chunk 
Type 
Accuracy 
(%) 
ADVP 
CONJP  
IJrecision 
(%) 
Recall 
(%) 
Overall 93.80 90.02 90.26 
ADJP 65.58 64.38 
74.14 76.79 
33.33 
INTJ 50.00 50.00 
LST 0.00 0.00 
NP  91.00 90.93 
PP  92.70 96.36 
PRT  71.13 65.09 
SBAR 86.35 61.50 
VP  90.71 91.22 
I Fz 
90.14 
64.98 
75.44 
33.33 
50.00 
0.00 
90.96 
94.50 
67.98 
71.83 
90.97 
Table 5: Performance of C4.5 on the CoNLL Test 
Set 
5 Conclus ions 
In this paper we presented a novel way to convert 
transformation rule lists, a common paradigm in 
natural anguage processing, into a form that is 
equivalent in its classification behavior, but is 
capable of providing probability estimates. Using 
this approach, favorable properties of transfor- 
mation rule lists that makes them popular for 
language processing are retained, while the many 
advantages of a probabilistic system axe gained. 
To demonstrate he efficacy of this approach, 
the resulting probabilities were tested in three 
ways: directly measuring the modeling accuracy 
on the test set via cross entropy, testing the 
goodness of the output probabilities in a active 
learning algorithm, and observing the rejection 
curves attained from these probability estimates. 
The experiments clearly demonstrate that the 
resulting probabilities perform at least as well as 
the ones generated by C4.5 decision trees, resulting 
in better performance in all cases. This proves that 
the resulting probabilistic lassifier is as least as 
good as other state-of-the-art p obabilistic models. 
The positive results obtained suggest hat the 
probabilistic lassifier obtained from transforma- 
tion rule lists can be successfully used in machine 
learning algorithms that require soft-decision clas- 
sifters, such as boosting or voting. Future research 
will include testing the behavior of the system 
under AdaBoost (Freund and Schapire, 1997). We 
also intend to investigate the effects that other 
decision tree growth and smoothing techniques 
may have on continued refinement of the converted 
rule list. 
6 Acknowledgements  
We thank Eric Brill, Fred Jelinek and David 
Yaxowsky for their invaluable advice and sugges- 
tions. In addition we would like to thank David 
Day, Ben Weliner and the anonymous reviewers 
for their useful comments and suggestions on the 
paper . . . .  
The views expressed in this paper are those of 
the authors and do not necessarily reflect he views 
33 
of the MITRE Corporation. It  was performed 
as a collaborative ffort at \]both MITRE and 
the Center for Language and ',Speech Processing, 
Johns Hopkins University, Baltimore, MD. It was 
supported by NSF grants numbered IRI-9502312 
and IRI-9618874, as well as the MITRE-Sponsored 
Research program. 
References 
L. Bahl, P. Brown, P. de Souza, and R. Mercer. 1989. 
A tree-based statistical language model for natural 
language speech recognition. IEEE Transactions on 
Acoustics, Speech and Signal Processing, 37:1001- 
1008. 
E. BriU and P. Resnik. 1994. A rule-based approach 
to prepositional phrase attachment disambiguation. 
In Proceedings of the Fifteenth International Con- 
ference on Computational Linguistics (COLING- 
199~), pages 1198--1204, Kyoto. 
E. BrllL 1995. Transformation-based rror-driven 
learning and natural language processing: A case 
study in part of speech tagging. Computational 
Linguistics, 21(4):543-565. 
E. Brill, 1996. Learning to Parse with Transforma- 
tions. In H. Bunt and M. Tomita (eds.) Recent 
Advances in Parsing Technology, Kluwer. 
CoNLL. 2000. Shared task for computational natu- 
ral language learning (CoNLL), 2000. http://lcg- 
ww w.uia.ac.be/conU2000/chunking. 
I. Dagan and S. Engelson. 1995. Committee-based 
sampling for training probabilistic lassifiers. In 
Proceedings ofInternational Conference on Machine 
Learning (ICML) 1995, pages 150-157. 
D. Day, J. Aberdeen, L. Hirschman, R. Kozierok, 
P. Robinson, and M. Vllaln. 1997. Mixed-initiative 
development of language processing systems. In 
Fifth Conference on Applied Natural Language Pro- 
cessing, pages 348-355. Association for Computa- 
tional Linguistics, March. 
T. G. Dietterich and G. Bakiri. 1995. Solving multi- 
class learning problems via error-correcting output 
codes. Journal of Artificial Intelligence Research, 
2:263-286. 
S. Engelson and I. Dagan. 1996. Minlmi~.ing manual 
annotation cost in supervised training fxom corpora. 
In Proceedings of ACL 1996, pages 319-326, Santa 
Cruz, CA. Association for Computational Linguis- 
tics. 
Y. Freund and R.E. Schapire. 1997. A decision- 
theoretic generalization of on-fine learning and an 
application to boosting. Journal of Computer and 
System Sciences, 55(1):119--139. 
Y. Fremad, H. S. Senng, E. Shamir, and N. Tishby. 
1997. Selective sampling using the query by com- 
mittee algorithm. Machine Learning, 28:133-168. 
D. Lewis and J. Catlett. 1994. Heterogeneous n- 
certainty sampling for supervised learning. In Pro- 
ceedings of the 11th International Conference on 
Machine Learning, pages 139---147. 
D. Lewis and W. Gale. 1994. A sequential algorithm 
for training text classifiers. In Proceedings ofA CM- 
SIGIR 1994, pages 3-12. ACM-SIGIR. 
R. Liere and P. Tadepalli. 1997. Active learning with 
committees for text categorization. In Proceedings 
of the Fourteenth National Conference on Artificial 
Intelligence, pages 591-596. AAAI. 
L. Mangu and E. Brill. 1997. Automatic rule acquisi- 
tion for spelling correction. In Proceedings of the 
Fourteenth International Conference on Machine 
Learning, pages 734-741, Nashville, Tennessee. 
M. P. Marcus, B. Santorini, and M. A. Mareinkiewicz. 
1993. Building a large annotated corpus of english: 
The Penn Treebank. Computational Linguistics, 
19(2):313-330. 
G. Ngai and D. Yarowsky. 2000. Rule writing or 
annotation: Cost-efficient resource usage for base 
noun phrase chunking. In Proceedings ofA CL 2000. 
Association for Computational Linguistics. 
C. E. Priebe, J.-S. Pang, and T. Olson. 1999. Opt lmiT. 
ing mine classification performance. In Proceedings 
of the JSM. American Statistical Association. 
J. R. Qnlnlan. 1993. C~.5: Programs for machine 
learning. Morgan Kanfmann, San Mateo, CA. 
L. Ramshaw and M. Marcus, 1999. Text Chunk- 
ing Using Transformation-based Learning. In S. 
Armstrong, K.W. Church, P. Isabelle, S. Mauzi, 
E. Tzoukermann and D. Yarowsky (eds.) Natural 
Language Processing Using Very Large Corpora, 
Kluwer. 
E. Roche and Y. Schabes. 1995. Computational 
linguistics. Deterministic Part of Speech Tagging 
with Finite State Transducers, 21(2):227-253. 
K. Samuel, S. Carberry, and K. Vijay-Shanker. 1998. 
Dialogue act tagging with transformation-based 
learning. In Proceedings of the 17th International 
Conference on Computational Linguistics and the 
36th Annual Meeting of the Association for Com- 
putational Linguistics, pages 1150-1156, Montreal, 
Quebec, Canada. 
H. S. Senng, M. Opper, and H. Sompolinsky. 1992. 
Query by committee. In Proceedings of the Fifth 
Annual A CM Workshop on Computational Learning 
Theory, pages 287-294. ACM. 
M. Vilain and D. Day. 1996. Finite-state parsing 
by rule sequences. In International Conference on 
Computational Linguistics, pages 274-279, Copen- 
hagen, Denmark, August. 
34 
 
	
	
 	 
  			 	

 
 
 
 	
  	 	
	
 	 
   	
	 	 
Modeling Consensus: Classifier Combination
for Word Sense Disambiguation
Radu Florian and David Yarowsky
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{rflorian,yarowsky}@cs.jhu.edu
Abstract
This paper demonstrates the substantial empirical
success of classifier combination for the word sense
disambiguation task. It investigates more than 10
classifier combination methods, including second
order classifier stacking, over 6 major structurally
different base classifiers (enhanced Na?ve Bayes,
cosine, Bayes Ratio, decision lists, transformation-
based learning and maximum variance boosted mix-
ture models). The paper also includes in-depth per-
formance analysis sensitive to properties of the fea-
ture space and component classifiers. When eval-
uated on the standard SENSEVAL1 and 2 data sets
on 4 languages (English, Spanish, Basque, and
Swedish), classifier combination performance ex-
ceeds the best published results on these data sets.
1 Introduction
Classifier combination has been extensively stud-
ied in the last decade, and has been shown to be
successful in improving the performance of diverse
NLP applications, including POS tagging (Brill and
Wu, 1998; van Halteren et al, 2001), base noun
phrase chunking (Sang et al, 2000), parsing (Hen-
derson and Brill, 1999) and word sense disambigua-
tion (Kilgarriff and Rosenzweig, 2000; Stevenson
and Wilks, 2001). There are several reasons why
classifier combination is useful. First, by consulting
the output of multiple classifiers, the system will im-
prove its robustness. Second, it is possible that the
problem can be decomposed into orthogonal feature
spaces (e.g. linguistic constraints and word occur-
rence statistics) and it is often better to train dif-
ferent classifiers in each of the feature spaces and
then combine their output, instead of designing a
complex system that handles the multimodal infor-
mation. Third, it has been shown by Perrone and
Cooper (1993) that it is possible to reduce the clas-
sification error by a factor of 

( is the number of
classifiers) by combination, if the classifiers? errors
are uncorrelated and unbiased.
The target task studied here is word sense disam-
biguation in the SENSEVAL evaluation framework
(Kilgarriff and Palmer, 2000; Edmonds and Cotton,
2001) with comparative tests in English, Spanish,
Swedish and Basque lexical-sample sense tagging
over a combined sample of 37730 instances of 234
polysemous words.
This paper offers a detailed comparative evalu-
ation and description of the problem of classifier
combination over a structurally and procedurally
diverse set of six both well established and orig-
inal classifiers: extended Na?ve Bayes, BayesRa-
tio, Cosine, non-hierarchical Decision Lists, Trans-
formation Based Learning (TBL), and the MMVC
classifiers, briefly described in Section 4. These
systems have different space-searching strategies,
ranging from discriminant functions (BayesRatio)
to data likelihood (Bayes, Cosine) to decision rules
(TBL, Decision Lists), and therefore are amenable
to combination.
2 Previous Work
Related work in classifier combination is discussed
throughout this article. For the specific task of
word sense disambiguation, the first empirical study
was presented in Kilgarriff and Rosenzweig (2000),
where the authors combined the output of the par-
ticipating SENSEVAL1 systems via simple (non-
weighted) voting, using either Absolute Majority,
Relative Majority, or Unanimous voting. Steven-
son and Wilks (2001) presented a classifier com-
bination framework where 3 disambiguation meth-
ods (simulated annealing, subject codes and selec-
tional restrictions) were combined using the TiMBL
memory-based approach (Daelemans et al, 1999).
Pedersen (2000) presents experiments with an en-
semble of Na?ve Bayes classifiers, which outper-
form all previous published results on two ambigu-
ous words (line and interest).
3 The WSD Feature Space
The feature space is a critical factor in classifier de-
sign, given the need to fuel the diverse strengths of
the component classifiers. Thus its quality is of-
ten highly correlated with performance. For this
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 25-32.
                         Proceedings of the Conference on Empirical Methods in Natural
An ancient stone church stands amid the fields,
the sound of bells ...
Feat. Type Word POS Lemma
Context ancient JJ ancient/J
Context stone NN stone/N
Context church NNP church/N
Context stands VBZ stand/V
Context amid IN amid/I
Context fields NN field/N
Context ... ... ...
Syntactic (predicate-argument) features
SubjectTo stands_Sbj VBZ stand_Sbj/V
Modifier stone_mod JJ ancient_mod/J
Ngram collocational features
-1 bigram stone_L JJ ancient_L/J
+1 bigram stands_R VBZ stand_R/V
1 trigram stone  stands JJVBZ stone/Jstands/V
... ... ... ...
Figure 1: Example sentence and extracted features from
the SENSEVAL2 word church
reason, we used a rich feature space based on raw
words, lemmas and part-of-speech (POS) tags in a
variety of positional and syntactical relationships to
the target word. These positions include traditional
unordered bag-of-word context, local bigram and
trigram collocations and several syntactic relation-
ships based on predicate-argument structure. Their
use is illustrated on a sample English sentence for
the target word church in Figure 1. While an exten-
sive evaluation of feature type to WSD performance
is beyond the scope of this paper, Section 6 sketches
an analysis of the individual feature contribution to
each of the classifier types.
3.1 Part-of-Speech Tagging and
Lemmatization
Part-of-speech tagger availability varied across the
languages that are studied here. An electronically
available transformation-based POS tagger (Ngai
and Florian, 2001) was trained on standard labeled
data for English (Penn Treebank), Swedish (SUC-
1 corpus), and Basque. For Spanish, an minimally
supervised tagger (Cucerzan and Yarowsky, 2000)
was used. Lemmatization was performed using an
existing trie-based supervised models for English,
and a combination of supervised and unsupervised
methods (Yarowsky and Wicentowski, 2000) for all
the other languages.
3.2 Syntactic Features
The syntactic features extracted for a target word
depend on the word?s part of speech:
 verbs: the head noun of the verb?s object, par-
ticle/preposition and prepositional object;
 nouns: the headword of any verb-object,
subject-verb or noun-noun relationships iden-
tified for the target word;
 adjectives: the head noun modified by the ad-
jective.
The extraction process was performed using heuris-
tic patterns and regular expressions over the parts-
of-speech surrounding the target word1.
4 Classifier Models for Word Sense
Disambiguation
This section briefly introduces the 6 classifier mod-
els used in this study. Among these models, the
Na?ve Bayes variants (NB henceforth) (Pedersen,
1998; Manning and Sch?tze, 1999) and Cosine dif-
fer slightly from off-the-shelf versions, and only the
differences will be described.
4.1 Vector-based Models: Enhanced Na?ve
Bayes and Cosine Models
Many of the systems used in this research share
a common vector representation, which captures
traditional bag-of-words, extended ngram and
predicate-argument features in a single data struc-
ture. In these models, a vector is created for each
document in the collection:   


 

 







, where 

is the number of times the feature


appears in document ,  is the number of words
in  and 

is a weight associated with the feature


2
. Confusion between the same word participat-
ing in multiple feature roles is avoided by append-
ing the feature values with their positional type (e.g.
stands_Sbj, ancient_L are distinct from stands and
ancient in unmarked bag-of-words context).
The notable difference between the extended
models and others described in the literature, aside
from the use of more sophisticated features than
the traditional bag-of-words, is the variable weight-
ing of feature types noted above. These differences
yield a boost in the NB performance (relative to ba-
sic Na?ve Bayes) of between 3.5% (Basque) and
10% (Spanish), with an average improvement of
7.25% over the four languages.
4.2 The BayesRatio Model
The BayesRatio model (BR henceforth) is a vector-
based model using the likelihood ratio framework
described in Gale et al (1992):
1The feature extraction on the in English data was per-
formed by first identifying text chunks, and then using heuris-
tics on the chunks to extract the syntactic information.
2The weight 

depends on the type of the feature 

: for
the bag-of-word features, this weight is inversely proportional
to the distance between the target word and the feature, while
for predicate-argument and extended ngram features it is a em-
pirically estimated weight (on a per language basis).
  

 	
 	
 

 	
 	


  	
  	
where  is the selected sense,  denotes documents
and  denotes features. By utilizing the binary ra-
tio for k-way modeling of feature probabilities, this
approach performs well on tasks where the data is
sparse.
4.3 The MMVC Model
The Mixture Maximum Variance Correction classi-
fier (MMVC henceforth) (Cucerzan and Yarowsky,
2002) is a two step classifier. First, the sense proba-
bility is computed as a linear mixture
  


    




   
where the probability  	 is estimated from
data and  	 is computed as a weighted normal-
ized similarity between the word 	 and the target
word 
 (also taking into account the distance in the
document between 	 and 
). In a second pass, the
sense whose variance exceeds a theoretically moti-
vated threshold is selected as the final sense label
(for details, see Cucerzan and Yarowsky (2002)).
4.4 The Discriminative Models
Two discriminative models are used in the exper-
iments presented in Section 5 - a transformation-
based learning system (TBL henceforth) (Brill,
1995; Ngai and Florian, 2001) and a non-
hierarchical decision lists system (DL henceforth)
(Yarowsky, 1996). For prediction, these systems
utilize local n-grams around the target word (up to
3 words/lemma/POS to the left/right), bag-of-words
and lemma/collocation (20 words around the tar-
get word, grouped by different window sizes) and
the syntactic features listed in Section 3.2.
The TBL system was modified to include redun-
dant rules that do not improve absolute accuracy on
training data in the traditional greedy training al-
gorithm, but are nonetheless positively correlated
with a particular sense. The benefit of this approach
is that predictive but redundant features in training
context may appear by themselves in new test con-
texts, improving coverage and increasing TBL base
model performance by 1-2%.
5 Models for Classifier Combination
One necessary property for success in combining
classifiers is that the errors produced by the com-
ponent classifiers should not be positively corre-
lated. On one extreme, if the classifier outputs are
0.0 0.2 0.4 0.6 0.8 1.0
 MMVC
 Cosine
 Bayes
 BayesRatio
 TBL
 DecisionLists
Figure 2: Empirically-derived classifier similarity
strongly correlated, they will have a very high inter-
agreement rate and there is little to be gained from
the joint output. On the other extreme, Perrone and
Cooper (1993) show that, if the errors made by the
classifiers are uncorrelated and unbiased, then by
considering a classifier that selects the class that
maximizes the posterior class probability average
  	

   	









 (1)
the error is reduced by a factor of 

. This case
is mostly of theoretical interest, since in practice
all the classifiers will tend to make errors on the
?harder? samples.
Figure 3(a) shows the classifier inter-agreement
among the six classifiers presented in Section 4, on
the English data. Only two of them, BayesRatio and
cosine, have an agreement rate of over 80%3, while
the agreement rate can be as low as 63% (BayesRa-
tio and TBL). The average agreement is 71.7%. The
fact that the classifiers? output are not strongly cor-
related suggests that the differences in performance
among them can be systematically exploited to im-
prove the overall classification. All individual clas-
sifiers have high stand-alone performance; each is
individually competitive with the best single SEN-
SEVAL2 systems and are fortuitously diverse in rel-
ative performance, as shown in Table 3(b). A den-
dogram of the similarity between the classifiers is
shown in Figure 2, derived using maximum linkage
hierarchical agglomerative clustering.
5.1 Major Types of Classifier Combination
There are three major types of classifier combina-
tion (Xu et al, 1992). The most general type is the
case where the classifiers output a posterior class
probability distribution for each sample (which can
be interpolated). In the second case, systems only
output a set of labels, together with a ordering of
preference (likelihood). In the third and most re-
strictive case, the classifications consist of just a sin-
gle label, without rank or probability. Combining
classifiers in each one of these cases has different
properties; the remainder of this section examines
models appropriate to each situation.
3The performance is measured using 5-fold cross validation
on training data.
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85





































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Cosine
Bayes
TBL
DL
BayesRatio
Cl
as
sif
ie
r A
gg
re
em
en
t (
% 
of 
da
ta)
Bayes Cosine BayesRatio DL TBL MMVC
MMVC
(a) Classifier inter-agreement on SENSEVAL2
English data
System SENSEVAL1 SENSEVAL2
EN EN ES EU SV
Baseline 63.2 48.3 45.9 62.7 46.2
NB 80.4 65.7 67.9 71.2 66.7
BR 79.8 65.3 69.0 69.6 68.0
Cosine 74.0 62.2 65.9 66.0 66.4
DL 79.9 63.2 65.1 70.7 61.5
TBL 80.7 64.4 64.7 69.4 62.7
MMVC 81.1 66.7 66.7 69.7 61.9
(b) Individual classifier performance; best performers are
shown in bold
Figure 3: Individual Classifier Properties (cross-validation on SENSEVAL training data)
5.2 Combining the Posterior Sense Probability
Distributions
One of the simplest ways to combine the poste-
rior probability distributions is via direct averaging
(Equation (1)). Surprisingly, this method obtains
reasonably good results, despite its simplicity and
the fact that is not theoretically motivated under a
Bayes framework. Its success is highly dependent
on the condition that the classifiers? errors are un-
correlated (Tumer and Gosh, 1995).
The averaging method is a particular case of
weighted mixture:4
 	  



 
	   

	  





	   

	  (2)
where 

  is the weight assigned to the clas-
sifier  in the mixture and 


  is the poste-
rior probability distribution output by classifier ;
for 


  


we obtain Equation (1).
The mixture interpolation coefficients can be
computed at different levels of granularity. For
instance, one can make the assumption that
 
    
 and then the coefficients will
be computed at word level; if  
    
then the coefficients will be estimated on the entire
data.
One way to estimate these parameters is by linear
regression (Fuhr, 1989): estimate the coefficients
that minimize the mean square error (MSE)










 	  





	    	 






(3)
where  
  is the target vector of the cor-
rect classification of word 
 in document d:
4Note that we are computing a probability conditioned both
on the target word  and the document , because the docu-
ments are associated with a particular target word ; this for-
malization works mainly for the lexical choice task.
 
    ?  

 , 

being the goldstan-
dard sense of 
 in  and ? the Kronecker function:
? 
  

 if 
  
 if 
  
As shown in Fuhr (1989), Perrone and Cooper
(1993), the solution to the optimization problem (3)
can be obtained by solving a linear set of equations.
The resulting classifier will have a lower square er-
ror than the average classifier (since the average
classifier is a particular case of weighted mixture).
Another common method to compute the  pa-
rameters is by using the Expectation-Maximization
(EM) algorithm (Dempster et al, 1977). One
can estimate the coefficients such as to max-
imize the log-likelihood of the data,  




 

	 . In this particular opti-
mization problem, the search space is convex, and
therefore a solution exists and is unique, and it can
be obtained by the usual EM algorithm (see Berger
(1996) for a detailed description).
An alternative method for estimating the parame-
ters 

is to approximate them with the performance
of the th classifier (a performance-based combiner)
(van Halteren et al, 1998; Sang et al, 2000)



    

_is_correct
  (4)
therefore giving more weight to classifiers that have
a smaller classification error (the method will be re-
ferred to as PB). The probabilities in Equation (4)
are estimated directly from data, using the maxi-
mum likelihood principle.
5.3 Combination based on Order Statistics
In cases where there are reasons to believe that the
posterior probability distribution output by a clas-
sifier is poorly estimated5, but that the relative or-
dering of senses matches the truth, a combination
5For instance, in sparse classification spaces, the Na?ve
Bayes classifier will assign a probability very close to 1 to the
most likely sense, and close to 0 for the other ones.
strategy based on the relative ranking of sense pos-
terior probabilities is more appropriate. The sense
posterior probability can be computed as
 
  





  


 

	






  




  (5)
where the rank of a sense  is inversely proportional
to the number of senses that are (strictly) more prob-
able than sense :
	


 	 











 
	
 

 	





 
	

This method will tend to prefer senses that appear
closer to the top of the likelihood list for most of the
classifiers, therefore being more robust both in cases
where one classifier makes a large error and in cases
where some classifiers consistently overestimate the
posterior sense probability of the most likely sense.
5.4 The Classifier Republic: Voting
Some classification methods frequently used in
NLP directly minimize the classification error and
do not usually provide a probability distribution
over classes/senses (e.g. TBL and decision lists).
There are also situations where the user does not
have access to the probability distribution, such as
when the available classifier is a black-box that only
outputs the best classification. A very common
technique for combination in such a case is by vot-
ing (Brill and Wu, 1998; van Halteren et al, 1998;
Sang et al, 2000). In the simplest model, each clas-
sifier votes for its classification and the sense that
receives the most number of votes wins. The behav-
ior is identical to selecting the sense with the highest
posterior probability, computed as
 
  





   ?  


 








   ?  


  (6)
where ? is the Kronecker function and 


  is
the classification of the th classifier. The 

co-
efficients can be either equal (in a perfect classifier
democracy), or they can be estimated with any of
the techniques presented in Section 5.2. Section
6 presents an empirical evaluation of these tech-
niques.
Van Halteren et al (1998) introduce a modified
version of voting called TagPair. Under this model,
the conditional probability that the word sense is 
given that classifier  outputs 

and classifier  out-
puts 

,  


   

 


   

, is com-
puted on development data, and the posterior prob-
ability is estimated as
  	 



?  

 		 



?  
	
 		
(7)
where 
	
	   	


 

	   
	
	 .
Each classifier votes for its classification and every
pair of classifiers votes for the sense that is most
likely given the joint classification. In the experi-
ments presented in van Halteren et al (1998), this
method was the best performer among the presented
methods. Van Halteren et al (2001) extend this
method to arbitrarily long conditioning sequences,
obtaining the best published POS tagging results on
four corpora.
6 Empirical Evaluation
To empirically test the combination methods pre-
sented in the previous section, we ran experiments
on the SENSEVAL1 English data and data from four
SENSEVAL2 lexical sample tasks: English(EN),
Spanish(ES), Basque(EU) and Swedish(SV). Un-
less explicitly stated otherwise, all the results in the
following section were obtained by performing 5-
fold cross-validation6 . To avoid the potential for
over-optimization, a single final evaluation system
was run once on the otherwise untouched test data,
as presented in Section 6.3.
The data consists of contexts associated with a
specific word to be sense tagged (target word); the
context size varies from 1 sentence (Spanish) to
5 sentences (English, Swedish). Table 1 presents
some statistics collected on the training data for the
five data sets. Some of the tasks are quite challeng-
ing (e.g. SENSEVAL2 English task) ? as illustrated
by the mean participating systems? accuracies in Ta-
ble 5.
Outlining the claim that feature selection is im-
portant for WSD, Table 2 presents the marginal loss
in performance of either only using one of the po-
sitional feature classes or excluding one of the po-
sitional feature classes relative to the algorithm?s
full performance using all available feature classes.
It is interesting to note that the feature-attractive
methods (NB,BR,Cosine) depend heavily on the
BagOfWords features, while discriminative methods
are most dependent on LocalContext features. For
an extensive evaluation of factors influencing the
WSD performance (including representational fea-
tures), we refer the readers to Yarowsky and Florian
(2002).
6.1 Combination Performance
Table 3 shows the fine-grained sense accuracy (per-
cent of exact correct senses) results of running the
6When parameters needed to be estimated, a 3-1-1 split was
used: the systems were trained on three parts, parameters esti-
mated on the fourth (in a round-robin fashion) and performance
tested on the fifth; special care was taken such that no ?test?
data was used in training classifiers or parameter estimation.
SE1 SENSEVAL2
EN EN ES EU SV
#words 42 73 39 40 40
#samples 12479 8611 4480 3444 8716
avg #senses/word 11.3 10.7 4.9 4.8 11.1
avg #samples/sense 26.21 9.96 23.4 17.9 19.5
Table 1: Training set characteristics
Performance drop relative to full system (%)
NB Cosine BR TBL DL
BoW Ftrs Only -6.4 -4.8 -4.8 -6.0 -3.2
Local Ftrs Only -18.4 -11.5 -6.1 -1.5 -3.3
Syntactic Ftrs Only -28.1 -14.9 -5.4 -5.4 -4.8
No BoW Ftrs -14.7 -8.1 -5.3 -0.5 -2.0
No Local Ftrs -3.5 -0.8 -2.2 -2.9 -4.5
No Syntactic Ftrs -1.1 -0.8 -1.3 -1.0 -2.3
Table 2: Individual feature type contribution to perfor-
mance. Fields marked with  indicate that the difference
in performance was not statistically significant at a 

level (paired McNemar test).
classifier combination methods for 5 classifiers, NB
(Na?ve Bayes), BR (BayesRatio), TBL, DL and
MMVC, including the average classifier accuracy
and the best classification accuracy. Before examin-
ing the results, it is worth mentioning that the meth-
ods which estimate parameters are doing so on a
smaller training size (3/5, to be precise), and this
can have an effect on how well the parameters are
estimated. After the parameters are estimated, how-
ever, the interpolation is done between probability
distributions that are computed on 4/5 of the train-
ing data, similarly to the methods that do not esti-
mate any parameters.
The unweighted averaging model of probability
interpolation (Equation (1)) performs well, obtain-
ing over 1% mean absolute performance over the
best classifier7, the difference in performance is
statistically significant in all cases except Swedish
and Spanish. Of the classifier combination tech-
niques, rank-based combination and performance-
based voting perform best. Their mean 2% absolute
improvement over the single best classifier is signif-
icant in all languages. Also, their accuracy improve-
ment relative to uniform-weight probability interpo-
lation is statistically significant in aggregate and for
all languages except Basque (where there is gener-
ally a small difference among all classifiers).
To ensure that we benefit from the performance
improvement of each of the stronger combination
methods and also to increase robustness, a final av-
eraging method is applied to the output of the best
performing combiners (creating a stacked classi-
fier). The last line in Table 3 shows the results ob-
tained by averaging the rank-based, EM-vote and
7The best individual classifier differs with language, as
shown in Figure 3(b).
SE1 SENSEVAL2
Method EN EN ES EU SV
Individual Classifiers
Mean Acc 79.5 65.0 66.6 70.4 65.9
Best Acc 81.1 66.7 68.8 71.2 68.0
Probability Interpolation
Averaging 82.7 68.0 69.3 72.2 68.16
MSE 82.8 68.1 69.7 71.0 69.2
EM 82.7 68.4 69.6 72.1 69.1
PB 82.8 68.0 69.4 72.2 68.7
Rank-based Combination
rank 83.1 68.6 71.0 72.1 70.3
Count-based Combination (Voting)
Simple Vote 82.8 68.1 70.9 72.1 70.0
TagPair 82.9 68.3 70.9 72.1 70.0
EM 83.0 68.4 70.5 71.7 70.0
PB 83.1 68.5 70.8 72.0 70.3
Stacking (Meta-Combination)
Prob. Interp. 83.2 68.6 71.0 72.3 70.4
Table 3: Classifier combination accuracy over 5 base
classifiers: NB, BR, TBL, DL, MMVC. Best perform-
ing methods are shown in bold.
Estimation Level word POS ALL Interp
Accuracy 68.1 68.2 68.0 68.4
CrossEntropy 1.623 1.635 1.646 1.632
Table 4: Accuracy for different EM-weighted probability
interpolation models for SENSEVAL2
PB-vote methods? output. The difference in perfor-
mance between the stacked classifier and the best
classifier is statistically significant for all data sets
at a significance level of at least , as measured
by a paired McNemar test.
One interesting observation is that for all meth-
ods of -parameter estimation (EM, PB and uniform
weighting) the count-based and rank-based strate-
gies that ignore relative probability magnitudes out-
perform their equivalent combination models using
probability interpolation. This is especially the case
when the base classifier scores have substantially
different ranges or variances; using relative ranks
effectively normalizes for such differences in model
behavior.
For the three methods that estimate the interpo-
lation weights ? MSE, EM and PB ? three vari-
ants were investigated. These were distinguished by
the granularity at which the weights are estimated:
at word level (


   


), at POS level
(


   

 
) and over the entire train-
ing set (


   

). Table 4 displays the results
obtained by estimating the parameters using EM at
different sample granularities for the SENSEVAL2
English data. The number in the last column is ob-
tained by interpolating the first three systems. Also
displayed is cross-entropy, a measure of how well
?1.2
?1
?0.8
?0.6
?
?
0.4
0 2
0
0.2
0.4
0.6 English Spanish Swedish Basque
Bayes BayesRatio Cosine DL TBL MMVC





 



 














































































































































































































































































































































































































Senseval2 dataset
D
if
fe
re
nc
e 
in
 A
cc
ur
ac
y 
vs
 
6?
w
ay
 C
om
bi
na
tio
n
(a) Performance drop when eliminating one classifier
(marginal performance contribution)
?3.5
?3
?2.5
?2
?1.5
?1
?0.5
0
0.5
1
Bayes BayesRatio
Cosine
DL
TBL
MMVC
Percent of available training data
10 20 40 50 80
D
iff
er
en
ce
 in
 c
la
ss
ifi
ca
tio
n 
ac
cu
ra
cy
 (%
)
(b) Performance drop when eliminating one classifer,
versus training data size
Figure 4: Individual basic classifiers? contribution to the final classifier combination performance.
the combination classifier estimates the sense prob-
abilities,   


 

 	

 
 .
6.2 Individual Systems Contribution to
Combination
An interesting issue pertaining to classifier combi-
nation is what is the marginal contribution to final
combined performance of the individual classifier.
A suitable measure of this contribution is the dif-
ference in performance between a combination sys-
tem?s behavior with and without the particular clas-
sifier. The more negative the accuracy difference on
omission, the more valuable the classifier is to the
ensemble system.
Figure 4(a) displays the drop in performance ob-
tained by eliminating in turn each classifier from the
6-way combination, across four languages, while
Figure 4(b) shows the contribution of each classifier
on the SENSEVAL2 English data for different train-
ing sizes (10%-80%)8. Note that the classifiers with
the greatest marginal contribution to the combined
system performance are not always the best single
performing classifiers (Table 3(b)), but those with
the most effective original exploitation of the com-
mon feature space. On average, the classifier that
contributes the most to the combined system?s per-
formance is the TBL classifier, with an average im-
provement of 

 across the 4 languages. Also,
note that TBL and DL offer the greatest marginal
contribution on smaller training sizes (Figure 4(b)).
6.3 Performance on Test Data
At all points in this article, experiments have been
based strictly on the original SENSEVAL1 and SEN-
SEVAL2 training sets via cross-validation. The of-
ficial SENSEVAL1 and SENSEVAL2 test sets were
8The latter graph is obtained by sampling repeatedly a
prespecified ratio of training samples from 3 of the 5 cross-
validation splits, and testing on the other 2.
unused and unexamined during experimentation to
avoid any possibility of indirect optimization on this
data. But to provide results more readily compara-
ble to the official benchmarks, a single consensus
system was created for each language using linear
average stacking on the top three classifier combi-
nation methods in Table 3 for conservative robust-
ness. The final frozen consensus system for each
language was applied once to the SENSEVAL test
sets. The fine-grained results are shown in Table
5. For each language, the single new stacked com-
bination system outperforms the best previously re-
ported SENSEVAL results on the identical test data9.
As far as we know, they represent the best published
results for any of these five SENSEVAL tasks.
7 Conclusion
In conclusion, we have presented a comparative
evaluation study of combining six structurally and
procedurally different classifiers utilizing a rich
common feature space. Various classifier combi-
nation methods, including count-based, rank-based
and probability-based combinations are described
and evaluated. The experiments encompass super-
vised lexical sample tasks in four diverse languages:
English, Spanish, Swedish, and Basque.
9To evaluate systems on the full disambiguation task, it is
appropriate to compare them on their accuracy at 100% test-
data coverage, which is equivalent to system recall in the offi-
cial SENSEVAL scores. However, it can also be useful to con-
sider performance on only the subset of data for which a sys-
tem is confident enough to answer, measured by the secondary
measure precision. One useful byproduct of the CBV method
is the confidence it assigns to each sample, which we measured
by the number of classifiers that voted for the sample. If one
restricts system output to only those test instances where all
participating classifiers agree, consensus system performance
is 83.4% precision at a recall of 43%, for an F-measure of 56.7
on the SENSEVAL2 English lexical sample task. This outper-
forms the two supervised SENSEVAL2 systems that only had
partial coverage, which exhibited 82.9% precision at a recall of
28% (F=41.9) and 66.5% precision at 34.4% recall (F=47.9).
SENSEVAL1 SENSEVAL2 Sense Classification Accuracy
English English Spanish Swedish Basque
Mean Official SENSEVAL Systems Accuracy 73.12.9 55.75.3 59.65.0 58.46.6 74.41.8
Best Previously Published SENSEVAL Accuracy 77.1% 64.2% 71.2% 70.1% 75.7%
Best Individual Classifier Accuracy 77.1% 62.5% 69.6% 68.6% 75.6%
New (Stacking) Accuracy 79.7% 66.5% 72.4% 71.9% 76.7%
Table 5: Final Performance (Frozen Systems) on SENSEVAL Lexical Sample WSD Test Data
The experiments show substantial variation in
single classifier performance across different lan-
guages and data sizes. They also show that this
variation can be successfully exploited by 10 differ-
ent classifier combination methods (and their meta-
voting consensus), each of which outperforms both
the single best classifier system and standard classi-
fier combination models on each of the 4 focus lan-
guages. Furthermore, when the stacking consensus
systems were frozen and applied once to the other-
wise untouched test sets, they substantially outper-
formed all previously known SENSEVAL1 and SEN-
SEVAL2 results on 4 languages, obtaining the best
published results on these data sets.
8 Acknowledgements
The authors would like to thank Noah Smith for his
comments on an earlier version of this paper, and
the anonymous reviewers for their useful comments.
This work was supported by NSF grant IIS-9985033
and ONR/MURI contract N00014-01-1-0685.
References
A. Berger. 1996. Convexity, maximum likelihood
and all that. http://www.cs.cmu.edu/afs/cs/user/aberger/
www/ps/convex.ps.
E. Brill and J. Wu. 1998. Classifier combination for improved
lexical disambiguation. In Proceedings of COLING-ACL?98,
pages 191?195.
E. Brill. 1995. Transformation-based error-driven learning and
natural language processing: A case study in part of speech
tagging. Computational Linguistics, 21(4):543?565.
S. Cucerzan and D. Yarowsky. 2000. Language independent
minimally supervised induction of lexical probabilities. In
Proceedings of ACL-2000, pages 270?277.
S. Cucerzan and D. Yarowsky. 2002. Augmented mixture models
for lexical disambiguation. In Proceedings of EMNLP-2002.
W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Timbl:
Tilburg memory based learner - version 1.0. Technical Report
ilk9803, Tilburg University, The Netherlands.
A.P. Dempster, N.M. Laird, , and D.B. Rubin. 1977. Maximum
likelihood from incomplete data via the EM algorithm. Jour-
nal of the Royal statistical Society, 39(1):1?38.
P. Edmonds and S. Cotton. 2001. SENSEVAL-2: Overview. In
Proceedings of SENSEVAL-2, pages 1?6.
N. Fuhr. 1989. Optimum polynomial retrieval funcions based
on the probability ranking principle. ACM Transactions on
Information Systems, 7(3):183?204.
W. Gale, K. Church, and D. Yarowsky. 1992. A method for
disambiguating word senses in a large corpus. Computers and
the Humanities, 26:415?439.
J. Henderson and E. Brill. 1999. Exploiting diversity in natural
language processing: Combining parsers. In Proceedings on
EMNLP99, pages 187?194.
A. Kilgarriff and M. Palmer. 2000. Introduction to the special
issue on senseval. Computer and the Humanities, 34(1):1-13.
A. Kilgarriff and J. Rosenzweig. 2000. Framework and re-
sults for English Senseval. Computers and the Humanities,
34(1):15-48.
C.D. Manning and H. Sch?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press.
G. Ngai and R. Florian. 2001. Transformation-based learning in
the fast lane. In Proceedings of NAACL?01, pages 40?47.
T. Pedersen. 1998. Na?ve Bayes as a satisficing model. In Work-
ing Notes of the AAAI Symposium on Satisficing Models.
T. Pedersen. 2000. A simple approach to building ensembles of
naive bayesian classifiers for word sense disambiguation. In
Proceedings of NAACL?00, pages 63?69.
M. P. Perrone and L. N. Cooper. 1993. When networks disagree:
Ensemble methods for hybrid neural networks. In R. J. Mam-
mone, editor, Neural Networks for Speech and Image Process-
ing, pages 126?142. Chapman-Hall.
E. F. Tjong Kim Sang, W. Daelemans, H. Dejean, R. Koeling,
Y. Krymolowsky, V. Punyakanok, and D. Roth. 2000. Apply-
ing system combination to base noun phrase identification. In
Proceedings of COLING 2000, pages 857?863.
M. Stevenson and Y. Wilks. 2001. The interaction of knowl-
edge sources in word sense disambiguation. Computational
Linguistics, 27(3):321?349.
K. Tumer and J. Gosh. 1995. Theoretical foundations of linear
and order statistics combiners for neural pattern classifiers.
Technical Report TR-95-02-98, University of Texas, Austin.
H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Improv-
ing data driven wordclass tagging by system combination. In
Proceedings of COLING-ACL?98, pages 491?497.
H. van Halteren, J. Zavrel, and W. Daelemans. 2001. Im-
proving accuracy in word class tagging through the combina-
tion fo machine learning systems. Computational Linguistics,
27(2):199?230.
L. Xu, A. Krzyzak, and C. Suen. 1992. Methods of com-
bining multiple classifires and their applications to handwrit-
ing recognition. IEEE Trans. on Systems, Man. Cybernet,
22(3):418?435.
D. Yarowsky and R. Florian. 2002. Evaluating sense disambigua-
tion performance across diverse parameter spaces. To appear
in Journal of Natural Language Engineering.
D. Yarowsky and R. Wicentowski. 2000. Minimally supervised
morphological analysis by multimodal alignment. In Pro-
ceedings of ACL-2000, pages 207?216.
D. Yarowsky. 1996. Homograph disambiguation in speech
synthesis. In J. Olive J. van Santen, R. Sproat and
J. Hirschberg, editors, Progress in Speech Synthesis, pages
159?175. Springer-Verlag.
Named Entity Recognition as a House of Cards: Classifier Stacking
Radu Florian
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218, USA
rflorian@cs.jhu.edu
1 Introduction
This paper presents a classifier stacking-based ap-
proach to the named entity recognition task (NER
henceforth). Transformation-based learning (Brill,
1995), Snow (sparse network of winnows (Mu?oz
et al, 1999)) and a forward-backward algorithm are
stacked (the output of one classifier is passed as in-
put to the next classifier), yielding considerable im-
provement in performance. In addition, in agree-
ment with other studies on the same problem, the
enhancement of the feature space (in the form of
capitalization information) is shown to be especially
beneficial to this task.
2 Computational Approaches
All approaches to the NER task presented in this
paper, except the one presented in Section 3, use the
IOB chunk tagging method (Tjong Kim Sang and
Veenstra, 1999) for identifying the named entities.
2.1 Feature Space and Baselines
A careful selection of the feature space is a very
important part of classifier design. The algorithms
presented in this paper are using only informa-
tion that can be extracted directly from the train-
ing data: the words, their capitalization informa-
tion and the chunk tags. While they can defi-
nitely incorporate additional information (such as
lists of countries/cities/regions, organizations, peo-
ple names, etc.), due to the short exposition space,
we decided to restrict them to this feature space.
Table 2 presents the results obtained by running
off-the-shelf part-of-speech/text chunking classi-
fiers; all of them use just word information, albeit
in different ways. The leader of the pack is the MX-
POST tagger (Ratnaparkhi, 1996). The measure of
choice for the NER task is F-measure, the harmonic
mean of precision and recall: 

 


 



, usu-
ally computed with   .
As observed by participants in the MUC-6 and -7
tasks (Bikel et al, 1997; Borthwick, 1999; Miller et
1: Capitalization information
2: Presence in
dictionary
first_cap, all_caps, all_lower,
number, punct, other
upper, lower,
both, none
Table 1: Capitalization information
al., 1998), an important feature for the NER task is
information relative to word capitalization. In an
approach similar to Zhou and Su (2002), we ex-
tracted for each word a 2-byte code, as summarized
in Table 1. The first byte specifies the capitaliza-
tion of the word (first letter capital, etc), while the
second specifies whether the word is present in the
dictionary in lower case, upper case, both or neither
forms. These two codes are extracted in order to of-
fer both a way of backing-off in sparse data cases
(unknown words) and a way of encouraging gen-
eralization. Table 2 shows the performance of the
fnTBL (Ngai and Florian, 2001) and Snow systems
when using the capitalization information, both sys-
tems displaying considerably better performance.
2.2 Transformation-Based Learning
Transformation-based learning (TBL henceforth) is
an error-driven machine learning technique which
works by first assigning an initial classification to
the data, and then automatically proposing, evalu-
ating and selecting the transformations that max-
imally decrease the number of errors. Each such
transformation, or rule, consists of a predicate and
a target. In our implementation of TBL ? fnTBL ?
predicates consist of a conjunction of atomic pred-
icates, such as feature identity (e.g. 


	
), membership in a set (e.g. B ORG 
	

   	

), etc.
TBL has some attractive qualities that make it
suitable for the language-related tasks: it can au-
tomatically integrate heterogenous types of knowl-
edge, without the need for explicit modeling (simi-
lar to Snow, Maximum Entropy, decision trees, etc);
it is error?driven, therefore directly minimizes the
Method Accuracy 

without capitalization information
TnT 94.78% 66.72
MXPOST 95.02% 69.04
Snow 94.27% 65.94
fnTBL 94.92% 68.06
with capitalization information
Snow (extended templates) 95.15% 71.36
fnTBL 95.57% 71.88
fnTBL+Snow 95.36% 73.49
Table 2: Comparative results for different methods on the
Spanish development data
ultimate evaluation measure: the error rate; and it
has an inherently dynamic behavior1. TBL has been
previously applied to the English NER task (Ab-
erdeen et al, 1995), with good results.
The fnTBL-based NER system is designed in the
same way as Brill?s POS tagger (Brill, 1995), con-
sisting of a morphological stage, where unknown
words? chunks are guessed based on their morpho-
logical and capitalization representation, followed
by a contextual stage, in which the full interaction
between the words? features is leveraged for learn-
ing. The feature templates used are based on a com-
bination of word, chunk and capitalization informa-
tion of words in a 7-word window around the target
word. The entire template list (133 templates) will
be made available from the author?s web page after
the conclusion of the shared task.
2.3 Snow
Snow ? Sparse Network of Winnows ? is an archi-
tecture for error-driven machine learning, consisting
of a sparse network of linear separator units over
a common predefined or incrementally learned fea-
ture space. The system assigns weights to each fea-
ture, and iteratively updates these weights in such
a way that the misclassification error is minimized.
For more details on Snow?s architecture, please re-
fer to Mu?oz et al (1999).
Table 2 presents the results obtained by Snow on
the NER task, when using the same methodology
from Mu?oz et al (1999), with the their templates2
and with the same templates as fnTBL.
1The quality of chunk tags evolves as the algorithm pro-
gresses; there is no mismatch between the quality of the sur-
rounding chunks during training and testing.
2In this experiment, we used the feature patterns described
in Mu?oz et al (1999): a combination of up to 2 words in a
3-word window around the target word and a combination of
up to 4 chunks in a 7-word window around the target word. All
throughout the paper, Snow?s default parameters were used.
70
70.5
71
71.5
72
72.5
73
73.5
0 2 4 6 8 10
Iteration Number
F?
m
ea
su
re
Figure 1: Performance of applying Snow to TBL?s out-
put, plotted against iteration number
2.4 Stacking Classifiers
Both the fnTBL and the Snow methods have
strengths and weaknesses:
 fnTBL?s strength is represented by its dynamic
modeling of chunk tags ? by starting in a sim-
ple state and using complex feature interac-
tions, it is able to reach a reasonable end-state.
Its weakness consists in its acute myopia: the
optimization is done greedily for the local con-
text, and the feature interaction is observed
only in the order in which the rules are se-
lected.
 Snow?s strength consists in its ability to model
interactions between the all features associated
with a sample. However, in order to obtain
good results, the system needs reliable contex-
tual information. Since the approach is not dy-
namic by nature, good initial chunk classifica-
tions are needed.
One way to address both weaknesses is to com-
bine the two approaches through stacking, by ap-
plying Snow on fnTBL?s output. This allows Snow
to have access to reasonably reliable contextual in-
formation, and also allows the output of fnTBL
to be corrected for multiple feature interaction.
This stacking approach has an intuitive interpreta-
tion: first, the corpus is dynamically labeled us-
ing the most important features through fnTBL
rules (coarse-grained optimization), and then is fine-
grained tuned through a few full-feature-interaction
iterations of Snow.
Table 2 contrasts stacking Snow and fnTBL with
running either fnTBL or Snow in isolation - an im-
provement of 1.6 F-measure points is obtained when
stacking is applied. Interestingly, as shown in Fig-
ure 1, the relation between performance and Snow-
iteration number is not linear: the system initially
takes a hit as it moves out of the local fnTBL maxi-
mum, but then proceeds to increase its performance,
Method Accuracy 

Spanish 98.42% 90.26
Dutch 98.54% 88.03
Table 3: Unlabeled chunking results obtained by fnTBL
on the development sets
finally converging after 10 iterations to a F-measure
value of 73.49.
3 Breaking-Up the Task
Mu?oz et al (1999) examine a different method of
chunking, called Open/Close (O/C) method: 2 clas-
sifiers are used, one predicting open brackets and
one predicting closed brackets. A final optimiza-
tion stage pairs open and closed brackets through a
global search.
We propose here a method that is similar in
spirit to the O/C method, and also to Carreras and
M?rquez (2001), Ar?valo et al (2002):
1. In the first stage, detect only the entity bound-
aries, without identifying their type, using the
fnTBL system3;
2. Using a forward-backward type algorithm (FB
henceforth), determine the most probable type
of each entity detected in the first step.
This method has some enticing properties:
 Detecting only the entity boundaries is a sim-
pler problem, as different entity types share
common features; Table 3 shows the perfor-
mance obtained by the fnTBL system ? the per-
formance is sensibly higher than the one shown
in Table 2;
 The FB algorithm allows for a global search
for the optimum, which is beneficial since both
fnTBL and Snow perform only local optimiza-
tions;
 The FB algorithm has access to both entity-
internal and external contextual features (as
first described in McDonald (1996)); further-
more, since the chunks are collapsed, the local
area is also larger in span.
The input to the FB algorithm consists of a series
of chunks 

     

, each spanning a sequence of
words


   






   


  


   


   


  


   

3For this task, Snow does not bring any improvement to the
fnTBL?s output.
Method Spanish Dutch
FB performance 76.49 73.30
FB on perfect chunk breaks 83.52 81.30
Table 4: Forward-Backward results (F-measure) on the
development sets
For each marked entity 
	
, the goal is to determine
its most likely type:4



 









 





	 



















   

















	


	
 








	
(1)
where 








   







represents the
entity-external/contextual probability, and









	


	









	
is the entity-internal
probability. These probabilities are computed
using the standard Markov assumption of inde-
pendence, and the forward-backward algorithm5.
Both internal and external models are using 5-gram
language models, smoothed using the modified
discount method of Chen and Goodman (1998).
In the case of unseen words, backoff to the cap-
italization tag is performed: if 


is unknown,
 



	
   	 


 
	
. Finally, the
probability 









	

	
	
is assumed to be
exponentially distributed.
Table 4 shows the results obtained by stacking
the FB algorithm on top of fnTBL. Comparing
the results with the ones in Table 2, one can ob-
serve that the global search does improve the perfor-
mance by 3 F-measure points when compared with
fnTBL+Snow and 5 points when compared with the
fnTBL system. Also presented in Table 4 is the per-
formance of the algorithm on perfect boundaries;
more than 6 F-measure points can be gained by
improving the boundary detection alone. Table 5
presents the detailed performance of the FB algo-
rithm on all four data sets, broken by entity type.
A quick analysis of the results revealed that most
errors were made on the unknown words, both in
4We use the notation 

 

   

.
5It is notable here that the best entity type for a chunk is
computed by selecting the best entity in all combinations of
the other entity assignments in the sentence. This choice is
made because it reflects better the scoring method, and makes
the algorithm more similar to the HMM?s forward-backward
algorithm (Jelinek, 1997, chapter 13) rather than the Viterbi
algorithm.
Spanish and Dutch: the accuracy on known words is
97.4%/98.9% (Spanish/Dutch), while the accuracy
on unknown words is 83.4%/85.1%. This suggests
that lists of entities have the potential of being ex-
tremely beneficial for the algorithm.
4 Conclusion
In conclusion, we have presented a classifier stack-
ing method which uses transformation-based learn-
ing to obtain a course-grained initial entity anno-
tation, then applies Snow to improve the classi-
fication on samples where there is strong feature
interaction and, finally, uses a forward-backward
algorithm to compute a global-best entity type
assignment. By using the pipelined processing,
this method improves the performance substan-
tially when compared with the original algorithms
(fnTBL, Snow+fnTBL).
5 Acknowledgements
The author would like to thank Richard Wicen-
towski for providing additional language resources
(such as lemmatization information), even if they
were ultimately not used in the research, David
Yarowsky for his support and advice during this
research, and Cristina Nita-Rotaru for useful com-
ments. This work was supported by NSF grant IIS-
9985033 and ONR/MURI contract N00014-01-1-
0685.
References
J. Aberdeen, D. Day, L. Hirschman, P. Robinson, and
M. Vilain. 1995. Mitre: Description of the Alembic
system used for MUC-6. In Proceedings of MUC-6,
pages 141?155.
M. Ar?valo, X. Carreras, L. M?rquez, M. A. Mart?,
L. Padr?, and M. J. Sim?n. 2002. A proposal
for wide-coverage Spanish named entity recognition.
Technical Report LSI-02-30-R, Universitat Polit?c-
nica de Catalunya.
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of ANLP-97, pages 194?201.
A. Borthwick. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, New York
University.
E. Brill. 1995. Transformation-based error-driven learn-
ing and natural language processing: A case study
in part of speech tagging. Computational Linguistics,
21(4):543?565.
X. Carreras and L. M?rquez. 2001. Boosting trees for
clause splitting. In Proceedings of CoNNL?01.
Spanish devel precision recall 

LOC 70.44% 83.45% 76.39
MISC 53.20% 63.60% 57.93
ORG 78.35% 73.00% 75.58
PER 86.28% 84.37% 85.31
overall 75.41% 77.60% 76.49
Spanish test precision recall 

LOC 82.06% 79.34% 80.68
MISC 59.71% 61.47% 60.58
ORG 78.51% 78.29% 78.40
PER 82.94% 89.93% 86.29
overall 78.70% 79.40% 79.05
Dutch deve precision recall 

LOC 81.15% 74.16% 77.50
MISC 72.02% 74.53% 73.25
ORG 79.92% 60.97% 69.17
PER 66.18% 84.04% 74.05
overall 73.09% 73.51% 73.30
Dutch test precision recall 

LOC 86.69% 77.69% 81.94
MISC 75.21% 68.80% 71.86
ORG 74.68% 66.59% 70.40
PER 69.39% 86.05% 76.83
overall 75.10% 74.89% 74.99
Table 5: Results on the development and test sets in
Spanish and Dutch
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Harvard University.
F. Jelinek, 1997. Information Extraction From Speech
And Text. MIT Press.
D. McDonald, 1996. Corpus Processing for Lexical
Aquisition, chapter Internal and External Evidence
in the Identification and Semantic Categorization of
Proper Names, pages 21?39. MIT Press.
S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwarz,
R. Stone, and R. Weischedel. 1998. Bbn: Description
of the SIFT system as used for MUC-7. In MUC-7.
M. Mu?oz, V. Punyakanok, D. Roth, and D. Zimak.
1999. A learning approach to shallow parsing. Tech-
nical Report 2087, Urbana, Illinois.
G. Ngai and R. Florian. 2001. Transformation-based
learning in the fast lane. In Proceedings of NAACL?01,
pages 40?47.
A. Ratnaparkhi. 1996. A maximum entropy model for
part of speech tagging. In Proceedings EMNLP?96,
Philadelphia.
E. F. Tjong Kim Sang and J. Veenstra. 1999. Represent-
ing text chunks. In Proceedings of EACL?99.
G. D. Zhou and J. Su. 2002. Named entity recognition
using a HMM-based chunk tagger. In Proceedings of
ACL?02.
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 546?554,
Beijing, August 2010
Learning to Predict Readability using Diverse Linguistic Features
Rohit J. Kate1 Xiaoqiang Luo2 Siddharth Patwardhan2 Martin Franz2
Radu Florian2 Raymond J. Mooney1 Salim Roukos2 Chris Welty2
1Department of Computer Science
The University of Texas at Austin
{rjkate,mooney}@cs.utexas.edu
2IBM Watson Research Center
{xiaoluo,spatward,franzm,raduf,roukos,welty}@us.ibm.com
Abstract
In this paper we consider the problem of
building a system to predict readability
of natural-language documents. Our sys-
tem is trained using diverse features based
on syntax and language models which are
generally indicative of readability. The
experimental results on a dataset of docu-
ments from a mix of genres show that the
predictions of the learned system are more
accurate than the predictions of naive hu-
man judges when compared against the
predictions of linguistically-trained expert
human judges. The experiments also com-
pare the performances of different learn-
ing algorithms and different types of fea-
ture sets when used for predicting read-
ability.
1 Introduction
An important aspect of a document is whether it
is easily processed and understood by a human
reader as intended by its writer, this is termed
as the document?s readability. Readability in-
volves many aspects including grammaticality,
conciseness, clarity, and lack of ambiguity. Teach-
ers, journalists, editors, and other professionals
routinely make judgements on the readability of
documents. We explore the task of learning to
automatically judge the readability of natural-
language documents.
In a variety of applications it would be useful to
be able to automate readability judgements. For
example, the results of a web-search can be or-
dered taking into account the readability of the
retrieved documents thus improving user satisfac-
tion. Readability judgements can also be used
for automatically grading essays, selecting in-
structional reading materials, etc. If documents
are generated by machines, such as summariza-
tion or machine translation systems, then they are
prone to be less readable. In such cases, a read-
ability measure can be used to automatically fil-
ter out documents which have poor readability.
Even when the intended consumers of text are
machines, for example, information extraction or
knowledge extraction systems, a readability mea-
sure can be used to filter out documents of poor
readability so that the machine readers will not ex-
tract incorrect information because of ambiguity
or lack of clarity in the documents.
As part of the DARPA Machine Reading Pro-
gram (MRP), an evaluation was designed and con-
ducted for the task of rating documents for read-
ability. In this evaluation, 540 documents were
rated for readability by both experts and novice
human subjects. Systems were evaluated based on
whether they were able to match expert readabil-
ity ratings better than novice raters. Our system
learns to match expert readability ratings by em-
ploying regression over a set of diverse linguistic
features that were deemed potentially relevant to
readability. Our results demonstrate that a rich
combination of features from syntactic parsers,
language models, as well as lexical statistics all
contribute to accurately predicting expert human
readability judgements. We have also considered
the effect of different genres in predicting read-
ability and how the genre-specific language mod-
els can be exploited to improve the readability pre-
dictions.
546
2 Related Work
There is a significant amount of published work
on a related problem: predicting the reading diffi-
culty of documents, typically, as the school grade-
level of the reader from grade 1 to 12. Some early
methods measure simple characteristics of docu-
ments like average sentence length, average num-
ber of syllables per word, etc. and combine them
using a linear formula to predict the grade level of
a document, for example FOG (Gunning, 1952),
SMOG (McLaughlin, 1969) and Flesh-Kincaid
(Kincaid et al, 1975) metrics. These methods
do not take into account the content of the doc-
uments. Some later methods use pre-determined
lists of words to determine the grade level of a
document, for example the Lexile measure (Sten-
ner et al, 1988), the Fry Short Passage measure
(Fry, 1990) and the Revised Dale-Chall formula
(Chall and Dale, 1995). The word lists these
methods use may be thought of as very simple
language models. More recently, language mod-
els have been used for predicting the grade level
of documents. Si and Callan (2001) and Collins-
Thompson and Callan (2004) train unigram lan-
guage models to predict grade levels of docu-
ments. In addition to language models, Heilman
et al (2007) and Schwarm and Ostendorf (2005)
also use some syntactic features to estimate the
grade level of texts.
Pitler and Nenkova (2008) consider a differ-
ent task of predicting text quality for an educated
adult audience. Their system predicts readabil-
ity of texts from Wall Street Journal using lex-
ical, syntactic and discourse features. Kanungo
and Orr (2009) consider the task of predicting
readability of web summary snippets produced by
search engines. Using simple surface level fea-
tures like the number of characters and syllables
per word, capitalization, punctuation, ellipses etc.
they train a regression model to predict readability
values.
Our work differs from this previous research in
several ways. Firstly, the task we have consid-
ered is different, we predict the readability of gen-
eral documents, not their grade level. The doc-
uments in our data are also not from any single
domain, genre or reader group, which makes our
task more general. The data includes human writ-
ten as well as machine generated documents. The
task and the data has been set this way because it
is aimed at filtering out documents of poor quality
for later processing, like for extracting machine-
processable knowledge from them. Extracting
knowledge from openly found text, such as from
the internet, is becoming popular but the quality
of text found ?in the wild?, like found through
searching the internet, vary considerably in qual-
ity and genre. If the text is of poor readability then
it is likely to lead to extraction errors and more
problems downstream. If the readers are going
to be humans instead of machines, then also it is
best to filter out poorly written documents. Hence
identifying readability of general text documents
coming from various sources and genres is an im-
portant task. We are not aware of any other work
which has considered such a task.
Secondly, we note that all of the above ap-
proaches that use language models train a lan-
guage model for each difficulty level using the
training data for that level. However, since the
amount of training data annotated with levels
is limited, they can not train higher-order lan-
guage models, and most just use unigram models.
In contrast, we employ more powerful language
models trained on large quantities of generic text
(which is not from the training data for readabil-
ity) and use various features obtained from these
language models to predict readability. Thirdly,
we use a more sophisticated combination of lin-
guistic features derived from various syntactic
parsers and language models than any previous
work. We also present ablation results for differ-
ent sets of features. Fourthly, given that the doc-
uments in our data are not from a particular genre
but from a mix of genres, we also train genre-
specific language models and show that including
these as features improves readability predictions.
Finally, we also show comparison between var-
ious machine learning algorithms for predicting
readability, none of the previous work compared
learning algorithms.
3 Readability Data
The readability data was collected and re-
leased by LDC. The documents were collected
547
from the following diverse sources or genres:
newswire/newspaper text, weblogs, newsgroup
posts, manual transcripts, machine translation out-
put, closed-caption transcripts and Wikipedia arti-
cles. Documents for newswire, machine transla-
tion and closed captioned genres were collected
automatically by first forming a candidate pool
from a single collection stream and then randomly
selecting documents. Documents for weblogs,
newsgroups and manual transcripts were also col-
lected in the same way but were then reviewed
by humans to make sure they were not simply
spam articles or something objectionable. The
Wikipedia articles were collected manually, by
searching through a data archive or the live web,
using keyword and other search techniques. Note
that the information about genres of the docu-
ments is not available during testing and hence
was not used when training our readability model.
A total of 540 documents were collected in this
way which were uniformly distributed across the
seven genres. Each document was then judged
for its readability by eight expert human judges.
These expert judges are native English speakers
who are language professionals and who have
specialized training in linguistic analysis and an-
notation, including the machine translation post-
editing task. Each document was also judged for
its readability by six to ten naive human judges.
These non-expert (naive) judges are native En-
glish speakers who are not language professionals
(e.g. editors, writers, English teachers, linguistic
annotators, etc.) and have no specialized language
analysis or linguistic annotation training. Both ex-
pert and naive judges provided readability judg-
ments using a customized web interface and gave
a rating on a 5-point scale to indicate how readable
the passage is (where 1 is lowest and 5 is highest
readability) where readability is defined as a sub-
jective judgment of how easily a reader can extract
the information the writer or speaker intended to
convey.
4 Readability Model
We want to answer the question whether a
machine can accurately estimate readability as
judged by a human. Therefore, we built a
machine-learning system that predicts the read-
ability of documents by training on expert hu-
man judgements of readability. The evaluation
was then designed to compare how well machine
and naive human judges predict expert human
judgements. In order to make the machine?s pre-
dicted score comparable to a human judge?s score
(details about our evaluation metrics are in Sec-
tion 6.1), we also restricted the machine scores to
integers. Hence, the task is to predict an integer
score from 1 to 5 that measures the readability of
the document.
This task could be modeled as a multi-class
classification problem treating each integer score
as a separate class, as done in some of the previ-
ous work (Si and Callan, 2001; Collins-Thompson
and Callan, 2004). However, since the classes
are numerical and not unrelated (for example, the
score 2 is in between scores 1 and 3), we de-
cided to model the task as a regression problem
and then round the predicted score to obtain the
closest integer value. Preliminary results verified
that regression performed better than classifica-
tion. Heilman et al (2008) also found that it
is better to treat the readability scores as ordinal
than as nominal. We take the average of the ex-
pert judge scores for each document as its gold-
standard score. Regression was also used by Ka-
nungo and Orr (2009), although their evaluation
did not constrain machine scores to be integers.
We tested several regression algorithms avail-
able in the Weka1 machine learning package, and
in Section 6.2 we report results for several which
performed best. The next section describes the
numerically-valued features that we used as input
for regression.
5 Features for Predicting Readability
Good input features are critical to the success of
any regression algorithm. We used three main cat-
egories of features to predict readability: syntac-
tic features, language-model features, and lexical
features, as described below.
5.1 Features Based on Syntax
Many times, a document is found to be unreadable
due to unusual linguistic constructs or ungram-
1http://www.cs.waikato.ac.nz/ml/weka/
548
matical language that tend to manifest themselves
in the syntactic properties of the text. There-
fore, syntactic features have been previously used
(Bernth, 1997) to gauge the ?clarity? of written
text, with the goal of helping writers improve their
writing skills. Here too, we use several features
based on syntactic analyses. Syntactic analyses
are obtained from the Sundance shallow parser
(Riloff and Phillips, 2004) and from the English
Slot Grammar (ESG) (McCord, 1989).
Sundance features: The Sundance system is a
rule-based system that performs a shallow syntac-
tic analysis of text. We expect that this analysis
over readable text would be ?well-formed?, adher-
ing to grammatical rules of the English language.
Deviations from these rules can be indications of
unreadable text. We attempt to capture such de-
viations from grammatical rules through the fol-
lowing Sundance features computed for each text
document: proportion of sentences with no verb
phrases, average number of clauses per sentence,
average sentence length in tokens, average num-
ber of noun phrases per sentence, average number
of verb phrases per sentence, average number of
prepositional phrases per sentence, average num-
ber of phrases (all types) per sentence and average
number of phrases (all types) per clause.
ESG features: ESG uses slot grammar rules to
perform a deeper linguistic analysis of sentences
than the Sundance system. ESG may consider
several different interpretations of a sentence, be-
fore deciding to choose one over the other inter-
pretations. Sometimes ESG?s grammar rules fail
to produce a single complete interpretation of a
sentence, in which case it generates partial parses.
This typically happens in cases when sentences
are ungrammatical, and possibly, less readable.
Thus, we use the proportion of such incomplete
parses within a document as a readability feature.
In case of extremely short documents, this propor-
tion of incomplete parses can be misleading. To
account for such short documents, we introduce
a variation of the above incomplete parse feature,
by weighting it with a log factor as was done in
(Riloff, 1996; Thelen and Riloff, 2002).
We also experimented with some other syn-
tactic features such as average sentence parse
scores from Stanford parser and an in-house maxi-
mum entropy statistical parer, average constituent
scores etc., however, they slightly degraded the
performance in combination with the rest of the
features and hence we did not include them in
the final set. One possible explanation could be
that averaging diminishes the effect of low scores
caused by ungrammaticality.
5.2 Features Based on Language Models
A probabilistic language model provides a predic-
tion of how likely a given sentence was generated
by the same underlying process that generated a
corpus of training documents. In addition to a
general n-gram language model trained on a large
body of text, we also exploit language models
trained to recognize specific ?genres? of text. If a
document is translated by a machine, or casually
produced by humans for a weblog or newsgroup,
it exhibits a character that is distinct from docu-
ments that go through a dedicated editing process
(e.g., newswire and Wikipedia articles). Below
we describe features based on generic as well as
genre-specific language models.
Normalized document probability: One obvi-
ous proxy for readability is the score assigned to
a document by a generic language model (LM).
Since the language model is trained on well-
written English text, it penalizes documents de-
viating from the statistics collected from the LM
training documents. Due to variable document
lengths, we normalize the document-level LM
score by the number of words and compute the
normalized document probability NP (D) for a
document D as follows:
NP (D) =
(
P (D|M)
) 1
|D| , (1)
where M is a general-purpose language model
trained on clean English text, and |D| is the num-
ber of words in the document D.
Perplexities from genre-specific language mod-
els: The usefulness of LM-based features in
categorizing text (McCallum and Nigam, 1998;
Yang and Liu, 1999) and evaluating readability
(Collins-Thompson and Callan, 2004; Heilman
et al, 2007) has been investigated in previous
work. In our experiments, however, since doc-
uments were acquired through several different
channels, such as machine translation or web logs,
549
we also build models that try to predict the genre
of a document. Since the genre information for
many English documents is readily available, we
trained a series of genre-specific 5-gram LMs us-
ing the modified Kneser-Ney smoothing (Kneser
and Ney, 1995; Stanley and Goodman, 1996). Ta-
ble 1 contains a list of a base LM and genre-
specific LMs.
Given a document D consisting of tokenized
word sequence {wi : i = 1, 2, ? ? ? , |D|}, its per-
plexity L(D|Mj) with respect to a LM Mj is
computed as:
L(D|Mj) = e
(
? 1|D|
P|D|
i=1 logP (wi|hi;Mj)
)
, (2)
where |D| is the number of words in D and hi are
the history words for wi, and P (wi|hi;Mj) is the
probability Mj assigns to wi, when it follows the
history words hi.
Posterior perplexities from genre-specific lan-
guagemodels: While perplexities computed from
genre-specific LMs reflect the absolute probabil-
ity that a document was generated by a specific
model, a model?s relative probability compared to
other models may be a more useful feature. To this
end, we also compute the posterior perplexity de-
fined as follows. Let D be a document, {Mi}Gi=1
be G genre-specific LMs, and L(D|Mi) be the
perplexity of the document D with respect to Mi,
then the posterior perplexity, R(Mi|D), is de-
fined as:
R(Mi|D) =
L(D|Mi)?G
j=1 L(D|Mj)
. (3)
We use the term ?posterior? because if a uni-
form prior is adopted for {Mi}Gi=1,R(Mi|D) can
be interpreted as the posterior probability of the
genre LM Mi given the document D.
5.3 Lexical Features
The final set of features involve various lexical
statistics as described below.
Out-of-vocabulary (OOV) rates: We conjecture
that documents containing typographical errors
(e.g., for closed-caption and web log documents)
may receive low readability ratings. Therefore,
we compute the OOV rates of a document with re-
spect to the various LMs shown in Table 1. Since
modern LMs often have a very large vocabulary,
to get meaningful OOV rates, we truncate the vo-
cabularies to the top (i.e., most frequent) 3000
words. For the purpose of OOV computation, a
document D is treated as a sequence of tokenized
words {wi : i = 1, 2, ? ? ? , |D|}. Its OOV rate
with respect to a (truncated) vocabulary V is then:
OOV (D|V) =
?D
i=1 I(wi /? V)
|D| , (4)
where I(wi /? V) is an indicator function taking
value 1 if wi is not in V , and 0 otherwise.
Ratio of function words: A characteristic of doc-
uments generated by foreign speakers and ma-
chine translation is a failure to produce certain
function words, such as ?the,? or ?of.? So we pre-
define a small set of function words (mainly En-
glish articles and frequent prepositions) and com-
pute the ratio of function words over the total
number words in a document:
RF (D) =
?D
i=1 I(wi ? F)
|D| , (5)
where I(wi ? F) is 1 ifwi is in the set of function
words F , and 0 otherwise.
Ratio of pronouns: Many foreign languages that
are source languages of machine-translated docu-
ments are pronoun-drop languages, such as Ara-
bic, Chinese, and romance languages. We conjec-
ture that the pronoun ratio may be a good indica-
tor whether a document is translated by machine
or produced by humans, and for each document,
we first run a POS tagger, and then compute the
ratio of pronouns over the number of words in the
document:
RP (D) =
?D
i=1 I(POS(wi) ? P)
|D| , (6)
where I(POS(wi) ? F) is 1 if the POS tag of wi
is in the set of pronouns, P , and 0 otherwise.
Fraction of known words: This feature measures
the fraction of words in a document that occur
either in an English dictionary or a gazetteer of
names of people and locations.
6 Experiments
This section describes the evaluation methodol-
ogy and metrics and presents and discusses our
550
Genre Training Size(M tokens) Data Sources
base 5136.8 mostly LDC?s GigaWord set
NW 143.2 newswire subset of base
NG 218.6 newsgroup subset of base
WL 18.5 weblog subset of base
BC 1.6 broadcast conversation subset of base
BN 1.1 broadcast news subset of base
wikipedia 2264.6 Wikipedia text
CC 0.1 closed caption
ZhEn 79.6 output of Chinese to English Machine Translation
ArEn 126.8 output of Arabic to English Machine Translation
Table 1: Genre-specific LMs: the second column contains the number of tokens in LM training data (in million tokens).
experimental results. The results of the official
evaluation task are also reported.
6.1 Evaluation Metric
The evaluation process for the DARPAMRP read-
ability test was designed by the evaluation team
led by SAIC. In order to compare a machine?s
predicted readability score to those assigned by
the expert judges, the Pearson correlation coef-
ficient was computed. The mean of the expert-
judge scores was taken as the gold-standard score
for a document.
To determine whether the machine predicts
scores closer to the expert judges? scores than
what an average naive judge would predict, a
sampling distribution representing the underlying
novice performance was computed. This was ob-
tained by choosing a random naive judge for every
document, calculating the Pearson correlation co-
efficient with the expert gold-standard scores and
then repeating this procedure a sufficient number
of times (5000). The upper critical value was set
at 97.5% confidence, meaning that if the machine
performs better than the upper critical value then
we reject the null hypothesis that machine scores
and naive scores come from the same distribution
and conclude that the machine performs signifi-
cantly better than naive judges in matching the ex-
pert judges.
6.2 Results and Discussion
We evaluated our readability system on the dataset
of 390 documents which was released earlier dur-
ing the training phase of the evaluation task. We
Algorithm Correlation
Bagged Decision Trees 0.8173
Decision Trees 0.7260
Linear Regression 0.7984
SVM Regression 0.7915
Gaussian Process Regression 0.7562
Naive Judges
Upper Critical Value 0.7015
Distribution Mean 0.6517
Baselines
Uniform Random 0.0157
Proportional Random -0.0834
Table 2: Comparing different algorithms on the readability
task using 13-fold cross-validation on the 390 documents us-
ing all the features. Exceeding the upper critical value of the
naive judges? distribution indicates statistically significantly
better predictions than the naive judges.
used stratified 13-fold cross-validation in which
the documents from various genres in each fold
was distributed in roughly the same proportion as
in the overall dataset. We first conducted experi-
ments to test different regression algorithms using
all the available features. Next, we ablated various
feature sets to determine how much each feature
set was contributing to making accurate readabil-
ity judgements. These experiments are described
in the following subsections.
6.2.1 Regression Algorithms
We used several regression algorithms available
in theWeka machine learning package and Table 2
shows the results obtained. The default values
551
Feature Set Correlation
Lexical 0.5760
Syntactic 0.7010
Lexical + Syntactic 0.7274
Language Model based 0.7864
All 0.8173
Table 3: Comparison of different linguistic feature sets.
in Weka were used for all parameters, changing
these values did not show any improvement. We
used decision tree (reduced error pruning (Quin-
lan, 1987)) regression, decision tree regression
with bagging (Breiman, 1996), support vector re-
gression (Smola and Scholkopf, 1998) using poly-
nomial kernel of degree two,2 linear regression
and Gaussian process regression (Rasmussen and
Williams, 2006). The distribution mean and the
upper critical values of the correlation coefficient
distribution for the naive judges are also shown in
the table.
Since they are above the upper critical value, all
algorithms predicted expert readability scores sig-
nificantly more accurately than the naive judges.
Bagged decision trees performed slightly better
than other methods. As shown in the following
section, ablating features affects predictive accu-
racy much more than changing the regression al-
gorithm. Therefore, on this task, the choice of re-
gression algorithm was not very critical once good
readability features are used. We also tested two
simple baseline strategies: predicting a score uni-
formly at random, and predicting a score propor-
tional to its frequency in the training data. As
shown in the last two rows of Table 2, these base-
lines perform very poorly, verifying that predict-
ing readability on this dataset as evaluated by our
evaluation metric is not trivial.
6.2.2 Ablations with Feature Sets
We evaluated the contributions of different fea-
ture sets through ablation experiments. Bagged
decision-tree was used as the regression algorithm
in all of these experiments. First we compared
syntactic, lexical and language-model based fea-
tures as described in Section 5, and Table 3 shows
2Polynomial kernels with other degrees and RBF kernel
performed worse.
the results. The language-model feature set per-
forms the best, but performance improves when it
is combined with the remaining features. The lex-
ical feature set by itself performs the worst, even
below the naive distribution mean (shown in Ta-
ble 2); however, when combined with syntactic
features it performs well.
In our second ablation experiment, we com-
pared the performance of genre-independent and
genre-based features. Since the genre-based fea-
tures exploit knowledge of the genres of text used
in the MRP readability corpus, their utility is
somewhat tailored to this specific corpus. There-
fore, it is useful to evaluate the performance of the
system when genre information is not exploited.
Of the lexical features described in subsection 5.3,
the ratio of function words, ratio of pronoun words
and all of the out-of-vocabulary rates except for
the base language model are genre-based features.
Out of the language model features described in
the Subsection 5.2, all of the perplexities except
for the base language model and all of the poste-
rior perplexities3 are genre-based features. All of
the remaining features are genre-independent. Ta-
ble 4 shows the results comparing these two fea-
ture sets. The genre-based features do well by
themselves but the rest of the features help fur-
ther improve the performance. While the genre-
independent features by themselves do not exceed
the upper critical value of the naive judges? dis-
tribution, they are very close to it and still out-
perform its mean value. These results show that
for a dataset like ours, which is composed of a mix
of genres that themselves are indicative of read-
ability, features that help identify the genre of a
text improve performance significantly.4 For ap-
plications mentioned in the introduction and re-
lated work sections, such as filtering less readable
documents from web-search, many of the input
documents could come from some of the common
genres considered in our dataset.
In our final ablation experiment, we evaluated
3Base model for posterior perplexities is computed using
other genre-based LMs (equation 3) hence it can not be con-
sidered genre-independent.
4We note that none of the genre-based features were
trained on supervised readability data, but were trained on
readily-available large unannotated corpora as shown in Ta-
ble 1.
552
Feature Set Correlation
Genre-independent 0.6978
Genre-based 0.7749
All 0.8173
Table 4: Comparison of genre-independent and genre-
based feature sets.
Feature Set By itself Ablated
from All
Sundance features 0.5417 0.7993
ESG features 0.5841 0.8118
Perplexities 0.7092 0.8081
Posterior perplexities 0.7832 0.7439
Out-of-vocabulary rates 0.3574 0.8125
All 0.8173 -
Table 5: Ablations with some individual feature sets.
the contribution of various individual feature sets.
Table 5 shows that posterior perplexities perform
the strongest on their own, but without them, the
remaining features also do well. When used by
themselves, some feature sets perform below the
naive judges? distribution mean, however, remov-
ing them from the rest of the feature sets de-
grades the performance. This shows that no indi-
vidual feature set is critical for good performance
but each further improves the performance when
added to the rest of the feature sets.
6.3 Official Evaluation Results
An official evaluation was conducted by the eval-
uation team SAIC on behalf of DARPA in which
three teams participated including ours. The eval-
uation task required predicting the readability of
150 test documents using the 390 training docu-
ments. Besides the correlation metric, two addi-
tional metrics were used. One of them computed
for a document the difference between the aver-
age absolute difference of the naive judge scores
from the mean expert score and the absolute dif-
ference of the machine?s score from the mean ex-
pert score. This was then averaged over all the
documents. The other one was ?target hits? which
measured if the predicted score for a document
fell within the width of the lowest and the highest
expert scores for that document, and if so, com-
System Correl. Avg. Diff. Target Hits
Our (A) 0.8127 0.4844 0.4619
System B 0.6904 0.3916 0.4530
System C 0.8501 0.5177 0.4641
Upper CV 0.7423 0.0960 0.3713
Table 6: Results of the systems that participated in the
DARPA?s readability evaluation task. The three metrics used
were correlation, average absolute difference and target hits
measured against the expert readability scores. The upper
critical values are for the score distributions of naive judges.
puted a score inversely proportional to that width.
The final target hits score was then computed by
averaging it across all the documents. The upper
critical values for these metrics were computed in
a way analogous to that for the correlation met-
ric which was described before. Higher score is
better for all the three metrics. Table 6 shows the
results of the evaluation. Our system performed
favorably and always scored better than the up-
per critical value on each of the metrics. Its per-
formance was in between the performance of the
other two systems. The performances of the sys-
tems show that the correlation metric was the most
difficult of the three metrics.
7 Conclusions
Using regression over a diverse combination of
syntactic, lexical and language-model based fea-
tures, we built a system for predicting the read-
ability of natural-language documents. The sys-
tem accurately predicts readability as judged by
linguistically-trained expert human judges and
exceeds the accuracy of naive human judges.
Language-model based features were found to be
most useful for this task, but syntactic and lexical
features were also helpful. We also found that for
a corpus consisting of documents from a diverse
mix of genres, using features that are indicative
of the genre significantly improve the accuracy of
readability predictions. Such a system could be
used to filter out less readable documents for ma-
chine or human processing.
Acknowledgment
This research was funded by Air Force Contract
FA8750-09-C-0172 under the DARPA Machine
Reading Program.
553
References
Bernth, Arendse. 1997. Easyenglish: A tool for improv-
ing document quality. In Proceedings of the fifth con-
ference on Applied Natural Language Processing, pages
159?165, Washington DC, April.
Breiman, Leo. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
Chall, J.S. and E. Dale. 1995. Readability Revisited: The
New Dale-Chall Readability Formula. Brookline Books,
Cambridge, MA.
Collins-Thompson, Kevyn and James P. Callan. 2004. A
language modeling approach to predicting reading diffi-
culty. In Proc. of HLT-NAACL 2004, pages 193?200.
Fry, E. 1990. A readability formula for short passages. Jour-
nal of Reading, 33(8):594?597.
Gunning, R. 1952. The Technique of Clear Writing.
McGraw-Hill, Cambridge, MA.
Heilman, Michael, Kevyn Collins-Thompson, Jamie Callan,
and Maxine Eskenazi. 2007. Combining lexical and
grammatical features to improve readability measures for
first and second language texts. In Proc. of NAACL-HLT
2007, pages 460?467, Rochester, New York, April.
Heilman, Michael, Kevyn Collins-Thompson, and Maxine
Eskenazi. 2008. An analysis of statistical models and fea-
tures for reading difficulty prediction. In Proceedings of
the Third Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 71?79, Columbus,
Ohio, June. Association for Computational Linguistics.
Kanungo, Tapas and David Orr. 2009. Predicting the read-
ability of short web summaries. In Proc. of WSDM 2009,
pages 202?211, Barcelona, Spain, February.
Kincaid, J. P., R. P. Fishburne, R. L. Rogers, and B.S.
Chissom. 1975. Derivation of new readability formulas
for navy enlisted personnel. Technical Report Research
Branch Report 8-75, Millington, TN: Naval Air Station.
Kneser, Reinhard and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc. of
ICASSP-95, pages 181?184.
McCallum, Andrew and Kamal Nigam. 1998. A comparison
of event models for naive Bayes text classification. In Pa-
pers from the AAAI-98 Workshop on Text Categorization,
pages 41?48, Madison, WI, July.
McCord, Michael C. 1989. Slot grammar: A system for
simpler construction of practical natural language gram-
mars. In Proceedings of the International Symposium on
Natural Language and Logic, pages 118?145, May.
McLaughlin, G. H. 1969. Smog: Grading: A new readabil-
ity formula. Journal of Reading, 12:639?646.
Pitler, Emily and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proc. of EMNLP 2008, pages 186?195,
Waikiki,Honolulu,Hawaii, October.
Quinlan, J. R. 1987. Simplifying decision trees. Interna-
tional Journal of Man-Machine Studies, 27:221?234.
Rasmussen, Carl and Christopher Williams. 2006. Gaussian
Processes for Machine Leanring. MIT Press, Cambridge,
MA.
Riloff, E. and W. Phillips. 2004. An introduction to the Sun-
dance and Autoslog systems. Technical Report UUCS-
04-015, University of Utah School of Computing.
Riloff, Ellen. 1996. Automatically generating extraction
patterns from untagged text. In Proc. of 13th Natl. Conf.
on Artificial Intelligence (AAAI-96), pages 1044?1049,
Portland, OR.
Schwarm, Sarah E. andMari Ostendorf. 2005. Reading level
assessment using support vector machines and statistical
language models. In Proc. of ACL 2005, pages 523?530,
Ann Arbor, Michigan.
Si, Luo and James P. Callan. 2001. A statistical model for
scientific readability. In Proc. of CIKM 2001, pages 574?
576.
Smola, Alex J. and Bernhard Scholkopf. 1998. A tutorial
on support vector regression. Technical Report NC2-TR-
1998-030, NeuroCOLT2.
Stanley, Chen and Joshua Goodman. 1996. An empirical
study of smoothing techniques for language modeling. In
Proc. of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL-96), pages 310?318.
Stenner, A. J., I. Horabin, D. R. Smith, and M. Smith. 1988.
The Lexile Framework. Durham, NC: MetaMetrics.
Thelen, M. and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern con-
texts. In Proc. of EMNLP 2002, Philadelphia, PA, July.
Yang, Yiming and Xin Liu. 1999. A re-examination of text
cateogrization methods. In Proc. of 22nd Intl. ACM SI-
GIR Conf. on Research and Development in Information
Retrieval, pages 42?48, Berkeley, CA.
554
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 335?345,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Improving Mention Detection Robustness to Noisy Input
Radu Florian, John F. Pitrelli, Salim Roukos and Imed Zitouni
IBM T.J. Watson Research Center
Yorktown Heights, NY, U.S.A.
{raduf,pitrelli,roukos,izitouni}us.ibm.com
Abstract
Information-extraction (IE) research typically
focuses on clean-text inputs. However, an IE
engine serving real applications yields many
false alarms due to less-well-formed input.
For example, IE in a multilingual broadcast
processing system has to deal with inaccu-
rate automatic transcription and translation.
The resulting presence of non-target-language
text in this case, and non-language mate-
rial interspersed in data from other applica-
tions, raise the research problem of making
IE robust to such noisy input text. We ad-
dress one such IE task: entity-mention de-
tection. We describe augmenting a statistical
mention-detection system in order to reduce
false alarms from spurious passages. The di-
verse nature of input noise leads us to pursue
a multi-faceted approach to robustness. For
our English-language system, at various miss
rates we eliminate 97% of false alarms on in-
puts from other Latin-alphabet languages. In
another experiment, representing scenarios in
which genre-specific training is infeasible, we
process real financial-transactions text con-
taining mixed languages and data-set codes.
On these data, because we do not train on data
like it, we achieve a smaller but significant im-
provement. These gains come with virtually
no loss in accuracy on clean English text.
1 Introduction
Information-extraction (IE) research is typically per-
formed on clean text in a predetermined language.
Lately, IE has improved to the point of being usable
for some real-world tasks whose accuracy require-
ments are reachable with current technology. These
uses include media monitoring, topic alerts, sum-
marization, population of databases for advanced
search, etc. These uses often combine IE with tech-
nologies such as speech recognition, machine trans-
lation, topic clustering, and information retrieval.
The propagation of IE technology from isolated
use to aggregates with such other technologies, from
NLP experts to other types of computer scientists,
and from researchers to users, feeds back to the IE
research community the need for additional inves-
tigation which we loosely refer to as ?information-
extraction robustness? research. For example:
1. Broadcast monitoring demands that IE handle
as input not only clean text, but also the tran-
scripts output by speech recognizers.
2. Multilingual applications, and the imperfection
of translation technology, require IE to contend
with non-target-language text input (Pitrelli et
al., 2008).
3. Naive users at times input to IE other material
which deviates from clean text, such as a PDF
file that ?looks? like plain text.
4. Search applications require IE to deal with
databases which not only possess clean text but
at times exhibit other complications like mark-
up codes particular to narrow, application-
specific data-format standards, for example, the
excerpt from a financial-transactions data set
shown in Figure 1.
Legacy industry-specific standards, such as il-
lustrated in this example, are part of long-
established processes which are cumbersome
to convert to a more-modern database format.
Transaction data sets typically build up over a
period of years, and as seen here, can exhibit
335
:54D://121000358
BANK OF BOSTON
:55D:/0148280005
NEVADA DEPT.OF VET.94C RECOV.FD
-5:MAC:E19DECA8CHK:641EB09B8968
USING OF FIELD 59: ONLY /INS/ WHEN
FOLLOWED BY BCC CODE IN CASE
OF QUESTIONS DONT HESITATE TO
CONTACT US QUOTING REFERENCE
NON-STC CHARGES OR VIA E-MAIL:
YOVANKA(UL)BRATASOVA(AT)BOA.CZ.
BEST REGARDS
BANKA OBCHODNIKA, A.S. PRAGUE, CZ
:58E::ADTX//++ ADDITIONAL
INFORMATION ++ PLEASE BE
INFORMED THAT AS A RESULT OF
THE PURCHASE OFFER ENDED ON 23
MAR 2008 CALDRADE LTD. IS
POSSESSING WITH MORE THEN 90
PER CENT VOTING RIGHT OF SLICE.
THEREFOR CALDRADE LTD. IS
EXERCISING PURCHASE RIGHTS
FOR ALL SLICE SHARES WHICH ARE
CURRENTLY NOT INHIS OWN.
PURCHASE PRICE: HUF 1.940 PER
SHARE. PLEASE :58E::ADTX//NOTE
THAT THOSE SHARES WHICH WILL
NOT BE PRESENTED TO THE OFFER
WILL BE CANCELLED AND INVALID.
:58:SIE SELBST
TRN/REF:515220 035
:78:RUECKGABE DES BETRAGES LT.
ANZBA43 M ZWECKS RUECKGABE IN
AUD. URSPR. ZU UNSEREM ZA MIT
REF. 0170252313279065 UND IHRE
RUECKG. :42:/BNF/UNSERE REF:
Figure 1: Example application-specific text, in this
case from financial transactions.
peculiar mark-up interspersed with meaning-
ful text. They also suffer complications arising
from limited-size entry fields and a diversity
of data-entry personnel, leading to effects like
haphazard abbreviation and improper spacing,
as shown. These issues greatly complicate the
IE problem, particularly considering that adapt-
ing IE to such formats is hampered by the exis-
tence of a multitude of such ?standards? and by
lack of sufficient annotated data in each one.
A typical state-of-the-art statistical IE engine will
happily process such ?noisy? inputs, and will typ-
ically provide garbage-in/garbage-out performance,
embarrassingly reporting spurious ?information? no
human would ever mistake. Yet it is also inappro-
priate to discard such documents wholesale: even
poor-quality inputs may have relevant information
interspersed. This information can include accurate
speech-recognition output, names which are recog-
nizable even in wrong-language material, and clean
target-language passages interleaved with the mark-
up. Thus, here we address methods to make IE ro-
bust to such varied-quality inputs. Specifically, our
overall goals are
? to skip processing non-language material such
as standard or database-specific mark-up,
? to process all non-target-language text cau-
tiously, catching interspersed target-language
text as well as text which is compatible with
the target language, e.g. person names which
are the same in the target- and non-target lan-
guage, and
? to degrade gracefully when processing anoma-
lous target-language material,
while minimizing any disruption of the processing
of clean, target-language text, and avoiding any ne-
cessity for explicit pre-classification of the genre of
material being input to the system. Such explicit
classification would be impractical in the presence
of the interleaving and the unconstrained data for-
mats from unpredetermined sources.
We begin our robustness work by addressing an
important and basic IE task: mention detection
(MD). MD is the task of identifying and classifying
textual references to entities in open-domain texts.
Mentions may be of type ?named? (e.g. John, Las
Vegas), ?nominal? (e.g. engineer, dentist)
or ?pronominal? (e.g. they, he). A mention also
336
has a specific class which describes the type of en-
tity it refers to. For instance, consider the following
sentence:
Julia Gillard, prime
minister of Australia,
declared she will enhance
the country?s economy.
Here we see three mentions of one person en-
tity: Julia Gillard, prime minister, and
she; these mentions are of type named, nominal,
and pronominal, respectively. Australia and
country are mentions of type named and nominal,
respectively, of a single geopolitical entity. Thus, the
MD task is a more general and complex task than
named-entity recognition, which aims at identifying
and classifying only named mentions.
Our approach to IE has been to use language-
independent algorithms, in order to facilitate reuse
across languages, but we train them with language-
specific data, for the sake of accuracy. Therefore, in-
put is expected to be predominantly in a target lan-
guage. However, real-world data genres inevitably
include some mixed-language/non-linguistic input.
Genre-specific training is typically infeasible due
to such application-specific data sets being unanno-
tated, motivating this line of research. Therefore, the
goal of this study is to investigate schemes to make a
language-specific MD engine robust to the types of
interspersed non-target material described above. In
these initial experiments, we work with English as
the target language, though we aim to make our ap-
proach to robustness as target-language-independent
as possible.
While our ultimate goal is a language-
independent approach to robustness, in these
initial experiments, English is the target language.
However, we process mixed-language material
including real-world data with its own peculiar
mark-up, text conventions including abbreviations,
and mix of languages, with the goal of English MD.
We approach robust MD using a multi-stage strat-
egy. First, non-target-character-set passages (here,
non-Latin-alphabet) are identified and marked for
non-processing. Then, following word-tokenization,
we apply a language classifier to a sliding variable-
length set of windows in order to generate fea-
tures for each word indicative of how much the text
around that word resembles good English, primar-
ily in comparison to other Latin-alphabet languages.
These features are used in a separate maximum-
entropy classifier whose output is a single feature to
add to the MD classifier. Additional features, pri-
marily to distinguish English from non-language in-
put, are added to MD as well. An example is the
minimum of the number of letters and the number of
digits in the ?word?, which when greater than zero
often indicates database detritus. Then we run the
MD classifier enhanced with these new robustness-
oriented features. We evaluate using a detection-
error-trade-off (DET) (Martin et al, 1997) anal-
ysis, in addition to traditional precision/recall/F -
measure.
This paper is organized as follows. Section 2 dis-
cusses previous work. Section 3 describes the base-
line maximum-entropy-based MD system. Section 4
introduces enhancements to the system to achieve
robustness. Section 5 describes databases used for
experiments, which are discussed in Section 6, and
Section 7 draws conclusions and plots future work.
2 Previous work on mention detection
The MD task has close ties to named-entity recog-
nition, which has been the focus of much recent re-
search (Bikel et al, 1997; Borthwick et al, 1998;
Tjong Kim Sang, 2002; Florian et al, 2003; Bena-
jiba et al, 2009), and has been at the center of sev-
eral evaluations: MUC-6, MUC-7, CoNLL?02 and
CoNLL?03 shared tasks. Usually, in computational-
linguistics literature, a named entity represents an
instance of either a location, a person, an organi-
zation, and the named-entity-recognition task con-
sists of identifying each individual occurrence of
names of such an entity appearing in the text. As
stated earlier, in this paper we are interested in
identification and classification of textual references
to object/abstraction mentions, which can be either
named, nominal or pronominal. This task has been
a focus of interest in ACE since 2003. The recent
ACE evaluation campaign was in 2008.
Effort to handle noisy data is still limited, espe-
cially for scenarios in which the system at decoding
time does not have prior knowledge of the input data
source. Previous work dealing with unstructured
data assumes the knowledge of the input data source.
As an example, E. Minkov et al (Minkov et al,
2005) assume that the input data is text from e-mails,
and define special features to enhance the detection
of named entities. Miller et al (Miller et al, 2000)
assume that the input data is the output of a speech
or optical character recognition system, and hence
extract new features for better named-entity recog-
nition. In a different research problem, L. Yi et al
eliminate the noisy text from the document before
337
performing data mining (Yi et al, 2003). Hence,
they do not try to process noisy data; instead, they
remove it. The approach we propose in this paper
does not assume prior knowledge of the data source.
Also we do not want to eliminate the noisy data, but
rather attempt to detect the appropriate mentions, if
any, that appear in that portion of the data.
3 Mention-detection algorithm
Similarly to classical NLP tasks such as base phrase
chunking (Ramshaw and Marcus, 1999) and named-
entity recognition (Tjong Kim Sang, 2002), we for-
mulate the MD task as a sequence-classification
problem, by assigning to each word token in the
text a label indicating whether it starts a specific
mention, is inside a specific mention, or is out-
side any mentions. We also assign to every non-
outside label a class to specify entity type e.g. per-
son, organization, location, etc. We are interested
in a statistical approach that can easily be adapted
for several languages and that has the ability to
integrate easily and make effective use of diverse
sources of information to achieve high system per-
formance. This is because, similar to many NLP
tasks, good performance has been shown to depend
heavily on integrating many sources of informa-
tion (Florian et al, 2004). We choose a Maximum
Entropy Markov Model (MEMM) as described pre-
viously (Florian et al, 2004; Zitouni and Florian,
2009). The maximum-entropy model is trained us-
ing the sequential conditional generalized iterative
scaling (SCGIS) technique (Goodman, 2002), and it
uses a Gaussian prior for regularization (Chen and
Rosenfeld, 2000)1.
3.1 Mention detection: standard features
The featues used by our mention detection systems
can be divided into the following categories:
1. Lexical Features Lexical features are imple-
mented as token n-grams spanning the current
token, both preceding and following it. For a
token xi, token n-gram features will contain the
previous n?1 tokens (xi?n+1, . . . xi?1) and the
following n? 1 tokens (xi+1, . . . xi+n?1). Set-
ting n equal to 3 turned out to be a good choice.
2. Gazetteer-based Features The gazetteer-
based features we use are computed on tokens.
1Note that the resulting model cannot really be called a
maximum-entropy model, as it does not yield the model which
has the maximum entropy (the second term in the product), but
rather is a maximum-a-posteriori model.
The gazetteers consist of several class of
dictionaries: including person names, country
names, company names, etc. Dictionar-
ies contain single names such as John or
Boston, and also phrases such as Barack
Obama, New York City, or The United
States. During both training and decoding,
when we encounter in the text a token or a
sequence of tokens that completely matches an
entry in a dictionary, we fire its corresponding
class.
The use of this framework to build MD systems
for clean English text has given very competitive re-
sults at ACE evaluations (Florian et al, 2006). Try-
ing other classifiers is always a good experiment,
which we didn?t pursue here for two reasons: first,
the MEMM system used here is state-of-the-art, as
proven in evaluations and competitions ? while it is
entirely possible that another system might get better
results, we don?t think the difference would be large.
Second, we are interested in ways of improving per-
formance on noisy data, and we expect any system
to observe similar degradation in performance when
presented with unexpected input ? showing results
for multiple classifier types might very well dilute
the message, so we stuck to one classifier type.
4 Enhancements for robustness
As stated above, our goal is to skip spans of charac-
ters which do not lend themselves to target-language
MD, while minimizing impact on MD for target-
language text, with English as the initial target lan-
guage for our experiments. More specifically, our
task is to process data automatically in any unprede-
termined format from any source, during which we
strive to avoid outputting spurious mentions on:
? non-language material, such as mark-up tags
and other data-set detritus, as well as non-text
data such as code or binaries likely mistakenly
submitted to the MD system,
? non-target-character-set material, here, non-
Latin-alphabet material, such as Arabic and
Chinese in their native character sets, and
? target-character-set material not in the target
language, here, Latin-alphabet languages other
than English.
It is important to note that this is not merely
a document-classification problem; this non-target
data is often interspersed with valid input text.
338
Mark-up is the obvious example of interspersing;
however, other categories of non-target data can also
interleave tightly with valid input. A few examples:
? English text is sometimes infixed right in a Chi-
nese sentence, such as
? some translation algorithms will leave un-
changed an untranslatable word, or will
transliterate it into the target language using a
character convention which may not be a stan-
dard known to the MD engine, and
? some target-alphabet-but-non-target-language
material will be compatible with the target
language, particularly people?s names. An
example with English as the target lan-
guage is Barack Obama in the Spanish
text ...presidente de Estados
Unidos, Barack Obama, dijo el
da 24 que ....
Therefore, to minimize needless loss of process-
able material, a robustness algorithm ideally does a
sliding analysis, in which, character-by-character or
word-by-word, material may be deemed to be suit-
able to process. Furthermore, a variety of strategies
will be needed to contend with the diverse nature of
non-target material and the patterns in which it will
appear among valid input.
Accordingly, the following is a summary of algo-
rithmic enhancements to MD:
1. detection of standard file formats, such as
SGML, and associated detagging,
2. segmentation of the file into target- vs. non-
target-character-set passages, such that the lat-
ter not be processed further,
3. tokenization to determine word and sentence
units, and
4. MD, augmented as follows:
? Sentence-level categorization of likeli-
hood of good English.
? If ?clean? English was detected, run the
same clean baseline model as described in
Section 3.
? If the text is determined to be a
bad fit to English, run an alternate
maximum-entropy model that is heavily
based on gazetteers, using only context-
independent (e.g. primarily gazetteer-
based) features, to catch isolated ob-
vious English/English-compatible names
embedded in otherwise-foreign text.
? If in between ?clean? and ?bad?, use
a ?mixed? maximum-entropy MD model
whose training data and feature set are
augmented to handle interleaving of En-
glish with mark-up and other languages.
These MD-algorithm enhancements will be de-
scribed in the following subsections.
4.1 Detection and detagging for standard file
formats
Some types of mark-up are well-known standards,
such as SGML (Warmer and van Egmond, 1989).
Clearly the optimal way of dealing with them is to
apply detectors of these specific formats, and associ-
ated detaggers, as done previously (Yi et al, 2003).
For this reason, standard mark-up is not a subject of
the current study; rather, our concern is with mark-
up peculiar to specific data sets, as described above,
and so while this step is part of our overall strategy,
it is not employed in the present experiments.
4.2 Character-set segmentation
Some entity mentions may be recognizable in a non-
target language which shares the target-language?s
character set, for example, a person?s name recog-
nizable by English speakers in an otherwise-not-
understandable Spanish sentence. However, non-
target character sets, such as Arabic and Chinese
when processing English, represent pure noise for
an IE system. Therefore, deterministic character-
set segmentation is applied, to mark non-target-
character-set passages for non-processing by the re-
mainder of the system, or, in a multilingual system,
to be diverted to a subsystem suited to process that
character set. Characters which can be ambiguous
with regard to character set, such as some punctua-
tion marks, are attached to target-character-set pas-
sages when possible, but are not considered to break
non-target-character-set passages surrounding them
on both sides.
4.3 Tokenization
Subsequent processing is based on determination of
the language of target-alphabet text. The fundamen-
tal unit of such processing is target-alphabet word,
necessitating tokenization at this point into word-
level units. This step includes punctuation sepa-
339
ration as well as the detction of sentence bound-
ary (Zimmerman et al, 2006).
4.4 Robust mention detection
After preprocessing steps presented earlier, we de-
tect mentions using a cascaded approach that com-
bines several MD classifiers. Our goal is to select
among maximum-entropy MD classifiers trained
separately to represent different degrees of ?nois-
iness? occurring in many genres of data, includ-
ing machine-translation output, informal communi-
cations, mixed-language material, varied forms of
non-standard database mark-up, etc. We somewhat-
arbitrarily choose to employ three classifiers as de-
scribed below. We select a classifier based on a
sentence-level determination of the material?s fit to
the target language. First, we build an n-gram lan-
guage model on clean target-language training text.
This language model is used to compute the perplex-
ity (PP ) of each sentence during decoding. The
PP indicates the quality of the text in the target-
language (i.e. English) (Brown et al, 1992); the
lower the PP , the cleaner the text. A sentence
with a PP lower than a threshold ?1 is considered
?clean? and hence the ?clean? baseline MD model
described in Section 3 is used to detect mentions
of this sentence. The clean MD model has access
to standard features described in Section 3.1. In
the case where a sentence looks particularly badly
matched to the target language, defined as PP > ?2,
we use a ?gazetteer-based? model based on a dic-
tionary look-up to detect mentions; we retreat to
seeking known mentions in a context-independent
manner reflecting that most of the context consists
of out-of-vocabulary words. The gazetteer-based
MD model has access only to gazetteer information
and does not look to lexical context during decod-
ing, reflecting the likelihood that in this poor ma-
terial, words surrounding any recognizable mention
are foreign and therefore unusable. In the case of an
in-between determination, that is, a sentence with
?1 < PP < ?2, we use a ?mixed? MD model, based
on augmenting the training data set and the feature
set as described in the next section. The values of ?1
and ?2 are estimated empirically on a separate devel-
opment data set that is also used to tune the Gaussian
prior (Chen and Rosenfeld, 2000). This set contains
a mix of clean English and Latin-alphabet-but-non-
English text that is not used for traning and evalua-
tion.
The advantage of this combination strategy is that
we do not need pre-defined knowledge of the text
source in order to apply an appropriate model. The
selection of the appropriate model to use for de-
coding is done automatically based on PP value of
the sentence. We will show in the experiments sec-
tion how this combination strategy is effective not
only in maintaining good performance on a clean
English text but also in improving performance on
non-English data when compared to other source-
specific MD models.
4.5 Mixed mention detection model
The mixed MD model is designed to process ?sen-
tences? mixing English with non-English, whether
foreign-language or non-language material. Our
approach is to augment model training compared
to the clean baseline by adding non-English,
mixed-language, and non-language material, and
to augment the model?s feature set with language-
identification features more localized than the
sentence-level perplexity described above, as well as
other features designed primarily to distinguish non-
language material such as mark-up codes.
4.5.1 Language-identification features
We apply an n-gram-based language classi-
fier (Prager, 1999) to variable-length sliding win-
dows as follows. For each word, we run 1- through
6-preceding-word windows through the classifier,
and 1- through 6-word windows beginning with the
word, for a total of 12 windows, yielding for each
window a result like:
0.235 Swedish
0.148 English
0.134 French
...
For each of the 12 results, we extract three fea-
tures: the identity of the top-scoring language, here,
Swedish; the confidence score in the top-scoring
language, here, 0.235; and the score difference be-
tween the target language (English for these ex-
periments) and the top-scoring non-target language,
here, 0.148 ? 0.235 = ?0.087. Thus we have
a 36-feature vector for each word. We bin these
and use them as input to a maximum-entropy clas-
sifier (separate from the MD classifier) which out-
puts ?English? or ?Non-English?, and a confidence
score. These scores in turn are binned into six cate-
gories to serve as a ?how-English-is-it? feature in the
augmented MD model. The language-identification
classifier and the maximum-entropy ?how-English?
classifier are each trained on text data separate from
340
each other and from the training and test sets for
MD.
4.5.2 Additional features
The following features are designed to capture
evidence of whether a ?word? is in fact linguistic
material or not: number of alphabetic characters,
number of characters, maximum consecutive rep-
etitions of a character, numbers of non-alphabetic
and non-alphanumeric characters, fraction of char-
acters which are alphabetic, fraction alphanumeric,
and number of vowels. These features are part of the
augmentation of the mixed MD model relative to the
clean MD model.
5 Data sets
Four data sets are used for our initial experiments.
One, ?English?, consists of 367 documents total-
ing 170,000 words, drawn from web news stories
from various sources and detagged to be plain text.
This set is divided into 340 documents as a train-
ing set and 27 for testing, annotated as described in
more detail elsewhere (Han, 2010). These data av-
erage approximately 21 annotated mentions per 100
words.
The second set, ?Latin?, consists of 23 detagged
web news articles from 11 non-English Latin-
alphabet languages totaling 31,000 words. Of these
articles, 12 articles containing 19,000 words are
used as a training set, with the remaining used for
testing, and each set containing all 11 languages.
They are annotated using the same annotation con-
ventions as ?English?, and from the perspective of
English; that is, only mentions which would be clear
to an English speaker are labeled, such as Barack
Obama in the Spanish example in Section 4. For
this reason, these data average only approximately 5
mentions per 100 words.
The third, ?Transactions?, consists of approxi-
mately 60,000 words drawn from a text data set
logging real financial transactions. Figure 1 shows
example passages from this database, anonymized
while preserving the character of the content.
This data set logs transactions by a staff of
customer-service representatives. English is the pri-
mary language, but owing to international clientele,
occasionally representatives communicate in other
languages, such as the German here, or in English
but mentioning institutions in other countries, here, a
Czech bank. Interspersed among text are codes spe-
cific to this application which delineate and identify
various information fields and punctuate long pas-
sages. The application also places constraints on
legal characters, leading to the unusual representa-
tion of underline and the ?at? sign as shown, mak-
ing for an e-mail address which is human-readable
but likely not obvious to a machine. Abbreviations
represent terms particularly common in this appli-
cation area, though they may not be obvious with-
out adapting to the application; these include stan-
dards like HUF, a currency code which stands for
Hungarian forint, and financial-transaction peculiar-
ities like BNF for ?beneficiary? as seen in Figure 1.
In short, good English is interspersed with non-
language content, foreign-language text, and rough
English like data-entry errors and haphazard abbre-
viations. These data average 4 mentions per 100
words.
Data sets with peculiarities analogous to those in
this Transactions set are commonplace in a variety
of settings. Training specific to data sets like this is
often infeasible due to lack of labeled data, insuffi-
cient data for training, and the multitude of such data
formats. For this reason, we do not train on Transac-
tions, letting our testing on this data set serve as an
example of testing on such data formats unseen.
6 Experiments
MD systems were trained to recognize the 116
entity-mention types shown in Table 1, annotated as
described previously (Han, 2010). The clean-data
classifier was trained on the English training data us-
ing the feature set described in Section 3.1. The clas-
sifier for ?mixed?-quality data and the ?gazetteer?
model were each trained on that set plus the ?Latin?
training set and the supplemental set. In addition,
?mixed? training included the additional features de-
scribed in Section 4.5. The framework used to build
the baseline MD system is similar to the one we used
in the ACE evaluation2. This system has achieved
competitive results with an F -measure of 82.7 when
trained on the seven main types of ACE data with
access to wordnet and part-of-speech-tag informa-
tion as well as output of other MD and named-entity
recognizers (Zitouni and Florian, 2008).
It is instructive to evaluate on the individual com-
ponent systems as well as the combination, despite
the fact that the individual components are not well-
suited to all the data sets, for example, the mixed
and gazetteer systems being a poorer fit to the En-
glish task than the baseline, and vice versa for the
2NIST?s ACE evaluation plan:
http://www.nist.gov/speech/tests/ace/index.htm
341
age event-custody facility people date
animal event-demonstration food percent duration
award event-disaster geological-object person e-mail-address
cardinal event-legal geo-political product measure
disease event-meeting law substance money
event event-performance location title-of-a-work phone-number
event-award event-personnel ordinal vehicle ticker-symbol
event-communication event-sports organ weapon time
event-crime event-violence organization web-address
Table 1: Entity-type categories used in these experiments. The eight in the right-most column are not
further distinguished by mention type, while the remaining 36 are further classified as named, nominal or
pronominal, for a total of 36 ? 3 + 8 = 116 mention labels.
English Latin Transactions
P R F P R F P R F
Clean 78.7 73.6 76.1 16.0 40.0 22.9 19.5 32.2 24.3
Mixed 77.9 69.7 73.6 78.5 55.9 65.3 37.1 47.8 41.7
Gazetteer 76.9 66.2 71.1 77.8 55.5 64.8 36.5 47.5 41.3
Combination 78.1 73.2 75.6 80.4 56.0 66.0 38.5 49.1 43.2
Table 2: Performance of clean, mixed, and gazetteer-based mention detection systems as well as their com-
bination. Performance is presented in terms of Precision (P), Recall (R), and F -measure (F).
non-target data sets. Precision/recall/F -measure re-
sults are shown in Table 2. Not surprisingly, the
baseline system, intended for clean data, performs
poorly on noisy data. The mixed and gazetteer sys-
tems, having a variety of noisy data in their train-
ing set, perform much better on the noisy conditions,
particularly on Latin-alphabet-non-English data be-
cause that is one of the conditions included in its
training, while Transactions remains a condition not
covered in the training set and so shows less im-
provement. However, because the mixed classifier,
and moreso the gazetteer classifier, are oriented to
noisy data, on clean data they suffer in performance
by 2.5 and 5 F -measure points, respectively. But
system combination serves us well: it recovers all
but 0.5 F -measure point of this loss, while also ac-
tually performing better on the noisy data sets than
the two classifiers specifically targeted toward them,
as can be seen in Table 2. It is important to note
that the major advantage of using the combination
model is the fact that we do not have to know the
data source in order to select the appropriate MD
model to use. We assume that the data source is
unknown, which is our claim in this work, and we
show that we obtain better performance than using
source-specific MD models. This reflects the fact
that a noisy data set will in fact have portions with
varying degrees of ?noise?, so the combination out-
performs any single model targeted to a single par-
ticular level of noise, enabling the system to con-
tend with such variability without the need for pre-
segregating sub-types of data for noise level. The
obtained improvement from the system combination
over all other models is statistically significant based
on the stratified bootstrap re-sampling significance
test (Noreen, 1989). We consider results statistically
significant when p < 0.05, which is the case in this
paper. This approach was used in the named-entity-
recognition shared task of CoNNL-20023.
It should be noted that some completely-non-
target types of data, such as non-target-character set
data, have been omitted from analysis here. In-
cluding them would make our system look compar-
atively stronger, as they would have only spurious
mentions and so generate false alarms but no correct
mentions in the baseline system, while our system
deterministically removes them.
As mentioned above, we view MD robustness pri-
marily as an effort to eliminate, relative to a base-
line system, large volumes of spurious ?mentions?
detected in non-target input content, while minimiz-
3http://www.cnts.ua.ac.be/conll2002/ner/
342
(a) DET plot for clean (baseline), mixed, gazetteer,
and combination MD systems on the Latin-alphabet-
non-English text. The clean system (upper curve)
performs far worse than the other three systems de-
signed to provide robustness; these systems in turn
perform nearly indistinguishably.
(b) DET plot for clean (baseline), mixed, gazetteer,
and combination MD systems on the Transactions
data set. The clean system (upper/longer curve)
reaches far higher false-alarm rates, while never ap-
proaching the lower miss rates achievable by any of
the other three systems, which in turn perform com-
parably to each other.
Figure 2: DET plots for Latin-alphabet-non-English and Transactions data sets
ing disruption of detection in target input. A sec-
ondary goal is recall in the event of occasional valid
mentions in such non-target material. Thus, as in-
put material degrades, precision increases in impor-
tance relative to recall. As such, we view precision
and recall asymmetrically on this task, and so rather
than evaluating purely in terms of F -measure, we
perform a detection-error-trade-off (DET) (Martin
et al, 1997) analysis, in which we plot a curve of
miss rate on valid mentions vs. false-alarm rate, with
the curve traced by varying a confidence threshold
across its range. We measure false-alarm and miss
rates relative to the number of actual mentions anno-
tated in the data set:
FA rate = # false alarms# annotated mentions (1)
Miss rate = # misses# annotated mentions (2)
where false alarms are ?mentions? output by the sys-
tem but not appearing in annotation, while misses
are mentions which are annotated but do not ap-
pear in the system output. Each mention is treated
equally in this analysis, so frequently-recurring en-
tity/mention types weigh on the results accordingly.
Figure 2a shows a DET plot for the clean, mixed,
gazetteer, and combination systems on the ?Latin?
data set, while Figure 2b shows the analogous plot
for the ?Transactions? data set. The drastic gains
made over the baseline system by the three experi-
mental systems are evident in the plots. For exam-
ple, on Latin, choosing an operating point of a miss
rate of 0.6 (nearly the best achievable by the clean
system), we find that the robustness-oriented sys-
tems eliminate 97% of the false alarms of the clean
baseline system, as the plot shows false-alarm rates
near 0.07 compared to the baseline?s of 2.08. Gains
on Transaction data are more modest, owing to this
case representing a data genre not included in train-
ing. It should be noted that the jaggedness of the
Transaction curves traces to the repetitive nature of
some of the terms in this data set.
In making a system more oriented toward robust-
ness in the face of non-target inputs, it is important
to quantify the effect of these systems being less-
oriented toward clean, target-language text. Figure 3
shows the analogous DET plot for the English test
set, showing that achieving robustness through the
combination system comes at a small cost to accu-
racy on the text the original system is trained to pro-
cess.
7 Conclusions
For information-extraction systems to be useful,
their performance must degrade gracefully when
confronted with inputs which deviate from ideal
and/or derive from unknown sources in unknown
formats. Imperfectly-translated, mixed-language,
marked-up text and non-language material must not
343
Figure 3: DET plot for clean (baseline), mixed,
gazetteer, and combination MD systems on clean English
text, verifying that performance by the clean system (low-
est curve) is very closely approximated by the combina-
tion system (second-lowest curve), while the mixed sys-
tem performs somewhat worse and the gazetteer system
(top curve), worse still, reflecting that these systems are
increasingly oriented toward noisy inputs.
be processed in a garbage-in-garbage-out fashion
merely because the system was designed only to
handle clean text in one language. Thus we have em-
barked on information-extraction-robustness work,
to improve performance on imperfect inputs while
minimizing disruption of processing of clean text.
We have demonstrated that for one IE task, mention
detection, a multi-faceted approach, motivated by
the diversity of input data imperfections, can elimi-
nate a large proportion of the spurious outputs com-
pared to a system trained on the target input, at a
relatively small cost of accuracy on that target input.
This outcome is achieved by a system-combination
approach in which a perplexity-based measure of
how well the input matches the target language is
used to select among models designed to deal with
such varying levels of noise. Rather than relying on
explicit recognition of genre of source data, the ex-
perimental system merely does its own assessment
of how much each sentence-sized chunk matches the
target language, an important feature in the case of
unknown text sources.
Chief among directions for further work is to con-
tinue to improve performance on noisy data, and to
strengthen our findings via larger data sets. Addi-
tionally, we look forward to expanding analysis to
different types of imperfect input, such as machine-
translation output, different types of mark-up, and
different genres of real data. Further work should
also explore the degree to which the approach to
achieving robustness must vary according to the tar-
get language. Finally, robustness work should be ex-
panded to other information-extraction tasks.
Acknowledgements
The authors thank Ben Han, Anuska Renta,
Veronique Baloup-Kovalenko and Owais Akhtar for
their help with annotation. This work was supported
in part by DARPA under contract HR0011-08-C-
0110.
References
Y. Benajiba, M. Diab, and P. Rosso. 2009. Arabic named
entity recognition: A feature-driven study. In the spe-
cial issue on Processing Morphologically Rich Lan-
guages of the IEEE Transaction on Audio, Speech and
Language.
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of ANLP-97, pages 194?201.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, J. C.
Lai, and R. L. Mercer. 1992. An estimate of an up-
per bound for the entropy of English. Computational
Linguistics, 18(1), March.
S. Chen and R. Rosenfeld. 2000. A survey of smooth-
ing techniques for ME models. IEEE Transaction on
Speech and Audio Processing.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In Conference on Computational Natural Lan-
guage Learning - CoNLL-2003, Edmonton, Canada,
May.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proceedings of HLT-NAACL 2004, pages
1?8.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006.
Factorizing complex models: A case study in men-
tion detection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 473?480, Sydney, Australia,
July. Association for Computational Linguistics.
J. Goodman. 2002. Sequential conditional generalized
iterative scaling. In Proceedings of ACL?02.
D. B. Han. 2010. Klue annotation guidelines - version
2.0. Technical Report RC25042, IBM Research, Au-
gust.
344
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocki. 1997. The DET curve in assessment
of detection task performance. In Proceedings of the
European Conference on Speech Communication and
Technology (Eurospeech), pages 1895?1898. Rhodes,
Greece.
D. Miller, S. Boisen, R. Schwartz, R. Stone, and
R. Weischedel. 2000. Named entity extraction from
noisy input: speech and OCR. In Proceedings of the
sixth conference on Applied natural language process-
ing, pages 316?324, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting personal names from email: Applying named
entity recognition to informal text. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 443?450, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
E. W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley Sons.
J. F. Pitrelli, B. L. Lewis, E. A. Epstein, M. Franz,
D. Kiecza, J. L. Quinn, G. Ramaswamy, A. Srivas-
tava, and P. Virga. 2008. Aggregating Distributed
STT, MT, and Information Extraction Engines: The
GALE Interoperability-Demo System. In Interspeech.
Brisbane, NSW, Australia.
J. M. Prager. 1999. Linguini: Language identification for
multilingual documents. In Journal of Management
Information Systems, pages 1?11.
L. Ramshaw and M. Marcus. 1999. Text chunking using
transformation-based learning. In S. Armstrong, K.W.
Church, P. Isabelle, S. Manzi, E. Tzoukermann, and
D. Yarowsky, editors, Natural Language Processing
Using Very Large Corpora, pages 157?176. Kluwer.
E. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independentnamed entity
recognition. In Proceedings of CoNLL-2002, pages
155?158. Taipei, Taiwan.
J. Warmer and S. van Egmond. 1989. The implementa-
tion of the Amsterdam SGML parser. Electron. Publ.
Origin. Dissem. Des., 2(2):65?90.
L. Yi, B. Liu, and X. Li. 2003. Eliminating noisy in-
formation in web pages for data mining. In KDD ?03:
Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 296?305, New York, NY, USA. ACM.
M. Zimmerman, D. Hakkani-Tur, J. Fung, N. Mirghafori,
L. Gottlieb, E. Shriberg, and Y. Liu. 2006. The
ICSI+ multilingual sentence segmentation system. In
Interspeech, pages 117?120, Pittsburgh, Pennsylvania,
September.
I. Zitouni and R. Florian. 2008. Mention detection
crossing the language barrier. In Proceedings of
EMNLP?08, Honolulu, Hawaii, October.
I. Zitouni and R. Florian. 2009. Cross-language informa-
tion propagation for Arabic mention detection. ACM
Transactions on Asian Language Information Process-
ing (TALIP), 8(4):1?21.
345
Proceedings of NAACL-HLT 2013, pages 878?887,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Finding What Matters in Questions
Xiaoqiang Luo, Hema Raghavan, Vittorio Castelli, Sameer Maskey and Radu Florian
IBM T.J. Watson Research Center
1101 Kitchawan Road, Yorktown Heights, NY 10598
{xiaoluo,hraghav,vittorio,smaskey,raduf}@us.ibm.com
Abstract
In natural language question answering (QA)
systems, questions often contain terms and
phrases that are critically important for re-
trieving or finding answers from documents.
We present a learnable system that can ex-
tract and rank these terms and phrases (dubbed
mandatory matching phrases or MMPs), and
demonstrate their utility in a QA system on In-
ternet discussion forum data sets. The system
relies on deep syntactic and semantic analysis
of questions only and is independent of rele-
vant documents. Our proposed model can pre-
dict MMPs with high accuracy. When used in
a QA system features derived from the MMP
model improve performance significantly over
a state-of-the-art baseline. The final QA sys-
tem was the best performing system in the
DARPA BOLT-IR evaluation.
1 Introduction
In most question answering (QA) systems and
search engines term-weights are assigned in a con-
text independent fashion using simple TF-IDF like
models (Robertson and Walker, 1994; Ponte and
Croft, 1998). Even the more recent advances
in information retrieval techniques for query term
weighting (Bendersky et al, 2010; Bendersky, 2011)
typically rely on bag-of-words models and cor-
pus statistics, such as inverse-document-frequency
(IDF), to assign weights to terms in questions. While
such solutions may work for keyword queries of the
type common on search engines such as Google,
they do not exploit syntactic and semantic informa-
tion when it comes to well formed natural language
questions. In this paper we propose a new model
that identifies important terms and phrases in a natu-
ral language question, providing better query analy-
sis that ultimately leads to significant improvements
in a QA system.
To motivate the work presented here, consider the
query ?How does one apply for a New York day care
license??. A bag-of-words model would likely as-
sign a high score to ?New licenses for day care cen-
ters in York county, PA? because of high word over-
lap, but it does not answer the question, and also
the state is wrong. A matching component that uses
the phrases ?New York,? ?day care,? and ?license?
is likely to do better. However, a better matching
component will understand that in the context of this
query all three phrases ?New York,? ?day care? and
?license? are important, and that ?New York? needs
to modify ?day care.? A snippet that does not con-
tain1 these important phrases, is unlikely an answer.
We call these important phrases mandatory match-
ing phrases (MMPs).
In this paper, we explore deep syntactic and se-
mantic analyses of questions to determine and rank
MMPs. Unlike existing work (Zhao and Callan,
2010; Bendersky et al, 2010; Bendersky, 2011),
where term/concept weights are learned from a set
of questions and judged documents based on corpus-
based statistics, we annotate questions and build a
trainable system to select and score MMPs. This
model relies heavily on existing syntactic parsers
and semantic-oriented named-entity recognizers, but
does not need question answer pairs. This is espe-
1
?contain? here means semantic equivalence or entailment,
not necessarily the exact words or phrases.
878
cially attractive at the initial system-building stage
when no or little answer data is available.
The main contributions of this paper are: firstly,
we propose a framework to select and rank impor-
tant question phrases (MMPs) for question answer-
ing in Section 3. This framework seamlessly incor-
porates lexical, syntactic and semantic information,
resulting in an MMP prediction F-measure as high
as 88.6%. Secondly, we show that features derived
from identified MMPs improve significantly a rele-
vance classification model, in Section 4.2. Thirdly,
we show that using the improved relevance model
into our QA system results in a statistically signifi-
cant 5 point improvement in F-measure, in Section
5. This finding is further corroborated by the results
on the official 2012 BOLT IR (IR, 2012) task where
the combined system yielded the best performance
in the evaluation.
2 Related Work
Popular information retrieval systems like
BM25 (Robertson and Walker, 1994) and language
models (Ponte and Croft, 1998) use unsupervised
techniques based on corpus statistics for term
weighting. Many of these techniques are variants
of the one proposed by (Luhn, 1958). Recently,
several researchers have studied approaches for term
weighting using supervised learning techniques.
However, much of this research has focused on
information retrieval task rather than on question
answering problems of the nature addressed in
this paper. (Bendersky and Croft, 2008) restricted
themselves to predicting key noun phrases, which
is perhaps sufficient for a retrieval task. However,
for questions like ?Find comments about how
American hedge funds legally avoid taxes,? the verb
?avoid? is perhaps as important as the noun phrase
?American hedge funds? and ?taxes?. Works like
that of (Lease et al, 2009) and (Zhao and Callan,
2010) predict importance at the word level. While
word level importance is perhaps sufficient for
an IR task, predicting the importance of phrases,
especially those derived from a parse tree, gives
a much richer representation that might also be
useful for better question understanding and thus
generate more relevant answers. Both (Lease et al,
2009; Zhao and Callan, 2010) propose supervised
methods that learn from a large set of queries and
relevance judgments on their answers. While this is
possible in a TREC Ad-hoc-retrieval-like task, such
a large training corpus of question-answer pairs is
unavailable for most scenarios. (Monz, 2007) learns
term weights for the IR component of a question
answering task. His work unlike ours does not aim
to find the answers to the questions.
Most QA systems in the literature have dealt
with answering factoid questions, where the an-
swer is a noun phrase in response to questions of
the form ?Who,? ?Where,? ?When.? Most sys-
tems have a question analysis component that rep-
resents the question as syntactic relations in a parse
or as deep semantic relations in a handcrafted on-
tology (Hermjakob et al, 2000; Chu-carroll et al,
2003; Moldovan et al, 2003). In addition certain
systems (Bunescu and Huang, 2010) aim to find the
?focus? of the question, that is, the noun-phrases in
the question that would co-refer with answers. Ad-
ditionally, much past work has focused on finding
the lexical answer type (Pinchak, 2006; Li and Roth,
2002). Since these papers considered a small num-
ber of answer types, rules over the detected relations
and answer types could be applied to find the rel-
evant answer. However, since our system answers
non-factoid questions that can have answer of arbi-
trary types, we want to use as few rules as possible.
The MMPs therefore become a critical component
of our system, both for question analysis and for rel-
evance detection.
3 Question Data and MMP Model
To train the MMP model, we first create a set of
questions and label their MMPs. The labeled data
is then used to train a statistical model to predict
MMPs for new questions as discussed next.
3.1 Question Corpus
We use a subset of the DARPA BOLT corpus (see
Section 5.1) containing forum postings in English.
Four annotators use a search tool to explore this
document collection. They can perform keyword
searches and retrieve forum threads from which they
generate questions. The program participants de-
cided a basic set of question types that are out-of-
scope of the current research agenda. Accordingly,
879
annotators cannot generate questions (1) that require
reasoning or calculation over the data to compute
the answers; (2) that are vague or ambiguous; (3)
that can be broken into multiple disjoint questions;
(4) that are multiple choice questions; (5) that are
factoid questions?the kinds that have already been
well studied in TREC (Voorhees, 2004). Any other
kind of question is allowed. Two other annotators,
who have neither browsed the corpus nor generated
the questions, mark selected spans of the questions
into one of two categories?MMP-Must and MMP-
maybe. The annotation tool allows arbitrary spans
to be highlighted and the annotators are instructed to
select spans corresponding to the smallest semantic
units. The phrases that are very likely to appear con-
tiguously in a relevant answer are marked as MMP-
Must. Annotators can mark multiple spans per ques-
tion, but not overlapping spans. We generated 201
annotated questions using this process.
Figure 1 contains an example, where ?American,?
?hedge fund,? and ?legally avoid taxes? are required
elements to find answers and are thus marked as
MMP-Musts (signified by enclosing rectangles). We
purposely annotate MMPs at the word level and not
in the parse tree, because this requires minimal lin-
guistic knowledge. We do, however, employ an
automatic procedure to attach MMPs to parse tree
nodes when generating MMP training instances.
3.2 MMP Training
Questions annotated in Section 3.1 are first pro-
cessed by an information extraction (IE) pipeline
consisting of syntactic parsing, mention detection
and coreference resolution (Florian et al, 2004; Luo
et al, 2004; Luo and Zitouni, 2005). After IE, we
have access to the syntactic structure represented by
a parse tree and semantic information represented
by coreferenced mentions (including those of named
entities).
To take advantage of the availability of the syn-
tactic and semantic information, we first attach the
MMP annotations to parse tree nodes of a question,
and, if necessary, we augment the parse tree.
There are several reasons why we want to embed
the MMPs into a parse tree. First, many constituents
in parse trees correspond to important phrases we
want to capture, especially proper names. Second,
after an MMP is attached to a tree node, the problem
VP0
VB
Find comments
NNS
about
IN
SBAR
PP
VP
how
WRB NNP NN
legally taxes
NNS
funds
RB NNS
NP
S
avoidhedge
NP
WHADVP
VBP
 American
GPE
NP NP1
NP
2
Figure 1: MMPs are aligned with tree nodes: MMPs
are shown in rectangular boxes along with their aligned
nodes (with slanted labels); augmented parse tree nodes
(i.e., NP1, NP2) in dashed nodes. Dotted edges under
NP0 are the structure before the tree is augmented.
of predicting MMPs reduces to classifying parse tree
nodes, and syntactic information can be naturally
built into the MMP classifier. Lastly, and more im-
portantly, associating MMPs with tree nodes opens
the door to explore features derived from the syn-
tactic parse tree. For instance, it is easy to read
bilexical dependencies from a parse tree (provided
that head information is propagated); with MMPs
aligned with the parse tree, bilexical dependencies
can be ranked by examining whether or not an MMP
phrase is a head or a dependent. This way, not
only are the dependencies in a question captured, but
MMP scores or ranks can be propagated to depen-
dencies as well. We will discuss more how MMP
features are computed in Section 4.2.2.
Annotators can mark MMPs that are not perfectly
aligned with a tree node. Hence, care has to be taken
when generating MMP training instances. As an ex-
ample, In Figure 1, ?American? and ?hedge funds?
are marked as two separate MMPs, but the Penn-
Tree-style parse tree has a flat ?NP0? constituent
spanning directly on ?American hedge fund,? illus-
trated in Figure 1 as dotted edges.
To anchor MMPs in the parse tree, we augment
it by combining the IE output and the MMP anno-
tation. In the aforementioned example, ?American?
is a named mention with the entity type GPE (geo-
political entity) and there is no non-terminal node
spanning it: so, a new node ?NP1? is created; ?hedge
funds? is marked as an MMP: so, a second node
(?NP2?) is created to anchor it.
880
A training instance for building the MMP model
is defined as a span along with an MMP label. For
instance, ?hedge funds? in Figure 1 will generate a
positive training instance as ?(5,6), +1?, where
(5,6) is the span of ?hedge funds? in the question
sentence, and +1 signifies that it is a positive train-
ing instance. For the purpose of this paper we use
only binary labels, mapping all MMP-Must to +1
and MMP-Skip and MMP-Maybe to ?1.
Formally, we use the following procedure to gen-
erate training instances:
Algorithm 1 Pseudo code to generate MMP training
instances.
Input: An input question tree with detected men-
tions and marked MMPs
Output: A list of MMP training instances
1: Foreach mention m in the question
2: if no node spans m, and m does not cross bracket
3: Find lowest node N dominating m
4: Insert a child node of N that spans exactly m
5: Foreach mention p in marked MMPs
6: Find lowest non-terminal Np dominating p
7: Generate a positive training example for Np
8: Mark Np as visited
9: Recursively generate instances for Np?s children
10: Generate a negative training instance for all un-
visited nodes in Step 5-9
Steps 1 to 4 augment the question tree by creating
a node for each named mention, provided that no ex-
isting node spans exactly the mention and the men-
tion does not cross-bracket tree constituents. Steps 5
to 8 generate positive training instances for marked
MMPs; step 9 recursively generates positive training
instances 2 for tree nodes dominated by Np, where
Np is the lowest non-terminal node dominating the
marked MMP p.
After MMP training instances are generated we
design and compute features for each instance, and
use them to train a classifier.
3.3 MMP Features and Classifier
We compute four types of features that will be used
in a statistical classifier. These features are designed
to characterize a phrase from the lexical, syntactic,
2One exception to this step is that if a node spans a single
stop word, then a negative training instance is generated.
semantic and corpus-level aspect. The weights asso-
ciated with these features are automatically learned
from training data.
We will use ?(NP1 American)? in Figure 1 as the
running example below.
Lexical Features: Lexical features are motivated by
the observation that spellings in English sometimes
offer important cues about word significance. For
example, an all-capitalized word often signifies an
acronym; an all-digit word in a question is likely a
year, etc. We compute the following lexical features
for a candidate MMP:
CaseFeatures: is the first word of an MMP
upper-case? Is it all capital letters? Does it contain
numeric letters? For ?(NP American)? in Figure 1,
the upper-case feature fires.
CommonQWord: Does the MMP contain question
words, including ?What,? ?When,? ?Who,? etc.
Syntactic Features: The second group of features
are computed from syntactic parse trees after anno-
tated MMPs are aligned with question parse-trees
as described previously.
PhraseLabel: this feature returns the phrasal label
of the MMP. For ?(NP American)? in Figure 1, the
feature value is ?NP.? This captures that an NP is
more likely an MMP than, say, an ADVP.
NPUnique: this Boolean feature fires if a phrase
is the only NP in a question, indicating that this
constituent probably should be matched. For ?(NP
American),? the feature value would be false.
PosOfPTN: these features characterize the position
of the parse tree node to which an MMP is anchored.
They compute: (1) the position of the left-most
word of the node; (2) whether the left-most word is
the beginning of the question; (3) the depth of the
anchoring node, defined as the length of the path to
the root node. For ?(NP American)? in Figure 1, the
features state that it is the 5th word in the sentence;
it is not the first word of the sentence; and the depth
of the node is 6 (where root has depth 0).
PhrLenToQLenRatio: This feature computes the
number of words in an MMP, and its relative ratio to
the sentence length. This feature controls the length
of MMPs at decoding time, since most of MMPs
are short.
Semantic Features (NETypes): The third group of
features are computed from named entities and aim
to capture semantic information. The feature tests if
881
a phrase is or contains a named entity, and, if this
is the case, the value is the entity type. For ?(NP
American)? in Figure 1, the feature value would be
?GPE.?
Corpus-based Features ( AvgCorpusIDF): This
group of features computes the average of the IDFs
of the words in this phrase. From the corpus IDF,
we also compute the ratio between the number of
stop words and the total number of words in the
MMP, and use it as another feature.
3.4 MMP Classification Results
We now show that we can reliably predict MMPs of
questions. We split our set of 201 annotated ques-
tions into a training set consisting of 174 questions
and a test set with the remaining 27 questions. We
use the procedure and features described in Sec-
tion 3 to train a logistic regression binary classifier
using WEKA. Then, the trained MMP classifier is
applied to the test set question trees. Since the class
bias is quite skewed (only 16% of the phrases are
marked as MMP-Must) we also use re-sampling at
training time to balance the prior probability of the
two classes. At testing time, a parser and a men-
tion detection algorithm (Florian et al, 2004; Luo et
al., 2004; Luo and Zitouni, 2005) are run on each
question. The detected mentions are then used to
augment the question parse trees. The MMP classi-
fier achieves an 88.6% F-measure (cf. Table 1, with
91.6% precision). This is a respectable number, con-
sidering the limited amount of training data. We ex-
perimented with decision trees and bagging as well
but found logistic regression to work the best.
Feature P R F1
AvgCorpusIDF 0.849 0.634 0.725
+NPUnique 0.868 0.634 0.732
+NETypes 0.867 0.662 0.750
+PhraseLabel 0.890 0.705 0.783
+CaseFeatures 0.829 0.820 0.824
+PosOfPTN 0.911 0.852 0.880
+PhrLenToQLenRatio 0.915 0.855 0.883
+commonQWord 0.916 0.858 0.886
Table 1: The performances of the MMP classifier while
incrementally adding features.
The examples in Table 2 illustrate the top three
MMPs produced by the model on two questions.
These results are encouraging: in the first exam-
ple the word AIDS is clearly the most ?important?
word, but IDF alone is not adequate to place it in the
top since AIDS is also a common verb (words are
lower-cased before IDF look-up). Similarly, in the
third example, the phrase ?the causes? has a much
higher MMP score than the phrase ?the concerns?
(MMP score of 0.109), even though the words ?con-
cerns? has a slightly higher IDF, 2.80, than the word
?causes?(2.68). However, in this question, under-
standing that the word ?causes? is critical to the
meaning of the question is critical and is captured
by the MMP model.
We analyzed feature importance for MMP classi-
fication by incrementally adding each feature group
to the model. The result is tabulated in Table 1. Not
surprisingly, syntactical (i.e., ?NPUnique,? ?Phrase-
Label? and ?PosOfPTN?) and semantic features
(i.e., ?NETypes?) are complementary to the corpus-
based statistics features (i.e., average IDF). Lexical
features also improve recall: the addition of ?Case-
Features? boosts the F-measure by 4 points. At first
sight, it is surprising that the feature group ?PosOf-
PTN,? which characterize the position of a candi-
date MMP relative to the sentence and relative to the
parse tree, has such a large impact?it improves the
F-measure by 5.6 points. However, a cursory brows-
ing of the training questions reveals that most MMPs
are short and concentrate towards the end of the sen-
tence. So this feature group helps by directing the
model to predict MMPs at the end of the sentence
and to prefer short phrases versus long ones.
4 Relevance Model with MMPs
We now validate our second hypothesis that MMPs
are effective for open domain question answering.
We demonstrate this through the improvement in
performance on relevance prediction. More specif-
ically, given a natural language question, the task
is one of finding relevant sentences in posts on on-
line forums. The relevance prediction component
is critical for question answering as has been seen
in TREC(Ittycheriah and Roukos, 2001) and more
recently in the Jeopardy challenge(Gondek et al,
2012). The improved relevance model further im-
proves our question answering system as seen in
Section 5.
882
Question Top 3 MMPs MMP-
score
Top words
by IDF
List statistics about changes in the de-
mographics of AIDS.
1: AIDS 0.955 demographics
2: changes 0.525 AIDS
3: the demographics 0.349 statistics
What are the concerns about the
causes of autism?
1: autism 0.989 autism
2: the causes 0.422 concerns
3: the causes of autism 0.362 causes
Table 2: Example questions and the top-3 phrases ranked by the MMP model.
4.1 Data for Relevance Model
The data to train and test the relevance model is ob-
tained as follows. First, a rudimentary version (i.e.,
key word search) of a QA system using Lucene is
built. The Lucene index comprised of a large num-
ber of threads in online forums released to the par-
ticipants of the BOLT-IR task(IR, 2012) for devel-
opment of our systems. The corpus is described in
more detail in Sec. 5. Top snippets returned by the
search engine are judged for relevancy by our an-
notators. The initial (small) batch of data is used
to train a relevance model which is deployed in the
system. The new model is in turn used to create
more answers for new questions. When more data
is collected, the relevance model is retrained and re-
deployed to collect more data. The process is iter-
ated for several months, and at the end of this pro-
cess, a total of 390 training questions are created and
about 28,915 snippets are judged by human annota-
tors, out of which about 6,528 are relevant answers.
These question-answers pairs are used to train the fi-
nal relevance model used in our question-answering
system. A separate held-out test set of 59 questions
is created and its system output is also judged by hu-
mans. This data set is our test set.
4.2 Relevance Prediction
A key component in our question-answering sys-
tem is the snippet relevance model, which is used
to compute the probability that a snippet is relevant
to a question. The relevance model is a conditional
distribution P (r|q, s;D), where r is a binary ran-
dom variable indicating if the candidate snippet s is
relevant to the question q. D is the document where
the snippet s is found.
In our question answering system, MMPs ex-
tracted from questions are used to compute the fea-
tures for the relevance model. To test their effective-
ness, we conduct a controlled experiment by com-
paring the system with MMP features with 2 base-
lines: (1) a system without MMP features; (2) a
baseline with each word as an MMP and the word?s
IDF as the MMP score.
4.2.1 Baseline Features
We list the features used in our baseline system,
where no MMP feature is used. The features can
be categorized into the following types. (1) Text
Match Features: One set of features are the cosine
scores between different representations of the query
and the snippet. In one version the query and snip-
pet words are used as is; in another version the query
and snippet are stemmed using porter stemmer; in
yet another the words are morphed to their roots by
a table extracted from WordNet. We also compute
the inclusion scores (the proportion of query words
found in the snippet) and other word overlap fea-
tures. (2) Answer Type Features: The top 3 pre-
dictions of a statistical classifier trained to predict
answer categories were used as features. (3) Men-
tion Match Features compute whether a named en-
tity in the query occurs in the snippet. The matching
takes into consideration the results from within and
cross document coreference resolution components
for nominal and pronominal mentions. (4) Event
match features use several hand-crafted dictionar-
ies containing terms exclusive to various types of
events like ?violence?, ?legal?, ?election?. Accord-
ingly a set of features that take a value of ?1? if
both the query and snippet contain the same event
type were designed. (5) Snippet Statistics: Several
features based on snippet length, the position of the
snippet in the post etc were created.
883
4.2.2 Features Derived from MMP
The MMPs extracted from questions are used to
compute features in the following ways.
As MMPs are aligned with a question?s syntactic
tree, they can be used to find answers by matching
a question constituent with that of a candidate snip-
pet. The MMP model also returns a score for each
phrase, which can be used to compute the degree to
which a question matches a candidate snippet.
In this section, we use s = wn1 to denote a snip-
pet with words w1, w2, ? ? ? , wn, and m to denote
a phrase from the MMP model along with a score
M(m). The features are listed below:
HardMatch: Let I(m ? s) be a 1 or 0 function
indicating if a snippet contains the MMP m, then
the hard match score is computed as:
HM(q, s) =
?
m?q M(m)I(m ? s)
?
m?q M(m)
.
SoftLMMatch: The SoftLMMatch score is a
language-model (LM) based score, similar to that
used in (Bendersky and Croft, 2008), except that
MMPs play the role of concepts. The snippet-side
language model score LM(v|s) is computed as:
LM(v|s) =
?n
i=1 I(wi = v) + 0.05
n + 0.05|V | ,
where wi is the ith in snippet s; I(wi = v) is an
indicator function, taking value 1 if wi is v and 0
otherwise; |V | is the vocabulary size.
The soft match score between a question q and a
snippet s is then:
SM(q, s) =
?
m?q
(
M(m)?w?m LM(w|s)
)
?
m?q M(m)
,
where m ? q denotes all MMPs in question q, and
similarly, w ? m signifying words in m.
MMPInclScore: An MMP m?s inclusion score is:
IS(m, s) =
?
w?m I(l(w, s) > ?)IDF (w)
?
w?m IDF (w)
,
where w ? m are the words in m; I(?) is the in-
dicator function taking value 1 when the argument
is true and 0 otherwise; ? is a constant threshold;
IDF (w) is the IDF of word w. l(w, s) is the sim-
ilarity of word w to the snippet s as: l(w, s) =
maxv?sJW (w, v), where JW (w, v) is the Jaro
Winkler similarity score between words w and v.
The MMP weighted inclusion score between the
question q and snippet s is computed as:
IS(q, s) =
?
m?q M(m)IS(m, s)
?
m?q M(m)
MMPRankDep: This feature, RD(q, s) first tests
if there exists a matched bilexcial dependency be-
tween q and s; if yes, it further tests if the head or
dependent in the matched dependency is the head of
any MMP.
Let m(i) be the ith ranked MMP; let ?wh, wd|q?
and ?uh, ud|s? be bilexical dependencies from q and
s, respectively, where wh and uh are the heads and
wd and ud are the dependents; let EQ(w, u) be a
function testing if the question word w and snip-
pet word u are a match. In our implementation,
EQ(w, u) is true if either w and u are exactly the
same, or their morphs are the same, or they head
the same entity, or their synset in WordNet overlap.
With these notations, RD(q, s) is true if and only if
EQ(wh, uh) ? EQ(wd, ud) ?wh ? m(i) ?wd ? m(j)
is true for some ?wh, wd|q?, for some ?uh, ud|s? and
for some i and j.
EQ(wh, uh)?EQ(wd, ud) requires that the ques-
tion dependency ?wh, wd|q? and the snippet depen-
dency ?uh, ud|s? match; wh ? m(i) ?wd ? m(j) re-
quires that the head word and dependent word are in
the ith-rank and jth rank MMP, respectively. There-
fore, RD(q, s) is a dependency feature enhanced
with MMPs.
To test the effectiveness of the MMP features, we
trained 3 snippet classifiers on the data described
in Section 4.1: one baseline system without MMP
features (henceforth ?no-MMP?); a second baseline
with words as MMPs and their IDFs as the scores
in the MMP model(henceforth ?IDF-as-MMP?); the
third system uses the MMPs generated by the model
from Section 3 and all MMP features described in
this section. We used two types of classifiers: deci-
sion tree (DTree) and logistic regression (Logit).
The classification results on a set of 59 questions
disjoint from the training set are shown in Table 3.
The numbers in the table are F-measure on answer
snippets (or positive snippets). Within a machine
884
Learner
Model DTree Logit
noMMP 0.426 0.458
IDF-as-MMP 0.413 0.455
MMP 0.451 0.470
Table 3: F-measure for Relevance Prediction.
learning method, the model with MMP features is
always the best. Between the two classifiers, the lo-
gistic regression models are consistently better than
the decision tree ones. The results show that MMP
features are very helpful to the relevance model.
5 End-to-End System Results
The question-answering system is used in the 2012
BOLT IR evaluation (IR, 2012). The task is to an-
swer questions against a corpus of posts collected
from Internet discussion forums in 3 languages:
Arabic, Chinese and English. There are 499K, 449K
and 262K threads in each of these languages. The
Arabic and Chinese posts were first translated into
English before being processed. We now describe
our experiments on the set of 59 questions devel-
oped internally and demonstrate the effectiveness of
an MMP based relevance model in the end-to-end
system. In the next subsection we discuss our per-
formance in the BOLT-IR evaluation done by NIST
for DARPA.
We now briefly describe the question-answering
system we developed for the DARPA BOLT IR task,
where we applied the MMP classifier and its fea-
tures. Users submit questions to the system in natu-
ral language; the BOLT program mandates that these
questions comply with the restrictions described in
Section 3.1. Questions are analyzed by a query pre-
processing stage that includes our MMP extraction
classifier. The preprocessed queries are converted
to search queries. These are sent to an Indri-based
search engine (Strohman et al, 2005), which re-
turns candidate passages, typically spanning numer-
ous sentences. Each sentence of the retrieved pas-
sages is analyzed by a relevance detection module,
consisting of a statistical classifier that uses, among
others, features computed from the MMPs extracted
from the questions. Sentences or spans that are
deemed relevant to the question by the relevance de-
tection module are further grouped into equivalence
classes that provide different information about the
answers. The system generates a single answer for
each equivalence class, since elements of the same
class are redundant with respect to each other. The
elements of each equivalence class are converted
into citations that support the corresponding answer.
The ultimate goal of the MMP model is to im-
prove the performance of our question-answering
system. To test the effectiveness of the MMP model,
we contrast the model trained in Section3 with an
IDF baseline, where each non-stop word in a ques-
tion is an MMP and its score is the corpus IDF. The
IDF baseline is what a typical question answering
system would do in absence of deep question analy-
sis. To have a fair comparison, the two systems are
tested on the same set of 59 questions as the rele-
vance model.
The results of the IDF baseline and MMP system
are tabulated in Table 4. Note that the recalls are
less than 1.0 because (1) annotated snippets come
from both systems; (2) the annotation is done for all
snippets in a window surrounding system snippets.
As can be seen from Table 4, the MMP system is
about 5 points better than the baseline system. The
precision is notably better by 2 points, and the re-
call is far better (by 7.7%) than that of the baseline.
We also compute the question-level F-measures and
conduct a Wilcoxon signed-rank test for paired sam-
ples. The test indicates that the MMP system is bet-
ter than the baseline system at p < 0.00066. There-
fore, the MMP system has a clear advantage over the
baseline system.
System Prec Recall F1
baseline .4228 .3679 .3935
MMP .4425 .4452 .4438
Table 4: End-to-End system result on 59 questions.
5.1 BOLT Evaluation Results
The BOLT evaluation consists of 146 questions,
mostly event- or topic- related, e.g., ?What are peo-
ple saying about the ending of NASA?s space shuttle
program??. A system answer, if correct, is mapped
manually to a facet, which is one semantic unit that
answers the question. For each question, facets
are collected across all participants? submission. A
885
facet-based F-measure is computed for each partic-
ipating site. The recall from which the official F-
measure is computed is weighted by snippet cita-
tions (a citation is a reference to the original docu-
ment that supports the correct facet). In other words,
a snippet with more citations leads to a higher recall
than one with less citations. The performances of
4 participating sites are listed in Table 5. Note that
the F-measure is weighted and is not necessarily a
number between the precision and the recall.
Facet Metric
Site Precision Recall (Weighted) F
SITE 1 0.2713 0.1595 0.1713
SITE 2 0.1500 0.1316 0.1109
SITE 3 0.1935 0.2481 0.1734
Ours 0.2729 0.2195 0.2046
Table 5: Official BOLT 2012 IR evaluation results.
.
Among 4 participating sites, our system has the
highest performance. SITE 1 has about the same
level of precision, with lower recall, while SITE 3
has the best recall, but lower precision. The results
validate that the MMP question analysis technique
presented in this paper is quite effective.
6 Conclusions
We propose a framework to select and rank manda-
tory matching phrases (MMP) for question answer-
ing. The framework makes full use of the lexical,
syntactic and semantic information in a question and
does not require answer data.
The proposed MMP framework is tested at 3 lev-
els in a full QA system and is shown to be very effec-
tive to improve its performance: first, we show that
it is possible to reliably predict MMPs from ques-
tions alone: the MMP classifier can achieve an F-
measure as high as 88.6%; second, phrases proposed
by the MMP model are incorporated into a snippet
relevance model and we show that it improves its
performance; third, the MMP framework is used in
an question answering system which achieved the
best performance in the official 2012 BOLT IR (IR,
2012) evaluation.
Acknowledgments
This work was partially supported by the Defense
Advanced Research Projects Agency under contract
No. HR0011-12-C-0015. The views and findings
contained in this material are those of the authors
and do not necessarily reflect the position or policy
of the U.S. government and no official endorsement
should be inferred.
References
Michael Bendersky and W. Bruce Croft. 2008. Discov-
ering key concepts in verbose queries. Proceedings of
the 31st annual international ACM SIGIR conference
on research and development in information retrieval
- SIGIR ?08, page 491.
Michael Bendersky, Donald Metzler, and W. Bruce Croft.
2010. Learning concept importance using a weighted
dependence model. Proceedings of the third ACM in-
ternational conference on Web search and data mining
- WSDM ?10, page 31.
Michael Bendersky. 2011. Parameterized concept
weighting in verbose queries. Proceedings of the 34th
annual international ACM SIGIR conference on re-
search and development in information retrieval.
Razvan Bunescu and Yunfeng Huang. 2010. Towards a
general model of answer typing: Question focus iden-
tification. In Proceedings of the 11th International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing).
Jennifer Chu-carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003. A multi-
strategy and multi-source approach to question an-
swering. In In Proceedings of Text REtrieval Confer-
ence.
R Florian, H Hassan, A Ittycheriah, H Jing, N Kamb-
hatla, X Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 1?8, Boston, Massachusetts, USA, May 2
- May 7. Association for Computational Linguistics.
D. C. Gondek, A. Lally, A. Kalyanpur, J. W. Murdock,
P. A. Duboue, L. Zhang, Y. Pan, Z. M. Qiu, and
C. Welty. 2012. A framework for merging and rank-
ing of answers in DeepQA. IBM Journal of Research
and Development, 56(3.4):14:1 ?14:12, may-june.
Ulf Hermjakob, Eduard H. Hovy, and Chin yew Lin.
2000. Knowledge-based question answering. In In
Proceedings of the 6th World Multiconference on Sys-
tems, Cybernetics and Informatics (SCI-2002, pages
772?781.
886
BOLT IR. 2012. Broad operational language translation
(BOLT). www.darpa.mil/Our_Work/I2O/
Programs/Broad_Operational_Language_
Translat%ion_(BOLT).aspx. [Online; ac-
cessed 10-Dec-2012].
Abraham Ittycheriah and Salim Roukos. 2001. IBM?s
statistical question answering system - TREC-11. In
Proceedings of the Text REtrieval Conference.
Matthew Lease, James Allan, and W. Bruce Croft. 2009.
Advances in Information Retrieval, volume 5478 of
Lecture Notes in Computer Science. Springer Berlin
Heidelberg, Berlin, Heidelberg, April.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th international confer-
ence on Computational linguistics - Volume 1, COL-
ING ?02, pages 1?7, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
H. P. Luhn. 1958. A business intelligence system. IBM
J. Res. Dev., 2(4):314?319, October.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
lingual coreference resolution with syntactic fea-
tures. In Proc. of Human Language Technology
(HLT)/Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proc. of ACL.
Dan Moldovan, Christine Clark, Sanda Harabagiu, and
Steve Maiorano. 2003. Cogex: a logic prover for
question answering. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages 87?
93.
Christof Monz. 2007. Model tree learning for query
term weighting in question answering. In Proceed-
ings of the 29th European conference on IR re-
search, ECIR?07, pages 589?596, Berlin, Heidelberg.
Springer-Verlag.
Christopher Pinchak. 2006. A probabilistic answer type
model. In In EACL, pages 393?400.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st annual international ACM SIGIR
conference on research and development in informa-
tion retrieval, SIGIR ?98, pages 275?281, New York,
NY, USA. ACM.
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of
the 17th annual international ACM SIGIR conference
on research and development in information retrieval,
SIGIR ?94, pages 232?241, New York, NY, USA.
Springer-Verlag New York, Inc.
Trevor Strohman, Donald Metzler, Howard Turtle, and
W. Bruce Croft. 2005. Indri: a language-model based
search engine for complex queries. Technical report,
in Proceedings of the International Conference on In-
telligent Analysis.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In TREC.
Le Zhao and Jamie Callan. 2010. Term necessity predic-
tion. Proceedings of the 19th ACM international con-
ference on Information and knowledge management -
CIKM ?10, page 259.
887
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1384?1394,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Sentence Compression Based Framework to Query-Focused
Multi-Document Summarization
Lu Wang1 Hema Raghavan2 Vittorio Castelli2 Radu Florian2 Claire Cardie1
1Department of Computer Science, Cornell University, Ithaca, NY 14853, USA
{luwang, cardie}@cs.cornell.edu
2IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA
{hraghav, vittorio, raduf}@us.ibm.com
Abstract
We consider the problem of using sentence
compression techniques to facilitate query-
focused multi-document summarization. We
present a sentence-compression-based frame-
work for the task, and design a series of
learning-based compression models built on
parse trees. An innovative beam search de-
coder is proposed to efficiently find highly
probable compressions. Under this frame-
work, we show how to integrate various in-
dicative metrics such as linguistic motivation
and query relevance into the compression pro-
cess by deriving a novel formulation of a com-
pression scoring function. Our best model
achieves statistically significant improvement
over the state-of-the-art systems on several
metrics (e.g. 8.0% and 5.4% improvements in
ROUGE-2 respectively) for the DUC 2006 and
2007 summarization task.
1 Introduction
The explosion of the Internet clearly warrants
the development of techniques for organizing and
presenting information to users in an effective
way. Query-focused multi-document summariza-
tion (MDS) methods have been proposed as one
such technique and have attracted significant at-
tention in recent years. The goal of query-focused
MDS is to synthesize a brief (often fixed-length)
and well-organized summary from a set of topic-
related documents that answer a complex ques-
tion or address a topic statement. The result-
ing summaries, in turn, can support a number of
information analysis applications including open-
ended question answering, recommender systems,
and summarization of search engine results. As
further evidence of its importance, the Document
Understanding Conference (DUC) has used query-
focused MDS as its main task since 2004 to foster
new research on automatic summarization in the
context of users? needs.
To date, most top-performing systems for
multi-document summarization?whether query-
specific or not?remain largely extractive: their
summaries are comprised exclusively of sen-
tences selected directly from the documents
to be summarized (Erkan and Radev, 2004;
Haghighi and Vanderwende, 2009; Celikyilmaz
and Hakkani-Tu?r, 2011). Despite their simplicity,
extractive approaches have some disadvantages.
First, lengthy sentences that are partly relevant
are either excluded from the summary or (if se-
lected) can block the selection of other important
sentences, due to summary length constraints.
In addition, when people write summaries, they
tend to abstract the content and seldom use
entire sentences taken verbatim from the original
documents. In news articles, for example, most
sentences are lengthy and contain both potentially
useful information for a summary as well as un-
necessary details that are better omitted. Consider
the following DUC query as input for a MDS
system:1 ?In what ways have stolen artworks
been recovered? How often are suspects arrested
or prosecuted for the thefts?? One manually gen-
erated summary includes the following sentence
but removes the bracketed words in gray:
A man suspected of stealing a million-dollar collection
of [hundreds of ancient] Nepalese and Tibetan art objects in
New York [11 years ago] was arrested [Thursday at his South
Los Angeles home, where he had been hiding the antiquities,
police said].
In this example, the compressed sentence is rela-
1From DUC 2005, query for topic d422g.
1384
tively more succinct and readable than the origi-
nal (e.g. in terms of Flesch-Kincaid Reading Ease
Score (Kincaid et al, 1975)). Likewise, removing
information irrelevant to the query (e.g. ?11 years
ago?, ?police said?) is crucial for query-focused
MDS.
Sentence compression techniques (Knight and
Marcu, 2000; Clarke and Lapata, 2008) are the
standard for producing a compact and grammat-
ical version of a sentence while preserving rel-
evance, and prior research (e.g. Lin (2003)) has
demonstrated their potential usefulness for generic
document summarization. Similarly, strides have
been made to incorporate sentence compression
into query-focused MDS systems (Zajic et al,
2006). Most attempts, however, fail to produce
better results than those of the best systems built
on pure extraction-based approaches that use no
sentence compression.
In this paper we investigate the role of sentence
compression techniques for query-focused MDS.
We extend existing work in the area first by inves-
tigating the role of learning-based sentence com-
pression techniques. In addition, we design three
types of approaches to sentence-compression?
rule-based, sequence-based and tree-based?and
examine them within our compression-based
framework for query-specific MDS. Our top-
performing sentence compression algorithm in-
corporates measures of query relevance, con-
tent importance, redundancy and language qual-
ity, among others. Our tree-based methods rely on
a scoring function that allows for easy and flexi-
ble tailoring of sentence compression to the sum-
marization task, ultimately resulting in significant
improvements for MDS, while at the same time
remaining competitive with existing methods in
terms of sentence compression, as discussed next.
We evaluate the summarization models on
the standard Document Understanding Confer-
ence (DUC) 2006 and 2007 corpora 2 for query-
focused MDS and find that all of our compression-
based summarization models achieve statistically
significantly better performance than the best
DUC 2006 systems. Our best-performing sys-
tem yields an 11.02 ROUGE-2 score (Lin and
Hovy, 2003), a 8.0% improvement over the best
reported score (10.2 (Davis et al, 2012)) on the
2We believe that we can easily adapt our system for tasks
(e.g. TAC-08?s opinion summarization or TAC-09?s update
summarization) or domains (e.g. web pages or wikipedia
pages). We reserve that for future work.
DUC 2006 dataset, and an 13.49 ROUGE-2, a
5.4% improvement over the best score in DUC
2007 (12.8 (Davis et al, 2012)). We also ob-
serve substantial improvements over previous sys-
tems w.r.t. the manual Pyramid (Nenkova and
Passonneau, 2004) evaluation measure (26.4 vs.
22.9 (Jagarlamudi et al, 2006)); human annota-
tors furthermore rate our system-generated sum-
maries as having less redundancy and compara-
ble quality w.r.t. other linguistic quality metrics.
With these results we believe we are the first
to successfully show that sentence compression
can provide statistically significant improvements
over pure extraction-based approaches for query-
focused MDS.
2 Related Work
Existing research on query-focused multi-
document summarization (MDS) largely relies
on extractive approaches, where systems usually
take as input a set of documents and select
the top relevant sentences for inclusion in the
final summary. A wide range of methods have
been employed for this task. For unsupervised
methods, sentence importance can be estimated
by calculating topic signature words (Lin and
Hovy, 2000; Conroy et al, 2006), combining
query similarity and document centrality within
a graph-based model (Otterbacher et al, 2005),
or using a Bayesian model with sophisticated
inference (Daume? and Marcu, 2006). Davis et
al. (2012) first learn the term weights by Latent
Semantic Analysis, and then greedily select
sentences that cover the maximum combined
weights. Supervised approaches have mainly
focused on applying discriminative learning for
ranking sentences (Fuentes et al, 2007). Lin and
Bilmes (2011) use a class of carefully designed
submodular functions to reward the diversity of
the summaries and select sentences greedily.
Our work is more related to the less studied
area of sentence compression as applied to (sin-
gle) document summarization. Zajic et al (2006)
tackle the query-focused MDS problem using a
compress-first strategy: they develop heuristics to
generate multiple alternative compressions of all
sentences in the original document; these then be-
come the candidates for extraction. This approach,
however, does not outperform some extraction-
based approaches. A similar idea has been stud-
ied for MDS (Lin, 2003; Gillick and Favre, 2009),
1385
but limited improvement is observed over extrac-
tive baselines with simple compression rules. Fi-
nally, although learning-based compression meth-
ods are promising (Martins and Smith, 2009;
Berg-Kirkpatrick et al, 2011), it is unclear how
well they handle issues of redundancy.
Our research is also inspired by probabilis-
tic sentence-compression approaches, such as the
noisy-channel model (Knight and Marcu, 2000;
Turner and Charniak, 2005), and its extension via
synchronous context-free grammars (SCFG) (Aho
and Ullman, 1969; Lewis and Stearns, 1968) for
robust probability estimation (Galley and McKe-
own, 2007). Rather than attempt to derive a new
parse tree like Knight and Marcu (2000) and Gal-
ley and McKeown (2007), we learn to safely re-
move a set of constituents in our parse tree-based
compression model while preserving grammati-
cal structure and essential content. Sentence-level
compression has also been examined via a dis-
criminative model McDonald (2006), and Clarke
and Lapata (2008) also incorporate discourse in-
formation by using integer linear programming.
3 The Framework
We now present our query-focused MDS frame-
work consisting of three steps: Sentence Rank-
ing, Sentence Compression and Post-processing.
First, sentence ranking determines the importance
of each sentence given the query. Then, a sen-
tence compressor iteratively generates the most
likely succinct versions of the ranked sentences,
which are cumulatively added to the summary, un-
til a length limit is reached. Finally, the post-
processing stage applies coreference resolution
and sentence reordering to build the summary.
Sentence Ranking. This stage aims to rank sen-
tences in order of relevance to the query. Un-
surprisingly, ranking algorithms have been suc-
cessfully applied to this task. We experimented
with two of them ? Support Vector Regres-
sion (SVR) (Mozer et al, 1997) and Lamb-
daMART (Burges et al, 2007). The former
has been used previously for MDS (Ouyang et
al., 2011). LambdaMart on the other hand has
shown considerable success in information re-
trieval tasks (Burges, 2010); we are the first to
apply it to summarization. For training, we use
40 topics (i.e. queries) from the DUC 2005 cor-
pus (Dang, 2005) along with their manually gener-
ated abstracts. As in previous work (Shen and Li,
Basic Features
relative/absolute position
is among the first 1/3/5 sentences?
number of words (with/without stopwords)
number of words more than 5/10 (with/without stopwords)
Query-Relevant Features
unigram/bigram/skip bigram (at most four words apart) overlap
unigram/bigram TF/TF-IDF similarity
mention overlap
subject/object/indirect object overlap
semantic role overlap
relation overlap
Query-Independent Features
average/total unigram/bigram IDF/TF-IDF
unigram/bigram TF/TF-IDF similarity with the centroid of the cluster
average/sum of sumBasic/SumFocus (Toutanova et al, 2007)
average/sum of mutual information
average/sum of number of topic signature words (Lin and Hovy, 2000)
basic/improved sentence scorers from Conroy et al (2006)
Content Features
contains verb/web link/phone number?
contains/portion of words between parentheses
Table 1: Sentence-level features for sentence ranking.
2011; Ouyang et al, 2011), we use the ROUGE-
2 score, which measures bigram overlap between
a sentence and the abstracts, as the objective for
regression.
While space limitations preclude a longer dis-
cussion of the full feature set (ref. Table 1), we
describe next the query-relevant features used for
sentence ranking as these are the most impor-
tant for our summarization setting. The goal of
this feature subset is to determine the similarity
between the query and each candidate sentence.
When computing similarity, we remove stopwords
as well as the words ?discuss, describe, specify,
explain, identify, include, involve, note? that are
adopted and extended from Conroy et al (2006).
Then we conduct simple query expansion based
on the title of the topic and cross-document coref-
erence resolution. Specifically, we first add the
words from the topic title to the query. And for
each mention in the query, we add other mentions
within the set of documents that corefer with this
mention. Finally, we compute two versions of the
features?one based on the original query and an-
other on the expanded one. We also derive the
semantic role overlap and relation instance over-
lap between the query and each sentence. Cross-
document coreference resolution, semantic role la-
beling and relation extraction are accomplished
via the methods described in Section 5.
Sentence Compression. As the main focus of
this paper, we propose three types of compression
methods, described in detail in Section 4 below.
Post-processing. Post-processing performs
coreference resolution and sentence ordering.
1386
Basic Features Syntactic Tree Features
first 1/3/5 tokens (toks)? POS tag
last 1/3/5 toks? parent/grandparent label
first letter/all letters capitalized? leftmost child of parent?
is negation? second leftmost child of parent?
is stopword? is headword?
Dependency Tree Features in NP/VP/ADVP/ADJP chunk?
dependency relation (dep rel) Semantic Features
parent/grandparent dep rel is a predicate?
is the root? semantic role label
has a depth larger than 3/5?
Rule-Based Features
For each rule in Table 2 , we construct a corresponding feature to
indicate whether the token is identified by the rule.
Table 3: Token-level features for sequence-based com-
pression.
We replace each pronoun with its referent unless
they appear in the same sentence. For sentence
ordering, each compressed sentence is assigned
to the most similar (tf-idf) query sentence. Then
a Chronological Ordering algorithm (Barzilay et
al., 2002) sorts the sentences for each query based
first on the time stamp, and then the position in
the source document.
4 Sentence Compression
Sentence compression is typically formulated as
the problem of removing secondary information
from a sentence while maintaining its grammati-
cality and semantic structure (Knight and Marcu,
2000; McDonald, 2006; Galley and McKeown,
2007; Clarke and Lapata, 2008). We leave other
rewrite operations, such as paraphrasing and re-
ordering, for future work. Below we describe
the sentence compression approaches developed
in this research: RULE-BASED COMPRESSION,
SEQUENCE-BASED COMPRESSION, and TREE-
BASED COMPRESSION.
4.1 Rule-based Compression
Turner and Charniak (2005) have shown that ap-
plying hand-crafted rules for trimming sentences
can improve both content and linguistic qual-
ity. Our rule-based approach extends existing
work (Conroy et al, 2006; Toutanova et al, 2007)
to create the linguistically-motivated compression
rules of Table 2. To avoid ill-formed output, we
disallow compressions of more than 10 words by
each rule.
4.2 Sequence-based Compression
As in McDonald (2006) and Clarke and Lapata
(2008), our sequence-based compression model
makes a binary ?keep-or-delete? decision for each
word in the sentence. In contrast, however, we
Figure 1: Diagram of tree-based compression. The
nodes to be dropped are grayed out. In this example,
the root of the gray subtree (a ?PP?) would be labeled
REMOVE. Its siblings and parent are labeled RETAIN
and PARTIAL, respectively. The trimmed tree is real-
ized as ?Malaria causes millions of deaths.?
view compression as a sequential tagging problem
and make use of linear-chain Conditional Ran-
dom Fields (CRFs) (Lafferty et al, 2001) to se-
lect the most likely compression. We represent
each sentence as a sequence of tokens, X =
x0x1 . . . xn, and generate a sequence of labels,
Y = y0y1 . . . yn, that encode which tokens are
kept, using a BIO label format: {B-RETAIN de-
notes the beginning of a retained sequence, I-
RETAIN indicates tokens ?inside? the retained se-
quence, O marks tokens to be removed}.
The CRF model is built using the features
shown in Table 3. ?Dependency Tree Features?
encode the grammatical relations in which each
word is involved as a dependent. For the ?Syntac-
tic Tree?, ?Dependency Tree? and ?Rule-Based?
features, we also include features for the two
words that precede and the two that follow the cur-
rent word. Detailed descriptions of the training
data and experimental setup are in Section 5.
During inference, we find the maximally likely
sequence Y according to a CRF with parameter
? (Y = argmaxY ? P (Y ?|X; ?)), while simulta-
neously enforcing the rules of Table 2 to reduce
the hypothesis space and encourage grammatical
compression. To do this, we encode these rules as
features for each token, and whenever these fea-
ture functions fire, we restrict the possible label
for that token to ?O?.
4.3 Tree-based Compression
Our tree-based compression methods are in line
with syntax-driven approaches (Galley and McK-
eown, 2007), where operations are carried out
on parse tree constituents. Unlike previous
work (Knight and Marcu, 2000; Galley and McK-
eown, 2007), we do not produce a new parse tree,
1387
Rule Example
Header [MOSCOW , October 19 ( Xinhua ) ?] Russian federal troops Tuesday continued...
Relative dates ...Centers for Disease Control confirmed [Tuesday] that there was...
Intra-sentential attribution ...fueling the La Nina weather phenomenon, [the U.N. weather agency said].
Lead adverbials [Interestingly], while the Democrats tend to talk about...
Noun appositives Wayne County Prosecutor [John O?Hara] wanted to send a message...
Nonrestrictive relative clause Putin, [who was born on October 7, 1952 in Leningrad], was elected in the presidential election...
Adverbial clausal modifiers [Starting in 1998], California will require 2 per cent of a manufacturer...
(Lead sentence) [Given the short time], car makers see electric vehicles as...
Within Parentheses ...to Christian home schoolers in the early 1990s [(www.homecomputermarket.com)].
Table 2: Linguistically-motivated rules for sentence compression. The grayed-out words in brackets are removed.
but focus on learning to identify the proper set of
constituents to be removed. In particular, when a
node is dropped from the tree, all words it sub-
sumes will be deleted from the sentence.
Formally, given a parse tree T of the sentence
to be compressed and a tree traversal algorithm,
T can be presented as a list of ordered constituent
nodes, T = t0t1 . . . tm. Our objective is to find a
set of labels, L = l0l1 . . . lm, where li ? {RETAIN,
REMOVE, PARTIAL}. RETAIN (RET) and RE-
MOVE (REM) denote whether the node ti is re-
tained or removed. PARTIAL (PAR) means ti is
partly removed, i.e. at least one child subtree of ti
is dropped.
Labels are identified, in order, according to the
tree traversal algorithm. Every node label needs
to be compatible with the labeling history: given
a node ti, and a set of labels l0 . . . li?1 predicted
for nodes t0 . . . ti?1, li =RET or li =REM is com-
patible with the history when all children of ti are
labeled as RET or REM, respectively; li =PAR is
compatible when ti has at least two descendents
tj and tk (j < i and k < i), one of which is
RETained and the other, REMoved. As such, the
root of the gray subtree in Figure 1 is labeled as
REM; its left siblings as RET; its parent as PAR.
As the space of possible compressions is expo-
nential in the number of leaves in the parse tree,
instead of looking for the globally optimal solu-
tion, we use beam search to find a set of highly
likely compressions and employ a language model
trained on a large corpus for evaluation.
A Beam Search Decoder. The beam search de-
coder (see Algorithm 1) takes as input the sen-
tence?s parse tree T = t0t1 . . . tm, an order-
ing O for traversing T (e.g. postorder) as a se-
quence of nodes in T , the set L of possible
node labels, a scoring function S for evaluat-
ing each sentence compression hypothesis, and
a beam size N . Specifically, O is a permuta-
tion on the set {0, 1, . . . ,m}?each element an
index onto T . Following O, T is re-ordered as
tO0tO1 . . . tOm , and the decoder considers each or-
dered constituent tOi in turn. In iteration i, all
existing sentence compression hypotheses are ex-
panded by one node, tOi , labeling it with all com-
patible labels. The new hypotheses (usually sub-
sentences) are ranked by the scorer S and the top
N are preserved to be extended in the next itera-
tion. See Figure 2 for an example.
Input : parse tree T , ordering O = O0O1 . . . Om,
L ={RET, REM, PAR}, hypothesis scorer S,
beam size N
Output: N best compressions
stack? ? (empty set);
foreach node tOi in T = tO0 . . . tOm doif i == 0 (first node visited) then
foreach label lO0 in L donewHypothesis h? ? [lO0 ];put h? into Stack;
end
else
newStack? ? (empty set);
foreach hypothesis h in stack do
foreach label lOi in L doif lOi is compatible thennewHypothesis h? ? h + [lOi ];put h? into newStack;
end
end
end
stack? newStack;
end
Apply S to sort hypotheses in stack in descending
order;
Keep the N best hypotheses in stack;
end
Algorithm 1: Beam search decoder.
Our BASIC Tree-based Compression in-
stantiates the beam search decoder with
postorder traversal and a hypothesis scorer
that takes a possible sentence compression?
a sequence of nodes (e.g. tO0 . . . tOk ) and
their labels (e.g. lO0 . . . lOk )?and returns?k
j=1 logP (lOj |tOj ) (denoted later as
ScoreBasic). The probability is estimated by
a Maximum Entropy classifier (Berger et al,
1388
Figure 2: Example of beam search decoding. For
postorder traversal, the three nodes are visited in a
bottom-up order. The associated compression hypothe-
ses (boxed) are ranked based on the scores in parenthe-
ses. Beam scores for other nodes are omitted.
Basic Features Syntactic Tree Features
projection falls w/in first 1/3/5 toks?? constituent label
projection falls w/in last 1/3/5 toks?? parent left/right sibling label
subsumes first 1/3/5 toks?? grandparent left/right sibling label
subsumes last 1/3/5 toks?? is leftmost child of parent?
number of words larger than 5/10?? is second leftmost child of parent?
is leaf node?? is head node of parent?
is root of parsing tree?? label of its head node
has word with first letter capitalized? has a depth greater than 3/5/10?
has word with all letters capitalized? Dependency Tree Features
has negation? dep rel of head node?
has stopwords? dep rel of parent?s head node?
Semantic Features dep rel of grandparent?s head node?
the head node has predicate? contain root of dep tree??
semantic roles of head node has a depth larger than 3/5??
Rule-Based Features
For each rule in Table 2 , we construct a corresponding feature to indicate
whether the token is identified by the rule.
Table 4: Constituent-level features for tree-based com-
pression. ? or ? denote features that are concatenated
with every Syntactic Tree feature to compose a new
one.
1996) trained at the constituent level using the
features in Table 4. We also apply the rules of
Table 2 during the decoding process. Concretely,
if the words subsumed by a node are identified
by any rule, we only consider REM as the node?s
label.
Given the N -best compressions from the de-
coder, we evaluate the yield of the trimmed trees
using a language model trained on the Giga-
word (Graff, 2003) corpus and return the compres-
sion with the highest probability. Thus, the de-
coder is quite flexible ? its learned scoring func-
tion allows us to incorporate features salient for
sentence compression while its language model
guarantees the linguistic quality of the compressed
string. In the sections below we consider addi-
tional improvements.
4.3.1 Improving Beam Search
CONTEXT-aware search is based on the intu-
ition that predictions on preceding context can
be leveraged to facilitate the prediction of the
current node. For example, parent nodes with
children that have all been removed (retained)
should have a label of REM (RET). In light of
this, we encode these contextual predictions as
additional features of S, that is, ALL-CHILDREN-
REMOVED/RETAINED, ANY-LEFTSIBLING-
REMOVED/RETAINED/PARTLY REMOVED,
LABEL-OF-LEFT-SIBLING/HEAD-NODE.
HEAD-driven search modifies the BASIC pos-
torder tree traversal by visiting the head node first
at each level, leaving other orders unchanged. In
a nutshell, if the head node is dropped, then its
modifiers need not be preserved. We adopt the
same features as CONTEXT-aware search, but re-
move those involving left siblings. We also add
one more feature: LABEL-OF-THE-HEAD-NODE-
IT-MODIFIES.
4.3.2 Task-Specific Sentence Compression
The current scorer ScoreBasic is still fairly naive
in that it focuses only on features of the sen-
tence to be compressed. However extra-sentential
knowledge can also be important for query-
focused MDS. For example, information regard-
ing relevance to the query might lead the de-
coder to produce compressions better suited for
the summary. Towards this goal, we construct
a compression scoring function?the multi-scorer
(MULTI)?that allows the incorporation of mul-
tiple task-specific scorers. Given a hypothesis at
any stage of decoding, which yields a sequence of
words W = w0w1...wj , we propose the following
component scorers.
Query Relevance. Query information ought to
guide the compressor to identify the relevant con-
tent. The query Q is expanded as described in
Section 3. Let |W ? Q| denote the number of
unique overlapping words betweenW andQ, then
scoreq = |W ?Q|/|W |.
Importance. A query-independent impor-
tance score is defined as the average Sum-
Basic (Toutanova et al, 2007) value in W ,
i.e. scoreim =?ji=1 SumBasic(wi)/|W |.
Language Model. We let scorelm be the proba-
bility of W computed by a language model.
Cross-Sentence Redundancy. To encourage di-
versified content, we define a redundancy score to
discount replicated content: scorered = 1? |W ?
C|/|W |, whereC is the words already selected for
the summary.
1389
The multi-scorer is defined as a linear
combination of the component scorers: Let
~? = (?0, . . . , ?4), 0 ? ?i ? 1, ????score =
(scoreBasic, scoreq, scoreim, scorelm, scorered),
S = scoremulti = ~? ? ????score (1)
The parameters ~? are tuned on a held-out tuning
set by grid search. We linearly normalize the score
of each metric, where the minimum and maximum
values are estimated from the tuning data.
5 Experimental Setup
We evaluate our methods on the DUC 2005, 2006
and 2007 datasets (Dang, 2005; Dang, 2006;
Dang, 2007), each of which is a collection of
newswire articles. 50 complex queries (topics) are
provided for DUC 2005 and 2006, 35 are collected
for DUC 2007 main task. Relevant documents for
each query are provided along with 4 to 9 human
MDS abstracts. The task is to generate a summary
within 250 words to address the query. We split
DUC 2005 into two parts: 40 topics to train the
sentence ranking models, and 10 for ranking algo-
rithm selection and parameter tuning for the multi-
scorer. DUC 2006 and DUC 2007 are reserved as
held out test sets.
Sentence Compression. The dataset
from Clarke and Lapata (2008) is used to
train the CRF and MaxEnt classifiers (Section 4).
It includes 82 newswire articles with one manually
produced compression aligned to each sentence.
Preprocessing. Documents are processed by a
full NLP pipeline, including token and sentence
segmentation, parsing, semantic role labeling,
and an information extraction pipeline consist-
ing of mention detection, NP coreference, cross-
document resolution, and relation detection (Flo-
rian et al, 2004; Luo et al, 2004; Luo and Zitouni,
2005).
Learning for Sentence Ranking and Compres-
sion. We use Weka (Hall et al, 2009) to train a
support vector regressor and experiment with var-
ious rankers in RankLib (Dang, 2011)3. As Lamb-
daMART has an edge over other rankers on the
held-out dataset, we selected it to produce ranked
sentences for further processing. For sequence-
based compression using CRFs, we employ Mal-
let (McCallum, 2002) and integrate the Table 2
rules during inference. NLTK (Bird et al, 2009)
3Default parameters are used. If an algorithm needs a val-
idation set, we use 10 out of 40 topics.
MaxEnt classifiers are used for tree-based com-
pression. Beam size is fixed at 2000.4 Sen-
tence compressions are evaluated by a 5-gram lan-
guage model trained on Gigaword (Graff, 2003)
by SRILM (Stolcke, 2002).
6 Results
The results in Table 5 use the official ROUGE soft-
ware with standard options5 and report ROUGE-
2 (R-2) (measures bigram overlap) and ROUGE-
SU4 (R-SU4) (measures unigram and skip-bigram
separated by up to four words). We compare our
sentence-compression-based methods to the best
performing systems based on ROUGE in DUC
2006 and 2007 (Jagarlamudi et al, 2006; Pingali
et al, 2007), system by Davis et al (2012) that
report the best R-2 score on DUC 2006 and 2007
thus far, and to the purely extractive methods of
SVR and LambdaMART.
Our sentence-compression-based systems
(marked with ?) show statistically significant
improvements over pure extractive summarization
for both R-2 and R-SU4 (paired t-test, p < 0.01).
This means our systems can effectively remove
redundancy within the summary through compres-
sion. Furthermore, our HEAD-driven beam search
method with MULTI-scorer beats all systems on
DUC 20066 and all systems on DUC 2007 except
the best system in terms of R-2 (p < 0.01). Its
R-SU4 score is also significantly (p < 0.01)
better than extractive methods, rule-based and
sequence-based compression methods on both
DUC 2006 and 2007. Moreover, our systems with
learning-based compression have considerable
compression rates, indicating their capability to
remove superfluous words as well as improve
summary quality.
Human Evaluation. The Pyramid (Nenkova
and Passonneau, 2004) evaluation was developed
to manually assess how many relevant facts or
Summarization Content Units (SCUs) are cap-
tured by system summaries. We ask a professional
annotator (who is not one of the authors, is highly
experienced in annotating for various NLP tasks,
and is fluent in English) to carry out a Pyramid
evaluation on 10 randomly selected topics from
4We looked at various beam sizes on the heldout data, and
observed that the performance peaks around this value.
5ROUGE-1.5.5.pl -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f
A -p 0.5 -t 0 -a -d
6The system output from Davis et al (2012) is not avail-
able, so significance tests are not conducted on it.
1390
DUC 2006 DUC 2007
System C Rate R-2 R-SU4 C Rate R-2 R-SU4
Best DUC system ? 9.56 15.53 ? 12.62 17.90
Davis et al (2012) ? 10.2 15.2 ? 12.8 17.5
SVR 100% 7.78 13.02 100% 9.53 14.69
LambdaMART 100% 9.84 14.63 100% 12.34 15.62
Rule-based 78.99% 10.62 ?? 15.73 ? 78.11% 13.18? 18.15?
Sequence 76.34% 10.49 ? 15.60 ? 77.20% 13.25? 18.23?
Tree (BASIC + ScoreBasic) 70.48% 10.49 ? 15.86 ? 69.27% 13.00? 18.29?
Tree (CONTEXT + ScoreBasic) 65.21% 10.55 ?? 16.10 ? 63.44% 12.75 18.07?
Tree (HEAD + ScoreBasic) 66.70% 10.66 ?? 16.18 ? 65.05% 12.93 18.15?
Tree (HEAD + MULTI) 70.20% 11.02 ?? 16.25 ? 73.40% 13.49? 18.46?
Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
preserved. R-2 (ROUGE-2) and R-SU4 (ROUGE-SU4) scores are multiplied by 100. ??? indicates that data is
unavailable. BASIC, CONTEXT and HEAD represent the basic beam search decoder, context-aware and head-driven
search extensions respectively. ScoreBasic and MULTI refer to the type of scorer used. Statistically significant
improvements (p < 0.01) over the best system in DUC 06 and 07 are marked with ?. ? indicates statistical
significance (p < 0.01) over extractive approaches (SVR or LambdaMART). HEAD + MULTI outperforms all the
other extract- and compression-based systems in R-2.
System Pyr Gra Non-Red Ref Foc Coh
Best DUC system (ROUGE) 22.9?8.2 3.5?0.9 3.5?1.0 3.5?1.1 3.6?1.0 2.9?1.1
Best DUC system (LQ) ? 4.0?0.8 4.2?0.7 3.8?0.7 3.6?0.9 3.4?0.9
Our System 26.4?10.3 3.0?0.9 4.0?1.1 3.6?1.0 3.4?0.9 2.8?1.0
Table 6: Human evaluation on our multi-scorer based system, Jagarlamudi et al (2006) (Best DUC system
(ROUGE)), and Lacatusu et al (2006) (Best DUC system (LQ)). Our system can synthesize more relevant content
according to Pyramid (?100). We also examine linguistic quality (LQ) in Grammaticality (Gra), Non-redundancy
(Non-Red), Referential clarity (Ref), Focus (Foc), and Structure and Coherence (Coh) like Dang (2006), each rated
from 1 (very poor) to 5 (very good). Our system has better non-redundancy than Jagarlamudi et al (2006) and is
comparable to Jagarlamudi et al (2006) and Lacatusu et al (2006) in other metrics except grammaticality.
the DUC 2006 task with gold-standard SCU an-
notation in abstracts. The Pyramid score (see Ta-
ble 6) is re-calculated for the system with best
ROUGE scores in DUC 2006 (Jagarlamudi et al,
2006) along with our system by the same annota-
tor to make a meaningful comparison.
We further evaluate the linguistic quality (LQ)
of the summaries for the same 10 topics in ac-
cordance with the measurement in Dang (2006).
Four native speakers who are undergraduate stu-
dents in computer science (none are authors) per-
formed the task, We compare our system based
on HEAD-driven beam search with MULTI-scorer
to the best systems in DUC 2006 achieving top
ROUGE scores (Jagarlamudi et al, 2006) (Best
DUC system (ROUGE)) and top linguistic quality
scores (Lacatusu et al, 2006) (Best DUC system
(LQ))7. The average score and standard deviation
for each metric is displayed in Table 6. Our sys-
tem achieves a higher Pyramid score, an indication
that it captures more of the salient facts. We also
7Lacatusu et al (2006) obtain the best scores in three lin-
guistic quality metrics (i.e. grammaticality, focus, structure
and coherence), and overall responsiveness on DUC 2006.
attain better non-redundancy than Jagarlamudi et
al. (2006), meaning that human raters perceive
less replicative content in our summaries. Scores
for other metrics are comparable to Jagarlamudi
et al (2006) and Lacatusu et al (2006), which
either uses minimal non-learning-based compres-
sion rules or is a pure extractive system. However,
our compression system sometimes generates less
grammatical sentences, and those are mostly due
to parsing errors. For example, parsing a clause
starting with a past tense verb as an adverbial
clausal modifier can lead to an ill-formed com-
pression. Those issues can be addressed by an-
alyzing k-best parse trees and we leave it in the
future work. A sample summary from our multi-
scorer based system is in Figure 3.
Sentence Compression Evaluation. We
also evaluate sentence compression separately
on (Clarke and Lapata, 2008), adopting the same
partitions as (Martins and Smith, 2009), i.e. 1, 188
sentences for training and 441 for testing. Our
compression models are compared with Hedge
Trimmer (Dorr et al, 2003), a discriminative
model proposed by McDonald (2006) and a
1391
System C Rate Uni-Prec Uni-Rec Uni-F1 Rel-F1
HedgeTrimmer 57.64% 0.72 0.65 0.64 0.50
McDonald (2006) 70.95% 0.77 0.78 0.77 0.55
Martins and Smith (2009) 71.35% 0.77 0.78 0.77 0.56
Rule-based 87.65% 0.74 0.91 0.80 0.63
Sequence 70.79% 0.77 0.80 0.76 0.58
Tree (BASIC) 69.65% 0.77 0.79 0.75 0.56
Tree (CONTEXT) 67.01% 0.79 0.78 0.76 0.57
Tree (HEAD) 68.06% 0.79 0.80 0.77 0.59
Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
all use single-scorer. Our context-aware and head-driven tree-based approaches outperform all the other systems
significantly (p < 0.01) in precision (Uni-Prec) without sacrificing the recalls (i.e. there is no statistically signifi-
cant difference between our models and McDonald (2006) / M & S (2009) with p > 0.05). Italicized numbers for
unigram F1 (Uni-F1) are statistically indistinguishable (p > 0.05). Our head-driven tree-based approach also pro-
duces significantly better grammatical relations F1 scores (Rel-F1) than all the other systems except the rule-based
method (p < 0.01).
Topic D0626H: How were the bombings of the US em-
bassies in Kenya and Tanzania conducted? What terror-
ist groups and individuals were responsible? How and
where were the attacks planned?
WASHINGTON, August 13 (Xinhua) ? President Bill
Clinton Thursday condemned terrorist bomb attacks at
U.S. embassies in Kenya and Tanzania and vowed to find
the bombers and bring them to justice. Clinton met with
his top aides Wednesday in the White House to assess the
situation following the twin bombings at U.S. embassies
in Kenya and Tanzania, which have killed more than 250
people and injured over 5,000, most of them Kenyans and
Tanzanians. Local sources said the plan to bomb U.S. em-
bassies in Kenya and Tanzania took three months to com-
plete and bombers destined for Kenya were dispatched
through Somali and Rwanda. FBI Director Louis Freeh,
Attorney General Janet Reno and other senior U.S. gov-
ernment officials will hold a news conference at 1 p.m.
EDT (1700GMT) at FBI headquarters in Washington ?to
announce developments in the investigation of the bomb-
ings of the U.S. embassies in Kenya and Tanzania,? the
FBI said in a statement. ...
Figure 3: Part of the summary generated by the multi-
scorer based summarizer for topic D0626H (DUC
2006). Grayed out words are removed. Query-
irrelevant phrases, such as temporal information or
source of the news, have been removed.
dependency-tree based compressor (Martins and
Smith, 2009)8. We adopt the metrics in Martins
and Smith (2009) to measure the unigram-level
macro precision, recall, and F1-measure with
respect to human annotated compression. In
addition, we also compute the F1 scores of
grammatical relations which are annotated by
RASP (Briscoe and Carroll, 2002) according
to Clarke and Lapata (2008).
In Table 7, our context-aware and head-driven
tree-based compression systems show statistically
significantly (p < 0.01) higher precisions (Uni-
8Thanks to Andre? F.T. Martins for system outputs.
Prec) than all the other systems, without decreas-
ing the recalls (Uni-Rec) significantly (p > 0.05)
based on a paired t-test. Unigram F1 scores (Uni-
F1) in italics indicate that the corresponding sys-
tems are not statistically distinguishable (p >
0.05). For grammatical relation evaluation, our
head-driven tree-based system obtains statistically
significantly (p < 0.01) better F1 score (Rel-F1
than all the other systems except the rule-based
system).
7 Conclusion
We have presented a framework for query-focused
multi-document summarization based on sentence
compression. We propose three types of com-
pression approaches. Our tree-based compres-
sion method can easily incorporate measures of
query relevance, content importance, redundancy
and language quality into the compression pro-
cess. By testing on a standard dataset using the
automatic metric ROUGE, our models show sub-
stantial improvement over pure extraction-based
methods and state-of-the-art systems. Our best
system also yields better results for human eval-
uation based on Pyramid and achieves comparable
linguistic quality scores.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant IIS-0968450 and a gift
from Boeing. We thank Ding-Jung Han, Young-
Suk Lee, Xiaoqiang Luo, Sameer Maskey, Myle
Ott, Salim Roukos, Yiye Ruan, Ming Tan, Todd
Ward, Bowen Zhou, and the ACL reviewers for
valuable suggestions and advice on various as-
pects of this work.
1392
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax directed
translations and the pushdown assembler. J. Comput. Syst.
Sci., 3(1):37?56.
Regina Barzilay, Noemie Elhadad, and Kathleen R. McKe-
own. 2002. Inferring strategies for sentence ordering in
multidocument news summarization. J. Artif. Int. Res.,
17(1):35?55, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011.
Jointly learning to extract and compress. ACL ?11, pages
481?490, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput. Linguist.,
22(1):39?71, March.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural
Language Processing with Python. O?Reilly Media.
T. Briscoe and J. Carroll. 2002. Robust accurate statistical
annotation of general text.
Christopher J.C. Burges, Robert Ragno, and Quoc Viet Le.
2007. Learning to rank with nonsmooth cost functions. In
B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Advances
in Neural Information Processing Systems 19, pages 193?
200. MIT Press, Cambridge, MA.
Christopher J. C. Burges. 2010. From RankNet to Lamb-
daRank to LambdaMART: An overview. Technical report,
Microsoft Research.
Asli Celikyilmaz and Dilek Hakkani-Tu?r. 2011. Discovery
of topically coherent sentences for extractive summariza-
tion. ACL ?11, pages 491?499, Stroudsburg, PA, USA.
Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global inference
for sentence compression an integer linear programming
approach. J. Artif. Int. Res., 31(1):399?429, March.
John M. Conroy, Judith D. Schlesinger, Dianne P. O?Leary,
and Jade Goldstein, 2006. Back to Basics: CLASSY 2006.
U.S. National Inst. of Standards and Technology.
Hoa T. Dang. 2005. Overview of DUC 2005. In Document
Understanding Conference.
Hoa Tran Dang. 2006. Overview of DUC 2006. In
Proc. Document Understanding Workshop, page 10 pages.
NIST.
Hoa T. Dang. 2007. Overview of DUC 2007. In Document
Understanding Conference.
Van Dang. 2011. RankLib. Online.
Hal Daume?, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. ACL ?06, pages 305?312,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sashka T. Davis, John M. Conroy, and Judith D. Schlesinger.
2012. Occams - an optimal combinatorial covering algo-
rithm for multi-document summarization. In ICDM Work-
shops, pages 454?463.
Bonnie J Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: a parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 03 on
Text summarization workshop - Volume 5, HLT-NAACL-
DUC ?03, pages 1 ? 8, Stroudsburg, PA, USA. Association
for Computational Linguistics, Association for Computa-
tional Linguistics.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-
based lexical centrality as salience in text summarization.
J. Artif. Int. Res., 22(1):457?479, December.
Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan
Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov,
and Salim Roukos. 2004. A statistical model for multilin-
gual entity detection and tracking. In HLT-NAACL, pages
1?8.
Maria Fuentes, Enrique Alfonseca, and Horacio Rodr??guez.
2007. Support vector machines for query-focused sum-
marization trained and evaluated on pyramid data. In Pro-
ceedings of the 45th Annual Meeting of the ACL on In-
teractive Poster and Demonstration Sessions, ACL ?07,
pages 57?60, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lexicalized
Markov grammars for sentence compression. NAACL
?07, pages 180?187, Rochester, New York, April. Asso-
ciation for Computational Linguistics.
Dan Gillick and Benoit Favre. 2009. A scalable global model
for summarization. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Process-
ing, ILP ?09, pages 10?18, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
David Graff. 2003. English Gigaword.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summarization.
NAACL ?09, pages 362?370, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. 2009.
The weka data mining software: an update. SIGKDD Ex-
plor. Newsl., 11(1):10?18, November.
Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva Varma,
2006. Query Independent Sentence Scoring approach to
DUC 2006.
J. Peter Kincaid, Robert P. Fishburne, Richard L. Rogers, and
Brad S. Chissom. 1975. Derivation of New Readability
Formulas (Automated Readability Index, Fog Count and
Flesch Reading Ease Formula) for Navy Enlisted Person-
nel. Technical report, February.
Kevin Knight and Daniel Marcu. 2000. Statistics-based sum-
marization - step one: Sentence compression. AAAI ?00,
pages 703?710. AAAI Press.
Finley Lacatusu, Andrew Hickl, Kirk Roberts, Ying Shi,
Jeremy Bensley, Bryan Rink, Patrick Wang, and Lara Tay-
lor, 2006. LCCs gistexter at duc 2006: Multi-strategy
multi-document summarization.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
1393
Proceedings of the Eighteenth International Conference
on Machine Learning, ICML ?01, pages 282?289, San
Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed
transduction. J. ACM, 15(3):465?488, July.
Hui Lin and Jeff Bilmes. 2011. A class of submodular func-
tions for document summarization. In Proceedings of the
49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume 1,
HLT ?11, pages 510?520, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2000. The automated ac-
quisition of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational
linguistics - Volume 1, COLING ?00, pages 495?501,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1,
pages 71?78.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression: a pilot study. In Pro-
ceedings of the sixth international workshop on Informa-
tion retrieval with Asian languages - Volume 11, AsianIR
?03, pages 1?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In
HLT/EMNLP.
Xiaoqiang Luo, Abraham Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based on
the bell tree. In ACL, pages 135?142.
Andre? F. T. Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and com-
pression. In Proceedings of the Workshop on Integer Lin-
ear Programming for Natural Langauge Processing, ILP
?09, pages 1?9, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.cs.umass.edu.
Ryan McDonald. 2006. Discriminative Sentence Compres-
sion with Soft Syntactic Constraints. In Proceedings of
the 11th?EACL, Trento, Italy, April.
Michael Mozer, Michael I. Jordan, and Thomas Petsche, ed-
itors. 1997. Advances in Neural Information Processing
Systems 9, NIPS, Denver, CO, USA, December 2-5, 1996.
MIT Press.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluating
content selection in summarization: The pyramid method.
In Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, HLT-NAACL 2004: Main Proceedings, pages 145?
152, Boston, Massachusetts, USA, May 2 - May 7. Asso-
ciation for Computational Linguistics.
Jahna Otterbacher, Gu?nes? Erkan, and Dragomir R. Radev.
2005. Using random walks for question-focused sentence
retrieval. In Proceedings of the conference on Human
Language Technology and Empirical Methods in Natural
Language Processing, HLT ?05, pages 915?922, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
You Ouyang, Wenjie Li, Sujian Li, and Qin Lu. 2011.
Applying regression models to query-focused multi-
document summarization. Inf. Process. Manage.,
47(2):227?237, March.
Prasad Pingali, Rahul K, and Vasudeva Varma, 2007. IIIT
Hyderabad at DUC 2007. U.S. National Inst. of Standards
and Technology.
Chao Shen and Tao Li. 2011. Learning to rank for query-
focused multi-document summarization. In Diane J.
Cook, Jian Pei, Wei Wang 0010, Osmar R. Zaane, and
Xindong Wu, editors, ICDM, pages 626?634. IEEE.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 2,
pages 901?904, Denver, USA.
Kristina Toutanova, Chris Brockett, Michael Gamon, Ja-
gadeesh Jagarlamudi, Hisami Suzuki, and Lucy Vander-
wende. 2007. The PYTHY Summarization System: Mi-
crosoft Research at DUC 2007. In Proc. of DUC.
Jenine Turner and Eugene Charniak. 2005. Supervised and
unsupervised learning for sentence compression. ACL
?05, pages 290?297, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Zajic, Bonnie J Dorr, Jimmy Lin, and R. Schwartz.
2006. Sentence compression as a component of a multi-
document summarization system. Proceedings of the
2006 Document Understanding Workshop, New York.
1394
