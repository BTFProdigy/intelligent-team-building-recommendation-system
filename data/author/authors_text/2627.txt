Proceedings of the 12th Conference of the European Chapter of the ACL, pages 309?317,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Semitic Morphological Analysis and Generation
Using Finite State Transducers with Feature Structures
Michael Gasser
Indiana University, School of Informatics
Bloomington, Indiana, USA
gasser@indiana.edu
Abstract
This paper presents an application of finite
state transducers weighted with feature
structure descriptions, following Amtrup
(2003), to the morphology of the Semitic
language Tigrinya. It is shown that
feature-structure weights provide an effi-
cient way of handling the templatic mor-
phology that characterizes Semitic verb
stems as well as the long-distance de-
pendencies characterizing the complex
Tigrinya verb morphotactics. A relatively
complete computational implementation
of Tigrinya verb morphology is described.
1 Introduction
1.1 Finite state morphology
Morphological analysis is the segmentation of
words into their component morphemes and the
assignment of grammatical morphemes to gram-
matical categories and lexical morphemes to lex-
emes. For example, the English noun parties
could be analyzed as party+PLURAL. Morpho-
logical generation is the reverse process. Both
processes relate a surface level to a lexical level.
The relationship between these levels has con-
cerned many phonologists and morphologists over
the years, and traditional descriptions, since the
pioneering work of Chomsky and Halle (1968),
have characterized it in terms of a series of ordered
content-sensitive rewrite rules, which apply in the
generation, but not the analysis, direction.
Within computational morphology, a very sig-
nificant advance came with the demonstration that
phonological rules could be implemented as fi-
nite state transducers (Johnson, 1972; Kaplan
and Kay, 1994) (FSTs) and that the rule ordering
could be dispensed with using FSTs that relate the
surface and lexical levels directly (Koskenniemi,
1983). Because of the invertibility of FSTs, ?two-
level? phonology and morphology permitted the
creation of systems of FSTs that implemented both
analysis (surface input, lexical output) and gener-
ation (lexical input, surface output).
In addition to inversion, FSTs are closed un-
der composition. A second important advance in
computational morphology was the recognition by
Karttunen et al (1992) that a cascade of composed
FSTs could implement the two-level model. This
made possible quite complex finite state systems,
including ordered alternation rules representing
context-sensitive variation in the phonological or
orthographic shape of morphemes, the morpho-
tactics characterizing the possible sequences of
morphemes (in canonical form) for a given word
class, and one or more sublexicons. For example,
to handle written English nouns, we could create a
cascade of FSTs covering the rules that insert an e
in words like bushes and parties and relate lexical
y to surface i in words like buggies and parties and
an FST that represents the possible sequences of
morphemes in English nouns, including all of the
noun stems in the English lexicon. The key fea-
ture of such systems is that, even though the FSTs
making up the cascade must be composed in a par-
ticular order, the result of composition is a single
FST relating surface and lexical levels directly, as
in two-level morphology.
1.2 FSTs for non-concatenative morphology
These ideas have revolutionized computational
morphology, making languages with complex
word structure, such as Finnish and Turkish, far
more amenable to analysis by traditional compu-
tational techniques. However, finite state mor-
phology is inherently biased to view morphemes
as sequences of characters or phones and words
as concatenations of morphemes. This presents
problems in the case of non-concatenative mor-
phology: discontinuous morphemes (circumfix-
309
ation); infixation, which breaks up a morpheme
by inserting another within it; reduplication, by
which part or all of some morpheme is copied;
and the template morphology (also called stem-
pattern morphology, intercalation, and interdigi-
tation) that characterizes Semitic languages, and
which is the focus of much of this paper. The stem
of a Semitic verb consists of a root, essentially
a sequence of consonants, and a pattern, a sort
of template which inserts other segments between
the root consonants and possibly copies certain of
them (see Tigrinya examples in the next section).
Researchers within the finite state framework
have proposed a number of ways to deal with
Semitic template morphology. One approach is to
make use of separate tapes for root and pattern at
the lexical level (Kiraz, 2000). A transition in such
a system relates a single surface character to mul-
tiple lexical characters, one for each of the distinct
sublexica.
Another approach is to have the transducers at
the lexical level relate an upper abstract charac-
terization of a stem to a lower string that directly
represents the merging of a particular root and pat-
tern. This lower string can then be compiled into
an FST that yields a surface expression (Beesley
and Karttunen, 2003). Given the extra compile-
and-replace operation, this resulting system maps
directly between abstract lexical expressions and
surface strings. In addition to Arabic, this ap-
proach has been applied to a portion of the verb
morphology system of the Ethio-Semitic language
Amharic (Amsalu and Demeke, 2006), which is
characterized by all of the same sorts of complex-
ity as Tigrinya.
A third approach makes use of a finite set of
registers that the FST can write to and read from
(Cohen-Sygal and Wintner, 2006). Because it can
remember relevant previous states, a ?finite-state
registered transducer? for template morphology
can keep the root and pattern separate as it pro-
cesses a stem.
This paper proposes an approach which is clos-
est to this last framework, one that starts with
familiar extension to FSTs, weights on the tran-
sitions. The next section gives an overview of
Tigrinya verb morphology. The following sec-
tion discusses weighted FSTs, in particular, with
weights consisting of feature structure descrip-
tions. Then I describe a system that applies this
approach to Tigrinya verb morphology.
2 Tigrinya Verb Morphology
Tigrinya is an Ethio-Semitic language spoken by
5-6 million people in northern Ethiopia and central
Eritrea. There has been almost no computational
work on the language, and there are effectively no
corpora or digitized dictionaries containing roots.
For a language with the morphological complexity
of Tigrinya, a crucial early step in computational
linguistic work must be the development of mor-
phological analyzers and generators.
2.1 The stem
A Tigrinya verb (Leslau, 1941 is a standard ref-
erence for Tigrinya grammar) consists of a stem
and one or more prefixes and suffixes. Most of
the complexity resides in the stem, which can be
described in terms of three dimensions: root (the
only strictly lexical component of the verb), tense-
aspect-mood (TAM), and derivational category.
Table 1 illustrates the possible combinations of
TAM and derivational category for a single root.1
A Tigrinya verb root consists of a sequence of
three, four, or five consonants. In addition, as
in other Ethio-Semitic languages, certain roots in-
clude inherent vowels and/or gemination (length-
ening) of particular consonants. Thus among the
three-consonant roots, there are three subclasses:
CCC, CaCC, CC C. As we have seen, the stem of
a Semitic verb can be viewed as the result of the in-
sertion of pattern vowels between root consonants
and the copying of root consonants in particular
positions. For Tigrinya, each combination of root
class, TAM, and derivational category is charac-
terized by a particular pattern.
With respect to TAM, there are four possibili-
ties, as shown in Table 1, conventionally referred
to in English as PERFECTIVE, IMPERFECTIVE,
JUSSIVE-IMPERATIVE, and GERUNDIVE. Word-
forms within these four TAM categories combine
with auxiliaries to yield the full range of possbil-
ities in the complex Tigrinya tense-aspect-mood
system. Since auxiliaries are written as separate
words or separated from the main verbs by an
apostrophe, they will not be discussed further.
Within each of the TAM categories, a Tigrinya
verb root can appear in up to eight different deriva-
1I use 1 for the high central vowel of Tigrinya, E for the
mid central vowel, q for the velar ejective, a dot under a char-
acter to represent other ejectives, a right quote to represent a
glottal stop, a left quote to represent the voiced pharyngeal
fricative, and to represent gemination. Other symbols are
conventional International Phonetic Alphabet.
310
simple pas/refl caus freqv recip1 caus-rec1 recip2 caus-rec2
perf fElEt
?
tEfEl(E)t
?
aflEt
?
fElalEt
?
tEfalEt
?
af alEt
?
tEfElalEt
?
af ElalEt
?imprf fEl( 1)t
?
f1l Et
?
af(1)l( )1t
?
fElalt
?
f alEt
?
af alt
?
f ElalEt
?
af Elalt
?jus/impv flEt
?
tEfElEt
?
afl1t
?
fElalt
?
tEfalEt
?
af alt
?
tEfElalEt
?
af Elalt
?ger fElit
?
tEfElit
?
aflit
?
fElalit
?
tEfalit
?
af alit
?
tEfElalit
?
af Elalit
?
Table 1: Stems based on the Tigrinya root ?flt
?
.
tional categories, which can can be characterized
in terms of four binary features, each with partic-
ular morphological consequences. These features
will be referred to in this paper as ?ps? (?passive?),
?tr? (?transitive?), ?it? (?iterative?), and ?rc? (?re-
ciprocal?). The eight possible combinations of
these features (see Table 1 for examples) are SIM-
PLE [-ps,-tr,-it,-rc], PASSIVE/REFLEXIVE [+ps,-
tr,-it,-rc], TRANSITIVE/CAUSATIVE: [-ps,+tr,-it,-
rc], FREQUENTATIVE [-ps,-tr,+it,-rc], RECIPRO-
CAL 1 [+ps,-tr,-it,+rc], CAUSATIVE RECIPROCAL
1 [-ps,+tr,-it,+rc], RECIPROCAL 2 [+ps,-tr,+it,-
rc], CAUSATIVE RECIPROCAL 2 [-ps,+tr,+it,-rc].
Notice that the [+ps,+it] and [+tr,+it] combina-
tions are roughly equivalent semantically to the
[+ps,+rc] and [+tr,+rc] combinations, though this
is not true for all verb roots.
2.2 Affixes
The affixes closest to the stem represent subject
agreement; there are ten combinations of person,
number, and gender in the Tigrinya pronominal
and verb-agreement system. For imperfective and
jussive verbs, as in the corresponding TAM cate-
gories in other Semitic languages, subject agree-
ment takes the form of prefixes and sometimes
also suffixes, for example, y1flEt
?
?that he know?,
y1flEt
?
u ?that they (mas.) know?. In the perfec-
tive, imperative, and gerundive, subject agreement
is expressed by suffixes alone, for example, fElEt
?
ki
?you (sg., fem.) knew?, fElEt
?
u ?they (mas.) knew!?.
Following the subject agreement suffix (if there
is one), a transitive Tigrinya verb may also include
an object suffix (or object agreement marker),
again in one of the same set of ten possible combi-
nations of person, number, and gender. There are
two sets of object suffixes, a plain set representing
direct objects and a prepositional set representing
various sorts of dative, benefactive, locative, and
instrumental complements, for example, y1fElt
?
En i
?he knows me?, y1fElt
?
El Ey ?he knows for me?.
Preceding the subject prefix of an imperfective
or jussive verb or the stem of a perfective, imper-
ative, or gerundive verb, there may be the prefix
indicating negative polarity, ay-. Non-finite neg-
ative verbs also require the suffix -n: y1fElt
?
En i ?he
knows me?; ay 1fElt
?
En 1n ?he doesn?t know me?.
Preceding the negative prefix (if there is one),
an imperfective or perfective verb may also in-
clude the prefix marking relativization, (z)1-, for
example, zifElt
?
En i ?(he) who knows me?. The rel-
ativizer can in turn be preceded by one of a set
of seven prepositions, for example, kabzifElt
?
En i
?from him who knows me?. Finally, in the per-
fective, imperfective, and gerundive, there is the
possibility of one or the other of several conjunc-
tive prefixes at the beginning of the verb (with-
out the relativizer), for example, kifElt
?
En i ?so
that he knows me? and one of several conjunc-
tive suffixes at the end of the verb, for example,
y1fElt
?
En 1n ?and he knows me?.
Given up to 32 possible stem templates (com-
binations of four tense-aspect-mood and eight
derivational categories) and the various possi-
ble combinations of agreement, polarity, rela-
tivization, preposition, and conjunction affixes, a
Tigrinya verb root can appear in well over 100,000
different wordforms.
2.3 Complexity
Tigrinya shares with other Semitic languages com-
plex variations in the stem patterns when the
root contains glottal or pharyngeal consonants or
semivowels. These and a range of other regu-
lar language-specific morphophonemic processes
can be captured in alternation rules. As in other
Semitic languages, reduplication also plays a role
in some of the stem patterns (as seen in Table 1).
Furthermore, the second consonant of the most
important conjugation class, as well as the con-
sonant of most of the object suffixes, geminates
in certain environments and not others (Buckley,
2000), a process that depends on syllable weight.
The morphotactics of the Tigrinya verb is re-
plete with dependencies which span the verb stem:
(1) the negative circumfix ay-n, (2) absence of the
311
negative suffix -n following a subordinating prefix,
(3) constraints on combinations of subject agree-
ment prefixes and suffixes in the imperfective and
jussive, (4) constraints on combinations of subject
agreement affixes and object suffixes.
There is also considerable ambiguity in the sys-
tem. For example, the second person and third per-
son feminine plural imperfective and jussive sub-
ject suffix is identical to one allomorph of the third
person feminine singular object suffix (y1fElt
?
a) ?he
knows her; they (fem.) know?). Tigrinya is written
in the Ge?ez (Ethiopic) syllabary, which fails to
mark gemination and to distinguish between syl-
lable final consonants and consonants followed by
the vowel 1. This introduces further ambiguity.
In sum, the complexity of Tigrinya verbs
presents a challenge to any computational mor-
phology framework. In the next section I consider
an augmentation to finite state morphology offer-
ing clear advantages for this language.
3 FSTs with Feature Structures
A weighted FST (Mohri et al, 2000) is a fi-
nite state transducer whose transitions are aug-
mented with weights. The weights must be ele-
ments of a semiring, an algebraic structure with
an ?addition? operation, a ?multiplication? opera-
tion, identity elements for each operation, and the
constraint that multiplication distributes over ad-
dition. Weights on a path of transitions through
a transducer are ?multiplied?, and the weights as-
sociated with alternate paths through a transducer
are ?added?. Weighted FSTs are closed under the
same operations as unweighted FSTs; in particu-
lar, they can be composed. Weighted FSTs are fa-
miliar in speech processing, where the semiring el-
ements usually represent probabilities, with ?mul-
tiplication? and ?addition? in their usual senses.
Amtrup (2003) recognized the advantages that
would accrue to morphological analyzers and gen-
erators if they could accommodate structured rep-
resentations. One familiar approach to repre-
senting linguistic structure is feature structures
(FSs) (Carpenter, 1992; Copestake, 2002). A
feature structure consists of a set of attribute-
value pairs, for which values are either atomic
properties, such as FALSE or FEMININE, or fea-
ture structures. For example, we might repre-
sent the morphological structure of the Tigrinya
noun gEzay ?my house? as [lex=gEza, num=sing,
poss=[pers=1, num=sg]]. The basic operation over
FSs is unification. Loosely speaking, two FSs
unify if their attribute-values pairs are compati-
ble; the resulting unification combines the features
of the FSs. For example, the two FSs [lex=gEza,
num=sg] and [poss=[pers=1, num=sg]] unify to
yield the FS [lex=gEza, num=sg, poss=[pers=1,
num=sg]]. The distinguished FS TOP unifies with
any other FS.
Amtrup shows that sets of FSs constitute a
semiring, with pairwise unification as the multi-
plication operator, set union as the addition opera-
tor, TOP as the identity element for multiplication,
and the empty set as the identity element for ad-
dition. Thus FSTs can be weighted with FSs. In
an FST with FS weights, traversing a path through
the network for a given input string yields an FS
set, in addition to the usual output string. The FS
set is the result of repeated unification of the FS
sets on the arcs in the path, starting with an initial
input FS set. A path through the network fails not
only if the current input character fails to match
the input character on the arc, but also if the cur-
rent accumulated FS set fails to unify with the FS
set on an arc.
Using examples from Persian, Amtrup demon-
strates two advantages of FSTs weighted with
FS sets. First, long-distance dependencies within
words present notorious problems for finite state
techniques. For generation, the usual approach
is to overgenerate and then filter out the illegal
strings below, but this may result in a much larger
network because of the duplication of state de-
scriptions. Using FSs, enforcing long-distance
constraints is straightforward. Weights on the rel-
evant transitions early in the word specify val-
ues for features that must agree with similar fea-
ture specifications on transitions later in the word
(see the Tigrinya examples in the next section).
Second, many NLP applications, such a machine
translation, work with the sort of structured rep-
resentations that are elegantly handled by FS de-
scriptions. Thus it is often desirable to have the
output of a morphological analyzer exhibit this
richness, in contrast to the string representations
that are the output of an unweighted finite state
analyzer.
4 Weighted FSTs for Tigrinya Verbs
4.1 Long-distance dependencies
As we have seen, Tigrinya verbs exhibit vari-
ous sorts of long-distance dependencies. The cir-
312
cumfix that marks the negative of non-subordinate
verbs, ay...n, is one example. Figure 1 shows
how this constraint can be handled naturally us-
ing an FST weighted with FS sets. In place of
the separate negative and affirmative subnetworks
that would have to span the entire FST in the abs-
cence of weighted arcs, we have simply the nega-
tive and affirmative branches at the beginning and
end of the weighted FST. In the analysis direction,
this FST will accept forms such as ay 1fElt
?
un ?they
don?t know? and y1fElt
?
u ?they know? and reject
forms such as ay 1fElt
?
u. In the generation direc-
tion, the FST will correctly generate a form such
as ay 1fElt
?
un given a initial FS that includes the
feature [pol=neg].
4.2 Stems: root and derivational pattern
Now consider the source of most of the complex-
ity of the Tigrinya verb, the stem. The stem may
be thought of as conveying three types of infor-
mation: lexical (the root of the verb), derivational,
and TAM. However, unlike the former two types,
the TAM category of the verb is redundantly coded
for by the combination of subject agreement af-
fixes. Thus, analysis of a stem should return at
least the root and the derivational category, and
generation should start with a root and a deriva-
tional category and return a stem. We can repre-
sent each root as a sequence of consonants, sep-
arated in some cases by the vowel a or the gem-
ination character ( ). Given a particular deriva-
tional pattern and a TAM category, extracting the
root from the stem is a straightforward matter with
an FST. For example, for the imperfective pas-
sive, the CC C root pattern appears in the template
C1C EC, and the root is what is left if the two vow-
els in the stem are skipped over.
However, we want to extract both the deriva-
tional pattern and the root, and the problem for
finite state methods, as discussed in Section 1.2,
is that both are spread throughout the stem. The
analyzer needs to alternate between recording ele-
ments of the root and clues about the derivational
pattern as it traverses the stem, and the generator
needs to alternate between outputting characters
that represent root elements and characters that
depend on the derivational pattern as it produces
the stem. The process is complicated further be-
cause some stem characters, such as the gemina-
tion character, may be either lexical (that is, a root
element) or derivational, and others may provide
information about both components. For exam-
ple, a stem with four consonants and a separating
the second and third consonants represents the fre-
quentative of a three-consonant root if the third
and fourth consonants are identical (e.g., fElalEt
??knew repeatedly?, root: flt
?
) and a four-consonant
root (CCaCC root pattern) in the simple deriva-
tional category if they are not (e.g., kElakEl ?pre-
vented?, root klakl).
As discussed in Section 1.2, one of the familiar
approaches to this problem, that of Beesley and
Karttunen (2003), precompiles all of the combina-
tions of roots and derivational patterns into stems.
The problem with this approach for Tigrinya is
that we do not have anything like a complete list
of roots; that is, we expect many stems to be novel
and will need to be able to analyze them on the fly.
The other two approaches discussed in 1.2, that of
Kiraz (2000) and that of Cohen-Sygal & Wintner
(2006), are closer to what is proposed here. Each
has an explicit mechanism for keeping the root and
pattern distinct: separate tapes in the case of Kiraz
(2000) and separate memory registers in the case
of Cohen-Sygal & Wintner (2006).
The present approach also divides the work of
processing the root and the derivational patterns
between two components of the system. However,
instead of the additional overhead required for im-
plementing a multi-tape system or registers, this
system makes use of the FSTs weighted with FSs
that are already motivated for other aspects of mor-
phology, as argued above. In this approach, the
lexical aspects of morphology are handled by the
ordinary input-output character correspondences,
and the grammatical aspects of morphology, in
particular the derivational patterns, are handled by
the FS weights on the FST arcs and the unifica-
tion that takes place as accumulated weights are
matched against the weights on FST arcs.
As explained in Section 2, we can represent
the eight possible derivational categories for a
Tigrinya verb stem in terms of four binary features
(ps, tr, rc, it). Each of these features is reflected
more or less directly in the stem form (though dif-
ferently for different root classes and for differ-
ent TAM categories). However, they are some-
times distributed across the stem: different parts
of a stem may be constrained by the presence of
a particular feature. For example, the feature +ps
(abbreviating [ps=True]) causes the gemination of
the stem-initial consonant under various circum-
313
02SBJ11
[pol=neg]
:
[pol=aff]
3 4STEM SBJ2
ay:
5
OBJ
:
6
n:
:
[pol=neg]
[pol=aff]
Figure 1: Handling Tigrinya (non-subordinate, imperfective) negation using feature structure weights.
Arcs with uppercase labels represents subnetworks that are not spelled out in the figure.
stances and also controls the final vowel in the
stem in the imperfective, and the feature +tr is
marked by the vowel a before the first root con-
sonant and, in the imperfective, by the nature of
the vowel that follows the first root consonant (E
where we would otherwise expect 1, 1 where we
would otherwise expect E.) That is, as with the
verb affixes, there are long-distance dependencies
within the verb stem.
Figure 2 illustrates this division of labor for the
portion of the stem FST that covers the CC C root
pattern for the imperfective. This FST (including
the subnetwork not shown that is responsible for
the reduplicated portion of the +it patterns) han-
dles all eight possible derivational categories. For
the root ?fs. m ?finish?, the stems are [-ps,-tr,-rc,-
it]: f1s
?
1m, [+ps,-tr,-rc,-it]: f1s
?
Em, [-ps,+tr,-rc,-it]:
afEs
?
1m, [-ps,-tr,-rc,+it]: fEs
?
as
?
1m, [+ps,-tr,+rc,-
it]: f as
?
Em, [-ps,+tr,+rc,-it]: af as
?
1m, [+ps,-tr,-
rc,+it]: f Es
?
as
?
Em, [-ps,+tr,-rc,+it]: af Es
?
as
?
1m.
What is notable is the relatively small number of
states that are required; among the consonant and
vowel positions in the stems, all but the first are
shared among the various derivational categories.
Of course the full stem FST, applying to all
combinations of the eight root classes, the eight
derivational categories, and the four TAM cate-
gories, is much larger, but the FS weights still
permit a good deal of sharing, including sharing
across the root classes and across the TAM cate-
gories.
4.3 Architecture
The full verb morphology processing system (see
Figure 3) consists of analysis and generation FSTs
for both orthographic and phonemically repre-
sented words, four FSTs in all. Eleven FSTs are
composed to yield the phonemic analysis FST (de-
noted by the dashed border in Figure 3), and two
additional FSTs are composed onto this FST to
yield the orthographic FST (denoted by the large
solid rectangle). The generation FSTs are created
by inverting the analysis FSTs. Only the ortho-
graphic FSTs are discussed in the remainder of
this paper.
At the most abstract (lexical) end is the heart of
the system, the morphotactic FST, and the heart of
this FST is the stem FST described above. The
stem FST is composed from six FSTs, including
three that handle the morphotactics of the stem,
one that handles root constraints, and two that han-
dle phonological processes that apply only to the
stem. A prefix FST and a suffix FST are then con-
catenated onto the composed stem FST to create
the full verb morphotactic FST. Within the whole
FST, it is only the morphotactic FSTs (the yellow
rectangles in Figure 3) that have FS weights.2
In the analysis direction, the morphotactic FST
takes as input words in an abstract canonical form
and an initial weight of TOP; that is, at this point
in analysis, no grammatical information has been
extracted. The output of the morphotactic FST
is either the empty list if the form is unanalyz-
able, or one or more analyses, each consisting
of a root string and a fully specified grammat-
ical description in the form of an FS. For ex-
ample, given the form ?ayt1f1l et
?
un, the morpho-
tactic FST would output the root flt. and the FS
[tam=imprf, der=[+ps,-tr,-rc,-it], sbj=[+2p,+plr,-
fem], +neg, obj=nil, -rel] (see Figure 3). That
is, this word represents the imperfective, nega-
tive, non-relativized passive of the verb root ?flt.
(?know?) with second person plural masculine sub-
ject and no object: ?you (plr., mas.) are not
known?. The system has no actual lexicon, so it
outputs all roots that are compatible with the in-
put, even if such roots do not exist in the language.
In the generation direction, the opposite happens.
In this case, the input root can be any legal se-
quence of characters that matches one of the eight
2The reduplication that characterizes [+it] stems and the
?anti-reduplication? that prevents sequences of identical root
consonants in some positions are handled with separate tran-
sitions for each consonant pair.
314
C1
C2_
V2 C3
C
a:
C1_
_:
?:
V1
?:
?:
a:
C
?:
?:
[+ps]
[-ps]
C
[-ps,+it]
[-ps,-it]
<CaC:C>
[+it]
[-tr]
C2
aC1
_:
?:
_
[+ps]
[+rc,-it]
[-rc,+it]
[+tr,-ps]
0
a
[-it]
C
Figure 2: FST for imperfective verb stems of root type CC C. <CaC:C> indicates a subnetwork, not
shown, which handles the reduplicated portion of +it stems, for example, fes
?
as
?
1m.
root patterns (there are some constraints on what
can constitute a root), though not necessarily an
actual root in the language.
The highest FST below the morphotactic FST
handles one case of allomorphy: the two allo-
morphs of the relativization prefix. Below this are
nine FSTs handling phonology; for example, one
of these converts the sequence a1 to E. At the bot-
tom end of the cascade are two orthographic FSTs
which are required when the input to analysis or
the output of generation is in standard Tigrinya or-
thography. One of these is responsible for the in-
sertion of the vowel 1 and for consonant gemina-
tion (neither of which is indicated in the orthogra-
phy); the other inserts a glottal stop before a word-
initial vowel.
The full orthographic FST consists of 22,313
states and 118,927 arcs. The system handles
verbs in all of the root classes discussed by
Leslau (1941), including those with laryngeals
and semivowels in different root positions and the
three common irregular verbs, and all grammati-
cal combinations of subject, object, negation, rel-
ativization, preposition, and conjunction affixes.
For the orthographic version of the analyzer, a
word is entered in Ge?ez script (UTF-8 encoding).
The program romanizes the input using the SERA
transcription conventions (Firdyiwek and Yaqob,
1997), which represent Ge?ez characters with the
ASCII character set, before handing it to the ortho-
graphic analysis FST. For each possible analysis,
the output consists of a (romanized) root and a FS
set. Where a set contains more than one FS, the
interpretation is that any of the FS elements con-
stitutes a possible analysis. Input to the generator
consists of a romanized root and a single feature
??????? 
fl?; [tam=+imprf, der=[+ps,-tr,-it,-rc],
      sbj=[+2p,+plr,-fem], +neg]]
Allomorphy
Phonology
Orthography
.
 
.
 
.
M
o
r
p
h
o
t
a
c
t
i
c
s
SuffixesPrefixes
...'ayt?f?l_??un...
S
t
e
m
 
(
R
o
o
t
+
P
a
t
t
e
r
n
)
.o.
.o.
.o.
.o.
.o.
.o.
.o.
.o.
.o.
.o.
.o.
Figure 3: Architecture of the system. Rectangles
represent FSTs, ?.o.?composition.
structure. The output of the orthographic gener-
ation FST is an orthographic representation, us-
ing SERA conventions, of each possible form that
is compatible with the input root and FS. These
forms are then converted to Ge?ez orthography.
The analyzer and generator are pub-
licly accessible on the Internet at
www.cs.indiana.edu/cgi-pub/gasser/L3/
morpho/Ti/v.
315
4.4 Evaluation
Systematic evaluation of the system is diffi-
cult since no Tigrinya corpora are currently
available. One resource that is useful, how-
ever, is the Tigrinya word list compiled by
Biniam Gebremichael, available on the Internet at
www.cs.ru.nl/ biniam/geez/crawl.php. Biniam ex-
tracted 227,984 distinct wordforms from Tigrinya
texts by crawling the Internet. As a first step to-
ward evaluating the morphological analyzer, the
orthographic analyzer was run on 400 word-
forms selected randomly from the list compiled by
Biniam, and the results were evaluated by a human
reader.
Of the 400 wordforms, 329 were unambigu-
ously verbs. The program correctly analyzed 308
of these. The 21 errors included irregular verbs
and orthographic/phonological variants that had
not been built into the FST; these will be straight-
forward to add. Fifty other words were not verbs.
The program again responded appropriately, given
its knowledge, either rejecting the word or analyz-
ing it as a verb based on a non-existent root. Thir-
teen other words appeared to be verb forms con-
taining a simple typographical error, and I was un-
able to identify the remaining eight words. For the
latter two categories, the program again responded
by rejecting the word or treating it as a verb based
on a non-existent root.
To test the morphological generator, the pro-
gram was run on roots belonging to all 21 of the
major classes discussed by Leslau (1941), includ-
ing those with glottal or pharyngeal consonants or
semivowels in different positions within the roots.
For each of these classes, the program was asked
to generate all possible derivational patterns (in the
third person singular masculine form). In addition,
for a smaller set of four root classes in the sim-
ple derivational pattern, the program was tested on
all relevant combinations of the subject and object
affixes3 and, for the imperfective and perfective,
on 13 combinations of the relativization, negation,
prepositional, and conjunctive affixes. For each
of the 272 tests, the generation FST succeeded in
outputting the correct form (and in some cases a
phonemic and/or orthographic alternative).
In conclusion, the orthographic morphological
analyzer and generator provide good coverage of
3With respect to their morphophonological behavior, the
subject affixes and object suffixes each group into four cate-
gories.
Tigrinya verbs. One weakness of the present sys-
tem results from its lack of a root dictionary. The
analyzer produces as many as 15 different analyses
of words, when in many cases only one contains a
root that exists in the language. The number could
be reduced somewhat by a more extensive filter
on possible root segment sequences; however, root
internal phonotactics is an area that has not been
extensively studied for Tigrinya. In any case, once
a Tigrinya root dictionary becomes available, it
will be straightforward to compose a lexical FST
onto the existing FSTs that will reject all but ac-
ceptable roots. Even a relatively small root dictio-
nary should also permit inferences about possible
root segment sequences in the language, enabling
the construction of a stricter filter for roots that are
not yet contained in the dictionary.
5 Conclusion
Progress in all applications for a language such as
Tigrinya is held back when verb morphology is
not dealt with adequately. Tigrinya morphology
is complex in two senses. First, like other Semitic
languages, it relies on template morphology, pre-
senting unusual challenges to any computational
framework. This paper presents a new answer
to these challenges, one which has the potential
to integrate morphological processing into other
knowledge-based applications through the inclu-
sion of the powerful and flexible feature structure
framework. This approach should extend to other
Semitic languages, such as Arabic, Hebrew, and
Amharic. Second, Tigrinya verbs are simply very
elaborate. In addition to the stems resulting from
the intercalation of eight root classes, eight deriva-
tional patterns and four TAM categories, there are
up to four prefix slots and four suffix slots; various
sorts of prefix-suffix dependencies; and a range
of interacting phonological processes, including
those sensitive to syllable structure, as well as
segmental context. Just putting together all of
these constraints in a way that works is signifi-
cant. Since the motivation for this project is pri-
marily practical rather than theoretical, the main
achievement of the paper is the demonstration that,
with some effort, a system can be built that actu-
ally handles Tigrinya verbs in great detail. Future
work will focus on fine-tuning the verb FST, de-
veloping an FST for nouns, and applying this same
approach to other Semitic languages.
316
References
Saba Amsalu and Girma A. Demeke. 2006. Non-
concatenative finite-state morphotactics of Amharic
simple verbs. ELRC Working Papers, 2(3).
Jan Amtrup. 2003. Morphology in machine translation
systems: Efficient integration of finite state trans-
ducers and feature structure descriptions. Machine
Translation, 18:213?235.
Kenneth R. Beesley and Lauri Karttunen. 2003. Fi-
nite State Morphology. CSLI Publications, Stan-
ford, CA, USA.
Eugene Buckley. 2000. Alignment and weight in the
Tigrinya verb stem. In Vicki Carstens and Frederick
Parkinson, editors, Advances in African Linguistics,
pages 165?176. Africa World Press, Lawrenceville,
NJ, USA.
Bob Carpenter. 1992. The Logic of Typed Fea-
ture Structures. Cambridge University Press, Cam-
bridge.
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Harper and Row, New York.
Yael Cohen-Sygal and Shuly Wintner. 2006. Finite-
state registered automata for non-concatenative mor-
phology. Computational Linguistics, 32:49?82.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stanford,
CA, USA.
Yitna Firdyiwek and Daniel Yaqob. 1997. The sys-
tem for Ethiopic representation in ascii. URL: cite-
seer.ist.psu.edu/56365.html.
C. Douglas Johnson. 1972. Formal Aspects of Phono-
logical Description. Mouton, The Hague.
Ronald M. Kaplan and Martin Kay. 1994. Regu-
lar models of phonological rule systems. Compu-
tational Linguistics, 20:331?378.
Lauri Karttunen, Ronald M. Kaplan, and Annie Zae-
nen. 1992. Two-level morphology with compo-
sition. In Proceedings of the International Con-
ference on Computational Linguistics, volume 14,
pages 141?148.
George A. Kiraz. 2000. Multitiered nonlinear mor-
phology using multitape finite automata: a case
study on Syriac and Arabic. Computational Linguis-
tics, 26(1):77?105.
Kimmo Koskenniemi. 1983. Two-level morphology: a
general computational model for word-form recog-
nition and production. Technical Report Publication
No. 11, Department of General Linguistics, Univer-
sity of Helsinki.
Wolf Leslau. 1941. Documents Tigrigna: Grammaire
et Textes. Libraire C. Klincksieck, Paris.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. Weighted finite-state transducers in speech
recognition. In Proceedings of ISCA ITRW on Auto-
matic Speech Recognition: Challenges for the Mil-
lenium, pages 97?106, Paris.
317
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 171?177, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
HLTDI: CL-WSD Using Markov Random Fields for SemEval-2013 Task 10
Alex Rudnick, Can Liu and Michael Gasser
Indiana University, School of Informatics and Computing
{alexr,liucan,gasser}@indiana.edu
Abstract
We present our entries for the SemEval-
2013 cross-language word-sense disambigua-
tion task (Lefever and Hoste, 2013). We
submitted three systems based on classifiers
trained on local context features, with some
elaborations. Our three systems, in increasing
order of complexity, were: maximum entropy
classifiers trained to predict the desired target-
language phrase using only monolingual fea-
tures (we called this system L1); similar clas-
sifiers, but with the desired target-language
phrase for the other four languages as features
(L2); and lastly, networks of five classifiers,
over which we do loopy belief propagation to
solve the classification tasks jointly (MRF).
1 Introduction
In the cross-language word-sense disambiguation
(CL-WSD) task, given an instance of an ambigu-
ous word used in a context, we want to predict the
appropriate translation into some target language.
This setting for WSD has an immediate application
in machine translation, since many words have mul-
tiple possible translations. Framing the resolution of
lexical ambiguities as an explicit classification task
has a long history, and was considered in early SMT
work at IBM (Brown et al, 1991). More recently,
Carpuat and Wu have shown how to use CL-WSD
techniques to improve modern phrase-based SMT
systems (Carpuat and Wu, 2007), even though the
language model and phrase-tables of these systems
mitigate the problem of lexical ambiguities some-
what.
In the SemEval-2013 CL-WSD shared task
(Lefever and Hoste, 2013), entrants are asked to
build a system that can provide translations for
twenty ambiguous English nouns, given appropri-
ate contexts ? here the particular usage of the am-
biguous noun is called the target word. The five tar-
get languages of the shared task are Spanish, Dutch,
German, Italian and French. In the evaluation, for
each of the twenty ambiguous nouns, systems are to
provide translations for the target word in each of
fifty sentences or short passages. The translations
of each English word may be single words or short
phrases in the target language, but in either case,
they should be lemmatized.
Following the work of Lefever and Hoste (2011),
we wanted to make use of multiple bitext corpora
for the CL-WSD task. ParaSense, the system of
Lefever and Hoste, takes into account evidence from
all of the available parallel corpora. Let S be the set
of five target languages and t be the particular target
language of interest at the moment; ParaSense cre-
ates bag-of-words features from the translations of
the target sentence into the languages S?{t}. Given
corpora that are parallel over many languages, this
is straightforward at training time. However, at test-
ing time it requires a complete MT system for each
of the four other languages, which is computation-
ally prohibitive. Thus in our work, we learn from
several parallel corpora but require neither a locally
running MT system nor access to an online transla-
tion API.
We presented three systems in this shared task,
all of which were variations on the theme of a max-
imum entropy classifier for each ambiguous noun,
trained on local context features similar to those
used in previous work and familiar from the WSD
literature. The first system, L1 (?layer one?), uses
maximum entropy classifiers trained on local con-
171
text features. The second system, L2 (?layer two?),
is the same as the L1 system, with the addition
of the correct translations into the other target lan-
guages as features, which at testing time are pre-
dicted with L1 classifiers. The third system, MRF
(?Markov random field?) uses a network of inter-
acting classifiers to solve the classification problem
for all five target languages jointly. Our three sys-
tems are all trained from the same data, which we
extracted from the Europarl Intersection corpus pro-
vided by the shared task organizers.
At the time of the evaluation, our simplest sys-
tem had the top results in the shared task for the
out-of-five evaluation for three languages (Spanish,
German, and Italian). However, after the evaluation
deadline, we fixed a simple bug in our MRF code,
and the MRF system then achieved even better re-
sults for the oof evaluation. For the best evaluation,
our two more sophisticated systems posted better re-
sults than the L1 version. All of our systems beat the
?most-frequent sense? baseline in every case.
In the following sections, we will describe our
three systems1, our training data extraction process,
the results on the shared task, and conclusions and
future work.
2 L1
The ?layer one? classifier, L1, is a maximum en-
tropy classifier that uses only monolingual features
from English. Although this shared task is described
as unsupervised, the L1 classifiers are trained with
supervised learning on instances that we extract pro-
grammatically from the Europarl Intersection cor-
pus; we describe the preprocessing and training data
extraction in Section 5.
Having extracted the relevant training sentences
from the aligned bitext for each of the five lan-
guage pairs, we created training instances with local
context features commonly used in WSD systems.
These are described in Figure 1. Each instance is
assigned the lemma of the translation that was ex-
tracted from the training sentence as its label.
We trained one L1 classifier for each target lan-
guage and each word of interest, resulting in 20?5 =
1Source is available at
http://github.iu.edu/alexr/semeval2013
? target word features
? literal word form
? POS tag
? lemma
? window unigram features (within 3 words)
? word form
? POS tag
? word with POS tag
? word lemma
? window bigram features (within 5 words)
? bigrams
? bigrams with POS tags
Figure 1: Features used in our classifiers
100 classifiers. Classifiers were trained with the
MEGA Model optimization package 2 and its corre-
sponding NLTK interface (Bird et al, 2009). Upon
training, we cache these classifiers with Python
pickles, both to speed up L1 experiments and also
because they are used as components of the other
models.
We combined the word tokens with their tags
in some features so that the classifier would not
treat them independently, since maximum entropy
classifiers learn a single weight for each feature.
Particularly, the ?POS tag? feature is distinct from
the ?word with tag? feature; for the tagged word
?house/NN?, the ?POS tag? feature would be NN ,
and the ?word with tag? feature is house NN .
3 L2
The ?layer two? classifier, L2, is an extension to
the L1 approach, with the addition of multilingual
features. Particularly, L2 makes use of the trans-
lations of the target word into the four target lan-
guages other than the one we are currently trying to
predict. At training time, since we have the transla-
tions of each of the English sentences into the other
target languages, the appropriate features are ex-
tracted from the corresponding sentences in those
languages. This is the same as the process by which
labels are given to training instances, described in
Section 5. At testing time, since translations of the
2http://www.umiacs.umd.edu/?hal/megam/
172
es de
nl
fr it
Figure 2: The network structure used in the MRF
system: a complete graph with five nodes where
each node represents a variable for the translation
into a target language
test sentences are not given, we estimate the transla-
tions for the target word in the four other languages
using the cached L1 classifiers.
Lefever and Hoste (2011) used the Google Trans-
late API to translate the source English sentences
into the four other languages, and extracted bag-of-
words features from these complete sentences. The
L2 classifiers make use of a similar intuition, but
they do not rely on a complete MT system or an
available online MT API; we only include the trans-
lations of the specific target word as features.
4 MRF
Our MRF model builds a Markov network (often
called a ?Markov random field?) of L1 classifiers
in an effort to find the best translation into all five
target languages jointly. This network has nodes
that correspond to the distributions produced by the
L1 classifiers, given an input sentence, and edges
with pairwise potentials that are derived from the
joint probabilities of target-language labels occur-
ring together in the training data. Thus the task of
finding the optimal translations into five languages
jointly is framed as a MAP (Maximum A Posteriori)
inference problem, where we try to maximize the
joint probability P (wfr, wes, wit, wde, wnl), given
the evidence of the features extracted from the
source-language sentence. The inference process is
performed using loopy belief propagation (Murphy
et al, 1999), which is an approximate but tractable
inference algorithm that, while it gives no guaran-
tees, often produces good solutions in practice.
The intuition behind using a Markov network for
this task is that, since we must make five decisions
for each source-language sentence, we should make
use of the correlations between the target-language
words. Correlations might occur in practice due to
cognates ? the languages in the shared task are fairly
closely related ? or they may simply reflect ambigu-
ities in the source language that are resolved in two
target languages.
So by building a Markov network in which all of
the classifiers can communicate (see Figure 2), we
allow nodes to influence the translation decisions of
their neighbors, but only proportionally to the cor-
relation between the translations that we observe in
the two languages.
We frame the MAP inference task as a minimiza-
tion problem; we want to find an assignment that
minimizes the sum of all of our penalty functions,
which we will describe next. First, we have a unary
function from each of the five L1 classifiers, which
correspond to nodes in the network. These func-
tions each assign a penalty to each possible label for
the target word in the corresponding language; that
penalty is simply the negative log of the probability
of the label, as estimated by the classifier.
Formally, a unary potential ?i, for some fixed set
of features f and a particular language i, is a func-
tion from a label l to some positive penalty value.
?i(l) = ?logP (Li = l|F = f)
Secondly, for each unordered pair of classifiers
(i, j) (i.e., each edge in the graph) there is a pairwise
potential function ?(i,j) that assigns a penalty to any
assignment of that pair of variables.
?(i,j)(li, lj) = ?logP (Li = li, Lj = lj)
Here by P (Li = li, Lj = lj), we mean the prob-
ability that, for a fixed ambiguous input word, lan-
guage i takes the label li and language j takes the
label lj . These joint probabilities are estimated from
the training data; we count the number of times
each pair of labels li and lj co-occurs in the train-
173
ing sentences and divide, with smoothing to avoid
zero probabilities and thus infinite penalties.
When it comes time to choose translations, we
want to find a complete assignment to the five vari-
ables that minimizes the sum of all of the penal-
ties assigned by the ? functions. As mentioned ear-
lier, we do this via loopy belief propagation, using
the formulation for pairwise Markov networks that
passes messages directly between the nodes rather
than first constructing a cluster graph (Koller and
Friedman, 2009, ?11.3.5.1).
As we are trying to compute the minimum-
penalty assignment to the five variables, we use the
min-sum version of loopy belief propagation. The
messages are mappings from the possible values
that the recipient node could take to penalty values.
At each time step, every node passes to each of
its neighbors a message of the following form:
?ti?j(Lj) = min
li?Li
[
?i(li) + ?(i,j)(li, lj)
+
?
k?S?{i,j}
?t?1k?i(li)
]
By this expression, we mean that the message
from node i to node j at time t is a function from
possible labels for node j to scalar penalty values.
Each penalty value is determined by minimizing
over the possible labels for node i, such that we find
the label li that minimizes sum of the unary cost for
that label, the binary cost for li and lj taken jointly,
and all of the penalties in the messages that node i
received at the previous time step, except for the one
from node j.
Intuitively, these messages inform a given neigh-
bor about the estimate, from the perspective of the
sending node and what it has heard from its other
neighbors, of the minimum penalty that would be
incurred if the recipient node were to take a given
label. As a concrete example, when the nl node
sends a message to the fr node at time step 10, this
message is a table mapping from all possible French
translations of the current target word to their as-
sociated penalty values. The message depends on
three things: the function ?nl (itself dependent on
the probability distribution output by the L1 classi-
fier), the binary potential function ?(nl,fr), and the
messages from es, it and de from time step 9. Note
that the binary potential functions are symmetric be-
cause they are derived from joint probabilities.
Loopy belief propagation is an approximate infer-
ence algorithm, and it is neither guaranteed to find
a globally optimal solution, nor even to converge
at all, but it does often find good solutions in prac-
tice. We run it for twenty iterations, which empir-
ically works well. After the message-passing iter-
ations, each node chooses the value that minimizes
the sum of the penalties from messages and from its
own unary potential function. To avoid accumulat-
ing very large penalties, we normalize the outgoing
messages at each time step and give a larger weight
to the unary potential functions. These normaliza-
tion and weighting parameters were set by hand, but
seem to work well in practice.
5 Training Data Extraction
For simplicity and comparability with previous
work, we worked with the Europarl Intersection
corpus provided by the task organizers. Europarl
(Koehn, 2005) is a parallel corpus of proceedings of
the European Parliament, currently available in 21
European languages, although not every sentence is
translated into every language. The Europarl Inter-
section is the intersection of the sentences from Eu-
roparl that are available in English and all five of the
target languages for the task.
In order to produce the training data for the classi-
fiers, we first tokenized the text for all six languages
with the default NLTK tokenizer and tagged the En-
glish text with the Stanford Tagger (Toutanova et
al., 2003). We aligned the untagged English with
each of the target languages using the Berkeley
Aligner (DeNero and Klein, 2007) to get one-to-
many alignments from English to target-language
words, since the target-language labels may be
multi-word phrases. We used nearly the default set-
tings for Berkeley Aligner, except that we ran 20
iterations each of IBM Model 1 and HMM align-
ment.
We used TreeTagger (Schmid, 1995) to lemma-
tize the text. At first this caused some confusion in
our pipeline, as TreeTagger by default re-tokenizes
input text and tries to recognize multi-word expres-
174
sions. Both of these, while sensible behaviors, were
unexpected, and resulted in a surprising number of
tokens in the TreeTagger output. Once we turned off
these behaviors, TreeTagger provided useful lem-
mas for all of the languages.
Given the tokenized and aligned sentences, with
their part-of-speech tags and lemmas, we used
a number of heuristics to extract the appropriate
target-language labels for each English-language in-
put sentence. For each target word, we extracted a
sense inventory Vi from the gold standard answers
from the 2010 iteration of this task (Lefever and
Hoste, 2009). Then, for each English sentence that
contains one of the target words used as a noun,
we examine the alignments to determine whether
that word is aligned with a sense present in Vi , or
whether the words aligned to that noun are a sub-
sequence of such a sense. The same check is per-
formed both on the lemmatized and unlemmatized
versions of the target-language sentence. If we do
find a match, then that sense from the gold stan-
dard Vi is taken to be the label for this sentence.
While a gold standard sense inventory will clearly
not be present for general translation systems, there
will be some vocabulary of possible translations for
each word, taken from a bilingual dictionary or the
phrase table in a phrase-based SMT system.
If a label from Vi is not found with the align-
ments, but some other word or phrase is aligned
with the ambiguous noun, then we trust the output
of the aligner, and the lemmatized version of this
target-language phrase is assigned as the label for
this sentence. In this case we used some heuristic
functions to remove stray punctuation and attached
articles (such as d? from French or nell? from Ital-
ian) that were often left appended to the tokens by
the default NLTK English tokenizer.
We dropped all of the training instances with
labels that only occurred once, considering them
likely alignment errors or other noise.
6 Results
There were two settings for the evaluation, best and
oof. In either case, systems may present multiple
possible answers for a given translation, although
in the best setting, the first answer is given more
weight in the evaluation, and the scoring encour-
ages only returning the top answer. In the oof set-
ting, systems are asked to return the top-five most
likely translations. In both settings, the answers are
compared against translations provided by several
human annotators for each test sentence, who pro-
vided a number of possible target-language transla-
tions in lemmatized form, and more points are given
for matching the more popular translations given by
the annotators. In the ?mode? variant of scoring,
only the one most common answer for a given test
sentence is considered valid. For a complete ex-
planation of the evaluation and its scoring, please
see the shared task description (Lefever and Hoste,
2013).
The scores for our systems3 are reported in Figure
3. In all of the settings, our systems posted some of
the top results among entrants in the shared task,
achieving the best scores for some evaluations and
some languages. For every setting and language,
our systems beat the most-frequent sense baseline,
and our best results usually came from either the L2
or MRF system, which suggests that there is some
benefit in using multilingual information from the
parallel corpora, even without translating the whole
source sentence.
For the best evaluation, considering only the
mode gold-standard answers, our L2 system
achieved the highest scores in the competition for
Spanish and German. For the oof evaluation, our
MRF system ? with its post-competition bug fix ?
posted the best results for Spanish, German and Ital-
ian in both complete and mode variants. Also, cu-
riously, our L1 system posted the best results in the
competition for Dutch in the oof variant.
For the best evaluation, our results were lower
than those posted by ParaSense, and in the stan-
dard best setting, they were also lower than those
from the c1lN system (van Gompel and van den
Bosch, 2013) and adapt1 (Carpuat, 2013). This,
combined with the relatively small difference be-
tween our simplest system and the more sophisti-
cated ones, suggests that there are many improve-
ments that could be made to our system; perhaps
3The oof scores for the MRF system reflect a small bug fix
after the competition.
175
system es nl de it fr
MFS 23.23 20.66 17.43 20.21 25.74
best 32.16 23.61 20.82 25.66 30.11
PS 31.72 25.29 24.54 28.15 31.21
L1 29.01 21.53 19.5 24.52 27.01
L2 28.49 22.36 19.92 23.94 28.23
MRF 29.36 21.61 19.76 24.62 27.46
(a) best evaluation results: precision
system es nl de it fr
MFS 53.07 43.59 38.86 42.63 51.36
best 62.21 47.83 44.02 53.98 59.80
L1 61.69 46.55 43.66 53.57 57.76
L2 59.51 46.36 42.32 53.05 58.20
MRF 62.21 46.63 44.02 53.98 57.83
(b) oof evaluation results: precision
system es nl de it fr
MFS 27.48 24.15 15.30 19.88 20.19
best 37.11 27.96 24.74 31.61 26.62
PS 40.26 30.29 25.48 30.11 26.33
L1 36.32 25.39 24.16 26.52 21.24
L2 37.11 25.34 24.74 26.65 21.07
MRF 36.57 25.72 24.01 26.26 21.24
(c) best evaluation results: mode precision
system es nl de it fr
MFS 57.35 41.97 44.35 41.69 47.42
best 65.10 47.34 53.75 57.50 57.57
L1 64.65 47.34 53.50 56.61 51.96
L2 62.52 44.06 49.03 54.06 53.57
MRF 65.10 47.29 53.75 57.50 52.14
(d) oof evaluation results: mode precision
Figure 3: Task results for our systems. Scores in bold are the best result for that language and evaluation
out of our systems, and those in bold italics are the best posted in the competition. For comparison, we
also give scores for the most-frequent-sense baseline (?MFS?), ParaSense (?PS?), the system developed by
Lefever and Hoste, and the best posted score for competing systems this year (?best?).
we could integrate ideas from the other entries in
the shared task this year.
7 Conclusions and future work
Our systems had a strong showing in the compe-
tition, always beating the MFS baseline, achiev-
ing the top score for three of the five languages in
the oof evaluation, and for two languages in the
best evaluation when considering the mode gold-
standard answers. The systems that took into ac-
count evidence from multiple sources had better
performance than the one using monolingual fea-
tures: our top result in every language came from
either the L2 or the MRF classifier for both eval-
uations. This suggests that it is possible to make
use of the evidence in several parallel corpora in a
CL-WSD task without translating every word in a
source sentence into many target languages.
We expect that the L2 classifier could be im-
proved by adding features derived from more classi-
fiers and making use of information from many dis-
parate sources. We would like to try adding classi-
fiers trained on the other Europarl languages, as well
as completely different corpora. The L2 classifier
approach only requires that the first-layer classifiers
make some prediction based on text in the source
language. They need not be trained from the same
source text, depend on the same features, or even
output words as labels. In future work we will ex-
plore all of these variations. One could, for exam-
ple, train a monolingual WSD system on a sense-
tagged corpus and use this as an additional informa-
tion source for an L2 classifier.
There remain a number of avenues that we would
like to explore for the MRF system; thus far, we
have used the joint probability of two labels to set
the binary potentials. We would like to investigate
other functions, especially ones that do not incur
large penalties for rare labels, as the joint probabil-
ity of two labels that often co-occur but are both rare
will be low. Also, in the current system, the relative
weights of the binary potentials and the unary po-
tentials were set by hand, with a very small amount
of empirical tuning. We could, in the future, tune the
176
weights with a more principled optimization strat-
egy, using a development set.
As with the L2 classifiers, it would be helpful in
the future for the MRF system to not require many
mutually parallel corpora for training ? however, the
current approach for estimating the edge potentials
requires the use of bitext for each edge in the net-
work. Perhaps these correlations could be estimated
in a semi-supervised way, with high-confidence au-
tomatic labels being used to estimate the joint dis-
tribution over target-language phrases. We would
also like to investigate approaches to jointly disam-
biguate many words in the same sentence, since lex-
ical ambiguity is not just a problem for a few nouns.
Aside from improvements to the design of our
CL-WSD system itself, we want to use it in a practi-
cal system for translating into under-resourced lan-
guages. We are now working on integrating this
project with our rule-based MT system, L3 (Gasser,
2012). We had experimented with a similar, though
less sophisticated, CL-WSD system for Quechua
(Rudnick, 2011), but in the future, L3 with the inte-
grated CL-WSD system should be capable of trans-
lating Spanish to Guarani, either as a standalone
system, or as part of a computer-assisted translation
tool.
References
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural Language Processing with Python. O?Reilly Me-
dia.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1991. Word-Sense Dis-
ambiguation Using Statistical Methods. In Proceed-
ings of the 29th Annual Meeting of the Association for
Computational Linguistics, pages 264?270.
Marine Carpuat and Dekai Wu. 2007. How Phrase
Sense Disambiguation Outperforms Word Sense Dis-
ambiguation for Statistical Machine Translation. In
11th Conference on Theoretical and Methodological
Issues in Machine Translation.
Marine Carpuat. 2013. NRC: A Machine Translation
Approach to Cross-Lingual Word Sense Disambigua-
tion (SemEval-2013 Task 10). In Proceedings of the
7th International Workshop on Semantic Evaluation
(SemEval 2013), Atlanta, USA.
John DeNero and Dan Klein. 2007. Tailoring Word
Alignments to Syntactic Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17?24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Michael Gasser. 2012. Toward a Rule-Based Sys-
tem for English-Amharic Translation. In LREC-2012:
SALTMIL-AfLaT Workshop on Language technology
for normalisation of less-resourced languages.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
The Tenth Machine Translation Summit, Phuket, Thai-
land.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
Els Lefever and Ve?ronique Hoste. 2009. SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 82?87, Boulder, Colorado, June.
Association for Computational Linguistics.
Els Lefever and Ve?ronique Hoste. 2013. SemEval-2013
Task 10: Cross-Lingual Word Sense Disambiguation.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), Atlanta, USA.
Els Lefever, Ve?ronique Hoste, and Martine De Cock.
2011. ParaSense or How to Use Parallel Corpora for
Word Sense Disambiguation. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 317?322, Portland, Oregon, USA, June. Asso-
ciation for Computational Linguistics.
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.
1999. Loopy Belief Propagation for Approximate In-
ference: An Empirical Study. In UAI ?99: Proceed-
ings of the Fifteenth Conference on Uncertainty in Ar-
tificial Intelligence, Stockholm, Sweden.
Alex Rudnick. 2011. Towards Cross-Language Word
Sense Disambiguation for Quechua. In Proceedings
of the Second Student Research Workshop associated
with RANLP 2011, pages 133?138, Hissar, Bulgaria,
September. RANLP 2011 Organising Committee.
Helmut Schmid. 1995. Improvements In Part-of-Speech
Tagging With an Application To German. In Proceed-
ings of the ACL SIGDAT-Workshop, pages 47?50.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In PROCEEDINGS OF HLT-NAACL, pages 252?259.
Maarten van Gompel and Antal van den Bosch. 2013.
WSD2: Parameter optimisation for Memory-based
Cross-Lingual Word-Sense Disambiguation. In Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013), Atlanta, USA.
177
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 102?108,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Lexical Selection for Hybrid MT with Sequence Labeling
Alex Rudnick and Michael Gasser
Indiana University, School of Informatics and Computing
{alexr,gasser}@indiana.edu
Abstract
We present initial work on an inex-
pensive approach for building large-
vocabulary lexical selection modules for
hybrid RBMT systems by framing lexi-
cal selection as a sequence labeling prob-
lem. We submit that Maximum Entropy
Markov Models (MEMMs) are a sensible
formalism for this problem, due to their
ability to take into account many features
of the source text, and show how we can
build a combination MEMM/HMM sys-
tem that allows MT system implemen-
tors flexibility regarding which words have
their lexical choices modeled with classi-
fiers. We present initial results showing
successful use of this system both in trans-
lating English to Spanish and Spanish to
Guarani.
1 Introduction
Lexical ambiguity presents a serious challenge for
rule-based machine translation (RBMT) systems,
since many words have several possible transla-
tions in a given target language, and more than
one of them may be syntactically valid in context.
A translation system must choose a translation for
each word or phrase in the input sentence, and
simply taking the most common translation will
often fail, as a word in the source language may
have translations in the target language with sig-
nificantly different meanings. Even when choos-
ing among near-synonyms, we would like to re-
spect selectional preferences and common collo-
cations to produce natural-sounding output text.
Writing lexical selection rules by hand is te-
dious and error-prone; even if informants familiar
with both languages are available, they may not be
able to enumerate the contexts under which they
would choose one translation alternative over an-
other. Thus we would like to learn from corpora
where possible.
Framing the resolution of lexical ambiguities
in machine translation as an explicit classification
task has a long history, dating back at least to early
SMT work at IBM (Brown et al, 1991). More re-
cently, Carpuat and Wu have shown how to use
word-sense disambiguation techniques to improve
modern phrase-based SMT systems (Carpuat and
Wu, 2007), even though the language model and
phrase tables of these systems can mitigate the
problem of lexical ambiguities somewhat. Treat-
ing lexical selection as a word-sense disambigua-
tion problem, in which the sense inventory for
each source-language word is its set of possible
translations, is often called cross-lingual WSD
(CL-WSD). This framing has received enough at-
tention to warrant shared tasks at recent SemEval
workshops; the most recent running of the task is
described in (Lefever and Hoste, 2013).
Intuitively, machine translation implies an ?all-
words? WSD task: we need to choose a transla-
tion for every word or phrase in the source sen-
tence, and the sequence of translations should
make sense taken together. Here we begin to ex-
plore CL-WSD not just as a classification task, but
as one of sequence labeling. We describe our ap-
proach and implementation, and present two ex-
periments. In the first experiment, we apply the
system to the SemEval 2013 shared task on CL-
WSD (Lefever and Hoste, 2013), translating from
English to Spanish, and in the second, we perform
an all-words labeling task, translating text from
the Bible from Spanish to Guarani. This is work
in progress and our code is currently ?research-
quality?, but we are developing the software in
the open1, with the intention of using it with free
RBMT systems and producing an easily reusable
package as the system matures.
1http://github.com/alexrudnick/clwsd
102
2 Related Work
To our knowledge, there has not been work specifi-
cally on sequence labeling applied to lexical selec-
tion for RBMT systems. However, there has been
work recently on using WSD techniques for trans-
lation into lower-resourced languages, such as the
English-Slovene language pair, as in (Vintar et al,
2012).
The Apertium team has a particular practical
interest in improving lexical selection in RBMT;
they recently have been developing a new sys-
tem, described in (Tyers et al, 2012), that learns
finite-state transducers for lexical selection from
the available parallel corpora. It is intended to be
both very fast, for use in practical translation sys-
tems, and to produce lexical selection rules that
are understandable and modifiable by humans.
Outside of the CL-WSD setting, there has been
work on framing all-words WSD as a sequence la-
beling problem. Particularly, Molina et al (2002)
have made use of HMMs for all-words WSD in a
monolingual setting.
3 Sequence Labeling with HMMs
In building a sequence-based CL-WSD system,
we first tried using the familiar HMM formalism.
An HMM is a generative model, giving us a for-
mula for P (S, T ) = P (T ) ? P (S|T ). Here by
S we mean a sequence of source-language words,
and by T we mean a sequence words or phrases in
the target language. In practice, the input sequence
S is a given, and we want to find the sequence T
that maximizes the joint probability, which means
predicting an appropriate label for each word in
the input sequence.
Using the (first-order) Markov assumption, we
approximate P (T ) as P (T ) =
?
iP (ti |ti?1 ),
where i denotes each index in the input sentence.
Then we imagine that each source-language word
si is generated by the corresponding unobserved
label ti , through the emission probabilities P (s|t).
This generative model is admittedly less intu-
itive for CL-WSD than for POS-tagging (where it
is more traditionally applied), in that it requires
the target-language words to be generated in the
source order.
Training the transition model ? roughly an n-
gram language model ? for target-language words
or phrases in the source order is straightforward
with sentence-aligned bitext. We use one-to-
many alignments in which each source word cor-
responds with zero or more target-language words,
and we take the sequence of target-language words
aligned with a given source word to be its label.
NULL labels are common; if a source word is not
aligned to a target word, it gets a NULL label.
Similarly , we can learn the emission probabilities,
P (s|t), simply by counting which source words
are paired with which target words and smoothing.
For decoding with this model, we can use
the Viterbi algorithm, especially for a first-order
Markov model ? although we must be careful
in the inner loops only to consider the possible
target-language words and not the entire target-
language vocabulary. The Viterbi algorithm may
still be used with second- or higher-order models,
although it slows down considerably. In the inter-
est of speed, in this work we performed decoding
for second-order HMMs with a beam search.
4 Sequence Labeling With MEMMs and
HMMs
Contrastingly, an MEMM is a discriminative se-
quence model, with which we can calculate the
conditional probability P (T |S) using individual
discriminative classifiers that model P (ti |F ) (for
some features F ). Like an HMM, an MEMM
models transitions over labels, although the in-
put sequence is considered given. This frees
us to include any features we like from the
source-language sentence. The ?Markov? aspect
of the MEMM is that, unlike a standard maxi-
mum entropy classifier, we can include informa-
tion from the previous k labels as features, for
a k-th order MEMM. So at every step in the se-
quence labeling, we want a classifier that models
P (ti |S, ti?1 ...ti?k ), and the probability of a se-
quence T is just the product of each of the individ-
ual transition probabilities.
To avoid the intractable task of building a single
classifier that might return thousands of different
labels, we could in principle build a classifier for
each individual word in the source-language vo-
cabulary, each of which will produce perhaps tens
of possible target-language labels. However, there
will be tens or hundreds of thousands of words in
the source-language vocabulary, and most word-
types will only occur very rarely; it may be pro-
hibitively expensive to train and store classifiers
for each of them.
We would like a way to focus our efforts
on some words, but not all, and to back off
103
to a simpler model when a classifier is not
available for a given word. Here, in order
to approximate P (ti |S, ti?1 ...ti?k ), we use an
HMM, as described in the previous section, with
which we can estimate P (si , ti |ti?1 ...ti?k ) as
P (ti |ti?1 ...ti?k ) ? P (si |ti). This gives us the
joint probability, which we divide by P (si) ? prior
probabilities of each source-language word must
be stored ahead of time ? and thus we can approx-
imate the conditional probability that we need to
continue the sequence labeling.
In the implementation, we can specify criteria
under which a source-language word will have its
translations explicitly modeled with a maximum
entropy classifier. When training a system, one
might choose, for example, the 100 most com-
mon ambiguous words, all words that are observed
a certain number of times in the training corpus,
or words that are particularly of interest for some
other reason.
At training time, we find all of the instances
of the words that we want to model with clas-
sifiers, along with their contexts, so that we can
extract appropriate features for training the clas-
sifiers. Then we train classifiers for those words,
and store the classifiers in a database for retrieval
at inference time.
For inference with this model, we use a beam
search rather than the Viterbi algorithm, for con-
venience and speed while using a second-order
Markov model. A sketch of the beam search im-
plementation is presented in Figure 1.
5 Experiments
So far, we have evaluated our sequence-labeling
system in two different settings, the English-
Spanish subset of a recent SemEval shared task
(Lefever and Hoste, 2013), and an all-words pre-
diction task in which we want to translate, from
Spanish to Guarani, each word in a test set sam-
pled from the Bible.
5.1 SemEval CL-WSD task
In the SemEval CL-WSD task, systems must pro-
vide translations for twenty ambiguous English
nouns given a small amount of context, typically a
single sentence. The test set for this task consists
of fifty short passages for each ambiguous word,
for a thousand test instances in total. Each pas-
sage contains one or a few uses of the ambiguous
word. For each test passage, the system must pro-
duce a translation of the noun of interest into the
target language. These translations may be a sin-
gle word or a short phrase in the target language,
and they should be lemmatized. The task allows
systems to produce several output labels, although
the scoring metric encourages producing one best
guess, which is matched against several reference
translations provided by human annotators. The
details of the scoring are provided in the task de-
scription paper, and the scores reported were cal-
culated with a script provided by the task organiz-
ers.
As a concrete example, consider the following
sentences from the test set:
(1) But a quick look at today?s letters to the
editor in the Times suggest that here at
least is one department of the paper that
could use a little more fact-checking.
(2) All over the ice were little Cohens, little
Levys, their names sewed in block letters
on the backs of their jerseys.
A system should produce carta (a message or
document) for Sentence (1) and letra or cara?cter
(a symbol or handwriting) for (2). During se-
quence labeling, our system chooses a translation
for each word in the sentence, but the scoring only
takes into account the translations for the words
marked in italics.
For simplicity and comparability with previous
work, we trained our system on the Europarl In-
tersection corpus, which was provided for devel-
oping CL-WSD systems in the shared task. The
Europarl Intersection is a subset of the sentences
from Europarl (Koehn, 2005) that are available in
English and all five of the target languages for the
task, although for these initial experiments, we
only worked with Spanish. There were 884603
sentences in our training corpus.
We preprocess the Europarl training data by to-
kenizing with the default NLTK tokenizer (Bird
et al, 2009), getting part-of-speech tags for the
English text with the Stanford Tagger (Toutanova
et al, 2003), and lemmatizing both sides with
TreeTagger (Schmid, 1995). We aligned the un-
tagged English text with the Spanish text using the
Berkeley Aligner (DeNero and Klein, 2007) to get
one-to-many alignments from English to Spanish,
since the target-language labels in this setting may
be multi-word phrases. We used nearly the de-
fault settings for Berkeley Aligner, except that we
104
def beam search(sequence, HMM, source word priors, classifiers):
???Search over possible label sequences, return the best one we find.???
candidates = [Candidate([], 0)] # empty label sequence with 0 penalty
for t in range(len(sequence)):
sourceword = sequence[t]
for candidate in candidates:
context = candidate.get context(t) # labels at positions (t?2, t?1)
if sourceword in classifiers:
features = extract features(sequence, t, context)
label distribution = classifiers[sourceword].prob classify(features)
else:
label distribution = Distribution()
for label in get vocabulary(sourceword):
label distribution[label] = (HMM.transition(context, label) +
HMM.emission(sourceword, label) ?
source word priors[sourceword])
# extend candidates for next time step to include labels for next word
add new candidates(candidate, label distribution, new candidates)
candidates = filter top k(new candidates, BEAMWIDTH)
return get best(candidates)
Figure 1: Python-style code sketch for MEMM/HMM beam search. Here we are using negative log-
probabilities, which we interpret as penalties to be minimized.
ran 20 iterations each of IBM Model 1 and HMM
alignment.
We trained classifiers for all of the test words,
and also for any words that appear more than 500
times in the corpus. The classifiers used the pre-
vious two labels and all of the tagged, lemmatized
words within three tokens on either side of the tar-
get word as features. Training was done with the
MEGA Model optimization package 2 and its cor-
responding NLTK interface.
At testing time, for each test instance, we
labeled the test sentences with four different
sequence labeling methods: first-order HMMs,
second-order HMMs, MaxEnt classifiers with no
sequence features, and the MEMMs with HMM
backoff. We then compared the system output
against the reference translations for the target
words using the script provided by the task orga-
nizers.
5.2 All-words Lexical Selection for
Spanish-Guarani
Since we are primarily interested in lexical selec-
tion for RBMT systems in lower-resource settings,
we also experimented with translating from Span-
ish to Guarani, using the Bible as bitext. In this
experiment, we labeled all of the text in the test
set using each of the different sequence labeling
models, and we report the classification accuracy
over the test set.
For example, for the following sentences ?
2http://www.umiacs.umd.edu/?hal/megam/
from Isaiah and Psalms, respectively ? the system
should predict the corresponding Guarani roots for
each Spanish word. Here we show the inflected
Spanish and Guarani text with English translation
for the sake of readability, although the system
was given the roots of the Spanish words as pro-
duced by the morphological analyzer.
(3) a. Plantare?is vin?as y comere?is su fruto.
b. Pen?oty? parral ha pe?u hi?a.
c. You will plant vineyards and eat their
fruit.
(4) a. Comieron y se saciaron.
b. Okaru hikua?i hygua?ta? meve.
c. They ate and were well filled.
In this example, the correct translation of comer
depends on transitivity: if transitive, it should be
an inflected form of ?u as in (3), if intransitive it
should be karu, as in (4).
In preparing the corpus, since different transla-
tions of the Bible do not necessarily have direct
correspondences between verse numbers (they are
not unique identifiers across language!), we se-
lected only the chapters that contain the same
number of verses in our Spanish and Guarani
translations. This only leaves 879 chapters out of
1189, for a total of 22828 bitext verses of roughly
one sentence each. We randomly sampled 100
verses from the corpus and set these aside as the
test set.
105
Here we trained the HMM and MEMM as be-
fore, but with lemmatized Spanish as the source
language, and the roots of Guarani words as the
target. As Guarani is a much more morphologi-
cally rich language than either English or Spanish,
this requires the use of a sophisticated morpholog-
ical analyzer, described in section 6. Due to the
much smaller data set, in this setting we stored
classifiers for any Spanish word that occurs more
than 20 times in the training data and backed off
to the HMM during decoding otherwise.
6 Morphological Analysis for Guarani
We analyze the Spanish and Guarani Bible us-
ing our in-house morphological analyzer, origi-
nally developed for Ethiopian Semitic languages
(Gasser, 2009). As in other, more familiar, mod-
ern morphological analyzers such as (Beesley and
Karttunen, 2003), analysis in our system is mod-
eled by cascades of finite-state transducers (FSTs).
To solve the problem of long-distance dependen-
cies, we extend the basic FST framework using an
idea introduced by Amtrup (2003). Amtrup starts
with the well-understood framework of weighted
FSTs, familiar from speech recognition. For
speech recognition, FST arcs are weighted with
probabilities, and a successful traversal of a path
through a transducer results in a probability that
is the product of the probabilities on the arcs that
are traversed, as well as an output string as in con-
ventional transducers. Amtrup showed that proba-
bilities could be replaced by feature structures and
multiplication by unification. In an FST weighted
with feature structures, the result of a success-
ful traversal is the unification of the feature struc-
ture ?weights? on the traversed arcs, as well as
an output string. Because a feature structure is
accumulated during the process of transduction,
the transducer retains a sort of memory of where
it has been, permitting the incorporation of long-
distance constraints such as those relating the neg-
ative prefix and suffix of Guarani verbs.
In our system, the output of the morphological
analysis of a word is a root and a feature struc-
ture representing the grammatical features of the
word. We implemented separate FSTs for Span-
ish verbs, for Guarani nouns, and for the two main
categories of Guarani verbs and adjectives. Since
Spanish nouns and adjectives have very few forms,
we simply list the alternatives in the lexicon for
these categories. For this paper, we are only con-
cerned with the roots of words in our corpora, so
we ignore the grammatical features that are output
with each word.
7 Results
The scores for the first experiment are presented
in Figure 2. Here we use the precision metric cal-
culated by the scripts for the SemEval shared task
(Lefever and Hoste, 2013), which compare the an-
swers produced by the system against several ref-
erence answers given by human annotators. There
are two ?most frequent sense? baselines reported.
The first one (?with tag?), is the baseline in which
we always take the most frequent label for a given
source word, conditioned on its POS tag. The
other MFS baseline is not conditioned on POS tag;
this was the baseline for the SemEval task. Per-
haps unsurprisingly, we see part-of-speech tagging
doing some of the lexical disambiguation work.
Neither of the HMM systems beat the most-
frequent-sense baselines, but both the non-
sequence MaxEnt classifier and the MEMM sys-
tem did, which suggests that the window fea-
tures are useful in selecting target-language words.
Furthermore, the MEMM system outperforms the
MaxEnt classifiers.
The scores for the second experiment are pre-
sented in Figure 3. Here we did not have human-
annotated reference translations for each word, so
we take the labels extracted from the alignments as
ground truth and can only report per-word classifi-
cation accuracy, rather than the more sophisticated
precision metric used in the shared task.
Here we see similar results. Neither of the
HMM systems beat the MFS baseline, and the tri-
gram model was noticeably worse. The training
set here is probably too sparse to train a good tri-
gram model. The MEMM system, however, did
beat the baseline, posting the highest results: just
over two-thirds of the time, we were able to predict
the correct label for each Spanish word, whereas
the most frequent label was correct about 60% of
the time.
8 Conclusions and Future Work
We have described a work-in-progress lexical se-
lection system that takes a sequence labeling ap-
proach, and shown some initial successes in us-
ing it for cross-language word sense disambigua-
tion tasks for English to Spanish and Spanish to
Guarani. We have demonstrated a hybrid se-
106
system features score (precision)
MFS (with tag) 24.97
MFS (without tag) 23.23
HMM1 current word, previous label 21.17
HMM2 current word, previous two labels 21.23
MaxEnt three-word window 25.64
MEMM three-word window, previous two labels 26.49
Figure 2: Results for the first experiment; SemEval 2013 CL-WSD task.
system features score (accuracy %)
MFS 60.39
HMM1 current word, previous label 57.40
HMM2 current word, previous two labels 43.04
MEMM three-word window, previous two labels 66.82
Figure 3: Results for the second experiment; all-words lexical selection on the Guarani Bible
quence labeling strategy that combines MEMMs
and HMMs, which will allow users to set parame-
ters sensibly for their computational resources and
available training data.
In future work, we will continue to refine the
approach, exploring different parameter settings,
such as beam widths, numbers of classifiers for
the MEMM component, and the effects of differ-
ent features as input to the classifiers. We are also
interested in making use of multilingual informa-
tion sources, as in the work of Lefever and Hoste
(2011). We may also consider more sophisticated
sequence tagging models, such as CRFs (Lafferty
et al, 2001), although we may not have enough
training data to make use of richer models.
Our goal for this work is practical; we are try-
ing to produce a hybrid Spanish-Guarani MT sys-
tem that can be used in Paraguay. We have a
small amount of Guarani training data available,
and plan to collect more. At the time of writing,
our lexical selection system is a prototype and not
yet integrated with our RBMT engine, but this in-
tegration is among our near-term goals.
A limitation of the current design is that we do
not yet have a good way to make use of monolin-
gual training data. In SMT, it is common practice
to train a language model for the target language
from a monolingual corpus that is much larger
than the available bitext. There is a substantial
amount of available Guarani text on the Web, but
our current model can only be trained on aligned
bitext. Given Guarani text that had been rear-
ranged into a Spanish-like word order, we could
build a better model for the transition probabilities
in the HMM component of the system. It might
be feasible to use a Guarani-language parser and
some linguistic knowledge for this purpose. We
will also investigate ways to translate multiword
expressions as a unit rather than word-by-word.
References
Jan Amtrup. 2003. Morphology in machine translation
systems: Efficient integration of finite state trans-
ducers and feature structure descriptions. Machine
Translation, 18.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI Publications.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1991. Word-
Sense Disambiguation Using Statistical Methods. In
Proceedings of the 29th Annual Meeting of the As-
sociation for Computational Linguistics.
Marine Carpuat and Dekai Wu. 2007. How Phrase
Sense Disambiguation Outperforms Word Sense
Disambiguation for Statistical Machine Translation.
In 11th Conference on Theoretical and Methodolog-
ical Issues in Machine Translation.
John DeNero and Dan Klein. 2007. Tailoring Word
Alignments to Syntactic Machine Translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics. Association
for Computational Linguistics, June.
Michael Gasser. 2009. Semitic morphological analy-
sis and generation using finite state transducers with
107
feature structures. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
The Tenth Machine Translation Summit.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML.
Els Lefever and Ve?ronique Hoste. 2013. SemEval-
2013 Task 10: Cross-Lingual Word Sense Disam-
biguation. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Els Lefever, Ve?ronique Hoste, and Martine De Cock.
2011. ParaSense or How to Use Parallel Corpora
for Word Sense Disambiguation. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies.
Antonio Molina, Ferran Pla, and Encarna Segarra.
2002. A Hidden Markov Model Approach to Word
Sense Disambiguation. In IBERAMIA.
Helmut Schmid. 1995. Improvements In Part-of-
Speech Tagging With an Application To German. In
Proceedings of the ACL SIGDAT-Workshop.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In PROCEEDINGS OF HLT-NAACL.
F. M. Tyers, F. Sa?nchez-Mart??nez, and M. L. Forcada.
2012. Flexible finite-state lexical selection for rule-
based machine translation. In Proceedings of the
17th Annual Conference of the European Associa-
tion of Machine Translation, EAMT12.
S?pela Vintar, Darja Fis?er, and Aljos?a Vrs?c?aj. 2012.
Were the clocks striking or surprising? Using WSD
to improve MT performance. In Proceedings of
the Joint Workshop on Exploiting Synergies be-
tween Information Retrieval and Machine Transla-
tion (ESIRMT) and Hybrid Approaches to Machine
Translation (HyTra).
108
