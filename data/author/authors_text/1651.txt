Selectional Restr ict ions in HPSG 
I on  Androutsopou los  Rober t  Da le  
Sottware and I (nowledge Engineer ing Laboratory  Language Tcclmology Group 
Inst i tute  of Intbrmat ics  and Te lecommmficat ions  Depar tment  of Comlmt ing  
Nat ional  Centre for Scientific Macquar ie University 
Research "Demokr i tos"  Sydney NSW 2109, Austral ia  
153 10 Ag. Paraskevi,  Athens,  Greece. e-maih Robert .Dale@mq.  edu.  au 
e-maih ionandr@?i t ,  demokr i tos ,  gr 
Abst rac t  
Selectional restrictions arc semantic sortal con- 
straints ilnposed on the particil)ants of lin- 
guistic constructions to capture contextually- 
dependent constraints on interpretation. De- 
spite their linfitations, selectional restrictions 
have t)roven very useflfl in natural bmguage ap- 
pli('ations, where they have been used frequently 
in word sense disambiguation, syntactic disam- 
biguation, and anaphora resolution. Given their 
practical wtlue, we explore two methods to in- 
corporate selectional restrictions in the HPSG 
theory, assuming that the reader is familiar with 
HPSG. The first method eml)loys ItPSG~S BACK- 
GROUND feature and a constraint-satisfaction 
comt)onent t)il)e-lined after the parser. The 
second method uses subsorts of retbrential in- 
dices, and 1)locks readings that violat(', sole(:- 
tional restrictions during parsing. While the- 
oretically less satisfactory, we have Ibund the 
second method particularly useflfl in the devel- 
opment of practical systems. 
1_ I n t roduct ion  
~lPhe term selectional restrictions refers to se- 
mantic sortal constraints imposed on the 1)ar -
ticipants of linguistic constructions. Selectional 
restrictions arc invoked, for example, to account 
tbr the oddity of (1) and (3) (cf. (2) and (4)). 
(1) ?Tom ate a keyboard. 
(2) Tom ate a banana. 
(3) ?Tom repaired the technician. 
(4) Tom repaired the keyboard. 
~IPo account br (1) and (2), one would typically 
introduce a constraint requiring the object of 
"to eat" to denote an edible entity. The odd- 
ity of (1) can then be attr ibuted to a violation 
of this constraint, since keyboards are typically 
not edible. Silnilarly, in (3) and (4) one could 
postulate that "to repair" can only be used with 
objects denoting artifacts. This constraint is vi- 
olated by (3), because technicians are typically 
persons, and persons are not artifacts. 
We note that selectional restrictions attempt 
to capture contextually-dei)endent constraints 
on interpretation. There is nothing inherently 
wrong with (1), and one can think of special 
contexts (e.g. where Tom is a circus pertbrmer 
whose act includes gnawing on comtmter pe- 
ripherals) where (1) is felicitous. The oddity 
of (1) is due to the fact that in most contexts 
l)eople do not eat keyboards. Similarly, (3) is 
ti;licitous in a science-fiction context where the 
technician is a robot, |rot not in most usual con- 
texts. Selectional restrictions are typically used 
to capture flints about tlm world which are gen- 
c.r~lly, but not necessarily, true. 
In w~rious forms, selectional restrictions have 
been used tbr many years, and their l imitations 
are well-known (Allen, 1995). For example, 
they cmmot account br lnetaphoric uses of lan- 
guage (e.g. (5)), and they run into 1,roblen,s in 
negated sentences (e.g. unlike (1), there is noth- 
ing odd about (6)). 
(5) My car drinks gasoline. 
(6) Tom cannot cat a keyboard. 
Despite their limitations, selectional restric- 
tions have proven very useflfl in practical appli- 
cations, and they have been employed in sev- 
eral large-scale natural language understanding 
systems (Martin et al, 1086) (Alshawi, 1992). 
Apart fl'om blocking pragmatically i lMbrmed 
sentences like (1) and (3), selectional restric- 
tions can also be used in word sense disanfl)igua- 
tion, syntactic dismnbiguation, and anaphora 
15 
resolution. In (7), for example, tile '))~qnter" 
refers to at computer peripheral, while in (8) it 
refers to a person. The correct sense of "printer" 
can be chosen in each case by requiring the ob- 
ject of "to repair" to denote an artifact, and the 
subject of "to call" (when referring to a phone 
call) to denote a person. 
(7) Tom repaired the printer. 
(8) The printer called this mornfilg. 
Silnilarly, (9) is from a syntactic point of view 
potentially ambiguous: the relative clause may 
refer either to the departments or the employ- 
ees. The correct reading can be chosen by speci- 
fying that the subject of "retire" (the relativised 
nominal in this case) must denote a person. 
(9) List tile employees of the overseas depart- 
nlents that will retire next year. 
Given tile value of selectional restrictions in 
t)ractical applications, we explore how they can 
be utilised in the HPS~ theory (Pollard and 
Sag, 1994), assuming that the reader is familiar 
with HPSG. Onr  proposals are based on expe- 
rience obtained from using IIPSG in a natural 
language database interface (Androutsot)oulos 
et al, 1998) and a dialogue system for a mobile 
robot. To the best of our knowledge, selectional 
restrictions have not been explored so far in the 
context of HPSG. 
We note that, although they often exploit 
similar techniques (e.g. semantic sort hierar- 
chies), selectional restrictions costitnte a differ- 
ent topic from linking theories (Davis, 1996). 
Roughly speaking, linking theories explore the 
relation between thematic roles (e.g. agent, pa- 
tient) and grammatical thnctions (e.g. subject, 
complement), while selectional restrictions at- 
tempt to account br the types of world entities 
that can fill the thematic roles. 
We discuss in sections 2 and 3 two ways that 
we have considered to incorporate selectional 
restrictions into HPSO. Section 4 concludes by 
comparing briefly the two approaches. 
2 Background res t r i c t ions  
The first way to accommodate selec- 
tional restrictions in HPSG USeS the 
CONTEXTIBACKGROUND (abbreviated here 
as CXlBG) feature, which Pollard and Sag 
(Pollard and Sag, 1994) reserve tbr "Micity 
conditions on the utterance context", "presup- 
positions or conventional iml)licatures", and 
"at)prot)riateness conditions" (op cit pp. 27, 
332). To express electional restrictions, we add 
qfpsoas (quantifier-flee parameterised states 
of atfairs) with a single semantic role (slot) 
iI1 CXIBG. 1 For exanlple, apart fi'om the eat 
qf'psoa in its NUCLEUS (NUt), the lexical sign 
tbr "ate" (shown in (10)) would introduce an 
edible qfpsoa in Bo, requiring \[\] (the entity 
denoted by tile object of "ate") to be edible. 
(10) "PIION (ate) 
CAT 
SS I LOC CONT \[ NUC 
cx I ~(~ 
\[ I,EAD lcTb l \ ] \ ]  
COMPS ( NP\[~J >i l l  
c.t \[,,~A'r,~N FlJ // 
In the case of lexical signs for proper names 
(e.g. (11)), the treatment of Pollard and Sag 
inserts a naming (namg) qfpsoa in BG, which 
requires the BEARER (BRER) to by identifiable 
in the context by means of the proper name. 
(11) also requires the bearer to be a man. 
(11) "PIION <ToTrt> 
oa,l, \[,,,:AD 
CONT \[INDEX 
\[RESTR ~}\] 
Tile ItPSO t)rinciples that control the propa- 
gation of the BG feature are not fully developed. 
For our purposes, however, tile simplistic prin- 
co)Ic of contextual consistency of Pollard and 
Sag will suffice. This principle causes the BG 
value of each phrase to be the union of tile BG 
values of its daughters. Assuming that the lex- 
ical sign of "keyboard" is (12), (10)-(12) cause 
(1) to receive (13), that requires \[\] to denote an 
edible keyboard. 
1To save space, we use qfpsoas wherever Pollard and 
Sag use quantified psoas. We also ignore tense and as- 
pect here. Consult (Androutsopoulos et al, 1998) for 
the treatment of tense and aspect in our ltpsG-based 
database interface. 
16 
(12) 
(13) 
/ \[INI)EX \[\] 
"PlION 
SS I LOC 
(~/bm, ate, a, keyboard) 
-II\],;AI) verb 1 
CAT SUIL1 <> \[ 
COMI'S ( )  J 
-QUANTS ( keybd \[INST \[~\]\])\] \[":a"E" Hm \] J CONT i NUC k I~A'rI':N eat 
|NAME :/bin|' /
namg ~ " l> 
\[ 
\[lNS'l' \[~l / edible L J 
A(:('ording t,, (13), to accept (1), one has to 
place it; ill ;~ st)ecial conte.xt where edible key- 
bonrds exist (e.g. (1) is thlMtous if it reM's to ;~ 
miniature cho(:ol~te keyt)oard). Su(:h (:ontexts, 
however, are rare, ;rod hen(:e (1) sounds gen- 
erally odd. Alternatively, one has to relax the 
B(~ constraint hat the keyboard 1111181; BC edi|)le. 
We assmne that special contexts ~dlow t)a.rticu - 
l~r BG constraints to be relaxed (this is how we 
wouht a(:(:omlt fin" the use of (1) ill ~L circus con- 
text), \])ut we (Io not ll;we any t'ornl~d lne(:hanisnl 
to sl)e(:itly exactly when B(~ (:(mstr;dnts ('~m l)e 
relnxcd. 
Similnr connnents apply to (3). Assuming 
th;Lt the sign of "req)aired" is (ld), nnd that 
the sign of "teclmi(:iml" is similar to (12) ex- 
cept that  it; introdu('es a technician index, (3) 
receives a sign theft requires the repairer to 1)e a 
technician who is all artifact. U k~(:hnicians, how- 
ever, are generally not artifacts, which accounts 
for the oddity of (3). 
(14) -PIION <repaired) 
l / / /  
L(.MP  < NI'  >J // 
"3SIL()C CONTINuc repair \[IIEPAI,\[I,3,) ~\]jl I
Let us now (:onsider \]lOW it (:omtmter sysl;em 
~o,,ld ~e(:o,ll~t for (~)-(~). For ex~ulU,>,, how 
entity 
abstract physical 
animate edible inanimate "" 
" ' "  lllall tech.ician ' /  X~ V "" keybd 
son animal non arfct ~ lc t  
edible animal edible_non afcl 
? " male tech "" 
. . . . . . . . .  ballalla 
Figure 1: A simt)listic semantic hierarchy 
WOilkl the system fig.re out fl'om (13) that (:1) 
is pragm~tieal ly odd? Among other things, it 
wouhl need to know that keyl)o~r(ts ~rc not edi- 
ble. Similarly, in (2) it would need to know that 
|)~m~m~s are edible, ~md in (3) (d) it; would need 
I;() 1)e nwarc that technicians are. llot artifacts, 
while keyboards m:e. Systenls that employ se- 
lc(:tionnl restri(:tions usunlly encode knowledge 
of this kind in the. fol:nl of sort hierarchies of 
worhl entii;ies. A siml)listic exmnt)le of such n 
hierm:chy is det)i('ted ill tigure 1. The hierarchy 
of tigure \] shows thnt nil lllell &lid technicians 
are 1)ersons, all 1)ersons are ~tniln;~|;e entities, all 
aninlate entities are t)\]lysi(:al ol)je(:ts, mitt so on. 
Some (1)ut not all) persons are 1)oth teehni(:ians 
:rod lnen at the same time; these t)ersons are 
nmml)ers of I;he male_tech sort. Similarly, all 
l)mlmms are edil)h; ;rod liot artifacts. No person 
is e(lil)le, because the sorts person and edible 
h~we no (:onnnon su|)sorts. 
It is, of course, extremely difficult to con- 
stru('t hierm'chies th~Lt include all the sorts of 
world entities. Ill natural  bmguage systenls that 
target sl)ecifi(: and restricted olmfins, however, 
constructing such hier;~rchies is feasible, because 
the relevant entity sorts and the possible hi- 
erarchical reb~tions between them are limited. 
In naturM lmlguage database interfimes, tbr ex- 
ample, the relevant entity sorts and the rela- 
tions between theln nre often identilied dur- 
ing the, (tesing of the database, in the tbrm 
of entity-relatiolMli 1) diagrams. We also note 
l;h;~t large-scah; smmmtic sort hierarchies are al- 
ready ill use ill artiticinl intelligence ~md natural  
17 
language gener~tion projects (tbr example, Cyc 
(Lenat, 1995) and KPML'S Upper Model (Bate- 
man, 1997)), and that the techniques that we 
discuss in this paper are in principle compatible 
with these hierarchies. 
To decide whether or not a sentence violates 
any selectional restrictions, we collect from the 
CONT and BO features of its sign ((13) in the 
case of (1)) all the single-role qfpsoas for which 
there is a sort in the hierarchy with the same 
name. (This rules out single-slot qt~)soas in- 
troduced by the CONTs of intransitive verbs.) 
The decision can then be seen as a constraint- 
satisthction problem, with the collected qfpsoas 
acting as constraints. (15) shows the constraints 
tbr (1), rewritten in a tbrm closer to predicate 
logic. HPSG indices (the boxed nmnbers) are 
used as variables. 
(15) kcybd(~\]) A man(~) A edible(~\]) 
Given two contstraints cl, c2 on the same vari- 
al)le, c~ subsumes c2 if the corresponding hier- 
archy sort of cl is an ancestor of that of c2 or 
if cl = c2. c~ and c2 can be replaced by a new 
single constraint c, if cl and c2 subsume c, and 
there is no other constraint d which is subsumed 
by cl,c2 and subsumes c. c and c' must be con- 
straints on the same variable as ct, c2, and must 
each correspond to a sort of the hierarchy. If the 
constraints of a sentence can be turned in this 
way into a tbrm where there is only one con- 
straint fbr each variable, then (and only then) 
the sentence violates no selectional restrictions. 
(15), and cdil, ,'am ot be re- 
p\]aced by a single constraint, because keybd and 
edible have no common subsorts. Hence, a se- 
lectional restriction is violated, which accounts 
for the oddity of (1). In contrast, in (2) the 
constraints would be as in (16). 
(16) banana(~) A man(m) A edible(~\]) 
banana(~\]) and cdible(F~) can now be re- 
placed by banana(F~), because both subsume 
banana(~\]), and no other constraint subsmned 
by both banana(~\]) and cdible(~) subsulnes 
banana(~). This leads to (17) and the conch> 
sion that (2) does not violate aw selectional 
restrictions. 
(17) banana(E\]) A man(D\]) 
This constraint-satisfaction reasoning, how- 
ever, requires a set)arate inferencing component 
that would be pipe-lined after the parser to rule 
out signs corresponding to sentences (or read- 
ings) that violate selectional restrictions. In the 
next section, we discuss an alternative approach 
that allows hierarchies of world entities to be 
represented using the existing HPSG framework, 
and to be exploited during parsing without an 
additional inferencing component. 
3 I ndex  subsor ts  
HPSG has already a hierarchy of feature struc- 
ture sorts (Pollard and Sag, 1994). This hierar- 
chy can be augmented to include a new part 
that encodes intbrmation about the types of 
entities that exist in the world. This can be 
achieved by partitioning the ref HPSO sort (cur- 
rently, a leaf node of the hierarchy of feature 
structures that contains all indices that refer 
to world entities) into subsorts that correst)ond 
to entity types. To encode the information of 
figure 1, rEf would have the snbsorts abstract 
and physical, physical would have the subsorts 
animate, edible, inanimate, and so on. That 
is, referential indices are partitioned into sorts, 
mid the indices of each sort can only be an- 
chored to world entities of the corresponding 
type (e.g. keybd indices can only be anchored 
to keyboards). 
With tiffs arrangement, the lexical sign for 
"ate" becomes (18). The Bo edible restriction 
of (10) has been replaced by the restriction that 
the index of the object must be of sort edible. 
(18) -PIION (at8) 
LCOVpS ( N~'CN 
L c?NTI \[I~aTEN I edible 
eat 
Similarly, the sign for "Tom" becomes (19) (cE 
(11)), and the sign for "keyboard" introduces an 
i ,dex of sort k vbd as shown in (9O) (cf. (12)). 
(19) "PITON (Tom) 
o,T no,4 \]\] 
CONT \[,mSTI~ (} J l /  
18 
\[ rm/N 1,:~fl, oa',.d) \] 
/ ' ' \[I{.I,,'STIt {} i l l  \[ ss  I J , oc  
\[cxJ BG {} JJ 
Unification of indices pro(:eeds in the, s;lille 
maturer as unificatioll of all other typed feature 
structm:es ((Jarlienter , 1!)!/2). 'Fhe parsing of 
(\]) iIOW fails, 1)ecause it al, te, nq)ts to unilly an 
il dox or (i,lt,:o,hl(::ed t/y with 
an index of so,*; t,:eybd (introduced t,y (20)), and 
no Ill'SO sorl; is sul)sumed l)y both. in (:ontrast, 
the parsing o17 (2) would su(:('eed, because the 
sign of "bmuma" would introduce an index of 
sort banana, which is a sut)sort of edible (Iigur(~ 
1); hence the two indi(:es can 1)(', ratified. (3) and 
(4) would l)e l)ro('essed sinfilarly. 
in (7) and (8), there would 1)e two lcxi(:nl 
signs for "ln'illtcr": one inl;ro(lu('ing ml index of 
sort pri'nter_pe'r.s'o'n, and one im:o(lu(:ing an in- 
dex of sort pri'nte'r_periph,(~'ral. (printe'r4)er.~'on 
and l)rinter_periph, cral would t)e daughters of 
person and art'@tel respectively in tigure 1.) 
The sign for "repairc, d", would require the index 
of its ol)je(:t to be of sort arl,'l\[fact, and l;he sign 
of "(:ail(~d" wou\](l re(tuire its sul)je('l; index to t)e 
of sort per,so'n. This (:orre(:tly admits only the 
reading where the rel)aire(l entity is a (:Omlml;er 
peripheral, ml(t l;tm (:aller is ;t t)(',rson. Simil~tr 
l l leC\] ial l isnls (;;/,ll })e llse(t to (l(~,\[;(!lTillille t;tlP, ( ;o f  
reel; reading of (9). 
With the al)proa(:h of this see, lion, it; is also 
possible to speciily seh;ctional restrictions in the 
declarations of qflIsoas in the Ill'SO hierarchy of 
feature structures, as shown in tigure 2, rather 
than in the lexi(:on. 2 When the same qft)soa is 
used in several exical signs, this saves having to 
repeat tile same, selectional restrictions in each 
one of the lexical signs. For example, the verbs 
"rq)air" and "Iix" iiiay both introduce a repair 
qfpsoa. The restriction that the repaired entity 
must be an artifact can lie sl)eeified once in the 
declaration of repair in the hierarchy of feature 
structures, rather than twice in the lexieal signs 
of ~'cl)air" and "fix". 
2Additional ayers can be included betwc,(m qfpsoa 
and the leaf sort;s, as sketched in section 8.5 of (Pollard 
and Sag, 1994), to group together qfpsoas with common 
selnalltifi roles. 
qfDso( 
\[EATI.H\[ (t,~i~r~ttt(;\] . . .  \[I~.E1)AIIIEIt l,('.7".~O,t \]
cat\[EATI,ZN edible \] ,v.pctirkRl.ZPAnU.:D artiJhct\] 
Figure 2: Declarations of qfpsoas 
4 Conc lus ions  
We have presented two met;hods to incorpo- 
rate selectional restrictions ill I lPSG: (i) express- 
ing selectional restrictions as BACKGROUND con- 
straints, and (it) enq)loying subsorts of referen- 
trial indices. The first method has the advantage 
that it requires no modification of the cmTent 
IIPSO feature structures. It also lnzdntains Pol- 
lard and Sag's distinction bel;ween "literal" mid 
"non-literal" meaning (expressed t)y CeNT and 
I~ACKGI/OUN\]) respe, ctively), a distinction whi('h 
is lflm'red in the second approach (e.g. nothing 
in (18) shows th~lt requiring the obje('t to denote 
an edil)le entity is part of the non-literal mean- 
ing; of. (10)). Unlike the tirst method, however, 
the second apt)roach re(tuires no additional ta- 
ft;renting comtionent br determining when se- 
lecl;ional restrictions h~tve been violated. With 
sentences that contain several potentially aiil- 
lfiguous words or phrases, t;11(,, second apl)roat:h 
is also more etlicienl;, ~ls it blocks signs that 
violalx', selectionnl testa'tel;ions during parsing. 
In the tirsl; aplm)ach, these signs remain un- 
detected uring parsing, and they may have a 
multiplicative ffect, h;ading to a large nmnber 
of parses, which then have to l)e checked individ- 
ually by the taft;renting component. We have 
timnd the se(:ond at)l)roach t)articularly useful 
in the develolnnent of practical systems. 
There is a deet)er question here al)out the 
proper place to maintain the kind of intbrnla- 
lion encoded in selectional restrictions. The 
applicabil ity of selectional restrictions is always 
context-dependent;  and for any selectional re- 
striction, we can ahnost always find a context 
where it does not hold. Our second method 
above effectively admits that we cromer develop 
a general tmrlIosc solution to the problem of 
meaning interprel;ation, and that we have to at- 
cept that our systems alwws operate in specific 
contexts. By committing to a particular con- 
text of interpretation, we 'compile into' what 
was tradit ional ly thought of as literal meaning a 
19 
set of contextually-determined constraints, and 
thus enable these constraints to assist in the 
HPSG language analysis without requiring an 
additional reasoning component. We take the 
view here that this latter approach is very ap- 
propriate in the construction ofreal applications 
which are, and are likely to be ibr the tbresee- 
able future, restricted to operating in limited 
domains. 
References  
J.F. Allen. 1995. Natural Language Under- 
standing. Benjamin/Cmnmings. 
H. Alshawi, editor. 1992. The Core Language 
Engine. MIT Press. 
I. Androutsopoulos, G.D. Ritchie, and 
P. Thanisch. 1998. Time, Tense and 
Aspect in Natural Lmlguage Database 
Interfaces. Natural Language Engineering, 
4(3):229-276. 
J.A. Batenlan. 1.997. Enat)ling Technology for 
Multilingual Natural Language Generation: 
the KPML Development Environment. Nat- 
ural Language Engineering, 3(1):15-55. 
B. Carpenter. 1992. The Logic of Typed Feature 
Structures. Number 32 in Canlbridge ~h'acts 
in Theoretical Computer Science. Cambridge 
University Press. 
T. Davis. 1996. Lczical Semantics and Link- 
ing in the Iticra~'chical Lezicon. Ph.D. thesis, 
Stanford University. 
D.B. Lenat. 1995. CYC: A Large-Scale Invest;- 
merit in Knowledge Infl'astructure. Cornm'a- 
nications of ACM, 38(11):33-38. 
P. Martin, D. Appelt, and F. Pereirm 1986. 
Transt)ortability and Generality in a Natural- 
Language Interface Systein. In B. Grosz, 
K. Sparek Jones, and B. Webber, editors, 
Readings in Natural Language PTvcessing, 
pages 585 593. Morgan KaufmamL 
C. Pollard and I.A. Sag. 1994. Head-Driven 
Phrase Structure Grammar. University of 
Chicago Press and Center tbr the Study of 
Language and Information, Stanford. 
20 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 553?560
Manchester, August 2008
What?s the Date?
High Accuracy Interpretation of Weekday Names
Pawe? Mazur
1,2
1
Institute of Applied Informatics,
Wroc?aw University of Technology
Wyb. Wyspia?nskiego 27,
50-370 Wroc?aw, Poland
Pawel.Mazur@pwr.wroc.pl
Robert Dale
2
2
Centre for Language Technology,
Macquarie University,
NSW 2109, Sydney, Australia
{mpawel,rdale}@ics.mq.edu.au
Abstract
In this paper we present a study on the
interpretation of weekday names in texts.
Our algorithm for assigning a date to a
weekday name achieves 95.91% accuracy
on a test data set based on the ACE
2005 Training Corpus, outperforming pre-
viously reported techniques run against
this same data. We also provide the first
detailed comparison of various approaches
to the problem using this test data set, em-
ploying re-implementations of key tech-
niques from the literature and a range of
additional heuristic-based approaches.
1 Introduction
Many temporal expressions in text are underspeci-
fied, requiring contextually-sourced information in
order to determine their correct interpretation. In
some cases, it is sufficient to determine what is
sometimes called the temporal focus, so that the
precise location of a relative temporal expression
on a timeline can be determined with respect to
this ?time of speaking?. Consider, for example, ex-
pressions like the following:
(1) three days ago
(2) last Monday
(3) in two weeks time
Once we know the temporal focus, calculation of
the temporal location referred to in each of these
cases is straightforward, since the temporal ex-
pressions themselves explicitly indicate what we
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
will call the direction of offset (here, respectively,
past, past and future). However, in other cases
there is no explicit indication of the direction of
offset from the temporal focus. This is most ob-
viously the case when bare expressions based on
calendar cycles?i.e., weekday names and month
names?are used, as in the following example:
(4) Jones met with Defense Minister Paulo Portas
on Tuesday and will meet Foreign Minis-
ter Antonio Martins da Cruz before leav-
ing Portugal Wednesday.
Here, the proper interpretation of the references
to Tuesday and Wednesday requires at the least a
correct syntactic analysis of the sentence, in order
to locate the controlling verb for each weekday
name. The tense of this verb can then be used
to determine the direction?either in the past or in
the future?in which we need to look to establish
the fully specified date referred to. In the case of
example (4), this means determining that Tuesday
is in the scope of the verb met, and that Wednes-
day is in the scope of the verb group will meet.
As we note below, it turns out that there are cases
where even the controlling verb does not provide
sufficient information to determine the direction of
offset. But even in those cases where the tense
of the verb does provide the relevant information,
there are two problems. First, especially when the
sentences considered are complex, there is a non-
negligible likelihood that the analysis returned by a
parser may not be correct, and this is especially the
case when the sentences in question contain struc-
tures such as prepositional phrases: the attachment
of these is notoriously a source of ambiguity, and
they just happen to often be the hosts to temporal
expressions. Second, even if a parser provides the
correct analysis, parsing technology is still compu-
553
tationally expensive to use when processing very
large bodies of text; if we are interested in time-
stamping events described in significant volumes
of data, we would prefer to have a faster, more
heuristic-based approach.
In this paper, we explore the development of a
fast and high accuracy algorithm for the interpre-
tation of weekday names, in particular with regard
to determining the direction of offset to be used
in the temporal interpretation of these expressions:
in essence, how can we determine whether the day
referred to is in the past or in the future?
The rest of the paper is structured as follows. In
Section 2 we present some general observations on
the interpretation of weekday names in text. Sec-
tion 3 provides a review of related work. In Sec-
tion 4 we describe the corpus used for evaluation,
and in Section 5 we describe in detail the various
algorithms we evaluated. Section 6 compares the
results of the various algorithms when applied to
the corpus, and Section 7 provides an error anal-
ysis. Finally, in Section 8 we draw some conclu-
sions and point to future work.
2 The Problem
The interpretation of relative temporal expres-
sions
1
can be seen as a two step process: (1) first
we have to determine a reference point for the in-
terpretation of the expression;
2
(2) then we have to
calculate the actual position of the referred-to time
on the timeline.
Once we have the reference point determined,
the interpretation of the offset from this reference
point requires us to determine the magnitude and
direction of offset. As noted above, in some cases
the tense of the controlling verb will indicate the
direction of offset; but prepositional attachment
ambiguity can easily damage the reliability of such
an approach, as demonstrated by the following
minimal pair:
(5) We can show you some pictures on Monday.
(6) We can show you some pictures from Monday.
In example (5), the correct PP attachment is re-
quired in order to determine that Monday is in the
1
In the literature, a variety of different terms are used:
(Schilder and Habel, 2001) call these expressions indexicals,
and (Han et al, 2006b) uses the term relative for what we
call anaphoric references: in our terminology, both deictic
and anaphorical expressions are relative.
2
This reference point is often referred to as the temporal
focus or temporal anchor.
scope of the verb group can show, allowing us to
infer that the Monday in question is in the future.
Example (6), on the other hand, is quite ambigu-
ous and requires world knowledge in order to de-
termine the correct attachment.
We are interested, therefore, in determining
whether some heuristic method might provide
good results. In the rest of this paper, we focus
on the determination of the direction of offset. We
will not explicitly address the question of deter-
mining the temporal focus: although this is clearly
a key ingredient, we have found that using the doc-
ument creation date performs well for the kinds of
documents (typically newswire stories and similar
document types) we are working with. More so-
phisticated strategies for temporal focus tracking
would likely be required in other genres.
3 Related Work
The literature contains a number of approaches to
the interpretation of weekday names, although we
are not aware of any pre-existing direct compari-
son of these approaches.
3
Filatova and Hovy (2001) assign time stamps to
clauses in which an event is mentioned. As part
of the overall process, they use a heuristic for the
interpretation of weekday names: if the day name
in a clause is the same as that of the temporal focus,
then the temporal focus is used;
4
otherwise, they
look for any ?signal words? or check the tense of
the verb in the clause. An analogous approach is
taken for the interpretation of month names.
Negri and Marseglia (2005), in their rule-based
system for temporal expression recognition and
normalisation, use what they call ?context words?,
such as following or later, to decide on the inter-
pretation of a weekday name. Consider the fol-
lowing example:
(7) He started studying on March 30 2004, and
passed the exam the following Friday.
Here, having identified the date March 30 2004
(which happens to be a Tuesday), they then recog-
nise the structure ?following + trigger? and reason
that the Friday is three days later.
3
Although Ahn et al (2007) compared their results with
those presented by Mani and Wilson (2000), they went on
to point out that, for a variety of reasons, the numbers they
provided were not really comparable.
4
Filatova and Hovy use the term reference point for what
we call the temporal focus.
554
There have also been machine-learning ap-
proaches to the interpretation of temporal expres-
sions. Ahn et al (2005) describes a system de-
veloped and tested on the ACE 2004 TERN test
corpus. Using lexical features, such as the oc-
currence of last or earlier in a context window
of three words, their maximum entropy classifier
picked the correct direction (?backward?, ?same?,
or ?forward?) with an accuracy of 59%; the addi-
tion of features encoding information about tense
increased the result to 61%.
Ahn et al (2007) go on to describe a system us-
ing a classifier based on support vector machines
and an extended set of features over a larger subset
of the data. This algorithm was used to determine
the direction of all relative temporal expressions,
not just the names of weekdays. They used three
sets of features:
1. Character type patterns, lexical features such
as weekday name and numeric year, a con-
text window of two words to the left, and sev-
eral parse-based features: the phrase type, the
phrase head and initial word (and POS tag),
and the dependency parent (and correspond-
ing relation) of the head.
2. The tense of the closest verb (w.r.t. depen-
dency path), the POS tag of the verb, and the
POS tags of any verbal elements directly re-
lated to this verb.
3. Features comparing year, month name and
day name of a temporal expression to those
of the document creation date.
Their experiments demonstrated that the third set
was the most useful.
Han et al (2006a) report on the development of
the Time Calculus for Natural Language (TCNL),
a compact formalism designed to capture the
meaning of temporal expressions in natural lan-
guage, which is built on top of their constraint-
based calendar model (see (Han and Lavie, 2004)).
In this formalism each temporal expression is con-
verted to a formula in TCNL, which then can be
processed to calculate the value of a temporal ex-
pression. Interpretation of weekday names uses
the tense of the nearest verb chunk and the pres-
ence of lexical items such as next. Their tempo-
ral focus tracking mechanism allows correct inter-
pretation of cases like ?I am free next week. How
about Friday??, where the TCNL formula for Fri-
day, being +{fri}, reflects the occurrence of next in
Table 1: ACE 2005 Training Corpus
Domain #Docs #Words # TIMEX2
BC 60 40415 626
BN 226 55967 1455
CTS 39 39845 409
NW 106 48399 1235
UN 49 37366 741
WL 119 37897 1003
Total 599 259889 5469
Table 2: Weekdays in ACE 2005 Training Corpus
Domain #Docs # TIMEX2 # per doc
BC 4 7 (1.91%) 1.75
BN 25 31 (8.47%) 1.24
CTS 2 2 (0.54%) 1.00
NW 102 292 (79.56%) 2.86
UN 3 3 (0.81%) 1.00
WL 19 32 (8.72%) 1.68
Total 155 367 (100%) 2.37
the preceding sentence.
Most closely relevant to the work described in
the present paper are the approaches described in
(Baldwin, 2002), (Jang et al, 2004) and (Mani and
Wilson, 2000). Since we have re-implemented ver-
sions of these algorithms for the present paper, we
leave description of these to Section 5.
4 Corpus and Experimental Setup
For this work we used the ACE 2005 Training Cor-
pus, which is publicly available and distributed by
the Linguistic Data Consortium (LDC).
5
It has al-
ready become the gold standard in the information
extraction community, especially for the temporal
expression recognition and normalisation (TERN)
task, and currently it provides the largest avail-
able corpus of annotated temporal expressions. Ta-
ble 1 presents some relevant statistics, and Table 2
shows the distribution of bare weekday names (as
TIMEX2 counts) in the corpus across the various
genres represented.
6
For the work described here, we used only those
documents in the corpus that contained at least one
weekday name; all subsequent analysis makes use
only of the gold standard annotations of the bare
weekday names in these documents, thus signifi-
cantly reducing corpus processing time. This re-
sults in a total of 367 instances, once errors (of
which there are quite a few) in the gold standard
5
The corpus?s catalogue number is LDC2006T06.
6
BC = Broadcast Conversations; BN = Broadcast News;
CTS = Conversational Telephone Speech; NW = Newswire;
UN = Usenet Newsgroups; and WL = Weblogs.
555
annotations have been repaired. We made the fol-
lowing changes to the gold standard data:
? One day name had been missed by the anno-
tators; we added this.
? Some 40 values were corrected from the for-
mat YYYY-Wnn-m to YYYY-MM-DD: al-
though both are correct in some sense, the
ACE guidelines indicate that the second is the
preferred form.
? Eight cases where the incorrect value had
been provided by the annotators were cor-
rected.
Specific details of these corrections, and the com-
plete data set used, are available on the web.
7
5 Evaluated Approaches
We implemented and evaluated a number of both
simple and more complex approaches to determin-
ing what date is meant in a text when a bare week-
day name is used. These methods, described be-
low, can be divided into two main classes: (a) 7-
day window based, and (b) tense analysis based.
Our new algorithm is a hybrid solution that incor-
porates ideas from both of these approaches.
5.1 Baselines
Our baselines are motivated by the observation that
days referred to by bare weekday names are typi-
cally temporally close to the temporal focus.
8
Past
7-day Window (inclusive): This baseline looks
for the specified day in a 7-day window whose
last day is the temporal focus. In other words, day
names are always assumed to refer to days in the
last week, including the ?day of speaking?.
Past 7-day window (exclusive): This is the same
as the approach just described, except that we look
for the referred-to day in the week leading up to
but not including the ?day of speaking?.
Future 7-day window (inclusive): This is the
future-oriented version of the first approach de-
scribed above: we look for the specified day in a
7-day window whose first day is the temporal fo-
cus. This assumes that all day name references are
to the present or future.
9
7
Visit http://TimexPortal.info.
8
Recall that in the present work we take the temporal focus
to be the document creation date.
9
An informal check of email data drove Han et al (2005)
to use the simple strategy of always assuming that weekday
names refer to days in the future.
Future 7-day window (exclusive): In this case the
7-day window starts on the day following the ?day
of speaking?.
5.2 Algorithms
5.2.1 Baldwin?s 7-Day Window
This algorithm was presented in (Baldwin, 2002;
Jang et al, 2004). It is similar to our window-
based baselines, but in this case the temporal focus
is the middle day of the 7-day window. This ap-
proach was used in their research after observing
that 96.97% of weekday name expressions in their
English corpus referred to dates within such a win-
dow. Suppose we have the following sentence in a
document with creation date 2003-06-16 (a Mon-
day):
(8) Police arrested her in Abilene, Texas, Saturday
where she had moved with a friend June 2.
The 7-day window then spans from Friday
(June 13) to Thursday (June 19). The reference
to Saturday is assigned (correctly) the value of the
second day in the window, i.e. 2003-06-14. Note
that this method will deliver the wrong result when
the referred-to day actually falls further than three
days either side of the temporal focus. Suppose,
for example, we have the following sentence in a
document written on 2005-01-01 (a Saturday):
(9) We got into Heathrow on Monday morning.
Here the 7-day window spans from Wednesday to
Tuesday, and so the reference to Monday will be
assigned the incorrect interpretation 2005-01-03.
5.2.2 Mani and Wilson?s Tense Estimation
In the system presented in (Mani and Wilson,
2000), weekday name interpretation is imple-
mented as part of a sequence of interpretation rules
for temporal expression interpretation more gener-
ally. This algorithm attempts to establish the tense
of what we have called the controlling verb in the
following way. First, it looks backwards from the
temporal expression in question to any previous
temporal expression in the sentence, or if there is
none, to the beginning of the sentence. If no verb
is found here, then it looks between the temporal
expression and the end of the sentence; and if a
verb is still not found, then it looks in front of any
preceding temporal expression found back to the
beginning of the sentence. If the verb found is in
past tense, the direction of offset is assumed to be
556
Table 3: Interpretation rules
Tense Example Direction
Present Continuous I am flying to New York on Monday. Future
Past Simple I wrote a paper on Monday. Past
Future Simple I will write a paper on Monday. Future
Present Perfect I have been writing a paper since Monday. Past
Bare Past Participle The draft written on Monday was useless. Past
Modal Verb I should finish the paper on Monday. Future
Modal Verb + have I should have submitted the paper on Monday. Past
backwards; if the tense is future, then the forward
direction is used. If the verb found is in present
tense, then the temporal expression is passed to a
further set of interpretation rules, which check for
things like the occurrence of lexical markers such
as since or until.
10
For example, in example (4),
repeated below, the algorithm would correctly pick
met for Tuesday and will meet for Wednesday, in-
terpreting Tuesday as a day in a past and Wednes-
day as a day in future.
(4) Jones met with Defense Minister Paulo Portas
on Tuesday and will meet Foreign Minis-
ter Antonio Martins da Cruz before leav-
ing Portugal Wednesday.
However, this approach will not correctly interpret
example (10):
(10) Still a decision has to made on what, if
any, punishment he will face in the wake
of that incident Tuesday night.
In this case, the wrong verb will be identified, and
the direction of offset will be incorrect.
5.2.3 Simple Tense Estimation
As an alternative to Mani and Wilson?s approach,
we also implemented a much simpler tense estima-
tion heuristic. This checks whether the sentence
contains any tokens with the VBD (i.e., past tense)
part of speech tag;
11
if one is found, then the direc-
tion of offset is assumed to be backwards, and if
not, then we use the forward direction. In the case
of example (4), this will assign the correct value to
Tuesday, but the wrong value to Wednesday.
10
We have reimplemented this algorithm based on the de-
scription given in the cited paper, but some details are unclear,
so we acknowledge that the original implementation might
produce slightly different results.
11
Where POS tags are required in our algorithms, we
used Mark Hepple?s part of speech tagger, an implementa-
tion of which is available as a plugin for the GATE platform
(http://gate.ac.uk).
5.2.4 Dependency-based Tense Determination
The two previous algorithms attempt to determine
the controlling verb using very simple heuristics.
Of course, a more reliable way of determining the
controlling verb is to use a parser. We used the
Stanford parser?s dependency information output
(see (de Marneffe et al, 2006)) to find the con-
trolling verb of a weekday name in a sentence.
This algorithm does this by traversing the result-
ing dependency tree from the node containing the
weekday name to its root until a verb is found, and
then following further dependencies to identify the
whole verbal sequence.
5.2.5 A Hybrid Algorithm
Heuristic methods for determining tense are risky,
especially as the distance between the controlling
verb and the temporal expression increases. We
therefore propose a hybrid approach that attempts
to leverage both tense estimation approaches like
Mani and Wilson?s, and Baldwin?s window-based
approach. This algorithm was developed on the
basis of an error analysis of the results of using
Baldwin?s algorithm. It embodies a two-step ap-
proach, where we first look only in the very local
environment for clues as to the tense of the control-
ling verb, then fall back on Baldwin?s algorithm if
no such evidence is found close by. First, we check
if the temporal preposition since appears immedi-
ately in front of a weekday name; if so, the direc-
tion of offset is assumed to be backwards; other-
wise, the algorithm looks for any verbs in a win-
dow of three tokens before and three tokens after
the temporal expression. If a verb is found, then
its tense is used to determine the direction (using
the same rules as in Mani and Wilson?s approach).
If no verb is found, then a 7-day window with the
temporal focus as the middle day is used, just as in
Baldwin?s algorithm.
557
Table 4: Results
Algorithm Errors Correct
Past 7-day Window (Inclusive) 51 316 (86.10%)
Past 7-day Window (Exclusive) 240 127 (34.60%)
Future 7-day Window (Inclus.) 129 238 (64.85%)
Future 7-day Window (Exclus.) 316 51 (13.90%)
Sentence Tense Estimation 38 329 (89.65%)
Dependency-Based 29 338 (92.10%)
Mani and Wilson?s 27 340 (92.64%)
Baldwin?s 7-day Window 21 346 (94.28%)
Voting 16 351 (95.64%)
Hybrid 15 352 (95.91%)
Table 5: Processing times
Algorithm Time [seconds]
Past 7-day Window (inclusive) 79.9
Past 7-day Window (exclusive) 79.7
Future 7-day Window (inclus.) 79.2
Future 7-day Window (exclus.) 79.4
Sentence Tense Estimation 80.6
Dependency-Based 616.5
Mani and Wilson?s 80.9
Baldwin?s 7-day Window 79.4
Voting 636.1
Hybrid 80.2
5.2.6 Voting
This algorithm uses a voting mechanism over
the output of Baldwin?s, Mani and Wilson?s, and
the Dependency-based Tense Determination algo-
rithms. If all values are different (no majority) then
Baldwin?s result is used.
5.3 Tense Interpretation Rules
Once the verb group is found by any particular al-
gorithm, it needs to be analysed to determine what
its tense is; this information is then used to de-
termine the direction of offset. The interpretation
rules are summarized in Table 3.
6 Results
Table 4 presents the results achieved with each of
the algorithms.
The 51% difference between the inclusive and
exclusive baselines is indicative of the fact that,
in this data, in over 50% of cases the correct
date was in fact the document creation date. This
phenomenon is due to the large proportion of
newswire data in the corpus; in this genre, it is
common to use the weekday name even when re-
porting on events that happen on the same day as
the reporting takes place. Also of note is that the
best performing baseline, ?Past 7-day window (in-
clusive)?, achieves 86.10% accuracy despite its be-
ing an extremely naive approach.
All the algorithms tested here performed bet-
ter than the baselines. The best performing algo-
rithm was the Hybrid method, which made 15 er-
rors, resulting in an accuracy of 95.91%; the Vot-
ing method came second with 16 errors. Bald-
win?s 7-day window algorithm correctly inter-
preted 94.28% of weekday names. The big advan-
tage of this algorithm, along with all the baselines,
is their complete resource independence: they do
not use any parsers or POS taggers.
Perhaps surprisingly, Mani and Wilson?s tense
estimation heuristic was more effective than tense
determination based on a dependency parse tree;
this reinforces our earlier point about the risks of
using parsers. It is also important to note that
there are huge differences in execution time for
parser-based approaches. Table 5 presents times
for processing the entire corpus for temporal ex-
pression recognition and interpretation; the parser-
based algorithm required 616 seconds, in contrast
to around 80 seconds for each of the other algo-
rithms.
12
There were 296 cases (80.65%) that were cor-
rectly interpreted by all of the following algo-
rithms: Sentence Tense Estimation, Mani and Wil-
son?s, Dependency-based Tense Determination,
Baldwin?s 7-day Window, and Hybrid. There are
also three cases where all these algorithms pro-
vided an incorrect value:
13
(11) reporter: friday night in the gaza strip and
a journalist is about to lose his life.
(12) president bush head to the g-8 summit in
france on friday with victory over saddam
hussein and in his pocket and a soaring ap-
proval rating by the american public, but
do europeans share the same enthusiasm
for the president?
(13) I will return this piece of shit on Fri-
day, only to rent another vehicle Monday
morning while we take the wife?s car to
the shop to get her 1400 bucks worth of
damage repaired.
12
Note that the parser was only called for those sentences
that contained bare weekday names, and not for other sen-
tences in these documents.
13
We present these examples with their original spelling
and casing.
558
In example (11), the algorithms interpreted Fri-
day night as a day in future. However, this text
is a case of what is sometimes called the histori-
cal present, where for rhetorical effect the author
speaks in present tense from a past point in time; it
is not obvious how any algorithm would determine
the correct answer here. Example (12) is ungram-
matical as a consequence of a missing ?s? in head;
consequently, the POS tagger did not annotate this
word as a verb, and the algorithms identified do
or bush as a verb, leading to the decision that the
referred-to friday is in the future; however, the gold
standard interpretation puts this in the past (note
that, even with the correct verb form of heads, all
the algorithms would still get the wrong date). It
so happens the correct date here is also outside
the 7-day window. In example (13), because the
weekday name used is the same as the day name
of the document creation date, all the algorithms
assigned the document creation date instead of a
date seven days later.
7 Error Analysis
The Hybrid Algorithm achieved the best accuracy
of 95.91%, which corresponds to 15 error cases.
These were as follows:
? Eight cases where there was no verb found in
the three-token neighbourhood of the tempo-
ral expression; in these cases the 7-day win-
dow method was used, but this did not find
the correct value.
? Three cases where the algorithm identified a
verb that was not the controlling verb; for ex-
ample, it picked will meet instead of met to
interpret Tuesday in the sentence given in ex-
ample (4).
? Two cases where the document creation date
was very misleading (see below).
? Two cases where past tense was used to talk
about plans for the future which were sub-
sequently cancelled, as in discussions were
scheduled to end Friday, when Kelly was to
fly. . . .
In 204 cases the algorithm interpreted the week-
day name based on a verb found in the three-token
neighbourhood; and in 163 cases it used the fall-
back 7-day window strategy. Since the Hybrid
Algorithm was built as an extension of Baldwin?s
method, it is worth knowing whether there were
any cases where the original 7-day window method
got the correct value and the Hybrid Algorithm got
it wrong. There were six such cases:
? Two of them occurred for documents with
a misleading document creation date. In a
typical example, a document with the times-
tamp 17-04-2004 (a Thursday) contained the
sentence ?Malaysia?s Appeal Court Friday re-
fused to overturn the conviction . . . ?. As the
document timestamp was used as the tempo-
ral focus, Friday was interpreted as a day in
the past, when in fact it was the day after the
timestamp.
? The other two cases demonstrate a weakness
in our approach, exemplified by the sentence
given in example (4): here the algorithm in-
correctly uses the verb group will meet when
interpreting Tuesday.
? The remaining two cases were cases where
the verb groups were scheduled to end and
scheduled to begin were used to talk about fu-
ture events.
In these last cases, the controlling verb is an in-
finitive, and there is no way, in the absence of ei-
ther world knowledge or a much more sophisti-
cated analysis of the text, of determining whether
the scheduled event is in the past or the future.
Sentences like these are a particular problem for
Mani and Wilson?s algorithm, where a signif-
icant number of misinterpretations involve sen-
tences in which the past tense is used to talk about
subsequently-changed plans for future, as in the
following:
(14) A summit between Sharon and his Pales-
tinian counterpart, Mahmoud Abbas, had
been planned for Wednesday but was post-
poned . . .
Here, this utterance could be legitimately produced
both before and after the Wednesday in question,
so no simple algorithm will be able to determine
the direction of offset.
8 Conclusions and Future Work
We have investigated the problem of the interpreta-
tion of bare weekday names in texts, and presented
a new heuristic which extends Baldwin?s (2002)
approach. Our evaluations on a widely-available
data set show that our Hybrid Algorithm was the
559
best performing algorithm, achieving an accuracy
of 95.91% with 15 errors out of 367 instances. The
algorithm is implemented within our DANTE sys-
tem for temporal expression interpretation (Dale
and Mazur, 2006; Mazur and Dale, 2007).
It seems quite possible that our heuristics
take advantage of phenomena that are specific
to newswire texts and other similar types of re-
portage. Although these are precisely the kinds of
texts where, in our own work, we need to provide
fast processing of large volumes of text, it remains
to be seen how these heuristics fare when faced
with a broader range of text types. In particular,
other text types are likely to require more sophis-
ticated approaches to temporal focus tracking than
we have used here.
Also, we have not attempted to replicate here the
machine learning approaches described in (Ahn et
al., 2005) and (Ahn et al, 2007), nor Han?s use of
constraint satisfaction problem methods (see (Han
et al, 2006a)). The comparative evaluation of
these is left for future work.
References
Ahn, D., S. F. Adafre, and M. de Rijke. 2005. Recog-
nizing and interpreting temporal expressions in open
domain texts. In We Will Show Them: Essays in
Honour of Dov Gabbay, Vol 1, pages 31?50.
Ahn, D., Joris van Rantwijk, and Maarten de Rijke.
2007. A Cascaded Machine Learning Approach
to Interpreting Temporal Expressions. In Proc. of
HLT: The Annual Conference of the North American
Chapter of the ACL (NAACL-HLT 2007).
Baldwin, J. 2002. Learning Temporal Annotation of
French News. Master?s thesis, Dept. of Linguistics,
Georgetown University, April.
Dale, R. and P. Mazur. 2006. Local Semantics in the
Interpretation of Temporal Expressions. In Proceed-
ings of the Workshop on Annotating and Reasoning
about Time and Events, pages 9?16, Sydney, Aus-
tralia, July.
de Marneffe, M.-C., B. MacCartney, and Ch. D. Man-
ning. 2006. Generating Typed Dependency Parses
from Phrase Structure Parses. In Proceedings of the
IEEE / ACL 2006 Workshop on Spoken Language
Technology.
Filatova, E. and E. Hovy. 2001. Assigning Time-
Stamps to Event-Clauses. In Harper, L., I. Mani,
and B. Sundheim, editors, Proceedings of the Work-
shop on Temporal and Spatial Information Process-
ing, pages 1?8, Morristown, NJ, USA.
Han, B. and A. Lavie. 2004. A Framework for Res-
olution of Time in Natural Language. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 3(1):11?32.
Han, B., D. Gates, and L. Levin. 2005. Anchoring
Temporal Expressions in Scheduling-related Emails.
In Katz, Graham, James Pustejovsky, and Frank
Schilder, editors, Annotating, Extracting and Rea-
soning about Time and Events, Dagstuhl Seminar
Proceedings.
Han, B., D. Gates, and L. Levin. 2006a. From Lan-
guage to Time: A Temporal Expression Anchorer.
In Proceedings of the Thirteenth International Sym-
posium on Temporal Representation and Reasoning,
pages 196?203.
Han, B., D. Gates, and L. Levin. 2006b. Understanding
Temporal Expressions in Emails. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 136?143, Morristown,
NJ, USA.
Jang, S. B., J. Baldwin, and I. Mani. 2004. Automatic
TIMEX2 Tagging of Korean News. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 3(1):51?65.
Mani, I. and G. Wilson. 2000. Robust Temporal Pro-
cessing of News. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics, pages 69?76, Morristown, NJ, USA.
Mazur, P. and R. Dale. 2007. The DANTE Tempo-
ral Expression Tagger. In Vetulani, Zygmunt, edi-
tor, Proceedings of the 3rd Language & Technology
Conference (LTC).
Negri, M. and L. Marseglia. 2005. Recognition and
Normalization of Time Expressions: ITC-IRST at
TERN 2004. Technical Report WP3.7, Information
Society Technologies, February.
Schilder, F. and Ch. Habel. 2001. From Temporal Ex-
pressions to Temporal Information: Semantic Tag-
ging of News Messages. In Harper, L., I. Mani, and
B. Sundheim, editors, Proc. of the Workshop on Tem-
poral and Spatial Information Processing, pages 1?
8, Morristown, NJ, USA.
560
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 543?552,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Seed and Grow: Augmenting Statistically Generated Summary Sentences
using Schematic Word Patterns
Stephen Wan?? Robert Dale? Mark Dras?
?Centre for Language Technology
Department of Computing
Macquarie University
Sydney, NSW 2113
swan,madras,rdale@ics.mq.edu.au
Ce?cile Paris?
?ICT Centre
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
We examine the problem of content selection
in statistical novel sentence generation. Our
approach models the processes performed by
professional editors when incorporating ma-
terial from additional sentences to support
some initially chosen key summary sentence,
a process we refer to as Sentence Augmen-
tation. We propose and evaluate a method
called ?Seed and Grow? for selecting such
auxiliary information. Additionally, we argue
that this can be performed using schemata, as
represented by word-pair co-occurrences, and
demonstrate its use in statistical summary sen-
tence generation. Evaluation results are sup-
portive, indicating that a schemata model sig-
nificantly improves over the baseline.
1 Introduction
In the context of automatic text summarisation, we
examine the problem of statistical novel sentence
generation, with the aim of moving from the current
state-of-the-art of sentence extraction to abstract-
like summaries. In particular, we focus on the task of
selecting content to include within a generated sen-
tence.
Our approach to novel sentence generation is to
model the processes underlying summarisation as
performed by professional editors and abstractors.
An example of the target output of this kind of gen-
eration is presented in Figure 1. In this example, the
human authored summary sentence was taken verba-
tim from the executive summary of a United Nations
proposal for the provision of aid addressing a partic-
ular humanitarian crisis. Such documents typically
exceed a hundred pages.
Human-Authored Summary Sentence:
Repeated [poor seasonal rains]1 [in 2004]2, culminating
in [food insecurity]3, indicate [another year]4 of crisis,
the scale of which is larger than last year?s and is further
[exacerbated by diminishing coping assets]5 [in both
rural and urban areas]6.
Key Source Sentence:
The consequences of [another year]4 of [poor rains]1 on
[food security]3 are severe.
Auxiliary Source Sentence(s):
However in addition to the needs of economic recovery
activities for IDPs, [food insecurity]3 [over the major-
ity of 2004]2 [has created great stress]5 on the poorest
families in the country, [both within the urban and rural
settings]6.
Figure 1: Alignment of a summary sentence to sentences
in the full document. Phrases of similar meaning are co-
indexed.
To write such summaries, we assume that the hu-
man abstractor begins by choosing key sentences
from the full document. Then, for each key sen-
tence, a set of auxiliary material is identified. The
key sentence is revised incorporating these auxil-
iary sentences to produce the eventual summary sen-
tence.
To study this phenomenon, a corpus of UN docu-
ments was collected and analysed.1 Each document
was divided into two parts comprising its executive
summary, and the remainder, referred to here as the
source. We manually aligned each executive sum-
mary sentence with one or more sentences from the
source, by choosing a key sentence that provided
1This corpus is described in detail in Section 5.1.
543
evidence for the content of the summary sentence
along with additional sentences that provided sup-
porting material.
We refer to the resulting corpus as the UN Con-
solidated Appeals Process (UN CAP) corpus. It is
a collection of sentence alignments, each referred to
as an aligned sentence tuple, which consists of:
1. A human authored summary sentence from the
executive summary;
2. A key sentence from the source;
3. Zero or more auxiliary sentences from the
source.
The key and any auxiliary sentences are referred to
collectively as the aligned source sentences.
We argue that some process that combines infor-
mation from multiple sentences is required if we are
to generate summary sentences similar to that por-
trayed in Figure 1. This is supported by our analysis
of the UN CAP corpus. Of the 580 aligned sentence
tuples, the majority, 61% of cases, appear to be ex-
amples of such a process.
Furthermore, the auxiliary sentences are clearly
necessary. We found that only 30% of the open-class
words in the summary are found in the key sentence.
If one selects all the open-class words from aligned
source sentences, recall increases to an upper limit
of 45% without yet accounting for stemming. This
upper bound is consistent with the upper limit of
50% found by Daume? III and Marcu (2005) which
takes into account stemming differences.
This demonstrates that the auxiliary material is
a valuable source of content which should be inte-
grated into the summary sentence, allowing an im-
provement in recall of up to 15% prior to account-
ing for morphological, synonym and paraphrase dif-
ferences. Of course, the trick is to improve recall
without hurting precision. A naive addition of all
words in the aligned source sentences incurs a drop
in precision from 30% to 23%. The problem thus is
one of selecting the relevant auxiliary content words
without introducing unimportant content. We refer
to this problem of incorporating material from aux-
iliary sentences to supplement a key sentence as Sen-
tence Augmentation.
In this paper, sentence augmentation is modelled
as a noisy channel process and has two facets: con-
tent selection and language modelling. This paper
focuses on the former, in which the system must
rank text segments?in this case, words?for inclu-
sion in the generated sentence. Given a ranked se-
lection of words, a language model would then order
them appropriately, as described in work on sentence
regeneration (for example, see Soricut and Marcu
(2005); Wan et al (2005)).
Provided with an aligned sentence tuple, the prob-
lem lies in effectively selecting words from the aux-
iliary sentences to bolster those taken from the key
sentence. Given that there are on average 2.7 aux-
iliary sentences per aligned sentence tuple, this ad-
ditional influx of words poses a considerable chal-
lenge.
We begin with the premise that, for documents
of a homogeneous type (in this case, the genre is
a funding proposal, and the domain is humanitarian
aid), it may be possible to identify patterns in the or-
ganisation of information in summaries. For exam-
ple, Figure 2 presents three summary sentences from
our corpus that share the same patterned juxtapo-
sition of two concepts DisplacedPersons and Host-
ingCommunities. Documents may exhibit common
patterns since they have a similar goal: namely, to
convince donors to give financial support. In the
above example, the juxtaposition highlights the fact
that those in need are not just those people from the
?epicenter? of the crisis but also those that look after
them.
We propose and evaluate a method called ?Seed
and Grow? for selecting content from auxiliary sen-
tences. That is, we first select the core meaning of
the summary, given here by the key sentence, and
then we find those pieces of additional information
that are conventionally juxtaposed with it.
Such patterns are reminiscent of Schemata, the or-
ganisations of propositional content introduced by
McKeown (1985). Schemata typically involve a
symbolic representation of each proposition?s se-
mantics. However, in our case, a text-to-text gener-
ation scenario, we are without such representations
and so must find other means to encode these pat-
terns.
To alleviate the situation, we turn to word-pair co-
occurrences to approximate schematic patterns. Fig-
544
Sentence 1:
The increased number of [internally displaced persons]1
and the continued presence of refugees have fur-
ther strained the scarce natural resources of [host
communities]2, stretching their capacity to the limit.
Sentence 2:
100,000 people, a significant portion of the population,
remain [displaced]1, burdening the already precarious
living conditions of [host families]2 in Dili and the
Districts.
Sentence 3:
The current humanitarian situation in Timor-Leste is
characterised by: An estimated [100,000 displaced
people]1 (10% of the population) living in camps and
with [host families]2 in the districts; A total or partial de-
struction of over 3,000 homes in Dili affecting at least
14,000 IDPs
Figure 2: Examples of the pattern ?DisplacedPersons[1],
HostingCommunities[2]?.
ure 2 showed that mentions of the plight of interna-
tionally displaced persons are often followed by de-
scriptions of the impact on the host communities that
look after them. In this particular example, this is
realised lexically in the co-occurrences of the words
displaced and host.
Corpus-based methods inspired by the notion of
schemata have been explored in the past by Lap-
ata (2003) and Barzilay and Lee (2004) for order-
ing sentences extracted in a multi-document sum-
marisation application. However, to our knowledge,
using word co-occurrences in this manner to repre-
sent schematic knowledge for the purposes of select-
ing content in a statistically-generated summary sen-
tence has not previously been explored.
This paper seeks to determine whether or not such
patterns exist in homogeneous data; and further-
more, whether such patterns can be used to better
select words from auxiliary sentences. In particular,
we propose the ?Seed and Grow? approach for this
task. The results show that even simple modelling
approaches are able to model this schematic infor-
mation.
In the remainder of this paper, we contrast our ap-
proach to related text-to-text research in Section 2.
The Content Selection model is presented in Section
3. Section 4 describes how a binary classification
model is used in a statistical text generation system.
Section 5 describes our evaluation of the model for a
summary generation task. We conclude, in Section
6, that domain-specific schematic patterns can be ac-
quired and applied to content selection for statistical
sentence generation.
2 Related Work
2.1 Content Selection in Text-to-Text Systems
Statistical text-to-text summarisation applications
have borrowed much from the related field of statis-
tical machine translation. In one of the first works to
present summarisation as a noisy channel approach,
Witbrock and Mittal (1999) presented a conditional
model for learning the suitability of words from a
news article for inclusion in headlines, or ?ultra-
summaries?. Inspired by this approach, and with
the intention of designing a robust statistical gener-
ation system, our work is also based on the noisy
channel model. Into this, we incorporate our con-
tent selection model, which includes Witbrock and
Mittal?s model supplemented with schema-based in-
formation.
Roughly, text-to-text transformations fall into
three categories: those in which information is com-
pressed, conserved, and augmented. We use these
distinctions to organise this overview of the litera-
ture.
In Sentence Compression work, a single sentence
undergoes pruning to shorten its length. Previ-
ous approaches have focused on statistical syntactic
transformations (Knight and Marcu, 2002). For con-
tent selection, discourse-level considerations were
proposed by Daume? III and Marcu (2002), who ex-
plored the use of Rhetorical Structure Theory (Mann
and Thompson, 1988). More recently, Clarke and
Lapata (2007) use Centering Theory (Grosz et al,
1995) and Lexical Chains (Morris and Hirst, 1991)
to identify which information to prune. Our work is
similar in incorporating discourse-level phenomena
for content selection. However, we look at schema-
like information as opposed to chains of references
and focus on the sentence augmentation task.
The work of Barzilay and McKeown (2005) on
Sentence Fusion introduced the problem of convert-
ing multiple sentences into a single summary sen-
545
tence. Each sentence set ideally tightly clusters
around a single news event. Thus, there is one gen-
eral proposition to be realised in the summary sen-
tence, identified by finding the common elements in
the input sentences. We see this as an example of
conservation. In our work, this general proposition
is equivalent to the core information for the sum-
mary sentence before the incorporation of supple-
mentary material.
In contrast to both compression and conservation
work, we focus on augmenting the information in
a key sentence. The closest work is that of Jing
and McKeown (1999) and Daume? III and Marcu
(2005), in which multiple sentences are processed,
with fragments within them being recycled to gener-
ate the novel generated text.
In both works, recyclable fragments are identified
by automatic means. Jing and McKeown (1999) use
models that are based on ?copy-and-paste? opera-
tions learnt from the behaviour of human abstrac-
tors as found in a corpus. Daume? III and Marcu
(2005) propose a model that encodes how likely it
is that different sized spans of text are skipped to
reach words and phrases to recycle.
While similar in task, our models differ substan-
tially in the nature of the phenomenon modelled. In
this work, we focus on content-based considerations
that model which words can be combined to build
up a new sentence.
2.2 Schemata and Text Generation
There exists related work from Natural Language
Generation (NLG) in finding material to build up
sentences. As mentioned above, our content selec-
tion model is inspired by work on schemata from
NLG (McKeown, 1985). Barzilay and Lee (2004)
showed that it is possible to obtain schema-like
knowledge automatically from a corpus for the pur-
poses of extracting sentences and ordering them.
However, their work represents patterns at the sen-
tence level, and is thus not directly comparable to
our work, given our focus on sentence generation.
In our system, what is required is a means to rank
words for use in generation. Thus, we focus on com-
monly occurring word co-occurrences, with the aim
of encoding conventions in the texts we are trying to
generate. In this respect, this is similar to work by
Lapata (2003), who builds a conditional model of
words across adjacent sentences, focusing on words
in particular semantic roles. Like Barzilay and Lee
(2004), this model was used to order extracted sen-
tences in summaries. In contrast, our work focuses
on word patterns found within a summary sentence,
not between sentences. Additionally, our tasks dif-
fer as we examine the statistical sentence generation
instead of sentence ordering.
3 Linguistic Intuitions behind Word
Selection
The ?Seed and Grow? approach proposed in this pa-
per divides the word-level content selection prob-
lem into two underlying subproblems. We address
these with two separate models, called the salience
and schematic models. The salience model chooses
the key content for the summary sentence while the
schematic model attempts to identify what else is
typically mentioned given those salient pieces of in-
formation.
3.1 A Salience Model: Learning ?Buzzwords?
There are a variety of methods for determining the
salient information in a text, and these underpin
most work in automatic text summarisation. As an
example of a salience model trained on corpus data,
Witbrock and Mittal (1999) introduced a method for
scoring summary words for inclusion within news
headlines. In their model, headlines were treated as
?ultra-summaries?. Their model learns which words
are typically used in headlines and encodes, at least
to some degree, which words are attention grabbing.
In the domain of funding proposals, key words
that grab attention may amount to domain-specific
buzzwords. Intuitively, a reader, perhaps someone
in charge of allocating donations, tends to look for
certain types of key information matching donation
criteria, and so human abstract authors will target
their summaries for this purpose.
We thus adapt the Witbrock and Mittal (1999)
model to identify such domain specific buzzwords
(BWM, for ?buzzword model?). For an aligned sen-
tence tuple, the probability that a word is selected
based on the salience of a word with respect to the
domain is defined as:
probbwm(select = 1|w) =
|summaryw|
|sourcew|
(1)
546
where summaryw is the set of aligned sentence tu-
ples that contain the word w in the summary sen-
tence and in the source sentences. The denomina-
tor, sourcew, is the set of aligned sentence tuples that
have the word w in either the key or an auxiliary sen-
tence.
As is implicit in this equation, we could just use
this buzzword model to select content not only from
the key sentence, but from the auxiliary sentences
as well. While it is intended ultimately to find the
key content of the summary, it can also serve as an
alternative baseline for auxiliary content selection to
compare against the ?Seed and Grow? model.
3.2 A Schema Model: Approximation via
Word co-Occurrences
To restate the problem at hand: the task is one
of finding elements of secondary importance that
schematically elaborate on the key information. We
do this by examining sample summary sentences for
conventional juxtapositions of concepts. As men-
tioned in Section 1, schemata are approximated here
with patterns of word-pair co-occurrences. Using a
corpus of human-authored summaries in the domain
of our application, it is thus possible to learn what
those common combinations of words are.
Roughly, the process is as follows. To begin with,
a seed set of words is chosen. The purpose of the
seed set is to represent the core proposition of the
summary sentence.
In this work, this core proposition is given by the
key sentence and so the non-stopwords belonging to
it are used to populate the seed set. In the ?Seed and
Grow? approach, we check to see which words from
auxiliary sentences pair well with words in the seed
set.
3.2.1 Collecting Word-level Patterns
Each training case in the corpus contains a single
human-authored summary sentence that can be used
to learn which pairs of words conventionally occur
in a summary. For each summary sentence, stop-
words are removed. Then, each pairing of words in
the sentence is used to update a pair-wise word co-
occurrence frequency table. When looking up and
storing a frequency, the order of words is ignored.
3.2.2 Scoring Word-Pair Co-occurrence
Strength
For any two words, w1 from the seed set and w2 from
an auxiliary sentence, the word-pair co-occurrence
probability is defined as follows:
probco-oc(w1,w2)
= freq(w1,w2)
freq(w1)+ freq(w2)? freq(w1,w2)
(2)
where f req(w1,w2) is a lookup in the word-pair co-
occurrence frequency table. This table stores co-
occurrence word pairs occurring in the summary
sentence.
3.2.3 Combining a Set of Co-occurrence Scores
Each auxiliary word now has a series of scores,
one for each comparison with a seed word. To rank
each auxiliary word, these need to be combined into
a single score for sorting.
When combining the set of co-occurrence scores,
one might want to account for the fact that each pair-
ing of a seed word with an auxiliary word might
not contribute equally to the overall selection of that
auxiliary word. Intuitively, a word in the seed set,
derived from the key sentence, may only make a
minor contribution to the core meaning of the sum-
mary sentence. For example, words that are part of
an adjunct phrase in the key sentence might not be
good candidates to elaborate upon. Thus, one might
want to weight these seed words lower, to reduce
their influence on triggering schematically associ-
ated words.
To allow for this, a seed weight vector is main-
tained, storing a weight per seed word. Different
weighting schemes are possible. For example, a
scheme might indicate the salience of a word. In
addition to the buzzword model (BWM) described
earlier, one might employ a standard vector space
approach (Salton and McGill, 1983) from Informa-
tion Retrieval, which uses term frequency scores
weighted with an inverse document frequency fac-
tor, or tf-idf. We also implement the case in which all
seed words are treated equally using binary weights,
where 1 indicates the presence of a seed word, and
0 indicates its absence. In the evaluations described
in Section 5, we refer to these three seed weighting
schemes as bwm and tf-idf, and binary respectively.
547
To find the probability of selecting an auxiliary
word using the schematic word-pair co-occurrence
model (WCM), an averaged probability is found
by normalising the sum of the weighted probabili-
ties, where weights are provided by one of the three
schemes above:
probwcm(wi) =
1
Z
?
|seed|
?
k=0
weightsk ?probco-oc(wi,wk) (3)
where seed is the set of seed words and wk is the kth
word in that set. The vector, weights, stores the seed
weights. The normalisation factor for the weighted
average, Z, is the number of auxiliary words.
Finally, since the WCM model only serves to se-
lect words from the auxiliary sentences, words from
the key sentence must be given scores as well. For
these words, the scoring is as follows:
probwcm(w) =
1
Z
(
1
|seed| + probwcm(w)
)
(4)
where Z is a normalisation across the set of seed
words.
4 Combining Buzzwords and Word-Pair
Co-Occurrence Models for Generation
As mentioned above, the noisy channel approach
is used for producing the augmented sentence. Al-
though the focus of this paper is on Content Selec-
tion, an overview of the end-to-end generation pro-
cess is presented for completeness.
Sentence augmentation is essentially a text-to-text
process: A key sentence and auxiliary material are
transformed into a single summary sentence. Fol-
lowing Witbrock and Mittal (1999), the task is to
search for the string of words that maximises the
probability prob(summary|source). Standardly re-
formulating this probability using Bayes? rule re-
sults in the following:
probcm(source|summary)?problm(summary) (5)
In this paper, we are concerned with the first
factor, probcm(source|summary), referred to as the
channel model (CM), which combines both the
buzzword (BWM) and word-pair co-occurrence
(WCM) models. An examination of differences be-
tween the two approaches revealed only a 20% word
overlap on the Jaccard metric.
In order to combine multiple models, we intend
to use machine learning approaches to combine the
information in each model in a similar manner to
Berger et al (1996). We are currently exploring the
use of logistic regression methods to learn a func-
tion that would treat, as features, the probabilities
defined by the salience and schematic content selec-
tion models. Although generation is possible using
each content selection model in isolation, evalua-
tions of the combined model are on-going and are
not presented in this paper.
5 Evaluation
In this evaluation, the task is to select n words from
the aligned source sentences for inclusion in a sum-
mary. As a gold-standard for comparison, we sim-
ply examine what words were actually chosen in the
summary sentence of the aligned sentence tuple. We
are specifically interested in open-class words, and
so a stopword list of closed-class words is used to
filter the sentences in each test case.
We evaluate against the set of open-class words
in the human-authored summary sentence using re-
call and precision metrics. Recall is the size of
the intersection of the selected and gold-standard
sets, normalised by the length of the gold-standard
sentence (in words). This recall metric is similar
to the ROUGE-1 metric, the unigram version of
the ROUGE metric (Lin and Hovy, 2003) used in
the Document Understanding Conferences2 (DUC).
Precision is the size of the intersection normalised
by the number of words selected. We also report the
F-measure, which is the harmonic mean of the recall
and precision scores.
Recall, precision and F-measure are measured at
various values of n ranging from 1 to the number of
open-class words in the gold-standard summary sen-
tence for a particular test case. For the purposes of
evaluation, differences in tokens due to morphology
were explored crudely via the use of Porter?s stem-
ming algorithm. However, the results from stem-
ming are not that different from exact token matches
when examining performance on the entire data set
2http://duc.nist.gov
548
Number of training cases 530
Average words in summary sentence 27.0
Average stopwords in summary sentence 10.3
Average number of auxiliary sentences 2.75
Word count: summary sentences 4630
Word count: source sentences 21356
Word type count in corpus 3800
Table 1: Statistics for the UN CAP training set
and so, for simplicity, these are omitted in this dis-
cussion.
5.1 The Data
The corpus is made up of a number of humanitar-
ian aid proposals called Consolidated Appeals Pro-
cess (UN CAP) documents, which are archived at
the United Nations website.3 135 documents from
the period 2002 to 2007 were downloaded by the au-
thors. A preprocessing stage extracted text from the
PDF files and segmented the documents into execu-
tive summary and source sections. These were then
automatically segmented further into sentences.
Executive summary sentences were manually
aligned by the authors to source key and auxiliary
sentences, producing a corpus of 580 aligned sen-
tence tuples referred to here as the UN CAP cor-
pus. Of these, 230 tuples were paraphrase cases (i.e.
without aligned auxiliary sentences). The remaining
550 cases were instances of sentence augmentation
(with at least one auxiliary sentence).
Of the 580 cases, 50 cases were set aside for test-
ing. The remaining 530 cases were used for train-
ing. Statistics for the training portion of the sentence
augmentation set are provided in Table 1.
In this paper, aligned sentence tuples are obtained
via manual annotation. Automatic construction
of these sentence-level alignments is possible and
has been explored by Jing and McKeown (1999).
We also envisage using tools for scoring sentence
similarity (for example, see Hatzivassiloglou et al
(2001)) for automatically constructing them; this is
the focus of work by Wan and Paris (2008).
3http://ochaonline3.un.org/humanitarianappeal/index.htm
5.2 The Baselines
Three baselines were used in this work: the random,
tf-idf and position baselines. A random word selec-
tor shows what performance might be achieved in
the absence of any linguistic knowledge.
We also sorted all words in the aligned source sen-
tences by their weighted tf-idf scores. This baseline
selects words in order until the desired word limit
is reached. This baseline is referred to as the tf-idf
baseline.
Finally, we selected words based on their sen-
tence order, choosing first those words from the key
sentence. When these are exhausted, auxiliary sen-
tences are sorted by their sentence positions in the
original document. Words from the first auxiliary
sentence are then chosen. This continues until ei-
ther the desired number of words have been chosen,
or no words remain. This baseline is known as the
position baseline.
5.3 Content Selection Results
We compare the three baselines to the two mod-
els presented in Section 3. These are the buzzword
salience model (BWM) and the schematic word-pair
co-occurrence model (WCM).
We begin by presenting recall, precision and F-
measure graphs when selecting from the aligned
source sentences, comprising the key and auxiliary
sentences. Figure 3 shows the results for the two
models against the three baselines. The two mod-
els, the positional, and the tf-idf baselines perform
better than the random baseline, as measured by a
two-tailed Wilcoxon Matched Pairs Signed Ranks
test (? = 0.05).
The WCM consistently out-performs the BWM
on all metrics, and the differences are statistically
significant. In fact, the BWM also generally per-
forms worse than the position and tf-idf baselines.
WCM and the position baseline both significantly
outperform the tf-idf baseline on all metrics for
longer sentence lengths.
That the position baseline and WCM should per-
form similarly is not really surprising since, in ef-
fect, the position baseline first chooses words from
the key sentence and then selects auxiliary words.
The difference essentially lies in how the auxiliary
words are chosen.
549
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0  5  10  15  20  25  30
R
ec
al
l
Number of Open-class Words Selected
WCM
BWM
Position
tf.idf
Random
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0  5  10  15  20  25  30
Pr
ec
is
io
n
Number of Open-class Words Selected
WCM
BWM
Position
tf.idf
Random
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0  5  10  15  20  25  30
Fm
ea
su
re
Number of Open-class Words Selected
WCM
BWM
Position
tf.idf
Random
Figure 3: Recall, Precision and F-measure performance
for open-class words from the entire input set (key and
auxiliary). Models presented are the Buzzword Model
(BWM), the Word-Pair Co-occurrence Model (WCM)
and position, tf-idf and random baselines.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0  5  10  15  20  25  30
Fm
ea
su
re
Number of Open-class Words Selected
WCM
position
Figure 4: F-measure scores for content selection on just
the auxiliary sentences. Models presented are the Word-
Pair Co-occurrence model (WCM) and the position base-
line.
The results of Figure 3 weakly support the
hypothesis that using schematic word-pair co-
occurrences helps improve performance over mod-
els without discourse-related features. The graphs
show that WCM edges above the position base-
line when the number of selected open-class words
ranges from 10 to 15. Note that the average num-
ber of open-class words in a human authored sum-
mary sentence is 16. The only significant difference
found was in the F-measure and precision scores for
19 selected open-class words. Nevertheless, a gen-
eral trend can be observed in which WCM performs
better than the position baseline.
Ultimately, however, what we want to do is select
auxiliary content to supplement the key sentence.
To examine the effect of two best performing ap-
proaches, WCM and the position baseline, on this
task, were both modified so that the key sentence
words were explicitly given a zero probability. Thus,
the recall, precision and F-measure scores obtained
are based solely on the ability of either to select aux-
iliary words. The F-measure scores are presented
Figure 4. WCM consistently outperforms the po-
sition baseline for the selection of auxiliary words.
Differences are significant for 6 or more selected
open-class words.
The results show that even when considering only
exact token matches, we can improve on the re-
call of open-class words, and do so without penalty
in precision. Our working hypothesis is that such
gains are possible because the corpus has a homo-
550
geneous quality and key patterns are sufficiently re-
peated even when the overall data set is of the or-
der of hundreds of cases. The benefit of using a
model encoding some schematic information is fur-
ther shown by the performance of WCM over the
position baseline when selecting words from auxil-
iary sentences.
This is an interesting finding given that do-
main independent methods are increasingly used
on domain-specific corpora such as financial and
biomedical texts, for which we may have access to
only a limited amount of data. We anticipate that as
we introduce methods to account for paraphrase and
synonym differences, performance might rise fur-
ther still.
5.4 Testing Seed Weighting Schemes
We can also weight seed words in the ?Seed and
Grow? approach in a variety of ways. To test
whether weighting schemes have any effect on con-
tent selection performance, we examined the use
of three schemes. We were particularly interested
in those schemes that indicate the contribution of
a seed word to the core meaning of a sentence.
These are the binary, tf-idf and buzzword weight-
ing schemes described in Section 3. We present
the F-measure graph for these three variants of the
schematic word-pair co-occurrence model (WCM)
in Figure 5.
The graphs show that there is no discernible dif-
ference between the seed weighting schemes. No
scheme significantly outperforms another. Thus, we
conclude that the choice of these particular seed
weighting schemes has no effect on performance. In
future work, we intend to examine whether weight-
ing schemes encoding syntactic information might
fare better, since such information might more accu-
rately represent the contribution of a substring to the
main clause of the sentence.
6 Conclusions and Future Work
In this paper, we argued a case for sentence augmen-
tation, a component that facilitates abstract-like text
summarisation. We showed that such a process can
account for summary sentences as authored by pro-
fessional editors. We proposed the use of schemata,
as approximated with a word-pair co-occurrence
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0  5  10  15  20  25  30
Fm
ea
su
re
Number of Open-class Words Selected
binary
tfidf
BWM
Figure 5: F-measure performance for open-class words
from the entire input set (key and auxiliary). Models
presented are variants of the Word-Pair Co-occurrence
Model (WCM) that differ in the seed weighting schemes.
model, and advocated a new schema-based ?Seed
and Grow? content selection model used for statisti-
cal sentence generation.
We also showed that domain-specific patterns,
schematic word-pair co-occurrences in this case, can
be acquired from a limited amount of data as indi-
cated by modest performance gains for content se-
lection using schemata information. We postulate
that this is particularly true when dealing with ho-
mogeneous data.
In future work, we intend to explore other string
matches corresponding to variations due to para-
phrases and synonymy. We would also like to study
the effects of corpus size when learning schematic
patterns. Finally, we are currently investigating the
use of machine learning methods to combine the
best of the Salience and Schemata models in order
to provide a single model for use in decoding.
7 Acknowledgments
We would like to thank the reviewers for their in-
sightful comments. This work was funded by the
CSIRO ICT Centre and Centre for Language Tech-
nology at Macquarie University.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 113?120, Boston,
551
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
James Clarke and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1?11.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL ? 2002), pages 449 ? 456,
Philadelphia, PA, July 6 ? 12.
Hal Daume? III and Daniel Marcu. 2005. Induction
of word and phrase alignments for automatic doc-
ument summarization. Computational Linguistics,
31(4):505?530, December.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
V. Hatzivassiloglou, J. Klavans, M. Holcombe, R. Barzi-
lay, M. Kan, and K. McKeown. 2001. Simfinder: A
flexible clustering tool for summarization. pages 41?
49. Association for Computational Linguistics.
Hongyan Jing and Kathleen McKeown. 1999. The de-
composition of human-written summary sentences. In
Research and Development in Information Retrieval,
pages 129?136.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 545?552, Sapporo, Japan.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In NAACL ?03: Proceedings of the 2003 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 71?78, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Kathleen R McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21?48.
G. Salton and M. J. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
Radu Soricut and Daniel Marcu. 2005. Towards de-
veloping generation algorithms for text-to-text appli-
cations. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 66?74, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Stephen Wan and Ce?cile Paris. 2008. In-browser sum-
marisation: Generating elaborative summaries biased
towards the reading context. In Proceedings of ACL-
08: HLT, Short Papers, pages 129?132, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Stephen Wan, Robert Dale Mark Dras, and Ce?cile Paris.
2005. Towards statistical paraphrase generation: pre-
liminary evaluations of grammaticality. In Proceed-
ings of The 3rd International Workshop on Paraphras-
ing (IWP2005), pages 88?95, Jeju Island, South Korea.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization (poster abstract): a statistical approach
to generating highly condensed non-extractive sum-
maries. In SIGIR ?99: Proceedings of the 22nd annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 315?316,
New York, NY, USA. ACM Press.
552
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 919?928,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Segmenting Email Message Text into Zones
Andrew Lampert ??
?CSIRO ICT Centre
PO Box 76
Epping 1710, Australia
andrew.lampert@csiro.au
Robert Dale ?
rdale@ics.mq.edu.au
Ce?cile Paris ?
?Centre for Language Technology
Macquarie University
North Ryde 2109, Australia
cecile.paris@csiro.au
Abstract
In the early days of email, widely-used
conventions for indicating quoted reply
content and email signatures made it easy
to segment email messages into their func-
tional parts. Today, the explosion of dif-
ferent email formats and styles, coupled
with the ad hoc ways in which people vary
the structure and layout of their messages,
means that simple techniques for identify-
ing quoted replies that used to yield 95%
accuracy now find less than 10% of such
content. In this paper, we describe Zebra,
an SVM-based system for segmenting the
body text of email messages into nine zone
types based on graphic, orthographic and
lexical cues. Zebra performs this task with
an accuracy of 87.01%; when the num-
ber of zones is abstracted to two or three
zone classes, this increases to 93.60% and
91.53% respectively.
1 Introduction
Email message bodies consist of different func-
tional parts such as email signatures, quoted re-
ply content and advertising content. We refer to
these as email zones. Many language process-
ing tools stand to benefit from better knowledge
of this message structure, facilitating focus on rel-
evant content in specific parts of a message. In
particular, access to zone information would al-
low email classification, summarisation and anal-
ysis tools to separate or filter out ?noise? and focus
on the content in specific zones of a message that
are relevant to the application at hand. Email con-
tact mining tools such as that developed by Culotta
et al (2004), for example, might access the email
signature zone, while tools that attempt to iden-
tify tasks or action items in email (e.g., (Bellotti
et al, 2003; Corston-Oliver et al, 2004; Bennett
and Carbonell, 2007; Lampert et al, 2007)) might
restrict themselves to the sender-authored and for-
warded content. Despite previous work on this
problem, there are no available tools that can re-
liably extract or identify the different functional
zones of an email message.
While there is no agreed standard set of email
zones, there are clearly different functional parts
within the body text of email messages. For ex-
ample, the content of an email disclaimer is func-
tionally different from the sender-authored content
and from the quoted reply content automatically
included from previous messages in the thread of
conversation. Of course, there are different dis-
tinctions that can be drawn between zones; in
this paper we explore several different categorisa-
tions based on our proposed set of nine underlying
email zones.
Although we focus on content in the body of
email messages, we recognise the presence of use-
ful information in the semi-structured headers, and
indeed make use of header information such as
sender and recipient names in segmenting the un-
structured body text.
Segmenting email messages into zones is a
challenging task. Accurate segmentation is ham-
pered by the lack of standard syntax used by dif-
ferent email clients to indicate different message
parts, and by the ad hoc ways in which people vary
the structure and layout of their messages. When
replying to a message, for example, it is often use-
ful to include all or part of the original message
that is being replied to. Different email clients in-
dicate quoted material in different ways. By de-
fault, some prefix every line of the quoted message
with a character such as ?>? or ?|?, while others in-
dent the quoted content or insert the quoted mes-
sage unmodified, prefixed by a message header.
Sometimes the new content is above the quoted
content (a style known as ?top-posting?); in other
cases, the new content may appear after the quoted
919
content (bottom-posting) or interleaved with the
quoted content (inline replying). Confounding the
issue further is that users are able to configure their
email client to suit their individual tastes, and can
change both the syntax of quoting and their quot-
ing style (top, bottom or inline replying) on a per-
message basis.
To address these challenges, in this paper we
describe Zebra, our email zone classification sys-
tem. First we describe how Zebra builds and im-
proves on previous work in Section 2. Section 3
then presents our set of email zones, along with
details of the email data we use for system train-
ing and experiments. In Section 4 we describe two
approaches to zone classification, one that is line-
based and one that is fragment-based. The perfor-
mance of Zebra across two, three and nine email
zone classification tasks is presented and analysed
in Section 5.
2 Related Work
Segmenting email messages into zones requires
both text segmentation and text classification. The
main focus of most work on text segmentation
is topic-based segmentation of news text (e.g.,
(Hearst, 1997; Beeferman et al, 1997)), but there
have been some previous attempts at identifying
functional zones in email messages.
Chen et al (1999) looked at both linguistic and
two-dimensional layout cues for extracting struc-
tured content from email signature zones in email
messages. The focus of their work was on extract-
ing information from already identified signature
blocks using a combination of two-dimensional
structural analysis and one-dimensional grammat-
ical constraints; the intended application domain
was as a component in a system for email text-
to-speech rendering. The authors claim that their
system can be modified to also identify signature
blocks within email messages, but their system
performs this task with a recall of only 53%. No
attempt is made to identify functional zones other
than email signatures.
Carvalho and Cohen?s (2004) Jangada system
attempted to identify email signatures within plain
text email messages and to extract email signa-
tures and reply lines. Unfortunately, the 20 News-
groups corpus1 they worked with contains 15-
year-old Usenet messages which are much more
homogeneous in their syntax than contemporary
1http://people.csail.mit.edu/jrennie/20Newsgroups/
email, particularly in terms of how quoted text
from previous messages is indicated. As a result,
using a very simple metric (a line-initial ?>? char-
acter) to identify reply lines achieves more than
95% accuracy. In contrast, this same simple met-
ric applied to the Enron email data we annotated
detects less than 10% of actual reply or forward
lines.
Usenet messages are also markedly different
from contemporary email when it comes to email
signatures. Most Usenet clients produced mes-
sages which conformed to RFC3676 (Gellens,
2004), a standard that formalised a ?long-standing
convention in Usenet news . . . of using two hy-
phens -- as the separator line between the body
and the signature of a message.? Unfortunately,
this convention has long since ceased to be ob-
served in email messages. Carvalho and Cohen?s
email signature detection approach also benefits
greatly from a simplifying assumption that signa-
tures are found in the last 10 lines of an email mes-
sage. While this holds true for their Usenet mes-
sage data, it is no longer the case for contemporary
email.
In attempting to use Carvalho and Cohen?s sys-
tem to identify signature blocks and reply lines
in our own work, we identified similar shortcom-
ings to those noted by Estival et al (2007). In
particular, Jangada did not accurately identify for-
warded or reply content in email data from the
Enron email corpus. We believe that the use of
older Usenet-style messages to train Jangada is a
significant factor in the systematic errors the sys-
tem makes in failing to identify quoted reply, for-
warded and signature content in messages format-
ted in the range of message formats and styles pop-
ularised by Microsoft Outlook. These errors are
a fundamental problem with Jangada, especially
since Outlook is the most common client used to
compose messages in our annotated email collec-
tion drawn from the Enron corpus. More gen-
erally, we note that Outlook is the most popular
email client in current use, with an estimated 350?
400 million users worldwide,2 representing any-
where up to 40% of all email users.3
More recently, as part of their work on profiling
2Xobni Co-founder Adam Smith and former Engi-
neering VP Gabor Cselle have both published Outlook
user statistics. See http://www.xobni.com/asmith/archives/66
and http://gaborcselle.com/blog/2008/05/xobnis-journey-to-
right-product.html.
3http://www.campaignmonitor.com/stats/email-clients/
920
authors of email messages, Estival et al (2007)
classified email bodies into five email zones. Their
paper does not provide results for five-zone classi-
fication, but they report accuracy of 88.16% using
a CRF classifier to distinguish three zones: reply,
author and signature. We use their classification
scheme as the starting point for our own set of
email zones.
3 Email Zones
As noted earlier, we refer to the different func-
tional components of email messages as email
zones. The zones we propose refine and extend
the five categories ? Author Text, Signature, Ad-
vertisement (automatically appended advertising),
Quoted Text (extended quotations such as song
lyrics or poems), and Reply Lines (including for-
warded and reply text) ? identified by Estival et
al. (2007).
We consider that each line of text in the body
of an email message belongs to one of nine more
fine-grained email zones. We intend our nine
email zones to be abstracted and adapted to suit
different tasks. To illustrate, we present the
zones below abstracted into three classes: sender-
authored content, boilerplate content, and content
quoted from other conversations. This is the zone
partition we use to generate the three-zone results
reported in Section 5. This categorisation is use-
ful for problems such as finding action items in
email messages: such detection tools would look
in text from the sender-authored message zones
for new action item information, and could also
look in quoted conversation content to link new
action item information (such as reported comple-
tions) to previous action item content.
Our nine email zones can also be reduced to a
binary scheme to distinguish text authored by the
sender from text authored by others. This distinc-
tion is useful for problems such as author attribu-
tion or profiling tasks. In this two-class case, the
sender-authored zones would be Author, Greeting,
Signoff and Signature, while the other-authored
zones would be Reply, Forward, Disclaimer, Ad-
vertising and Attachment. This is the partition
of zones we use in our two-zone experiments re-
ported in Section 5.
3.1 Sender Zones
Sender zones contain text written by the current
email sender. The Greeting and Signoff zones are
sub-zones of the Author zone, usually appearing
as the first and last items respectively in the Author
zone. Thus, our proposed sender zones are:
1. Author: New content from the current email
sender. This specifically excludes any text
authored by the sender that is included from
previous messages.
2. Greeting: Terms of address and recipient
names at the beginning of a message (e.g.,
Dear/Hi/Hey Noam).
3. Signoff: The message closing (e.g.,
Thanks/Cheers/Regards, John).
3.2 Quoted Conversation Zones
Quoted conversation zones include both content
quoted in reply to previous messages in the same
conversation thread and forwarded content from
other conversations.4 Our quoted conversation
zones are:
4. Reply: Content quoted from a previous mes-
sage in the same conversation thread, includ-
ing any embedded signatures, attachments,
advertising, disclaimers, author content and
forwarded content. Content in a reply content
zone may include previously sent content au-
thored by the current sender.
5. Forward: Content from an email message
outside the current conversation thread that
has been forwarded by the current email
sender, including any embedded signatures,
attachments, advertising, disclaimers, author
content and reply content.
3.3 Boilerplate Zones
Boilerplate zones contain content that is reused
without modification across multiple email mes-
sages. Our proposed boilerplate zones are:
6. Signature: Content containing contact or
other information that is automatically in-
serted in a message. In contrast to disclaimer
or advertising content, signature content is
usually templated content written once by
the email author, and automatically or semi-
automatically included in email messages. A
4Although we recognise the need for the Quoted Text zone
proposed by Estival et al (2007), no such data occurs in our
collection of annotated email messages. We therefore omit
this zone from our current set.
921
user may also use a Signature in place of a
Signoff; in such cases, we still mark the text
as a Signature.
7. Advertising: Advertising material in an
email message. Such material often appears
at the end of a message (e.g., Do you Ya-
hoo!?), but may also appear prefixed or in-
line with the content of the message, (e.g., in
sponsored mailing lists).
8. Disclaimer: Legal disclaimers and privacy
statements, often automatically appended.
9. Attachment: Automated text indicating or
referring to attached documents, such as that
shown in line 16 of Figure 1. Note that this
zone does not apply to manually authored ref-
erence to attachments, nor to the actual con-
tent of attachments (which we do not clas-
sify).
3.4 Email Data and Annotation
The training data for our zone classifier consists of
11881 annotated lines from almost 400 email mes-
sages drawn at random from the Enron email cor-
pus (Klimt and Yang, 2004).5 We use the database
dump of the corpus released by Andrew Fiore and
Jeff Heer.6 This version of the corpus has been
processed to remove duplicate messages and to
normalise sender and recipient names, resulting in
just over 250,000 email messages. No attachments
are included. Following Estival et al (2007), we
used only a single annotator since the task revealed
itself to be relatively uncontroversial. Each line in
the body text of selected messages was marked by
the annotator (one of the authors) as belonging to
one of the nine zones. After removing blank lines,
which we do not attempt to classify, we are left
with 7922 annotated lines as training data for Ze-
bra. The frequency of each zone within this anno-
tated dataset is shown in Table 3.
Figure 1 shows an example of an email mes-
sage with each line annotated with the appropriate
email zone. Two zone annotations are shown for
each line (in separate columns), one using the nine
fine-grained zones and the second using the ab-
stracted three-zone scheme described in Section 3.
Note, however, that not all of the nine fine-grained
5This annotated dataset is available from
http://zebra.thoughtlets.org/.
6http://bailando.sims.berkeley.edu/enron/enron.sql.gz
zones, nor all of the three abstracted zones, are ac-
tually present in this particular message.
4 Zone Segmentation and Classification
Our email zone classification system is based
around an SVM classifier using features that cap-
ture graphic, orthographic and lexical information
about the content of an email message.
To classify the zones in an email message, we
experimented with two approaches. The first em-
ploys a two-stage approach that segments a mes-
sage into zone fragments and then classifies those
fragments. Our second method simply classifies
lines independently, returning a classification for
each non-blank line in an email message. Our hy-
pothesis was that classifying larger text fragments
would lead to better performance due to the text
fragments containing more cues about the zone
type.
4.1 Zone Fragment Classification
Zone fragment classification is a two-step process.
First it predicts the zone boundaries using a simple
heuristic, then it classifies the resulting zone frag-
ments, the sets of content lines that lie between
these hypothesised boundaries.
In order to determine how well we can detect
zone boundaries, we first need to establish the cor-
rect zone boundaries in our collection of zone-
annotated email messages.
4.1.1 Zone Boundaries
A zone boundary is defined as a continuous collec-
tion of one or more lines that separate two differ-
ent email zones. Lines that separate two zones and
are blank, contain only whitespace or contain only
punctuation characters are called buffer lines.
Since classification of blank lines between
zones is often ambiguous, empty or whitespace-
only buffer lines are not included as content in any
zone, and thus are not classified. Instead, they are
treated as strictly part of the zone boundary. In
Figure 1, these lines are shown without any zone
annotation. Zone boundary lines that are included
as content in a zone have their zone annotation
styled in bold and underlined. The important point
here is that zone boundaries are specific to a zone
classification scheme. For nine-zone classifica-
tion of the message in Figure 1, there are six zone
boundaries: line 2, lines 10?11, line 12, line 15,
lines 17?20, and lines 30?33. For three-zone clas-
922
Figure 1: An example email message marked with both nine- and three-zone annotations.
sification, the only zone boundary consists of line
12, separating the sender and boilerplate zones.
Based on these definitions, there are three dif-
ferent types of zone boundaries:
1. Blank boundaries contain only empty or
whitespace-only buffer lines. Lines in these
zone boundaries are strictly separate from the
zone content. An example is Line 12 in Fig-
ure 1, for both the three- and nine-zone clas-
sification.
2. Separator boundaries contain only
buffer lines, but must contain at least
one punctuation-character buffer line that is
retained as content in one or both zones. In
Figure 1, an example is the zone boundary
containing lines 17?20 that separates the
Attachment and Disclaimer zones for nine-
zone classification, since line 20 is retained
as part of the Disclaimer zone content.
3. Adjoining boundaries consist of the last
content line of the earlier zone and the first
content line of the following zone. These
boundaries occur where no buffer lines ex-
ist between the two zones. An example is
the zone boundary containing lines 10 and 11
that separates the Author and Signoff zones in
Figure 1 for nine-zone classification.
923
4.1.2 Hypothesising Zone Boundaries
To identify zone boundaries in unannotated email
data, we employ a very simple heuristic approach.
Specifically, we consider every line in the body of
an email message that matches any of the follow-
ing criteria to be a zone boundary:
1. A blank line;
2. A line containing only whitespace; or
3. A line beginning with four or more repeated
punctuation characters, optionally prefixed
by whitespace.
Our efforts to apply more sophisticated
machine-learning techniques to identifying zone
boundaries could not match the 90.15% recall
achieved by this simple heuristic. The boundaries
missed by the simple heuristic are all adjoining
boundaries, where two zones are not separated
by any buffer lines. An example of a boundary
that is not detected by our heuristic is the zone
boundary between the Author and Signoff zones
in Figure 1 formed by lines 10 and 11.
Obviously, our simple boundary heuristic de-
tects actual boundaries as well as spurious
boundaries that do not actually separate differ-
ent email zones. Unsurprisingly, the number of
spurious boundaries is large. The precision of
our simple heuristic across our annotated set of
email messages is 22.5%, meaning that less than
1 in 4 hypothesised zone boundaries is an actual
boundary. The underlying email zones average
more than 12 lines in length, including just over
8 lines of non-blank content. Due to the num-
ber of spurious boundaries, fragments contain less
than half this amount ? approximately 3 lines of
non-blank content on average. One of the most
common types of spurious boundaries detected are
the blank lines that frequently separate paragraphs
within a single zone.
For three-zone classification, the set of pre-
dicted boundaries remains the same, but there are
less actual boundaries to find, so recall increases to
96.3%. However, because many boundaries from
the nine-zone classification are not boundaries for
the three-zone classification, precision decreases
to 14.7%.
4.1.3 Classifying Zone Fragments
Having segmented the email message into candi-
date zone fragments, we classify these fragments
using the SMO implementation provided by Weka
(Witten and Frank, 2005) with the features de-
scribed in Section 4.3.
Although our boundary detection heuristic has
better than 90% recall, the small number of ac-
tual boundaries that are not detected result in some
zone fragments containing lines from more than
one underlying email zone. In these cases, we con-
sider the mode of all annotation values for lines
in the fragment (i.e., the most frequent zone an-
notation) to be the gold-standard zone type for
the fragment. This, of course, may mean that we
somewhat unfairly penalise the accuracy of our au-
tomated classification when Zebra detects a zone
that is indeed present in the fragment, but is not
the most frequent zone.
4.2 Line Classification
Our line-based classification approach simply ex-
tracts all non-blank lines from an email message
and classifies lines one-by-one, using the same
features as for fragment-based classification. This
approach is the same as the signature and reply
line classification approach used by Carvalho and
Cohen (2004).
4.3 Classification Features
We use a variety of graphic, orthographic and lex-
ical features for classification in Zebra. The same
features are applied in both the line-based and the
fragment-based zone classification (to either indi-
vidual lines or zone fragments). In the description
of our features, we refer to both single lines and
zone fragments (collections of contiguous lines) as
text fragments.
4.3.1 Graphic Features
Our graphic features capture information about the
presentation and layout of text in an email mes-
sage, independent of the actual words used. This
information is a crucial source of information for
identifying zones. Such information includes how
the text is organised and ordered, as well as the
?shape? of the text. The specific features we em-
ploy are:
? the number of words in the text fragment;
? the number of Unicode code points (i.e.,
characters) in the text fragment;
? the start position of the text fragment (equal
to one for the first line in the message, two for
the second line and increasing monotonically
924
through the message; we also normalise the
result for message length);
? the end position of the text fragment (calcu-
lated as above and again normalised for mes-
sage length);
? the average line length (in characters) within
the text fragment (equal to the line length for
line-based text fragments);
? the length of the text fragment (in characters)
relative to the previous fragment;
? the length of the text fragment (in characters)
relative to the following fragment;
? the number of blank lines preceding the text
fragment; and
? the number of blank lines following the text
fragment.
4.3.2 Orthographic Features
Our orthographic features capture information
about the use of distinctive characters or charac-
ter sequences including punctuation, capital let-
ters and numbers. Like our graphic features, or-
thographic features tend to be independent of the
words used in an email message. The specific or-
thographic features we employ include:
? whether all lines start with the same character
(e.g., ?>?);
? whether a prior text fragment in the message
contains a quoted header;
? whether a prior text fragment in the message
contains repeated punctuation characters;
? whether the text fragment contains a URL;
? whether the text fragment contains an email
address;
? whether the text fragment contains a se-
quence of four or more digits;
? the number of capitalised words in the text
fragment;
? the percentage of capitalised words in the text
fragment;
? the number of non-alpha-numeric characters
in the text fragment;
? the percentage of non-alpha-numeric charac-
ters in the text fragment;
? the number of numeric characters in the text
fragment;
? the percentage of numeric characters in the
text fragment;
? whether the message subject line contains a
reply syntax marker such as Re: ; and
? whether the message subject line contains a
forward syntax marker such as Fw:.
4.3.3 Lexical Features
Finally, our lexical features capture information
about the words used in the email text. We use
unigrams to capture information about the vocab-
ulary and word bigram features to capture short
range word order information. More specifically,
the lexical features we apply to each text fragment
include:
? each word unigram, calculated with a mini-
mum frequency threshold cutoff of three, rep-
resented as a separate binary feature;
? each word bigram, calculated with a mini-
mum frequency threshold cutoff of three, rep-
resented as a separate binary feature;
? whether the text fragment contains the
sender?s name;
? whether a prior text fragment in the message
contains the sender?s name;
? whether the text fragment contains the
sender?s initials; and
? whether the text fragment contains a recipi-
ent?s name.
Features that look for instances of sender or recip-
ient names are less likely to be specific to a par-
ticular business or email domain. These features
use regular expressions to find name occurrences,
based on semi-structured information in the email
message headers. First, we extract and normalise
the names from the email headers to identify the
relevant person?s given name and surname. Our
features then capture whether one or both of the
given name or surname are present in the current
text fragment. Features which detect user initials
make use of the same name normalisation code to
retrieve a canonical form of the user?s name, from
which their initials are derived.
5 Results and Discussion
Table 1 shows Zebra?s accuracy in classifying
email zones. The results are calculated using 10-
fold cross-validation. Accuracy is shown for three
tasks ? nine-, three- and two-zone classification
? using both line and zone-fragment classifica-
tion. Performance is compared against a majority
class baseline in each case.
Zebra?s performance compares favourably with
previously published results. While it is difficult to
925
2 Zones 3 Zones 9 Zones
Zebra Baseline Zebra Baseline Zebra Baseline
Lines 93.60% 61.14% 91.53% 58.55% 87.01% 30.94%
Fragments 92.09% 62.18% 91.37% 59.44% 86.45% 30.36%
Table 1: Classification accuracy compared against a majority baseline
2 Zones 3 Zones 9 Zones
Zebra Baseline Zebra Baseline Zebra Baseline
Lines 90.62% 61.14% 86.56% 58.55% 81.05% 30.94%
Fragments 91.14% 62.18% 89.44% 59.44% 82.55% 30.36%
Table 2: Classification accuracy, without word n-gram features, compared against a majority baseline
directly compare, since not all systems are freely
available and they are not trained or tested over the
same data, our three-zone classification (identify-
ing sender, boilerplate and quoted reply content) is
very similar to the three-zone task for which (Es-
tival et al, 2007) report 88.16% accuracy for their
system and 64.22% accuracy using Carvalho and
Cohen?s Jangada system. Zebra outperforms both,
achieving 91.53% accuracy using a line-based ap-
proach. In the two-zone task, where we attempt
to identify sender-authored lines, Zebra achieves
93.60% accuracy and an F-measure of 0.918, ex-
ceeding the 0.907 F-measure reported for Estival
et al?s system tuned for exactly this task.
Interestingly, the line-based approach provides
slightly better performance than the fragment-
based approach for each of the two-zone, three-
zone and nine-zone classification tasks. As noted
earlier, our original hypothesis was that zone frag-
ments would contain more information about the
sequence and text shape of the original message,
and that this would lead to better performance for
fragment-based classification.
When we restrict our feature set to those that
look only at the text of the line or zone fragment,
the fragment-based approach does perform better
than the line-based one. Using only word uni-
gram features, for example, our fragment classi-
fier achieves 78.7% accuracy. Using the same fea-
tures, the line-based classifier achieves only 57.5%
accuracy. When we add further features that cap-
ture sequence and shape information from outside
the text fragment being classified (e.g., the length
of a text segment compared to the text segment
before and after, and whether a segment occurs
after another segment containing repeated punc-
tuation or the sender?s name), the line-based ap-
proach achieves a greater increase in accuracy than
the fragment-based approach. This presumably is
because individual lines intrinsically have less in-
formation about the message context, and so ben-
efit more from the information added by the new
features.
We also experimented with removing all word
unigram and bigram features to explore the classi-
fier?s portability across different domains. This re-
moved all vocabulary and word order information
from our feature set. In doing so, our feature set
was reduced to less than thirty features, consist-
ing of mostly graphic and orthographic informa-
tion. The few remaining lexical features captured
only the presence of sender and recipient names,
which are independent of any particular email do-
main. As expected, performance did drop, but not
dramatically. Table 2 shows that average perfor-
mance without n-grams (across two-, three- and
nine-zone tasks) for line-based classification drops
by 4.67%. In contrast, fragment-based classifica-
tion accuracy drops by less than half this amount
? an average of 2.26%. This suggests that, as we
originally hypothesised, there are additional non-
lexical cues in zone fragments that give informa-
tion about the zone type. This makes the zone
fragment approach potentially more portable for
use across email data from different enterprise do-
mains.
Of course, classification accuracy gives only a
limited picture of Zebra?s performance. Table 4
shows precision and recall results for each zone in
the nine-zone line-based classification task. Per-
926
Total Author Signature Disclaim Advert Greet Signoff Reply Fwd Attach
Author 2415 2197 56 9 4 14 31 43 53 8
Signature 383 93 203 4 0 0 20 28 31 4
Disclaim 97 30 4 52 0 0 0 2 9 0
Advert 83 47 1 1 20 0 0 7 7 0
Greet 85 8 0 0 0 74 2 0 1 0
Signoff 195 30 5 0 0 0 147 11 2 0
Reply 2451 49 10 3 2 1 10 2222 154 0
Fwd 2187 72 13 7 8 1 3 125 1958 0
Attach 26 4 0 0 0 0 0 1 1 20
Table 3: Confusion Matrix for 9 Zone Line Classification
formance clearly varies significantly across the
different zones. For Author, Greeting, Reply and
Forward zones, performance is good, with F-
measure > 0.8. This is encouraging, given that
many email tools, such as action-item detection
and email summarisation would benefit from an
ability to separate author content from reply con-
tent and forwarded content. The Advertising, Sig-
nature and Disclaimer zones show the poorest per-
formance, particularly in terms of Recall. The
Advertising and Disclaimer zones are almost cer-
tainly hindered by a lack of training data; they are
two of the smallest zones in terms of number of
lines of training data. The relatively poor Signa-
ture class performance is more interesting. Given
the potential confusion between Signoff content
and Signatures that function as Signoffs, one might
expect confusion between Signoff and Signature
zones, but Table 3 shows this is not the case.
Instead, there is significant confusion between
Signature and Author content, with almost 25%
of Signature lines misclassified as Author lines.
When word n-grams are removed from the fea-
ture set, the number of these misclassifications in-
creases to almost 50%. These results reinforce our
observation that the task of email signature extrac-
tion is much more difficult that it was in the days
of Usenet messages.
6 Conclusion
Identifying functional zones in email messages is
a challenging task, due in large part to the diver-
sity in syntax used by different email software, and
the dynamic manner in which people employ dif-
ferent styles in authoring email messages. Zebra,
our system for segmenting and classifying email
message text into functional zones, achieves per-
Zone Precision Recall F-Measure
Author 0.868 0.910 0.889
Signature 0.695 0.530 0.601
Disclaimer 0.684 0.536 0.601
Advertising 0.588 0.241 0.342
Greeting 0.822 0.871 0.846
Signoff 0.690 0.754 0.721
Reply 0.911 0.907 0.909
Forward 0.884 0.895 0.889
Attachment 0.625 0.769 0.690
Table 4: Precision and recall for nine-zone line
classification
formance that exceeds comparable systems, and
that is at a level to be practically useful to email
researchers and system builders. In addition to re-
leasing our annotated email dataset, the Zebra sys-
tem will also be available for others to use7.
Because we employ a non-sequential learn-
ing algorithm, we encode sequence information
into the feature set. In future work, we plan
to determine the effectiveness of using a sequen-
tial learning algorithm like Conditional Random
Fields (CRF). We note, however, that Carvalho
and Cohen (2004) demonstrate that using a non-
sequential learning algorithm with sequential fea-
tures, as we do, has the potential to meet or exceed
the performance of sequential learning algorithms.
Acknowledgments
The authors are grateful to the anonymous review-
ers for their insightful comments and suggestions.
7See http://zebra.thoughtlets.org for access to the anno-
tated data and Zebra system
927
References
Douglas Beeferman, Adam Berger, and John Lafferty.
1997. Text segmentation using exponential models.
In Proceedings of the 2nd Conference on Empiri-
cal Methods in Natural Language Processing, pages
35?46, Providence, RI.
Victoria Bellotti, Nicolas Ducheneaut, Mark Howard,
and Ian Smith. 2003. Taking email to task: The
design and evaluation of a task management centred
email tool. In Computer Human Interaction Confer-
ence, CHI, pages 345?352, Ft Lauderdale, Florida,
USA, April 5-10.
Paul N Bennett and Jaime G Carbonell. 2007. Com-
bining probability-based rankers for action-item de-
tection. In Proceedings of NAACL HLT 2007, pages
324?331, Rochester, NY, April.
Vitor R Carvalho and William W Cohen. 2004. Learn-
ing to extract signature reply lines from email. In
Proceedings of First Conference on Email and Anti-
Spam (CEAS), Mountain View, CA, July 30-31.
Hao Chen, Jianying Hu, and Richard W Sproat. 1999.
Integrating geometrical and linguistic analysis for
email signature block parsing. ACM Transactions
on Information Systems, 17(4):343?366, October.
ISSN: 1046-8188.
Simon H. Corston-Oliver, Eric Ringger, Michael Ga-
mon, and Richard Campbell. 2004. Task-focused
summarization of email. In ACL-04 Workshop: Text
Summarization Branches Out, pages 43?50, July.
Aron Culotta, Ron Bekkerman, and Andrew McCal-
lum. 2004. Extracting social networks and contact
information from email and the web. In Proceedings
of the Conference on Email and Anti-Spam (CEAS).
Dominique Estival, Tanja Gaustad, Son Bao Pham,
Will Radford, and Ben Hutchinson. 2007. Author
profiling for English emails. In Proceedings of the
10th Conference of the Pacific Association for Com-
putational Linguistics, pages 263?272, Melbourne,
Australia, Sept 19-21.
R. Gellens. 2004. RFC3676: The text/plain format and
delsp parameters, February.
Marti A. Hearst. 1997. Texttiling: Segmenting text
into multi-paragraph subtopic passages. Computa-
tional Linguistics, 23(1):33?64.
Bryan Klimt and Yiming Yang. 2004. Introducing the
Enron corpus. In Proceedings of the Conference on
Email and Anti-Spam (CEAS).
Andrew Lampert, Ce?cile Paris, and Robert Dale. 2007.
Can requests-for-action and commitments-to-act be
reliably identified in email messages? In Proceed-
ings of the 12th Australasian Document Comput-
ing Symposium, pages 48?55, Melbourne, Australia,
December 10.
Ian Witten and Eiba Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Mor-
gan Kaufmann, San Francisco, 2nd edition.
928
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 852?860,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Improving Grammaticality in Statistical Sentence Generation:
Introducing a Dependency Spanning Tree Algorithm with an Argument
Satisfaction Model
Stephen Wan?? Mark Dras? Robert Dale?
?Centre for Language Technology
Department of Computing
Macquarie University
Sydney, NSW 2113
swan,madras,rdale@ics.mq.edu.au
Ce?cile Paris?
?ICT Centre
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
Abstract-like text summarisation requires
a means of producing novel summary sen-
tences. In order to improve the grammati-
cality of the generated sentence, we model
a global (sentence) level syntactic struc-
ture. We couch statistical sentence genera-
tion as a spanning tree problem in order to
search for the best dependency tree span-
ning a set of chosen words. We also intro-
duce a new search algorithm for this task
that models argument satisfaction to im-
prove the linguistic validity of the gener-
ated tree. We treat the allocation of modi-
fiers to heads as a weighted bipartite graph
matching (or assignment) problem, a well
studied problem in graph theory. Using
BLEU to measure performance on a string
regeneration task, we found an improve-
ment, illustrating the benefit of the span-
ning tree approach armed with an argu-
ment satisfaction model.
1 Introduction
Research in statistical novel sentence generation
has the potential to extend the current capabili-
ties of automatic text summarisation technology,
moving from sentence extraction to abstract-like
summarisation. In this paper, we describe a new
algorithm that improves upon the grammaticality
of statistically generated sentences, evaluated on a
string regeneration task, which was first proposed
as a surrogate for a grammaticality test by Ban-
galore et al (2000). In this task, a system must
regenerate the original sentence which has had its
word order scrambled.
As an evaluation task, string regeneration re-
flects the issues that challenge the sentence gen-
eration components of machine translation, para-
phrase generation, and summarisation systems
(Soricut and Marcu, 2005). Our research in sum-
marisation utilises the statistical generation algo-
rithms described in this paper to generate novel
summary sentences.
The goal of the string regeneration task is to re-
cover a sentence once its words have been ran-
domly ordered. Similarly, for a text-to-text gen-
eration scenario, the goal is to generate a sen-
tence given an unordered list of words, typically
using an n-gram language model to select the best
word ordering. N-gram language models appear
to do well at a local level when examining word
sequences smaller than n. However, beyond this
window size, the sequence is often ungrammati-
cal. This is not surprising as these methods are un-
able to model grammaticality at the sentence level,
unless the size of n is sufficiently large. In prac-
tice, the lack of sufficient training data means that
n is often smaller than the average sentence length.
Even if data exists, increasing the size of n corre-
sponds to a higher degree polynomial complexity
search for the best word sequence.
In response, we introduce an algorithm for
searching for the best word sequence in a way
that attempts to model grammaticality at the sen-
tence level. Mirroring the use of spanning tree al-
gorithms in parsing (McDonald et al, 2005), we
present an approach to statistical sentence genera-
tion. Given a set of scrambled words, the approach
searches for the most probable dependency tree, as
defined by some corpus, such that it contains each
word of the input set. The tree is then traversed to
obtain the final word ordering.
In particular, we present two spanning tree al-
gorithms. We first adapt the Chu-Liu-Edmonds
(CLE) algorithm (see Chu and Liu (1965) and Ed-
monds (1967)), used in McDonald et al (2005),
to include a basic argument model, added to keep
track of linear precedence between heads and
modifiers. While our adapted version of the CLE
algorithm finds an optimal spanning tree, this does
852
not always correspond with a linguistically valid
dependency tree, primarily because it does not at-
tempt to ensure that words in the tree have plausi-
ble numbers of arguments.
We propose an alternative dependency-
spanning tree algorithm which uses a more
fine-grained argument model representing argu-
ment positions. To find the best modifiers for
argument positions, we treat the attachment of
edges to the spanning tree as a weighted bipartite
graph matching problem (or the assignment
problem), a standard problem in graph theory.
The remainder of this paper is as follows. Sec-
tion 2 outlines the graph representation of the
spanning tree problem. We describe a standard
spanning tree algorithm in Section 3. Section 4 de-
fines a finer-grained argument model and presents
a new dependency spanning tree search algorithm.
We experiment to determine whether a global de-
pendency structure, as found by our algorithm,
improves performance on the string regeneration
problem, presenting results in Section 5. Related
work is presented in Section 6. Section 7 con-
cludes that an argument model improves the lin-
guistic plausibility of the generated trees, thus im-
proving grammaticality in text generation.
2 A Graph Representation of
Dependencies
In couching statistical generation as a spanning
tree problem, this work is the generation analog
of the parsing work by McDonald et al (2005).
Given a bag of words with no additional con-
straints, the aim is to produce a dependency tree
containing the given words. Informally, as all de-
pendency relations between each pair of words are
possible, the set of all possible dependencies can
be represented as a graph, as noted by McDon-
ald et al (2005). Our goal is to find the subset of
these edges corresponding to a tree with maximum
probability such that each vertex in the graph is
visited once, thus including each word once. The
resulting tree is a spanning tree, an acyclic graph
which spans all vertices. The best tree is the one
with an optimal overall score. We use negative log
probabilities so that edge weights will correspond
to costs. The overall score is the sum of the costs
of the edges in the spanning tree, which we want
to minimise. Hence, our problem is the minimum
spanning tree (MST) problem.
We define a directed graph (digraph) in a stan-
dard way, G = (V,E) where V is a set of vertices
and E ? {(u, v)|u, v ? V } is a set of directed
edges. For each sentence w = w1 . . . wn, we de-
fine the digraph Gw = (Vw, Ew) where Vw =
{w0, w1, . . . , wn}, with w0 a dummy root vertex,
and Ew = {(u, v)|u ? Vw, v ? Vw \ {w0}}.
The graph is fully connected (except for the root
vertex w0 which is only fully connected outwards)
and is a representation of possible dependencies.
For an edge (u, v), we refer to u as the head and v
as the modifier.
We extend the original formulation of McDon-
ald et al (2005) by adding a notion of argument
positions for a word, providing points to attach
modifiers. Adopting an approach similar to John-
son (2007), we look at the direction (left or right)
of the head with respect to the modifier; we con-
sequently define a set D = {l, r} to represent
this. Set D represents the linear precedence of the
words in the dependency relation; consequently,
it partially approximates the distinction between
syntactic roles like subject and object.
Each edge has a pair of associated weights, one
for each direction, defined by the function s :
E?D ? R, based on a probabilistic model of de-
pendency relations. To calculate the edge weights,
we adapt the definition of Collins (1996) to use di-
rection rather than relation type (represented in the
original as triples of non-terminals). Given a cor-
pus, for some edge e = (u, v) ? E and direction
d ? D, we calculate the edge weight as:
s((u, v), d) = ?log probdep(u, v, d) (1)
We define the set of part-of-speech (PoS) tags P
and a function pos : V ? P , which maps vertices
(representing words) to their PoS, to calculate the
probability of a dependency relation, defined as:
probdep(u, v, d)
= cnt((u, pos(u)), (v, pos(v)), d)
co-occurs((u, pos(u)), (v, pos(v))) (2)
where cnt((u, pos(u)), (v, pos(v)), d) is the num-
ber of times where (v, pos(v)) and (u, pos(u))
are seen in a sentence in the training data, and
(v, pos(v)) modifies (u, pos(u)) in direction d.
The function co-occurs((u, pos(u)), (v, pos(v)))
returns the number of times that (v, pos(v)) and
(u, pos(u)) are seen in a sentence in the training
data. We adopt the same smoothing strategy as
Collins (1996), which backs off to PoS for unseen
dependency events.
853
3 Generation via Spanning Trees
3.1 The Chu-Liu Edmonds Algorithm
Given the graph Gw = (Vw, Ew), the Chu-Liu
Edmonds (CLE) algorithm finds a rooted directed
spanning tree, specified by Tw, which is an acyclic
set of edges in Ew minimising
?
e?Tw,d?D s(e, d).
The algorithm is presented as Algorithm 1.1
There are two stages to the algorithm. The first
stage finds the best edge for each vertex, connect-
ing it to another vertex. To do so, all outgoing
edges of v, that is edges where v is a modifier, are
considered, and the one with the best edge weight
is chosen, where best is defined as the smallest
cost. This minimisation step is used to ensure that
each modifier has only one head.
If the chosen edges Tw produce a strongly con-
nected subgraph Gmw = (Vw, Tw), then this is the
MST. If not, a cycle amongst some subset of Vw
must be handled in the second stage. Essentially,
one edge in the cycle is removed to produce a sub-
tree. This is done by finding the best edge to join
some vertex in the cycle to the main tree. This has
the effect of finding an alternative head for some
word in the cycle. The edge to the original head
is discarded (to maintain one head per modifier),
turning the cycle into a subtree. When all cycles
have been handled, applying a greedy edge selec-
tion once more will then yield the MST.
3.2 Generating a Word Sequence
Once the tree has been generated, all that remains
is to obtain an ordering of words based upon it.
Because dependency relations in the tree are either
of leftward or rightward direction, it becomes rel-
atively trivial to order child vertices with respect
to a parent vertex. The only difficulty lies in find-
ing a relative ordering for the leftward (to the par-
ent) children, and similarly for the rightward (to
the parent) children.
We traverse Gmw using a greedy algorithm to or-
der the siblings using an n-gram language model.
Algorithm 2 describes the traversal in pseudo-
code. The generated sentence is obtained by call-
ing the algorithm with w0 and Tw as parameters.
The algorithm operates recursively if called on an
1Adapted from (McDonald et al, 2005) and
http://www.ce.rit.edu/? sjyeec/dmst.html . The dif-
ference concerns the direction of the edge and the edge
weight function. We have also folded the function ?contract?
in McDonald et al (2005) into the main algorithm. Again
following that work, we treat the function s as a data
structure permitting storage of updated edge weights.
/* initialisation */
Discard the edges exiting the w0 if any.1
/* Chu-Liu/Edmonds Algorithm */
begin2
Tw ? (u, v) ? E : ?v?V,d?Darg min
(u,v)
s((u, v), d)
3
if Mw = (Vw, Tw) has no cycles then return Mw4
forall C ? Tw : C is a cycle in Mw do5
(e, d)? arg min
e?,d?
s(e?, d?) : e ? C
6
forall c = (vh, vm, ) ? C and dc ? D do7
forall e? = (vi, vm) ? E and d? ? D do8
s(e?, d?)? s(e?, d?)? s(c, dc)? s(e, d)9
end10
end11
s(e, d)? s(e, d) + 112
end13
Tw ? (u, v) ? E : ?v?V,d?Darg min
(u,v)
s((u, v), d)
14
return Mw15
end16
Algorithm 1: The pseudo-code for the Chu-Liu
Edmonds algorithm with our adaptation to in-
clude linear precedence.
inner node. If a vertex v is a leaf in the dependency
tree, its string realisation realise(v) is returned.
We keep track of ordered siblings with two lists,
one for each direction. If the sibling set is left-
wards, the ordered list, Rl, is initialised to be the
singleton set containing a dummy start token with
an empty realisation. If the sibling set is right-
wards then the ordered list, Rr is initialised to be
the realisation of the parent.
For some sibling set C ? Vw to be ordered, the
algorithm chooses the next vertex, v ? C, to insert
into the appropriate ordered list, Rx, x ? D, by
maximising the probability of the string of words
that would result if the realisation, realise(v), were
concatenated with Rx.
The probability of the concatenation is calcu-
lated based on a window of words around the join.
This window length is defined to be 2?floor(n/2),
for some n, in this case, 4.
If the siblings are leftwards, the window con-
sists of the last min(n ? 1, |Rl|) previously cho-
sen words concatenated with the first min(n ?
1, |realise(v)|). If the siblings are rightwards, the
window consists of the last min(n?1, |realise(v)|)
previously chosen words concatenated with the
first min(n ? 1, |Rr|). The probability of a win-
dow of words, w0 . . . wj , of length j+1 is defined
by the following equation:
probLMO(w0 . . . wj)
=
j?k?1
?
i=0
probMLE(wi+k|wi . . . wi+k?1)
(3)
854
/* LMO Algorithm */
input : v, Tw where v ? Vw
output: R ? Vw
begin1
if isLeaf(v) then2
return {realise(v)}3
end4
else5
Cl ? getLeftChildren(v, Tw)6
Cr ? getRightChildren(v, Tw)7
Rl ? {start}8
Rr ? {realise(v)}9
while Cl 6= {} do10
c? arg max
c?Cl
probngram(LMO(c, Tw) ? Rl)11
Rl ? realise(c, Tw) ? Rl12
Cl ? Cl \ {c}13
end14
while Cr 6= {} do15
c? arg max
c?Cr
probngram(Rr ? LMO(c, Tw))16
Rr ? Rr ? realise(c, Tw)17
Cr ? Cr \ {c}18
end19
return Rl ? Rr20
end21
end22
Algorithm 2: The Language Model Ordering al-
gorithm for linearising an Tw.
where k = min(n? 1, j ? 1), and,
probMLE(wi+k|wi . . . wi+k?1)
= cnt(wi . . . wi+k)
cnt(wi . . . wi+k?1)
(4)
where probMLE(wi+k|wi . . . wi+k?1) is the max-
imum likelihood estimate n-gram probability. We
refer to this tree linearisation method as the Lan-
guage Model Ordering (LMO).
4 Using an Argument Satisfaction Model
4.1 Assigning Words to Argument Positions
One limitation of using the CLE algorithm for
generation is that the resulting tree, though max-
imal in probability, may not conform to basic lin-
guistic properties of a dependency tree. In partic-
ular, it may not have the correct number of argu-
ments for each head word. That is, a word may
have too few or too many modifiers.
To address this problem, we can take into ac-
count the argument position when assigning a
weight to an edge. When attaching an edge con-
necting a modifier to a head to the spanning tree,
we count how many modifiers the head already
has. An edge is penalised if it is improbable that
the head takes on yet another modifier, say in the
example of an attachment to a preposition whose
argument position has already been filled.
However, accounting for argument positions
makes an edge weight dynamic and dependent on
surrounding tree context. This makes the search
for an optimal tree an NP-hard problem (McDon-
ald and Satta, 2007) as all possible trees must be
considered to find an optimal solution.
Consequently, we must choose a heuristic
search algorithm for finding the locally optimum
spanning tree. By representing argument positions
that can be filled only once, we allow modifiers
to compete for argument positions and vice versa.
The CLE algorithm only considers this competi-
tion in one direction. In line 3 of Algorithm 1,
only heads compete for modifiers, and thus the so-
lution will be sub-optimal. In Wan et al (2007),
we showed that introducing a model of argument
positions into a greedy spanning tree algorithm
had little effect on performance. Thus, to consider
both directions of competition, we design a new
algorithm for constructing (dependency) spanning
trees that casts edge selection as a weighted bipar-
tite graph matching (or assignment) problem.
This problem is to find a weighted alignments
between objects of two distinct sets, where an ob-
ject from one set is uniquely aligned to some ob-
ject in the other set. The optimal alignment is one
where the sum of alignment costs is minimal. The
graph of all possible assignments is a weighted bi-
partite graph. Here, to discuss bipartite graphs, we
will extend our notation in a fairly standard way,
to write Gp = (U, V,Ep), where U, V are the dis-
joint sets of vertices and Ep the set of edges.
In our paper, we treat the assignment between
attachment positions and words as an assignment
problem. The standard polynomial-time solution
to the assignment problem is the Kuhn-Munkres
(or Hungarian) algorithm (Kuhn, 1955).2
4.2 A Dependency-Spanning Tree Algorithm
Our alternative dependency-spanning tree algo-
rithm, presented as Algorithm 3, incrementally
adds vertices to a growing spanning tree. At
each iteration, the Kuhn-Munkres method assigns
words that are as yet unattached to argument posi-
tions already available in the tree. We focus on the
bipartite graph in Section 4.3.
Let the sentence w have the dependency graph
Gw = (Vw, Ew). At some arbitrary iteration of the
algorithm (see Figure 1), we have the following:
? Tw ? Ew, the set of edges in the spanning
tree constructed so far;
2GPL code: http://sites.google.com/site/garybaker/
hungarian-algorithm/assignment
855
Partially determined spanning tree:
w0
made
john
? l0
? r1 cups
of
? l0
? l1
for
? l0
? l3
johnl0 mader1 ofl0 cupsl1 forl0 madel3
Hw1 Hw2 Hw3 Hw4 Hw5 Hw6
Mw1 Mw2 Mw3 Mw4 Mw5 Mw6
coffee everyone yesterday ?1 ?2 ?3
Figure 1: A snapshot of the generation process.
Each word in the tree has argument positions to
which we can assign remaining words. Padding
Mw with ? is described in Section 4.3.
? Hw = {u, v | (u, v) ? Tw}, the set of ver-
tices in Tw, or ?attached vertices?, and there-
fore potential heads; and
? Mw = Vw\Hw, the set of ?unattached ver-
tices?, and therefore potential modifiers.
For the potential heads, we want to define the set
of possible attachment positions available in the
spanning tree where the potential modifiers can at-
tach. To talk about these attachment positions, we
define the set of labels L = {(d, j)|d ? D, j ?
N}, an element (d, j) representing an attachment
point in direction d, position j. Valid attachment
positions must be in sequential order and not miss-
ing any intermediate positions (e.g. if position 2
on the right is specified, position 1 must be also):
so we define for some i ? N, 0 ? i < N , a set
Ai ? L such that if the label (d, j) ? Ai then the
label (d, k) ? Ai for 0 ? k < j. Collecting these,
we define A = {Ai | 0 ? i < N}.
To map a potential head onto the set of attach-
ment positions, we define a function q : Hw ? A.
So, given some v ? Hw, q(v) = Ai for some
0 ? i < N . In talking about an individual attach-
ment point (d, j) ? q(v) for potential head v, we
/* initialisation */
Hw ? {w0}1
Mw ? V ?2
Uw ? {w0R1}3
U ?w ? {}4
Tw ? {}5
/* The Assignment-based Algorithm */
begin6
while Mw 6= {} and U ?w 6= Uw do7
U ?w ? Uw8
foreach ?u, (d, j)), v? ? Kuhn-Munkres(Gpw =9
(Uw,M?w, E
p
w)) do
Tw ? Tw ? {(u, v)}10
if u ? Hw then11
Uw ? Uw \ {u}12
end13
Uw ? Uw ? next(q(u))14
Uw ? Uw ? next(q(m))15
q(m)? q(m) \ next(q(m))16
q(h)? q(h) \ next(q(h))17
Mw ?Mw \ {m}18
Hw ? Hw ? {m}19
end20
end21
end22
Algorithm 3: The Assignment-based Depen-
dency Tree Building algorithm.
use the notation vdj . For example, when referring
to the second argument position on the right with
respect to v, we use vr2.
For the implementation of the algorithm, we
have defined q, to specify attachment points, as
follows, given some v ? Hw:
q(v) =
?
?
?
?
?
?
?
{vr1} if v = w0, the root
{vl1} if pos(v) is a preposition
L if pos(v) is a verb
{vlj |j ? N} otherwise
Defining q allows one to optionally incorporate
linguistic information if desired.
We define the function next : q(v) ? A, v ?
Hw that returns the position (d, j) with the small-
est value of j for direction d. Finally, we write the
set of available attachment positions in the span-
ning tree as U ? {(v, l) | v ? Hw, l ? q(v)}.
4.3 Finding an Assignment
To construct the bipartite graph used for the as-
signment problem at line 9 of Algorithm 3, given
our original dependency graph Gw = (Vw, Ew),
and the variables defined from it above in Sec-
tion 4.2, we do the following. The first set of
vertices, of possible heads and their attachment
points, is the set Uw. The second set of ver-
tices is the set of possible modifiers augmented
by dummy vertices ?i (indicating no modifica-
tion) such that this set is at least as large as Uw :
M ?w = Mw?{?0, . . . , ?max(0,|Uw|?|Mw|)}. The bi-
856
partite graph is then Gpw = (Uw,M ?w, Epw), where
Epw = {(u, v) |u ? Uw, v ? M ?w}.
The weights on the edges of this graph incor-
porate a model of argument counts. The weight
function is of the form sap : Ep ? R. We
consider some e ? Epw: e = (v?, v) for some
v? ? Uw, v ? M ?w; and v? = (u, (d, j)) for some
u ? Vw, d ? D, j ? N. s(u,M ?w) is defined to re-
turn the maximum cost so that the dummy leaves
are only attached as a last resort. We then define:
sap(e)
= ?log(probdep(u, v, d) ? probarg(u, d, j))
(5)
where probdep(u, v, d) is as in equation 2, using
the original dependency graph defined in Section
2; and probarg(u, d, j), an estimate of the prob-
ability that a word u with i arguments assigned
already can take on more arguments, is defined as:
probarg(u, d, j)
=
??
i=j+1 cntarg(u, d, i)
cnt(u, d) (6)
where cntarg(u, d, i) is the number of times word
u has been seen with i arguments in direction
d; and cnt(u, d) = ?i?N cntarg(u, d, i). As the
probability of argument positions beyond a certain
value for i in a given direction will be extremely
small, we approximate this sum by calculating the
probability density up to a fixed maximum, in this
case 7 argument positions, and assume zero prob-
ability beyond that.
5 Evaluation
5.1 String Generation Task
The best-performing word ordering algorithm is
one that makes fewest grammatical errors. As a
surrogate measurement of grammaticality, we use
the string regeneration task. Beginning with a
human-authored sentence with its word order ran-
domised, the goal is to regenerate the original sen-
tence. Success is indicated by the proportion of the
original sentence regenerated, as measured by any
string comparison method: in our case, using the
BLEU metric (Papineni et al, 2002). One benefit
to this evaluation is that content selection, as a fac-
tor, is held constant. Specifically, the probability
of word selection is uniform for all words.
The string comparison task and its associated
metrics like BLEU are not perfect.3 The evalu-
ation can be seen as being overly strict. It as-
sumes that the only grammatical order is that of the
original human authored sentence, referred to as
the ?gold standard? sentence. Should an approach
chance upon an alternative grammatical ordering,
it would penalised. However, all algorithms and
baselines compared would suffer equally in this
respect, and so this will be less problematic when
averaging across multiple test cases.
5.2 Data Sets and Training Procedures
The Penn Treebank corpus (PTB) was used to pro-
vide a model of dependency relations and argu-
ment counts. It contains about 3 million words
of text from the Wall Street Journal (WSJ) with
human annotations of syntactic structures. Depen-
dency events were sourced from the events file of
the Collins parser package, which contains the de-
pendency events found in training sections 2-22 of
the corpus. Development was done on section 00
and testing was performed on section 23.
A 4-gram language model (LM) was also ob-
tained from the PTB training data, referred to as
PTB-LM. Additionally, a 4-gram language model
was obtained from a subsection of the BLLIP?99
Corpus (LDC number: LDC2000T43) containing
three years of WSJ data from 1987 to 1989 (Char-
niak et al, 1999). As in Collins et al (2004),
the 1987 portion of the BLLIP corpus containing
20 million words was also used to create a lan-
guage model, referred to here as BLLIP-LM. N-
gram models were smoothed using Katz?s method,
backing off to smaller values of n.
For this evaluation, tokenisation was based on
that provided by the PTB data set. This data
set alo delimits base noun phrases (noun phrases
without nested constituents). Base noun phrases
were treated as single tokens, and the rightmost
word assumed to be the head. For the algorithms
tested, the input set for any test case consisted of
the single tokens identified by the PTB tokenisa-
tion. Additionally, the heads of base noun phrases
were included in this input set. That is, we do not
regenerate the base noun phrases.4
3Alternative grammaticality measures have been devel-
oped recently (Mutton et al, 2007). We are currently explor-
ing the use of this and other metrics.
4This would correspond to the use of a chunking algo-
rithm or a named-entity recogniser to find noun phrases that
could be re-used for sentence generation.
857
Algorithms PTB-LM BLLIP-LM
Viterbi baseline 14.9 18.0
LMO baseline 24.3 26.0
CLE 26.4 26.8
AB 33.6 33.7
Figure 2: String regeneration as measured in
BLEU points (maximum 100)
5.3 Algorithms and Baselines
We compare the baselines against the Chu-Liu
Edmonds (CLE) algorithm to see if spanning
tree algorithms do indeed improve upon conven-
tional language modelling. We also compare
the Assignment-based (AB) algorithm against the
baselines and CLE to see if, additionally, mod-
elling argument assignments improves the re-
sulting tree and thus the generated word se-
quence. Two baseline generators based on n-
gram language-models were used, representing
approaches that optimise word sequences based on
the local context of the n-grams.
The first baseline re-uses the LMO greedy se-
quence algorithm on the same set of input words
presented to the CLE and AB algorithms. We ap-
ply LMO in a rightward manner beginning with
a start-of-sentence token. Note that this baseline
generator, like the two spanning tree algorithms,
will score favourably using BLEU since, mini-
mally, the word order of the base noun phrases will
be correct when each is reinserted.
Since the LMO baseline reduces to bigram gen-
eration when concatenating single words, we test
a second language model baseline which always
uses a 4-gram window size. A Viterbi-like gen-
erator with a 4-gram model and a beam of 100 is
used to generate a sequence. For this baseline, re-
ferred to as the Viterbi baseline, base noun phrases
were separated into their constituent words and in-
cluded in the input word set.
5.4 Results
The results are presented in Table 2. Significance
was measured using the sign test and the sampling
method outlined in (Collins et al, 2005). We will
examine the results in the PTB-LM column first.
The gain of 10 BLEU points by the LMO baseline
over the Viterbi baseline shows the performance
improvement that can be gained when reinserting
the base noun phrases.
AB: the dow at this point was down about 35 points
CLE: was down about this point 35 points the dow at
LMO: was this point about at down the down 35 points
Viterbi: the down 35 points at was about this point down
Original: at this point, the dow was down about 35 points
Figure 3: Example generated sentences using the
BLLIP-LM.
The CLE algorithm significantly out-performed
the LMO baseline by 2 BLEU points, from which
we conclude that incorporating a model for global
syntactic structure and treating the search for a
dependency tree as a spanning problem helps for
novel sentence generation. However, the real im-
provement can be seen in the performance of the
AB system which significantly out-performs all
other methods, beating the CLE algorithm by 7
BLEU points, illustrating the benefits of a model
for argument counts and of couching tree building
as an iterative set of argument assignments.
One might reasonably ask if more n-gram data
would narrow the gap between the tree algorithms
and the baselines, which encode global and lo-
cal information respectively. Examining results in
the BLLIP-LM column, all approaches improve
with the better language model. Unsurprisingly,
the improvements are most evident in the base-
lines which rely heavily on the language model.
The margin narrows between the CLE algorithm
and the LMO baseline. However, the AB algo-
rithm still out-performs all other approaches by
7 BLEU points, highlighting the benefit in mod-
elling dependency relations. Even with a language
model that is one order of magnitude larger than
the PTB-LM, the AB still maintains a sizeable lead
in performance. Figure 3 presents sample gener-
ated strings.
6 Related Work
6.1 Statistical Surface Realisers
The work in this paper is similar to research in
statistical surface realisation (for example, Langk-
ilde and Knight (1998); Bangalore and Rambow
(2000); Filippova and Strube (2008)). These start
with a semantic representation for which a specific
rendering, an ordering of words, must be deter-
mined, often using language models to govern tree
traversal. The task in this paper is different as it is
a text-to-text scenario and does not begin with a
representation of semantics.
858
The dependency model and the LMO lineari-
sation algorithm are based heavily on word order
statistics. As such, the utility of this approach is
limited to human languages with minimal use of
inflections, such as English. Approaches for other
language types, for example German, have been
explored (Filippova and Strube, 2007).
6.2 Text-to-Text Generation
As a text-to-text approach, our work is more sim-
ilar to work on Information Fusion (Barzilay et
al., 1999), a sub-problem in multi-document sum-
marisation. In this work, sentences presenting the
same information, for example multiple news arti-
cles describing the same event, are merged to form
a single summary by aligning repeated words and
phrases across sentences.
Other text-to-text approaches for generating
novel sentences also aim to recycle sentence frag-
ments where possible, as we do. Work on phrase-
based statistical machine translation has been
applied to paraphrase generation (Bannard and
Callison-Burch, 2005) and multi-sentence align-
ment in summarisation (Daume? III and Marcu,
2004). These approaches typically use n-gram
models to find the best word sequence.
The WIDL formalism (Soricut and Marcu,
2005) was proposed to efficiently encode con-
straints that restricted possible word sequences,
for example dependency information. Though
similar, our work here does not explicitly repre-
sent the word lattice.
For these text-to-text systems, the order of ele-
ments in the generated sentence is heavily based
on the original order of words and phrases in the
input sentences from which lattices are built. Our
approach has the benefit of considering all possi-
ble orderings of words, corresponding to a wider
range of paraphrases, provided with a suitable de-
pendency model is available.
6.3 Parsing and Semantic Role Labelling
This paper presents work closely related to parsing
work by McDonald et al (2005) which searches
for the best parse tree. Our work can be thought of
as generating projective dependency trees (that is,
without crossing dependencies).
The key difference between parsing and gener-
ation is that, in parsing, the word order is fixed,
whereas for generation, this must be determined.
In this paper, we search across all possible tree
structures whilst searching for the best word or-
dering. As a result, an argument model is needed
to identify linguistically plausible spanning trees.
We treated the alignment of modifiers to head
words as a bipartite graph matching problem. This
is similar to work in semantic role labelling by
Pado? and Lapata (2006). The alignment of an-
swers to question types as a semantic role labelling
task using similar methods was explored by Shen
and Lapata (2007).
Our work is also strongly related to that of
Wong and Mooney (2007) which constructs sym-
bolic semantic structures via an assignment pro-
cess in order to provide surface realisers with in-
put. Our approach differs in that we do not be-
gin with a fixed set of semantic labels. Addition-
ally, our end goal is a dependency tree that encodes
word precedence order, bypassing the surface re-
alisation stage.
7 Conclusions
In this paper, we presented a new use of spanning
tree algorithms for generating sentences from an
input set of words, a task common to many text-
to-text scenarios. The algorithm finds the best de-
pendency trees in order to ensure that the result-
ing string has grammaticality modelled at a global
(sentence) level. Our algorithm incorporates a
model of argument satisfaction which is treated as
an assignment problem, using the Kuhn-Munkres
assignment algorithm. We found a significant im-
provement using BLEU to measure improvements
on the string regeneration task. We conclude that
our new algorithm based on the assignment prob-
lem and an argument model finds trees that are lin-
guistically more plausible, thereby improving the
grammaticality of the generated word sequence.
References
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In Proceedings of the 18th Conference on
Computational Linguistics, Saarbru?cken, Germany.
Srinivas Bangalore, Owen Rambow, and Steve Whit-
taker. 2000. Evaluation metrics for generation.
In Proceedings of the first international conference
on Natural language generation, Morristown, NJ,
USA.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
859
ciation for Computational Linguistics, Ann Arbor,
Michigan.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings
of the 37th conference on Association for Computa-
tional Linguistics, Morristown, NJ, USA.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 1999. Bllip 1987-89
wsj corpus release 1. Technical report, Linguistic
Data Consortium.
Y. J. Chu and T. H. Liu. 1965. On the shortest
arborescence of a directed graph. Science Sinica,
v.14:1396?1400.
Christopher Collins, Bob Carpenter, and Gerald Penn.
2004. Head-driven parsing for word lattices. In Pro-
ceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, Morristown, NJ,
USA.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, Morristown, NJ, USA.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the Thirty-Fourth Annual Meeting of the As-
sociation for Computational Linguistics, San Fran-
cisco.
Hal Daume? III and Daniel Marcu. 2004. A phrase-
based hmm approach to document/abstract align-
ment. In Proceedings of EMNLP 2004, Barcelona,
Spain..
J. Edmonds. 1967. Optimum branchings. J. Research
of the National Bureau of Standards, 71B:233?240.
Katja Filippova and Michael Strube. 2007. Generating
constituent order in german clauses. In Proceedings
of the 45th Annual Meeting on Association for Com-
putational Linguistics. Prague, Czech Republic.
Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Con-
ference on Empirical Methods in Natural Language
Processing, Waikiki, Honolulu, Hawaii.
Mark Johnson. 2007. Transforming projective bilex-
ical dependency grammars into efficiently-parsable
cfgs with unfold-fold. In Proceedings of the 45th
Annual Meeting on Association for Computational
Linguistics. Prague, Czech Republic.
H.W. Kuhn. 1955. The hungarian method for the as-
signment problem. Naval Research Logistics Quar-
terly, 219552:83?97 83?97.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of N-grams in derivation. In Proceedings
of the Ninth International Workshop on Natural Lan-
guage Generation, New Brunswick, New Jersey.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proceedings of the Tenth International
Conference on Parsing Technologies, Prague, Czech
Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, Morristown, NJ, USA.
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. Gleu: Automatic evaluation of
sentence-level fluency. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, Prague, Czech Republic.
Sebastian Pado? and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for seman-
tic projection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, Morristown, NJ, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, Philadelphia, July.
Dan Shen and Mirella Lapata. 2007. Using seman-
tic roles to improve question answering. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, Prague,
Czech Republic.
Radu Soricut and Daniel Marcu. 2005. Towards devel-
oping generation algorithms for text-to-text applica-
tions. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, Ann
Arbor, Michigan.
Stephen Wan, Robert Dale, Mark Dras, and Ce?cile
Paris. 2007. Global revision in summarisation:
Generating novel sentences with prim?s algorithm.
In Proceedings of 10th Conference of the Pacific As-
sociation for Computational Linguistic, Melbourne,
Australia.
Yuk Wah Wong and Raymond Mooney. 2007. Genera-
tion by inverting a semantic parser that uses statisti-
cal machine translation. In Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, Rochester, New York.
860
Towards Statistical Paraphrase Generation: Preliminary Evaluations of
Grammaticality
Stephen Wan
 
Mark Dras
 
Robert Dale
 
 
Center for Language Technology
Div of Information Communication Sciences
Macquarie University
Sydney, NSW 2113
swan,madras,rdale@ics.mq.edu.au
Ce?cile Paris


Information and Communication
Technologies
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
Summary sentences are often para-
phrases of existing sentences. They
may be made up of recycled fragments
of text taken from important sentences
in an input document. We investigate
the use of a statistical sentence gener-
ation technique that recombines words
probabilistically in order to create new
sentences. Given a set of event-related
sentences, we use an extended version
of the Viterbi algorithm which employs
dependency relation and bigram proba-
bilities to find the most probable sum-
mary sentence. Using precision and
recall metrics for verb arguments as a
measure of grammaticality, we find that
our system performs better than a bi-
gram baseline, producing fewer spuri-
ous verb arguments.
1 Introduction
Human authored summaries are more than just
a list of extracted sentences. Often the sum-
mary sentence is a paraphrase of a sentence in the
source text, or else a combination of phrases and
words from important sentences that have been
pieced together to form a new sentence. These
sentences, referred to as Non-Verbatim Sentences,
can replace extracted text to improve readability
and coherence in the summary.
Consider the example in Figure 1 which
presents an alignment between a human authored
summary sentence and a source sentence. The
Summary Sentence:
Every province in the country, except one, endured sporadic fighting, looting
or armed banditry in 2003.
Source Sentence:
However, as the year unfolded, every province has been subjected to fighting,
looting or armed banditry, with the exception of just one province (Kirundo,
in northern Burundi).
Figure 1: An aligned summary and source sen-
tence.
text is taken from a corpus of Humanitarian Aid
Proposals1 produced by the United Nations for
the purpose of convincing donors to support a re-
lief effort.
The example illustrates that sentence extraction
alone cannot account for the breadth of human au-
thored summary sentences. This is supported by
evidence presented in (Jing and McKeown, 1999)
and (Daume? III and Marcu, 2004).
Moving towards the goal of abstract-like auto-
matic summary generation challenges us to con-
sider mechanisms for generating non-verbatim
sentences. Such a mechanism can usefully be
considered as automatically generating a para-
phrase.2 We treat the problem as one in which a
new and previously unseen summary sentence is
to be automatically produced given some closely
related sentences extracted from a source text.
Following on from (Witbrock and Mittal,
1999), we use and extend the Viterbi algorithm
(Forney, 1973) for the purposes of generating
non-verbatim sentences. This approach treats
1These are available publically at
http://www.reliefweb.com.
2Paraphrase here includes sentences generated in an In-
formation Fusion task (Barzilay et al, 1999).
88
sentence generation as a search problem. Given
a set of words (taken from some set of sentences
to paraphrase), we search for the most likely se-
quence given some language model. Intuitively,
we want the generated string to be grammatical
and to accurately reflect the content of the source
text.
Within the Viterbi search process, each time we
append a word to the partially generated sentence,
we consider how well it attaches to a dependency
structure. The focus of this paper is to evaluate
whether or not a series of iterative considerations
of dependency structure results in a grammatical
generated sentence. Previous preliminary evalu-
ations (Wan et al, 2005) indicate that the gen-
erated sequences contain less fragmented text as
measured by an off-the-shelf dependency parser;
more fragments would indicate a grammatically
problematic sentence.
However, while encouraging, such an evalu-
ation says little about what the actual sentence
looks like. For example, such generated text
might only be useful if it contains complete
clauses. Thus, in this paper, we use the precision
and recall metric to measure how many generated
verb arguments, as extracted from dependency re-
lations, are correct.
The remainder of this paper is structured as fol-
lows. Section 2 provides an overview introducing
our approach. In Section 3, we briefly illustrate
our algorithm with examples. A brief survey of
related work is presented in Section 4. We present
our grammaticality experiments in Section 5. We
conclude with further work in Section 6.
2 An Overview of our Approach to
Statistical Sentence Generation
One could characterise the search space as being
a series of nested sets. The outer most set would
contain all possible word sequences. Within this,
a smaller set of strings exhibiting some semblance
of grammaticality might be found, though many
of these might be gibberish. Further nested sets
are those that are grammatical, and within those,
the set of paraphrases that are entailed by the in-
put text.
However, given that we limit ourselves to sta-
tistical techniques and avoid symbolic logic, we
cannot make any claim of strict entailment. We
Original Text
A military transporter was scheduled to take off in the afternoon from Yokota
air base on the outskirts of Tokyo and fly to Osaka with 37,000 blankets .
Mondale said the United States, which has been flying in blankets and is
sending a team of quake relief experts, was prepared to do more if Japan
requested .
United States forces based in Japan will take blankets to help earthquake
survivors Thursday, in the U.S. military?s first disaster relief operation in
Japan since it set up bases here.
Our approach with Dependencies
6: united states forces based in blankets
8: united states which has been flying in blankets
11: a military transporter was prepared to osaka with 37,000 blankets
18: mondale said the afternoon from yokota air base on the united states which
has been flying in blankets
20: mondale said the outskirts of tokyo and is sending a military transporter
was prepared to osaka with 37,000 blankets
23: united states forces based in the afternoon from yokota air base on the
outskirts of tokyo and fly to osaka with 37,000 blankets
27: mondale said the afternoon from yokota air base on the outskirts of tokyo
and is sending a military transporter was prepared to osaka with 37,000 blan-
kets
29: united states which has been flying in the afternoon from yokota air base
on the outskirts of tokyo and is sending a team of quake relief operation in
blankets
31: united states which has been flying in the afternoon from yokota air base
on the outskirts of tokyo and is sending a military transporter was prepared to
osaka with 37,000 blankets
34: mondale said the afternoon from yokota air base on the united states which
has been flying in the outskirts of tokyo and is sending a military transporter
was prepared to osaka with 37,000 blankets
36: united states which has been flying in japan will take off in the after-
noon from yokota air base on the outskirts of tokyo and is sending a military
transporter was prepared to osaka with 37,000 blankets
Figure 2: A selection of example output. Sen-
tences are prefixed by their length.
thus propose an intermediate set of sentences
which conserve the content of the source text
without necessarily being entailed. These are re-
ferred to as the set of verisimilitudes, of which
properly entailed sentences are a subset. The aim
of our choice of features and our algorithm exten-
sion is to reduce the search space from gibberish
strings to that of verisimilitudes. While generat-
ing verisimilitudes is our end goal, in this paper,
we are concerned principally with the generating
of grammatical sentences.
To do so, the extension adds an extra feature
propagation mechanism to the Viterbi algorithm
such that features are passed along a word se-
quence path in the search space whenever a new
word is appended to it. Propagated features are
used to influence the choice of subsequent words
suitable for appending to a partially generated
sentence. In our case, our feature is a depen-
dency structure of the word sequence correspond-
ing to the search path. Our present dependency
representation is based on that of (Kittredge and
89
Mel?cuk, 1983). However, it contains only the
head and modifier of a relation, ignoring relation-
ship labels for the present.
Algorithmically, after appending a word to a
path, a dependency structure of the partially gen-
erated string is obtained probabilistically. Along
with bigram information, the long-distance con-
text of dependency head information of the pre-
ceding word sequence will be useful in generat-
ing better sentences by filtering out all words that
might, at a particular position in the string, lead
to a spurious dependency relation in the final sen-
tence. Example output is presented in Figure 2.
As the dependency ?parsing? mechanism is lin-
ear3 and is embedded within the Viterbi algo-
rithm, the result is an O( 

) algorithm.
By examining surface-syntactic dependency
structure at each step in the search, resulting sen-
tences are likely to be more grammatical. This
marraige of models has been tested in other fields
such as speech recognition (Chelba and Jelinek,
1998) with success. Although it is an impover-
ished representation of semantics, considering de-
pendency features in our application context may
also serendipitously assist verisimilitude genera-
tion.
3 The Extended Viterbi Algorithm:
Propagating Dependency Structure
In this section, we present an overview of the
main features of our algorithm extension. We di-
rect the interested reader to our technical paper
(Wan et al, 2005) for full details.
The Viterbi algorithm (for a comprehensive
overview, see (Manning and Schu?tze, 1999)) is
used to search for the best path across a network
of nodes, where each node represents a word in
the vocabulary. The best sentence is a string of
words, each one emitted by the corresponding vis-
ited node on the path.
Arcs between nodes are weighted using a com-
bination of two pieces of information: a bigram
probability corresponding to that pair of words;
and a probability corresponding to the likelihood
of a dependency relation between that pair of
words. Specifically, the transition probability
3The parse is thus not necessarily optimal, in the sense of
guaranteeing the most likely parse.
defining these weights is the average of the depen-
dency transition probability and the bigram prob-
ability.
To simplify matters in this evaluation, we
assume that the emission probability is always
one. The emission probability is interpreted
as being a Content Selection mechanism that
chooses words that are likely to be in a summary.
Thus, in this paper, each word has an equally
likely chance of being selected for the sentence.
Transition Probability is defined as:
	
		 
Last Words
What?s the Future for Computational Linguistics?
Robert Dale?
Macquarie University, Sydney
You are reading the last issue of Computational Linguistics that will appear in printed
hardcopy form. The beginning of 2009 heralds a new era for the journal in at least
two major respects: As of the first issue of volume 35, Computational Linguistics will be
published only electronically, and it will be open access. As editor, I?d like to take this
opportunity to use these last words that will tumble from the presses to provide some
explanation of the changes afoot at the journal, and to offer some thoughts on where
this might take us in the future.
1. Why Open Access?
There are a number of definitions of the term ?open access? in circulation, but almost all
share the key principle that scientific literature should be freely available for all to read,
download, copy, distribute, and use (with appropriate attribution) without restriction.
At the time of writing, the vast bulk of scholarly literature is not open access: Either
you pay for access directly as an individual subscriber (for example, in the case of
Computational Linguistics, via your annual membership subscription to the Association
for Computational Linguistics), or you gain access via an institutional subscription
(typically, your library?s annual subscription to MIT Press).
There is an increasingly widely held view that this is just not right. Given that
almost all the research published in scholarly journals is paid for by the taxpayer, it?s
reasonable to ask what justification there can be for restricting public access to this
research by requiring that a further payment be made to read about it. And ?toll access?,
as it is sometimes called, is not only bad for the reading public; it has been frequently
argued that it is bad for authors too, because any barriers to access may decrease the
likelihood of citation and the general impact of the reported work.
So, as announced earlier this year at the annual meeting of the Association for
Computational Linguistics in Columbus, Ohio, Computational Linguistics will become
an open access journal as of the first issue of 2009. In fact, for some time the journal
has had a rather unusual access status, whereby its content becomes freely available as
part of the online ACL Anthology one year after publication.1 As of the first issue of
Volume 35, even this delay will be removed, with the journal?s contents being freely
available as soon as they are published.
? Centre for Language Technology, Macquarie University, Sydney, New South Wales 2109, Australia.
E-mail: rdale@ics.mq.edu.au.
1 See http://aclweb.org/anthology-new/. In practice, for some material this one-year embargo on free
access is closer to two years, as a consequence of the batching-up of uploads to the Anthology on an
annual cycle, and the inevitable administrative delays.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
2. Why Electronic-only?
That the journal is going electronic-only and open access at the same time are not
unrelated events. Print publication is a major part of the cost of producing the journal,
and this is a cost that is very considerably subsidized by the revenues generated by
institutional subscriptions. Going open access means that institutions no longer pay
these subscriptions, and so we need a sustainable financial model that does not depend
on this income. Ceasing the production of the printed version of the journal is an
important step in that direction. In an environmentally conscious age, it also has the
additional benefit of removing the need to fly, four times a year, small plastic-wrapped
wads of paper to two thousand ACL members around the world.
At the same time, there is no necessary connection between going open access
and going electronic-only. In principle a journal could be open access and yet still be
distributed in printed form, although then an alternative funding source has to be found
for print production. One approach is that adopted by those publishers (such as Elsevier
and Cambridge University Press) who offer authors the option of their work being
published open access in exchange for a fee, which can be anywhere between $500 and
$2,000 per article. This ?dissemination fee? model has become accepted practice in some
disciplines, but not?so far, at least?in ours. There are other business models one could
adopt to sustain print production. For example, free local newspapers and many trade
magazines are also effectively open access, with their production costs being paid for
by advertisers; but it seems unlikely that advertising revenue could sustain a scholarly
journal like CL. Another alternative would be to seek sponsorship from corporations
like Google and Microsoft. But in the Internet age, it just seems perverse to bend over
backwards to find ways to kill more trees.
This is not to say that going electronic-only makes the journal free of production
costs. Although the journal?s editor, editorial board, and external reviewers provide
their labor free of charge, it still costs real money to produce the journal: In partic-
ular, CL will continue to use the professional services of the MIT Press in managing
the high standards of copyediting and typesetting to which our readers have become
accustomed. For the foreseeable future at least, these costs will continue to be met by
the ACL, as owner of the journal.
3. What Else is Changing?
The shift to electronic publication means that publishing an article in Computational
Linguistics should be considerably faster than before, for a number of reasons. Under the
old model, it can take up to six months from the time that the editorial office dispatches
the author?s final copy of an article to the publisher, to the time the article appears in
print. Around half of this time is taken up by the print production process. The delay
is further exacerbated by the fact that the first article that is ready for an issue has to
wait until the remainder of the issue is ready before it can be published. By moving to
electronic-only publication, articles will now appear online as soon as they have gone
through the copy-editing process, on an article-by-article basis. You?ll be able to access
articles even before they have completed the copy-editing cycle. This, in fact, is not a
new thing: for the last year or so, we have been publishing what are referred to as the
?first revises? of articles?these being the proofs that are sent to the authors for checking
before final edits are made?via the MIT Press Web site under the label ?Early Access?.
But the unavailability of an easy way to let individual subscribers see this toll-access
622
Dale What?s the Future for Computational Linguistics?
content meant that very few people were aware of its existence. With the move to open
access, the Early Access articles will also be freely available to all.
Another consequence of being electronic-only is that there will be no artificial back-
log resulting from the page limits imposed by physical print production. Previously,
the journal?s cost model meant that we could publish 160 or so pages each quarter; the
removal of print production provides us with more flexibility, and the likelihood that
the journal will publish a greater number of articles (although, as indicated previously,
there will still be costs associated with this: Copyeditors and typesetters charge per
page). Being freed from paper bindings does not mean that processing backlogs will
completely disappear, because there are other places in the publication process where
delays can occur. For example, an unpredictable copyediting workload may lead to
delays when our demand for copyediting services outstrips the available resources.
We hope that readers who keenly look forward to receiving an issue of the journal
in their pigeonhole every three months will not suffer too much from a sense of loss.
Members of the ACL will instead be able to subscribe to a number of electronic alerting
options which either direct them to a download location for the latest journal content,
or provide the content in the form of email attachments. The full details of our elec-
tronic delivery mechanisms are still being worked out at the time of writing, but ACL
members will already have been notified of them by the time this issue is published.
We are taking advantage of the shift to electronic-only publication to institute a
number of other changes. Most visible for authors will be our move to an online journal
manuscript management system, which should introduce efficiencies that will shave
a few more days off the time our reviewing process takes. For readers, each article
published in the journal will be available in both PDF format and in XML format, the
latter allowing easy reading on screen.
4. What Does the Longer-term Future Look Like?
Over the next year or two we intend to introduce a number of further changes that be-
come more straightforward as a consequence of our primary existence being electronic.
Possibilities we are exploring include making available for download supplementary
materials such as the code and data associated with articles, and providing each article
with an online discussion forum where its content can be discussed. The editor wel-
comes suggestions and ideas for how we can improve the value of the journal to its
readership along these lines.
There is a longer-term question, though, about the nature of journals and whether
they are a sustainable means of scholarly dissemination in the Internet age. The pub-
lication landscape has changed significantly in the last 10 years. One challenge to the
position of journals and existing forms of scholarly publication more generally comes in
the shape of blogs and other online discussion fora. Obviously these are not rigorously
reviewed in the way that journals are, but they are open to other means of quality
assessment, via machinery like page rank and reader voting. For some purposes, the
low cost and democratic openness these offer may outweigh the deficiencies that arise
from the lack of a conventional peer-review process. Ultimately, how these alternative
fora will fare as a means of scholarly publication may be determined by whether we can
develop a credit assignment mechanism that gives due recognition for great ideas that
first surface in these more informal spaces. This would have to be more fine-grained
than our current model of journal impact factors and bald citation counts; but more
traditional forms of publication would surely benefit from improvements here too.
623
Computational Linguistics Volume 34, Number 4
Another challenge comes from online shared knowledge repositories such as
Wikipedia and, more recently, Google?s Knols project. Who needs a static survey article
in a journal in an era of Web resources that are constantly kept up-to-date? The art of
writing survey articles for journals will no doubt continue to be practiced for some time,
but not because it?s the best way to provide this kind of material; conventional survey
articles will persist for only as long as there is no alternative means of gaining academic
credit for work of this kind. It?s entirely possible that, within a few years, Knols will
frequently be cited in the pages of journals like this one.
These are future challenges for journals in any discipline. In our field, journals face
a challenge from another direction, closer to home. It has often been noted that, in
computer science generally, conference publication counts as much as journal publi-
cation. In fact, a paper in a good conference can be worth much more than an article in
a mid-ranking journal. Given the substantial additional effort that is usually required
to produce a journal article, and the longer reviewing process that is subsequently
involved, it?s perhaps a surprise that journals receive any submissions at all. So why
don?t we just let journals die, and allow conferences to become the primary means of
scientific communication in the field?
Conferences play an important role in many disciplines, but they do not fulfil all
the requirements for the ongoing development of a healthy scientific base. Over the last
few years, there has been increasing disquiet about the way our premier conferences are
evolving. A common complaint is that only ?safe? work delivering incremental results
is accepted, which leads to a vicious cycle where this is the only type of work that
gets submitted, and maybe even the only type of work that gets done. One also hears
complaints that acceptance rates are too low, and that the events have become, well, just
plain dull. Other disciplines adopt a different model, where journals remain the primary
repositories for well-considered and rigorously reviewed research that is considered
fit to enter the archives of knowledge in the field; conferences are first and foremost
meeting places, where acceptance rates are high, and much of the work presented is
work-in-progress, the initial outings of ideas that are yet to be further developed. Some
would argue that such events are more exciting and engaging than conferences in our
field, where the assumed high calibre of the work presented can be thrown into question
by occasionally random and hasty reviewing processes.
The defining property of a scholarly journal, and one that persists independent of
the medium used for content delivery, is the adherence to a robust and thorough quality
control process, all the way from the use of carefully chosen expert reviewers and
feedback?response cycles to detailed copyediting that improves the wording andmakes
sure bibliographic references are complete. Stripped of its covers in an electronic age, a
journal is a brand; a journal name serves as an indicator of quality. The stamp of quality
a journal provides is even more important in an age of abundant publication; with so
much to read, it becomes imperative that journals short-circuit the quality assessment
processes that readers would otherwise have to undertake themselves.
It is inevitable that journals will change in unforeseen ways in the coming decades,
but they will continue to serve their function as trusted gateways to trustable resources.
Here at Computational Linguistics, we aim to take on board ideas that arise from newer
forms of media and find ways of adopting and adapting them; but we also aim to
continue to be seen as a source of the highest quality research in the field. I hope you?ll
continue reading, and that you?ll consider Computational Linguistics the publication
forum of choice in our discipline.
624
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 33?36,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Intermediate Representation for
the Interpretation of Temporal Expressions
Pawe? Mazur and Robert Dale
Centre for Language Technology
Macquarie University
NSW 2109 Sydney Australia
{mpawel,rdale}@ics.mq.edu.au
Abstract
The interpretation of temporal expressions
in text is an important constituent task for
many practical natural language process-
ing tasks, including question-answering,
information extraction and text summari-
sation. Although temporal expressions
have long been studied in the research
literature, it is only more recently, with
the impetus provided by exercises like
the ACE Program, that attention has been
directed to broad-coverage, implemented
systems. In this paper, we describe our
approach to intermediate semantic repre-
sentations in the interpretation of temporal
expressions.
1 Introduction
In this paper, we are concerned with the interpreta-
tion of temporal expressions in text: that is, given
an occurrence in a text of an expression like that
marked in italics in the following example, we
want to determine what point in time is referred
to by that expression.
(1) We agreed that we would meet at 3pm on
the first Tuesday in November.
In this particular case, we need to make use of the
context of utterance to determine which November
is being referred to; this might be derived on the
basis of the date stamp of the document contain-
ing this sentence. Then we need to compute the
full time and date the expression corresponds to.
If the utterance in (1) was produced, say, in July
2006, then we might expect the interpretation to be
equivalent to the ISO-format expression 2006-11-
07T15:00.1 The derivation of such interpretation
was the focus of the TERN evaluations held under
the ACE program. Several teams have developed
systems which attempt to interpret both simple and
much more complex temporal expressions; how-
ever, there is very little literature that describes in
any detail the approaches taken. This may be due
to a perception that such expressions are relatively
easy to identify and interpret using simple pat-
terns, but a detailed analysis of the range of tem-
poral expressions that are covered by the TIDES
annotation guidelines demonstrates that this is not
the case. In fact, the proper treatment of some tem-
poral expressions requires semantic and pragmatic
processing that is considerably beyond the state of
the art.
Our view is that it is important to keep in mind
a clear distinction between, on the one hand, the
conceptual model of temporal entities that a partic-
ular approach adopts; and, on the other hand, the
specific implementation of that model that might
be developed for a particular purpose. In this pa-
per, we describe both our underlying framework,
and an implementation of that framework. We be-
lieve the framework provides a basis for further
development, being independent of any particular
implementation, and able to underpin many dif-
ferent implementations. By clearly separating the
underlying model and its implementation, this also
opens the door to clearer comparisons between
different approaches.
We begin by summarising existing work in the
area in Section 2; then, in Section 3, we describe
our underlying model; in Section 4, we describe
how this model is implemented in the DANTE
1Clearly, other aspects of the document context might
suggest a different year is intended; and we might also add
the time zone to this value.
33
system.2
2 Relation to Existing Work
The most detailed system description in the pub-
lished literature is that of the Chronos system from
ITC-IRST (Negri and Marseglia, 2005). This sys-
tem uses a large set of hand-crafted rules, and
separates the recognition of temporal expressions
from their interpretation. The ATEL system de-
veloped by the Center for Spoken Language Re-
search (CSLR) at University of Colorado (see (Ha-
cioglu et al, 2005)) uses SVM classifiers to detect
temporal expressions. Alias-i?s LingPipe also re-
ported results for extraction, but not interpretation,
of temporal expressions at TERN 2004.
In contrast to this collection of work, which
comes at the problem from a now-traditional in-
formation extraction perspective, there is also of
course an extensive prior literature on the semantic
of temporal expressions. Some more recent work
attempts to bridge the gap between these two re-
lated enterprises; see, for example, Hobbs and Pan
(2004).
3 The Underlying Model
We describe briefly here our underlying concep-
tual model; a more detailed description is provided
in (Dale and Mazur, 2006).
3.1 Processes
We take the ultimate goal of the interpretation of
temporal expressions to be that of computing, for
each temporal expression in a text, the point in
time or duration that is referred to by that expres-
sion. We distinguish two stages of processing:
Recognition: the process of identifying a tempo-
ral expression in text, and determining its ex-
tent.
Interpretation: given a recognised temporal ex-
pression, the process of computing the value
of the point in time or duration referred to by
that expression.
In practice, the processes involved in determining
the extent of a temporal expression are likely to
make use of lexical and phrasal knowledge that
mean that some of the semantics of the expres-
sion can already be computed. For example, in
2DANTE stands for Detection and Normalisation of Tem-
poral Expressions.
order to identify that an expression refers to a day
of the week, we will in many circumstances need
to recognize whether one of the specific expres-
sions {Monday, Tuesday, ... Sunday} has been
used; but once we have recognised that a specific
form has been used, we have effectively computed
the semantics of that part of the expression.
To maintain a strong separation between recog-
nition and interpretation, one could simply recom-
pute this partial information in the interpretation
phase; this would, of course, involve redundancy.
However, we take the view that the computation
of partial semantics in the first step should not be
seen as violating the strong separation; rather, we
distinguish the two steps of the process in terms of
the extent to which they make use of contextual in-
formation in computing values. Then, recognition
is that phase which makes use only of expression-
internal information and preposition which pre-
cedes the expression in question; and interpreta-
tion is that phase which makes use of arbitrarily
more complex knowledge sources and wider doc-
ument context. In this way, we motivate an in-
termediate form of representation that represents a
?context-free? semantics of the expression.
The role of the recognition process is then to
compute as much of the semantic content of a tem-
poral expression as can be determined on the basis
of the expression itself, producing an intermediate
partial representation of the semantics. The role of
the interpretation process is to ?fill in? any gaps in
this representation by making use of information
derived from the context.
3.2 Data Types
We view the temporal world as consisting of two
basic types of entities, these being points in time
and durations; each of these has an internal hi-
erarchical structure. It is convenient to represent
these as feature structures like the following:3
(2) ?
?
?
?
?
?
?
?
?
?
?
point
DATE
?
?
DAY 11
MONTH 6
YEAR 2005
?
?
TIME
?
?
HOUR 3
MINUTE 00
AMPM pm
?
?
?
?
?
?
?
?
?
?
?
?
?
3For reasons of limitations of space, we will ignore dura-
tions in the present discussion; their representation is similar
in spirit to the examples provided here.
34
Our choice of attribute?value matrices is not ac-
cidental; in particular, some of the operations we
want to carry out on the interpretations of both
partial and complete temporal expressions can be
conveniently expressed via unification, and this
representation is a very natural one for such op-
erations.
This same representation can be used to indi-
cate the interpretation of a temporal expression at
various stages of processing, as outlined below. In
particular, note that temporal expressions differ in
their explicitness, i.e. the extent to which the in-
terpretation of the expression is explicitly encoded
in the temporal expression; they also differ in their
granularity, i.e. the smallest temporal unit used
in defining that point in time or duration. So, for
example, in a temporal reference like November
11th, interpretation requires us to make explicit
some information that is not present (that is, the
year); but it does not require us to provide a time,
since this is not required for the granularity of the
expression.
In our attribute?value matrix representation, we
use a special NULL value to indicate granularities
that are not required in providing a full interpre-
tation; information that is not explicitly provided,
on the other hand, is simply absent from the rep-
resentation, but may be added to the structure dur-
ing later stages of interpretation. So, in the case
of an expression like November 11th, the recogni-
tion process may construct a partial interpretation
of the following form:
(3) ?
?
?
?
point
DATE
[
DAY 11
MONTH 6
]
TIME NULL
?
?
?
?
The interpretation process may then monotoni-
cally augment this structure with information from
the context that allows the interpretation to be
made fully explicit:
(4) ?
?
?
?
?
?
point
DATE
?
?
DAY 11
MONTH 6
YEAR 2006
?
?
TIME NULL
?
?
?
?
?
?
The representation thus very easily accommodates
relative underspecification, and the potential for
further specification by means of unification, al-
though our implementation also makes use of
other operations applied to these structures.
4 Implementation
4.1 Data Structures
We could implement the model above directly in
terms of recursive attribute?value structures; how-
ever, for our present purposes, it turns out to
be simpler to implement these structures using a
string-based notation that is deliberately consis-
tent with the representations for values used in the
TIMEX2 standard (Ferro et al, 2005). In that no-
tation, a time and date value is expressed using the
ISO standard; uppercase Xs are used to indicate
parts of the expression for which interpretation is
not available, and elements that should not receive
a value are left null (in the same sense as our NULL
value above). So, for example, in a context where
we have no way of ascertaining the century be-
ing referred to, the TIMEX2 representation of the
value of the underlined temporal expression in the
sentence We all had a great time in the ?60s is sim-
ply VAL="XX6".
We augment this representation in a number
of ways to allow us to represent intermediate
values generated during the recognition process;
these extensions to the representation then serve
as means of indicating to the interpretation process
what operations need to be carried out.
4.1.1 Representing Partial Specification
We use lowercase xs to indicate values that the
interpretation process is required to seek a value
for; and by analogy, we use a lowercase t rather
than an uppercase T as the date?time delimiter in
the structure to indicate when the recogniser is not
able to determine whether the time is am or pm.
This is demonstrated in the following examples;
T-VAL is the attribute we use for intermediate
TIMEX values produced by the recognition pro-
cess.
(5) a. We?ll see you in November.
b. T-VAL="xxxx-11"
(6) a. I expect to see you at half past eight.
b. T-VAL="xxxx-xx-xxt08:59"
(7) a. I saw him back in ?69.
b. T-VAL="xx69"
(8) a. I saw him back in the ?60s.
b. TVAL="xx6"
4.1.2 Representing Relative Specification
To handle the partial interpretation of relative date
and time expressions at the recognition stage, we
35
use two extensions to the notation. The first pro-
vides for simple arithmetic over interpretations,
when combined with a reference date determined
from the context:
(9) a. We?ll see you tomorrow.
b. T-VAL="+0000-00-01"
(10) a. We saw him last year.
b. T-VAL="-0001"
The second provides for expressions where a more
complex computation is required in order to deter-
mine the specific date or time in question:
(11) a. We?ll see him next Thursday.
b. T-VAL=">D4"
(12) a. We saw him last November.
b. T-VAL="<M11"
4.2 Processes
For the recognition process, we use a large collec-
tion of rules written in the JAPE pattern-matching
language provided within GATE (see (Cunning-
ham et al, 2002)). These return intermediate val-
ues of the forms described in the previous section.
Obviously other approaches to recognizing tem-
poral expressions and producing their intermedi-
ate values could be used; in DANTE, there is also
a subsequent check carried out by a dependency
parser to ensure that we have captured the full ex-
tent of the temporal expression.
DANTE?s interpretation process then does the
following. First it determines if the candidate tem-
poral expression identified by the recogniser is in-
deed a temporal expression; this is to deal with
cases where a particular word or phrase annotated
by the recognizer (such as time) can have both
temporal or non-temporal interpretations. Then,
for each candidate that really is a temporal expres-
sion, it computes the interpretation of that tempo-
ral expression.
This second step involves different operations
depending on the type of the intermediate value:
? Underspecified values like xxxx-11 are
combined with the reference date derived
from the document context, with temporal di-
rectionality (i.e., is this date in the future or
in the past?) being determined using tense
information from the host clause.
? Relative values like +0001 are combined
with the reference date in the obvious man-
ner.
? Relative values like >D4 and <M11 make
use of special purpose routines that know
about arithmetic for days and months, so that
the correct behaviour is observed.
5 Conclusions
We have sketched an underlying conceptual model
for temporal expression interpretation, and pre-
sented an intermediate semantic representation
that is consistent with the TIMEX2 standard. We
are making available a corpus of examples tagged
with these intermediate representations; this cor-
pus is derived from the nearly 250 examples in
the TIMEX2 specification, thus demonstrating the
wide coverage of the representation. Our hope is
that this will encourage collaborative development
of tools based on this framework, and further de-
velopment of the conceptual framework itself.
6 Acknowledgements
We acknowledge the support of DSTO, the Aus-
tralian Defence Science and Technology Organi-
sation, in carrying out the work described here.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphical
development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniver-
sary Meeting of the ACL.
R. Dale and P. Mazur. 2006. Local semantics in the
interpretation of temporal expressions. In Proceed-
ings of the Coling/ACL2006 Workshop on Annotat-
ing and Reasoning about Time and Events.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. TIDES 2005 Standard for the Anno-
tation of Temporal Expressions. Technical report,
MITRE, September.
K. Hacioglu, Y. Chen, and B. Douglas. 2005. Au-
tomatic Time Expression Labeling for English and
Chinese Text. In Alexander F. Gelbukh, editor,
Computational Linguistics and Intelligent Text Pro-
cessing, 6th International Conference, CICLing?05,
LNCS, pages 548?559. Springer.
Jerry R. Hobbs and Feng Pan. 2004. An ontology
of time for the semantic web. ACM Transactions
on Asian Language Information Processing, 3(1),
March.
M. Negri and L. Marseglia. 2005. Recognition and
Normalization of Time Expressions: ITC-irst at
TERN 2004. Technical Report WP3.7, Information
Society Technologies, February.
36
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 344?351,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
GLEU: Automatic Evaluation of Sentence-Level Fluency
Andrew Mutton? Mark Dras? Stephen Wan?,? Robert Dale?
?Centre for Language Technology ?Information and Communication Technologies
Macquarie University CSIRO
NSW 2109 Australia NSW 2109 Australia
madras@ics.mq.edu.au
Abstract
In evaluating the output of language tech-
nology applications?MT, natural language
generation, summarisation?automatic eval-
uation techniques generally conflate mea-
surement of faithfulness to source content
with fluency of the resulting text. In this
paper we develop an automatic evaluation
metric to estimate fluency alone, by examin-
ing the use of parser outputs as metrics, and
show that they correlate with human judge-
ments of generated text fluency. We then de-
velop a machine learner based on these, and
show that this performs better than the indi-
vidual parser metrics, approaching a lower
bound on human performance. We finally
look at different language models for gener-
ating sentences, and show that while individ-
ual parser metrics can be ?fooled? depending
on generation method, the machine learner
provides a consistent estimator of fluency.
1 Introduction
Intrinsic evaluation of the output of many language
technologies can be characterised as having at least
two aspects: how well the generated text reflects
the source data, whether it be text in another lan-
guage for machine translation (MT), a natural lan-
guage generation (NLG) input representation, a doc-
ument to be summarised, and so on; and how well it
conforms to normal human language usage. These
two aspects are often made explicit in approaches
to creating the text. For example, in statistical MT
the translation model and the language model are
treated separately, characterised as faithfulness and
fluency respectively (as in the treatment in Jurafsky
and Martin (2000)). Similarly, the ultrasummarisa-
tion model of Witbrock and Mittal (1999) consists
of a content model, modelling the probability that a
word in the source text will be in the summary, and
a language model.
Evaluation methods can be said to fall into two cate-
gories: a comparison to gold reference, or an appeal
to human judgements. Automatic evaluation meth-
ods carrying out a comparison to gold reference tend
to conflate the two aspects of faithfulness and flu-
ency in giving a goodness score for generated out-
put. BLEU (Papineni et al, 2002) is a canonical ex-
ample: in matching n-grams in a candidate transla-
tion text with those in a reference text, the metric
measures faithfulness by counting the matches, and
fluency by implicitly using the reference n-grams as
a language model. Often we are interested in know-
ing the quality of the two aspects separately; many
human judgement frameworks ask specifically for
separate judgements on elements of the task that cor-
respond to faithfulness and to fluency. In addition,
the need for reference texts for an evaluation metric
can be problematic, and intuitively seems unneces-
sary for characterising an aspect of text quality that
is not related to its content source but to the use of
language itself. It is a goal of this paper to provide
an automatic evaluation method for fluency alone,
without the use of a reference text.
One might consider using a metric based on lan-
guage model probabilities for sentences: in eval-
344
uating a language model on (already existing) test
data, a higher probability for a sentence (and lower
perplexity over a whole test corpus) indicates bet-
ter language modelling; perhaps a higher probability
might indicate a better sentence. However, here we
are looking at generated sentences, which have been
generated using their own language model, rather
than human-authored sentences already existing in
a test corpus; and so it is not obvious what language
model would be an objective assessment of sentence
naturalness. In the case of evaluating a single sys-
tem, using the language model that generated the
sentence will only confirm that the sentence does
fit the language model; in situations such as com-
paring two systems which each generate text using
a different language model, it is not obvious that
there is a principled way of deciding on a fair lan-
guage model. Quite a different idea was suggested
in Wan et al (2005), of using the grammatical judge-
ment of a parser to assess fluency, giving a measure
independent of the language model used to gener-
ate the text. The idea is that, assuming the parser
has been trained on an appropriate corpus, the poor
performance of the parser on one sentence relative
to another might be an indicator of some degree of
ungrammaticality and possibly disfluency. In that
work, however, correlation with human judgements
was left uninvestigated.
The goal of this paper is to take this idea and de-
velop it. In Section 2 we look at some related work
on metrics, in particular for NLG. In Section 3, we
verify whether parser outputs can be used as esti-
mators of generated sentence fluency by correlating
them with human judgements. In Section 4, we pro-
pose an SVM-based metric using parser outputs as
features, and compare its correlation against human
judgements with that of the individual parsers. In
Section 5, we investigate the effects on the various
metrics from different types of language model for
the generated text. Then in Section 6 we conclude.
2 Related Work
In terms of human evaluation, there is no uniform
view on what constitutes the notion of fluency, or its
relationship to grammaticality or similar concepts.
We mention a few examples here to illustrate the
range of usage. In MT, the 2005 NIST MT Evalu-
ation Plan uses guidelines1 for judges to assess ?ad-
equacy? and ?fluency? on 5 point scales, where they
are asked to provide intuitive reactions rather than
pondering their decisions; for fluency, the scale de-
scriptions are fairly vague (5: flawless English; 4:
good English; 3: non-native English; 2: disfluent
English; 1: incomprehensible) and instructions are
short, with some examples provided in appendices.
Zajic et al (2002) use similar scales for summari-
sation. By contrast, Pan and Shaw (2004), for their
NLG system SEGUE tied the notion of fluency more
tightly to grammaticality, giving two human evalu-
ators three grade options: good, minor grammatical
error, major grammatical/pragmatic error. As a fur-
ther contrast, the analysis of Coch (1996) was very
comprehensive and fine-grained, in a comparison of
three text-production techniques: he used 14 human
judges, each judging 60 letters (20 per generation
system), and required them to assess the letters for
correct spelling, good grammar, rhythm and flow,
appropriateness of tone, and several other specific
characteristics of good text.
In terms of automatic evaluation, we are not aware
of any technique that measures only fluency or sim-
ilar characteristics, ignoring content, apart from that
of Wan et al (2005). Even in NLG, where, given the
variability of the input representations (and hence
difficulty in verifying faithfulness), it might be ex-
pected that such measures would be available, the
available metrics still conflate content and form.
For example, the metrics proposed in Bangalore et
al. (2000), such as Simple Accuracy and Generation
Accuracy, measure changes with respect to a refer-
ence string based on the idea of string-edit distance.
Similarly, BLEU has been used in NLG, for example
by Langkilde-Geary (2002).
3 Parsers as Evaluators
There are three parts to verifying the usefulness of
parsers as evaluators: choosing the parsers and the
metrics derived from them; generating some texts
for human and parser evaluation; and, the key part,
getting human judgements on these texts and corre-
lating them with parser metrics.
1http://projects.ldc.upenn.edu/TIDES/
Translation/TranAssessSpec.pdf
345
3.1 The Parsers
In testing the idea of using parsers to judge fluency,
we use three parsers, from which we derive four
parser metrics, to investigate the general applicabil-
ity of the idea. Those chosen were the Connexor
parser,2 the Collins parser (Collins, 1999), and the
Link Grammar parser (Grinberg et al, 1995). Each
produces output that can be taken as representing
degree of ungrammaticality, although this output is
quite different for each.
Connexor is a commercially available dependency
parser that returns head?dependant relations as well
as stemming information, part of speech, and so on.
In the case of an ungrammatical sentence, Connexor
returns tree fragments, where these fragments are
defined by transitive head?dependant relations: for
example, for the sentence Everybody likes big cakes
do it returns fragments for Everybody likes big cakes
and for do. We expect that the number of fragments
should correlate inversely with the quality of a sen-
tence. For a metric, we normalise this number by
the largest number of fragments for a given data set.
(Normalisation matters most for the machine learner
in Section 4.)
The Collins parser is a statistical chart parser that
aims to maximise the probability of a parse using dy-
namic programming. The parse tree produced is an-
notated with log probabilities, including one for the
whole tree. In the case of ungrammatical sentences,
the parser will assign a low probability to any parse,
including the most likely one. We expect that the
log probability (becoming more negative as the sen-
tence is less likely) should correlate positively with
the quality of a sentence. For a metric, we normalise
this by the most negative value for a given data set.
Like Connexor, the Link Grammar parser returns in-
formation about word relationships, forming links,
with the proviso that links cannot cross and that in
a grammatical sentence all links are indirectly con-
nected. For an ungrammatical sentence, the parser
will delete words until it can produce a parse; the
number it deletes is called the ?null count?. We ex-
pect that this should correlate inversely with sen-
tence quality. For a metric, we normalise this by
the sentence length. In addition, the parser produces
2http://www.connexor.com
another variable possibly of interest. In generating
a parse, the parser produces many candidates and
rules some out by a posteriori constraints on valid
parses. In its output the parser returns the number of
invalid parses. For an ungrammatical sentence, this
number may be higher; however, there may also be
more parses. For a metric, we normalise this by the
total number of parses found for the sentence. There
is no strong intuition about the direction of correla-
tion here, but we investigate it in any case.
3.2 Text Generation Method
To test whether these parsers are able to discriminate
sentence-length texts of varying degrees of fluency,
we need first to generate texts that we expect will be
discriminable in fluency quality ranging from good
to very poor. Below we describe our method for gen-
erating text, and then our preliminary check on the
discriminability of the data before giving them to hu-
man judges.
Our approach to generating ?sentences? of a fixed
length is to take word sequences of different lengths
from a corpus and glue them together probabilisti-
cally: the intuition is that a few longer sequences
glued together will be more fluent than many shorter
sequences. More precisely, to generate a sentence of
length n, we take sequences of length l (such that l
divides n), with sequence i of the form wi,1 . . . wi,l,
where wi, is a word or punctuation mark. We start
by selecting sequence 1, first by randomly choos-
ing its first word according to the unigram probabil-
ity P (w1,1), and then the sequence uniformly ran-
domly over all sequences of length l starting with
w1,1; we select subsequent sequences j (2 ? j ?
n/l) randomly according to the bigram probability
P (wj,1 |wj?1,l). Taking as our corpus the Reuters
corpus,3 for length n = 24, we generate sentences
for sequence sizes l = 1, 2, 4, 8, 24 as in Figure 1.
So, for instance, the sequence-size 8 example was
constructed by stringing together the three consecu-
tive sequences of length 8 (There . . . to; be . . . have;
to . . . .) taken from the corpus.
These examples, and others generated, appear to
be of variable quality in accordance with our intu-
ition. However, to confirm this prior to testing them
3http://trec.nist.gov/data/reuters/
reuters.html
346
Extracted (Sequence-size 24)
Ginebra face Formula Shell in a sudden-death playoff on Sun-
day to decide who will face Alaska in a best-of-seven series for
the title.
Sequence-size 8
There is some thinking in the government to be nearly as dra-
matic as some people have to be slaughtered to eradicate the
epidemic.
Sequence-size 4
Most of Banharn?s move comes after it can still be averted the
crash if it should again become a police statement said.
Sequence-size 2
Massey said in line with losses, Nordbanken is well-placed to
benefit abuse was loaded with Czech prime minister Andris
Shkele, said.
Sequence-size 1
The war we?re here in a spokesman Jeff Sluman 86 percent jump
that Spain to what was booked, express also said.
Figure 1: Sample sentences from the first trial
Description Correlation
Small 0.10 to 0.29
Medium 0.30 to 0.49
Large 0.50 to 1.00
Table 1: Correlation coefficient interpretation
out for discriminability in a human trial, we wanted
see whether they are discriminable by some method
other than our own judgement. We used the parsers
described in Section 3.1, in the hope of finding a
non-zero correlation between the parser outputs and
the sequence lengths.
Regarding the interpretation of the absolute value of
(Pearson?s) correlation coefficients, both here and in
the rest of the paper, we adopt Cohen?s scale (Co-
hen, 1988) for use in human judgements, given in
Table 1; we use this as most of this work is to do with
human judgements of fluency. For data, we gener-
ated 1000 sentences of length 24 for each sequence
length l = 1, 2, 3, 4, 6, 8, 24, giving 7000 sentences
in total. The correlations with the four parser out-
puts are as in Table 2, with the medium correlations
for Collins and Link Grammar (nulled tokens) indi-
cating that the sentences are indeed discriminable to
some extent, and hence the approach is likely to be
useful for generating sentences for human trials.
3.3 Human Judgements
The next step is then to obtain a set of human judge-
ments for this data. Human judges can only be ex-
pected to judge a reasonably sized amount of data,
Metric Corr.
Collins Parser 0.3101
Connexor -0.2332
Link Grammar Nulled Tokens -0.3204
Link Grammar Invalid Parses 0.1776
GLEU 0.4144
Table 2: Parser vs sequence size for original data set
so we first reduced the set of sequence sizes to be
judged. To do this we determined for the 7000
generated sentences the scores according to the (ar-
bitrarily chosen) Collins parser, and calculated the
means for each sequence size and the 95% confi-
dence intervals around these means. We then chose
a subset of sequence sizes such that the confidence
intervals did not overlap: 1, 2, 4, 8, 24; the idea was
that this would be likely to give maximally discrim-
inable sentences. For each of these sequences sizes,
we chose randomly 10 sentences from the initial set,
giving a set for human judgement of size 50.
The judges consisted of twenty volunteers, all native
English speakers without explicit linguistic training.
We gave them general guidelines about what consti-
tuted fluency, mentioning that they should consider
grammaticality but deliberately not giving detailed
instructions on the manner for doing this, as we were
interested in the level of agreement of intuitive un-
derstanding of fluency. We instructed them also that
they should evaluate the sentence without consider-
ing its content, using Colourless green ideas sleep
furiously as an example of a nonsensical but per-
fectly fluent sentence. The judges were then pre-
sented with the 50 sentences in random order, and
asked to score the sentences according to their own
scale, as in magnitude estimation (Bard et al, 1996);
these scores were then normalised in the range [0,1].
Some judges noted that the task was difficult be-
cause of its subjectivity. Notwithstanding this sub-
jectivity and variation in their approach to the task,
the pairwise correlations between judges were high,
as indicated by the maximum, minimum and mean
values in Table 3, indicating that our assumption
that humans had an intuitive notion of fluency
and needed only minimal instruction was justified.
Looking at mean scores for each sequence size,
judges generally also ranked sentences by sequence
size; see Figure 2. Comparing human judgement
347
Statistic Corr.
Maximum correlation 0.8749
Minimum correlation 0.4710
Mean correlation 0.7040
Standard deviation 0.0813
Table 3: Data on correlation between humans
Figure 2: Mean scores for human judges
correlations against sequence size with the same cor-
relations for the parser metrics (as for Table 2, but on
the human trial data) gives Table 4, indicating that
humans can also discriminate the different generated
sentence types, in fact (not surprisingly) better than
the automatic metrics.
Now, having both human judgement scores of some
reliability for sentences, and scoring metrics from
three parsers, we give correlations in Table 5. Given
Cohen?s interpretation, the Collins and Link Gram-
mar (nulled tokens) metrics show moderate correla-
tion, the Connexor metric almost so; the Link Gram-
mar (invalid parses) metric correlation is by far the
weakest. The consistency and magnitude of the first
three parser metrics, however, lends support to the
idea of Wan et al (2005) to use something like these
as indicators of generated sentence fluency. The aim
of the next section is to build a better predictor than
the individual parser metrics alone.
Metric Corr.
Humans 0.6529
Collins Parser 0.4057
Connexor -0.3804
Link Grammar Nulled Tokens -0.3310
Link Grammar Invalid Parses 0.1619
GLEU 0.4606
Table 4: Correlation with sequence size for human
trial data set
Metric Corr.
Collins Parser 0.3057
Connexor -0.3445
Link-Grammar Nulled Tokens -0.2939
Link Grammar Invalid Parses 0.1854
GLEU 0.4014
Table 5: Correlation between metrics and human
evaluators
4 An SVM-Based Metric
In MT, one problem with most metrics like BLEU
is that they are intended to apply only to document-
length texts, and any application to individual sen-
tences is inaccurate and correlates poorly with
human judgements. A neat solution to poor
sentence-level evaluation proposed by Kulesza and
Shieber (2004) is to use a Support Vector Machine,
using features such as word error rate, to estimate
sentence-level translation quality. The two main in-
sights in applying SVMs here are, first, noting that
human translations are generally good and machine
translations poor, that binary training data can be
created by taking the human translations as posi-
tive training instances and machine translations as
negative ones; and second, that a non-binary metric
of translation goodness can be derived by the dis-
tance from a test instance to the support vectors. In
an empirical evaluation, Kulesza and Shieber found
that their SVM gave a correlation of 0.37, which
was an improvement of around half the gap between
the BLEU correlations with the human judgements
(0.25) and the lowest pairwise human inter-judge
correlation (0.46) (Turian et al, 2003).
We take a similar approach here, using as features
the four parser metrics described in Section 3. We
trained an SVM,4 taking as positive training data
the 1000 instances of sentences of sequence length
24 (i.e. sentences extracted from the corpus) and
as negative training data the 1000 sentences of se-
quence length 1. We call this learner GLEU.5
As a check on the ability of the GLEU SVM to dis-
tinguish these ?positive? sentences from ?negative?
ones, we evaluated its classification accuracy on a
(new) test set of size 300, split evenly between sen-
tences of sequence length 24 and sequence length 1.
4We used the package SVM-light (Joachims, 1999).
5For GrammaticaLity Evaluation Utility.
348
This gave 81%, against a random baseline of 50%,
indicating that the SVM can classify satisfactorily.
We now move from looking at classification accu-
racy to the main purpose of the SVM, using distance
from support vector as a metric. Results are given
for correlation of GLEU against sequence sizes for
all data (Table 2) and for the human trial data set
(Table 4); and also for correlation of GLEU against
the human judges? scores (Table 5). This last indi-
cates that GLEU correlates better with human judge-
ments than any of the parsers individually, and is
well within the ?moderate? range for correlation in-
terpretation. In particular, for the GLEU?human cor-
relation, the score of 0.4014 is approaching the min-
imum pairwise human correlation of 0.4710.
5 Different Text Generation Methods
The method used to generate text in Section 3.2 is
a variation of the standard n-gram language model.
A question that arises is: Are any of the metrics de-
fined above strongly influenced by the type of lan-
guage model used to generate the text? It may be the
case, for example, that a parser implementation uses
its own language model that predisposes it to favour
a similar model in the text generation process. This
is a phenomenon seen in MT, where BLEU seems to
favour text that has been produced using a similar
statistical n-gram language model over other sym-
bolic models (Callison-Burch et al, 2006).
Our previous approach used only sequences of
words concatenated together. To define some new
methods for generating text, we introduced varying
amounts of structure into the generation process.
5.1 Structural Generation Methods
PoStag In the first of these, we constructed a
rough approximation of typical sentence grammar
structure by taking bigrams over part-of-speech
tags.6 Then, given a string of PoS tags of length
n, t1 . . . tn, we start by assigning the probabilities
for the word in position 1, w1, according to the con-
ditional probability P (w1 | t1). Then, for position j
(2 ? j ? n), we assign to candidate words the value
P (wj | tj)?P (wj |wj?1) to score word sequences.
6We used the supertagger of Bangalore and Joshi (1999).
So, for example, we might generate the PoS tag tem-
plate Det NN Adj Adv, take all the words corre-
sponding to each of these parts of speech, and com-
bine bigram word sequence probability with the con-
ditional probability of words with respect to these
parts of speech. We then use a Viterbi-style algo-
rithm to find the most likely word sequence.
In this model we violate the Markov assumption of
independence in much the same way as Witbrock
and Mittal (1999) in their combination of content
and language model probabilities, by backtracking
at every state in order to discourage repeated words
and avoid loops.
Supertag This is a variant of the approach above,
but using supertags (Bangalore and Joshi, 1999) in-
stead of PoS tags. The idea is that the supertags
might give a more fine-grained definition of struc-
ture, using partial trees rather than parts of speech.
CFG We extracted a CFG from the ?10% of the
Penn Treebank found in the NLTK-lite corpora.7
This CFG was then augmented with productions de-
rived from the PoS-tagged data used above. We then
generated a template of length n pre-terminal cate-
gories using this CFG. To avoid loops we biased the
selection towards terminals over non-terminals.
5.2 Human Judgements
We generated sentences according to a mix of the
initial method of Section 3.2, for calibration, and
the new methods above. We again used a sentence
length of 24, and sequence lengths for the initial
method of l = 1, 8, 24. A sample of sentences gen-
erated for each of these six types is in Figure 3.
For our data, we generated 1000 sentences per gen-
eration method, giving a corpus of 6000 sentences.
For the human judgements we also again took 10
sentences per generation method, giving 60 sen-
tences in total. The same judges were given the same
instructions as previously.
Before correlating the human judges? scores and
the parser outputs, it is interesting to look at how
each parser treats the sentence generation methods,
and how this compares with human ratings (Ta-
ble 6). In particular, note that the Collins parser rates
the PoStag- and Supertag-generated sentences more
7http://nltk.sourceforge.net
349
Extracted (Sequence-size 24)
After a near three-hour meeting and last-minute talks with Pres-
ident Lennart Meri, the Reform Party council voted overwhelm-
ingly to leave the government.
Sequence-size 8
If Denmark is closely linked to the Euro Disney reported a net
profit of 85 million note: the figures were rounded off.
Sequence-size 1
Israelis there would seek approval for all-party peace now com-
plain that this year, which shows demand following year and 56
billion pounds.
POS-tag, Viterbi-mapped
He said earlier the 9 years and holding company?s government,
including 69.62 points as a number of last year but market.
Supertag, Viterbi-mapped
That 97 saying he said in its shares of the market 74.53 percent,
adding to allow foreign exchange: I think people.
Context-Free Grammar
The production moderated Chernomyrdin which leveled gov-
ernment back near own 52 over every a current at from the said
by later the other.
Figure 3: Sample sentences from the second trial
sent. type s-24 s-8 s-1 PoS sup. CFG
Collins 0.52 0.48 0.41 0.60 0.57 0.36
Connexor 0.12 0.16 0.24 0.26 0.25 0.43
LG (null) 0.02 0.06 0.10 0.09 0.11 0.18
LG (invalid) 0.78 0.67 0.56 0.62 0.66 0.53
GLEU 1.07 0.32 -0.96 0.28 -0.06 -2.48
Human 0.93 0.67 0.44 0.39 0.44 0.31
Table 6: Mean normalised scores per sentence type
highly even than real sentences (in bold). These
are the two methods that use the Viterbi-style algo-
rithm, suggesting that this probability maximisation
has fooled the Collins parser. The pairwise correla-
tion between judges was around the same on average
as in Section 3.3, but with wider variation (Table 7).
The main results, determining the correlation of the
various parser metrics plus GLEU against the new
data, are in Table 8. This confirms the very vari-
able performance of the Collins parser, which has
dropped significantly. GLEU performs quite consis-
tently here, this time a little behind the Link Gram-
mar (nulled tokens) result, but still with a better
correlation with human judgement than at least two
Statistic Corr.
Maximum correlation 0.9048
Minimum correlation 0.3318
Mean correlation 0.7250
Standard deviation 0.0980
Table 7: Data on correlation between humans
Metric Corr.
Collins Parser 0.1898
Connexor -0.3632
Link-Grammar Nulled Tokens -0.4803
Link Grammar Invalid Parses 0.1774
GLEU 0.4738
Table 8: Correlation between parsers and human
evaluators on new human trial data
Metric Corr.
Collins Parser 0.2313
Connexor -0.2042
Link-Grammar Nulled Tokens -0.1289
Link Grammar Invalid Parses -0.0084
GLEU 0.4312
Table 9: Correlation between parsers and human
evaluators on all human trial data
judges with each other. (Note also that the GLEU
SVM was not retrained on the new sentence types.)
Looking at all the data together, however, is where
GLEU particularly displays its consistency. Aggre-
gating the old human trial data (Section 3.3) and the
new data, and determining correlations against the
metrics, we get the data in Table 9. Again the SVM?s
performance is consistent, but is now almost twice
as high as its nearest alternative, Collins.
5.3 Discussion
In general, there is at least one parser that correlates
quite well with the human judges for each sentence
type. With well-structured sentences, the probabilis-
tic Collins parser performs best; on sentences that
are generated by a poor probabilistic model lead-
ing to poor structure, Link Grammar (nulled tokens)
performs best. This supports the use of a machine
learner taking as features outputs from several parser
types; empirically this is confirmed by the large ad-
vantage GLEU has on overall data (Table 9).
The generated text itself from the Viterbi-based gen-
erators as implemented here is quite disappoint-
ing, given an expectation that introducing structure
would make sentences more natural and hence lead
to a range of sentence qualities. In hindsight, this
is not so surprising; in generating the structure tem-
plate, only sequences (over tags) of size 1 were used,
which is perhaps why the human judges deemed
them fairly close to sentences generated by the origi-
350
nal method using sequence size 1, the poorest of that
initial data set.
6 Conclusion
In this paper we have investigated a new approach to
evaluating the fluency of individual generated sen-
tences. The notion of what constitutes fluency is
an imprecise one, but trials with human judges have
shown that even if it cannot be exactly defined, or
even articulated by the judges, there is a high level
of agreement about what is fluent and what is not.
Given this data, metrics derived from parser out-
puts have been found useful for measuring fluency,
correlating up to moderately well with these human
judgements. A better approach is to combine these
in a machine learner, as in our SVM GLEU, which
outperforms individual parser metrics. Interestingly,
we have found that the parser metrics can be fooled
by the method of sentence generation; GLEU, how-
ever, gives a consistent estimate of fluency regard-
less of generation type; and, across all types of gen-
erated sentences examined in this paper, is superior
to individual parser metrics by a large margin.
This all suggests that the approach has promise, but
it needs to be developed further for pratical use. The
SVM presented in this paper has only four features;
more features, and in particular a wider range of
parsers, should raise correlations. In terms of the
data, we looked only at sentences generated with
several parameters fixed, such as sentence length,
due to our limited pool of judges. In future we would
like to examine the space of sentence types more
fully. In particular, we will look at predicting the flu-
ency of near-human quality sentences. More gener-
ally, we would like to look also at how the approach
of this paper would relate to a perplexity-based met-
ric; how it compares against BLEU or similar mea-
sures as a predictor of fluency in a context where ref-
erence sentences are available; and whether GLEU
might be useful in applications such as reranking of
candidate sentences in MT.
Acknowledgements
We thank Ben Hutchinson and Mirella Lapata for discussions,
and Srinivas Bangalore for the TAG supertagger. The sec-
ond author acknowledges the support of ARC Discovery Grant
DP0558852.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguistics,
25(2):237?265.
Srinivas Bangalore, Owen Rambow, and Steve Whittaker.
2000. Evaluation metrics for generation. In Proceedings of the
First International Natural Language Generation Conference
(INLG2000), Mitzpe Ramon, Israel.
E. Bard, D. Robertson, and A. Sorace. 1996. Magnitude esti-
mation and linguistic acceptability. Language, 72(1):32?68.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the Role of Bleu in Machine Translation
Research. In Proceedings of EACL, pages 249?256.
Jose? Coch. 1996. Evaluating and comparing three text-
production strategies. In Proceedings of the 16th International
Conference on Computational Linguistics (COLING?96), pages
249?254.
J. Cohen. 1988. Statistical power analysis for the behavioral
sciences. Erlbaum, Hillsdale, NJ, US.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
Dennis Grinberg, John Lafferty, and Daniel Sleator. 1995. A
robus parsing algorithm for link grammars. In Proceedings of
the Fourth International Workshop on Parsing Technologies.
Thorsten Joachims. 1999. Making Large-Scale SVM Learning
Practical. MIT Press.
Daniel Jurafsky and James Martin. 2000. Speech and Lan-
guage Processing: An Introduction to Natural Languge Pro-
cessing, Computational Linguistics, and Speech Recognition.
Prentice-Hall.
Alex Kulesza and Stuart Shieber. 2004. A learning approach to
improving sentence-level MT evaluation. In Proceedings of the
10th International Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, Baltimore, MD, US.
Irene Langkilde-Geary. 2002. An empirical verification of cov-
erage and correctness for a general-purpose sentence generator.
In Proceedings of the International Natural Language Genera-
tion Conference (INLG) 2002, pages 17?24.
Shimei Pan and James Shaw. 2004. Segue: A hybrid case-
based surface natural language generator. In Proceedings of
the International Conference on Natural Language Generation
(INLG) 2004, pages 130?140.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. Technical Report RC22176, IBM.
Joseph Turian, Luke Shen, and I. Dan Melamed. 2003. Evalua-
tion of Machine Translation and its evaluation. In Proceedings
of MT Summit IX, pages 23?28.
Stephen Wan, Robert Dale, Mark Dras, and Ce?cile Paris. 2005.
Searching for grammaticality: Propagating dependencies in the
Viterbi algorithm. In Proceedings of the 10th European Natural
Language Processing Wworkshop, Aberdeen, UK.
Michael Witbrock and Vibhu Mittal. 1999. Ultra-
summarization: A statistical approach to generating highly con-
densed non-executive summaries. In Proceedings of the 22nd
International Conference on Research and Development in In-
formation Retrieval (SIGIR?99).
David Zajic, Bonnie Dorr, and Richard Schwartz. 2002. Au-
tomatic headline generation for newspaper stories. In Pro-
ceedings of the ACL-2002 Workshop on Text Summarization
(DUC2002), pages 78?85.
351
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 301?304,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Validating the web-based evaluation of NLG systems
Alexander Koller
Saarland U.
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Donna Byron
Northeastern U.
dbyron@ccs.neu.edu
Justine Cassell
Northwestern U.
justine@northwestern.edu
Robert Dale
Macquarie U.
Robert.Dale@mq.edu.au
Sara Dalzel-Job
U. of Edinburgh
S.Dalzel-Job@sms.ed.ac.uk
Jon Oberlander
U. of Edinburgh
Johanna Moore
U. of Edinburgh
{J.Oberlander|J.Moore}@ed.ac.uk
Abstract
The GIVE Challenge is a recent shared
task in which NLG systems are evaluated
over the Internet. In this paper, we validate
this novel NLG evaluation methodology by
comparing the Internet-based results with
results we collected in a lab experiment.
We find that the results delivered by both
methods are consistent, but the Internet-
based approach offers the statistical power
necessary for more fine-grained evaluations
and is cheaper to carry out.
1 Introduction
Recently, there has been an increased interest in
evaluating and comparing natural language gener-
ation (NLG) systems on shared tasks (Belz, 2009;
Dale and White, 2007; Gatt et al, 2008). However,
this is a notoriously hard problem (Scott and Moore,
2007): Task-based evaluations with human experi-
mental subjects are time-consuming and expensive,
and corpus-based evaluations of NLG systems are
problematic because a mismatch between human-
generated output and system-generated output does
not necessarily mean that the system?s output is
inferior (Belz and Gatt, 2008). This lack of evalua-
tion methods which are both effective and efficient
is a serious obstacle to progress in NLG research.
The GIVE Challenge (Byron et al, 2009) is a
recent shared task which takes a third approach to
NLG evaluation: By connecting NLG systems to
experimental subjects over the Internet, it achieves
a true task-based evaluation at a much lower cost.
Indeed, the first GIVE Challenge acquired data
from over 1100 experimental subjects online. How-
ever, it still remains to be shown that the results
that can be obtained in this way are in fact com-
parable to more established task-based evaluation
efforts, which are based on a carefully selected sub-
ject pool and carried out in a controlled laboratory
environment. By accepting connections from arbi-
trary subjects over the Internet, the evaluator gives
up control over the subjects? behavior, level of lan-
guage proficiency, cooperativeness, etc.; there is
also an issue of whether demographic factors such
as gender might skew the results.
In this paper, we provide the missing link by
repeating the GIVE evaluation in a laboratory en-
vironment and comparing the results. It turns out
that where the two experiments both find a signif-
icant difference between two NLG systems with
respect to a given evaluation measure, they always
agree. However, the Internet-based experiment
finds considerably more such differences, perhaps
because of the higher number of experimental sub-
jects (n = 374 vs. n = 91), and offers other oppor-
tunities for more fine-grained analysis as well. We
take this as an empirical validation of the Internet-
based evaluation of GIVE, and propose that it can
be applied to NLG more generally. Our findings
are in line with studies from psychology that indi-
cate that the results of web-based experiments are
typically consistent with the results of traditional
experiments (Gosling et al, 2004). Nevertheless,
we do find and discuss some effects of the uncon-
trolled subject pool that should be addressed in
future Internet-based NLG challenges.
2 The GIVE Challenge
In the GIVE scenario (Byron et al, 2009), users
try to solve a treasure hunt in a virtual 3D world
that they have not seen before. The computer has
complete information about the virtual world. The
challenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
From the perspective of the users, GIVE con-
sists in playing a 3D game which they start from
a website. The game displays a virtual world and
allows the user to move around in the world and
manipulate objects; it also displays the generated
301
instructions. The first room in each game is a tuto-
rial room in which users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Players
can either finish a game successfully, lose it by
triggering an alarm, or cancel the game at any time.
When a user starts the game, they are randomly
connected to one of the three worlds and one of the
NLG systems. The GIVE-1 Challenge evaluated
five NLG systems, which we abbreviate as A, M,
T, U, and W below. A running GIVE NLG system
has access to the current state of the world and to
an automatically computed plan that tells it what
actions the user should perform to solve the task. It
is notified whenever the user performs some action,
and can generate an instruction and send it to the
client for display at any time.
3 The experiments
The web experiment. For the GIVE-1 challenge,
1143 valid games were collected over the Internet
over the course of three months. These were dis-
tributed over three evaluation worlds (World 1: 374,
World 2: 369, World 3: 400). A game was consid-
ered valid if the game client didn?t crash, the game
wasn?t marked as a test run by the developers, and
the player completed the tutorial.
Of these games, 80% were played by males and
10% by females (the remaining 10% of the partic-
ipants did not specify their gender). The players
were widely distributed over countries: 37% con-
nected from IP addresses in the US, 33% from
Germany, and 17% from China; the rest connected
from 45 further countries. About 34% of the par-
ticipants self-reported as native English speakers,
and 62% specified a language proficiency level of
at least ?expert? (3 on a 5-point scale).
The lab experiment. We repeated the GIVE-1
evaluation in a traditional laboratory setting with
91 participants recruited from a college campus.
In the lab, each participant played the GIVE game
once with each of the five NLG systems. To avoid
learning effects, we only used the first game run
from each subject in the comparison with the web
experiment; as a consequence, subjects were dis-
tributed evenly over the NLG systems. To accom-
modate for the much lower number of participants,
the laboratory experiment only used a single game
world ? World 1, which was known from the online
version to be the easiest world.
Among this group of subjects, 93% self-rated
their English proficiency as ?expert? or better; 81%
were native speakers. In contrast to the online ex-
periment, 31% of participants were male and 65%
were female (4% did not specify their gender).
Results: Objective measures. The GIVE soft-
ware automatically recorded data for five objec-
tive measures: the percentage of successfully com-
pleted games and, for the successfully completed
games, the number of instructions generated by
the NLG system, of actions performed by the user
(such as pushing buttons), of steps taken by the
user (i.e., actions plus movements), and the task
completion time (in seconds).
Fig. 1 shows the results for the objective mea-
sures collected in both experiments. To make the
results comparable, the table for the Internet ex-
periment only includes data for World 1. The task
success rate is only evaluated on games that were
completed successfully or lost, not cancelled, as
laboratory subjects were asked not to cancel. This
brings the number of Internet subjects to 322 for
the success rate, and to 227 (only successful games)
for the other measures.
Task success is the percentage of successfully
completed games; the other measures are reported
as means. The chart assigns systems to groups A
through C or D for each evaluation measure. Sys-
tems in group A are better than systems in group
B, and so on; if two systems have no letter in com-
mon, the difference between them is significant
with p < 0.05. Significance was tested using a ?
2
-
test for task success and ANOVAs for instructions,
steps, actions, and seconds. These were followed
by post hoc tests (pairwise ?
2
and Tukey) to com-
pare the NLG systems pairwise.
Results: Subjective measures. Users were
asked to fill in a questionnaire collecting subjec-
tive ratings of various aspects of the instructions.
For example, users were asked to rate the overall
quality of the direction giving system (on a 7-point
scale), the choice of words and the referring ex-
pressions (on 5-point scales), and they were asked
whether they thought the instructions came at the
right time. Overall, there were twelve subjective
measures (see (Byron et al, 2009)), of which we
only present four typical ones for space reasons.
For each question, the user could choose not to
answer. On the Internet, subjects made consider-
able use of this option: for instance, 32% of users
302
Objective Measures Subjective Measures
task
success
instructions steps actions seconds overall
choice
of words
referring
expressions
timing
A 91% A 83.4 B 99.8 A 9.4 A 123.9 A 4.7 A 4.7 A 4.7 A 81% A
M 76% B 68.1 A 145.1 B 10.0 AB 195.4 BC 3.8 AB 3.8 B 4.0 B 70% ABC
T 85% AB 97.8 C 142.1 B 9.7 AB 174.4 B 4.4 B 4.4 AB 4.3 AB 73% AB
U 93% AB 99.8 C 142.6 B 10.3 B 194.0 BC 4.0 B 4.0 B 4.0 B 51% C
W 24% C 159.7 D 256.0 C 9.6 AB 234.1 C 3.8 AB 3.8 B 4.2 AB 50% BC
A 100% A 78.2 AB 93.4 A 9.9 A 143.9 A 5.7 A 4.7 A 4.8 A 92% A B
M 95% A 66.3 A 141.8 B 10.5 A 211.8 B 5.4 A 3.8 B 4.3 A 95% A B
T 93% A 107.2 CD 134.6 B 9.6 A 205.6 B 4.9 A 4.5 A B 4.4 A 64% A B
U 100% A 88.8 BC 128.8 B 9.8 A 195.1 AB 5.7 A 4.7 A 4.3 A 100% A
W 17% B 134.5 D 213.5 C 10.0 A 252.5 B 5.0 A 4.5 A B 4.0 A 100% B
Figure 1: Objective and selected subjective measures on the web (top) and in the lab (bottom).
didn?t fill in the ?overall evaluation? field of the
questionnaire. In the laboratory experiment, the
subjects were asked to fill in the complete question-
naire and the response rate is close to 100%.
The results for the four selected subjective mea-
sures are summarized in Fig. 1 in the same way as
the objective measures. Also as above, the table
is based only on successfully completed games in
World 1. We will justify this latter choice below.
4 Discussion
The primary question that interests us in a compar-
ative evaluation is which NLG systems performed
significantly better or worse on any given evalua-
tion measure. In the experiments above, we find
that of the 170 possible significant differences (=
17 measures ? 10 pairs of NLG systems), the labo-
ratory experiment only found six that the Internet-
based experiment didn?t find. Conversely, there
are 26 significant differences that only the Internet-
based experiment found. But even more impor-
tantly, all pairwise rankings are consistent across
the two evaluations: Where both systems found a
significant difference between two systems, they al-
ways ranked them in the same order. We conclude
that the Internet experiment provides significance
judgments that are comparable to, and in fact more
precise than, the laboratory experiment.
Nevertheless, there are important differences be-
tween the laboratory and Internet-based results. For
instance, the success rates in the laboratory tend
to be higher, but so are the completion times. We
believe that these differences can be attributed to
the demographic characteristics of the participants.
To substantiate this claim, we looked in some detail
at differences in gender, language proficiency, and
questionnaire response rates.
First, the gender distribution differed greatly be-
Web
games reported mean
success 227 = 61% 93% 4.9
lost 92 = 24% 48% 3.4
cancelled 55 = 15% 16% 3.3
Lab
# games reported mean
success 73 = 80% 100% 5.4
lost 18 = 20% 94% 3.3
cancelled 0 ? ?
Figure 2: Skewed results for ?overall evaluation?.
tween the Internet experiment (10% female) and
the laboratory experiment (65% female). This is
relevant because gender had a significant effect
on task completion time (women took longer) and
on six subjective measures including ?overall eval-
uation? in the laboratory. We speculate that the
difference in task completion time may be related
to well-known gender differences in processing
navigation instructions (Moffat et al, 1998).
Second, the two experiments collected data from
subjects of different language proficiencies. While
93% of the participants in the laboratory experi-
ment self-rated their English proficiency as ?expert?
or better, only 62% of the Internet participants did.
This partially explains the lower task success rates
on the Internet, as Internet subjects with English
proficiencies of 3?5 performed significantly better
on ?task success? than the group with proficiencies
1?2. If we only look at the results of high-English-
proficiency subjects on the Internet, the success
rates for all NLG systems except W rise to at least
86%, and are thus close to the laboratory results.
Finally, the Internet data are skewed by the ten-
dency of unsuccessful participants to not fill in the
questionnaire. Fig. 2 summarizes some data about
the ?overall evaluation? question. Users who didn?t
complete the task successfully tended to judge the
303
systems much lower than successful users, but at
the same time tended not to answer the question
at all. This skew causes the mean subjective judg-
ments across all Internet subjects to be artificially
high. To avoid differences between the laboratory
and the Internet experiment due to this skew, Fig. 1
includes only judgments from successful games.
In summary, we find that while the two experi-
ments made consistent significance judgments, and
the Internet-based evaluation methodology thus
produces meaningful results, the absolute values
they find for the individual evaluation measures
differ due to the demographic characteristics of the
participants in the two studies. This could be taken
as a possible deficit of the Internet-based evalua-
tion. However, we believe that the opposite is true.
In many ways, an online user is in a much more
natural communicative situation than a laboratory
subject who is being discouraged from cancelling
a frustrating task. In addition, every experiment ?
whether in the laboratory or on the Internet ? suf-
fers from some skew in the subject population due
to sampling bias; for instance, one could argue that
an evaluation that is based almost exclusively on na-
tive speakers in universities leads to overly benign
judgments about the quality of NLG systems.
One advantage of the Internet-based approach
to data collection over the laboratory-based one is
that, due to the sheer number of subjects, we can de-
tect such skews and deal with them appropriately.
For instance, we might decide that we are only
interested in the results from proficient English
speakers and ignore the rest of the data; but we
retain the option to run the analysis over all partici-
pants, and to analyze how much each system relies
on the user?s language proficiency. The amount
of data also means that we can obtain much more
fine-grained comparisons between NLG systems.
For instance, the second and third evaluation world
specifically exercised an NLG system?s abilities to
generate referring expressions and navigation in-
structions, respectively, and there were significant
differences in the performance of some systems
across different worlds. Such data, which is highly
valuable for pinpointing specific weaknesses of a
system, would have been prohibitively costly and
time-consuming to collect with laboratory subjects.
5 Conclusion
In this paper, we have argued that carrying out task-
based evaluations of NLG systems over the Internet
is a valid alternative to more traditional laboratory-
based evaluations. Specifically, we have shown
that an Internet-based evaluation of systems in the
GIVE Challenge finds consistent significant differ-
ences as a lab-based evaluation. While the Internet-
based evaluation suffers from certain skews caused
by the lack of control over the subject pool, it does
find more differences than the lab-based evaluation
because much more data is available. The increased
amount of data also makes it possible to compare
the quality of NLG systems across different evalua-
tion worlds and users? language proficiency levels.
We believe that this type of evaluation effort
can be applied to other NLG and dialogue tasks
beyond GIVE. Nevertheless, our results also show
that an Internet-based evaluation risks certain kinds
of skew in the data. It is an interesting question for
the future how this skew can be reduced.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
A. Belz. 2009. That?s nice ... what can you do with it?
Computational Linguistics, 35(1):111?118.
D. Byron, A. Koller, K. Striegnitz, J. Cassell, R. Dale,
J. Moore, and J. Oberlander. 2009. Report on the
First NLG Challenge on Generating Instructions in
Virtual Environments (GIVE). In Proceedings of the
12th European Workshop on Natural Language Gen-
eration (Special session on Generation Challenges).
R. Dale and M. White, editors. 2007. Proceedings
of the NSF/SIGGEN Workshop for Shared Tasks and
Comparative Evaluation in NLG, Arlington, VA.
A. Gatt, A. Belz, and E. Kow. 2008. The TUNA
challenge 2008: Overview and evaluation results.
In Proceedings of the 5th International Natural
Language Generation Conference (INLG?08), pages
198?206.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
S. Moffat, E. Hampson, and M. Hatzipantelis. 1998.
Navigation in a ?virtual? maze: Sex differences and
correlation with psychometric measures of spatial
ability in humans. Evolution and Human Behavior,
19(2):73?87.
D. Scott and J. Moore. 2007. An NLG evaluation com-
petition? Eight reasons to be cautious. In (Dale and
White, 2007).
304
Evangelising Language Technology:
A Practically-Focussed Undergraduate Program
Robert Dale, Diego Moll? Aliod and Rolf Schwitter
Centre for Language Technology
Division of Information and Communication Sciences
Macquarie University, Sydney, Australia
{rdale|diego|rolfs}@ics.mq.edu.au
Abstract
This paper describes an
undergraduate program in Language
Technology that we have developed
at Macquarie University. We
question the industrial relevance of
much that is taught in NLP courses,
and emphasize the need for a
practical orientation as a means to
growing the size of the field. We
argue that a more evangelical
approach, both with regard to
students and industry, is required.
The paper provides an overview of
the material we cover, and makes
some observations for the future on
the basis of our experiences so far.
1 Introduction
This paper describes our experiences in setting
up an undergraduate program in language
technology, with a particular emphasis on the
philosophy that lies behind the decisions we
have made in designing this program.
In Section 2, we sketch the background to
the program, and outline the perspective we
take on teaching in this area. Against this
backdrop, in Section 3, we describe the
orientation and content of the program in some
detail. In Section 4 we discuss the evaluation of
the program, identify some lessons we have
learned regarding what works and what
doesn?t, and point to where we intend to go in
the future.
2 Background
2.1 How The Program Came About
Our program is hosted by the Department of
Computing at Macquarie, which offers a typical
range of computer science courses. At this
university, standard undergraduate degree
programs are three years in length. Students
may elect to stay on for a fourth year in order to
obtain an honours degree, although in a
marketable area like computing, relatively few
students stay on beyond third year. The
teaching year is split into two thirteen week
semesters, with the first semester running from
March through June and the second semester
from August to November.
In 2000, we obtained government funding
to set up an undergraduate program in
language technology.1 To obtain this funding,
we argued that skills in the language
technologies were critical to the development of
the next generations of computer interfaces,
echoing statements made by many both in
industry and academia. Central to our proposal
was the identification of the twin streams of (a)
spoken language interaction and (b) smart text
processing, particularly with regard to the Web;
we took the view that these two major areas
would define the future of commercial NLP
activities over the next five years. Our proposal
emphasised heavily a practical orientation,
whereby we set our goal to be the training of
knowledge workers who will design and
develop practical applications in these areas.
Our proposal was supported by a number of
industry partners, including the Australian
branches of Motorola, Sun Microsystems,
Philips Speech Systems, and the government
research agency CSIRO.
1 We will refrain from entering into an argument
here as to the appropriate semantic distinctions
between the terms ?language technology?, ?natural
language processing? and ?computational linguistics?.
For current purposes, we?ll simply assume that all
three terms effectively cover approximately the same
territory.
                     July 2002, pp. 27-32.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
2.2 Our Philosophical Orientation
Our perception was that, in many institutions,
natural language processing and computational
linguistics courses tended to share two
particular characteristics.
First, relatively few institutions have more
than one course at undergraduate level that
provides material in this area. In many cases,
material in NLP or CL appears only as part of a
more general course on Artificial Intelligence.
This is of course determined by a range of local
factors, including inevitably the interests and
knowledge of available staff. However, an
important factor in many institutions that do not
have a long-established and strong research
group in the area is the widely-held sentiment
that NLP is a somewhat peripheral topic, or a
subject of purely theoretical interest. This makes
it hard for those staff who are interested in
teaching in this area to argue for a significant
presence in the curriculum.
A second observation is that the material
taught in introductory courses often tends to
focus on what we might call computational
syntax: writing grammars and building parsers.
Again, there are good reasons for this: some
would argue that you can?t do much else until
this material is covered, and this is clearly the
corner of NLP that is most well-established with
consolidated results, as reflected by the balance
of coverage found in texts such as [Allen 1995]
and [Jurafsky and Martin 2000], and, perhaps
less so than in the past, the topic coverage at
conferences such as ACL and Coling.2
With regard to the first of these
observations, we take a strong position. If, as a
community, we believe our own rhetoric about
NLP being critical for machine interfaces and
information processing technologies of the
future, then NLP needs to become a much more
central part of computing curricula: every
student should be exposed to this area. Our
desire, presumably shared by most who work in
the area, is to see the field of NLP grow, with
many more knowledgeable practitioners,
particularly in industry.
2 One of the authors recently completed a book
project that had as its goal the production of a
resource that would meet this concern by providing a
more balanced coverage of different aspects of NLP:
see Dale et al[2000]. Unfortunately, this book is too
large and expensive in its current form for use in our
courses.
With regard to our second observation,
however, we take the view that the focus
adopted in much undergraduate teaching in this
area does not support this goal as well as it
might. Teaching students about grammars and
parsers may serve as a suitable introduction to
further study in the area, but the bulk of
students who undertake undergraduate degrees
will go on to work in industry; only a minority
are likely to work in research laboratories or
undertake doctoral studies. Consequently,
those graduates who find themselves in a
position where they might have the opportunity
to use language processing techniques for the
development of sophisticated applications are
unlikely to have the full range of tools they need
at their disposal. The relatively narrow focus of
much undergraduate NLP teaching may also be
in part responsible for the fairly widespread
view amongst the uninitiated that NLP is
basically about parsing and not much else. This
perception results in occasional postings to
bulletin boards where senders from outside the
NLP research community request a ?parser?,
with their queries expressed in terms that make
it clear that they believe this one component will
solve all their NLP problems.
2.3 The Importance of the Job Market
We believe that if NLP is really to grow into a
field of substantial visibility and worth in the
wider industry community, there is a need to
raise the status of study in NLP beyond that of a
niche interest. The key to making this happen is
to emphasize the practical utility of work in the
field.
There is a real chicken-and-egg situation
here. We will only see an explosion in the
number of real NLP applications if there are
more well-rounded NLP practitioners working
in industry exploring and developing those
applications; but students are very savvy about
the job market, and, faced with a choice, are
unlikely to choose an NLP course over, say, a
networking course, when faced with the relative
proportions of job ads they see in the press and
on the web.
There are two related consequences of this.
First, evangelism is critical: we need to get more
trained students out there, offering NLP
solutions to problems. At the same time, we
need to give students concepts and techniques
that enable them to provide those solutions. We
need to provide material that students can see is
relevant, and that can be used in many contexts.
In our analysis, the job market for skills in
language processing, to the extent that it is
identifiable, consists of two major segments.
First, and most obviously, there are
companies that develop voice applications: there
are a great many companies now working in
this area, and voice recognition is a recognized
industry sector.
Second, there are companies that might use
NLP techniques in developing applications that
process, maintain and reuse documents,
whether on the desktop or on the Web. While
the first of these segments is quite clearly
identifiable, it is much more difficult to identify
a sector that focuses on using NLP techniques
on text. With some notable exceptions (and
these are largely small startups), we do not tend
to find companies whose focus is NLP. This is
not really surprising; NLP is just one tool
amongst many that might be used in document
processing, and document processing is
something that crops up in many contexts.
We therefore have a particular challenge
here: we need to communicate to students that
NLP is something they may be able to use in
their future careers, but we can?t point to many
job ads that specifically request NLP skills. The
intuition of those working in the field is that this
stuff ought to be something that can make a
difference in the processing of documents, but
there is not a lot of visible evidence that it is
being used in those situations. Anecdotal
personal experience suggests that many
companies would benefit from the application
of NLP skills but are not aware of this. One
suspects that organizations may often be
making use of techniques that we might want to
think of as NLP, but that these techniques are
not recognized as such.
3 The Program
Given the above, our goal was to construct a
range of courses that covered a broad range of
material that students might be able to use in
their subsequent careers. To emphasise the
practical orientation of what we wanted to do,
we deliberately pitched the program as being
concerned with Language Technology, rather
than as a program in either Natural Language
Processing or Computational Linguistics.
There is clearly something of an evangelical
element to this: we wanted to make students
aware of a broad range of techniques that we
would label Language Technology, with the
goal that, over time and as these students enter
the work force, an awareness would start to
spread that these techniques are widely usable.
This is not a short-term strategy: it takes several
years for the results of these efforts to permeate
through the system to a stage where they can be
evaluated, but it is essential to get started.
In this section, we present a summary of the
material we deliver in the courses that make up
our program. More detail on each of these
courses, and the program as a whole, can be
found at http://www.clt.mq.edu.au/Teaching.
The program consists of four courses that focus
principally on Language Technology, and an
additional course that looks more broadly at
technologies for working with the web. Figure 1
shows the prerequisite structure that currently
holds between these courses.
3.1 Comp248: An Introduction to
Natural Language Processing
Taught in the second half of second year, this is
the course in our program that most closely
matches the typical undergraduate NLP course.
The design of this course was driven by a desire
to show students that they could build a useful,
functioning application using NLP techniques;
to this end, we felt it was important not to teach
only computational syntax, but also something
about semantics. Our position here is that
syntactic processing is only a means to an end,
348:
Intelligent Text
Processing
248:
Introduction to
NLP
249:
Web Technology
349:
Interactive NL
Systems
448:
Advanced
Topics in NLP
Figure 1 : The Prerequisite Structure
and we felt it important to quickly get students
to the stage where they could actually see some
practical import of what they were doing. To
this end, in the first half of the course we take a
fairly standard approach to teaching Prolog,
whereby the students do some rudimentary
morphological processing, build some Definite
Clause Grammars, and learn about parsing
techniques. In the second half of the course, we
add semantics to the mix: although we teach an
introduction to lambda calculus at this stage, for
the practical work we focus on a much
shallower approach to semantics (effectively
semantic grammars), and the students build a
NL database query system that allows them to
ask questions of a database of flights. Along the
way they learn about unification-based
grammar, case frames, lexical resources,
WordNet, and semantic networks. The guiding
principle throughout is relevance to building a
practical application.
3.2 Comp249: Web Technology
Although this course is part of our Language
Technology program, it does not contain a
significant language technology element (at
least as the term is currently construed). It turns
out that the background material taught here
has proven to be very useful in other courses we
teach, so we are considering binding this course
more tightly to the others. The course covers:
Perl programming, web design, client-server
computing, search engines, XML and related
technologies, database integration, privacy and
security, VoiceXML, and content management;
inevitably, with such broad coverage, most
topics are treated relatively briefly.
Our goal for this course is to target a
student body who have little awareness of what
NLP is and to get them to see LT in a wider
perspective. The success of this course, which is
by far the most popular of the units in the
program, has led us to explore better ways of
leveraging this interest.
3.3 Comp348: Intelligent Text
Processing
At the third year level, we offer two courses that
take the second year material as a base. We
noted earlier that we viewed the job market as
consisting of two relatively distinct sectors, one
concerned with voice processing and one
concerned with document processing. This
perception is very deliberately reflected in the
individual biases of the third year offerings;
Comp348 addresses the needs of document
processing, whereas Comp349, discussed later,
leans more towards voice processing.
The course on intelligent text processing
covers basics of text processing using Perl;
tokenisation and sentence segmentation, text
summarisation; information retrieval; corpus-
based approaches, part of speech tagging, word
sense disambiguation, information extraction;
and machine translation. Again, this is a lot of
material to cover, and inevitably we only skim
the surface of many topics. However, in the first
offering of the course, students did significant
assignments in both text summarisation (using
sentence extraction) and information extraction.
The latter assignment was run roughly along the
lines of the Message Understanding
Conferences: using conference announcements
as a data set, the students were provided with a
training set on the basis of which they built an
information extraction system; this was then
tested against unseen data, and scores were
automatically derived. Now in its second
offering, our intention is to use anaphor
resolution as the focus of an assignment.
Our goal in this course is to provide
students with a toolset for text processing from
a language technology perspective. We focus on
relatively shallow methods, since these are the
methods students are most likely to find
themselves using in their subsequent careers.
Our driving aim here is for our alumni to
recognize that LT provides solutions.
3.4 Comp349: Interactive Natural
Language Systems
As already indicated, this course aims to
provide knowledge that students need in order
to be effective in the voice processing industry
sector.
The focus here is on, effectively, text- and
speech-based dialog systems. In the first half of
the course, we cover a significant amount of
relatively theoretical material, covering question
answering systems, database interfaces, and
answer extraction. Students build a quite
sophisticated text-based natural language query
system.
In the second half of the course, we attempt
to apply the theoretical ideas in the very
practical context of building spoken language
dialog systems. We begin by using the CSLU
Toolkit3, which the students use to build a voice
banking application. We then introduce
VoiceXML in some detail; using a PC-based
development environment, students build a
simple flight reservations system.4
We place a heavy emphasis here on aspects
of voice user inferface (VUI) design; in the
practical half of the course, the materials we use
take a similar approach to that taken in vendor
courses that aim to train dialog designers and
grammar writers. At the same time, we have as
an important aim a clear exposition of the
relationship between the ideas explored in
research systems and commercially deployed
systems; in practice it can be very hard to see a
path from the former to the latter. We make
clear to students that our goal is to teach them
how to build practical dialog applications now,
but to get them to think about what the next
generations of such applications might be in the
light of the results that come out of research
laboratories.
3.5 Comp448: Advanced Topics in
Natural Language Processing
For those students who stay on for a fourth year,
we run a course that is more driven by a
selection of specific research topics. At the time
of writing, the first offering of this course is
being delivered. We are using the course to
cover in more depth core topics that are only
really touched upon in earlier courses, with
more detailed exploration of word sense
disambiguation, anaphora resolution, discourse
structure and natural language generation. The
course is seminar-based, with a high proportion
3 This toolkit provides an excellent environment
for teaching students to think about issues such as
dialog flow, as well as introducing them to many
other aspects of spoken language dialog systems. See
http:// cslu.cse.ogi.edu/toolkit/.
4 We have experimented with a number of
different VoiceXML development environments
which are freely available over the web; each has its
advantages and disadvantages. Currently we?ve had
most success with Motorola?s MADK : see
http://developers.motorola.com/developers/. At the
time of writing, however, this does not support the
new VoiceXML 2.0 standard, so we are considering
other alternatives.
of student presentations, and an assignment in
anaphor resolution.
The level of interest amongst students at
this level is such that we expect to offer
additional honours level courses later in the
current academic year.
4 Outcomes and Issues
The program has been operating since the
second half of 2000. Since that time, we have
taught Comp248 twice and Comp349 once;
Comp249 and Comp348 are currently being
taught for the second time; and Comp448 is
being taught for the first time.
It is too early to establish to what extent the
material we have taught is impacting on
graduates? work practices: the first students to
complete degrees that incorporate our courses
are only now graduating. However, we have
made use of a number of feedback and review
mechanisms over the last 18 months, and these
have already provided us with new ideas for
how to improve what we are trying to do.
4.1 Evaluating Course Content
We make use of the typical infrastructure made
available for evaluation purposes: student-staff
liaison committees, formal questionnaires, and
also a significant amount of informal feedback
through discussions with students. We also
have a management advisory board with
representation from industry; this meets twice a
year to review the development of the program
and to comment on its industrial relevance.
Generally, the courses have been very well
received by the students who take them. Our
advisory board is very comfortable with the
material we teach, but we suffer here from the
problem that the voice recognition industry is
better represented here than the hard-to-define
document processing industry alluded to
earlier. So, we have strong evidence that
students find the material interesting,
challenging and informative; our industry
partners think we are going in the right
direction; but we have yet to demonstrate that
the wider industry community will see a benefit
from students who have grasped this material.
4.2 Course Materials
We have faced a not insignificant problem in
finding appropriate course materials for these
courses, with the consequence that we have had
to develop most things from scratch. For the
first offering of Comp248, the introductory NLP
course, we used Allen [1995]; in the second
offering, we found Covington [1994] to be more
useful. Although this is technically out of print,
Prentice Hall has a technology for producing
short print runs on demand.
The materials problem was more severe in
our third year courses, since there are no even
vaguely adequate textbooks for the material we
wanted to cover. We provide students with a
comprehensive reading packet, but it is not easy
to find appropriate survey or introductory
readings in the various topic areas we cover. As
a consequence of this we are exploring the
possibility of writing a textbook that covers the
material in each of these courses.
5 Lesssons Learned and Future
Directions
Eighteen months from the start of the program,
we are reasonably assured that we are going in
the right direction; some things, inevitably,
require fine tuning. We note here some key
consequences of our experiences so far.
5.1 Voice Captures the Imagination
Perhaps not surprisingly, it is the study of voice
recognition that has really captured students?
imaginations. The level of enthusiasm
generated in a laboratory full of students
wearing headsets talking to their machines is
wonderful to watch (although the working
environment doesn?t do a lot for speech
recognizer accuracy). With this in mind, we are
reworking our second year course, Comp248, so
that it will contain some of the voice material
currently used in third year. We are also
considering an emphasis here on technology
that students might meet outside of the
curriculum, such as chatterbots. Our strategy
here is to entice students into the area with
appealing content, and draw them into the more
theoretically challenging material in later
courses.
5.2 Document Processing as a Theme
It has become obvious that our Web Technology
course could play a more coherent role in our
program. One obvious direction we are
pursuing is to cement the two strands identified
earlier even further, by seeing the Web
Technology course specifically as a precursor for
the Intelligent Text Processing course. At the
same time, we are considering broadening the
third year course to cover Document Processing
more generally, as a way of making its relevance
more apparent; a shift of this kind might also
permit the inclusion of more material on
information retrieval and related technologies,
which are of some significance from an industry
perspective.
5.3 Linguistic Background
We have met the common, and not unexpected,
problem that some students do not have a
sufficient grasp of linguistic matters to perform
satisfactorily in this area. To this end, we have
initiated the introduction of a first year course
that covers basic aspects of linguistics, logic and
computation, taught by ourselves in conjunction
with the University?s Departments of
Philosophy and Linguistics.
5.4 Conclusions
So far, our program has been seen as very
successful from an academic perspective, and
has generated significant interest amongst
students. Our next challenge is to persuade the
wider industry to see students with this training
as very valuable assets. We have instituted an
alumni program that will attempt to track these
students, with the expectation of some
preliminary feedback being available by the end
of the calendar year.
References
James Allen [1995] Natural Language
Understanding. Benjamin Cummings, Menlo
Park, CA.
Michael Covington [1994] Natural Language
Processing for Prolog Programmers. Prentice Hall,
NJ.
Robert Dale, Hermann Moisl and Harold
Somers [2000] Handbook of Natural Language
Processing. Marcel Dekker, NY.
Daniel Jurafsky and James Martin [2000] Speech
and Language Processing: An Introduction to
Natural Language Processing, Computational
Linguistics and Speech Recognition. Prentice Hall,
NJ.
Using the WordNet Hierarchy for Associative Anaphora Resolution
Josef Meyer and Robert Dale
Centre for Language Technology
Macquarie University
Sydney, Australia
  jmeyer|rdale  @ics.mq.edu.au
Abstract
In this paper, we explore how the taxo-
nomic inheritance hierarchy in a seman-
tic net can contribute to the resolution
of associative anaphoric expressions. We
present the results of some preliminary ex-
periments and discuss both their implica-
tions and the scope for improvements to
the technique.
1 Introduction
Anaphor resolution is widely recognised as a key
problem in natural language processing, and has cor-
respondingly received a significant amount of atten-
tion in the literature. However, from a computa-
tional perspective, the primary focus of this work
is the resolution of pronominal anaphora. There is
significantly less work on full definite NP anaphora,
and less still on what we will term here associative
anaphora: that is, the phenomonen in which a defi-
nite referring expression is used to refer to an entity
not previously mentioned in a text, but the existence
of which can be inferred by virtue of some previ-
ously mentioned entity. Although these referring ex-
pressions have been widely discussed in the linguis-
tics, psychology and philosophy literature, compu-
tational approaches are relatively rare (with a few
notable exceptions, such as the work of (Poesio et
al., 1997) and (Vieira, 1998).
A typical example from the literature is the use
of the definite noun phrase reference in the second
sentence in example (1):1
1In these examples, italics are used to indicate anaphors.
(1) A bus came around the corner.
The driver had a mean look in her eye.
Here, the hearer is likely to infer that the driver re-
ferred to in the second sentence belongs to the bus
mentioned in the first sentence. For our purposes,
we consider the driver to be the textual antecedent
of the anaphor, and the relationship between the ref-
erents of the anaphor and antecedent to be a part-of
relationship. From a computational point of view,
these anaphoric forms are problematic because their
resolution would seem to require the encoding of
substantial amounts of world knowledge. In this pa-
per, we explore how evidence derived from a corpus
might be combined with a semantic hierarchy such
as WordNet to assist in the resolution process. Ef-
fectively, our goal is to extend the semantic network
with information about pairs of senses that are ?as-
sociated? in a way that licenses possible associative
anaphoric references. Our technique using involves
unsupervised learning from a parsed corpus.
Section 2 provides some background context and
presents our perspective on the problem. In Sec-
tion 3, we describe the corpus we are using, and
the techniques we have been exploring. Section 4
describes the current results of this exploration, and
Section 5 draws some conclusions and points to a
number of directions for future work.
2 The Problem
The phenomenon of associative anaphora as in-
troduced above has been widely discussed in the
linguistics literature: see, for example, (Hawkins,
1978; Clark and Marshall, 1981; Prince, 1981;
Heim, 1982). However, as noted above, compu-
tational approaches to resolving such anaphora are
much less common. This is hardly surprising: given
the almost limitless bounds on what can be asso-
ciated with a previously mentioned entity, using
knowledge-based approaches of the kind that were
commonly discussed in earlier literature (see, for
example, (Grosz, 1977; Sidner, 1979)) is a ma-
jor undertaking, and probably unrealistic for prac-
tical broad coverage NLP tasks. On the other hand,
the absence of surface level cues makes associative
anaphora difficult to handle using the sort of shallow
processing techniques that have become dominant
over the last decade.
Our focus on the present paper is on those asso-
ciative anaphors where there is a textual antecedent.
The linguistic context provides us with a set of can-
didate antecedents, and our goal, for a given asso-
ciative anaphor, is to identify the correct antecedent.
Several antecedents may refer to the same entity;
given an appropriate coreference resolution mecha-
nism, this is non-problematic. Also, we are not con-
cerned here with with determining the precise nature
of the relationship that holds between the associative
anaphor and its antecedent, although in most cases
we consider this will be one of meronymy. All we
require is the ability to be able to establish a con-
nection between the entities mentioned in a text, ef-
fectively knitting the semantic fabric underlying the
discourse.
As a way of moving towards this result, our mo-
tivating observation is a simple one, and one that
has been explored in other areas (see, for example,
(Hearst, 1992; Knott and Dale, 1992)): that seman-
tic relationships which are left implicit for a reader
to infer in some contexts may also occur explicitly
in others, as in example (2):
(2) A bus nearly collided with a car.
The driver of the bus had a mean look in her
eye.
Here, we have prima facie evidence of the existence
of a relationship between drivers and buses. Our
goal is to see whether this kind of evidence can be
gathered from a corpus and then used in cases where
the association between the two entities is not made
explicit.
3 Extracting Evidence from a Corpus
3.1 The Corpus
For the work described here, the corpus we are us-
ing consists of just over 2000 encyclopaedia articles
drawn from the electronic versions of Grolier?s En-
cyclopaedia and Microsoft?s Encarta. All the articles
used are descriptions of animals, with 1289 from
Grolier?s and 932 from Encarta. Manual analysis of
portions of the corpus suggests that it contains a sig-
nificant number of instances of associative anaphora.
Some interesting examples are presented below:
(3) The head of a ground beetle is narrower than
its body; long, thin, threadlike antennae jut out
from the sides of the head.
The mouthparts are adapted for crushing and
eating insects, worms, and snails.
(4) Beetles undergo complete metamorphosis.
The larvae are cylindrical grubs, with three
pairs of legs on the thorax; the pupae are usu-
ally encased in a thin, light-colored skin with
the legs free; the adults have biting mouth parts,
in some cases enormously developed.
These examples should make it clear that identifying
the antecedent is already a difficult enough problem;
identifying the nature of the relationship between the
entities referred to is significantly more complicated,
and often requires quite sophisticated semantic no-
tions.
3.2 Our Approach
If we were pursuing this work from a knowledge-
based perspective, we might expect to have avail-
able a collection of axioms that could be used in re-
solving associative anaphoric expressions. So, for
example, we might have an axiom that states that
buses have drivers; this axiom, and many others like
it, would then be brought to bear in identifying an
appropriate antecedent.
As noted earlier, we are not concerned in the
present paper with the precise nature of the associ-
ation: for our purposes, it is sufficient to know that
an association exists. As indicated, the possibility
of such a relationship can be derived from a corpus.
Our approach, then, is to mine a corpus for explicit
statements of association, and to use this evidence
as a source for constructing what we will call asso-
ciative axioms; these axioms can then be used as
one component in an anaphor resolution process.
Statements of association take a number of differ-
ent forms, and one issue we face is that these are of
varying reliability, a point we will return to in Sec-
tion 5. In the present work we focus on two forms
of statements of association that we suspect are of
quite high reliability: genitive constructions and of
NP constructions, as in examples (5a) and (5b) be-
low.
(5) a. The stingray?s head is not well defined,
and there is no dorsal or caudal fin.
b. The head of the stingray is not well de-
fined, and there is no dorsal or caudal fin.
Given a unmodified NP like the head, we want to
identify the entity in the preceding text with which
this is associated. Suppose the stingray is one of a
number of candidate antecedent NPs in the context.
If the corpus contains expressions such as those ital-
icised in (5a) and (5b), then we have prima facie ev-
idence that the antecedent might be the stingray.
Of course, such an approach is prone to the prob-
lems of data sparseness. The chance of finding such
explicit evidence elsewhere in a corpus is low, unless
the corpus is very large indeed. Our response to this
is, again, similar to the solution taken by other tasks
that face this problem: we try to find useful general-
isations that allow us to overcome the data sparse-
ness problem. The source for our generalisations
is WordNet (Fellbaum, 1998), although it could in
principle be any available taxonomic or ontological
knowledge source.
WordNet tells us that heads are body parts, and
that stingrays are fish; thus, the appearance of ex-
amples like (5a) and (5b) above could be considered
as evidence that fish have body parts. This could, for
example, be used to infer that the expression the tuna
is a possible antecedent for an associative anaphor
the gills, as in example (6).
(6) The tuna has no respiratory mechanism to en-
sure the flow of water over the gills.
Our goal is to see what useful relationships we might
be able to mine from explicit statements in a cor-
pus, and then to use these relationships as a factor
in determining antecedents of associative anaphora.
The key problem we face is in determining the ap-
propriateness or reliability of the generalisations we
extract.
4 An Experiment
4.1 Associative Constructions
To support the generalisations that we wish to ex-
tract from the corpus, we need to identify cases
where the anaphoric element appears in a syntactic
configuration that makes the presence of an associa-
tive relationship explicit; we refer to these syntactic
configurations as associative constructions. Exam-
ples of such associative constructions are the forms
 
NP of NP  and   Genitive NP  as in example (5)
above. In these constructions, we will refer to the
head of the first NP in the case of the pattern
 
NP of
NP  , and the N in the case of the pattern
 
Genitive
N  , as the head of the associative construction, and
to the other head noun in each case as the modifier
of the associative construction; thus, in the example
under discussion, the head is head and the modifier
is stingray.
To identify associative constructions, we first
process our texts using Conexor?s FDG parser
(Tapanainen and Jarvinen, 1997). We then use a col-
lection of regular expression matching procedures
to identify the NPs in the text. A further filter
over the extracted NPs identifies the expressions that
meet the patterns described above; we find 17164 in-
stances of the
 
NP of NP  construction over 11322
types, and 5662 instances of the
 
Genitive N  con-
struction over 2133 types. The data is of course
fairly skewed. For example, the statement of associ-
ation member of family occurs 193 times in the cor-
pus, and bird of prey occurs 25 times. It is clear from
a rudimentary analysis of this data that many of the
high frequency forms are of a semantic type other
than that which we are interested in. Also, not all
expressions which match our patterns for associative
constructions actually express associative construc-
tions. Some of these can be filtered out using simple
heuristics and stop word lists; for example, we know
that the relationship expressed by the of in number
of N is not of interest to us. Other candidates that
can be ignored are terms like north of, south of, and
so on.
Given these analyses as evidence of associations,
we then refer to any
 
head, modifier  pair for which
we have evidence as a lexical associative axiom.
From example (5) we thus have the following lex-
ical associative axiom:
(7) have(stingray, head)
The ?have? predicate effectively encodes what we
might think of as ?unspecified association?.
4.2 Generalising Associative Axioms
There are 1092
 
NP of NP  forms that appear twice
in the corpus, and 9391 that appear only once; and
it is these low frequency constructions that appear
more relevant to our purpose. Given the low fre-
quencies, we therefore want to generalise the lexi-
cal associative axioms we can derive directly from
the text. WordNet?s hypernymic relationships give
us an easy way to do this. Thus, an expression like
the leg of the okapi supports a number of associative
axioms, including the following:2
(8) have(okapi, leg)
have(okapi, LIMB)
have(GIRAFFE, leg)
have(GIRAFFE, LIMB)
...
have(LIVING THING, BODY PART)
Of course, there are two notable problems with this
that lead to inappropriate generalisations.
First, since many or most lexical items in Word-
Net have multiple senses, we will produce incorrect
generalisations: the above is fine for the sense of leg
as ?a structure in animals that is similar to a human
leg and used for locomotion? (sense 2), but there
are eight other senses in WordNet, including such
things as ?a section or portion of a journey or course?
(sense 9). Generalisations derived from these senses
will clearly be in error. This could be addressed, of
course, by first applying a word sense disambigua-
tion process to the source texts.
The second problem is that it is not always valid
to assume that a property (or relationship) holds for
all subtypes of a given type of entity just because
it holds for a few; for example, although we know
that okapis have legs, and okapis are a type of living
2Small caps are used here to indicate generalised terms.
organism, it would be incorrect to assume that trees
(which are also living organisms) or snakes (which
are also animals) have legs.
Notwithstanding these problems, for each gener-
alisation we make, we take the view that we have
some evidence. If we measure this as the number of
instances that support the generalisation, then, as we
go higher up the WordNet taxonomy, our putative
evidence for a generalisation will increase. At the
same time, however, as the generality increases, the
less potentially useful the generalisations are likely
to be in anaphora resolution.
We refer to each generalisation step as an expan-
sion of the axiom, and to the result as a derived
associative axiom. We would like to have some
indication, therefore, of how useful a given degree
of expansion is, so that we are in a better position
to decide on the appropriate trade off between the
increased evidence and decreased utility of a given
generalisation.
4.3 Evaluating the Axioms
For an evaluation of the effectiveness of our associa-
tive axioms, we focussed on four particular heads:
body, color, head and tip, as in the following exam-
ples:
(9) a. its head, the snake?s head, the head of the
stingray
b. its color, the snake?s color, color of the
skin, color of its coat
c. its body, the female?s body, the bird?s body
d. its tip, the tip of the island, the tip of the
beak
For each of these heads, we automatically extracted
all the contexts of occurrence from the corpus: we
defined a context of occurrence to be an occurrence
of the head without a modifier (thus, a suspected as-
sociative anaphor) plus its two preceding sentences.3
Omitting those cases where the antecedent was not
present in the context, this delivered 230 contexts
for body, 19 for color, 189 for head, and 33 for
tip. Then, we automatically identified all the NPs
in each context; these constitute the candidate an-
tecedent sets for the associative anaphors, referred
3An informal analysis suggests that the antecedent of an as-
sociative anaphor generally occurs no further back than the two
previous sentences. Of course, this parameter can be adjusted.
to here as the initial candidate sets. We then man-
ually annotated each instance in this test set to indi-
cate the true antecedents of the associative anaphor;
since the antecedent entity may be referred to more
than once in the context, for each anaphor this gives
us a target antecedent set (henceforth the target set).
To test the utility of our axioms, we then used the
lexical and derived axioms to filter the initial candi-
date set, varying the number of generalisation steps
from zero (i.e., using only lexical associative ax-
ioms) to five (i.e., using derived axioms generated by
synset lookup followed by four levels of hypernym
lookup): at each step, those candidates for which
we do not have evidence of association are removed,
with the remaining elements being referred to as the
selected set. Ideally, of course, the axioms should
reduce the candidate set without removing elements
that are in the target set.
One measure of the effectiveness of the filters is
the extent to which they reduce the candidate sets:
so, for example, if the context in a test instance con-
tains four possible antecedents, and the filter only
permits one of these and rejects the other three, we
have reduced the candidate set to 25% of its origi-
nal size. We will call this the reduction factor of
the filter for that instance. The mean reduction fac-
tor provides a crude measure of the usefulness of the
filter, since it reduces the search space for later pro-
cessing stages.
Reducing the size of the search space is, of course,
only useful if the search space ends up containing
the correct result. Since the target set is defined as
a set of coreferent elements, we hold that the search
space contains the correct result provided it contains
at least one element in the target set. So another
useful measure in evaluating the effectiveness of a
filter is the ratio of the number of cases in which the
the intersection of the target set and the selected set
(henceforth the overlap set) was non-empty to the
total number of cases considered. We refer to this as
the overall accuracy of the filter.
Table 1 summarises the overall accuracy and
mean reduction factor for each of the four anaphoric
heads we considered in this evaluation, measured at
each level of generalisation of the associative ax-
ioms extracted from the corpus. What we would like
our filtering to achieve is a low reduction factor (i.e.,
the selected set should be small) but a high overall
accuracy (the filter should rarely remove an actual
antecedent). As a baseline to evaluate against, we
set the selected set to consist of the subjects of the
previous sentences in the context, since these would
seem to constitute reasonable guesses at the likely
antecedent.
As can be seen, the synset lookup step (generali-
sation level 1) does not have a significant effect for
any of the words. For all of the words there is a sig-
nificant worsening in the reduction ratio after a sin-
gle hypernym lookup: not surprisingly, as we gener-
alise the axioms their ability to filter out candidates
that are not in the target set decreases. This is ac-
companied by an increase in accuracy over the next
two steps, indicating that the more specific axioms
have a tendency to rule out the correct antecedents.
This clearly highlights the trade-off between the two
measures.
The second set of measures that we used is based
on the precision and recall figures for each applica-
tion of a filter to a set of candidate antecedents. The
single-case recall is the ratio of the size of the over-
lap set to the size of the target set (i.e, how many real
antecedents remain after filtering), while the single-
case precision is the ratio of the size of the overlap
set to the size of the selected set (i.e., what propor-
tion of the selected set are real antecedents).
Table 2 shows the mean of the single-case preci-
sion and recall values, along with the combined F-
measure, taken over all of the cases to which the fil-
ters were applied. As might be expected from the
previous results, there is an obvious trade-off be-
tween precision and recall, with precision dropping
sharply after a single level of hypernym lookup, and
recall beginning to increase after one or two levels.
Although the F-measure indicates generally poor
performance relative to the baseline, this is largely
due to low precision, which would be improved by
combining the semantic filter with other selection
mechanisms, such as salience-based selection; this
is the focus of current work.
It is worth noting that with both sets of figures,
there are substantial differences between the scores
for each of the words. The filter performed best on
tip, reasonably on head and body, and fairly poorly
on color.
Level of generalisation
Anaphor measure None 1 2 3 4 5 Baseline
color reduction 0.15 0.15 0.42 0.64 0.71 0.74 0.08
accuracy 0.63 0.63 0.63 0.74 0.79 0.79 0.37
body reduction 0.14 0.17 0.63 0.76 0.79 0.79 0.07
accuracy 0.57 0.58 0.79 0.88 0.91 0.91 0.45
head reduction 0.14 0.15 0.54 0.72 0.80 0.80 0.07
accuracy 0.49 0.49 0.66 0.84 0.88 0.89 0.49
tip reduction 0.13 0.14 0.37 0.64 0.72 0.77 0.06
accuracy 0.64 0.64 0.85 0.85 0.88 0.91 0.55
Table 1: Variation of reduction factor and accuracy with an increasing level of generalisation in the associa-
tive axioms used for filtering.
Level of generalisation
Anaphor stat initial 0 1 2 3 4 5 Baseline
color precision 0.10 0.45 0.45 0.16 0.10 0.10 0.10 0.37
recall 1.00 0.56 0.56 0.59 0.69 0.79 0.79 0.31
body precision 0.10 0.37 0.32 0.12 0.11 0.11 0.11 0.47
recall 1.00 0.44 0.46 0.71 0.83 0.87 0.87 0.33
head precision 0.10 0.31 0.29 0.11 0.10 0.10 0.10 0.51
recall 1.00 0.39 0.39 0.58 0.79 0.84 0.85 0.39
tip precision 0.07 0.37 0.33 0.18 0.09 0.08 0.08 0.56
recall 1.00 0.64 0.64 0.85 0.85 0.88 0.91 0.55
Table 2: Variation of precision and recall with an increasing level of generalisation in the associative axioms
used for filtering.
5 Conclusions and Further Work
Our intention in this paper has been to explore how
we might automatically derive from a corpus a set of
axioms that can be used in conjunction with an exist-
ing anaphor resolution mechanism; in particular, it is
likely that in conjunction with an approach based on
saliency, the axioms could serve as one additional
factor to be included in computing the relative like-
lihood of competing antecedents.
The preliminary results presented above do not in
themselves make a strong case for the usefulness
of the technique presented in this paper. However,
they do suggest a number of possibilities for further
work. In particular, we have begun to consider the
following.
First, we can make use of word sense disam-
biguation to reduce the negative consequences of
generalising to synsets. Second, we intend to ex-
plore whether it is possible to determine an appro-
priate level of generalisation based on the class of
the anaphor and antecedent. Third, there is scope
for building on existing work on learning selectional
preferences for WSD and the resolution of syntactic
ambiguity; we suspect that, in particular, the work
on learning class-to-class selectional preferences by
(Agirre and Martinez, 2001) may be useful here.
We are also looking for better ways to assess the
results of using the axioms. Two directions here
are clear. First, so far we have only a relatively
small number of hand-annotated examples, from a
single source. Increasing the number of examples
will let us investigate questions like whether differ-
ent choices of parameters are appropriate to different
classes of anaphor. Second, it should be possible to
refine the evaluation metrics: it is likely that even
without looking at the effect of different filters in
the context of a particular anaphora resolution sys-
tem, we could provide a more meaningful analysis
of their probable impact.
In our current work, we have not explored the pos-
sibility of using information about associations that
is explicitly encoded in existing machine-acessible
ontologies. WordNet, for example, actually encodes
meronym relationships. Our reason for not relying
on this information in the first place was the lim-
ited set of relationships that were encoded, and the
fact that associative relationships were encoded far
less reliably than the hypernym relationship. How-
ever, it would be interesting to compare the results
that could be obtained by using the ontology as a
source for associative axioms with those that could
be achieved by automatically deriving axioms from
the data.
Another direction we have not explored is the
complementary information about anaphora resolu-
tion that derives from explicit statements of asso-
ciation: in line with the Gricean maxims, the au-
thor?s decision to use an expression such as the leg of
the okapi may constitute evidence that there is more
than one previously mentioned entity in the context
that may have legs. This information might be used,
for example, to rule out an otherwise most likely an-
tecedent.
In conclusion, we have shown in this paper how
associative axioms can be derived automatically
from a corpus, and we have explored how these ax-
ioms can be used to filter the set of candidate an-
tecedents for instances of associative anaphora. Our
initial evaluation of the impact of using these filters
suggests that they are of limited value; yet the intu-
ition that generalisations of this kind should be use-
ful remains strong, and so our next steps are to find
ways of refining and improving the approach.
References
E. Agirre and D. Martinez. 2001. Learning class-to-
class selectional preferences. In Proceedings of the
ACL CONLL Workshop. Toulouse, France.
H. Clark and C. Marshall, 1981. Definite reference and
mutual knowledge. Cambridge University Press, New
York.
C. Fellbaum, editor. 1998. WordNet. MIT Press.
B. Grosz. 1977. The Representation and Use of Focus in
Dialogue Understanding. Ph.D. thesis, Stanford Uni-
versity.
J. Hawkins. 1978. Definiteness and Indefiniteness:
a study in reference and grammaticality prediction.
Croom Helm, London.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the Four-
teenth International Conference on Computational
Linguistics.
I. Heim. 1982. The Semantics of Definite and Indefi-
nite Noun Phrases. Ph.D. thesis, University of Mas-
sachusetts at Amherst.
Alistair Knott and Robert Dale. 1992. Using linguis-
tic phenomena to motivate a set of rhetorical relations.
Technical Report HCRC/RP-39, Human Communica-
tion Research Centre, University of Edinburgh.
M. Poesio, R. Vieira, and S. Teufel. 1997. Resolving
bridging references in unrestricted text. In Proceed-
ings of the ACL-97 Workshop on Operational Factors
in Practical, Robust, Anaphora Resolution For Unre-
stricted Texts.
E. Prince. 1981. Toward a taxonomy of given-new infor-
mation. In P. Cole, editor, Radical Pragmatics, pages
223?256. Academic Press, New York.
C. Sidner. 1979. Towards a computational theory of def-
inite anaphora comprehension in English discourse.
Ph.D. thesis, MIT.
P. Tapanainen and T. Jarvinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Confer-
ence on Applied Natural Language Processing, pages
64?71. Association for Computational Linguistics.
R. Vieira. 1998. Definite Description Processing in Un-
restricted Text. Ph.D. thesis, University of Edinburgh.
Using Thematic Information in Statistical Headline Generation 
Stephen Wan 
Center for Language 
Technology 
Macquarie University 
Sydney, Australia 
swan@ics.mq.edu.au 
Mark Dras 
Center for Language 
Technology 
Macquarie University 
Sydney, Australia 
madras@ics.mq.edu.au 
C?cile Paris 
CSIRO Mathematical 
and Information 
Sciences 
Locked Bag 17 
North Ryde 1670 
Sydney, Australia 
Cecile.Paris@csiro.au 
Robert Dale 
Center for Language 
Technology 
Macquarie University 
Sydney, Australia 
rdale@ics.mq.edu.au 
 
Abstract  
We explore the problem of single 
sentence summarisation.  In the news 
domain, such a summary might 
resemble a headline.  The headline 
generation system we present uses 
Singular Value Decomposition (SVD) to 
guide the generation of a headline 
towards the theme that best represents 
the document to be summarised.   In 
doing so, the intuition is that the 
generated summary will more accurately 
reflect the content of the source 
document.  This paper presents SVD as 
an alternative method to determine if a 
word is a suitable candidate for 
inclusion in the headline.  The results of 
a recall based evaluation comparing 
three different strategies to word 
selection, indicate that thematic 
information does help improve recall. 
1 Introduction 
Ours is an age where many documents are 
archived electronically and are available 
whenever needed. In the midst of this plethora of 
information, the successful completion of a 
research task is affected by the ease with which 
users can quickly identify the relevant electronic 
documents that satisfy their information needs.  
To do so, a researcher often relies on generated 
summaries that reflect the contents of the 
original document.    
We explore the problem of single sentence 
summarisation, the primary focus of this paper.  
Instead of identifying and extracting the most 
important sentence, we generate a new sentence 
from scratch.   The resulting sentence summary 
may not occur verbatim in the source document 
but may instead be a paraphrase combining key 
words and phrases from the text.   
As a precursor to single sentence summarisation, 
we first explore the particular case of headline 
generation in the news domain, specifically 
English news.  Although headlines are often 
constructed to be sensationalist, we regard 
headline generation as an approximation to 
single sentence summarisation, given that a 
corpus of single sentence summaries does not 
exist.   
Our system re-uses words from the news article 
to generate a single sentence summary that 
resembles a headline.  This is done by selecting 
and then appending words from the source 
article. This approach has been explored by a 
number of researchers (eg. see Witbrock and 
Mittal, 1999; Jin and Hauptmann, 2002) and we 
will describe their work further in the next 
section.  In existing approaches, a word is 
selected on the basis of two criteria: how well it 
acts as a summary word, and how grammatical it 
will be given the preceding summary words that 
have already been chosen. 
The purpose of this paper is to present work 
which investigates the use of Singular Value 
Decomposition (SVD) as a means of 
determining if a word is a good candidate for 
inclusion in the headline. 
To introduce the notion of using SVD for single 
sentence summarisation in this paper, we 
examine the simplest summarisation scenario.  
Thus, presently we are only concerned with 
single document summarisation.  In addition, we 
limit the focus of our discussion to the 
generation of generic summaries.  
In the remainder of this paper, we describe our 
motivation for using SVD by describing 
difficulties in generating headlines in Section 2.  
In Section 3, as motivation for our approach, we 
illustrate how words can be used out of context, 
resulting in factually incorrect statements.  
Section 4 provides an overview of related work. 
In Section 5, we give a detailed description of 
how we generate the sentence summary 
statistically and how we use SVD to guide the 
generation process.  In Section 6, we present our 
experimental design in which we evaluated our 
approach, along with the results and 
corresponding discussion.  Finally, in Section 7, 
we present our conclusions and future work. 
2 The Veracity of Generated Summaries 
Berger and Mittal (2000) describe limitations to 
the generation of headlines by recycling words 
from the article.  One such limitation is that the 
proposition expressed by the generated summary 
is not guaranteed to reflect the information in the 
source text.  As an example, they present two 
sentences of differing meaning which uses the 
same words.   We present their example in 
Example 1, which illustrates the case in which 
the subject and object are swapped.   
The dog bit the postman 
The postman bit the dog. 
Example 1. An example of different propositions 
presented in two sentences which use the same 
words. 
However, we believe that the veracity of the 
generated sentence, with respect to the original 
document, is affected by a more basic problem 
than variation in word order.   Because words 
from any part of a source document can be 
combined probabilistically, there is a possibility 
that words can be used together out of context.  
We refer to this as Out-of-Context error.   Figure 
1 presents an example of a generated headline in 
which the adverb wrongly reports stock price 
movement.  It also presents the actual context in 
which that adverb was used. 
Generated headline 
?singapore stocks shares rebound?? 
 
Actual headline: 
?Singapore shares fall, seen higher after 
holidays.? 
 
Original context of use of ?rebound?: 
?Singapore shares closed down below the 
2,200 level on Tuesday but were expected to 
rebound immediately after Chinese Lunar 
New Year and Muslim Eid Al-Fitr holidays, 
dealers said.?
 
Figure 1.  An error in the generated headline due 
to a word being re-used out of context. 
Out-of-Context errors arise due to limitations in 
the two criteria for selecting words mentioned in 
Section 1.  While, for selection purposes, a word 
is scored according to its goodness as candidate 
summary word, word order is determined by a 
notion of grammaticality, modelled 
probabilistically using ngrams of lexemes.  
However, the semantic relationship implied by 
probabilistically placing two words next to each 
other, for example an adjective and a noun, 
might be suspect.  As the name ?Out-of-
Context? suggests, this is especially true if the 
words were originally used in non-contiguous 
and unrelated contexts.  This limitation in the 
word selection criteria can be characterized as 
being due to a lack of long distance relationship 
information. 
3 Our Approach to ?Encouraging Truth? 
In response to this limitation, we explore the use 
of a matrix operation, Singular Value 
Decomposition (SVD) to guide the selection of 
words.  Although our approach still does not 
guarantee factual correctness with respect to the 
source document, it has the potential to alleviate 
the Out-of-Context problem by improving the 
selection criteria of words for inclusion in the 
generated sentence, by considering the original 
contexts in which words were used.  With this 
improved criteria, we hope to "encourage truth" 
by incorporating long distance relationships 
between words.  Conceptually, SVD provides an 
analysis of the data which describes the 
relationship between the distribution of words 
and sentences.  This analysis includes a 
grouping of sentences based on similar word 
distributions, which correspond to what we will  
refer to here as the main themes of the 
document.1  By incorporating this information 
into the word selection criteria, the generated 
sentence will "gravitate" towards a single theme.  
That is, it will tend to use words from that 
theme, reducing the chance that words are 
placed together out of context.   
By reflecting the content of the main theme, the 
summary may be informative (Borko, 1975).  
That is, the primary piece of information within 
the source document might be included within 
the summary. However, it would remiss of us to 
claim that this quality of the summary is 
guaranteed.  In general, the generated summaries 
are at least useful to gauge what the source text 
is about, a characteristic described by Borko as 
being indicative.   
Figure 2 presents the generated summary using 
SVD for the same test article presented in Figure 
1.  In this case, the summary is informative as 
not only are we told that the article is about a 
stock market, but the movement in price in this 
example is correctly determined. 
Generated headline using SVD: 
?singapore shares fall? 
Figure 2. The headline generated using an SVD-
based word selection criterion.  The movement 
in share price is correct. 
4 Related Work 
As the focus of this paper is on statistical single-
sentence summarisation we will not focus on 
preceding work which generates summaries 
greater in length than a sentence.  We direct the 
reader to Paice (1990) for an overview of 
summarisation based on sentence extraction.  
Examples of recent systems include Kupiec et 
al. (1995) and Brandow et al (1995).    For 
examples of work in producing abstract-like 
summaries, see Radev and McKeown (1998), 
which combines work in information extraction 
                                                 
1
 Theme is a term that is used in many ways by many 
researchers, and generally without any kind of formal 
definition.  Our use of the term here is akin to the 
notion that underlies work on text segmentation, 
where sentences naturally cluster in terms of their 
?aboutness?. 
and natural language processing.  Hybrid 
methods for abstract-like summarisation which 
combine statistical and symbolic approaches 
have also been explored; see, for example, 
McKeown et al (1999), Jing and McKeown 
(1999), and Hovy and Lin (1997). 
Statistical single sentence summarisation has 
been explored by a number of researchers (see 
for example, Witbrock and Mittal, 1999; Zajic et 
al., 2002).  We build on the approach employed 
by Witbrock and Mittal (1999) which we will 
describe in more detail in Section 3.   
Interestingly, in the work of Witbrock and Mittal 
(1999), the selection of words for inclusion in 
the headline is decided solely on the basis of 
corpus statistics and does not use statistical 
information about the distribution of words in 
the document itself.  Our work differs in that we 
utilise an SVD analysis to provide information 
about the document to be summarized, 
specifically its main theme.    
Discourse segmentation for sentence extraction 
summarisation has been studied in work such as 
Boguraev and Neff (2000) and Gong and Liu 
(2001).  The motivation behind discovering 
segments in a text is that a sentence extraction 
summary should choose the most representative 
sentence for each segment, resulting in a 
comprehensive summary.  In the view of Gong 
and Liu (2001), segments form the main themes 
of a document.  They present a theme 
interpretation of the SVD analysis, as it is used 
for discourse segmentation, upon which our use 
of the technique is based.  However, Gong and 
Liu use SVD for creating sentence extraction 
summaries, not for generating a single sentence 
summary by re-using words. 
In subsequent work to Witbrock and Mittal 
(1999), Banko et al (2000) describe the use of 
information about the position of words within 
four quarters of the source document.  The 
headline candidacy score of a word is weighted 
by its position in one of quarters.  We interpret 
this use of position information as a means of 
guiding the generation of a headline towards the 
central theme of the document, which for news 
articles typically occurs in the first quarter.  
SVD potentially offers a more general 
mechanism for handling the discovery of the 
central themes and their positions within the 
document.   
Jin et al (2002) have also examined a statistical 
model for headlines in the context of an 
information retrieval application.  Jin and 
Hauptmann (2001) provide a comparison of a 
variety of learning approaches used by 
researchers for modelling the content of 
headlines including the Iterative Expectation-
Maximisation approach, the K-Nearest 
neighbours approach, a term vector approach 
and the approach of Witbrock and Mittal (1999).  
In this comparison, the approach of Witbrock 
and Mittal (1999) fares favourably, ranking 
second after the term vector approach to title 
word retrieval (see Jin and Hauptmann, 2001, 
for details).   However, while it performs well, 
the term vector approach Jin et al (2002) 
advocate doesn't explicitly try to model the way 
a headline will usually discuss the main theme 
and may thus be subject to the Out-of-Context 
problem. 
Finally, for completeness, we mention the work 
of Knight and Marcu (2000), who examine 
single sentence compression.  Like Witbrock 
and Mittal (1999), they couch summarisation as 
a noisy channel problem.  Under this framework, 
the summary is a noise-less source of 
information and the full text is the noisy result.  
However, in contrast to our approach, Knight 
and Marcu (2000) handle parse trees instead of 
the raw text.  Their system learns how to 
simplify parse trees of sentences extracted from 
the document to be summarized, to uncover the 
original noise-less forms. 
5 Generating a Single Sentence Summary 
In this section, we describe our approach to 
single sentence summarisation.  As mentioned 
earlier, our approach is based on that of 
Witbrock and Mittal (1999).  It differs in the 
way we score words for inclusion in the 
headline.  Section 5.1 presents our re-
implementation of Witbrock and Mittal?s (1999) 
framework and introduces the Content Selection 
strategy they employ.  Section 5.2 describes our 
extension using SVD resulting in two alternative 
Content Selection strategies.  
5.1 Searching for a Probable Headline 
We re-implemented the work described in 
Witbrock and Mittal (1999) to provide a single 
sentence summarisation mechanism.  For full 
details of their approach, we direct the reader to 
their paper (Witbrock and Mittal, 1999).  A brief 
overview of our implementation of their 
algorithm is presented here. 
Conceptually, the task is twofold.  First, the 
system must select n words from a news article 
that best reflect its content.  Second, the best 
(grammatical) word ordering of these n words 
must be determined.  Witbrock and Mittal 
(1999) label these two tasks as Content Selection 
and Realisation.  Each of these criteria are 
scored probabilistically, whereby the probability 
is estimated by prior collection of corpus 
statistics.   
To estimate Content Selection probability for 
each word, we use the Maximum Likelihood 
Estimate (MLE).  In an offline training stage, the 
system counts the number of times a word is 
used in a headline, with the condition that it 
occurs in the corresponding news article.  To 
form the probability, this frequency data is 
normalised by the number of times the word is 
used in articles across the whole corpus.  This 
particular strategy of content selection, we refer 
to this as the Conditional probability.   
The Realisation criterion is determined simply 
by the use of bigram statistics, which are again 
collected over a training corpus during the 
training stage.  The MLE of the probability of 
word sequences is calculated using these bigram 
statistics.  Bigrams model the grammaticality of 
a word given the preceding word that has 
already been chosen. 
It should be noted that both the Content 
Selection and Realisation criteria influence 
whether a word is selected for inclusion in the 
headline.  For example, a preposition might 
poorly reflect the content of a news article and 
score a low Content Selection probability.  
However, given the context of the preceding 
word, it may be the only likely choice. 
In both the training stage and the headline 
generation stage, the system employs the same 
preprocessing.  The preprocessing, which 
mirrors that used by Witbrock and Mittal (1999), 
replaces XML markup tags and punctuation 
(except apostrophes) with whitespace.  In 
addition, the remaining text is transformed into 
lower case to make string matching case 
insensitive.  The system performs tokenisation 
by using whitespace as a word delimiter. 
In Witbrock and Mittal?s approach (1999), the 
headline generation problem reduces to finding 
the most probable path through a bag of words 
provided by the source document, essentially a 
search problem.  They use the beam search 
variety of the Viterbi algorithm (Forney, 1973) 
to efficiently search for the headline.  In our 
implementation, we provided the path length as 
a parameter to this search mechanism.  In 
addition, we used a beam size of 20.   
To use the Viterbi algorithm to search for a path, 
the probability of adding a new word to an 
existing path is computed by combining the 
Content selection probability, the Realisation 
probability and the probability of the existing 
path, which is recursively defined. Combining 
each component probability is done by finding 
the logs of the probabilities and adding them 
together.  The Viterbi algorithm sorts the paths 
according to the path probabilities, directing the 
search towards the more probable word 
sequences first.  The use of repeated words in 
the path is not permitted. 
5.2 Using Singular Value Decomposition for 
Content Selection 
As an alternative to the Conditional probability, 
we examine the use of SVD in determining the 
Content Selection probability.  Before we 
outline the procedure for basing this probability 
on SVD, we will first outline our interpretation 
of the SVD analysis, based on that of Gong and 
Liu (2001).  Our description is not intended to 
be a comprehensive explanation of SVD, and we 
direct the reader to Manning and Sch?tze (2000) 
for a description of how SVD is used in 
information retrieval. 
Conceptually, when used to analyse documents, 
SVD can discover relationships between word 
co-occurrences in a collection of text.  For 
example, in the context of information retrieval, 
this provides one way to retrieve additional 
documents that contain synonyms of query 
terms, where synonymy is defined by similarity 
of word co-occurrences.  By discovering 
patterns in word co-occurrences, SVD also 
provides information that can be used to cluster 
documents based on similarity of themes. 
In the context of single document 
summarisation, we require SVD to cluster 
sentences based on similarities of themes.   The 
SVD analysis provides a number of related 
pieces of information relating to how words and 
sentences relate to these themes.  One such piece 
of information is a matrix of scores, indicating 
how representative the sentence is of each 
theme.  Thus, for a sentence extraction 
summary, Gong and Liu (2001) would pick the 
top n themes, and for each of these themes, use 
this matrix to choose the sentence that best 
represents it.   
For single sentence summarisation, we assume 
that the theme of the generated headline will 
match the most important theme of the article.  
The SVD analysis orders its presentation of 
themes starting with the one that accounts for 
the greatest variation between sentences.  The 
SVD analysis provides another matrix which 
scores how well each word relates to each 
theme.  Given a theme, scores for each word, 
contained in a column vector of the matrix, can 
then normalised to form a probability.  The 
remainder of this section provides a more 
technical description of how this is done. 
To begin with, we segment a text into sentences.  
Our sentence segmentation preprocessing is 
quite simple and based on the heuristics found in 
Manning and Sch?tze (2000).  After removing 
stopwords, we then form a terms by sentences 
matrix, A.  Each column of A represents a 
sentence.  Each row represents the usage of a 
word in various sentences. Thus the frequency 
of word t in sentence s is stored in the cell  Ats.  
This gives us an t * s matrix, where t ? s.  That 
is, we expect the lexicon size of a particular 
news article to exceed the number of sentences.   
For such a matrix, the SVD of A is a process 
that provides the right hand side of the following 
equation: 
A = U.S. Vtranspose  
where U is  a t * r matrix, S is an r * r matrix, 
and V is an s * r matrix.  The dimension size r is 
the rank of A, and is less than or equal to the 
number of columns of A, in this case, s.    The 
matrix S is a diagonal matrix with interesting 
properties, the most important of which is that 
the diagonal is sorted by size.  The diagonal 
values indicate the variation across sentences for 
a particular theme, where each theme is 
represented by a separate diagonal element.  The 
matrix V indicates how representative a sentence 
is of a score.  Similarly the matrix U indicates 
how related to the themes each word is.  A 
diagram of this is presented in Figure 3. 
Before describing how we use each of these 
matrices, it is useful to outline what SVD is 
doing geometrically.  Each sentence, a column 
in the matrix A, can be thought of as an object in 
t dimensional space.  SVD uncovers the 
relations between dimensions. For example, in 
the case of text analysis, it would discover 
relationships between words such as synonyms.  
In a trivial extreme of this case where two 
sentences differ only by a synonym, SVD would 
ideally discover that the two synonyms have 
very similar word co-occurrences.  In the 
analysis matrices of U, S and V, the redundant 
dimensions corresponding to these highly 
similar words might be removed, resulting in a 
reduced number of dimensions, r, required to 
represent the sentences.   
 
Figure 3.  A diagram of our interpretation of the 
SVD matrices as it relates to single sentence 
summarisation. 
Of the resulting matrices, V is an indication of 
how each sentence relates to each theme, 
indicated by a score.  Thus, following Gong and 
Liu (2001), a plausible candidate for the most 
important sentence is found by taking the first 
column vector of V (which has s elements), and 
finding the element with the highest value.  This 
sentence will be the one which is most 
representative of the theme.  The index of that 
element is the index of the sentence to extract.   
However, our aim is not to extract a sentence but 
to utilise the theme information.  The U matrix 
of the analysis provides information about how 
well words correspond to a particular theme.  
We examine the first column of the U matrix, 
sum the elements and then normalize each 
element by the sum to form a probability.  This 
probability, which we refer to as the SVD 
probability, is then used as the Content Selection 
probability in the Viterbi search algorithm. 
As an alternative to using the SVD probability 
and the Conditional Probability in isolation, a 
Combined Probability is calculated using the 
harmonic mean of the two.  The harmonic mean 
was used in case the two component 
probabilities differed consistently in their 
respective orders of magnitude.  Intuitively, 
when calculating a combined probability, this 
evens the importance of each component 
probability. 
To summarize, we end up with three alternative 
strategies in estimating the Content Selection 
Probability: the Conditional Probability, the 
SVD Probability and the Combined Probability. 
6 Experiments  
6.1 Data 
In our experiments, we attempted to match the 
experimental conditions of Witbrock and Mittal 
(1999).  We used news articles from the first six 
months of the Reuters 1997 corpus (Jan 1997 to 
June 1997).  Specifically, we only examined 
news articles from the general Reuters category 
(GCAT) which covers primarily politics, sport 
and economics.   This category was chosen not 
because of any particular domain coverage but 
because other categories exhibited frequent use 
of tabular presentation.  The GCAT category 
contains in excess of 65,000 articles.  Following 
Witbrock and Mittal (1999), we randomly 
selected 25,000 articles for training and a further 
1000 articles for testing, ensuring that there was 
no overlap between the two data sets.  During 
the training stage, we collected bigrams from the 
headline data, and the frequency of words 
occurring in headlines. 
6.2 Experiment Design 
We conducted an evaluation experiment to 
compare the performance of the three Content 
Selection strategies that we identified in Section 
5: the Conditional probability, the SVD 
probability, and the Combined probability.  We 
measure performance in terms of recall, i.e. how 
many of the words in the actual headline match 
words in the generated headline.2  The recall 
metric is normalised to form a percentage by 
dividing the word overlap by the number of 
words in the actual headline.   
For each test article, we generated headlines 
using each of the three strategies.  For each 
strategy, we generated headlines of varying 
lengths, ranging from length 1 to 13, where the 
latter is the length of the longest headline found 
in the test set.  We then compared the different 
strategies for generated headlines of equal 
length.   
To determine if differences in recall scores were 
significant, we used the Wilcoxon Matched Pairs 
Signed Ranks (WMPSR) test (Seigel and 
Castellan, 1988).  In our case, for a particular 
pair of Content Selection strategies, the alternate 
hypothesis was that the choice of Content 
Selection strategy affects recall performance.  
The null hypothesis held that there was no 
difference between the two content selection 
strategies.  Our use of the non-parametric test 
was motivated by the observation that recall 
scores were not normally distributed.  In fact, 
our results showed a positive skew for recall 
scores.  To begin with, we compared the recall 
scores of the SVD strategy and the Conditional 
strategy in one evaluation.  The strategy that was 
found to perform better was then compared with 
the Combined strategy. 
                                                 
2
 Word overlap, whilst the easiest way to evaluate the 
summaries quantitatively, is an imprecise measure 
and must be interpreted with the knowledge that non-
recall words in the generated headline might still 
indicate clearly what the source document is about. 
In addition to the recall tests, we conducted an 
analysis to determine the extent to which the 
SVD strategy and the Conditional probability 
strategy were in agreement about which words 
to select for inclusion in the generated headline.  
For this analysis, we ignored the bigram 
probability of the Realisation component and 
just measured the agreement between the top n 
ranking words selected by each content selection 
strategy.  Over the test set, we counted how 
many words were selected by both strategies, 
just one strategy, and no strategies.  By 
normalising scores by the number of test cases, 
we determine the average agreement across the 
test set.  We ran this experiment for a range of 
different values of N, ranging from 1 to 13, the 
length of the longest headline in the test set.   
6.3 Results 
6.3.1 Recall Comparison 
The results for the comparison of recall scores 
are presented in Table 1 and Table 2.  Table 1 
shows results of the WMPSR test when 
comparing the SVD strategy with the 
Conditional strategy.3  Since the Conditional 
strategy was found to perform better, we then 
compared this with the Combined strategy, as 
shown in Table 2.  From Table 1, it is clear that, 
for all sentence lengths, there is a significant 
difference between the SVD strategy and the 
Conditional strategy, and so we reject the null 
hypothesis.  Similarly, Table 2 shows that there 
is a significant difference between the 
Conditional strategy and the Combined strategy, 
and again we reject the null hypothesis. We 
conclude that SVD probability alone is 
outperformed by the Conditional probability; 
however, using both probabilities together leads 
to a better performance. 
 
 
 
 
                                                 
3
 The performance of our Conditional strategy is 
roughly comparable to the results obtained by Banko, 
Mittal and Witbrock (2000), in which they report 
recall scores between 20% to 25%, depending on the 
length of the generated headline.   
Sentence 
Length 
Average 
Recall : 
SVD 
Average 
Recall : 
Cond. Probability 
Reject  
H0 
1 03.68% 03.98% p ? 0.0 yes 
2 07.02% 06.97% p ?  0.5 yes 
3 10.05% 11.44% p ? 0.0 yes 
4 12.39% 13.90% p ? 0.0 yes 
5 14.21% 15.73% p ?0.0 yes 
6 15.57% 17.84% p ?1.1e-05 yes 
7 16.59% 19.14% p ? 1.8e-07 yes 
8 17.74% 20.30% p ? 1.3e-07 yes 
9 18.74% 21.33% p ? 1.3e-06 yes 
10 19.73% 22.44% p ? 1.0e-06 yes 
11 20.19% 23.50% p ? 2.2e-10 yes 
12 20.85% 24.54% p ? 4.4e-13 yes 
13 21.13% 25.13% p ? 1.4e-12 yes 
Table 1. A comparison of recall scores for the 
SVD strategy and the Conditional strategy. 
Sentence 
Length 
Average 
Recall : 
Cond 
Average 
Recall :  
Combined Probability 
Reject  
H0 
1 03.98% 04.05% p ? 0.1305 yes 
2 06.97% 08.60% p ? 2.8e-13 yes 
3 11.44% 12.34% p ? 0.0007 yes 
4 13.90% 15.44% p ? 8.5e-09 yes 
5 15.73% 17.33% p ? 1.9e-09 yes 
6 17.84% 18.72% p ? 0.0003 yes 
7 19.14% 20.34% p ? 1.3e-05 yes 
8 20.30% 21.48% p ? 2.9e-06 yes 
9 21.33% 22.60% p ? 4.0e-06 yes 
10 22.44% 23.82% p ? 1.2e-06 yes 
11 23.50% 24.56% p ? 0.0003 yes 
12 24.54% 25.44% p ? 0.0008 yes 
13 25.13% 26.37% p ? 8.6e-06 yes 
Table 2. A comparison of recall scores for the 
Conditional strategy and the Combined strategy.   
6.3.2 Agreement between Strategies 
The agreement between strategies is presented in 
Table 3.  Interestingly, of the words recalled, the 
majority have only been selected by one content 
selection strategy.  That is, the set of words 
recalled by one content selection strategy do not 
necessarily subsume the set recalled by the 
other.  This supports the results obtained in the 
recall comparison in which a combined strategy 
leads to higher recall.  Interestingly, the last 
column in the table shows that the potential 
combined recall is greater than the recall 
achieved by the combined strategy; we will 
return to this point in Section 6.4. 
 
 
 
 
Sentence 
Length 
Selected 
by neither 
method 
Selected by 
only 1 
method 
Selected 
by both 
methods 
Total 
Recall 
1 91.6% 8.0% 0.3% 8.3% 
2 84.7% 14.1% 1.0% 15.1% 
3 79.9% 17.5% 2.5% 20.0% 
4 76.6% 19.3% 3.9% 23.2% 
5 73.8% 21.0% 5.1% 26.1% 
6 71.4% 22.1% 6.4% 28.5% 
7 69.6% 22.4% 7.8% 30.2% 
8 67.9% 22.9% 9.1% 32.0% 
9 66.4% 23.2% 12.3% 35.5% 
10 65.0% 23.5% 11.3% 34.8% 
11 63.9% 23.6% 12.3% 35.9% 
12 63.0% 23.6% 13.2% 36.8% 
13 62.1% 23.5% 14.3% 37.8% 
Table 3.  Agreement of words chosen between 
the SVD strategy and the Conditional 
probability strategy to content selection 
6.4 Discussion 
The SVD strategy ultimately did not perform as 
well ass we might have hoped.  There are a 
number of possible reasons for this. 
1. Whilst using the Combined probability did 
lead to a significantly improved result, this 
increase in recall was only small.  Indeed, 
the analysis of the agreement between the 
Conditional strategy and the SVD strategy 
indicates that the current method of 
combining the two probabilities is not 
optimal and that there is still considerable 
margin for improvement. 
2. Even though the recall of the SVD strategy 
was poorer by a only a few percent, the lack 
of improvement in recall is perplexing, 
given that we expected the thematic 
information to ensure words were used in 
correct contexts. There are several possible 
explanations, each warranting further 
investigation.  It may be the case that the 
themes identified by the SVD analysis were 
quite narrow, each encompassing only small 
number of sentences.  If this is the case, 
certain words occurring in sentences outside 
the theme would be given a lower 
probability even if they were good headline 
word candidates.  Further investigation is 
necessary to determine if this is a short-
coming of our SVD strategy or an artefact of 
the domain.  For example, it might be the 
case that the sentences of news articles are 
already thematically quite dissimilar.   
3. One might also question our experimental 
design.  Perhaps the kind of improvement 
brought about when using the SVD 
probability cannot be measured by simply 
counting recall.  Instead, it may be the case 
that an evaluation involving a panel of 
judges is required to determine if the 
generated text is qualitatively better in terms 
of how faithful the summary is to the 
information in the source document.  For 
example, a summary that is more accurate 
may not necessarily result in better recall.  
Finally, it is conceivable that the SVD 
strategy might be more sensitive to 
preprocessing stages such as sentence 
delimitation and stopword lists, which are 
not necessary when using the Conditional 
strategy.  
Despite these outstanding questions, there are 
pragmatic benefits when using SVD.  The 
conditional strategy requires a paired training set 
of summaries and source documents.  In our 
case, this was easily obtained by using headlines 
in lieu of single sentence summaries.  However, 
in cases where a paired corpus is not available 
for training, the SVD strategy might be more 
appropriate, given that the performance does not 
differ considerably.  In such a situation, a 
collection of documents is only necessary for 
collecting bigram statistics. 
7 Conclusion 
Combining both the SVD probability and 
Conditional probability marginally improves 
recall, lending support to the intuition that 
thematic information may help generate better 
single sentence summaries.  However, there are 
still many unanswered questions.  In future 
work, we intend to investigate these techniques 
in a domain other than news text so that we can 
draw conclusions as to how well these strategies 
generalise to other genres.  We also intend to 
conduct user evaluations to gauge the quality of 
the generated summaries for both the 
Conditional and the SVD strategies.  Indeed, a 
user-based evaluation would be extremely 
helpful in determining if the thematic 
information provided by the SVD strategy does 
help improve the veracity of the generated 
summaries.    
References  
Banko M., Mittal V., and Witbrock M. (2000) 
Headline generation based on statistical translation. 
In Proceedings of the 38th Annual Meeting of the 
Association for Computational Linguistics. 
Boguraev B., and Neff M. (2000) Discourse 
segmentation in aid of document summarization. In 
Proceedings of the Hawaii International 
Conference on System Sciences (HICSS- 33), 
Minitrack on Digital Documents Understanding. 
Maui, Hawaii: IEEE. 
Borko, H., and Bernier, C. (1975) Abstracting 
Concepts and Methods. New York: Academic 
Press. 
Brandow, R., Mitze, K., and Rau, L. (1995) 
Automatic condensation of electronic publications 
by sentence selection. In Information Processing 
and Management, 31(5), pages 675-685. 
Forney G. D. (1973) The Viterbi Algorithm.  In the 
Proceedings of the IEEE, pages 268-278. 
Gong Y., and Liu, X. (2001) Generic Text 
Summarization Using Relevance Measure and 
Latent Semantic Analysis. In the Proceedings 
SIGIR 2001: pages 19-25. 
Hovy, E. and Lin, C. (1997) Automated text 
summarization in SUMMARIST.  In the 
Proceedings of ACL-EACL?97 Workshop on 
Intelligent Scalable Text Summarization, pages 18-
24. 
Jin, R., and Hauptmann, A. (2001) Learning to Select 
Good Title Words: An New Approach based on 
Reversed Information Retrieval.  In the 
Proceedings of the Eighteen International 
Conference on Machine Learning (ICML 2001), 
Williams College,MA, June 28-July 1. 
Jin, R., Zhai, C., and Hauptmann, A. (2002) Title 
language model for information retrieval. In the 
Proceedings of the 25th Annual International ACM 
SIGIR Conference on Research and Development 
in Information Retrieval (SIGIR 2002), Tampere, 
Finland, August 11-15. 
Jing, H., and McKeown, K. (1999) The 
decomposition of human-written summary 
sentences. In the Proceedings of the 22nd 
Conference on Research and Development in 
Information Retrieval (SIGIR--99).  
Knight, K. and Marcu, D. (2000) Statistics-based 
summarization---Step one: Sentence compression. 
In Proceedings of AAAI-2000. 
Kupiec, J., Pedersen, J., and Chen, F. (1995) A 
Trainable Document Summarizer. In Proceedings 
of the 18th Annual International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval. Fox, E., Ingwersen, P., and 
Fidel, R. (Editors), pages 68?73. 
Manning C. and Sch?tze H. (2000) Foundations of 
Statistical Natural Language Processing.  MIT 
Press: Cambridge MA. 
Marcu, D. (2000) The Theory and Practice of 
Discourse Parsing and Summarization. 
Cambridge: The MIT Press. 
McKeown, K., Klavans, J., Hatzivassiloglou, V., 
Barzilay, R., and Eskin, E. (1999) Towards 
multidocument summarization by reformulation: 
Progress and prospects. In the Proceedings of the 
Sixteenth National Conference on Artificial 
Intelligence (AAAI--99). 
Paice, C. (1990) Constructing Literature Abstracts by 
Computers: Techniques and Prospects.  In 
Information Processing and Management, Vol. 26, 
No. 1, pages 171?186. 
Radev, D. and McKeown, K. (1998) Generating 
natural language summaries from multiple on-line 
sources. Computational Linguistics, 24(3):469-500, 
September.  
Siegel, Sidney and Castellan, Jr. N. John. (1988) 
Nonparametric Statistics For The Behavioral 
Sciences. McGraw-Hill, Inc., second edition. 
Witbrock, M., and Mittal, V. (1999) 
Ultrasummarization: A statistical approach to 
generating highly condensed non-extractive 
summaries. In the Proceedings of the 22nd 
International Conference on Research and 
Development in Information Retrieval (SIGIR '99). 
Zajic D., Door B., and Schwartz R. (2002) Automatic 
Headline Generation for Newspaper Stories. In the 
Proceedings of the Document Understanding 
Conference (DUC 2002). 
Searching for Grammaticality: Propagating Dependencies in the Viterbi
Algorithm
Stephen Wan12 Robert Dale1 Mark Dras1
1Centre for Language Technology
Div. of Information Communication Sciences
Macquarie University
Sydney, Australia
swan,rdale,madras@ics.mq.edu.au
Ce?cile Paris2
2Information and Communication
Technologies
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
In many text-to-text generation scenarios (for in-
stance, summarisation), we encounter human-
authored sentences that could be composed by re-
cycling portions of related sentences to form new
sentences. In this paper, we couch the generation
of such sentences as a search problem. We in-
vestigate a statistical sentence generation method
which recombines words to form new sentences.
We propose an extension to the Viterbi algorithm
designed to improve the grammaticality of gener-
ated sentences. Within a statistical framework, the
extension favours those partially generated strings
with a probable dependency tree structure. Our
preliminary evaluations show that our approach
generates less fragmented text than a bigram base-
line.
1 Introduction
In abstract-like automatic summary generation, we often re-
quire the generation of new and previously unseen summary
sentences given some closely related sentences from a source
text. We refer to these as Non-Verbatim Sentences. These
sentences are used instead of extracted sentences for a variety
of reasons including improved conciseness and coherence.
The need for a mechanism to generate such sentences is
supported by recent work showing that sentence extraction is
not sufficient to account for the scope of written human sum-
maries. Jing and McKeown [1999] found that only 42% of
sentences from summaries of news text were extracted sen-
tences. This is also supported by the work of Knight and
Marcu [2002] (cited by [Daume? III and Marcu, 2004]), which
finds that only 2.7% of human summary sentences are ex-
tracts. In our own work on United Nations Humanitarian
Aid Proposals, we noticed that only 30% of sentences are
extracted from the source document, either verbatim or with
trivial string replacements.
While the figures do vary, it shows that additional mecha-
nisms beyond sentence extraction are needed. In response to
this, our general research problem is one in which given a set
of related sentences, a single summary sentence is produced
by recycling words from the input sentence set.
In this paper, we adopt a statistical technique to allow easy
portability across domains. The Viterbi algorithm [Forney,
1973] is used to search for the best traversal of a network of
words, effectively searching through the sea of possible word
sequences. We modify the algorithm so that it narrows the
search space not only to those sequences with a semblance of
grammaticality (via n-grams), but further still to those that are
grammatical sentences preserving the dependency structure
found in the source material.
Consider the following ungrammatical word sequence typ-
ical of that produced by an n-gram generator, ?The quick
brown fox jumped over the lazy dog slept by the log ?. One
diagnosis of the problem is that the word dog is also used
as the subject of the second verb slept. Ideally, we want to
avoid such sequences since they introduce text fragments, in
this case ?slept by the log?. We could, for example, record
the fact that dog is already governed by the verb jumped, and
thus avoid appending a second governing word slept.
To do so, our extension propagates dependency features
during the search and uses these features to influence the
choice of words suitable for appending to a partially gener-
ated sentence. Dependency relations are derived from shal-
low syntactic dependency structure [Kittredge and Mel?cuk,
1983]. Specifically, we use representations of relations be-
tween a head and modifier, ignoring relationship labels for
the present.
Within the search process, we constrain the choice of fu-
ture words by preferring words that are likely to attach to
the dependency structure of the partially generated sentence.
Thus, sequences with plausible structures are ranked higher.
The remainder of the paper is structured as follows. In Sec-
tion 2, we describe the problem in detail and our approach.
We outline our use of the Viterbi algorithm in Section 3. In
Section 4, we describe how this is extended to cater for de-
pendency features. We compare related research in Section 5.
A preliminary evaluation is presented and discussed in Sec-
tion 6. Finally, we conclude with future work in Section 7.
2 Narrowing the Search Space: A Description
of the Statistical Sentence Generation
Problem
In this work, sentence generation is essentially a search for
the most probable sequence of words, given some source text.
However, this constitutes an enormous space which requires
efficient searching. Whilst reducing a vocabulary to a suit-
able subset narrows this space somewhat, we can use statis-
tical models, representing properties of language, to prune
the search space of word sequences further to those strings
that reflect real language usage. For example, n-gram models
limit the word sequences examined to those that seem gram-
matically correct, at least for small windows of text.
However, n-grams alone often result in sentences that,
whilst near-grammatical, are often just gibberish. When com-
bined with a (word) content selection model, we narrow the
search space even further to those sentences that appear to
make sense. Accordingly, approaches such as Witbrock and
Mittal [1999] and Wan et al [2003] have investigated models
that improve the choice of words in the sentence. Witbrock
and Mittal?s content model chooses words that make good
headlines, whilst that of Wan et al attempts to ensure that,
given a short document like a news article, only words from
sentences of the same subtopic are combined to form a new
sentences. In this paper, we narrow the search space to those
sequences that conserve dependency structures from within
the input text.
Our algorithm extension essentially passes along the long-
distance context of dependency head information of the pre-
ceding word sequence, in order to influence the choice of the
next word appended to the sentence. This dependency struc-
ture is constructed statistically by an O(n) algorithm, which
is folded into the Viterbi algorithm. Thus, the extension is
in an O(n4) algorithm. The use of dependency relations fur-
ther constrains the search space. Competing paths through
the search space are ranked taking into account the proposed
dependency structures of the partially generated word se-
quences. Sentences with probable dependency structures are
ranked higher. To model the probability of a dependency re-
lation, we use the statistical dependency models inspired by
those described in Collins [1996].
3 Using The Viterbi Algorithm for Sentence
Generation
We assume that the reader is familiar with the Viterbi al-
gorithm. The interested reader is referred to Manning and
Schutze [1999] for a more complete description. Here, we
summarise our re-implementation (described in [Wan et al,
2003]) of the Viterbi algorithm for summary sentence gener-
ation, as first introduced by Witbrock and Mittal [1999].
In this work, we begin with a Hidden Markov Model
(HMM) where the nodes (ie, states) of the graph are uniquely
labelled with words from a relevant vocabulary. To obtain a
suitable subset of the vocabulary, words are taken from a set
of related sentences, such as those that might occur in a news
article (as is the case for the original work by Witbrock and
Mittal). In this work, we use the clusters of event related sen-
tences from the Information Fusion work by Barzilay et al
[1999]. The edges between nodes in the HMM are typically
weighted using bigram probabilities extracted from a related
corpus.
The three probabilities of the unmodified Viterbi algorithm
are defined as follows:
Transition Probability (using the Maximum Likelihood Esti-
mate to model bigram probabilities)1:
ptrngram (wi+1|wi) =
count(wi, wi+1)
count(wi)
Emission Probability: (For the purposes of testing the new
transition probability function described in Section 4, this is
set to 1 in this paper):
pem(w) = 1
Path Probability is defined recursively as:
ppath(w0, . . . , wi+1) =
ptrngram (wi+1|wi)? pem(w)? ppath(w0 . . . wi)
The unmodified Viterbi algorithm as outlined here would
generate word sequences just using a bigram model. As noted
above, such sequences will often be ungrammatical.
4 A Mechanism for Propagating Dependency
Features in the Extended Viterbi Algorithm
In our extension, we modify the definition of the Transition
Probability such that not only do we consider bigram prob-
abilities but also dependency-based transition probabilities.
Examining the dependency head of the preceding string then
allows us to consider long-distance context when append-
ing a new word. The algorithm ranks highly those words
with a plausible dependency relation to the preceding string,
with respect to the source text being generated from (or sum-
marised).
However, instead of considering just the likelihood of a
dependency relation between adjacent pairs of words, we can
consider the likelihood of a word attaching to the dependency
tree structure of the partially generated sentence. Specifically,
it is the rightmost root-to-leaf branch that can still be modified
or governed by the appending of a new word to the string.
This rightmost branch is stored as a stack. It is updated and
propagated to the end of the path each time we add a word.
Thus, our extension has two components: Dependency
Transition and Head Stack Reduction. Aside from these mod-
ifications, the Viterbi algorithm remains the same.
In the remaining subsections, we describe in detail how the
dependency relations are computed and how the stack is re-
duced. In Figure 3, we present pseudo-code for the extended
Viterbi algorithm.
4.1 Scoring a Dependency Transition
Dependency Parse Preprocessing of Source Text
The Dependency Transition is simply an additional weight on
the HMM edge. The transition probability is the average of
the two transition weights based on bigrams and dependen-
cies:
ptr(wi+1|w1) =
average(ptrngram (wi+1|w1), ptrdep (wi+1|w1))
Before we begin the generation process, we first use a depen-
dency parser to parse all the sentences from the source text to
1Here the subscripts refer to the fact that this is a transition prob-
ability based on n-grams. We will later propose an alternative using
dependency transitions.
obtain dependency trees. A traversal of each dependency tree
yields all parent-child relationships, and we update an adja-
cency matrix of connectivity accordingly. Because the status
of a word as a head or modifier depends on the word order in
English, we consider relative word positions to determine if
a relation has a forward or backward2 direction. Forward and
backward directional relations are stored in separate matri-
ces. The Forward matrix stores relations in which the head is
to the right of modifier in the sentence. Conversely, the Back-
ward matrix stores relations in the head to left of the modifier.
This distinction is required later in the stack reduction step.
As an example, given the two strings (using characters
in lieu of words) ?d b e a c? and ?b e d c a? and the
corresponding dependency trees:
a
b
d e
c
c
d
b
e
a
we obtain the following adjacency matrices:
Forward (or Right-Direction) Adjacency Matrix
?
?
?
?
0 a b c d e
a 0 1 0 0 0
b 0 0 0 1 0
c 0 0 0 1 0
d 0 1 0 0 0
e 0 0 0 0 0
?
?
?
?
Backward (or Left-Direction) Adjacency Matrix
?
?
?
?
0 a b c d e
a 0 0 1 0 0
b 0 0 0 0 2
c 1 0 0 0 0
d 0 0 0 0 0
e 0 0 0 0 0
?
?
?
?
We refer to the matrices as Adjright and Adjleft respectively.The cell value in each matrix indicates the number of times
word i (that is, the row index) governs word j (that is, the
column index).
Computing the Dependency Transition Probability
We define the Dependency Transition weight as:
ptrdep (wi+1|wi) =
p(Depsym(wi+1, headStack(wi))
where Depsym is the symmetric relation stating that some
dependency relation occurs between a word and any of the
words in the stack, irrespective of which is the head. Intu-
itively, the stack is a compressed representation of the depen-
dency tree corresponding to the preceding words. The prob-
ability indicates how likely it is that the new word can attach
itself to this incrementally built dependency tree, either as a
modifier or a governer. Since the stack is cumulatively passed
on at each point, we need only consider the stack stored at the
preceding word.
This is estimated as follows:
p(Depsym(wi+1, headStack(wi))) =
max
h?headStack(wi)
p(Depsym(wi+1, h))
2These are defined analogously to similar concepts in Combina-
tory Categorial Grammar [Steedman, 2000].
the quick brown fox jumps
jumps over the lazy dog .
[
the
]
[
quick
the
]
[
brown
quick
the
]
[
fox
]
[
jumps
]
[
over
jumps
]
[
the
over
jumps
]
[ lazy
the
over
jumps
]
[
dog
over
jumps
]
Figure 1: A path through a lattice. Although separated on
two lines, it represents a single sequence of words. The stack
(oriented upwards) grows and shrinks as we add words. Note
that the modifiers to dog are popped off before it is pushed on.
Note also that modifiers of existing items on the stack, such
as over are merely pushed on. Words with no connection to
previously seen stack items are also pushed on (eg. quick) in
the hope that a head will be found later.
Here, we assume that a word can only attach to the tree
once at a single node; hence, we find the node that max-
imises the probability of node attachment. The relation-
ship Depsym(a, b) is modelled using a simplified version of
Collins? [1996] dependency model.
Because of the status of word as the head relies on thepreservation of word order, we keep track of the direction-
ality of a relation. For two words a and b where a precedes b
in the generated string,
p(Depsym(a, b)) ?
Adjright(a, b) + Adjleft(b, a)
cnt(co-occur(a, b))
where Adjright and Adjleft are the right and left adjacencymatrices. Recall that row indices are heads and column in-
dices are modifiers.
4.2 Head Stack Reduction
Once we decide that a newly considered path is better than
any other previously considered one, we update the head
stack to represent the extended path. At any point in time,
the stack represents the rightmost root-to-leaf branch of the
dependency tree (for the generated sentence) that can still
be modified or governed by concatenating new words to the
string.3 Within the stack, older words may be modified by
newer words. Our rules for modifying the stack are designed
to cater for a projective4 dependency grammar.
There are three possible alternative outcomes of the reduc-
tion. The first is that the proposed top-of-stack (ToS) has
no dependency relation to any of the existing stack items, in
which case the stack remains unchanged. For the second and
third cases, we check each item on the stack and keep a record
3Note that we can scan through the stack as well as push onto
and pop from the top; this is thus the same type of stack as used in,
for example, Nested Stack Automata.
4That is, if wi depends on wj , all words in between wi and wj
are also dependent on wj .
reduceHeadStack(aNode, aStack) returns aStack
Nodenew ?aNode
Stack ?aStack # duplicate
Nodemax ?NULL
Edgeprob ?0
# Find best chunk
While notEmpty(aStack)
Head ?pop(aStack)
if p(depsym (Nodenew, Head)) > Edgeprob
Nodemax ?Head
Edgeprob ?depsym(Nodenew, Head)
# Keep only best chunk
While top(aStack) 6= Nodemax
pop(aStack)
# Determine new head of existing string
if isReduced(Nodenew,Nodemax)
pop(aStack)
else
push(Nodenew, aStack)
Figure 2: Pseudocode for the Head Stack Reduction operation
only of the best probable dependency between the proposed
ToS and the appropriate stack item. The second outcome,
then, is that the proposed ToS is the head of some item on
the stack. All items up to and including that stack item are
popped off and the proposed ToS is pushed on. The third out-
come is that it modifies some item on the stack. All stackitems up to (but not including) the stack item are popped off
and the proposed ToS is pushed on. The pseudocode is pre-
sented in Figure 2. An example of stack manipulation is pre-
sented in Figure 1. We rely on two external functions. The
first function, depsym/2, has already been presented above.
The second function, isReduced/2, relies on an auxiliary
function returning the probability of one word being governed
by the other, given the relative order of the words. This is in
essence our parsing step, determining which word governs
the other. The function is defined as follows:
isReduced(w1, w2) =
p(isHeadRight(w1, w2)) > p(isHeadLeft(w1, w2))
where w1 precedes w2, and:
p(isHeadRight(w1, w2))
? Adjright(w1, w2)
cnt(hasRelation(w1, w2, wherei(w1) < i(w2)))
and similarly,
p(isHeadLeft(w1, w2))
?
Adjleft(w2, w1)
cnt(hasRelation(w1, w2, wherei(w1) < i(w2)))
where hasRelation/2 is the number of times we see
the two words in a dependency relation, and where i(wi)
returns a word position in the corpus sentence. The
function isReduced/2 makes calls to p(isHeadRight/2)
andp(isHeadLeft/2). It returns true if the first parameter
viterbiSearch(maxLength, stateGraph) returns bestPath
numStates ?getNumStates(stateGraph)
viterbi ?a matrix[numStates+2,maxLength+2]
viterbi[0,0].score ?1.0
for each time step t from 0 to maxLength do
# Termination Condition
if ((viterbi[endState, t].score 6= 0)
AND isAcceptable(endState.headStack))
# Backtrace from endState and return path
# Continue appending words
for each state s from 0 to numStates do
for each transition s? from s
newScore ?
viterbi[s,t].score ? ptr(s?|s) ? pem(s?)
if ((viterbi[s?,t+1].score = 0) OR
(newScore > viterbi[s?, t+1]))
viterbi[s?,t+1].score ?newScore
viterbi[s?,t+1].headStack ?
reduceHeadStack(s?,viterbi[s,t].headStack)
backPointer[s?,t+1] ?s
Backtrace from viterbi[endState,t] and return path
Figure 3: Extended Viterbi Algorithm
is the head of the second, and false otherwise. In the com-
parison, the denominator is constant. We thus need only the
numerator in these auxiliary functions.
Collins? distance heuristics [1996] weight the probability
of a dependency relation between two words based on the
distance between them. We could implement a similar strat-
egy by favouring small reductions in the head stack. Thus a
reduction with a more recent stack item which is closer to the
proposed ToS would be less penalised than an older one.
5 Related Work
There is a wealth of relevant research related to sentence gen-
eration. We focus here on a discussion of related work from
statistical sentence generation and from summarisation.
In recent years, there has been a steady stream of research
in statistical text generation. We focus here on work which
generates sentences from some sentential semantic represen-
tation via a statistical method. For examples of related sta-
tistical sentence generators see Langkilde and Knight [1998]
and Bangalore and Rambow [2000]. These approaches be-
gin with a representation of sentence semantics that closely
resembles that of a dependency tree. This semantic represen-
tation is turned into a word lattice. By ranking all traversals
of this lattice using an n-gram model, the best surface realisa-
tion of the semantic representation is chosen. The system then
searches for the best path through this lattice. Our approach
differs in that we do not start with a semantic representation.
Instead, we paraphrase the original text. We search for the
best word sequence and dependency tree structure concur-
rently.
Research in summarisation has also addressed the prob-
lem of generating non-verbatim sentences; see [Jing and
McKeown, 1999], [Barzilay et al, 1999] and more recently
[Daume? III and Marcu, 2004]. Jing presented a HMM for
learning alignments between summary and source sentences
trained using examples of summary sentences generated by
humans. Daume III also provides a mechanism for sub-
sentential alignment but allows for alignments between multi-
ple sentences. Both these approaches provide models for later
recombining sentence fragments. Our work differs primarily
in granularity. Using words as a basic unit potentially offers
greater flexibility in pseudo-paraphrase generation since we
able to modify the word sequence within the phrase.
It should be noted, however, that a successful execution of
our algorithm is likely to conserve constituent structure (ie. a
coarser granularity) via the use of dependencies, whilst still
making available a flexibility at the word level. Addition-
ally, our use of dependencies allows us to generate not only a
string but a dependency tree for that sentence.
6 Evaluation
In this section, we outline our preliminary evaluation of gram-
maticality in which we compare our dependency based gener-
ation method against a baseline. To study any improvements
in grammaticality, we compare our dependency based gener-
ation method against a baseline consisting of sentences gen-
erated using bigram model.
In the evaluation, we do not use any smoothing algorithms
for dependency counts. For both our approach and the base-
line, Katz?s Back-off smoothing algorithm is used for bigram
probabilities.
For our evaluation cases, we use the Information Fusion
data collected by [Barzilay et al, 1999]. This data is made
up of news articles that have been first grouped by topic,
and then component sentences further clustered by similar-
ity of events. There are 100 sentence clusters and on average
there are 4 sentences per cluster. Each sentence in the cluster
is initially passed through the Connexor dependency parser
(www.connexor.com) to obtain dependency relations. Each
sentence cluster forms an evaluation case in which we gener-
ate a single sentence. Example output and the original text of
the cluster is presented in Figure 4.
To give both our approach and the baseline the greatest
chance of generating a sentence, we obtain our bigrams from
our evaluation cases.5 Aside from this preprocessing to col-
lect input sentence bigrams and dependencies, there is no
training as such. For each evaluation case, both our system
and the baseline method generates a set of answer strings,
from 3 to 40 words in length.
For each generated output of a given sentence length, we
count the number of times the Connexor parser resorts to re-
turning partial parses. This count, albeit a noisy one, is used
as our measure of ungrammaticality. We calculate the aver-
age ungrammaticality score across evaluation cases for each
sentence length.
5Note that this is permissible in this case because we are not
making any claims about the coverage of our model.
Original Text
A military transporter was scheduled to take off in the afternoon from Yokota air base
on the outskirts of Tokyo and fly to Osaka with 37,000 blankets .
Mondale said the United States, which has been flying in blankets and is sending a
team of quake relief experts, was prepared to do more if Japan requested .
United States forces based in Japan will take blankets to help earthquake survivors
Thursday, in the U.S. military?s first disaster relief operation in Japan since it set up
bases here.
Our approach with Dependencies and End of Sentence Check
6: united states forces based in blankets
8: united states which has been flying in blankets
11: a military transporter was prepared to osaka with 37,000 blankets
18: mondale said the afternoon from yokota air base on the united states which has been
flying in blankets
20: mondale said the outskirts of tokyo and is sending a military transporter was pre-
pared to osaka with 37,000 blankets
23: united states forces based in the afternoon from yokota air base on the outskirts of
tokyo and fly to osaka with 37,000 blankets
27: mondale said the afternoon from yokota air base on the outskirts of tokyo and is
sending a military transporter was prepared to osaka with 37,000 blankets
29: united states which has been flying in the afternoon from yokota air base on the
outskirts of tokyo and is sending a team of quake relief operation in blankets
31: united states which has been flying in the afternoon from yokota air base on the out-
skirts of tokyo and is sending a military transporter was prepared to osaka with 37,000
blankets
34: mondale said the afternoon from yokota air base on the united states which has
been flying in the outskirts of tokyo and is sending a military transporter was prepared
to osaka with 37,000 blankets
36: united states which has been flying in japan will take off in the afternoon from
yokota air base on the outskirts of tokyo and is sending a military transporter was pre-
pared to osaka with 37,000 blankets
Figure 4: A cluster of related sentences and sample generated
output from our system. Leftmost numbers indicate sentence
length.
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 5
 3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38
Un
gr
am
m
at
ica
lity
 S
co
re
Sentence Length
Ungrammaticality Errors across Sentence Lengths
Baseline
System
Figure 5: Ungrammaticality scores for generated output.
Higher scores indicates worse performance.
The results are presented in Figure 5. Our approach almost
always performs better than the baseline, producing less er-
rors per sentence length. Using the Wilcoxon Signed Rank
Text (alpha = 0.5), we found that for sentences of length
greater than 12, the differences were usually significant.
7 Conclusion and Future Work
In this paper, we presented an extension to the Viterbi al-
gorithm that statistically determines dependency structure of
partially generated sentences and selects of words that are
likely to attach to this structure. The resulting sentence is
more grammatical than that generated using a bigram base-
line. In future work, we intend to conduct experiments to see
whether the smoothing approaches chosen are successful in
parsing without introducing spurious dependency relations.
We would also like to re-integrate the emission probability
(that is, the word content selection model). We are also in
the process of developing a measure of consistency. Finally,
we intend to provide a comparison evaluation with Barzilay?s
Information Fusion work.
8 Acknowledgements
This work was funded by the Centre for Language Technol-
ogy at Macquarie University and the CSIRO Information and
Communication Technology Centre. We would like to thank
the research groups of both organisations for useful com-
ments and feedback.
References
[Bangalore and Rambow, 2000] Srinivas Bangalore and
Owen Rambow. Exploiting a probabilistic hierarchical
model for generation. In Proceedings of the 18th Con-
ference on Computational Linguistics (COLING?2000),
July 31 - August 4 2000, Universita?t des Saarlandes,
Saarbru?cken, Germany, 2000.
[Barzilay et al, 1999] Regina Barzilay, Kathleen R. McKe-
own, and Michael Elhadad. Information fusion in the con-
text of multi-document summarization. In Proceedings of
the 37th conference on Association for Computational Lin-
guistics, pages 550?557, Morristown, NJ, USA, 1999. As-
sociation for Computational Linguistics.
[Collins, 1996] Michael John Collins. A new statistical
parser based on bigram lexical dependencies. In Arivind
Joshi and Martha Palmer, editors, Proceedings of the
Thirty-Fourth Annual Meeting of the Association for Com-
putational Linguistics, pages 184?191, San Francisco,
1996. Morgan Kaufmann Publishers.
[Daume? III and Marcu, 2004] Hal Daume? III and Daniel
Marcu. A phrase-based hmm approach to docu-
ment/abstract alignment. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 119?126,
Barcelona, Spain, July 2004. Association for Computa-
tional Linguistics.
[Forney, 1973] G. David Forney. The viterbi algorithm. Pro-
ceedings of The IEEE, 61(3):268?278, 1973.
[Jing and McKeown, 1999] Hongyan Jing and Kathleen
McKeown. The decomposition of human-written sum-
mary sentences. In Research and Development in Infor-
mation Retrieval, pages 129?136, 1999.
[Kittredge and Mel?cuk, 1983] Richard I. Kittredge and Igor
Mel?cuk. Towards a computable model of meaning-text
relations within a natural sublanguage. In IJCAI, pages
657?659, 1983.
[Knight and Marcu, 2002] Kevin Knight and Daniel Marcu.
Summarization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107, 2002.
[Langkilde and Knight, 1998] Irene Langkilde and Kevin
Knight. The practical value of N-grams in derivation. In
Eduard Hovy, editor, Proceedings of the Ninth Interna-
tional Workshop on Natural Language Generation, pages
248?255, New Brunswick, New Jersey, 1998. Association
for Computational Linguistics.
[Manning and Schu?tze, 1999] Christopher D. Manning and
Hinrich Schu?tze. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cambridge, Mas-
sachusetts, 1999.
[Steedman, 2000] Mark Steedman. The syntactic process.
MIT Press, Cambridge, MA, USA, 2000.
[Wan et al, 2003] Stephen Wan, Mark Dras, Cecile Paris,
and Robert Dale. Using thematic information in statistical
headline generation. In The Proceedings of the Workshop
on Multilingual Summarization and Question Answering
at ACL 2003, Sapporo, Japan, July 2003.
[Witbrock and Mittal, 1999] Michael J. Witbrock and
Vibhu O. Mittal. Ultra-summarization (poster abstract):
a statistical approach to generating highly condensed
non-extractive summaries. In SIGIR ?99: Proceedings of
the 22nd annual international ACM SIGIR conference on
Research and development in information retrieval, pages
315?316, New York, NY, USA, 1999. ACM Press.
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 9?16,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Local Semantics in the Interpretation of Temporal Expressions
Robert Dale? and Pawe? Mazur?
Centre for Language Technology
Macquarie University, NSW 2109, Sydney, Australia
?Robert.Dale@mq.edu.au,
?mpawel@ics.mq.edu.au
Abstract
Work on the interpretation of temporal ex-
pressions in text has generally been pur-
sued in one of two paradigms: the for-
mal semantics approach, where an attempt
is made to provide a well-grounded theo-
retical basis for the interpretation of these
expressions, and the more pragmatically-
focused approach represented by the de-
velopment of the TIMEX2 standard, with
its origins in work in information extrac-
tion. The former emphasises formal ele-
gance and consistency; the latter empha-
sises broad coverage for practical applica-
tions. In this paper, we report on the de-
velopment of a framework that attempts
to integrate insights from both perspec-
tives, with the aim of achieving broad cov-
erage of the domain in a well-grounded
manner from a formal perspective. We
focus in particular on the development
of a compact notation for representing
the semantics of underspecified tempo-
ral expressions that can be used to pro-
vide component-level evaluation of sys-
tems that interpret such expressions.
1 Introduction
Obtaining a precise semantic representation for ut-
terances related to time is interesting both from a
theoretical point of view, as there are many com-
plex phenomena to be addressed, and for purely
practical applications such as information extrac-
tion, question answering, or the ordering of events
on a timeline.
In the literature, work on the interpretation of
temporal expressions comes from two directions.
On the one hand, work in formal semantics (see,
for example, (Pratt and Francez, 2001)) aims to
provide a formally well-grounded approach to the
representation of the semantics of these expres-
sions, but such approaches are difficult to scale
up to the broad coverage required for practical ap-
plications; on the other hand, work that has its
roots in information extraction, while it empha-
sizes broad coverage, often results in the use of
ad hoc representations. The most developed work
in this direction is focused around the TimeML
markup language1 (described, for example, in
(Pustejovsky et al, 2003) and in the collection
edited by Mani et al (2005)).
Some work attempts to bring these two tradi-
tions together: notable in this respect is Schilder?s
(2004) work on temporal expressions in German
newswire text, and Hobbs and Pan?s (2004) work
on the axiomatisation in terms of OWL-Time. Sa-
quete et al (2002) present an approach that views
time expressions as anaphoric references.
We take the view that an important step to-
wards a truly broad coverage yet semantically
well-founded approach is to recognize that there
is a principled distinction to be made between the
interpretation of the semantics of a temporal ex-
pression devoid of its context of use, and the fuller
interpretation of that expression when the context
is taken into account. The first of these, which we
refer to here as the local semantics of a tempo-
ral expression, should be derivable in a composi-
tional manner from the components of the expres-
sion; determining the value of the second, which
we refer to as the global semantics of the expres-
sion, may require arbitrary inference and reason-
1Note that with TimeML one can annotate not only tem-
poral expressions, but also events and relations between
events and temporal expressions.
9
ing. Such a distinction is implicit in other ac-
counts: Schilder?s (2004) use of lambda expres-
sions allows representation of partially specified
temporal entities, and the temporary variables that
Negri and Marseglia (2005) construct during the
interpretation of a given temporal expression cap-
ture something of the same notion.
Our proposal here is to reify this level of inter-
mediate representation based on a formalization in
terms of recursive attribute?value matrices. This
has two distinct advantages: it provides a conve-
nient representation of underspecification, and it
leads naturally to a compositional approach to the
construction of the semantics of temporal expres-
sions via unification. We also provide a compact
encoding of this representation that is essentially
an extension of the existing TIMEX2 representa-
tion for temporal expressions. This brings the ad-
vantages that (a) existing tools and machinery for
evaluation can be used to determine how well a
given implementation derives these local semantic
values; and (b) performance in the determination
of local semantics and global semantics can be
tested independently. To ensure breadth of cover-
age, we have developed our representation on the
basis of the 256 examples of temporal expressions
provided in the TIMEX2 guidelines (Ferro et al,
2005). To make it possible to compare systems on
their performance in producing these intermediate
representations, we make available this set of ex-
amples annotated in-line with the representations
described here.
The rest of this paper is structured as follows. In
Section 2, we describe the architecture of DANTE,
a system which embodies our approach to the de-
tection and normalisation of temporal expressions;
in particular, we focus on the architecture em-
ployed in this approach, and on the particular lev-
els of representation that it makes use of. In Sec-
tion 3, we argue for an intermediate representa-
tional level that captures the semantics of temporal
expressions independent of the context of their in-
terpretation, and introduce the idea of using recur-
sive attribute?value matrices to represent the se-
mantics of temporal expressions. In Section 4, we
provide an encoding of these attribute?value ma-
trices in a compact string-based representation that
is effectively an extension of the ISO-based date?
time format representations used in the TIMEX2
standard, thus enabling easy evaluation of system
performance using existing tools. In Section 5
we discuss how the approach handles construc-
tions that contain one TIMEX embedded within
another. Finally, in Section 6 we draw some con-
clusions and point to future work.
2 The DANTE System
2.1 Processing Steps
In our work, our goal is very close to that for which
the TIMEX2 standard was developed: we want to
annotate each temporal expression in a document
with an indication of its interpretation, in the form
of an extended ISO-format date and time string,
normalised to some time zone. So, for example,
suppose we have the following italicised temporal
expression in an email message that was sent from
Sydney on Monday 10th April 2006:
(1) I expect that we will be able to present this
at the meeting on Friday at 11am.
In the context of our application, this temporal ex-
pression should be marked up as follows:
(2) <TIMEX2 VAL="2006-04-14T01:00GMT">
Friday at 11am</TIMEX2>
We have to do three things to achieve the desired
result:
? First, we have to detect the extent of the tem-
poral expression in the text. We refer to
this process as temporal expression recog-
nition.
? Then, we have to use information from the
document context to turn the recognized ex-
pression into a fully specified date and time.
We refer to this as temporal expression in-
terpretation.
? Finally, we have to normalise this fully spec-
ified date and time to a predefined time zone,
which in the case of the present example is
Greenwich Mean Time. We refer to this as
temporal expression normalisation.2
2Note that this third step is not required by the TIMEX
guidelines, but is an additional requirement in the context of
our particular application. This also means that our use of the
term ?normalisation? here is not consistent with the standard
usage in the TIMEX context; however, we would argue that
our distinction between interpretation and normalisation de-
scribes more accurately the nature of the processes involved
here.
10
We observe that, at the time that the extent of a
temporal expression within a text is determined, it
is also possible to derive some semantic represen-
tation of that expression irrespective of the wider
context within which it needs to be interpreted:
for example, by virtue of having recognized an
occurrence of the string Friday in a text, we al-
ready know that this is a reference to a specific day
of the week. Most existing systems for the inter-
pretation of temporal expressions probably make
use of such a level of representation. Schilder?s
(2004) approach captures the semantics here in
terms of a lambda expression like ?xFriday(x);
Negri and Marseglia (2005) capture information
at this stage of processing via a collection of tem-
porary attributes.
In our system, each of the three steps above cor-
responds to a distinct processing component in the
DANTE system architecture. These components
communicate in terms of a number of distinct rep-
resentations, which we now describe.
2.2 The Text
This level of representation corresponds simply to
the strings that constitute temporal expressions in
text. These are understood to be linguistic con-
structions whose referents are entities in the tem-
poral domain: either points in time, or periods of
time. In the above example, the text representation
is simply the string Friday at 11am.
2.3 Local Semantics
We use this term to refer to a level of representa-
tion that corresponds to the semantic content that
is derivable directly from the text representation;
in the case of temporal expressions that are argu-
ments to prepositions, this includes the interpreta-
tion of the preposition. Such representations are
often incomplete, in that they do not denote a par-
ticular point or period on the time line; however,
usually they do partially specify points or peri-
ods, and constrain the further interpretation of the
string.
2.4 In-Document Semantics
We use this term to refer to the fully explicit in-
terpretation of the text string, to the extent that
this can be determined from the document itself,
in conjunction with any metadata associated with
the document. This level of representation corre-
sponds to the information encoded in the attributes
of the TIMEX2 tag as defined in the TIMEX
guidelines.
2.5 Global Semantics
The TIMEX guidelines do not have anything to
say beyond the representation described in the pre-
vious section. In our application, however, we
are also required to normalise all temporal expres-
sions to a specific time zone. This requires that
some further temporal arithmetic be applied to the
semantics of the found expressions. To calculate
this, we simply have to determine the difference
between the time zone of the document containing
the temporal reference and the target time zone,
here Greenwich Mean Time. The document may
not always be explicitly marked with information
about the time zone of its creation; in such cases,
this has to be inferred from information about the
location of the author or sender of the message.
3 Representing Temporal Expressions
In this section, we describe a conceptualisation of
the semantics of temporal expressions in terms of
recursive attribute?value matrices.
3.1 Temporal Entities
As is conventional in this area of research, we view
the temporal world as consisting of two basic types
of entities, these being points in time and dura-
tions; each of these has an internal hierarchical
structure. We can represent these in the following
manner:3
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
point
TIMEANDDATE
?
?
?
?
?
?
?
?
?
?
?
TIME
?
?
HOUR 15
MINS 00
SECS 00
?
?
DATE
?
?
?
?
DAY
[
DAYNAME D4
DAYNUM 13
]
MONTH 5
YEAR 2006
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
ZONE Z
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
The example above corresponds to the semantics
of the temporal expression 3pm Thursday 13th
May 2006 GMT; in the ISO date and time format
used in the TIMEX2 standard, this would be writ-
ten as follows:
(3) 2006-05-13T15:00:00Z
3For reasons of limitations of space, we will ignore dura-
tions in the present discussion; their representation is similar
in spirit to the examples provided here.
11
Each atomic feature in the attribute?value struc-
ture thus corresponds to a specific position in the
ISO format date?time string.
3.2 Underspecification
Of course, very few temporal expressions in text
are fully specified. The attribute?value matrix rep-
resentation makes it easy to represent the con-
tent of underspecified temporal expressions. For
example, the content of the temporal expression
Thursday in a sentence like We will meet on
Thursday can be expressed as follows:
?
?
point
TIMEANDDATE
[
DATE
[
DAY
[
DAYNAME D4
]
]
]
?
?
On the other hand, a reference to 13th May in a
sentence like We will meet on 13th May has this
representation:
?
?
?
?
point
TIMEANDDATE
[
DATE
[
DAY
[
DAYNUM 13
]
MONTH 05
]]
?
?
?
?
In the cases just described, the semantic repre-
sentation corresponds to the entire temporal noun
phrase in each case. The same form of represen-
tation is easy to use in a compositional seman-
tic framework: each constituent in a larger tem-
poral expression provides a structure that can be
unified with the structures corresponding to the
other constituents of the expression to provide a
semantics for the expression as a whole. The val-
ues of the atomic elements in such an expression
come from the lexicon; multiword sequences that
are best considered atomic (such as, for example,
idioms) can also be assigned semantic representa-
tions in the same way. The value of a composite
structure is produced by unifying the values of its
constituents. Unifying the two structures above,
for example, gives us the following representation
for Thursday 13th May:4
?
?
?
?
?
?
point
TIMEANDDATE
?
?
?
DATE
?
?
?
DAY
[
DAYNAME D4
DAYNUM 13
]
MONTH 05
?
?
?
?
?
?
?
?
?
?
?
?
So, these structures provide a convenient repre-
sentation for what we have referred to above as the
4For simplicity here we assume that the syntactic structure
of such an expression is captured by the context-free grammar
rule ?NP? NP NP?. Other treatments are possible.
local semantics of a temporal expression, and cor-
respond to the output of the recognition stage of
our processing architecture.
3.3 Interpretation
We can now define the task of interpretation in
terms of the content of these structures. We as-
sume a granularity ordering over what we might
think of as the defining attributes in a temporal
representation:
(4) year > month > daynum > hour >
minute > second
These are, of course, precisely the elements that
are represented explicitly in an ISO date?time ex-
pression.
Interpretation of a partially specified temporal
expression then requires ensuring that there is a
value for every defining attribute that is of greater
granularity than the smallest granularity present in
the partially specified representation. We refer to
this as the granularity rule in interpretation.
In the case of the example in the previous sec-
tion, the granularity rule tells us that in order to
compute the full semantic value of the expression
we have to determine a value for YEAR, but not
for HOUR, MINS or SECS. This interpretation
process may require a variety of forms of reason-
ing and inference, as discussed below, and is quali-
tatively different from the computation of the local
semantics.
In the context of our application, a third stage,
the normalisation process, then requires taking the
further step of adding a ZONE attribute with a spe-
cific value, and translating the rest of the construc-
tion into this time zone if it represents a time in
another time zone.
4 A Compact Encoding
The structures described in the previous section
are relatively unwieldy in comparison to the sim-
ple string structures used as values in the TIMEX
standard. To enable easy evaluation of a sys-
tem?s ability to construct these intermediate se-
mantic representations, we would like to use a rep-
resentation that is immediately usable by existing
evaluation tools. To achieve this goal, we define
a number of extensions to the standard TIMEX2
string representation for values of the VAL at-
tribute; these extensions allow us to capture the
range of distinctions we need. To save space, we
12
also use these representations here to show the
coverage of the annotation scheme that results.
In our implementation, we represent the local
semantic content via an additional set of attributes
on TIMEX elements that mirrors exactly the set of
attributes used by the TIMEX2 standard: thus we
have T-VAL, T-ANCHOR VAL and so on. This
means that markup applied to a text distinguishes
intermediate and final semantic values, making it
possible to evaluate on just intermediate values,
just final values, or both. In what follows, we will
also use these intermediate attributes to make clear
which level of representation is under discussion.
4.1 Partially Specified Dates and Times
As noted above, many references to dates or times
are not fully specified in a text, with the result that
some parts will have to be computed from the con-
text during the interpretation stage. Typical exam-
ples are as follows:
(5) a. We?ll see you in November.
b. I expect to see you at half past eight.
In the recursive attribute?value notation intro-
duced above, the missing information in each case
corresponds to those features that are absent in the
structure as determined by the granularity rule in-
troduced in Section 3.3.
In our string-based notation, we use lowercase
xs to indicate those elements for which a value
needs to be found, but which are not available at
the time the local semantics are computed; and
we capture the granularity requirement by omit-
ting from the string representation those elements
that do not require a value.5 Table 1 provides a
range of examples that demonstrate various forms
of underspecification.
A lowercase x thus corresponds to a variable.
By analogy with this extension, we also use a low-
ercase t instead of the normal ISO date?time sep-
arator of T to indicate that the time may need fur-
ther specification: consider the third and fourth ex-
amples in Table 1, where it is not clear whether the
time specified is a.m. or p.m.
For partially-specified dates and times, the
string-based encoding thus both captures the local
5Note that this does not mean the same thing as the use
of an uppercase X in the TIMEX2 guidelines: an uppercase
X means effectively that no value can be determined. Of
course, if no value can be found for a variable element during
the interpretation process, then the corresponding lowercase
x will be replaced by an uppercase X .
String Representation
9 pm xxxx-xx-xxT21
11:59 pm xxxx-xx-xxT23:59
eleven in the morning xxxx-xx-xxT11:00
ten minutes to 3 xxxx-xx-xxt02:50
15 minutes after the hour xxxx-xx-xxtxx:15
the nineteenth xxxx-xx-19
January 3 xxxx-01-03
November xxxx-11
summer xxxx-SU
?63 xx63
the ?60s xx6
Table 1: Underspecified Dates and Times
semantic content of the temporal expression, and
provides a specification of what information the
interpretation process has to add. If the temporal
focus is encoded in the same form of representa-
tion, then producing the final interpretation is of-
ten a simple process of merging the two structures,
with the values already specified in the interme-
diate representation taking precedence over those
in the representation of the temporal focus. Ex-
pressions involving references to named months
require a decision as to whether to look for the
next or previous instance of the month, typically
determined by the tense of the major clause con-
taining the reference.
4.2 Representing Weekdays
In recognition that the year-based calendar and
the week-based calendar are not aligned, our in-
termediate representation embodies a special case
borrowed from the TIMEX2 notation for days of
the week that require context for their specifica-
tion. Consider example (6a), uttered on Friday
14th April 2006; the intermediate semantic repre-
sentation is provided in example (6b), and the final
interpretation is provided in example (6c).
(6) a. We left on Tuesday.
b. T-VAL="D2"
c. VAL="2006-04-11"
This is not as convenient as the ISO-like encod-
ing, and requires special case handling in the inter-
preter; however, a more comprehensive single rep-
resentation would require abandoning the ISO-like
encoding and the benefits it brings, so we choose
to use the two formats in concert.
13
String Representation
today +0000-00-00
tomorrow +0000-00-01
yesterday ?0000-00-01
five days ago ?0000-00-05
last month ?0000-01
last summer ?0001-SU
two weeks ago ?0000-W02
this weekend +0000-W00-WE
this year +0000
three years ago ?0003
the next century +01
Table 2: Relative dates in ISO-like format.
The same notation supports references to parts
of specific days, as presented in example (7).
(7) a. We left on Tuesday morning.
b. T-VAL="D2TMO"
c. VAL="2006-04-11TMO"
4.3 Relative Dates and Times
A relative date or time reference is one that re-
quires a calendar arithmetic operation to be carried
out with respect to some temporal focus in the text.
Typical examples are as follows:
(8) a. We?ll see him tomorrow.
b. We saw him last year.
c. We?ll see him next Thursday.
d. We saw him last November.
We distinguish three subtypes here: relative dates
and times whose local semantics can be expressed
in an ISO-like format; relative references to days
and months by name; and less specific references
to past, present and future times.
For the first of these, we extend the ISO format
with a preceding ?+? or ??? to indicate the direc-
tion from the current temporal focus. Some exam-
ples of dates are provided in Table 2, and some ex-
amples of date?time combinations are provided in
Table 3. Note the both the date and time elements
in a relative reference can be independently either
absolute or relative: compare the representations
for in six hours time and at 6am today.
This representation leads to a very intuitive
coordinate-based arithmetic for computing the fi-
nal semantic interpretation of a given expression:
the interpreter merely adds the temporal focus and
String Representation
sixty seconds later +0000-00-00T+00:00:60
five minutes ago +0000-00-00T?00:05
in six hours time +0000-00-00T+06:00
at 6 a.m. today +0000-00-00T06:00
last night ?0000-00-01TNI
Table 3: Relative times in ISO-like format.
String Representation
last Monday <D1
next Wednesday >D3
last March <M03
next March >M03
Table 4: Relative References to Days and Months
the intermediate value element-by-element from
the smallest unit upwards, using carry arithmetic
where appropriate.
Relative references to named days and months
require a different treatment, in line with the no-
tation introduced in Section 4.2. Table 4 shows
the intermediate values used for a number of such
expressions.
A further variation on this notation also allows
us to specify a local semantics for expressions like
the first Tuesday in temporal expressions like the
first Tuesday in July, or like the last year in the last
year of the millenium; see Table 5. To produce
final interpretations of these, the interpreter has to
construct the set of elements that correspond to the
head noun (for example, a list of the ISO dates that
correspond to the Tuesdays in a given month), and
then select the nth element from that set.
5 Handling Embedded Constructions
The TIMEX specification allows for the embed-
ding of one TIMEX within another. Consider an
example like the following:
String Representation
the first Tuesday 1D2
the second day 2D
the last Tuesday $D2
the last day $D
Table 5: Ordinally-specified Elements
14
Figure 1: The syntactic structure of an embedded
TIMEX
(9) <TIMEX2>the first Tuesday
in<TIMEX2>July</TIMEX2></TIMEX2>
The bulk of the embedded TIMEXs provided as
examples in the TIMEX guidelines are, like this
one, of the form [NP PP], where the head NP
contains a TIMEX, and the PP contains another
TIMEX that is modified by the head NP. Syntac-
tically, these structures are of the form shown in
Figure 1.
For our purposes, it is convenient to first think
of these structures as consisting of three, rather
than two, TIMEXs, corresponding to the three
subscripted NP nodes in this tree. The outermost
TIMEX, corresponding to NP0, is the one whose
value we are ultimately interested in; this is com-
puted by combining the semantics of the two con-
stituent TIMEXs, corresponding to NP1 and NP2,
and the preposition indicates how this combination
should be carried out.
Structurally, the recognizer may first determine
that there are two separate TIMEXs here:
(10) <TIMEX2>the first
Tuesday</TIMEX2> in
<TIMEX2>July</TIMEX2>
Each of these TIMEXs can be given the appro-
priate local semantics by the recognizer; the rec-
ognizer then reorganizes this structure to mirror
the embedding required by the TIMEX guidelines,
to produce the structure shown in example (10)
above; effectively, NP1 disappears as a distinct
constituent, and its intermediate semantics are in-
herited by NP0.
We then leave it to the interpreter to combine the
intermediate semantics of NP0 with the intermedi-
ate semantics of NP2 to produce a final semantics
for NP0: schematically, we have
(11) NP0(VAL) = NP0(T-VAL) ? NP2(T-VAL)
where ??? is the combinatory operation that corre-
sponds to the preposition used. The operation re-
quired is specified by the recognizer as the value of
the temporary attribute T-REL, which represents
the semantics of the preposition.
The following three examples demonstrate a va-
riety of possibilities, showing both the intermedi-
ate (T-VAL) and final (VAL) semantic interpreta-
tions in each case:
(12) <TIMEX2 VAL="1999" T-VAL="$Y"
T-REL="OF">the last year of
<TIMEX2 VAL="1" T-VAL="+0">this
millennium</TIMEX2></TIMEX2>
(13) <TIMEX2 VAL="1998-01-31"
T-VAL="$D" T-REL="OF">the last
day of <TIMEX2 VAL="1998-01"
T-VAL="xxxx-01">January
</TIMEX2></TIMEX2>
(14) <TIMEX2 VAL="1998-01-31"
T-VAL="$D" T-REL="OF">the last
day of <TIMEX2 VAL="1998-01"
T-VAL="1998-01">January
1998</TIMEX2></TIMEX2>
Note that, when the embedded TIMEX is fully
specified, as in the last example here, it would be
possible for the recognizer to calculate the final
value of the whole expression; however, for con-
sistency we leave this task to the interpreter.
The semantics of the indicated T-REL depend
on the types of its arguments. In the cases above,
for example, the operation is one of selecting an
ordinally-specified element of a list; but where the
entity is a period rather than a point, as in the first
six months of 2005, the operation is one of delim-
iting the period in question.
Of course, other forms of embedding are pos-
sible. In appositions, the syntactic structure can
be thought of as [NP NP]; as in the case of em-
bedded PPs, the TIMEX representation effectively
promotes the semantics of the first NP to be the se-
mantics of the whole. Again, we show both VAL
and T-VAL values here, and the relevant T-REL.
(15) <TIMEX2 VAL="1998-12-29"
T-VAL="xxxx-xx-xx"
15
T-REL="EQUAL">my birthday,
<TIMEX2 VAL="1998-12-29"
T-VAL="xxxx-12-29">December
twenty-ninth</TIMEX2></TIMEX2>
(16) <TIMEX2 VAL="196" T-VAL="196"
T-REL="EQUAL">the 1960s, <TIMEX2
VAL="196" T-VAL="PXD">the days of
free love</TIMEX2></TIMEX2>
Here, the fact that the T-REL is EQUAL causes
the interpreter to combine the values of the two
TIMEXs, with points taking precedence over du-
rations.
6 Conclusions
In this paper, we have argued that, in the context of
interpreting temporal expressions, there is value in
identifying a level of semantic representation that
corresponds to the meaning of these expressions
outside of any particular document context. This
idea is not in itself new, and many existing sys-
tems appear to make use of such representations.
However, we have proposed that this level of rep-
resentation be made explicit; and by providing an
encoding of this level of representation that is an
extension of the existing TIMEX2 annotations in
terms of element attributes and their values, we
make it possible to assess the performance of sys-
tems with respect to intermediate values, final val-
ues, or both, using standard evaluation tools.
We have developed the representation described
here on the basis of the set of 265 examples pro-
vided in the TIMEX2 guidelines (Ferro et al,
2005), and this set of annotated examples is avail-
able to the community.6 The approach described
here is implemented in DANTE, a text process-
ing system which produces normalised values for
all TIMEXs found in a document. The recogni-
tion component of the system, which constructs
the intermediate representations described here, is
implemented via just over 200 rules written in the
JAPE language:7 time expressions are thus recog-
nised using finite state patterns, but we then ap-
ply a syntactic check, using the Connexor parser,
to ensure that we have identified the full extent of
each temporal expression, appropriately extending
the extent when this is not the case.
6See www.clt.mq.edu.au/timex.
7JAPE is provided as part of the GATE tools (Cunning-
ham et al, 2002).
We are currently testing this representation and
its means of derivation against the data from the
2004 TERN competition. Our results are broadly
comparable to those achieved by other systems
(for example, Chronos or TempEx), though they
can not be compared directly since the reported
evaluations at the TERN competition use data
which are not public and therefore not available
to us.
7 Acknowledgements
We acknowledge the support of the Defence Sci-
ence and Technology Organisation in carrying out
the work described here.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphical
development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniver-
sary Meeting of the ACL.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. TIDES 2005 Standard for the Anno-
tation of Temporal Expressions. Technical report,
MITRE, September.
J. R. Hobbs and F. Pan. 2004. An ontology of time for
the semantic web. ACM Transactions on Asian Lan-
guage Information Processing, 3(1):66?85, March.
I. Mani, J. Pustejovsky, and R. Gaizauskas, editors.
2005. The Language of Time. Oxford Univ. Press.
M. Negri and L. Marseglia. 2005. Recognition and
normalization of time expressions: ITC-IRST at
TERN 2004. Technical Report WP3.7, Information
Society Technologies, February.
I. Pratt and N. Francez. 2001. Temporal prepositions
and temporal generalized quantifiers. Linguistics
and Philosophy, 24:187?222.
J. Pustejovsky, J. Castan?o, R. Ingria, R. Gaizauskas
R. Saur, A. Setzer, and G. Katz. 2003. TimeML:
Robust Specification of Event and Temporal Expres-
sions in Text. In IWCS-5, Fifth International Work-
shop on Computational Semantics.
E. Saquete, P. Mart??nez-Barco, and R. Mun?oz. 2002.
Recognising and Tagging Temporal Expressions in
Spanish. In Proc. of LREC?02: Workshop on Anno-
tation Standards for Temporal Information in Natu-
ral Language, Las Palmas, Spain.
F. Schilder. 2004. Extracting meaning from tempo-
ral nouns and temporal prepositions. ACM Trans-
actions on Asian Language Information Processing,
3(1):33?50, March.
16
Proceedings of the Fourth International Natural Language Generation Conference, pages 63?70,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Algorithms for Generating Referring Expressions:
Do They Do What People Do?
Jette Viethen
Centre for Language Technology
Macquarie University
Sydney NSW 2109
jviethen@ics.mq.edu.au
Robert Dale
Centre for Language Technology
Macquarie University
Sydney NSW 2109
robert.dale@mq.edu.au
Abstract
The natural language generation litera-
ture provides many algorithms for the
generation of referring expressions. In
this paper, we explore the question of
whether these algorithms actually produce
the kinds of expressions that people pro-
duce. We compare the output of three ex-
isting algorithms against a data set consist-
ing of human-generated referring expres-
sions, and identify a number of significant
differences between what people do and
what these algorithms do. On the basis of
these observations, we suggest some ways
forward that attempt to address these dif-
ferences.
1 Introduction
The generation of referring expressions (hence-
forth GRE) ? that is, the process of working
out what properties of an entity should be used
to describe it in such a way as to distinguish
it from other entities in the context ? is a re-
current theme in the natural language generation
literature. The task is discussed informally in
some of the earliest work on NLG (in particular,
see (Winograd, 1972; McDonald, 1980; Appelt,
1981)), but the first formally explicit algorithm
was introduced in (Dale, 1989); this algorithm,
often referred to as the Full Brevity (FB) algo-
rithm, has served as a starting point for many sub-
sequent GRE algorithms. To overcome its limita-
tion to one-place predicates, Dale and Haddock
(1991) introduced a constraint-based procedure
that could generate referring expressions involv-
ing relations; and as a response to the computa-
tional complexity of ?greedy? algorithms like FB,
Reiter and Dale (Reiter and Dale, 1992; Dale and
Reiter, 1995) introduced the psycholinguistically
motivated Incremental Algorithm (IA). In recent
years there have been a number of important ex-
tensions to the IA. The Context-Sensitive exten-
sion (Krahmer and Theune, 2002) is able to gen-
erate referring expressions for the most salient en-
tity in a context; the Boolean Expressions algo-
rithm (van Deemter, 2002) is able to derive ex-
pressions containing boolean operators, as in the
cup that does not have a handle; and the Sets
algorithm (van Deemter, 2002) extends the ba-
sic approach to references to sets, as in the red
cups. Some approaches reuse parts of other al-
gorithms: the Branch and Bound algorithm (Krah-
mer et al, 2003) uses the Full Brevity algorithm,
but is able to generate referring expressions with
both attributes and relational descriptions using a
graph-based technique. There are many other al-
gorithms described in the literature: see, for exam-
ple, (Horacek, 1997; Bateman, 1999; Stone, 2000;
Gardent, 2002). Their general aim is to produce
naturalistic referring expressions, often explicitly
by means of an attempt to follow the same kinds
of principles that we believe people might be fol-
lowing when they produce language ? such as the
Gricean maxims (Grice, 1975). However, the al-
gorithms have rarely been tested against real data
from human referring expression generation.1
In this paper, we present a data set containing
human-produced referring expressions in a limited
domain. Focussing specifically on the algorithms
1The only exceptions we know of to this deficit are not
directly concerned with the kinds of properties people select,
but with phenomena such as how people group entities to-
gether (Funakoshi et al, 2004; Gatt, 2006), or with multi-
modal referring expressions where the linguistic part is not
necessarily distinguishing by itself (van der Sluis and Krah-
mer, 2004).
63
presented in (Dale, 1989), (Dale and Haddock,
1991) and (Reiter and Dale, 1992), we explore
how well these algorithms perform in the same
context. There are significant differences between
the referring expressions produced by humans,
and those produced by the algorithms; we explore
these differences and consider what it means for
work in the generation of referring expressions.
The remainder of this paper is structured as fol-
lows. In Section 2, we introduce the data set
of human-produced referring expressions we use;
in Section 3, we introduce the representational
framework we use to model the domain underly-
ing this data; in Section 4 we introduce the three
algorithms considered in this paper; in Section 5
we discuss the results of using these algorithms
on the data that represents the model of our do-
main; in Section 6 we discuss the differences be-
tween the output of the algorithms and the human-
produced data; and in Section 7 we draw some
conclusions and suggest some steps towards ad-
dressing the issues we have identified.
2 The Data
Our human-produced referring expressions are
drawn from a physical experimental setting con-
sisting of four filing cabinets, each of which is
four drawers high, located in a fairly typical aca-
demic office. The cabinets are positioned directly
next to each other, so that the drawers form a four-
by-four grid; each drawer is labelled with a num-
ber between 1 and 16 and is coloured either blue,
pink, yellow, or orange. There are four drawers of
each colour which are distributed randomly over
the grid, as shown in Figure 1.
Subjects were given a randomly generated num-
ber between 1 and 16, and asked to produce a de-
scription of the numbered drawer using any prop-
erties other than the number. There were 20 partic-
ipants in the experiment, resulting in a total of 140
referring expressions. Here are some examples of
the referring expressions produced:
(1) the top drawer second from the right [d3]
(2) the orange drawer on the left [d9]
(3) the orange drawer between two pink ones
[d12]
(4) the bottom left drawer [d16]
Since the selection of which drawer to describe
was random, we do not have an equal number of
Figure 1: The filing cabinets
descriptions of each drawer; in fact, the data set
ranges from two descriptions of Drawer 1 to 12 de-
scriptions of Drawer 16. One of the most obvious
things about the data set is that even the same per-
son may refer to the same entity in different ways
on different occasions, with the differences being
semantic as well as syntactic.
We are interested in comparing how algorithms
for referring expression generation differ in their
outputs from what people do; since these al-
gorithms produce distinguishing descriptions, we
therefore removed from the data set 22 descrip-
tions which were ambiguous or referred to a set of
drawers. This resulted in a total of 118 distinct re-
ferring expressions, with an average of 7.375 dis-
tinct referring expressions per drawer.
As the algorithms under scrutiny here are not
concerned with the final syntactic realisation of
the referring expression produced, we also nor-
malised the human-produced data to remove su-
perficial variations such as the distinction between
relative clauses and reduced relatives, and between
different lexical items that were synonymous in
context, such as column and cabinet.
Four absolute properties used for describing the
drawers can be identified in the natural data pro-
duced by the human participants. These are the
colour of the drawer; its row and column; and in
those cases where the drawer is situated in one of
the corners of the grid, its cornerhood.2 A number
of the natural descriptions also made use of the
2A question we will return to below is that of how we
decide whether to view a particular property as a one-place
predicate or as a relation.
64
Property Count % (out of possible)
Row 95 79.66% (118)
Column 88 73.73% (118)
Colour 63 53.39% (118)
Corner 11 40.74% (27)
Relation 15 12.71% (118)
Table 1: The properties used in descriptions
following relational properties that hold between
drawers: above, below, next to, right of, left of and
between. In Table 1, Count shows the number of
descriptions using each property, and the percent-
ages show the ratio of the number of descriptions
using each property to the number of descriptions
for drawers that possess this property (hence, only
27 of the descriptions referred to corner drawers).
We have combined all uses of relations into one
row in this table to save space, since, interestingly,
their overall use is far below that of the other prop-
erties: 103 descriptions (87.3%) did not use rela-
tions.
Most algorithms in the literature aim at gen-
erating descriptions that are as short as possi-
ble, but will under certain circumstances pro-
duce redundancy. Some authors, for example
(van Deemter and Halldo?rsson, 2001), have sug-
gested that human-produced descriptions are of-
ten not minimal, and this is an intuition that we
would generally agree with. However, a strong
tendency towards minimality is evident in the
human-produced data here: only 29 out of 118 de-
scriptions (24.6%) contain redundant information.
Here are a few examples:
? the yellow drawer in the third column from
the left second from the top [d6]
? the blue drawer in the top left corner [d1]
? the orange drawer below the two yellow
drawers [d14]
In the first case, either the colour or column proper-
ties are redundant; in the second, colour and corner,
or only the grid information, would have been suf-
ficient; and in the third, it would have been suffi-
cient to mention one of the two yellow drawers.
3 Knowledge Representation
In order to use an algorithm to generate referring
expressions in this domain, we must first decide
how to represent the domain. It turns out that this
raises some interesting questions.
We use the symbols {d1, d2 . . . d16} as our
unique identifying labels for the 16 drawers.
Given some di, the goal of any given algorithm
is then to produce a distinguishing description of
that entity with respect to a context consisting of
the other 15 drawers.
As is usual, we represent the properties of the
domain in terms of attribute?value pairs. Thus we
have, for example:
? d2: ?colour, orange?, ?row, 1?, ?column, 2?,
?right-of, d1?, ?left-of, d3?, ?next-to, d1?, ?next-to,
d3?, ?above, d7?
This drawer is in the top row, so it does not have a
property of the form ?below, d2?.
The four corner drawers additionally possess
the property ?position, corner?. Cornerhood can
be inferred from the row and column informa-
tion; however, we added this property explicitly
because several of the natural descriptions use the
property of cornerhood, and it seems plausible that
this is a particularly salient property in its own
right.
This raises the question of what properties
should be encoded explicitly, and which should
be inferred. Note that in the example above, we
explicitly encode relational properties that could
be computed from others, such as left-of and right-
of. Since none of the algorithms explored here
uses inference over knowledge base properties, we
opted here to ?level the playing field? to enable
fairer comparison between human-produced and
machine-produced descriptions.
A similar question of the role of inference arises
with regard to the transitivity of spatial relations.
For example, if d1 is above d9 and d9 is above
d16 , then it can be inferred that d1 is transitively
above d16. In a more complex domain, the imple-
mentation of this kind of knowledge might play
an important role in generating usful referring ex-
pressions. However, the uniformity of our domain
results in this inferred knowledge about transitive
relations being of little use; in fact, in most cases,
the implementation of transitive inference might
even result in the generation of unnatural descrip-
tions, such as the orange drawer (two) right of the
blue drawer for d12.
Another aspect of the representation of relations
that requires a decision is that of generalisation:
65
next-to is a generalisation of the relations left-of and
right-of. The only algorithm of those we exam-
ine here that provides a mechanism for exploring
a generalisation hierarchy is the Incremental Al-
gorithm (Reiter and Dale, 1992), and this cannot
handle relations; so, we take the shortcut of ex-
plicitly representing the next-to relation for every
left-of and right-of relation in the knowledge base.
We then implement special-case handling that en-
sures that, if one of these facts is used, the more
general or more specific case is also deleted from
the set of properties still available for the descrip-
tion.3
4 The Algorithms
As we have already noted above, there is a con-
siderable literature on the generation of referring
expressions, and many papers in the area provide
detailed algorithms. We focus here on the follow-
ing algorithms:
? The Full Brevity algorithm (Dale, 1989) at-
tempts to build a minimal distinguishing de-
scription by always selecting the most dis-
criminatory property available; see Algo-
rithm 1.
Let L be the set of properties to be realised in our
description; let P be the set of properties known to be
true of our intended referent r (we assume that P is
non-empty); and let C be the set of distractors (the
contrast set). The initial conditions are thus as follows:
? C = {?all distractors?};
? P = {?all properties true of r?};
? L = {}
In order to describe the intended referent r with respect
to the contrast set C, we do the following:
1. Check Success:
if |C| = 0 then return L as a distinguishing
description
elseif P = ? then fail
else goto Step 2.
2. Choose Property:
for each pi ? P do:
Ci ? C ? {x|pi(x)}
Chosen property is pj , where Cj is the smallest set.
goto Step 3.
3. Extend Description (wrt the chosen pj):
L ? L ? {pj}
C ? Cj
P ? P ? {pj}
goto Step 1.
Algorithm 1: The Full Brevity Algorithm
3This is essentially a hack; however, there is clearly a need
for some mechanism for handling what we might think of
as equivalence classes of properties, and this is effectively a
simple approach to this question.
1. Check Success
if Stack is empty then return L as a DD
elseif |Cv| = 1 then pop Stack & goto Step 1
elseif Pr = ? then fail
else goto Step 2
2. Choose Property
for each property pi ? Pr do
p?i ? [r\v]pi
Ni ? N ? p?i
Chosen prediction is pj , where Nj contains
the smallest set Cv for v.
goto Step 3
3. Extend Description (w.r.t the chosen p)
Pr ? Pr ? {p}
p ? [r\v]p
for every other constant r? in p do
associate r? with a new, unique variable v?
p ? [r?\v?]p
push Describe(r?,v?) onto Stack
initialise a set P ?r of facts true of r?
N ? N ? p
goto Step 1
Algorithm 2: The Relational Algorithm
MakeReferringExpression(r, C, P ) L ? {}
for each member Ai of list P do
V = FindBestValue(r, Ai, BasicLevelValue(r, Ai))
if RulesOut(?Ai, V ?) 6= nil
then L ? L ? {?Ai, V ?}
C ? C ? RulesOut(?Ai, V ?)
endif
if C = {} then
if ?type, X? ? L for some X
then return L
else return L ? {?type, BasicLevelValue(r,
type)?}
endif
endif
return failure
FindBestValue(r, A, initial-value)
if UserKnows(r, ?A, initial-value?) = true
then value ? initial-value
else value ? no-value
endif
if (more-specific-value ? MoreSpecificValue(r, A,
value)) 6= nil ?
(new-value ? FindBestValue(A,
more-specific-value)) 6= nil ?
(|RulesOut(?A, new-value?)| > |RulesOut(?A,
value?)|)
then value ? new-value
endif
return value
RulesOut(?A, V ?)
if V = no-value
then return nil
else return {x : x ? C ? UserKnows(x, ?A, V ?) =
false}
endifAlgorithm 3: The Incremental Algorithm
66
? The relational algorithm from (Dale and Had-
dock, 1991) uses constraint satisfaction to in-
corporate relational properties while avoiding
infinite regress; see Algorithm 2.
? the Incremental Algorithm (Reiter and Dale,
1992; Dale and Reiter, 1995) considers the
available properties to be used in a descrip-
tion via a preference ordering over those
properties; see Algorithm 3.
For the purpose of this study, the algorithms were
implemented in Common LISP. The mechanism
described in (Dale and Reiter, 1995) to handle
generalisation hierarchies for values for the dif-
ferent properties, referred to in the algorithm here
as FindBestValue, was not implemented since, as
discussed earlier, our representation of the domain
does not make use of a hierarchy of properties.
5 The Output of the Algorithms
Using the knowledge base described in Section 3,
we applied the algorithms from the previous sec-
tion to see whether the referring expressions they
produced were the same as, or similar to, those
produced by the human subjects. This quickly
gave rise to some situations not explicitly ad-
dressed by some of the algorithms; we discuss
these in Section 5.1 below. Section 5.2 discusses
the extent to which the behaviour of the algorithms
matched that of the human data.
5.1 Preference Orderings
The Incremental Algorithm explicitly encodes a
preference ordering over the available properties,
in an attempt to model what appear to be semi-
conventionalised strategies for description that
people use. This also has the consequence of
avoiding a problem that faces the other two algo-
rithms: since the Full Brevity Algorithm and the
Relational Algorithm choose the most discrimina-
tory property at each step, they have to deal with
the case where several properties are equally dis-
criminatory. This turns out to be a common sit-
uation in our domain. Both algorithms implicitly
assume that the choice will be made randomly in
these cases; however, it seems to us more natural
to control this process by imposing some selection
strategy. We do this here by borrowing the idea
of preference ordering from the Incremental Algo-
rithm, and using it as a tie-breaker when multiple
properties are equally discriminatory.
Not including type information (i.e., the fact that
some di is a drawer), which has no discrimina-
tory power and therefore will never be chosen by
any of the algorithms,4 there are only four differ-
ent properties available for the Full Brevity Algo-
rithm and the Incremental Algorithm: row, column,
colour, and position. This gives us 4! = 24 different
possible preference orderings. Since some of the
human-produced descriptions use all four proper-
ties, we tested these two algorithms with all 24
preference orderings.
For the Relational Algorithm, we added the five
relations next to, left of, right of, above, and below.
This results in 9! = 362,880 possible preference
orderings; far too many to test. Since we are
primarily interested in whether the algorithm can
generate the human-produced descriptions, we re-
stricted our testing to those preference orderings
that started with a permutation of the properties
used by the participants; in addition to the 24 pref-
erence orderings above, there are 12 preference or-
derings that incorporate the relational properties.
5.2 Coverage of the Human Data
Overall, the Full Brevity Algorithm is able to gen-
erate 82 out of the 103 non-relational descriptions
from the natural data, providing a recall of 79.6%.
The recall score for the Incremental Algorithm is
95.1%, generating 98 of the 103 descriptions. As
these algorithms do not attempt to generate rela-
tional descriptions, the relational data is not taken
into account in evaluating the performance here.
Both algorithms are able to generate all the
non-relational minimal descriptions found in the
human-produced data. The Full Brevity Algo-
rithm unintentionally replicates the redundancy
found in nine descriptions, and the Incremental
Algorithm produces all but five of the 29 redun-
dant descriptions.
Perhaps surprisingly, the Relational Algorithm
does not generate any of the human-produced de-
scriptions. We will return to consider why this is
the case in the next section.
6 Discussion
There are two significant differences to be consid-
ered here: first, the coverage of redundant descrip-
tions by the Full Brevity and Incremental Algo-
4Consistent with much other work in the field, we as-
sume that the head noun will always be added irrespective
of whether it has any discriminatory power.
67
rithms; and second, the inability of the Relational
Algorithm to replicate any of the human data.
6.1 Coverage of Redundancy
Neither the Full Brevity Algorithm nor the Incre-
mental Algorithm presumes to be able to generate
relational descriptions; however, both algorithms
are able to produce each of the minimal descrip-
tions from the set of natural data with at least one
of the preference orderings. Both also generate
several of the redundant descriptions in the nat-
ural data set, but do not capture all of the human-
generated redundancies.
The Full Brevity Algorithm has as a primary
goal the avoidance of redundant descriptions, so
it is a sign of the algorithm being consistent with
its specification that it covers fewer of the redun-
dant expressions than the Incremental Algorithm.
On the other hand, the fact that it produces any
redundant descriptions signals that the algorithm
doesn?t quite meet its specification. The cases
where the Full Brevity Algorithm produces redun-
dancy are when an entity shares with another en-
tity at least two property-values and, after choos-
ing one of these properties, the next property to
be considered is the other shared one, since it has
the same or a higher discriminatory power than all
other properties. This is a situation that was not
considered in the original algorithm; it is related
to the problem of what to do when two properties
have the same discriminatory power, as noted ear-
lier. In our domain, the situation arises for corner
drawers with the same colour (d4 and d16), and
drawers that are not in a corner but for which there
is another drawer of the same colour in each of the
same row and column (d7 and d8).
The Incremental Algorithm, on the other hand,
generates redundancy when an object shares at
least two property-values with another object and
the two shared properties are the first to be con-
sidered in the preference ordering. This is pos-
sible for corner drawers with the same colour (d4
and d16) and for drawers for which there is another
drawer of the same colour in either the same row,
the same column, or both (d5, d6, d7, d8, d10, d11,
d13, d15).
In these terms, the Incremental Algorithm is
clearly a better model of the human behaviour than
the Full Brevity Algorithm. However, we may ask
why the algorithm does not cover all the redun-
dancy found in the human descriptions. The re-
dundant descriptions which the algorithm does not
generate are as follows:
(5) the blue drawer in the top left corner [d1]
(6) the yellow drawer in the top right corner [d4]
(7) the pink drawer in the top of the column sec-
ond from the right [d3]
(8) the orange drawer in the bottom second from
the right [d14]
(9) the orange drawer in the bottom of the second
column from the right [d14]
The Incremental Algorithm stops selecting prop-
erties when a distinguishing description has been
constructed. In Example (6), for example, the
algorithm would select any of the following, de-
pending on the preference ordering used:
(10) the yellow drawer in the corner
(11) the top left yellow drawer
(12) the drawer in the top left corner
The human subject, however, has added informa-
tion beyond what is required. This could be ex-
plained by our modelling of cornerhood: in Ex-
amples (5) and (6), one has the intuition that the
noun corner is being added simply to provide a
nominal head to the prepositional phrase in an
incrementally-constructed expression of the form
the blue drawer in the top right . . . , in much
the same way as the head noun drawer is added,
whereas we have treated it as a distinct property
that adds discriminatory power. This again em-
phasises the important role the underlying repre-
sentation plays in the generation of referring ex-
pressions: if we want to emulate what people do,
then we not only need to design algorithms which
mirror their behaviour, but these algorithms have
to operate over the same kind of data.
6.2 Relational Descriptions
The fact that the Relational Algorithm generates
none of the human-generated descriptions is quite
disturbing. On closer examination, it transpires
that this is because, in this domain, the discrimi-
natory power of relational properties is generally
always greater than that of any other property, so
a relational property is chosen first. As noted ear-
lier, relational properties appear to be dispreferred
68
in the human data, so the Relational Algorithm is
already disadvantaged. The relatively poor per-
formance of the algorithm is then compounded by
its insistence on continuing to use relational prop-
erties: an absolute property will only be chosen
when either the currently described drawer has no
unused relational properties left, or the number
of distractors has been reduced so much that the
discriminatory power of all remaining relational
properties is lower than that of the absolute prop-
erty, or the absolute property has the same discrim-
inatory power as the best relational one and the ab-
solute property appears before all relations in the
preference ordering.
Consequently, whereas a typical human de-
scription of drawer d2 would be the orange drawer
above the blue drawer, the Relational Algorithm
will produce the description the drawer above the
drawer above the drawer above the pink drawer.
Not only are there no descriptions of this form in
the human-produced data set, but they also sound
more like riddles someone might create to inten-
tionally make it hard for the hearer to figure out
what is meant.
There are a variety of ways in which the be-
haviour of this algorithm might be repaired. We
are currently exploring whether Krahmer et als
(2003) graph-based approach to GRE is able to
provide a better coverage of the data: this algo-
rithm provides the ability to make use of differ-
ent search strategies and weighting mechanisms
when adding properties to a description, and such
a mechanism might be used, for example, to coun-
terbalance the Relational Algorithm?s heavy bias
towards the relations in this domain.
7 Conclusions and Future Work
We have noted a number of regards in which the
algorithms we have explored here do not produce
outputs that are the same as those produced by hu-
mans. Some comments on the generalisability of
these results are appropriate.
First, our results may be idiosyncratic to the
specifics of the particular domain of our experi-
ment. We would point out, however, that the do-
main is more complex, and arguably more real-
istic, than the much-simplified experimental con-
texts that have served as intuitions for earlier work
in the field; we have in mind here in particular the
experiments discussed in (Ford and Olson, 1975),
(Sonnenschein, 1985) and (Pechmann, 1989). In
the belief that the data provides a good test set
for the generation of referring expressions, we are
making the data set publicly available 5, so others
may try to develop algorithms covering the data.
A second concern is that we have only explored
the extent to which three specific algorithms are
able to cover the human data. Many of the other al-
gorithms in the literature take these as a base, and
so are unlikely to deliver significantly different re-
sults. The major exceptions here may be (a) van
Deemter?s (2002) algorithm for sets; recall that we
excluded from the human data used here 16 ref-
erences that involved sets; and, as noted above,
(b) Krahmer et als (2003) graph-based approach
to GRE, which may perform better than the Re-
lational Algorithm on descriptions using relations.
In future work, we intend to explore to what extent
our findings extend to other algorithms.
In conclusion, we point to two directions where
we believe further work is required.
First, as we noted early in this paper, it is clear
that there can be many different ways of refer-
ring to the same entity. Existing algorithms are
all deterministic and therefore produce exactly one
?best? description for each entity; but the human-
produced data clearly shows that there are many
equally valid ways of describing an entity. We
need to find some way to account for this in our
algorithms. Our intuition is that this is likely to
be best cashed out in terms of different ?refer-
ence strategies? that different speakers adopt in
different situations; we are reminded here of Car-
letta?s (1992) distinction between risky and cau-
tious strategies for describing objects in the Map
Task domain. More experimentation is required in
order to determine just what these strategies are:
are they, for example, characterisable as things
like ?Produce a referring expression that is as short
as possible? (the intuition behind the Full Brevity
Algorithm), ?Just say what comes to mind first and
keep adding information until the description dis-
tinguishes the intended referent? (something like
the Incremental Algorithm), or perhaps a strategy
of minimising the cognitive effort for either the
speaker or the hearer? Further psycholinguistic
experiments and data analysis are required to de-
termine the answers here.
Our second observation is that the particular re-
sults we have presented here are, ultimately, en-
5The data set is publicly available from
http://www.ics.mq.edu.au/?jviethen/drawers
69
tirely dependent upon the underlying representa-
tions we have used, and the decisions we have
made in choosing how to represent the properties
and relations in the domain. We believe it is im-
portant to draw attention to the fact that precisely
how we choose to represent the domain has an im-
pact on what the algorithms will do. If we are
aiming for naturalism in our algorithms for refer-
ring expression generation, then ideally we would
like our representations to mirror those used by hu-
mans; but, of course, we don?t have direct access
to what these are.
There is clearly scope for psychological exper-
imentation, perhaps along the lines initially ex-
plored by (Rosch, 1978), to determine some con-
straints here. In parallel, we are considering fur-
ther exploration into the variety of representations
that can be used, particularly with regard to the
question of which properties are considered to be
?primitive?, and which are generated by some in-
ference mechanism; this is a much neglected as-
pect of the referring expression generation task.
References
D. E. Appelt. 1981. Planning Natural Language Ut-
terances to Satisfy Multiple Goals. Ph.D. thesis,
Stanford University.
J. Bateman. 1999. Using aggregation for selecting
content when generating referring expressions. In
Proceedings of the 37th Meeting of the ACL, pages
127?134.
J. C. Carletta. 1992. Risk-taking and Recovery in Task-
oriented Dialogue. Ph.D. thesis, University of Edin-
burgh.
R. Dale and N. Haddock. 1991. Generating referring
expressions involving relations. In Proceedings of
the 5th Meeting of the EACL, pages 161?166, Berlin,
Germany.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(2):233?
263.
R. Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Meeting of the ACL, pages
68?75.
W. Ford and D. Olson. 1975. The elaboration of
the noun phrase in children?s description of objects.
Journal of Experimental Child Psychology, 19:371?
382.
K. Funakoshi, S. Watanabe, N. Kuriyama, and T. Toku-
naga. 2004. Generating referring expressions using
perceptual groups. In Proceedings of the 3rd INLG,
pages 51?60.
C. Gardent. 2002. Generating minimal definite de-
scriptions. In Proceedings of the 40th Meeting of
the ACL, pages 96?103.
A. Gatt. 2006. Structuring knowledge for reference
generation: A clustering algorithm. In Proceedings
of the 11th Meeting of the EACL.
H. P. Grice. 1975. Logic and conversation. In P. Cole
and J. Morgan, editors, Syntax and Semantics Vol-
ume 3: Speech Acts, pages 43?58. Academic Press.
H. Horacek. 1997. An algorithm for generating ref-
erential descriptions with flexible interfaces. In Pro-
ceedings of the 35th Meeting of the ACL, pages 127?
134.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In
K. van Deemter and R. Kibble, editors, Informa-
tion Sharing: Reference and Presupposition in Lan-
guage Generation and Interpretation, pages 223?
264. CSLI.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
D. D. McDonald. 1980. Natural Language Generation
as a Process of Decision-making Under Constraints.
Ph.D. thesis, Massachusetts Institute of Technology.
T. Pechmann. 1989. Incremental speech produc-
tion and referential overspecification. Linguistics,
27:89?110.
E. Reiter and R. Dale. 1992. A fast algorithm for the
generation of referring expressions. In Proceedings
of the 14th Meeting of the ACL, pages 232?238.
E. Rosch. 1978. Principles of categorization. In Cog-
nition and Categorization, pages 27?48. Lawrence
Erlbaum, Hillsdale, NJ.
S. Sonnenschein. 1985. The development of referen-
tial communication skills: Some situations in which
speakers give redundant messages. Journal of Psy-
cholinguistic Research, 14:489?508.
M. Stone. 2000. On identifying sets. In Proceedings
of the 1st INLG, pages 116?123.
K. van Deemter and M. M. Halldo?rsson. 2001. Logi-
cal form equivalence: The case referring expressions
generation. In Proceedings of the 8th ENLG.
K. van Deemter. 2002. Generating referring expres-
sions: Boolean extensions of the incremental algo-
rithm. Computational Linguistics, 28(1):37?52.
I. van der Sluis and E. Krahmer. 2004. Evaluating
multimodal NLG using production experiments. In
Proceedings of the 4th LREC, pages 209?212, 26-28
May.
T. Winograd. 1972. Understanding Natural Language.
Academic Press.
70
Proceedings of the 12th European Workshop on Natural Language Generation, pages 58?65,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Referring Expression Generation through Attribute-Based Heuristics
Robert Dale and Jette Viethen
Centre for Language Technology
Macquarie University
Sydney, Australia
rdale@ics.mq.edu.au|jviethen@ics.mq.edu.au
Abstract
In this paper, we explore a corpus of
human-produced referring expressions to
see to what extent we can learn the referen-
tial behaviour the corpus represents. De-
spite a wide variation in the way subjects
refer across a set of ten stimuli, we demon-
strate that component elements of the re-
ferring expression generation process ap-
pear to generalise across participants to a
significant degree. This leads us to pro-
pose an alternative way of thinking of re-
ferring expression generation, where each
attribute in a description is provided by a
separate heuristic.
1 Introduction
The last few years have witnessed a considerable
move towards empiricism in referring expression
generation; this is evidenced both by the growing
body of work that analyses and tries to replicate
the content of corpora of human-produced refer-
ring expressions, and particularly by the signifi-
cant participation in the TUNA and GREC chal-
lenge tasks built around such activities (see, for
example, (Belz and Gatt, 2007; Belz et al, 2008;
Gatt et al, 2008)). One increasingly widespread
observation?obvious in hindsight, but surpris-
ingly absent from much earlier work on referring
expression generation?is that one person?s refer-
ential behaviour differs from that of another: given
the same referential task, different subjects will
choose different referring expressions to identify
a target referent. Faced with this apparent lack of
cross-speaker consistency in how to refer to enti-
ties, we might question the validity of any exercise
that tries to develop an algorithm on the basis of
data from multiple speakers.
In this paper we revisit the corpus of data
that was introduced and discussed in (Viethen
and Dale, 2008a; Viethen and Dale, 2008b) with
the objective of determining what referential be-
haviour, if any, might be learned automatically
from the data. We find that, despite the apparent
diversity of the data when we consider the pro-
duction of referring expressions across subjects,
a closer examination reveals that individual at-
tributes within referring expressions do appear to
be selected on the basis of contextual factors with
a high degree of consistency. This suggests that re-
ferring behaviour might be best thought of as con-
sisting of a combination of lower-level heuristics,
with each individual?s overall referring behaviour
being constructed from a potentially distinct com-
bination of these common heuristics.
In Section 2 we describe the corpus we use for
the experiments in this paper. In Section 3 we ex-
plore to what extent we can use this corpus to learn
an algorithm for referring expression generation;
in Section 4 we look more closely at the nature of
individual variation within the corpus. Section 5
briefly discusses related work on the use of ma-
chine learning in referring expression generation,
and Section 6 draws some conclusions and points
to future work.
2 The Corpus
2.1 General Overview
The corpus we use was collected via a data gath-
ering experiment described in (Viethen and Dale,
2008a; Viethen and Dale, 2008b). The purpose of
the data gathering was to gain some insight into
how human subjects use relational referring ex-
pressions, a relatively unexplored aspect of refer-
ring expression generation. Participants visited a
website, where they first saw an introductory page
with a set of simple instructions and a sample stim-
ulus scene consisting of three objects. Each par-
ticipant was then assigned one of two trial sets of
ten scenes each; the two trial sets are superficially
58
Figure 1: The stimulus scenes. The letters indi-
cate which schema from Figure 2 each column of
scenes is based on.
different, but the elements of the sets are pairwise
identical in terms of the factors explored in the re-
search. The complete set of 20 scenes is shown in
Figure 1, where Trial Set 1 consists of Scenes 1
through 10, and Trial Set 2 consists of Scenes 11
through 20.1
The scenes were presented successively in a
preset order, which was the same for each partic-
ipant. Below each scene, the participant had to
complete the sentence Please pick up the . . . in a
text box before clicking on a button to see the next
scene. The task was to describe the target referent
in the scene (marked by a grey arrow) in a way that
would enable a friend looking at the same scene to
pick it out from the other objects.
The experiment was completed by 74 partici-
pants from a variety of different backgrounds and
ages; most were university-educated and in their
early or mid twenties. For reasons discussed in
(Viethen and Dale, 2008b), the data of 11 partici-
pants was discarded. Of the remaining 63 partici-
pants, 29 were female, while 34 were male.
2.2 Stimulus Design
The design of the stimuli used in the experiment is
described in detail in (Viethen and Dale, 2008a).
1Scene 1 is paired with Scene 11, Scene 2 with Scene
12, and so on; in each pair, the only differences are the
colour scheme used and the left?right orientation, with these
variations being introduced to make the experiment less
monotonous for subjects; (Viethen and Dale, 2008a) report
that these characteristics of the scenes appear to have no sig-
nificant effect on the forms of reference used.
Figure 2: The schemata which form the basis for
the stimulus scenes.
We provide a summary of the key points here.
In order to explore even the most basic hypothe-
ses with respect to the use of relational expres-
sions, which was the aim of the original study,
scenes containing at least three objects were re-
quired. One of these objects is the intended ref-
erent, which is referred to here as the target. The
subject has to describe the target in such a way as
to distinguish it from the other two objects in the
scene. Although the scenes presented to the sub-
jects are such that spatial relations are never nec-
essary to distinguish the target, they are set up so
that one of the two non-target objects was clearly
closer to the target. This object is referred to as the
(potential) landmark; and we call the third object
in the scene the distractor.
To minimise the number of variables in the ex-
periments, scenes are restricted to only two kinds
of objects, cubes and balls. The objects also vary
in two dimensions: colour (either green, blue,
yellow, or red); and size (either large or small).
To further reduce the number of factors in the
scene design, the landmark and distractor are al-
ways placed clearly side by side, and the target is
located on top of or directly in front of the land-
mark.
Finally, to reduce the set of possible stimuli to a
manageable number, five schemata (see Figure 2)
were created as a basis for the final stimulus set.
The design of these schemata was informed by a
number of research questions with regard to the
use of relations; see (Viethen and Dale, 2008b). A
schema determines the type and size of each object
in the scenes that are based on it, and determines
which objects share colour. So, for example, in
scenes based on Schema C, the target is a small
ball; the landmark is a large cube with different
colour from the target; and the distractor is a large
ball sharing its colour with the target.
59
Label Pattern Example
A ?tg col, tg type? the blue cube
B ?tg col, tg type, rel, lm col, lm type? the blue cube in front of the red ball
C ?tg col, tg type, rel, lm size, lm col, lm type? the blue cube in front of the large red ball
D ?tg col, tg type, rel, lm size, lm type? the blue cube in front of the large ball
E ?tg col, tg type, rel, lm type? the blue cube in front of the ball
F ?tg size, tg col, tg type? the large blue cube
G ?tg size, tg col, tg type, rel, lm col, lm type? the large blue cube in front of the red ball
H ?tg size, tg col, tg type, rel, lm size, lm col, lm type? the large blue cube in front of the large red ball
I ?tg size, tg col, tg type, rel, lm size, lm type? the large blue cube in front of the large ball
J ?tg size, tg col, tg type, rel, lm type? the large blue cube in front of the ball
K ?tg size, tg type? the large cube
L ?tg size, tg type, rel, lm size, lm type? the large cube in front of the large ball
M ?tg size, tg type, rel, lm type? the large cube in front of the ball
N ?tg type? the cube
O ?tg type, rel, lm col, lm type? the cube in front of the red ball
P ?tg type, rel, lm size, lm col, lm type? the cube in front of the large red ball
Q ?tg type, rel, lm size, lm type? the cube in front of the large ball
R ?tg type, rel, lm type? the cube in front of the ball
Table 1: The 18 different patterns corresponding to the different forms of description that occur in the
GRE3D3 corpus.
From each schema, four distinct scenes were
generated, resulting in the 20 stimulus scenes
shown in Figure 1. As noted above, there are really
only 10 distinct ?underlying? scene types here, so
in the remainder of this paper we will talk in terms
of Scenes 1 through 10, where the data from the
pairwise matched scenes are conflated.
2.3 The GRE3D3 Corpus2
Before conducting any quantitative data analysis,
some syntactic and lexical normalisation was car-
ried out on the data provided by the participants.
In particular, spelling mistakes were corrected;
normalised names were used for colour values and
head nouns (for example, box was replaced by
cube); and complex syntactic structures such as
relative clauses were replaced with semantically
equivalent simpler ones such as adjectives. These
normalisation steps should be of no consequence
to the analysis presented here, since we are solely
interested in exploring the semantic content of re-
ferring expressions, not their lexical and syntactic
surface structure.
For the purposes of the machine learning exper-
iments described in this paper, we made a few fur-
ther changes to the data set in order to keep the
number of properties and their possible values low.
We removed locative expressions that made refer-
2The data set resulting from the experiment described
above is known as the GRE3D3 Corpus; the name stands for
?Generation of Referring Expressions in 3D scenes with 3
Objects?.
ence to a part of the scene (58 instances) and ref-
erences to size as the same (4 instances); so, for
example, the blue cube on top of the green cube
in the right and the blue cube on top of the green
cube of the same size both became the blue cube
on top of the green cube. We also removed the
mention of a third object from ten descriptions in
order to keep the number of possible objects per
description to a maximum of two. These changes
resulted in seven descriptions no longer satisfying
the criterion of being fully distinguishing, so we
removed these descriptions from the corpus.
3 Learning Description Patterns
The resulting corpus consists of 623 descriptions.
Every one of these is an instance of one of the 18
patterns shown in Table 1; for ease of reference,
we label these patterns A through R. Each pattern
indicates the sequence of attributes used in the de-
scription, where each attribute is identified by the
object it describes (tg for target, lm for landmark)
and the attribute used (col, size and type for colour,
size and type respectively).
Most work on referring expression generation
attempts to determine what attributes should be
used in a description by taking account of aspects
of the context of reference. An obvious question
is then whether we can learn the description pat-
terns in this data from the contexts in which they
were produced. To explore this, we chose to cap-
ture the relevant aspects of context by means of
the notion of characteristics of scenes. The char-
60
Label Attribute Values
tg type = lm type Target and Landmark share Type TRUE, FALSE
tg type = dr type Target and Distractor share Type TRUE, FALSE
lm type = dr type Landmark and Distractor share Type TRUE, FALSE
tg col = lm col Target and Landmark share Colour TRUE, FALSE
tg col = dr col Target and Distractor share Colour TRUE, FALSE
lm col = dr col Landmark and Distractor share Colour TRUE, FALSE
tg size = lm size Target and Landmark share Size TRUE, FALSE
tg size = dr size Target and Distractor share Size TRUE, FALSE
lm size = dr size Landmark and Distractor share Size TRUE, FALSE
rel Relation between Target and Landmark on top of, in front of
Table 2: The 10 characteristics of scenes
acteristics of scenes which we hypothesize might
have an impact on the choice of referential form
are those summarised in Table 2; these are pre-
cisely the characteristics that were manipulated in
the design of the schemata in Figure 2.
Of course, there is no one correct answer for
how to refer to the target in any given scene.
Figure 3 shows the distribution of different pat-
terns across the different scenes; so, for exam-
ple, some scenes (Scenes 4, 5, 9 and 10) result
in only five semantically distinct referring expres-
sion forms, whereas Scene 7 results in 12 distinct
referring expression forms. All of these are distin-
guishing descriptions, so all are acceptable forms
of reference, although some contain more redun-
dancy than others. Most obvious from the chart
is that, for many scenes, there is a predominant
form of reference used; so, for example, pattern F
(?tg size, tg col, tg type?) accounts for 43 (68%)
of the descriptions used in Scene 4, and pattern
A (?tg col, tg type?) is very frequently used in a
number of scenes.3
We used Weka (Witten and Eibe, 2005) with the
J48 decision tree classifer to see what correspon-
dences might be learned between the character-
isics of the scenes listed in Table 2 and the forms
of referring expression used for the target refer-
ents, as shown in Table 1. The pruned decision
tree learned by this method predicted the actual
form of reference used in only 48% of cases under
10-fold cross-validation, but given that there are
many ?gold standard? descriptions for each scene,
3The chart as presented here is obviously too small to en-
able detailed examination, and our use of colour coding will
be of no value in a monchrome rendering of the paper; how-
ever, the overall shape of the data is sufficient to demonstrate
the points we make here.
this low score is hardly surprising; a mechanism
which learns only one answer will inevitably be
?wrong? in many cases. More revealing, however,
is the rule learned from the data:
if tg type = dr type
then use F (?tg size, tg col, tg type?)
else use A (?tg col, tg type?)
endif
Patterns A and F are the two most prevalent pat-
terns in the data, and indeed one or other appears
at least once in the human data for each scene;
consequently, the learned rule is able to produce
a ?correct? answer for every scene.4
4 Individual Variation
One of the most striking things about the data in
this corpus is the extent to which different subjects
appear to do different things when they construct
referring expressions, as demonstrated by the dis-
tribution of patterns in Figure 3. Another way of
looking at this variation is to characterise the be-
haviour of each subject in terms of the sequence of
descriptions they provide in response to the set of
10 stimuli.
Across the 63 subjects, there are 47 different se-
quences; of these, only four occur more than once
(in other words, 43 subjects did not produce the
same sequence of descriptions for the ten scenes as
anyone else). The recurrent sequences, i.e. those
used by at least two people, are shown in Table 3.
Note that the most frequently recurring sequence,
4The fact that the rule is conditioned on a property of the
distractor object may be an artefact of the stimulus set con-
struction; this would require a more diverse set of scenes to
determine.
61
Figure 3: The profile of different description patterns (A through R) for each of the 10 scenes. The length
of the bar indicates how often each of the 18 patterns is used.
which matches the behaviour of nine separate sub-
jects, consists only of uses of patterns A and F.
It remains to be seen to what extent a larger data
set would demonstrate more convergence; how-
ever, the point to be made at present is that any
attempt to predict the behaviour of a given speaker
by means of a model of referring behaviour is go-
ing to have to take account of a great deal of indi-
viual variation.
Nonetheless, we re-ran the J48 classifier de-
scribed in the previous section, this time using
the participant ID as well as the scene character-
istics in Table 2 as features. This improved pattern
prediction to 57.62%. This suggests that individ-
ual differences may indeed be capturable from the
data, although we would need more data than the
mere 10 examples we have from each subject to
learn a good predictive model.
In the face of this lack of data, another approach
is to look for commonalities in the data in terms
of the constituent elements of the different ref-
erence patterns used for each scene. This way
of thinking about the data was foreshadowed by
(Viethen and Dale, 2008b), who observed that the
subjects could be separated into those who always
used relations, those who never used relations, and
those who sometimes used relations. This leads
us to consider whether there are characteristics of
scenes or speakers which are highly likely to result
in specific attributes being used in descriptions. If
this is the case, a decision tree learner should be
able to learn for each individual attribute whether
it should be included in a given situation.
An appropriate baseline for any experiments
here is the success rate of simply including or not
including each attribute (basically a 0-R majority
class classifier), irrespective of the characteristics
of the scene. Table 4 compares the results for
this ?context-free? approach with one model that
is trained on the characteristics of scenes, and an-
other that takes both the characteristics of scenes
and the participant ID into account.5
Interestingly, the ?context-free? strategies work
suprisingly well for predicting the inclusion of
some attributes in the human data. As has been
noted in other work (see for example (Viethen et
al., 2008)), colour is often included in referring ex-
pressions irrespective of its discriminatory power,
and this is borne out by the data here. Perhaps
more suprising is the large degree to which the in-
clusion of landmark size is captured by a context-
free strategy.
5As before, the results reported are for the accuracy of a
pruned J48 decision tree, under 10-fold cross-validation.
62
Improvement on all attribues other than tar-
get colour improves when we take into account
the characteristics of the scenes, consistent with
our assumptions that context does matter. When
we add participant ID to the features used in the
learner, performance improves further still, indi-
cating that there are speaker-specific consistencies
in the data.
It is instructive to look at the rules learned on
the basis of the scene characteristics. Not surpris-
ingly, the rule derived for target colour inclusion is
simply to always include the colour (i.e., the same
context-free colour inclusion rule that proves most
effective in modelling the data without reference
to scene characteristics). The rules for including
the other attributes on the basis of scene charac-
teristics (but not participant ID) are shown in Fig-
ure 4.
The rules learned when we include participant
ID are more conplex, but can be summarised in a
way that demonstrates how this approach can re-
veal something about the variety of ways in which
speakers appear to approach the task of referring
expression generation. Focussing, as an example,
just on the question of whether or not to use the
target object?s colour in a referring expression, we
find the following:
? 48 participants always used colour, irrespec-
tive of the context (this corresponds to the
baseline rule learned above).
? The other participants always use colour if
the target and the landmark are of the same
type (which again is intuitively quite appro-
priate).
? When the landmark and the target are not
of the same type, we see more variation in
behaviour; 19 participants simply don?t use
colour, and the behaviour of seven can be
captured via a more complex analysis: four
use colour if the target and the distractor are
the same size, two use colour if the target and
distractor are of the same size and the target
is on top of the landmark, and one uses colour
if the target and distractor share colour.
Again, the specific details of the rules learned here
are probably not particularly significant, based as
they are on a limited data set and a set of stimuli
that may give elevated status to incidental proper-
ties. However, the general point remains that we
Target Size:
if tg type = dr type then include tg size
Relation:
if rel = on top of and lm size = dr size
then include rel
Landmark Colour:
if we have used a relation then include lm col
Landmark Size:
if we have used a relation and tg col = lm col
then include lm size
Figure 4: Rules learned on the basis of scene char-
acteristics
can use this kind of analysis to identify possible
rules for the inclusion of individual attributes in
referring expressions.
What this suggests is that we might be able to
capture the behaviour of individual speakers not
in terms of an overall strategy, but as a compos-
ite of heuristics, where each heuristic accounts for
the inclusion of a specific attribute. The rules, or
heuristics, shown in Figure 4 are just those which
are most successful in predicting the data; but
there can be many other rules that might be used
for the inclusion of particular attributes. So, for
example, I might be the kind of speaker who just
automatically includes the colour of an intended
referent without any analysis of the scene; and I
might be the kind of speaker who always uses a
relation to a nearby landmark in describing the in-
tended referent. Or I might be the kind of speaker
who surveys the scene and takes note of whether
the landmark?s colour is distinctive; and so on.
Thought of in this way, each speaker?s approach
to reference is like a set of ?parallel gestalts? that
contribute information to the description being
constructed. The particular rules for inclusion that
any speaker uses might vary depending on their
personal past history, and perhaps even on the ba-
sis of situation-specific factors that on a given oc-
casion might lean the speaker towards either being
?risky? or ?cautious? (Carletta, 1992).
As alluded to earlier, the specific content of the
rules shown in Figure 4 may appear idiosyncratic;
they are just what the limited data in the corpus
63
Pattern Sequence (?Scene#,DescriptionPattern?) Number of subjects
?1,A?, ?2,A?, ?3,G?, ?4,F?, ?5,A?, ?6,A?, ?7,A?, ?8,G?, ?9,F?, ?10,A? 2
?1,B?, ?2,B?, ?3,G?, ?4,H?, ?5,B?, ?6,B?, ?7,B?, ?8,G?, ?9,H?, ?10,B? 2
?1,N?, ?2,N?, ?3,K?, ?4,F?, ?5,A?, ?6,N?, ?7,N?, ?8,K?, ?9,F?, ?10,A? 6
?1,A?, ?2,A?, ?3,F?, ?4,F?, ?5,A?, ?6,A?, ?7,A?, ?8,F?, ?9,F?, ?10,A? 9
Table 3: Sequences of description patterns found more than once
Attribute to Include Baseline (0-R) Using Scene Using Scene
Characteristics Characteristics
and Participant
Target Colour 78.33% 78.33% 89.57%
Target Size 57.46% 90.85% 90.85%
Relation 64.04% 65.00% 81.22%
Landmark Colour 74.80% 87.31% 93.74%
Landmark Size 88.92% 95.02% 95.02%
Table 4: Accuracy of Learning Attribute Inclusion; statistically significant increases (p<.01) in bold.
supports, and some elements of the rules may be
due to artefacts of the specific stimuli used in the
data gathering. We would require a more diverse
set of stimuli to determine whether this is the case,
but the basic point stands: we can find correlations
between characteristics of the scenes and the pres-
ence or absence of particular attributes in referring
expressions, even if we cannot predict so well the
particular combinations of these correlations that
a given speaker will use in a given situation.
5 Related Work
There is a significant body of work on the use
of machine learning in referring expression gen-
eration, although typically focussed on aspects of
the problem that are distinct from those considered
here.
In the context of museum item descriptions,
Poesio et al (1999) explore the decision of what
type of referring expression NP to use to refer to
a given discourse entity, using a statistical model
to choose between using a proper name, a definite
description, or a pronoun. More recently, Stoia et
al. (2006) attempt a similar task, but this time in
an interactive navigational domain; as well as de-
termining what type of referring expression to use,
they also try to learn whether a modifier should be
included. Cheng et al (2001) try to learn rules for
the incorporation of non-referring modifiers into
noun phrases.
A number of the contributions to the 2008 GREC
and TUNA evaluation tasks (Gatt et al, 2008) have
made use of machine learning techniques. The
GREC task is primarily concerned with the choice
of form of reference (i.e. whether a proper name, a
descriptive NP or a pronoun should be used), and
so is less relevant to the focus of the present pa-
per. Much of the work on the TUNA Task is rel-
evant, however, since this also is concerned with
determining the content of referring expressions
in terms of the attributes used to build a distin-
guishing description. In particular, Fabbrizio et al
(2008) explore the impact of individual style and
priming on attribute selection for referring expres-
sion generation, and Bohnet (2008) uses a nearest-
neighbour learning technique to acquire an indi-
vidual referring expression generation model for
each person.
Other related approaches to attribute selection
in the context of the TUNA task are explored in
(Gerva?s et al, 2008; de Lucena and Paraboni,
2008; Kelleher and Mac Namee, 2008; King,
2008).
6 Conclusions
We know that people?s referential behaviour varies
significantly. Despite this apparent variation, we
have demonstrated above that there does appear to
be a reasonable correlation between characteristics
of the scene and the incorporation of particular at-
tributes in a referring expression. One way to con-
ceptualise this is that the decision as to whether or
64
not to incorporate a given feature such as colour
or size may vary from speaker to speaker; this is
evidenced by the data. We might think of these as
individual reference strategies; a good example of
such a strategy, widely attested across many exper-
iments, is the decision to include colour in a refer-
ring expression independent of its discriminatory
power, perhaps because it is an easily perceivable
and often-useful attribute. The overall approach to
reference that is demonstrated by a given speaker
then consists of the gathering together of a number
of strategies; the particular combinations may vary
from speaker to speaker, but as is demonstrated by
the analysis in this paper, some of the strategies
are widely used.
In current work, we are gathering a much larger
data set using more complex stimuli. This will al-
low the further development and testing of the ba-
sic ideas proposed in this paper as well as their
integration into a full referring expression genera-
tion algorithm.
References
Anja Belz and Albert Gatt. 2007. The attribute selec-
tion for GRE challenge: Overview and evaluation
results. In Proceedings of UCNLG+MT: Language
Generation and Machine Translation, pages 75?83,
Copenhagen, Denmark.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2008. The GREC challenge 2008: Overview and
evaluation results. In Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
pages 183?191, Salt Fork OH, USA.
Bernd Bohnet. 2008. The fingerprint of human refer-
ring expressions and their surface realization with
graph transducers. In Proceedings of the 5th Inter-
national Conference on Natural Language Genera-
tion, pages 207?210, Salt Fork OH, USA.
Jean C. Carletta. 1992. Risk-taking and Recovery in
Task-Oriented Dialogue. Ph.D. thesis, University of
Edinburgh.
Hua Cheng, Massimo Poesio, Renate Henschel, and
Chris Mellish. 2001. Corpus-based NP modifier
generation. In Proceedings of the Second Meeting
of the North American Chapter of the Association
for Computational Linguistics, Pittsburgh PA, USA.
Diego Jesus de Lucena and Ivandre? Paraboni. 2008.
USP-EACH: Frequency-based greedy attribute se-
lection for referring expressions generation. In Pro-
ceedings of the Fifth International Natural Lan-
guage Generation Conference, pages 219?220, Salt
Fork OH, USA.
Giuseppe Di Fabbrizio, Amanda J. Stent, and Srinivas
Bangalore. 2008. Referring expression generation
using speaker-based attribute selection and trainable
realization (ATTR). In Proceedings of the Fifth In-
ternational Natural Language Generation Confer-
ence, Salt Fork OH, USA.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA challenge 2008: Overview and evaluation re-
sults. In Proceedings of the Fifth International Nat-
ural Language Generation Conference, pages 198?
206, Salt Fork OH, USA.
Pablo Gerva?s, Raquel Herva?s, and Carlos Leo?n. 2008.
NIL-UCM: Most-frequent-value-first attribute se-
lection and best-scoring-choice realization. In Pro-
ceedings of the Fifth International Natural Lan-
guage Generation Conference, pages 215?218, Salt
Fork OH, USA.
John D. Kelleher and Brian Mac Namee. 2008. Refer-
ring expression generation challenge 2008: DIT sys-
tem descriptions. In Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
pages 221?223, Salt Fork OH, USA.
Josh King. 2008. OSU-GP: Attribute selection using
genetic programming. In Proceedings of the Fifth
International Natural Language Generation Confer-
ence, pages 225?226, Salt Fork OH, USA.
Massimo Poesio, Renate Henschel, Janet Hitzeman,
and Rodger Kibble. 1999. Statistical NP genera-
tion: A first report. In Proceedings of the ESSLLI
Workshop on NP Generation, Utrecht, The Nether-
lands.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2006. Noun phrase
generation for situated dialogs. In Proceedings of
the 4th International Conference on Natural Lan-
guage Generation, pages 81?88, Sydney, Australia.
Jette Viethen and Robert Dale. 2008a. Generating
referring expressions: What makes a difference?
In Australasian Language Technology Association
Workshop 2008, pages 160?168, Hobart, Australia.
Jette Viethen and Robert Dale. 2008b. The use of
spatial relations in referring expression generation.
In Proceedings of the 5th International Conference
on Natural Language Generation, pages 59?67, Salt
Fork OH, USA.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t
Theune, and Pascal Touset. 2008. Controlling re-
dundancy in referring expressions. In Proceedings
of the 6th Language Resources and Evaluation Con-
ference, Marrakech, Morocco.
Ian H. Witten and Frank Eibe. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
65
Proceedings of the 12th European Workshop on Natural Language Generation, pages 165?173,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Report on the First NLG Challenge on
Generating Instructions in Virtual Environments (GIVE)
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the first installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE), a new
shared task for the NLG community. We
motivate the design of the challenge, de-
scribe how we carried it out, and discuss
the results of the system evaluation.
1 Introduction
This paper reports on the methodology and results
of the First Challenge on Generating Instructions
in Virtual Environments (GIVE-1), which we ran
from March 2008 to February 2009. GIVE is a
new shared task for the NLG community. It pro-
vides an end-to-end evaluation methodology for
NLG systems that generate instructions which are
meant to help a user solve a treasure-hunt task in a
virtual 3D world. The most innovative aspect from
an NLG evaluation perspective is that the NLG
system and the user are connected over the Inter-
net. This makes it possible to cheaply collect large
amounts of evaluation data.
Five NLG systems were evaluated in GIVE-
1 over a period of three months from November
2008 to February 2009. During this time, we
collected 1143 games that were played by users
from 48 countries. As far as we know, this makes
GIVE-1 the largest evaluation effort in terms of
experimental subjects ever. We have evaluated the
five systems both on objective measures (success
rate, completion time, etc.) and subjective mea-
sures which were collected by asking the users to
fill in a questionnaire.
GIVE-1 was intended as a pilot experiment in
order to establish the validity of the evaluation
methodology and understand the challenges in-
volved in the instruction-giving task. We believe
that we have achieved these purposes. At the same
time, we provide evaluation results for the five
NLG systems which will help their developers im-
prove them for participation in a future challenge,
GIVE-2. GIVE-2 will retain the successful aspects
of GIVE-1, while refining the task to emphasize
aspects that we found to be challenging. We invite
the ENLG community to participate in designing
GIVE-2.
Plan of the paper. The paper is structured as
follows. In Section 2, we will describe and moti-
vate the GIVE Challenge. In Section 3, we will
then describe the evaluation method and infras-
tructure for the challenge. Section 4 reports on
the evaluation results. Finally, we conclude and
discuss future work in Section 5.
2 The GIVE Challenge
In the GIVE scenario, subjects try to solve a trea-
sure hunt in a virtual 3D world that they have not
seen before. The computer has a complete sym-
bolic representation of the virtual world. The chal-
lenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
Complete maps of the game worlds used in the
evaluation are shown in Figs. 3?5: In these worlds,
players must pick up a trophy, which is in a wall
safe behind a picture. In order to access the tro-
165
Figure 1: What the user sees when playing with
the GIVE Challenge.
phy, they must first push a button to move the pic-
ture to the side, and then push another sequence of
buttons to open the safe. One floor tile is alarmed,
and players lose the game if they step on this tile
without deactivating the alarm first. There are also
a number of distractor buttons which either do
nothing when pressed or set off an alarm. These
distractor buttons are intended to make the game
harder and, more importantly, to require appropri-
ate reference to objects in the game world. Finally,
game worlds contained a number of objects such
as chairs and flowers that did not bear on the task,
but were available for use as landmarks in spatial
descriptions generated by the NLG systems.
2.1 Why a new NLG evaluation paradigm?
The GIVE Challenge addresses a need for a new
evaluation paradigm for natural language gener-
ation (NLG). NLG systems are notoriously hard
to evaluate. On the one hand, simply compar-
ing system outputs to a gold standard using auto-
matic comparison algorithms has limited value be-
cause there can be multiple generated outputs that
are equally good. Finding metrics that account
for this variability and produce results consistent
with human judgments and task performance mea-
sures is difficult (Belz and Gatt, 2008; Stent et
al., 2005; Foster, 2008). Human assessments of
system outputs are preferred, but lab-based eval-
uations that allow human subjects to assess each
aspect of the system?s functionality are expensive
and time-consuming, thereby favoring larger labs
with adequate resources to conduct human sub-
jects studies. Human assessment studies are also
difficult to replicate across sites, so system devel-
opers that are geographically separated find it dif-
ficult to compare different approaches to the same
problem, which in turn leads to an overall diffi-
culty in measuring progress in the field.
The GIVE-1 evaluation was conducted via a
client/server architecture which allows any user
with an Internet connection to provide system
evaluation data. Internet-based studies have been
shown to provide generous amounts of data in
other areas of AI (von Ahn and Dabbish, 2004;
Orkin and Roy, 2007). Our implementation allows
smaller teams to develop a system that will partici-
pate in the challenge, without taking on the burden
of running the human evaluation experiment, and
it provides a direct comparison of all participating
systems on the same evaluation data.
2.2 Why study instruction-giving?
Next to the Internet-based data collection method,
GIVE also differs from other NLG challenges by
its emphasis on generating instructions in a vir-
tual environment and in real time. This focus on
instruction giving is motivated by a growing in-
terest in dialogue-based agents for situated tasks
such as navigation and 3D animations. Due to its
appeal to younger students, the task can also be
used as a pedagogical exercise to stimulate interest
among secondary-school students in the research
challenges found in NLG or Computational Lin-
guistics more broadly.
Embedding the NLG task in a virtual world en-
courages the participating research teams to con-
sider communication in a situated setting. This
makes the NLG task quite different than in other
NLG challenges. For example, experiments have
shown that human instruction givers make the in-
struction follower move to a different location in
order to use a simpler referring expression (RE)
(Stoia et al, 2006). That is, RE generation be-
comes a very different problem than the classi-
cal non-situated Dale & Reiter style RE genera-
tion, which focuses on generating REs that are sin-
gle noun phrases in the context of an unchanging
world.
On the other hand, because the virtual environ-
ments scenario is so open-ended, it ? and specif-
ically the instruction-giving task ? can potentially
be of interest to a wide range of NLG researchers.
This is most obvious for research in sentence plan-
ning (GRE, aggregation, lexical choice) and real-
ization (the real-time nature of the task imposes
high demands on the system?s efficiency). But if
166
extended to two-way dialog, the task can also in-
volve issues of prosody generation (i.e., research
on text/concept-to-speech generation), discourse
generation, and human-robot interaction. Finally,
the game world can be scaled to focus on specific
issues in NLG, such as the generation of REs or
the generation of navigation instructions.
3 Evaluation Method and Logistics
Now we describe the method we applied to obtain
experimental data, and sketch the software infras-
tructure we developed for this purpose.
3.1 Software architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components (shown in Fig. 2):
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide. When a user starts the
client, it connects to the Matchmaker and is ran-
domly assigned an NLG server and a game world.
The client and NLG server then communicate over
the course of one game. At the end of the game,
the client displays a questionnaire to the user, and
the game log and questionnaire data are uploaded
to the Matchmaker and stored in a database. Note
that this division allows the challenge to be con-
ducted without making any assumptions about the
internal structure of an NLG system.
The GIVE software is implemented in Java and
available as an open-source Google Code project.
For more details about the software, see (Koller et
al., 2009).
3.2 Subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the internet.
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 2: The GIVE architecture.
Collecting data from anonymous users over the
Internet presents a variety of issues that a lab-
based experiment does not. An Internet-based
evaluation skews the demographic of the subject
pool toward people who use the Internet, but prob-
ably no more so than if recruiting on a college
campus. More worrisome is that, without a face-
to-face meeting, the researcher has less confidence
in the veracity of self-reported demographic data
collected from the subject. For the purposes of
NLG software, the most important demographic
question is the subject?s fluency in English. Play-
ers of the GIVE 2009 challenge were asked to self-
report their command of English, age, and com-
puter experience. English proficiency did interact
with task completion, which leads us to conclude
that users were honest about their level of English
proficiency. See section 4.4 below for a discus-
sion of this interaction. All-in-all, we feel that the
advantage gained from the large increase in the
size of the subject pool offsets any disadvantage
accrued from the lack of accurate demographic in-
formation.
3.3 Materials
Figs. 3?5 show the layout of the three evaluation
worlds. The worlds were intended to provide vary-
ing levels of difficulty for the direction-giving sys-
tems and to focus on different aspects of the prob-
lem. World 1 is very similar to the development
world that the research teams were given to test
their system on. World 2 was intended to focus
on object descriptions - the world has only one
room which is full of objects and buttons, many of
which cannot be distinguished by simple descrip-
tions. World 3, on the other hand, puts more em-
phasis on navigation directions as the world has
many interconnected rooms and hallways.
The difference between the worlds clearly bears
out in the task completion rates reported below.
167
plant
chair
alarm
lamp
tutorial room
couch
safe
Figure 3: World 1
lamp
plant
chair
alarm
tutorial room
safe
Figure 4: World 2
plant
chair
lamp
safe
tutorial room
alarm
Figure 5: World 3
3.4 Timeline
After the GIVE Challenge was publicized in
March 2008, eight research teams signed up for
participation. We distributed an initial version of
the GIVE software and a development world to
these teams. In the end, four teams submitted
NLG systems. These were connected to a cen-
tral Matchmaker instance that ran for about three
months, from 7 November 2008 to 5 February
2009. During this time, we advertised participa-
tion in the GIVE Challenge to the public in order
to obtain experimental subjects.
3.5 NLG systems
Five NLG systems were evaluated in GIVE-1:
1. one system from the University of Texas at
Austin (?Austin? in the graphics below);
2. one system from Union College in Schenec-
tady, NY (?Union?);
3. one system from the Universidad Com-
plutense de Madrid (?Madrid?);
4. two systems from the University of Twente:
one serious contribution (?Twente?) and one
more playful one (?Warm-Cold?).
Of these systems, ?Austin? can serve as a base-
line: It computes a plan consisting of the actions
the user should take to achieve the goal, and at
each point in the game, it realizes the first step
in this plan as a single instruction. The ?Warm-
Cold? system generates very vague instructions
that only tell the user if they are getting closer
(?warmer?) to their next objective or if they are
moving away from it (?colder?). We included this
system in the evaluation to verify whether the eval-
uation methodology would be able to distinguish
such an obviously suboptimal instruction-giving
strategy from the others.
Detailed descriptions of these systems
as well as each team?s own analysis of
the evaluation results can be found at
http://www.give-challenge.org/
research/give-1.
4 Results
We now report on the results of GIVE-1. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures.
Notice that some of our evaluation measures are
in tension with each other: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?), such as the ?Austin? baseline, will lead the
user to completing the task in a minimum number
of steps; but it will require more instructions than
a system that aggregates these. This is intentional,
and emphasizes both the pilot experiment char-
acter of GIVE-1 and our desire to make GIVE a
friendly comparative challenge rather than a com-
petition with a clear winner.
4.1 Demographics
Over the course of three months, we collected
1143 valid games. A game counted as valid if the
game client didn?t crash, the game wasn?t marked
as a test game by the developers, and the player
completed the tutorial.
Of these games, 80.1% were played by males
and 9.9% by females; a further 10% didn?t specify
their gender. The players were widely distributed
over countries: 37% connected from an IP address
in the US, 33% from an IP address in Germany,
and 17% from China; Canada, the UK, and Aus-
tria also accounted for more than 2% of the partic-
168
037,5
75,0
112,5
150,0
N
o
v
 
7
D
e
c
 
1
J
a
n
 
1
F
e
b
 
1
F
e
b
 
5
# games per day
German
press release
US
press release
posted to
SIGGEN list
covered by
Chinese blog
Figure 6: Histogram of the connections per day.
ipants each, and the remaining 2% of participants
connected from 42 further countries. This imbal-
ance stems from very successful press releases that
were issued in Germany and the US and which
were further picked up by blogs, including one
in China. Nevertheless, over 90% of the partici-
pants who answered this question self-rated their
English proficiency as ?good? or better. About
75% of users connected with a client running on
Windows, with the rest split about evenly among
Linux and Mac OS X.
The effect of the press releases is also plainly
visible if we look at the distribution of the valid
games over the days from November 7 to Febru-
ary 5 (Fig. 6). There are huge peaks at the
very beginning of the evaluation period, coincid-
ing with press releases through Saarland Univer-
sity in Germany and Northwestern University in
the US, which were picked up by science and tech-
nology blogs on the Web. The US peak contains
a smaller peak of connections from China, which
were sparked by coverage in a Chinese blog.
4.2 Objective measures
We then extracted objective and subjective mea-
surements from the valid games. The objective
measures are summarized in Fig. 7. For each sys-
tem and game world, we measured the percent-
age of games which the users completed success-
fully. Furthermore, we counted the numbers of in-
structions the system sent to the user, measured
the time until task completion, and counted the
number of low-level steps executed by the user
(any key press, to either move or manipulate an
object) as well as the number of task-relevant ac-
tions (such as pushing a button to open a door).
? task success (Did the player get the trophy?)
? instructions (Number of instructions pro-
duced by the NLG system.?)
? steps (Number of all player actions.?)
? actions (Number of object manipulation
action.?)
? second (Time in seconds.?)
?
Measured from the end of the tutorial until the
end of the game.
Figure 7: Objective measurements
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
success
40% 71% 35% 73% 18%
A A
B B
C
instructions
83.2 58.3 121.2 80.3 190.0
A
B B
C
D
steps
103.6 124.3 160.9 117.5 307.4
A A
B B
C
D
actions
11.2 8.7 14.3 9.0 14.3
A A
B
C C
seconds
129.3 174.8 207.0 175.2 312.2
A
B B
C
D
Figure 8: Objective measures by system. Task
success is reported as the percentage of suc-
cessfully completed games. The other measures
are reported as the mean number of instruc-
tions/steps/actions/seconds, respectively. Letters
group indistinguishable systems; systems that
don?t share a letter were found to be significantly
different with p < 0.05.
169
To ensure comparability, we only counted success-
fully completed games for all these measures, and
only started counting when the user left the tutorial
room. Crucially, all objective measures were col-
lected completely unobtrusively, without requiring
any action on the user?s part.
Fig. 8 shows the results of these objective mea-
sures. This figure assigns systems to groups A,
B, etc. for each evaluation measure. Systems in
group A are better than systems in group B, etc.;
if two systems don?t share the same letter, the dif-
ference between these two systems is significant
with p < 0.05. Significance was tested using a
?2-test for task success and ANOVAs for instruc-
tions, steps, actions, and seconds. These were fol-
lowed by post-hoc tests (pairwise ?2 and Tukey)
to compare the NLG systems pairwise.
Overall, there is a top group consisting of
the Austin, Madrid, and Union systems: While
Madrid and Union outperform Austin on task suc-
cess (with 70 to 80% of successfully completed
games, depending on the world), Austin signifi-
cantly outperforms all other systems in terms of
task completion time. As expected, the Warm-
Cold system performs significantly worse than all
others in almost all categories. This confirms the
ability of the GIVE evaluation method to distin-
guish between systems of very different qualities.
4.3 Subjective measures
The subjective measures, which were obtained by
asking the users to fill in a questionnaire after each
game, are shown in Fig. 9. Most of the questions
were answered on 5-point Likert scales (?overall?
on a 7-point scale); the ?informativity? and ?tim-
ing? questions had nominal answers. For each
question, the user could choose not to answer.
The results of the subjective measurements are
summarized in Fig. 10, in the same format as
above. We ran ?2-tests for the nominal variables
informativity and timing, and ANOVAs for the
scale data. Again, we used post-hoc pairwise ?2-
and Tukey-tests to compare the NLG systems to
each other one by one.
Here there are fewer significant differences be-
tween different groups than for the objective mea-
sures: For the ?play again? category, there is
no significant difference at all. Nevertheless,
?Austin? is shown to be particularly good at navi-
gation instructions and timing, whereas ?Madrid?
outperforms the rest of the field in ?informativ-
7-point scale items:
overall: What is your overall evaluation of the quality of the
direction-giving system? (very bad 1 . . . 7 very good)
5-point scale items:
task difficulty: How easy or difficult was the task for you to
solve? (very difficult 1 2 3 4 5 very easy)
goal clarity: How easy was it to understand what you were
supposed to do? (very difficult 1 2 3 4 5 very easy)
play again: Would you want to play this game again? (no
way! 1 2 3 4 5 yes please!)
instruction clarity: How clear were the directions? (totally
unclear 1 2 3 4 5 very clear)
instruction helpfulness: How effective were the directions at
helping you complete the task? (not effective 1 2 3 4 5
very effective)
choice of words: How easy to understand was the system?s
choice of wording in its directions to you? (totally un-
clear 1 2 3 4 5 very clear)
referring expressions: How easy was it to pick out which ob-
ject in the world the system was referring to? (very hard
1 2 3 4 5 very easy)
navigation instructions: How easy was it to navigate to a par-
ticular spot, based on the system?s directions? (very
hard 1 2 3 4 5 very easy)
friendliness: How would you rate the friendliness of the sys-
tem? (very unfriendly 1 2 3 4 5 very friendly)
Nominal items:
informativity: Did you feel the amount of information you
were given was: too little / just right / too much
timing: Did the directions come ... too early / just at the right
time / too late
Figure 9: Questionnaire items
ity?. In the overall subjective evaluation, the ear-
lier top group of Austin, Madrid, and Union is
confirmed, although the difference between Union
and Twente is not significant. However, ?Warm-
Cold? again performs significantly worse than all
other systems in most measures. Furthermore, al-
though most systems perform similarly on ?infor-
mativity? and ?timing? in terms of the number of
users who judged them as ?just right?, there are
differences in the tendencies: Twente and Union
tend to be overinformative, whereas Austin and
Warm-Cold tend to be underinformative; Twente
and Union tend to give their instructions too late,
whereas Madrid and Warm-Cold tend to give them
too early.
170
A
us
ti
n
M
ad
ri
d
Tw
en
te
U
ni
on
W
ar
m
-C
ol
d
task
difficulty
4.3 4.3 4.0 4.3 3.5
A A A A
B
goal clarity
4.0 3.7 3.9 3.7 3.3
A A A A
B
play again
2.8 2.6 2.4 2.9 2.5
A A A A A
instruction
clarity
4.0 3.6 3.8 3.6 3.0
A A A
B B B
C
instruction
helpfulness
3.8 3.9 3.6 3.7 2.9
A A A A
B
informativity
46% 68% 51% 56% 51%
A
B B B B
overall
4.9 4.9 4.3 4.6 3.6
A A A
B B
C
choice of
words
4.2 3.8 4.1 3.7 3.5
A A
B B
C C C
referring
expressions
3.4 3.9 3.7 3.7 3.5
A A A
B B B B
navigation
instructions
4.6 4.0 4.0 3.7 3.2
A
B B B
C
timing
78% 62% 60% 62% 49%
A
B B B
C C
friendliness
3.4 3.8 3.1 3.6 3.1
A A A
B B B
Figure 10: Subjective measures by system. Infor-
mativity and timing are reported as the percentage
of successfully completed games. The other mea-
sures are reported as the mean rating received by
the players. Letters group indistinguishable sys-
tems; systems that don?t share a letter were found
to be significantly different with p < 0.05.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, there may be other factors which also influ-
ence the outcome of our objective and subjective
measures. We tested the following five factors:
evaluation world, gender, age, computer expertise,
and English proficiency (as reported by the users
on the questionnaire). We found that there is a sig-
nificant difference in task success rate for different
evaluation worlds and between users with different
levels of English proficiency.
The interaction graphs in Figs. 11 and 12 also
suggest that the NLG systems differ in their ro-
bustness with respect to these factors. ?2-tests
that compare the success rate of each system in
the three evaluation worlds show that while the
instructions of Union and Madrid seem to work
equally well in all three worlds, the performance
of the other three systems differs dramatically be-
tween the different worlds. Especially World 2
was challenging for some systems as it required
relational object descriptions, such as the blue but-
ton on the left of another blue button.
The players? English skills also affected the sys-
tems in different ways. While Austin, Madrid and
Warm Cold don?t manage to lead players with only
basic English skills to success as often as other
players, Union?s and Twente?s success rates do not
depend on the players? English skills (?2-tests do
not find significant differences in success rate be-
tween players with different levels of English pro-
ficiency for these two systems). However, if we
remove the players with the lowest level of En-
glish proficiency, language skills do not have an
effect on the task success rate anymore for any of
the systems.
5 Conclusion
In this document, we have described the first in-
stallment of the GIVE Challenge, our experimen-
tal methodology, and the results. Altogether, we
collected 1143 valid games for five NLG systems
over a period of three months. Given that this was
the first time we organized the challenge, that it
was meant as a pilot experiment from the begin-
ning, and that the number of games was sufficient
to get significant differences between systems on
a number of measures, we feel that GIVE-1 was a
success. We are in the process of preparing sev-
eral diagnostic utilities, such as heat maps and a
tool that lets the system developer replay an indi-
171
Figure 11: Effect of the evaluation worlds on the
success rate of the NLG systems.
vidual game, which will help the participants gain
further insight into their NLG systems.
Nevertheless, there are a number of improve-
ments we will make to GIVE for future install-
ments. For one thing, the timing of the challenge
was not optimal: A number of colleagues would
have been interested in participating, but the call
for participation came too late for them to acquire
funding or interest students in time for summer
projects or MSc theses. Secondly, although the
software performed very well in handling thou-
sands of user connections, there were still game-
invalidating issues with the 3D graphics and the
networking code that were individually rare, but
probably cost us several hundred games. These
should be fixed for GIVE-2. At the same time,
we are investigating ways in which the networking
and matchmaking core of GIVE can be factored
out into a separate, challenge-independent system
on which other Internet-based challenges can be
built. Among other things, it would be straightfor-
ward to use the GIVE platform to connect two hu-
man users and observe their dialogue while solv-
ing a problem. Judicious variation of parameters
(such as the familiarity of users or the visibility of
an instruction giving avatar) would allow the con-
struction of new dialogue corpora along such lines.
Finally, GIVE-1 focused on the generation of
navigation instructions and referring expressions,
in a relatively simple world, without giving the
Figure 12: Effect of the players? English skills on
the success rate of the NLG systems.
user a chance to talk back. The high success rate
of some systems in this challenge suggests that
we need to widen the focus for a future GIVE-
2 ? by allowing dialogue, by making the world
more complex (e.g., allowing continuous rather
than discrete movements and turns), by making the
communication multi-modal, etc. Such extensions
would require only rather limited changes to the
GIVE software infrastructure. We plan to come to
a decision about such future directions for GIVE
soon, and are looking forward to many fruitful dis-
cussions about this at ENLG.
Acknowledgments. We are grateful to the par-
ticipants of the 2007 NSF/SIGGEN Workshop on
Shared Tasks and Evaluation in NLG and many
other colleagues for fruitful discussions while we
were designing the GIVE Challenge, and to the
organizers of Generation Challenges 2009 and
ENLG 2009 for their support and the opportunity
to present the results at ENLG. We also thank the
four participating research teams for their contri-
butions and their patience while we were working
out bugs in the GIVE software. The creation of
the GIVE infrastructure was supported in part by
a Small Projects grant from the University of Ed-
inburgh.
172
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. Stoia, D. M. Shockley, D. K. Byron, and E. Fosler-
Lussier. 2006. Noun phrase generation for situated
dialogs. In Proceedings of INLG, Sydney.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
173
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 45?53,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Designing a Citation-Sensitive Research Tool:
An Initial Study of Browsing-Specific Information Needs
Stephen Wan?, Ce?cile Paris?,
? ICT Centre,
CSIRO, Australia
Firstname.Lastname@csiro.au
Michael Muthukrishna?, Robert Dale?
?Centre for Language Technology
Faculty of Science
Macquarie University, Australia
rdale@science.mq.edu.au
Abstract
Practitioners and researchers need to stay
up-to-date with the latest advances in
their fields, but the constant growth in
the amount of literature available makes
this task increasingly difficult. We in-
vestigated the literature browsing task via
a user requirements analysis, and identi-
fied the information needs that biomed-
ical researchers commonly encounter in
this application scenario. Our analysis re-
veals that a number of literature-based re-
search tasks are preformed which can be
served by both generic and contextually
tailored preview summaries. Based on this
study, we describe the design of an im-
plemented literature browsing support tool
which helps readers of scientific literature
decide whether or not to pursue and read a
cited document. We present findings from
a preliminary user evaluation, suggesting
that our prototype helps users make rele-
vance judgements about cited documents.
1 Introduction
Practitioners and researchers in all fields face
a great challenge in attempting to keep up-to-
date with the literature relevant to their work.
In this context, search engines provide a useful
tool for information discovery; but search is just
one modality for gathering information. We also
regularly read through documents and expect to
find additional relevant information in referenced
(cited or hyperlinked) documents. This results in
a browsing-based activity, where we explore con-
nections through related documents.
This browsing behaviour is increasingly sup-
ported today as publishers of scientific material
deliver hyperlinked documents via a variety of
media including Adobe?s Portable Document For-
mat (PDF) as well as the more conventional web
hypertext format. Given appropriate document
databases and knowledge of referencing conven-
tions, it is relatively straightforward to support
the automatic downloading of cited documents:
such functionality already exists within reference
managers such as JabRef 1 and Sente2. This
?blind downloading?, however, does not address
the question of the relevancy of the linked docu-
ment for the reader at the time of reading. Apart
from the publication details of the reference and
the citation context, readers are provided with very
little information on the basis of which to de-
termine whether the cited document is worth ex-
ploring more thoroughly. Given the potentially
large number of citations that may be encountered,
this results in the following browsing-specific sce-
nario: how can we help a user quickly determine
whether the cited document is indeed worth down-
loading, perhaps paying for, and reading?
In the study presented here, we focussed on the
needs of biomedical researchers, who are often
time-poor and yet apparently spend 18% of their
time gathering and reviewing information (Hersh,
2008). They regularly search through reposito-
ries of online scholarly literature to update their
expert knowledge; in this domain, the penalty for
not staying up-to-date with the latest advances can
be severe, potentially affecting medical experi-
ments. In our work, we found that two thirds of re-
searchers regularly engaged in browsing scientific
literature. Given the prevalent use of the browsing
modality, we believe that novel research tools are
needed to help readers make decisions about the
relevance of cited material.
To better understand the user?s information
needs that arise when reading and browsing
through academic literature, and to ascertain what
NLP techniques we might be able to use to
help support them, we conducted a user require-
1jabref.sourceforge.net
2www.thirdstreetsoftware.com
45
ments analysis. It revealed a number of common
problems faced by readers of scientific literature.
These served to focus our efforts in designing and
implementing a browsing support tool for scien-
tific literature, referred to here as CSIBS.
CSIBS helps readers decide which cited docu-
ments to read by providing them with information
which is useful at the point when citations are en-
countered. The application provides information
about the cited document and identifies important
sentences in that document, based on the user?s
current reading context. The key observation here
is that the reading context can indicate why the
reader might be interested in the cited document.
In addition to meta-data about the cited document,
and its abstract, a contextualised preview is shown
within the same browser in which the citing docu-
ment is being viewed (for example, Adobe Acro-
bat Reader or a web browser), thus avoiding an
interruption to the user?s primary reading activ-
ity. This contextualised preview contains impor-
tant sentences from the cited document that are re-
lated to the reading context.
We present related work on understanding in-
formation needs in Section 2; we outline our user
requirements analysis in the domain of scientific
literature in Section 3; and the results of the analy-
sis and our understanding of the browsing-specific
information needs are presented in Section 4. In
Section 5, we describe a tool developed to meet
the most pressing of these information needs. Sec-
tion 6 presents a feedback from an initial evalua-
tion. We conclude by discussing our overall find-
ings in Section 7.
2 Related Work
2.1 Information Needs
Existing work on information needs, beginning
with Taylor (1962), typically focuses on mapping
from a particular query to the underlying inter-
est of the user. In a recent example of such
work, Henrich and Luedecke (2007) describes
methods for constructing lists of domain-specific
key words which may correspond well to user
interests. However, we are interested in relat-
ing information needs to user tasks in scenarios
in which there is no explicit query, as in Bystrm
et al (1995); in particular, our work focuses on
browsing scenarios. Toms (2000) presents a study
of browsing behaviour over electronic texts and
examines the differences between searching and
browsing. In that work, browsing is performed
across multiple news articles where the links be-
tween articles are inferred based on topic simi-
larity. In contrast, we consider explicit hyper-
text links which are linguistically embedded in the
document as citations, where the embedding text
serves as link anchors.
2.2 Information Needs in Biomedicine
Ely et al (2000) present an overview of the infor-
mation needs of practicing clinicians, deriving a
set of commonly asked questions. Although we
are interested in doctors as users, the type of in-
formation needs presented in this paper relate to
the activity of conducting scientific investigation,
rather than that of treating a patient.
Task-based analyses of the biomedical domain
have been studied by Bartlett and Neugebauer
(2008) and Tran et al (2004). Their analyses, like
ours, are task-based and use qualitative studies to
uncover the underlying uses of information. How-
ever, the tasks outlined in these related works are
focused on a specific set of information needs in a
research area: for example, the determination of a
functional analysis of gene sequences. Our work
differs in that we wish to take a more general view
in order to elicit information needs to do with sci-
entific research, at least at the level of biomedical
sciences.
The information needs and tasks of academic
users have been studied previously by Belkin
(1994), who focuses on scholarly publications in
the humanities domain. We perform an investi-
gation along similar lines, but with a focus on
academic literature used to conduct scientific re-
search.
2.3 Using Scientific Literature
The genre of academic literature, and the devel-
opment of technologies to support researchers as
users, has been studied by several groups work-
ing in automatic text summarisation. Teufel and
Moens (2002) describe a summarisation approach
that extracts text from documents and highlights
the rhetorical role that an extract plays within
the originating document (for example, stating the
Aim of an experiment). Qazvinian and Radev
(2008) present an approach to summarising aca-
demic documents based on finding citation con-
texts in the entire set of published literature for the
document in question. Both approaches, however,
treat the cited document in isolation of the read-
46
ing context and do not actively support the reading
task.
3 Understanding How Researchers
Browse through Scientific Literature
To determine what readers of scientific literature
want to know about cited documents, we con-
ducted a user requirements analysis. Our method
is based on Grounded Theory (Glaser and Strauss,
1967), a commonly used approach in Human
Computer Interaction (Corbin and Strauss, 2008).
We began by interviewing subjects from an appro-
priate user demographic and recording their verbal
descriptions about a real scenario situated in their
day-to-day activities. Following this, we designed
a questionnaire for wider participation which pre-
sented scenario-based questions attempting to un-
cover their information needs and tasks. Partic-
ipants were asked to provide free text answers.
The responses were then collated and analysed for
commonalities, bringing to the fore those issues
that were salient across the participants. We report
on the questionnaire design and responses in this
paper.
Beginning with such a study can reduce the
risk of building tools that have only limited util-
ity. This is particularly true of new and less un-
derstood application scenarios, such as the one ex-
plored here.
3.1 Questionnaire Design
An online questionnaire was used to reach par-
ticipants who actively read academic literature.3
To encourage participation, the questionnaire was
limited to 10 questions, which were formulated in-
dependently of any particular scientific domain.
We were explicit about the aims of the question-
naire by providing an initial brief, stating that the
feedback from participants would be used to de-
velop new tools for browsing through scientific lit-
erature. Within the questionnaire, to prepare par-
ticipants for our scenario-based questions, the first
few questions were basic and concerned the gen-
eral usage of scientific literature. For example,
we asked about the high-level reasons for which
they used scientific literature (e.g., ?To learn about
a new topic?; ?To update your knowledge on a
particular topic?). Participants could also specify
3The online questionnaire tool, SurveyMonkey
(www.surveymonkey.com), was used to implement
the questionnaire as an online interactive form.
their own reasons. In addition, we also asked them
about the frequency of their literature browsing ac-
tivity.
The main section of the questionnaire consisted
of a series of questions, corresponding to the is-
sues we wanted to explore:
1. What information needs do researchers have
of a cited document, and what specific tasks
does this information serve?
2. What makes it difficult for researchers to find
the answers to their questions about cited
documents?
3. What tasks are potential targets for automa-
tion?
Questions were to be answered with free text
responses, focussed by presenting a scenario in
which the researcher encounters a citation whilst
reading a scientific publication. The first question
above aims to better understand the researchers?
information needs and tasks; the second and third
are concerned with ideas for potential applications
which could benefit from NLP and IR research.
To address the first research issue, participants
were asked to recall a recent experience in which,
while reading a publication, they had encountered
a citation. Within this context, participants were
asked to describe what questions they may have
had of the cited document. To clarify how these
questions relate to a specific context of use, re-
spondents were then asked to relate the questions
they identified back to some task undertaken as
part of their research work.
Responses regarding the difficulties encoun-
tered in satisfying information needs were col-
lected with respect to the participants earlier re-
sponses. So as to not bias the participant, the
question was phrased neutrally. We asked what as-
pects of scientific literature and current technology
made it easy or hard to find answers to the partic-
ipants? personal research questions. We examined
responses with the aim of determining how tech-
nology might reduce the burden of knowledge dis-
covery. Responses were again focused by using
the same scenario as in the previous question.
The third research issue was explored via two
separate questions. The first presented the partici-
pants with a scenario in which they had access to
a non-expert human assistant who could perform
one or more simple tasks identified in their ear-
lier responses; they were then asked what kinds
47
of tasks they would delegate to such an assistant.
A second, more direct, question was presented re-
quiring participants to describe which tools they
would like to use, or to suggest new tools that
would help them in the future, when it came to
browsing through scientific literature.
Finally, optional questions about the partici-
pants? research backgrounds were presented at the
end of the questionnaire. These were deliberately
placed last to reduce barriers to completion.
4 Questionnaire Data Analysis
4.1 Analysing the Results
We recruited users with a background in biomed-
ical life sciences since we had access to an ex-
tensive corpus of documents in this domain with
which to build some kind of application. Note,
however, that our questions were not specific to
this domain, and the questionnaire could poten-
tially be re-run with participants from a different
scientific background.
We contacted 36 users who might be interested
in life sciences publications. Of these, 24 partici-
pants started the questionnaire, and 18 completed
it. Of the 24 participants, two thirds indicated that
they browsed through academic literature at least
once a week.
The written responses were separately analysed
by three of the authors. Responses to each ques-
tion were examined, checking for repeated terms
and concepts that could form the basis of clus-
tering. Salient information needs were matched
to corresponding tasks, and commonly mentioned
areas of difficulty and suggestions for delega-
tion were grouped. Once each author had per-
formed his or her own analysis, the salient group-
ings for each question were collaboratively deter-
mined, consolidating the three analyses performed
in isolation. The most salient groupings were then
examined for potential tasks that might be auto-
mated.
4.2 Questionnaire Data
We now present the results of the analysis. These
are organised with respect to each of the three re-
search issues.
4.2.1 Questions of the Cited Document
Figure 1 presents the most frequently indicated in-
formation needs and the most frequent tasks that
were identified. The information needs can be
Information Needs Freq
[md] About accessing the full text 9
[co] Article details (Definition, Methods, Results) 7
[md] About the authors 6
[md] About the publication date 5
[co] About relevance to own work 4
[md] The abstract 3
[co] The references 3
Participant Task Freq
Deciding whether to believe the citation 4
Finding baselines for experiments 3
Comparing own ideas to article 3
Finding information to justify the citation 3
Finding information about methods 2
Finding additional references 2
Updating clinical knowledge 2
Conducting a survey of the literature 2
Identifying key researchers in the field 2
Updating research knowledge 2
Figure 1: Principal information needs and tasks of
participants with regard to citations. In the first
table, information needs are prefixed by ?md? for
meta-data and ?co? for content-oriented. ?Freq? in-
dicates the number of occurrences in the results.
grouped into two main categories. The first, which
we refer to as meta-data needs, refers to informa-
tion about the document external to the document
content itself. These needs could be met by a se-
ries of database queries about the document, in-
volving, for example, the author information and
the citation counts for the document. We note
that, often, the abstract can also be retrieved via
a database query (and thus does not require any
in-depth text analysis of the cited document), al-
though technically this is not meta-data. In terms
of the underlying task, this kind of generic infor-
mation may be used in deciding whether to trust
the cited source.
The second category of information needs,
which we refer to as being content-oriented, can
be met by providing information sourced from
within the cited document. This type of informa-
tion facilitates multiple tasks. For example, these
might include understanding why a document was
cited, or finding new baselines to design new ex-
periments. We refer to these tasks in general as
citation-focused, as some underlying information
need is triggered by the text that the participant has
just read, whether this is for advancing one?s un-
derstanding of a topic, or pursuing a specific line
of scientific inquiry.
48
4.2.2 Difficulties in Finding Answers
This question required participants to voluntarily
reflect on their own research practices, a process
that is influenced partially by their expertise in
research and their exposure to different research
tools. Some responses described features of soft-
ware that were appealing, while others related to
the difficulties faced by researchers in finding rel-
evant information. In this paper, we present only
the subset of responses that concern the difficulties
encountered, since this will influence the function-
ality of new research tools. These responses are
presented in Figure 2.
Difficulties Freq
Finding the exact text to justify the citation 3
Poor writing 2
Comparing documents 1
Resolving references to the same object 1
Figure 2: Difficulties in finding information.
In general, the difficulties concerned some kind
of analysis of text. We note that these tasks
are largely citation-focused, requiring content-
oriented information. Examples of comments re-
garding this task are presented in Figure 3. For ex-
ample, participants wanted to know how the cited
document compared the citing document from the
perspective of experimental design. However, the
citation-focused task that was most commonly
mentioned as difficult was that of justifying cita-
tions. Participants mentioned that reading through
the entire cited document for this purpose was a
tedious task, particularly when looking for infor-
mation in poorly written documents.
4.2.3 Tasks for Automation
Our analysis of responses to the task automation
questions revealed two interesting outcomes: del-
egation occurred often with the use of key words,
and participants expressed the need for tools to
express relationships between domain concepts.
These are presented in Figure 4.
Responses to the question regarding task del-
egation revealed that for research-oriented tasks,
participants felt the need to direct assistants
through the use of key words. This is consistent
to responses to earlier questions detailing what
aspects of current technology were attractive, in-
cluding user interface conventions such as key
word highlighting. Otherwise, the other reported
Citation usually does not include the position of the informa-
tion in the cited article . . . it might be necessary to read all of
the article to find it in another reference and so on.
If the first report was only citing the second report for a small
piece of information, that information may be hard to locate
in the second report.
The original reference may have just cited a very small com-
ponent of the second report, either just a comment made in
the discussion or a supplemental figure . . . It may take a while
to locate and justify the citation if it isn?t the major finding of
the report.
If I see a citation in a report that I am interested in, I gen-
erally want to know if the cited report actually supports the
statement in the original report. Very often ? way too often ?
citations do not. For all important citations I track down the
original cited work and verify that it actually says what it is
supposed to.
Figure 3: Some sample responses from users with
regard to justifying citations; emphases added.
Automation Possibilities Freq
Search cited document for key words 4
Search for further publications using key words 3
Refine search using related concepts 6
Figure 4: Potential candidates for a new research
tool.
delegated task was that of simple database entry of
publication records. We interpret these responses
as indicating that participants are not overly will-
ing to hand over responsibility for complex tasks
to assistants. If delegation of more research-
oriented activities occurs, participants want to
understand how and why results were obtained.
While responses were made assuming delegation
to human assistants, we believe that such issues
are even more crucial for results obtained via au-
tomated means.
Suggested novel features centered upon a bet-
ter representation of relationships between do-
main concepts to be used for query refinement.
Responses included expressions such as ?refined
search?, a handling of user-specified ?mind maps?
(for repeated searches), and the use of ?trails? ex-
plaining how results connected to search terms,
key words and the author.
5 Prototype Requirements
As a result of these findings, we chose to build a
tool that meets the two types of information needs
revealed in the initial user requirements study. The
49
purpose of the resulting tool, CSIBS, is to help
readers prioritise which cited documents are worth
spending time to download and read further. In
this way, CSIBS helps readers to browse and nav-
igate through a dense network of cited documents.
To facilitate this task in accordance with the
elicited user requirements, CSIBS produces an
alternate version of a published article that has
been prepared with pop-up previews of cited doc-
uments. Each preview contains meta-data, the ab-
stract and content-oriented information. It is pro-
vided to the user to help perform research tasks
that arise as a consequence of encountering a cita-
tion and needing to investigate further. The pre-
view is not intended to serve as a surrogate for
the cited document. Rather, it is aimed at help-
ing readers make relevance judgements about ci-
tations.
The meta-data helps the user to appraise the ci-
tation and to make a value judgement about the
work cited. The abstract provides a generic sum-
mary of the cited document, indicating the scope
of the work cited. The content-oriented informa-
tion supports any citation-focused tasks, for exam-
ple citation justification, through the provision of
detailed information sourced from within the cited
document. We refer to this as a Contextualised
Preview. It is constructed using automatic text
summarisation techniques that tailor the resulting
summary to the user?s current interests, here ap-
proximately represented by the citation context:
that is, the sentence in which the citation is lin-
guistically embedded. We briefly describe CSIBS,
in this section; for a full description, see Wan et al
(2009).
Each preview appears in a pop-up text box ac-
tivated by moving the mouse over the citation.
The specific interaction (a double click versus a
?mouse-over?) depends on whether the article is
displayed via a web browser or as a PDF docu-
ment. Figure 5 shows the resulting pop-up for the
PDF display.
5.1 A Meta-Data Summary and Abstract
Participants often wanted a generic summary out-
lining the overall scope and contributions of the
cited work. This is typically available via the ab-
stract. Additionally, CSIBS presents a variety of
meta-data returned from queries to an online pub-
lications database:4
4www.embase.com
? The full reference: This provides readers
with the date of publication and the journal
title, amongst other things.
? Author Information: CSIBS can include data
to help the reader establish a level of trust
in the citation, primarily focusing on infor-
mation about the authors? affiliations and the
number of related citations in the research
area.
? The citation count for the cited document:
Participants indicated that this was useful in
appraising the cited article.
These pieces of information were commonly iden-
tified as useful in helping readers make value
judgements about the cited work. This is perhaps
an artifact of the biomedical domain, where re-
search has a critical nature and concerns health
and medical issues.
5.2 A Contextualised Preview
To generate the contextualised preview of the cited
document, the system finds the set of sentences
that relate to the citation context, employing ap-
proaches for summarising documents that exploit
anchor text (Wan and Paris, 2008). Following
Spark Jones (1998), we specify the purpose of the
contextualised summary along particular dimen-
sions, indicated here in italics:
? The situation is tied to a particular context of
use: an in-browser summary triggered by a
citation and its citing context.
? An audience of expert researchers is as-
sumed.
? The intended usage of the summary is one of
preview. We assume that the reader is making
a relevance judgement as to whether or not to
download (and, if necessary, buy) the cited
document. Specifically, the information pre-
sented should help the reader determine the
level of trust to place in the document, un-
derstand why the article is cited, and decide
whether or not to read it.
? The summary is intended only to provide
a partial coverage of the whole document,
specifically focused on content that directly
relates to the citation context.
? The style of the summary is intended to be
indicative. That is, it should present specific
50
Figure 5: A sample pop-up with an automatically generated summary, triggered by a mouse action over
the citation. Extracted sentences are grouped together by section titles. Words that match with the
citation context are coloured and emboldened.
details to facilitate a relevance judgement, al-
lowing the user to determine if the cited docu-
ment can be used to source more information
on a topic, as opposed to just mentioning it in
passing.
To create the preview summary, the cited docu-
ment is downloaded from a publisher?s database5
in its XML form and then segmented into sec-
tions, paragraphs and sentences. Each sentence in
the cited document is compared with the citation
context in order to find the best justification sen-
tences for that particular citation. Due to the lim-
ited space available in the pop-up, the number of
extracted sentences is capped at a predefined limit,
currently set to four. Using vector space methods
(Salton and McGill, 1983) weighted with term fre-
quency (and omitting stop words), the best match-
ing sentence is defined as the one scoring the high-
est on the cosine similarity metric with the citation
context. The attractiveness of this approach lies
in its simplicity, resulting in a fast computation of
5www.sciencedirect.com
a preview (? 0.03 seconds), making the process
amenable to batch processing of multiple docu-
ments or, in the future, live generation of previews
at runtime. To help with the readability of the re-
sulting preview, the system also extracts structural
information from the cited document. In particu-
lar, for each extracted sentence, the system identi-
fies the section in which it belongs; the extracted
sentences are then grouped by section, and pre-
sented with their section headings, as illustrated in
Figure 5.
CSIBS focuses on returning precise results, so
that the system does not exacerbate any existing
information overload problems by burdening the
reader with poorly matching sentences. To achieve
this, we currently use exact matches to words in
the citation context; in on-going work, we are ex-
ploring methods to relax this constraint without
hurting performance. In line with our user require-
ments analysis, we have designed the tool so that
the user is able to easily see how the summary was
constructed. Matching tokens are highlighted, al-
lowing the reader to understand why specific sen-
51
tences were extracted.
6 Initial Feedback
6.1 Evaluation Overview
We built a prototype version of CSIBS and con-
ducted a preliminary qualitative evaluation. The
goal was to examine how participants would react
to the pop-up previews. The feedback allows us to
further clarify our analysis and subsequent devel-
opment.
We asked participants to view a number of pop-
up previews in order to answer the question: Is
the Citation Justified? This was one of the more
difficult questions that researchers found challeng-
ing when making a relevancy judgement. The ac-
tual judgements are not important in this evalua-
tion. Instead, we gauged the reported utility of the
prototype based on the participants? self-reported
confidence when performing the task. To capture
this information, participants were asked to score
their confidence on a 3-point Likert scale.
Three biomedical researchers, all of whom had
taken part in our original user requirements analy-
sis, participated in the evaluation. Each participant
was shown nine different passages containing a ci-
tation context, each situated in a different FEBS
Letters6 publication (which was also presented in
full to the participants). At each viewing of a ci-
tation context, two supporting texts were provided
with which the participant was asked to answer the
citation justification question. For all participants,
the first supporting text was produced by a base-
line system that simply provided the full reference
of the citation. The second was either the abstract
or the contextualised preview, which in this eval-
uation was limited to three sentences. Meta-data
was not presented for this study as we specifically
wanted feedback on the citation justification task.
The small sample size does not permit hypoth-
esis testing. However, we are encouraged by the
comparable positive gains in self-reported confi-
dence scores (Abstract: +1.2 versus CSIBS: +2.2)
compared to simply showing the full reference.
Since both preview types were positive, we as-
sume that these types of information facilitated the
relevance judgements. Participants also reported
that, for the contextualised preview, 2 out of 3 sen-
tences were found to be useful on average.
6The journal of the Federation of Europeans Biochemical
Societies.
The qualitative feedback also supported CSIBS.
One participant made some particularly interest-
ing observations regarding selected sentences and
the structure of the cited document. Specifically,
useful sentences tended to be located deeper in the
cited document, for example in the methods sec-
tions This participant suggested that, for an expert
user, showing sentences from the earlier sections
of a publication was not useful; for the same rea-
son, the abstract might be too general and not help-
ful in justifying a citation. Finally, this participant
remarked that, in those situations where each doc-
ument downloaded from a proprietary repository
incurs a fee, the citation-sensitive previews would
be very useful in deciding whether to download
the document.
7 Conclusions
In this paper, we presented an analysis of
browsing-specific information needs in the do-
main of scientific literature. In this context, users
have information needs that are not realised as
search queries; rather these remain implicit in the
minds of users as they browse through hyperlinked
documents. Our analysis sheds light on these in-
formation needs, and the tasks being performed in
their pursuit, using a set of scenario-based ques-
tions.
The analysis revealed two tasks often performed
by participants: the appraisal task and the citation-
focused task. CSIBS was designed to support the
underlying needs by providing meta-data informa-
tion, the abstract, and a contextualised preview for
each citation. The user requirement of search re-
finement was not directly addressed in this work,
but could be met by techniques of query refine-
ment in IR, synonym-based expansion in sum-
marisation, and of course, additional user speci-
fied key terms. In future work, we will explore
these possibilities. Our results to date are encour-
aging for the use of NLP techniques to support
readers prioritise which cited documents to read
when browsing through scientific literature.
Acknowledgments
We would like to thank all the participants who
took part in our study. We would also like to thank
Julien Blondeau and Ilya Anisimoff, who helped
to implement the prototype.
52
References
Joan C. Bartlett and Tomasz Neugebauer. 2008. A
task-based information retrieval interface to support
bioinformatics analysis. In IIiX ?08: Proceedings of
the second international symposium on Information
interaction in context, pages 97?101, New York, NY,
USA. ACM.
Nicholas J. Belkin. 1994. Design principles for
electronic textual resources: Investigating users and
uses of scholarly information. In Current Issues in
Computational Linguistics: In Honour of Donald
Walker.Kluwer, pages 1?18. Kluwer.
Katriina Bystrm, Katriina Murtonen, Kalervo Jrvelin,
Kalervo Jrvelin, and Kalervo Jrvelin. 1995. Task
complexity affects information seeking and use.
In Information Processing and Management, pages
191?213.
Juliet Corbin and Anselm L. Strauss. 2008. Basics of
qualitative research : techniques and procedures for
developing grounded theory. Sage, 3rd edition.
John W Ely, Jerome A Osheroff, Paul N Gorman,
Mark H Ebell, M Lee Chambliss, Eric A Pifer, and
P Zoe Stavri. 2000. A taxonomy of generic clini-
cal questions: classification study. British Medical
Journal, 321:429?432.
Barney G. Glaser and Anselm L. Strauss. 1967. The
Discovery of Grounded Theory: Strategies for Qual-
itative Research. Aldine de Gruyter, New York.
Andreas Henrich and Volker Luedecke. 2007. Char-
acteristics of geographic information needs. In GIR
?07: Proceedings of the 4th ACM workshop on Ge-
ographical information retrieval, pages 1?6, New
York, NY, USA. ACM.
W. R. Hersh. 2008. Information Retrieval. Springer.
Information Retrieval for biomedical researchers.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In The 22nd International Conference on
Computational Linguistics (COLING 2008), Mach-
ester, UK, August.
G. Salton and M. J. McGill. 1983. Introduction to
modern information retrieval. McGraw-Hill, New
York.
Karen Spark Jones. 1998. Automatic summarizing:
factors and directions. In I. Mani and M. Maybury,
editors, Advances in Automatic Text Summarisation.
MIT Press, Cambridge MA.
Robert S Taylor. 1962. Process of asking questions.
American Documentation, 13:391?396, October.
Simone Teufel and Marc Moens. 2002. Summa-
rizing scientific articles: experiments with rele-
vance and rhetorical status. Computional Linguis-
tics, 28(4):409?445.
Elaine G. Toms. 2000. Understanding and facilitating
the browsing of electronic text. International Jour-
nal of Human-Computing Studies, 52(3):423?452.
D Tran, C Dubay, P Gorman, and W. Hersh. 2004. Ap-
plying task analysis to describe and facilitate bioin-
formatics tasks. Studies in Health Technology and
Informatics, 107107(Pt 2):818?22.
Stephen Wan and Ce?cile Paris. 2008. In-browser sum-
marisation: Generating elaborative summaries bi-
ased towards the reading context. In The 46th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: Short
Paper, Columbus, Ohio, June.
Stephen Wan, Ce?cile Paris, and Robert Dale. 2009.
Whetting the appetite of scientists: Producing sum-
maries tailored to the citation context. In Proceed-
ings of the Joint Conference on Digital Libraries.
53
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1371?1378,
Beijing, August 2010
Detecting Speech Repairs Incrementally
Using a Noisy Channel Approach
Simon Zwarts, Mark Johnson and Robert Dale
Centre for Language Technology
Macquarie University
{simon.zwarts|mark.johnson|robert.dale}@mq.edu.au
Abstract
Unrehearsed spoken language often
contains disfluencies. In order to cor-
rectly interpret a spoken utterance,
any such disfluencies must be identi-
fied and removed or otherwise dealt
with. Operating on transcripts of
speech which contain disfluencies, our
particular focus here is the identifica-
tion and correction of speech repairs
using a noisy channel model. Our aim
is to develop a high-accuracy mecha-
nism that can identify speech repairs
in an incremental fashion, as the ut-
terance is processed word-by-word.
We also address the issue of the evalu-
ation of such incremental systems. We
propose a novel approach to evalua-
tion, which evaluates performance in
detecting and correcting disfluencies
incrementally, rather than only assess-
ing performance once the processing of
an utterance is complete. This demon-
strates some shortcomings in our ba-
sic incremental model, and so we then
demonstrate a technique that improves
performance on the detection of disflu-
encies as they happen.
1 Introduction
One of the most obvious differences between
written language and spoken language is the
fact that the latter presents itself incremen-
tally over some time period. Most natural lan-
guage processing applications operate on com-
plete sentences; but for real time spontaneous
speech, there are potential benefits to incre-
mentally processing the input so that a system
can stay responsive and interact directly be-
fore a speaker?s utterance is complete. Work
in psycholinguistics supports the view that the
human parsing mechanism works incremen-
tally, with partial semantic interpretations be-
ing produced before the complete utterance
has been heard (Marslen-Wilson, 1973). Our
interest is in developing similarly incremental
processing techniques for natural language in-
terpretation, so that, for example, a speech
recognizer might be able to interject during
a long utterance to object, cut the speaker
short, or correct a mistaken assumption; such
a mechanism is even required for the appro-
priate timing of backchannel signals. Addi-
tionally the incremental nature of the model
allows potential application of this model in
speech recognition models.
Another feature of unrehearsed spoken lan-
guage that has no obvious correlate in written
language is the presence of disfluencies.1 Dis-
fluencies are of different types, ranging from
simple filled pauses (such as um and uh) to
more complicated structures where the se-
quence of words that make up the utterance is
?repaired? while it is being produced. Whereas
simpler disfluencies may be handled by sim-
ply deleting them from the sequence of words
under consideration, the editing terms in a
speech repair are part of the utterance, and
therefore require more sophisticated process-
ing.
There are three innovations in the present
paper. First, we demonstrate that a noisy
channel model of speech repairs can work ac-
curately in an incremental fashion. Second,
we provide an approach to the evaluation of
1Although some disfluencies can be considered
grammatical errors, they are generally quite distinct
in both cause and nature from the kinds of grammat-
ical errors found in written text.
1371
such an incremental model. Third, we tackle
the problem of the early detection of speech
repairs, and demonstrate a technique that de-
creases the latency (as measured in tokens)
involved in spotting that a disfluency has oc-
curred.
The rest of the paper is structured as fol-
lows. Section 2 provides some background
on speech repairs and existing approaches to
handling them, including Johnson and Char-
niak?s (2004) model, which we use as a start-
ing point for our incremental model. Section
3 describes our model in detail, focusing on
the noisy channel model and the incremental
component of this model. Section 4 introduces
some considerations that arise in the develop-
ment of techniques for the evaluation of in-
cremental disfluency detection; we then pro-
vide a quantitative assessment of our perfor-
mance using these techniques. Our evaluation
reveals that our basic incremental model does
not perform very well at detecting disfluencies
close to where they happen, so in Section 5 we
present a novel approach to optimise detection
of these disfluencies as early as possible. Fi-
nally Section 6 concludes and discusses future
work.
2 Speech Repairs
We adopt the terminology and definitions in-
troduced by Shriberg (1994) to discuss disflu-
ency. We are particularly interested in what
are called repairs. These are the hardest
types of disfluency to identify since they are
not marked by a characteristic vocabulary.
Shriberg (1994) identifies and defines three
distinct parts of a repair, referred to as the
reparandum, the interregnum and the re-
pair. Consider the following utterance:
I want a flight
reparandum? ?? ?
to Boston,
uh, I mean
? ?? ?
interregnum
to Denver
? ?? ?
repair
on Friday (1)
The reparandum to Boston is the part of the
utterance that is being edited out; the inter-
regnum uh is a filler, which may not always be
present; and the repair to Denver replaces the
reparandum.
Given an utterance that contains such a re-
pair, we want to be able to correctly detect
the start and end positions of each of these
three components. We can think of each word
in an utterance as belonging to one of four
categories: fluent material, reparandum, in-
terregnum, or repair. We can then assess the
accuracy of techniques that attempt to detect
disfluencies by computing precision and recall
values for the assignment of the correct cate-
gories to each of the words in the utterance,
as compared to the gold standard as indicated
by annotations in the corpus.
An alternative means of evaluation would
be to simply generate a new signal with the
reparandum and filler removed, and compare
this against a ?cleaned-up? version of the ut-
terance; however, Core and Schubert (1999)
argue that, especially in the case of speech
repairs, it is important not to simply throw
away the disfluent elements of an utterance,
since they can carry meaning that needs to
be recovered for proper interpretation of the
utterance. We are therefore interested in the
first instance in a model of speech error detec-
tion, rather than a model of correction.
Johnson and Charniak (2004) describe such
a model, using a noisy-channel based approach
to the detection of the start and end points of
reparanda, interregna and repairs. Since we
use this model as our starting point, we pro-
vide a more detailed explanation in Section 3.
The idea of using a noisy channel model
to identify speech repairs has been explored
for languages other than English. Honal and
Schultz (2003) use such a model, compar-
ing speech disfluency detection in spontaneous
spoken Mandarin against that in English. The
approach performs well in Mandarin, although
better still in English.
Both the models just described operate on
transcripts of completed utterances. Ideally,
however, when we deal with speech we would
like to process the input word by word as it is
received. Being able to do this would enable
tighter integration in both speech recognition
1372
and interpretation, which might in turn im-
prove overall accuracy.
The requirement for incrementality is recog-
nised by Schuler et al (2010), who employ
an incremental Hierarchical Hidden Markov
Model (HHMM) to detect speech disfluen-
cies. The HHMM is trained on manually an-
notated parse trees which are transformed by
a right corner transformation; the HHMM is
then used in an incremental fashion on un-
seen data, growing the parse structure each
time a new token comes in. Special subtrees
in this parse can carry a marker indicating
that the span of the subtree consists of tokens
corresponding to a speech disfluency. Schuler
et al?s approach thus provides scope for de-
tecting disfluencies in an incremental fashion.
However, their reported accuracy scores are
not as good as those of Johnson and Char-
niak (2004): they report an F-score of 0.690
for their HHMM+RCT model, as compared
to 0.797 for Johnson and Charniak?s parser
model.
Our aim in this paper, then, is to investigate
whether it is possible to adapt Johnson and
Charniak?s model to process utterances incre-
mentally, without any loss of accuracy. To
define the incremental component more pre-
cisely, we investigate the possibility of mark-
ing the disfluencies as soon as possible during
the processing of the input. Given two models
that provide comparable accuracy measured
on utterance completion, we would prefer a
model which detects disfluencies earlier.
3 The Model
In this section, we describe Johnson and Char-
niak?s (2004) noisy channel model, and show
how this model can be made incremental.
As a data set to work with, we use the
Switchboard part of the Penn Treebank 3 cor-
pus. The Switchboard corpus is a corpus of
spontaneous conversations between two par-
ties. In Penn Treebank 3, the disfluencies are
manually annotated. Following Johnson and
Charniak (2004), we use all of sections 2 and
3 for training; we use conversations 4[5-9]* for
a held-out training set; and conversations 40*,
41[0-4]* and 415[0-3]* as the held-out test set.
3.1 The Noisy Channel Model
To find the repair disfluencies a noisy channel
model is used. For an observed utterance with
disfluencies, y, we wish to find the most likely
source utterance, x?, where:
x? = argmaxx p(x | y) (2)
= argmaxx p(y | x) p(x)
Here we have a channel model p(y|x) which
generates an utterance y given a source x and
a language model p(x). We assume that x
is a substring of y, i.e., the source utterance
can be obtained by marking words in y as a
disfluency and effectively removing them from
this utterance.
Johnson and Charniak (2004) experiment
with variations on the language model; they
report results for a bigram model, a trigram
model, and a language model using the Char-
niak Parser (Charniak, 2001). Their parser
model outperforms the bigram model by 5%.
The channel model is based on the intuition
that a reparandum and a repair are generally
very alike: a repair is often almost a copy of
the reparandum. In the training data, over
60% of the words in a reparandum are lexically
identical to the words in the repair. Exam-
ple 1 provides an example of this: half of the
repair is lexically identical to the reparandum.
The channel model therefore gives the high-
est probability when the reparandum and re-
pair are lexically equivalent. When the poten-
tial reparandum and potential repair are not
identical, the channel model performs dele-
tion, insertion or substitution. The proba-
bilities for these operations are defined on a
lexical level and are derived from the training
data. This channel model is formalised us-
ing a Synchronous Tree Adjoining Grammar
(STAG) (Shieber and Schabes, 1990), which
matches words from the reparandum to the
repair. The weights for these STAG rules are
learnt from the training text, where reparanda
and repairs are aligned to each other using a
minimum edit-distance string aligner.
1373
For a given utterance, every possible ut-
terance position might be the start of a
reparandum, and every given utterance po-
sition thereafter might be the start of a re-
pair (to limit complexity, a maximum distance
between these two points is imposed). Ev-
ery disfluency in turn can have an arbitrary
length (again up to some maximum to limit
complexity). After every possible disfluency
other new reparanda and repairs might occur;
the model does not attempt to generate cross-
ing or nested disfluencies, although they do
very occasionally occur in practice. To find
the optimal selection for reparanda and re-
pairs, all possibilities are calculated and the
one with the highest probability is selected.
A chart is filled with all the possible start
and end positions of reparanda, interregna
and repairs; each entry consists of a tuple
?rmbegin, irbegin, rrbegin, rrend?, where rm is the
reparandum, ir is the interregnum and rr is
the repair. A Viterbi algorithm is used to find
the optimal path through the utterance, rank-
ing each chart entry using the language model
and channel model. The language model, a
bigram model, can be easily calculated given
the start and end positions of all disfluency
components. The channel model is slightly
more complicated because an optimal align-
ment between reparandum and repair needs
to be calculated. This is done by extending
each partial analysis by adding a word to the
reparandum, the repair or both. The start po-
sition and end position of the reparandum and
repair are given for this particular entry. The
task of the channel model is to calculate the
highest probable alignment between reparan-
dum and repair. This is done by initialising
with an empty reparandum and repair, and
?growing? the analysis one word at a time. Us-
ing a similar approach to that used in calculat-
ing the edit-distance between reparandum and
repair, the reparandum and repair can both be
extended with one of four operations: deletion
(only the reparandum grows), insertion (only
the repair grows), substitution (both grow),
or copy (both grow). When the reparandum
and the repair have their length correspond-
ing to the current entry in the chart, the chan-
nel probability can be calculated. Since there
are multiple alignment possibilities, we use dy-
namic programming to select the most proba-
ble solutions. The probabilities for insertion,
deletion or substitution are estimated from
the training corpus. We use a beam-search
strategy to find the final optimum when com-
bining the channel model and the language
model.
3.2 Incrementality
Taking Johnson and Charniak?s model as a
starting point, we would like to develop an in-
cremental version of that algorithm. We sim-
ulate incrementality by maintaining for each
utterance to be processed an end-of-prefix
boundary; tokens after this boundary are
not available for the model to use. At each
step in our incremental model, we advance this
boundary by one token (the increment), un-
til finally the entire utterance is available. We
make use of the notion of a prefix, which is
a substring of the utterance consisting of all
tokens up to this boundary marker.
Just as in the non-incremental model, we
keep track of all the possible reparanda and re-
pairs in a chart. Every time the end-of-prefix
boundary advances, we update the chart: we
add all possible disfluencies which have the
end position of the repair located one token
before the end-of-prefix boundary, and we add
all possible start points for the reparandum,
interregna and repair, and end points for the
reparandum and interregna, given the order-
ing constraints of these components.
In our basic incremental model, we leave the
remainder of the algorithm untouched. When
the end-of-prefix boundary reaches the end of
the utterance, and thus the entire utterance
is available, this model results in an iden-
tical analysis to that provided by the non-
incremental model, since the chart contains
identical entries, although calculated in a dif-
ferent order. Intuitively, this model should
perform well when the current prefix is very
close to being a complete utterance; and it
should perform less well when a potential dis-
1374
fluency is still under construction, since these
situations are not typically found in the train-
ing data. We will return to this point further
below.
We do not change the training phase of the
model and we assume that the optimal values
found for the non-incremental model are also
optimal for the incremental model, since most
weights which need to be learned are based on
lexical values. Other weights are bigram based
values, and values dealing with unknown to-
kens (i.e., tokens which occur in the test data,
but not in the training data); it is not unrea-
sonable to assume these weights are identical
or very similar in both the incremental and
the non-incremental model.
4 Evaluation Models and Their
Application
As well as evaluating the accuracy of the anal-
ysis returned at the end of the utterance, it
seems reasonable to also evaluate how quickly
and accurately an incremental algorithm de-
tects disfluencies on a word-by-word basis as
the utterance is processed. In this section, we
provide the methodological background to our
approach, and in Section 5.2 we discuss the
performance of our model when evaluated in
this way.
Incremental systems are often judged solely
on the basis of their output when the utter-
ance being processed is completed. Although
this does give an insight into how well a system
performs overall, it does not indicate how well
the incremental aspects of the mechanism per-
form. In this section we present an approach
to the evaluation of a model of speech repair
detection which measures the performance of
the incremental component.
One might calculate the accuracy over all
prefixes using a simple word accuracy score.
However, because each prefix is a superstring
of each previous prefix, such a calculation
would not be fair: tokens that appear in early
in the utterance will be counted more often
than tokens that appear later in the utterance.
In theory, the analysis of the early tokens can
change at each prefix, so arguably it would
make sense to reevaluate the complete analy-
sis so far at every step. In practice, however,
these changes do not happen, and so this mea-
surement would not reflect the performance of
the system correctly.
Our approach is to define a measure of re-
sponsiveness: that is, how soon is a dis-
fluency detected? We propose to measure
responsiveness in two ways. The time-to-
detection score indicates how many tokens
following a disfluency are read before the given
disfluency is marked as one; the delayed ac-
curacy score looks n tokens back from the
boundary of the available utterance and, when
there is a gold standard disfluency-marked to-
ken at that distance, counts how often these
tokens are marked correctly.
We measure the time-to-detection score by
two numbers, corresponding to the number of
tokens from the start of the reparandum and
the number of tokens from the start of the re-
pair. We do this because disfluencies can be of
different lengths. We assume it is unlikely that
a disfluency will be found before the reparan-
dum is completed, since the reparandum it-
self is often fluent. We measure the time-to-
detection by the first time a given disfluency
appears as one.
Since the model is a statistical model, it
is possible that the most probable analysis
marks a given word at position j as a disflu-
ency, while in the next prefix the word in the
same position is now no longer marked as be-
ing disfluent. A prefix later this word might
be marked as disfluent again. This presents
us with a problem. How do we measure when
this word was correctly identified as disfluent:
the first time it was marked as such or the sec-
ond time? Because of the possibility of such
oscillations, we take the first marking of the
disfluency as the measure point. Disfluencies
which are never correctly detected are not part
of the time-to-detection score.
Since the evaluation starts with disfluencies
found by the model, this measurement has
precision-like properties only. Consequently,
there are easy ways to inflate the score arti-
ficially at the cost of recall. We address this
1375
by also calculating the delayed accuracy. This
is calculated at each prefix by looking back n
tokens from the prefix boundary, where n = 0
for the prefix boundary. For each n we cal-
culate the accuracy score at that point over
all prefixes. Each token is only assessed once
given a set value of n, so we do not suffer
from early prefixes being assessed more often.
However, larger values of n do not take all to-
kens into account, since the last y tokens of
an utterance will not play a part in the ac-
curacy when y < n. Since we evaluate given
a gold standard disfluency, this measurement
has recall-like properties.
Together with the final accuracy score over
the entire utterance, the time-to-detection
and delayed accuracy scores provide different
insights and together give a good measure-
ment of the responsiveness and performance
of the model.
Our incremental model has the same fi-
nal accuracy as the original non-incremental
model; this corresponds to an F-score (har-
monic mean) of 0.778 on a word basis.
We found the average time to detection,
measured in tokens for this model to be 8.3
measured from the start of reparandum and
5.1 from the start of repair. There are situ-
ations where disfluencies can be detected be-
fore the end of the repair; by counting from
the start rather than the end of the disfluency
components, we provide a way of scoring in
such cases. To provide a better insight into
what is happening, we also report the average
distance since the start of the reparandum.
We find that the time to detect is larger than
the average repair length; this implies that,
under this particular model, most disfluencies
are only detected after the repair is finished.
In fact the difference is greater than 1, which
means that in most cases it takes one more to-
ken after the repair before the model identifies
the disfluency.
Table 1 shows the delayed accuracy. We can
see that the score first rises quickly after which
the increases become much smaller. As men-
tioned above, a given disfluency detection in
theory might oscillate. In practice, however,
oscillating disfluencies are very rare, possibly
because a bigram model operates on a very lo-
cal level. Given that oscillation is rare, a quick
stabilisation of the score indicates that, when
we correctly detect a disfluency, this happens
rather quickly after the disfluency has com-
pleted, since the accuracy for the large n is
calculated over the same tokens as the accu-
racy for the smaller n (although not in the
same prefix).
5 Disfluencies around Prefix
Boundaries
5.1 Early detection algorithm
Our model uses a language model and a chan-
nel model to locate disfluencies. It calculates
a language model probability for the utterance
with the disfluency taken out, and it calculates
the probability of the disfluency itself with the
STAG channel model.
Consider the following example utterance
fragment where a repair disfluency occurs:
. . . wi
reparandum? ?? ?rni+1 rni+2
repair? ?? ?rri+3 rri+4 wi+5 . . . (3)
Here, the subscripts indicate token position in
sequence; w is a token outside the disfluency;
and rn is a reparandum being repaired by
the repair rr. The language model estimates
the continuation of the utterance without the
disfluency. The model considers whether the
utterance continuation after the disfluency is
probable given the language model; the rel-
evant bigram here is p(rri+3|wi), continuing
with p(rri+4|rri+3). However, under the in-
cremental model, it is possible the utterance
has only been read as far as token i + 3, in
which case the probability p(wi+4|wi+3) is un-
defined.
We would like to address the issue of look-
ing beyond a disfluency under construction.
We assume the issue of not being able to look
for an utterance continuation after the repair
component of the disfluency can be found back
in the incremental model scores. A disfluency
is usually only detected after the disfluency is
completely uttered, and always requires one
1376
n tokens back 1 2 3 4 5 6
accuracy 0.500 0.558 0.631 0.665 0.701 0.714
Table 1: delayed accuracy, n tokens back from the end of prefixes
n tokens back 1 2 3 4 5 6
accuracy 0.578 0.633 0.697 0.725 0.758 0.770
Table 2: delayed accuracy under the updated model
more token in the basic model. In the given
instance this means it is unlikely that we will
detect the disfluency before i + 5.
In order to make our model more respon-
sive, we propose a change which makes it
possible for the model to calculate channel
probabilities and language model probabili-
ties before the repair is completed. Assum-
ing we have not yet reached the end of utter-
ance, we would like to estimate the continua-
tion of the utterance with the relevant bigram
p(rri+4|rri+3). Since rri+4 is not yet avail-
able we cannot calculate this probability. The
correct thing to do is to sum over all possible
continuations, including the end of utterance
token (for the complete utterance, as opposed
to the current prefix). This results in the fol-
lowing bigram estimation:
?
t?vocabulary
p(t|wi) (4)
This estimation is not one we need to derive
from our data set, since p is a true probability.
In this case, the sum over all possible continu-
ations (this might include an end of utterance
marker, in which case the utterance is already
complete) equals 1. We therefore modify the
algorithm so that it takes this into account.
This solves the problem of the language model
assessing the utterance with the disfluency cut
out, when nothing from the utterance contin-
uation after a disfluency is available.
The other issue which needs to be addressed
is the alignment of the reparandum with the
repair when the repair is not yet fully avail-
able. Currently the model is encouraged to
align the individual tokens of the reparandum
with those of the repair. The algorithm has
lower estimations when the reparandum can-
not be fully aligned with the repair because
the reparandum and repair differ considerably
in length.
We note that most disfluencies are very
short: reparanda and repairs are often only
one or two tokens each in length, and the inter-
regnum is often empty. To remove the penalty
for an incomplete repair, we allow the repair to
grow one token beyond the prefix boundary;
given the relative shortness of the disfluencies,
this seems reasonable. Since this token is not
available, we cannot calculate the lexical sub-
stitution value. Instead we define a new opera-
tion in the channel model: in addition to dele-
tion, insertion, copy, and substitution, we add
an additional substitution operation, the in-
cremental completion substitution. This
operation does not compete with the copy op-
eration or the normal substitution operation,
since it is only defined when the last token of
the repair falls at the prefix boundary.
5.2 Results for the Early detection
algorithm
The results of these changes are reflected
in new time-to-detection and delayed accu-
racy scores. Again we calculated the time-
to-detection, and found this to be 7.5 from
the start of reparandum and 4.6 from the
start of repair. Table 2 shows the results un-
der the new early completion model using the
delayed accuracy method. We see that the
updated model has lower time-to-detection
scores (close to a full token earlier); for de-
layed accuracy, we note that the scores sta-
bilise in a similar fashion, but the scores for
the updated model rise slightly more quickly.
1377
6 Conclusions and Future Work
We have demonstrated an incremental model
for finding speech disfluencies in spoken lan-
guage transcripts. When we consider com-
plete utterances, the incremental model pro-
vides identical results to those of a non-
incremental model that delivers state-of-the-
art accuracy in speech repair detection. We
have investigated a number of measures which
allow us to evaluate the model on an incremen-
tal level. Most disfluencies are identified very
quickly, typically one or two tokens after the
disfluency has been completed. We addressed
the problems of the model around the end of
prefix boundaries. These are repairs which are
either still in the process of being uttered or
have just been completed. We have addressed
this issue by making some changes to how the
model deals with prefix boundaries, and we
have shown that this improves the responsive-
ness of the model.
The work reported in this paper uses a n-
gram model as a language model and a STAG
based model for the repair. We would like
to replace the n-gram language model with a
better language model. Previous work (John-
son and Charniak, 2004) has shown that dis-
fluency detection can be improved by replac-
ing the n-gram language model with a statis-
tical parser. Besides a reported 5% accuracy
improvement, this also provides a structural
analysis, something which an n-gram model
does not. We would like to investigate a sim-
ilar extension in our incremental approach,
which will require the integration of an in-
cremental statistical parser with our noisy
channel model. While transcripts of spoken
texts come with manually annotated sentence
boundaries, real time spoken language does
not. The language model in particular takes
these sentence boundaries into account. We
therefore propose to investigate the proper-
ties of this model when sentence boundaries
are removed.
Acknowledgements
This work was supported by the Australian
Research Council as part of the Thinking
Head Project, ARC/NHMRC Special Re-
search Initiative Grant # TS0669874. We
thank the anonymous reviewers for their help-
ful comments.
References
Charniak, Eugene. 2001. Immediate-head pars-
ing for language models. In Proceedings of the
39th Annual Meeting on Association for Com-
putational Linguistics, pages 124?131.
Core, Mark and Lenhart Schubert. 1999. A model
of speech repairs and other disruptions. In
AAAI Fall Symposium on Psychological Mod-
els of Communication in Collaborative Systems,
pages 48?53.
Honal, Matthias and Tanja Schultz. 2003. Correc-
tion of Disfluencies in Spontaneous Speech us-
ing a Noisy-Channel Approach. In Proceedings
of the 8th Eurospeech Conference.
Johnson, Mark and Eugene Charniak. 2004. A
tag-based noisy channel model of speech repairs.
In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics,
pages 33?39.
Marslen-Wilson, W. 1973. Linguistic structure
and speech shadowing at very short latencies.
Nature, 244:522?533.
Schuler, William, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2010. Broad-
Coverage Parsing using Human-Like Mem-
ory Constraints. Computational Linguistics,
36(1):1?30.
Shieber, Stuart M. and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proceed-
ings of the 13th International Conference on
Computational Linguistics, pages 253?258.
Shriberg, Elizabeth. 1994. Preliminaries to a
Theory of Speech Disuencies. Ph.D. thesis, Uni-
versity of California, Berkeley.
1378
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 913?922,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
WikiWars: A New Corpus for Research on Temporal Expressions
Pawe? Mazur1,2
1Institute of Applied Informatics,
Wroc?aw University of Technology
Wyb. Wyspian?skiego 27,
50-370 Wroc?aw, Poland
pawel@mazur.wroclaw.pl
Robert Dale2
2Centre for Language Technology,
Macquarie University,
NSW 2109, Sydney, Australia
Pawel.Mazur@mq.edu.au
Robert.Dale@mq.edu.au
Abstract
The reliable extraction of knowledge from text
requires an appropriate treatment of the time
at which reported events take place. Unfortu-
nately, there are very few annotated data sets
that support the development of techniques for
event time-stamping and tracking the progres-
sion of time through a narrative. In this paper,
we present a new corpus of temporally-rich
documents sourced from English Wikipedia,
which we have annotated with TIMEX2 tags.
The corpus contains around 120000 tokens,
and 2600 TIMEX2 expressions, thus compar-
ing favourably in size to other existing corpora
used in these areas. We describe the prepa-
ration of the corpus, and compare the profile
of the data with other existing temporally an-
notated corpora. We also report the results
obtained when we use DANTE, our temporal
expression tagger, to process this corpus, and
point to where further work is required. The
corpus is publicly available for research pur-
poses.
1 Introduction
The reliable processing of temporal information is
an important step in many NLP applications, such
as information extraction, question answering, and
document summarisation. Consequently, the tasks
of identifying and assigning values to temporal ex-
pressions have recently received significant attention,
resulting in the creation of mature corpus annotation
guidelines (e.g. TIMEX21 and TimeML2), publicly
1See http://fofoca.mitre.org.
2See http://timeml.org.
available annotated corpora (ACE,3 TimeBank4) and
a number of automatic taggers (see, for example,
(Mani and Wilson, 2000; Schilder, 2004; Hacioglu et
al., 2005; Negri and Marseglia, 2005; Saquete, 2005;
Han et al, 2006; Ahn et al, 2007)).
However, existing corpora have their limitations.
In particular, the documents in these corpora tend to
be limited in length and, in consequence, discourse
structure. This impacts on the number, range and
variety of temporal expressions they contain. Ex-
isting research carried out on the interpretation of
temporal expressions, e.g. by (Baldwin, 2002; Ahn
et al, 2005; Mazur and Dale, 2008), suggests that
many temporal expressions in documents, especially
news stories, can be interpreted fairly simply as be-
ing relative to a reference date that is typically the
document creation date. This phenomenon does not
carry over to longer, more narrative-style documents
that describe extended sequences of events, as found,
for example, in biographies or descriptions of pro-
tracted geo-political events. Consequently, existing
corpora are not ideal as development data for systems
intended to work on such historical narrations.
In this paper we introduce a new annotated corpus
of temporal expressions that is intended to address
this shortfall. The corpus, which we call WikiWars,
consists of 22 documents from English Wikipedia
that describe the historical course of wars. Despite
the small number of documents, their length means
that the corpus yields a large number of temporal
expressions, and poses new challenges for tracking
3See corpora LDC2005T07 and LDC2006T06 in the LDC
catalogue (http://www.ldc.upenn.edu).
4See corpus LDC2006T08 in the LDC catalogue.
913
temporal focus through extended texts. The corpus
has been made available for others to use;5 to give
an indication of the difficulty of processing the tem-
poral phenomena in the texts, we also report on the
performance of DANTE, our temporal expression
tagger, on detecting and interpreting the temporal
expressions in the corpus.
The rest of this paper is organised as follows. In
Section 2 we describe related work, focusing on the
TIMEX2 annotation scheme, and existing corpora
that contain annotations of temporal expressions us-
ing this scheme. Section 3 describes the process of
creation of the WikiWars corpus. In Section 4 we
comment on some artefacts of Wikipedia articles that
impact on the annotation process and the use of this
corpus. Then, in Section 5 we analyse the differences
between the WikiWars corpus and the widely-used
ACE corpora. In Section 6 we report on the perfor-
mance of our temporal expression tagger on this data
set. Finally, in Section 7, we conclude.
2 Related Work
At the time of writing, there are two mature, wide-
coverage schemes for the annotation of temporal in-
formation in texts: TIMEX2 (Ferro et al, 2005) and
TimeML (Pustejovsky et al, 2003; Boguraev et al,
2005), which is soon to become an ISO standard
(Pustejovsky et al, 2010).
These schemes were used to annotate corpora that
are often used in research on temporal expression
recognition and normalisation: the series of corpora
used for training and evaluation in the Automatic
Content Extraction (ACE) program6 run in 2004,
2005 and 2007, and the TimeBank Corpus.
The ACE corpora were prepared for the devel-
opment and evaluation of systems participating in
the ACE program. However, the evaluation corpora
have never been publicly released, and thus are cur-
rently, for all practical purposes, unavailable. The
ACE 2004 corpus contains news data only (broad-
cast news, newspaper and newswire), while the ACE
2005 and 2007 corpora contain news (broadcast and
newswire), conversations (broadcast and telephone),
UseNet discussions and web blogs. The 2005 and
2007 ACE corpora are annotated with the latest ver-
5See www.TimexPortal.info/WikiWars.
6See www.itl.nist.gov/iad/mig/tests/ace.
sion of TIMEX2 (2005), while the 2004 corpus is
annotated with the older 2003 version of TIMEX2;
however, the differences are not very significant.
Apart from the unavailability of the evaluation
data, there are two issues with the ACE corpora. One
is that most of the documents are relatively short, so
that the average number of temporal expressions per
document is low (typically between seven and nine
per document, including the document time stamp
as a metadata element). This results in very lim-
ited temporal discourse structure, and relatively few
underspecified and relative temporal expressions. Un-
fortunately, these are the more difficult temporal ex-
pressions to handle, and so the ACE corpora may
not serve as a good baseline for performance more
generally.
A second problem is that the ACE corpora appear
to contain a significant number of errors in the gold
standard annotations, with respect to both the anno-
tated extents and the semantic values assigned, which
do not always follow the TIMEX2 guidelines.
TimeBank v1.2 is a revised and improved version
of TimeBank 1.1 resulting in a number of errors fixed
and inconsistencies removed (see (Boguraev et al,
2007)). Unfortunely, this corpus has the same lim-
itations as the ACE corpora in regard to document
length and complexity of discourse structure. Fur-
ther, TimeBank is annotated with TimeML, a scheme
more complex than TIMEX2 since it also encom-
passes the tagging of events and temporal relations.
However, TIMEX2 is sufficiently sophisticated for
the annotation of most types of temporal expressions,
and our review of the literature reveals that the ma-
jority of existing temporal taggers output TIMEX2
annotations. Since automatic conversion between
TIMEX2 and TimeML annotations is not straightfor-
ward, TimeBank is of limited use for those who work
specifically with TIMEX2.
3 Creating WikiWars
Given the above concerns, we were particularly inter-
ested in developing a corpus that would allow more
rigorous testing of techniques for tracking time across
extended narratives, since these give rise to more
complex temporal phenomena than are found in sim-
pler documents. To avoid copyright issues that might
arise in the development and distribution of such a
914
corpus, we decided to use Wikipedia as a source. Af-
ter considering various types of historical narrative,
we settled on descriptions of the course of wars and
conflicts as being particularly rich in the kinds of
phenomena we wanted to explore.
3.1 Selecting Data
We queried Google with two phrases, ?most famous
wars in history? and ?the biggest wars?, and in each
case chose the top-ranked result. One of the pages
found proposed a list of the 10 most famous wars
in history, and the other listed the names of the 20
biggest wars that happened in the 20th century, mea-
sured in terms of the number of military deaths. We
combined the two lists, eliminated duplicates, and
searched Wikipedia for articles describing these wars.
Wikipedia did not contain an article for one war, and
we considered two articles as inappropriate for our
purposes since they did not describe the course of the
wars, but rather some general information about the
conflicts. This resulted in a final set of 22 articles.
More details of the selection process and the URLs
of the chosen Wikipedia articles are provided in the
documentation distributed with the corpus.
3.2 Text Extraction and Preprocessing
To prepare the corpus, we first manually copied text
from those sections of the webpages that described
the course of the wars. This involved manual re-
moval of picture captions and cross-page links. We
then ran a script over the results of this extraction pro-
cess to convert some Unicode characters into ASCII
(ligatures, spaces, apostrophes, hyphens and other
punctuation marks), and to remove citation links and
a variety of other Wikipedia annotations.
Finally, we converted each of the text files into
an SGML file: each document was wrapped in one
DOC tag, inside which there are DOCID, DOCTYPE
and DATETIME tags. The document time stamp is the
date and time at which we downloaded the page from
Wikipedia to our local repository. The proper content
of the article is wrapped in a TEXT tag. This docu-
ment structure intentionally follows that of the ACE
2005 and 2007 documents, so as to make the pro-
cessing and evaluation of the WikiWars data highly
compatible with the tools used to process the ACE
corpora.
3.3 Creating Gold Standard Annotations
Having prepared the input SGML documents, we
then processed them with the DANTE temporal
expression tagger (see Mazur and Dale (2007)).
DANTE outputs the original SGML documents aug-
mented with an inline TIMEX2 annotation for each
temporal expression found. These output files can
be imported to Callisto,7 an annotation tool that sup-
ports TIMEX2 annotations. Using a temporal ex-
pression tagger as a first-pass annotation tool not
only significantly reduces the amount of human an-
notation effort required (creating a tag from scratch
requires a number of clicks in the annotation tool),
but also helps to minimize the number of errors that
arise from overlooking markable expressions through
?annotator blindness?. The annotations produced by
DANTE were then manually corrected in Callisto
via the following process. First, Annotator 1 (the
first author) corrected all the annotations produced
by DANTE, both in terms of extent and the values
provided for TIMEX2 attributes. This process also
included the annotation of any temporal expression
missed by the automatic tagger, and the removal of
spurious matches. Then, Annotator 2 (the second au-
thor) checked all the revised annotations and prepared
a list of errors found and doubts or queries in regard
to potentially problematic annotations. Annotator 1
then verified and fixed the errors, after discussion in
the case of disagreements.
The final SGML files containing inline annotations
were then transformed into ACE APF XML annota-
tion files, this being the stand-off markup format
developed for ACE evaluations. This transformation
was carried out using the tern2apf tool developed
by NIST for the ACE 2004 evaluations, with some
modifications introduced by us to adjust the tool to
support ACE 2005 documents and to add a document
ID as part of the ID of a TIMEX2 annotation (so that
all annotations would have corpus-wide unique IDs).
The resulting corpus is thus available in two for-
mats: one contains the original documents enriched
with inline annotations, and the other consists of
stand-off annotations in the ACE APF format.
7See http://callisto.mitre.org.
915
3.4 Some Deficiencies of TIMEX2
The annotation process described above revealed
some issues with the use of TIMEX2 in practice.
First, the flexibility of the TIMEX2 scheme, which
can be at first seen as an advantage, actually makes
it ambiguous. One instance of this phenomenon re-
lates to the fact that the TIMEX2 guidelines state that
the provision of some attribute values for what are
called event-based expressions (such as three weeks
after the siege of Boston began or the first year of the
American invasion) is optional. Since our corpus has
a significant number of such expressions, the deci-
sion as to whether or not to provide semantic values
in such cases has a potentially large impact on the
perceived performance of a tagger. In such cases,
we decided only to provide the value when it is very
clear from the article itself what the value should be.
Another area where TIMEX2 is not ideal is in
regard to the annotation of time zones. First, only
whole-hour time differences are supported, which
eliminates some time zones (e.g. Afghanistan lies
in UTC+04:30). Second, time zone information is
supposed to be marked only for expressions which
have it explicitly stated. However, it can often be
inferred from the context that subsequent unadorned
time references should inherit the same time zone as
an earlier time reference.
We also found that, in a not insignificant number
of cases, it is impossible to provide a precise and
correct value for a temporal expression. For example,
the TIMEX2 guidelines stipulate that the anchors
of durations cannot have a MOD attribute, so if the
anchor is mid-August, the value of the anchor must
refer to August, which is not entirely correct as the
semantics of mid- is lost.
TIMEX2 only supports nonspecific expressions
which have explicit information about granularity.
Expressions such as a very short time or a short
period of time therefore cannot be provided with any
value, since the context does not indicate whether the
period involved should be measured in days, weeks,
or months. One might consider using the typical
durations of events of the corresponding types in
such cases, but this solution also has problems (see
(Pan et al, 2006)).
As is acknowledged in the TIMEX2 guidelines,
the treatment of set expressions (i.e. recurring times
and durations and frequencies, e.g. twice a month) is
underdeveloped. One rule states that set expressions
should not be anchored (Ferro et al, 2005, p. 42);
this has the consequence that the full semantics of the
expression annually since 1955 cannot be provided,
and the expression is therefore treated as two separate
expressions, annually and 1955.
Finally, alternative calendars are not supported, so
an expression like February in the pre-revolutionary
Russian calendar cannot receive a value unless it ap-
pears in an appositive construction which provides
an alternative description. Similarly, consider Exam-
ple (1):
(1) On 9 November 1799 (18 Brumaire of the Year VIII)
Napoleon Bonaparte staged the coup of 18 Brumaire
which installed the Consulate.
Here, 18 Brumaire of the Year VIII is a date in an
alternative calendar used in France, but we annotated
only the Year VIII based on the trigger year. Note
that 18 Brumaire also occurs later in the sentence,
but is not annotated.
3.5 Corpus Statistics
The corpus contains 22 documents with a total of
almost 120,000 tokens8 and 2,671 temporal expres-
sions annotated in TIMEX2 format. In Table 1 we
compare the WikiWars corpus with the other exist-
ing corpora. While the ACE 2005 Training corpus
remains the largest corpus, WikiWars is larger than
the ACE 2005 and 2007 evaluation corpora and the
TimeBank v1.2 corpus, both in terms of number of
tokens and TIMEX2 annotations. WikiWars has an
order of magnitude more temporal expressions in
each document, and a slightly higher density of tem-
poral expressions than the other corpora.
Table 2 presents statistics on the individual doc-
uments that make up the corpus. The documents
vary considerably in size, the smallest consisting of
only 1,455 tokens, and the largest being eight times
larger at 11,640 tokens. The density of TIMEX2 an-
notations varies from 1 in 23.1 tokens to 1 in 72.1
tokens, but for the majority of documents the ratio
lies between 30 and 60.
8All token counts presented in Tables 1 and 2 were obtained
using GATE?s default English tokeniser; hyphenated words, e.g.
British-held and co-operation, were treated as single tokens. For
more information on GATE see (Cunningham et al, 2002).
916
Corpus Docs KB Tokens
Temp.
Expr.
Tokens
TIMEX
TIMEX
Doc
ACE05 Train. 599 1,733 318,785 5,469 58.3 9.13
ACE05 Eval. 155 350 63,217 1,154 54.8 7.45
ACE07 Eval. 254 561 104,779 2,028 51.7 7.98
WikiWars 22 631 119,468 2,671 44.7 121.41
TimeBank1.2 183 816 78,444 1,414 55.5 7.73
Table 1: Statistics of the Wikipedia War corpus compared
to those of other corpora.
4 The Nature of Wikipedia Articles
Wikipedia articles may be edited by a large number
of people over a significant number of revisions. We
checked how often the articles constituting WikiWars
were modified in the period from January 2008 to
February 2010. On average, each article was changed
almost 52 times per month, with the monthly number
of changes for a single article ranging from 1 to 372.9
The minimum average for an individual document
was 13.08 (17 AlgerianWar), and the maximum was
171.77 (07 IraqWar).
The nature of the revision process in Wikipedia
leads to some artefacts that may be not typical
of other document sources, such as news, where
the text is usually carefully prepared by its author
and checked by an editor. This is not to say that
Wikipedia content is necessarily of low quality; this
is an encyclopedia with many people and bots con-
trolling its quality, and there exist manuals of style
for authors to help them avoid errors and ambigu-
ity and to ensure maximum consistency.10 However,
given the large number of editors with various de-
grees of fluency and experience in writing and edit-
ing, it would not be surprising if some parts of the
texts are not perfect. In the process of preparing the
gold standard annotations for the WikiWars corpus,
we have made the following observations.
9Note that these numbers are for the articles as a whole,
and not just the sections which we extracted (although these
are usually the major part of the article). Additionally, these
edits include both major changes (e.g. adding a new section),
minor changes (e.g. correcting a grammar error or adding a
comma), vandalism (deletion of the page content or the on-
purpose provision of false information) and restoring the page
after an act of vandalism has been detected.
10See, for example, the manual of style concerning format-
ing dates and numbers, located at http://en.wikipedia.
org/wiki/Wikipedia:DATE.
Document ID Tokens TIMEX2 TokensTIMEX2
01 WW2 5,593 169 33.1
02 WW1 10,370 264 39.3
03 AmCivWar 3,529 75 47.1
04 AmRevWar 5,695 146 39.0
05 VietnamWar 11,640 243 47.9
06 KoreanWar 5,992 147 40.8
07 IraqWar 8,404 247 34.0
08 FrenchRev 9,631 174 55.4
09 GrecoPersian 7,393 129 57.3
10 PunicWars 3,475 57 61.0
11 ChineseCivWar 3,905 103 37.9
12 IranIraq 4,508 98 46.0
13 RussianCivWar 3,924 103 38.1
14 FirstIndochinaWar 3,085 70 44.1
15 MexicanRev 3,910 77 50.8
16 SpanishCivilWar 1,455 63 23.1
17 AlgerianWar 7,716 130 59.4
18 SovietsInAfghanistan 5,306 110 48.2
19 RussoJap 2,760 62 44.5
20 PolishSoviet 5,137 106 48.5
21 NigerianCivilWar 2,091 29 72.1
22 2ndItaloAbyssinianWar 3,949 69 57.2
Total for the whole corpus 119,468 2,671 44.7
Average per document 5,430 121 ?
Standard deviation 2,663 63 ?
Table 2: Statistics of the Wikipedia War corpus.
4.1 Broken Narratives
In some articles we have found situations where a
sentence does not appear to cohere with those on
either side of it. This may be the result of a num-
ber of modifications made by different authors, or
it may be due to a lack of writing skill on the part
of the person who wrote the paragraph in question.
Example (2) below provides an example of this phe-
nomenon: the sentence about de Gaulle being elected
president contains a temporal expression which pro-
gresses the temporal focus in the narrative to 1959,
but the later context of the article strongly suggests
that the subsequent reference to October is in fact
October 1958.
(2) ALN commandos committed numer-
ous acts of sabotage in France in
August[1958], and the FLN mounted a desper-
ate campaign of terror in Algeria to intimidate
Muslims into boycotting the referendum. Despite
threats of reprisal, however, 80 percent of the Muslim
electorate turned out to vote in September[1958], and
of these 96 percent approved the constitution. In
February 1959, de Gaulle was elected president of
the new Fifth Republic. He visited Constantine in
917
October[1958] to announce a program to end the war
and create an Algeria closely linked to France.
It would appear that the reference to February 1959 is
a later addition to the text which has been made with-
out the surrounding text being appropriately revised
to accommodate this change. Clearly such instances
of incoherence will cause problems for any process
that attempts to track the temporal focus.
4.2 Ambiguous Writing
We have also found cases of a lack of precision in
writing, which leads to ambiguous statements. Con-
sider the following example:
(3) The Afghan government, having secured a treaty in
December 1978 that allowed them to call on Soviet
forces, repeatedly requested the introduction of troops
in Afghanistan in the spring and summer of 1979.
They requested Soviet troops to provide security and
to assist in the fight against the mujahideen rebels.
On April 14, 1979, the Afghan government requested
that the USSR send 15 to 20 helicopters with their
crews to Afghanistan, and on June 16, the Soviet gov-
ernment responded and sent a detachment of tanks,
BMPs, and crews to guard the government in Kabul
and to secure the Bagram and Shindand airfields. In
response to this request, an airborne battalion, com-
manded by Lieutenant Colonel A. Lomakin, arrived
at the Bagram Air Base on July 7. [. . . ]
After a month, the Afghan requests were no longer
for individual crews and subunits, but for regiments
and larger units. In July, the Afghan government
requested that two motorized rifle divisions be sent
to Afghanistan. The following day, they requested an
airborne division in addition to the earlier requests.
Here, in the first paragraph there are four temporal
expressions related to the Afghan government asking
for troops and equipment. There is also one date
related to the Soviets? reply to these requests and
sending of tanks, and one date related to the arrival
of an airborne battalion. The second paragraph starts
with after a month; the first possible interpretation is
that this is a month after the 7th July mentioned in
the previous paragraph; i.e. the month would end on
the 6th of August. But the following sentence reveals
that this is not the case, as it mentions some requests
for larger units that were made in July. Usually a
narrative progresses forwards in time, not backwards,
so the month must start either on 14th April or 16th
June: if the second sentence elaborates the first one,
then it is a month from 16th June; if it just mentions
one of the requests for larger units, then it is probably
a month from 14th April.
It is also unclear whether the second paragraph
talks about the same request for airborne forces which
was mentioned in the first paragraph: both these
events are dated July. The phrase In response to
this request is in fact placed very oddly, as its pre-
ceding sentence does not mention any request, but
rather talks about the Soviets? response to requests.
This may suggest that what at first looks just like a
careless and ambiguous use of the expression after a
month is in fact a larger problem of lack of coherency
in these two paragraphs.
4.3 Use of Deictic Expressions
One of the articles, 07 IraqWar, contained a num-
ber of deictic temporal expressions, indicative of the
fact that the events described were happening con-
temporaneously to the time of writing (as is often the
case in news stories); for example:
(4) a. Democrats plan to push legislation this spring
that would force the Iraqi government to spend
its own surplus to rebuild.
b. A protester said that despite the approval of the
Interim Security pact, the Iraqi people would
break it in a referendum next year.
Obviously, after some time these expressions will no
longer make sense, since there is no ?at-the-time-of-
writing? time stamp associated with these sentences:
for the reader of a Wikipedia article, the reference
date is the time of reading. In the case of the above
example, these sentences were written in April and
December 2008, respectively.11 Arguably, these sen-
tences should be corrected, making the temporal ex-
pressions fully-specified (e.g. in spring of 2009 and
in 2009), or context-dependent (e.g. in spring of
that year and the following year) if there is a context
in the article which supports their correct interpreta-
tion. Of course, not only the temporal expressions
need to be revised, but also the tense and aspect of
the verbs used in the sentences. In the gold stan-
dard annotations, however, we provided the values
by interpreting these expressions with respect to the
document time stamp (i.e. 2010-SP and 2010), as
the text itself does not provide any evidence that other
dates were intended.
11Somewhat laborious document archaeology allows this in-
formation to be extracted from Wikipedia?s archive.
918
Pos Count Token class or lexical form
1 4650 NUMBER DIGIT 2
2 1942 :
3 1499 -
4 1329 NUMBER DIGIT 4
5 828 ARTICLE
6 765 TEMPORALUNIT
7 634 TEMPORALUNIT PLURAL
8 555 PREPOSITION
9 528 now
10 411 t
11 403 WEEKDAYNAME
12 335 NUMBER WORD
13 329 MONTHNAME
14 242 MONTHNAME ABBR
15 240 DAYPART
16 233 DEMONSTRATIVE
17 224 ,
Pos Count Token class or lexical form
18 222 today
19 202 NUMBER DIGIT 1
20 191 last
21 171 WEEKDAYNAME ABBR
22 145 NUMBER DIGIT 8
23 113 ago
24 108 former
25 96 time
26 79 right
27 69 new
28 69 future
29 67 gmt
30 65 next
31 63 past
32 61 yesterday
33 59 few
34 50 every
Pos Count Token class or lexical form
35 49 AMPM
36 48 ORDINAL DIGIT
37 48 ?
38 45 recently
39 43 year-old
40 42 later
41 41 tonight
42 39 christmas
43 36 tomorrow
44 36 current
45 35 couple
46 34 recent
47 33 earlier
48 32 and
49 31 early
50 31 DIRECT FREQ
51 31 ?s
Table 3: The most frequent tokens in TEs in the ACE 2005 Training corpus.
Pos Count Token class or lexical form
1 1181 MONTHNAME
2 1157 NUMBER DIGIT 4
3 674 NUMBER DIGIT 2
4 490 ARTICLE
5 288 PREPOSITION
6 221 NUMBER DIGIT 1
7 211 TEMPORALUNIT
8 206 TEMPORALUNIT PLURAL
9 165 ,
10 133 NUMBER WORD
11 99 SEASON
12 98 NUMBER DIGIT 3
13 82 bc
14 76 now
15 70 time
16 67 early
17 63 DEMONSTRATIVE
Pos Count Token class or lexical form
18 59 :
19 51 end
20 49 -
21 47 late
22 37 DAYPART
23 36 later
24 36 former
25 32 next
26 27 same
27 25 period
28 22 t
29 20 mid-
30 18 war
31 18 few
32 14 following
33 14 ORDINAL DIGIT
34 13 s
Pos Count Token class or lexical form
35 13 first
36 11 future
37 11 earlier
38 11 .
39 11 ?s
40 9 previous
41 9 christmas
42 8 last
43 8 AMPM
44 7 battle
45 7 DIRECT FREQ
46 6 short
47 6 several
48 6 season
49 6 recent
50 6 past
51 6 ?
Table 4: The most frequent tokens in TEs in the WikiWars corpus.
4.4 Use of Time Zone Information
Consider the following example, which comes from
the article 01 WW2:
(5) On December 7 (December 8 in Asian time zones),
1941, Japan attacked British and American holdings
with near simultaneous offensives against Southeast
Asia and the Central Pacific.
The italicized temporal expression is difficult to de-
tect, and it is not clear how it should be annotated.
But it is also imprecise with respect to which time
zone is intended: Asia encompasses 10 time zones.
Therefore it is impossible to fully interpret the ex-
pression. Note also that the expression combines a
time zone with a date, rather than with a time. While
uncommon, this is not incorrect; but the TIMEX2
guidelines do not explicitly allow for this circum-
stance.
4.5 Quotes Missing a Time Stamp
Occasionally it happens that an article contains a
quoted utterance, but there is no indication of when
the utterance was made. For example, in the docu-
ment 05 VietnamWar we find the following:
(6) Nixon said in an announcement, ?I am tonight an-
nouncing plans for the withdrawal of an additional
150,000 American troops to be completed during the
919
spring of next year. This will bring a total reduction
of 265,500 men in our armed forces in Vietnam below
the level that existed when we took office 15 months
ago.?
It is impossible to determine what dates are meant
by the three temporal expressions present in the an-
nouncement. In some cases this information may be
provided in citation footnotes, but this is not always
the case; when this is absent, such expressions can
only be annotated at the level of textual extent and a
localised, context-dependent semantics.
5 Comparing WikiWars to the ACE Data
A comparison of WikiWars with the ACE corpora
reveals some interesting differences.
5.1 Vocabulary Differences
First, we found differences on the level of the lexical
triggers that signal the presence of temporal expres-
sions. Because of space limitations, we provide here
only the main findings.
Tables 3 and 4 present the 51 most frequent to-
kens, including punctuation, in the ACE 2005 Train-
ing and WikiWars corpus, respectively. Some to-
kens are combined into what we call trigger classes;
for example, all weekday names belong to the class
WEEKDAYNAME.12
We can see that there are many classes that fall
into the top 51 positions for both corpora, e.g. the
names of temporal units (such as month and year).
But there are also clear differences. Month names
are the most frequent class in WikiWars, while they
are not so frequent in ACE. Similarly, year seasons
ranked very highly in WikiWars, but do not figure
in the rankings shown for ACE. On the other hand,
weekday names are quite frequent in the ACE corpus,
but do not occur in the table for WikiWars. This
suggests that these corpora make different use of
temporal expressions: in WikiWars we find many
references to the more distant past, thus the high use
of month names, but ACE documents tend to discuss
12The entries in the table correspond to the lexical and punctu-
ation clues that drive detection of temporal expressions: the high
rank of colons and dashes comes from their use in document
time stamps, which are considered markable by the TIMEX2
guidelines. The T token is a separator that often occurs in times-
tamps, e.g. 2005-01-25T11:08:00; the question mark appears
very often because some of the ACE timestamps are of the form
????-??-??T19:33:00.
temporally local issues, so they are more likely to
refer to days in the weeks preceding and following
the reference date.
Looking at individual tokens, we can see that de-
ictic expressions such as today, tonight, yesterday
and tomorrow are in the top 51 positions for ACE,
but almost never occur in WikiWars: there are only
three instances of today, two of tomorrow and one
of tonight in the corpus, and all of these appear only
in quoted speech. Similarly, ago occurred 113 times
in ACE, but only twice in WikiWars: once in quoted
speech, and once used incorrectly instead of earlier in
a context-dependent expression. Other tokens which
are frequent in ACE but rare in WikiWars are recent,
recently, current and currently.
5.2 Temporal Discourse Structure
A more interesting property that WikiWars exhibits,
and which is noticeably absent from the simpler ACE
data, is what we might think of as a discourse mech-
anism for resetting the temporal focus. This is a
feature of complex texts in general, rather than some-
thing that is specific to Wikipedia as a source. In
these cases, the discourse does not follow a single
global timeline from the beginning to the end of the
document, but is rather divided into subdiscourses
which describe separate chains of events that often
have common temporal starting points. This is typi-
cal in the description of big, often international, con-
flicts, where one can distinguish several theaters of
the war, i.e. the eastern and western theaters.
In most cases the switch to a different ?part of the
story? can be determined not only by analysing the
events and their geographic locations, but by recog-
nizing that the first date appearing in the new subdis-
course is generally fully specified. This is, however,
not always the case, as shown in the following exam-
ple extracted from the article 01 WW2:
(7) In northern Serbia, the Red Army, with limited sup-
port from Bulgarian forces, assisted the partisans in a
joint liberation of the capital city of Belgrade on Oc-
tober 20[1944]. A few days later, the Soviets launched
a massive assault against German-occupied Hungary
that lasted until the fall of Budapest in February 1945.
[. . . ]
By the start of July[1944], Commonwealth forces in
Southeast Asia had repelled the Japanese sieges in As-
sam, pushing the Japanese back to the Chindwin River
while the Chinese captured Myitkyina. In China, the
Japanese were having greater successes, having fi-
920
nally captured Changsha in mid-June[1944] and the
city of Hengyang by early August[1944]. Soon after,
they [. . . ] by the end of November[1944] and success-
fully linking up their forces in China and Indochina
by the middle of December[1944].
Clearly, quite sophisticated processing is required to
handle this phenomenon adequately.
6 Automated Processing of WikiWars
After we developed the WikiWars corpus, we used it
to evaluate our temporal expression tagger, DANTE,
which had been developed for participation in ACE.
Performance at finding temporal expressions in text is
traditionally reported, for example by (Mani and Wil-
son, 2000; Negri and Marseglia, 2005; Teisse`dre et
al., 2010), in terms of precision, recall and F-measure.
These can, however, be calculated in two ways, le-
nient and strict, corresponding to two tasks: detec-
tion (where a single character overlap between the
gold standard and system annotation counts as a cor-
rect answer) and recognition (where an exact overlap
is required).
Table 5 shows our tagger?s initial performance on
the data. While the lenient F-measure for extent
recognition was comparable to that obtained for the
ACE 2005 Training corpus (0.82 vs 0.78), the recall
was much lower: 0.75 vs 0.87. The difference in
strict results was even larger, where both precision
and recall were lower for WikiWars than for ACE,
resulting in an F-measure of 0.38. When evaluating
also the VAL attribute, the strict F-measure was quite
low for both corpora, but significantly lower for Wiki-
Wars: 0.17 vs 0.33. This illustrates how illusive it
may be to trust the performance of a tagger measured
on a single, possibly biased, data set.
In the light of the results of our comparison in Sec-
tion 5, it is clear that at some of the performance loss
here is simply due to domain differences with respect
to lexical triggers. So, we extended DANTE?s cov-
erage with approximately 20 temporal triggers and
modifiers to include the more common vocabulary
that appeared in the WikiWars data; we also modified
the recognition grammar to reduce the number of
spurious matches and extent errors. These changes
resulted in the improvements shown in Table 6. The
performance on extent recognition improves signif-
icantly for both sets of data, but the gap between
extent recognition and evaluation of the VAL attribute
Lenient Strict
Corpus and Task Prec Rec F Prec Rec F
WW - Extent only 0.90 0.75 0.82 0.42 0.35 0.38
WW - Extent + VAL 0.22 0.18 0.20 0.19 0.16 0.17
ACE - Extent only 0.71 0.87 0.78 0.53 0.65 0.58
ACE - Extent +VAL 0.34 0.42 0.37 0.30 0.36 0.33
Table 5: Initial performance of DANTE on WikiWars and
the ACE 2005 Training corpus.
Lenient Strict
Corpus and Task Prec Rec F Prec Rec F
WW - Extent only 0.98 0.99 0.99 0.95 0.95 0.95
WW - Extent + VAL 0.59 0.60 0.59 0.58 0.59 0.58
ACE - Extent only 0.88 0.93 0.90 0.75 0.79 0.77
ACE - Extent +VAL 0.63 0.67 0.65 0.57 0.60 0.58
Table 6: Current performance of DANTE on WikiWars
and the ACE 2005 Training corpus.
is much larger on WikiWars. This is most likely be-
cause the strategy of using the document time stamp
for the interpretation of context-dependent expres-
sions does not work at all for WikiWars documents,
whereas it works well for ACE documents, in line
with our earlier comments in regard to the genres of
the documents. This emphasises the need to develop
sophisticated methods for temporal focus tracking if
we are to extend current time-stamping technologies
beyond the relatively simplistic temporal structures
found in currently available corpora.
7 Conclusions and Future Work
We have presented a new corpus based on the his-
torical descriptions of 22 wars sourced from En-
glish Wikipedia, and we have described in detail
the methodology adopted to construct the corpus; the
corpus can be easily extended in the same way. We
annotated temporal expressions in these documents
with TIMEX2 tags, which provide both the textual
extents and the semantics of the expressions in the
context of whole article.
Following an analysis of the differences between
our new corpus and existing data sets, we then pre-
sented the results of automatic processing of the cor-
pus. This demonstrates that differences in the vo-
cabulary used for temporal expressions can be fairly
straightforwardly incorporated in a tagging tool, but
that appropriate processing of temporal structure in
complex documents requires more sophisticated tech-
niques than those required to handle existing corpora.
The WikiWars Corpus provides data that tests these
capabilities.
921
References
David Ahn, Sisay Fissaha Adafre, and Maarten de Rijke.
2005. Recognizing and Interpreting Temporal Expres-
sions in Open Domain Texts. In We Will Show Them:
Essays in Honour of Dov Gabbay, Vol 1, pages 31?50,
October.
David Ahn, Joris van Rantwijk, and Maarten de Rijke.
2007. A cascaded machine learning approach to in-
terpreting temporal expressions. In Proceedings of
Human Language Technologies: The Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL-HLT 2007),
Rochester, NY, USA, April.
Jennifer Baldwin. 2002. Learning Temporal Annotation
of French News. Master?s thesis, Dept. of Linguistics,
Georgetown University, April.
Branimir Boguraev, Jose Castan?o, Rob Gaizauskas, Bob
Ingria, Graham Katz, Bob Knippen, Jessica Littman,
Inderjeet Mani, James Pustejovsky, Antonio Sanfil-
ippo, Andrew See, Andrea Setzer, Roser Saur??, Am-
ber Stubbs, Beth Sundheim, Svetlana Symonenko, and
Marc Verhagen. 2005. TimeML 1.2.1 ? A Formal
Specification Language for Events and Temporal Ex-
pressions, October.
Branimir Boguraev, James Pustejovsky, Rie Ando, and
Marc Verhagen. 2007. TimeBank evolution as a com-
munity resource for TimeML parsing. Language Re-
sources and Evaluation, 41(1):91?115, 02.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva,
and Valentin Tablan. 2002. GATE: A framework and
graphical development environment for robust NLP
tools and applications. In Proceedings of the 40th An-
niversary Meeting of the ACL.
Lisa Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. TIDES 2005 Standard for the Annotation
of Temporal Expressions. Technical report, MITRE,
September.
Kadri Hacioglu, Ying Chen, and Benjamin Douglas. 2005.
Automatic time expression labeling for english and
chinese text. In Alexander F. Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing, 6th
International Conference, CICLing?05, Lecture Notes
in Computer Science, pages 548?559, Mexico City,
Mexico, February. Springer.
Benjamin Han, Donna Gates, and Lori Levin. 2006. From
language to time: A temporal expression anchorer. In
Proceedings of the Thirteenth International Symposium
on Temporal Representation and Reasoning (TIME?06),
pages 196?203. IEEE Computer Society, June.
Inderjeet Mani and George Wilson. 2000. Robust tem-
poral processing of news. In Proceedings of the 38th
Annual Meeting on Association for Computational Lin-
guistics (ACL ?00), pages 69?76, Morristown, NJ, USA,
October. Association for Computational Linguistics.
Pawel Mazur and Robert Dale. 2007. The DANTE Tem-
poral Expression Tagger. In Zygmunt Vetulani, editor,
Proceedings of the 3rd Language And Technology Con-
ference (LTC), Poznan, Poland, October.
Pawel Mazur and Robert Dale. 2008. What?s the Date?
High Accuracy Interpretation of Weekday Names. In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 553?
560, Manchester, UK, August. Coling 2008 Organizing
Committee.
Matteo Negri and Luca Marseglia. 2005. Recognition
and normalization of time expressions: Itc-irst at tern
2004. Technical Report WP3.7, Information Society
Technologies, February.
Feng Pan, R. Mulkar, and J. R. Hobbs. 2006. Learning
event durations from event descriptions. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 393?400,
Sydney, Australia, July. Association for Computational
Linguistics.
James Pustejovsky, J. Castan?o, R. Ingria, R. Saur??,
R. Gaizauskas, A. Setzer, and G. Katz. 2003. TimeML:
Robust Specification of Event and Temporal Expres-
sions in Text. In IWCS-5, Fifth International Workshop
on Computational Semantics, Tilburg, The Netherlands,
January.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. ISO-TimeML: An International
Standard for Semantic Annotation. In Bente Maegaard
Joseph Mariani Jan Odjik Stelios Piperidis Mike Rosner
Daniel Tapias Nicoletta Calzolari (Conference Chair),
Khalid Choukri, editor, Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC?10), Valletta, Malta, May. European
Language Resources Association (ELRA).
Estela Saquete. 2005. Temporal Expression Recognition
and Resolution applied to Event Ordering. Ph.D. thesis,
Departamento de Lenguages y Sistemas Informaticos,
Universidad de Alicante, June.
Frank Schilder. 2004. Extracting meaning from temporal
nouns and temporal prepositions. ACM Transactions
on Asian Language Information Processing (TALIP),
3(1):33?50, March.
Charles Teisse`dre, Delphine Battistelli, and Jean-Luc
Minel. 2010. Resources for calendar expressions se-
mantic tagging and temporal navigation through texts.
In Proceedings of LREC2010, May.
922
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1158?1167,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Generating Subsequent Reference in Shared Visual Scenes:
Computation vs. Re-Use
Jette Viethen1,2
jette.viethen@mq.edu.au
1TiCC
University of Tilburg
Tilburg, The Netherlands
Robert Dale2
robert.dale@mq.edu.au
2Centre for Language Technology
Macquarie University
Sydney, Australia
Markus Guhe3
m.guhe@ed.ac.uk
3School of Informatics
University of Edinburgh
Edinburgh, UK
Abstract
Traditional computational approaches to re-
ferring expression generation operate in a de-
liberate manner, choosing the attributes to be
included on the basis of their ability to dis-
tinguish the intended referent from its dis-
tractors. However, work in psycholinguis-
tics suggests that speakers align their refer-
ring expressions with those used previously in
the discourse, implying less deliberate choice
and more subconscious reuse. This raises the
question as to which is a more accurate char-
acterisation of what people do. Using a cor-
pus of dialogues containing 16,358 referring
expressions, we explore this question via the
generation of subsequent references in shared
visual scenes. We use a machine learning ap-
proach to referring expression generation and
demonstrate that incorporating features that
correspond to the computational tradition does
not match human referring behaviour as well
as using features corresponding to the process
of alignment. The results support the view that
the traditional model of referring expression
generation that is widely assumed in work on
natural language generation may not in fact
be correct; our analysis may also help explain
the oft-observed redundancy found in human-
produced referring expressions.
1 Introduction
Computational work on referring expression genera-
tion (REG) has an extensive history, and a wide vari-
ety of algorithms have been proposed, dealing with
various facets of what is recognised to be a com-
plex problem. Almost all of this work sees the task
as being concerned with choosing those attributes
of an intended referent that distinguish it from the
other entities with which it might be confused (see,
for example, Dale (1989), Dale and Reiter (1995),
Krahmer et al (2003), van Deemter and Krahmer
(2007), Gardent and Striegnitz (2007)). Indepen-
dently, an alternative way of thinking about refer-
ence has arisen within the psycholinguistics com-
munity: there is now a long tradition of work that
explores how a dialogue participant?s forms of ref-
erence are influenced by those previously used for
a given entity. Most recently, this line of work has
been discussed in terms of the notions of alignment
(Pickering and Garrod, 2004) and conceptual pacts
(Clark and Wilkes-Gibbs, 1986; Brennan and Clark,
1996).
We suspect that neither approach tells the full
story, and so we are interested in exploring whether
the two perspectives should be integrated. Using a
large corpus of referring expressions in task-oriented
dialogues, this paper presents a machine learning
approach that allows us to combine features corre-
sponding to the two perspectives. Our results show
that models based on the alignment perspective out-
perform models based on traditional REG considera-
tions, as well as a number of simpler baselines.
The paper is structured as follows. In Section 2,
we outline the two perspectives on subsequent ref-
erence, and summarise related work. In Section 3,
we describe the iMAP Corpus and the referring ex-
pressions it contains. In Section 4, we describe the
approach we take to learning models of referential
behaviour using this data, and in Section 5 we dis-
cuss the results of a number of experiments based
1158
on this approach, followed by an error analysis in
Section 6. Section 7 draws some conclusions and
discusses future work.
2 Related Work
2.1 The Algorithmic Approach
We use the term algorithmic approach here to re-
fer to the perspective that is common to the consid-
erable body of work within computational linguis-
tics on the problem of referring expression gener-
ation developed over the last 20 years. Much of
this work takes as its starting point the characterisa-
tion of the problem expressed in (Dale, 1989). This
work has focused on the design of algorithms which
take into account the context of reference in order to
decide what properties of an entity should be men-
tioned in order to distinguish that entity from others
with which it might be confused. Early work was
concerned with subsequent reference in discourse,
inspired by Grosz and Sidner?s (1986) observations
on how the attentional structure of a discourse made
particular referents accessible at any given point.
More recently, attention has shifted to initial ref-
erence in visual domains, driven in large part by
the availability of the TUNA dataset and the shared
tasks that make use of it (Gatt et al, 2008). The con-
struction of distinguishing descriptions has consis-
tently been a key consideration in this body of work.
Scenarios that require the generation of references
in multi-turn dialogues that concern visual scenes
are likely to be among the first where we can ex-
pect computational approaches to referring expres-
sion generation to be practically useful. Surpris-
ingly, however, the more recent work on initial refer-
ence in visual domains and the earlier work on sub-
sequent reference in discourse remain somewhat dis-
tinct and separate from each other, despite much the
same algorithms having been used in both. There
is very little work that brings these two strands to-
gether by looking at both initial and subsequent ref-
erences in dialogues that concern visual scenes. An
exception here is the machine learning approach de-
veloped by Stoia et al (2006), who aimed at building
a dialogue system for a situated agent giving instruc-
tions in a virtual 3D world. However, their approach
was concerned with choosing the type of reference
to use (definite or indefinite, pronominal, bare or
modified head noun), and not with the content of the
reference; and their data set consisted of only 1242
referring expressions.
2.2 The Alignment Approach
Meanwhile, starting with the early work of Carroll
(1980), a quite distinct strand of research in psy-
cholinguistics has explored how a speaker?s form of
reference to an entity is impacted by the way that en-
tity has been previously referred to in the discourse
or dialogue. The general idea behind what we will
call the alignment approach is that a conversational
participant will often adopt the same semantic, syn-
tactic and lexical alternatives as the other party in a
dialogue. This perspective is most strongly associ-
ated with the work of Pickering and Garrod (2004).
With respect to reference in particular, speakers are
said to form conceptual pacts in their use of lan-
guage (Clark and Wilkes-Gibbs, 1986; Brennan and
Clark, 1996). Although there is disagreement about
the exact mechanisms that enable alignment and
conceptual pacts, the implication of much of this
work is that one speaker introduces an entity by
means of some description, and then (perhaps after
some negotiation) both conversational participants
share this form of reference, or a form of reference
derived from it, when they subsequently refer to that
entity.
Recent work by Goudbeek and Krahmer (2010)
supports the view that subconscious alignment does
indeed take place at the level of content selection for
referring expressions. The participants in their study
were more likely to use a dispreferred attribute to
describe a target referent if this attribute had recently
been used in a description by a confederate.
There is some work within natural language gen-
eration that attempts to model the process of align-
ment (Buschmeier et al, 2009; Janarthanam and
Lemon, 2009), but this is predominantly concerned
with what we might think of as the ?lexical perspec-
tive?, focussing on lexical choice rather than the se-
lection of appropriate semantic content for distin-
guishing descriptions.
2.3 Combined Models
This paper is not the first to look at how the algorith-
mic approach and the alignment approach might be
integrated in REG. An early machine learning ap-
1159
Figure 1: An example pair of maps.
proach to content selection was presented by Jor-
dan and Walker (2000; 2005); they were also in-
terested in an exploration of the validity of differ-
ent psycholinguistic models of reference produc-
tion, including Grosz and Sidner?s (1986) model
of discourse structure, the conceptual pacts model
of Clark and colleagues, and the intentional influ-
ences model developed by Jordan (2000). However,
their data set consists of only 393 referring expres-
sions, compared to our 16,358, and these expres-
sions had functions other than identification; most
importantly, the entities referred to were not part of
a shared visual scene as is the case in our data.
Gupta and Stent (2005) instantiated Dale and Re-
iter?s (1995) Incremental Algorithm with a prefer-
ence ordering that favours the attributes that were
used in the previous mention of the same referent. In
a second variant, they even require these attributes
to be included in a subsequent reference. Differ-
ently from most other work on REG, they extended
the task to include ordering of the attributes in the
surface form. They therefore create a special evalu-
ation metric that takes ordering into account, which
makes it hard to compare the performance they re-
port to that of any system that is not concerned with
attribute ordering, such as ours. Their evaluation set
was also considerably smaller than ours: they used
1294 and 471 referring expressions from two differ-
ent corpora, compared to our test set of 4947 refer-
ring expressions.
More recently in (Viethen et al, 2010), we pre-
sented a rule-based system that addressed a specific
instance of the problem we consider here, using the
same corpus as we do: we singled out 2579 first ref-
erences to landmarks by the second speaker (?second
speaker initial references?) and attempted to repro-
duce these using a system based on Dale and Re-
iter?s (1995) Incremental Algorithm. Although the
data set was a subset of the one used here, the system
did not reach the same performance (see Section 5).
3 Referring Expressions in the iMAP
Corpus
The iMAP Corpus (Louwerse et al, 2007) is a col-
lection of 256 dialogues between 32 participant-
pairs who contributed 8 dialogues each. Both par-
ticipants had a map of the same environment, but
one participant?s map showed a route winding its
way between the landmarks on the map; see Fig-
ure 1. The task was for this participant (the in-
struction giver, IG) to describe this route in such a
way that their partner (the instruction follower, IF)
could draw it onto their map; this was complicated
by some discrepancies between the two maps, such
1160
as missing landmarks, the unavailability of colour in
some regions due to ink stains, and small differences
between some landmarks.
The landmarks differ from each other in type,
colour, and one other attribute, which is different
for each type of landmark. For example, there are
different kinds of birds (eagle, ostrich, penguin . . . );
fish differ by their patterns (dotted, checkered, plain
. . . ), aliens have different shapes (circular, hexago-
nal . . . ), and bugs appear in small clusters of differ-
ing numbers. In addition to these inherent attributes
of the landmarks, participants used spatial relations
to other items on the map. Each referring expression
in the corpus is annotated with a unique identifier
corresponding to the landmark that it describes and
the semantic values of the attributes that it contains.
This collection of annotations forms the basic data
we use in our experiments.
For each landmarkR referred to in a dialogue, we
view the sequence of references to this landmark as
a coreference chain, notated ?R1, R2, . . . , Rn?. By
convention, R1 is termed the initial reference, and
all other references in the chain are subsequent ref-
erences. From the corpus as a whole we extracted
34,127 referring expressions in 9558 chains. The av-
erage length of a chain is 4.74; and the longest coref-
erence chain contains 43 references. References
may be contributed to a chain by either speaker, and
can be arbitrarily far apart: in the data, 4201 refer-
ences are in the utterance immediately following the
preceding reference in the chain, but the distance be-
tween references in a chain can be as high as 423
utterances.
We removed from the data any annotation that
was not concerned with the four landmark attributes,
type, colour, relation, or the landmark?s other dis-
tinguishing attribute. For example, ?semantically
empty? head nouns such as thing or set. Ordi-
nal numbers that were annotated as the use of the
number attribute were re-tagged as spatial relations,
as these usually described the position of the target
within a line of landmarks.
As a result of the removal of annotations not per-
taining to the use of the four landmark attributes,
2785 referring expressions had no annotation left;
we removed these instances from the final data set.
We also do not attempt to replicate the remaining
5552 plural referring expressions or the 3062 pro-
Content Pattern Count Proportion
?other? 5893 36.0%
?other, type? 3684 22.5%
?other, colour? 1630 10.0%
?other, colour, type? 1021 6.2%
?colour? 969 5.9%
?relation? 777 4.7%
?other, relation? 587 3.6%
?type? 574 3.5%
?colour, type? 434 2.7%
?other, relation, type? 312 1.9%
?relation, type? 236 1.4%
?colour, relation? 99 0.6%
?other, colour, relation? 81 0.5%
?other, colour, relation, type? 44 0.3%
?colour, relation, type? 17 0.1%
Total 16,358
Table 1: The 15 content patterns by frequency.
nouns found in the corpus.1 However, we do in-
clude all of these instances in the feature extraction
step, on the assumption that they might impact on
the content of subsequent references. Similarly, we
filter out 6369 initial references after we have ex-
tracted features from them, since we focus here on
the generation of subsequent reference only. The re-
maining 16,358 referring expressions form the data
which we use in our experiments.
Contrary to findings from other corpora, in which
colour was used much more frequently (Gatt, 2007;
Viethen and Dale, 2008), the colour attribute was
used in only 26.3% of the referring expressions in
our data set. This is probably due to the often low
reliability of colour in this task caused by the ink
stains. The proportion of referring expressions men-
tioning the target?s type might, at 38.7%, also seem
low. This can be explained by the fact that one quar-
ter of the landmarks, namely birds and buildings, are
more likely to be described in terms of their specific
kind than in terms of their generic type. This also
helps explain why the overall use of the other at-
tribute, which for some landmarks was their kind,
was used in 81.0% of all instances. Spatial relations
were used in 13.16% of the referring expressions,
comparable to other corpora in the literature.
1The additional issues that arise in generating plural refer-
ences and deciding when to use pronouns considerably compli-
cate the problem; see (Gatt, 2007).
1161
We can think of each referring expression as be-
ing a linguistic realisation of a content pattern: this
is the collection of attributes that are used in that
instance. The attributes can be derived from the
property-level annotation given in the corpus. So,
for example, if a particular reference appears as the
noun phrase the blue penguin, annotated seman-
tically as ?blue, penguin?, then the corresponding
content pattern is ?colour, kind?. Our aim is to repli-
cate the content pattern of each referring expression
in the corpus. Table 1 lists the 15 content patterns
that occur in our data set in order of frequency.
4 Modelling Referential Behaviour
4.1 The Two Perspectives
Our task is defined simply as follows: for each sub-
sequent referenceR in the corpus, can we predict the
content pattern that will be used in that reference?
As we noted at the outset of the paper, the literature
would appear to suggest two distinct approaches to
this problem. What we have characterised as the al-
gorithmic approach can be summarised thus:
At the point where a reference is required,
a speaker determines the relevant features
of other entities in the context, then com-
putes the content of a referring expression
which distinguishes the intended referent
from the other entities.
The alignment approach, on the other hand, can be
summarised thus:
Speakers align the forms of reference they
use to be similar or identical to references
that have been used before. In particular,
once a form of reference to the intended
referent has been established, they tend to
re-use that form of reference, or perhaps
an abbreviated version of it.
The alignment approach would appear to be prefer-
able on the grounds of computational cost: we
would expect that retrieving a previously-used refer-
ring expression, or parts thereof, generally requires
less computation than building a new referring ex-
pression from scratch.
On the other hand, if the context has changed
in any way, then a previously-used form of ref-
erence may no longer be effective in identifying
Map Features
Main Map type most frequent type of LM on this map
Main Map other other attribute if the most frequent type of LM
Mixedness are other LM types present on this map?
Ink Orderliness shape of the ink blot(s) on the IF?s map
Lmprop Features
other Att type of the other attribute of the target
[att] Value value for each att of target
[att] Difference was att of target different between the two
maps?
Missing was target missing one of the maps?
Inked Out was target inked] out on the IG?s map?
Speaker Features
Dyad ID ID of the pair of participant-pair
Speaker ID ID of the person who uttered this RE
Speaker Role was the speaker the IG or the IF?
Table 2: The Ind feature set.
the intended referent, and recomputation may be
required.2 This is precisely the consideration on
which the initial work on referring expression gen-
eration was based, inspired by Grosz and Sidner?s
(1986) observations about how the changing atten-
tional structure of a discourse moves different en-
tities in and out of focus. However, a straightfor-
ward recomputation of reference based on the cur-
rrent context carries the risk that the most effective
set of properties to use may change quite radically;
if no account is taken of the history of previous ref-
erences to the entity, it?s conceivable that one could
produce a description that is so different from the
previous description that they are virtually unreco-
gisable as descriptions of the same entity. Ideally,
what we want to do is modify a previous description
to do the job.
These observations suggest that, in order to
choose the most appropriate form of reference for an
entity, we need to simultaneously take account of:
? the other entities from which it must be distin-
guished, both in the visual context and in the
preceding discourse (in other words, exactly
the information that traditional algorithmic ap-
proaches consider);
? how this entity, and perhaps other entities, have
been referred to in the past (precisely the infor-
mation that the alignment approach considers).
2Unfortunately, determining what counts as a change of con-
text, especially in visual scenes, is fraught with difficulty.
1162
TradREG Features (Visual)
Count Vis Distractors number of visual distractors
Prop Vis Same [att] proportion of visual distractors with
same att
Dist Closest distance to the closest visual distrac-
tor
Closest Same [att] has the closest distractor the same
att?
Dist Closest Same [att] distance to the closest distractor of
same att as target
Cl Same type Same [att] has the closest distractor of the same
type also the same att?
TradREG Features (Discourse)
Count Intervening LMs number of other LMs mentioned since
the last mention of the target
Prop Intervening [att] proportion of intervening LMs for
which att was used AND which have
the same att as target
Table 3: The TradREG feature set.
The set of features we describe next attempts to cap-
ture these two aspects of the problem.
4.2 Features
The number of factors that can be hypothesised as
having an impact on the form of a referring expres-
sion in a dialogic setting associated with a visual do-
main is very large. Attempting to incorporate all of
these factors into parameters for rule-based systems,
and then experimenting with different settings for
these parameters, is prohibitively complex. Instead,
we here capture a wide range of factors as features
that can be used by a machine learning algorithm to
automatically induce from the data a classifier that
predicts for a given set of features the attributes that
should be used in a referring expression.
The features we extracted from the data set are
listed in Tables 2?4.3 They fall into five subsets.
Map Features capture design characteristics of the
maps the current dialogue is about; Speaker Fea-
tures capture the identity and role of the partici-
pants; and LMprop Features capture the inherent
visual properties of the target referent. For our ex-
periments, we group the Map, LMprop and Speaker
feature sets into one theory-independent set (Ind).
Most importantly for our present considerations,
3In these tables, att is an abbreviatory variable that is instan-
tiated once for each of the four attributes type, colour, relation,
and the other distinguishing attribute of the landmark. The ab-
breviation LM stands for landmark
Alignment Features (Recency)
Last Men Speaker Same who made the last mention of target?
Last Mention [att] was att used in the last mention of
target?
Dist Last Mention Utts distance to the last mention of target
in utterances
Dist Last Mention REs distance to the last mention of target
in REs
Dist Last [att] LM Utts distance in utterances to last use of
att for target
Dist Last [att] LM REs distance in REs to last use of att for
target
Dist Last [att] Dial Utts distance in utterances to last use of
att
Dist Last [att] Dial REs distance in REs to last use of att
Dist Last RE Utts distance to last RE in utterances
Last RE [att] was att mentioned in the last RE?
Alignment Features (Frequency)
Count [att] Dial how often has att been used in the dialogue?
Count [att] LM how often has att been used for target?
Quartile quartile of the dialogue the RE was uttered in
Dial No number of dialogues already completed +1
Mention No number of previous mentions of target +1
Table 4: The Alignment feature set.
TradREG Features capture factors that the tradi-
tional computational approaches to referring expres-
sion generation take account of, in particular prop-
erties of the discourse and visual distractors; and
Alignment Features capture factors that we would
expect to play a role in the psycholinguistic models
of alignment and conceptual pacts.
4.3 The Models
For the experiments described here, we used a 70?30
split to divide the data into a training set (11,411 in-
stances) and a test set (4,947 instances). In addition
to the main prediction class content pattern, the split
was stratified for Speaker ID and Quartile to ensure
that training and test set contained the same pro-
portion of descriptions from each speaker and each
quartile of the dialogues. We used the J48 algorithm
implemented in the Weka toolkit (Witten and Frank,
2005) to train decision trees with the task of judging,
based on the given features, which content pattern
should be used.
First, we have three separate baseline models:
HeadNounOnly generates only the property that is
the most likely head noun for the target, which
is kind for birds and buildings and type for all
1163
other landmarks. This is a form of ?reduced
reference? strategy.
RepeatLast represents a very simplistic alignment
approach. It generates the same content pattern
that was used in the previous mention of the
target referent.
MajorityClass generates the content pattern most
commonly used in the training set.
We then have a number of models that use subsets
of the features described above:
AllFeatures is a decision tree trained on all fea-
tures;
TradREG is a decision tree trained on the
TradREG features only;
Alignment is a decision tree trained on the Align-
ment features only;
Ind is a decision tree trained on the Ind features
only;
Alignment+Ind is a decision tree trained on all but
the TradREG features;
TradREG+Ind is a decision tree trained on all but
the Alignment features; and
TradREG+Alignment is a decision tree trained on
all but the Ind features.
5 Results
In this section we report how the models described
in the previous section performed on the held-out
test set in comparison to each other and to the three
baselines.
We use Accuracy and average DICE score for our
comparisons; these are the most commonly used
measures in the REG literature (see, for example,
Gatt et al, 2008). Given two sets of attributes, A
and B, DICE is computed as
(1) DICE = 2? |A ?B||A|+ |B| .
This gives some measure of the overlap between two
referring expressions, assigning a partial score if the
two sets share attributes but are not identical. The
Accuracy of a system is the proportion of test in-
stances for which it achieves a DICE score of 1, sig-
nifying a perfect match.
col other type rel Comb. Pattern
Acc Acc Acc Acc Acc DICE
HeadOnly n/a n/a n/a n/a 23.1 0.49
RepLast n/a n/a n/a n/a 38.4 0.55
Majority 73.8 81.0 61.7 86.8 36.0 0.65
predicts no yes no no ?other?
Trad 74.6 84.8 77.1 87.0 47.3 0.73
Align 83.6 84.1 80.7 87.5 54.6 0.78
Ind 81.9 82.8 81.4 88.0 52.7 0.78
Align+Ind 86.1 85.3 82.4 88.7 58.2 0.81
Trad+Ind 82.2 84.1 81.1 87.1 52.5 0.78
Trad+Align 84.1 84.0 80.1 86.8 53.9 0.78
AllFeatures 86.2 85.8 83.2 88.5 58.8 0.81
Table 5: Performance of our models compared to the
baselines. Model names are abbreviated for space rea-
sons. The Accuracy (given in %) of all models is signifi-
cantly better than that of the highest performing baseline
at p<.01 according to the ?2 statistic.
We tested two different ways of generating con-
tent patterns based on the different feature sets de-
scribed above: PatternAtOnce builds a decision
tree that chooses one of the 15 content patterns that
occur in our data set; whereas CombinedPattern
builds attribute-specific decision trees (one for each
of the four attributes that occur in the data: colour,
other, type, and relation), and then combines their
predictions into a complete content pattern. We
found that CombinedPattern slightly outperformed
PatternAtOnce, although the difference is not statis-
tically significant for all feature sets. For space rea-
sons, we report in what follows only on the slightly
better-performing CombinedPattern model.
Table 5 compares the performances of the three
baselines and the decision trees based on the five fea-
ture subsets for each of the individual attributes and
for the combined content pattern; note that the Head-
NounOnly and RepeatLast baselines do not make
attribute-specific predictions. The table shows that
the learned systems outperform all three baselines
for the individual attributes as well as for the com-
bined content pattern.
A comparison of the Alignment feature set and
the TradREG feature set shows that the former out-
performs the latter for the attribute-specific trees
which predict the use of the colour attribute and the
1164
use of relation, and that the combined patterns re-
sulting from the Alignment trees are a better match
of the human-produced patterns both in terms of Ac-
curacy (p<.01 for all three categories, using ?2) and
DICE. Interestingly, even the theory-independent
Ind features outperform the TradREG features.
The comparison between TradREG+Ind and
Alignment+Ind again shows a clear advantage for
the Alignment features: dropping them from the
complete feature set significantly hurts performance
compared to AllFeatures (?2=80.5, p<.01), while
dropping the TradREG features has no significant
impact. Also consistent with the results of the three
individual feature sets, dropping the Ind features
hurts performance more than dropping the TradREG
features, but less than dropping the Alignment fea-
tures. Training on the complete feature set (All-
Features) achieves the highest performance, which
is significantly better than that of all other features
sets (p<.01 using ?2) except Alignment+Ind.
These results suggest that considerations at the
heart of traditional REG approaches do not play as
important a role as those postulated by alignment-
based models for the selection of semantic content
for subsequent referring expressions.
We also note that the Accuracy scores achieved
by our learned systems are similar to the best num-
bers previously reported in the REG literature. While
Jordan and Walker?s (2005) data set is not directly
comparable, they achieved a maximum of 59.9%
Accuracy, against our 58.8%. Stoia et al?s (2006)
best Accuracy was 31.2%, albeit on a slightly dif-
ferent task. Even in the arguably much simpler
non-dialogic domains of the REG competitions con-
cerned with pure content selection, the best perform-
ing system achieved only 53% Accuracy (see Gatt et
al., 2008). The most comparable approach, the rule-
based system we presented in (Viethen et al, 2010)
for a subset of the data used here, was not able to
outperform a RepeatLast baseline at 40.2% Accu-
racy and an average DICE score of 0.67.
6 Error Analysis
An important question to ask is how wrong the mod-
els really are when they do not succeed in perfectly
matching a human-produced reference in our test
set. It might be that they choose a completely dif-
Acc Dice Super Sub Inter Noover
Trad 47.3 0.75 14.4 22.2 5.5 10.5
Align 54.6 0.78 16.0 16.1 3.9 9.4
Ind 52.7 0.78 17.1 17.2 3.9 9.0
Align+Ind 58.2 0.81 16.0 14.8 3.1 7.9
Trad+Ind 52.5 0.78 17.4 17.5 3.8 8.8
Trad+Align 53.9 0.78 17.1 15.6 4.3 9.0
AllFeature 58.8 0.81 16.5 14.5 3.1 7.2
Table 6: The proportions of test instances for which each
model produced a subset, a superset, some other form of
intersection or no-overlap to the human reference.
ferent set of attributes from those included by the
human speaker; however, the Accuracy score also
counts as incorrect any set that only partly overlaps
with the reference found in the test set.
The DICE score gives us a partial answer to this
question, as it assigns a score that is based on the
size of the overlap between the attribute set cho-
sen by the model and that included by the human
speaker. A DICE score that is equal to the Accuracy
score would mean that each referring expression was
either reproduced perfectly, or that a set of attributes
was chosen that did not overlap with the original
one at all. The fact that all our models achieved
DICE scores much higher than their Accuracy scores
shows that they only rarely got it completely wrong.
Table 6 gives a more fine-grained picture by list-
ing, for each model, what percentage of the refer-
ring expressions it produced contained a subset of
the attributes included in the human reference, what
percentage were a superset, what percentage had
another form of partial intersection, and what per-
centage had no commonality with the human refer-
ence. Interestingly, a large number of the referring
expressions produced by the model trained only on
TradREG features are subsets of the human refer-
ence. This indicates that human speakers tend to in-
clude more attributes than are strictly speaking nec-
essary to distinguish the landmark.4 The Alignment
model does not as often produce a subset of the gold
standard content pattern, suggesting that it might be
alignment considerations that account for some of
4That humans often produce ?redundant? descriptions, in op-
position to the target behaviour of some of the early REG algo-
rithms, is of course an oft-observed fact.
1165
both both 1st 2nd either pot.
corr. wrong corr. corr. corr. Acc
Trad vs Ind 1797 1794 545 811 3153 63.7
Trad vs Align 1742 1647 600 958 3300 66.7
Trad vs Align+Ind 1849 1574 493 1031 3373 68.2
Align vs Trad+Ind 1908 1557 792 690 3390 68.5
Align vs Ind 1872 1511 828 736 3436 69.5
Ind vs Trad+Align 1840 1511 768 828 3436 69.5
Table 7: Comparison of the predictions for the combined
content pattern between the models trained on mutually
exclusive feature sets.
the apparent redundancy that human-produced refer-
ring expressions contain.
A second important question is whether the differ-
ent feature sets are doing the same work, or whether
they complement each other. Table 7 lists for those
pairings of our learned models which were based on
mutually exclusive feature sets how many referring
expressions both models predicted correctly, how
many both failed to predict, and how many were pre-
dicted correctly by either of the two models.
Note the high numbers in the columns listing the
counts of instances which both models got either
correct or wrong: these show that there is con-
siderable overlap between all pairings. The small-
est agreement lies at 3424 instances (68.2%) be-
tween TradREG (the least successful model) and
Alignment+Ind (the most successful model). How-
ever, they also each predict correct solutions that the
other misses: 493 (10.0%) for TradREG and 1031
(20.8%) for Alignment+Ind.
The last two columns of Table 7 show the number
of instances that at least one of the two models in
each pairing got correct and the proportion out of
all test instances that this number represents. This
proportion is the maximum Accuracy that could be
achieved by a model that combines the two models
in a pairing and then correctly chooses which one to
use in each instance. The maximum Accuracies that
could be achieved in this way on our data set lie just
below 70%, significantly higher than any numbers
reported in the literature on the task of generating
subsequent reference.
7 Conclusions
Using the largest corpus of referring expressions
to date, we have shown how both the traditional
computational view of REG and the alternative psy-
cholinguistic alignment approach can be captured
via a large set of features for machine learning. Ad-
ditionally, we defined a number of theory indepen-
dent features. Using this approach we have pre-
sented three main findings.
First, we have demonstrated that a model using all
these features to predict content patterns in subse-
quent references in shared visual scenes delivers an
Accuracy of 58.8% and a DICE score of 0.81, out-
performing models based only on features inspired
by one of the two approaches. However, we found
that the features based on traditional REG considera-
tions do not contribute as much to this score as those
based on the alignment approach, and that dropping
the traditional REG features does not significantly
hurt the performance of a model based on alignment
and theory-independent features.
Second, our error analysis showed that the main
reason for the low performance of a model based
on traditional algorithmic features is that it often
chooses too few attributes. The fact that the model
based on the alignment features does not make this
mistake so frequently suggests that it may be the
psycholinguistic considerations incorporated in our
alignment features that lead people to add those ad-
ditional attributes.
Finally, while the different models make the same
correct predictions about the content of referring ex-
pressions in many cases, there are also a consider-
able number of cases where the models based on
either the traditional algorithmic features (10.0%)
or the alignment and independent features (20.8%)
alone make correct predictions that the other gets
wrong; this suggests that a system with the ability
to choose the correct model in each of those cases
(perhaps based on a hypothesis as to whether or not
the relevant context has changed) could reach an ac-
curacy of almost 70% on our data set. In future work
we plan to identify further features that will allow us
to inform this choice so that we can move towards
this level of performance.
1166
References
Susan E. Brennan and Herbert H. Clark. 1996. Concep-
tual pacts and lexical choice in conversation. Journal
of Experimental Psychology: Learning, Memory, and
Cognition, 22:1482?1493.
Hendrik Buschmeier, Kirsten Bergmann, and Stefan
Kopp. 2009. An alignment-capable microplanner for
natural language generation. In Proceedings of the
12th European Workshop on Natural Language Gen-
eration, pages 82?89, Athens, Greece.
John M. Carroll. 1980. Naming and describing in social
communication. Language and Speech, 23:309?322.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22(1):1?
39.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics, Vancouver B.C.,
Canada.
Claire Gardent and Kristina Striegnitz. 2007. Generat-
ing bridging definite descriptions. In Harry C. Bunt
and Reinhard Muskens, editors, Computing Meaning,
volume 3, pages 369?396. Kluwer, Dordrecht, The
Netherlands.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The TUNA
Challenge 2008: Overview and evaluation results. In
Proceedings of the 5th International Conference on
Natural Language Generation, pages 198?206, Salt
Fork OH, USA.
Albert Gatt. 2007. Generating Coherent Reference to
Multiple Entities. Ph.D. thesis, University of Ab-
erdeen, UK.
Martijn Goudbeek and Emiel Krahmer. 2010. Pref-
erences versus adaptation during referring expression
generation. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 55?59, Uppsala, Sweden.
Barbara J. Grosz and Candance L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Surabhi Gupta and Amanda Stent. 2005. Automatic
evaluation of referring expression generation using
corpora. In Proceedings of the Workshop on Using
Corpora for Natural Language Generation, pages 1?
6, Brighton, UK.
Srinivasan Janarthanam and Oliver Lemon. 2009. Learn-
ing lexical alignment policies for generating referring
expressions for spoken dialogue systems. In Pro-
ceedings of the 12th European Workshop on Natu-
ral Language Generation (ENLG 2009), pages 74?
81, Athens, Greece, March. Association for Compu-
tational Linguistics.
Pamela W. Jordan and Marilyn Walker. 2000. Learning
attribute selections for non-pronominal expressions.
In Proceedings of the 38th Annual Meeting on As-
sociation for Computational Linguistics, Hong Kong,
China.
Pamela W. Jordan and Marilyn Walker. 2005. Learning
content selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelligence Re-
search, 24:157?194.
Pamela W. Jordan. 2000. Intentional Influences on Ob-
ject Redescriptions in Dialogue: Evidence from an
Empirical Study. Ph.D. thesis, University of Pitts-
burgh, Pittsburgh PA, USA.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Lingustics, 29(1):53?72.
Max M. Louwerse, Nick Benesh, Mohammed E. Hoque,
Patrick Jeuniaux, Gwyneth Lewis, Jie Wu, and Megan
Zirnstein. 2007. Multimodal communication in face-
to-face computer-mediated conversations. In Proceed-
ings of the 28th Annual Conference of the Cognitive
Science Society, pages 1235?1240.
Martin J. Pickering and Simon Garrod. 2004. Toward a
mechanistic psychology of dialogue. Behavioral and
Brain Sciences, 27(2):169?226.
Laura Stoia, Darla M. Shockley, Donna K. Byron, and
Eric Fosler-Lussier. 2006. Noun phrase generation
for situated dialogs. In Proceedings of the 4th Interna-
tional Conference on Natural Language Generation,
pages 81?88, Sydney, Australia, July.
Kees van Deemter and Emiel Krahmer. 2007. Graphs
and Booleans: On the generation of referring expres-
sions. In Harry C. Bunt and Reinhard Muskens, edi-
tors, Computing Meaning, volume 3, pages 397?422.
Kluwer, Dordrecht, The Netherlands.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-
ceedings of the 5th International Conference on Natu-
ral Language Generation, pages 59?67, Salt Fork OH,
USA.
Jette Viethen, Simon Zwarts, Robert Dale, and Markus
Guhe. 2010. Dialogue reference in a visual domain.
In Proceedings of the 7th International Conference on
Language Resources and Evaluation, Valetta, Malta.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco CA, USA.
1167
Proceedings of the EACL 2009 Demonstrations Session, pages 33?36,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
The Software Architecture for the
First Challenge on Generating Instructions in Virtual Environments
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Kristina Striegnitz
Union College
striegnk@union.edu
Abstract
The GIVE Challenge is a new Internet-
based evaluation effort for natural lan-
guage generation systems. In this paper,
we motivate and describe the software in-
frastructure that we developed to support
this challenge.
1 Introduction
Natural language generation (NLG) systems are
notoriously hard to evaluate. On the one hand,
simply comparing system outputs to a gold stan-
dard is not appropriate because there can be mul-
tiple generated outputs that are equally good, and
finding metrics that account for this variability and
produce results consistent with human judgments
and task performance measures is difficult (Belz
and Gatt, 2008; Stent et al, 2005; Foster, 2008).
On the other hand, lab-based evaluations with hu-
man subjects to assess each aspect of the system?s
functionality are expensive and time-consuming.
These characteristics make it hard to compare dif-
ferent systems and measure progress.
GIVE (?Generating Instructions in Virtual En-
vironments?) (Koller et al, 2007) is a research
challenge for the NLG community designed to
provide a new approach to NLG system evalua-
tion. In the GIVE scenario, users try to solve
a treasure hunt in a virtual 3D world that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual envi-
ronment. The challenge for the NLG system is
to generate, in real time, natural-language instruc-
tions that will guide the users to the successful
completion of their task (see Fig. 1). One cru-
cial advantage of this generation task is that the
NLG system and the user can be physically sepa-
rated. This makes it possible to carry out a task-
based evaluation over the Internet ? an approach
that has been shown to provide generous amounts
Figure 1: The GIVE Challenge.
of data in earlier studies (von Ahn and Dabbish,
2004; Orkin and Roy, 2007).
In this paper, we describe the software archi-
tecture underlying the GIVE Challenge. The soft-
ware connects each player in a 3D game world
with an NLG system over the Internet. It is imple-
mented and open source, and can be a used online
during EACL at www.give-challenge.org.
In Section 2, we give an introduction to the GIVE
evaluation methodology by describing the experi-
ence of a user participating in the evaluation, the
nature of the data we collect, and our scientific
goals. Then we explain the software architecture
behind the scenes and sketch the API that concrete
NLG systems must implement in Section 3. In
Section 4, we present some preliminary evaluation
results, before we conclude in Section 5.
2 Evaluation method
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
33
b2 b3b4 b5
b6
b7
b1
player
b8b9
b10
b11 b14b13b12
safe 
door
b1 opens doorto room 3
b9 moves picture to
b8: part of safe sequencereveal safe
? to win you have to retrieve the trophy from the safe in room 1? use button b9 to move the picture (and get access to the safe)
? if the alarm sounds, the game is over and you have lost
? press buttons b8, b6, b13, b13, b10 (in this order) to open the safe;if a button is pressed in the wrong order, the whole sequence is reset
b14 makes alarm soundb10, b13: part of safe sequence door to room 2b7 opens/closesstepping on this tiletriggers alarm
alarm
room 3
b2 turns off alarm tileb3 opens/closes door to room 2
b6: part of safe sequence
room 1
b5 makes alarm sound
room 2
door
door
lampcouch
chair
flower
pictu
retrophy
Figure 2: The map of a virtual world.
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system.
The map of one of the game worlds is shown in
Fig. 2: In this world, players must pick up a trophy,
which is in a wall safe behind a picture. In order
to access the trophy, they must first push a button
to move the picture to the side, and then push an-
other sequence of buttons to open the safe. One
floor tile is alarmed, and players lose the game
if they step on this tile without deactivating the
alarm first. There are also a number of distrac-
tor buttons which either do nothing when pressed
or set off an alarm. These distractor buttons are in-
tended to make the game harder and, more impor-
tantly, to require appropriate reference to objects
in the game world. Finally, game worlds can con-
tain a number of objects such as chairs and flowers
which are irrelevant for the task, but can be used
as landmarks by a generation system.
Users are asked to fill out a before- and after-
game questionnaire that collects some demo-
graphic data and asks the user to rate various as-
pects of the instructions they received. Every ac-
tion that players take in a game world, and every
instruction that a generation system generates for
them, is recorded in a database. In addition to the
questionnaire data, we are thus able to compute a
number of objective measures such as:
? the percentage of users each system leads to
a successful completion of the task;
? the average time, the average number of in-
structions, and the average number of in-
game actions that this success requires;
? the percentage of generated referring expres-
sions that the user resolves correctly; and
? average reaction times to instructions.
It is important to note that we have designed
the GIVE Challenge not as a competition, but as
a friendly evaluation effort where people try to
learn from each other?s successes. This is reflected
in the evaluation measures above, which are in
tension with one another: For instance, a system
which gives very low-level instructions (?move
forward?; ?ok, now move forward?; ?ok, now turn
left?) will enjoy short reaction times, but it will re-
quire more instructions than a system that aggre-
gates these. To further emphasize this perspective,
we will also provide a number of diagnostic tools,
such as heat maps that show how much time users
spent on each tile, or a playback function which
displays an entire game run in real time.
In summary, the GIVE Challenge is a novel
evaluation effort for NLG systems. It is motivated
by real applications (such as pedestrian navigation
and the generation of task instructions), makes
no assumptions about the internal structure of an
NLG system, and emphasizes the situated genera-
tion of discourse in a simulated physical environ-
ment. The game world is scalable; it can be made
more complex and it can be adapted to focus on
specific issues in natural language generation.
3 Architecture
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components:
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide.
34
Game Client
Matchmaker
NLG Server
NLG Server
NLG Server
Figure 3: The GIVE architecture.
When a user starts the client, it connects over
the Internet to the Matchmaker. The Matchmaker
then selects a game world and an NLG server at
random, and requests the NLG server to spawn
a new server instance. It then sends the game
world to the client and the server instance and dis-
connects from them, ready to handle new connec-
tions from other clients. The client and the server
instance play one game together: Whenever the
user does something, the client sends a message
about this to the server instance, and the server in-
stance can also send a message back to the client
at any time, which will then be displayed as an in-
struction. When the game ends, the client and the
server instance disconnect from each other. The
server instance sends a log of all game events to
the Matchmaker, and the client sends the ques-
tionnaire results to the Matchmaker; these then are
stored in the database for later analysis.
All of these components are implemented in
Java. This allows the client to be portable across
all major operating systems, and to be started di-
rectly from the website via Java Web Start without
the need for software installation. We felt it was
important to make startup of the client as effort-
less as possible, in order to maximize the num-
ber of users willing to play the game. Unsurpris-
ingly, we had to spend the majority of the pro-
gramming time on the 3D graphics (based on the
free jMonkeyEngine library) and the networking
code. We could have reduced the effort required
for these programming tasks by building upon an
existing virtual 3D world system such as Second
Life. However, we judged that the effort needed to
adapt such a system to our needs would have been
at least as high (in particular, we would have had
to ensure that the user could only move according
to the rules of the GIVE game and to instrument
the virtual world to obtain real-time updates about
events), and the result would have been less exten-
abstract class NlgSystem:
void connectionEstablished();
void connectionDisconnected();
void handleStatusInformation(Position playerPosition,
Orientation playerOrientation,
List?String? visibleObjects);
void handleAction(Atom actionInstance,
List?Formula? updates);
void handleDidNotUnderstand();
void handleMoveTurnAction(Direction direction);
. . .
Figure 4: The interface of an NLG system.
sible to future installments of the challenge.
Since we provided all the 3D, networking, and
database code, the research teams being evaluated
were able to concentrate on the development of
their NLG systems. Our only requirement was
that they implement a concrete subclass of the
class NlgSystem, shown in Fig. 4. This involves
overriding the six abstract callback methods in
this class with concrete implementations in
which the NLG system reacts to specific events.
The methods connectionEstablished
and connectionDisconnected are called
when users enter the game world and when
they disconnect from the game. The method
handleAction gets called whenever the user
performs some physical action, such as pushing a
button, and specifies what changed in the world
due to this action; handleMoveTurnAction
gets called whenever the user moves;
handleDidNotUnderstand gets called
whenever users press the H key to signal that
they didn?t understand the previous instruction;
and handleStatusInformation gets called
once per second and after each user action to
inform the server of the player?s position and
orientation and the visible objects. Ultimately,
each of these method calls gets triggered by a
message that the client sends over the network
in reaction to some event; but this is completely
hidden from the NLG system developer.
The NLG system can use the method send to
send a string to the client to be displayed. It also
has access to various methods querying the state of
the game world and to an interface to an external
planner which can compute a sequence of actions
leading to the goal.
4 First results
For this first installment of the GIVE Challenge,
four research teams from the US, the Netherlands,
35
and Spain provided generation systems, and a
number of other research groups expressed their
interest in participating, but weren?t able to partic-
ipate due to time constraints. Given that this was
the first time we organized this task, we find this
a very encouraging number. All four of the teams
consisted primarily of students who implemented
the NLG systems over the Northern-hemisphere
summer. This is in line with our goal of tak-
ing this first iteration as a ?dry run? in which we
could fine-tune the software, learn about the easy
and hard aspects of the challenge, and validate the
evaluation methodology.
Public involvement in the GIVE Challenge was
launched with a press release in early Novem-
ber 2008; the Matchmaker and the NLG servers
were then kept running until late January 2009.
During this time, online users played over 1100
games, which translates into roughly 75 game runs
for each experimental condition (i.e., five differ-
ent NLG systems paired with three different game
worlds). To our knowledge, this makes GIVE the
largest NLG evaluation effort yet in terms of ex-
perimental subjects.
While we have not yet carried out the detailed
evaluation, the preliminary results look promising:
a casual inspection shows that there are consider-
able differences in task success rate among the dif-
ferent systems.
While there is growing evidence from differ-
ent research areas that the results of Internet-based
evaluations are consistent with more traditional
lab-based experiments (e.g., (Keller et al, 2008;
Gosling et al, 2004)), the issue is not yet set-
tled. Therefore, we are currently conducting a lab-
based evaluation of the GIVE NLG systems, and
will compare those results to the qualitative and
quantitative data provided by the online subjects.
5 Conclusion
In this paper, we have sketched the GIVE Chal-
lenge and the software infrastructure we have de-
veloped for it. The GIVE Challenge is, to the
best of our knowledge, the largest-scale NLG eval-
uation effort with human experimental subjects.
This is made possible by connecting users and
NLG systems over the Internet; we collect eval-
uation data automatically and unobtrusively while
the user simply plays a 3D game. While we will
report on the results of the evaluation in more de-
tail at a later time, first results seem encouraging
in that the performance of different NLG systems
differs considerably.
In the future, we will extend the GIVE Chal-
lenge to harder tasks. Possibilities includ mak-
ing GIVE into a dialogue challenge by allowing
the user to speak as well as act in the world; run-
ning the challenge in a continuous world rather
than a world that only allows discrete movements;
or making it multimodal by allowing the NLG
system to generate arrows or virtual human ges-
tures. All these changes would only require lim-
ited changes to the GIVE software architecture.
However, the exact nature of future directions re-
mains to be discussed with the community.
References
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings of ACL-08:HLT, Short Papers, pages
197?200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95?103, Salt Fork, OH.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
F. Keller, S. Gunasekharan, N. Mayo, and M. Corley.
2008. Timing accuracy of web experiments: A case
study using the WebExp software package. Behav-
ior Research Methods, to appear.
A. Koller, J. Moore, B. di Eugenio, J. Lester, L. Stoia,
D. Byron, J. Oberlander, and K. Striegnitz. 2007.
Shared task proposal: Instruction giving in virtual
worlds. In M. White and R. Dale, editors, Work-
ing group reports of the Workshop on Shared Tasks
and Comparative Evaluation in Natural Language
Generation. Available at http://www.ling.
ohio-state.edu/nlgeval07/report.html.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39?60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
36
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 984?992,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Detecting Emails Containing Requests for Action
Andrew Lampert ??
?CSIRO ICT Centre
PO Box 76
Epping 1710
Australia
andrew.lampert@csiro.au
Robert Dale
?Centre for Language Technology
Macquarie University 2109
Australia
rdale@science.mq.edu.au
Cecile Paris
CSIRO ICT Centre
PO Box 76
Epping 1710
Australia
cecile.paris@csiro.au
Abstract
Automatically finding email messages that
contain requests for action can provide valu-
able assistance to users who otherwise strug-
gle to give appropriate attention to the ac-
tionable tasks in their inbox. As a speech
act classification task, however, automatically
recognising requests in free text is particularly
challenging. The problem is compounded by
the fact that typical emails contain extrane-
ous material that makes it difficult to isolate
the content that is directed to the recipient of
the email message. In this paper, we report
on an email classification system which iden-
tifies messages containing requests; we then
show how, by segmenting the content of email
messages into different functional zones and
then considering only content in a small num-
ber of message zones when detecting requests,
we can improve the accuracy of message-level
automated request classification to 83.76%, a
relative increase of 15.9%. This represents
an error reduction of 41% compared with the
same request classifier deployed without email
zoning.
1 Introduction
The variety of linguistic forms that can be used
to express requests, and in particular the frequency
with which indirect speech acts are used in email, is
a major source of difficulty in determining whether
an email message contains one or more requests.
Another significant problem arises from the fact that
whether or not a request is directed at the recipient of
the email message depends on where in the message
the request is found. Most obviously, if the request is
part of a replied-to message that is contained within
the current message, then it is perhaps more likely
that this request was directed at the sender of the
current message. However, separating out content
intended for the recipient from other extraneous con-
tent is not as simple as it might appear. Segmenting
email messages into their different functional parts
is hampered by the lack of standard syntax used by
different email clients to indicate different message
parts, and by the ad hoc ways in which people vary
the structure and layout of messages.
In this paper, we present our results in classifying
messages according to whether or not they contain
requests, and then show how a separate classifier
that aims to determine the nature of the zones that
make up an email message can improve upon these
results. Section 2 contains some context and moti-
vation for this work before we briefly review rele-
vant related work in Section 3. Then, in Section 4,
we describe a first experiment in request classifica-
tion using data gathered from a manual annotation
experiment. In analysing the errors made by this
classifier, we found that a significant number of er-
rors seemed to arise from the inclusion of content in
parts of a message (e.g., quoted reply content) that
were not authored by the current sender, and thus
were not relevant other than as context for interpret-
ing the current message content. Based on this anal-
ysis, we hypothesised that segmenting messages into
their different functional parts, which we call email
zones, and then using this information to consider
only content from certain parts of a message for re-
quest classification, would improve request classifi-
984
cation performance.
To test this hypothesis, we developed an SVM-
based automated email zone classifier configured
with graphic, orthographic and lexical features; this
is described in more detail in (Lampert et al, 2009).
Section 5 describes how we improve request classi-
fication performance using this email zone classifier.
Section 6 summarises the performance of our re-
quest classifiers, with and without automated email
zoning, along with an analysis of the contribution of
lexical features to request classification, discussion
of request classification learning curves, and a de-
tailed error analysis that explores the sources of re-
quest classification errors. Finally, in Section 7, we
offer pointers to future work and some concluding
remarks.
2 Background and Motivation
Previous research has established that users rou-
tinely use email for managing requests in the work-
place ? e.g., (Mackay, 1988; Ducheneaut and Bel-
lotti, 2001). Such studies have highlighted how
managing multiple ongoing tasks through email
leads to information overload (Whittaker and Sid-
ner, 1996; Bellotti et al, 2003), especially in the
face of an ever-increasing volume of email. The
result is that many users have difficulty giving ap-
propriate attention to requests hidden in their email
which require action or response. A particularly lu-
cid summary of the requirements placed on email
users comes from work by Murray (1991), whose
ethnographic research into the use of electronic mes-
saging at IBM highlighted that:
[Managers] would like to be able to track
outstanding promises they have made,
promises made to them, requests they?ve
made that have not been met and requests
made of them that they have not fulfilled.
This electronic exchange of requests and commit-
ments has previously been identified as a fundamen-
tal basis of the way work is delegated and com-
pleted within organisations. Winograd and Flores
were among the first to recognise and attempt to
exploit this with their Coordinator system (Wino-
grad and Flores, 1986). Their research into organ-
isational communication concluded that ?Organisa-
tions exist as networks of directives and commis-
sives?. It is on this basis that our research explores
the use of requests (directive speech acts) and com-
mitments (commissive speech acts) in email. In this
paper, we focus on requests; feedback from users
of the request and commitment classifier plug-in for
Microsoft Outlook that we have under development
suggests that, at least within the business context of
our current users, requests are the more important of
the two phenomena.
Our aim is to create tools that assist email users
to identify and manage requests contained in incom-
ing and outgoing email. We define a request as an
utterance that places an obligation on an email re-
cipient to schedule an action; perform (or not per-
form) an action; or to respond with some speech
act. A simple example might be Please call when
you have a chance. A more complicated request is
David will send you the latest version if there have
been any updates. If David (perhaps cc?ed) is a re-
cipient of an email containing this second utterance,
the utterance functions as a (conditional) request for
him, even though it is addressed as a commitment to
a third-party. In real-world email, requests are fre-
quently expressed in such subtle ways, as we discuss
in Section 4.
A distinction can be drawn between message-
level identification?i.e., the task of determining
whether an email message contains a request ?
and utterance-level identification?i.e., determin-
ing precisely where and how the request is ex-
pressed. In this paper, we focus on the task of
message-level identification, since utterance-level
identification is a significantly more problematic
task: it is often the case that, while we might agree
that a message contains a request or commitment,
it is much harder to determine the precise extent of
the text that conveys this request (see (Lampert et
al., 2008b) for a detailed discussion of some of the
issues here).
3 Related Work
Our request classification work builds on influential
ideas proposed by Winograd and Flores (1986) in
taking a language/action perspective and identifying
speech acts in email. While this differs from the ap-
proach of most currently-used email systems, which
985
routinely treat the content of email messages as ho-
mogeneous bags-of-words, there is a growing body
of research applying ideas from Speech Act Theory
(Austin, 1962; Searle, 1969) to analyse and enhance
email communication.
Khosravi and Wilks (1999) were among the first
to automate message-level request classification in
email. They used cue-phrase based rules to clas-
sify three classes of requests: Request-Action,
Request-Information and Request-Permission. Un-
fortunately, their approach was quite brittle, with the
rules being very specific to the computer support do-
main from which their email data was drawn.
Cohen, Carvalho and Mitchell (2004) developed
machine learning-based classifiers for a number of
email speech acts. They performed manual email
zoning, but didn?t explore the contribution this made
to the performance of their various speech act clas-
sifiers. For requests, they report peak F-measure of
0.69 against a majority class baseline accuracy of
approximately 66%. Cohen, Carvalho and Mitchell
found that unweighted bigrams were particularly
useful features in their experiments, out-performing
other features applied. They later applied a series of
text normalisations and n-gram feature selection al-
gorithms to improve performance (Carvalho and Co-
hen, 2006). We apply similar normalisations in our
work. While difficult to compare due to the use of a
different email corpus that may or may not exclude
annotation disagreements, our request classifier per-
formance exceeds that of the enhanced classifier re-
ported in (Carvalho and Cohen, 2006).
Goldstein and Sabin (2006) have also worked on
related email classification tasks. They use verb
classes, along with a series of hand-crafted form-
and phrase-based features, for classifying what they
term email genre, a task which overlaps signifi-
cantly with email speech act classification. Their
results are difficult to compare since they include a
mix of form-based classifications like response with
more intent-based classes such as request. For re-
quests, the results are rather poor, with precision of
only 0.43 on a small set of personal mail.
The SmartMail system (Corston-Oliver et al,
2004) is probably the most mature previous work
on utterance-level request classification. SmartMail
attempted to automatically extract and reformulate
action items from email messages for the purpose of
adding them to a user?s to-do list. The system em-
ployed a series of deep linguistic features, including
phrase structure and semantic features, along with
word and part-of-speech n-gram features. The au-
thors found that word n-grams were highly predic-
tive for their classification task, and that there was
little difference in performance when the more ex-
pensive deep linguistic features were added. Based
on this insight, our own system does not employ
deeper linguistic features. Unfortunately, the re-
sults reported reveal only the aggregate performance
across all classes, which involves a mix of both
form-based classes (such as signature content ad-
dress lines and URL lines), and intent-based classes
(such as requests and promises). It is thus very dif-
ficult to directly compare the results with our sys-
tem. Additionally, the experiments were performed
over a large corpus of messages that are not avail-
able for use by other researchers. In contrast, we
use messages from the widely-available Enron email
corpus (Klimt and Yang, 2004) for our own experi-
ments.
While several of the above systems involve man-
ual processes for removing particular parts of mes-
sage bodies, none employ a comprehensive, auto-
mated approach to email zoning.
We focus on the combination of email zoning
and request classification tasks and provide details
of how email zoning improves request classification
? a task not previously explored. To do so, we re-
quire an automated email zone classifier. We exper-
imented with using the Jangada system (Carvalho
and Cohen, 2004), but found similar shortcomings
to those noted by Estival et al (2007). In particular,
Jangada did not accurately identify forwarded or re-
ply content in email messages from the email Enron
corpus that we use. We achieved much better perfor-
mance with our own Zebra zone classifier (Lampert
et al, 2009); it is this system that we use for email
zoning throughout this paper.
4 Email Request Classification
Identifying requests requires interpretation of the in-
tent that lies behind the language used. Given this, it
is natural to approach the problem as one of speech
act identification. In Speech Act Theory, speech
acts are categories like assertion and request that
986
capture the intentions underlying surface utterances,
providing abstractions across the wide variety of dif-
ferent ways in which instances of those categories
might be realised in linguistic form. In this paper
we focus on the speech acts that represent requests,
where people are placing obligations upon others via
actionable content within email messages.
The task of building automated classifiers is dif-
ficult since the function of conveying a request does
not neatly map to a particular set of language forms;
requests often involve what are referred to as indi-
rect speech acts. While investigating particular sur-
face forms of language is relatively unproblematic,
it is widely recognised that ?investigating a collec-
tion of forms that represent, for example, a partic-
ular speech act leads to the problem of establish-
ing which forms constitute that collection? (Archer
et al, 2008). Email offers particular challenges as
it has been shown to exhibit a higher frequency of
indirect speech acts than other media (Hassell and
Christensen, 1996). We approach the problem by
gathering judgments from human annotators and us-
ing this data to train supervised machine learning al-
gorithms.
Our request classifier works at the message-level,
marking emails as requests if they contain one or
more request utterances. As noted earlier, we define
a request as an utterance from the email sender that
places an obligation on a recipient to schedule an
action (e.g., add to a calendar or task list), perform
an action, or respond. Requests may be conditional
or unconditional in terms of the obligation they im-
pose on the recipient. Conditional requests require
action only if a stated condition is satisfied. Previous
annotation experiments have shown that conditional
requests are an important phenomena and occur fre-
quently in email (Scerri et al, 2008; Lampert et al,
2008a). Requests may also be phrased as either a
direct or indirect speech act.
Although some linguists distinguish between
speech acts that require a physical response and
those that require a verbal or information response,
e.g., (Sinclair and Coulthard, 1975), we follow
Searle?s approach and make no such distinction. We
thus consider questions requiring an informational
response to be requests, since they place an obliga-
tion on the recipient to answer.1
Additionally, there are some classes of request
which have been the source of systematic human
disagreement in our previous annotation experi-
ments. One such class consists of requests for
inaction. Requests for inaction, sometimes called
prohibitives (Sadock and Zwicky, 1985), prohibit
action or request negated action. An example is:
Please don?t let anyone else use the computer in the
office. As they impose an obligation on the sender,
we consider requests for inaction to be requests.
Similarly, we consider that meeting announcements
(e.g., Today?s Prebid Meeting will take place in
EB32c2 at 3pm) and requests to read, open or oth-
erwise act on documents attached to email messages
(e.g., See attached) are also requests.
Several complex classes of requests are particu-
larly sensitive to the context for their interpretation.
Reported requests are one such class. Some reported
requests, such as Paul asked if you could put to-
gether a summary of your accomplishments in an
email, clearly function as a request. Others do not
impose an obligation on the recipient, e.g., Sorry for
the delay; Paul requested your prize to be sent out
late December. The surrounding context must be
used to determine the intent of utterances like re-
ported requests. Such distinctions are often difficult
to automate.
Other complex requests include instructions.
Sometimes instructions are of the kind that one
might ?file for later use?. These tend to not be
marked as requests. Other instructions, such as Your
user id and password have been set up. Please fol-
low the steps below to access the new environment,
are intended to be executed more promptly. Tem-
poral distance between receipt of the instruction and
expected action is an important factor to distinguish
between requests and non-requests. Another influ-
encing property is the likelihood of the trigger event
that would lead to execution of the described ac-
tion. While the example instructions above are likely
to be executed, instructions for how to handle sus-
pected anthrax-infected mail are (for most people)
unlikely to be actioned.
Further detail and discussion of these and other
1Note, however, that not all questions are requests. Rhetori-
cal questions are perhaps the most obvious class of non-request
questions.
987
challenges in defining and interpreting requests in
email can be found in (Lampert et al, 2008b). In
particular, that paper includes analysis of a series of
complex edge cases that make even human agree-
ment in identifying requests difficult to achieve.
4.1 An Email Request Classifier
Our request classifier is based around an SVM clas-
sifier, implemented using Weka (Witten and Frank,
2005). Given an email message as input, complete
with header information, our binary request classi-
fier predicts the presence or absence of request ut-
terances within the message.
For training our request classifier, we use email
from the database dump of the Enron email corpus
released by Andrew Fiore and Jeff Heer.2 This ver-
sion of the corpus has been processed to remove du-
plicate messages and to normalise sender and recipi-
ent names, resulting in just over 250,000 email mes-
sages. No attachments are included.
Our request classifier training data is drawn from
a collection of 664 messages that were selected at
random from the Enron corpus. Each message was
annotated by three annotators, with overall kappa
agreement of 0.681. From the full dataset of 664
messages, we remove all messages where annota-
tors disagreed for training and evaluating our request
classifier, in order to mitigate the effects of annota-
tion noise, as discussed in (Beigman and Klebanov,
2009). The unanimously agreed data set used for
training consists of 505 email messages.
4.2 Request Classification Features
The features we use in our request classifier are:
? message length in characters and words;
? number and percentage of capitalised words;
? number of non alpha-numeric characters;
? whether the subject line contains markers of
email replies or forwards (e.g. Re:, Fw:);
? the presence of sender or recipient names;
? the presence of sentences that begin with a
modal verb (e.g., might, may, should, would);
? the presence of sentences that begin with a
question word (e.g, who, what, where, when,
why, which, how);
2http://bailando.sims.berkeley.edu/enron/enron.sql.gz
? whether the message contains any sentences
that end with a question mark; and
? binary word unigram and word bigram fea-
tures for n-grams that occur at least three times
across the training set.
Before generating n-gram features, we normalise
the message text as shown in Table 1, in a manner
similar to Carvalho and Cohen (2006). We also add
tokens marking the start and end of sentences, de-
tected using a modified version of Scott Piao?s sen-
tence splitter (Piao et al, 2002), and tokens marking
the start and end of the message.
Symbol Used Pattern
numbers Any sequence of digits
day Day names or abbreviations
pronoun-object Objective pronouns: me, her, him, us, them
pronoun-subject Subjective pronouns: I, we, you, he, she, they
filetype .doc, .pdf, .ppt, .txt, .xls, .rtf
multi-dash 3 or more sequential ?-? characters
multi-underscore 3 or more sequential ? ? characters
Table 1: Normalisation applied to n-gram features
Our initial request classifier achieves an accuracy of
72.28%. Table 2 shows accuracy, precision, recall
and F-measure results, calculated using stratified 10-
fold cross-validation, compared against a majority
class baseline. Given the well-balanced nature of
our training data (52.08% of messages contain a re-
quest), this is a reasonable basis for comparison.
Majority Baseline No Zoning Classifier
Request Non-Request Request Non-Request
Accuracy 52.08% 72.28%
Precision 0.521 0.000 0.729 0.716
Recall 1.000 0.000 0.745 0.698
F-Measure 0.685 0.000 0.737 0.707
Table 2: Request classifier results without email zoning
An error analysis of the predictions from our initial
request classifier uncovered a series of classification
errors that appeared to be due to request-like sig-
nals being picked up from parts of messages such as
email signatures and quoted reply content. It seemed
likely that our request classifier would benefit from
an email zone classifier that could identify and ig-
nore such message parts.
988
5 Improving Request Classification with
Email Zoning
Requests in email do not occur uniformly across the
zones that make up the email message. There are
specific zones of a message in which requests are
likely to occur.
Unfortunately, accurate classification of email
zones is difficult, hampered by the lack of standard
syntax used by different email clients to indicate dif-
ferent message parts, and by the ad hoc ways in
which people vary the structure and layout of their
messages. For example, different email clients indi-
cate quoted material in a variety of ways. Some pre-
fix every line of the quoted message with a character
such as ?>? or ?|?, while others indent the quoted
content or insert the quoted message unmodified,
prefixed by a message header. Sometimes the new
content is above the quoted content (a style known
as top-posting); in other cases, the new content may
appear after the quoted content (bottom-posting) or
interleaved with the quoted content (inline reply-
ing). Confounding the issue further is that users are
able to configure their email client to suit their in-
dividual tastes, and can change both the syntax of
quoting and their quoting style (top, bottom or in-
line replying) on a per message basis.
Despite the likelihood of some noise being in-
troduced through mis-classification of email zones,
our hypothesis was that even imperfect information
about the functional parts of a message should im-
prove the performance of our request classifier.
Based on this hypothesis, we integrated Zebra
(Lampert et al, 2009), our SVM-based email zone
classifier, to identify the different functional parts of
email messages. Using features that capture graphic,
orthographic and lexical information, Zebra classi-
fies and segments the body text into nine different
email zones: author content (written by the cur-
rent sender), greetings, signoffs, quoted reply con-
tent, forwarded content, email signatures, advertis-
ing, disclaimers, and automated attachment refer-
ences. Zebra has two modes of operation, classi-
fying either message fragments ? whitespace sepa-
rated sets of contiguous lines ? or individual lines.
We configure Zebra for line-based zone classifica-
tion, and use it to extract only lines classified as au-
thor, greeting and signoff text. We remove the con-
tent of all other zones before we evaluate features
for request classification.
6 Results and Discussion
Classifying the zones in email messages and ap-
plying our request classifier to only relevant mes-
sage parts significantly increases the performance
of the request classifier. As noted above, without
zoning, our request classifier achieves accuracy of
72.28% and a weighted F-measure (weighted be-
tween the F-measure for requests and non-requests
based on the relative frequency of each class) of
0.723. Adding the zone classifier, we increase the
accuracy to 83.76% and the weighted F-measure to
0.838. This corresponds to a relative increase in
both accuracy and weighted F-measure of 15.9%,
which in turn corresponds to an error reduction of
more than 41%. Table 3 shows a comparison of
the results of the non-zoning and zoning request
classifiers, generated using stratified 10-fold cross-
validation. In a two-tailed paired t-test, run over ten
iterations of stratified 10-fold cross-validation, the
increase in accuracy, precision, recall and f-measure
were all significant at p=0.01.
No Zoning With Zoning
Request Non-Request Request Non-Request
Accuracy 72.28% 83.76%*
Precision 0.729 0.716 0.849* 0.825*
Recall 0.745 0.698 0.837* 0.839*
F-Measure 0.737 0.707 0.843* 0.832*
Table 3: Request classifier results with and without email
zoning (* indicates a statistically significant difference at
p=0.01)
6.1 Lexical Feature Contribution
As expected, lexical information is crucial to re-
quest classification. When we experimented with re-
moving all lexical (n-gram) features, the non-zoning
request classifier accuracy dropped to 57.62% and
the zoning request classifier accuracy dropped to
61.78%. In contrast, when we apply only n-gram
features, we achieve accuracy of 71.49% for the
non-zoning classifier and 83.36% for the zoning
classifier. Clearly, lexical information is critical for
accurate request classification, regardless of whether
email messages are zoned.
989
Using Information Gain, we ranked the n-gram
features in terms of their usefulness. Table 4 shows
the top-10 unigrams and bigrams for our non-zoning
request classifier. Using these top-10 n-grams (plus
our non-n-gram features), we achieve only 66.34%
accuracy. These top-10 n-grams do not seem to
align well with linguistic intuitions, illustrating how
the noise from irrelevant message parts hampers per-
formance. In particular, there were several similar,
apparently automated messages that were annotated
(as non-requests) which appear to be the source of
several of the top-10 n-grams. This strongly sug-
gests that without zoning, the classifier is not learn-
ing features from the training set at a useful level of
generality.
Word Unigrams Word Bigrams
Word 1 Word 2
pronoun-object let pronoun-object
please pronoun-object know
iso start-sentence no
pronoun-subject start date
hourahead hour :
attached ; hourahead
let hourahead hour
westdesk start-sentence start
parsing westdesk /
if iso final
Table 4: Top 10 useful n-grams for our request classifier
without zoning, ranked by Information Gain
In contrast, once we add the zoning classifier, the
top-10 unigrams and bigrams appear to correspond
much better with linguistic intuitions about the lan-
guage of requests. These are shown in Table 5. Us-
ing these top-10 n-grams (plus our non-n-gram fea-
tures), we achieve 80% accuracy. This suggests that,
even with our relatively small amount of training
data, the zone classifier helps the request classifier
to extract fairly general n-gram features.
Interestingly, although lexical features are very
important, the top three features ranked by Informa-
tion Gain are non-lexical: message length in words,
the number of non-alpha-numeric characters in the
message and the number of capitalised words in the
message.
Word Unigrams Word Bigrams
Word 1 Word 2
please ? end-sentence
? pronoun-object know
pronoun-object let pronoun-object
if start-sentence please
pronoun-subject if pronoun-subject
let start-sentence thanks
to please let
know pronoun-subject have
thanks thanks comma
do start date
Table 5: Top 10 useful n-grams for our request classifier
with zoning, ranked by Information Gain
6.2 Learning Curves
Figure 1 shows a plot of accuracy, precision and
recall versus the number of training instances
used to build the request classifier. These re-
sults are calculated over zoned email bodies, us-
ing the average across ten iterations of stratified
10-fold cross-validation for each different sized
set of training instances, implemented via the
FilteredClassifier with the Resample fil-
ter in Weka. Given our pool of 505 agreed mes-
sage annotations, we plot the recall and precision for
training instance sets of size 50 to 505 messages.
There is a clear trend of increasing performance
as the training set size grows. It seems reasonable to
assume that more data should continue to facilitate
better request classifier performance. To this end,
we are annotating more data as part of our current
and future work.
6.3 Error Analysis
To explore the errors made by our request classifier,
we examined the output of our zoning request clas-
sifier using our full feature set, including all word
n-grams.
Approximately 20% of errors relate to requests
that are implicit, and thus more difficult to detect
from surface features. Another 10% of errors are
due to attempts to classify requests in inappropri-
ate genres of email messages. In particular, both
marketing messages and spam frequently include
request-like, directive utterances which our annota-
tors all agreed would not be useful to mark as re-
990
Figure 1: Learning curve showing recall, accuracy and
precision versus the number of training instances
quests for an email user. Not unreasonably, our clas-
sifier is sometimes confused by the content of these
messages, mistakenly marking requests where our
annotators did not. We intend to resolve these classi-
fication errors by filtering out such messages before
we apply the request classifier.
Another 5% of errors are due to request content
occurring in zones that we ignore. The most com-
mon case is content in a forwarded zone. Sometimes
email senders forward a message as a form of task
delegation; because we ignore forwarded content,
our request classifier misses such requests. We did
experiment with including content from forwarded
zones (in addition to the author, greeting and sig-
noff zones), but found that this reduced the perfor-
mance of our request classifier, presumably due to
the additional noise from irrelevant content in other
forwarded material. Forwarded messages are thus
somewhat difficult to deal with. One possible ap-
proach would be to build sender-specific profiles that
might allow us to deal with forwarded content (and
potentially content from other zones) differently for
different users, essentially learning to adapt to the
different styles of different email users.
A further 5% of errors involve errors in the zone
classifier, which leads to incorrect zone labels be-
ing applied to zone content that we would wish to
include for our request classifier. Examples include
author content being mistakenly identified as signa-
ture content. In such cases, we incorrectly remove
relevant content from the body text that is passed
to our request classifier. Improvements to the zone
classifier would resolve these issues.
As part of our annotation task, we also asked
coders to mark the presence of pleasantries. We
define a pleasantry as an utterance that could be a
request in some other context, but that does not func-
tion as a request in the context of use under consid-
eration. Pleasantries are frequently formulaic, and
do not place any significant obligation on the recip-
ient to act or respond. Variations on the phrase Let
me know if you have any questions are particularly
common in email messages. The context of the en-
tire email message needs to be considered to distin-
guish between when such an utterance functions as
a request and when it should be marked as a pleas-
antry. Of the errors made by our request classifier,
approximately 5% involve marking messages con-
taining only pleasantries as containing a request.
The remaining errors are somewhat diverse.
Close to 5% involve errors interpreting requests as-
sociated with attached files. The balance of almost
50% of errors involve a wide range of issues, from
misspellings of key words such as please to a lack
of punctuation cues such as question marks.
7 Conclusion
Request classification, like any form of automated
speech act recognition, is a difficult task. Despite
this inherent difficulty, the automatic request clas-
sifier we describe in this paper correctly labels re-
quests at the message level in 83.76% of email mes-
sages from our annotated dataset. Unlike previous
work that has attempted to automate the classifi-
cation of requests in email, we zone the messages
without manual intervention. This improves accu-
racy by 15.9% relative to the performance of the
same request classifier without the assistance of an
email zone classifier to focus on relevant message
parts. Although some zone classification errors are
made, error analysis reveals that only 5% of errors
are due to zone misclassification of message parts.
This suggests that, although zone classifier perfor-
mance could be further improved, it is likely that
focusing on improving the request classifier using
the existing zone classifier performance will lead to
greater performance gains.
991
References
Dawn Archer, Jonathan Culpeper, and Matthew Davies,
2008. Corpus Linguistics: An International Hand-
book, chapter Pragmatic Annotation, pages 613?642.
Mouton de Gruyter.
John L Austin. 1962. How to do things with words. Har-
vard University Press.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with annotation noise. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th IJCNLP, pages 280?287, Singapore.
Victoria Bellotti, Nicolas Ducheneaut, Mark Howard,
and Ian Smith. 2003. Taking email to task: The
design and evaluation of a task management centred
email tool. In Computer Human Interaction Confer-
ence, CHI, pages 345?352, Ft Lauderdale, Florida.
Vitor R Carvalho and William W Cohen. 2004. Learning
to extract signature reply lines from email. In Pro-
ceedings of First Conference on Email and Anti-Spam
(CEAS), Mountain View, CA, July 30-31.
Vitor R. Carvalho and William W. Cohen. 2006. Improv-
ing email speech act analysis via n-gram selection. In
Proceedings of HLT/NAACL 2006 - Workshop on Ana-
lyzing Conversations in Text and Speech, pages 35?41,
New York.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Conference on Empirical Meth-
ods in Natural Language Processing, pages 309?316,
Barcelona, Spain.
Simon H. Corston-Oliver, Eric Ringger, Michael Gamon,
and Richard Campbell. 2004. Task-focused summa-
rization of email. In ACL-04 Workshop: Text Summa-
rization Branches Out, pages 43?50.
Nicolas Ducheneaut and Victoria Bellotti. 2001. E-mail
as habitat: an exploration of embedded personal infor-
mation management. Interactions, 8(5):30?38.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 263?272, Melbourne, Australia.
Jade Goldstein and Roberta Evans Sabin. 2006. Using
speech acts to categorize email and identify email gen-
res. In Proceedings of the 39th Hawaii International
Conference on System Sciences, page 50b.
Lewis Hassell and Margaret Christensen. 1996. Indi-
rect speech acts and their use in three channels of com-
munication. In Proceedings of the First International
Workshop on Communication Modeling - The Lan-
guage/Action Perspective, Tilburg, The Netherlands.
Hamid Khosravi and Yorick Wilks. 1999. Routing email
automatically by purpose not topic. Journal of Natural
Language Engineering, 5:237?250.
Bryan Klimt and Yiming Yang. 2004. Introducing the
Enron corpus. In Proceedings of the Conference on
Email and Anti-Spam (CEAS).
Andrew Lampert, Robert Dale, and Ce?cile Paris. 2008a.
The nature of requests and commitments in email mes-
sages. In Proceedings of EMAIL-08: the AAAI Work-
shop on Enhanced Messaging, pages 42?47, Chicago.
Andrew Lampert, Robert Dal e, and Ce?cile Paris. 2008b.
Requests and commitments in email are more complex
than you think: Eight reasons to be cautious. In Pro-
ceedings of Australasian Language Technology Work-
shop (ALTA2008), pages 55?63, Hobart, Australia.
Andrew Lampert, Robert Dale, and Ce?cile Paris. 2009.
Segmenting email message text into zones. In Pro-
ceedings of Empirical Methods in Natural Language
Processing, pages 919?928, Singapore.
Wendy E. Mackay. 1988. More than just a communica-
tion system: Diversity in the use of electronic mail. In
ACM conference on Computer-supported cooperative
work, pages 344?353, Portland, Oregon, USA.
Denise E. Murray. 1991. Conversation for Action: The
Computer Terminal As Medium of Communication.
John Benjamins Publishing Co.
Scott S L Piao, Andrew Wilson, and Tony McEnery.
2002. A multilingual corpus toolkit. In Proceedings
of 4th North American Symposium on Corpus Linguis-
tics, Indianapolis.
Jerry M. Sadock and Arnold Zwicky, 1985. Language
Typology and Syntactic Description. Vol.I Clause
Structure, chapter Speech act distinctions in syntax,
pages 155?96. Cambridge University Press.
Simon Scerri, Myriam Mencke, Brian David, and
Siegfried Handschuh. 2008. Evaluating the ontology
powering smail ? a conceptual framework for seman-
tic email. In Proceedings of the 6th LREC Conference,
Marrakech, Morocco.
John R. Searle. 1969. Speech Acts: An Essay in the
Philosophy of Language. Cambridge University Press.
John Sinclair and Richard Malcolm Coulthard. 1975. To-
wards and Analysis of Discourse - The English used by
Teachers and Pupils. Oxford University Press.
Steve Whittaker and Candace Sidner. 1996. Email over-
load: exploring personal information management of
email. In ACM Computer Human Interaction confer-
ence, pages 276?283. ACM Press.
Terry Winograd and Fernando Flores. 1986. Under-
standing Computers and Cognition. Ablex Publishing
Corporation, Norwood, New Jersey, USA, 1st edition.
ISBN: 0-89391-050-3.
Ian Witten and Eiba Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
992
Proceedings of the Fourth International Natural Language Generation Conference, pages 125?126,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Introduction to the INLG?06 Special Session on
Sharing Data and Comparative Evaluation
The idea for this special session had its origins in discussions with many different members of
the NLG community at the 2005 Workshop on Using Corpora for Natural Language Generation
(UCNLG?05, held in conjunction with the Corpus Linguistics 2005 conference at the Univer-
sity of Birmingham in July 2005), and subsequently at the 10th European Natural Language
Generation Workshop (ENLG?05, held at the University of Aberdeen in August 2005). At the
latter event, the excitement about introducing shared tasks was infectious: the topic hijacked
several of the organised discussion groups, it was the focus of conversation at many tables dur-
ing lunch-breaks, and even the end of the conference didn?t put a stop to it, with discussions
carrying right on until the taxis to the airport arrived.
There was some common ground: nobody said it wouldn?t be a good idea to be able to
directly compare different NLG techniques, most people even seemed to agree that sharable
data and tasks were the way to go. But opinion was sharply divided about how it was to be
achieved. There were?with only a small degree of caricature?two main camps: the bulls
argued for a suck-it-and-see approach, for throwing a task at the research community and then
sitting back to see what would happen; the bears warned that using one data set was not a good
idea until there was community buy-in to a particular data set and a particular task specification
over that data set.
Some of the bears were worried that NLG would inevitably emulate MT and end up with a
single task, fixed inputs and gold-standard outputs, using a single automatic metric of similarity
to assess the quality of generated texts against the gold standard, and moreover, require millions
of dollars in direct funding. It would be impossible to decide what the inputs to the task should
look like, because after all, as Yorick Wilks had pointed out years before, determining the
inputs to NLG was like counting back from infinity to 1 (in contrast to NLU, which, being more
akin to counting from 1 to infinity, seemed at least a little more manageable). The community
would either become hopelessly mired in the task of trying to agree on an input type and task,
or else agree one by dictat and alienate the majority of researchers. Finally, the field would
become obsessed with the single task and the scores produced by the single metric, and all true
scientific enquiry would be stifled.
The bulls envisaged an entirely different future, where many different tasks and bench-
mark datasets co-existed peacefully, where some tasks did have associated inputs and outputs,
but others had more abstract system specifications. NLG was not inherently different from
NLU at all, in fact the output representations used in the latter were just as much there by
gentle(wo)man?s agreement as any common inputs to NLG would be. The strong NLG tradi-
tions of user-oriented and task-based evaluations using human evaluators would be part of the
evaluation paradigm in shared-task evaluations, while parallel research might look at?but not
impose?bespoke automatic methods for NLG. Money would be needed for data resource cre-
ation, but not necessarily for anything else; evidence that this was possible could be found in
successful and vibrant shared-task initiatives run on a shoe-string, such as CoNLL and SENSE-
VAL. The community would create its own forum for reviewing, updating and adding tasks and
evaluation methods. NLG would be invigorated, great scientific progress would result, com-
mercial deployment of NLG technology and regular papers in Computational Linguistics and
ACL proceedings would surely follow.
One thing was clear: opinions abounded, most of them strong ones. Shared-task evaluation
had been firmly put on the NLG agenda. So, we thought, what better than to create a larger,
125
more enduring forum for continuing discussions, in the shape of an INLG special session? We
are pleased to say that the response from the NLG community has been very positive, and that
the papers in this section of the proceedings and the oral presentations at the special session
itself represent both the bullish and the bearish camps. Belz and Kilgarriff present a generally
bullish, but occasionally bearish, history of shared-task initiatives in NLP, and the lessons that
NLG might learn from it, while Reiter and Belz present a proposal for a series of shared tasks
in data-to-text NLG. Van Deemter et al look at the generation of referring expressions and
propose a method for eliciting reference texts for evaluation of GRE algorithms. Paris argues for
NLG system evaluation practices similar to the ISO standards for software evaluation, including
criteria such as flexibility, portability and maintainability.
Among the oral presentations, Scott and Moore exemplify the bearish position but do ar-
gue in favour of a standardised architecture and interface specifications to eventually enable
cross-system comparison. Horacek considers the input problem and advocates the gradual and
collective development of a generic ?generation specification? formalism. Varges recommends
that NLG deliberately take a different route from NLU and encourage a diversity of tasks and
representations.
Kathy McKeown?s invited talk is perfectly poised between the two camps: from her expe-
rience with DUC, TREC and GALE, she concludes that every evaluation programme must expect
to have to weather a stormy period of initial disagreement and even hostility, before eventually
reaching calmer waters where growing agreement and acceptance enable the true benefits of
the programme to take effect.
Consensus-spotters will be able to identify several areas of interest: certainly nobody wants
to follow the example of MT and parsing, and become beholden to a single metric and auto-
mated gold-standard evaluation; some degree of standardisation in sub-tasks and representa-
tions is desirable, but should evolve over time; and perhaps most unanimously, the diversity of
current NLG research with its many different tasks and interests must be preserved.
Even a small amount of common ground can be enough for debate to flourish and consensus
to grow. We hope that the snapshot of opinion presented at this special session will be the
beginning of a long history of comparative evaluation in NLG.
Anja Belz and Robert Dale (Organisers; one bull and one bear)
126
The Use of Spatial Relations in Referring Expression Generation
Jette Viethen
Centre for Language Technology
Macquarie University
Sydney, Australia
jviethen@ics.mq.edu.au
Robert Dale
Centre for Language Technology
Macquarie University
Sydney, Australia
rdale@ics.mq.edu.au
Abstract
There is a prevailing assumption in the litera-
ture on referring expression generation that re-
lations are used in descriptions only ?as a last
resort?, typically on the basis that including
the second entity in the relation introduces an
additional cognitive load for either speaker or
hearer. In this paper, we describe an experiemt
that attempts to test this assumption; we de-
termine that, even in simple scenes where the
use of relations is not strictly required in order
to identify an entity, relations are in fact often
used. We draw some conclusions as to what
this means for the development of algorithms
for the generation of referring expressions.
1 Introduction
In recent years, researchers working on referring
expression generation have increasingly moved to-
wards collecting their own data on the human pro-
duction of referring expressions (REs) (Krahmer and
Theune, 2002; van der Sluis and Krahmer, 2004;
Gatt and van Deemter, 2006; Belz and Varges,
2007); and the recent Attribute Selection in the
Generation of Referring Expressions (ASGRE) Chal-
lenge used the TUNA corpus (Gatt et al, 2007),
which is the most extensive collection of referring
expressions to date. While there is a substantial
body of experimental work in psycholinguistics that
looks at the human production of referring expres-
sions (see, amongst more recent work, (Clark and
Wilkes-Gibbs, 1986; Stevenson, 2002; Haywood et
al., 2003; Jordan and Walker, 2005)) the large range
of factors that play a role in language production
mean that it is often the case that the specific ques-
tion that one is interested in has not been studied
before. So, NLG researchers have tended towards
data gathering exercises that explore some specific
aspect of referring expression generation, focussing
on hypotheses relevant to algorithm development.
This paper is in the same mold. We are particuarly
interested in how people use spatial relations in re-
ferring expressions, and so in this paper we describe
an experiment that explores the generation of rela-
tional referring expressions in a simple scene. Sec-
tion 2 elaborates on our reasons for exploring this
aspect of reference. Section 3 describes the exper-
iment and provides some discussion of the results:
our primary conclusion is that the assumption in the
literature that relations are used ?as a last resort? does
not appear to hold; relations are often used, even in
simple scenes, when they are not strictly required,
and it is likely that they would be more heavily used
in more complex real-world scenes. We conclude
in Section 4 with some observations as to how the
results presented here might impact on the develop-
ment of algorithms for referring expression genera-
tion, and outline some future work.
2 Spatial Relations in Referring
Expression Generation
The bulk of the existing literature on referring ex-
pression generation (see, for example, Dale (1989),
Dale and Reiter (1995), van Deemter (2006), Ho-
racek (2004), Gatt and van Deemter (2006)) gener-
ally focuses on the use of non-relational properties,
which can either be absolute (for example, colour) or
relative (for example, size). We are interested in the
59
use of relational expressions, and in particular the
use of spatial relations; the contexts of use we are
interested in are task-specific, where, for example,
we might want an omniscient domestic agent to tell
us where we have placed a lost object (You left your
keys under the folder on the desk . . . ), or to identify
a hearer-new object in a cluttered scene (the maga-
zine at the bottom of the pile of papers next to the
lampshade in the corner). To develop agents with
these kinds of referential capabilities, we want to ac-
quire data that will inform the development of algo-
rithms, either by automatically checking their ability
to replicate the corpus, or as a baseline for assessing
the performance of humans in an identification task
based on the output of these algorithms.
In this paper, we describe an experiment that
looks at how and when people use spatial relations
in a simple scene. More specifically, we aim to ex-
plore the hypothesis that relations are always dispre-
ferred over non-relational properties. This hypothe-
sis appears to underly most approaches to referring
expression generation that handle relations:
Gardent (2002) adopts a constraint based ap-
proach to deal with relations specifically geared at
generating referring expressions that are as short as
possible. As including a relation in a referring ex-
pression always entails the additional mention of
at least a head noun for the related object, this ap-
proach inherently prefers properties over relations.
Krahmer and Theune (2002) extend the Incremen-
tal Algorithm (IA; Dale and Reiter (1995)) to han-
dle relations. This requires a preference list over all
properties and relations to be specified in advance.
They explicitly choose to put spatial relations right
at the end of that preference list, on the basis that ?It
seems an acceptable assumption that people prefer
to describe an object in terms of simple properties,
and only shift to relations when properties do not
suffice [. . . ] it takes less effort to consider and de-
scribe only one object?.
As the referents in Varges? 2005 domain are all
points on a map distinguishable only by their spatial
relations to other objects, he has no choice but to use
relations. However, he also adopts brevity as a main
criterion for choosing which spatial relations to use.
Kelleher and Kruijff (2005, 2006) cite Clark and
Wilkes-Gibbs? (1986) Principle of Minimal Cooper-
ative Effort and Dale and Reiter?s (1995) Principle
of Sensitivity, as well as van der Sluis and Krah-
mer?s (2004) production study, to motivate the or-
dering over the types of properties that can be used
by their system; accordingly, their system only in-
cludes spatial (and hence relational) information in
a referring expression if it is not possible to construct
a description from non-relational properties.
These approaches would appear to favour the pro-
duction of referring expressions containing long se-
quences of non-relational properties when a single
relational property might do the job. We are inter-
ested, then, in whether it really is the case that rela-
tional expressions are dispreferred, and in determin-
ing when they might in fact be preferred.
To date, we are not aware of any substantial data
sets that would allow this question to be explored.
Both the TUNA corpus (Gatt et al, 2007) and the
Macquarie Drawer data (Viethen and Dale, 2006)
contain too few relational descriptions to allow us
to draw conclusions about any kind of patterns; the
GREC corpus (Belz and Varges, 2007) is not con-
cerned with content selection at all, but rather stud-
ies the form of referring expressions used over a
whole text; i.e. the choice between fully descriptive
NPs, reduced NPs, one-anaphora and pronouns.
There are a number of corpora resulting from ex-
periments involving human participants which con-
tain referring expressions, such as Brennan and
Clark?s (1996) collection of tangram descriptions,
the HCRC Map Task Corpus (Thompson et al,
1993), the COCONUT corpus (Jordan and Walker,
2005), and Byron and Fosler-Lussier?s (2006) OSU
Quake corpus. However, these contain whole
conversations between communicative partners co-
operating on a task, making it difficult to factor out
the impact of prior discourse context on the referring
expressions used.
3 The Data Gathering Experiment
3.1 General overview
We conducted a web-based production experiment
to elicit referring expressions describing singular ob-
jects in very simple scenes. The study was aimed
at shedding light on the question of whether spatial
relations are indeed as dispreferred as suggested by
the literature in those situations where non-relational
descriptions are possible.
60
The Desiderata section of the report from the
Workshop on Shared Tasks and Comparative Eval-
uation in NLG (Paris et al, 2007) emphasises the
difficulties inherent in evaluating NLG systems due
to the context dependency of language production:
the output appropriate for any given referring ex-
pression generation system entirely depends on the
particular task being performed. The data gathered
in this experiment is intended to inform the develop-
ment and evaluation of algorithms for the production
of one-shot, fully distinguishing descriptions of sim-
ple objects in 3-dimensional scenes. The experiment
is focussed on the adequate use of spatial relations in
referring expressions.
In designing the materials for the experiment, we
were conscious of a number of factors which we
might expect to have an influence on the use of spa-
tial relations: the prominence of other properties
such as colour and size (i.e. whether most objects
are of the same or similar size and colour, so that
none are very distinct from the point of view of their
direct properties); how easy it is to distinguish the
target from the other objects around it; how easy it
is to identify the target in the scene without using
any locational information; and the visual salience
of other objects which could serve as relatees in re-
lational descriptions.
3.2 Method
3.2.1 Participants
In total, 74 participants completed the experiment.
They were recruited by emailing 120 native English
speakers and asking them to pass on the call for par-
ticipation to other native or fluent English speakers.
This resulted in a range of participants from a wide
variety of backgrounds and age groups; most partic-
ipants were in their early or mid twenties.
One participant indicated they were colour-blind,
and another requested that their data be discarded.
The data for a further nine participants was ex-
cluded from the analysis for reasons outlined in Sec-
tion 3.2.4 below. Of the remaining 63 participants,
34 were male and 29 were female.
3.2.2 Materials
The stimuli for this study consisted of 20 jpeg
images of simple scenes generated using Google
SketchUp. Each scene contained three objects; each
Figure 1: Trial Set 1: The five base configurations 1?5
and their counterparts 6?10 using the other type of target?
landmark relation and orientation.
object was either a sphere or a cube. The objects
could also be either large or small and were one of
two colours; scenes either contained blue and green
objects, or red and yellow objects. The target ob-
ject, to be described by the participant, was marked
by a grey arrow pointing to it; the target was al-
ways located either directly in front of or on top of
one of the other two objects. We will refer to this
other related object as the landmark, although there
is of course no guarantee that participants actually
included it into the description as the ground object
in a spatial relation. The third object, which we refer
to as the distractor, was located either to the left or
the right of the target and landmark objects.
The 20 scenes are generated from five base con-
figurations, differing in the type and size of the ob-
jects pictured. Figure 1 shows the five base configu-
rations. They can be categorised by the length of the
shortest possible description for the target object:
? in two of the base configurations it is possible
to identify the target object using its type only;
? in one base configuration size alone would suf-
61
fice, although in line with past observations in
the literature we would expect that type is al-
ways included as well;
? in one base configuration, colour and type are
both necessary; and
? in the final base configuration, both size and
colour are necessary, and again we would ex-
pect type to be included.
Importantly, there is no configuration in which the
spatial relations between the objects are required in
order to identify the target.
For each base configuration, we generated two
scenes: in one scene, the target is located on top of
the landmark object, and in the other, the target lies
in front of the landmark. This allows us to investi-
gate whether people prefer to use one type of spatial
relation more than the other.
Five of the resulting 10 scenes were in the blue?
green colour scheme, while the other five used red
and yellow. The different colour schemes were an
attempt to decrease the monotony of the task, so
that we could show each participant more scenes.
These 10 scenes, numbered 1 through 10, consti-
tuted our first trial set. A second trial set, with scenes
numbered 11 through 20, was generated by produc-
ing the mirror image of each scene and using the
opposite colour scheme. Mirroring the scenes had
the same purpose as using the two different colour
schemes. However, to be able to control any un-
wanted effect of these two variables we always used
both variants.1
3.2.3 Procedure
On the experiment website, each participant was
shown the scenes from one of the two trial sets in
the order of the scene numbers. Under each scene,
they had to complete the sentence Please, pick up
the . . . as if they were describing the object marked
by the arrow to an onlooker.
To encourage the use of fully distinguishing refer-
ring expressions, participants were told that they had
only one chance at describing the object. They were
shown a sample scene for which they could provide
an unrecorded (and unchecked) description. After
1For brevity, where relevant we will use the form ?Scenes
n+m? to refer to paired scenes across the two trial sets.
being presented with all ten scenes in the trial, par-
ticipants were asked to complete an exit question-
naire, which also gave them the option of having
their data discarded, and asked for their opinion on
whether the task became easier over time, and any
other comments they might wish to make.
3.2.4 Data Processing
740 descriptions were elicited in the experiment. 10
of these were discarded in line with the participant?s
request, and 10 because the participant reported that
they were colour-blind. After the data was cleaned
and parsed, another 90 descriptions from 9 partici-
pants were discarded:
? One participant had consistently produced ex-
tremely long and complex descriptions using
the ternary relation between and direct refer-
ence to the onlooker, the ground and parts of
the objects: a typical example is the red cube
which rests on the ground and is between you
and the yellow cube of equal size. While these
descriptions are interesting, in relation to the
rest of the data they are such outliers that no
real conclusions can be drawn from them.
? A further eight participants consistently used
highly under-specified descriptions. We de-
cided to discard the data from these participants
since it seemed that they had not understood
the need to provide a distinguishing descrip-
tion, rather than, for example, just indicating
the type of the object.2
This resulted in a total of 630 referring expressions,
with 30 for each scene in Trial Set 1 and 33 for each
scene in Trial Set 2. We then applied some normal-
isation steps: the data was stripped of punctuation
marks and other extraneous material (such as repe-
tition of the Please, pick up the); in four cases, the
dynamic spatial preposition from was deleted from
descriptions such as the green ball from on top of
the blue cube;3 and spelling was normalised. The
2Of course, underspecified descriptions are justified in many
circumstances, and in real-life situations may even be necessary.
However, the simple scenes used in this study do not fall into
these classes.
3We are only interested in the static locative in these expres-
sions; the use of the dynamic preposition is most likely due to
the movement implied by the indicated picking-up action.
62
Figure 2: Number of participants who delivered
n (0. . . 10) relational descriptions.
second object was stripped from comparatives such
as the smaller of the two green cubes and converted
to the form the smaller green cube, which in the con-
text of our simple scenes is semantically equivalent.
3.3 Results
Over a third (231 or 36.6%) of the 630 descriptions
in the resulting corpus use spatial relations despite
the fact that relations were never necessary for the
identification of the target. These 231 relational de-
scriptions were produced by 40 (63.5%) of the 63
participants, while 23 (36.5%) of the participants
never used spatial relations. This suggests that the
use of relations is very much dependent on personal
preference, a hypothesis that is further supported by
the fact that 11 (i.e. over 25%) of the relation-using
participants did so in all 10 referring expressions
they delivered. Figure 2 shows the number of par-
ticipants who produced exactly n descriptions con-
taining at least one spatial relation, for n in the range
{0 . . . 10}.
From the above, we might hypothesise that some
participants adopt a strategy of always using rela-
tional properties, and that others adopt a strategy of
avoiding relational properties as much as possible.
We further analysed the descriptions produced by
participants who did not follow either of these two
exclusive strategies to see how their choices varied
across the different scenes; the spread is shown in
Figure 3. Looking only at the descriptions produced
by participants who sometimes, but not always, used
spatial relations allows us to get a clearer view on
which objects received most and least relational de-
scriptions. This in turn affords an analysis of the
impact the different features in the respective scenes
Figure 3: % of relational descriptions for each scene out
of all relational descriptions produced by participants not
using an exclusive strategy. Scenes are paired with their
counterpart using the other target?landmark relation.
have on the use of spatial relations.
41.7% of the remaining descriptions used rela-
tions. Interestingly, 63.6% of these relational de-
scriptions were used for scenes where the target was
located on top of the landmark object, while only
36.4% were from scenes where the target was in
front of the landmark, suggesting that the use of the
in-front-of relation may be relatively dispreferred.
Because the first scene always had the target on
top of the landmark, this preference for using rela-
tional descriptions in on-top-of scenes might be due
to a training effect that discourages people from us-
ing relations over time. However, if we do not take
into account descriptions for the first 4 scenes of
each trial set, this ratio is still large: 58.8% of the
the remaining relational descriptions stemmed from
scenes where the target was on top of the landmark,
41.2% of them from scenes with an on-top-of rela-
tion.
As expected, the orientation of the scenes and the
colour scheme used did not have a significant im-
pact on the use of spatial relations. For both these
variables, the difference between values in use of re-
lations was under 6 percentage points.
3.4 Discussion
We noted earlier that existing relation-handling re-
ferring expression generation algorithms generally
disprefer relations and only add them to a descrip-
tion if absolutely necessary. This in essence mimics
the behaviour of our participants who adopted the
63
exclusive Never-Use-Relations strategy.4 These al-
gorithms therefore only represent slightly more than
one third of the participants in our study.
The analysis of the descriptions given by people
who did not follow one of the two exclusive strate-
gies indicates that the distribution of relational de-
scriptions over the scenes is not random. In addition
to modelling exclusive strategies, then, we may also
want to capture in an algorithm the reasons why re-
ferring expressions for some scenes are more likely
to include spatial relations than others.
In the remainder of this section we consider the
conclusions that can be drawn from our data regard-
ing the factors that impact on the choice of whether
to use spatial relations in a referring expression.
Spatial Relations Are Used Even When Unnec-
essary: The main observation that can be made
is that even in very simple scenes, where locatives
are not necessary to distinguish the target from the
other objects present, people show a tendency to use
spatial relations to describe a target object to an on-
looker. This contradicts the prevailing approach to
the use of relations in referring expression genera-
tion. It is important to bear in mind that the scenes
used in this study were extremely simple and could
easily be taken in at one glance; it seems likely that
when faced with a more crowded scene containing
more complex objects, the tendency to incorporate
possibly unnecessary spatial relations into descrip-
tions would increase.
Training Effect: Note in Figure 4 that the targets
in Scenes 1+11 received a disproportionally high
number of descriptions containing spatial relations.
While this fact could be attributed to the similar fea-
tures of the two scenes (they only differed in ori-
entation and colour scheme), it is much more likely
that this is due to Scenes 1 and 11 being the first
scenes of the respective trial sets. The drop-off in
relational descriptions from beginning to end of the
trial sets almost certainly results from a training ef-
fect, where people realised over time that relations
were not necessary in any of the scenes. If we only
consider the first two scenes in each trial set, where
no training effect has taken hold, we find that 36 of
4On the assumption that these participants would also resort
to relations if they had to.
Figure 4: % of relational descriptions for each scene out
of all relational descriptions produced by participants not
using an exclusive strategy. Scenes are paired with their
counterpart from the other trial set.
the 58 (62.1%) descriptions for these scenes use spa-
tial relations. The presence of some kind of training
effect was also reported in the exit questionnaire by
half of the participants.
This training effect in itself is an interesting phe-
nomenon. It suggests that people are much more
likely to use spatial relations when they come anew
to the task of identifying an object rather than when
they are describing an object in a similar domain on
a subsequent occasion.
Landmark Salience Encourages Use of Relations:
Figure 4 shows that the highest spike in usage of spa-
tial relations was recorded for Scenes 3+13; interest-
ingly another, although much less pronounced, peak
occurs for their counterpart scenes only differing in
the type of target?landmark relation, 8+18.
These peaks cannot be explained by the training
effect; in fact, they seem to be running contrary to it,
indicating that some other reasonably strong factors
are prompting the use of relations in these scenes.
Scenes 3, 8, 13, and 18 are the only scenes in
which the landmark object is distinguishable from
both other objects only by its type (cube) or its
colour (see Figure 1). In addition, in each case the
landmark is large resulting in high visual salience for
the landmark. This in turn makes the relation to the
landmark a salient feature of the target. The salience
of the relation then causes people to add it to an al-
ready distinguishing description or even to prefer it
64
over the use of absolute properties.
on top of Is Preferred over in front of: Although
these four scenes all share the same base set of ob-
jects, the usage of spatial relations is considerably
higher for Scenes 3+13 than for 8+18. This could
either be entirely due to the training effect, but may
also be influenced by the only difference between
these two scenes: in Scenes 3+13, the target sits on
top of the landmark, while in Scenes 8+18 it is ly-
ing in front of the landmark. The overall compari-
son of the data for scenes featuring an on-top-of re-
lation with that for scenes with an in-front-of relation
suggests that this also is a factor. Even if we only
take into account Scenes 5?10 and 15?20, where we
might expect the effect of training to have stabilised,
people were almost one and a half times more likely
to use a relation in a scene where the target was on
top of rather than in front of the landmark (30 vs. 21
of the 111 relational descriptions for those scenes
from people not using an exclusive strategy).
This finding is in accordance with Kelleher and
Kruijff?s (2006) approach of preferring topological
spatial relations over projective ones. The seman-
tics of projective spatial relations, such as in front
of, depend on a frame of reference defining direc-
tions from some origin (in this case the landmark
object), while topological relations, such as on top
of, are semantically defined by relations such as in-
tersection, containedness, and contiguity, and pose
a lighter cognitive load on both discourse partners
(see Tenbrink (2005) for an overview).
The impact of landmark salience and the pref-
erence for the on-top-of relation can also explain
the low use of spatial relations for Scenes 4+14,
6+16 and 10+20 (see Figure 1). In these scenes it
is very hard or even impossible to distinguish the
landmark from the other objects using only non-
relational properties, and the target is located in front
of rather than on top of it. The possibility of de-
scribing the target in Scenes 6+16 only by its type
or colour may be the reason for the extremely low
usage of spatial relations in these scenes.
4 Conclusions
4.1 Consequences for Algorithm Development
We noted above that some participants adopted a
Never-Use-Relations strategy, and some adopted
an Always-Use-Relations stratgy. This might be
modelled by the use of a parameter akin to the
the Risky/Cautious distinction proposed by Carletta
(1992) in her work on references in the Map Task
corpus. The effect of this parameter in the context
of the Incremental Algorithm would be to put spa-
tial relations either at the front or at the end of the
preference list of properties; this would ensure that
they are either considered first for inclusion into a
referring expression, or only when the other proper-
ties of the target do not suffice.
A more interesting problem is how to model the
apparent preference of our participants to use rela-
tions in some scenes more than in others. Follow-
ing our discussion above, the factors that lead to this
preference seem to include the folllowing:
? the ease with which a potential landmark can
be distinguished from the other objects in the
scene;
? the visual salience of a potential landmark (in
our case its size);
? the type of spatial relation between the target
and a potential landmark; and
? the ease with which the target can be described
without the use of spatial relations.
The visual salience of the target object is likely to
also play a role; however, this was not tested in the
current study, since all target objects were small.
Factors like these can be incorporated into a refer-
ring expression generation algorithm by taking them
into account in the step that calculates which prop-
erty of the target object should next be considered
for inclusion in the referring expression. Instead of
using a static preference list over all possible do-
main properties, a preference score for each prop-
erty needs to be determined ?at run time?. Such a
dynamic approach would also allow the considera-
tion of the discourse salience of a property (perhaps
due to its recent use in another referring expression),
as well as the consideration that some properties are
more likely to be used in combination with other
specific properties. An example of this phenomenon
is the combination of the property hair-colour with
either has-hair or has-beard in the TUNA data. If
hair-colour is included in a referring expression, at
65
least one of the other two properties is present as
well.
The preference scores of the properties in a re-
ferring expression under construction would then
combine into an adequacy score for the overall de-
scription, similar to Edmonds? (1994) concept of the
speaker?s confidence that a referring expression suf-
fices for the communicative task at hand.
4.2 Future Work
As a next step, we aim to run experiments to sepa-
rately confirm the impact that each of the different
factors listed in Section 3.4 has on the use of spa-
tial relations in referring expressions. In parallel,
we will evaluate the human-produced descriptions
in task-based evaluation schemes to assess whether
the use of relations in certain categories of scenes is
advantageous for an onlooker trying to identify the
object that is being referred to.
Ultimately, the aim of this research is to develop
an algorithm that incorporates the findings from both
types of studies into the generation of referring ex-
pressions. Such an algorithm should not simply
mimic the behaviour that our participants have dis-
played during the production experiment, but also
take into account the findings of the task-based
study, to ensure both naturalness and usefulness for
the listener.
References
Anja Belz and Sebastian Varges. 2007. Generation of re-
peated references to discourse entities. In Proceedings
of the 11th European Workshop on Natural Language
Generation, pages 9?16.
Susan E. Brennan and Herbert H. Clark. 1996. Concep-
tual pacts and lexical choice in conversation. Journal
of Experimental Psychology: Learning, Memory, and
Cognition, 22:1482?1493.
Donna K. Byron and Eric Fosler-Lussier. 2006.
The OSU Quake 2004 corpus of two-party situated
problem-solving dialogs. In Proceedings of the 15th
Language Resources and Evaluation Conference.
Jean C. Carletta. 1992. Risk-taking and Recovery in
Task-Oriented Dialogue. Ph.D. thesis, University of
Edinburgh.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22(1):1?
39.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions. In
Proceedings of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics, Vancouver, BC.
Philip G. Edmonds. 1994. Collaboration on reference to
objects that are not mutually known. In Proceedings of
the 15th International Conference on Computational
Linguistics, Kyoto, Japan.
Claire Gardent. 2002. Generating minimal definite de-
scriptions. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
Philadelphia, USA.
Albert Gatt and Kees van Deemter. 2006. Conceptual
coherence in the generation of referring expressions.
In Proceedings of the 21st COLING and the 44th ACL
Conference, Sydney, Australia.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation, pages 49?56.
Sarah Haywood, Martin J. Pickering, and Holly P. Brani-
gan. 2003. Co-operation and co-ordination in the
production of noun phrases. In Proceedings of the
25th Annual Meeting of the Cognitive Science Society,
pages 533?538, Boston, MA.
Helmut Horacek. 2004. On referring to sets of objects
naturally. In Proceedings of the 3rd International Con-
ference on Natural Language Generation, pages 70?
79, Brockenhurst, UK.
Pamela W. Jordan and Marilyn A. Walker. 2005. Learn-
ing content selection rules for generating object de-
scriptions in dialogue. Journal of Artificial Intelli-
gence Research, 24:157?194.
John Kelleher and Geert-Jan M. Kruijff. 2005. A
context-dependent model of proximity in physically
situated environments. In Proceedings of the 2nd
ACL-SIGSEM Workshop on The Linguistic Dimen-
sions of Prepositions and their Use in Computational
Linguistics Formalisms and Applications, Colchester,
U.K.
John Kelleher and Geert-Jan M. Kruijff. 2006. Incre-
mental generation of spatial referring expressions in
situated dialog. In Proceedings of the 21st COLING
and the 44th ACL Conference, Sydney, Australia.
Emiel Krahmer and Marie?t Theune. 2002. Efficient
context-sensitive generation of referring expressions.
In Kees van Deemter and Rodger Kibble, editors, In-
formation Sharing: Reference and Presupposition in
Language Generation and Interpretation, pages 223?
264. CSLI Publications, Stanford, CA.
66
Ce?cile Paris, Donia Scott, Nancy Green, Kathy McCoy,
and David McDonald. 2007. Desiderata for evalua-
tion of natural language generation. In Robert Dale
and Michael White, editors, Proceedings of the Work-
shop on Shared Tasks and Comparative Evaluation in
Natural Language Generation, pages 9?15, Arlington,
VA.
Rosemary Stevenson. 2002. The role of salience in the
production of referring expressions: A psycholinguis-
tic perspective. In Kees van Deemter and Rodger Kib-
ble, editors, Information Sharing: Reference and Pre-
supposition in Language Generation and Interpreta-
tion. CSLI, Stanford.
Thora Tenbrink. 2005. Semantics and application of spa-
tial dimensional terms in English and German. Tech-
nical Report Series of the Transregional Collabora-
tive Research Center SFB/TR 8 Spatial Cognition, No.
004-03/2005, Universities of Bremen and Freiburg,
Germany.
Henry S. Thompson, Anne Anderson, Ellen Gurman
Bard, Gwyneth Doherty-Sneddon, Alison Newlands,
and Cathy Sotillo. 1993. The HCRC map task cor-
pus: natural dialogue for speech recognition. In Pro-
ceedings of the 1993 Workshop on Human Language
Technology, pages 25?30, Princeton, New Jersey.
Kees van Deemter. 2006. Generating referring expres-
sions that involve gradable properties. Computational
Linguistics, 32(2):195?222.
Ielka van der Sluis and Emiel Krahmer. 2004. The
influence of target size and distance on the produc-
tion of speech and gesture in multimodal referring
expressions. In Proceedings of the 8th International
Conference on Spoken Language Processing (INTER-
SPEECH 2004), Jeju, Korea.
Sebastian Varges. 2005. Spatial descriptions as referring
expressions in the maptask domain. In Proceedings
of the 10th European Workshop On Natural Language
Generation, Aberdeen, UK.
Jette Viethen and Robert Dale. 2006. Algorithms for
generating referring expressions: Do they do what
people do? In Proceedings of the 4th International
Conference on Natural Language Generation, pages
63?70, Sydney, Australia, July.
67
Report on the Second NLG Challenge on
Generating Instructions in Virtual Environments (GIVE-2)
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Andrew Gargett
Saarland University
gargett@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the second installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE-2), a shared
task for the NLG community which took
place in 2009-10. We evaluated seven
NLG systems by connecting them to 1825
users over the Internet, and report the re-
sults of this evaluation in terms of objec-
tive and subjective measures.
1 Introduction
This paper reports on the methodology and results
of the Second Challenge on Generating Instruc-
tions in Virtual Environments (GIVE-2), which
we ran from August 2009 to May 2010. GIVE
is a shared task for the NLG community which
we ran for the first time in 2008-09 (Koller et al,
2010). An NLG system in this task must generate
instructions which guide a human user in solving
a treasure-hunt task in a virtual 3D world, in real
time. For the evaluation, we connect these NLG
systems to users over the Internet, which makes
it possible to collect large amounts of evaluation
data cheaply.
While the GIVE-1 challenge was a success, in
that it evaluated five NLG systems on data from
1143 game runs in the virtual environments, it
was limited in that users could only move and
turn in discrete steps in the virtual environments.
This made the NLG task easier than intended; one
of the best-performing GIVE-1 systems generated
instructions of the form ?move three steps for-
ward?. The primary change in GIVE-2 compared
to GIVE-1 is that users could now move and turn
freely, which makes expressions like ?three steps?
meaningless, and makes it hard to predict the pre-
cise effect of instructing a user to ?turn left?.
We evaluated seven NLG systems from six in-
stitutions in GIVE-2 over a period of three months
from February to May 2010. During this time,
we collected 1825 games that were played by
users from 39 countries, which is an increase of
over 50% over the data we collected in GIVE-
1. We evaluated each system both on objec-
tive measures (success rate, completion time, etc.)
and subjective measures which were collected by
asking the users to fill in a questionnaire. We
completely revised the questionnaire for the sec-
ond challenge, which now consists of relatively
fine-grained questions that can be combined into
more high-level groups for reporting. We also in-
troduced several new objective measures, includ-
ing the point in the game in which users lost
or cancelled, and an experimental ?back-to-base?
task intended to measure how much users learned
about the virtual world while interacting with the
NLG system.
Plan of the paper. The paper is structured as fol-
lows. In Section 2, we describe and motivate the
GIVE-2 Challenge. In section 3, we describe the
evaluation method and infrastructure. Section 4
reports on the evaluation results. Finally, we con-
clude and discuss future work in Section 5.
2 The GIVE Challenge
GIVE-2 is the second installment of the GIVE
Challenge (?Generating Instructions in Virtual En-
vironments?), which we ran for the first time in
2008-09. In the GIVE scenario, subjects try to
solve a treasure hunt in a virtual 3Dworld that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual world.
The challenge for the NLG system is to gener-
ate, in real time, natural-language instructions that
will guide the users to the successful completion
of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
Figure 1: What the user sees when playing with
the GIVE Challenge.
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
In each game world we used in GIVE-2, players
must pick up a trophy, which is in a wall safe be-
hind a picture. In order to access the trophy, they
must first push a button to move the picture to the
side, and then push another sequence of buttons to
open the safe. One floor tile is alarmed, and play-
ers lose the game if they step on this tile without
deactivating the alarm first. There are also a num-
ber of distractor buttons which either do nothing
when pressed or set off an alarm. These distractor
buttons are intended to make the game harder and,
more importantly, to require appropriate reference
to objects in the game world. Finally, game worlds
contained a number of objects such as chairs and
flowers that did not bear on the task, but were
available for use as landmarks in spatial descrip-
tions generated by the NLG systems.
The crucial difference between this task and
the (very similar) GIVE-1 task was that in GIVE-
2, players could move and turn freely in the vir-
tual world. This is in contrast to GIVE-1, where
players could only turn by 90 degree increments,
and jump forward and backward by discrete steps.
This feature of the way the game controls were set
up made it possible for some systems to do very
well in GIVE-1 with only minimal intelligence,
using exclusively instructions such as ?turn right?
and ?move three steps forward?. Such instructions
are unrealistic ? they could not be carried over to
instruction-giving in the real world ?, and our aim
was to make GIVE harder for systems that relied
on them.
3 Method
Following the approach from the GIVE-1 Chal-
lenge (Koller et al, 2010), we connected the NLG
systems to users over the Internet. In each game
run, one user and one NLG system were paired up,
with the system trying to guide the user to success
in a specific game world.
3.1 Software infrastructure
We adapted the GIVE-1 software to the GIVE-2
setting. The GIVE software infrastructure (Koller
et al, 2009a) consists of three different mod-
ules: The client, which is the program which the
user runs on their machine to interact with the
virtual world (see Fig. 1); a collection of NLG
servers, which generate instructions in real-time
and send them to the client; and a matchmaker,
which chooses a random NLG server and virtual
world for each incoming connection from a client
and stores the game results in a database.
The most visible change compared to GIVE-1
was to modify the client so it permitted free move-
ment in the virtual world. This change further ne-
cessitated a number of modifications to the inter-
nal representation of the world. To support the de-
velopment of virtual worlds for GIVE, we changed
the file format for world descriptions to be much
more readable, and provided an automatic tool
for displaying virtual worlds graphically (see the
screenshots in Fig. 2).
3.2 Recruiting subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the Internet
and in traditional newspapers. We further adver-
tised GIVE at the Cebit computer expo as part of
the Saarland University booth. Recruiting anony-
mous experimental subjects over the Internet car-
ries known risks (Gosling et al, 2004), but we
showed in GIVE-1 that the results obtained for
the GIVE Challenge are comparable and more in-
formative than those obtained from a laboratory-
World 1 World 2 World 3
Figure 2: The three GIVE-2 evaluation worlds.
based experiment (Koller et al, 2009b).
We also tried to leverage social networks for re-
cruiting participants by implementing and adver-
tising a Facebook application. Because of a soft-
ware bug, only about 50 participants could be re-
cruited in this way. Thus tapping the true poten-
tial of social networks for recruiting participants
remains a task for the next installment of GIVE.
3.3 Evaluation worlds
Fig. 2 shows the three virtual worlds we used in the
GIVE-2 evaluation. Overall, the worlds were more
difficult than the worlds used in GIVE-1, where
some NLG-systems had success rates around 80%
in some of the worlds. As for GIVE-1, the three
worlds were designed to pose different challenges
to the NLG systems. World 1 was intended to be
more similar to the development world and last
year?s worlds. It did have rooms with more than
one button of the same color, however, these but-
tons were not located close together. World 2 con-
tained several situations which required more so-
phisticated referring expressions, such as rooms
with several buttons of the same color (some of
them close together) and a grid of buttons. Fi-
nally, World 3 was designed to exercise the sys-
tems? navigation instructions: one room contained
a ?maze? of alarm tiles, and another room two
long rows of buttons hidden in ?booths? so that
they were not all visible at the same time.
3.4 Timeline
After the GIVE-2 Challenge was publicized in
June 2009, fifteen researchers and research teams
declared their interest in participating. We dis-
tributed a first version of the software to these
teams in August 2009. In the end, six teams sub-
mitted NLG systems (two more than in GIVE-1);
one team submitted two independent NLG sys-
tems, bringing the total number of NLG systems
up to seven (two more than in GIVE-1). These
were connected to a central matchmaker that ran
for a bit under three months, from 23 February to
17 May 2010.
3.5 NLG systems
Seven NLG systems were evaluated in GIVE-2:
? one system from the Dublin Institute of Tech-
nology (?D? in the discussion below);
? one system from Trinity College Dublin
(?T?);
? one system from the Universidad Com-
plutense de Madrid (?M?);
? one system from the University of Heidelberg
(?H?);
? one system from Saarland University (?S?);
? and two systems from INRIA Grand-Est in
Nancy (?NA? and ?NM?).
Detailed descriptions of these systems as well
as each team?s own analysis of the evalua-
tion results can be found at http://www.
give-challenge.org/research.
4 Results
We now report the results of GIVE-2. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures. The
data for the objective measures are extracted from
the logs of the interactions; whereas the data for
the subjective measures are obtained from a ques-
tionnaire which asked subjects to rate various as-
pects of the NLG system they interacted with.
Notice that some of our evaluation measures are
in tension with each other: For instance, a sys-
tem which gives very low-level instructions may
allow the user to complete the task more quickly
(there is less chance of user errors), but it will re-
quire more instructions than a system that aggre-
gates these. This is intentional, and emphasizes
our desire to make GIVE a friendly comparative
challenge rather than a competition with a clear
winner.
4.1 Demographics
Over the course of three months, we collected
1825 valid games. This is an increase of almost
60% over the number of valid games we collected
in GIVE-1. A game counted as valid if the game
client did not crash, the game was not marked as a
test game by the developers, and the player com-
pleted the tutorial.
Of these games, 79.0% were played by males
and 9.6% by females; a further 11.4% did not
specify their gender. These numbers are compa-
rable to GIVE-1. About 42% of users connected
from an IP address in Germany; 12% from the US,
8% from France, 6% from Great Britain, and the
rest from 35 further countries. About 91% of the
participants who answered the question self-rated
their English language proficiency as ?good? or
better. About 65% of users connected from vari-
ous versions of Windows, the rest were split about
evenly between Linux and MacOS.
4.2 Objective measures
The objective measures are summarize in Fig. 3.
In addition to calculating the percentage of games
users completed successfully when being guided
by the different systems, we measured the time
until task completion, the distance traveled until
task completion, and the number of actions (such
as pushing a button to open a door) executed. Fur-
thermore, we counted howmany instructions users
received from each system, and how many words
these instructions contained on average. All objec-
tive measures were collected completely unobtru-
sively, without requiring any action on the user?s
part. To ensure comparability, we only counted
successfully completed games.
task success: Did the player get the trophy?
duration: Time in seconds from the end of the tu-
torial until the retrieval of the trophy.
distance: Distance traveled (measured in distance
units of the virtual environment).
actions: Number of object manipulation actions.
instructions: Number of instructions produced
by the NLG system.
words per instruction: Average number of
words the NLG system used per instruction.
Figure 3: Objective measures.
Fig. 4 shows the results of these objective mea-
sures. Task success is reported as the percent-
age of successfully completed games. The other
measures are reported as the mean number of sec-
onds/distance units/actions/instructions/words per
instruction, respectively. The figure also assigns
systems to groups A, B, etc. for each evaluation
measure. For example, users interacting with sys-
tems in group A had a higher task success rate,
needed less time, etc. than users interacting with
systems in group B. If two systems do not share
the same letter, the difference between these two
systems is significant with p < 0.05. Significance
was tested using a ?2-test for task success and
ANOVAs for the other objective measures. These
were followed by post-hoc tests (pairwise ?2 and
Tukey) to compare the NLG systems pairwise.
In terms of task success, the systems fall pretty
neatly into four groups. Note that systems D and
T had very low task success rates. That means
that, for these systems, the results for the other ob-
jective measures may not be reliable because they
are based on just a handful of games. Another
aspect in which systems clearly differed is how
many words they used per instruction. Interest-
ingly, the three systems with the best task success
rates also produced the most succinct instructions.
The distinctions between systems in terms of the
other measures is less clear.
4.3 Subjective measures
The subjective measures were obtained from re-
sponses to a questionnaire that was presented to
users after each game. The questionnaire asked
users to rate different statements about the NLG
D H M NA NM S T
task
success
9% 11% 13% 47% 30% 40% 3%
A A
B
C C C
D D
duration
888 470 407 344 435 467 266
A A A A A
B B B B B
C
distance
231 164 126 162 167 150 89
A A A A A A
B B B B B
actions
25 22 17 17 18 17 14
A A A A A A A
instructions
349 209 463 224 244 244 78
A A A A A A
B B
words per
instruction
15 11 16 6 10 6 18
A A
B
C
D
E E
Figure 4: Results for the objective measures.
system using a continuous slider. The slider posi-
tion was translated to a number between -100 and
100. Figs. 7 and 6 show the statements that users
were asked to rate as well as the results. These
results are based on all games, independent of the
success. We report the mean rating for each item,
and, as before, systems that do not share a letter,
were found to be significantly different (p< 0.05).
We used ANOVAs and post-hoc Tukey tests to test
for significance. Note that some items make a pos-
itive statement about the NLG system (e.g., Q1)
and some make a negative statement (e.g., Q2).
For negative statements, we report the reversed
scores, so that in Figs. 7 and 6 greater numbers are
always better, and systems in group A are always
better than systems in group B.
In addition to the items Q1?Q22, the ques-
tionnaire contained a statement about the over-
all instruction quality: ?Overall, the system gave
me good directions.? Furthermore notice that the
other items fall into two categories: items that as-
sess the quality of the instructions (Q1?Q15) and
items that assess the emotional affect of the in-
teraction (Q16?Q22). The ratings in these cate-
D H M NA NM S T
overall
quality
question
-33 -18 -12 36 18 19 -25
A
B B
C C C C
quality
measures
(summed)
-183 -148 -18 373 239 206 -44
A A A
B B B B
emotional
affect
measures
(summed)
-130 -103 -90 20 -5 0 -88
A A A A
B B B B B
C C C C C
Figure 5: Results for item assessing overall in-
struction quality and the aggregated quality and
emotional affect measures.
gories can be aggregated into just two ratings by
summing over them. Fig. 5 shows the results for
the overall question and the aggregated ratings for
quality measures and emotional affect measures.
The three systems with the highest task success
rate get rated highest for overall instruction qual-
ity. The aggregated quality measure also singles
out the same group of three systems.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, some other factors also influence the out-
comes of our objective and subjective measures.
As in GIVE-1, we find that there is a significant
difference in task success rate for different evalua-
tion worlds and between users with different levels
of English proficiency. Fig. 8 illustrates the effect
of the different evaluation worlds on the task suc-
cess rate for different systems, and Fig. 9 shows
the effect that a player?s English skills have on the
task success rate. As in GIVE-1, some systems
seem to be more robust than others with respect to
changes in these factors.
None of the other factors we looked at (gender,
age, and computer expertise) have a significant ef-
fect on the task success rate. With a few excep-
tions the other objective measures were not influ-
enced by these demographic factors either. How-
ever, we do find a significant effect of age on the
time and number of actions a player needs to re-
trieve the trophy: younger players are faster and
need fewer actions. And we find that women travel
a significantly shorter distance than men on their
way to the trophy. Interestingly, we do not find
D H M NA NM S T
Q1: The system used words and phrases
that were easy to understand.
45 26 41 62 54 58 46
A A A A
B B B B
C C C
Q2: I had to re-read instructions to under-
stand what I needed to do.
-26 -9 3 40 8 19 0
A
B B B B
C C C
D D
Q3: The system gave me useful feedback
about my progress.
-17 -30 -31 9 11 -13 -27
A A
B B B B
C C C C
Q4: I was confused about what to do next.
-35 -27 -18 29 9 5 -31
A
B B
C C C C
Q5: I was confused about which direction
to go in.
-32 -20 -16 21 8 3 -25
A A
B B
C C C C
Q6: I had no difficulty with identifying
the objects the system described for me.
-21 -11 -5 18 13 20 -21
A A A
B B
C C C C
Q7: The system gave me a lot of unnec-
essary information.
-22 -9 6 15 10 10 -6
A A A A
B B B B
C C C
D D D
D H M NA NM S T
Q8: The system gave me too much infor-
mation all at once.
-28 -8 9 31 8 21 15
A A A
B B B B
C C
Q9: The system immediately offered help
when I was in trouble.
-15 -13 -13 32 3 -5 -23
A
B B B B B
C C C C
Q10: The system sent instructions too
late.
15 15 9 38 39 14 8
A A
B B B B B
Q11: The system?s instructions were de-
livered too early.
15 5 21 39 12 30 28
A A A
B B B B
C C C C
D D D D
Q12: The system?s instructions were vis-
ible long enough for me to read them.
-67 -21 -19 6 -14 0 -18
A A
B B B
C C C C
D
Q13: The system?s instructions were
clearly worded.
-20 -9 1 32 23 26 6
A A A
B B B
C C C
D D
Q14: The system?s instructions sounded
robotic.
16 -6 8 -4 -1 5 1
A A A A A A
B B B B B B
Q15: The system?s instructions were
repetitive.
-28 -26 -11 -31 -28 -26 -23
A A A A A
B B B B B B
Figure 7: Results for the subjective measures assessing the quality of the instructions.
D H M NA NM S T
Q16: I really wanted to find that trophy.
-10 -13 -9 -11 -8 -7 -12
A A A A A A A
Q17: I lost track of time while solving the
overall task.
-13 -18 -21 -16 -18 -11 -20
A A A A A A A
Q18: I enjoyed solving the overall task.
-21 -23 -20 -8 -4 -5 -21
A A A A A A
B B B B B
Q19: Interacting with the system was re-
ally annoying.
-14 -20 -12 8 -2 -2 -14
A A A
B B B B B
C C C C
Q20: I would recommend this game to a
friend.
-36 -39 -31 -30 -25 -24 -31
A A A A A A A
Q21: The system was very friendly.
0 -1 5 30 20 19 5
A A A
B B B B
C C C C
D D D D
Q22: I felt I could trust the system?s in-
structions.
-21 -6 -3 37 23 21 -13
A A A
B B B B
Figure 6: Results for the subjective measures as-
sessing the emotional affect of the instructions.
Figure 8: Effect of the evaluation worlds on the
success rate of the NLG systems.
Figure 9: Effect of the players? English skills on
the success rate of the NLG systems.
a significant effect of gender on the time players
need to retrieve the trophy as in GIVE-1 (although
the mean duration is somewhat higher for female
than for male players; 481 vs. 438 seconds).
5 Conclusion
In this paper, we have described the setup and re-
sults of the Second GIVE Challenge. Altogether,
we collected 1825 valid games for seven NLG sys-
tems over a period of three months. Given that this
is a 50% increase over GIVE-1, we feel that this
further justifies our basic experimental methodol-
ogy. As we are writing this, we are preparing de-
tailed results and analyses for each participating
team, which we hope will help them understand
and improve the performance of their systems.
The success rate is substantially worse in GIVE-
2 than in GIVE-1. This is probably due to the
Figure 10: Points at which players lose/cancel.
harder task (free movement) explained in Sec-
tion 2 and to the more complex evaluation worlds
(see Section 3.3). It was our intention to make
GIVE-2 more difficult, although we did not antic-
ipate such a dramatic drop in performance. GIVE-
2.5 next year will use the same task as GIVE-2 and
we hope to see an increase in task success as the
participating research teams learn from this year?s
results.
It is also noticeable that players gave mostly
negative ratings in response to statements about
immersion and engagement (Q16-Q20). We dis-
cussed last year how to make the task more engag-
ing on the one hand and how to manage expecta-
tions on the other hand, but none of the suggested
solutions ended up being implemented. It seems
that we need to revisit this issue.
Another indication that the task may not be able
to capture participants is that the vast majority of
cancelled and lost games end in the very begin-
ning. To analyze at what point players lose or give
up, we divide the game into phases demarcated
by manipulations of buttons that belong to the 6-
button safe sequence. Fig. 10 illustrates in which
phase of the game players lose or cancel.
We are currently preparing the GIVE-2.5 Chal-
lenge, which will take place in 2010-11. GIVE-2.5
will be very similar to GIVE-2, so that GIVE-2
systems will be able to participate with only mi-
nor changes. In order to support the development
of GIVE-2.5 systems, we have collected a multi-
lingual corpus of written English and German in-
structions in the GIVE-2 environment (Gargett et
al., 2010). We expect that GIVE-3 will then extend
the GIVE task substantially, perhaps in the direc-
tion of full dialogue or of multimodal interaction.
Acknowledgments. GIVE-2 was only possible
through the support and hard work of a number of
colleagues, especially Konstantina Garoufi (who
handled the website and other publicity-related is-
sues), Ielka van der Sluis (who contributed to the
design of the GIVE-2 questionnaire), and several
student assistants who programmed parts of the
GIVE-2 system. We thank the press offices of
Saarland University, the University of Edinburgh,
and Macquarie University for their helpful press
releases. We also thank the organizers of Gener-
ation Challenges 2010 and INLG 2010 for their
support and the opportunity to present our results,
and the seven participating research teams for their
contributions.
References
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The GIVE-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), Malta.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009a. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Sara Dalzel-Job, Jo-
hanna Moore, and Jon Oberlander. 2009b. Validat-
ing the web-based evaluation of NLG systems. In
Proceedings of ACL-IJCNLP 2009 (Short Papers),
Singapore.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2010. The first challenge on
generating instructions in virtual environments. In
E. Krahmer and M. Theune, editors, Empirical
Methods in Natural Language Generation, volume
5790 of LNCS, pages 337?361. Springer.
Helping Our Own:
Text Massaging for Computational Linguistics as a New Shared Task
Robert Dale
Centre for Language Technology
Macquarie University
Sydney, Australia
Robert.Dale@mq.edu.au
Adam Kilgarriff
Lexical Computing Ltd
Brighton
United Kingdom
adam@lexmasterclass.com
Abstract
In this paper, we propose a new shared task
called HOO: Helping Our Own. The aim is
to use tools and techniques developed in com-
putational linguistics to help people writing
about computational linguistics. We describe
a text-to-text generation scenario that poses
challenging research questions, and delivers
practical outcomes that are useful in the first
case to our own community and potentially
much more widely. Two specific factors make
us optimistic that this task will generate useful
outcomes: one is the availability of the ACL
Anthology, a large corpus of the target text
type; the other is that CL researchers who are
non-native speakers of English will be moti-
vated to use prototype systems, providing in-
formed and precise feedback in large quantity.
We lay out our plans in detail and invite com-
ment and critique with the aim of improving
the nature of the planned exercise.
1 Introduction
A forbidding challenge for many scientists whose
first language is not English is the writing of ac-
ceptable English prose. There is a concern?
perhaps sometimes imagined, but real enough to be
a worry?that papers submitted to conferences and
journals may be rejected because the use of language
is jarring and makes it harder for the reader to follow
what the author intended. While this can be a prob-
lem for native speakers as well, non-native speakers
typically face a greater obstacle.
The Association for Computational Linguistics?
mentoring service is one part of a response.1 A men-
toring service can address a wider range of problems
than those related purely to writing; but a key moti-
vation behind such services is that an author?s mate-
rial should be judged on its research content, not on
the author?s skills in English.
This problem will surface in any discipline where
authors are required to provide material in a lan-
guage other than their mother tongue. However, as
a discipline, computational linguistics holds a priv-
ileged position: as scientists, language (of different
varieties) is our object of study, and as technologists,
language tasks form our agenda. Many of the re-
search problems we focus on could assist with writ-
ing problems. There is already existing work that
addresses specific problems in this area (see, for ex-
ample, (Tetreault and Chodorow, 2008)), but to be
genuinely useful, we require a solution to the writing
problem as a whole, integrating existing solutions to
sub-problems with new solutions for problems as yet
unexplored.
Our proposal, then, is to initiate a shared task that
attempts to tackle the problem head-on; we want to
?help our own? by developing tools which can help
non-native speakers of English (NNSs) (and maybe
some native ones) write academic English prose of
the kind that helps a paper get accepted.
The kinds of assistance we are concerned
with here go beyond that which is provided by
commonly-available spelling checkers and grammar
checkers such as those found in Microsoft Word
(Heidorn, 2000). The task can be simply expressed
as a text-to-text generation exercise:
1See http://acl2010.org/mentoring.htm.
Given a text, make edits to the text to im-
prove the quality of the English it con-
tains.
This simple characterisation masks a number of
questions that must be answered in order to fully
specify a task. We turn to these questions in Sec-
tion 3, after first elaborating on why we think this
task is likely to deliver useful results.
2 Why This Will Work
2.1 Potential Users
We believe this initiative has a strong chance of suc-
ceeding simply because there will be an abundance
of committed, serious and well-informed users to
give feedback on proposed solutions. A famil-
iar problem for technological developments in aca-
demic research is that of capturing the time and in-
terest of potential users of the technology, to obtain
feedback about what works in a real world task set-
ting, with an appropriate level of engagement.
It is very important to NNS researchers that their
papers are not rejected because the English is not
good or clear enough. They expect to invest large
amounts of time in honing the linguistic aspects of
their papers. One of us vividly recalls an explana-
tion by a researcher that, prior to submitting a pa-
per, he took his draft and submitted each sentence
in turn, in quotation marks (to force exact matches
only), to Google. If there were no Google hits, it
was unlikely that the sentence was satisfactory En-
glish and it needed reworking; if there were hits, the
hits needed checking to ascertain whether they ap-
peared to be written by another non-native speaker.2
To give that researcher a tool that improves on this
situation should not be too great a challenge.
For HOO, we envisage that the researchers them-
selves, as well as their colleagues, will want to use
the prototype systems when preparing their confer-
ence and journal submissions. They will have the
skills and motivation to integrate the use of proto-
types into their paper-writing.
2See the Microsoft ESL Assistant at
http://www.eslassistant.com as an embodiment of
a similar idea.
2.2 The ACL Anthology
Over a number of years, the ACL has sponsored
the ongoing development of the ACL Anthology, a
large collection of papers in the domain of computa-
tional linguistics. This provides an excellent source
for the construction of language models for the task
described here. The more recently-prepared ACL
Anthology Reference Corpus (Bird et al, 2008), in
which 10,921 of the Anthology texts (around 40 mil-
lion words) have been made available in plain text
form, has also been made accessible via the Sketch
Engine, a leading corpus query tool.3
The corpus is not perfect, of course: not every-
thing in the ACL Anthology is written in flawless
English; the ARC was prepared in 2007, so new top-
ics, vocabulary and ideas in CL will not be repre-
sented; and the fact that the texts have been auto-
matically extracted from PDF files means that there
are errors from the conversion process.
3 The Task in More Detail
3.1 How Do We Measure Quality?
To be able to evaluate the performance of systems
which attempt to improve the quality of a text,
we require some means of measuring text quality.
One approach would be to develop measures, or
make use of existing measures, of characteristics
of text quality such as well-formedness and read-
ability (see, for example, (Dale and Chall, 1948;
Flesch, 1948; McLaughlin, 1969; Coleman and
Liau, 1975)). Given a text and a version of that text
that had been subjected to rewriting, we could then
compare both texts using these metrics. However,
there is always a concern that the metrics may not re-
ally measure what they are intended to measure (see,
for example, (Le Vie Jr, 2000)); readability metrics
have often been criticised for not being good mea-
sures of actual readability. The measures also tend
to be aggregate measures (for example, providing an
average readability level across an entire text), when
the kinds of changes that we are interested in evalu-
ating are often very local in nature.
Given these concerns, we opt for a different route:
for the initial pilot run of the proposed task, we in-
tend to provide a set of development data consisting
3See http://sketchengine.co.uk/open.
of 10 conference papers in two versions: an original
version of the paper, and an improved version where
errors in expression and language use have been cor-
rected. We envisage that participants will focus on
developing techniques that attempt to replicate the
kinds of corrections found in the improved versions
of the papers. For evaluation, we will provide a fur-
ther ten papers in their original versions, and each
participant?s results will then be compared against a
held-back set of corrected versions for these papers.
We would expect the evaluation to assess the follow-
ing:
? Has the existence of each error annotated in the
manually revised versions been correctly iden-
tified?
? Have the spans or extents of the errors been ac-
curately identified?
? Has the type of error, as marked in the annota-
tions, been correctly identified?
? How close is the automatically-produced cor-
rection to the manually-produced correction?
? What corrections are proposed that do not cor-
respond to errors identified in the manually-
corrected text?
With respect to this last point: we anticipate looking
closely at all such machine-proposed-errors, since
some may indeed be legitimate. Either the human
annotators may have missed them, or may not have
considered them significant enough to be marked. If
there are many such cases, we will need to review
how we handle ?prima facie false positives? in the
evaluation metrics.
Evaluation of the aspects described above can
be achieved automatically; there is also scope, of
course, for human evaluation of the overall relative
quality of the system-generated texts, although this
is of course labour intensive.
3.2 Where Does the Source Data Come From?
We have two candidates which we aim to explore
as sources of data for the exercise. It is almost cer-
tain the first of these two options will yield mate-
rial which is denser in errors, and closer to the kinds
of source material that any practical application will
have to work with; however, the pragmatics of the
situation mean that we may have to fall back on our
second option.
First, we intend to approach the Mentoring Chairs
for the ACL conferences over the last few years with
our proposal; then, with their permission, we ap-
proach the authors of papers that were submitted for
mentoring. If these authors are willing, we use their
initial submissions to the mentoring process as the
original document set.
If this approach yields an insufficient number of
papers (it may be that some authors are not willing
to have their drafts made available in this way, and
it would not be possible to make them anonymous)
then we will source candidate papers from the ACL
Anthology. The process we have in mind is this:
? Identify a paper whose authors are non-native
English speakers.
? If a quick reading of the paper reveals a mod-
erately high density of correctable errors with
in the first page, that paper becomes a candi-
date for the data set; if it contains very few cor-
rectable errors, the paper is ruled as inappropri-
ate.
? Repeat this process until we have a sufficiently
large data set.
We then contact the authors to determine whether
they are happy for their papers to be used in this ex-
ercise. If they are not, the paper is dropped and the
next paper?s author is asked.
3.3 Where do the Corrections Come From?
For the initial pilot, two copy-editors (who may or
may not be the authors of this paper) hand-correct
the papers in both the development and evaluation
data sets. For a full-size exercise there should be
more than two such annotators, just as there should
be more than ten papers in each of the development
and evaluation sets, but our priority here is to test the
model before investing further in it.
The copy-editors will then compare corrections,
and discuss differences. The possible cases are:
1. One annotator identifies a correction that the
other does not.
2. Both annotators identify different corrections
for the same input text fragment.
We propose to deal with instances of the first type as
follows:
? The two annotators will confer to determine
whether one has simply made a mistake?as
many authors can testify, no proofreader will
find all the errors in a text.
? If agreement on the presence or absence of an
error cannot be reached, the instance will be
dealt with as described below for cases of the
second type, with absence of an error being
considered a ?null correction?.
Instances of the second type will be handled as fol-
lows:
? If both annotators agree that both alternatives
are acceptable, then both alternatives will be
provided in the gold standard.
? If no agreement can be reached, then neither
alternative will be provided in the gold standard
(which effectively means that a null correction
is recorded).
Other strategies, such as using a third annotator as
a tie-breaker, can be utilised if the task generates a
critical mass of interest and volunteer labour.
3.4 What Kinds of Corrections?
Papers can go through very significant changes and
revisions during the course of their production: large
portions of the material can be added or removed,
the macro-structure can be re-organised substan-
tially, arguments can be refined or recast. Ideally, a
writing advisor might help with large-scale concerns
such as these; however, we aim to start at a much
simpler level, focussing on what is sometimes re-
ferred to as a ?light copy-edit?. This involves a range
of phenomena which can be considered sentence-
internal:
? domain- and genre-specific spelling errors, in-
cluding casing errors;
? dispreferred or suboptimal lexical choices;
? basic grammatical errors, including common
ESL problems like incorrect preposition and
determiner usage;
? reduction of syntactic complexity;
? stylistic infelicities which, while not grammati-
cally incorrect, are unwieldy and impact on flu-
ency and ease of reading.
The above are all identifiable and correctable within
the context of a single sentence; however, we also in-
tend to correct inconsistencies across the document
as whole:
? consistency of appropriate tense usage;
? spelling and hyphenation instances where there
is no obvious correct answer, but a uniformity
is required.
We envisage that the process of marking up the gold-
standard texts will allow us to develop more formal
guidelines and taxonomic descriptions for use sub-
sequent to the pilot exercise. There are, of course,
existing approaches to error markup that can pro-
vide a starting point here, in particular the schemes
used in the large-scale exercises in learner error
annotation undertaken at CECL, Louvain-la-Neuve
(Dagneaux et al, 1996) and at Cambridge ESOL
(Nicholls, 2003).
3.5 How Should the Task be Approached?
There are many ways in which the task could be ad-
dressed; it is open to both rule-based and statistical
solutions. An obvious way to view the task is as a
machine translation problem from poor English to
better English; however, supervised machine learn-
ing approaches may be ruled out by the absence of
an appropriately large training corpus, something we
may not see until the task has generated significant
momentum (or more volunteer annotators at an early
stage!).
There is clearly a wealth of existing research on
grammar and style checking that can be brought
to bear. Although grammar and style checking
has been in the commercial domain now for three
decades, the task may provide a framework for the
first comparative test of many of these applications.
Because the nature of errors is so diverse, this
task offers the opportunity to exercise a broad range
of approaches to the problem, and also allows for
narrowly-focussed solutions that attempt to address
specific problems with high accuracy.
4 Some Potential Problems
Our proposal is not without possible problems and
detrimental side effects.
Clearly there are ethical issues that need to be
considered carefully; even if an author is happy for
their data to be used in this way, one might find ret-
rospective embarrassment at eponynmous error de-
scriptions entering the common vocabulary in the
field?it?s one thing to be acknowledged for Kneser-
Ney smoothing, but perhaps less appealing to be fa-
mous for the Dale-Kilgarriff adjunct error.
Our suggestion that the ACL Anthology might be
used as a source for language modelling brings its
own downsides: in particular, if anything is likely
to increase the oft-complained-about sameness of
CL papers, this will! There is also an ethical is-
sue around the fine line between what such systems
will do and plagiarism; one might foresee the advent
of a new scholastic crime labelled ?machine-assisted
style plagiarism?.
There are no doubt other issues we have not yet
considered; again, feedback on potential pitfalls is
eagerly sought.
5 Next Steps
Our aim is to obtain feedback on this proposal from
conference participants and others, with the aim of
refining our plan in the coming months. If we sense
that there is a reasonable degree of interest in the
task, we would aim to publish the initial data set well
before the end of the year, with a first evaluation tak-
ing place in 2011.
In the name of better writing, CLers of the world
unite?you have nothing to lose but your worst sen-
tences!
Acknowledgements
We thank the two anonymous reviewers for useful
feedback on this proposal, and Anja Belz for encour-
aging us to develop the idea.
References
Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,
Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir Radev, and Yee Fan Tan. 2008. The
acl anthology reference corpus: A reference dataset for
bibliographic research in computational linguistics. In
Proceedings of the Language Resources and Evalua-
tion Conference (LREC 2008), location = Marrakesh,
Morocco.
Meri Coleman and T. L. Liau. 1975. A computer read-
ability formula designed for machine scoring. Journal
of Applied Psychology, 60:283?284.
E Dagneaux, S Denness, S Granger, and F Meunier.
1996. Error tagging manual version 1.1. Technical
report, Centre for English Corpus Linguistics, Univer-
site? Catholique de Louvain.
Edgar Dale and Jeanne S. Chall. 1948. A formula for
predicting readability. Educational research bulletin,
27:11?20.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of Applied Psychology, 32:221?233.
George Heidorn. 2000. Intelligent writing assistance. In
R Dale, H Moisl, and H Somers, editors, Handbook of
Natural Language Processing, pages 181?207. Marcel
Dekker Inc.
Donald S. Le Vie Jr. 2000. Documentation metrics:
What do you really want to measure? Intercom.
G. Harry McLaughlin. 1969. SMOG grading ? a new
readability formula. Journal of Reading, pages 639?
646.
D Nicholls. 2003. The cambridge learner corpus: error
coding and analysis for lexicography and ELT. In Pro-
ceedings of the Corpus Linguistics 2003 Conference
(CL 2003), page 572.
J R Tetreault and M S Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics.
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 12?22,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
GRE3D7: A Corpus of Distinguishing Descriptions
for Objects in Visual Scenes
Jette Viethen1,2
jette.viethen@mq.edu.au
1TiCC
University of Tilburg
Tilburg, The Netherlands
Robert Dale2
robert.dale@mq.edu.au
2Centre for Language Technology
Macquarie University
Sydney, Australia
Abstract
Recent years have seen a trend towards em-
pirically motivated and more data-driven ap-
proaches in the field of referring expression
generation (REG). Much of this work has fo-
cussed on initial reference to objects in visual
scenes. While this scenario of use is one of
the strongest contenders for real-world appli-
cations of referring expression generation, ex-
isting data sets still only embody very sim-
ple stimulus scenes. To move this research
forward, we require data sets built around in-
creasingly complex scenes, and we need much
larger data sets to accommodate their higher
dimensionality. To control the complexity,
we also need to adopt a hypothesis-driven ap-
proach to scene design. In this paper, we de-
scribe GRE3D7, the largest corpus of human-
produced distinguishing descriptions available
to date, discuss the hypotheses that underlie its
design, and offer a number of analyses of the
4480 descriptions it contains.
1 Introduction
Whenever we engage in any form of discourse we
need to find a way to describe to our readers or
listeners the entities that we are talking or writing
about. This act of referring to real-world entities is
one of the central tasks in human language produc-
tion. Of course, it is also central when a machine
is charged with the task of generating natural lan-
guage, which makes referring expression generation
(REG) an important subtask in any natural language
generation (NLG) system.
It is therefore not surprising that REG has attracted
a great deal of attention from the NLG community
over the past three decades. A key factor that has
led to the popularity of REG is the widespread agree-
ment that the central task involved is content selec-
tion: choosing those attributes of a target referent
that best distinguish it from other distractor enti-
ties around it (Dale and Reiter, 1995; van Deemter,
2000; Gardent, 2002; Krahmer et al, 2003; Ho-
racek, 2003; van der Sluis, 2005; Kelleher and Krui-
jff, 2006; Gatt, 2007; Viethen and Dale, 2008).
Recent work in particular has concentrated on the
development of algorithms concerned with the gen-
eration of context-free identifying descriptions of
objects, as emphasised by three shared-task evalu-
ation competitions (STECs) targeting this particular
problem (Belz and Gatt, 2007; Gatt et al, 2008; Gatt
et al, 2009). Referring expressions of this kind are
often referred to as distinguishing descriptions. We
are still far from a full understanding of how such
descriptions should best be generated. Much work
remains to be done before many issues, such as, for
example, the generation of relational descriptions
and over-specified descriptions or the number of the
surrounding objects to be taken into account in vi-
sual settings, can be considered resolved.
Although many authors have explicitly or implic-
itly acknowledged the importance of generating re-
ferring expressions that sound natural (Dale, 1989;
Dale and Reiter, 1995; Gardent et al, 2004; Ho-
racek, 2004; van der Sluis and Krahmer, 2004;
Kelleher and Kruijff, 2006; Gatt, 2007; Gatt et al,
2007), much of the original work in REG was nei-
ther developed based on empirical evidence about
12
Figure 1: The screen showing the first stimulus scene.
how humans refer, nor evaluated against human-
produced referring expressions. The REG STECs on
the task of content determination form part of a re-
cent trend towards more data-oriented development
and evaluation of REG algorithms that responds di-
rectly to this concern (Gupta and Stent, 2005; Jordan
and Walker, 2005; Gatt et al, 2007; Viethen et al,
2010; Belz and Gatt, 2007; Gatt et al, 2008; Gatt et
al., 2009).
However, the existing data sets used in these
experiments involve very simple and usually ab-
stract visual displays of objects rather than coher-
ent scenes. This is a reasonable starting point for
bootstrapping research; but if we want to develop
algorithms that can be used in real-world scenarios,
we ultimately need to work with scenes which are
much more realistic. At the same time, given the
non-deterministic nature of choice in the production
of natural language, corpora based on these scenes
need to be very large, and should ideally contain re-
ferring expressions from as many different speakers
as possible for each target referent in each referential
scenario. The choice of stimuli and data collection
procedure should provide a controlled environment
that allows the isolation of a small number of factors
influencing the choices that have to be made by the
participants, in order to facilitate the replication of
the same controlled environment for REG algorithms
attempting the same reference task in an evaluation
situation. The way forward, we believe, is to build a
succession of corpora with incrementally more com-
plex scenes.
In this paper, we describe the design of a data col-
lection experiment for distinguishing descriptions
and give an overview of the resulting corpus, which
is, at 4480 instances, the largest corpus of distin-
guishing descriptions developed to date.1 Consis-
tent with the common focus on initial reference in
visual scenes, we used visual stimuli containing a
small number of simple objects (cubes and balls) in
a 3D scene, similar to our much smaller GRE3D3
Corpus (Viethen and Dale, 2008), and elicited indi-
vidual descriptions in the absence of a complicating
preceding discourse. Additionally, we introduced
factors that allow the study of the use of spatial re-
lations in referring expressions by creating stimu-
lus scenes that encourage the use of relations be-
tween objects, but do not require them. Most ex-
isting REG algorithms that can make use spatial rela-
tions between objects only do so if no distinguishing
description can be found otherwise (Dale and Had-
dock, 1991; Gardent, 2002; Krahmer and Theune,
2002; van der Sluis and Krahmer, 2005; Kelleher
and Kruijff, 2006), often based on the argument that
mentioning two entities imposes a higher cognitive
load than referring to only one entity. We are inter-
ested in investigating in how far this behaviour cor-
responds to the human use of spatial relations in dis-
tinguishing descriptions, as well as testing a number
of concrete hypotheses about the factors that might
lead people to use spatial relations.
2 Stimulus Design
The stimulus scenes used for the GRE3D7 corpus
are three-dimensional scenes containing only sim-
ple geometric shapes, created in Google SketchUp.
Each stimulus scene contains seven objects; these
are grouped into three pairs of two and one single
object. The target object is always part of one of
the pairs and the second object of that pair is what
we call the landmark object in these scenes. We at-
tempted to place the target?landmark pair as close
to the centre of the scene as possible to encourage
the use of the target?s direct object properties and its
spatial relations to other objects, rather than its over-
all location in the scene, as in in the left. The other
two object pairs were placed slightly further back to
1The corpus is available for download online at
www.clt.mq.edu.au/research/projects/gre3d7.
13
the left and right of the target?landmark pair, and
the single object was always placed in the far right
or the far left of the scene. Objects were of one of
two types (ball or cube) and otherwise distinguish-
able by their size and colour. Each object could be
either large or small, and in each scene we used only
two colours. Figure 1 shows a close-up of one of
the scenes as presented to the subjects, and Figure 2
shows the complete set of stimulus scenes.
The design of the stimulus scenes was based on a
number of hypotheses about the factors that might
influence people?s use of spatial relations to the
landmark object. The two main hypotheses are con-
cerned with the influence of the landmark object?s
size on its visual salience and the likelihood of the
target?landmark relation being used in a referring
expression:
Hypothesis 1: A large landmark is more salient
than a small one because it occupies more of
the visual space of a scene. Therefore, a large
landmark is more likely to be mentioned in a re-
ferring expression via its spatial relation to the
target referent than a small landmark.
Hypothesis 2: A landmark that shares its size with
a number of other objects in the scene is less
salient than one that is unique in size. There-
fore, a landmark with unique size is more likely
to be mentioned in a referring expression via
its spatial relation to the target referent than a
landmark with a common size.
Hypotheses 1 and 2 are concerned with the land-
mark?s overall salience in the scene, or what is usu-
ally called bottom-up salience in the literature on
visual attention (cf., Yantis, 1998). A second con-
sideration that might influence the use of relations is
the top-down salience of the target and landmark ob-
jects, as determined by the task the participants are
performing. At the time when the landmark?s visual
salience is taken into account, the participants are
focusing their attention on the target object. As the
landmark is the closest object to the target, it is likely
that the difference or similarity between these two
objects plays a particularly important role in the de-
cision whether to include the relation between them
or not. Two conflicting hypotheses can be formu-
lated here:
Hypothesis 3: The difference between the land-
mark and the target object impacts on the visual
salience of the landmark because it impacts on
the landmark?s overall uniqueness in the scene.
Therefore, a landmark that is visually different
from the target is more likely to be included in a
referring expression than one that looks similar
to the target.
Hypothesis 4: The more similar the landmark and
target objects are, the more they appear as one
visual unit rather than two separate objects. If
they are perceived and conceptualised as a vi-
sual unit, they are more likely to be mentioned
together. Therefore, the more similar the land-
mark is to the target, the more likely it is to be
included in a referring expression.
The fifth hypothesis that this experiment is designed
to test concerns the preference that participants in
psycholinguistic work have shown for vertical rela-
tions over horizontal ones (Lyons, 1977; Bryant et
al., 1992; Gapp, 1995; Bryant et al, 2000; Landau,
2003; Arts, 2004; Tenbrink, 2004). To make sure
that the landmark is never obscured by the target ob-
ject, we use lateral relations rather than frontal ones
in this experiment.
Hypothesis 5: A target placed on top of a landmark
object is more likely to be described in terms
of its spatial relation to the landmark than a tar-
get that is sitting directly adjacent to the left or
right of the landmark.
We report the results of putting these five hypothe-
ses to the test in Section 5.4. To be able to per-
form these tests systematically, the experiment was
designed as a 2?2?2?2?2 grid with the following
five variables:
? LM Size: the landmark is either large or small.
[Large/Small]
? LM Size Rare: the size of the landmark is ei-
ther a common size in the scene, or it is as
rare as possible, and possibly unique. If it is
common and the landmark is large, it shares
its size with two of the objects; if it is small,
with three. These numbers are not the same
because in each scene in which the landmark
14
size was common, three objects were large
and four small. In +LM Size Rare scenes that
are also +TG Size = LM Size, the landmark
shares size only with the target. Only if the
scene is ?TG Size = LM Size can the land-
mark?s size be truly unique in the scene. [+/?]
? TG Size = LM Size: target and landmark are
either the same size or different. [+/?]
? TG Col = LM Col: The target and the land-
mark are either of the same colour or different
in colour. [+/?]
? Relation: The relation between the target and
the landmark is either vertical (the target is on
top of the landmark) or lateral, in which case
the target is placed directly to the left or right
of the landmark. [Vertical/Lateral]
This resulted in 32 experimental conditions. We cre-
ated one stimulus scene for each of these conditions.
We then split the stimuli into two trial sets along the
factor TG Size = LM Size, so that this variable be-
came a between-participant factor, while the other
four are within-participant factors.
We followed a number of other criteria for the de-
sign of the stimulus scenes to ensure maximum ex-
perimental control over the factors influencing the
content of the referring expressions provided by our
participants:
Target uniqueness: The target was always distin-
guishable in terms of its inherent properties alone,2
which means that the relation to the landmark or
other external properties, such as the location in the
scene, were never necessary to fully distinguish the
target from all other objects in the scene.
Landmark uniqueness: As the target, the land-
mark was always distinguishable in terms of its in-
herent properties alone.
Colour balance: Each scene followed one of two
colour schemes: either blue?green or red?yellow.
The colour schemes were distributed in a balanced
way across the five experimental variables, so that
2We use the term inherent property to refer to any property
of an entity which that entity has independent of the context in
which it appears.
half of the scenes in each condition were blue?green
and the other half red?yellow. The colour scheme
was not expected to have an influence on the con-
tent of the referring expressions people produced. In
each scene, four objects were of one colour of the
colour scheme for this scene and three had the other
colour.
Relation balance: The relation between the target
and the object was never unique. One of the two
other object pairs in each scene was arranged in the
same spatial relation as the target?landmark pair and
the third pair had the other relation. However, the
objects in the pair with the same relation were never
of the same types as the target and landmark, so that
a description containing the type of the target, a re-
lation to the landmark and the type of the landmark
was always fully distinguishing.
Constant landmark and target types: The land-
mark was always a cube, in order to avoid scenes
where the target would have to be balanced on top
of a ball, which might look unnatural. The target
was always a ball to make sure that the similarity in
type between these two objects was always constant.
No obscured objects: The objects were placed in
the scenes in such a way that no object occluded any
other. In particular, as mentioned above, there were
no frontal relations within the object pairs, to avoid
larger objects obscuring smaller ones completely or
to a large degree.
Figure 2 shows the 2?2?2?2?2 grid of the 32
stimuli scenes. Scenes 1?16, shown on a green back-
ground, constitute Trial Set 1, and Scenes 17?32,
shown on a blue background, constitute Trial Set 2.
3 Procedure and Participants
The data gathering experiment was designed as a
self-paced on-line language production study. Par-
ticipants visited a website, where they first saw an
introductory page with a set of simple instructions
and a sample stimulus scene. Each participant was
assigned one of the two trial sets containing 16 stim-
ulus scenes each. After the instruction page, the
scenes were presented consecutively in an order that
was randomised for every participant. Below each
scene, the participants had to complete the sentence
15
TG_Col 
=/= 
LM_Col
TG_Col 
= 
LM_Col
TG_Col 
=/= 
LM_Col
1 135 9
2 106 14
151173
4 8 1612
TG_Col 
= 
LM_Col
29252117
18 22 3026
3123 2719
3224 2820
LM_Size 
Common
LM_Size 
Rare
LM_Size 
Common
LM_Size 
Rare
V
e
r
t
i
c
a
l
 
R
e
l
a
t
i
o
n
L
a
t
e
r
a
l
 
R
e
l
a
t
i
o
n
LM Large LM Small
LM_Size 
Common
LM_Size 
Rare
LM_Size 
Common
LM_Size 
Rare
LM Large LM Small
TG_Size =/= LM_Size TG_Size = LM_Size
Figure 2: The 32 stimulus scenes for GRE3D7: The left half constitutes Trial Set 1 and the right half is Trial Set 2.
Please pick up the . . . in a text box before click-
ing a button labelled ?DONE? to move on to the next
scene, as shown in Figure 1. The task was to de-
scribe the target referent in the scene (marked by a
grey arrow) in a way that would enable a friend look-
ing at the same scene to pick it out from the other
objects. To encourage the use of fully distinguish-
ing descriptions, participants were told that they had
only one chance at describing the object.
Before each of the 16 stimulus scenes, the partic-
ipants were shown a filler scene, which means each
participant had to describe 32 scenes in total. The
main motivation for using filler scenes was to min-
imise the decline in relation use over time, which
might otherwise happen if participants realised that
relations were never necessary.
The filler scenes were also designed with the in-
tention of making the experiment less monotonous,
and to stop participants from noticing the strict de-
sign features of the stimulus scenes. In particular,
each participant saw: four scenes with twelve ob-
jects in all four colours, as opposed to the two-colour
schemes; two scenes containing only three objects;
and ten further filler scenes which intentionally vio-
lated the above design criteria. The filler scenes for
each participant were chosen such that in eleven or
twelve scenes the target was a cube instead of a ball,
in two scenes the landmark was a ball, in four scenes
there was no obvious landmark close to the target, in
eight scenes the target was unique (i.e. it could not
be described by its inherent visual properties alone),
in nine or ten scenes the target and landmark shared
type, and in two or three scenes target and landmark
were of the same size; for participants who saw Trial
Set 2 all stimulus scenes also had a target and land-
mark of the same size.
The sequence of the 32 scenes that were shown to
a particular participant was determined by the fol-
lowing three steps:
1. Pick the opposite trial set to the one that the last
participant saw and randomise its order.
2. Pick the set of 16 filler scenes to be shown to
this participant and randomise their order.
3. Interleave the two sets so that each stimulus
scene is preceded by one filler scene.
After having described all 32 scenes in the trial,
participants were asked to complete an exit ques-
tionnaire, which gave them the option of having
their data discarded and asked for their opinion on
whether the task became easier over time and any
other comments they might wish to make.
The experiment was started by 318 native English
speakers, of which 294 completed all 32 scenes.
They were recruited by word of mouth via a widely-
circulated call for participation and two electronic
mailing lists.3 The participants were predominantly
in their twenties or thirties and mostly university-
educated. A slight majority (54%) were female.
None of them reported colour-blindness. Each re-
ferring expression in the corpus is tagged with an
anonymous ID number linking it to some simple de-
mographic data about the contributing participant,
including gender, age, type of English spoken, and
field of education.
3The Corpora List and the SIGGEN List.
16
4 Data Filtering and Annotation
Of the 294 participants who completed the experi-
ment, five consistently used only type, although the
target?s type was never fully distinguishing in any of
the stimulus scenes. For example, these participants
described the target in Figure 1 simply as ball, which
does not distinguish it from the two other balls in
the scene. We discarded the data of these partici-
pants under the assumption that they had not under-
stood the instruction that their descriptions were to
uniquely identify the target. Two participants? data
were discarded because they provided text that was
unrelated to the displayed scenes. Of the remaining
287 participants, 140 saw Trial Set 2 and 147 saw
Trial Set 1. The data from seven randomly-chosen
participants from Trial Set 1 were discarded to bal-
ance the corpus in terms of the between-participant
feature TG Size = LM Size. Each person described
the 16 scenes contained in either of the trial sets, re-
sulting in a corpus of 4480 descriptions in total, with
140 descriptions for each scene. No other corpus of
referring expressions contains as many descriptions
for each referential scenario from different speak-
ers, which makes this corpus ideal for the study of
speaker-specific preferences and non-deterministic
choices in content selection.
Only five of the 4480 descriptions used the ternary
spatial relation between, and one description men-
tioned two distinct spatial relations, one to the in-
tended landmark and one to another object. The re-
lation to the third object in these six descriptions was
disregarded in the analysis presented here.
In order to be able to analyse the semantic content
of the referring expressions, we semi-automatically
annotated the inherent attributes and relations con-
tained in each of them. The attributes annotated are
? type[ball, cube]
? colour[blue, green, red, yellow]
? size[large, small]
? location[right, left, front, top, bottom, centre]
? relation[horizontal, vertical]
Each attribute (except relation) is prefixed by either
tg or lm to mark which of the objects it pertains
to. For example, tg size indicates that the size of the
target was mentioned.
% of total % of all 600
4480 relational
attribute count descriptions descriptions
tg size 2587 57.8 ?
tg colour 4423 98.7 ?
tg location 81 1.8 ?
relation 600 13.4 ?
lm size 327 7.3 54.5
lm colour 521 11.6 86.8
lm location 10 0.2 1.7
Table 1: Attribute counts in GRE3D7
In the 83 descriptions containing comparatives,
such as Example (1), we ignored the second object
that the target was being compared to. In all of these
cases, the target?s colour and type were also men-
tioned, which means that in the context of the sim-
ple scenes at stake here, Example (1) is semantically
equivalent to Example (2).
(1) the smaller of the two red balls
(2) the small red ball
The question of how to deal with the relative na-
ture of size is a separate, non-trivial, issue; see (van
Deemter, 2000; van Deemter, 2006).
5 Analysis of the GRE3D7 Corpus
In this section we examine the content of the 4480
descriptions that make up the GRE3D7 Corpus. We
first give an overview of the use of the non-relational
attributes, and then proceed to investigate the hy-
potheses from Section 2 regarding the use of spatial
relations.
The target object?s type was mentioned in each
description in the corpus, and each relational de-
scription contained the landmark object?s type. Ta-
ble 1 shows the number of descriptions containing
each of the other attributes.
5.1 Sparing Use of location
Only 81 descriptions (1.8%) made reference to the
target referent?s location in the scene, as in Exam-
ple (3); and of the 600 relational descriptions in the
corpus, only ten (1.7%) contained the location of the
landmark, as in Example (4).
(3) the large yellow ball on the left [Scene 9]
17
(4) the small ball next to the large cube on the left
hand side [Scene 6]
There were no descriptions containing both
tg location and lm location. This might indicate
that participants who used a relation were more
likely to conceptualise the target?landmark pair
as a unit with just one location rather than as two
individual entities. However, the corpus was not
designed to investigate this issue and the numbers
for use of location are too low to draw any definite
conclusions.
5.2 Abundant Use of colour
Colour was used in the vast majority of descriptions:
98.7% of all descriptions included the colour of the
target object and 86.8% of the relational descriptions
included the colour of the landmark object. A high
number of descriptions containing colour could be
expected, as colour was part of the shortest possi-
ble minimal description not containing any spatial
information (we call this the inherent MD of the tar-
get) for 20 of the 32 scenes (all but Scenes 17?24
and 29?32). However, the fact that colour was also
included in the majority of the descriptions contain-
ing spatial information, in the form of a relation or
the location, confirms previous findings to the effect
that colour is often included in descriptions redun-
dantly (Belke and Meyer, 2002; Arts, 2004; Gatt,
2007).
5.3 Utilitarian Use of size
The target?s size was mentioned in 57.8% of all de-
scriptions, and the landmark?s size in 54.8% of the
relational descriptions.
Considering that tg size was part of the inherent
MD in only 12 of 32 scenes (37.5%) of the stimu-
lus scenes (Scenes 2, 4, 9?12, 18, 20 and 25?28),
57.8% seems like a high proportion of descriptions
to be using this attribute. The use of tg size for
scenes where it was part of the inherent MD was at
90.2% very high, but this only accounts for just un-
der 60% of all the descriptions that contained this
attribute. The remaining 40% of descriptions con-
taining tg size were given for scenes in which this
attribute was not strictly necessary to distinguish the
target from the other objects.
Findings from eye-tracking experiments in psy-
cholinguistics have shown that size is rarely used in
situations where it adds no discriminatory power to
the referring expression at all, and that it is more
likely to be used to compare to or distinguish from
other objects of the same type, while the same is
not true for colour (Sedivy, 2003; Brown-Schmidt
and Tanenhaus, 2006). Let us therefore consider in
particular the scenes where tg size was not part of
the inherent MD, and look at the differing utility of
tg size in these scenes: 12 of the 20 scenes where
tg size was not necessarily part of the inherent MD
(Scenes 1, 3, 5?8, 13?16, 17, 19, 21?24 and 29?
32) nonetheless contained another object that shared
the target?s type (ball) but not its size (Scenes 1, 3,
17, 19, 21?24 and 29?32). In these scenes, tg size
remains a useful attribute to use, even if tg type is
also included.
Based on the psycholinguistic findings mentioned
above, one might expect that the use of tg size is
higher for these scenes because here it helps distin-
guish from another object of the same type rather
than only from objects of a different type. This hy-
pothesis is supported by the data: tg size was used
in 45.6% of the descriptions for scenes where it was
not part of the inherent MD but there was another ob-
ject of same type and different size as the target. For
scenes where tg size could only distinguish the tar-
get from objects of the other type, it was only used
in 27.3% of cases (?2=94.97, df=1, p.01).
5.4 The Use of Spatial Relations
600 of the 4480 descriptions in the GRE3D7 Cor-
pus (13.4%) mentioned a spatial relation. This was
despite the fact that spatial information was not re-
quired in any of the stimulus scenes. Most existing
approaches to spatial relations in REG would there-
fore never include a relation for any of the stimuli.
In this section, we examine the circumstances un-
der which the participants of the GRE3D7 data col-
lection experiment used the spatial relation between
the target object and the intended landmark. We
will first examine participant-dependent and tempo-
ral factors and then move on to analyse the impact
that the design features of the scenes, described in
Section 2, had on the use of relations.
General Factors
We first checked for broad participant-dependent
preferences for or against using relations in the
18
GRE3D7 Corpus. The behaviour of participants
who use an exclusive strategy of either always or
never including a relation in their referring expres-
sions would be easy to predict in a computational
model and does not contribute to any variation
across different scenes. In order to gain a clear un-
derstanding of this variation, we will concentrate on
the data from participants who varied their use of
relations between scenes.
Half of the participants (50.3%) adopted an ex-
clusive strategy regarding the use of relations. How-
ever, the split between the two exclusive strategies
was very uneven: 135 participants never used a spa-
tial relation and only six used a spatial relation for
all 16 stimulus scenes they saw. In the following, we
analyse the data from the 139 participants who used
a relation for some scenes but not for others. On av-
erage, these participants used a relation in 22.7% of
their descriptions.
In (Viethen and Dale, 2008), we observed a ?lazi-
ness effect? whereby participants? use of relations
decreased over the course of the experiment. A num-
ber of participants mentioned in the exit interview
that they noticed over time that relations were never
required and stopped using them. Such a conscious,
or semi-conscious, adjustment masks people?s nat-
ural propensity to use a relation in a reference situ-
ation where they come anew at the task rather than
describing one object after another.
In the GRE3D7 collection experiment, each par-
ticipant saw eight filler scenes in which spatial rela-
tions were required to distinguish the target. These
filler scenes were included to stop participants from
consciously noticing that relations were never re-
quired in the stimulus scenes. We hoped that this
would reduce the laziness effect and thereby pro-
duce results that better approximate people?s natu-
ral tendency to use a relation. However, Figure 3
shows that, despite the use of these filler scenes, the
use of relations declined over the course of the ex-
periment. Participants who did not follow an exclu-
sive strategy clearly used more relations for scenes
they saw early on than for those they saw towards
the end. We divided the data set into quartiles in
order to test the statistical significance of this de-
cline. The falling trend was statistically significant
at p.01 (?2=55.42, df=3). However, any tem-
poral effect in GRE3D7 should not interfere with
!"#
$"#
%!"#
%$"#
&!"#
&$"#
'!"#
'$"#
(!"#
($"#
%)*
#
&+,
#
'-,
#
(*.
#
$*.
#
/*.
#
0*.
#
1*.
#
2*.
#
%!*
.#
%%*
.#
%&*
.#
%'*
.#
%(*
.#
%$*
.#
%/*
.#!"
#$#
"%#
&'#
(')*
+,%
#&,
+'-
*./
"0$
%#
&.'
1/*&*.'0&'2*3$#",+'4"5*"'
Figure 3: Temporal effect on use of relation
!"#$%&
!'#"%&
!(#"%&
!)#*%&
()#"%&
!!#"%&
(*#$%&
!+#$%&
('#,%&
"$#$%&
$%&
)%&
($%&
()%&
!$%&
!)%&
"$%&
-./0123& -./0123/4563& 78/0123&9&-./0123& 78/:;<&9&-./:;<& 43<5=;>&
!"#$
#"%#
&''#(
')*+,
%#&
,+'-
*./"
0$%#
&.'
3?@3AB3C&<;D& 3?@3AB3C&E1FE&
!"#$$ $#%&' (%)' (%)' (%)' $#('%#$*#$!' *#$!' *#$!' +'%(,-#$
./01023/45/53675726889/54:;4<26;3/63/=>>?@A
./
./
./
Figure 4: Effect of design variables on use of relation
between-stimulus effects, as the stimuli were pre-
sented in a randomised order.
Influence of Scene Features on Relation Use
We will now turn to the examination of Hypothe-
ses 1?5 from Section 2. Figure 4 shows the impact
that each of the five variables of the scene design had
on the use of relations. The left (green) columns rep-
resent the conditions for which we expected fewer
relations to be used, and the right (yellow) columns
represent the conditions for which we expected a
higher use of relations, according to Hypotheses 1?
3 and 5. Hypothesis 4 expected the reverse results
for TG Size = LM Size and TG Col = LM Col. All
factors except LM Size and TG Size = LM Size had
a statistically significant effect.
Hypotheses 1 and 2, which expected a large
landmark with a rare or unique size to be more
salient and therefore more likely to be used, are
not supported by the data here. LM Size did not
have a reliable effect (?2=0.16, df=1, p>.6) and
19
LM Size Rare shows the opposite effect of the one
we expected: a relation to a landmark with a com-
mon size is significantly more likely to be included
in a referring expression than one to a landmark with
a rare or unique size (?2=56.19, df=1, p.01). On
closer inspection, this is likely to be due to a fac-
tor that was not explicitly tested or controlled for in
this experiment: the length of the inherent MD of the
target referent. In most scenes with a common land-
mark size (all but Scenes 1, 3, 17, and 19), all three
inherent attributes (size, colour and type) are neces-
sary to distinguish the target from the other objects
without using locational information. In all scenes
where the landmark?s size is rare or unique, colour
and type suffice. In other words, targets which are
harder to describe using inherent visual properties
only are more likely to be described by a relation to
a nearby landmark.
Hypotheses 3 and 4 predicted two mutually ex-
clusive scenarios based on the assumption that the
similarity between the target and the landmark ob-
ject is of special importance, as the participant?s vi-
sual attention is likely to be focussed on these two
objects. Hypothesis 3 predicted that a visual dif-
ference between the landmark and the target would
increase the landmark?s salience and therefore the
use of the spatial relation to this landmark. Hypoth-
esis 4 predicted that high visual similarity between
target and landmark might result in these two ob-
jects being conceptualised as a unit, which would
increase the likelihood of both objects being men-
tioned. The target and landmark object were al-
ways of different types, so their similarity depends
on their size and their colour, captured in the vari-
ables TG Size = LM Size and TG Col = LM Col.
TG Size = LM Size did not show a significant ef-
fect on the use of relations (?2=2.29, df=1, p>.1).
The effect of TG Col = LM Col favours Hypothe-
sis 4, as a landmark of the same colour as the target
is more likely to be included in the target?s descrip-
tion than one that has a different colour from the tar-
get (?2=11.18, df=1, p0.01).
The variable Relation had the expected effect: A
vertical relation is significantly more likely to be
used than a lateral one (?2=69.00, df=1, p.01).
This confirms Hypotheses 5.
6 Conclusion
We have described the GRE3D7 Corpus, a collec-
tion of human-produced distinguishing descriptions
that is considerably larger than any other existing
corpus. The collection also uses scenes that are a
degree more complex than those found in existing
corpora; these are based on a principled design in
order to provide a measure of control over what can
be learned from the data. In this paper we have de-
scribed the details of the collection experiment and
have presented an analysis of the impacts that the de-
sign variables had on the content of the resulting de-
scriptions. The main outcomes of this analysis are:
Colour is used in 99% of all descriptions. It is
also used redundantly in 87% of all relational de-
scriptions. This is in accordance with findings in
other corpora and psycholinguistic studies.
Size is used when it is distinguishing. The size
of the target referent was much more likely to be
included when it was useful in distinguishing from
another object in the scene, especially those of the
same type.
Just over half of the participants follow an ex-
clusive strategy for the use of relations. A large
proportion of participants (135) opted to never use
a relation, while a much smaller number of people
(6) used a relation in all of their descriptions. The
remaining 139 participants are responsible for the
variation in the data, as they used a relation to de-
scribe the target in some but not all scenes.
The target?landmark relation is used more often
if it is vertical than if it is lateral. This confirms
previous psycholinguistic findings showing that hu-
mans prefer vertical relations and prepositions over
horizontal, and in particular lateral, ones.
If a landmark shares colour with the target it is
more likely to be used in a referring expression. This
lends support to the hypothesis that visual similar-
ity between target and landmark increases the likeli-
hood of the relation between them being used.
The data thus sheds additional light on the nature
of human-produced descriptions of objects in visual
scenes. It also, of course, provides a rich corpus of
data that can be readily used to evaluate the perfor-
mance of computational algorithms for the genera-
tion of referring expressions.
20
References
Anja Arts. 2004. Overspecification in Instructive Texts.
Ph.D. thesis, University of Tilburg, The Netherlands.
Eva Belke and Antje S. Meyer. 2002. Tracking the
time course of multidimensional stimulus discrimina-
tion: Analysis of viewing patterns and processing time
during same-different decisions. European Journal of
Cognitive Psychology, 14(2):237?266.
Anja Belz and Albert Gatt. 2007. The Attribute Selection
for GRE Challenge: Overview and evaluation results.
In Proceedings of the Workshop on Using Corpora for
NLG: Language Generation and Machine Translation
(UCNLG+MT), pages 75?83, Copenhagen, Denmark.
Sarah Brown-Schmidt and Michael K. Tanenhaus. 2006.
Watching the eyes when talking about size: An investi-
gation of message formulation and utterance planning.
Journal of Memory and Language, 54:592?609.
David J. Bryant, Barbara Tversky, and Nancy Franklin.
1992. Internal and external spatial frameworks rep-
resenting described scenes. Journal of Memory and
Language, 31:74?98.
David J. Bryant, Barbara Tversky, and M. Lanca.
2000. Retrieving spatial relations from observation
and memory. In Emile van der Zee and Urpo Nikanne,
editors, Cognitive interfaces: Constraints on linking
cognitive information, pages 94?115. Oxford Univer-
sity Press, Oxford, UK.
Robert Dale and Nicholas Haddock. 1991. Content de-
termination in the generation of referring expressions.
Computational Intelligence, 7(4):252?265.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Robert Dale. 1989. Cooking up referring expressions.
In Proceedings of the 27th Annual Meeting of the As-
sociation for Computational Linguistics, pages 68?75,
Vancouver BC, Canada.
Klaus-Peter Gapp. 1995. Angle, distance, shape, and
their relationship to projective relations. In Proceed-
ings of the 17th Annual Meeting of the Cognitive Sci-
ence Society, pages 112?117, Pittsburgh PA, USA.
Claire Gardent, He?le`ne Manue?lian, Kristina Striegnitz,
and Marilisa Amoia. 2004. Generating definite de-
scriptions: Non incrementality, inference and data.
In Thomas Pechmann and Christopher Habel, edi-
tors, Multidisciplinary Approaches to Language Pro-
duction, pages 53?86. Walter de Gruyter, Berlin, Ger-
many.
Claire Gardent. 2002. Generating minimal definite de-
scriptions. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 96?103, Philadelphia PA, USA.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation, pages 49?56, Schlo? Dagstuhl,
Germany.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The TUNA
Challenge 2008: Overview and evaluation results. In
Proceedings of the 5th International Conference on
Natural Language Generation, pages 198?206, Salt
Fork OH, USA.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA-
REG Challenge 2009: Overview and evaluation re-
sults. In Proceedings of the 12th European Work-
shop on Natural Language Generation, pages 174?
182, Athens, Greece.
Albert Gatt. 2007. Generating Coherent Reference to
Multiple Entities. Ph.D. thesis, University of Ab-
erdeen, UK.
Surabhi Gupta and Amanda Stent. 2005. Automatic
evaluation of referring expression generation using
corpora. In Proceedings of the Workshop on Using
Corpora for Natural Language Generation, pages 1?
6, Brighton, UK.
Helmut Horacek. 2003. A best-first search algorithm
for generating referring expressions. In Proceedings
of the 10th Conference of the European Chapter of
the Association for Computational Linguistics, pages
103?106, Budapest, Hungary.
Helmut Horacek. 2004. On referring to sets of objects
naturally. In Proceedings of the 3rd International Con-
ference on Natural Language Generation, pages 70?
79, Brockenhurst, UK.
Pamela W. Jordan and Marilyn Walker. 2005. Learning
content selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelligence Re-
search, 24:157?194.
John Kelleher and Geert-Jan Kruijff. 2006. Incremen-
tal generation of spatial referring expressions in situ-
ated dialog. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
Annual Meeting of the Association for Computational
Linguistics, pages 1041?1048, Sydney, Australia.
Emiel Krahmer and Marie?t Theune. 2002. Efficient
context-sensitive generation of referring expressions.
In Kees van Deemter and Rodger Kibble, editors, In-
formation Sharing: Reference and Presupposition in
Language Generation and Interpretation, pages 223?
264. CSLI Publications, Stanford CA, USA.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Lingustics, 29(1):53?72.
Barbara Landau. 2003. Axes and direction in spatial
language and spatial cognition. In Emilie van der Zee
21
and Jon M. Slack, editors, Representing Direction in
Language and Space, pages 18?38. Oxford University
Press, Oxford, UK.
John Lyons. 1977. Semantics, volume 2. Cambridge
University Press, Cambridge, UK.
Julie C. Sedivy. 2003. Pragmatic versus form-based ac-
counts of referential contrast: Evidence for effects of
informativity expectations. Journal of Psycholinguis-
tic Research, 32(1):3?23.
Thora Tenbrink. 2004. Identifying objects on the basis
of spatial contrast: An empirical study. In Christian
Freksa, Markus Knauff, Bernd Krieg-Brckner, Bern-
hard Nebel, and Thomas Barkowsky, editors, Spatial
cognition IV: Reasoning, action, interaction, number
3343 in Lecture Notes in Computer Science, pages
124?146. Springer, Berlin/Heidelberg, Germany.
Kees van Deemter. 2000. Generating vague descrip-
tions. In Proceedings of the 1st International Con-
ference on Natural Language Generation, pages 179?
185, Mitzpe Ramon, Israel.
Kees van Deemter. 2006. Generating referring expres-
sions that involve gradable properties. Computational
Linguistics, 32(2):195?222.
Ielka van der Sluis and Emiel Krahmer. 2004. Evalu-
ating multimodal NLG using production experiments.
In Proceedings of the 3rd International Conference on
Language Resources and Evaluation, Lisbon, Portu-
gal, 26-28 May.
Ielka van der Sluis and Emiel Krahmer. 2005. Towards
the generation of overspecified multimodal referring
expressions. In Proceedings of the Symposium on Di-
alogue Modelling and Generation at the 15th Annual
Meeting of the Society for Text and Discourse, Ams-
terdam, The Netherlands, 6-9 July.
Ielka van der Sluis. 2005. Multimodal Reference, Stud-
ies in Automatic Generation of Multimodal Referring
Expressions. Ph.D. thesis, Tilburg University, The
Netherlands.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-
ceedings of the 5th International Conference on Natu-
ral Language Generation, pages 59?67, Salt Fork OH,
USA.
Jette Viethen, Simon Zwarts, Robert Dale, and Markus
Guhe. 2010. Dialogue reference in a visual domain.
In Proceedings of the 7th International Conference on
Language Resources and Evaluation, Valetta, Malta.
Steven Yantis. 1998. Control of visual attention. In
Harold Pashler, editor, Attention, chapter 6, pages
223?256. Psychology Press, Hove, UK.
22
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 54?62,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
HOO 2012:
A Report on the Preposition and Determiner Error Correction Shared Task
Robert Dale, Ilya Anisimoff and George Narroway
Centre for Language Technology
Macquarie University
Sydney NSW 2109
Australia
rdale@acm.org, anisimoff@gmail.com, george.narroway@me.com
Abstract
Incorrect usage of prepositions and determin-
ers constitute the most common types of er-
rors made by non-native speakers of English.
It is not surprising, then, that there has been
a significant amount of work directed towards
the automated detection and correction of such
errors. However, to date, the use of differ-
ent data sets and different task definitions has
made it difficult to compare work on the topic.
This paper reports on the HOO 2012 shared
task on error detection and correction in the
use of prepositions and determiners, where
systems developed by 14 teams from around
the world were evaluated on the same previ-
ously unseen errorful text.
1 Introduction
It is widely recognized that the correct usage of
determiners and prepositions in English is a ma-
jor problem area for non-native speakers of the lan-
guage.1 The issues here have been explored and
discussed extensively in the literature; an excellent
and up-to-date summary is available in (Leacock et
al., 2010). However, the various teams that have at-
tempted to tackle these problems so far have tended
to use slightly different task specifications, and dif-
ferent data sets for evaluation; this makes it very dif-
1We use the broad term ?non-native speaker?, abbreviated
?NNS?, in this paper; other work makes a distinction between
ESL (English as a Second Language) speakers (who live and
speak in a primarily English-speaking environment) or EFL
(English as a Foreign Language) speakers (who are learning En-
glish in a non-English-speaking country.
ficult to compare the results achieved using different
approaches.
To address this problem, the aim of the HOO 2012
Shared Task was to provide a forum for the compar-
ative evaluation of different approaches to the cor-
rection of these errors.2 The shared task provides a
common training dataset, a shared evaluation frame-
work, and a set of previously unseen test data.
These proceedings contain detailed reports by all
14 teams who participated in HOO 2012. The
present paper provides a summary of the task and its
evaluation, and a report on the results of that evalu-
ation.
Section 2 provides an overview of the task and the
timeline across which it was carried out; Section 3
provides details of the participating teams; Section 4
describes the training and test data in more detail;
Section 5 presents the results of the evaluation; and
Section 6 provides some concluding remarks and
discussion, reflecting on lessons learned.
2 The Task
Non-native speakers who are learning English find
prepositions and determiners particularly problem-
atic. The selection of the appropriate preposition in
a given context often appears to be a matter of id-
iom or convention rather than being governed by a
consistent set of rules; and selecting a determiner
2HOO stands for ?Helping Our Own?, a reflection of the his-
torical origins of the exercise as an attempt to develop tools to
help researchers in natural language processing to write better
papers: see (Dale and Kilgarriff, 2010) for the background to
this enterprise and (Dale and Kilgarriff, 2011) for a report on
the pilot round of the task held in 2011.
54
Team ID Group or Institution Subtasks Runs
CU Computer Laboratory, University of Cambridge, UK DRC 8
ET Educational Testing Service, New Jersey, USA DR 3
JU Jadavpur University, Kolkata, India DRC 1
KU Natural Language Processing Lab, Korea University, Seoul, Korea DRC 10
LE KU Leuven, Belgium DRC 2
NA NAIST, Japan DRC 8
NU National University of Singapore, Singapore DRC 1
TC Department of Computer Science and Statistics, Trinity College Dublin, Ireland DRC 10
TH NLPLAB, National Tsing Hua University, Hsinchu, Taiwan DRC 4
UD UKP Lab, Technische Universita?t Darmstadt, Germany DRC 3
UI Cognitive Computation Group, University of Illinois, USA DRC 10
UT Theoretical Computational Linguistics Group, University of Tu?bingen, Germany DRC 10
VA Valkuil.net, The Netherlands DRC 6
VT VTEX, Vilnius, Lithuania DRC 9
Table 1: Participating teams
depends on a complex of contextual factors which
is particularly challenging for those whose native
language does not make use of determiners. The
literature suggests that mistakes in the use of the
determiners and prepositions account for 20?50%
of grammar and usage errors; the extent to which
a learner has problems with determiners varies de-
pending on their native language, while the degree of
difficulty experienced with prepositions is less var-
ied (see Chapter 3 in (Leacock et al, 2010)).
For the shared task, we made use of data drawn
from the CLC FCE Dataset, a set of 1,244 exam
scripts written by candidates sitting the Cambridge
ESOL First Certificate in English (FCE) examina-
tion in 2000 and 2001, and made available by Cam-
bridge Universiy Press; see (Yannakoudakis et al,
2011). This data is described in more detail below.
The version of the data we provided to teams as
training data consisted of the original text as written
by the examination subjects, so it contains many er-
rors besides the preposition and determiner errors; it
thus provides a quite realistic challenge, as opposed
to artificial data sets where the only errors present
are the particular errors of interest. The training data
we provided consisted of the raw, errorful texts, and
for each text file a set of gold-standard standoff an-
notations indicating the locations of the preposition
and determiner errors and their corrections, which
we extracted from the CUP data annotations.
The task consisted in attempting to generate sets
of standoff annotations that matched those in the
gold standard. Teams were to be evaluated on three
subtasks: detection, recognition and correction. The
first of these is a measure of a system?s success in
determining that something is wrong in a text and
that it requires fixing; the second requires also that
the precise extent of the error be identified, and the
correct type assigned; and the third requires that a
correction matching that in the gold standard be of-
fered. Scores on each of these subtasks were com-
puted for preposition and determiner errors com-
bined, and for preposition and determiner errors sep-
arately; thus, each participating system run could
generate up to nine distinct scores. In addition, we
also provided teams with detection, recognition and
correction scores for each of the six base error types
(see Table 2); some teams report on these statistics
in their individual reports.
The training data and evaluation tools were made
available on 27th January 2012; test data was re-
leased on April 6th 2012, with submissions of results
from teams due on April 13th 2012. Teams therefore
had 10 weeks to develop a system that could handle
the training data, and one week to provide results on
the test data.
3 The Participants
At the time we released the training data, 26 teams
registered interest in the task. The test data, re-
ceipt of which required a signed agreement with
Cambridge University Press, was requested by 15
teams; one of these teams subsequently withdrew
55
Type Tag Original Correction
Replacement Preposition RT I could only travel on July I could only travel in July
Missing Preposition MT I am looking forward your reply I am looking forward to your reply
Unwanted Preposition UT I have booked a flight to home I have booked a flight home
Replacement Determiner RD wich was situeded on a seaside wich was situeded on the seaside
Missing Determiner MD I will give you all information I will give you all the information
Unwanted Determiner UD One of my hobbies is the photography One of my hobbies is photography
Table 2: Examples of the six base error types
from the competition. The 14 teams who completed
the shared task are listed in Table 1.3
4 The Data
4.1 Basic Statistics
The training data consisted of 1000 files drawn from
the publicly available FCE dataset. These were con-
verted from the native FCE format into the HOO
data format, which was slightly revised from the ver-
sion used in HOO 2011 (see (Dale and Kilgarriff,
2011)). The original data was marked up with all the
errors found by the CUP annotator, but we discarded
annotations of errors other than the six base types we
were interested in, and converted the remaining er-
rors into standoff annotations. The six types, with
examples of each, are shown in Table 2;4 Figure 1
shows a fragment of an FCE data file, and Figure 2
shows a standoff annotation example extracted from
this file in the HOO format.
Elements of some of these files were removed to
dispose of nested edits and other phenomena that
caused difficulties in the preprocessing of the data.5
The resulting set of training data comprised a total
of 374680 words, for an average of 375 words per
file.
The test data consisted of a further 100 previously
unseen files provided to us for this shared task by
CUP. These were processed in the same manner as
the training data. The test data comprised 18013
words, for an average of 180 words per file. Counts
3The ?Subtasks? column indicates which subtasks the team
took part in: detection (D), recognition (R) and correction (C).
The ?Runs? column is explained later.
4For the present exercise we used the preposition and deter-
miner error tags as provided in the CLE tagset. The full CLE
tagset is described in (Nicholls, 2003).
5This preprocessing step was not perfect, and we subse-
quently discovered it had introduced some noise into the data.
<p>
First I must say that most <#UT>of</#UT>
people don?t see any problems with
<#RV>growing|increasing</#RV>
<#RD>a|the</#RD> list of
<#UP>car?s|car</#UP> owners.
Some of them think that it shows how
<#SX>reach|rich</#SX> our country is.
</p>
Figure 1: A fragment of an FCE data file
<edit type="RD" index="0005"
file="0006" part="1"
start="427" end="428">
<original>a</original>
<corrections>
<correction>the</correction>
</corrections>
</edit>
Figure 2: A standoff error annotation
of the different error types in the training and test
data are provided in Table 3, demonstrating that the
error rate remained fairly constant across training
and test data. However, whereas the training data
included information on author?s first language (L1)
and age range, the L1 information was not present
in the test data, thus removing a potentially useful
feature that some teams may have hoped to exploit.
4.2 Revisions to the Gold-Standard Data
Note that Table 3 shows counts for two versions of
the gold-standard test data: the original version as
derived from the CUP-provided data set (?Test A?),
and a revised version (?Test B?) which incorporates
corrections to errors found in the annotations.
The evaluation process quickly revealed that there
appeared to be cases of annotation error in the orig-
inal test data. This concerned us because it meant
that system performance was being under-reported:
56
Type # Training # Test A # Test B
UT 822 43 39
MT 1105 57 56
RT 2618 136 148
Prep 4545 236 243
UD 1048 53 62
MD 2230 125 131
RD 609 39 37
Det 3887 217 230
Total 8432 453 473
Words/Error 44.18 39.77 38.08
Table 3: Data statistics
in particular, systems were identifying real errors in
the source texts which had not been annotated in
the gold standard, and were consequently being pe-
nalised for finding spurious errors which were not in
fact spurious.
To address this problem, once teams had submit-
ted their results, we allowed a brief period where
teams could review the gold-standard data to iden-
tify possible corrections to that data. Table 4 shows
the number of revisions requested by each team, and
the number of these revisions that were accepted.
Note that there were a significant number of revi-
sions (99) that were requested by more than one
team, so the total count of revision requests is larger
than the actual number of revisions considered. Of
the total 357 requests, 205 were acted on, in some
cases not in the manner requested by the team; 152
requests led to no changes being made to the anno-
tations.
Note that the teams? original sets of submitted ed-
its were compared against this revised gold standard,
so there was no sense in which a system?s behaviour
could be tuned to the test data. However, clearly
any given team might stand to benefit from iden-
tifying particular errors their system had identified
that were not in the gold standard, effectively tuning
the test data to system behaviour. Consequently, we
provide results below for both the original and the
revised data sets, and briefly discuss the impact of
these corrections.
5 Results
Each team was allowed to submit up to 10 sepa-
rate ?runs? over the test data, thus allowing them to
Team Requested Acted On
CU 51 30
ET 22 18
LE 5 5
NU 83 59
UI 151 54
UT 45 39
Totals 357 205
Table 4: Requests for corrections to the gold-
standard data
have different configurations of their systems evalu-
ated; the number of runs submitted by each team is
shown in Table 1. We report here only on the best-
performing runs from each team.
Teams were asked to indicate whether they had
used only publicly-available data to train their sys-
tems, or whether they had made use of privately-held
data: only the ET and CU teams used privately-held
data, and in the latter case only for a subset of their
runs. In the tabulated results provided here, reported
runs that involve privately-held data are marked with
an asterisk.
The results of the evaluation are provided here in
six tables. Tables 5 and 6 provide results for prepo-
sition and determiner errors combined; Tables 7 and
8 provide results for preposition errors only; and Ta-
bles 9 and 10 provide results for determiner errors
only. In each pair, the first table shows results before
the revisions described in Section 4.2 were carried
out, and the second table shows the results using the
revised gold-standard data. Each table shows preci-
sion, recall and F-score (computed as the harmonic
mean) for each of detection, recognition and correc-
tion; for each of these, the best score is shown in
bold.6 Note that team ET did not participate in the
correction subtask.
The scores for all teams improve as a consequence
of the revisions being made to the data. The result of
a paired t-test on the ?before? and ?after? combined
preposition and determiner scores across teams was
statistically significant (t = ?3.17, df(12), p < .01);
6The precise definitions of these measures as imple-
mented in the evaluation tool, and further details on
the evaluation process, are provided in (Dale and Nar-
roway, 2012) and elaborated on at the HOO website at
www.correcttext.org/hoo2012.
57
F-scores improved by a mean value of 2.32. The
same analyses for preposition scores also resulted
in significant improvement (t = ?3.29, df(12),
p < .01), with a mean improvement in F-scores
of 2.6. A smaller (but still statistically significant)
improvement in determiner scores was also present
(t = ?2.86, df(12), p < .05), with a mean improve-
ment in F-scores of 1.99.
There are also positive correlations between the
rankings before and after revisions. Pearson corre-
lation coefficients for the ?before? and ?after? scores
for prepositions and determiners combined, prepo-
sitions only, and determiners only (respectively) are
.993, .985 and .996. All correlation coefficients are
significant at p < .001, n = 13 (teams).
However, some systems improve more than oth-
ers. An obvious question to ask, then, is whether the
benefit that a team achieves is positively correlated
with the number of accepted corrections they pro-
posed; a calculation of Pearson?s correlation coeffi-
cient suggests that this is indeed the case (r = 0.821,
p = 0.044 (one-tailed)).7 This suggests, then, that
the ?before? results may be a more reliable indicator
of comparative performance.
6 Discussion and Conclusions
In this section, we make some observations on
lessons learned with regard to various aspects of the
shared task.
6.1 Data Acquisition
Data annotated with non-native speaker errors has
significant commercial value, and so is not easy
to find in the public domain. We were fortunate
to be able to take advantage of the recently-made-
available FCE dataset as training data, but this left
us with the problem of acquiring previously unseen
test data. To address this, we entered into negotia-
tion with Cambridge University Press with the aim
of acquiring some additional previously unreleased
data. We started this process in December 2011, but
it quickly became apparent that some of the legal
aspects would necessarily make this a slow process.
7Computed here on the combined preposition and deter-
miner scores, and taking account only of the five teams that
proposed corrections, these being UI, NU, LE, UT and CU. ET
was not included in this calculation since they did not submit to
the corrections subtask.
As a back-up plan, we informed teams that we might
have to fall back on some of the already-available
FCE data as test data; to this end, we asked teams
only to use versions and subsets of the FCE data that
we made directly available. We thus selected 1000
files from the 1200 that make up the public FCE data
set as training data, and reserved the remainder as a
source of possible test data.
This is clearly not an ideal situation; fortunately,
we finally signed agreements for the use of a new
set of FCE data in the week before the test data was
due to be released, but this was leaving things rather
tight. The moral here is that one needs to be confi-
dent of one?s data sources early on in the process.
6.2 Data Quality
As discussed above, it became apparent that there
were what appeared to be annotation errors in our
data. This is perhaps inevitable given the nature of
the source data, which was annotated by only one
annotator (subsequent to some prior automatic pro-
cessing). The issue of reliability of annotation in this
area has been noted by others (see, for example, the
discussion in Chapter 5 in (Leacock et al, 2010)).
Assuming that we agree an error is present?and this
is not always in itself straightforward?there is often
more than one way to correct that error; however,
the FCE annotation scheme does not permit multiple
possible corrections, so in the source data we used,
there was only ever one correction per error. Our
revision process identified a number of cases where
alternative corrections were equally acceptable, and
fortunately the HOO annotation scheme allowed us
to incorporate multiple possible corrections; but it?s
quite clear that we did not identify all cases where
multiple corrections were valid.8
This is a significant issue. If we cannot entirely
trust our gold-standard data, then we cannot place
too much trust in the results of evaluations carried
out using that data. Of course, annotation quality
is a problem in any task, but it may be more se-
vere in cases like the present one because the judge-
ments here are often less clear cut: whereas there
is rarely dispute as to whether a given string consti-
tutes a named entity, it is not always so clear that
8The HOO scheme also allows optional edits, but we did not
make use of these here since it complicates the scoring process;
see (Dale and Kilgarriff, 2011) for discussion.
58
Detection Recognition Correction
Team Run P R F Run P R F Run P R F
CU 2 13.12 34.88 19.07 7 8.13 41.5 13.6 0 70.0 4.64 8.7
ET 1 33.59* 37.97* 35.65* 1 30.27* 34.22* 32.12* ? ? ? ?
JU 1 6.93 7.28 7.1 1 6.3 6.62 6.46 1 2.52 2.65 2.58
KU 0 4.61 49.23 8.43 0 2.67 28.48 4.88 0 1.45 15.45 2.65
LE 0 37.38 26.49 31.01 0 33.33 23.62 27.65 0 31.15 22.08 25.84
NA 3 40.19 28.04 33.03 3 36.39 25.39 29.91 3 29.43 20.53 24.19
NU 0 57.42 26.49 36.25 0 55.98 25.83 35.35 0 45.45 20.97 28.7
TC 9 5.33 25.61 8.82 9 4.18 20.09 6.92 9 2.66 12.8 4.41
TH 1 17.74 48.12 25.92 1 15.38 41.72 22.47 1 9.44 25.61 13.79
UD 2 8.94 31.13 13.88 2 5.51 19.21 8.57 2 1.2 4.19 1.87
UI 8 37.22 43.71 40.2 1 34.23 36.64 35.39 1 26.39 28.26 27.29
UT 6 37.46 25.39 30.26 7 32.01 23.18 26.89 7 21.95 15.89 18.44
VA 3 12.5 15.23 13.73 3 10.87 13.25 11.94 3 6.16 7.51 6.77
VT 5 10.6 5.08 6.87 5 10.14 4.86 6.57 5 8.76 4.19 5.67
Table 5: Results before revisions, all errors
Detection Recognition Correction
Team Run P R F Run P R F Run P R F
CU 2 14.04 35.73 20.16 7 8.69 42.49 14.43 6 5.73 28.54 9.54
ET 1 38.09* 41.23* 39.59* 1 35.55* 38.48* 36.95* ? ? ? ?
JU 1 8.19 8.25 8.22 1 7.56 7.61 7.59 1 3.15 3.17 3.16
KU 0 5.01 51.16 9.12 0 3.04 31.08 5.54 0 1.86 19.03 3.39
LE 0 41.12 27.91 33.25 0 36.45 24.74 29.47 0 34.27 23.26 27.71
NA 3 45.25 30.23 36.25 3 40.82 27.27 32.7 3 33.86 22.62 27.12
NU 0 70.33 31.08 43.11 0 69.38 30.66 42.52 0 61.72 27.27 37.83
TC 8 6.56 26.0 10.48 8 4.91 19.45 7.84 8 3.09 12.26 4.94
TH 1 19.2 49.89 27.73 1 17.33 45.03 25.03 1 10.82 28.12 15.63
UD 2 9.95 33.19 15.31 2 5.77 19.24 8.87 2 1.33 4.44 2.05
UI 2 43.56 42.92 43.24 1 38.97 39.96 39.46 1 32.58 33.4 32.99
UT 7 39.94 27.7 32.71 7 35.67 24.74 29.21 5 31.58 17.76 22.73
VA 3 13.22 15.43 14.24 3 11.59 13.53 12.49 3 7.25 8.46 7.8
VT 5 11.52 5.29 7.25 5 11.06 5.07 6.96 5 9.68 4.44 6.09
Table 6: Results after revisions, all errors
something is an error, or where that error should be
located. The incorporation of optional and multiple
corrections in the HOO framework was intended to
address this kind of problem, but the value of these
features is only delivered if the scheme is used dur-
ing annotation, rather than being applied after anno-
tation has already been carried out.
6.3 The Annotation Scheme and Evaluation
Tools
Given real non-native speaker data that contains a
wide range of errors other than those that we were
particularly concerned with in this shared task, we
were faced with three alternatives in how we pre-
pared the data for use in the task.
1. We could provide the data with all original er-
rors in place.
2. We could provide the data with all but the
preposition and determiner errors corrected.
3. We could provide the data with selected errors
corrected or replaced.
The problem with the first of these options, of
course, is that other errors that appear in the context
59
Detection Recognition Correction
Team Run P R F Run P R F Run P R F
CU 2 14.88 59.32 23.79 2 9.99 39.83 15.97 0 61.11 4.66 8.66
ET 1 31.95* 42.37* 36.43* 1 27.16* 36.02* 30.97* ? ? ? ?
JU 1 6.1 7.63 6.78 1 5.42 6.78 6.03 1 3.05 3.81 3.39
KU 0 3.39 66.95 6.46 0 2.51 49.58 4.79 0 1.27 25.0 2.41
LE 0 32.81 17.8 23.08 0 27.34 14.83 19.23 0 25.78 13.98 18.13
NA 6 41.13 24.58 30.77 3 36.43 19.92 25.75 3 30.23 16.53 21.37
NU 0 56.99 22.46 32.22 0 53.76 21.19 30.4 0 41.94 16.53 23.71
TC 9 6.49 29.66 10.65 9 5.19 23.73 8.52 9 3.06 13.98 5.02
TH 1 17.39 59.32 26.9 1 14.16 48.31 21.9 1 9.19 31.36 14.22
UD 2 11.84 36.86 17.92 2 9.66 30.08 14.62 1 7.63 4.24 5.45
UI 1 38.21 45.34 41.47 5 31.05 40.25 35.06 1 20.36 24.15 22.09
UT 2 39.35 25.85 31.2 7 35.76 22.88 27.91 0 25.45 11.86 16.18
VA 0 13.44 14.41 13.91 0 11.46 12.29 11.86 0 7.51 8.05 7.77
VT 7 12.24 2.54 4.21 7 12.24 2.54 4.21 7 12.24 2.54 4.21
Table 7: Results before revisions, preposition errors only
Detection Recognition Correction
Team Run P R F Run P R F Run P R F
CU 2 15.41 59.43 24.47 2 10.63 40.98 16.88 0 66.67 4.92 9.16
ET 1 35.14* 45.08* 39.5* 1 32.27* 41.39* 36.27* ? ? ? ?
JU 1 7.12 8.61 7.79 1 6.44 7.79 7.05 1 3.73 4.51 4.08
KU 0 3.67 70.08 6.98 0 2.9 55.33 5.51 0 1.7 32.38 3.23
LE 0 35.16 18.44 24.19 0 29.69 15.57 20.43 0 28.13 14.75 19.35
NA 6 48.23 27.87 35.32 6 41.84 24.18 30.65 6 33.33 19.26 24.42
NU 0 72.04 27.46 39.76 0 70.97 27.05 39.17 0 60.22 22.95 33.23
TC 8 7.72 29.92 12.27 8 5.92 22.95 9.41 9 3.34 14.75 5.45
TH 1 18.76 61.89 28.79 1 16.27 53.69 24.98 1 10.68 35.25 16.4
UD 2 12.65 38.11 19.0 2 10.2 30.74 15.32 1 9.16 4.92 6.4
UI 1 41.43 47.54 44.27 1 37.14 42.62 39.69 1 26.79 30.74 28.63
UT 2 41.94 26.64 32.58 2 39.35 25.0 30.58 0 35.45 15.98 22.03
VA 0 14.23 14.75 14.49 0 12.65 13.11 12.88 0 8.7 9.02 8.85
VT 7 16.33 3.28 5.46 7 16.33 3.28 5.46 7 16.33 3.28 5.46
Table 8: Results after revisions, preposition errors only
of a preposition or determiner error could confuse a
system focussed only on preposition or determiner
errors; if the surrounding context contains errors,
then it cannot be relied upon to deliver the kinds
of features that one would expect to find in well-
formed text. To partially address this, many teams
ran a spelling correction process on the texts prior
to applying their techniques; but this only catches a
small proportion of the potential problems.
However, the second option has the opposite
problem: by removing all the other errors from the
text, we would be providing a very artificial dataset
where one assumes some other process has fixed all
the other errors before the errors of interest here
are addressed. While there are some types of er-
rors that might sensibly be addressed before others
in a pipeline, in general this is not a very plausible
model; any real system is going to have to address
noisy data containing many different kinds of errors
simultaneously.
A third alternative, that of selectively removing
or correcting errors, is something of a middle road,
and has been used in other work using the CLC data:
in particular, Gamon (2010) removes from the data
sentences where some other error appears immedi-
ately next to a preposition or determiner error.
60
Detection Recognition Correction
Team Run P R F Run P R F Run P R F
CU 6 7.8 49.31 13.48 6 6.86 43.32 11.84 6 5.25 33.18 9.07
ET 0 51.67* 28.57* 36.8* 0 50.83* 28.11* 36.2* ? ? ? ?
JU 1 7.73 6.45 7.04 1 7.73 6.45 7.04 1 1.66 1.38 1.51
KU 0 12.85 10.6 11.62 0 6.7 5.53 6.06 0 6.15 5.07 5.56
LE 0 40.41 35.94 38.05 0 37.31 33.18 35.12 0 34.72 30.88 32.68
NA 1 37.43 32.26 34.65 1 36.36 31.34 33.66 1 28.88 24.88 26.73
NU 0 57.76 30.88 40.24 0 57.76 30.88 40.24 0 48.28 25.81 33.63
TC 3 8.68 8.76 8.72 3 7.76 7.83 7.8 3 4.11 4.15 4.13
TH 1 17.69 34.56 23.4 1 17.69 34.56 23.4 1 9.91 19.35 13.1
UD 2 6.41 24.88 10.19 1 1.98 6.45 3.03 0 0.0 0.0 0.0
UI 0 40.0 37.79 38.86 0 38.05 35.94 36.97 0 35.61 33.64 34.6
UT 5 34.38 25.35 29.18 5 31.87 23.5 27.06 6 25.75 19.82 22.4
VA 3 11.04 15.21 12.79 3 10.37 14.29 12.02 3 5.02 6.91 5.81
VT 5 9.82 7.37 8.42 5 9.82 7.37 8.42 5 7.98 5.99 6.84
Table 9: Results before revisions, determiner errors only
Detection Recognition Correction
Team Run P R F Run P R F Run P R F
CU 6 8.53 51.09 14.63 6 7.37 44.1 12.63 6 5.91 35.37 10.13
ET 0 57.5* 30.13* 39.54* 0 56.67* 29.69* 38.97* ? ? ? ?
JU 1 9.39 7.42 8.29 1 9.39 7.42 8.29 1 2.21 1.75 1.95
KU 0 14.53 11.35 12.75 0 6.7 5.24 5.88 0 6.15 4.8 5.39
LE 0 44.56 37.55 40.76 0 40.93 34.5 37.44 0 38.34 32.31 35.07
NA 1 41.18 33.62 37.02 1 39.57 32.31 35.58 1 33.16 27.07 29.81
NU 0 68.1 34.5 45.8 0 68.1 34.5 45.8 0 62.93 31.88 42.32
TC 8 5.17 20.96 8.3 3 7.31 6.99 7.14 8 2.8 11.35 4.49
TH 1 19.34 35.81 25.11 1 19.34 35.81 25.11 1 11.08 20.52 14.4
UD 1 8.07 24.89 12.19 1 1.98 6.11 2.99 0 0.0 0.0 0.0
UI 0 43.9 39.3 41.47 2 45.98 34.93 39.7 0 41.46 37.12 39.17
UT 5 39.38 27.51 32.39 5 35.63 24.89 29.31 6 30.54 22.27 25.76
VA 3 11.71 15.28 13.26 3 10.7 13.97 12.12 3 6.02 7.86 6.82
VT 5 9.82 6.99 8.16 5 9.82 6.99 8.16 5 7.98 5.68 6.63
Table 10: Results after revisions, determiner errors only
In the end, we opted for the first alternative here,
on the grounds that this is the best approximation to
the real task of non-native speaker error correction.
The third alternative would also have been possible,
but we were concerned about the impact on the size
of our test data set that would result from carrying
out this process across the board. However, in the re-
vision step described in Section 4.2, we did remove
instances of a particular error type, where a preposi-
tion error was immediately followed by a verb error;
consider the following sentence and its correction.
(1) a. What do you do for trying to save the wild
life?
b. What do you do to try to save the wild life?
The compound nature of these errors meant that
teams were unlikely to correct them; and it might be
argued that they are not preposition errors in the con-
ventional sense. However, we did not remove these
instances uniformly, so some still remain in the test
data.
An orthogonal issue with regard to the HOO an-
notation scheme is that we require precise identifi-
cation of error locations and accurate specification
of these locations at a character-offset level in our
61
standoff edit notation. It is often inaccuracies at
this level that contribute to the differences between a
team?s detection score and the corresponding recog-
nition score. While precise character offset infor-
mation is important for some error correction tasks
(for example, one would not want an automated cor-
rector to insert corrections misplaced by one charac-
ter), arguably it is too strict in the present circum-
stances. Dahlmeier and Ng (2012) propose an al-
ternative evaluation scheme which, along with other
properties, overcomes this by operating in terms of
tokens rather than character offsets.
6.4 Summary
Overall, we were immensely pleased with the level
of interest in this shared task. The HOO 2012 train-
ing data and evaluation tools are publicly available,
so interested parties who did not take part in the
shared task can still try their hand retrospectively;
unfortunately, our contract with CUP means that the
test data used in this round is not publicly available.
Our future plans include packaging a subset of the
initially held-out public FCE data set as a new test
set, with the aim of establishing a standardised train-
ing and testing setup in the same way as Section 23
of the Wall Street Journal corpus is conventionally
used as a test set. We have strongly encouraged the
use of publicly available data sets, and have asked
teams to be as detailed as possible in their reports in
the interests of replicability; we hope this will make
it possible for new entrants to the area to get up to
speed quickly.
Of course, the FCE data also supports work on
many other kinds of errors. We expect to address
subsets of these in future HOO rounds.
Acknowledgements
We?d like to acknowledge the kind assistance of
various people in making this shared task possi-
ble: Ted Briscoe for seeding the enterprise by work-
ing to make the FCE data publicly available; Di-
ane Nicholls and Adam Kilgarriff for encourage-
ment and advice along the way; Ann Fiddes at Cam-
bridge University Press for providing the previously
unseen test data; Richard Cox for statistics; and
Joel Tetreault, Claudia Leacock and Jill Burstein for
agreeing to host the shared task at the Building Ed-
ucational Applications Workshop. Macquarie Uni-
versity provided financial support via a Research
Development Grant. Finally, of course, we?d like
to thank all the teams for participating.
References
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better evalu-
ation for grammatical error correction. In Proceedings
of the 2012 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, Montre?al, Canada, 3rd?
8th June 2012.
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text massaging for computational linguistics
as a new shared task. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
pages 261?266, Dublin, Ireland, 7th?9th July 2010.
Robert Dale and Adam Kilgarriff. 2011. Helping our
own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation, Nancy, France, 28th?30th Septem-
ber 2011.
Robert Dale and George Narroway. 2012. A frame-
work for evaluating text correction. In Proceed-
ings of the Eighth International Conference on Lan-
guage Resources and Evaluation (LREC2012), Istan-
bul, Turkey, 21st?27th May 2012.
Michael Gamon. 2010. Using mostly native data to cor-
rect errors in learners writing. In Proceedings of the
Eleventh Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL), pages 163?171, Los Angeles, USA.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical Er-
ror Detection for Language Learners. Synthesis Lec-
tures on Human Language Technologies. Morgan and
Claypool.
Diane Nicholls. 2003. The Cambridge Learner
Corpus?error coding and analysis for lexicography
and ELT. In D Archer, P Rayson, A Wilson, and
T McEnery, editors, Proceedings of the Corpus Lin-
guistics 2003 Conference, pages 572?581, Lancaster,
UK, 29th March?2nd April 2001.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, Portland, Ore-
gon, USA, 19th?24th June 2011.
62
