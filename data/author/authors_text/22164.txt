Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 200?209,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Filling the Gap:
Semi-Supervised Learning for Opinion Detection Across Domains
Ning Yu
Indiana University
nyu@indiana.edu
Sandra Ku?bler
Indiana University
skuebler@indiana.edu
Abstract
We investigate the use of Semi-Supervised
Learning (SSL) in opinion detection both in
sparse data situations and for domain adapta-
tion. We show that co-training reaches the best
results in an in-domain setting with small la-
beled data sets, with a maximum absolute gain
of 33.5%. For domain transfer, we show that
self-training gains an absolute improvement in
labeling accuracy for blog data of 16% over
the supervised approach with target domain
training data.
1 Introduction
Rich and free opinions published electronically and,
more recently, on the WWW offer ample opportuni-
ties to discover individual?s attitudes towards certain
topics, products, or services. To capitalize on this
enormous body of opinions, researchers have been
working in the area of opinion mining since the late
1990s. Opinion detection seeks to automatically de-
termine the presence or absence of opinions in a text,
and it is therefore a fundamental task for opinion
mining.
In order to capture subtle and creative opinions,
opinion detection systems generally assume that a
large body of opinion-labeled data are available.
However, collections of opinion-labeled data are of-
ten limited, especially at the granularity level of sen-
tences; and manual annotation is tedious, expensive
and error-prone. The shortage of opinion-labeled
data is less challenging in some data domains (e.g.,
reviews) than in others (e.g., blog posts). A sim-
ple method for improving accuracies in challenging
domains would be to borrow opinion-labeled data
from a non-target data domain; but this approach
often fails because opinion detection strategies de-
signed for one data domain generally do not perform
well in another domain. One reason for failure of
the simple transfer approach is that the information
used for opinion detection is typically lexical, and
lexical means of expressing opinions may vary not
only from domain to domain, but also from register
to register. For example, while the word ?awesome?
is a good indicator of an opinion in blogs, it is less
likely to occur in the same role in newspaper texts.
While it is difficult to obtain opinion-labeled data,
one can easily collect almost infinite unlabeled user-
generated data that contain opinions. The use of
Semi-Supervised Learning (SSL), motivated by lim-
ited labeled data and plentiful unlabeled data in the
real world, has achieved promising results in vari-
ous NLP studies (e.g., (Fu?rstenau and Lapata, 2009;
Talukdar and Pereira, 2010)), yet it has not been
fully investigated for use in opinion detection. Al-
though studies have shown that simple SSL meth-
ods are promising for extracting opinion features
or patterns using limited opinion-labeled data (e.g.,
(Wiebe and Riloff, 2005)), few efforts have been
made either to apply SSL directly to opinion detec-
tion or to examine more sophisticated SSL methods.
This research is intended to fill the gap regarding ap-
plication of SSL in opinion detection. We investi-
gate a range of SSL algorithms with a focus on self-
training and co-training in three types of electronic
documents: edited news articles, semi-structured
movie reviews, and the informal and unstructured
content of the blogosphere. We conclude that SSL
is a successful method for handling the shortage of
opinion labeled data and the domain transfer prob-
lem.
200
2 Background and Related Work
There is a wide range of literature on opinion detec-
tion. We concentrate here on supervised and semi-
supervised approaches.
2.1 Supervised Learning for Opinion Detection
Supervised learning algorithms that can automati-
cally learn important opinion-bearing features from
an annotated corpus have been adopted and inves-
tigated for opinion detection and yielded satisfying
results (Wiebe et al, 2004; Yu and Hatzivassiloglou,
2003; Zhang and Yu, 2007). With no classifica-
tion techniques developed specifically for opinion
detection, state-of-the-art topical supervised classifi-
cation algorithms can achieve performance compa-
rable to complex linguistic approaches when using
binary values (i.e., presence or absence) and incor-
porating different types of features. Commonly used
opinion-bearing features include bag-of-words, POS
tags, ngrams, low frequency words or unique words
(Wiebe et al, 2004; Yang et al, 2007), semantically
oriented adjectives (e.g., ?great?, ?poor?) and more
complex linguistic patterns. Both the scale and qual-
ity of the annotated corpus play an important role in
the supervised learning approach.
2.2 SSL for Opinion Detection
In contrast to supervised learning, SSL learns from
both labeled and unlabeled data. SSL assumes that,
although unlabeled data hold no information about
classes (e.g., ?opinion? or ?non-opinion?), they do
contain information about joint distribution over
classification features. Therefore, when a limited set
of labeled data is available in the target domain, us-
ing SSL with unlabeled data is expected to achieve
an improvement over supervised learning.
Self-training Self-training is the simplest and
most commonly adopted form of SSL for opinion
detection. Self-training was originally used to fa-
cilitate automatic identification of opinion-bearing
features. For example, Riloff and Wiebe (2003) pro-
posed a bootstrapping process to automatically iden-
tify subjective patterns. Self-training has also been
applied directly for identifying subjective sentences
by following a standard self-training procedure: (1)
train an initial supervised classifier on the labeled
data; (2) apply this classifier to unlabeled data and
select the most confidently labeled data, as deter-
mined by the classifier, to augment the labeled data
set; and (3) re-train the classifier by restarting the
whole process. Wiebe and Riloff (2005) used a self-
trained Na??ve Bayes classifier for classifying subjec-
tive sentences and achieved better recall with modest
precision over several rule-based classifiers.
One shortcoming of self-training is that the result-
ing data may be biased: That is, the final labeled data
may consist of examples that are easiest for this par-
ticular opinion detector to identify.
Co-training The core idea of co-training is to use
two classifiers and trade additional examples be-
tween them, assuming that the resulting union of
classified examples is more balanced than examples
resulting from using either classifier alone. When
labeling new examples, a final decision is made by
combining the predictions of the two updated learn-
ers. The original co-training algorithm assumes re-
dundancy in the training data and thus more than
one view can be used to represent and classify each
example independently and successfully (Blum and
Mitchell, 1998). For example, an image can be nat-
urally represented by its text description or by its
visual attributes. Even when a natural split in the
feature set is not available, studies have shown that
the key to co-training is the existence of two largely
different initial learners, regardless of whether they
are built by using two feature sets or two learning
algorithms (Wang and Zhou, 2007).
When there are different views for the target ex-
amples, co-training is conceptually clearer than self-
training, which simply mixes features. Since co-
training uses each labeled example twice, it requires
less labeled data and converges faster than self-
training. However, the lack of natural feature splits
has kept researchers from exploring co-training for
opinion detection. To the best of our knowledge,
the only co-training application for opinion detec-
tion was reported by Jin et al (2009), who created
disjoint training sets for building two initial classi-
fiers and successfully identified opinion sentences in
camera reviews by selecting auto-labeled sentences
agreed upon by both classifiers.
EM-Based SSL Expectation-Maximization (EM)
refers to a class of iterative algorithms for
maximum-likelihood estimation when dealing with
201
incomplete data. Nigam et al (1999) combined
EM with a Na??ve Bayes classifier to resolve the
problem of topical classification, where unlabeled
data were treated as incomplete data. The EM-NB
SSL algorithm yielded better performance than ei-
ther an unsupervised lexicon-based approach or a
supervised approach for sentiment classification in
different data domains, including blog data (Aue and
Gamon, 2005; Takamura et al, 2006). No opinion
detection applications of EM-based SSL have been
reported in the literature.
S3VMs Semi-Supervised Support Vector Ma-
chines (S3VMs) are a natural extension of SVMs in
the semi-supervised spectrum. They are designed to
find the maximal margin decision boundary in a vec-
tor space containing both labeled and unlabeled ex-
amples. Although SVMs are the most favored super-
vised learning method for opinion detection, S3VMs
have not been used in opinion detection. Graph-
based SSL learning has been successfully applied to
opinion detection (Pang and Lee, 2004) but is not
appropriate for dealing with large scale data sets.
2.3 Domain Adaptation for Opinion Detection
When there are few opinion-labeled data in the
target domain and/or when the characteristics of
the target domain make it challenging to detect
opinions, opinion detection systems usually borrow
opinion-labeled data from other data domains. This
is especially common in opinion detection in the bl-
ogosphere (Chesley et al, 2006). To evaluate this
shallow approach, Aue and Gamon (2005) com-
pared four strategies for utilizing opinion-labeled
data from one or more non-target domains and con-
cluded that using non-targeted labeled data without
an adaptation strategy is less efficient than using la-
beled data from the target domain, even when the
majority of labels are assigned automatically by a
self-training algorithm.
Blitzer et al (2007) and Tan et al (2009) imple-
mented domain adaptation strategies for sentiment
analysis. Although promising, their domain adapta-
tion strategies involved sophisticated and computa-
tionally expensive methods for selecting general fea-
tures to link target and non-target domains.
3 Motivation and Objective
While SSL is especially attractive for opinion de-
tection because it only requires a small number of
labeled examples, the studies described in the previ-
ous section have concentrated on simple SSL meth-
ods. We intend to fill this research gap by comparing
the feasibility and effectiveness of a range of SSL
approaches for opinion detection. Specifically, we
aim to achieve the following goals:
First, to gain a more comprehensive understand-
ing of the utility of SSL in opinion detection. We
examine four major SSL methods: self-training, co-
training, EM-NB, and S3VM. We focus on self-
training and co-training because they are both wrap-
per approaches that can be easily adopted by any ex-
isting opinion detection system.
Second, to design and evaluate co-training strate-
gies for opinion detection. Since recent work has
shown that co-training is not restricted by the orig-
inal multi-view assumption for target data and that
it is more robust than self-training, we evaluate new
co-training strategies for opinion detection.
Third, to approach domain transfer using SSL,
assuming that SSL can overcome the problem of
domain-specific features by gradually introducing
targeted data and thus diminishing bias from the
non-target data set.
4 SSL Experiments
Our research treats opinion detection as a binary
classification problem with two categories: subjec-
tive sentences and objective sentences. It is evalu-
ated in terms of classification accuracy.
Since a document is normally a mixture of facts
and opinions (Wiebe et al, 2001), sub-document
level opinion detection is more useful and meaning-
ful than document-level opinion detection. Thus, we
conduct all experiments on the sentence level.
The remainder of this section explains the data
sets and tools used in this study and presents the ex-
perimental design and parameter settings.
4.1 Data Sets
Three types of data sets have been explored in opin-
ion detection studies: news articles, online reviews,
and online discourse in blogs or discussion forums.
These three types of text differ from one another in
202
terms of structure, text genre (e.g., level of formal-
ity), and proportion of opinions found therein. We
selected a data set from each type in order to inves-
tigate the robustness and adaptability of SSL algo-
rithms for opinion detection and to test the feasibil-
ity of SSL for domain adaptation.
Movie Review One of the standard data sets in
opinion detection is the movie review data set cre-
ated by Pang and Lee (2004). It contains 5,000 sub-
jective sentences or snippets from the Rotten Toma-
toes pages and 5,000 objective sentences or snip-
pets from IMDB plot summaries, all in lowercase.
Sentences containing less than 10 tokens were ex-
cluded and the data set was labeled automatically
by assuming opinion inheritance: every sentence in
an opinion-bearing document expresses an opinion,
and every sentence in a factual document is factual.
Although this assumption appears to be acceptable
for movie review data, it is generally unreliable for
other domains.
News Article The Wall Street Journal part of the
Penn Treebank III has been manually augmented
with opinion related annotations. This set is widely
used as a gold-standard corpus in opinion detection
research. According to the coding manual (Wiebe
et al, 1999), subjective sentences are those express-
ing evaluations, opinions, emotions, and specula-
tions. For our research, 5,174 objective sentences
and 5,297 subjective sentences were selected based
on the absence or presence of manually labeled sub-
jective expressions.
JDPA Blog Post The JDPA corpus (Kessler et al,
2010) is a new opinion corpus released in 2010. It
consists of blog posts that express opinions about
automobile and digital cameras with named entities
and sentiments expressed about them manually an-
notated. For our purpose, we extracted all sentences
containing sentiment-bearing expressions as subjec-
tive sentences and manually chose objective sen-
tences from the rest by eliminating subjective sen-
tences that were not targeted to any labeled entities.
After this process, we had approximately 10,000
subjective sentences and 4,348 objective sentences.
To balance the number of subjective and objective
sentences, we used 4,348 sentences from each cate-
gory.
4.2 Data Preparation
We removed a small number of stop words. No
stemming was conducted since the literature shows
no clear gain from stemming in opinion detection.
One reason for this may be that stemming actually
erases subtle opinion cues such as past tense verbs.
All words were converted to lowercase and numbers
were replaced by a placeholder #. Both unigrams
and bigrams were generated for each sentence.
Each data set was randomly split into three por-
tions: 5% of the sentences were selected as the eval-
uation set and were not available during SSL and
supervised learning (SL) runs; 90% were treated as
unlabeled data (U) for SSL runs and i% (1 ? i ? 5)
as labeled data (L) for both SL and SSL runs. For
each SSL run, a baseline SL run was designed with
the same number of labeled sentences (i%) and a
full SL run was designed with all available sentences
(90% + i%). If effective, an SSL run would signifi-
cantly outperform its corresponding baseline SL run
and approach the performance of a full SL run.
4.3 Experimental Design
We conducted three groups of experiments: 1) to in-
vestigate the effectiveness of the SSL approach for
opinion detection; 2) to explore different co-training
strategies; and 3) to evaluate the applicability of SSL
for domain adaptation.
4.3.1 General Settings for SSL
The Na??ve Bayes classifier was selected as the
initial classifier for self-training because of its abil-
ity to produce prediction scores and to work well
with small labeled data sets. We used binary values
for unigram and bigram features, motivated by the
brevity of the text unit at the sentence level as well
as by the characteristics of opinion detection, where
occurrence frequency has proven to be less influen-
tial. We implemented two feature selection options:
Chi square and Information Gain.
Parameters for SSL included: (1) Threshold k for
number of iterations. If k is set to 0, the stopping
criterion is convergence; (2) Number of unlabeled
sentences available in each iteration u (u << U );
(3) Number of opinion and non-opinion sentences,
p and n, to augment L during each iteration; and (4)
Weighting parameter ? for auto-labeled data. When
? is set to 0, auto-labeled and labeled data are treated
203
equally; when ? is set to 1, feature values in an
auto-labeled sentence are multiplied by the predic-
tion score assigned to the sentence.
We used the WEKA data mining software (Hall
et al, 2009) for data processing and classifica-
tion of the self-training and co-training experi-
ments. EM implemented in LingPipe (Alias-i, 2008)
was used for the EM-NB runs. S3VMs imple-
mented in SVMlight (Joachims, 1999) and based
on local search were adopted for the S3VM runs.
Since hyper-parameter optimization for EM-NB and
S3VM is not the focus of this research and prelim-
inary explorations on parameter settings suggested
no significant benefit, default settings were applied
for EM-NB and S3VM.
4.3.2 Co-Training Strategies
For co-training, we investigated five strategies for
creating two initial classifiers following the criteria
that these two classifiers either capture different fea-
tures or based on different learning assumptions.
Two initial classifiers were generated: (1) Us-
ing unigrams and bigrams respectively to create two
classifiers based on the assumption that low-order
n-grams and high-order n-grams contain redundant
information and represent different views of an ex-
ample: content and context; (2) Randomly splitting
feature set into two; (3) Randomly splitting train-
ing set into two; (4) Applying two different learn-
ing algorithms (i.e., Na??ve Bayes and SVM) with
different biases; and (5) Applying a character-based
language model (CLM) and a bag-of-words (BOW)
model where the former takes into consideration the
sequence of words while the latter does not. In prac-
tice, for strategy (1), bigrams were used in combina-
tion with unigrams because bigrams alone are weak
features when extracted from limited labeled data at
sentence level.
Auto-labeled sentences were selected if they were
assigned a label that both classifiers agreed on with
highest confidence. Because our initial classifiers vi-
olated the original co-training assumptions, forcing
agreement between confident predictions improved
the maintenance of high precision.
4.3.3 Self-Training for Domain Adaptation
Based on the literature and our preliminary re-
sults (Yu and Ku?bler, 2010), movie reviews achieve
# Labeled Examples
Type 100 200 300 400 500 all
Self-tr 85.2 86.6 87.0 87.2 86.6
SL 63.8 73.6 77.2 79.4 80.2 89.4
Co-tr. 92.2 93.8 92.6 93.2 91.4
SL 75.8 80.8 82.6 85.2 84.8 95.2
EM-NB 88.1 88.7 88.6 88.4 89.0
SL 73.5 78.7 81.3 82.8 83.9 91.6
S3VM 59.0 68.4 67.8 67.0 75.2
SL 70.0 72.8 75.6 76.2 80.0 90.0
Table 1: Classification accuracy(%) of SSL and SL on
movie reviews
the highest accuracy while news articles and blog
reviews are considerably more challenging. Thus,
we decided to use movie reviews as source data
and news articles and blog posts as target data do-
mains. While the data split for the target domain re-
mains the same as in section 4.2, all sentences in the
source domain, except for the 5% evaluation data,
were treated as labeled data. For example, in order
to identify opinion-bearing sentences from the blog
data set, all 9,500 movie review sentences and i%
of blog sentences were used as labeled data, 90% of
blog sentences were used as unlabeled data, and 5%
as evaluation data. We also applied a parameter to
gradually decrease the weight of the source domain
data, similar to the work done by Tan et al (2009).
5 Results and Evaluation
Overall, our results suggest that SSL improves ac-
curacy for opinion detection although the contribu-
tion of SSL varies across data domains and different
strategies need to be applied to achieve optimized
performance. For the movie review data set, almost
all SSL runs outperformed their corresponding base-
line SL runs and approached full SL runs; for the
news article data set, SSL performance followed a
similar trend but with only a small rate of increase;
for the blog post data set, SSL runs using only blog
data showed no benefits over the SL baseline, but
with labeled movie review data, SSL runs produced
results comparable with full SL result.
5.1 SSL vs. SL
Table 1 reports the performance of SSL and SL runs
on movie review data based on different numbers
204
of initial labeled sentences. Both the self- and co-
training runs reported here used the same parame-
ter settings: k=0, u=20, p=2, n=2, ? =0, with no
feature selection. The co-training results in Table
1 used a CLM and a BOW model (see section 5.2).
SL runs for co-training classified sentences based on
the highest score generated by two classifiers; SL
runs for S3VM applied the default SVM setting in
SVMlight; and SL runs for EM-NB used the Na??ve
Bayes classifier in the EM-NB implementation in
LingPipe.
Table 1 shows that, except for S3VM, SSL al-
ways outperforms the corresponding SL baseline on
movie reviews: When SSL converges, it achieves
improvement in the range of 8% to 34% over the SL
baseline. The fewer initial labeled data, the more
benefits an SSL run gained from using unlabeled
data. For example, using 100 labeled sentences, self-
training achieved a classification accuracy of 85.2%
and outperformed the baseline SL by 33.5%. Al-
though this SSL run was surpassed by 4.9% by the
full SL run using all labeled data, a great amount
of effort was saved by labeling only 100 sentences
rather than 9,500. Co-training produced the best
SSL results. For example, with only 200 labeled
sentences, co-training yielded accuracy as high as
93.8%. Overall, SSL for opinion detection on movie
reviews shows similar trends to SSL for traditional
topical classification (Nigam and Ghani, 2000).
However, the advantages of SSL were not as sig-
nificant in other data domains. Figure 1 demon-
strates the performance of four types of SSL runs
relative to corresponding baseline and full SL runs
for all three data sets. All SSL runs reported here
used 5% data as labeled data. Lines with different
patterns indicate different data sets, green triangles
mark baseline SL runs, green dots mark full SL runs,
and red crosses mark SSL runs. Numbers next to
symbols indicate classification accuracy. For each
line, if the red cross is located above the triangle,
it indicates that the SSL run improved over the SL
baseline; and, the closer the red cross to the upper
dot, the more effective was the SSL run. Figure 1
shows that S3VM degrades in performance for all
three data sets and we exclude it from the follow-
ing discussion. From movie reviews to news articles
to blog posts, the classification accuracy of baseline
SL runs as well as the improvement gained by SSL
Figure 1: Classification accuracy(%) of SSL and SL on
three data sets (i=5)
runs decreased: With greater than 80% baseline ac-
curacy on movie reviews, SSL runs were most effec-
tive; with slightly above 70% baseline accuracy on
news articles, self-training actually decreased per-
formance of the corresponding SL baseline while
co-training and EM-NB outperformed the SL base-
line only slightly; and with 60% or so baseline accu-
racy on blog posts, none of the SSL methods showed
improvement. We assume that the lower the baseline
accuracy, the worse the quality of auto-labeled data,
and, therefore, the less advantages is application of
SSL. We also found that the average sentence length
in blog posts (17 words) is shorter than the average
sentence length in either movie reviews (23.5 words)
or news articles (22.5 words), which posed an addi-
tional challenge because there is less information for
the classifier in terms of numbers of features.
Overall, for movie reviews and news articles, co-
training proved to be most robust and effective and
EM-NB showed consistent improvement over the
SL baseline. For news articles, EM-NB increased
accuracy from 63.5% to 68.8% with only 100 la-
beled sentences. For movie reviews, a close look at
EM-NB iterations shows that, with only 32 labeled
sentences, EM-NB was able to achieve 88% clas-
sification accuracy, which is close to the best per-
formance of simple Na??ve Bayes self-training using
300 labeled sentences. This implies that the prob-
205
Figure 2: Performance of four co-training strategies on movie review data
lem space of opinion detection may be successfully
described by the mixture model assumption of EM.
As for blog posts, since the performance of the base-
line classifiers was only slightly better than chance
(50%), we needed to improve the baseline accuracy
in order for SSL to work. One solution was to intro-
duce high quality features. We augmented feature
set with domain independent opinion lexicons that
have been suggested as effective in creating high
precision opinion classifier, but improvement was
only minimal. An alternative solution was to bor-
row more labeled data from non-blog domains(s).
Section 5.3 discusses dealing with a ?difficult? data
domain using data from an ?easy? domain.
The preliminary exploration of different parame-
ter settings for both self- and co-training showed no
significant benefit gained by setting the weight pa-
rameter ? or applying feature selection; and using
a larger number of unlabeled sentences u available
for each iteration did not improve results. Further
investigation is needed for an in-depth explanation.
5.2 Co-training
The best co-training runs reported in Table 1 and
Figure 1 used an 8-grams CLM to train one clas-
sifier and a BOW model to train the other classifier.
These two classifiers differ both in feature represen-
tation (i.e., character vs. word) and in learning al-
gorithm (language model vs. pure statistical model).
To investigate whether the two different classifiers
improve each other?s performance during iterations,
we analyzed the CLM and BOW classifiers individ-
ually. When comparing the BOW classifier during
co-training iterations to the performance of corre-
sponding SL runs based on BOW, the former us-
ing both CLM and BOW classifiers always outper-
formed the latter, indicating that the BOW classi-
fier learned from CLM. Similarly, the CLM classi-
fier also gained from the BOW classifier during co-
training.
Figure 2 shows that for the movie review do-
main, other simple co-training configurations also
produced promising results by using different fea-
ture sets (e.g., unigrams and the union of unigrams
and bigrams, or randomly split feature sets) or differ-
ent training sets. In the news domain, we observed
similar trends. This shows the robustness and great
potential of co-training. Because even with the lo-
gistic model to output probabilistic scores for the
SVM classifier, the difference in probabilities was
too small to select a small number of top predic-
tions, adding an SVM classifier for co-training did
206
not improve accuracy and is not discussed here.
An observation of the performance of self-
training and co-training over iterations confirmed
that co-training used labeled data more effectively
for opinion detection than self-training, as sug-
gested for traditional topical classification. We
found that, overall, co-training produces better per-
formance than self-training and reaches optimal per-
formance faster. For instance, with 500 labeled sen-
tences, a self-training run reached an optimal classi-
fication accuracy of 88.2% after adding 4,828 auto-
matically annotated sentences for training, while the
co-training run reached an optimal performance of
89.4% after adding only 2,588 sentences.
5.3 Domain Transfer
Even without any explicit domain adaptation meth-
ods, results indicate that simple self-training alone
is promising for tackling domain transfer between
the source domain movie reviews and the target do-
mains news articles and blog posts.
Target domain news articles We used 9,500 la-
beled movie review sentences to train a Na??ve Bayes
classifier for news articles. Although this classi-
fier produced a fairly good classification accuracy
of 89.2% on movie review data, its accuracy was
poor (64.1%) on news data (i.e., domain-transfer
SL), demonstrating the severity of the domain trans-
fer problem. Self-training with Na??ve Bayes using
unlabeled data from the news domain (i.e., domain-
transfer SSL run) improved the situation somewhat:
it achieved a classification accuracy of 75.1% sur-
passing the domain-transfer SL run by more than
17%. To finvestigate how well SSL handles the do-
main transfer problem, a full in-domain SL run that
used all labeled news sentences was also performed.
This full SL run achieved 76.9% classification accu-
racy, only 1.8% higher than the domain-transfer SSL
run, which did not use any labeled news data.
Target domain blog posts Because blog data are
more challenging than news data, we kept 5% blog
data as labeled data. Both SSL runs with and without
out-of-domain data are depicted in Figure 3. Self-
training using only blog data decreases SL baseline
performance (dashed black line). Keeping the same
settings, we added additional labeled data from the
movie reviews, and self-training (gray line) came
Figure 3: Self-training for domain transfer between
movie reviews (source domain) and blogs (target domain)
closer to the performance of the full SL run (red
line), which used 90% of the labeled blog data. We
then added a control factor that reduced the impact
of movie review data gradually (i.e., a decrease of
0.001 in each iteration). Using this control, the self-
training run (solid black line) reached and occasion-
ally exceeded the performance of the full SL run.
6 Conclusion and Future Work
We investigated major SSL methods for identify-
ing opinionated sentences in three domains. For
movie review data, SSL methods attained state-of-
the-art results with a small number of labeled sen-
tences. Even without a natural feature split, dif-
ferent co-training strategies increased the baseline
SL performance and outperformed other SSL meth-
ods. Due to the nature of the movie review data, we
suspect that opinion detection on movie reviews is
an ?easy? problem because it relies, strictly speak-
ing, on distinguishing movie reviews from plot sum-
maries, which also involves genre classification. For
other manually created data sets that are expected
to reflect real opinion characteristics, the SSL ap-
proach was impeded by low baseline precision and
showed limited improvement. With the addition of
out-of-domain labeled data, however, self-training
exceeded full SL. This constitutes a successful new
approach to domain adaptation.
Future work will include integrating opinion lex-
icons to bootstrap baseline precision and exploring
co-training for domain adaptation.
207
References
Alias-i. 2008. LingPipe (version 4.0.1). Available from
http://alias-i.com/lingpipe.
Anthony Aue and Michel Gamon. 2005. Customizing
sentiment classifiers to new domains: A case study. In
Proceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP),
Borovets, Bulgaria.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL), pages 440?
447, Prague, Czech Republic.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory, pages 92?100, Madison, WI.
Paula Chesley, Bruce Vincent, Li Xu, and Rohini K. Sri-
hari. 2006. Using verbs and adjectives to automati-
cally classify blog sentiment. In Proceedings of AAAI-
CAAW-06, the Spring Symposia on Computational Ap-
proaches to Analyzing Weblogs, Menlo Park, CA.
Hagen Fu?rstenau and Mirella Lapata. 2009. Semi-
supervised semantic role labeling. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL), pages 220?228, Athens, Greece.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
OpinionMiner: A novel machine learning system for
web opinion mining. In Proceedings of the 15th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, Paris, France.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT-Press.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The ICWSM 2010 JDPA sen-
timent corpus for the automotive domain. In 4th Inter-
national AAAI Conference on Weblogs and Social Me-
dia Data Workshop Challenge (ICWSM-DWC), Wash-
ington, D.C.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the
effectiveness and applicability of co-training. In Pro-
ceedings of the Ninth International Conference on In-
formation and Knowledge Management, McLean, VA.
Kamal Nigam, Andrew Kachites Mccallum, Sebastian
Thrun, and Tom Mitchell. 1999. Text classification
from labeled and unlabeled documents using EM. Ma-
chine Learning, 39:103?134.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, Barcelona, Spain.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), Sapporo, Japan.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2006. Latent variable models for semantic orientations
of phrases. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL), Trento, Italy.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1473?1481,
Uppsala, Sweden.
Songbo Tan, Xueqi Cheng, Yufen Wang, and Hongbo Xu.
2009. Adapting naive Bayes to domain adaptation for
sentiment analysis. In Proceedings of the 31st Eu-
ropean Conference on Information Retrieval (ECIR),
Toulouse, France.
Wei Wang and Zhi-Hua Zhou. 2007. Analyzing co-
training style algorithms. In Proceedings of the 18th
European Conference on Machine Learning, Warsaw,
Poland.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of the 6th International
Conference on Intelligent Text Processing and Compu-
tational Linguistics (CICLing), Mexico City, Mexico.
Janyce Wiebe, Rebecca Bruce, and Thomas O?Hara.
1999. Development and use of a gold standard data
set for subjectivity classifications. In Proceedings of
the 37th Annual Meeting of the Association for Com-
putational Linguistics (ACL), College Park, MD.
Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie
Martin, and Theresa Wilson. 2001. A corpus study of
evaluative and speculative language. In Proceedings
of the 2nd ACL SIGdial Workshop on Discourse and
Dialogue, Aalborg, Denmark.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational Linguistics, 30(3):277?308.
Kiduk Yang, Ning Yu, and Hui Zhang. 2007. WIDIT
in TREC-2007 blog track: Combining lexicon-based
methods to detect opinionated blogs. In Proceed-
ings of the 16th Text Retrieval Conference (TREC),
Gaithersburg, MD.
208
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Sapporo, Japan.
Ning Yu and Sandra Ku?bler. 2010. Semi-supervised
learning for opinion detection. In Proceedings of the
IEEE/WIC/ACM International Conference on Web In-
telligence and Intelligent Agent Technology, volume 3,
pages 249?252, Toronto, Canada.
Wei Zhang and Clement Yu. 2007. UIC at TREC 2007
blog track. In Proceedings of the 16th Text Retrieval
Conference (TREC), Gaithersburg, MD.
209
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2?11,
Dublin, Ireland, August 24 2014.
Feature Selection for Highly Skewed Sentiment Analysis Tasks
Can Liu
Indiana University
Bloomington, IN, USA
liucan@indiana.edu
Sandra K?ubler
Indiana University
Bloomington, IN, USA
skuebler@indiana.edu
Ning Yu
University of Kentucky
Lexington, KY, USA
ning.yu@uky.edu
Abstract
Sentiment analysis generally uses large feature sets based on a bag-of-words approach, which
results in a situation where individual features are not very informative. In addition, many data
sets tend to be heavily skewed. We approach this combination of challenges by investigating
feature selection in order to reduce the large number of features to those that are discriminative.
We examine the performance of five feature selection methods on two sentiment analysis data
sets from different domains, each with different ratios of class imbalance.
Our finding shows that feature selection is capable of improving the classification accuracy only
in balanced or slightly skewed situations. However, it is difficult to mitigate high skewing ratios.
We also conclude that there does not exist a single method that performs best across data sets and
skewing ratios. However we found that TF ? IDF
2
can help in identifying the minority class
even in highly imbalanced cases.
1 Introduction
In recent years, sentiment analysis has become an important area of research (Pang and Lee, 2008;
Bollen et al., 2011; Liu, 2012). Sentiment analysis is concerned with extracting opinions or emotions
from text, especially user generated web content. Specific tasks include monitoring mood and emotion;
differentiating opinions from facts; detecting positive or negative opinion polarity; determining opinion
strength; and identifying other opinion properties. At this point, two major approaches exists: lexicon
and machine learning based. The lexicon-based approach uses high quality, often manually generated
features. The machine learning-based approach uses automatically generated feature sets, which are from
various sources of evidence (e.g., part-of-speech, n-grams, emoticons) in order to capture the nuances of
sentiment. This means that a large set of features is extracted, out of which only a small subset may be
good indicators for the sentiment.
One major problem associated with sentiment analysis of web content is that for many topics, these
data sets tend to be highly imbalanced. There is a general trend that users are willing to submit positive
reviews, but they are much more hesitant to submit reviews in the medium to low ranges. For example,
for the YouTube data set that we will use, we collected comments for YouTube videos from the comedy
category, along with their ratings. In this data set, more than 3/4 of all ratings consist of the highest rating
of 5. For other types of user generated content, they opposite may be true.
Heavy skewing in data sets is challenging for standard classification algorithms. Therefore, the data
sets generally used for research on sentiment analysis are balanced. Researchers either generate balanced
data sets during data collection, by sampling a certain number of positive and negative reviews, or by se-
lecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review
data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set
allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the
movie review data set has been used as a benchmark data set that allows for comparisons of various sen-
timent analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2
and Savoy (2012), O?Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed com-
petitive feature selection methods evaluated on the movie review data set. However, the generalizability
of such feature selection methods to imbalanced data sets, which better represent real world situations,
has not been investigated in much detail. Forman (2003) provides an extensive study of feature selection
methods for highly imbalanced data sets, but he uses document classification as task.
This current paper investigates the robustness of three feature selection methods that Forman (2003)
has shown to be successful, as well as two variants of TF ? IDF . The three methods are Odds-Ratio
(OR), Information Gain (IG), and Binormal Separation (BNS). BNS has been found to perform signif-
icantly better than other methods in more highly skewed tasks (Forman, 2003). The two variants of
TF ? IDF differ in the data set used for calculating document frequency. We investigate the behavior
of these methods on a subtask of sentiment analysis, namely the prediction of user ratings. For this, we
will use data sets from two different domains in order to gain insight into whether or not these feature
selection methods are robust across domains and across skewing ratios: One set consists of user reviews
from Epicurious
1
, an online community where recipes can be exchanged and reviewed, the other set
consists of user reviews of YouTube comedy videos.
The remainder of this paper is organized as follows: In section 2, we explain the rationale for applying
feature selection and introduce the feature selection methods that are examined in this paper. Section
3 introduces the experimental settings, including a description of the two data sets, data preprocessing,
feature representation, and definition of the binary classification tasks. In section 4, we present and
discuss the results for the feature selection methods, and in section 5, we conclude.
2 Feature Selection and Class Skewing
In a larger picture, feature selection is a method (applicable both in regression and classification prob-
lems) to identify a subset of features to achieve various goals: 1) to reduce computational cost, 2) to
avoid overfitting, 3) to avoid model failure, and 4) to handle skewed data sets for classification tasks.
We concentrate on the last motivation, even though an improvement of efficiency and the reduction of
overfitting are welcome side effects. The feature selection methods studied in this paper have been used
in text classification as well, which is a more general but similar task using n-gram features. However,
since all measures are intended for binary classification problems, we reformulate the rating prediction
into a binary classification problem (see section 3.5).
Feature selection methods can be divided into wrapper and filter methods. Wrapper methods use
the classification outcome on a held-out data set to score feature subsets. Standard wrapper methods
include forward selection, backward selection, and genetic algorithms. Filter methods, in contrast, use
an independent measure rather than the error rate on the held-out data. This means that they can be
applied to larger feature sets, which may be unfeasible with wrapper methods. Since sentiment analysis
often deals with high dimensional feature representation, we will concentrate on filter methods for our
feature selection experiments.
Previous research (e.g. (Brank et al., 2002b; Forman, 2003)) has shown that Information Gain and
Odds Ratio have been used successfully across different tasks and that Binormal Separation has good
recall for the minority class under skewed class distributions. So we will investigate them in this paper.
Other filter methods are not investigated in this paper due to two main concerns: We exclude Chi-
squared and Z-score, statistical tests because they require a certain sample size. Our concern is that
their estimation for rare words may not be accurate. We also exclude Categorical Proportion Difference
and Probability Proportion Difference since they do not normalize over the sample of size of positive
and negative classes. Thus, our concern is that they may not provide a fair estimate for features from a
skewed data sets.
2.1 Notation
Following Zheng et al. (2004), feature selection methods can be divided into two groups: one-sided and
two-sided measures. One-sided measures assign a high score to positively-correlated features and a low
1
www.epicurious.com
3
score to negative features while two-sided measures prefer highly distinguishing features, independent
of whether they are positively or negatively correlated. Zheng et al. (2004) note that the ratio of positive
and negative features affects precision and recall of the classification, especially for the minority class.
For one-sided methods, we have control over this ratio by selecting a specified number of features one
each side; for two-sided methods, however, we do not have this control. In this paper, we will keep a 1:1
ratio for one-sided methods. For example, if we select 1 000 features, we select the 500 highest ranked
features for the positive class, and the 500 highest ranked features for the negative class. When using
two-sided methods, the 1 000 highest ranked features are selected.
For the discussion of the feature selection methods, we use the following notations:
? S: target or positive class.
? S: negative class.
? D
S
: The number of documents in class S.
? D
S
: The number of documents in class S.
? D
Sf
: The number of documents in class S where feature f occurs.
? D
Sf
: The number of documents in class S where feature f occurs.
? T
Sf
: The number of times feature f occurs in class S.
2.2 Feature Selection Methods
In addition to Information Gain, Odds Ratio and Bi-Normal Separation, TF ? IDF is included for
comparison purposes. We define these measures for binary classification as shown below.
Information Gain (IG): IG is a two-sided measure that estimates how much is known about an unob-
served random variable given an observed variable. It is defined as the entropy of one random variable
minus the conditional entropy of the observed variable. Thus, IG is the reduced uncertainty of class S
given a feature f :
IG = H(S)?H(S|f) =
?
f?{0,1}
?
S?{0,1}
P (f, S)log
P (f, S)
P (f)P (S)
Brank et al. (2002b) analyzed feature vector sparsity and concluded that IG prefers common features
over extremely rare ones. IG can be regarded as the weighted average of Mutual Information, and rare
features are penalized in the weighting. Thus they are unlikely to be chosen (Li et al., 2009). Forman
(2003) observed that IG performs better when only few features (100-500) are used. Both authors agreed
that IG has a high precision with respect to the minority class.
Odds Ratio (OR): OR (Mosteller, 1968) is a one-sided measure that is defined as the ratio of the odds
of feature f occurring in class S to the odds of it occurring in class S. A value larger than 1 indicates that
a feature is positively correlated with class S, a value smaller than 1 indicates it is negatively correlated:
OR = log
P (f, S)(1? P (f, S))
P (f, S)(1? P (f, S))
Brank et al. (2002b) showed that OR requires a high number of features to achieve a given feature
vector sparsity because it prefers rare terms. Features that occur in very few documents of class S and
do not occur in S have a small denominator, and thus a rather large OR value.
4
Bi-Normal Separation (BNS): BNS (Forman, 2003) is a two-sided measure that regards the proba-
bility of feature f occurring in class S as the area under the normal distribution bell curve. The whole
area under the bell curve corresponds to 1, and the area for a particular feature has a corresponding
threshold along the x-axis (ranging from negative infinite to positive infinite). For a feature f , one can
find the threshold that corresponds to the probability of occurring in the positive class, and the threshold
corresponding to the probability of occurring in S. BNS measures the separation in these two thresholds:
BNS = |F
?1
(
D
Sf
D
S
)? F
?1
(
D
Sf
D
S
)|
where F
?1
is the inverse function of the standard normal cumulative probability distribution. As we can
see, the F
?1
function exaggerates an input more dramatically when the input is close to 0 or 1 which
means that BNS perfers rare words.
Term Frequency * Inverse Document Frequency (TF*IDF): TF ? IDF was originally proposed
for information retrieval tasks, where it measures how representative a term is for the document in which
it occurs. When TF ? IDF is adopted for binary classification, we calculate the TF ? IDF of a feature
w.r.t. the positive class (normalized) and the TF ? IDF w.r.t. the negative class (normalized). We obtain
the absolute value of the difference of these two measures. If a feature is equally important in both
classes and thus would not contribute to classification, it receives a small value. The larger the value,
the more discriminative the feature. We apply two variants of TF ? IDF , depending on how IDF is
calculated:
TF ? IDF
1
= (0.5 +
0.5? T
Sf
max
i
(T
Sf
i
)
)? log(
D
S
+ D
S
D
Sf
)
TF ? IDF
2
= (0.5 +
0.5? T
Sf
max
i
(T
Sf
i
)
)? log(
D
S
D
Sf
)
In the first variant, TF ? IDF
1
, document frequency is based on the whole set of examples while in
the second variant, TF ? IDF
2
, document frequency is based only on the class under consideration, S.
3 Experimental Setup
3.1 Data Sets
Epicurious Data Set: We developed a web crawler to scrape user reviews for 10 146 recipes, published
on the Epicurious website before and on April 02, 2013. On the website, each recipe is assigned a rating
of 1 to 4 forks, including the intermediate values of 1.5, 2.5, and 3.5. This is an accumulated rating over
all user reviews. (Reviews with ratings of 0 were excluded, they usually indicate that recipes have not
received any ratings.) We rounded down all the half ratings, e.g., 1.5 forks counts as 1 fork, based on
the observation that users are generous when rating recipes. Our experiments classify each recipe by
aggregating over all its reviews. While a little more than half of the recipes received 1 to 10 reviews,
there are recipes with more than 100 reviews. To avoid an advantage for highly reviewed recipes, we
randomly selected 10 reviews if a recipe has more than 10 reviews. Recipes with less than 3 reviews
were eliminated since they do not provide enough information. After these clean-up steps, the data set
has the distribution of ratings shown in table 1.
YouTube Data Set: Using the Google YouTube Data API, we collected average user ratings and user
comments for a set of YouTube videos in the category Comedy. Each video is rated from 1 to 5. The
distribution of ratings among all YouTube videos is very skewed, as illustrated in figure 1. Most videos
are rated highly; very few are rated poorly. The 1% quantile is 1.0; the 6.5% quantile is 3.0; the 40%
quantile is 4.75; the 50% quantile is 4.85; and the 77% quantile is 5.0. We selected a set of 3 000 videos.
Videos with less than 5 comments or with non-English comments are discarded.
5
rating no.
1 fork 44 recipes
2 forks 304 recipes
3 forks 1416 recipes
4 forks 1368 recipes
Table 1: The distribution of ratings in the Epicurious data set.
1 2 3 4 5
0.0
0.5
1.0
1.5
2.0
2.5
Video Rating
Den
sity
Figure 1: Skewing in the YouTube data set.
3.2 Data Preprocessing
Before feature extraction, basic preprocessing is conducted for both data sets individually. For the Epi-
curious data set, we perform stemming using the Porter Stemmer (Porter, 1980) to normalize words, and
rare words (? 4 occurrences) are removed. On the YouTube data set, we perform spelling correction
and normalization because the writing style is rather informal. Our normalizer collapses repetitions into
their original forms plus a suffix ?RPT?, thus retaining this potentially helpful clue for reviewer?s strong
emotion without increasing the features due to creative spelling. For example, ?loooooove? is changed
to ?loveRPT? and?lolololol? to ?lolRPT?. The normalizer also replaces all emoticons by either?EMOP?
for positive emoticons or ?EMON? for negative emoticons. Besides a standard English dictionary, we
also use the Urban Dictionary
2
since it has a better coverage of online abbreviations.
We do not filter stop words for two reasons: 1) Stop words are domain dependent, and some En-
glish stop words may be informative for sentiment analysis, and 2) uninformative words that are equally
common in both classes will be excluded by feature selection if the method is successful.
3.3 Feature representation
Since our focus is on settings with high numbers of features, we use a bag-of-words approach, in which
every word represents one feature, and its term frequency serves as its value. Different feature weighting
methods, including binary weighting, term frequency, and TF?IDF have been adopted in past sentiment
analysis studies (e.g., (Pang et al., 2002; Paltoglou and Thelwall, 2010)). (Pang et al., 2002) found that
simply using binary feature weighting performed better than using more complicated weightings in a
task of classifying positive and negative movie reviews. However, movie reviews are relatively short, so
there may not be a large difference between binary features and others. Topic classification usually uses
term frequency as feature weighting. TF ? IDF and variants were shown to perform better than binary
weighting and term frequency for sentiment analysis (Paltoglou and Thelwall, 2010).
Since our user rating prediction tasks aggregate all user comments into large documents and predict
ratings per recipe/YouTube video, term frequency tends to capture richer information than binary fea-
2
http://www.urbandictionary.com/
6
Epicurious YouTube
ratio no. NEG no. POS ratio no. NEG no. POS
1:8 348 2 784 1:10 56 559
1:1.57 348 547 1:1.57 356 559
1:1 348 348 1:1 559 559
Table 2: Skewing ratios and sizes of positive and negative classes for both data sets.
tures. Thus, we use term frequency weighting for simplicity, not to deviate from the focus of feature
selection methods. Since there is a considerable variance in term frequency in the features, we normalize
the feature values to [0,1] to avoid large feature values from dominating the vector operations in classifier
optimization.
For the Epicurious data, the whole feature set consists of 10 677 unigram features. For YouTube, the
full feature set of features consists of 23 232 unigram features. We evaluate the performance of feature
selection methods starting at 500 features, at a step-size of 500. For the Epicurious data, we include up to
10 500 features. For the YouTube data, we stop at 15 000 features due to prohibitively long classification
times.
3.4 Classifier
The classifier we use in this paper is Support Vector Machines (SVMs) in the implementation of
SVM
light
(Joachims, 1999). Because algorithm optimization is not the focus of this study, we use the
default linear kernel and other default parameter values. Classification results are evaluated by accuracy
as well as precision and recall for individual classes.
3.5 Binary Classification
Since all feature selection methods we use in our experiments are defined under a binary classification
scenario, we need to redefine the rating prediction task. For both data sets, this means, we group the
recipes and videos into a positive and a negative class. A baseline classifier predicts every instance as the
majority class. For both data sets, the majority class is positive.
For the Epicurious data set, 1-fork and 2-fork recipes are grouped into the negative class (NEG), and
3-fork and 4-fork recipes are grouped into the positive class (POS), yielding a data set of 348 NEG and
2 784 POS recipes (skewing ratio: 1:8). The different skewing ratios we use are shown in table 2. 2/3 of
the data is used for training, and 1/3 for testing, with the split maintaining the class ratio. Note that for
the less skewed settings, all NEG instances were kept while POS instances were sampled randomly.
For the YouTube data set, we sample from all videos with rating 5 for the positive class and from
all videos with ratings between and including 1 and 3 for the negative class. This yields 559 POS and
559 NEG videos. The different skewing ratios we use are shown in table 2. 7/8 of the data is used for
training, and 1/8 for testing, with the split maintaining the class ratio.
4 Results
4.1 Results for the Epicurious Data Set
The results for the Epicurious data set with different skewing ratios are shown in figure 2. The accuracy
of the baseline is 50% for the 1:1 ratio, 61% for 1:1.57, and 88.9% for 1:8.
The results show that once we use a high number of features, all the feature selection methods perform
the same. This point where they conflate is reached at around 4 000 features for the ratios of 1:1 and
1:1.57. There, accuracy reaches around 71%. For the experiment with the highest skewing, this point is
reached much later, at around 8 000 features. For this setting, we also reach a higher accuracy of around
89%, which is to be expected since we have a stronger majority class. Note that once the conflation point
is reached, the accuracy also corresponds to the accuracy when using the full feature set. This accuracy
is always higher than that of the baseline.
7
2000 4000 6000 8000 10000
0.66
0.68
0.70
0.72
0.74
0.76
Number of Features Chosen
Accura
cy l
l
l
l l l
l l l l l l l l l l l l l l l
l
l l l
l
l
l
l
l l
Epicurious 1:1? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline = 0.5
2000 4000 6000 8000 100000
.60
0.65
0.70
0.75
0.80
Number of Features Chosen
Accura
cy
l
l
l
l
l
l
l l l l l l l l l l l l l l l
l
l
l
l
l
l l
l l l
l l l l
Epicurious 1:1.57 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
2000 4000 6000 8000 100000
.880
0.885
0.890
0.895
0.900
Number of Features Chosen
Accura
cy
l l l l l l l l l l
l l l l l l l l l l
l
l l
l l
l
l l l
l
l
l l l
l l l
Epicurious 1:8 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
Figure 2: The results for the Epicurious data set.
2000 4000 6000 8000 10000
0.0
0.2
0.4
0.6
0.8
1.0
Number of Features Chosen
Prec
ision
l l l l l l l l l l l l l l l l l l l l l
l
l
l
Epicurious 1:8 ? Precision NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
2000 4000 6000 8000 10000
0.00
0.02
0.04
0.06
0.08
0.10
Number of Features Chosen
Reca
ll
l l l l l l l l l l l l l l l l l l l l l
l
l
l
Epicurious 1:8 ? Recall NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
Figure 3: Precision and recall for the negative cases in the Epicurious set, given a 1:8 skewing.
The results also show that the most pronounced differences between feature selection methods occur in
the balanced data set. In the set with the highest skewing, the differences are minor, and only TF ?IDF
2
improves over the baseline when using 1 000 features.
Another surprising result is that TF ?IDF
2
, OR, and BNS have a tendency to fluctuate between higher
and lower results than the setting using all features. This means that it is difficult to find a good cut-off
point for these methods. TF ? IDF
1
and IG show clear performance gains for the balanced setting, but
they also show more fluctuation in settings with higher skewing.
From these results, we can conclude that for sentiment analysis tasks, feature selection is useful only
in a balanced or slightly skewed cases if we are interested in accuracy. However, a look at the precision
and recall given the highest skewing (see figure 3) shows that TF ? IDF
2
in combination with a small
number of features is the only method that finds at least a few cases of the minority class. Thus if a
good performance on the minority class examples is more important than overall accuracy, TF ? IDF
2
is a good choice. One explanation is that TF ? IDF
2
concentrates on one class and can thus ignore the
otherwise overwhelming positive class completely. TF ? IDF
1
and OR have the lowest precision, and
BNS fluctuates. Where recall is concerned, TF ? IDF
1
and IG reach the highest recall given a small
feature set.
4.2 Results for the YouTube Data Set
The results for the YouTube data set with different skewing ratios are shown in figure 4. The accuracy of
the baseline is 50% for the 1:1 ratio, 61.08% for 1:1.57, and 90.9% for 1:10.
The results show that even though the YouTube data set is considerably smaller than the Epicurious
one, is does profit from larger numbers of selected features: For the balanced and the low skewing, there
8
0 5000 10000 15000
0.50
0.55
0.60
0.65
0.70
0.75
Number of Features Chosen
Accura
cy
l
l l l
l
l l l
l l l l l
l l l l l l l l l l l l l l l l l
l l
l l l l
l
l
l
l
l l l
l l l l l l
l l
l l
l
l l l
l
l l
l
l l
l l l l l l l
l
Youtube Equal ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
0 5000 10000 150000
.60
0.65
0.70
0.75
Number of Features Chosen
Accura
cy
l
l l l l
l l
l l
l l l l
l
l l l l
l
l l l
l l l
l l l l l
l
l l
l l l l l l
l l l
l
l l
l l
l
l l
l l
l
l l l l
l l
l l
l
l l l l l l l l l l l l l l l
Youtube 1:1.57 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
0 5000 10000 150000
.80
0.85
0.90
0.95
Number of Features Chosen
Accura
cy
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l
l
l
Youtube 1:10 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
Figure 4: The results for the YouTube set.
0 5000 10000 15000
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Number of Features Chosen
Prec
ision
l
l
l
l
l
l
l l
l l l l l l l l l l
l l l l
l
l l l l l l l
l l l l l l l l l
l l l l l l l l
l l
l l
l
l
l
l
l l l l l l l l l l
Youtube 1:1.57 ? Precision NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
0 5000 10000 15000
0.0
0.1
0.2
0.3
0.4
0.5
Number of Features Chosen
Reca
ll
l l
l l l
l l
l l
l l l l
l
l l l l
l
l l l l l l l l l l l
l
l l
l l l l l l
l l l
l
l l
l l
l
l l
l l
l
l l l l l l l l l l l l l l l l l l l l l l l l
Youtube 1:1.57 ? Recall NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
Figure 5: Precision and recall for the negative cases in the YouTube set, given a 1:1.57 skewing.
is no point at which the methods conflate. The results for the highly skewed case show that no feature
selection method is capable of finding cases of the minority class: all methods consistently identify only
one instance of the negative class. However, this may be a consequence of the small data set. In terms of
accuracy, we see that a combination of a small number of features with either IG or TF ? IDF
1
provides
the best results. For the YouTube data set, BNS also performs well but requires a larger set of features for
reaching its highest accuracy. We assume that this is the case because BNS has a tendency to prefer rare
words. Note that we did not test for number of features greater than 15 000 because of the computation
cost, but we can see that the performance curve for different feature selection methods tends to conflate
to the point that represents the full feature set.
If we look at the performance of different feature selection methods on identification of minority class
instances, we find that TF ? IDF
2
again manages to increase recall in the highly skewed case, but this
time at the expense of precision. For the 1:1.57 ratio, all methods reach a perfect precision when a high
number of features is selected, see figure 5. TF ? IDF
2
is the only method that reaches this precision
with small numbers of features, too. However, this is not completely consistent.
4.3 Discussion
If we compare the performance curves across data sets and skewing ratios and aim for high accuracy,
we see that there is no single feature selection method that is optimal in all situations. Thus, we have to
conclude that the choice of feature selection method is dependent on the task. In fact, the performance
of a feature selection method could depend on many factors, such as the difficulty of the classification
task, the preprocessing step, the feature generation decision, the data representation scheme (Brank et
al., 2002a), or the classification model (e.g., SVM, Maximum Entropy, Naive Bayes).
9
We have also shown on two different data sets, each with three skewing ratios, that it is difficult for
feature selection methods to mitigate the effect of highly skewed class distributions while we can still
improve performance by using a reduced feature set for slightly skewed cases. Thus, the higher the skew-
ing of the data set is, the more difficult it is to find a feature selection method that has a positive effect,
and parameters, such as feature set size, have to be optimized very carefully. Thus, feature selection is
much less effective in highly skewed user rating tasks than in document classification.
However, if the task requires recall of the minority class, our experiments have shown that TF ?IDF
2
is able to increase this measure with a small feature set, even for highly imbalanced cases.
5 Conclusion and Future Work
In this paper, we investigated whether feature selection methods reported to be successful for document
classification perform robustly in sentiment classification problems with a highly skewed class distribu-
tion. Our findings show that feature selection methods are most effective when the data sets are balanced
or moderately skewed, while for highly imbalanced cases, we only saw an improvement in recall for the
minority class.
In the future, we will extend feature selection methods ? originally defined for binary classification
scenarios ? to handle multi-class classification problems. A simple way of implementing this is be to
break multi-class classification into several 1-vs.-all or 1-vs.-1 tasks, perform feature selection on these
binary tasks and then aggregate them. Another direction that we want to take is integrating more complex
features, such as parsing or semantic features, into the classification task to investigate how they influence
feature selection. In addition, we will compare other approaches against feature selection for handling
highly skewed data sets, including classification by rank, ensemble learning methods, and memory-
based learning with a instance-specific weighting. Finally, a more challenging task is to select features
for sentiment analysis problems where no annotation (feature selection under unsupervised learning) or
few annotations (feature selection under semi-supervised learning) are available.
References
Basant Agarwal and Namita Mittal. 2012. Categorical probability proportion difference (CPPD): A feature selec-
tion method for sentiment classification. In Proceedings of the 2nd Workshop on Sentiment Analysis where AI
meets Psychology (SAAIP), pages 17?26.
Basant Agarwal and Namita Mittal. 2013. Sentiment classification using rough set based hybrid feature selection.
In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media
Analysis (WASSA), pages 115?119, Atlanta, GA.
Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2011. Twitter mood predicts the stock market. Journal of Compu-
tational Science, 2:1?8.
Janez Brank, Marko Grobelnik, Nata?sa Milic-Frayling, and Dunja Mladenic. 2002a. An extensive empirical study
of feature selection metrics for text classification. In Proceedings of the Third International Conference on Data
Mining Methods and Databases for Engineering, Finance and Other Fields, Bologna, Italy.
Janez Brank, Marko Grobelnik, Nata?sa Milic-Frayling, and Dunja Mladenic. 2002b. Feature selection using
Linear Support Vector Machines. Technical Report MSR-TR-2002-63, Microsoft Research.
George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of
Machine Learning Research, 3:1289?1305.
Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Sch?olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector Learning. MIT-Press.
Olena Kummer and Jaques Savoy. 2012. Feature selection in sentiment analysis. In Proceeding of the Conf?erence
en Recherche d?Infomations et Applications (CORIA), pages 273?284, Bordeaux, France.
Shoushan Li, Rui Xia, Chengqing Zong, and Chu-Ren Huang. 2009. A framework of feature selection methods
for text categorization. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Language Processing of the AFNLP, pages 692?700, Suntec,
Singapore.
10
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies, pages 142?150, Portland, OR.
Frederick Mosteller. 1968. Association and estimation in contingency tables. Journal of the American Statistical
Association, 63(321):1?28.
Tim O?Keefe and Irena Koprinska. 2009. Feature selection and weighting methods in sentiment analysis. In Pro-
ceedings of the 14th Australasian Document Computing Symposium (ADCS), pages 67?74, Sydney, Australia.
Georgios Paltoglou and Mike Thelwall. 2010. A study of information retrieval weighting schemes for sentiment
analysis. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages
1386?1395, Uppsala, Sweden.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Lin-
guistics, Barcelona, Spain.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 79?86, Philadelphia, PA.
Martin Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130?137.
Zhaohui Zheng, Xiayun Wu, and Rohini Srihari. 2004. Feature selection for text categorization on imbalanced
data. ACM SIGKDD Explorations Newsletter, 6(1):80?89.
11
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 12?21,
Dublin, Ireland, August 24 2014.
?My Curiosity was Satisfied, but not in a Good Way?:
Predicting User Ratings for Online Recipes
Can Liu, Chun Guo, Daniel Dakota , Sridhar Rajagopalan, Wen Li, Sandra K
?
ubler
Indiana University
{liucan, chunguo, ddakota, srrajago, wl9, skuebler}@indiana.edu
Ning Yu
University of Kentucky
ning.yu@uky.edu
Abstract
In this paper, we develop an approach to automatically predict user ratings for recipes at Epicuri-
ous.com, based on the recipes? reviews. We investigate two distributional methods for feature se-
lection, Information Gain and Bi-Normal Separation; we also compare distributionally selected
features to linguistically motivated features and two types of frameworks: a one-layer system
where we aggregate all reviews and predict the rating vs. a two-layer system where ratings of
individual reviews are predicted and then aggregated. We obtain our best results by using the
two-layer architecture, in combination with 5 000 features selected by Information Gain. This
setup reaches an overall accuracy of 65.60%, given an upper bound of 82.57%.
1 Introduction
Exchanging recipes over the internet has become popular over the last decade. There are numerous sites
that allow us to upload our own recipes, to search for and to download others, as well as to rate and
review recipes. Such sites aggregate invaluable information. This raises the question how such sites can
select good recipes to present to users. Thus, we need to automatically predict their ratings.
Previous work (Yu et al., 2013) has shown that the reviews are the best rating predictors, in comparison
to ingredients, preparation steps, and metadata. In this paper, we follow their approach and investigate
how to use the information contained in the reviews to its fullest potential. Given that the rating classes
are discrete and that the distances between adjacent classes are not necessarily equivalent, we frame this
task as a classification problem, in which the class distribution is highly skewed, posing the question of
how to improve precision and recall especially for the minority classes to achieve higher overall accuracy.
One approach is to identify n-gram features of the highest discriminating power among ratings, from a
large number of features, many of which are equally distributed over ratings. An alternative strategy is
to select less surface-oriented, but rather linguistically motivated features. Our second question concerns
the rating predictor architecture. One possibility is to aggregate all reviews for a recipe, utilizing rich
textual information at one step (one-layer architecture). The other possibility is to rate individual reviews
first, using shorter but more precise language clues, and then aggregate them (two-layer). The latter
approach avoids the problem of contradictory reviews for a given review, but it raises the question on
how to aggregate over individual ratings. We will investigate all these approaches.
The remainder of the paper is structured as follows: First, we review related work in section 2. Then, in
section 3, we motivate our research questions in more detail. Section 4 describes the experimental setup,
including the data preparation, feature extraction, classifier, and evaluation. In section 5, we present
the results for the one-layer experiments, and in section 6 for the two-layer experiments. Section 7
investigates a more realistic gold standard. We then conclude in section 8.
2 Related Work
This section provides a brief survey for sentiment analysis on online reviews.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
12
During the last decade or more, there has been significant body of sentiment analysis studies on online
reviews. Two major approaches exist: lexicon-based and machine learning. A lexicon-based approach
requires prior knowledge of important sentiment features to build a list of sentiment-bearing words (or
phrases), which are often domain independent. Examples of such lexicons include the Multi-Perspective
Question Answering (MPQA) subjectivity lexicon (Wilson et al., 2005) and the Linguistic Inquiry and
Word Count (LIWC) dictionary (Pennebaker et al., 2014). The sentiment of a review is determined
by various ways of aggregating information about the sentiment-bearing words (phrases), such as their
frequency and sentiment scores. The machine learning approach dominantly adopts supervised learning
algorithms, which treat sentiment analysis as a text classification task. In this case, sentiment features
are generated from a pre-labeled corpus. Given the lack of annotated data, semi-supervised learning is
adopted (Yu, 2014; Yu and Ku?bler, 2011). For this study, we focus on a specific language domain of
online recipe reviews, which has user ratings, thus we choose supervised learning. We also adopt one
existing linguistic lexicon to provide extra features for our classification models.
The earliest sentiment analysis on online reviews was done by Pang et al. (2002); they applied several
supervised learning algorithms to classify online movie reviews into a positive and a negative class.
This study found that machine learning methods outperformed human annotators. It also found that
bigrams did not improve the classification performance, whether used alone or with unigrams, which has
been confirmed by many following studies. However, Cui et al. (2006) later conjectured that when the
training corpus is large enough, adding bigrams to unigrams improved the accuracy of binary product
review classification. A great number of diverse features were proven to be beneficial to capture subtle
sentiments across studies and a ?kitchen sink? approach is often adopted for sentiment analysis (Yang
et al., 2008). However, when features are noisy and redundant, researcher have found it beneficial to
identify the most telling ones (Gamon, 2004; Ng et al., 2006).
While it is useful to differentiate positive and negative reviews, a finer level of distinction can help
users better compare online reviews. As a matter of fact, even extra half star ratings can have dramatic
economic impact (Anderson and Magruder, 2012). To predict multi-level ratings, either multiclass clas-
sification or regression methods can be applied (Koppel and Schler, 2006; Yu et al., 2013). Pang and Lee
(2005) have also proposed an alternative meta-algorithm based on metric labeling for predicting three or
four sentiment classes for movie reviews. In their experiments, the meta-algorithm outperformed SVMs
in either one-versus-all or regression mode. In order to adopt this meta-algorithm, however, one needs to
determine an effective review similarity measure, which is not always straightforward.
If an item receives multiple reviews and/or comes from multiple sources, an overall rating needs to be
generated for this item. Yu et al. (2013) generated this overall rating by treating all the reviews from one
recipe as one long review. In this study, we are going to investigate how to integrate review-level rating
predictions to generate a recipe-level prediction. Rating aggregation has been studied intensively for
collaborative filtering, where the user/rater?s bias is adjusted (e.g., the trustworthy user?s rating has more
influence than others (McGlohon et al., 2010)). Since our current study does not take raters? information
into consideration, we are going to stay with the sample aggregation method. A study by Garcin et al.
(2009) suggests that among mean, median, and mode, the median is often a better choice as it is not as
sensitive to outliers as the mean.
3 Research Questions
As described in the previous section, many studies use only word unigrams or bigrams. We use word
and part-of-speech (POS) n-grams, with n ranging from 1 to 3. This approach generates a large number
of features, creating a very noisy and high dimensional data set, which also makes classifier training
and testing slow. For this reason, we first investigate the effect of feature selection. The next question
concerns the usefulness of linguistically and socio-linguistically motivated features. This results in a
small, but ideally meaningful set of features. The last research question that we approach in this paper
concerns whether classifying recipes on the recipe level is too coarse. In general, we have a wide range
of reviews, each of which is accompanied by a user rating. Thus, it is possible to conduct review-level
classification and then aggregate the ratings.
13
3.1 Feature Selection
Our primary feature set is based on word and POS n-grams. This results in an extremely large feature
set of 449 144 features, many of which do not serve any discriminatory function. A common first step to
trimming the feature set is to delete stop words. However, in the cooking domain, it is unclear whether
stop words would help. Feature selection is used to identify n-grams tightly associated with individual
ratings. Additionally, a extremely high dimensional feature representation makes model training and
testing more time consuming, and is likely to suffer from overfitting - given a large number of parameters
needed to describe the model. Due to the exponential computation time required by wrapper approaches
for feature selection, we use filtering approaches which are based on statistics about the distribution of
features. Previous research (Liu et al., 2014) indicates that Bi-Normal Separation (BNS) (Forman, 2003)
and Information Gain (IG) yield best results for this task. Information Gain is defined as follows:
IG = H(S) ? H(S|f) =
?
f?{0,1}
?
S?{0,1}
P (f, S)log
P (f, S)
P (f)P (S)
where S is the positive class, f a feature, and P (f, S) the joint probability of the feature f occurring
with class S. Bi-Normal Separation finds the separation of the probability of a feature occurring in the
positive class vs. the negative class, normalized by F
?1
, which is the inverse function of the standard
normal cumulative probability distribution. Bi-Normal Separation is defined as follows:
BNS = |F
?1
(
D
Sf
D
S
) ? F
?1
(
D
Sf
D
S
)|
where D
S
is the number of documents in class S, D
S
the number of documents in class S, D
Sf
the
number of documents in class S where feature f occurs, and D
S,f
the number of documents in class
S where feature f occurs. The F
?1
function exaggerates an input more dramatically when the input is
close to 0 or 1, which means that BNS prefers rare words.
Since both metrics are defined for binary classification, the features are chosen in terms of a separation
of the recipes into ?bad? ratings (1-fork and 2-fork) versus ?good? ratings (3-fork and 4-fork), on the
assumption that the selected features will be predictive for the more specific classes as well. For
review-based experiments, the features are chosen with regard to ?good? and ?bad? individual reviews.
3.2 Linguistically Motivated Features
Linguistic features In order to examine whether linguistic information can improve prediction accu-
racy, linguistically motivated features were extracted from the data. We selected seven features based
on the assumption that they reveal a sense of involvedness or distance of the reviewer, i.e., that authors
distance themselves from a recipe to indicate negative sentiment and show more involvedness to indicate
positive sentiment. These seven features are:
1. The percentage of personal pronouns per sentence.
2. The number of words per sentence.
3. The total number of words in the review.
4. The percentage of passive sentences per review.
5. The number of punctuation marks per sentence.
6. The number of capitalized characters per sentence.
7. The type/token ratio per review.
Features such as words per sentence, total words, and the type/token ratio are seen as indicating the
complexity of the review.
14
Our hypothesis is that the longer the review, the more likely it indicates a negative sentiment as the
review may go at lengths to indicate why something was negative.
Similarly, using the passive voice can be viewed as distancing oneself from the review indicating a
sense of impartial judgement, most likely associated with negativity, as one tends to actively like some-
thing (i.e. ?We liked it? versus ?It wasn?t well seasoned.?). Since some reviews with strong emotions are
written in all capital letters as well as contain many punctuation marks (particularly ?!?), these features
are also collected as possible indicators of sentiment.
Lexicon-based features In addition, we used an existing lexicon, the Linguistic Inquiry and Word
Count (LIWC) dictionary (Pennebaker et al., 2014), to analyze several emotional and cognitive dimen-
sions in the recipe reviews. This lexicon is chosen over other sentiment lexicons because it covers a broad
range of categories beyond simply positive and negative emotions. Briefly, it contains general descriptor
categories (e.g., percentage of words captured by the dictionary), standard linguistic dimensions (e.g.,
percentage of words in the text that are pronouns), word categories tapping psychological constructs
(e.g., biological processes), personal concern categories (e.g., work), spoken language dimensions (e.g.,
accent), and punctuation categories. Details of these dimensions can be found in the LIWC 2007 manual.
For our study, we first extracted all the features from a review set independent from our training/test
set. We then selected the LIWC features with highest power to differentiate four rating classes based
on Information Gain. Below are the 15 selected features. Note that the linguistic features here are
document-level features, not sentence-level features, as proposed above.
? Linguistic Processes
? Negations (e.g., never, no): 57 words
? 1st person plural (e.g., we, us): 12 words
? Exclamation mark
? Psychological Processes
? Affective process: this high level category contains 915 positive/negative emotions, anxiety,
anger and sadness related terms.
? Positive emotion (e.g., love, nice, sweet): 406 words
? Negative emotion (e.g., hurt, ugly, nasty): 499 words
? Sadness (e.g., crying, grief): 101 words
? Exclusive (e.g., but, exclude): 17 words
? Tentative (e.g., maybe, perhaps, guess): 155 words
? Causation (e.g., because, hence): 108 words
? Discrepancy (e.g., should, would, could) : 76 words
? Certainty (e.g., always, never): 83 words
? Sexual (e.g., love): 96 words
? Feel (e.g., feel, touch): 75 words
? Personal Concerns
? Leisure (e.g, cook, chat, movie): 229 words
It is not surprising that emotion related features are selected, but it is interesting to see that cognitive
processes features (i.e., causation, tentative, discrepancy, certainty and exclusive) are also highly related
to ratings. Taking a close look at the means of feature values across four ratings, we observe that people
tend to use words in the tentative, discrepancy, exclusive categories when they write negative recipe
reviews. For terms in causation, however, it is the opposite: People write about reasons when writing
positive reviews. Some further investigation is needed to explain why this is the case. We also see that the
higher the rating, the more likely it is that people use first person plural pronouns. This may be due to the
fact that only when people like a recipe, they will tend to share the food with others. Other observations
15
1 fork 108
2 fork 787
3 fork 5 648
4 fork 3 546
Table 1: The distribution of ratings in the Epicurious data.
include: The sexual features are positively correlated with high ratings, which is mainly due to the word
?love? in its non-sexual meaning. People tend to use more words from the perception processes category
feel when they complain about a recipe.
3.3 One-Layer Prediction Versus Two-Layer Prediction
The one-layer or recipe-based approach consider all reviews per recipe as a single document. This
approach has rich textual information, especially when a large number of reviews exist for a recipe.
However, the concern with this approach is that the reviews in themselves may be varied. There are
recipes whose reviews range from the lowest to the highest rating. Given such a range of individual
ratings, we can assume that the recipe-based approach will be faced with a contradictory feature set for
certain recipes. For this reason, we also investigate a two-layer or review-based approach. Here, every
individual review is rated automatically. In a second step, we aggregate over all reviews per recipe.
Aggregation can either take the form of majority voting, average, or of a second classifier which takes
the aggregated ratings as features to make a final decision. However, this approach will suffer from often
very short reviews, which do not allow the extraction of sufficient features as well as from the inequality
in the number of reviews per recipe.
4 Experimental Setup
4.1 Data Set
We scraped user reviews for 10 089 recipes, published on the Epicurious website
1
before and on April 02,
2013. Typically, a recipe contains three parts: ingredients, cooking instructions, and user reviews. In our
experiments, we focus exclusively on the reviews. Each user review has a rating for this recipe, ranging
from 1 fork to 4 forks. There is also an overall rating per recipe, which is the average of all reviews
ratings as well as ratings submitted without reviews. Half forks are possible for recipe rating but not for
review ratings. These recipes were pre-processed to remove reviews with zero ratings. Recipes that had
no reviews were then subsequently removed. In order to counter the effect of the wide variance in the
number of reviews per recipe, we randomly sampled 10 reviews from recipes with more than 10 reviews.
We had performed initial experiments with all reviews, which resulted in only minor differences. At the
review level, rare words (unigrams occurring less than four times) were removed for two reasons: 1)
Extremely rare words are likely to be noise rather than sentiment-bearing clues; 2) the feature selection
method BNS is biased towards rare words; 3) such words do not generalize well. The recipes were then
tagged using the Stanford POS Tagger (Toutanova et al., 2003).
The data set is severely skewed with regard to the number of recipes per fork: Users seem to be more
willing to review good recipes. To lessen the effect of imbalance in the rating classifier, all half fork
reviews were added to their corresponding full star reviews (i.e., 1.5 fork was added to the 1 fork data).
This resulted in the data split of 10 089 recipes shown in table 1. Even after collapsing the half stars,
there is still a very large skewing of the data towards the higher ratings. This means, feature selection is
important to mitigate the imbalance to a certain degree.
4.2 Features
In addition to the linguistic features described in section 3.2, we also extracted n-gram features: word
unigrams, bigrams, and trigrams as well as POS tag unigrams, bigrams, and trigrams. Since the data
1
http://www.epicurious.com
16
Method 750 900 1 000 1 500 3 000 6 000
BNS ? ? 31.33 ? 42.00 50.67
IG 62.00 62.00 62.33 62.33 61.00 58.67
Table 2: Results for feature selection based on Bi-Normal Separation (BNS) and Information Gain (IG).
includes tokens particular to the web, modifications were made to the data to help with the processing of
these types of tokens. URLs were replaced with a single URL token and tagged with a unique ?URL? tag.
Emoticons were defined as either positive or negative and subsequently replaced by EMOP or EMON
respectively. Since it is unclear for this task whether more frequent feature should receive a larger weight,
we normalized features values to a range of [0,1].
4.3 Classifiers
Preliminary experiments were run to determine the best classifier for the task. We decided on Support
Vector Machines in the implementation of SVM multi-class V1.01 (Crammer and Singer, 2002) for both
review-level and recipe-level rating prediction. Initial experiments showed that SVM multi-class V1.01
reaches higher results on our skewed data set than the current V2.20. For this reason, all experiments
reported in this paper are based on V1.01 with its default settings, i.e., using a linear kernel.
To aggregate review-level ratings into a recipe-level prediction, we experimented with both the max-
imum entropy classifier in the implementation of the Stanford Classifier (Manning and Klein, 2003)
and the SVM multi-class classifier. We included Maxent Classifier because given the small number of
features it is no longer clear whether SVM is advantageous.
4.4 Baseline
The baseline was established following Yu et al. (2013) as selecting the label of majority class (3-fork)
to tag all recipes, producing an accuracy of 56.00% for both one-layer and two-layer systems.
4.5 Evaluation
Evaluation was performed using 3-fold cross validation. Since the data is skewed, we report Precision
(P), Recall (R), and F-Scores (F) for all classes across each experiment, along with standard accuracy.
5 Results for One-Layer Prediction
5.1 Feature Selection
We first investigated the effect of feature selection, varying the number of included features from 750 to
6 000. Results for the two methods and different feature thresholds are shown in table 2. Since previous
work (Liu et al., 2014) showed that BNS has a tendency to select infrequent n-grams and would need
a larger number of features than IG to achieve good performance, we tested the higher ranges of 1 000,
3 000, 6 000 features. None of these experiments yields an accuracy higher than the baseline of 56.00%.
On the other hand, the performance of Information Gain peaks at 1 000 and 1 500 features, and we reach
an absolute increase in accuracy of 6.33%. Given these experiments, for all following experiments, we
use the combination of Information Gain and 1 000 n-gram features.
5.2 Linguistically Motivated Features
Here, we test the contribution of the linguistically motivated features introduced in section 3.2. To allow
a comparison to previous experiments, we report the baseline and the results for using Information Gain.
For the two sets of linguistically motivated features, we used the following combination of features:
1. Lexicon-based features (Lex) combined with linguistic features (Ling) (22 features).
2. Lexicon-based features (Lex) combined with the 1 000 features selected by Information Gain (IG)
(1015 features).
17
1 fork 2 fork 3 fork 4 fork
P R F P R F P R F P R F Acc.
Base 0.00 0.00 0.00 0.00 0.00 0.00 56.00 100.00 72.00 0.00 0.00 0.00 56.00
IG 33.33 1.00 2.00 31.33 12.00 17.33 66.00 73.67 69.67 58.33 58.00 58.00 62.33
Lex+Ling 0.00 0.00 0.00 0.00 0.00 0.00 56.00 100.00 72.00 0.00 0.00 0.00 56.00
IG+Lex 39.00 2.00 3.67 31.67 10.00 15.00 65.33 75.33 69.67 59.67 55.67 57.33 62.67
IG+Ling 0.00 0.00 0.00 32.00 3.33 6.00 63.67 81.00 71.33 62.67 49.67 55.33 63.33
IG+Lex+Ling 0.00 0.00 0.00 32.00 3.33 6.00 63.67 81.00 71.33 62.67 49.67 55.33 63.33
Table 3: Results for manually selected features.
1 fork 2 fork 3 fork 4 fork
no. feat. P R F P R F P R F P R F Acc.
1000 61.57 58.11 59.79 53.70 37.42 44.11 63.04 42.14 50.51 71.30 89.08 79.20 67.80
2000 61.65 58.27 59.91 52.96 39.40 45.18 63.37 43.19 51.37 71.86 88.70 79.40 68.11
3000 62.50 58.51 60.44 52.98 40.88 46.15 62.90 44.49 52.12 72.45 88.10 79.51 68.34
4000 62.45 58.45 60.38 52.38 41.05 46.03 62.99 45.54 52.86 72.83 87.70 79.58 68.46
5000 62.32 57.00 59.54 51.66 41.17 45.82 62.21 46.15 52.99 73.05 87.24 79.52 68.31
Table 4: Results on individual reviews for the two-layer experiments.
3. Linguistic features (Ling) combined with the 1 000 features selected by Information Gain (IG)
(1007 features).
4. A combination of all three sets of features (IG+Lex+Ling) (1022 features).
The results for these experiments are reported in table 3. These results show that a combination of
the two sets of linguistically motivated features does not increase accuracy over the baseline. In fact, the
classification is identical to the baseline, i.e., all recipes are grouped into the majority class of 3-fork. We
assume that the linguistically motivated features are too rare to be useful. If we add the lexicon-based
features to the ones selected by Information Gain, we reach a minimal improvement over only the IG
features: accuracy increases from 62.33% to 62.67%. This increase is mostly due to a better performance
on the minority class of 1 fork. If we add the 7 linguistic features to the IG features, we reach the highest
accuracy of 63.33%. However, this is due to a more pronounced preference for selecting the majority
class. Adding the lexicon-based features to this feature set does not give any further improvements.
6 Results for Two-Layer Prediction
In this section, we investigate the two-layer or review-based prediction. For these experiments, we per-
formed feature selection on the individual reviews using IG. Adding the linguistically motivated features
considerably decreased performance. We assume that these features do not generalize well on the shorter
reviews.
Note that the task approached here is a difficult task since the recipe rating on Epicurious is not the
average over all the ratings associated to the individual reviews but also includes ratings by user who did
not write a review. If we average over all the sampled gold standard review ratings per recipe, we reach
an accuracy of 82.57%. This is the upper bound that we can reach in these experiments.
6.1 Classifying Individual Reviews
First, we look at the phase in which individual reviews are classified. The results of this set of experiments
is shown in table 4. Note that there are three important trends here: 1) The accuracy of the SVM classifier
is higher than for classifying recipes. The comparison needs to be taken with a grain of salt because
these are two different tasks. However, this is an indication that it is possible to reach higher results
based on aggregating over individual reviews. 2) For this task, we reach the highest results by using
4 000 features, i.e., a considerably higher number of features than the optimal set for the recipe-based
experiments, where 1 000 features sufficed. We suspect that we need more features in this setting because
the individual reviews are shorter so that individual features do not generalize as well as for complete
recipes. 3) The classification of individual reviews is less skewed than for complete recipes. The F-scores
18
1 fork 2 fork 3 fork 4 fork
no. f. sys. P R F P R F P R F P R F Acc.
1000 avg 44.64 36.98 40.45 60.00 23.03 33.28 75.72 51.91 61.59 52.38 86.01 65.11 61.48
maxent 43.87 33.33 37.88 58.30 19.73 29.48 73.90 58.77 65.47 55.03 81.40 65.67 63.41
svm 62.21 62.21 62.21 56.18 16.03 24.94 72.24 58.14 64.43 53.92 80.82 68.68 62.21
2000 avg 44.61 38.83 41.52 61.45 24.29 34.82 76.12 53.96 63.15 53.45 85.53 65.79 62.58
maxent 43.17 34.27 38.21 61.47 21.23 31.56 74.43 60.93 67.01 56.27 80.93 66.38 64.58
svm 63.29 63.29 63.29 56.53 16.79 25.89 72.65 60.32 65.91 55.14 80.26 65.37 63.29
3000 avg 42.71 38.86 40.69 61.08 26.57 37.03 75.62 54.33 63.23 53.71 84.60 65.71 62.64
maxent 42.40 35.20 38.47 61.90 23.40 33.96 74.00 61.90 67.41 56.83 79.73 66.36 64.84
svm 63.53 63.53 63.53 54.09 17.41 26.34 72.14 61.45 66.37 55.80 79.02 65.41 63.53
4000 avg 38.64 34.22 36.30 61.09 24.89 35.37 75.23 55.91 64.15 54.34 83.81 65.93 63.07
maxent 38.37 31.47 34.58 60.67 22.47 32.79 73.80 63.03 67.99 57.47 79.00 66.54 65.16
svm 64.03 64.03 64.03 52.92 17.53 26.33 72.24 62.85 67.22 56.51 78.17 65.60 64.03
5000 avg 39.23 37.05 38.11 59.20 24.89 35.05 75.38 56.25 64.42 54.54 83.64 66.03 63.23
maxent 38.03 33.40 35.56 58.37 22.00 31.96 73.97 64.17 68.72 58.13 78.57 66.82 65.60
svm 64.68 64.68 64.68 50.19 17.40 25.84 72.52 64.18 68.10 57.45 77.95 66.15 64.68
Table 5: Results on aggregating reviews for the two-layer experiments.
for the non-majority classes are considerably higher than in the recipe-based setting. Thus, we expect to
obtain more balanced results across classes in the aggregation as well.
6.2 Predicting Recipe Ratings by Aggregating Reviews
When aggregating review predictions to recipe rating, we use three methods: 1) Taking the average of
the review ratings from the previous step; 2) using SVM; and 3) using a maximum entropy classifier
(Maxent), the Stanford Classifier. When calculating the average over review rating predictions, the final
average is rounded up. The results are reported in table 5. When using SVM and the maximum entropy
classifier, we use four features, corresponding to the four ratings. The feature values are calculated as
the percentage of reviews from the target recipe that were assigned to this fork rating by our review-level
classifier.
Overall, the maximum entropy classifier yields the best performance, independent of the number of
features used for the review-level classifier. The highest performance we reach by using 5 000 features
and the maximum entropy classifier. Calculating the average results in the worst performance. Although
Epicurious calculates the average user ratings based on review ratings and singular ratings, keep in mind
that we use at most 10 reviews per recipe, hence only capture part of the image. This may explain why
simply calculating the average does not work well. When looking at the F-scores for each fork in table 5,
however, the maximum entropy classifier produces lower performance than average and SVM classifier
for the 1 fork and 2 fork classes. For 1 fork, SVM has the highest F-scores for different numbers of
features, followed by the averaging approach while for 2 fork, the average approach produced the highest
F-scores. One possible explanation is that recipes with lower ratings have relatively small numbers of
reviews and thus may be less impacted by our sampling.
7 Towards a More Realistic Gold Standard
When we aggregate over the individual review rating using the average, the results are only slightly better
than the one-layer results. For example, the best performance using the average reaches an accuracy of
63.23%, as opposed to the one-layer accuracy of 62.33% in table 2 (note that these settings use only IG
features). One reason for this low performance is that Epicurious averages all review ratings to generate
a recipe rating, independent of whether there is review attached to the rating or not. Since our text-based
classifiers make their decisions only based on the reviews, the question is how well we actually predict
the average rating if only ratings attached to reviews were used in the calculation. In this way, we can
evaluate how well our approach works if we assume that all the information is available to the classifier.
Consequently, we calculated a new gold standard, averaging gold ratings of individual reviews in the
recipe sample. We investigate this effect based on the two-layer setting where reviews are aggregated
via averaging. The results of this set of experiments are shown in table 6 for the two-layer approach and
in table 7 for the one-liner approach. We report results using the gold label based on the ratings from
19
1 fork 2 fork 3 fork 4 fork
sys. P R F P R F P R F P R F Acc.
EPI 39.23 37.05 38.11 59.20 24.89 35.05 75.38 56.25 64.42 54.54 83.64 66.03 63.23
EPI-AVG 56.41 51.16 53.66 62.73 62.73 62.73 76.89 65.66 70.83 67.10 85.39 75.15 71.10
Table 6: Evaluation on a more realistic gold standard for two-layer experiments.
1 fork 2 fork 3 fork 4 fork
P R F P R F P R F P R F Acc.
Base 0.00 0.00 0.00 0.00 0.00 0.00 52.00 100.00 68.00 0.00 0.00 0.00 52.00
IG 8.33 1.00 1.67 29.67 11.00 16.00 64.33 70.00 67.00 64.00 66.00 64.67 63.33
Lex+Ling 0.00 0.00 0.00 0.00 0.00 0.00 51.33 99.33 67.67 41.33 1.00 2.00 51.00
IG+Lex 11.00 1.00 2.00 29.00 9.67 14.67 64.00 70.33 67.00 64.00 65.33 64.67 63.33
IG+Ling 16.67 1.00 2.00 31.00 6.33 10.67 63.00 72.67 67.67 65.00 63.33 64.00 63.33
IG+Lex+Ling 16.67 1.00 2.00 31.33 6.67 11.00 63.00 72.67 67.33 65.00 63.33 64.00 63.33
Table 7: Evaluation on a more realistic gold standard for one-layer experiments.
Epicurious (EPI) and based on the new gold standard (EPI-AVG). These results show that based on this
more realistic gold standard, averaging over the individual reviews results in an accuracy of 71.10%,
however with an upper bound of 100% instead of 82.57%. The results for the on-layer experiments are
not as sensitive to this new gold standard. The baseline, which loses 4%, shows that now, the task is
more difficult. All combinations involving IG selected features reach an accuracy of 63.33%, the same
as for the Epicurious gold standard (see table 3).
8 Conclusion and Future Work
In this study, we have explored various strategies for predicting recipe ratings based on user reviews.
This is a difficult task due to systemic reasons, user bias, as well as exogenous factors: 1) There are
user ratings that do not come with reviews, which means that they constitute hidden information for our
classifiers (so that we have an upper bound of 82.57% in overall accuracy). 2) Ratings are not entirely
supported by text, i.e., some ratings seem to be independent from the review text, due to user behavior
(e.g., people tend to give higher ratings in good weather than in bad weather (Bakhshi et al., 2014)).
Our experiments suggest that a two-layer approach, which predicts review-level ratings and aggregates
them for the recipe-level rating, reaches a higher accuracy than the one-layer approach that aggregates
all reviews and predicts on the recipe level directly, with a 3.6% absolute improvement in accuracy. If
we evaluate the two-layer results on a more realistic gold standard, we achieve an even higher increase
of 12.3%.
Our experiments also suggest that with feature selection, automatically generated n-gram features can
produce reasonable results without manually generated linguistic cues and lexicons, although the latter
does show a slight improvement, especially for minority classes.
A few directions can be taken for our future study: 1) Handling short reviews with better methods for
dealing with sparse features. 2) The feature selection is conducted within a binary classification scenario
(1- and 2-forks vs. 3- and 4-forks). It is worth exploring the effect of feature selection within four 1 vs.
all scenarios (i.e., 1-fork against the rest, etc.). 3) We will explore aspect-level sentiment classification
to provide a finer-grained summary of the recipes.
References
Michael Anderson and Jeremy Magruder. 2012. Learning from the crowd: Regression discontinuity estimates of
the effects of an online review database. The Economic Journal, 122:957?989.
Saeideh Bakhshi, Partha Kanuparthy, and Eric Gilbert. 2014. Demographics, weather and online reviews: A study
of restaurant recommendations. In Proceedings of the WWW conference, Seoul, Korea.
Koby Crammer and Yoram Singer. 2002. On the algorithmic implementation of multiclass kernel-based vector
machines. Journal of Machine Learning Research, 2:265?292.
20
Hang Cui, Vibhu Mittal, and Mayur Datar. 2006. Comparative experiments on sentiment classification for online
product reviews. In Proceedings of the 21st National Conference on Artificial Intelligence, AAAI?06, pages
1265?1270, Boston, Massachusetts.
George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of
Machine Learning Research, 3:1289?1305.
Michael Gamon. 2004. Sentiment classification on customer feedback data: Noisy data, large feature vectors,
and the role of linguistic analysis. In Proceedings of the 20th International Conference on Computational
Linguistics (COLING), pages 841?847, Geneva, Switzerland.
Florent Garcin, Boi Faltings, Radu Jurca, and Nadine Joswig. 2009. Rating aggregation in collaborative filtering
systems. In Proceedings of the Third ACM Conference on Recommender Systems, pages 349?352, New York,
NY.
Moshe Koppel and Jonathan Schler. 2006. The importance of neutral examples in learning sentiment. Computa-
tional Intelligence Journal, 22:100?109. Special Issue on Sentiment Analysis.
Can Liu, Sandra Ku?bler, and Ning Yu. 2014. Feature selection for highly skewed sentiment analysis tasks. In
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), Dublin,
Ireland.
Christopher Manning and Dan Klein. 2003. Optimization, maxent models, and conditional estimation without
magic. Tutorial at HLT-NAACL 2003 and ACL 2003.
Mary McGlohon, Natalie Glance, and Zach Reiter. 2010. Star quality: Aggregating reviews to rank products and
merchants. In Proceedings of Fourth International Conference on Weblogs and Social Media (ICWSM), pages
114?121, Washington, DC.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006. Examining the role of linguistic knowledge sources
in the automatic identification and classification of reviews. In Proceedings of COLING/ACL, pages 611?618,
Sydney, Australia.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with re-
spect to rating scales. In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguistics,
ACL, pages 115?124, Ann Arbor, MI.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine
learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
EMNLP, pages 79?86, Philadelphia, PA.
James Pennebaker, Roger Booth, and Martha Francis, 2014. Linguistic inqury and word count: LIWC 2007
operator?s manual. http://homepage.psy.utexas.edu/HomePage/Faculty/Pennebaker/
Reprints/LIWC2007_OperatorManual.pdf.
Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252?259, Edmonton,
Canada.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
347?354, Vancouver, Canada.
Kiduk Yang, Ning Yu, and Hui Zhang. 2008. WIDIT in TREC2007 blog track: Combining lexicon-basedmethods
to detect opinionated blogs. In Proceedings of the 16th Text Retrieval Conference, Gaithersburg, MD.
Ning Yu and Sandra Ku?bler. 2011. Filling the gap: Semi-supervised learning for opinion detection across domains.
In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL, pages 200?
209, Portland, OR.
Ning Yu, Desislava Zhekova, Can Liu, and Sandra Ku?bler. 2013. Do good recipes need butter? Predicting user
ratings of online recipes. In Proceedings of the IJCAI Workshop on Cooking with Computers, Beijing, China.
Ning Yu. 2014. Exploring co-training strategies for opinion detection. Journal of the Association for Information
Science and Technology.
21
