Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 393?400,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Event Durations from Event Descriptions 
 
 
Feng Pan, Rutu Mulkar, and Jerry R. Hobbs 
Information Sciences Institute (ISI), University of Southern California 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 
{pan, rutu, hobbs}@isi.edu 
 
  
 
Abstract 
We have constructed a corpus of news ar-
ticles in which events are annotated for 
estimated bounds on their duration. Here 
we describe a method for measuring in-
ter-annotator agreement for these event 
duration distributions. We then show that 
machine learning techniques applied to 
this data yield coarse-grained event dura-
tion information, considerably outper-
forming a baseline and approaching hu-
man performance. 
1 Introduction 
Consider the sentence from a news article: 
George W. Bush met with Vladimir Putin in 
Moscow. 
How long was the meeting?  Our first reaction 
to this question might be that we have no idea.  
But in fact we do have an idea.  We know the 
meeting was longer than 10 seconds and less 
than a year.  How much tighter can we get the 
bounds to be?  Most people would say the meet-
ing lasted between an hour and three days. 
There is much temporal information in text 
that has hitherto been largely unexploited, en-
coded in the descriptions of events and relying 
on our knowledge of the range of usual durations 
of types of events.  This paper describes one part 
of an exploration into how this information can 
be captured automatically.  Specifically, we have 
developed annotation guidelines to minimize dis-
crepant judgments and annotated 58 articles, 
comprising 2288 events; we have developed a 
method for measuring inter-annotator agreement 
when the judgments are intervals on a scale; and 
we have shown that machine learning techniques 
applied to the annotated data considerably out-
perform a baseline and approach human per-
formance.   
This research is potentially very important in 
applications in which the time course of events is 
to be extracted from news. For example, whether 
two events overlap or are in sequence often de-
pends very much on their durations.  If a war 
started yesterday, we can be pretty sure it is still 
going on today.  If a hurricane started last year, 
we can be sure it is over by now. 
The corpus that we have annotated currently 
contains all the 48 non-Wall-Street-Journal (non-
WSJ) news articles (a total of 2132 event in-
stances), as well as 10 WSJ articles (156 event 
instances), from the TimeBank corpus annotated 
in TimeML (Pustejovky et al, 2003). The non-
WSJ articles (mainly political and disaster news) 
include both print and broadcast news that are 
from a variety of news sources, such as ABC, 
AP, and VOA. 
In the corpus, every event to be annotated was 
already identified in TimeBank.  Annotators 
were instructed to provide lower and upper 
bounds on the duration of the event, encompass-
ing 80% of the possibilities, excluding anoma-
lous cases, and taking the entire context of the 
article into account. For example, here is the 
graphical output of the annotations (3 annotators) 
for the ?finished? event (underlined) in the sen-
tence 
After the victim, Linda Sanders, 35, had fin-
ished her cleaning and was waiting for her 
clothes to dry,... 
 
393
This graph shows that the first annotator be-
lieves that the event lasts for minutes whereas the 
second annotator believes it could only last for 
several seconds. The third annotates the event to 
range from a few seconds to a few minutes. A 
logarithmic scale is used for the output because 
of the intuition that the difference between 1 sec-
ond and 20 seconds is significant, while the dif-
ference between 1 year 1 second and 1 year 20 
seconds is negligible.  
A preliminary exercise in annotation revealed 
about a dozen classes of systematic discrepancies 
among annotators? judgments.  We thus devel-
oped guidelines to make annotators aware of 
these cases and to guide them in making the 
judgments.  For example, many occurrences of 
verbs and other event descriptors refer to multi-
ple events, especially but not exclusively if the 
subject or object of the verb is plural.  In ?Iraq 
has destroyed its long-range missiles?, there is 
the time it takes to destroy one missile and the 
duration of the interval in which all the individ-
ual events are situated ? the time it takes to de-
stroy all its missiles.  Initially, there were wide 
discrepancies because some annotators would 
annotate one value, others the other.  Annotators 
are now instructed to make judgments on both 
values in this case.  The use of the annotation 
guidelines resulted in about 10% improvement in 
inter-annotator agreement (Pan et al, 2006), 
measured as described in Section 2. 
There is a residual of gross discrepancies in 
annotators? judgments that result from differ-
ences of opinion, for example, about how long a 
government policy is typically in effect.  But the 
number of these discrepancies was surprisingly 
small. 
The method and guidelines for annotation are 
described in much greater detail in (Pan et al, 
2006).  In the current paper, we focus on how 
inter-annotator agreement is measured, in Sec-
tion 2, and in Sections 3-5 on the machine learn-
ing experiments.  Because the annotated corpus 
is still fairly small, we cannot hope to learn to 
make fine-grained judgments of event durations 
that are currently annotated in the corpus, but as 
we demonstrate, it is possible to learn useful 
coarse-grained judgments.   
Although there has been much work on tem-
poral anchoring and event ordering in text 
(Hitzeman et al, 1995; Mani and Wilson, 2000; 
Filatova and Hovy, 2001; Boguraev and Ando, 
2005), to our knowledge, there has been no seri-
ous published empirical effort to model and learn 
vague and implicit duration information in natu-
ral language, such as the typical durations of 
events, and to perform reasoning over this infor-
mation. (Cyc apparently has some fuzzy duration 
information, although it is not generally avail-
able; Rieger (1974) discusses the issue for less 
than a page; there has been work in fuzzy logic 
on representing and reasoning with imprecise 
durations (Godo and Vila, 1995; Fortemps, 
1997), but these make no attempt to collect hu-
man judgments on such durations or learn to ex-
tract them automatically from texts.) 
2 Inter-Annotator Agreement 
Although the graphical output of the annotations 
enables us to visualize quickly the level of agree-
ment among different annotators for each event, 
a quantitative measurement of the agreement is 
needed. 
The kappa statistic (Krippendorff, 1980; Car-
letta, 1996) has become the de facto standard to 
assess inter-annotator agreement. It is computed 
as: 
)(1
)()(
EP
EPAP
?
?=?  
P(A) is the observed agreement among the an-
notators, and P(E) is the expected agreement, 
which is the probability that the annotators agree 
by chance.  
In order to compute the kappa statistic for our 
task, we have to compute P(A) and P(E), but 
those computations are not straightforward.  
P(A): What should count as agreement among 
annotators for our task?  
P(E): What is the probability that the annota-
tors agree by chance for our task? 
2.1 What Should Count as Agreement? 
Determining what should count as agreement is 
not only important for assessing inter-annotator 
agreement, but is also crucial for later evaluation 
of machine learning experiments. For example, 
for a given event with a known gold standard 
duration range from 1 hour to 4 hours, if a ma-
chine learning program outputs a duration of 3 
hours to 5 hours, how should we evaluate this 
result? 
In the literature on the kappa statistic, most au-
thors address only category data; some can han-
dle more general data, such as data in interval 
scales or ratio scales. However, none of the tech-
niques directly apply to our data, which are 
ranges of durations from a lower bound to an 
upper bound. 
394
 
Figure 1: Overlap of Judgments of [10 minutes, 
30 minutes] and [10 minutes, 2 hours]. 
 
In fact, what coders were instructed to anno-
tate for a given event is not just a range, but a 
duration distribution for the event, where the 
area between the lower bound and the upper 
bound covers about 80% of the entire distribution 
area. Since it?s natural to assume the most likely 
duration for such distribution is its mean (aver-
age) duration, and the distribution flattens out 
toward the upper and lower bounds, we use the 
normal or Gaussian distribution to model our 
duration distributions. If the area between lower 
and upper bounds covers 80% of the entire dis-
tribution area, the bounds are each 1.28 standard 
deviations from the mean.  
Figure 1 shows the overlap in distributions for 
judgments of [10 minutes, 30 minutes] and [10 
minutes, 2 hours], and the overlap or agreement 
is 0.508706. 
2.2 Expected Agreement 
What is the probability that the annotators agree 
by chance for our task? The first quick response 
to this question may be 0, if we consider all the 
possible durations from 1 second to 1000 years 
or even positive infinity. 
However, not all the durations are equally pos-
sible. As in (Krippendorff, 1980), we assume 
there exists one global distribution for our task 
(i.e., the duration ranges for all the events), and 
?chance? annotations would be consistent with 
this distribution. Thus, the baseline will be an 
annotator who knows the global distribution and 
annotates in accordance with it, but does not read 
the specific article being annotated. Therefore, 
we must compute the global distribution of the 
durations, in particular, of their means and their 
widths. This will be of interest not only in deter-
mining expected agreement, but also in terms of  
-5 0 5 10 15 20 25 30
0
20
40
60
80
100
120
140
160
180
Means of Annotated Durations
N
um
be
r o
f A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 2: Distribution of Means of Annotated 
Durations. 
 
what it says about the genre of news articles and 
about fuzzy judgments in general. 
We first compute the distribution of the means 
of all the annotated durations. Its histogram is 
shown in Figure 2, where the horizontal axis 
represents the mean values in the natural loga-
rithmic scale and the vertical axis represents the 
number of annotated durations with that mean. 
There are two peaks in this distribution. One is 
from 5 to 7 in the natural logarithmic scale, 
which corresponds to about 1.5 minutes to 30 
minutes. The other is from 14 to 17 in the natural 
logarithmic scale, which corresponds to about 8 
days to 6 months. One could speculate that this 
bimodal distribution is because daily newspapers 
report short events that happened the day before 
and place them in the context of larger trends.  
We also compute the distribution of the widths 
(i.e., Xupper ? Xlower) of all the annotated durations, 
and its histogram is shown in Figure 3, where the 
horizontal axis represents the width in the natural 
logarithmic scale and the vertical axis represents 
the number of annotated durations with that 
width. Note that it peaks at about a half order of 
magnitude (Hobbs and Kreinovich, 2001).  
Since the global distribution is determined by 
the above mean and width distributions, we can 
then compute the expected agreement, i.e., the 
probability that the annotators agree by chance, 
where the chance is actually based on this global 
distribution. 
Two different methods were used to compute 
the expected agreement (baseline), both yielding 
nearly equal results. These are described in detail 
in (Pan et al, 2006). For both, P(E) is about 0.15. 
 
395
-5 0 5 10 15 20 25
0
50
100
150
200
250
300
350
400
Widths of Annotated Durations
N
um
be
r 
of
 A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 3: Distribution of Widths of Annotated 
Durations. 
3 Features 
In this section, we describe the lexical, syntactic, 
and semantic features that we considered in 
learning event durations. 
3.1 Local Context 
For a given event, the local context features in-
clude a window of n tokens to its left and n to-
kens to its right, as well as the event itself, for n 
= {0, 1, 2, 3}. The best n determined via cross 
validation turned out to be 0, i.e., the event itself 
with no local context. But we also present results 
for n = 2 in Section 4.3 to evaluate the utility of 
local context. 
A token can be a word or a punctuation mark. 
Punctuation marks are not removed, because they 
can be indicative features for learning event du-
rations. For example, the quotation mark is a 
good indication of quoted reporting events, and 
the duration of such events most likely lasts for 
seconds or minutes, depending on the length of 
the quoted content. However, there are also cases 
where quotation marks are used for other pur-
poses, such as emphasis of quoted words and 
titles of artistic works. 
For each token in the local context, including 
the event itself, three features are included: the 
original form of the token, its lemma (or root 
form), and its part-of-speech (POS) tag. The 
lemma of the token is extracted from parse trees 
generated by the CONTEX parser (Hermjakob 
and Mooney, 1997) which includes rich context 
information in parse trees, and the Brill tagger 
(Brill, 1992) is used for POS tagging. 
The context window doesn?t cross the bounda-
ries of sentences. When there are not enough to-
kens on either side of the event within the win-
dow, ?NULL? is used for the feature values. 
Features Original Lemma POS 
Event signed sign VBD 
1token-after the the DT 
2token-after plan plan NN 
1token-before Friday Friday NNP 
2token-before on on IN 
Table 1: Local context features for the ?signed? 
event in sentence (1) with n = 2. 
 
The local context features extracted for the 
?signed? event in sentence (1) is shown in Table 
1 (with a window size n = 2). The feature vector 
is [signed, sign, VBD, the, the, DT, plan, plan, 
NN, Friday, Friday, NNP, on, on, IN]. 
 
(1) The two presidents on Friday signed the 
plan. 
3.2 Syntactic Relations 
The information in the event?s syntactic envi-
ronment is very important in deciding the dura-
tions of events. For example, there is a difference 
in the durations of the ?watch? events in the 
phrases ?watch a movie? and ?watch a bird fly?. 
For a given event, both the head of its subject 
and the head of its object are extracted from the 
parse trees generated by the CONTEX parser. 
Similarly to the local context features, for both 
the subject head and the object head, their origi-
nal form, lemma, and POS tags are extracted as 
features. When there is no subject or object for 
an event, ?NULL? is used for the feature values. 
For the ?signed? event in sentence (1), the 
head of its subject is ?presidents? and the head of 
its object is ?plan?. The extracted syntactic rela-
tion features are shown in Table 2, and the fea-
ture vector is [presidents, president, NNS, plan, 
plan, NN]. 
3.3 WordNet Hypernyms 
Events with the same hypernyms may have simi-
lar durations. For example, events ?ask? and 
?talk? both have a direct WordNet (Miller, 1990) 
hypernym of ?communicate?, and most of the 
time they do have very similar durations in the 
corpus. 
However, closely related events don?t always 
have the same direct hypernyms. For example, 
?see? has a direct hypernym of ?perceive?, 
whereas ?observe? needs two steps up through 
the hypernym hierarchy before reaching ?per-
ceive?. Such correlation between events may be 
lost if only the direct hypernyms of the words are 
extracted. 
396
Features Original Lemma POS 
Subject presidents president NNS 
Object plan plan NN 
Table 2: Syntactic relation features for the 
?signed? event in sentence (1). 
 
Feature 1-hyper 2-hyper 3-hyper 
Event write communicate interact 
Subject corporate executive executive 
adminis-
trator 
Object idea content cognition 
Table 3: WordNet hypernym features for the 
event (?signed?), its subject (?presidents?), and 
its object (?plan?) in sentence (1). 
 
It is useful to extract the hypernyms not only 
for the event itself, but also for the subject and 
object of the event. For example, events related 
to a group of people or an organization usually 
last longer than those involving individuals, and 
the hypernyms can help distinguish such con-
cepts. For example, ?society? has a ?group? hy-
pernym (2 steps up in the hierarchy), and 
?school? has an ?organization? hypernym (3 
steps up). The direct hypernyms of nouns are 
always not general enough for such purpose, but 
a hypernym at too high a level can be too general 
to be useful. For our learning experiments, we 
extract the first 3 levels of hypernyms from 
WordNet. 
Hypernyms are only extracted for the events 
and their subjects and objects, not for the local 
context words. For each level of hypernyms in 
the hierarchy, it?s possible to have more than one 
hypernym, for example, ?see? has two direct hy-
pernyms, ?perceive? and ?comprehend?. For a 
given word, it may also have more than one 
sense in WordNet. In such cases, as in (Gildea 
and Jurafsky, 2002), we only take the first sense 
of the word and the first hypernym listed for each 
level of the hierarchy. A word disambiguation 
module might improve the learning performance. 
But since the features we need are the hypernyms, 
not the word sense itself, even if the first word 
sense is not the correct one, its hypernyms can 
still be good enough in many cases. For example, 
in one news article, the word ?controller? refers 
to an air traffic controller, which corresponds to 
the second sense in WordNet, but its first sense 
(business controller) has the same hypernym of 
?person? (3 levels up) as the second sense (direct 
hypernym). Since we take the first 3 levels of 
hypernyms, the correct hypernym is still ex-
tracted. 
 
P(A) P(E) Kappa 
0.528 0.740 0.877 
0.500 0.755 
Table 4: Inter-Annotator Agreement for Binary 
Event Durations. 
 
When there are less than 3 levels of hy-
pernyms for a given word, its hypernym on the 
previous level is used. When there is no hy-
pernym for a given word (e.g., ?go?), the word 
itself will be used as its hypernyms. Since 
WordNet only provides hypernyms for nouns 
and verbs, ?NULL? is used for the feature values 
for a word that is not a noun or a verb.  
For the ?signed? event in sentence (1), the ex-
tracted WordNet hypernym features for the event 
(?signed?), its subject (?presidents?), and its ob-
ject (?plan?) are shown in Table 3, and the fea-
ture vector is [write, communicate, interact, cor-
porate_executive, executive, administrator, idea, 
content, cognition]. 
4 Experiments 
The distribution of the means of the annotated 
durations in Figure 2 is bimodal, dividing the 
events into those that take less than a day and 
those that take more than a day. Thus, in our first 
machine learning experiment, we have tried to 
learn this coarse-grained event duration informa-
tion as a binary classification task. 
4.1 Inter-Annotator Agreement, Baseline, 
and Upper Bound 
Before evaluating the performance of different 
learning algorithms, the inter-annotator agree-
ment, the baseline and the upper bound for the 
learning task are assessed first.  
Table 4 shows the inter-annotator agreement 
results among 3 annotators for binary event dura-
tions. The experiments were conducted on the 
same data sets as in (Pan et al, 2006). Two 
kappa values are reported with different ways of 
measuring expected agreement (P(E)), i.e., 
whether or not the annotators have prior knowl-
edge of the global distribution of the task. 
The human agreement before reading the 
guidelines (0.877) is a good estimate of the upper 
bound performance for this binary classification 
task. The baseline for the learning task is always 
taking the most probable class. Since 59.0% of 
the total data is ?long? events, the baseline per-
formance is 59.0%. 
 
 
397
Class Algor. Prec. Recall F-Score
SVM 0.707 0.606 0.653 
NB 0.567 0.768 0.652 Short 
C4.5 0.571 0.600 0.585 
SVM 0.793 0.857 0.823 
NB 0.834 0.665 0.740 
Long 
 
C4.5 0.765 0.743 0.754 
Table 5: Test Performance of Three Algorithms. 
4.2 Data 
The original annotated data can be straightfor-
wardly transformed for this binary classification 
task. For each event annotation, the most likely 
(mean) duration is calculated first by averaging 
(the logs of) its lower and upper bound durations. 
If its most likely (mean) duration is less than a 
day (about 11.4 in the natural logarithmic scale), 
it is assigned to the ?short? event class, otherwise 
it is assigned to the ?long? event class. (Note that 
these labels are strictly a convenience and not an 
analysis of the meanings of ?short? and ?long?.) 
We divide the total annotated non-WSJ data 
(2132 event instances) into two data sets: a train-
ing data set with 1705 event instances (about 
80% of the total non-WSJ data) and a held-out 
test data set with 427 event instances (about 20% 
of the total non-WSJ data). The WSJ data (156 
event instances) is kept for further test purposes 
(see Section 4.4). 
4.3 Experimental Results (non-WSJ) 
Learning Algorithms. Three supervised learn-
ing algorithms were evaluated for our binary 
classification task, namely, Support Vector Ma-
chines (SVM) (Vapnik, 1995), Na?ve Bayes 
(NB) (Duda and Hart, 1973), and Decision Trees 
C4.5 (Quinlan, 1993). The Weka (Witten and 
Frank, 2005) machine learning package was used 
for the implementation of these learning algo-
rithms. Linear kernel is used for SVM in our ex-
periments. 
Each event instance has a total of 18 feature 
values, as described in Section 3, for the event 
only condition, and 30 feature values for the lo-
cal context condition when n = 2. For SVM and 
C4.5, all features are converted into binary fea-
tures (6665 and 12502 features). 
Results. 10-fold cross validation was used to 
train the learning models, which were then tested 
on the unseen held-out test set, and the perform-
ance (including the precision, recall, and F-score1  
                                                 
1 F-score is computed as the harmonic mean of the preci-
sion and recall: F = (2*Prec*Rec)/(Prec+Rec). 
Algorithm Precision  
Baseline 59.0% 
C4.5 69.1% 
NB 70.3% 
SVM 76.6% 
Human Agreement 87.7% 
Table 6: Overall Test Precision on non-WSJ 
Data. 
 
for each class) of the three learning algorithms is 
shown in Table 5. The significant measure is 
overall precision, and this is shown for the three 
algorithms in Table 6, together with human a-
greement (the upper bound of the learning task) 
and the baseline. 
We can see that among all three learning algo-
rithms, SVM achieves the best F-score for each 
class and also the best overall precision (76.6%). 
Compared with the baseline (59.0%) and human 
agreement (87.7%), this level of performance is 
very encouraging, especially as the learning is 
from such limited training data. 
Feature Evaluation. The best performing 
learning algorithm, SVM, was then used to ex-
amine the utility of combinations of four differ-
ent feature sets (i.e., event, local context, syntac-
tic, and WordNet hypernym features). The de-
tailed comparison is shown in Table 7.  
We can see that most of the performance 
comes from event word or phrase itself. A sig-
nificant improvement above that is due to the 
addition of information about the subject and 
object. Local context does not help and in fact 
may hurt, and hypernym information also does 
not seem to help2. It is of interest that the most 
important information is that from the predicate 
and arguments describing the event, as our lin-
guistic intuitions would lead us to expect. 
4.4 Test on WSJ Data 
Section 4.3 shows the experimental results with 
the learned model trained and tested on the data 
with the same genre, i.e., non-WSJ articles. 
In order to evaluate whether the learned model 
can perform well on data from different news 
genres, we tested it on the unseen WSJ data (156 
event instances). The performance (including the 
precision, recall, and F-score for each class) is 
shown in Table 8. The precision (75.0%) is very 
close to the test performance on the non-WSJ  
                                                 
2 In the ?Syn+Hyper? cases, the learning algorithm with and 
without local context gives identical results, probably be-
cause the other features dominate. 
398
Event Only (n = 0) Event Only + Syntactic Event + Syn + Hyper Class 
Prec. Rec. F Prec. Rec. F Prec. Rec. F 
Short 0.742 0.465  0.571 0.758 0.587 0.662 0.707    0.606 0.653 
Long 0.748 0.908 0.821 0.792 0.893 0.839 0.793 0.857 0.823 
Overall Prec. 74.7% 78.2% 76.6% 
 Local Context (n = 2) Context + Syntactic Context + Syn + Hyper 
Short 0.672 0.568 0.615 0.710 0.600    0.650 0.707    0.606 0.653 
Long 0.774 0.842 0.806 0.791 0.860 0.824 0.793 0.857 0.823 
Overall Prec. 74.2% 76.6% 76.6% 
Table 7: Feature Evaluation with Different Feature Sets using SVM. 
 
Class Prec. Rec. F 
Short 0.692   0.610 0.649
Long 0.779   0.835 0.806
Overall Prec. 75.0% 
Table 8: Test Performance on WSJ data. 
 
P(A) P(E) Kappa 
0.151 0.762 0.798 
0.143 0.764 
Table 9: Inter-Annotator Agreement for Most 
Likely Temporal Unit. 
 
data, and indicates the significant generalization 
capacity of the learned model. 
5 Learning the Most Likely Temporal 
Unit 
These encouraging results have prompted us to 
try to learn more fine-grained event duration in-
formation, viz., the most likely temporal units of 
event durations (cf. (Rieger 1974)?s ORDER-
HOURS, ORDERDAYS). 
For each original event annotation, we can ob-
tain the most likely (mean) duration by averaging 
its lower and upper bound durations, and assign-
ing it to one of seven classes (i.e., second, min-
ute, hour, day, week, month, and year) based on 
the temporal unit of its most likely duration.  
However, human agreement on this more fine-
grained task is low (44.4%). Based on this obser-
vation, instead of evaluating the exact agreement 
between annotators, an ?approximate agreement? 
is computed for the most likely temporal unit of 
events. In ?approximate agreement?, temporal 
units are considered to match if they are the same 
temporal unit or an adjacent one. For example, 
?second? and ?minute? match, but ?minute? and 
?day? do not. 
Some preliminary experiments have been con-
ducted for learning this multi-classification task. 
The same data sets as in the binary classification 
task were used. The only difference is that the 
class for each instance is now labeled with one 
Algorithm Precision  
Baseline 51.5% 
C4.5 56.4% 
NB 65.8% 
SVM 67.9% 
Human Agreement 79.8% 
Table 10: Overall Test Precisions. 
 
of the seven temporal unit classes. 
The baseline for this multi-classification task 
is always taking the temporal unit which with its 
two neighbors spans the greatest amount of data. 
Since the ?week?, ?month?, and ?year? classes 
together take up largest portion (51.5%) of the 
data, the baseline is always taking the ?month? 
class, where both ?week? and ?year? are also 
considered a match. Table 9 shows the inter-
annotator agreement results for most likely tem-
poral unit when using ?approximate agreement?. 
Human agreement (the upper bound) for this 
learning task increases from 44.4% to 79.8%. 
10-fold cross validation was also used to train 
the learning models, which were then tested on 
the unseen held-out test set. The performance of 
the three algorithms is shown in Table 10. The 
best performing learning algorithm is again SVM 
with 67.9% test precision. Compared with the 
baseline (51.5%) and human agreement (79.8%), 
this again is a very promising result, especially 
for a multi-classification task with such limited 
training data. It is reasonable to expect that when 
more annotated data becomes available, the 
learning algorithm will achieve higher perform-
ance when learning this and more fine-grained 
event duration information. 
Although the coarse-grained duration informa-
tion may look too coarse to be useful, computers 
have no idea at all whether a meeting event takes 
seconds or centuries, so even coarse-grained es-
timates would give it a useful rough sense of how 
long each event may take. More fine-grained du-
ration information is definitely more desirable 
for temporal reasoning tasks. But coarse-grained 
399
durations to a level of temporal units can already 
be very useful. 
6 Conclusion 
In the research described in this paper, we have 
addressed a problem -- extracting information 
about event durations encoded in event descrip-
tions -- that has heretofore received very little 
attention in the field.  It is information that can 
have a substantial impact on applications where 
the temporal placement of events is important.  
Moreover, it is representative of a set of prob-
lems ? making use of the vague information in 
text ? that has largely eluded empirical ap-
proaches in the past.  In (Pan et al, 2006), we 
explicate the linguistic categories of the phenom-
ena that give rise to grossly discrepant judgments 
among annotators, and give guidelines on resolv-
ing these discrepancies.  In the present paper, we 
describe a method for measuring inter-annotator 
agreement when the judgments are intervals on a 
scale; this should extend from time to other sca-
lar judgments.  Inter-annotator agreement is too 
low on fine-grained judgments.  However, for the 
coarse-grained judgments of more than or less 
than a day, and of approximate agreement on 
temporal unit, human agreement is acceptably 
high.  For these cases, we have shown that ma-
chine-learning techniques achieve impressive 
results.   
Acknowledgments 
This work was supported by the Advanced Re-
search and Development Activity (ARDA), now 
the Disruptive Technology Office (DTO), under 
DOD/DOI/ARDA Contract No. NBCHC040027. 
The authors have profited from discussions with 
Hoa Trang Dang, Donghui Feng, Kevin Knight, 
Daniel Marcu, James Pustejovsky, Deepak Ravi-
chandran, and Nathan Sobo. 
References 
B. Boguraev and R. K. Ando. 2005. TimeML-
Compliant Text Analysis for Temporal Reasoning. 
In Proceedings of International Joint Conference 
on Artificial Intelligence (IJCAI). 
E. Brill. 1992. A simple rule-based part of speech 
tagger. In Proceedings of the Third Conference on 
Applied Natural Language Processing. 
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
gustics, 22(2):249?254. 
R. O. Duda and P. E. Hart. 1973. Pattern Classifica-
tion and Scene Analysis. Wiley, New York. 
E. Filatova and E. Hovy. 2001. Assigning Time-
Stamps to Event-Clauses. Proceedings of ACL 
Workshop on Temporal and Spatial Reasoning. 
P. Fortemps. 1997. Jobshop Scheduling with Impre-
cise Durations: A Fuzzy Approach. IEEE Transac-
tions on Fuzzy Systems Vol. 5 No. 4. 
D. Gildea and D. Jurafsky. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics, 
28(3):245-288. 
L. Godo and L. Vila. 1995. Possibilistic Temporal 
Reasoning based on Fuzzy Temporal Constraints. 
In Proceedings of International Joint Conference 
on Artificial Intelligence (IJCAI). 
U. Hermjakob and R. J. Mooney. 1997. Learning 
Parse and Translation Decisions from Examples 
with Rich Context. In Proceedings of the 35th An-
nual Meeting of the Association for Computational 
Linguistics (ACL). 
J. Hitzeman, M. Moens, and C. Grover. 1995. Algo-
rithms for Analyzing the Temporal Structure of 
Discourse. In Proceedings of EACL. Dublin, Ire-
land. 
J. R. Hobbs and V. Kreinovich. 2001. Optimal Choice 
of Granularity in Commonsense Estimation: Why 
Half Orders of Magnitude, In Proceedings of Joint 
9th IFSA World Congress and 20th NAFIPS Inter-
national Conference, Vacouver, British Columbia. 
K. Krippendorf. 1980. Content Analysis: An introduc-
tion to its methodology. Sage Publications. 
I. Mani and G. Wilson. 2000. Robust Temporal Proc-
essing of News. In Proceedings of the 38th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL). 
G. A. Miller. 1990. WordNet: an On-line Lexical Da-
tabase. International Journal of Lexicography 3(4). 
F. Pan, R. Mulkar, and J. R. Hobbs. 2006. An Anno-
tated Corpus of Typical Durations of Events. In 
Proceedings of the Fifth International Conference 
on Language Resources and Evaluation (LREC), 
Genoa, Italy. 
J. Pustejovsky, P. Hanks, R. Saur?, A. See, R. Gai-
zauskas, A. Setzer, D. Radev, B. Sundheim, D. 
Day, L. Ferro and M. Lazo. 2003. The timebank 
corpus. In Corpus Linguistics, Lancaster, U.K. 
J. R. Quinlan. 1993. C4.5: Programs for Machine 
Learning. Morgan Kaufmann, San Francisco. 
C. J. Rieger. 1974. Conceptual memory: A theory and 
computer program for processing and meaning 
content of natural language utterances. Stanford 
AIM-233. 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer-Verlag, New York. 
I. H. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques, 2nd 
Edition, Morgan Kaufmann, San Francisco. 
400
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 38?45,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extending TimeML with Typical Durations of Events 
 
 
Feng Pan, Rutu Mulkar, and Jerry R. Hobbs 
Information Sciences Institute (ISI), University of Southern California 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 
{pan, rutu, hobbs}@isi.edu 
 
  
 
Abstract 
In this paper, we demonstrate how to ex-
tend TimeML, a rich specification lan-
guage for event and temporal expressions 
in text, with the implicit typical durations 
of events, temporal information in text 
that has hitherto been largely unexploited. 
Event duration information can be very 
important in applications in which the 
time course of events is to be extracted 
from text. For example, whether two 
events overlap or are in sequence often 
depends very much on their durations.     
1 Introduction 
Temporal information processing has become 
more and more important in many natural lan-
guage processing (NLP) applications, such as 
question answering (Harabagiu and Bejan, 2005; 
Moldovan et. al., 2005; Saur? et. al., 2005), 
summarization (Mani and Schiffman, 2005), and 
information extraction (Surdeanu et. al., 2003). 
Temporal anchoring and event ordering are 
among the most important kinds of temporal in-
formation needed for NLP applications. Al-
though there has been much work on extracting 
and inferring such information from texts 
(Hitzeman et al, 1995; Mani and Wilson, 2000; 
Filatova and Hovy, 2001; Boguraev and Ando, 
2005), none of this work has exploited the im-
plicit event duration information from the text.  
Consider the sentence from a news article: 
George W. Bush met with Vladimir Putin in 
Moscow. 
How long was the meeting?  Our first reaction 
to this question might be that we have no idea.  
But in fact we do have an idea.  We know the 
meeting was longer than 10 seconds and less 
than a year.  How much tighter can we get the 
bounds to be?  Most people would say the meet-
ing lasted between an hour and three days. 
There is much temporal information in text 
that has hitherto been largely unexploited, en-
coded in the descriptions of events and relying 
on our knowledge of the range of usual durations 
of types of events, which can be very important 
in applications in which the time course of events 
is to be extracted from news.  For example, 
whether two events overlap or are in sequence 
often depends very much on their durations.  If a 
war started yesterday, we can be pretty sure it is 
still going on today.  If a hurricane started last 
year, we can be sure it is over by now. 
To extract such implicit event duration infor-
mation from texts automatically, we developed a 
corpus annotated with typical durations of events 
(Pan et al, 2006a) which currently contains all 
the 48 non-Wall-Street-Journal (non-WSJ) news 
articles (a total of 2132 event instances), as well 
as 10 WSJ articles (156 event instances), from 
the TimeBank corpus annotated in TimeML 
(Pustejovky et al, 2003).  
Because the annotated corpus is still fairly 
small, we cannot hope to learn to make fine-
grained judgments of event durations that are 
currently annotated in the corpus, but as we show 
in greater detail in (Pan et al, 2006b), it is possi-
ble to learn useful coarse-grained judgments that 
considerably outperform a baseline and approach 
human performance. 
This paper describes our work on extending 
TimeML with annotations of typical durations of 
events, which can enrich the expressiveness of 
TimeML, and provides NLP applications that 
exploit TimeML with this additional implicit 
event duration information for their temporal 
information processing tasks. 
In Section 2 we first describe the corpus of 
typical durations of events, including the annota-
tion guidelines, the representative event classes 
with examples, the inter-annotator agreement 
38
study, and the machine learning results. TimeML 
and its event classes will be described in Section 
3, and we will discuss how to integrate event du-
ration annotations into TimeML in Section 4.  
2 Annotating and Learning Typical Du-
ration of Events 
In the corpus of typical durations of events, every 
event to be annotated was already identified in 
the TimeBank corpus.  Annotators are asked to 
provide lower and upper bounds on the duration 
of the event, and a judgment of level of confi-
dence in those estimates on a scale from one to 
ten. An interface was built to facilitate the anno-
tation. Graphical output is displayed to enable us 
to visualize quickly the level of agreement 
among different annotators for each event. For 
example, here is the output of the annotations (3 
annotators) for the ?finished? event (in bold) in 
the sentence 
After the victim, Linda Sanders, 35, had fin-
ished her cleaning and was waiting for her 
clothes to dry,... 
 
 
This graph shows that the first annotator believes 
that the event lasts for minutes whereas the sec-
ond annotator believes it could only last for sev-
eral seconds. The third annotates the event to 
range from a few seconds to a few minutes. A 
logarithmic scale is used for the output. 
2.1 Annotation Instructions 
Annotators are asked to identify upper and lower 
bounds that would include 80% of the possible 
cases, excluding anomalous cases.   
The judgments are to be made in context.  
First of all, information in the syntactic environ-
ment needs to be considered before annotating, 
and the events need to be annotated in light of 
the information provided by the entire article. 
Annotation is made easier and more consistent if 
coreferential and near-coreferential descriptions 
of events are identified initially. 
When the articles were completely annotated 
by the three annotators, the results were analyzed 
and the differences were reconciled. Differences 
in annotation could be due to the differences in 
interpretations of the event; however, we found 
that the vast majority of radically different judg-
ments can be categorized into a relatively small 
number of classes. Some of these correspond to 
aspectual features of events, which have been 
intensively investigated (e.g., Vendler, 1967; 
Dowty, 1979; Moens and Steedman, 1988; Pas-
sonneau, 1988). We then developed guidelines to 
cover those cases (see the next section). 
2.2 Event Classes 
Action vs. State: Actions involve change, such 
as those described by words like "speaking", 
"gave", and "skyrocketed". States involve things 
staying the same, such as being dead, being dry, 
and being at peace. When we have an event in 
the passive tense, sometimes there is an ambigu-
ity about whether the event is a state or an action. 
For example, 
Three people were injured in the attack. 
Is the ?injured? event an action or a state? This 
matters because they will have different dura-
tions. The state begins with the action and lasts 
until the victim is healed. Besides the general 
diagnostic tests to distinguish them (Vendler, 
1967; Dowty, 1979), another test can be applied 
to this specific case: Imagine someone says the 
sentence after the action had ended but the state 
was still persisting. Would they use the past or 
present tense? In the ?injured? example, it is 
clear we would say ?Three people were injured 
in the attack?, whereas we would say ?Three 
people are injured from the attack.? Our annota-
tion interface handles events of this type by al-
lowing the annotator to specify which interpreta-
tion he is giving. If the annotator feels it?s too 
ambiguous to distinguish, annotations can be 
given for both interpretations. 
 
Aspectual Events:  Some events are aspects of 
larger events, such as their start or finish. Al-
though they may seem instantaneous, we believe 
they should be considered to happen across some 
interval, i.e., the first or last sub-event of the lar-
ger event. For example,   
 After the victim, Linda Sanders, 35, had fin-
ished her cleaning and was waiting for her 
clothes to dry,? 
The ?finished? event should be considered as the 
last sub-event of the larger event (the ?cleaning? 
event), since it actually involves opening the 
door of the washer, taking out the clothes, clos-
ing the door, and so on. All this takes time. This 
39
interpretation will also give us more information 
on typical durations than simply assuming such 
events are instantaneous. 
 
Reporting Events: These are everywhere in the 
news. They can be direct quotes, taking exactly 
as long as the sentence takes to read, or they can 
be summarizations of long press conferences. We 
need to distinguish different cases: 
Quoted Report: This is when the reported 
content is quoted. The duration of the event 
should be the actual duration of the utterance of 
the quoted content. The time duration can be eas-
ily verified by saying the sentence out loud and 
timing it. For example, 
"It looks as though they panicked," a detective 
said of the robbers. 
This probably took between 1 and 3 seconds; it?s 
very unlikely it took more than 10 seconds. 
Unquoted Report: This is when the reporting 
description occurs without quotes that could be 
as short as just the duration of the actual utter-
ance of the reported content (lower bound), and 
as long as the duration of a briefing or press con-
ference (upper bound). 
If the sentence is very short, then it's likely 
that it is one complete sentence from the 
speaker's remarks, and a short duration should be 
given; if it is a long, complex sentence, then it's 
more likely to be a summary of a long discussion 
or press conference, and a longer duration should 
be given. For example, 
The police said it did not appear that anyone 
else was injured. 
A Brooklyn woman who was watching her 
clothes dry in a laundromat was killed Thursday 
evening when two would-be robbers emptied 
their pistols into the store, the police said. 
If the first sentence were quoted text, it would be 
very much the same. Hence the duration of the 
?said? event should be short. In the second sen-
tence everything that the spokesperson (here the 
police) has said is compiled into a single sen-
tence by the reporter, and it is unlikely that the 
spokesperson said only a single sentence with all 
this information. Thus, it is reasonable to give 
longer duration to this ?said? event. 
 
Multiple Events: Many occurrences of verbs 
and other event descriptors refer to multiple 
events, especially, but not exclusively, if the sub-
ject or object of the verb is plural.  For example,   
Iraq has destroyed its long-range missiles.  
Both single (i.e., destroyed one missile) and ag-
gregate (i.e., destroyed all missiles) events hap-
pened. This was a significant source in dis-
agreements in our first round of annotation. 
Since both judgments provide useful informa-
tion, our current annotation interface allows the 
annotator to specify the event as multiple, and 
give durations for both the single and aggregate 
events.  
 
Events Involving Negation: Negated events 
didn't happen, so it may seem strange to specify 
their duration. But whenever negation is used, 
there is a certain class of events whose occur-
rence is being denied. Annotators should con-
sider this class, and make a judgment about the 
likely duration of the events in it. In addition, 
there is the interval during which the nonoccur-
rence of the events holds. For example,  
He was willing to withdraw troops in ex-
change for guarantees that Israel would not be 
attacked. 
There is the typical amount of time of ?being 
attacked?, i.e., the duration of a single attack, and 
a longer period of time of ?not being attacked?. 
Similarly to multiple events, annotators are asked 
to give durations for both the event negated and 
the negation of that event.   
 
Positive Infinite Durations: These are states 
which continue essentially forever once they be-
gin. For example, 
He is dead. 
Here the time continues for an infinite amount 
of time, and we allow this as an annotation. 
2.3 Inter-Annotator Agreement 
Although the graphical output of the annotations 
enables us to visualize quickly the level of agree-
ment among different annotators for each event, 
a quantitative measurement of the agreement is 
needed. The kappa statistic (Krippendorff, 1980; 
Carletta, 1996) has become the de facto standard 
to assess inter-annotator agreement. It is com-
puted as: 
)(1
)()(
EP
EPAP
?
?=?  
P(A) is the observed agreement among the an-
notators, and P(E) is the expected agreement,  
 
40
 
Figure 1: Overlap of Judgments of [10 minutes, 
30 minutes] and [10 minutes, 2 hours]. 
 
which is the probability that the annotators agree 
by chance.  
2.3.1  What Should Count as Agreement? 
Determining what should count as agreement is 
not only important for assessing inter-annotator 
agreement, but is also crucial for later evaluation 
of machine learning experiments.  
We first need to decide what scale is most ap-
propriate. One possibility is just to convert all the 
temporal units to seconds. However, this would 
not correctly capture our intuitions about the 
relative relations between duration ranges. For 
example, the difference between 1 second and 20 
seconds is significant; while the difference be-
tween 1 year 1 second and 1 year 20 seconds is 
negligible. In order to handle this problem, we 
use a logarithmic scale for our data. After first 
converting from temporal units to seconds, we 
then take the natural logarithms of these values. 
This logarithmic scale also conforms to the half 
orders of magnitude (HOM) (Hobbs and Kreino-
vich, 2001) which was shown to have utility in 
several very different linguistic contexts. 
In the literature on the kappa statistic, most au-
thors address only category data; some can han-
dle more general data, such as data in interval 
scales or ratio scales (Krippendorff, 1980; Car-
letta, 1996). However, none of the techniques 
directly apply to our data, which are ranges of 
durations from a lower bound to an upper bound. 
In fact, what coders were instructed to anno-
tate for a given event is not just a range, but a 
duration distribution for the event, where the 
area between the lower bound and the upper 
bound covers about 80% of the entire distribution 
area. Since it?s natural to assume the most likely 
duration for such distribution is its mean (aver-
age) duration, and the distribution flattens out 
toward the upper and lower bounds, we use the  
-5 0 5 10 15 20 25 30
0
20
40
60
80
100
120
140
160
180
Means of Annotated Durations
N
um
be
r o
f A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 2: Distribution of Means of Annotated 
Durations. 
 
normal or Gaussian distribution to model our 
duration distributions. 
In order to determine a normal distribution, we 
need to know two parameters: the mean and the 
standard deviation. For our duration distributions 
with given lower and upper bounds, the mean is 
the average of the bounds. Under the assumption 
that the area between lower and upper bounds 
covers 80% of the entire distribution area, the 
lower and upper bounds are each 1.28 standard 
deviations from the mean.  
With this data model, the agreement between 
two annotations can be defined as the overlap-
ping area between two normal distributions. The 
agreement among many annotations is the aver-
age overlap of all the pairwise overlapping areas. 
For example, the overlap of judgments of [10 
minutes, 30 minutes] and [10 minutes, 2 hours] 
are as in Figure 1. The overlap or agreement is 
0.508706. 
2.3.2  Expected Agreement 
As in (Krippendorff, 1980), we assume there ex-
ists one global distribution for our task (i.e., the 
duration ranges for all the events), and ?chance? 
annotations would be consistent with this distri-
bution. Thus, the baseline will be an annotator 
who knows the global distribution and annotates 
in accordance with it, but does not read the spe-
cific article being annotated. Therefore, we must 
compute the global distribution of the durations, 
in particular, of their means and their widths. 
This will be of interest not only in determining 
expected agreement, but also in terms of what it 
says about the genre of news articles and about 
fuzzy judgments in general. 
We first compute the distribution of the means 
of all the annotated durations. Its histogram is 
shown in Figure 2, where the horizontal axis 
41
-5 0 5 10 15 20 25
0
50
100
150
200
250
300
350
400
Widths of Annotated Durations
N
um
be
r o
f A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 3: Distribution of Widths of Annotated 
Durations. 
 
represents the mean values in the natural loga-
rithmic scale and the vertical axis represents the 
number of annotated durations with that mean. 
We also compute the distribution of the widths 
(i.e., upper bound ? lower bound) of all the anno-
tated durations, and its histogram is shown in 
Figure 3, where the horizontal axis represents the 
width in the natural logarithmic scale and the 
vertical axis represents the number of annotated 
durations with that width. 
Two different methods were used to compute 
the expected agreement (baseline), both yielding 
nearly equal results. These are described in detail 
in (Pan et al, 2006a). For both, P(E) is about 
0.15. 
Experimental results show that the use of the 
annotation guidelines resulted in about 10% im-
provement in inter-annotator agreement, meas-
ured as described in this section, see (Pan et al, 
2006a) for details. 
2.4 Machine Learning Experiments 
2.4.1  Features 
Local Context. For a given event, the local con-
text features include a window of n tokens to its 
left and n tokens to its right, as well as the event 
itself. The best n was determined via cross vali-
dation. A token can be a word or a punctuation 
mark. For each token in the local context, includ-
ing the event itself, three features are included: 
the original form of the token, its lemma (or root 
form), and its part-of-speech (POS) tag. 
Syntactic Relations. The information in the 
event?s syntactic environment is very important 
in deciding the durations of events. For a given 
event, both the head of its subject and the head of 
its object are extracted from the parse trees gen-
erated by the CONTEX parser (Hermjakob and 
Mooney, 1997). Similarly to the local context 
features, for both the subject head and the object 
head, their original form, lemma, and POS tags 
are extracted as features. 
 WordNet Hypernyms. Events with the same 
hypernyms may have similar durations. But 
closely related events don?t always have the 
same direct hypernyms. We extract the hy-
pernyms not only for the event itself, but also for 
the subject and object of the event, since events 
related to a group of people or an organization 
usually last longer than those involving individu-
als, and the hypernyms can help distinguish such 
concepts. For our learning experiments, we ex-
tract the first 3 levels of hypernyms from Word-
Net (Miller, 1990). 
2.4.2  Learning Coarse-grained Binary 
Event Durations 
The distribution of the means of the annotated 
durations in Figure 2 is bimodal, dividing the 
events into those that take less than a day and 
those that take more than a day. Thus, in our first 
machine learning experiment, we have tried to 
learn this coarse-grained event duration informa-
tion as a binary classification task. 
Data. The original annotated data can be 
straightforwardly transformed for this binary 
classification task. For each event annotation, the 
most likely (mean) duration is calculated first by 
averaging (the logs of) its lower and upper bound 
durations. If its most likely (mean) duration is 
less than a day (about 11.4 in the natural loga-
rithmic scale), it is assigned to the ?short? event 
class, otherwise it is assigned to the ?long? event 
class. (Note that these labels are strictly a con-
venience and not an analysis of the meanings of 
?short? and ?long?.) 
We divide the total annotated non-WSJ data 
(2132 event instances) into two data sets: a train-
ing data set with 1705 event instances (about 
80% of the total non-WSJ data) and a held-out 
test data set with 427 event instances (about 20% 
of the total non-WSJ data). The WSJ data (156 
event instances) is kept for further test purposes. 
Results. The learning results in Figure 4 show 
that among all three learning algorithms explored 
(Na?ve Bayes (NB), Decision Trees C4.5, and 
Support Vector Machines (SVM)), SVM with 
linear kernel achieves the best overall precision 
(76.6%). Compared with the baseline (59.0%) 
and human agreement (87.7%), this level of per-
formance is very encouraging, especially as the 
learning is from such limited training data. 
 
42
 
Figure 4: Overall Test Precision on non-WSJ 
Data. 
 
Feature evaluation in (Pan et al, 2006b) shows 
that most of the performance comes from event 
word or phrase itself. A significant improvement 
above that is due to the addition of information 
about the subject and object. Local context does 
not help and in fact may hurt, and hypernym in-
formation also does not seem to help. It is grati-
fying to see that the most important information 
is that from the predicate and arguments describ-
ing the event, as our linguistic intuitions would 
lead us to expect. 
In order to evaluate whether the learned model 
can perform well on data from different news 
genres, we tested it on the unseen WSJ data (156 
event instances). A precision of 75.0%, which is 
very close to the test performance on the non-
WSJ data, proves the great generalization capac-
ity of the learned model. 
Some preliminary experimental results of 
learning the more fine-grained event duration 
information, i.e., the most likely temporal unit 
(cf. (Rieger 1974)?s ORDERHOURS, ORDERDAYS), 
are shown in (Pan et al, 2006b). SVM again 
achieves the best performance with 67.9% test 
precision (baseline 51.5% and human agreement 
79.8%) in ?approximate agreement? where tem-
poral units are considered to match if they are the 
same temporal unit or an adjacent one. 
3 TimeML and Its Event Classes 
TimeML (Pustejovsky et al, 2003) is a rich 
specification language for event and temporal 
expressions in natural language text. Unlike most 
previous attempts at event and temporal specifi-
cation, TimeML separates the representation of 
event and temporal expressions from the anchor-
ing or ordering dependencies that may exist in a 
given text. 
TimeML includes four major data structures: 
EVENT, TIMEX3, SIGNAL, AND LINK. 
EVENT is a cover term for situations that happen 
or occur, and also those predicates describing 
states or circumstances in which something ob-
tains or holds true. TIMEX3, which extends 
TIMEX2 (Ferro, 2001), is used to mark up ex-
plicit temporal expressions, such as time, dates, 
and durations. SIGNAL is used to annotate sec-
tions of text, typically function words that indi-
cate how temporal objects are related to each 
other (e.g., ?when?, ?during?, ?before?). The set 
of LINK tags encode various relations that exist 
between the temporal elements of a document, 
including three subtypes: TLINK (temporal 
links), SLINK (subordination links), and ALINK 
(aspectual links). 
Our event duration annotations can be inte-
grated into the EVENT tag. In TimeML each 
event belongs to one of the seven event classes, 
i.e., reporting, perception, aspectual, I-action, I-
state, state, occurrence. TimeML annotation 
guidelines1 give detailed description for each of 
the classes: 
Reporting. This class describes the action of a 
person or an organization declaring something, 
narrating an event, informing about an event, etc 
(e.g., say, report, tell, explain, state). 
Perception. This class includes events involv-
ing the physical perception of another event (e.g., 
see, watch, view, hear). 
Aspectual. In languages such as English and 
French, there is a grammatical device of aspec-
tual predication, which focuses on different fac-
ets of event history, i.e., initiation, reinitiation, 
termination, culmination, continuation (e.g., be-
gin, stop, finish, continue). 
I-Action. An I-Action is an Intensional Action. 
It introduces an event argument (which must be 
in the text explicitly) describing an action or 
situation from which we can infer something 
given its relation with the I-Action (e.g., attempt, 
try, promise). 
I-State.  This class of events are similar to the 
previous class. This class includes states that re-
fer to alternative or possible worlds (e.g., believe, 
intend, want). 
State. This class describes circumstances in 
which something obtains or holds true (e.g., on 
board, kidnapped, peace). 
Occurrence. This class includes all the many 
other kinds of events describing something that 
happens or occurs in the world (e.g., die, crash, 
build, sell). 
                                                 
1http://www.cs.brandeis.edu/~jamesp/arda/time/time
MLdocs/annguide12wp.pdf 
43
4 Integrating Event Duration Annota-
tions into TimeML 
Our event duration annotations can be integrated 
into TimeML by adding two more attributes to 
the EVENT tag for the lower bound and upper 
bound duration annotations (e.g., ?lowerBound-
Duration? and ?upperBoundDuration? attributes). 
To minimize changes of the existing TimeML 
specifications caused by the integration, we can 
try to share as much as possible our event classes 
as described in Section 2.2 with the existing ones 
in TimeML as described in Section 3.  
We can see that four event classes are shared 
with very similar definitions, i.e., reporting, as-
pectual, state, and action/occurrence. For the 
other three event classes that only belong to Ti-
meML (i.e., perception, I-action, I-state), the I-
action and perception classes can be treated as 
special subclasses of the action/occurrence class, 
and the I-state class as a special subclass of the 
state class. 
However, there are still three classes that only 
belong to the event duration annotations (i.e., 
multiple, negation, and positive infinite). The 
positive infinite class can be treated as a special 
subclass of the state class with a special duration 
annotation for positive infinity.  
Each multiple event has two annotations. For 
example, for 
Iraq has destroyed its long-range missiles.  
there is the time it takes to destroy one missile 
and the duration of the interval in which all the 
individual events are situated ? the time it takes 
to destroy all its missiles.  
Since the single event is usually more likely to 
be encountered in multiple documents, and thus 
the duration of the single event is usually more 
likely to be shared and re-used, to simplify the 
specification, we can take only the duration an-
notation of the single events for the multiple 
event class, and the single event can be assigned 
with one of the seven TimeML event classes. For 
example, the ?destroyed? event in the above ex-
ample is assigned with the occurrence class in 
TimeBank. 
The events involving negation can be simpli-
fied similarly. Since the event negated is usually 
more likely to be encountered in multiple docu-
ments, we can take only the duration annotation 
of the negated event for this class. For example, 
in 
He was willing to withdraw troops in ex-
change for guarantees that Israel would not be 
attacked. 
the event negated is the ?being attacked? event 
and it is assigned with the occurrence class in 
TimeBank.  Alternatively, TimeML could be 
extended to treat negations of events as states. 
The format used for annotated durations is 
consistent with that for the value of the DURA-
TION type in TimeML. For example, the sen-
tence 
The official said these sites could only be vis-
ited by a special team of U.N. monitors and dip-
lomats. 
can be marked up in TimeML as: 
 
The official <EVENT eid="e63" 
class="REPORTING"> said </EVENT> 
these sites <SIGNAL sid="s65" 
>could</SIGNAL> only be <EVENT 
eid="e64" class="OCCURRENCE"> 
visited </EVENT> by a special team 
of <ENAMEX TYPE="ORGANIZATION"> U.N. 
</ENAMEX> monitors and diplomats. 
 
If we annotate the ?said? event with the dura-
tion annotation of [5 seconds, 5 minutes], and the 
?visited? event with [10 minutes, 1 day], the ex-
tended mark-up becomes: 
 
The official <EVENT eid="e63" 
class="REPORTING" lowerBoundDura-
tion="PT5S" upperBoundDura-
tion="PT5M"> said </EVENT> these 
sites <SIGNAL sid="s65" 
>could</SIGNAL> only be <EVENT 
eid="e64" class="OCCURRENCE" lower-
BoundDuration="PT10M" upperBoundDu-
ration="P1D"> visited </EVENT> by a 
special team of <ENAMEX 
TYPE="ORGANIZATION"> U.N. </ENAMEX> 
monitors and diplomats. 
5 Conclusion 
In this paper we have demonstrated how to ex-
tend TimeML with typical durations of events. 
We can see that the extension is very straight-
forward. Other interesting temporal information 
can be extracted or learned. For example, for 
each event class, we can generate its own mean 
and widths graphs, and learn their durations 
separately from other classes, which may capture 
different duration characteristics associated with 
each event class.   
44
Acknowledgments 
This work was supported by the Advanced Re-
search and Development Activity (ARDA), now 
the Disruptive Technology Office (DTO), under 
DOD/DOI/ARDA Contract No. NBCHC040027. 
The authors have profited from discussions with 
Hoa Trang Dang, Donghui Feng, Kevin Knight, 
Daniel Marcu, James Pustejovsky, Deepak Ravi-
chandran, and Nathan Sobo. 
References 
B. Boguraev and R. K. Ando. 2005. TimeML-
Compliant Text Analysis for Temporal Reasoning. 
In Proceedings of the International Joint Confer-
enceon Artificial Intelligence (IJCAI). 
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
gustics, 22(2):249?254. 
D. R. Dowty. 1979. Word Meaning and Montague 
Grammar, Dordrecht, Reidel. 
L. Ferro. 2001. Instruction Manual for the Annotation 
of Temporal Expressions. Mitre Technical Report 
MTR 01W0000046, the MITRE Corporation, 
McLean, Virginia. 
E. Filatova and E. Hovy. 2001. Assigning Time-
Stamps to Event-Clauses. Proceedings of ACL 
Workshop on Temporal and Spatial Reasoning. 
S. Harabagiu and C. Bejan. 2005. Question Answer-
ing Based on Temporal Inference. In Proceedings 
of the AAAI-2005 Workshop on Inference for Tex-
tual Question Answering, Pittsburgh, PA. 
U. Hermjakob and R. J. Mooney. 1997. Learning 
Parse and Translation Decisions from Examples 
with Rich Context. In Proceedings of the 35th An-
nual Meeting of the Association for Computational 
Linguistics (ACL). 
J. Hitzeman, M. Moens, and C. Grover. 1995. Algo-
rithms for Analyzing the Temporal Structure of 
Discourse. In Proceedings of EACL. Dublin, Ire-
land. 
J. R. Hobbs and V. Kreinovich. 2001. Optimal Choice 
of Granularity in Commonsense Estimation: Why 
Half Orders of Magnitude, In Proceedings of Joint 
9th IFSA World Congress and 20th NAFIPS Inter-
national Conference, Vacouver, British Columbia. 
K. Krippendorf. 1980. Content Analysis: An introduc-
tion to its methodology. Sage Publications. 
I. Mani and G. Wilson. 2000. Robust Temporal Proc-
essing of News. In Proceedings of Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL). 
I. Mani and B. Schiffman. 2005. Temporally Anchor-
ing and Ordering Events in News. In J. Pustejovsky 
and R. Gaizauskas ed. Time and Event Recognition 
in Natural Language. John Benjamins. 
G. A. Miller. 1990. WordNet: an On-line Lexical Da-
tabase. International Journal of Lexicography 3(4). 
M. Moens and M. Steedman. 1988. Temporal Ontol-
ogy and Temporal Reference. Computational Lin-
guistics 14(2): 15-28. 
D. Moldovan, C. Clark, and S. Harabagiu. 2005. 
Temporal Context Representation and Reasoning. 
In Proceedings of the International Joint Confer-
enceon Artificial Intelligence (IJCAI). 
F. Pan, R. Mulkar, and J. R. Hobbs. 2006a. An Anno-
tated Corpus of Typical Durations of Events. To 
appear in Proceedings of the Fifth International 
Conference on Language Resources and Evalua-
tion (LREC), Genoa, Italy. 
F. Pan, R. Mulkar, and J. R. Hobbs. 2006b. Learning 
Event Durations from Event Descriptions. To ap-
pear in Proceedings of the 44th Conference of the 
Association for Computational Linguistics (COL-
ING-ACL), Sydney, Australia. 
R. J. Passonneau. 1988. A Computational Model of 
the Semantics of Tense and Aspect. Computational 
Linguistics 14:2.44-60. 
J. Pustejovsky, J. Castano, R. Ingria, R. Saur?, R. Gai-
zauskas, A. Setzer, and G. Katz. 2003. TimeML: 
Robust specification of event and temporal expres-
sions in text. In Proceedings of the AAAI Spring 
Symposium on New Directions in Question-
Answering. 
C. J. Rieger. 1974. Conceptual memory: A theory and 
computer program for processing and meaning 
content of natural language utterances. Stanford 
AIM-233. 
R. Saur?, R. Knippen, M. Verhagen and J. Puste-
jovsky. 2005. Evita: A Robust Event Recognizer 
for QA Systems. In Proceedings of HLT/EMNLP. 
M. Surdeanu, S. Harabagiu, J. Williams, and P. 
Aarseth. 2003. Using predicate-argument structures 
for information extraction. In Proceedings of the 
41th Annual Conference of the Association for 
Computational Linguistics (ACL-03), pages 8?15. 
Z. Vendler. 1967. Linguistics in Philosophy, Ithaca, 
Cornell University Press. 
45
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 54?59,
Prague, June 2007. c?2007 Association for Computational Linguistics
On the Role of Lexical and World Knowledge in RTE3 
Peter Clark1, William R. Murray1, John Thompson1, Phil Harrison1,  
Jerry Hobbs2, Christiane Fellbaum3  
1Boeing Phantom Works, The Boeing Company, Seattle, WA 98124 
2USC/ISI, 4676 Admiralty Way, Marina del Rey, CA 90292 
3Princeton University, NJ 08544 
{peter.e.clark,william.r.murray,john.a.thompson,philip.harrison}@boeing.com, 
hobbs@isi.edu, fellbaum@clarity.princeton.edu 
 
 
Abstract 
To score well in RTE3, and even more so 
to create good justifications for entailments, 
substantial lexical and world knowledge is 
needed. With this in mind, we present an 
analysis of a sample of the RTE3 positive 
entailment pairs, to identify where and 
what kinds of world knowledge are needed 
to fully identify and justify the entailment, 
and discuss several existing resources and 
their capacity for supplying that knowledge. 
We also briefly sketch the path we are fol-
lowing to build an RTE system (Our im-
plementation is very preliminary, scoring 
50.9% at the time of RTE). The contribu-
tion of this paper is thus a framework for 
discussing the knowledge requirements 
posed by RTE and some exploration of 
how these requirements can be met. 
1 Introduction 
The Pascal RTE site defines entailment between 
two texts T and H as holding "if, typically, a hu-
man reading T would infer that H is most likely 
true" assuming "common human understanding of 
language as well as common background knowl-
edge." While a few RTE3 entailments can be rec-
ognized using simple syntactic matching, the ma-
jority rely on significant amounts of this "common 
human understanding" of lexical and world knowl-
edge. Our goal in this paper is to analyze what that 
knowledge is, create a preliminary framework for 
it, and explore a few available sources for it. In the 
short term, such knowledge can be (and has been) 
used to drive semantic matching of the T and H 
dependency/parse trees and their semantic repre-
sentations, as many prior RTE systems perform, 
e.g., (Hickl et al, 2006). In the long term, com-
puters should be able to perform deep language 
understanding to build a computational model of 
the scenario being described in T, to reason about 
the entailment, answer further questions, and create 
meaningful justifications. With this longer term 
goal in mind, it is useful to explore the types of 
knowledge required. It also gives a snapshot of the 
kinds of challenges that RTE3 poses. 
 
The scope of this paper is to examine the underly-
ing lexical/world knowledge requirements of RTE, 
rather than the more syntactic/grammatical issues 
of parsing, coreference resolution, named entity 
recognition, punctuation, coordination, typographi-
cal errors, etc. Although there is a somewhat blurry 
line between the two, this separation is useful for 
bounding the analysis. It should be noted that the 
more syntactic issues are themselves vast in RTE, 
but here we will not delve into them. Instead, we 
will perform a thought experiment in which they 
have been handled correctly. 
2 Analysis 
Based on an analysis of 100 (25%) of the positive 
entailments in the RTE3 test set, we have divided 
the knowledge requirements into several rough 
categories, which we now present. We then sum-
marize the frequency with which examples in this 
sample fell into these categories. The examples 
below are fragments of the original test questions, 
abbreviated and occasionally simplified. 
2.1 Syntactic Matching 
In a few cases, entailment can be identified by syn-
tactic matching of T and H, for example: 
54
489.T "The Gurkhas come from Nepal and??  
489.H "The Gurkhas come from Nepal." 
Other examples include 299, 489, and 456. In 
some cases, the syntactic matching can be very 
complex, e.g., examples 152, 724. 
2.2 Synonyms 
Synonymy is often needed to recognize entailment, 
648.T "?go through ? licencing procedures..." 
648.H "?go through the licencing processes." 
Other examples include 286 ("dismiss"/"throw 
out"), 37 ("begin?/"start"), 236 ("wildfire"/"bush 
fire"), and, arguably, 462 ("revenue"/"proceeds"). 
2.3 Generalizations (Hypernyms) 
Similarly, subsumption (generalization) relation-
ships between word senses need to be recognized 
(whether or not a fixed set of senses are used), eg. 
148.T "Beverly served...at WEDCOR" 
148.H "Beverly worked for WEDCOR." 
Others include 178 ("succumbed" as a kind of 
"killed"), and 453 ("take over" as a kind of "buy"). 
2.4 Noun Redundancy 
Sometimes a noun in a compound can be dropped: 
607.T "single-run production process..." 
607.H "Single-run production..." 
Other examples include 269 ("increasing preva-
lence of" ? "increasing"), 604 ("mini-mill proc-
ess" ? "mini-mill"), and (at the phrase level) 668 
("all segments of the public" ? "the public"). 
2.5 Noun-Verb Relations  
Often derivationally related nouns and verbs occur 
in the pairs. To identify and justify the entailment, 
the relationship and its nature is needed, as in: 
 
480 "Marquez is a winner..." ?"Marquez won..." 
 
Other examples include 286 ("pirated", "piracy"), 
and 75 ("invent", "invention"). In some cases, the 
deverbal noun denotes the verb's event, in other 
cases it denotes one of the verb?s arguments (e.g., 
"winner" as the subject/agent of a "win" event).  
2.6 Compound Nouns 
Some examples require inferring the semantic rela-
tion between nouns in a compound, e.g., 
168 "Sirius CEO Karmazin" ? "Karmazin is an 
executive of Sirius" 
583 "physicist Hawking" ? "Hawking is a physi-
cist" 
In some cases this is straightforward, others require 
more detailed knowledge of the entities involved. 
2.7 Definitions 
Although there is somewhat of a fuzzy boundary 
between word and world knowledge, we draw this 
distinction here. Some examples of RTE pairs 
which require knowing word meanings are: 
667 "? found guilty..." ? "?convicted..." 
328 "sufferers of coeliac disease..." ? "coeliacs..." 
 
The second example is particularly interesting as 
many readers (and computers) will not have en-
countered the word "coeliacs" before, yet a person 
can reasonably infer its meaning on the fly from 
context and morphology - something challenging 
for a machine to do. Definitions of compound 
nouns are also sometimes needed, e.g., ?family 
planning? (612) and ?cough syrup? (80).  
2.8 World Knowledge: General 
A large number of RTE pairs require non-
definitional knowledge about the way the world 
(usually) is, e.g.,: 
273 "bears kill people"  ? "bears attack people" 
 
People recognize this entailment as they know 
(have heard about) how people might be killed by 
a bear, and hence can justify why the entailment is 
valid. (They know that the first step in the bear 
killing a person is for the bear to attack that person.) 
Some other examples are: 
499 "shot dead" ? "murder" 
705 "under a contract with" ? "cooperates with" 
721 "worked on the law" ? "discussed the law" 
731 "cut tracts of forest" ? "cut trees in the forest" 
732 "establishing tree farms"  
? "trees were planted" 
639 "X's plans for reorganizing"  
? "X intends to reorganize" 
328 "the diets must be followed by <person>"  
? "the diets are for <person>" 
722 X and Y vote for Z ? X and Y agree to Z. 
 
All these cases appeal to a person's world knowl-
edge concerning the situation being described. 
55
2.9 World Knowledge: Core Theories 
In addition to this more specific knowledge of the 
world, some RTE examples appeal to more general, 
fundamental knowledge (e.g., space, time, plans, 
goals). For example 
 
6.T "Yunupingu is one of the clan of..." 
6.H "Yunupingu is a member of..." 
 
appeals to a basic rule of set inclusion, and 10 (a 
negative entailment: "unsuccessfully sought elec-
tion" ? not elected) appeals to core notions of 
goals and achievement. Several examples appeal to 
core spatial knowledge, e.g.: 
 
491.T "...come from the high mountains of Nepal." 
491.H "...come from Nepal." 
 
178.T "...3 people in Saskatchewan succumbed to 
the storm." 
178.H "...a storm in Saskatchewan." 
 
491 appeals to regional inclusion (if X location Y, 
and Y is in Z, then X location Z), and 178 appeals 
to colocation (if X is at Y, and X physically inter-
acts with Z, then Z is at Y). Other spatial examples 
include 236 ("around Sydney" ? "near Sydney"), 
and 129 ("invasion of" ? "arrived in"). 
2.10 World Knowledge: Frames and Scripts 
Although loosely delineated, another category of 
world knowledge concerns stereotypical places, 
situations and the events which occur in them, with 
various representational schemes proposed in the 
AI literature, e.g., Frames (Minsky 1985), Scripts 
(Schank 1983). Some RTE examples require rec-
ognizing the implicit scenario ("frame", "script", 
etc.) which T describes to confirm the new facts or 
relationships introduced in H are valid. A first ex-
ample is: 
 
358.T "Kiesbauer was target of a letter bomb..."  
358.H "A letter bomb was sent to Kiesbauer." 
 
A person recognizes H as entailed by T because 
he/she knows the "script" for letter bombing, 
which includes sending the bomb in the mail. Thus 
a person could also recognize alternative verbs in 
358.H as valid (e.g., "mailed", "delivered") or in-
valid (e.g., "thrown at", "dropped on"), even 
though these verbs are all strongly associated with 
words in T. For a computer to fully explain the 
entailment, it would need similar knowledge.  
 
As a second example, consider: 
 
538.T "...the O. J. Simpson murder trial..." 
538.H "O. J. Simpson was accused of murder." 
 
Again, this requires knowing about trials: That 
they involve charges, a defendant, an accusation, 
etc., in order to validate H as an entailed expansion 
of T. In this example, there is also a second twist to 
it as the noun phrase in 538.T equally supports the 
hypothesis "O. J. Simpson was murdered." (e.g., 
consider replacing "O. J." with "Nicole"). It is only 
a reference elsewhere in the T sentence to "Simp-
son's attorneys" that suggests Simpson is still alive, 
and hence couldn't have been the victim, and hence 
must be the accused, that clarifies 538.H as being 
correct, a highly complex chain of reasoning.  
 
As a third example, consider: 
736.T "In a security fraud case, Milken was sen-
tenced to 10 years in prison." 
736.H "Milken was imprisoned for security fraud." 
 
This example is particularly interesting, as one 
needs to recognize security fraud as Milken's crime, 
a connection which not stated in T. A human 
reader will recognize the notion of sentencing, and 
thus expect to see a convict and his/her crime, and 
hence can align these expectations with T, validat-
ing H. Thus again, deep knowledge of sentencing 
is needed to understand and justify the entailment. 
 
Some other examples requiring world knowledge 
to validate their expansions, include 623 ("narcot-
ics-sniffing dogs" ? "dogs are used to sniff out 
narcotics"), and 11 ("the Nintendo release of the 
game" ? "the game is produced by Nintendo"). 
2.11 Implicative Verbs 
Some RTE3 examples contain complement-taking 
verbs that make an implication (either positive or 
negative) about the complement. For example: 
 
668 "A survey shows that X..." ? "X..." 
657 "...X was seen..." ? "...X..." 
725 ?...decided to X..." ? "...X..." 
716 "...have been unable to X..." ? "...do not X" 
56
 Table 1: Frequency of different entailment 
phenomena from a sample of 100 RTE3 pairs. 
In the first 3 the implication is positive, but in the 
last the implication is negative. (Nairn et al 2006) 
provide a detailed analysis of this type of behavior. 
In fact, this notion of implicature (one part of a 
sentence making an implication about another part) 
extends beyond single verbs, and there are some 
more complex examples in RTE3, e.g.: 
 
453 "...won the battle to X..." ? "...X..." 
 
784.T "X  reassures Russia it has nothing to fear..." 
784.H "Russia fears..." 
 
In this last example the implication behavior is 
quite complex: (loosely) If X reassures Y of Z, 
then Y is concerned about not-Z. 
2.12 Metonymy/Transfer 
In some cases, language allows us to replace a 
word (sense) with a closely related word (sense):  
708.T "Revenue from stores funded..." 
708.H "stores fund..." 
 
Rules for metonymic transfer, e.g., (Fass 2000), 
can be used to define which transfers are allowed. 
Another example is 723 ??pursue its drive to-
wards X? ? ??pursue X?. 
2.13 Idioms/Protocol/Slang 
Finally, some RTE pairs rely on understanding 
idioms, slang, or special protocols used in lan-
guage, for example: 
 
12 "Drew served as Justice. Kennon returned to 
claim Drew's seat"  ? "Kennon served as Justice." 
486 "name, 1890-1970" ? "name died in 1970" 
408 "takes the title of" ? "is" 
688 "art finds its way back" ? "art gets returned" 
 
The phrases in these examples all have special 
meaning which cannot be easily derived composi-
tionally from their words, and thus require special 
handling within an entailment system. 
2.14 Frequency Statistics 
Table 1 shows the number of positive entailments 
in our sample of 100 that fell into the different 
categories (some fell into several). While there is a 
certain subjectivity in the boundaries of the catego-
ries, the most significant observation is that very 
few entailments depend purely on syntactic ma-
nipulation and simple lexical knowledge (syno-
nyms, hypernyms), and that the vast majority of 
entailments require significant world knowledge, 
highlighting one of the biggest challenges of RTE. 
In addition, the category of general world knowl-
edge -- small, non-definitional facts about the way 
the world (usually) is -- is the largest, suggesting 
that harvesting and using this kind of knowledge 
should continue to be a priority for improving per-
formance on RTE-style tasks. 
3 Sources of World Knowledge 
While there are many potential sources of the 
knowledge that we have identified, we describe 
three in a bit more detail and how they relate to the 
earlier analysis. 
3.1 WordNet 
WordNet (Fellbaum, 1998) is one of the most ex-
tensively used resources in RTE already and in 
computational linguistics in general. Despite some 
well-known problems, it provides broad coverage 
of several key relationships between word senses 
(and words), in particular for synonyms, hy-
pernyms (generalizations), meronyms (parts), and 
semantically (?morphosemantically?) related 
forms. From the preceding analysis, WordNet does 
contain the synonyms {"procedure","process"}, 
{"dismiss","throw out"}, {"begin","start"}, but 
does not contain the compound "wild fire" and 
(strictly correctly) does not contain {"reve-
nue","proceeds"} as synonyms. In addition, the 
57
three hypernyms mentioned in the earlier analysis 
are in WordNet. WordNet alo links (via the ?mor-
phosemantic? link) the 3 noun-verb pairs men-
tioned earlier (win/winner, pirated/piracy, in-
vent/invention) ? however it does not currently 
distinguish the nature of that link (e.g., agent, re-
sult, event). WordNet is currently being expanded 
to include this information, as part of the 
AQUAINT program.  
 
Two independently developed versions of the 
WordNet glosses expressed in logic are also avail-
able: Extended WordNet (Moldovan and Rus, 
2001) and a version about to be released with 
WordNet3.0 (again developed under AQUAINT). 
These in principle can help with definitional 
knowledge. From our earlier analysis, it turns out 
"convicted" is conveniently defined in WordNet as 
"pronounced or proved guilty" potentially bridging 
the gap for pair 667, although there are problems 
with the logical interpretation of this particular 
gloss in both resources mentioned. WordNet does 
have "coeliac", but not in the sense of a person 
with coeliac disease1. 
 
In addition, several existing ?core theories? (e.g., 
TimeML, IKRIS) that can supply some of the fun-
damental knowledge mentioned earlier (e.g., space, 
time, goals) are being connected to WordNet under 
the AQUAINT program. 
3.2 The DIRT Paraphrase Database 
Paraphrases have been used successfully by several 
RTE systems (e.g., Hickl et al, 2005). With re-
spect to our earlier analysis, we examined Lin and 
Pantel's (2001) paraphrase database built with their 
system DIRT, containing 12 million entries. While 
there is of course noise in the database, it contains 
a remarkable amount of sensible world knowledge 
as well as syntactic rewrites, albeit encoded as 
shallow rules lacking word senses. 
 
Looking at the examples from our earlier analysis 
of general world knowledge, we find that three are 
supported by paraphrase rules in the database: 
273: X kills Y ? X attacks Y 
499: X shoots Y ? X murders Y 
                                                 
1  This seems to be an accidental gap; WordNet contains 
many interlinked disease-patient noun pairs, incl. "dia-
betes-diabetic," "epilepsy-eplileptic," etc. 
721: X works on Y ? X discusses Y 
 
And one that could be is not, namely: 
705: X is under a contract with Y ? X coop-
erates with Y (not in the database) 
 
Other examples are outside the scope of DIRT's 
approach (i.e., ?X pattern1 Y? ? ?X pattern2 Y?), 
but nonetheless the coverage is encouraging. 
3.3 FrameNet 
In our earlier analysis, we identified knowledge 
about stereotypical situations and their events as 
important for RTE. FrameNet (Baker et al 1998) 
attempts to encode this knowledge. FrameNet was 
used with some success in RTE2 by Burchardt and 
Frank (2005). FrameNet's basic unit - a Frame - is 
a script-like conceptual schema that refers to a 
situation, object, or event along with its partici-
pants (Frame Elements), identified independent of 
their syntactic configuration. 
 
We earlier discussed how 538.T "...the O. J. Simp-
son murder trial..." might entail 538.H "O. J. Simp-
son was accused of murder." This case applies to 
FrameNet?s Trial frame, which includes the Frame 
Elements Defendant and Charges, with Charges 
being defined as "The legal label for the crime that 
the Defendant is accused of.", thus stating the rela-
tionship between the defendant and the charges, 
unstated in T but made explicit in H, validating the 
entailment. However,  the lexical units instantiat-
ing the Frame Elements are not yet disambiguated 
against a lexical database, limiting full semantic 
understanding. Moreover, FrameNet's  world 
knowledge is stated informally in English descrip-
tions, though it may be possible to convert these to 
a more machine-processable form either manually 
or automatically. 
3.4 Other Resources 
We have drawn attention to these three resources 
as they provide some answers to the requirements 
identified earlier. Several other publicly available 
resources could also be of use, including VerbNet 
(Univ Colorado at Boulder), the Component Li-
brary (Univ Texas at Austin), OpenCyc (Cycorp), 
SUMO, Stanford's additions to WordNet, and the 
Tuple Database (Boeing, following Schubert's 
2002 approach), to name but a few. 
58
4 Sketch of our RTE System 
Although not the primary purpose of this paper, we 
briefly sketch the path we are following to build an 
RTE system able to exploit the above resources. 
Our implementation is very preliminary, scoring 
50.9% at the time of RTE and 52.6% now (55.0% 
on the 525 examples it is able to analyze, guessing 
"no entailment" for the remainder). Nevertheless, 
the following shows the direction we are moving in 
 
Like several other groups, our basic approach is to 
generate logic for the T and H sentences, and then 
explore the application of inference rules to elabo-
rate T, or transform H, until H matches T. Parsing 
is done using a broad coverage chart parser. Sub-
sequently, an abstracted form of the parse tree is 
converted into a logical form, for example: 
 299.H "Tropical storms cause severe damage." 
subject(_Cause1, _Storm1) 
sobject(_Cause1, _Damage1) 
mod(_Storm1, _Tropical1) 
mod(_Damage1, _Severe1) 
input-word(_Storm1, "storm", noun) 
[same for other words] 
 
Entailment is determined if every clause in the se-
mantic representation of H semantically matches 
(subsumes) some clause in T. Two variables in a 
clause match if their input words are the same, or 
some WordNet sense of one is the same as or a 
hypernym of the other. In addition, the system 
searches for DIRT paraphrase rules that can trans-
form the sentences into a form which can then 
match. The explicit use of WordNet and DIRT re-
sults in comprehensible, machine-generated justifi-
cations when entailments are found, , e.g.,: 
 
T: "The Salvation Army operates the shelter under 
a contract with the county." 
H: "The Salvation Army collaborates with the 
county." 
Yes! Justification: I have general knowledge that: 
  IF X works with Y THEN X collaborates with Y 
Here: X = the Salvation Army, Y = the county 
Thus, here: 
        I can see from T: the Salvation Army works 
with the county (because "operate" and "work" 
mean roughly the same thing) 
Thus it follows that:  
  The Salvation Army collaborates with the county.  
We are continuing to develop our system and ex-
pand the number of knowledge sources it uses.  
5 Summary 
To recognize and justify textual entailments, and 
ultimately understand language, considerable lexi-
cal and world knowledge is needed. We have pre-
sented an analysis of some of the knowledge re-
quirements of RTE3, and commented on some 
available sources of that knowledge. The analysis 
serves to highlight the depth and variety of knowl-
edge demanded by RTE3, and contributes a rough 
framework for organizing these requirements. Ul-
timately, to fully understand language, extensive 
knowledge of the world (either manually or auto-
matically acquired) is needed. From this analysis, 
RTE3 is clearly providing a powerful driving force 
for research in this direction. 
Acknowledgements 
This work was performed under the DTO 
AQUAINT program, contract N61339-06-C-0160.  
References 
Collin Baker, Charles Fillmore, and John Lowe. 1998. 
"The Berkeley FrameNet Project". Proc 36th ACL 
Aljoscha Burchardt and Anette Frank. 2005. Approach-
ing Textual Entailment with LFG and FrameNet 
Frames. in 2nd PASCAL RTE Workshop, pp 92-97. 
Dan Fass. 1991. "Met*: A Method for Discriminating 
Metonymy and Metaphor by Computer". In Compu-
tational Linguistics 17 (1), pp 49-90. 
Christiane Fellbaum. 1998. ?WordNet: An Electronic 
Lexical Database.? The MIT Press. 
Andrew Hickl, Jeremy Bensley, John Williams, Kirk 
Roberts, Bryan Rink, and Ying Shi. ?Recognizing 
Textual Entailment with LCC?s Groundhog System?, 
in Proc 2nd PASCAL RTE Workshop, pp 80-85. 
Dekang Lin and Patrick Pantel. 2001. "Discovery of 
Inference Rules for Question Answering". Natural 
Language Engineering 7 (4) pp 343-360.  
Marvin Minsky. 1985. "A Framework for Representing 
Knowledge". In Readings in Knowledge Repn. 
Dan Moldovan and Vasile Rus, 2001. Explaining An-
swers with Extended WordNet, ACL 2001.  
Rowan Nairn, Cleo Condoravdi and Lauri Karttunen. 
2006. Computing Relative Polarity for Textual Infer-
ence. In the Proceedings of ICoS-5. 
Len Schubert. 2002. "Can we Derive General World 
Knowledge from Texts?", Proc. HLT'02, pp84-87. 
59
Augmenting WordNet for Deep
Understanding of Text
Peter Clark1
Christiane Fellbaum2
Jerry R. Hobbs3
Phil Harrison1
William R. Murray1
John Thompson1
1The Boeing Company (USA)
2Princeton University (USA)
3University of Southern California (USA)
email: peter.e.clark@boeing.com
Abstract
One of the big challenges in understanding text, i.e., constructing an over-
all coherent representation of the text, is that much information needed
in that representation is unstated (implicit). Thus, in order to ?fill in
the gaps? and create an overall representation, language processing sys-
tems need a large amount of world knowledge, and creating those knowl-
edge resources remains a fundamental challenge. In our current work,
we are seeking to augment WordNet as a knowledge resource for lan-
guage understanding in several ways: adding in formal versions of its
word sense definitions (glosses); classifying the morphosemantic links
between nouns and verbs; encoding a small number of ?core theories?
about WordNet?s most commonly used terms; and adding in simple rep-
resentations of scripts. Although this is still work in progress, we describe
our experiences so far with what we hope will be a significantly improved
resource for the deep understanding of language.
45
46 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
1 Introduction
Much information that text is intended to convey is not explicitly stated. Rather, the
reader constructs a mental model of the scene described by the text, including many
?obvious? features that were not explicitly mentioned. By one estimate, the ratio of
explicit to implicit facts is 1:8 (Graesser, 1981), making the task of understanding
text, i.e., constructing a coherent representation of the scene that the author intended
to convey, very difficult, even given the generally reasonable quality of syntactic in-
terpretation that today?s systems produce. For example, given the sentence:
A soldier was killed in a gun battle.
a reader will infer that (probably):
The soldier was shot; The soldier died; There was a fight; etc.
even though none of these facts are explicitly stated. A person is able to draw these
plausible conclusions because of the large amounts of world knowledge he/she has,
and his/her ability to use them to construct an overall mental model of the scene being
described.
A key requirement for this task is access to a large body of world knowledge. How-
ever, machines are currently poorly equipped in this regard. Although a few knowl-
edge encoding projects are underway, e.g., Cyc (Lenat and Guha, 1989), developing
such resources continues to be a major challenge, and any contribution to this task
has significant potential benefit. WordNet (Miller, 1995; Fellbaum, 1998) presents an
unique avenue for making inroads into this problem: It already has broad coverage,
multiple lexicosemantic connections, and significant knowledge encoded (albeit infor-
mally) in its glosses. It can thus be viewed as on the way to becoming an extensively
leveragable, ?lightweight? knowledge base for reasoning. In fact, WordNet aleady
plays a central role in many question-answering systems e.g., 21 of the 26 teams in
the recent PASCAL RTE3 challenge used WordNet (Giampiccolo et al, 2007), and
most other large-scale resources already include mappings to it and thus can leverage
it easily. In our work we are developing several augmentations to WordNet to improve
its utility further, and we report here on our experiences to date.
Althoughwe are performing experimentswith recognizing textual entailment (RTE)
(determining whether a hypothesis sentence H follows from some text T), it is impor-
tant to note that RTE is not our end-goal. Many existing RTE systems, e.g., (Adams
et al, 2007; Chambers et al, 2007) largely work by statistically scoring the match be-
tween T and H, but this to an extent sidesteps ?deep? language understanding, namely
building a coherent, internal representation of the overall scenario the input text was
intended to convey. RTE is one way of measuring success in this endeavor, but it is
also possible to do moderately well in RTE without the system even attempting to
?understand? the scenario the text is describing. It is yet to be seen whether very high
performance in RTE can be obtained without some kind of deep language understand-
ing of the entire scene that a text conveys.
We are testing our work with BLUE, Boeing?s Language Understanding Engine,
which we first describe. We then present the WordNet augmentations that we are de-
veloping, and our experience with these as well as with the DIRT paraphrase database.
Augmenting WordNet for Deep Understanding of Text 47
The contribution of this paper is some preliminary insight into avenues and challenges
for creating and leveraging more world knowledge, in the context of WordNet, for
deeper language understanding.
2 Text Interpretation and Subsumption
2.1 Text Interpretation
For text interpretation we are using BLUE, Boeing?s Language Understanding Engine
(Clark and Harrison, 2008), comprising a parser, logical form (LF) generator, and fi-
nal logic generator. Parsing is performed using SAPIR, a mature, bottom-up, broad
coverage chart parser (Harrison and Maxwell, 1986). The parser?s cost function is
biased by a database of manually and corpus-derived ?tuples? (good parse fragments),
as well as hand-coded preference rules. During parsing, the system also generates
a logical form (LF), a semi-formal structure between a parse and full logic, loosely
based on Schubert and Hwang (1993). The LF is a simplified and normalized tree
structure with logic-type elements, generated by rules parallel to the grammar rules,
that contains variables for noun phrases and additional expressions for other sentence
constituents. Some disambiguation decisions are performed at this stage (e.g., struc-
tural, part of speech), while others are deferred (e.g., word senses, semantic roles),
and there is no explicit quantifier scoping. A simple example of an LF is shown below
(items starting with underscores _?A?I? denote variables):
;;; LF for "A soldier was killed in a gun battle."
(DECL ((VAR _X1 "a" "soldier")
(VAR _X2 "a" "battle" (NN "gun" "battle")))
(S (PAST) NIL "kill" _X1 (PP "in" _X2)))
The LF is then used to generate ground logical assertions of the form r(x,y), con-
taining Skolem instances, by applying a set of syntactic rewrite rules recursively to it.
Verbs are reified as individuals, Davidsonian-style. An example of the output is:
;;; logic for "A soldier was killed in a gun battle."
object(kill01,soldier01)
in(kill01,battle01)
modifier(battle01,gun01)
plus predicates associating each Skolem with its corresponding input word. At this
stage of processing, the predicates are syntactic relations (subject(x,y), object(x,y),
modifier(x,y), and all the prepositions, e.g., in(x,y)). Definite coreference is computed
by a special module which uses the (logic for the) referring noun phrase as a query
on the database of assertions. Another module performs special structural transforma-
tions, e.g., when a noun or verb should map to a predicate rather than an individual.
Two additional modules perform (currently naive) word sense disambiguation (WSD)
and semantic role labelling (SRL), described further in Clark and Harrison (2008).
However, for our RTE experiments we have found it more effective to leave senses
and roles underspecified, effectively considering all valid senses and roles (for the
given lexical features) during reasoning until instantiated by the rules that apply.
48 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
2.2 Subsumption
A basic operation for reasoning is determining if one set of clauses subsumes (is more
general than, is thus implied by) another, e.g., (the logic for) ?A person likes a person?
subsumes ?A man loves a woman?. This basic operation is used both to determine if
an axiom applies, and in RTE to determine if a text H subsumes (is implied by) a text
T or its axiom-expanded elaboration. A set S1 of clauses subsumes another S2 if each
clause in S1 subsumes some (different) member of S2. A clause C1 subsumes another
C2 if both (for binary predicates) of C1?s arguments subsume the corresponding argu-
ments in C2, and C1 and C2?s predicates ?match?. An argument A1 subsumes another
A2 if some word sense for A1?s associated word is equal or more general (a hypernym
of) some word sense of A2?s associated word (thus effectively considering all possible
word senses for A1 and A2)1. We also consider adjectives related by WordNet?s ?sim-
ilar? link, e.g., ?clean? and ?pristine?, to be equal. Two syntactic predicates ?match?
(i.e., are considered to denote the same semantic relation) according to the following
rules:
1. both are the same;
2. either is the predicate ?of? or ?modifier?;
3. the predicates ?subject? and ?by? match (for passives);
4. the two predicates are in a small list of special cases that should match e.g., ?on?
and ?onto?.
These rules for matching syntactic roles are clearly an approximation to match-
ing semantic roles, but have performed better in our experiments than attempting to
explicitly assign (with error) semantic roles early on and then matching on those.
In addition, in language, ideas can be expressed using different parts of speech
(POS) for the same basic notion, e.g., verb or noun as in ?The bomb destroyed the
shrine? or ?The destruction of the shrine by the bomb? (Gurevich et al, 2006). To
handle these cross-POS variants, when finding the word senses of a word (above) our
system considers all POS, independent of its POS in the original text. Combined with
the above predicate-matching rules, this is a simple and powerful way of aligning
expressions using different POSs, e.g.:
? ?The bomb destroyed the shrine? and ?The destruction of the shrine by the
bomb? (but not ?The destruction of the bomb by the shrine?) are recognized as
equivalent.
? ?A person attacks with a bomb? and ?There is a bomb attack by a person? are
recognized as equivalent.
? ?There is a wrecked car?, ?The car was wrecked?, and ?The car is a wreck?
(adjective, verb, and noun forms) are recognized as equivalent.
Although clearly these heuristics can go wrong, they provide a basic mechanism
for assessing simple equivalence and subsumption between texts.
1Clearly this can go wrong, e.g., if the contexts of T and H are different so repeated/matching words
have incompatible intended senses, although such discontinuities are unusual in natural text.
Augmenting WordNet for Deep Understanding of Text 49
2.3 Experimental Test Bed
As an experimental test bed we have developed a publically available RTE-style test
suite2 of 250 pairs (125 entailed, 125 not entailed). As our goal is deeper semantic
processing, the texts are syntactically simpler than the PASCAL RTE sets (at www.
pascal-network.org) but semantically challenging to process. We use examples
from this test suite (and others) in this paper.
3 Exploiting Lexical & World Knowledge
3.1 Use of WordNet?s Glosses
Translation to Logic WordNet?s word sense definitions (glosses) appear to contain
substantial amounts of world knowledge that could help with semantic interpretation
of text, and we have been exploring leveraging these by translating them into first-
order logic. We have also experimented with Extended WordNet (XWN), a similar
database constructed several years ago by Moldovan and Rus (2001).
To do the translation, a different language interpreter, developed by ISI, was used
(for historic reasons? BLUE was not available at the time the translations were done,
and has not been exercised or extended for definition processing). ISI?s system works
as follows: First each gloss is converted into a sentence of the form ?word is gloss?
and parsed using the Charniak parser. Then the parse tree is then converted into a log-
ical syntax by a system called LFToolkit, developed by Nishit Rathod. In LFToolkit,
lexical items are translated into logical fragments involving variables. Finally, as syn-
tactic relations are recognized, variables in the constituents are identified as equal. For
example, ?John works? is translated into John(x1) & work(e,x2)& present(e), where e
is a working event, and then a rule which recognizes ?John? as the subject of ?works?
sets x1 and x2 equal to each other. Rules of this sort were developed for a large
majority of English syntactic constructions. ISI?s system was then used to translate
the modified WordNet glosses into axioms. For example (rewritten from the original
eventuality notation):
;;; "ambition#n2: A strong drive for success"
ambition(x1) -> a(x1) & strong(x1) & drive(x1) & for(x1,x6) & success(x6)
Predicates are assigned word senses using the new-ly released WordNet sense-
tagged gloss corpus3. This process was applied to all ? 110,000 glosses, but with
particular focus on glosses for the 5,000 ?core? (most frequently used) synsets. It
resulted in good translations for 59.4% of the 5,000 core glosses, with lower quality
for the entire gloss corpus. Where there was a failure, it was generally the result of a
bad parse, with constructions for which no LFToolkit rules had been written. In these
cases, the constituents are translated into logic, so that no information is lost; what
is lost is the equalities between variables that provides the connections between the
constituents. For instance, in the ?John works? example, we would know that there
was someone named John and that somebody works, but we would not know that they
were the same person. Altogether 98.1% of the 5,000 core glosses were translated
into correct axioms (59.4%) or axioms that had all the propositional content but were
2http://www.cs.utexas.edu/~pclark/bpi-test-suite/
3http://wordnet.princeton.edu/glosstag
50 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
disconnected in this way (38.7%). The remaining 1.9% of these glosses had bizarrely
wrong parses due to noun-adjective ambiguities or to complex conjunction ambigui-
ties.
Using the Glosses We have used a combination of these logicalized glosses and those
from XWN to infer implicit information from text. Although the quality of the logic
is generally poor (for a variety of reasons, in particular that the glosses were never
intended for machine processing in the first place), our software was able to infer
conclusions that help answer a few entailment problems, for example:
T: Britain puts curbs on immigrant labor from Bulgaria and Romania.
H: Britain restricted workers from Bulgaria.
using the logic for the definition:
restrict#v1: "restrict", "restrain": place limits on.
plus WordNet?s knowledge that: ?put? and ?place? are synonyms; ?curb? and ?limit?
are synonyms; and a laborer is a worker. In our experiments, the glosses were used to
answer 5 of the 250 entailment questions (4 correctly). More commonly, the glosses
came ?tantalizingly close? to providing the needed knowledge. For example, for:
T: A Union Pacific freight train hit five people.
H: A locomotive was pulling the train.
it seems that the definition:
train#n1: "train", "railroad train": public transport provided
by a line of railway cars coupled together and drawn by a locomotive.
is very close to providing the needed knowledge. However, unfortunately it defines
a train as ?public transport provided by cars pulled by a locomotive? rather than just
?cars pulled by a locomotive? (the locomotive pulls the cars, not the train/public-
transport), hence the hypothesis H is not concluded. Similarly:
T: The Philharmonic orchestra draws large crowds.
H: Large crowds were drawn to listen to the orchestra.
essentially requires knowledge that crowds (typically) listen to orchestras. WordNet?s
glosses come very close to providing this, with knowledge that:
orchestra = collection of musicians
musician = someone who plays musical instrument
music = sound produced by musical instruments
listen = hear = perceive sound
However, the connection that the playing results in sound production is missing,
and hence again H cannot be inferred. These experiences with the WordNet glosses
were very common. In summary, our experience is the WordNet glosses provided
some value, being used 5 times (4 correctly) on the 250 examples in our test suite,
Augmenting WordNet for Deep Understanding of Text 51
with the short, simple definitions (e.g., bleed = lose blood) being the most reliable.
The low quality of the logic was a problem (definitional text is notoriously difficult to
interpret automatically (Ide and Veronis, 1993)), although often the knowledge came
close. Finally, 110,000 rules (approx. one per gloss) is actually quite a small number;
typically only 10?s of rules fired per sentence, rarely containing the implications we
were looking for.
3.2 Typed Morphosemantic Links
WordNet contains approximately 21,000 links connecting derivationally related verb
and noun senses, e.g., employ#v2-employee#n1; employ#v2-employment#n3. These
links turn out to be essential for mapping between verbal and nominalized expressions
(e.g. using ?destroy?-?destruction?, as mentioned earlier). However, the current links
do not state the semantic type of the relation (e.g., that employee#n1 is the UNDER-
GOER of an employ#v2 event; employment#n3 is the employ#v2 EVENT itself),
which limits WordNet?s ability to help perform semantic role labeling. In addition,
not being able to distinguish the semantics of the relationships can cause errors in
reasoning, for example distinguishing between H1 and H2 in:
T: Detroit produces fast cars.
H1: Detroit?s product is fast.
H2?: Detroit?s production is fast. [NOT entailed]
T: The Zoopraxiscope was invented by Mulbridge.
H1: Mulbridge was the inventor of the Zoopraxiscope.
H2?: Mulbridge was the invention of the Zoopraxiscope. [NOT entailed]
To type these links, we have used a semi-automatic process: First, the computer
makes a ?guess? at the appropriate semantic relation based on the morphological re-
lationship between the noun and the verb (e.g., ?A?IJ-er?A?I? nouns usually refer to the
agent), and the location of the two synsets in WordNet?s taxonomy. Second, a human
validates and corrects these, a considerably faster progress than entering them from
scratch. 9 primary semantic relations (as well as 5 rarer ones) were used, namely:
agent (e.g., employ#v2-employer#n1)
undergoer/patient (e.g., employ#v2-employee#n1)
instrument (e.g., shred#v1-shredder#n1)
recipient (e.g., grant#v2-grantee#n1)
result (e.g., produce#v2-product#n2)
body-part (e.g., adduct#v1-adductor#n1)
vehicle (e.g., cruise#v4-cruiser#n3)
location (e.g., bank#v3-bank#n4)
identity/equality (eg employ#v2-employment#n1)
The resulting database of 21,000 typed links was recently completed, constituting
a major new addition to WordNet in support of deep language processing. One of the
surprising side results of this effort was discovering how often the normal morpholog-
ical defaults (e.g., ?-er? nouns refer to agents) are violated, described in more detail in
Fellbaum et al (2007). We are now in the process of incorporating the database into
our software.
52 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
3.3 Core Theories
While WordNet?s glosses and links contain world knowledge about specific entities
and relations, there is also more fundamental knowledge about language and the world
? e.g., about space, time, and causality ? which is essential for understanding many
types of text, yet is unlikely to be expressed in dictionary definitions or automatically
learnable. To address this need, we are also encoding by hand a number of theories to
support deeper reasoning (in the style of lexical decomposition). We have axiomatized
a number of abstract core theories that underlie the way we talk about events and event
structure (Hobbs, 2008). Among these are theories of composite entities (things made
of other things), scalar notions (of which space, time, and number are specializations),
change of state, and causality. For example, in the theory of change of state, the
predication change(e1,e2) says there is a change of state from state e1 to state e2. The
predication changeFrom(e1) says there is a change out of state e1. The predication
changeTo(e2) says there is a change into state e2. An inference from changeFrom(e1)
is that e1 no longer holds. An inference from changeTo(e2) is that e2 now does hold.
In the theory of causality (Hobbs, 2005), the predication cause(e1,e2), for e1 causes
e2, is explicated. One associated inference is that if the causing happens, then the
effect e2 happens. A defeasible inference is that not-cause-not often is the same as
cause:
not(cause(x,not(e)))? cause(x,e)
In the rightward direction this is of course sometimes wrong, but if we go to the
trouble of saying that the negation of something was not caused, then very often it is
a legitimate conclusion that the causing did happen.
We are connecting these theories with WordNet by mapping the core (5,000 most
common) WordNet synsets to the theory predicates. For example, the core part of
WordNet contains 450 word senses having to do with events and event structure, and
we are in the process of encoding their meanings in terms of core theory predicates.
For example, if x lets e happen (WordNet sense let#v1), then x does not cause e not to
happen:
let#v1(x,e)? not(cause(x,not(e)))
One sense of ?go? is ?changeTo?, as in ?I go crazy?
go#v4(x,e)? changeTo(e)
(The entity x is the subject of the eventuality e.) If x frees y (the verb sense of ?free?),
then x causes a change to y being free (in the adjective sense of ?free?):
free#v1(x,y)? cause(x,changeTo(free#a1(y)))
Given these mappings and the core theories themselves, this is enough to answer
the entailment pair:
T: The captors freed the hostage.
H: The captors let the hostage go free.
Augmenting WordNet for Deep Understanding of Text 53
via successive application of the above axioms:
(part of) H interpretation? let(x,go(y,free(y)))
? not(cause(x,not(changeTo(free#a1(y)))))
? cause(x,changeTo(free#a1(y)))
? free#v1(x,y)
We are still in the early stages of developing this resource and have not yet evaluated
it, but we have already seen a number of examples of its potential utility in the text
inference problem such as above.
3.4 Scripts
Simple inference rules, such as in the above resources, provide a direct means of
drawing conclusions from a few words in the input text. However, they are largely
context-independent, i.e., not sensitive to the bigger picture which the surrounding
text provides. Consider the following example:
T: A dawn bomb attack devastated a major shrine.
H: The bomb exploded.
In this case, it is hard to express the required knowledge (to conclude H follows
from T) as simple rules (e.g., the rules ?bomb ?E?? bomb explode? or ?bomb attack
? bomb explode? are not adequate, as we do not want H to follow from ?The police
destroyed the bomb.? or ?The bomb attack was thwarted?). Rather, when a person
reads T, he/she recognizes a complete scenario from multiple bits of evidence (possi-
bly in multiple sentences), and integrates what is read with that scenario. This kind of
top-down, expectation-driven process seems essential for creating an overall, coherent
representation of text.
Although scripts are an old idea (e.g., (Schank and Abelson, 1977)) there are rea-
sons their use may be more feasible today. First, rapid advances in paraphrasing sug-
gests that the matching problem ? deciding if some text is expressing part of of a
script ? may be substantially eased. (Script work in the ?70s required stories to
be worded in exactly the right way to fire a script). Second, two new approches for
amassing knowledge are available today that were not available previously, namely au-
tomated learning from corpora, and use of Web volunteers (e.g., (Chklovski, 2005)),
and may be applicable to script acquisition (Script work in the ?70s typically worked
with tiny databases of scripts). Finally, techniques for language processing have sub-
stantially improved, making core tasks (e.g., parsing) less problematic, and opening
the possibility to easy authoring of scripts in English, followed by machine interpre-
tation. FrameNet (Baker et al, 1998) already provides a few small scripts, but does
not currently encode the complex scenarios that we would like; a vastly expanded
resource would be highly useful.
We are in the early stages of exploring this avenue, encoding scripts as a list of
simple English sentences, which are then automatically translated to WordNet-sense
tagged logic using our software. For example, a ?bombing? script looks:
A building is bombed by an attacker.
The attacker plants the bomb in the building.
54 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
The bomb explodes.
The explosion damages or destroys the building.
The explosion injures or kills people in the building.
In addition, some of these sentences are flagged as ?salient?. If any salient sentence
matches (subsumes) part of the text, then the script is triggered. When triggered,
a standard graph-matching algorithm searches for the maximal overlap between the
clauses in the (interpreted) script and the clauses in the text, and then the script is
unified with the text according to that maximal overlap, thus asserting the additional
facts contained in the script to the text under consideration. In the example earlier,
the script is triggered by, and matched with, the text, thus aligning ?building? with
?shrine?, and asserting additional facts including (the logic representation of) ?The
bomb explodes? and ?The bomb was planted in the building.?.
3.5 Using DIRT Paraphrases
Like others, we have also explored the use of the DIRT paraphrase database for rea-
soning, and we report our experiences here for comparison. The database contains
12 million rules, discovered automatically from text, of form (X relation1 Y) ? (X
relation2 Y), where relation is a path in the dependency tree/parse between constitu-
tents X and Y. Although they are noisy (informally, about 50% seem reliable), they
provided some leverage for us also, for example correctly answering:
T: William Doyle works for an auction house in Manhattan.
H?: William Doyle never goes to Manhattan. [NOT entailed]
using the DIRT rule ?IF Y works in X THEN Y goes to X? combined with negation,
and
T: The president visited Iraq in September.
H: The president traveled to Iraq.
using the (slightly strange but plausible) DIRT rule ?IF Y is visited by X THEN X
flocks to Y? and that ?A?IJflock?A?I? is a type (hyponym) of ?A?IJtravel?A?I?. In our
experiments, DIRT rules were used 47 times (27 correctly) on our 250 example test
suite. The main cause of incorrect answers was questionable/incorrect rules in the
database, e.g.:
T: The US troops stayed in Iraq.
H?: The US troops left Iraq. [NOT entailed]
was found to be entailed using the DIRT rule ?IF Y stays in X THEN Y leaves X?.
In addition, DIRT does not distinguish word senses (e.g., according to DIRT, shooting
a person/basket implies killing the person/basket and scoring a person/basket), also
contributing errors.
Despite this, the DIRT rules were useful because they go beyond just the defini-
tional knowledge in WordNet. For example, according to DIRT ?X marries Y? im-
plies, among other things: Y marries X; X lives with Y; X kisses Y; X has a child with
Y; X loves Y ? all examples of plausible world knowledge. The main limitations we
Augmenting WordNet for Deep Understanding of Text 55
found were they were noisy, did not account for word senses, and only cover one rule
pattern (X r1 Y? X r2 Y). So, for example, a rule like ?X buys Y? X pays Money?
is outside the expressive scope of DIRT.
4 Preliminary Evaluation
Although this is work in progress, we have evaluated some of these augmentations
using our test suite. As our ultimate goal is deeper understanding of text, we have de-
liberately eschewed using statistical similarity measures between T and H, and instead
used abductive reasoning to create an axiom-elaborated representation of T, and then
seen if it is subsumed by H. Although not using statistical similarity clearly hurts our
score, in particular assuming ?no entailment? when the elaborated representation of T
is not subsumed by H, we believe this keeps us appropriately focused on our longer-
term goal of deeper understanding of text. The results on our 250 pairs currently are:
H or ?H predicted by: Corrrect Incorrect
Simple syntax manipulation 11 3
WordNet taxonomy + morphosemantics 14 1
WordNet logicalized glosses 4 1
DIRT paraphrase rules 27 20
H or ?H not predicted: Corrrect Incorrect
(assumed not entailed) 97 72
Thus our overall score on this test suite is 61.2%. We have also run our software on
the PASCALRTE3 dataset (Giampiccolo et al, 2007), scoring 55.7% (excluding cases
where no initial logical representation could be constructed due to parse/LF generation
failures). In some cases, other known limitations of WordNet (eg. hypernym errors,
fine-grained senses) also caused errors in our tests (outside the scope of this paper).
However, the most significant problem, at least for these tests, was lack of world
knowledge.
5 Conclusion
A big challenge for deep understanding of text ? constructing a coherent represen-
tation of the scene it is intended to convey ? is the need for large amounts of world
knowledge. We have described our work-in-progress to augment WordNet in vari-
ous ways so it can better provide some of this knowledge, and described some initial
experiences with those augmentations, as well as with the DIRT database. Existing
WordNet aleady provides extensive leverage for language processing, as evidenced
by the large number of groups using it. The contribution of this paper is some pre-
liminary insight into avenues and challenges for further developing this resource. Al-
though somewhat anecdotal at this stage, our experience suggests the augmentations
have promise for further improving deep language processing, and we hope will result
in a significantly improved resource.
Acknowledgements This work was performed under the DTO AQUAINT program,
contract N61339-06-C-0160.
56 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
References
Adams, R., G. Nicolae, C. Nicolae, and S. Harabagiu (2007). Textual entailment
through extended lexical overlap and lexico-semantic matching. In Proc. ACL-
PASCAL Workshop on Textual and Entailment and Paraphrasing, pp. 119?124.
Baker, C. F., C. J. Fillmore, and J. B. Lowe (1998). The Berkeley FrameNet project.
In C. Boitet and P. Whitelock (Eds.), Proc 36th ACL Conf., CA, pp. 86?90. Kauf-
mann.
Chambers, N., D. Cer, T. Grenager, D. Hall, C. K. MacCartney, M.-C. de Marneffe,
D. R. Yeh, and C. D. Manning (2007). Learning alignments and leveraging natural
logic. In Proc. ACL-PASCAL Workshop on Textual and Entailment and Paraphras-
ing, pp. 165?170.
Chklovski, T. (2005). Collecting paraphrase corpora from volunteer contributors. In
Proc 3rd Int Conf on Knowledge Capture (KCap?05), NY, pp. 115?120. ACM.
Clark, P. and P. Harrison (2008, September). Boeing?s NLP System and the Chal-
lenges of Semantic Representation. In J. Bos and R. Delmonte (Eds.), Semantics
in Text Processing. STEP 2008 Conference Proceedings, Venice, Italy.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. Cambridge, MA:
MIT Press.
Fellbaum, C., A. Osherson, and P. Clark (2007). Putting semantics into wordnet?s
morphosemantic links. In Proc. 3rd Language and Technology Conference, Poz-
nan, Poland.
Giampiccolo, D., B. Magnini, I. Dagan, and B. Dolan (2007). Textual entailment
through extended lexical overlap and lexico-semantic matching. In Proc. ACL-
PASCAL Workshop on Textual and Entailment and Paraphrasing, pp. 1?9.
Graesser, A. C. (1981). Prose Comprehension Beyond the Word. NY: Springer.
Gurevich, O., R. Crouch, T. King, and V. de Paiva (2006). Deverbal nouns in knowl-
edge representation. In Proc. FLAIRS?06.
Harrison, P. and M. Maxwell (1986). A new implementation of GPSG. In Proc. 6th
Canadian Conf on AI (CSCSI-86), pp. 78?83.
Hobbs, J. (2005). Toward a useful notion of causality for lexical semantics. Journal
of Semantics 22, 181?209.
Hobbs, J. (2008). Encoding commonsense knowledge. Technical report, USC/ISI.
http://www.isi.edu/?hobbs/csk.html.
Ide, N. and J. Veronis (1993). Extracting knowledge-bases from machine-readable
dictionaries: Have we wasted our time? In Proc KB&KB?93 Workshop, pp. 257?
266.
Augmenting WordNet for Deep Understanding of Text 57
Lenat, D. and R. Guha (1989). Building Large Knowledge-Based Systems. MA:
Addison-Wesley.
Miller, G. (1995). WordNet: a lexical database for english. Comm. of the
ACM 38(11), 39?41.
Moldovan, D. and V. Rus (2001). Explaining answers with extended wordnet. In
Proc. ACL?01.
Schank, R. and R. Abelson (1977). Scripts, Plans, Goals and Understanding. Hills-
dale, NJ: Erlbaum.
Schubert, L. and C. Hwang (1993). Episodic logic: A situational logic for NLP. In
Situation Theory and Its Applications, pp. 303?337.
Refining the Meaning of Sense
Labels in PDTB: ?Concession?
Livio Robaldo
University of Turin (Italy)
email: robaldo@di.unito.it
Eleni Miltsakaki
University of Pennsylvania (USA)
email: elenimi@linc.cis.upenn.edu
Jerry R. Hobbs
University of Southern California (USA)
email: hobbs@isi.edu
Abstract
The most recent release of PDTB 2.0 contains annotations of senses of
connectives. The PDTB 2.0 manual describes the hierarchical set of
senses used in the annotation and offers rough semantic descriptions of
each label. In this paper, we refine the semantics of concession sub-
stantially and offer a formal description of concessive relations and the
associated inferences drawn by the reader, utilizing basic notions from
Hobbs?s logic, including the distinction between causes and causal com-
plexes (Hobbs, 2005). This work is part of a larger project on the se-
mantics of connectives which aims at developing formal descriptions of
discourse relations, useful for processing real data.
207
208 Robaldo, Miltsakaki, and Hobbs
1 Introduction
As the demand for more powerful NLP applications increases, there is also an in-
creasing need to develop algorithms for automated processing of discourse relations
and models for deriving the inferences drawn by the reader. PDTB 2.0 (Prasad et al,
2008), released in January 2008, contains annotations of discourse connectives and
their arguments, attribution, and sense labels giving rough semantic descriptions of
the connectives. The availability of such a richly annotated corpus promises to boost
our understanding of the structure and meaning of discourse and will facilitate the
development of efficient algorithms for identifying discourse connectives and their
arguments.
However, in order to be able to derive appropriate inferences associated with dis-
course relations, we need to develop useful semantic analyses of the meaning of con-
nectives so that they will generate the same range of inferences made by humans. In
this paper we take a first step in that direction, offering a simple formal analysis of
concessive relations, thus refining the semantics of the concessive sense labels used
in PDTB 2.0. Our analysis uses basic notions of causality developed in Hobbs (1998,
2005), capitalizing on the distinction between causes and causal complexes and on
the semantics of defeasible causality. Concessive meaning involves the failure of a
general defeasible causal relation in this specific instance.
The paper is organized as follows. Section 2 gives an overview of the PDTB 2.0,
focusing on the annotation of the senses of connectives, especially ?concession?. In
Section 3, we present an overview of the framework we are adopting for our formal
analysis, namely, Hobbs?s logic of causality, and our basic claims about how the se-
mantics of defeasible causality contributes to the semantics of concession. Section 4
presents the semantic analysis of ?concession?. In Section 5, we report briefly on the
distribution of concessive labels in PDTB 2.0 and conclude in Section 6.
2 Sense labels in PDTB
The Penn Discourse Treebank provides annotations of the argument structure of dis-
course connectives, attribution (e.g., ?ownership? of the relation by the writer or other
individual), and semantic labels for all the annotated connectives (Prasad et al, 2008).
This annotation of discourse connectives and their arguments draws on a lexical ap-
proach to discourse structure (Webber et al, 2003; Webber and Joshi, 2003), viewing
discourse connectives as discourse-level predicates that take two abstract objects such
as events, states, and propositions (Asher, 1993) as their arguments.
Two major types of discourse connectives are annotated in PDTB: a) explicit con-
nectives including subordinate conjunctions, coordinate conjunctions and adverbials,
and b) implicit connectives that are inserted between two adjacent sentences to cap-
ture the meaning of the inferred relation when no explicit connective is present. The
PDTB 2.0 is, to date, the largest annotation effort at the discourse level, including ap-
proximately 40,000 triples in the form (Connective, Arg1, Arg2). Arg2 is the second
argument in the text in the case of coordinating conjunctions, and is the complement
of subordinating conjunctions. In the case of adverbs, Arg2 is the element which the
adverb modifies syntactically. In cases of ambiguity, sense labels indicate the intended
sense in the given context. In all other cases, sense labels provide semantic descrip-
Refining the Meaning of Sense Labels in PDTB: ?Concession? 209
tions of the relations conveyed by the connectives, both explicit and implicit.
The tagset of senses is organized hierarchically (Miltsakaki et al, 2008). The top
level, or class level, has four tags representing four major semantic classes: ?TEMPO-
RAL?, ?CONTINGENCY?, ?COMPARISON? and ?EXPANSION?. For each class,
a second level of types is defined to further refine the semantics of the class levels.
For example, ?CONTINGENCY? has two types ?Cause? (relating two situations via
a direct cause-effect relation) and ?Condition? (relating a hypothetical scenario with
its (possible) consequences). A third level of subtype specifies the semantic contribu-
tion of each argument. For ?CONTINGENCY?, its ?Cause? type has two subtypes ?
?reason? (which applies when the connective indicates that the situation specified in
Arg2 is interpreted as the cause of the situation specified in Arg1, as often with the
connective because) and ?result? (which is used when the connective indicates that
the situation described in Arg2 is interpreted as the result of the situation presented in
Arg1). That is, ?reason? occurs when Arg2 causes Arg1; ?result? occurs when Arg1
causes Arg2.
Connectives can also be used to relate arguments pragmatically as in John is in
the house because the lights are on or If you?re thirsty, there?s beer in the fridge,
where the relation involbes the belief in or the telling of the condition rather than the
condition itself. For these rhetorical or pragmatic uses of connectives, a small set of
pragmatic sense tags has been defined ? specifically, ?Pragmatic Cause?, ?Pragmatic
Condition?, ?Pragmatic Contrast? and ?Pragmatic Concession?.
2.1 ?Concession? in PDTB
?Concession? is a type of the class-level category ?COMPARISON?. The class tag
?COMPARISON? applies when the connective indicates that a discourse relation is
established between Arg1 and Arg2 in order to highlight prominent differences be-
tween the two situations. Semantically, the truth of both arguments is independent of
the connective or the established relation. ?COMPARISON? has two types that further
specify its semantics. In some cases, Arg1 and Arg2 share a predicate or a property
and the difference is highlighted with respect to the values assigned to this property.
This interpretation is tagged with the type ?Contrast?.
There are also cases in which the highlighted differences are related to expectations
raised by one argument which are then denied by the other. This intepretation is
tagged with the type ?Concession?. According to the description in the PDTB 2.0
manual, the type ?Concession? applies when the connective indicates that one of the
arguments describes a situation A which normally causesC, while the other asserts (or
implies) ?C. Alternatively, one argument denotes a fact that triggers a set of potential
consequences, while the other denies one or more of them.
Two ?Concession? subtypes are defined in terms of the argument creating an ex-
pectation and the one denying it. Specifically, when Arg2 creates an expectation that
Arg1 denies, it is tagged as ?expectation?, shown in (1.c-d). When Arg1 creates an
expectation that Arg2 denies, it is tagged as ?contra-expectation?, shown in (1.e-f).
Examples (1.a-b) are made-up sentences we use for explanation and will be discussed
here and in the next section. All other examples are taken from PDTB 2.0. Each dis-
course fragment in (1) distinguishes between a discourse connective (underlined), and
two sentence-arguments: Arg1 (italics) and Arg2 (boldface).
210 Robaldo, Miltsakaki, and Hobbs
(1) a. Although John studied hard, he did not pass the exam. (expectation)
b. Although running is considered healthy, it is not advisable for persons
with heart problems. (expectation)
c. Although they represent only 2% of the population, they control nearly
one-third of discretionary income. (expectation)
d. While acquiring a big brand-name company can be a shortcut to
growth, it can also bring a host of unforeseen problems (expectation)
e. The Texas oilman has acquired a 26.2% stake valued at more than $1.2
billion in an automotive-lighting company, Koito Manufacturing Co.
But he has failed to gain any influence at the company. (contra-
expectation)
f. Mr. Cannell?s allegations of cheating ?are purely without foundation?,
and based on unfair inferences. However the state will begin keeping
closer track of achievement-test preparation booklets next spring..
(contra-expectation)
(1.a) is an example of ?expectation?: Arg2 (John studied hard) creates the expecta-
tion that John passed the exam, which is precisely denied by Arg1. The same holds
for (1.b-d). Note that (1.b), unlike (1.a, c-d), expresses a general concessive relation,
i.e., it does not refer to particular contingent events. (1.e-f) are instances of contra-
expectation, where the expectation is created by Arg1. In (1.e), the fact that the Texas
oilman acquired the indicated stake value creates the expectation that he gained influ-
ence at the company, while, in (1.f), since Mr. Cannell?s allegations of cheating are
purely without foundation (in the speaker?s judgement), we do not expect the state to
start tracking the test preparation.
3 Toward a formal definition of ?Concession?
Based on our analysis of the range of PDTB tokens tagged with a concessive label, we
offer here a more detailed semantic analysis of the meaning of concessive relations.
Since the direction of the concessive relation is not relevant, the argument that creates
the expectation and the argument that denies it are respectively termed as Argcexp and
Argdexp. We claim that a concessive relation arises from a contrast between the effects
of two causal relations cc and cd holding in the domain. c and d stand for ?creates? and
?denies?, respectively. The relation denoted by cc is the causal relation that creates the
expectation, and cd the one that denies it. The effects of these causal relations, as well
as their causes, are taken to be eventualities1.
In this paper, we use the letter e for most eventualities, possibly with some subscript
or superscript.2 We make use of the subscripts x1 and x2, respectively, to distinguish
between the causes and the effects in a causal relation cx. Therefore, the causes in
cc and cd are indicated by ec1 and ed1 respectively, and the effects by ec2 and ed2,
respectively. ec2 is the ?created expectation?; its cause ec1 is conveyed by Argcexp. ed2
is an eventuality that denies ec2, and it is explicitly described in Argdexp. The cause of
1The term ?eventuality? is borrowed from (Bach, 1981). It covers both standard notions of ?state? and
?event?.
2As we will see, also causal relations are eventualities; so the names cc and cd are an exception to this
rule.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 211
ed2, i.e., ed1, is usually unknown. Also ec2 is, in principle, unknown, but in most cases
it can be taken as the negation of ed2.
For instance, in the context of (1.a), the eventuality John studied hard (ec1) creates
the expectation John passed the exam (ec2). Nevertheless, Argdexp says that John did
not pass the exam actually (ed2). The reason of ed2 is unknown and has to be found in
the context. In other words, the context, whether explicit or inferred, should include
another eventuality that caused John?s failure, despite his studying hard. For example,
the next sentence might be John was very tired during the exam (ed1).
In order to formalize this account of concession, we need a defeasible notion of
causality. Many authors propose such an account of causality, e.g. (Achinstein, 1965;
Shoham, 1990; Simon, 1991; Bell, 1999, 2003), and Giunchiglia et al (2004). The ac-
count we use is that of Hobbs (2005). This distinguishes between the monotonic, pre-
cise notion of ?causal complex? and the nonmonotonic, defeasible notion of ?cause?.
The former gives us mathematical rigor; the latter is more useful for everyday rea-
soning and can be characterized in terms of the former. As Hobbs (2005) explains,
when we flip a switch to turn on a light, we say that flipping the switch ?caused? the
light to turn on. But for this to happen, many other factors had to be in place. The
bulb had to be intact, the switch had to be connected to the bulb, the power had to
be on in the city, and so on. The set of all the states and events that have to hold or
happen for an effect e to happen are called the ?causal complex? of e. Thus, the flip-
ping of the switch and the normal states of the bulb, the wiring, and the power supply
would all be in the causal complex for the turning on of the light. In a causal complex,
the majority of participating eventualities are normally true and therefore presumed
to hold. In the light bulb case, unless otherwise indicated, it is normally true that the
bulb is not burnt out, that the wiring is intact, that the power is on in the city, and so
on. But the light switch could be on or off; neither can be presumed. Those eventu-
alities that cannot normally be assumed to be true are identified as causes (cf. Kayser
and Nouioua, 2008). They are useful in planning, because they are often the actions
that the planner or some other agent must perform. They are useful in explanation
and prediction because they frequently constitute the new information. They are less
useful in diagnosis, where the whole causal complex has to be considered.
Note that in practice, we can never specify all the eventualities in a causal complex
for an event. So while the notion of causal complex gives us a precise way of thinking
about causality, it is not adequate for the kind of practical reasoning we do in planning,
explaining, and predicting. For this, we need the defeasible notion of ?cause?.
3.1 Background on Hobbs?s logic
Hobbs (1998) proposed a wide coverage logical framework for natural language based
on the notion of reification. Reification is the action of making states and events first-
class individuals in the logic, so they can be referred to by constants and variables.
We ?reify? eventualities, from the Latin word ?re(s)? for ?thing?: we take them to
be things. The framework distinguishes two parallel sets of predicates: primed and
unprimed. The unprimed predicates are the ordinary predicates we are used to in
logical representations of language. For example, (give a b c) says that a gives b to c.
When we assert this, we are saying that it actually takes place in the real world. The
primed predicate is used to talk about the reified eventualities. The expression (give?
212 Robaldo, Miltsakaki, and Hobbs
e a b c) says that e is a giving event by a of b to c. Eventualities may be possible
or actual. When they are actual, this is simply one of their properties. To say that a
state e actually obtains in the real world or that an event e actually occurs in the real
world, we write (Rexist e). That is, e really exists in the real world. If I want to fly, my
wanting really exists, but my flying does not. This is represented as:3
(Rexist e) ? (want? e I e1) ? (fly? e1 I)
Therefore, contrary to (p x), (p? e x) does not say that e actually occurs, only that if
it did, it would be a ?p? event. The relation between primed and unprimed predicates
is then formalized by the following axiom schema:
(forall (x) (iff (p x) (exists(e) (and(p? e x)(Rexist e)))))
Eventualities can be treated as the objects of human thoughts. Reified eventualities
are inserted as parameters of such predicates as believe, think, want, etc. These predi-
cates can be applied in a recursive fashion. The fact that John believes that Jack wants
to eat an ice cream is represented as an eventuality e such that4
(believe? e John e1) ? (want? e1 Jack e2) ?
(eat? e2 Jack Ic) ? (iceCream? e3 Ic)
In Hobbs?s notation, every relation on eventualities, including logical operators, causal
and temporal relations, and even tense and aspect, may be reified into another eventu-
ality. For instance, by asserting (imply? e e1 e2), we reify the implication from e1 to e2
into an eventuality e. e has to be thought as ?the state holding between e1 and e2 such
that whenever e1 really exists, e2 really exists too?. Negation is represented as (not? e1
e2): e1 is the eventuality of the e2?s not existing. Some problems arise with negation,
in that what is generally negated is an eventuality type rather than an eventuality token
or instance. In order to deal with more general cases of concession, we will refer to
eventualities that are inconsistentwith other ones. Two eventualities e1 and e2 are said
to be inconsistent iff they (respectively) imply two other eventualities e3 and e4 such
that e3 is the negation of e4. The definition is as follows:
(forall (e1 e3)
(iff (inconsistent e1 e2)
(and (eventuality e1) (eventuality e2)
(exists (e3 e4) (and (imply e1 e3)
(imply e2 e4)(not? e3 e4))))))
3.2 Typical elements, eventuality types and tokens
Among the things we can think about are both specific eventualities, like Fido is bark-
ing, and general or abstract types of eventualities, like Dogs bark. We do not want to
treat these as radically different kinds of entities. We would like both, at some level, to
3In order to increase readability, we will often make use of the symbol ? in place of the unprimed
predicate and.
4The formula expresses the de-re reading of the sentence, where e1, e2 , e3 , John, Jack, Ic are first order
constants.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 213
be treated simply as eventualities that can be the content of thoughts. To this end, the
logical framework includes the notion of typical element (from Hobbs (1983, 1995,
1998)). The typical element of a set is the reification of the universally quantified
variable ranging over the elements of the set (cf. McCarthy (1977)). Typical elements
are first-order individuals. The introduction of typical elements arises from the need
to move from the standard set-theoretic notation
s = {x | p(x) }
or its logical equivalent,
(forall (x) (iff (member x s) (p x)))
to a simple statement that p is true of a ?typical element? of s by reifying typical
elements. The principal property of typical elements is that all properties of typical
elements are inherited by the real members of the set.
It is important not to confuse the concept of typical element with the standard con-
cept of ?prototype?, which allows defeasibility, i.e., properties that are not inherited
by all of the real members of the set. Asserting a predicate on a typical element of a
set is logically equivalent to the multiple assertions of that predicate on all elements
of the set. Talking about typical elements of sets of eventualities leads to the distinc-
tion between eventuality types and eventuality tokens. The logic defines the following
concepts, for which we omit formal details5: a) Eventualities types (aka abstract even-
tualities): eventualities that involve at least one typical element among their arguments
or arguments of their arguments (we can call these ?parameters?), b) Partially instan-
tiated eventuality types (aka partial instances): a particular kind of eventuality type
resulting from instantiating some of the parameters of the abstract eventuality either
with real members of their sets or with typical elements of subsets, and c) Eventuality
tokens (aka instances: a particular kind of partially instantiated eventuality type with
no parameters. It is a consequence of universal instantiation that any property that
holds of an eventuality type is true of any partial instance of it.
Hobbs?s logical framework is particularly suitable to the study of the semantics of
discourse connectives, in that it allows focusing on their meaning while leaving under-
specified the details about the eventualities involved. In other words, we can simply
assume the existence of two eventualities e1 and e2 coming from the two arguments
Arg1 and Arg2 respectively. e1 and e2 may be either eventuality tokens, on atomic
arguments, as in (1.a), or eventuality tokens, on collective arguments, as in (1.c), or
(partially instantiated) eventuality types, as in (1.b), or any other kind of eventuality.
The semantics of concession proposed below uniformily applies to all these cases.
3.3 Hobbs?s Account of Causality
The account of causality described above in the introduction is represented in terms
of two predicates: (cause? cx ex1 ex2) and (causalComplex s ex2). cause? says that cx is
the state holding between ex1 and ex2 such that the former is a non-presumable cause
5Actually, ?instance? is slightly more general, since if s is a set, x is its typical element, and y is a
member of s, y is an instance of x, even though it is not an eventuality. Nevertheless, in this paper we
assume ?instances? and ?eventuality tokens? to be synonymous.
214 Robaldo, Miltsakaki, and Hobbs
of the latter. causalComplex says that s is the set of all presumable or non-presumable
eventualities that are involved in causing ex2. Obviously, ex1 belongs to s. Thus, in the
light example, the predicate cause applies to the flipping of the switch, while the states
of the bulb, the wiring, and the power supply would all be in the causal complex s.
Several axioms characterize the predicates cause and causalComplex. Some of them
relate causality with time6, some relate causality with probability, and so on Hobbs
(2005).
It is clear that the theory must not include an axiom stating that, whenever a causal
relation cx and its cause ex1 really exist, the corresponding effect ex2 really exists too.
The inclusion of such an axiom would lead to a non-defeasible causality. Rather, we
need an axiom stating that an effect really exists just in case all the eventualities in its
causal complex really exist:
(forall (s e)
(if (and (causalComplex s e)
(forall (e1) (if (member e1 s) (Rexist e1)))
(Rexist e)))
Nevertheless, as pointed out above, we can never specify all the eventualities in a
causal complex. Even in simple sentences like (1.a), the eventualities in the causal
complex are not easy to list, and the real causes may not coincide with what we think
the causes are in that context. For example, recalling our analysis of (1.a) above:
ec1=?John studied hard?
ec2=?John passed the exam?
ed1=?John was tired during the exam?
ed2=?John did not pass the exam?
cc=?ec1 causes ec2?; cd=?ed1 causes ed2?
One approach at this point would be to say that both ec1 and the negation of ed1
belong to the causal complex of ec2, with ec1 being the non-presumable cause of ec2.
But this would mean that not being tired during exams is a kind of ?precondition? for
passing exams by studying hard, which is obviously false in many contexts. Note,
however, that there is an arbitrary quality to what we designate as being in a causal
complex, because causality forms chains and we can start the chain at any point. John
was tired caused the situation that he did not manage to concentrate, which caused
the situation that he made a lot of errors in the exam, which caused the situation that
the teacher decided to fail him. One could argue that the last of these eventualities is
the real cause of ed2. Similarly, one could argue that ec1 is not the real cause of ec2:
John studied hard causes the situation that he makes few errors in the exam . . . and the
teacher decides not to fail him. The predicate cause is defeasibly transitive, however,
so these considerations do not affect our account of concession. Furthermore, we do
not take the negation of ed1 as necessarily belonging to the causal complex for ec2.
Rather, we claim that ed1, besides being the cause of ed2, is the cause of another
eventuality edp that is inconsistent with an element ecp in the causal complex for ec2.
6As argued also by Giordano and Schwind (2004), the effect caused by an eventuality can take place in
the current or in a subsequent instant.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 215
In (1.a), ecp may be simply John does not have any particular health problem that
jeopardizes his passing the exam. ed1 caused both John?s failure and an health status
that jeopardizes the passing of his exam. This is what we mean here by ?denying of
an expectation?.
In our analysis of concession, we distinguish between abstract causalities like hard
studying causes passing exams, and causality tokens like John?s tiredness caused
John?s failure. Note that asserting (Rexist c) on an abstract causal relation c amounts
to asserting (Rexist c?) for any (partial) instance c? of c. But recall that cause is only
defeasible. Both the abstract causal principle and its partial instance are simplified
stand-ins for rules that involve entire causal complexes, not all of whose elements may
obtain. Thus, just because hard studying causes passing exams, we cannot invariably
conclude that if John really studied, he really passed the exam.
4 The meaning of concessive relations
Our basic claim is that the meaning of concessive relations is triggered by a contrast
between two causal relations cc and cd such that one or more eventualities in the causal
complex of ec2 (the expectation created by cc), is denied by ed2 (the effect of cd). cc,
cd , ed2, and ec1 (the cause in cc) really exist in the world, or are at least believed to
exist by the speaker/writer. Furthermore, all eventualities in the causal complex for
ed2, including the non-presumable cause ed1, which is unknown in many cases, really
exist too. Argcexp conveys ec1, while Argdexp conveys ed2.
We also claim that in all cases of concession it seems that what really creates the
expectation is a causal relation cac that is an abstraction of cc. cc really exists in the
world precisely because cac really exists and cc is a partial instance of it. In other words,
the real existence of cc is inherited from cac. On the other hand, there is not necessarily
an abstract counterpart cad for cd that also really exists in the world. For instance,
in (1.a), it seems that what creates the expectation is the assumption that the causal
relation studying hard causes passing exams (cac) really exists in the context. John?s
hard studying causes John?s passing exams (cc) is just an instance of cac. This instance
really exists in the world too. However, since causality is defeasible, the fact that John
really studied hard (ec1) does not entail the real existence of John really passed the
exam (ec2). In fact, this is precisely denied by Argdexp: John did not pass the exam
(ed2). The cause of John?s failure, e.g., John?s tiredness (ed1), is (or is the cause of
an eventuality edp that is) inconsistent with an element ecp of the causal complex for
(ec2), namely, John does not have any particural health problem that jeopardizes the
passing of his exam. Note that we do not necessarily infer that being tired causes
failing an exam: tiredness was the cause of the failure in this particular scenario only.
Therefore, we assert that cd really exists, but we do not advocate the existence of a
more abstract causal relation cad that really exists too.
To summarize, the semantics of concession we propose is formalized in (2). The
conjuncts (Rexist cc) and (Rexist ed1) have been omitted in (2) because they may be
inferred from (Rexist cac) and (Rexist ed2). sc is the causal complex associated with
cc. ec1 and ed2 are given to us in Argcexp and Argdexp respectively, while all other
eventualities may be inferred by abduction from the contextual knowledge; some hints
about how this may be done are provided in Hobbs (2005).
216 Robaldo, Miltsakaki, and Hobbs
(2) (exist (cc ca c ec1 ec2 cd ed1 ed2 sc ecp edp)
(cause? cc ec1 ec2) ? (cause? cd ed1 ed2) ? (Rexist ca c) ?
(partialInstance cc ca c) ? (Rexist cd) ? (Rexist ec1) ?
(Rexist ed2) ? (cause ed1 edp) ? (Rexist edp) ?
(inconsistent ec2 ed2) ? (causalComplex sc ec2) ?
(memberecp sc) ? (inconsistent edp ecp))
Let us now examine how the semantics given in (2) applies for corpus examples
tagged as ?expectation? or ?contra-expectation?. Let us analyze (1.b) in the light of
the semantics proposed in (2). The abstract causality that creates the expectation (ca
c) is Something that is considered healthy for humans is advisable for them7. This
is partially instantiated in Since running is considered healthy for persons with heart
problems, it is advisable for them (cc). Nevertheless, the fact that running is really
considered healthy in the context (ec1) does not suffice to assert that running is really
advisable for persons with heart problems (ec2). There is a particular reason why
running is not advisable for persons with heart problems (ed2), e.g. their hearts do
not tolerate a heartbeat increase (ed1). Since running causes a heartbeat increase, the
heart can tolerate a heartbeat increase (ecp) is in the causal complex for ec2 and it is
inconsistent with ed2.
Similarly, in (1.c), which is taken from the PDTB, it is true that representing a
low percentage of the population causes controlling low percentage of income (cac).
Therefore, they represent 2% of population (ec1) causes they control low percentage
of income (ec2). Nevertheless, ec2 does not really exists in the context, in that it is
inconsistent with they control nearly one-third of income (ed2). There must be another
reason for why ec2 does not hold. For instance, either they are very rich, or they do
not have as many basic expenses as other people, or a more complex condition. This
unknown cause, i.e. ed1, both makes ed2 true and ec2 false in the context.
The last example highlights the point that finding the eventualities involved in (2) is
strongly dependent upon contextual knowledge. 2% is not taken to be a low percentage
in any context. For instance, 2% mercury in the water may be considered a high
percentage of pollution. Analogously, one third may be considered a high percentage
in that context, especially if compared with 2% of population, but it may be a low or
medium percentage in many other contexts. The analysis of examples (1.d-e) in terms
of the definition in (2) is analogous.
5 A survey of concessive relations in PDTB 2.0
PDTB 2.0 contains 1193 tokens of explicit connectives which are annotated with one
sense tagged as ?Concession?, ?contra-expectation? and ?expectation?. There are also
another 20 tokens that have been annotated with double senses, one of which is the
concessive type or subtypes. Table (1) shows the distribution of concessive labels for
the 1193 tokens. Explicit connectives with a concessive label assigned to less than
10 tokens are grouped under ?other?. The rest of the connectives shown in Table (1)
amount to 98% of all ?contra-expectation? and 95% of all ?expectation? tokens. The
7This is a paraphrase of Something being considered healthy for humans causes it to be advisable for
humans.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 217
Table 1: Concessive labels in PDTB 2.0
CONN ?contra-exp.? ?exp.? ?Concession? Total
although 21 132 1 154 (13%)
but 494 12 2 508 (42.5%)
even if 3 31 1 35 (3%)
even though 15 52 5 72 (6%)
however 70 2 5 77 (6.5%)
nevertheless 19 0 0 19 (1.5%)
nonetheless 17 0 0 17 (1.5%)
still 79 2 1 82 (7%)
though 30 53 1 84 (7%)
while 3 79 1 83 (7%)
yet 32 0 0 32 (2.5%)
other 13 17 0 30 (2.5%)
Total 796 380 17 1193
most common connective annotated with the ?Concession? type or one of its two sub-
types is ?but? with 508 tokens (42% of all concessive labels), followed by ?although?
with 154 tokens (13% of all concesive labels).
We are currently evaluating the robustness of the proposed refined semantics for
concessive labels in PDTB 2.0 starting with the most the most common concessive
connectives. While the validation process for the entire corpus is still work in progress,
preliminary results on 25% of ?but? tokens indicate that the semantics of concession
based on defeasible causality applies straightforwardly to more than 60% of the data.
In future work, we hope to be able to offer a more comprehensive account of all the
concessive labels in PDTB 2.0 including cases of concession in which the created
expectation arises from an implication rather than from a causal relation (about 23%),
as in (3)
(3) Although working for U.S. intelligence, Mr. Noriega was hardly helping the
U.S. exclusively. (expectation)
In (3), it is strange to say that working for U.S. intelligence normally ?causes?
helping U.S. exclusively. Rather, the former seems a kind of necessary condition or
job requirement for the latter: working for U.S. intelligence implies (among other
things) helping U.S. exclusively. Suppose that someone discovers that Mr. Noriega is
not helping the U.S. exclusively. Mr. Noriega is arguably breaking a rule or flauting an
expectation. Therefore, working for U.S. intelligence ?implies? rather than ?causes?
helping U.S. exclusively.
It is unsurprising that there are cases of concession based on implication rather than
causality, because the two concepts are very close to each other. One could think of
implication as a kind of abstract, informational, or ?denatured? causality. Both obey a
kind of (defeasible) modus ponens. When the cause or antecedent happens or holds,
so does the effect or consequent. The other key property of causal complexes is that
218 Robaldo, Miltsakaki, and Hobbs
all the eventualities in it are relevant, in a sense that is made precise in Hobbs (2005).
This notoriously does not hold for material implication, but as many have argued,
it probably does hold for felicitous uses of our everyday notion of implication. In
addition, there are easy conversions between causality and implication. If A causes B,
then the fact that A happens (defeasibly) implies that B happens. If P implies Q in the
everyday sense, then one?s belief in P (defeasibly) causes one?s belief in Q. In fact,
implicational cases of concession could be viewed as instances of metonymy, where
?believe? is the coercion relation, and hence really causal cases of concession.
6 Conclusion
We presented a formal description of the meaning of concession, a substantial refine-
ment of the rough semantics given in the manual of sense annotations of connectives
in PDTB 2.0. Our analysis builds on Hobbs?s logic of defeasible causality enabled
by the crucial distinction between causes and causal complexes. Our basic claim is
that concession is triggered by the contrast between two causal relations. The causal
relation between the content of one argument of the relation and some implicit even-
tuality (the expectation created based on the content of the argument) and the content
of another causal relation, that between the eventuality described in second argument
and its implicit cause. This second causal relation picks an element of the causal com-
plex that we would normally assume to hold and challenges it, hence the notion of
defeasible causality.
This work illustrates the mutual benefit that corpus annotation and formal analysis
can provide to each other. Corpus examples constitute a forcing function on the formal
analysis; definitions must accommodate the complexities one finds in the real world.
On the other hand, all good annotation rests on solid theory, and formal analysis can
help in the adjudication of difficult examples. The particular analysis we give in this
paper for the concession relation can clarify issues that arise in annotation, and can
also form the basis for recognizing these relations using a knowledge-rich inferencing
system.
References
Achinstein, P. (1965). ?Defeasible? Problems. The Journal of Philosophy 62(21),
629?633.
Asher, N. (1993). Reference to Abstract Objects. Kluwer, Dordrecht.
Bach, E. (1981). On Time, Tense, and Aspect: An Essay in English Metaphysics. In
P. Cole (Ed.), Radical Pragmatics, pp. 63?81. Academic Press, New York.
Bell, J. (1999). Primary and secondary events. In M. Thielscher (Ed.), Proc. of the
IJCAI-99 Workshop on Nonmonotonic Reasoning, Action and Change, pp. 65?72.
Bell, J. (2003). A common sense theory of causation. In P. Blackburn, C. Ghidini,
R. Turner, and F. Giunchiglia (Eds.), Modeling and Using Context: Fourth Interna-
tional and Interdisciplinary Conference, Context 2003, Berlin, pp. 40?53. Springer-
Verlag.
Refining the Meaning of Sense Labels in PDTB: ?Concession? 219
Giordano, L. and C. Schwind (2004). Conditional logic of actions and causation.
Artificial Intelligence 157(1?2), 239?279.
Giunchiglia, E., J. Lee, V. Lifschitz, N. McCain, and H. Turner (2004). Nonmonotonic
causal theories. Artificial Intelligence 153(1?2), 49?104.
Hobbs, J. (1983). An Improper Treatment of Quantification in Ordinary English. In
Proc. of the 21st Annual Meeting of the Association for Computational Linguistics,
Cambridge, Massachusetts, pp. 57?63.
Hobbs, J. (1995). Monotone Decreasing Quantifiers in a Scope-Free Logical Form. In
K. van Deemter and S. Peters (Eds.), Semantic Ambiguity and Underspecification,
CSLI Lecture Notes, pp. 55?76. CSLI.
Hobbs, J. (1998). The Logical Notation: Ontological Promiscuity. In Discourse and
Inference, Chapter 2.
Hobbs, J. (2005). Towards a Useful Notion of Causality for Lexical Semantics. Jour-
nal of Semantics 22(2), 181?209.
Kayser, D. and F. Nouioua (2008). From the Description of an Accident to its Causes.
submitted to Artificial Intelligence.
McCarthy, J. (1977). Epistemological Problems of Artificial Intelligence. In Proc.
of International Joint Conference on Artificial Intelligence, Cambridge, Mas-
sachusetts, pp. 1038?1044.
Miltsakaki, E., L. Robaldo, A. Lee, and A. Joshi (2008). Sense Annotation in the
Penn Discourse Treebank. In Proc. of Computational Linguistics and Intelligent
Text Processing, Volume 4919 of LNCS, pp. 275?286. Springer.
Prasad, R., N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber
(2008). The Penn Discourse Treebank 2.0. In Proc. of the 6th Int. Conf. on Lan-
guage Resources and Evaluation.
Prasad, R., E. Miltsakaki, N. Dinesh, A. Lee, A. Joshi, B. Webber, and L. Robaldo
(2008). The Penn Discourse Treebank 2.0. Annotation Manual. Technical Report
IRCS-06-01, IRCS Technical Report, Institute of Research in Cognitive Science,
University of Pennsylvania.
Shoham, Y. (1990). Nonmonotonic reasoning and causation. Cognitive Science 14,
213?252.
Simon, H. (1991). Nonmonotonic reasoning and causation: Comment. Cognitive
Science 49, 517?528.
Webber, B. and A. Joshi (2003). Anchoring a lexicalized tree-adjoining grammar for
discourse. In M. Stede, L. Wanner, and E. Hovy (Eds.), Discourse Relations and
Discourse Markers: Proceedings of the Conference, pp. 86?92.
Webber, B., A. Joshi, M. Stone, and A. Knott (2003). Anaphora and discourse struc-
ture. Computational Linguistics 29(4), 545?587.
Implementing Weighted Abduction in Markov Logic
James Blythe
USC ISI
blythe@isi.edu
Jerry R. Hobbs
USC ISI
hobbs@isi.edu
Pedro Domingos
University of Washington
pedrod@cs.washington.edu
Rohit J. Kate
University of Wisconsin-Milwaukee
katerj@uwm.edu
Raymond J. Mooney
University of Texas at Austin
mooney@cs.utexas.edu
Abstract
Abduction is a method for finding the best explanation for observations. Arguably
the most advanced approach to abduction, especially for natural language processing, is
weighted abduction, which uses logical formulas with costs to guide inference. But it
has no clear probabilistic semantics. In this paper we propose an approach that imple-
ments weighted abduction in Markov logic, which uses weighted first-order formulas to
represent probabilistic knowledge, pointing toward a sound probabilistic semantics for
weighted abduction. Application to a series of challenge problems shows the power and
coverage of our approach.
1 Introduction
Abduction is inference to the best explanation.1 Typically, one uses it to find the best hypothesis ex-
plaining a set of observations, e.g., in diagnosis and plan recognition. In natural language processing the
content of an utterance can be viewed as a set of observations, and the best explanation then constitutes
the interpretation of the utterance. Hobbs et al [7] described a variety of abduction called ?weighted
abduction? for interpreting natural language discourse. The key idea was that the best interpretation of
a text is the best explanation or proof of the logical form of the text, allowing for assumptions. What
counted as ?best? was defined in terms of a cost function which favored proofs with the fewest number of
assumptions and the most salient and plausible axioms, and in which the pervasive redundancy implicit
in natural language discourse was exploited. It was argued in that paper that such interpretation problems
as coreference and syntactic ambiguity resolution, determining the specific meanings of vague predicates
and lexical ambiguity resolution, metonymy resolution, metaphor interpretation, and the recognition of
discourse structure could be seen to ?fall out? of the best abductive proof.
Specifically, weighted abduction has the following features:
1. In a goal expression consisting of an existentially quantified conjunction of positive literals, each
literal is given a cost that represents the utility of proving that literal as opposed to assuming it.
That is, a low cost on a literal will make it more likely for it to be assumed, whereas a high cost
will result in a greater effort to find a proof.
1We are indebted to Jesse Davis, Parag Singla and Marc Sumner for discussions about this work. This research was
supported in part by the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force
Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172, in part by the Office of Naval Research under contract
no. N00014-09-1-1029, and in part by the Army Research Office under grant W911NF-08-1-0242. Any opinions, findings, and
conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of
the DARPA, AFRL, ONR, ARO, or the US government.
55
2. Costs are passed back across the implication in Horn clauses according to weights on the conjuncts
in the antecedents. Specifically, if a consequent costs $c and the weight on a conjunct in the
antecedent is v, then the cost on that conjunct will be $vc. Note that if the weights add up to less
than one, backchaining on the rule will be favored, as the cost of the antecedent will be less than
the cost of the consequent. If the weights add up to more than one, backchaining will be disfavored
unless a proof can be found for one or more of the conjuncts in the antecedent, thereby providing
partial evidence for the consequent.
3. Two literals can be factored or unified, where the result is given the minimum cost of the two,
providing no contradiction would result. This is a frequent mechanism for coreference resolution.
In practice, only a shallow or heuristic check for contradiction is done.
4. The lowest-cost proof is the best interpretation, or the best abductive proof of the goal expression.
However, there are two significant problems with weighted abduction as it was originally presented.
First, it required a large knowledge base of commonsense knowledge. This was not available when
weighted abduction was first described, but since that time there have been substantial efforts to build up
knowledge bases for various purposes, and at least two of these have been used with promising results
in an abductive setting?Extended WordNet [6] for question-answering and FrameNet [11] for textual
inference.
The second problem with weighted abduction was that the weights and costs did not have a prob-
abilistic semantics. This, for example, hampers automatic learning of weights from data or existing
resources. That is the issue we address in the present paper.
In the last decade and a half, a number of formalisms for adding uncertain reasoning to predicate logic
have been developed that are well-founded in probability theory. Among the most widely investigated
is Markov logic [14, 4]. In this paper we show how weighted abduction can be implemented in Markov
logic. This demonstrates that Markov logic networks can be used as a powerful mechanism for interpret-
ing natural language discourse, and at the same time provides weighted abduction with something like a
probabilistic semantics.
In Section 2 we briefly describe Markov logic and Markov logic networks. Section 3 then describes
how weighted abduction can be implemented in Markov logic. In Section 4 we describe experiments in
which fourteen published examples of the use of weighted abduction in natural language understanding
are implemented in Markov logic networks, with good results. Section 5 on current and future directions
briefly describes an ongoing experiment in which we are attempting to scale up to apply this procedure
to the textual inference problem with a knowledge base derived from FrameNet with tens of thousands
of axioms.
2 Markov Logic Networks and Related Work
Markov logic [14, 4] is a recently developed theoretically sound framework for combining first-order
logic and probabilistic graphical models. A traditional first-order knowledge base can be seen as a set of
hard constraints on the set of possible worlds: if a world violates even one formula, its probability is zero.
In order to soften these constraints, Markov logic attaches a weight to each first-order logic formula in
the knowledge base. Such a set of weighted first-order logic formulae is called a Markov logic network
(MLN). A formula?s weight reflects how strong a constraint it imposes on the set of possible worlds: the
higher the weight, the lower the probability of a world that violates it; however, that probability need not
be zero. An MLN with all infinite weights reduces to a traditional first-order knowledge base with only
hard constraints.
56
Formally, an MLN L is a set of formula?weight pairs (Fi, wi). Given a set of constants, it defines
a joint probability distribution over a set of boolean variables X = (X1, X2...) corresponding to the
possible groundings (using the given constants) of the literals present in the first-order formulae:
P (X = x) = 1Z exp(
?
iwini(x))
where ni(x) is the number of true groundings of Fi in x and Z is a normalization term obtained by
summing P (X = x) over all values of X .
Semantically, an MLN can be viewed as a set of templates for constructing Markov networks [12],
the undirected counterparts of Bayesian networks. An MLN and a set of constants produce a Markov
network in which each ground literal is a node and every pair of ground literals that appear together in
some grounding of some formula are connected by an edge. Different sets of constants produce different
Markov networks; however, there are certain regularities in their structure and parameters. For example,
all groundings of the same formula have the same weight.
Probabilistic inference for an MLN (such as finding the most probable truth assignment for a given
set of ground literals, or finding the probability that a particular formula holds) can be performed by
first producing the ground Markov network and then using well known inference techniques for Markov
networks, like Gibbs sampling. Given a knowledge base as a set of first-order logic formulae, and a
database of training examples each consisting of a set of true ground literals, it is also possible to learn
appropriate weights for the MLN formulae which maximize the probability of the training data. An open-
source software package for MLNs, called Alchemy 2, is also available with many built-in algorithms
for performing inference and learning.
Much of the early work on abduction was done in a purely logical framework (e.g., [13, 3, 9, 10].
Typically the choice between alternative explanations is made on the basis of parsimony; the shortest
proofs with the fewest assumptions are favored. However, a significant limitation of these purely logical
approaches is that they are unable to reason under uncertainty or estimate the likelihood of alternative
explanations. A probabilistic form of abduction is needed in order to account for uncertainty in the
background knowledge and to handle noisy and incomplete observations.
In Bayesian networks [12] background knowledge with its uncertainties is encoded in a directed
graph. Then, given a set of observations, probabilistic inference over the graph structure is done to
compute the posterior probability of alternative explanations. However, Bayesian networks are based on
propositional logic and cannot handle structured representations, hence preventing their use in situations,
characteristic of natural language processing, that involve an unbounded number of entities with a variety
of relations between them.
In recent years there have been a number of proposals attempting to combine the probabilistic nature
of Bayesian networks with structured first-order representations. It is impossible here to review this liter-
ature here. A a good review of much of it can be found in [5], and in [14] there are detailed comparisonss
of various models to MLNs.
Charniak and Shimony [2] define a variant of weighted abduction, called ?cost-based abduction? in
which weights are attached to terms rather than to rules or to antecedents in rules. Thus, the term Pi
has the same cost whatever rule it is used in. The cost of an assignment to the variables in the domain
is the sum of the costs of the variables that are true in the assignment. Charniak and Shimony provide
a probabilistic semantics for their approach by showing how to construct a Bayesian network from a
domain such that a most probable explanation solution to the Bayes net corresponds to a lowest-cost
solution to the abduction problem. However, in natural language applications the utility of proving a
proposition can vary by context; weighted abduction accomodates this, whereas cost-based abduction
does not.2http://alchemy.cs.washington.edu
57
3 Weighted Abduction and MLNs
Kate and Mooney [8] show how logical abduction can be implemented in Markov logic networks. They
use forward inference in MLNs to perform abduction by adding clauses with reverse implications. Uni-
versally quantified variables from the left hand side of rules are converted to existentially quantified
variables in the reversed clause. For example, suppose we have the following rule saying that mosquito
bites transmit malaria:
mosquito(x) ? infected(x,Malaria) ? bite(x, y) ? infected(y,Malaria)
This would be translated into the soft rule
[w] infected(y,Malaria) ? ?x[mosquito(x) ? infected(x,Malaria) ? bite(x, y)]
Where there is more than one possible explanation, they include a closure axiom saying that one of the
explanations must hold. Since blood transfusions also cause malaria, they have the hard rule
infected(y,Malaria) ?
?x[mosquito(x) ? infected(x,Malaria) ? bite(x, y)]
??x[infected(x,Malaria) ? transfuse(Blood, x, y)].
Kate and Mooney also add a soft mutual exclusivity clause that states that no more than one of the
possible explanations is true.
In translating between weighted abduction and Markov logic, we need similarly to specify the axioms
in Markov logic that correspond to a Horn clause axiom in weighted abduction. In addition, we need to
describe the relation between the numbers in weighted abduction and the weights on the Markov logic
axioms. Hobbs et al [7] give only broad, informal guidelines about how the numbers correspond to
probabilities. In this development, we elaborate on how the numbers can be defined more precisely
within these guidelines in a way that links with the weights in Markov logic, thereby pointing to a
probabilistic semantics for the weighted abduction numbers.
There are two sorts of numbers in weighted abduction?the weights on conjuncts in the antecedents
of Horn clause axioms, and the costs on conjuncts in goal expressions, which are existentially quantified
conjunctions of positive literals. We deal first with the weights, then with the costs.
The space of events over which probabilities are taken is the set of proof graphs constituting the best
interpretations of a set of texts in a corpus. Thus, by the probability of p(x) given q(x), we mean the
probability that p(x) will occur in a proof graph in which q(x) occurs.
The translation from weighted abduction axioms to Markov logic axioms can be broken into two
steps. First we consider the ?or? node case, determining the relative costs of axioms that have the same
consequent. Then we look at the ?and? node case, determining how the weights should be distributed
across the conjuncts in the antecedent of a Horn clause, given the total weight for the antecedent.
Weights on Antecedents in Axioms. First consider a set of Horn clause axioms all with the same
consequent, where we collapse the antecedent into a single literal, and for simplicity allow x to stand for
all the universally quantified variables in the antecedent, and assume the consequent to have only those
variables. That is, we convert all axioms of the form
p1(x) ? . . . ? q(x)
into axioms of the form
Ai(x) ? q(x), where p1(x) ? . . . ? Ai(x)
To convert this into Markov logic, we first introduce the hard constraint
Ai(x) ? q(x).
In addition, given a goal of proving q(x), in weighted abduction we will want to backchain on at least
(and usually at most) one of these axioms or we will want simply to assume q(x). Thus, we can introduce
another hard constraint with the disjunction of these antecedents as well as a literal AssumeQ(x) that
means q(x) is assumed rather than proved.
58
q(x) ? A1(x) ? A2(x) ? . . . ? An(x) ? AssumeQ(x).
Then we need to introduce soft constraints to indicate that each of these disjuncts is a possible explana-
tion, or proof, of q(x), with an associated probability, or weight.
[wi] q(x) ? Ai(x), . . .
[w0] q(x) ? AssumeQ(x)
The probability that AssumeQ(x) is true is the conditional probability P0 that none of the antecedents
is true given that q(x) is true.
P0 = P (?[A1(x) ? A2(x) ? . . . ? An(x)] | q(x))
In weighted abduction, when the antecedent weight is greater than one, we prefer assuming the conse-
quent to assuming the antecedent. When the antecedent weight is less than one we prefer to assume the
antecedent. If the probability that an antecedent Ai(x) is the explanation of q(x) is greater than P0, it
should be given a weighted abduction weight vi less than 1, making it more likely to be chosen.3 Cor-
respondingly, if it is less than P0, it should be given a weight vi greater than 1, making it less likely
to be chosen. In general, the weighted abduction weights should be in reverse order of the conditional
probabilities Pi that Ai(x) is the explanation of q(x).
Pi = P (Ai(x) | q(x))
If we assign the weights vi in weighted abduction to be
vi = logPilogP0
then this is consistent with informal guidelines in [7] on the meaning of these weights. We use the logs
of the probabilities rather than the probabilities themselves to moderate the effect of one axiom being
very much more probable than any of the others.
Kate and Mooney [8], in their translation of logical abduction into Markov logic, also include soft
constraints stipulating that the different possible explanations Ai(x) are normally mutually exclusive.
We do not do that here, but we get a kind of soft mutual exclusivity constraint by virtue of the axioms
below that levy a cost for any literal that is taken to be true. In general, more parimonious explanations
will be favored.
Nevertheless, in most cases a single explanation will suffice. When this is true, the probability of
Ai(x) holding when q(x) holds is e
wi
Z . Then a reasonable approximation for the relation between the
weighted abduction weights vi and the Markov logic weights wi is
wi = ?vilogP0
Weights on Conjuncts in Antecedents. Next consider how cost is spread across the conjuncts in the
antecedent of a Horn clause in weighted abduction. Here we use u?s to represent the weighted abduction
weights on the conjuncts.
p1(x)u1 ? p2(x)u2 ? ... ? A(x)
The u?s should somehow represent the semantic contribution of each conjunct to the conclusion. That is,
given that the conjunct is true, what is the probability that it is part of an explanation of the consequent?
Conjuncts with a higher such probability should be given higher weights u; they play a more significant
role in explaining A(x).
Let Pi be the conditional probability of the consequent given the ith conjunct in the antecedent.
Pi = P (A(x)|pi(x))
and let Z be a normalization factor.
Z = ?ni=1 Pi
3We use vi for these weighted abduction weights and wi for Markov logic weights.
59
Let v be the weight of the entire antecedent as determined above.
Then it is consistent with the guidelines in [7] to define the weights on the conjuncts as follows:
ui = vPiZ
The weights ui will sum to v and each will correspond to the semantic contribution of its conjunct to the
consequent.
In Markov logic, weights apply only to axioms as a whole, not parts of axioms. Thus, the single
axiom above must be decomposed into one axiom for each conjunct and the dependencies must be
written as
[wi] pi(x) ? A(x), . . .
The relation between the weighted abduction weights ui and the Markov logic weights wi can be
approximated by
ui = ve
?wi
Z
Costs on Goals. The other numbers in weighted abduction are the costs associated with the conjuncts
in the goal expression. In weighted abduction these costs function as utilities. Some parts of the goal
expression are more important to interpret correctly than others; we should try harder to prove these
parts, rather than simply assuming them. In language it is important to recognize the referential anchor
of an utterance in shared knowledge. Thus, those parts of a sentence most likely to provide this anchor
have the highest utility. If we simply assume them, we lose their connection with what is already known.
Those parts of a sentence most likely to be new information will have a lower cost, because we usually
would not be able to prove them in any case.
Consider the two sentences
The smart man is tall.
The tall man is smart.
The logical form for each of them will be
(?x)[smart(x) ? tall(x) ?man(x)]
In weighted abduction, an interpretation of the sentence is a proof of the logical form, allowing assump-
tions. In the first sentence we want to prove smart(x) to anchor the sentence referentially. Then tall(x)
is new information; it will have to be assumed. We will want to have a high cost on smart(x) to force
the proof procedure to find this referential anchor. The cost on tall(x) will be low, to allow it to be
assumed without expending too much effort in trying to locate that fact in shared knowledge.
In the second sentence, the case is the reverse.
Let?s focus on the first sentence and assume we know that educated people are smart and big people
are tall, and furthermore that John is educated and Bill is big.
educated(x)1.2 ? smart(x)
big(x)1.2 ? tall(x)
educated(J), big(B)
In weighted abduction, the best interpretation will be that the smart man is John, because he is educated,
and we pay the cost for assuming he is tall. The interpretation we want to avoid is one that says x is Bill;
he is tall because he is big, and we pay the cost of assuming he is smart. Weighted abduction with its
differential costs on conjuncts in the goal expression favors the first and disfavors the second.
In weighted abduction, only assumptions cost; literals that are proved cost nothing. When the above
axioms are translated into Markov logic, it would be natural to capture the differential costs by attaching a
negative weight to smart(x) to model the cost associated with assuming it. However, this weight would
apply to any assignment in which smart(J) is true, regardless of whether it was assumed, derived from
60
an assumed fact, or derived from a known fact. A potential solution might be to attach the negative weight
to AssumeSmart(x). But the first axiom above allows us to bypass the negative weight on smart(x).
We can hypothesize that x is Bill, pay a low cost on AssumeEducated(B), derive smart(B), and get
the wrong assignment. Thus it is not enough to attach a negative weight to high-cost conjuncts in the
goal expression. This negative weight would have to be passed back through the whole knowledge base,
making the complexity of setting the weights at problem time in the MLN knowledge base equal to the
complexity of running the inference problem.
An alternative solution, which avoids this problem when the forward inferences are exact, is to use
a set of predicates that express knowing a fact without any assumptions. In the current example, we
would add Ksmart(x) for knowing that an entity is smart. The facts asserted in the data base are now
Keducated(J) and Kbig(B). For each hard axiom involving non-K predicates, we have a correspond-
ing axiom that expresses the relation between the K-predicates, and we have a soft axiom allowing us to
cross the border between the K predicates and their non-K counterparts.
Keducated(x) ? Ksmart(x)., . . .
[w] Ksmart(x) ? smart(x), . . .
Here the positive weight w attached is chosen to counteract the negative weight we would attach to
smart(x) to reflect the high cost of assuming it.
This removes the weight associated with assuming smart(x) regardless of the inference path that
leads to knowing smart(x) (KSmart(x))). Further, this translation takes linear time in the size of
the goal expression to compute, since we do not need to know the equivalent weighted abduction cost
assigned to the possible antecedents of smart(x).
If the initial facts do not include KEducated(B) and instead educated(B) must be assumed, then
the negative weight associated with smart(B) is still present. In this solution, there is no danger that
the inference process can by-pass the cost of assuming smart(B), since it is attached to the required
predicate and can only be removed by inferring KSmart(B).
Finally, there is a tendency in Markov logic networks for assignments of high probability for proposi-
tions for which there is no evidence one way or the other. To suppress this, we associate a small negative
weight with every predicate. In practice, it has turned out that a weight of ?1 effectively suppresses this
behavior.
4 Experimental Results
We have tested our approach on a set of fourteen challenge problems from [7] and subsequent papers,
designed to exercise the principal features of weighted abduction and show its utility for solving natural
language interpretation problems. The knowledge bases used for each of these problems are sparse,
consisting of only the axioms required for solving the problems plus a few distractors.
An example of a relatively simple problem is #5 in the table below, resolving ?he? in the text
I saw my doctor last week. He told me to get more exercise.
where we are given a knowledge base that says a doctor is a person and a male person is a ?he?. Solving
the problem requires assuming the doctor is male.
(?x)[doctor(x)1.2 ? person(x)]
(?x)[male(x).6 ? person(x).6 ? he(x)]
The logical form fragment to prove is (?x)he(x), where we know doctor(D).
A problem of intermediate difficulty (#7) is resolving the three lexical ambiguities in the sentence
The plane taxied to the terminal.
61
where we are given a knowledge base saying that airplanes and wood smoothers are planes, planes
moving on the ground and people taking taxis are both described as ?taxiing?, and computer terminals
and airport terminals are both terminals.
An example of a difficult problem is #12, finding the coherence relation, thereby resolving the pro-
noun ?they?, between the sentences
The police prohibited the women from demonstrating. They feared violence.
The axioms specify relations between fearing, not wanting, and prohibiting, as well as the defeasible
transitivity of causality and the fact that a causal relation between sentences makes the discourse coher-
ent.
The weights in the axioms were mostly distributed evenly across the conjuncts in the antecedents and
summed to 1.2.
For each of these problems, we compare the performance of the method described here with a man-
ually constructed gold standard and also with a method based on Kate and Mooney?s (KM) approach.
In this method, weights were assigned to the reversed clauses based on the negative log of the sum of
weights in the original clause. This approach does not capture different weights for different antecedents
of the same rule, and so has less fidelity to weighted abduction than our approach. In each case, we used
Alchemy?s probabilistic inference to determine the most probable explanation (MPE) [12].
In some of the problems the system should make more than one assumption, so there are 22 assump-
tions in total over all 14 problems in the gold standard. Using our method, 18 of the assumptions were
found, while 15 were found using the KM method. Table 1 shows the number of correct assumptions
found and the running time for the two approaches for each problem. Our method in particular provides
good coverage, with a recall of over 80% of the assumptions made in the gold standard. It has a shorter
running time overall, approximately 5.3 seconds versus 8.7 seconds for the reversal method. This is
largely due to one problem in the test set, problem #9, where the running time for the KM method is
relatively high because the technique finds a less sparse network, leading to larger cliques. There were
two problems in the test set that neither approach could solve. One of these contains predicates that have
a large number of arguments, leading to large clique sizes.
5 Current and Future Directions
In other work [11] we are experimenting with using weighted abduction with a knowledge base with tens
of thousands of axioms derived from FrameNet for solving problems in recognizing textual entailment
(RTE2) from the Pascal dataset [1]. For a direct comparison between standard weighted abduction and
the Markov logic approach described here, we are also experimenting with using the latter on the same
task with the same knowledge base.
For each text-hypothesis pair, the sentences are parsed and a logical form is produced. The output for
the first sentence forms the specific knowledge the system has while the output for the second sentence
is used as the target to be explained. If the cost of the best explanation is below a threshold we take the
target sentence to be true given the initial information.
It is a major challenge to scale our approach to handle all the problems from the RTE2 development
and test sets. We are not yet able to address the most complex of these using inference in Markov logic
networks. However, we have devised a number of pre-processing steps to reduce the complexity of the
resultant network, which significantly increase the number of problems that are tractable.
The FrameNet knowledge base contains a large number of axioms with general coverage. For any
individual entailment problem, most of them are irrelevant and can be removed after a simple graphical
analysis. We are able to remove more irrelevant axioms and predicates with an iterative approach that in
62
Our Method KM Method Gold
Problem score seconds score seconds standard
1 3 300 3 16 3
2 1 250 1 265 1
3 1 234 1 266 1
4 2 234 2 203 2
5 1 218 1 218 1
6 1 218 0 265 1
7 3 300 3 218 3
8 1 200 1 250 1
9 2 421 0 5000 2
10 1 2500 1 1500 3
11 0 0 1
12 0 0 1
13 1 250 1 250 1
14 1 219 1 219 1
Total 18 5344 15 8670 22
Table 1: Performance on each problem in our test set, comparing two encodings of weighted abduction
into Markov logic networks and a gold standard.
each iteration both drops axioms that are shown to be irrelevant and simplifies remaining axioms in such
a way as not to change the probability of entailment.
We also simplify predications by removing unnecessary arguments. The most natural way to convert
FrameNet frames to axioms is to treat a frame as a predicate whose arguments are the frame elements for
all of its roles. After converting to Markov logic, this results in rules having large numbers of existentially
quantified variables in the consequent. This can lead to a combinatorial explosion in the number of
possible ground rules. Many of the variables in the frame predicate are for general use and can be pruned
in the particular entailment. Our approach essentially creates abstractions of the original predicates that
preserve all the information that is relevant to the current problem but greatly reduces the number of
ground instances to consider.
Before implementing these pre-processing steps, only two or three problems could be run to com-
pletion on a Macbook Pro with 8 gigabytes of RAM. After making them, 28 of the initial 100 problems
could be run to completion.
Work on this effort continues.
6 Summary
Weighted abduction is a logical reasoning framework that has been successfully applied to solve a num-
ber of interesting and important problems in computational natural-language semantics ranging from
word sense disambiguation to coreference resolution. However, its method for representing and combin-
ing assumption costs to determine the most preferred explanation is ad hoc and without a firm theoretical
foundation. Markov Logic is a recently developed formalism for combining first-order logic with prob-
abilistic graphical models that has a well-defined formal semantics in terms of specifying a probability
distribution over possible worlds. This paper has presented a method for mapping weighted abduction
63
to Markov logic, thereby providing a sound probabilistic semantics for the approach and also allowing
it to exploit the growing toolbox of inference and learning algorithms available for Markov logic. Com-
plementarily, it has also demonstrated how Markov logic can thereby be applied to help solve important
problems in computational semantics.
References
[1] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpek-
tor. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entailment, Venice, Italy, 2006.
[2] Eugene Charniak and Solomon E. Shimony. Cost-based abduction and map explanation. Artificial Artificial
Intelligence Journal, 66(2):345?374, 1994.
[3] P. T. Cox and T. Pietrzykowski. Causes for events: Their computation and applications. In J. Siekmann,
editor, 8th International Conference on Automated Deduction (CADE-8), Berlin, 1986. Springer-Verlag.
[4] P. Domingos and D. Lowd. Markov Logic: An Interface Layer for Artificial Intelligence. Morgan & Claypool,
San Rafael, CA, 2009.
[5] L. Getoor and B. Taskar, editors. Introduction to Statistical Relational Learning. MIT Press, Cambridge,
MA, 2007.
[6] S. Harabagiu and D.I. Moldovan. Lcc?s question answering system. In 11th Text Retrieval Conference,
TREC-11, Gaithersburg, MD., 2002.
[7] Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt, and Paul A. Martin. Interpretation as abduction. Artifi-
cial Intelligence, 63(1-2):69?142, 1993.
[8] Rohit Kate and Ray Mooney. Probabilistic abduction using markov logic networks. In IJCAI 09 Workshop
on Plan, Activity and Intent Recognition, 2009.
[9] Hector J. Levesque. A knowledge-level account of abduction. In Eleventh International Joint Conference on
Artificial Intelligence, volume 2, pages 1061?1067, Detroit, Michigan, 1989.
[10] Hwee Tou Ng and Raymond J. Mooney. The role of coherence in constructing and evaluating abductive
explanations. In P. O?Rorke, editor, Working Notes, AAAI Spring Symposium on Automated Abduction,
Stanford, California, March 1990.
[11] E. Ovchinnikova, N. Montazeri, T. Alexandrov, J. Hobbs, M. McCord, and R. Mulkar-Mehta. Abductive
reasoning with a large knowledge base for discourse processing. In Proceedings of the 9th International
Conference on Computational Semantics, Oxford, United Kingdom, 2011.
[12] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann,
San Francisco, CA, 1988.
[13] Harry E. Pople. On the mechanization of abductive logic. In Third International Joint Conference on Artificial
Intelligence, pages 147?152, Stanford, California, August 1973.
[14] M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62:107?136, 2006.
64
Elaborating a Knowledge Base for Deep Lexical Semantics
Niloofar Montazeri and Jerry R. Hobbs
Information Sciences Institute
University of Southern California
Marina del Rey, California
Abstract
We describe the methodology for constructing axioms defining event-related words, anchored
in core theories of change of state and causality. We first derive from WordNet senses a smaller
set of abstract, general ?supersenses?. We encode axioms for these, and we test them on textual
entailment pairs. We look at two specific examples in detail to illustrate both the power of the
method and the holes in the knowledge base that it exposes. Then we address the problem of holes
more systematically, asking, for example, what kinds of ?pairwise interactions? are possible for core
theory predicates like change and cause.1
1 Introduction
From the sentence
Russia is blocking oil from entering Ukraine.
we would like to be able to conclude
Oil can not be delivered to Ukraine.
But doing this requires fairly complex inference, because the words ?block?, ?enter?, ?can?, ?not? and
?deliver? carve up the world in different ways. Our approach is to define words such as these by means
of axioms that link with underlying core theories2 explicating such very basic concepts as change of
state and causality. Given the logical form of sentences like these two, we apply these axioms to express
the meaning of the sentences in more fundamental predicates, and do a certain amount of defeasible
reasoning in the core theories to determine that the second follows from the first.
More generally, we are engaged in an enterprise we call ?deep lexical semantics? (Hobbs, 2008), in
which we develop various core theories of fundamental commonsense phenomena and define English
word senses by means of axioms using predicates explicated in these theories. Among the core theories
are cognition, microsociology, and the structure of events. The last of these is the focus of this paper. We
use textual entailment pairs like the above to test out subsets of related axioms. This process enforces a
1This research was supported in part by the Defense Advanced Research Projects Agency (DARPA) Machine Reading
Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172, and in part by the Office of
Naval Research under contract no. N00014-09-1-1029.. Any opinions, findings, and conclusion or recommendations expressed
in this material are those of the author(s) and do not necessarily reflect the view of the DARPA, AFRL, ONR, or the US
government.
2http://www.isi.edu/ hobbs/csk.html.
195
uniformity in the way axioms are constructed, and also exposes missing inferences in the core theories.
The latter is a major issue in this paper.
In Section 2 we describe three aspects of the framework we are working in?the logical form we
use, abductive interpretation and defeasibility, and the core theories of change of state and causality. In
Section 3 we describe the methodology we use for constructing axioms, deriving from WordNet senses
a smaller set of abstract, general ?supersenses?, encoding axioms for these, and testing them on textual
entailment pairs. In Section 4 we look at two specific examples to illustrate both the power of the method
and the holes in the knowledge base that it exposes. In Section 5 we address the problem of holes more
systematically, specifically asking, for example, what kinds of ?pairwise interactions? are possible for
core theory predicates like change and cause.
2 Framework
We use a logical notation in which states and events (eventualities) are reified. Specifically, if the ex-
pression (p x) says that p is true of x, then (p? e x) says that e is the eventuality of p being true
of x. Eventuality e may exist in the real world (Rexist), in which case (p x) holds, or it may only
exist in some modal context, in which case that is expressed simply as another property of the possible
individual e. (In this paper we use a subset of Common Logic3 for the syntax of our notation.)
The logical form of a sentence is a flat conjunction of existentially quantified postive literals, with
about one literal per morpheme. (For example, logical words like ?not? and ?or? are treated as expressing
predications about possible eventualities.) We have developed software4 to translate Penn TreeBank-style
trees (as well as other syntactic formalisms) into this notation. The underlying core theories are expressed
as axioms in this notation (Hobbs, 1985).
The interpretation of a text is taken to be the lowest-cost abductive proof of the logical form of
the text, given the knowledge base. That is, to interpret a text we prove the logical form, allowing for
assumptions at cost, and pick the lowest-cost proof. Factors involved in computing costs include, besides
the number of assumptions, the salience of axioms, the plausibility of axioms expressing defeasible
knowledge, and consiliance or the degree to which the pervasive implicit redundancy of natural language
texts is exploited. We have demonstrated that many interpretation problems are solved as a by-product
of finding the lowest-cost proof. This method has been implemented in an abductive theorem-prover
called Mini-Tacitus5 that has been used in a number of applications (Hobbs et al, 1993; Mulkar et al,
2007), and is used in the textual entailment problems described here. We are also working toward a
probabilistic semantics for the cost of proofs (Blythe et al, 2011). Abductive interpretation accounts for
script-like understanding of text?a script predicate provides the most economical interpretation (Hobbs
et al, 1993)?but also enables interpretation of novel texts.
Most commonsense knowledge is defeasible, i.e., it can be defeated. This is represented in our
framework by having a unique ?et cetera? proposition in the antecedent of Horn clauses that cannot be
proved but can be assumed at a cost corresponding to the likeliehood that the conclusion is true. For
example, the axiom
(forall (x) (if (and (bird x)(etc-i x))(fly x)))
would say that if x is a bird and other unspecified conditions hold, (etc-i), then x flies. No other
axioms enable proving (etc-i x), but it can be assumed, and hence participate in the lowest cost
3http://common-logic.org/.
4http://www.rutumulkar.com/download/NL-Pipeline/NL-Pipeline.php.
5http://rutumulkar.com/download/TACITUS/tacitus.php.
196
proof. The index i is unique to this axiom. In this paper rather than invent new indices for each axiom, we
will use the abbreviation (etc) to indicate the defeasibility of the rule. (This approach to defeasibility
is similar to circumscription (McCarthy, 1980).)
We have articulated a number of core theories6. The two most relevant to this paper are the theory
of change of state and the theory of causality. The predication (change? e e1 e2) says that e is
a change of state whose initial state is e1 and whose final state is e2. The chief properties of change
are that there is some entity whose state is undergoing change, that change is defeasibly transitive,
that e1 and e2 cannot be the same unless there has been an intermediate state that is different, and that
change is consistent with the before relation from our core theory of time. Since many lexical items
focus only on the initial or the final state of a change, we introduce for convenience the predications
(changeFrom? e e1) and (changeTo? e e2), defined in terms of change.
The chief distinction in our core theory of causality is between the notions of causalComplex and
cause. A causal complex includes all the states and events that have to happen or hold in order for the
effect to happen. A cause is that contextually relevant element of the causal complex that is somehow
central to the effect, whether because it is an action the agent performs, because it is not normally true,
or for some other reason. Most of our knowledge about causality is expressed in terms of the predicate
cause, rather than in terms of causal complexes, because we rarely if ever know the complete causal
complex. Typically planning, explanation, and the interpretation of texts (though not diagnosis) involves
reasoning about cause. Among the principal properties of cause are that it is defeasibly transitive,
that events defeasibly have causes, and that cause is consistent with before.
We also have a core theory of time, and the times of states and events can be represented as temporal
properties of the reified eventualities. The theory of time has an essential function in axioms for words
explicitly referencing time, such as ?schedule? and ?delay?. But for most of the words we are explicating
in this effort, we base our approach to the dynamic aspects of the world on the cognitively more basic
theory of change of state. For example, the word ?enter? is axiomatized as a change of state from being
outside to being inside, and the fact that being outside comes before being inside follows from the axiom
relating the predicates change and before.
We find that reifying states and events as eventualities and treating them as first-class individuals is
preferable to employing the event calculus (Gruninger and Menzel, 2010; Mueller, 2006) which makes a
sharp distinction between the two, because language makes no distinction in where they can appear and
we can give them a uniform treatment.
3 Methodology
Our methodology consists of three steps.
1. Analyzing the structure of a word?s WordNet senses.
2. Writing axioms for the most general senses
3. Testing the axioms on textual entailment pairs.
Our focus in this paper is on words involving the concepts of change of state and causality, or event
words, such as ?block?, ?delay?, ?deliver?, ?destroy?, ?enter?, ?escape?, ?give?, ?hit?, ?manage?, and
?provide?. For each word, we analyze the structure of its WordNet senses. Typically, there will be pairs
that differ only in, for example, constraints on their arguments or in that one is inchoative and the other
6http://www.isi.edu/ hobbs/csk.html.
197
causative. This analysis generally leads to a radial structure indicating how one sense leads by incre-
ments, logically and perhaps chronologically, to another word sense (Lakoff, 1987). The analysis also
leads us to posit ?supersenses? that cover two or more WordNet senses. (Frequently, these supersenses
correspond to senses in FrameNet (Baker et al, 2003) or VerbNet (Kipper et al, 2006), which tend to be
coarser grained; sometimes the desired senses are in WordNet itself.)
For example, for the verb ?enter?, three WordNet senses involve a change into a state:
V2: become a participant
V4: play a part in
V9: set out on an enterprise
Call this supersense S1. Two other senses add a causal role to this:
V5: make a record of
V8: put or introduce into something
Two more senses specialize supersense S1 by restricting the target state to be in a physical location:
V1: come or go into
V6: come on stage
One other sense specializes S1 by restricting the target state to be membership in a group.
V3: register formally as a participant or member
Knowing this radial structure of the senses helps enforce uniformity in the construction of the axioms. If
the senses are close, their axioms should be almost the same.
We are currently only constructing axioms for the most general or abstract senses or supersenses.
In this way, although we are missing some of the implications of the more specialized senses, we are
capturing the most basic topological structure in the meanings of the words. Moreover, the specialized
senses usually tap into some specialized domain that needs to be axiomatized before the axioms for these
senses can be written.
In constructing the axioms in the event domain, we are very much informed by the long tradition of
work on lexical decomposition in linguistics (e.g., Gruber, 1965; Jackendoff, 1972). Our work differs
from this in that our decompositions are done as logical inferences and not as tree transformations as in
the earliest linguistic work, they are not obligatory but only inferences that may or may not be part of the
lowest-cost abductive proof, and the ?primitives? into which we decompose the words are explicated in
theories that enable reasoning about the concepts.
Figure 1 shows the radial structure of the senses for the word ?enter?, together with the axioms that
characterize each sense. A link between two word senses means an incremental change in the axiom for
one gives the axiom for the other. For example, the axiom for enter-S2 says that if x1 enters x2 in
x3, then x1 causes a change to the eventuality i1 in which x2 is in x3; and the expanded axiom for
enter-S1.1 states that if x1 enters x2, then there is a change to a state e1 in which x1 is in x2. So
enter-S2 and enter-S1.1 are closely related and thus linked together.
Abstraction is a special incremental change where one sense S1.1 specializes another sense S1 ei-
ther by adding more predicates to or specializing some of the predicates in S1?s axiom. We represent
abstractions via arrows pointing from the subsenses to the supersenses. In Figure 1, enter-S1.1 and
enter-S1.2 both specialize enter-S1. The predicate enter-S1.1 adds an extra predicate de-
scribing e1 as an in eventuality and enter-S1.2 specializes e1 to membership in x2, where x2 is a
group.
198
Figure 1: Senses of and axioms for the verb ?enter?
The supersenses capture the basic topology of the senses they subsume. The extra information that
the subsenses convey are typically the types and properties of the arguments, such as being a place or a
process, or qualities of the causing event, such as being sudden or forceful.
For each set of inferentially related words we construct textual entailment pairs, where the hypothesis
(H) intuitively follows from text (T), and use these for testing and evaluation. The person writing the
axioms does not know what the pairs are, and the person constructing the pairs does not know what the
axioms look like.
The ideal test then is whether given a knowledge base K consisting of all the axioms, H cannot be
proven from K alone, but H can be proven from the union of K and the best intepretation of T. This is
often too stringent a condition, since H may contain irrelevant material that doesn?t follow from T, so an
alternative is to determine whether the lowest cost abductive proof of H given K plus T is substantially
lower than the lowest cost abductive proof of H given K alone, where ?substantially lower? is defined by
a threshold that can be trained (Ovchinnikova et al, 2011).
4 Two Examples
Here we work through two examples to illustrate how textual entailment problems are handled in our
framework. In these examples, given a text T and a hypothesis H, we ask if H can be proven from T,
perhaps with a small number of low-cost assumptions.
Because the examples we deal with involve a great deal of embedding, we need to use the primed
predicates, keeping the eventuality arguments explicit.
We also assume in these examples that lexical disambiguation has been done correctly. With more
context, lexical disambiguation should fall out of the best interpretation, but it is unreasonable to ex-
pect that in these short examples. In practice we run the examples both with disambiguated and with
nondisambiguated predicates.
In these examples we do not show the costs, although they are used by our system.
The first example is the pair
T: Russia is blocking oil from entering Ukraine.
H: Oil cannot be delivered to Ukraine.
199
The relevant part of the logical form of the text is
(and (block-V3? b1 x1 e1)(enter-S2? e1 o1 u1))
That is, there is a blocking event b1 in which Russia x1 blocks eventuality e1 from occurring, and e1 is
the eventuality of oil o1 entering Ukraine u1. The -V3 on block indicates that it is the third WordNet
sense of the verb ?block? and the -S2 suffix on enter indicates that it is the second supersense of
?enter?.
The relevant part of the logical form of the hypothesis is
(and (not? n2 c2) (can-S1? c2 x2 d2) (deliver-S2? d2 x2 o2 u2))
That is, n2 is the eventuality that c2 is not the case, where c2 is some x2?s being able to do d2, where
d2 is x2?s delivering oil o2 to Ukraine u2. Note that we don?t know yet that the oil and Ukraine in the
two sentences are coreferential.
The axiom relating the third verb sense of ?block? to the underlying core theories is
AX4: (forall (c1 x1 e1)
(if (block-V3? c1 x1 e1)
(exist (n1 p1)
(and (cause? c1 x1 n1)(not? n1 p1)(possible? p1 e1)))))
This rule says that for x1 to block some eventuality e1 is for x1 to cause e1 not to be possible. (In this
example, for expositional simplicity, we have allowed the eventuality c1 of blocking be the same as the
eventuality of causing, where properly they should be closely related but not identical.)
The other axioms needed in this example are
AX1: (forall (c1 e1)
(if (and (possible? c1 e1)(etc))
(exist (x1)(can-S1? c1 x1 e1))))
AX2: (forall (d1 x1 c1 r1 x2 x3)
(if (and (cause? d1 x1 c1)(changeTo? c1 r1)(rel? r1 x2 x3)
(deliver-S2? d1 x1 x2 x3))))
AX3: (forall (c1 x1 x2)
(if (enter-S2? c1 x1 x2)
(exist (i1)(and changeTo? c1 i1)(in? i1 x1 x2))))
AX1 says that defeasibly, if an eventuality e1 is possible, then someone can do it. AX2 says that if x1
causes a change to a situation r1 in which x2 in in some relation to x3, then in a very general sense
(S2), x1 has delivered x2 to x3. AX3 says that if c1 is the eventuality of x1 entering x2, then c1 is
the change into a state i1 in which x1 is in x2.
Starting with the logical form of H as the initial interpretation and applying axioms AX1 and AX2,
we get interpretation H1:
H1: (and (not? n2 c2) (possible? c2 d2) (cause? d2 x2 c1)
(changeTo? c1 r1)(rel? r1 o2 u2))
At this point we are stuck in our effort to back-chain to T. An axiom is missing, namely, one that says
that ?in? is a relation between two entities.
AX5: (forall (r x1 x2) (if (in? r1 x1 x2)(rel? r1 x1 x2)))
Using AX5, we can back-chain from H1 and derive interpretation H2:
H2: (and (not? n2 c2)(possible? c2 d2)(cause? d2 x2 c1)
(changeTo? c1 r1)(in? r1 o2 u2))
We can then further back-chain with AX3 to interpretation H3:
200
H3: (and (not? n2 c2)(possible? c2 d2)(cause? d2 x2 c1)
(enter-S2? c1 o2 u2))
Again, we need a missing axiom, AX6, to get closer to the logical form of T:
AX6: (forall (p e1)
(if (and (possible? p,e1)(etc))
(exist (c x1) (and (possible? p c)(cause? c x1 e1)))))
That is, if something is possible, it is possible for something to cause it. Using this axiom, we can derive
H4: (and (not? n2 c2)(possible? c2 c1)(enter-S2? c1 o2 u2))
The final missing axiom, AX7, says that if x1 causes eventuality c2 not to occur, then c2 doesn?t occur.
AX7: (forall (n x1 n1 c2)
(if (and (cause? n x1 n1)(not? n1 c2))( not? n c2)))
Using this we derive interpretation H5.
H5: (and (cause? n2 x3 n)(not? n c2)(possible? c2 c1)(enter-S2? c1 o2 u2))
We can now apply the rule for ?block?, identifying b1 and n2, x1 and x3, e1 and c1, o1 and o2, and
u1 and u2, yielding H6 and establishing the entailment relation between H and T.
H6: (and (block-V3? n2 x3 c1)(enter-S2? c1 o2 u2))
Our second example is the text-hypothesis pair
T: The plane managed to escape the attack.
H: The plane was not captured.
The relevant parts of the logical forms of T and H are as follows:
T: (and (manage-V1? m1 p1 e1)(escape-S1? e1 p1 a1))
H: (and (not? n2 c2)(capture-S1? c2 x2 p2))
The axioms relating these words to the core theories are as follows:
AX1: (forall (cp c x2 n chf a y1 x3 y0 x2)
(if (and (changeTo? cp c)(cause? c x2 n)(not? n chf)
(changeFrom? chf a)(at? a y1 x3)(arg? y0 x2))
(capture? cp y0 y1)))
AX2: (forall (es x0 x1)
(if (escape? es x0 x1)
(exist (ch a)
(and (cause? es x0 ch)(changeFrom? ch a)(at? a x0 x1)))))
AX3: (forall (m y0 e1)
(if (manage? m y0 e1) (Rexist (m e1))))
201
The first says that a change to a situation in which x2 is causing y1 not to change location is a capturing
by some y0 of y1. The second says that escaping implies causing a change from being at a location.
The third says that if you manage to do e1, then e1 occurs.
Using these axioms, we would like to establish the entailment relation from T to H. However, in
order for this reasoning to go through, we need several more axioms?saying that if an eventuality does
not hold, there has been no change to that eventuality, and nothing has caused it to occur; that double
negation cancels out; and that if something is caused, it occurs.
It may seem at first blush that any new text-hypothesis pair will reveal new axioms that must be
encoded, and that therefore it is hopeless ever to achieve completeness in the theories. But a closer
examination reveals that the missing axioms all involve relations among the most fundamental predicates,
like cause, change, not, and possible. These are axioms that should be a part of the core theories
of change and causality. They are not a random collection of facts, any one of which may turn out to
be necessary for any given example. Rather we can investigate the possibilities systematically. That
investigation is what we describe in the following section.
5 Relations among Fundamental Predicates
For completeness in the core theories, we need to look at pairs of fundamental predicates and ask what re-
lations hold between them, what their composition yields, and for each such axiom whether it is defeasi-
ble or indefeasible. The predicates we consider are possible, Rexist, not, cause, changeFrom,
and changeTo.
The first type of axiom formulates the relationship between two predicates. For example, the rule
relating cause and Rexist is
(forall (x e) (if (cause x e)(Rexist e)))
That is, if something is caused, then it actually occurs. Other rules of this type are as follows:
(forall (x e) (if (Rexist e)(possible e)))
(forall (e) (if (and (Rexist e)(etc))(exist (x)(cause x e))))
(forall (e2)
(if (changeTo e2)
(exist (e1)(and (changeFrom e1)(not? e1 e2)))))
(forall (e1)
(if (changeFrom e1)
(exist (e2)(and (changeTo e2)(not? e2 e1)))))
(forall (e) (if (changeTo e)(Rexist e)))
(forall (e) (if (changeFrom e)(not e)))
(forall (e) (if (and (Rexist e)(etc))(changeTo e)))
That is, if something occurs, it is possible and, defeasibly, something causes it. If there is a change to
some state obtaining, then there is a change from its not obtaining, and vice versa. If there is a change
to something, then it obtains, and if there is a change from something, then it no longer obtains. If some
state obtains, then defeasibly there was a change from something else to that state obtaining.
The second type of axiom involves the composition of predicates, and gives us rules of the form
202
(forall (e1 e2 x) (if (and (p? e1 e2)(q? e2 x)) (r? e1 x)))
That is, when p is applied to q, what relation r do we get?
Figure 2 shows the axioms encoding these compositions. The rows correspond to the (p? e1
e2)?s and the columns correspond to the (q? e2 x)?s, and the cell contains the consequents (r? e1
x). If the rule is defeasible, the cell indicates that by adding (etc) to the antecedent. The consequents
in italics are derivable from other rules.
Figure 2: Axioms expressing compositions of fundamental predicates
For example, in the possible-possible cell, the rule says that if it is possible that something
is possible, then it is possible. To take a more complex example, the changeFrom-cause cell says
that if there is a change from some entity causing (or maintaining) a state, then defeasibly there will be a
change from that state. So if a glass is released, it will fall.
We have also looked at axioms whose pattern is the converse of those in Figure 2. For example, if
something does not hold, then it was not caused. Many of the axioms used in the examples are of this
sort.
6 Conclusion
If we are ever to have sophisticated natural language understanding, our systems will have to be able to
draw inferences like the ones illustrated here, and therefore they will need axioms of this complexity or
something equivalent. Because of their complexity, we cannot expect to be able to acquire the axioms
automatically by statistical methods. But that does not mean the situation is bleak. We have shown in
this paper that there is a systematic methodology for developing axioms characterizing the meanings of
words in a way that enforces uniformity and for elaborating the core theories these axioms are anchored
in. Doing this for several thousand of the most common words in English would produce a huge gain in
the inferential power of our systems, as illustrated by the textual entailment examples in this paper, and
would be an enterprise no greater in scope than the manual construction of other widely used resources
such as WordNet and FrameNet.
203
References
[1] Baker, C., Fillmore, C., Cronin, B.: The Structure of the Framenet Database, International Journal of Lexi-
cography, Volume 16.3: (2003) 281-296.
[2] J. Blythe, J. Hobbs, P. Domingos, R. Kate, and R. Mooney, 2011. ?Implementing Weighted Abduction in
Markov Logic?, Proceedings of the 9th International Conference on Computational Semantics, Oxford,
United Kingdom.
[3] Gruber, Jeffrey C., 1965. Studies in Lexical Relations, unpublished Ph.D. dissertation, Massachusetts Institute
of Technology, Cambridge, Massachusetts.
[4] Gruninger, Michael, and Christopher Menzel, 2010. ?The Process Specification Language (PSL) Theory and
Applications?, AI Magazine, Vol 24, No 3.
[5] Hobbs, Jerry R. 1985. ?Ontological Promiscuity.? Proceedings, 23rd Annual Meeting of the Association for
Computational Linguistics, pp. 61-69. Chicago, Illinois, July 1985.
[6] Hobbs, Jerry R., 2008. ?Deep Lexical Semantics?, Proceedings, 9th International Conference on Intelligent
Text Processing and Computational Linguistics (CICLing-2008), Haifa, Israel, February 2008.
[7] Hobbs, Jerry R., Mark Stickel, Douglas Appelt, and Paul Martin, 1993. ?Interpretation as Abduction?, Arti-
ficial Intelligence, Vol. 63, Nos. 1-2, pp. 69-142.
[8] Jackendoff, Ray S. 1972. Semantic interpretation in generative grammar. Cambridge, MA: The MIT Press.
[9] Kipper, Karin, Anna Korhonen, Neville Ryant, and Martha Palmer (2006) Extensive Classifications of En-
glish verbs. Proceedings, 12th EURALEX International Congress. Turin, Italy. September, 2006.
[10] Lakoff, George, 1987. Women, Fire, and Dangerous Things: What Categories Reveal About the Mind, Uni-
versity of Chicago Press, Chicago.
[11] McCarthy, John, 1980. ?Circumscription: A Form of Nonmonotonic Reasoning?, Artificial Intelligence, 13:
27-39.
[12] Mueller, Erik T., 2006. Commonsense Reasoning, Morgan Kaufmann Publishers, Inc., San Mateo, California.
[13] R. Mulkar, J.R. Hobbs, and E. Hovy, 2007. ?Learning from Reading Syntactically Complex Biology Texts?,
Proceedings of the AAAI Spring Symposium Commonsense?07. Stanford University, CA, 2007.
[14] E. Ovchinnikova, N. Montazeri, T. Alexandrov, J. Hobbs, M. McCord, and R. Mulkar-Mehta, 2011. ?Abduc-
tive Reasoning with a Large Knowledge Base for Discourse Processing?, Proceedings of the 9th International
Conference on Computational Semantics, Oxford, United Kingdom.
204
Abductive Reasoning with a Large Knowledge Base
for Discourse Processing
Ekaterina Ovchinnikova
University of Osnabru?ck
eovchinn@uos.de
Niloofar Montazeri
USC ISI
niloofar@isi.edu
Theodore Alexandrov
University of Bremen
theodore@uni-bremen.de
Jerry R. Hobbs
USC ISI
hobbs@isi.edu
Michael C. McCord
IBM Research
mcmccord@us.ibm.com
Rutu Mulkar-Mehta
USC ISI
me@rutumulkar.com
Abstract
This paper presents a discourse processing framework based on weighted abduction. We elabo-
rate on ideas described in Hobbs et al (1993) and implement the abductive inference procedure in a
system called Mini-TACITUS. Particular attention is paid to constructing a large and reliable knowl-
edge base for supporting inferences. For this purpose we exploit such lexical-semantic resources as
WordNet and FrameNet. We test the proposed procedure and the obtained knowledge base on the
Recognizing Textual Entailment task using the data sets from the RTE-2 challenge for evaluation. In
addition, we provide an evaluation of the semantic role labeling produced by the system taking the
Frame-Annotated Corpus for Textual Entailment as a gold standard.
1 Introduction
In this paper, we elaborate on a semantic processing framework based on a mode of inference called
abduction, or inference to the best explanation. In logics, abduction is a kind of inference which arrives
at an explanatory hypothesis given an observation. Hobbs et al (1993) describe how abductive reasoning
can be applied to the discourse processing problem viewing the process of interpreting sentences in
discourse as the process of providing the best explanation of why the sentence would be true. In this
framework, interpreting a sentence means 1) proving its logical form, 2) merging redundancies where
possible, and 3) making assumptions where necessary. As the reader will see later in this paper, abductive
reasoning as a discourse processing technique helps to solve many pragmatic problems such as reference
resolution, the interpretation of noun compounds, the resolution of some kinds of syntactic, and semantic
ambiguity as a by-product. We adopt this approach. Specifically, we use a system we have built called
Mini-TACITUS1 (Mulkar et al, 2007) that provides the expressivity of logical inference but also allows
probabilistic, fuzzy, or defeasible inference and includes measures of the ?goodness? of abductive proofs
and hence of interpretations of texts and other situations.
The success of a discourse processing system based on inferences heavily depends on a knowledge
base. The main contribution of this paper is in showing how a large and reliable knowledge base can be
obtained by exploiting existing lexical semantic resources and can be successfully applied to reasoning
tasks on a large scale. In particular, we experiment with axioms extracted from WordNet, see Fellbaum
(1998), and FrameNet, see Ruppenhofer et al (2006). In axiomatizing FrameNet we rely on the study
described in Ovchinnikova et al (2010).
We evaluate our inference system and the obtained knowledge base in recognizing textual entailment
(RTE). As the reader will see in the following sections, inferences carried out by Mini-TACITUS are
fairly general and not tuned for a particular application. We decided to test our approach on RTE because
this is a well-defined task that captures major semantic inference needs across many natural language
1http://www.rutumulkar.com/download/TACITUS/tacitus.php
225
processing applications, such as question answering, information retrieval, information extraction, and
document summarization. For evaluation, we have chosen the RTE-2 data set (Bar-Haim et al, 2006),
because besides providing text-hypothesis pairs and a gold standard this data set has been annotated with
FrameNet frame and role labels (Burchardt and Pennacchiotti, 2008) which gives us the possibility of
evaluating our frame and role labeling based on the axioms extracted from FrameNet.
2 NL Pipeline and Abductive Reasoning
Our natural language pipeline produces interpretations of texts given the appropriate knowledge base. A
text is first input to the English Slot Grammar (ESG) parser (McCord, 1990, 2010). For each segment,
the parse produced by ESG is a dependency tree that shows both surface and deep structure. The deep
structure is exhibited via a word sense predication for each node, with logical arguments. These logical
predications form a good start on a logical form (LF) for the whole segment. An add-on to ESG converts
the parse tree into a LF in the style of Hobbs (1985). The LF is a conjunction of predications, which have
generalized entity arguments that can be used for showing relationships among the predications. These
LFs are used by the downstream components.
The interpretation of the text is carried out by an inference system called Mini-TACITUS using
weighted abduction as described in detail in Hobbs et al (1993). Mini-TACITUS tries to prove the logical
form of the text, allowing assumptions where necessary. Where the system is able to prove parts of the
LF, it is anchoring it in what is already known from the overall discourse or from a knowledge base.
Where assumptions are necessary, it is gaining new information. Obviously, there are many possible
proofs in this procedure. A cost function on proofs enables the system to chose the ?best? (the cheapest)
interpretation. The key factors involved in assigning a cost are the following: 1) proofs with fewer
assumptions are favored, 2) short proofs are favored over long ones, 3) plausible axioms are favored over
less plausible axioms, and 4) proofs are favored that exploit the inherent implicit redundancy in text.
Let us illustrate the procedure with a simple example. Suppose that we want to construct the best
interpretation of the sentence John composed a sonata. As a by-product, the procedure will disambiguate
between two readings of compose, namely between the ?form? reading instantiated for example in the
sentence Three representatives composed a committee, and the ?create art? meaning instantiated in the
given sentence. After being processed by the parser, the sentence will be assigned the following logical
form where the numbers (20) after every proposition correspond to the default costs of these proposi-
tions.2 The total cost of this logical form is equal to 60.
John(x1):20 & compose(e1,x1,x2):20 & sonata(x2):20
Suppose our knowledge base contains the following axioms:
1) form(e0,x1,x2):90 ? compose(e0,x1,x2)
2) create art(e0,x1,x2):50 & art piece(x2):40 ? compose(e0,x1,x2)
3) art piece(x1):90 ? sonata(x1)
Unlike deductive axioms, abductive axioms should be read ?right to left?. Thus, the propositions on
the right hand side (compose, sonata) correspond to an input, whereas the left hand side propositions
will be assumed given the input. The number assigned to each proposition on the left hand side shows
what percentage of the total input cost the assumption of this proposition will cost.3 For example, if the
proposition compose costs 20 then the assumption of form will cost 18.
Two interpretations can be constructed for the given logical form. The first one is the result of the
application of axioms 1 and 3. Note that the costs of the backchained propositions (compose, sonata) are
2The actual value of the default costs of the input propositions does not matter, because, as the reader will see in this section,
the axiom weights which affect the costs of the resulting interpretations are given as percentages of the input proposition costs.
The only heuristic we use here concerns setting all costs of the input propositions to be equal (all propositions cost 20 in the
discussed example). This heuristic needs a further investigation to be approved or modified.
3The axiom weights in the given example are arbitrary.
226
set to 0, because their costs are now carried by the newly introduces assumptions (form, art piece). The
total cost of the first interpretation I1 is equal to 56.
I1: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 & form(e1,x1,x2):18 & art piece(x2):18
The second interpretation is constructed in two steps. First, axioms 2 and 3 are applied as follows.
I21: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 &
create art(e1,x1,x2):10 & art piece(x2):8 & art piece(x2):18
The total cost of I21 is equal to 56. This interpretation is redundant, because it contains the propo-
sition art piece twice. The procedure will merge propositions with the same predicate, setting the cor-
responding arguments of these propositions to be equal and assigning the minimum of the costs to the
result of merging. The idea behind such mergings is that if an assumption has already been made then
there is no need to make it again. The final form of the second interpretation I22 with the cost of 38
is as follows. The ?create art? meaning of compose has been brought forward because of the implicit
redundancy in the sentence which facilitated the disambiguation.
I22: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 & create art(e1,x1,x2):10 &
art piece(x2):8
Thus, on each reasoning step the procedure 1) applies axioms to propositions with non-zero costs
and 2) merges propositions with the same predicate, assigning the lowest cost to the result of merging.
Reasoning terminates when no more axioms can be applied.4 The procedure favors the cheapest inter-
pretations. Among them, the shortest proofs are favored, i.e. if two interpretations have the same cost
then the one which has been constructed with fewer axiom application steps is considered to be ?better?.
It is easy to see that changing weights of axioms can crucially influence the reasoning process. Axiom
weights can help to propagate more frequent and reliable inferences and to distinguish between ?real?
abduction and deduction. For example, an axiom backchaining from dog to animal should in the general
case have a weight below 100, because it is cheap to assume that there is an animal if there is a dog; it is
a reliable deduction. On the contrary, assuming dog given animal should have a weight above 100.
In order to avoid undesirable mergings, we introduce non-merge constraints. For example, in the
sentence John reads a book and Bill reads a book the two read propositions should not be merged
because they refer to different actions. This is ensured by the following non-merge constraint: if not all
arguments of two propositions (which are not nouns) with the same predicate can be merged, then these
propositions cannot be merged. The constraint implies that in the sentence above two read propositions
cannot be merged, because John being the first argument of the first read cannot be merged with Bill.5
This constraint is a heuristic; it corresponds to the intuition that it is unlikely that the same noun refers to
different objects in a short discourse, while for other parts of speech it is possible. An additional corpus
study is needed in order to prove or disprove it.
The described procedure provides solutions to a whole range of natural language pragmatics prob-
lems, such as resolving ambiguity, discovering implicit relations in nouns compounds, prepositional
phrases, or discourse structure. Moreover, this account of interpretation solves the problem of where to
stop drawing inferences, which could easily be unlimited in number; an inference is appropriate if it is
part of the lowest-cost proof of the logical form.
Adapting Mini-TACITUS to a Large-Scale Knowledge Base
Mini-TACITUS (Mulkar et al, 2007) began as a simple backchaining theorem-prover intended to be a
more transparent version of the original TACITUS system, which was based on Stickel?s PTTP system
(Stickel, 1988). Originally, Mini-TACITUS was not designed for treating large amounts of data. A clear
and clean reasoning procedure rather than efficiency was in the focus of its developers. In order to make
the system work with the large-scale knowledge base, we had to perform several optimization steps and
add a couple of new features.
4In practice, we use the depth parameter d and do not allow an inference chain with more that d steps.
5Recall that only propositions with the same predicate can be merged, therefore John and Bill cannot be merged.
227
For avoiding the reasoning complexity problem, we have introduced two parameters. The time pa-
rameter t is used to restrict the processing time. After the processing time exceeds t the reasoning
terminates and the best interpretation so far is output. The time parameter ensures that an interpretation
will be always returned by the procedure even if reasoning could not be completed in a reasonable time.
The depth parameter d restricts the depth of the inference chain. Suppose that a proposition p occurring
in the input has been backchained and a proposition p? has been introduced as a result. Then, p? will be
backchained and so on. The number of such iterations cannot exceed d. The depth parameter reduces
the number of reasoning steps.
Since Mini-TACITUS processing time increases exponentially with the input size (sentence length
and number of axioms), making such a large set of axioms work was an additional issue. For speeding
up reasoning it was necessary to reduce both the number of the input propositions and the number of
axioms. In order to reduce the number of axioms, a two-step reduction of the axiom set is performed.
First, only the axioms which could be evoked by the input propositions or as a result of backchaining
from the input are selected for each reasoning task. Second, the axioms which could never lead to any
merging are filtered out. Concerning the input propositions, those which could never be merged with the
others (even after backchaining) are excluded from the reasoning process.
3 Knowledge Base
As described in the previous section, the Mini-TACITUS inferences are based on a knowledge base (KB)
consisting of a set of axioms. In order to obtain a reliable KBwith a sufficient coverage we have exploited
existing lexical-semantic resources.
First, we have extracted axioms from WordNet (Fellbaum, 1998), version 3.0, which has already
proved itself to be useful in knowledge-intensive NLP applications. The central entity in WordNet is
called a synset. Synsets correspond to word senses, so that every lexeme can participate in several
synsets. For every word sense, WordNet indicates the frequency of this particular word sense in the
WordNet annotated corpora. We have used the lexeme-synset mapping for generating axioms, with the
corresponding frequencies of word senses converted into the axiom weights. For example, in the axioms
below, the verb compose is mapped to its sense 2 in WordNet which participates in synset-X.
compose-2(e1,x1,x2):80 ? compose(e1,x1,x2)
synset-X(e0,e1):100 ? compose-2(e1,x1,x2)
Moreover, we have converted the following WordNet relations defined on synsets into axioms: hy-
pernymy, instantiation, entailment, similarity, meronymy. Hypernymy and instantiation relations pre-
suppose that the related synsets refer to the same entity (the first axiom below), whereas other types of
relations relate synsets referring to different entities (the second axiom below). All axioms based on
WordNet relations have the weights equal to 100.
synset-1(e0,e1):100 ? synset-2(e0,e1)
synset-1(e0,e1):100 ? synset-2(e2,e3)
WordNet alo provides morphosemantic relations which relate verbs and nouns, e.g., buy-buyer.
WordNet distinguishes between 14 types of such relations.We use relation types in order to define the
direction of the entailment and map the arguments. For example, the ?agent? relation (buy-buyer) stands
for a bi-directional entailment such that the noun is the first (agentive) argument of the verb:
buy-1(e0,x1,x2):100 ? buyer-1(x1)
buyer-1(x1):100 ? buy-1(e0,x1,x2)
Additionally, we have exploited the WordNet synset definitions. In WordNet the definitions are given
in natural language form. We have used the extended WordNet resource6 which provides logical forms
for the definition in WordNet version 2.0. We have adapted logical forms from extended WordNet to our
6http://xwn.hlt.utdallas.edu/
228
representation format and converted them into axioms; for example the following axiom represents the
meaning of the synset containing such lexemes as horseback. These axioms have the total weight of 100.
on(e2,e1,x2):25 & back(e3,x2):25 & of (e4,x2,x1):25 & horse(e5,x1):25 ? synset-X(e0,x0)
The second resource which we have used as a source of axioms is FrameNet, release 1.5, see Rup-
penhofer et al (2006). FrameNet has a shorter history in NLP applications thanWordNet, but lately more
and more researchers have been demonstrating its potential to improve the quality of question answering
(Shen and Lapata, 2007) and recognizing textual entailment (Burchardt et al, 2009). The lexical mean-
ing of predicates in FrameNet is represented in terms of frames which describe prototypical situations
spoken about in natural language. Every frame contains a set of roles corresponding to the participants of
the described situation. Predicates with similar semantics are assigned to the same frame; e.g. both give
and hand over refer to the GIVING frame. For most of the lexical elements FrameNet provides syntactic
patterns showing the surface realization of these lexical elements and their arguments. Syntactic patterns
also contain information about their frequency in the FrameNet annotated corpora. We have used the
patterns and the frequencies for deriving axioms such as for example the following.
GIVING(e1,x1,x2,x3):70 & DONOR(e1,x1):0 & RECIPIENT(e1,x2):0 & THEME(e1,x3):0 ?
give(e1,x1,x3) & to(e2,e1,x2)
HIRING(e1,x1,x3):90 & EMPLOYER(e1,x1) & EMPLOYEE(e1,x3) ?
give(e1,x1,x2,x3):10 & job(x2)
The first pattern above corresponds to the phrases like John gave a book to Mary and the second ?
less frequent ? to phrases like John gave Mary a job. It is interesting to note that application of such
axioms provides a solution to the problem of semantic role labeling as a by-product. As in the statis-
tical approaches, more frequent patterns will be favored. Moreover, patterns helping to detect implicit
redundancy will be brought forward.
FrameNet alo introduces semantic relations defined on frames such as inheritance, causation or
precedence; for example the GIVING and GETTING frames are connected with the causation relation.
Roles of the connected frames are also linked, e.g. DONOR in GIVING is linked with SOURCE in GETTING.
Frame relations have no formal semantics in FrameNet. In order to generate corresponding axioms, we
have used the previous work on axiomatizing frame relations and extracting new relations from corpora
(Ovchinnikova et al, 2010). Weights of the axioms derived from frame relations depend on corpus-based
similarity of the lexical items assigned to the corresponding frames. An example of an axiomatized
relation is given below.7
GIVING(e0,x1,x2,x3):120 & DONOR(e0,x1):0 & RECIPIENT(e0,x2):0 & THEME(e0,x3):0 &
causes(e0,e1):0 ? GETTING(e1,x2,x3,x1) & SOURCE(e1,x1) & RECIPIENT(e1,x2) & THEME(e1,x3)
Both WordNet and FrameNet are manually created resources which ensures a relatively high quality
of the resulting axioms as well as the possibility of exploiting the linguistic information provided for
structuring the axioms. Although manual creation of resources is a very time-consuming task, WordNet
and FrameNet, being long-term projects, have an extensive coverage of English vocabulary. The cover-
age of WordNet is currently larger than that of FrameNet (155 000 vs. 12 000 lexemes). However, the
fact that FrameNet introduces complex argument structures (roles) for frames and provides mappings of
these structures makes FrameNet especially valuable for reasoning.
The complete list of axioms we have extracted from these resources is given in table 1.
4 Recognizing Textual Entailment
As the reader can see from the previous sections, the discourse processing procedure we have presented
is fairly general and not tuned for any particular type of inferences. We have evaluated the procedure and
7The ?causes? predicate is supposed to be linked to an underlying causation theory, see for example
http://www.isi.edu/?hobbs/bgt-cause.text. However, in the described experimental settings we have left the abstract theories
out and evaluated only the axioms extracted from the lexical-semantic resources.
229
Table 1: Statistics for extracted axioms
Axiom type Source Numb. of axioms
Lexeme-synset mappings WN 3.0 422,000
Lexeme-synset mappings WN 2.0 406,000
Synset relations WN 3.0 141,000
Derivational relations WN 3.0 (annotated) 35,000
Synset definitions WN 2.0 (parsed, annotated) 120,500
Lexeme-frame mappings FN 1.5 50,000
Frame relations FN 1.5 + corpora 6,000
the KB derived from WordNet and FrameNet on the Recognizing Textual Entailment (RTE) task, which
is a generic task that seems to capture major semantic inference needs across many natural language
processing applications. In this task, the system is given a text and a hypothesis and must decide whether
the hypothesis is entailed by the text plus commonsense knowledge.
Our approach is to interpret both the text and the hypothesis using Mini-TACITUS, and then see
whether adding information derived from the text to the knowledge base will reduce the cost of the best
abductive proof of the hypothesis as compared to using the original knowledge base only. If the cost
reduction exceeds a threshold determined from a training set, then we predict entailment.
A simple example would be the text John gave a book to Mary and the hypothesis Mary got a book.
Our pipeline constructs the following logical forms for these two sentences.
T: John(x1):20 & give(e1,x1,x2):20 & book(x3):20 & to(e2,e1,x3):20 & Mary(x3):20
H: Mary(x1):20 & get(e1,x1,x2):20 & book(x2):20
These logical forms constitute the Mini-TACITUS input. Mini-TACITUS applies the axioms from
the knowledge base to the input logical forms in order to reduce the overall cost of the interpretations.
Suppose that we have three FrameNet axioms in our knowledge base. The first one maps give to to the
GIVING frame, the second one maps get to GETTING and the third one relates GIVING and GETTING with
the causation relation. The first two axioms have the weights of 90 and the third 120. As a result of the
application of the axioms the following best interpretations will be constructed for T and H.
I(T): John(x1):20 & give(e1,x1,x2):0 & book(x3):20 & to(e2,e1,x3):0 & Mary(x3):20 &
GIVING(e0,x1,x2,x3):18
I(H): Mary(x1):20 & get(e1,x1,x2):0 & book(x2):20 & GETTING(e0,x1,x2):18
The total cost of the best interpretation for H is equal to 58. Now the best interpretation of T will
be added to H with the zero costs (as if T has been totally proven) and we will try to prove H once
again. First of all, merging of the propositions with the same names will result in reducing costs of the
propositions Mary and book to 0, because they occur in T:
I(T+H): John(x1):0 & give(e1,x1,x2):0 & book(x3):0 & to(e2,e1,x3):0 & Mary(x3):0 &
GIVING(e0,x1,x2,x3):0 & get(e1,x1,x2):0 & GETTING(e0,x1,x2):18
The only proposition left to be proved is GETTING. Using the GETTING-GIVING relation as described
in the previous section, this proposition can be backchained on to GIVING which will merge with GIVING
coming from the T sentence. H appears to be proven completely with respect to T; the total cost of its
best interpretation given T is equal to 0. Thus, using knowledge from T helped to reduce the cost of the
best interpretation of H from 58 to 0.
The approach presented does not have any special account for logical connectors such as if, not, or
etc. Given a text If A then B and a hypothesis A and B our procedure will most likely predict entailment.
At the moment our RTE procedure mainly accounts for the informational content of texts, being able to
detect the ?aboutness? overlap of T and H. In our framework, a fuller treatment of the logical structure
230
of the natural language would presuppose a more complicated strategy of merging redundancies.
5 Evaluation Results
We have evaluated our procedure on the RTE-2 dataset 8, see Bar-Haim et al (2006) . The RTE-2
dataset contains the development and the test set, both including 800 text-hypothesis pairs. Each dataset
consists of four subsets, which correspond to typical success and failure settings in different applications:
information extraction (IE), information retrieval (IR), question answering (QA), and summarization
(SUM). In total, 200 pairs were collected for each application in each dataset.
As a baseline we have processed the datasets with an empty knowledge base. Then we have done 2
runs, first, using axioms extracted fromWordNet 3.0 plus FrameNet, and, second, using axioms extracted
from the WordNet 2.0 definitions. In both runs the depth parameter was set to 3. The development
set was used to train the threshold as described in the previous section.9 Table 2 contains results of
our experiments.10 Accuracy was calculated as the percentage of pairs correctly judged. The results
suggest that the proposed method seems to be promising as compared to the other systems evaluated
on the same task. Our best run gives 63% accuracy. Two systems participating the RTE-2 Challenge
had 73% and 75% accuracy, two systems achieved 62% and 63%, while most of the systems achieved
55%-61%, cf. Bar-Haim et al (2006). For our best run (WN 3.0 + FN), we present the accuracy data
for each application separately (table 2). The distribution of the performance of Mini-TACITUS on the
four datasets corresponds to the average performance of systems participating in RTE-2 as reported by
Garoufi (2007). The most challenging task in RTE-2 appeared to be IE. QA and IR follow, and finally,
SUM was titled the ?easiest? task, with a performance significantly higher than that of any other task.11
It is worth noting that the performance of Mini-TACITUS increases with the increasing time of pro-
cessing. This is not surprising. We use the time parameter t for restricting the processing time. The
smaller t is, the fewer chances Mini-TACITUS has for applying all relevant axioms. The experiments
carried out suggest that optimizing the system computationally could lead to producing significantly bet-
ter results. Tracing the reasoning process, we found out that given a long sentence and a short processing
time Mini-TACITUS had time to construct only a few interpretations, and the real best interpretation was
not always among them.
The lower performance of the system using the KB based on axioms extracted from extended Word-
Net can be easily explained. At the moment we define non-merge constraints (see section 2) for the input
propositions only. The axioms extracted from the synset definitions introduce a lot of new lexemes into
the logical form, since these axioms define words with the help of other words rather than abstract con-
cepts. These new lexemes, especially those which are frequent in English, result in undesired mergings
(e.g., mergings of frequent prepositions), since no non-merge constraints are defined for them. In order
to fix this problem, we will need to implement dynamic non-merge constraints which will be added on
the fly if a new lexeme is introduced during reasoning. The WN 3.0 + FN axiom set does not fall into
this problem, because these axioms operate on frames and synsets rather than on lexemes.
In addition, for the run using axioms derived from FrameNet, we have evaluated how well we do
in assigning frames and frame roles. For Mini-TACITUS, semantic role labeling is a by-product of
constructing the best interpretation. But since this task is considered to be important as such in the NLP
community, we provide an additional evaluation for it. As a gold standard we have used the Frame-
Annotated Corpus for Textual Entailment, FATE, see Burchardt and Pennacchiotti (2008). This corpus
provides frame and semantic role label annotations for the RTE-2 challenge test set.12 It is important to
8http://pascallin.ecs.soton.ac.uk/Challenges/RTE2/
9Interpretation costs were normalized to the number of propositions in the input.
10?Time? stands for the value of the time parameter ? processing time per sentence, in minutes; ?Numb. of ax.? stands for
the average number of axioms per sentence.
11In order to get a better understanding of which parts of our KB are useful for computing entailment and for which types of
entailment, in future, we are planning to use the detailed annotation of the RTE-2 dataset describing the source of the entailment
which was produced by Garoufi (2007). We would like to thank one of our reviewers for giving us this idea.
12FATE was annotated with the FrameNet 1.3 labels, while we have been using 1.5 version for extracting axioms. However,
231
Table 2: Evaluation results for the RTE-2 test set
KB Accuracy Time
Numb. of ax.
T H
No KB 57% 1 0 0
WN 3.0 + FN 62% 20 533 237
WN 3.0 + FN 63% 30 533 237
Ext. WN 2.0 60% 20 3700 1720
Ext. WN 2.0 61% 30 3700 1720
Task Accuracy
SUM 75%
IR 64%
QA 62%
IE 50%
Table 3: Evaluation of frames/roles labeling towards FATE
System
Frame match
Recall
Role match
Precision Recall
Shalmaneser 0.55 0.54 0.37
Shalmaneser + Detour 0.85 0.52 0.36
Mini-TACITUS 0.65 0.55 0.30
note that FATE annotates only those frames which are relevant for computing entailment. Since Mini-
TACITUS makes all possible frame assignments for a sentence, we provide only the recall measure for
the frame match and leave the precision out.
The FATE corpus was also used as a gold standard for evaluating the Shalmaneser system (Erk and
Pado, 2006) which is a state-of-the-art system for assigning FrameNet frames and roles. In table 2 we
replicate results for Shalmaneser alone and Shalmaneser boosted with the WordNet Detour to FrameNet
(Burchardt et al, 2005). The WN-FN Detour extended the frame labels assigned by Shalmaneser with
the labels related via the FrameNet hierarchy or by the WordNet inheritance relation, cf. Burchardt et al
(2009). In frame matching, the number of frame labels in the gold standard annotation that can also be
found in the system annotation (recall) was counted. Role matching was evaluated only on the frames
that are correctly annotated by the system. The number of role labels in the gold standard annotation
that can also be found in the system annotation (recall) as well as the number of role labels found by
the system which also occur in the gold standard (precision) were counted.13 Table 3 shows that given
FrameNet axioms, the performance of Mini-TACITUS on semantic role labeling is compatible with those
of the system specially designed to solve this task.
6 Conclusion and Future Work
This paper presents a discourse processing framework underlying the abductive reasoner called Mini-
TACITUS. We have shown that interpreting texts using weighted abduction helps solve pragmatic prob-
lems in discourse processing as a by-product. In this paper, particular attention was paid to the construc-
tion of a large and reliable knowledge base populated with axioms extracted from such lexical-semantic
resources as WordNet and FrameNet. The reasoning procedure as well as the knowledge base were eval-
uated in the Recognizing Textual Entailment task. The data for evaluation were taken from the RTE-2
Challenge. First, we have evaluated the accuracy of the entailment prediction. Second, we have eval-
in the new FN version the number of frames and roles increases and there is no message about removed frames in the General
Release Notes R1.5, see http://framenet.icsi.berkeley.edu. Therefore we suppose that most of the frames and roles used for the
FATE annotation are still present in FN 1.5.
13We do not compare filler matching, because the FATE syntactic annotation follows different standards as the one produced
by the ESG parser, which makes aligning fillers non-trivial.
232
uated frame and role labeling using the Frame-Annotated Corpora for Textual Entailment as the gold
standard. In both tasks our system showed performance compatible with those of the state-of-the art
systems. Since the inference procedure and the axiom set are general and not tuned for a particular task,
we consider the results of our experiments to be promising concerning possible manifold applications of
Mini-TACITUS.
The experiments we have carried out have shown that there is still a lot of space for improving the
procedure. First, for successful application of Mini-TACITUS on a large scale the system needs to be
computationally optimized. In its current state, Mini-TACITUS requires too much time for producing
satisfactory results. As our experiments suggest (cf. table 2), speeding up reasoning may lead to signif-
icant improvements in the system performance. Since Mini-TACITUS was not originally designed for
large-scale processing, its implementation is in many aspects not effective enough. We hope to improve
it by changing the data structure and re-implementing some of the main algorithms.
Second, in the future we plan to elaborate our treatment of natural language expressions standing for
logical connectors such as implication if, negation not, disjunction or and others. Quantifiers such as
all, each, some also require a special treatment. This advance is needed in order to achieve more precise
entailment inferences, which are at the moment based in our approach on the core information content
(?aboutness?) of texts. Concerning the heuristic non-merge constraints preventing undesired mergings
as well as the heuristic for assigning default costs (see section 2), in the future we would like to perform
a corpus study for evaluating and possibly changing these heuristics.
Another future direction concerns the enlargement of the knowledge base. Hand-crafted lexical-
semantic resources such as WordNet and FrameNet provide both an extensive lexical coverage and a
high-value semantic labeling. However, such resources still lack certain features essential for captur-
ing some of the knowledge required for linguistic inferences. First of all, manually created resources
are static; updating them with new information is a slow and time-consuming process. By contrast,
commonsense knowledge and the lexicon undergo daily updates. In order to accommodate dynamic
knowledge, we plan to make use of the distributional similarities of words in a large Web-corpus such
as for example Wikipedia. Many researchers working on RTE have already been using word similarity
for computing similarity between texts and hypotheses, e.g., Mehdad et al (2010). In our approach, we
plan to incorporate word similarities into the reasoning procedure making them affect proposition costs
so that propositions implied by the context (similar to other words in the context) will become cheaper
to prove. This extension might give us a performance improvement in RTE, because it will help to relate
those propositions from H for which there are no appropriate axioms in the KB to propositions in T.
Lexical-semantic resources as knowledge sources for reasoning have another shortcoming: They
imply too little structure. WordNet and FrameNet enable some argument mappings of related synsets or
frames, but they cannot provide a more detailed concept axiomatization. We are engaged in two types of
efforts to obtain more structured knowledge. The first effort is the manual encoding of abstract theories
explicating concepts that pervade natural language discourse, such as causality, change of state, and
scales, and the manual encoding of axioms linking lexical items to these theories. A selection of the core
theories can be found at http://www.isi.edu/ hobbs/csk.html. The second effort concerns making use of
the existing ontologies. The recent progress of the Semantic Web technologies has stimulated extensive
development of the domain-specific ontologies as well as development of inference machines specially
designed to reason with these ontologies.14 In practice, domain-specific ontologies usually represent
detailed and structured knowledge about particular domains (e.g. geography, medicine etc.). We intend
to make Mini-TACITUS able to use this knowledge through querying an externally stored ontology with
the help of an existing reasoner. This extension will give us a possibility to access elaborated domain-
specific knowledge which might be crucial for interpretation of domain-specific texts.
We believe that implementation of the mentioned improvements and extensions will make Mini-
TACITUS a powerful reasoning system equipped with enough knowledge to solve manifold NLP tasks on
a large scale. In our view, the experiments with the axioms extracted from the lexical-semantic resources
presented in this paper show the potential of weighted abduction for natural language reasoning and open
14www.w3.org/2001/sw/,http://www.cs.man.ac.uk/ sattler/reasoners.html
233
new ways for its application.
References
Bar-Haim, R., I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, and I. Szpektor (2006). The
second PASCAL recognising textual entailment challenge. In Proc. of the Second PASCAL Challenges
Workshop on Recognising Textual Entailment.
Burchardt, A., K. Erk, and A. Frank (2005). A WordNet Detour to FrameNet. In Sprachtechnologie,
mobile Kommunikation und linguistische Resourcen, Volume 8.
Burchardt, A. and M. Pennacchiotti (2008). FATE: a FrameNet-Annotated Corpus for Textual Entail-
ment. In Proc. of LREC?08.
Burchardt, A., M. Pennacchiotti, S. Thater, and M. Pinkal (2009). Assessing the impact of frame seman-
tics on textual entailment. Natural Language Engineering 15(4), 527?550.
Erk, K. and S. Pado (2006). Shalmaneser - a flexible toolbox for semantic role assignment. In Proc. of
LREC?06, Genoa, Italy.
Fellbaum, C. (Ed.) (1998). WordNet: An Electronic Lexical Database (First ed.). MIT Press.
Garoufi, K. (2007). Towards a better understanding of applied textual entailment: Annotation and eval-
uation of the rte-2 dataset. Master?s thesis, Saarland University.
Hobbs, J. R. (1985). Ontological promiscuity. In Proceedings, 23rd Annual Meeting of the Association
for Computational Linguistics, Chicago, Illinois, pp. 61?69.
Hobbs, J. R., M. Stickel, and P. Martin (1993). Interpretation as abduction. Artificial Intelligence 63,
69?142.
McCord, M. C. (1990). Slot grammar: A system for simpler construction of practical natural language
grammars. In Natural Language and Logic: International Scientific Symposium, Lecture Notes in
Computer Science, pp. 118?145. Springer Verlag.
McCord, M. C. (2010). Using Slot Grammar. Technical report, IBM T. J. Watson Research Center. RC
23978Revised.
Mehdad, Y., A. Moschitti, and F. M. Zanzotto (2010). Syntactic/semantic structures for textual entailment
recognition. In Proc. of HLT ?10: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, pp. 1020?1028.
Mulkar, R., J. R. Hobbs, and E. Hovy (2007). Learning from Reading Syntactically Complex Biol-
ogy Texts. In Proc.of the 8th International Symposium on Logical Formalizations of Commonsense
Reasoning. Palo Alto.
Ovchinnikova, E., L. Vieu, A. Oltramari, S. Borgo, and T. Alexandrov (2010). Data-Driven and Onto-
logical Analysis of FrameNet for Natural Language Reasoning. In Proc. of LREC?10, Valletta, Malta.
Ruppenhofer, J., M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk (2006). FrameNet II: Extended
Theory and Practice. International Computer Science Institute.
Shen, D. and M. Lapata (2007). Using Semantic Roles to Improve Question Answering. In Proc. of
EMNLP-CoNLL, pp. 12?21.
Stickel, M. E. (1988). A prolog technology theorem prover: Implementation by an extended prolog
compiler. Journal of Automated Reasoning 4(4), 353?380.
234
Granularity in Natural Language Discourse
Rutu Mulkar-Mehta, Jerry Hobbs and Eduard Hovy
University of Southern California, Information Sciences Institute
me@rutumulkar.com, hobbs@isi.edu, hovy@isi.edu
Abstract
This paper discusses the phenomenon of granularity in natural language1. By ?granularity? we
mean the level of detail of description of an event or object. Humans can seamlessly shift their gran-
ularity perspective while reading or understanding a text. To emulate this mechanism, we describe
a set of features that identify the levels of granularity in text, and empirically verify this feature set
using a human annotation study for granularity identification. This theory is the foundation for any
system that can learn the (global) behavior of event descriptions from (local) behavior descriptions.
This is the first research initiative, to our knowledge, for identifying granularity shifts in natural
language descriptions.
1 Introduction
Granularity is the concept of breaking down an event into smaller parts or granules such that each indi-
vidual granule plays a part in the higher level event. For example, the activity of driving to the grocery
store involves some fine-grained events like opening the car door, starting the engine, planning the route,
and driving to the destination. Each of these may in turn be decomposed further into finer levels of
granularity. For instance, planning the route might involve entering an address into GPS and following
directions. The phenomenon of granularity is observed in various domains, including scientific literature,
game reports, and political descriptions. In scientific literature, the process of photosynthesis on closer
examination is made up of smaller individual fine-grained processes such as the light dependent reaction
and the light independent reaction.
Granularity is not a new concept. It has been studied actively in various disciplines. In philosophy, Bit-
tner and Smith (2001) have worked on formalizing granularity and part-hood relations. In information
retrieval, Lau et al (2009) have used granularity concepts to extract relevant detail of information result-
ing from a given search query. In theoretical computer science and ontology development, Keet (2008)
has worked on formalizing the concept of entity granularity and hierarchy and applied it biological sci-
ences. In natural language processing, Mani (1998) has worked on applying concepts of granularity to
polysemy and Hobbs (1985) has worked on using granularity for decomposing complex theories into
simple theories.
Although all of the above work emphasizes the importance of granularity relations for language un-
derstanding and formalization, none of it has attempted to observe whether granularity structures exist in
natural language texts, explored whether granularity structures can be identified and extracted automati-
cally, or tried to analyze how harvesting granularity relations can possibly help with other NLP problems.
This paper focuses on two items: First, we present a model of granularity as it exists in natural language
(Section 2); and second, we present an annotation study which we conducted to verify the proposed
model of granularity in natural language (Section 3).
1This research was supported by the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program
under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172. Any opinions, findings, and conclusion
or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA,
AFRL, ONR, or the US government.
360
(a) (b)
Figure 1: 1(a): Granularity in Natural Language Descriptions; 1(b): Instantiating Natural Language to
the Granularity model
2 Modeling Granularity in Natural Language Texts
Humans can easily shift through various levels of granularity in understanding text. However, for auto-
mated granularity identification and extraction, it is necessary to explicitly recognize the identifiers that
indicate a shift in granularity. Figure 1(a) illustrates our theory of granularity. A granularity structure
exists only if at least two levels of information are present in text, such that the events at the coarse gran-
ularity can be decomposed into the events at the fine granularity, and the events at the fine granularity
combine together to form at least one segment of the event at the coarse granularity. In Figure 1(a),
Gc represents the phrase or sentence with coarse granularity information and Gf represents a phrase
or sentence with fine granularity information. Three types of relations can exist between the objects at
coarse and fine granularity: part-whole relationships between entities, part-whole relationships between
events, and causal relationships between the fine and coarse granularities. These relations signal a shift
in granularity. Instantiating text phrases into this model will expose granularities of text. For example,
consider the following sentence:
The San Francisco 49ers moved ahead 7?3 11 minutes into the game when William Floyd scored a two-yard
touchdown run.
The event of the player scoring a touchdown (the second clause of the sentence) is a decomposition
of the event of the team moving forward in the game (the first clause), and thus a finer granularity rep-
resentation of the San Francisco 49ers moving ahead in the game. When instantiated in our model of
granularity (Figure 1(a)), the graphical representation is shown in Figure 1(b).
Having described the overall model of granularity, we now elaborate on the components of the gran-
ularity model, namely part-whole relations and causal relations.
2.1 Part-Whole Relations
Two types of part-whole relations are present: meronymic and mereologic. Mereology (for more details
read Keet (2008)) is a partial ordering relation that is reflexive, transitive, and antisymmetric. According
to the concept of mereology, if x, y and z are three entities, then: x is a part of x; if x is part of y and y is
part of z then x is part of z; and if x is part of y then y cannot be part of x. However, various types of part-
whole relations that occur in natural language, such as member of, do not satisfy the transitivity relation,
in which case they will be mereologic but not meronymic: they might be ontologically accurate but
not linguistically correct. For instance, if John?s arm is part of John, and John is a member of a football
team, the transitivity relation that John?s arm is part of a football team, is not a valid meronymic relation.
Another instance which is mereologic but not meronymic is the following: A cup is made of steel, and
steel is made of molecules. Therefore a cup is made of molecules. The concept of mereology does not
361
reflect the way part of is used in natural language, and so mereology cannot be used for linguistic based
research.
One of the early works on part-whole relations in natural language (meronymy) Winston et al (1987)
was later refined in their empirical experiments Chaffin et al (1988). Winston et al discuss meronymic
relations and a taxonomy for representing them. They introduce six types of part-whole relationships:
(i) Component-Integral (e.g., pedal is a component of the integral bike), (ii) Member-Collection (e.g.,
a ship is a member of the collection, a fleet), (ii) Portion-Mass (e.g., a slice is a portion of the mass, a
pie), (iv) Stuff-Object (e.g., steel is one of the ingredients/stuff of the object car), (v) Feature-Activity
(e.g., paying is one of the features of the whole activity of shopping), (vi) Place-Area (e.g., Everglades
is a place within the area of Florida). The definition and classification in Winston et al (1987) for
part-whole relations is very relevant for language based analysis of part-whole relations. For granularity
identification in our work, the Feature-Activity type relation is used as the part-whole relation for events,
and the rest are part-whole relations for entities.
2.2 Causal Relations
Girju and Moldovan (2002) provide a broad compilation of causality research ranging from philosophy,
planning in AI, commonsense reasoning, and computational linguistics. Causation in computational
linguistics is the only form of causality that is relevant for granularity identification and extraction. The
following are the categories of causal constructs relevant for granularity identification and extraction:
? Causal Connectives: These are usually prepositional (such as because of, thanks to, due to), adver-
bial (such as for this reason, the result that), or clause links (such as because, since, for).
? Causation Verbs: These usually have a causal relation integrated with the verb. For example, kill,
melt (represent a causal link with the resulting situation), poison, hang, clean (represent a causal
link with the a part of the causing event)
? Conditionals: Girju and Moldovan (2002) describe conditionals as complex linguistic structures
typically of the form If S1 then S2. These structures represent causation, temporal relations, among
other relations, and are very complex structures in language.
3 Evaluation of the Granularity Model in Natural Language
We conducted an evaluation study to judge the ?goodness? of the granularity model proposed. In
this study the annotators were asked to annotate granularity relations between two given paragraphs.
Paragraph-based analysis was preferred to event-word-based analysis because people reason much more
easily with paragraph descriptions than with individual event mentions 2. The annotation set consisted of
paragraph pairs from three domains: travel articles (confluence.org), Timebank annotated data Pan et al
(2006), and Wikipedia articles on games. We selected a total of 37 articles: 10 articles about travel, 10
about games, and 17 from Timebank. Both paragraphs of a given question were selected from the same
article and referred to the same overall concept.
3.1 Annotation Task
The articles were uploaded to Mechanical Turk and were annotated by non-expert annotators (regular
Turkers). The entire set of 37 articles was annotated by 5 people. The annotators were given a pair
of paragraphs and were asked four questions about the relations between them: (i) Is one paragraph a
subevent of the other paragraph?, (ii) Did one paragraph cause the other paragraph?, (iii) Is one paragraph
less detailed and the other paragraph more detailed?, (iv) Did one paragraph happen after the other para-
graph? They were then presented with the comments of other annotators, and asked whether they agreed
2This was deduced as a result of an earlier annotation study for granularity identification using individual words as events.
362
(a) (b)
Figure 2: 2(a) shows the Inter-Annotator agreement for 37 articles and 2(b) shows the Pairwise Kappa
Agreement for 37 articles and 5 annotators
with any of the other annotations or explanations. The annotators were asked to provide a justification of
their choices.
3.2 Results
The Kappa statistic (Cohen (1960)) is the standard for measuring inter-annotator agreement: k =
(p(a)?p(e))
(1?p(e)) , where p(a) is the observed agreement and p(e) is the chance agreement between annota-
tors. More refined than simple Percentage Agreement, Kappa corrects for chance agreements.
In our study, two annotators were considered to be in agreement if they agreed with questions (i)
Subevents, (iii) More or less detail and (iv) Sequence. Unfortunately question (ii) Causality, as pro-
vided to the annotators, could not be taken into account for agreement measurement as individuals had
different conceptualizations of causality, and a crisp definition of causality was not provided to them.
For instance, consider the following two paragraphs:
1: I wanted to visit the confluence point located in the extreme southwest of Hunan Province.
2: To get to the confluence, I caught the Hong Kong-to-Shanghai intercity train on Friday afternoon.
Analysis: Some annotators annotated para2 causes para1, providing the explanation that the goal para1 could
be achieved due to the events of para2. Others annotated para1 causes para2, providing the justification that the
events of para2 only exist to fulfill the original goal para1. We are interested in the first type of causality, i.e.,
causality which explains how a given event happens. All the annotators agreed that a sub-event explains how an
event happens, or a sub-event causes an event. We counted this in lieu of our causality question (ii).
Figure 2(a) shows the overall agreement of the five annotators on the 37 articles and Figure 2(b) shows
the pairwise Kappa agreement for the five annotators. All the annotators agreed in 33/37 cases (23 article
pairs were annotated as having a granularity shift, 10 articles were annotated as having no granularity
shift). The average pairwise Kappa was 0.85. If the newspaper articles were removed, the overall agree-
ment was 100% for all the annotators. High agreement implied good quality of the annotation guidelines,
and provided evidence that people shift through various levels of granularity while reading and under-
standing text.
3.3 Analysis of the Causes of Disagreement
Where disagreements occurred, different interpretations of the same text were observed to be a major
cause. All these disagreements were limited to the newspaper articles. For instance, consider the follow-
ing:
363
1: Some 1,500 ethnic Albanians marched Sunday in downtown Istanbul, burning Serbian flags.
2: The police barred the crowd from reaching the Yugoslavian consulate in downtown Istanbul, but allowed them
to demonstrate on nearby streets.
Positive Granularity Shift: Some annotators commented that ?demonstrations? happen as a part of a ?march?.
So, para2 is a sub-event of para1.
Negative Granularity Shift: Other annotators felt that para2 happened after para1, and so there was no granular-
ity shift.
Overall, we can observe that although disagreement arises due to individual and unique interpretations
of text, people agree based on the discriminating features provided to them (part-whole relations and
causality) when identifying granularity shifts. This shows that part-whole relations and causality provide
a good set of features for identifying granularity shifts.
4 Conclusion and Future Work
In this paper we present the phenomenon of granularity as it occurs in natural language texts. We validate
our model of granularity with the help of an annotation study. We are currently developing a system for
automatic granularity extraction. We will compare its performance with state of the art techniques for
answering causality-style questions to empirically evaluate the significance of granularity structures for
automated Question Answering.
References
Bittner, T. and B. Smith (2001). Granular partitions and vagueness. Proceedings of the international
conference on Formal Ontology in Information Systems - FOIS ?01, 309?320.
Chaffin, R., D. J. Herrmann, and M. E. Winston (1988). An empirical taxonomy of part-whole relations:
Effects of part-whole relation type on relation identification. Language and Cognitive Processes 3(1).
Cohen, J. (1960). A coefficientof agreement for nominal scales. Educational and Psychological Mea-
surement 20, 37?46.
Girju, R. and D. Moldovan (2002). Mining Answers for Causation. Proceedings of American Association
of Artificial Intelligence, 15?25.
Hobbs, J. R. (1985). Granularity. In Proceedings of the Ninth International Joint Conference on Artificial
Intelligence, 432?435.
Keet, C. M. (2008). A Formal Theory of Granularity. Ph. D. thesis, Faculty of Computer Science, Free
University of Bozen-Balzano, Italy.
Lau, R. Y. K., C. C. L. Lai, and Y. Li (2009). Mining Fuzzy Ontology for a Web-Based Granular
Information Retrieval System. Lecture Notes in Computer Science, 239?246.
Mani, I. (1998). A Theory of Granularity and its Application to Problems of Polysemy and Underspec-
ification of Meaning. In Principles of Knowledge Representation and Reasoning: Proceedings of the
Sixth International Conference (KR?98), 245?255.
Pan, F., R. Mulkar, and J. R. Hobbs (2006). An Annotated Corpus of Typical Durations of Events. In
Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC),
77?83.
Winston, M. E., R. Chaffin, and D. Herrmann (1987, October). A Taxonomy of Part-Whole Relations.
Cognitive Science 11(4), 417?444.
364
Proceedings of the Second Workshop on Metaphor in NLP, pages 33?41,
Baltimore, MD, USA, 26 June 2014.
c
?2014 Association for Computational Linguistics
Abductive Inference for Interpretation of Metaphors
Ekaterina Ovchinnikova*, Ross Israel*, Suzanne Wertheim
+
,
Vladimir Zaytsev*, Niloofar Montazeri*, Jerry Hobbs*
* USC ISI, 4676 Admiralty Way, CA 90292, USA
{katya,israel,vzaytsev,niloofar,hobbs}@isi.edu
+
Worthwhile Research & Consulting, 430 1/2 N Genesee Av., Los Angeles, CA 90036, USA
worthwhileresearch@gmail.com
Abstract
This paper presents a metaphor interpre-
tation pipeline based on abductive infer-
ence. In this framework following (Hobbs,
1992) metaphor interpretation is modelled
as a part of the general discourse pro-
cessing problem, such that the overall dis-
course coherence is supported. We present
an experimental evaluation of the pro-
posed approach using linguistic data in
English and Russian.
1 Introduction
In this paper, we elaborate on a semantic pro-
cessing framework based on a mode of inference
called abduction, or inference to the best expla-
nation. In logic, abduction is a kind of inference
which arrives at an explanatory hypothesis given
an observation. (Hobbs et al., 1993) describe how
abduction can be applied to the discourse process-
ing problem, viewing the process of interpreting
sentences in discourse as the process of providing
the best explanation of why the sentence would be
true. (Hobbs et al., 1993) show that abductive rea-
soning as a discourse processing technique helps
to solve many pragmatic problems such as refer-
ence resolution, the interpretation of noun com-
pounds, detection of discourse relations, etc. as a
by-product. (Hobbs, 1992) explains how abduc-
tion can be applied to interpretation of metaphors.
The term conceptual metaphor (CM) refers
to the understanding of one concept or concep-
tual domain in terms of the properties of another
(Lakoff and Johnson, 1980; Lakoff, 1987). For ex-
ample, development can be understood as move-
ment (e.g., the economy moves forward, the en-
gine of the economy). In other words, a concep-
tual metaphor consists in mapping a target con-
ceptual domain (e.g., economy) to a source do-
main (e.g., vehicle) by comparing their properties
(e.g., an economy develops like a vehicle moves).
In text, conceptual metaphors are represented by
linguistic metaphors (LMs), i.e. natural language
phrases expressing the implied comparison of two
domains.
We present a metaphor interpretation approach
based on abduction. We developed an end-to-
end metaphor interpretation system that takes text
potentially containing linguistic metaphors as in-
put, detects linguistic metaphors, maps them to
conceptual metaphors, and interprets conceptual
metaphors in terms of both logical predicates and
natural language expressions. Currently, the sys-
tem can process linguistic metaphors mapping
predefined target and source domains.
We perform an experimental evaluation
of the proposed approach using linguistic
data in two languages: English and Rus-
sian. We select target concepts and generate
potential sources for them as described at
github.com/MetaphorExtractionTools/mokujin.
For top-ranked sources, we automatically find cor-
responding linguistic metaphors. These linguistic
metaphors are each then validated by three expert
linguists. For the validated linguistic metaphors,
we generate natural language interpretations,
which are also validated by three experts.
2 Related Work
Automatic interpretation of linguistic metaphors is
performed using two principal approaches: 1) de-
riving literal paraphrases for metaphorical expres-
sions from corpora (Shutova, 2010; Shutova et
al., 2012) and 2) reasoning with manually coded
knowledge (Hobbs, 1992; Narayanan, 1999; Barn-
den and Lee, 2002; Agerri et al., 2007; Veale and
Hao, 2008).
(Shutova, 2010; Shutova et al., 2012) present
methods for deriving paraphrases for linguis-
tic metaphors from corpora. For example, the
metaphorical expression "a carelessly leaked re-
33
port" is paraphrased as "a carelessly disclosed re-
port". This approach currently focuses on single-
word metaphors expressed by verbs only and does
not explain the target?source mapping.
The KARMA (Narayanan, 1999) and the ATT-
Meta (Barnden and Lee, 2002; Agerri et al., 2007)
systems perform reasoning with manually coded
world knowledge and operate mainly in the source
domain. The ATT-Meta system takes logical ex-
pressions that are representations of a small dis-
course fragment as input; i.e., it does not work
with natural language. KARMA focuses on dy-
namics and motion in space. For example, the
metaphorical expression the government is stum-
bling in its efforts is interpreted in terms of motion
in space: stumbling leads to falling, while falling
is a conventional metaphor for failing.
(Veale and Hao, 2008) suggest to derive
common-sense knowledge from WordNet and cor-
pora in order to obtain concept properties that can
be used for metaphor interpretation. Simple in-
ference operations, i.e. insertions, deletions and
substitution, allow the system to establish links be-
tween target and source concepts.
(Hobbs, 1992) understands metaphor interpre-
tation as a part of the general discourse processing
problem. According to Hobbs, a metaphorical ex-
pression should be interpreted in context. For ex-
ample, John is an elephant can be best interpreted
as "John is clumsy" in the context Mary is grace-
ful, but John is an elephant. In order to obtain
context-dependent interpretations, (Hobbs, 1992)
uses abductive inference linking parts of the dis-
course and ensuring discourse coherence.
3 Metaphor Interpretation System
Our abduction-based metaphor interpretation sys-
tem is shown in Fig. 1. Text fragments possibly
containing linguistic metaphors are given as in-
put to the pipeline. The text fragments are parsed
and converted into logical forms (section 3.1).
The logical forms are input to the abductive rea-
soner (section 3.2) that is informed by a knowl-
edge base (section 4). The processing component
labelled "CM extractor & scorer" extracts con-
ceptual metaphors from the logical abductive in-
terpretations and outputs scored CMs and Target-
Source mappings (section 3.3). The Target-Source
mappings are then translated into natural language
expressions by the NL generator module (sec-
tion 3.4).
3.1 Logical Form Generation
A logical form (LF) is a conjunction of propo-
sitions which have argument links showing rela-
tionships among phrase constituents. We use logi-
cal representations of natural language texts as de-
scribed in (Hobbs, 1985). In order to obtain LFs
we convert dependency parses into logical repre-
sentations in two steps: 1) assign arguments to
each lemma, 2) apply rules to dependencies in or-
der to link arguments.
Consider the dependency structure for the sen-
tence, John decided to leave: [PRED decide
[SUBJ John] [OBJ leave]]. First, we
generate unlinked predicates for this structure:
John(e
1
, x
1
)?decide(e
2
, x
2
, x
3
)?leave(e
3
, x
4
).
Then, based on the dependency labels, we link
argument x
1
with x
2
, x
3
with e
3
, and x
1
with
x
4
to obtain the following LF: John(e
1
, x
1
) ?
decide(e
2
, x
1
, e
3
) ? leave(e
3
, x
1
).
LFs are preferable to dependency structures in
this case because they generalize over syntax and
link arguments using long-distance dependencies.
Furthermore, we need logical representations in
order to apply abductive inference.
In order to produce logical forms for English,
we use the Boxer semantic parser (Bos et al.,
2004). As one of the possible formats, Boxer
outputs logical forms of sentences in the style of
(Hobbs, 1985). For Russian, we use the Malt de-
pendency parser (Nivre et al., 2006). We devel-
oped a converter turning Malt dependencies into
logical forms in the style of (Hobbs, 1985).
1
3.2 Abductive Inference
In order to detect conceptual metaphors and in-
fer explicit mappings between target and source
domains, we employ a mode of inference called
weighted abduction (Hobbs et al., 1993). This
framework is appealing because it is a realization
of the observation that we understand new mate-
rial by linking it with what we already know.
Abduction is inference to the best explanation.
Formally, logical abduction is defined as follows:
Given: Background knowledge B, observations
O, where both B and O are sets of first-order log-
ical formulas,
Find: A hypothesis H such that H ?B |= O,H ?
B 6|=?, where H is a set of first-order logical for-
mulas.
1
The converter is freely available at
https://github.com/eovchinn/Metaphor-ADP.
34
Figure 1: Abduction-based metaphor interpretation system.
Typically, there exist several hypotheses H ex-
plaining O. To rank hypotheses according to plau-
sibility and select the best hypothesis, we use
the framework of weighted abduction (Hobbs et
al., 1993). Frequently, the best interpretation re-
sults from identifying two entities with each other,
so that their common properties only need to be
proved or assumed once. Weighted abduction fa-
vors those interpretations that link parts of obser-
vations together and supports discourse coherence,
which is crucial for discourse interpretation.
According to (Hobbs, 1985), metaphor interpre-
tation can be modelled as abductive inference re-
vealing conceptual overlap between the target and
the source domain. Consider the abductive inter-
pretation produced for the sentence We intend to
cure poverty, Fig. 2. In the top line of the figure,
we have the LF (cf. Sec. 3.1), where we can see
that a person (x
1
) is the agent for the verbs intend
(e
1
) and cure (e
2
) and that poverty (x
2
) is the ob-
ject of cure. In the first box in the next row, we
see that cure invokes the source concepts of DIS-
EASE, CURE, and DOCTOR, where DISEASE is
the object of CURE, and DOCTOR is the subject.
In the same row, we see that poverty invokes the
POVERTY concept in the target domain. Impor-
tantly, POVERTY and DISEASE share the same
argument (x
2
), which refers to poverty.
The next row contains two boxes with ellipses,
representing long chains of common-sense infer-
ences in the source and target domains of DIS-
EASE and POVERTY, respectively. For DIS-
EASE we know that linguistic tokens such as ill-
ness, sick, disease, etc. cause the afflicted to expe-
rience loss of health, loss of energy, and a general
lack of productivity. For POVERTY, we know that
tokens such as poor, broke, poverty mean that the
experiencer of poverty lacks money to buy things,
take care of basic needs, or have access to trans-
portation. The end result of both of these frame-
works is that the affected individuals (or commu-
nities) cannot function at a normal level, with re-
spect to unaffected peers. We can use this common
meaning of causing the individual to not function
to link the target to the source.
The next three rows provide the mapping
from the meaning of the source (CURE, DOC-
TOR, DISEASE) concepts to the target concept
(POVERTY). As explained above, we can con-
sider DISEASE as a CAUSING-AGENT that can
CAUSE NOT FUNCTION; POVERTY can be ex-
plained the same way, at a certain level of abstrac-
tion. Essentially, the interpretation of poverty in
this sentence is that it causes some entity not to
function, which is what a DISEASE does as well.
For CURE, we see that cure can CAUSE NOT EX-
IST, while looking for a CAUSING-AGENT (per-
son) and an EXISTING DISEASE (poverty).
In our system, we use the implementation of
weighted abduction based on Integer Linear Pro-
gramming (ILP) (Inoue and Inui, 2012), which
makes the inference scalable.
3.3 CM Extractor and Scorer
The abductive reasoning system produces an inter-
pretation that contains mappings of lexical items
into Target and Source domains. Any Target-
Source pair detected in a text fragment constitutes
a potential CM. For some text fragments, the sys-
tem identifies multiple CMs. We score Target-
Source pairs according to the length of the depen-
dency path linking them in the predicate-argument
structure. Consider the following text fragment:
opponents argue that any state attempting to force
an out-of-state business to do its dirty work of tax
collection violates another state?s right to regulate
its own corporate residents and their commerce
35
Figure 2: Abductive interpretation for the sentence We intend to cure poverty.
Suppose our target domain is TAXATION, trig-
gered by tax collection in the sentence above. In
our corpus, we find realizations of the CM TAXA-
TION is an ENEMY (fight against taxes). The lex-
eme opponent triggers the STRUGGLE/ENEMY
domain. However, the sentence does not trigger
the CM TAXATION is an ENEMY. Instead, it in-
stantiates the CM TAXATION is DIRT (dirty work
of tax collection). The length of the dependency
path between dirty and tax is equal to 2, whereas
the path between opponent and tax is equal to
9. Therefore, our procedure ranks TAXATION is
DIRT higher, which corresponds to the intuition
that target and source words should constitute a
syntactic phrase in order to trigger a CM.
3.4 NL Representation of Metaphor
Interpretation
The output of the abduction engine is similar to
the logical forms provided in Fig. 2. In order to
make the output more reader friendly, we produce
a natural language representation of the metaphor
interpretation using templates for each CM. For
example, the text their rivers of money mean they
can offer far more than a single vote would invoke
the WEALTH is WATER CM, and the abduction
engine would output: LARGE-AMOUNT[river],
THING-LARGE-AMOUNT[money]. We then
take this information and use it as input for the
NL generation module to produce: "river" implies
that there is a large amount of "money".
4 Knowledge Base
In order to process metaphors with abduction, we
need a knowledge base that encodes the informa-
tion about the source domain, the target domain,
and the relationships between sources and targets.
We develop two distinct sets of axioms: lexical ax-
ioms that encode lexical items triggering domains,
and mapping axioms that encode knowledge used
to link source and target domains. We will discuss
the details of each axiom type next.
4.1 Lexical Axioms
Every content word or phrase that can be expected
to trigger a source or target domain is included as a
lexical axiom in the knowledge base. For example,
the STRUGGLE domain contains words like war,
fight, combat, conquer, weapon, etc. An example
of how a lexical axiom encodes the system logic is
given in (1). On the left side, we have the linguistic
token, fight, along with its part-of-speech, vb, and
the argument structure for verbs where e
0
is the
eventuality (see (Hobbs, 1985)) of the action of
fighting, x is the subject of the verb, and y is the
object. On the right side, STRUGGLE is linked to
the action of fighting, the subject is marked as the
AGENT, and the object is marked as the ENEMY.
(1) fight-vb(e
0
, x, y) ? STRUGGLE(e
0
)?
AGENT (x, e
0
) ? ENEMY (y, e
0
)
The lexicon is not limited to single-token en-
tries; phrases can be included as single entries; For
example, the ABYSS domain has phrases such as
climb out of as a single entry. Encoding phrases
often proves useful, as function words can often
help to distinguish one domain from others. In
this case, climbing out of something usually de-
notes an abyss, whereas climbing up or on usually
does not. The lexical axioms also include the POS
36
for each word. Thus a word like fight can be en-
tered as both a noun and a verb. In cases where a
single lexical axiom could be applied to multiple
domains, one can create multiple entries for the
axiom with different domains and assign weights
so that a certain domain is preferred over others.
Initial lexical axioms for each domain were de-
veloped based on intuition about each domain.
We then utilize ConceptNet (Havasi et al., 2007)
as a source for semi-automatically extracting a
large-scale lexicon. ConceptNet is a multilingual
semantic network that establishes links between
words and phrases. We query ConceptNet for
our initial lexical axioms to return a list of related
words and expressions.
4.2 Mapping Axioms
Mapping axioms provide the underlying meanings
for metaphors and link source and target domains.
All of these axioms are written by hand based
on common-sense world knowledge about each
target-source pair. For each CM, we consider a
set of LMs that are realizations of this CM in an
effort to capture inferences that are common for
all of the LMs. We consider the linguistic contexts
of the LMs and overlapping properties of the tar-
get and source domains derived from corpora as
described in section 5.1.
We will outline the process of axiomatizing the
STRUGGLE domain here. We know that a verb
like fight includes concepts for the struggle it-
self, an agent, and an enemy. In the context of
a STRUGGLE, an enemy can be viewed as some
entity a that attempts to, or actually does, inhibit
the functioning of some entity b, often through ac-
tual physical means, but also psychologically, eco-
nomically, etc. The struggle, or fight, itself then,
is an attempt by a to rid itself of b so that a can en-
sure normal functionality. So, given a phrase like
poverty is our enemy, the intended meaning is that
poverty is hindering the functionality of some en-
tity (an individual, a community, a country, etc.)
and is seen as a problem that must be fought,
i.e. eliminated. In a phrase like the war against
poverty, war refers to an effort to stop the exis-
tence of poverty. These inferences are supported
by the overlapping property propositions extracted
from English Gigaword as described in Sec. 5.1,
e.g., scourge of X, country fights X, country pulls
of X, suffer from X, fight against X.
To extend the example in (1), consider (2).
Here, we encode a STRUGGLE action, e.g. fight,
as CAUSE NOT EXIST, the AGENT of the
fight as CAUSING-AGENT, and the ENEMY as
EXISTING-THING. Then, for a verb phrase like
we fight poverty, we is the AGENT that engages in
causing poverty, the ENEMY, to not exist.
(2) STRUGGLE(e
0
) ? AGENT (x, e
0
) ?
ENEMY (y, 2
0
)?CAUSE(e
0
)?CAUSED(n, e
0
)?
NOT (n, ex) ? EXIST (ex) ? CAUSING ?
AGENT (x, e
0
) ? EXISTING? THING(y, ex)
We use 75 mapping axioms to cover the valid
LMs discussed in Sec. 5.2. Some interesting
trends emerge when examining the core meanings
of the LMs. Following (Hobbs, 2005), we found
that over 65% of the valid LMs in this study could
be explained in terms of causality. The next most
prevalent aspect that these metaphors touch upon
is that of functionality (nearly 35%), with some of
these overlapping with the causality aspect where
the meaning has to do with X causing Y to function
or not function.
Many of the CMs covered in this study have
fairly transparent interpretations based on these
ideas of causality and functionality, such as
POVERTY is DISEASE, where the main under-
lying meaning is that a disease causes the suf-
ferer not to function properly. However, for some
CMs, the interpretation can be more difficult to
pin down. For example, the interpretation of
WEALTH is a GAME is quite opaque. Given a
sentence such as, Wealth is a game and you better
start playing the game, there are no obvious con-
nections to concepts such as causality or function-
ality. Instead, game raises such ideas as competi-
tion, winning, and losing. In the literal context of a
game, the competition itself, who the competitors
are, and what it means to win or lose are usually
clearly defined, but this is not so when speaking
metaphorically about wealth. To derive a meaning
of game that can apply to wealth, we must look
at a higher level of abstraction and define game as
the instantiation of a positive or negative outcome,
i.e. to win is to achieve a positive outcome, or
gain wealth. In the same sentence play implies that
some voluntary action must be taken to achieve a
positive outcome.
For some metaphors, a simple transfer of the
source properties to the target does not result in
a coherent interpretation at all. Given, for exam-
ple, the CM POVERTY is a PRICE, one LM from
this study is, poverty is the price of peace. In this
case, the meaning has to do with some notion of
37
an exchange, where a negative consequence must
be accepted in order to achieve a desired outcome.
However, the metaphorical meaning of price dif-
fers from the literal meaning of the word. In literal
contexts, price refers to an amount of money or
goods with inherent value that must be given to ac-
quire something; the buyer has a supply of money
or goods that they willingly exchange for their
desired item. In the metaphorical sense, though,
there often is no buyer, and there is certainly not
an inherent value that can be assigned to poverty,
nor can one use a supply of it to acquire peace.
Another issue concerns cultural differences.
While writing the axioms to deal with English and
Russian source-target pairs we noticed that a ma-
jority of the axioms applied equally well to both
languages. However, there are some subtle dif-
ferences of aspect that impact the interpretation
of similar CMs across the two languages. Look-
ing again at the WEALTH is a GAME metaphor,
the Russian interpretation involves some nuance
of a lack of importance about the subject that
does not seem to be present in English when us-
ing words like game and play. Note that there
may be some notion of carelessness for English
(see Sec. 5.3), but for Russian, the notion of being
carefree, which is not the same as careless, about
wealth has a strong prevalence.
5 Experimental Validation
5.1 Source Generation
Following from the definition of metaphor, the tar-
get and the source domain share certain proper-
ties. In natural language, concepts and properties
are represented by words and phrases. There is
a long-standing tradition for considering compu-
tational models derived from word co-occurrence
statistics as being capable of producing reason-
able property-based descriptions of concepts (Ba-
roni and Lenci, 2008). We use proposition stores
to derive salient properties of concepts that can be
potentially compared in a metaphor.
A proposition store is a collection of proposi-
tions such that each proposition is assigned its fre-
quency in a corpus. Propositions are tuples of
words that have a determined pattern of syntactic
relations among them (Clark and Harrison, 2009;
Pe?as and Hovy, 2010; Tsao and Wible, 2013).
For example, the following propositions can be ex-
tracted from the sentence John decided to go to
school:
(NV John decide)
(NV John go)
(NVPN John go to school)
...
We generated proposition stores from parsed
English Gigaword (Parker et al., 2011) and Rus-
sian ruWac (Sharoff and Nivre, 2011). Given the
proposition stores, we generate potential sources
for a seed target lexeme l in three steps:
1. Find all propositions P
l
containing l.
2. Find all potential source lexemes S such that
for each s ? S there are propositions p, p
?
in the proposition store such that l occurs at
position i in p and s occurs at position i in p
?
.
The set of propositions containing l and s at
the same positions is denoted by P
l,s
.
3. Weight potential sources s ? S using the fol-
lowing equation:
weight
l
(s) =
?
p?P
l,s
weight
l
(t), (1)
The source generation procedure and
its validations are described in detail at
github.com/MetaphorExtractionTools/mokujin.
2
In the experiment described below, we gener-
ated potential sources for the target domains of
POVERTY and WEALTH.
5.2 Linguistic Metaphors Extraction and
Validation
For each potential CM, we look for supporting
LMs in corpora. A a large number of LMs sup-
porting a particular CM suggests that this CM
might be cognitively plausible. We use a simple
method for finding LMs. If a target lexeme and
a source lexeme are connected by a dependency
relation in a sentence, then we assume that this
dependency structure contains a LM. For exam-
ple, in the phrases medicine against poverty and
chronic poverty, the target word (poverty) is re-
lated via dependency arc with the source words
(medicine, chronic). LMs were extracted from En-
glish Gigaword (Parker et al., 2011) and Russian
ruWac (Sharoff and Nivre, 2011).
For the generated CMs, we select seed lexemes
for target and source domains. We expand the
2
The tools for generating proposition stores
and the obtained resources are freely available at
https://ovchinnikova.me/proj/metaphor.html.
38
sets of these target and source lexemes with se-
mantically related lexemes using English and Rus-
sian ConceptNet (Speer and Havasi, 2013) and top
ranked patterns from the proposition stores. For
example, the expansion of the lexeme disease re-
sults in the following set of lexemes: {disease,
symptom, syndrome, illness, unwellness, sickness,
sick, medicine, treatment, treat, cure, doctor, ... }
For each language, we select 20 top-ranked
sources per target. Then we randomly select at
most 10 sentences per each target-source pair.
These sentences are validated by 3 linguist experts
each. For each sentence, the experts are asked if
it contains a metaphor comparing an indicated tar-
get domain with an indicated source domain. The
inter-annotator agreement on the validation task is
defined as the percentage of judgements on which
the three experts agree. Agreement is 81% for En-
glish and 80% for Russian.
Tables 1 and 2 show 10 potential sources per
target with the best agreement. Column ALL pro-
vides the number of sentences per a proposed CM
such that all experts agreed that the sentence con-
tains a metaphor. Column TWO provides the num-
ber of sentences such that any two experts agreed
on, and Column ONE shows the number of sen-
tences such that a single expert thought it con-
tained a metaphor.
target source ALL TWO ONE
w
e
a
l
t
h
blood 10 10 10
water 9 10 10
drug 9 10 10
food 9 9 10
body 9 9 10
power 8 9 10
game 8 9 9
security 7 9 10
resource 7 7 9
disease 7 8 9
p
o
v
e
r
t
y
war 10 10 10
abyss 10 10 10
violence 9 9 10
price 8 9 9
location 7 8 8
disease 7 7 7
crime 4 5 6
crop 3 7 9
terrorism 3 3 5
cost 2 3 7
Table 1: Validation of English linguistic
metaphors found for potential sources.
5.3 Metaphor Interpretation Validation
Metaphor interpretations were generated for posi-
tively validated linguistic metaphors, as described
?
?
?
?
?
?
?
?
?
(
w
e
a
l
t
h
)
??????? (energy) 10 10 10
???? (water) 10 10 10
??????? (freedom) 10 10 10
?????? (power) 9 10 10
??? (god) 9 10 10
????? (blood) 9 10 10
???? (way) 9 10 10
???? (game) 8 10 10
????? (glory) 4 5 5
????? (ware) 3 8 10
?
?
?
?
?
?
?
?
(
p
o
v
e
r
t
y
)
???????? (abyss) 10 10 10
???? (enemy) 9 10 10
??????? (disease) 9 9 9
?????? (power) 8 10 10
???? (body) 6 6 6
???? (pain) 5 10 10
???????? (despair) 5 10 10
???? (price) 4 4 4
?????? (death) 3 5 6
????? (fear) 3 9 10
Table 2: Validation of Russian linguistic
metaphors found for potential sources.
in Sec. 3.4. Each interpretation was validated by
three expert linguists. We calculated strict and
relaxed agreement for the validated data. Strict
agreement is calculated over three categories: cor-
rect (C), partially correct (P), and incorrect (I). Re-
laxed agreement is calculated over two categories:
C/P and I. Partially correct means that the valida-
tor felt that something was missing from the inter-
pretation, but that what was there was not wrong.
Table 3 presents the validation results for both lan-
guages. As can be seen in the table, strict agree-
ment (AgrS) is 62% and 52% and strict system
accuracy (AccS ALL) is 62% and 50% for En-
glish and Russian, respectively. Relaxed agree-
ment (AgrR) results is 93% and 83%, and relaxed
accuracy (AccR ALL) is 91% and 78%.
Validators often marked things as only partially
correct if they felt that the interpretation was lack-
ing some aspect that was critical to the meaning of
the metaphor. A common feeling amongst the val-
idators, for example, is that the interpretation for
people who are terrorized by poverty should in-
clude some mention of "fear" as a crucial aspect
of the metaphor, as the interpretation provided
states only that "terrorize" implies that "poverty"
is causing "people" not to function. However, the
end result of "fear" itself is often that the experi-
encer cannot function, as in paralyzed by fear.
Tables 4 and 5 contain interpretation system ac-
curacy results by CM. We calculated the percent-
age of LMs evoking this CM that were validated
as C vs. I (strict) or P/C vs. I (relaxed) by all three
39
AgrS AgrR AccS ALL AccS TWO AccS ONE AccR ALL AccR TWO AccR ONE
English 0.62 0.93 0.62 0.84 0.98 0.91 0.97 0.99
Russian 0.52 0.83 0.50 0.76 0.96 0.78 0.93 0.99
Table 3: Validation results for metaphor interpretation for English and Russian.
(ALL), or just two (TWO) validators. In most of
the cases, the system performs well on "simple"
CMs related to the concepts of causation and func-
tioning (e.g., WEALTH is POWER), cf. section 4,
whereas its accuracy is lower for richer metaphors
(e.g., WEALTH is a GAME).
target source
ALL TWO
S R S R
w
e
a
l
t
h
blood 0.8 1 1 1
water 1 1 1 1
drug 0.44 0.78 0.89 0.89
food 0.89 1 1 1
body 0.67 0.78 0.78 0.78
power 1 1 1 1
game 0.63 1 1 1
security 0.14 0.88 0.71 1
resource 1 1 1 1
disease 0 1 1 1
p
o
v
e
r
t
y
war 0.9 0.9 1 1
abyss 0 0.5 0.4 1
violence 0 1 0.11 1
price 0.88 0.88 0.88 1
location 1 1 1 1
disease 0.43 0.86 0.86 0.86
crime 0.75 1 1 1
crop 1 1 1 1
terrorism 0 1 0.33 1
cost 1 1 1 1
Table 4: Accuracy of English interpretations for
each CM.
The data used in the described experiments, sys-
tem output, and expert validations are available
at http://ovchinnikova.me/suppl/AbductionSystem-
Metaphor-Validation.7z.
6 Conclusion and Future Work
The developed abduction-based metaphor
interpretation pipeline is available at
https://github.com/eovchinn/Metaphor-ADP
as a free open-source project. This pipeline
produces favorable results, with metaphor in-
terpretations that are rated as at least partially
correct, for over 90% of all valid metaphors it is
given for English, and close to 80% for Russian.
Granted, the current research is performed using a
small, controlled set of metaphors, so these results
could prove difficult to reproduce on a large scale
where any metaphor is possible. Still, the high
accuracies achieved on both languages indicate
T source
ALL TWO
S R S R
?
?
?
?
?
?
?
?
?
(
w
e
a
l
t
h
)
??????? (energy) 0.4 0.8 0.9 1
???? (water) 0 0.9 0.6 0.9
??????? (freedom) 1 1 1 1
?????? (power) 1 1 1 1
??? (god) 0.67 1 0.89 1
????? (blood) 1 1 1 1
???? (way) 0.78 0.78 0.89 0.89
???? (game) 0.1 0.2 0.2 0.3
????? (glory) 0 0.75 0.75 1
????? (ware) 0 0 0 1
?
?
?
?
?
?
?
?
(
p
o
v
e
r
t
y
)
???????? (abyss) 0.7 1 1 1
???? (enemy) 0.56 1 1 1
??????? (disease) 0.33 0.89 0.67 1
?????? (power) 0.5 0.5 1 1
???? (body) 0.17 0.17 0.17 0.83
???? (pain) 1 1 1 1
???????? (despair) 0.6 0.6 1 1
???? (price) 0.75 0.75 1 1
?????? (death) 0 0 0.33 1
????? (fear) 0 1 0.67 1
Table 5: Accuracy of Russian interpretations for
each CM.
that the approach is sound and there is potential
for future work.
The current axiomatization methodology is
based mainly on manually writing mapping ax-
ioms based on the axiom author?s intuition. Ob-
viously, this approach is subject to scrutiny re-
garding the appropriateness of the metaphors and
faces scalability issues. Thus, developing new au-
tomatic methods to construct the domain knowl-
edge bases is a main area for future consideration.
The mapping axioms present a significant chal-
lenge as far producing reliable output automati-
cally. One area for consideration is the afore-
mentioned prevalence of certain underlying mean-
ings such as causality and functionality. Gather-
ing enough examples of these by hand could lead
to generalizations in argument structure that could
then be applied to metaphorical phrases in cor-
pora to extract new metaphors with similar mean-
ings. Crowd-sourcing is another option that could
be applied to both axiom writing tasks in order to
develop a large-scale knowledge base in consid-
erably less time and at a lower cost than having
experts build the knowledge base manually.
40
References
R. Agerri, J.A. Barnden, M.G. Lee, and A.M. Walling-
ton. 2007. Metaphor, inference and domain-
independent mappings. In Proc. of RANLP?07,
pages 17?23.
J. A. Barnden and M. G. Lee. 2002. An artificial intel-
ligence approach to metaphor understanding. Theo-
ria et Historia Scientiarum, 6(1):399?412.
M. Baroni and A. Lenci. 2008. Concepts and proper-
ties in word spaces. Italian Journal of Linguistics,
20(1):55?88.
J. Bos, S. Clark, M. Steedman, J. R. Curran, and
J. Hockenmaier. 2004. Wide-coverage semantic
representations from a ccg parser. In Proc. of COL-
ING?04, pages 1240?1246.
P. Clark and P. Harrison. 2009. Large-scale extrac-
tion and use of knowledge from text. In Proc. of the
5th international conference on Knowledge capture,
pages 153?160. ACM.
Catherine Havasi, Robert Speer, and Jason Alonso.
2007. Conceptnet 3: a flexible, multilingual se-
mantic network for common sense knowledge. In
Recent Advances in Natural Language Processing,
Borovets, Bulgaria, September.
J. R. Hobbs, M. Stickel, P. Martin, and D. Edwards.
1993. Interpretation as abduction. Artificial Intelli-
gence, 63:69?142.
J. R. Hobbs. 1985. Ontological promiscuity. In Proc.
of ACL, pages 61?69, Chicago, Illinois.
J. R. Hobbs. 1992. Metaphor and abduction. In
A. Ortony, J. Slack, and O. Stock, editors, Com-
munication from an Artificial Intelligence Perspec-
tive: Theoretical and Applied Issues, pages 35?58.
Springer, Berlin, Heidelberg.
Jerry R. Hobbs. 2005. Toward a useful concept of
causality for lexical semantics. Journal of Seman-
tics, 22(2):181?209.
N. Inoue and K. Inui. 2012. Large-scale cost-based
abduction in full-fledged first-order predicate logic
with cutting plane inference. In Proc. of JELIA,
pages 281?293.
G. Lakoff and M. Johnson. 1980. Metaphors we Live
by. University of Chicago Press.
G. Lakoff. 1987. Women, fire, and dangerous things:
what categories reveal about the mind. University
of Chicago Press.
S. Narayanan. 1999. Moving right along: A computa-
tional model of metaphoric reasoning about events.
In Proc. of AAAI/IAAI, pages 121?127.
J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser:
A data-driven parser-generator for dependency pars-
ing. In Proc. of LREC?06, volume 6, pages 2216?
2219.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English gigaword fifth edition. LDC.
A. Pe?as and E. H. Hovy. 2010. Filling knowledge
gaps in text for machine reading. In Proc. of COL-
ING?10, pages 979?987.
S. Sharoff and J. Nivre. 2011. The proper place of
men and machines in language technology: Process-
ing Russian without any linguistic knowledge. In
Proc. Dialogue 2011, Russian Conference on Com-
putational Linguistics.
E. Shutova, T. Van de Cruys, and A. Korhonen. 2012.
Unsupervised metaphor paraphrasing using a vector
space model. In COLING (Posters), pages 1121?
1130.
E. Shutova. 2010. Automatic metaphor interpretation
as a paraphrasing task. In Proc. of NAACL?10.
R. Speer and C. Havasi. 2013. Conceptnet 5: A large
semantic network for relational knowledge. In The
People?s Web Meets NLP, pages 161?176. Springer.
N. Tsao and D. Wible. 2013. Word similarity us-
ing constructions as contextual features. In Proc.
JSSP?13, pages 51?59.
T. Veale and Y. Hao. 2008. A fluid knowledge repre-
sentation for understanding and generating creative
metaphors. In Proc. of COLING?08, pages 945?952.
ACL.
41
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 10?12,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Case, Constructions, FrameNet, and the Deep Lexicon
Jerry R. Hobbs
Information Sciences Institute
University of Southern California
Marina del Rey, California
Abstract
Three major contributions that Charles
Fillmore made in linguistics play an im-
portant role in the enterprise of deep
lexical semantics, which is the effort to
link lexical meaning to underlying abstract
core theories. I will discuss how case re-
lates to lexical decompositions, how moti-
vated constructions span the borderline be-
tween syntax and semantics, and how the
frames of FrameNet provide an excellent
first step in deep inference.
1 Deep Lexical Semantics
Deep lexical semantics (Hobbs, 2008) is the effort
to construct formal theories of abstract phenom-
ena, such as composite entities, the figure-ground
relation, scales, change of state, and causality, and
to link the most common words in English to these
theories with axioms explicating their meanings.
This work has been deeply influenced by the work
of Charles Fillmore in at least three ways ? the
insights underlying case grammar, in the impor-
tance of being able to represent constructions, and
in the development of FrameNet. In this talk I will
describe how each of these issues is dealt with in
deep lexical semantics. First I will sketch three of
the underlying core theories.
Composite Entities and the Figure-Ground Re-
lation: A composite entity is a thing made of other
things. This is intended to cover physical objects
like a telephone, mixed objects like a book, ab-
stract objects like a theory, and events like a con-
cert. It is characterized by a set of components,
a set of properties of the components, a set of re-
lations among its components (the structure), and
relations between the entity as a whole and its en-
vironment (including its function). The predicate
at relates an external entity, the figure, to a com-
ponent in a composite entity, the ground. Differ-
ent figures and different grounds give us different
meanings for at.
Spatial location: Pat is at the back of the
store.
Location on a scale: Nuance closed at
58.
Membership in an organization: Pat is
now at Google.
Location in a text: The table is at the end
of the article.
Time of an event: At that moment, Pat
stood up.
Event at event: Let?s discuss that at
lunch.
At a predication: She was at ease in his
company.
When at is specialized in this way, we tap into a
whole vocabulary for talking about the domain.
Change of State: The predication
change(e
1
, e
2
) says that state e
1
changes
into state e
2
. Its principal properties are that
e
1
and e
2
should have an entity in common ? a
change of state is a change of state of something.
States e
1
and e
2
are not the same unless there
is an intermediate state. The predicate change
is defeasibly transitive; in fact, backchaining on
the transitivity axiom is one way to refine the
granularity on processes.
Causality: We distinguish between the ?causal
complex? for an effect and the concept ?cause?. A
causal complex includes all the states and events
that have to happen or hold in order for the effect
to occur. We say that flipping a switch causes the
light to go on. But many other conditions must
be in the causal complex ? the light bulb can?t be
burnt out, the wiring has to be intact, the power
has to be on in the city, and so on. The two key
properties of a causal complex are that when ev-
erything in the causal complex happens or holds,
so will the effect, and that everything that is in the
10
causal complex is relevant in a sense that can be
made precise. ?Causal complex? is a rigorous or
monotonic notion, but its utility in everyday life is
limited because we almost never can specify ev-
erything in it.
?Cause? by contrast is a defeasible or nonmono-
tonic notion. It selects out of a causal complex
a particular eventuality that in a sense is the ?ac-
tive? part of the causal complex, the thing that isn?t
necessarily normally true. Flipping the switch, in
most contexts, is the action that causes the light to
come on. Causes are the focus of planning, predic-
tion, explanation, and interpreting discourse, but
not diagnosis, since in diagnosis, something that
normally happens or holds, doesn?t.
As illustrations, here is how two verbs are de-
fined in terms of these core theories. The transitive
sense of ?move?, as in ?x moves y from z to w? is
captured by the axiom
move(x, y, z, w)
? cause(x, e
1
)
? change
?
(e
1
, e
2
, e
3
)
? at
?
(e
2
, y, z)
? at
?
(e
3
, y, w)
That is, x causes a change e
1
from the state e
2
in
which y is at z to the state e
3
in which y is at w.
The verb ?let? as in ?x lets e happen? means x
does not cause e not to happen. The axiom is
let(x, e)
? not(e
1
) ? cause
?
(e
1
, x, e
2
)
?not
?
(e
2
, e)
2 Case
The various case roles proposed by Filllmore
(1968) and many others since then can be under-
stood in terms of the roles entities play in these ax-
iomatic decompositions. In the axiom for move,
x is the agent. An agent is an entity that is viewed
as being capable of initiating a causal chain, and
the agent of an action is the agent that initiated it.
What Fillmore originally called the object and
has since been called the patient and, more
bizarrely, the theme is the entity that undergoes the
change of state or location. In the move axiom, y
plays this role.
When the property that changes in the object is a
real or metaphorical ?at? relation, as in move, then
Z is the source and w is the goal. An instrument
is an entity that the agent causes to go through a
change of state where this change plays an inter-
mediate role in the causal chain. Other proposed
case roles can be analyzed similarly.
The more similar verbs are to ?move?, the eas-
ier it is to assign case labels to their arguments.
When verbs are not very similar to ?move?, e.g.,
?outnumber?, assigning case labels becomes more
problematic, a factor no doubt in Fillmore?s deci-
sion not to utilize a small fixed list in FrameNet.
Nevertheless, the abstractness of the underly-
ing core theories, particularly the theory of com-
posite entities, ensures that this understanding of
case applies to the verbal lexicon widely. Thus, al-
though case labels play no formal role in deep lexi-
cal semantics, the insights of case grammar can be
captured and inform the analyses of specific verb
meanings.
3 Constructions
In the 1980s Fillmore and his colleagues at Berke-
ley developed the theory of Construction Gram-
mar (Fillmore et al., 1988). I take constructions
to be fragments of language that elemplify gen-
eral compositional principles, but have a conven-
tionalized meaning which is one of perhaps many
meanings licensed by the general lexical and com-
positional structure, but is the sole, or at least the
usual, interpretation normally assigned to it in dis-
course.
An example will perhaps make this clear. The
contraction ?let?s? has a particular meaning, sub-
sumed by, but much more specific than, ?let us?.
?Let us go.? could mean the same as ?Let?s go,?
although it sounds stilted. But it could also be
something kidnap victims say to the kidnapper. By
general principles, ?let?s go? could have either of
these meanings. But in fact it only has the first.
Thus, ?let?s? can be viewed as a conventional-
ization of one specific interpretation of ?let us?.
The source interpretation is this: ?Let?s? is a con-
traction for ?let us?. A rule of contraction would
tell us that when the string ?let us? describes a
parameterized situation, the string ?let?s? can de-
scribe the same situation. Thus, the best expla-
nation for the occurrence of ?let?s? is that it is a
contraction of ?let us?, ?Let?s? is only used in im-
perative sentences, so the implicit subject is ?you?.
The verb ?to let? means, as in the axiom above, ?to
not cause not?. Thus, ?let us go.? means ?Don?t
you cause us not to go.? So far, this supports both
meanings above. Now the set of people designated
11
by ?us? may or may not include you in general,
but in the desired interpretation it does. One way
for you to cause us not to go, provided you are a
part of us, is for you not to go yourself. The sen-
tence ?Let?s go.? tells you not to cause us not to
go by not going yourself. This abductive interpre-
tation is straightforwardly represented in a proof
graph. This is the conventionalized meaning asso-
ciated with the ?let?s? construction.
In normal usage we do not unpack this graph
structure, but it nevertheless provides the conven-
tional interpretation?s motivation, a term I believe
I first heard from Fillmore in a discussion group
in 1980. The conventional interpretation of ?let?s
go? is not completely arbitrary. We can unpack it,
and often need to in interpreting discourse. The re-
ply could be ?No, you go alone? or ?No, let?s stay
here.? Each of these taps into a different aspect of
the conventional interpretation?s motivation.
Constructions are not phrases like ?let?s go? or
parameterized phrases like ?let?s VP? but frag-
ments of a proof graph encoding the motivated
syntactic and compositional semantic structure as
well as the conventionalized interpretation. They
are normally deployed in a block, but they can be
effortlessly unpacked when one needs to.
4 FrameNet
The FrameNet frames (Baker et al., 2003) can
be viewed as providing the first level of axioms
mapping words and phrases into underlying core
theories. For example, ?let? is mapped into a
frame of enablement (not-cause-not), along with
the verbs ?permit? and ?allow? and the parame-
terized phrase ?make possible?. The frames are
not expressed in the FrameNet resource as ax-
ioms. However, FrameNet was converted into log-
ical axioms by Ovchinnikova (Ovchinnikova et al.
2013), and she and her colleagues have shown that
an abduction engine using a knowledge base de-
rived from these sources is competitive with the
best of the statistical systems in recognizing tex-
tual entailment and in semantic role labelling.
The FrameNet project, in addition, has demon-
strated that a concerted, long-term effort, when
intelligently thought out with a sensitivity to the
nature of language, can produce a highly valu-
able resource for deep, knowledge-based process-
ing of natural language. This was certainly among
Charles Fillmore?s greatest contributions to com-
putational linguistics.
References
Baker, C., Charles Fillmore, and B. Cronin, 2003. The
Structure of the Framenet Database, International
Journal of Lexicography, Volume 16.3, 281-296.
Fillmore, Charles, 1968, The Case for Case, in Bach
and Harms (Eds.), Universals in Linguistic Theory,
New York: Holt, Rinehart, and Winston, 1-88.
Fillmore, Charles, Paul Kay, and Catherine O?Connor,
1988. Regularity and Idiomaticity in Grammatical
Constructions: The Case of let alone, Language, 64,
501-38.
Hobbs, Jerry R. 2008. Deep Lexical Semantics. Pro-
ceedings of 9th International Conference on Intelli-
gent Text Processing and Computational Linguistics
(CICLing-2008), Haifa, Israel.
Ovchinnikova, Ekaterina, Niloofar Montazeri, Teodor
Alexandrov, Jerry R. Hobbs, Michael C. McCord,
and Rutu Mulkar-Mehta. 2013. Abductive Reason-
ing with a Large Knowledge Base for Discourse Pro-
cessing. In H. Hunt, J. Bos, and S. Pulman, (Eds.),
Computing Meaning, 4:104-124.
12
