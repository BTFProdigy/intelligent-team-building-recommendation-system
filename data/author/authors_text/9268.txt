Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 678?687, Prague, June 2007. c?2007 Association for Computational Linguistics
Online Learning of Relaxed CCG Grammars for Parsing to Logical Form
Luke S. Zettlemoyer and Michael Collins
MIT CSAIL
lsz@csail.mit.edu,mcollins@csail.mit.edu
Abstract
We consider the problem of learning to
parse sentences to lambda-calculus repre-
sentations of their underlying semantics and
present an algorithm that learns a weighted
combinatory categorial grammar (CCG). A
key idea is to introduce non-standard CCG
combinators that relax certain parts of the
grammar?for example allowing flexible
word order, or insertion of lexical items?
with learned costs. We also present a new,
online algorithm for inducing a weighted
CCG. Results for the approach on ATIS
data show 86% F-measure in recovering
fully correct semantic analyses and 95.9%
F-measure by a partial-match criterion, a
more than 5% improvement over the 90.3%
partial-match figure reported by He and
Young (2006).
1 Introduction
Recent work (Mooney, 2007; He and Young, 2006;
Zettlemoyer and Collins, 2005) has developed learn-
ing algorithms for the problem of mapping sentences
to underlying semantic representations. In one such
approach (Zettlemoyer and Collins, 2005) (ZC05),
the input to the learning algorithm is a training set
consisting of sentences paired with lambda-calculus
expressions. For instance, the training data might
contain the following example:
Sentence: list flights to boston
Logical Form: ?x.flight(x) ? to(x, boston)
In this case the lambda-calculus expression denotes
the set of all flights that land in Boston. In ZC05
it is assumed that training examples do not include
additional information, for example parse trees or
a) on may four atlanta to denver delta flight 257
?x.month(x,may) ? day number(x, fourth)?
from(x, atlanta) ? to(x, denver)?
airline(x, delta air lines) ? flight(x)?
flight number(x, 257)
b) show me information on american airlines from fort worth
texas to philadelphia
?x.airline(x, american airlines)?
from(x, fort worth) ? to(x, philadelphia)
c) okay that one?s great too now we?re going to go on april
twenty second dallas to washington the latest nighttime
departure one way
argmax(?x.flight(x) ? from(x, dallas)?
to(x,washington) ? month(x, april)?
day number(x, 22) ? during(x, night)?
one way(x), ?y.depart time(y))
Figure 1: Three sentences from the ATIS domain.
other derivations. The output from the learning algo-
rithm is a combinatory categorial grammar (CCG),
together with parameters that define a log-linear dis-
tribution over parses under the grammar. Experi-
ments show that the approach gives high accuracy on
two database-query problems, introduced by Zelle
and Mooney (1996) and Tang and Mooney (2000).
The use of a detailed grammatical formalism such
as CCG has the advantage that it allows a system to
handle quite complex semantic effects, such as co-
ordination or scoping phenomena. In particular, it
allows us to leverage the considerable body of work
on semantics within these formalisms, for example
see Carpenter (1997). However, a grammar based
on a formalism such as CCG can be somewhat rigid,
and this can cause problems when a system is faced
with spontaneous, unedited natural language input,
as is commonly seen in natural language interface
applications. For example, consider the sentences
shown in figure 1, which were taken from the ATIS
travel-planning domain (Dahl et al, 1994). These
sentences exhibit characteristics which present sig-
nificant challenges to the approach of ZC05. For ex-
678
ample, the sentences have quite flexible word order,
and include telegraphic language where some words
are effectively omitted.
In this paper we describe a learning algorithm that
retains the advantages of using a detailed grammar,
but is highly effective in dealing with phenomena
seen in spontaneous natural language, as exempli-
fied by the ATIS domain. A key idea is to extend
the approach of ZC05 by allowing additional non-
standard CCG combinators. These combinators re-
lax certain parts of the grammar?for example al-
lowing flexible word order, or insertion of lexical
items?with learned costs for the new operations.
This approach has the advantage that it can be seam-
lessly integrated into CCG learning algorithms such
as the algorithm described in ZC05.
A second contribution of the work is a new, on-
line algorithm for CCG learning. The approach in-
volves perceptron training of a model with hidden
variables. In this sense it is related to the algorithm
of Liang et al (2006). However it has the addi-
tional twist of also performing grammar induction
(lexical learning) in an online manner. In our exper-
iments, we show that the new algorithm is consid-
erably more efficient than the ZC05 algorithm; this
is important when training on large training sets, for
example the ATIS data used in this paper.
Results for the approach on ATIS data show 86%
F-measure accuracy in recovering fully correct se-
mantic analyses, and 95.9% F-measure by a partial-
match criterion described by He and Young (2006).
The latter figure contrasts with a figure of 90.3% for
the approach reported by He and Young (2006).1
Results on the Geo880 domain also show an im-
provement in accuracy, with 88.9% F-measure for
the new approach, compared to 87.0% F-measure
for the method in ZC05.
2 Background
2.1 Semantics
Training examples in our approach consist of sen-
tences paired with lambda-calculus expressions. We
use a version of the lambda calculus that is closely
related to the one presented by Carpenter (1997).
There are three basic types: t, the type of truth val-
1He and Young (2006) do not give results for recovering
fully correct parses.
ues; e, the type for entities; and r, the type for real
numbers. Functional types are defined by specify-
ing their input and output types, for example ?e, t?
is the type of a function from entities to truth val-
ues. In general, declarative sentences have a logical
form of type t. Question sentences generally have
functional types.2 Each expression is constructed
from constants, logical connectors, quantifiers and
lambda functions.
2.2 Combinatory Categorial Grammars
Combinatory categorial grammar (CCG) is a syn-
tactic theory that models a wide range of linguistic
phenomena (Steedman, 1996; Steedman, 2000).
The core of a CCG grammar is a lexicon ?. For
example, consider the lexicon
flights := N : ?x.flight(x)
to := (N\N)/NP : ?y.?f.?x.f(x) ? to(x, y)
boston := NP : boston
Each entry in the lexicon is a pair consisting of a
word and an associated category. The category con-
tains both syntactic and semantic information. For
example, the first entry states that the word flights
can have the category N : ?x.flight(x). This cat-
egory consists of a syntactic type N , together with
the semantics ?x.flight(x). In general, the seman-
tic entries for words in the lexicon can consist of any
lambda-calculus expression. Syntactic types can ei-
ther be simple types such as N , NP , or S, or can be
more complex types that make use of slash notation,
for example (N\N)/NP .
CCG makes use of a set of combinators which
are used to combine categories to form larger pieces
of syntactic and semantic structure. The simplest
such rules are the functional application rules:
A/B : f B : g ? A : f(g) (>)
B : g A\B : f ? A : f(g) (<)
The first rule states that a category with syntactic
type A/B can be combined with a category to the
right of syntactic type B to create a new category
of type A. It also states that the new semantics
will be formed by applying the function f to
the expression g. The second rule handles argu-
ments to the left. Using these rules, we can parse the
2For example, many question sentences have semantics of
type ?e, t?, as in ?x.flight(x) ? to(x, boston).
679
following phrase to create a new category of typeN :
flights to boston
N (N\N)/NP NP
?x.flight(x) ?y.?f.?x.f(x) ? to(x, y) boston
>
(N\N)
?f.?x.f(x) ? to(x, boston)
<
N
?x.flight(x) ? to(x, boston)
The top-most parse operations pair each word with a
corresponding category from the lexicon. The later
steps are labeled ?> (for each instance of forward
application) or ?< (for backward application).
A second set of combinators in CCG grammars
are the rules of functional composition:
A/B : f B/C : g ? A/C : ?x.f(g(x)) (> B)
B\C : g A\B : f ? A\C : ?x.f(g(x)) (< B)
These rules allow for an unrestricted notion of con-
stituency that is useful for modeling coordination
and other linguistic phenomena. As we will see, they
also turn out to be useful when modeling construc-
tions with relaxed word order, as seen frequently in
domains such as ATIS.
In addition to the application and composition
rules, we will also make use of type raising and co-
ordination combinators. A full description of these
combinators goes beyond the scope of this paper.
Steedman (1996; 2000) presents a detailed descrip-
tion of CCG.
2.3 Log-Linear CCGs
We can generalize CCGs to weighted, or probabilis-
tic, models as follows. Our models are similar to
several other approaches (Ratnaparkhi et al, 1994;
Johnson et al, 1999; Lafferty et al, 2001; Collins,
2004; Taskar et al, 2004). We will write x to de-
note a sentence, and y to denote a CCG parse for a
sentence. We use GEN(x; ?) to refer to all possi-
ble CCG parses for x under some CCG lexicon ?.
We will define f(x, y) ? Rd to be a d-dimensional
feature?vector that represents a parse tree y paired
with an input sentence x. In principle, f could in-
clude features that are sensitive to arbitrary sub-
structures within the pair (x, y). We will define
w ? Rd to be a parameter vector. The optimal parse
for a sentence x under parameters w and lexicon ?
is then defined as
y?(x) = arg max
y?GEN(x;?)
w ? f(x, y) .
Assuming sufficiently local features3 in f , search for
y? can be achieved using dynamic-programming-
style algorithms, typically with some form of beam
search.4 Training a model of this form involves
learning the parameters w and potentially also the
lexicon ?. This paper focuses on a method for learn-
ing a (w,?) pair from a training set of sentences
paired with lambda-calculus expressions.
2.4 Zettlemoyer and Collins 2005
We now give a description of the approach of Zettle-
moyer and Collins (2005). This method will form
the basis for our approach, and will be one of the
baseline models for the experimental comparisons.
The input to the ZC05 algorithm is a set of train-
ing examples (xi, zi) for i = 1 . . . n. Each xi is
a sentence, and each zi is a corresponding lambda-
expression. The output from the algorithm is a pair
(w,?) specifying a set of parameter values, and a
CCG lexicon. Note that for a given training example
(xi, zi), there may be many possible parses y which
lead to the correct semantics zi.5 For this reason
the training problem is a hidden-variable problem,
where the training examples contain only partial in-
formation, and the CCG lexicon and parse deriva-
tions must be learned without direct supervision.
A central part of the ZC05 approach is a function
GENLEX(x, z) which maps a sentence x together
with semantics z to a set of potential lexical entries.
The function GENLEX is defined through a set of
rules?see figure 2?that consider the expression z,
and generate a set of categories that may help in
building the target semantics z. An exhaustive set
of lexical entries is then generated by taking all cat-
egories generated by the GENLEX rules, and pair-
ing themwith all possible sub-strings of the sentence
x. Note that our lexicon can contain multi-word en-
tries, where a multi-word string such as New York
can be paired with a CCG category. The final out-
3For example, features which count the number of lexical
entries of a particular type, or features that count the number of
applications of a particular CCG combinator.
4In our experiments we use a parsing algorithm that is simi-
lar to a CKY-style parser with dynamic programming. Dynamic
programming is used but each entry in the chart maintains a full
semantic expression, preventing a polynomial-time algorithm;
beam search is used to make the approach tractable.
5This problem is compounded by the fact that the lexicon
is unknown, so that many of the possible hidden derivations
involve completely spurious lexical entries.
680
Rules Example categories produced from the logical form
Input Trigger Output Category argmax(?x.flight(x) ? from(x, boston), ?x.cost(x))
constant c NP : c NP : boston
arity one predicate p N : ?x.p(x) N : ?x.flight(x)
arity one predicate p S\NP : ?x.p(x) S\NP : ?x.flight(x)
arity two predicate p2 (S\NP )/NP : ?x.?y.p2(y, x) (S\NP )/NP : ?x.?y.from(y, x)
arity two predicate p2 (S\NP )/NP : ?x.?y.p2(x, y) (S\NP )/NP : ?x.?y.from(x, y)
arity one predicate p1 N/N : ?g.?x.p1(x) ? g(x) N/N : ?g.?x.flight(x) ? g(x)
literal with arity two predicate p2
and constant second argument c N/N : ?g.?x.p2(x, c) ? g(x) N/N : ?g.?x.from(x, boston) ? g(x)
arity two predicate p2 (N\N)/NP : ?y.?g.?x.p2(x, y) ? g(x) (N\N)/NP : ?y.?g.?x.from(x, y) ? g(x)
an argmax /min with second
argument arity one function f NP/N : ?g. argmax /min(g, ?x.f(x)) NP/N : ?g. argmax(g, ?x.cost(x))
arity one function f S/NP : ?x.f(x) S/NP : ?x.cost(x)
arity one function f (N\N)/NP : ?y.?f.?x.g(x) ? f(x) >/< y (N\N)/NP : ?y.?f.?x.g(x) ? cost(x) > y
no trigger S/NP : ?x.x, S/N : ?f.?x.f(x) S/NP : ?x.x, S/N : ?f.?x.f(x)
Figure 2: Rules used in GENLEX. Each row represents a rule. The first column lists the triggers that identify some sub-structure
within a logical form. The second column lists the category that is created. The third column lists categories that are created when
the rule is applied to the logical form at the top of this column. We use the 10 rules described in ZC05 and add two new rules,
listed in the last two rows above. This first new rule is instantiated for greater than (>) and less than (<) comparisions. The second
new rule has no trigger; it is always applied. It generates categories that are used to learn lexical entries for semantically vacuous
sentence prefixes such as the phrase show me information on in the example in figure 1(b).
put from GENLEX(x, z) is a large set of potential
lexical entries, with the vast majority of those en-
tries being spurious. The algorithm in ZC05 embeds
GENLEX within an overall learning approach that
simultaneously selects a small subset of all entries
generated by GENLEX and estimates parameter val-
uesw. Zettlemoyer and Collins (2005) present more
complete details. In section 4.2 we describe a new,
online algorithm that uses GENLEX.
3 Parsing Extensions: Combinators
This section describes a set of CCG combinators
which we add to the conventional CCG combinators
described in section 2.2. These additional combi-
nators are natural extensions of the forward appli-
cation, forward composition, and type-raising rules
seen in CCG. We first describe a set of combina-
tors that allow the parser to significantly relax con-
straints on word order. We then describe a set of
type-raising rules which allow the parser to cope
with telegraphic input (in particular, missing func-
tion words). In both cases these additional rules
lead to significantly more parses for any sentence
x given a lexicon ?. Many of these parses will be
suspect from a linguistic perspective; broadening the
set of CCG combinators in this way might be con-
sidered a dangerous move. However, the learning
algorithm in our approach can learn weights for the
new rules, effectively allowing the model to learn to
use them only in appropriate contexts; in the exper-
iments we show that the rules are highly effective
additions when used within a weighted CCG.
3.1 Application and Composition Rules
The first new combinators we consider are the
relaxed functional application rules:
A\B : f B : g ? A : f(g) (&)
B : g A/B : f ? A : f(g) (.)
These are variants of the original application
rules, where the slash direction on the principal cat-
egories (A/B or A\B) is reversed.6 These rules al-
low simple reversing of regular word order, for ex-
ample
flights one way
N N/N
?x.flight(x) ?f.?x.f(x) ? one way(x)
.
N
?x.flight(x) ? one way(x)
Note that we can recover the correct analysis for this
fragment, with the same lexical entries as those used
for the conventional word order, one-way flights.
A second set of new combinators are the relaxed
functional composition rules:
A\B : f B/C : g ? A/C : ?x.f(g(x)) (& B)
B\C : g A/B : f ? A\C : ?x.f(g(x)) (. B)
These rules are variantions of the standard func-
tional composition rules, where the slashes of the
principal categories are reversed.
6Rules of this type are non-standard in the sense that they
violate Steedman?s Principle of Consistency (2000); this princi-
ple states that rules must be consistent with the slash direction
of the principal category. Steedman (2000) only considers rules
that do not violate this principle?for example, crossed compo-
sition rules, which we consider later, and which Steedman also
considers, do not violate this principle.
681
An important point is that that these new compo-
sition and application rules can deal with quite flex-
ible word orders. For example, take the fragment to
washington the latest flight. In this case the parse is
to washington the latest flight
N\N NP/N N
?f.?x.f(x)? ?f. argmax(f, ?x.flight(x)
to(x,washington) ?y.depart time(y))
.B
NP\N
?f. argmax(?x.f(x)?
to(x,washington), ?y.depart time(y))
&
NP
argmax(?x.flight(x) ? to(x,washington),
?y.depart time(y))
Note that in this case the substring the latest has cat-
egory NP/N , and this prevents a naive parse where
the latest first combines with flight, and to washing-
ton then combines with the latest flight. The func-
tional composition rules effectively allow the latest
to take scope over flight and to washington, in spite
of the fact that the latest appears between the two
other sub-strings. Examples like this are quite fre-
quent in domains such as ATIS.
We add features in the model which track the oc-
currences of each of these four new combinators.
Specifically, we have four new features in the def-
inition of f; each feature tracks the number of times
one of the combinators is used in a CCG parse. The
model learns parameter values for each of these fea-
tures, allowing it to learn to penalise these rules to
the correct extent.
3.2 Additional Rules of Type-Raising
We now describe new CCG operations designed to
deal with cases where words are in some sense miss-
ing in the input. For example, in the string flights
Boston to New York, one style of analysis would
assume that the preposition from had been deleted
from the position before Boston.
The first set of rules is generated from the follow-
ing role-hypothesising type shifting rules template:
NP : c ? N\N : ?f.?x.f(x) ? p(x, c) (TR)
This rule can be applied to any NP with semantics
c, and any arity-two function p such that the second
argument of p has the same type as c. By ?any? arity-
two function, we mean any of the arity-two func-
tions seen in training data. We define features within
the feature-vector f that are sensitive to the number
of times these rules are applied in a parse; a separate
feature is defined for each value of p.
In practice, in our experiments most rules of this
form have p as the semantics of some preposition,
for example from or to. A typical example of a use
of this rule would be the following:
flights boston to new york
N NP N\N
?x.flight(x) bos ?f.?x.f(x)
?to(x, new york)
TR
N\N
?f.?x.f(x) ? from(x, bos)
<
N
?f.?x.flight(x) ? from(x, bos)
<
N
?x.flight(x) ? to(x, new york) ? from(x, bos)
The second rule we consider is the null-head type
shifting rule:
N\N : f ? N : f(?x.true) (TN)
This rule allows parses of fragments such as Amer-
ican Airlines from New York, where there is again a
word that is in some sense missing (it is straightfor-
ward to derive a parse for American Airlines flights
from New York). The analysis would be as follows:
American Airlines from New York
N/N N\N
?f.?x.f(x) ? airline(x, aa) ?f.?x.f(x) ? from(x, new york)
TN
N
?x.from(x, new york)
>
N
?x.airline(x, aa) ? from(x, new york)
The new rule effectively allows the preposi-
tional phrase from New York to type-shift to
an entry with syntactic type N and semantics
?x.from(x, new york), representing the set of all
things from New York.7
We introduce a single additional feature which
counts the number of times this rule is used.
3.3 Crossed Composition Rules
Finally, we include crossed functional composition
rules:
A/B : f B\C : g ? A\C : ?x.f(g(x)) (>B?)
B/C : g A\B : f ? A/C : ?x.f(g(x)) (<B?)
These rules are standard CCG operators but they
were not used by the parser described in ZC05.
When used in unrestricted contexts, they can sig-
nificantly relax word order. Again, we address this
7Note that we do not analyze this prepositional phrase as
having the semantics ?x.flight(x) ? from(x, new york)?
although in principle this is possible?as the flight(x) predi-
cate is not necessarily implied by this utterance.
682
dallas to washington the latest on friday
NP (N\N)/NP NP NP/N (N\N)/NP NP
dallas ?y.?f.?x.f(x) washington ?f. argmax(f, ?y.?f.?x.f(x) friday
?to(x, y) ?y.depart time(y)) ?day(x, y)
TR > >
N\N N\N N\N
?f.?x.f(x) ? from(x, dallas) ?f.?x.f(x) ? to(x,washington) ?f.?x.f(x) ? day(x, friday)
<B TN
N\N N
?f.?x.f(x) ? from(x, dallas) ? to(x,washington) ?x.day(x, friday)
.B
NP\N
?f. argmax(?x.f(x) ? from(x, dallas) ? to(x,washington), ?y.depart time(y))
&
NP
argmax(?x.day(x, friday) ? from(x, dallas) ? to(x,washington), ?y.depart time(y))
Figure 3: A parse with the flexible parser.
problem by introducing features that count the num-
ber of times they are used in a parse.8
3.4 An Example
As a final point, to see how these rules can interact
in practice, see figure 3. This example demonstrates
the use of the relaxed application and composition
rules, as well as the new type-raising rules.
4 Learning
This section describes an approach to learning in our
model. We first define the features used and then de-
scribe a new online learning algorithm for the task.
4.1 Features in the Model
Section 2.3 described the use of a function f(x, y)
which maps a sentence x together with a CCG parse
y to a feature vector. As described in section 3,
we introduce features for the new CCG combina-
tors. In addition, we follow ZC05 in defining fea-
tures which track the number of times each lexical
item in ? is used. For example, we would have one
feature tracking the number of times the lexical entry
flights := N : ?x.flights(x) is used in a parse,
and similar features for all other members of ?.
Finally, we introduce new features which directly
consider the semantics of a parse. For each predicate
f seen in training data, we introduce a feature that
counts the number of times f is conjoined with itself
at some level in the logical form. For example, the
expression ?x.flight(x) ? from(x, new york) ?
from(x, boston) would trigger the new feature for
8In general, applications of the crossed composition rules
can be lexically governed, as described in work on Multi-Modal
CCG (Baldridge, 2002). In the future we would like to incorpo-
rate more fine-grained lexical distinctions of this type.
the from predicate signaling that the logical-form
describes flights with more than one origin city. We
introduce similar features which track disjunction as
opposed to conjunction.
4.2 An Online Learning Algorithm
Figure 4 shows a learning algorithm that takes a
training set of (xi, zi) pairs as input, and returns
a weighted CCG (i.e., a pair (w,?)) as its output.
The algorithm is online, in that it visits each ex-
ample in turn, and updates both w and ? if neces-
sary. In Step 1 on each example, the input xi is
parsed. If it is parsed correctly, the algorithm im-
mediately moves to the next example. In Step 2,
the algorithm temporarily introduces all lexical en-
tries seen in GENLEX(xi, zi), and finds the highest
scoring parse that leads to the correct semantics zi.
A small subset of GENLEX(xi, zi)?namely, only
those lexical entries that are contained in the highest
scoring parse?are added to ?. In Step 3, a simple
perceptron update (Collins, 2002) is performed. The
hypothesis is parsed again with the new lexicon, and
an update to the parameters w is made if the result-
ing parse does not have the correct logical form.
This algorithm differs from the approach in ZC05
in a couple of important respects. First, the ZC05 al-
gorithm performed learning of the lexicon ? at each
iteration in a batch method, requiring a pass over the
entire training set. The new algorithm is fully online,
learning both ? and w in an example-by-example
fashion. This has important consequences for the
efficiency of the algorithm. Second, the parameter
estimation method in ZC05 was based on stochastic
gradient descent on a log-likelihood objective func-
tion. The new algorithm makes use of perceptron
683
Inputs: Training examples {(xi, zi) : i = 1 . . . n} where
each xi is a sentence, each zi is a logical form. An initial
lexicon ?0. Number of training iterations, T .
Definitions: GENLEX(x, z) takes as input a sentence x and
a logical form z and returns a set of lexical items as de-
scribed in section 2.4. GEN(x; ?) is the set of all parses
for x with lexicon ?. GEN(x, z; ?) is the set of all parses
for x with lexicon ?, which have logical form z. The
function f(x, y) represents the features described in sec-
tion 4.1. The function L(y) maps a parse tree y to its
associated logical form.
Initialization: Set parameters w to initial values described in
section 6.2. Set ? = ?0.
Algorithm:
? For t = 1 . . . T, i = 1 . . . n :
Step 1: (Check correctness)
? Let y? = argmaxy?GEN(xi;?) w ? f(xi, y) .
? If L(y?) = zi, go to the next example.
Step 2: (Lexical generation)
? Set ? = ? ? GENLEX(xi, zi) .
? Let y? = argmaxy?GEN(xi,zi;?) w ? f(xi, y) .
? Define ?i to be the set of lexical entries in y?.
? Set lexicon to ? = ? ? ?i .
Step 3: (Update parameters)
? Let y? = argmaxy?GEN(xi;?) w ? f(xi, y) .
? If L(y?) 6= zi :
? Set w = w + f(xi, y?) ? f(xi, y?) .
Output: Lexicon ? together with parameters w.
Figure 4: An online learning algorithm.
updates, which are simpler and cheaper to compute.
As in ZC05, the algorithm assumes an initial lex-
icon ?0 that contains two types of entries. First, we
compile entries such as Boston := NP : boston
for entities such as cities, times and month-names
that occur in the domain or underlying database. In
practice it is easy to compile a list of these atomic
entities. Second, the lexicon has entries for some
function words such as wh-words, and determiners.9
5 Related Work
There has been a significant amount of previ-
ous work on learning to map sentences to under-
lying semantic representations. A wide variety
9Our assumption is that these entries are likely to be domain
independent, so it is simple enough to compile a list that can
be reused in new domains. Another approach, which we may
consider in the future, would be to annotate a small subset of
the training examples with full CCG derivations, from which
these frequently occurring entries could be learned.
of techniques have been considered including ap-
proaches based on machine translation techniques
(Papineni et al, 1997; Ramaswamy and Kleindienst,
2000; Wong and Mooney, 2006), parsing techniques
(Miller et al, 1996; Ge and Mooney, 2006), tech-
niques that use inductive logic programming (Zelle
and Mooney, 1996; Thompson and Mooney, 2002;
Tang and Mooney, 2000; Kate et al, 2005), and
ideas from string kernels and support vector ma-
chines (Kate and Mooney, 2006; Nguyen et al,
2006). In our experiments we compare to He and
Young (2006) on the ATIS domain and Zettlemoyer
and Collins (2005) on the Geo880 domain, be-
cause these systems currently achieve the best per-
formance on these problems.
The approach of Zettlemoyer and Collins (2005)
was presented in section 2.4. He and Young (2005)
describe an algorithm that learns a probabilistic
push-down automaton that models hierarchical de-
pendencies but can still be trained on a data set that
does not have full treebank-style annotations. This
approach has been integrated with a speech recog-
nizer and shown to be robust to recognition errors
(He and Young, 2006).
There is also related work in the CCG litera-
ture. Clark and Curran (2003) present a method for
learning the parameters of a log-linear CCG pars-
ing model from fully annotated normal?form parse
trees. Watkinson and Manandhar (1999) present an
unsupervised approach for learning CCG lexicons
that does not represent the semantics of the train-
ing sentences. Bos et al (2004) present an al-
gorithm that learns CCG lexicons with semantics
but requires fully?specified CCG derivations in the
training data. Bozsahin (1998) presents work on us-
ing CCG to model languages with free word order.
In addition, there is related work that focuses on
modeling child language learning. Siskind (1996)
presents an algorithm that learns word-to-meaning
mappings from sentences that are paired with a set
of possible meaning representations. Villavicencio
(2001) describes an approach that learns a categorial
grammar with syntactic and semantic information.
Both of these approaches use sentences from child-
directed speech, which differ significantly from the
natural language interface queries we consider.
Finally, there is work on manually developing
parsing techniques to improve robustness (Carbonell
684
and Hayes, 1983; Seneff, 1992). In contrast, our ap-
proach is integrated into a learning framework.
6 Experiments
The main focus of our experiments is on the ATIS
travel planning domain. For development, we used
4978 sentences, split into a training set of 4500 ex-
amples, and a development set of 478 examples. For
test, we used the ATIS NOV93 test set which con-
tains 448 examples. To create the annotations, we
created a script that maps the original SQL annota-
tions provided with the data to lambda-calculus ex-
pressions.
He and Young (2006) previously reported results
on the ATIS domain, using a learning approach
which also takes sentences paired with semantic an-
notations as input. In their case, the semantic struc-
tures resemble context-free parses with semantic (as
opposed to syntactic) non-terminal labels. In our ex-
periments we have used the same split into train-
ing and test data as He and Young (2006), ensur-
ing that our results are directly comparable. He and
Young (2006) report partial match figures for their
parser, based on precision and recall in recovering
attribute-value pairs. (For example, the sentence
flights to Boston would have a single attribute-value
entry, namely destination = Boston.) It is sim-
ple for us to map from lambda-calculus expressions
to attribute-value entries of this form; for example,
the expression to(x,Boston) would be mapped to
destination = Boston. He and Young (2006) gave
us their data and annotations, so we can directly
compare results on the partial-match criterion. We
also report accuracy for exact matches of lambda-
calculus expressions, which is a stricter criterion.
In addition, we report results for the method on
the Geo880 domain. This allows us to compare
directly to the previous work of Zettlemoyer and
Collins (2005), using the same split of the data into
training and test sets of sizes 600 and 280 respec-
tively. We use cross-validation of the training set, as
opposed to a separate development set, for optimiza-
tion of parameters.
6.1 Improving Recall
The simplest approach to the task is to train the
parser and directly apply it to test sentences. In our
experiments we will see that this produces results
which have high precision, but somewhat lower re-
call, due to some test sentences failing to parse (usu-
ally due to words in the test set which were never
observed in training data). A simple strategy to alle-
viate this problem is as follows. If the sentence fails
to parse, we parse the sentence again, this time al-
lowing parse moves which can delete words at some
cost. The cost of this deletion operation is optimized
on development data. This approach can signifi-
cantly improve F-measure on the partial-match cri-
terion in particular. We report results both with and
without this second pass strategy.
6.2 Parameters in the Approach
The algorithm in figure 4 has a number of param-
eters, the set {T, ?, ?, ?}, which we now describe.
The values of these parameters were chosen to op-
timize the performance on development data. T is
the number of passes over the training set, and was
set to be 4. Each lexical entry in the initial lexicon
?0 has an associated feature which counts the num-
ber of times this entry is seen in a parse. The initial
parameter value in w for all features of this form
was chosen to be some value ?. Each of the new
CCG rules?the application, composition, crossed-
composition, and type-raising rules described in sec-
tion 3?has an associated parameter. We set al of
these parameters to the same initial value ?. Finally,
when new lexical entries are added to ? (in step 2
of the algorithm), their initial weight is set to some
value ?. In practice, optimization on development
data led to a positive value for ?, and negative val-
ues for ? and ?.
6.3 Results
Table 1 shows accuracy for the method by the exact-
match criterion on the ATIS test set. The two pass
strategy actually hurts F-measure in this case, al-
though it does improve recall of the method.
Table 2 shows results under the partial-match cri-
terion. The results for our approach are higher
than those reported by He and Young (2006) even
without the second, high-recall, strategy. With the
two-pass strategy our method has more than halved
the F-measure error rate, giving improvements from
90.3% F-measure to 95.9% F-measure.
Table 3 shows results on the Geo880 domain. The
685
Precision Recall F1
Single-Pass Parsing 90.61 81.92 86.05
Two-Pass Parsing 85.75 84.6 85.16
Table 1: Exact-match accuracy on the ATIS test set.
Precision Recall F1
Single-Pass Parsing 96.76 86.89 91.56
Two-Pass Parsing 95.11 96.71 95.9
He and Young (2006) ? ? 90.3
Table 2: Partial-credit accuracy on the ATIS test set.
new method gives improvements in performance
both with and without the two pass strategy, showing
that the new CCG combinators, and the new learn-
ing algorithm, give some improvement on even this
domain. The improved performance comes from a
slight drop in precision which is offset by a large in-
crease in recall.
Table 4 shows ablation studies on the ATIS data,
where we have selectively removed various aspects
of the approach, to measure their impact on perfor-
mance. It can be seen that accuracy is seriously de-
graded if the new CCG rules are removed, or if the
features associated with these rules (which allow the
model to penalize these rules) are removed.
Finally, we report results concerning the effi-
ciency of the new online algorithm as compared to
the ZC05 algorithm. We compared running times
for the new algorithm, and the ZC05 algorithm, on
the geography domain, with both methods making
4 passes over the training data. The new algorithm
took less than 4 hours, compared to over 12 hours
for the ZC05 algorithm. The main explanation for
this improved performance is that on many training
examples,10 in step 1 of the new algorithm a cor-
rect parse is found, and the algorithm immediately
moves on to the next example. Thus GENLEX is
not required, and in particular parsing the example
with the large set of entries generated by GENLEX
is not required.
7 Discussion
We presented a new, online algorithm for learn-
ing a combinatory categorial grammar (CCG), to-
gether with parameters that define a log-linear pars-
ing model. We showed that the use of non-standard
CCG combinators is highly effective for parsing sen-
10Measurements on the Geo880 domain showed that in the 4
iterations, 83.3% of all parses were successful at step 1.
Precision Recall F1
Single-Pass Parsing 95.49 83.2 88.93
Two-Pass Parsing 91.63 86.07 88.76
ZC05 96.25 79.29 86.95
Table 3: Exact-match accuracy on the Geo880 test set.
Precision Recall F1
Full Online Method 87.26 74.44 80.35
Without control features 70.33 42.45 52.95
Without relaxed word order 82.81 63.98 72.19
Without word insertion 77.31 56.94 65.58
Table 4: Exact-match accuracy on the ATIS development set
for the full algorithm and restricted versions of it. The sec-
ond row reports results of the approach without the features
described in section 3 that control the use of the new combi-
nators. The third row presents results without the combinators
from section 3.1 that relax word order. The fourth row reports
experiments without the type-raising combinators presented in
section 3.2.
tences with the types of phenomena seen in sponta-
neous, unedited natural language. The resulting sys-
tem achieved significant accuracy improvements in
both the ATIS and Geo880 domains.
Acknowledgements
Wewould like to thank Yulan He and Steve Young
for their help with obtaining the ATIS data set. We
also acknowledge the support for this research. Luke
Zettlemoyer was funded by a Microsoft graduate
research fellowship and Michael Collins was sup-
ported by the National Science Foundation under
grants 0347631 and DMS-0434222.
References
Jason Baldridge. 2002. Lexically Specified Derivational Con-
trol in Combinatory Categorial Grammar. Ph.D. thesis,
University of Edinburgh.
Johan Bos, Stephen Clark, Mark Steedman, James R. Curran,
and Julia Hockenmaier. 2004. Wide-coverage semantic rep-
resentations from a CCG parser. In Proceedings of the 20th
International Conference on Computational Linguistics.
Cem Bozsahin. 1998. Deriving the predicate-argument struc-
ture for a free word order language. In Proceedings of the
36th Annual Meeting of the Association for Computational
Linguistics.
Jaime G. Carbonell and Philip J. Hayes. 1983. Recovery strate-
gies for parsing extragrammatical language. American Jour-
nal of Computational Linguistics, 9.
Bob Carpenter. 1997. Type-Logical Semantics. The MIT Press.
Stephen Clark and James R. Curran. 2003. Log-linear models
for wide-coverage CCG parsing. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Process-
ing.
686
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Processing.
Michael Collins. 2004. Parameter estimation for statistical
parsing models: Theory and practice of distribution-free
methods. In Harry Bunt, John Carroll and Giorgio Satta,
editors, New Developments in Parsing Technology. Kluwer.
Deborah A. Dahl, Madeleine Bates, Michael Brown, William
Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao,
Alexander Rudnicky, and Elizabeth Shriberg. 1994. Ex-
panding the scope of the atis task: the atis-3 corpus. In ARPA
Human Language Technology Workshop.
Ruifang Ge and Raymond J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
Yulan He and Steve Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech and Lan-
guage.
Yulan He and Steve Young. 2006. Spoken language under-
standing using the hidden vector state model. Speech Com-
munication Special Issue on Spoken Language Understand-
ing for Conversational Systems.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ?unification-
based? grammars. In Proceedings of the Association for
Computational Linguistics.
Rohit J. Kate and Raymond J. Mooney. 2006. Using string-
kernels for learning semantic parsers. In Proceedings of the
44th Annual Meeting of the Association for Computational
Linguistics.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005.
Learning to transform natural to formal languages. In Pro-
ceedings of the 20th National Conference on Artificial Intel-
ligence.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of the
18th International Conference on Machine Learning.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach to ma-
chine translation. In Proceedings of the 44th Annual Meeting
of the Association for Computational Linguistics.
Scott Miller, David Stallard, Robert J. Bobrow, and Richard L.
Schwartz. 1996. A fully statistical approach to natural lan-
guage interfaces. In Proceedings of the Association for Com-
putational Linguistics.
Raymond J. Mooney. 2007. Learning for semantic parsing. In
Computational Linguistics and Intelligent Text Processing:
Proceedings of the 8th International Conference.
Le-Minh Nguyen, Akira Shimazu, and Xuan-Hieu Phan. 2006.
Semantic parsing with structured SVM ensemble classifica-
tion models. In Proceedings of the COLING/ACL 2006Main
Conference Poster Sessions.
K. A. Papineni, S. Roukos, and T. R. Ward. 1997. Feature-
based language understanding. In Proceedings of European
Conference on Speech Communication and Technology.
Ganesh N. Ramaswamy and Jan Kleindienst. 2000. Hierar-
chical feature-based translation for scalable natural language
understanding. In Proceedings of 6th International Confer-
ence on Spoken Language Processing.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994.
A maximum entropy model for parsing. In Proceedings of
the International Conference on Spoken Language Process-
ing.
Stephanie Seneff. 1992. Robust parsing for spoken language
systems. In Proceedings of the IEEE Conference on Acous-
tics, Speech, and Signal Processing.
Jeffrey M. Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning map-
pings. Cognition, 61(2-3).
Mark Steedman. 1996. Surface Structure and Interpretation.
The MIT Press.
Mark Steedman. 2000. The Syntactic Process. The MIT Press.
Lappoon R. Tang and Raymond J. Mooney. 2000. Automated
construction of database interfaces: Integrating statistical
and relational learning for semantic parsing. In Joint Confer-
ence on Empirical Methods in Natural Language Processing
and Very Large Corpora.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In Pro-
ceedings of the Conference on Empirical Methods in Natural
Language Processing.
Cynthia A. Thompson and Raymond J. Mooney. 2002. Acquir-
ing word-meaning mappings for natural language interfaces.
Journal of Artificial Intelligence Research, 18.
Aline Villavicencio. 2001. The acquisition of a unification-
based generalised categorial grammar. Ph.D. thesis, Uni-
versity of Cambridge.
Stephen Watkinson and Suresh Manandhar. 1999. Unsuper-
vised lexical learning with categorial grammars using the
LLL corpus. In Proceedings of the 1st Workshop on Learn-
ing Language in Logic.
Yuk Wah Wong and Raymond Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In Pro-
ceedings of the Human Language Technology Conference of
the NAACL.
John M. Zelle and Raymond J. Mooney. 1996. Learning to
parse database queries using inductive logic programming.
In Proceedings of the 14th National Conference on Artificial
Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proceedings of
the 21st Conference on Uncertainty in Artificial Intelligence.
687
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 783?792,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Generative Model for Parsing Natural Language to Meaning
Representations
Wei Lu1, Hwee Tou Ng1,2, Wee Sun Lee1,2
1Singapore-MIT Alliance
2Department of Computer Science
National University of Singapore
luwei@nus.edu.sg
{nght,leews}@comp.nus.edu.sg
Luke S. Zettlemoyer
CSAIL
Massachusetts Institute of Technology
lsz@csail.mit.edu
Abstract
In this paper, we present an algorithm for
learning a generative model of natural lan-
guage sentences together with their for-
mal meaning representations with hierarchi-
cal structures. The model is applied to the
task of mapping sentences to hierarchical rep-
resentations of their underlying meaning. We
introduce dynamic programming techniques
for efficient training and decoding. In exper-
iments, we demonstrate that the model, when
coupled with a discriminative reranking tech-
nique, achieves state-of-the-art performance
when tested on two publicly available cor-
pora. The generative model degrades robustly
when presented with instances that are differ-
ent from those seen in training. This allows
a notable improvement in recall compared to
previous models.
1 Introduction
To enable computers to understand natural human
language is one of the classic goals of research in
natural language processing. Recently, researchers
have developed techniques for learning to map sen-
tences to hierarchical representations of their under-
lying meaning (Wong and Mooney, 2006; Kate and
Mooney, 2006).
One common approach is to learn some form of
probabilistic grammar which includes a list of lexi-
cal items that models the meanings of input words
and also includes rules for combining lexical mean-
ings to analyze complete sentences. This approach
performs well but is constrained by the use of a sin-
gle, learned grammar that contains a fixed set of
lexical entries and productions. In practice, such
a grammar may lack the rules required to correctly
parse some of the new test examples.
In this paper, we develop an alternative approach
that learns a model which does not make use of
an explicit grammar but, instead, models the cor-
respondence between sentences and their meanings
with a generative process. This model is defined
over hybrid trees whose nodes include both natu-
ral language words and meaning representation to-
kens. Inspired by the work of Collins (2003), the
generative model builds trees by recursively creating
nodes at each level according to a Markov process.
This implicit grammar representation leads to flexi-
ble learned models that generalize well. In practice,
we observe that it can correctly parse a wider range
of test examples than previous approaches.
The generative model is learned from data that
consists of sentences paired with their meaning rep-
resentations. However, there is no explicit labeling
of the correspondence between words and meaning
tokens that is necessary for building the hybrid trees.
This creates a challenging, hidden-variable learning
problem that we address with the use of an inside-
outside algorithm. Specifically, we develop a dy-
namic programming parsing algorithm that leads to
O(n3m) time complexity for inference, where n is
the sentence length and m is the size of meaning
structure. This approach allows for efficient train-
ing and decoding.
In practice, we observe that the learned generative
models are able to assign a high score to the correct
meaning for input sentences, but that this correct
meaning is not always the highest scoring option.
783
To address this problem, we use a simple rerank-
ing approach to select a parse from a k-best list of
parses. This pipelined approach achieves state-of-
the-art performance on two publicly available cor-
pora. In particular, the flexible generative model
leads to notable improvements in recall, the total
percentage of sentences that are correctly parsed.
2 Related Work
In Section 9, we will compare performance with
the three existing systems that were evaluated on
the same data sets we consider. SILT (Kate et al,
2005) learns deterministic rules to transform either
sentences or their syntactic parse trees to meaning
structures. WASP (Wong and Mooney, 2006) is a
system motivated by statistical machine translation
techniques. It acquires a set of synchronous lexical
entries by running the IBM alignment model (Brown
et al, 1993) and learns a log-linear model to weight
parses. KRISP (Kate and Mooney, 2006) is a dis-
criminative approach where meaning representation
structures are constructed from the natural language
strings hierarchically. It is built on top of SVMstruct
with string kernels.
Additionally, there is substantial related research
that is not directly comparable to our approach.
Some of this work requires different levels of super-
vision, including labeled syntactic parse trees (Ge
and Mooney, 2005; Ge and Mooney, 2006). Others
do not perform lexical learning (Tang and Mooney,
2001). Finally, recent work has explored learning
to map sentences to lambda-calculus meaning rep-
resentations (Wong and Mooney, 2007; Zettlemoyer
and Collins, 2005; Zettlemoyer and Collins, 2007).
3 Meaning Representation
We restrict our meaning representation (MR) for-
malism to a variable free version as presented in
(Wong and Mooney, 2006; Kate et al, 2005).
A training instance consists of a natural language
sentence (NL sentence) and its corresponding mean-
ing representation structure (MR structure). Con-
sider the following instance taken from the GEO-
QUERY corpus (Kate et al, 2005):
The NL sentence ?How many states do
not have rivers ?? consists of 8 words, in-
cluding punctuation. The MR is a hierarchical tree
QUERY : answer (NUM)
NUM : count (STATE)
STATE : exclude (STATE STATE)
STATE : state (all) STATE : loc 1 (RIVER)
RIVER : river (all)
Figure 1: An example MR structure
structure, as shown in Figure 1.
Following an inorder traversal of this MR tree, we
can equivalently represent it with the following list
of meaning representation productions (MR produc-
tions):
(0) QUERY : answer (NUM)
(1) NUM : count (STATE)
(2) STATE : exclude (STATE1 STATE2)
(3) STATE : state (all)
(4) STATE : loc 1 (RIVER)
(5) RIVER : river (all)
Each such MR production consists of three com-
ponents: a semantic category, a function symbol
which can be omitted (considered empty), and a list
of arguments. An argument can be either a child se-
mantic category or a constant. Take production (1)
for example: it has a semantic category ?NUM?, a
function symbol ?count?, and a child semantic cate-
gory ?STATE? as its only argument. Production (5)
has ?RIVER? as its semantic category, ?river? as the
function symbol, and ?all? is a constant.
4 The Generative Model
We describe in this section our proposed generative
model, which simultaneously generates a NL sen-
tence and an MR structure.
We denote a single NL word as w, a contiguous
sequence of NL words as w, and a complete NL
sentence as w?. In the MR structure, we denote a
semantic category as M. We denote a single MR
production as ma, or Ma : p?(Mb,Mc), where Ma
is the semantic category for this production, p? is the
function symbol, and Mb,Mc are the child semantic
categories. We denote ma as an MR structure rooted
by an MR production ma, and m?a an MR structure
for a complete sentence rooted by an MR production
ma.
The model generates a hybrid tree that represents
a sentence w? = w1 . . . w2 . . . paired with an MR
structure m?a rooted by ma.
784
Ma
ma
w1 Mb
mb
. . . . . .
w2 Mc
mc
. . . . . .
Figure 2: The generation process
Figure 2 shows part of a hybrid tree that is gen-
erated as follows. Given a semantic category Ma,
we first pick an MR production ma that has the form
Ma : p?(Mb,Mc), which gives us the function sym-
bol p? as well as the child semantic categories Mb
and Mc. Next, we generate the hybrid sequence of
child nodes w1 Mb w2 Mc, which consists of NL
words and semantic categories.
After that, two child MR productions mb and mc
are generated. These two productions will in turn
generate other hybrid sequences and productions, re-
cursively. This process produces a hybrid tree T ,
whose nodes are either NL words or MR produc-
tions. Given this tree, we can recover a NL sentence
w by recording the NL words visited in depth-first
traversal order and can recover an MR structure m
by following a tree-specific traversal order, defined
by the hybrid-patterns we introduce below. Figure 3
gives a partial hybrid tree for the training example
from Section 3. Note that the leaves of a hybrid tree
are always NL tokens.
. . .
STATE
STATE : exclude (STATE STATE)
STATE
STATE : state(all)
states
do not STATE
STATE : loc 1(RIVER)
have RIVER
RIVER : river(all)
rivers
Figure 3: A partial hybrid tree
With several independence assumptions, the
probability of generating
?
w?, m?,T ? is defined as:
P(w?, m?,T ) = P(Ma) ? P(ma|Ma) ? P(w1 Mb w2 Mc|ma)
?P(mb|ma, arg = 1) ? P(. . . |mb)
?P(mc|ma, arg = 2) ? P(. . . |mc) (1)
where ?arg? refers to the position of the child se-
mantic category in the argument list.
Motivated by Collins? syntactic parsing models
(Collins, 2003), we consider the generation process
for a hybrid sequence from an MR production as a
Markov process.
Given the assumption that each MR production
has at most two semantic categories in its arguments
(any production can be transformed into a sequence
of productions of this form), Table 1 includes the list
of all possible hybrid patterns.
# RHS Hybrid Pattern # Patterns
0 m ? w 1
1 m ? [w]Y[w] 4
2 m ? [w]Y[w]Z[w] 8m ? [w]Z[w]Y[w] 8
Table 1: A list of hybrid patterns, [] denotes optional
In this table, m is an MR production, Y and Z
are respectively the first and second child seman-
tic category in m?s argument list. The symbol w
refers to a contiguous sequence of NL words, and
anything inside [] can be optionally omitted. The
last row contains hybrid patterns that reflect reorder-
ing of one production?s child semantic categories
during the generation process. For example, con-
sider the case that the MR production STATE :
exclude (STATE1 STATE2) generates a hybrid se-
quence STATE1 do not STATE2, the hybrid pattern
m ? YwZ is associated with this generation step.
For the example hybrid tree in Figure 2, we can
decompose the probability for generating the hybrid
sequence as follows:
P(w1 Mb w2 Mc|ma) = P(m ? wYwZ|ma) ? P(w1|ma)
?P(Mb|ma, w1) ? P(w2|ma, w1,Mb)
?P(Mc|ma, w1,Mb, w2) ? P(END|ma, w1,Mb, w2,Mc) (2)
Note that unigram, bigram, or trigram assump-
tions can be made here for generating NL words and
semantic categories. For example, under a bigram
assumption, the second to last term can be written
as P(Mc|ma, w1,Mb, w2) ? P(Mc|ma, wk2), where
wk2 is the last word in w2. We call such additional
information that we condition on, the context.
Note that our generative model is different from
the synchronous context free grammars (SCFG) in
a number of ways. A standard SCFG produces a
correspondence between a pair of trees while our
model produces a single hybrid tree that represents
785
the correspondence between a sentence and a tree.
Also, SCFGs use a finite set of context-free rewrite
rules to define the model, where the rules are possi-
bly weighted. In contrast, we make use of the more
flexible Markov models at each level of the genera-
tive process, which allows us to potentially produce
a far wider range of possible trees.
5 Parameter Estimation
There are three categories of parameters used in the
model. The first category of parameters models
the generation of new MR productions from their
parent MR productions: e.g., P(mb|ma, arg = 1);
the second models the generation of a hybrid se-
quence from an MR production: e.g., P(w1|ma),
P(Mb|ma, w1); the last models the selection of a hy-
brid pattern given an MR production, e.g., P(m ?
wY|ma). We will estimate parameters from all cate-
gories, with the following constraints:
1.
?
m? ?(m?|m j, arg=k)=1 for all j and k = 1, 2.
These parameters model the MR structures, and
can be referred to as MR model parameters.
2.
?
t ?(t|m j,?)=1 for all j, where t is a NL word,
the ?END? symbol, or a semantic category. ?
is the context associated with m j and t.
These parameters model the emission of NL
words, the ?END? symbol, and child semantic
categories from an MR production. We call
them emission parameters.
3.
?
r ?(r|m j) = 1 for all j, where r is a hybrid
pattern listed in Table 1.
These parameters model the selection of hybrid
patterns. We name them pattern parameters.
With different context assumptions, we reach dif-
ferent variations of the model. In particular, we con-
sider three assumptions, as follows:
Model I We make the following assumption:
?(tk|m j,?) = P(tk|m j) (3)
where tk is a semantic category or a NL word, and
m j is an MR production.
In other words, generation of the next NL word
depends on its direct parent MR production only.
Such a Unigram Model may help in recall (the num-
ber of correct outputs over the total number of in-
puts), because it requires the least data to estimate.
Model II We make the following assumption:
?(tk|m j,?) = P(tk|m j, tk?1) (4)
where tk?1 is the semantic category or NL word to
the left of tk, i.e., the previous semantic category or
NL word.
In other words, generation of the next NL word
depends on its direct parent MR production as well
as the previously generated NL word or semantic
category only. This model is also referred to as Bi-
gram Model. This model may help in precision (the
number of correct outputs over the total number of
outputs), because it conditions on a larger context.
Model III We make the following assumption:
?(tk|m j,?) =
1
2 ?
(
P(tk|m j) + P(tk|m j, tk?1)
)
(5)
We can view this model, called the Mixgram
Model, as an interpolation between Model I and II.
This model gives us a balanced score for both preci-
sion and recall.
5.1 Modeling Meaning Representation
The MR model parameters can be estimated inde-
pendently from the other two. These parameters can
be viewed as the ?language model? parameters for
the MR structure, and can be estimated directly from
the corpus by simply reading off the counts of occur-
rences of MR productions in MR structures over the
training corpus. To resolve data sparseness problem,
a variant of the bigram Katz Back-Off Model (Katz,
1987) is employed here for smoothing.
5.2 Learning the Generative Parameters
Learning the remaining two categories of parameters
is more challenging. In a conventional PCFG pars-
ing task, during the training phase, the correct cor-
respondence between NL words and syntactic struc-
tures is fully accessible. In other words, there is a
single deterministic derivation associated with each
training instance. Therefore model parameters can
be directly estimated from the training corpus by
counting. However, in our task, the correct corre-
spondence between NL words and MR structures is
unknown. Many possible derivations could reach
the same NL-MR pair, where each such derivation
forms a hybrid tree.
786
The hybrid tree is constructed using hidden vari-
ables and estimated from the training set. An effi-
cient inside-outside style algorithm can be used for
model estimation, similar to that used in (Yamada
and Knight, 2001), as discussed next.
5.2.1 The Inside-Outside Algorithm with EM
In this section, we discuss how to estimate the
emission and pattern parameters with the Expecta-
tion Maximization (EM) algorithm (Dempster et al,
1977), by using an inside-outside (Baker, 1979) dy-
namic programming approach.
Denote ni ? ?mi, wi? as the i-th training instance,
where mi and wi are the MR structure and the NL
sentence of the i-th instance respectively. We also
denote nv ? ?mv, wv? as an aligned pair of MR
substructure and contiguous NL substring, where
the MR substructure rooted by MR production mv
will correspond to (i.e., hierarchically generate) the
NL substring wv. The symbol h is used to de-
note a hybrid sequence, and the function Parent(h)
gives the unique MR substructure-NL subsequence
pair which can be decomposed as h. Parent(nv) re-
turns the set of all possible hybrid sequences un-
der which the pair nv can be generated. Similarly,
Children(h) gives the NL-MR pairs that appear di-
rectly below the hybrid sequence h in a hybrid tree,
and Children(n) returns the set of all possible hybrid
sequences that n can be decomposed as. Figure 4
gives a packed tree structure representing the rela-
tions between the entities.
hp1 ? Parent(nv) . . . . . . hpm ? Parent(nv)
nv? ? ?mv? , wv? ? nv ? ?mv, wv?
hc1 ? Children(nv) . . . . . . hcn ? Children(nv)
Hybrid Sequence Contains
Can be Decomposed As
Figure 4: A packed tree structure representing the relations
between hybrid sequences and NL-MR pairs
The formulas for computing inside and outside
probabilities as well as the equations for updating
parameters are given in Figure 5. We use a CKY-
style parse chart for tracking the probabilities.
5.2.2 Smoothing
It is reasonable to believe that different MR pro-
ductions that share identical function symbols are
likely to generate NL words with similar distribu-
tion, regardless of semantic categories. For example,
The inside (?) probabilities are defined as
? If nv ? ?mv, wv? is leaf
?(nv) = P(wv|mv) (6)
? If nv ? ?mv, wv? is not leaf
?(nv) =
?
h?Children(nv)
(
P(h|mv) ?
?
nv??Children(h)
?(nv? )
)
(7)
The outside (?) probabilities are defined as
? If nv ? ?mv, wv? is root
?(nv) = 1 (8)
? If nv ? ?mv, wv? is not root
?(nv) =
?
h?Parent(nv)
(
?
(
Parent(h)
)
?P
(
h|Parent(h)
)
?
?
nv??Children(h),v?,v
?(nv? )
)
(9)
Parameter Update
? Update the emission parameter
The count ci(t, mv,?k), where t is a NL word
or a semantic category, for an instance pair ni ?
?mi, wi?:
ci(t, mv,?k) =
1
?(ni) ?
?
(t,mv ,?k) in h?Children(mv)
(
?(niv)
?P(h|mv) ?
?
niv??Children(h)
?(niv? )
)
The emission parameter is re-estimated as:
??(t|mv,?k) =
?
i ci(t, mv,?k)?
t?
?
i ci(t?, mv,?k)
(10)
? Update the pattern parameter
The count ci(r, mv), where r is a hybrid pattern,
for an instance pair ni ? ?mi, wi?:
ci(r, mv) =
1
?(ni) ?
?
(r,mv) in h?Children(mv)
(
?(niv)
?P(h|mv) ?
?
niv??Children(h)
?(niv? )
)
The pattern parameter is re-estimated as:
??(r|mv) =
?
i ci(r, mv)?
r?
?
i ci(r?, mv)
(11)
Figure 5: The inside/outside formulas as well as update
equations for EM
RIVER : largest (RIVER) and CITY : largest (CITY)
are both likely to generate the word ?biggest?.
In view of this, a smoothing technique is de-
ployed. We assume half of the time words can
787
be generated from the production?s function symbol
alone if it is not empty. Mathematically, assuming
ma with function symbol pa, for a NL word or se-
mantic category t, we have:
?(t|ma,?) =
{ ?e(t|ma,?) If pa is empty(
?e(t|ma,?) + ?e(t|pa,?)
)
/2 otherwise
where ?e models the generation of t from an MR
production or its function symbol, together with the
context ?.
6 A Dynamic Programming Algorithm for
Inside-Outside Computation
Though the inside-outside approach already em-
ploys packed representations for dynamic program-
ming, a naive implementation of the inference algo-
rithm will still require O(n6m) time for 1 EM iter-
ation, where n and m are the length of the NL sen-
tence and the size of the MR structure respectively.
This is not very practical as in one of the corpora we
look at, n and m can be up to 45 and 20 respectively.
In this section, we develop an efficient dynamic
programming algorithm that enables the inference
to run in O(n3m) time. The idea is as follows. In-
stead of treating each possible hybrid sequence as
a separate rule, we efficiently aggregate the already
computed probability scores for hybrid sequences
that share identical hybrid patterns. Such aggregated
scores can then be used for subsequent computa-
tions. By doing this, we can effectively avoid a large
amount of redundant computations. The algorithm
supports both unigram and bigram context assump-
tions. For clarity and ease of presentation, we pri-
marily make the unigram assumption throughout our
discussion.
We use ? (mv, wv) to denote the inside probabil-
ity for mv-wv pair, br[mv, wv, c] to denote the aggre-
gated probabilities for the MR sub-structure mv to
generate all possible hybrid sequences based on wv
with pattern r that covers its c-th child only. In addi-
tion, we use w(i, j) to denote a subsequence of w with
start index i (inclusive) and end index j (exclusive).
We also use ?r~mv, wv to denote the aggregated in-
side probability for the pair ?mv, wv?, if the hybrid
pattern is restricted to r only. By definition we have:
? (mv, wv) =
?
r
?(r|mv)??r~mv, wv??(END|mv) (12)
Relations between ?r and br can also be estab-
lished. For example, if mv has one child semantic
category, we have:
?m?wY~mv, wv = bm?wY[mv, wv, 1] (13)
For the case when mv has two child semantic cat-
egories as arguments, we have, for example:
?m?wYZw~mv, w(i, j) =
?
i+2?k? j?2
bm?wY[mv, w(i,k), 1]
?bm?Yw[mv, w(k, j), 2] (14)
Note that there also exist relations amongst b
terms for more efficient computation, for example:
bm?wY[mv, w(i, j), c] = ?(wi|mv)
?
(
bm?wY[mv, w(i+1, j), c] + bm?Y[mv, w(i+1, j), c]
)
(15)
Analogous but more complex formulas are used
for computing the outside probabilities. Updating of
parameters can be incorporated into the computation
of outside probabilities efficiently.
7 Decoding
In the decoding phase, we want to find the optimal
MR structure m?? given a new NL sentence w?:
m?
? = arg max
m?
P(m?|w?) = arg max
m?
?
T
P(m?,T |w?) (16)
where T is a possible hybrid tree associated with
the m?-w? pair. However, it is expensive to compute
the summation over all possible hybrid trees. We
therefore find the most likely hybrid tree instead:
m?
?=arg max
m?
max
T
P(m?,T |w?)=arg max
m?
max
T
P(w?, m?,T ) (17)
We have implemented an exact top-k decoding al-
gorithm for this task. Dynamic programming tech-
niques similar to those discussed in Section 6 can
also be applied when retrieving the top candidates.
We also find the Viterbi hybrid tree given a NL-
MR pair, which can be done in an analogous way.
This tree will be useful for reranking.
8 Reranking and Filtering of Predictions
Due to the various independence assumptions we
have made, the model lacks the ability to express
some long range dependencies. We therefore post-
process the best candidate predictions with a dis-
criminative reranking algorithm.
788
Feature Type Description Example
1. Hybrid Rule A MR production and its child hybrid form f1 : STATE : loc 1(RIVER) ? have RIVER
2. Expanded Hybrid Rule A MR production and its child hybrid form expanded f2 : STATE : loc 1(RIVER) ? ?have, RIVER : river(all)?
3. Long-range Unigram A MR production and a NL word appearing below in tree f3 : STATE : exclude(STATE STATE) ? rivers
4. Grandchild Unigram A MR production and its grandchild NL word f4 : STATE : loc 1(RIVER) ? rivers
5. Two Level Unigram A MR production, its parent production, and its child NL word f5 : ?RIVER : river(all), STATE : loc 1(RIVER)? ? rivers
6. Model Log-Probability Logarithm of base model?s joint probability log (P?(w, m,T )).
Table 2: All the features used. There is one feature for each possible combination, under feature type 1-5. It takes value 1 if
the combination is present, and 0 otherwise. Feature 6 takes real values.
8.1 The Averaged Perceptron Algorithm with
Separating Plane
The averaged perceptron algorithm (Collins, 2002)
has previously been applied to various NLP tasks
(Collins, 2002; Collins, 2001) for discriminative
reranking. The detailed algorithm can be found in
(Collins, 2002). In this section, we extend the con-
ventional averaged perceptron by introducing an ex-
plicit separating plane on the feature space.
Our reranking approach requires three compo-
nents during training: a GEN function that defines
for each NL sentence a set of candidate hybrid trees;
a single correct reference hybrid tree for each train-
ing instance; and a feature function ? that defines a
mapping from a hybrid tree to a feature vector. The
algorithm learns a weight vector w that associates a
weight to each feature, such that a score w??(T ) can
be assigned to each candidate hybrid tree T . Given
a new instance, the hybrid tree with the highest score
is then picked by the algorithm as the output.
In this task, the GEN function is defined as the
output hybrid trees of the top-k (k is set to 50 in our
experiments) decoding algorithm, given the learned
model parameters. The correct reference hybrid tree
is determined by running the Viterbi algorithm on
each training NL-MR pair. The feature function is
discussed in section 8.2.
While conventional perceptron algorithms usually
optimize the accuracy measure, we extend it to allow
optimization of the F-measure by introducing an ex-
plicit separating plane on the feature space that re-
jects certain predictions even when they score high-
est. The idea is to find a threshold b after w is
learned, such that a prediction with score below b
gets rejected. We pick the threshold that leads to the
optimal F-measure when applied to the training set.
8.2 Features
We list in Table 2 the set of features we used. Ex-
amples are given based on the hybrid tree in Figure
3. Some of the them are adapted from (Collins and
Koo, 2005) for a natural language parsing task. Fea-
tures 1-5 are indicator functions (i.e., it takes value
1 if a certain combination as the ones listed in Table
2 is present, 0 otherwise), while feature 6 is real val-
ued. Features that do not appear more than once in
the training set are discarded.
9 Evaluation
Our evaluations were performed on two corpora,
GEOQUERY and ROBOCUP. The GEOQUERY cor-
pus contains MR defined by a Prolog-based lan-
guage used in querying a database on U.S. geogra-
phy. The ROBOCUP corpus contains MR defined by
a coaching language used in a robot coaching com-
petition. There are in total 880 and 300 instances for
the two corpora respectively. Standard 10-fold cross
validations were performed and the micro-averaged
results are presented in this section. To make our
system directly comparable to previous systems, all
our experiments were based on identical training and
test data splits of both corpora as reported in the ex-
periments of Wong and Mooney (2006).
9.1 Training Methodology
Given a training set, we first run a variant of IBM
alignment model 1 (Brown et al, 1993) for 100 iter-
ations, and then initialize Model I with the learned
parameter values. This IBM model is a word-to-
word alignment model that does not model word
order, so we do not have to linearize the hierarchi-
cal MR structure. Given this initialization, we train
Model I for 100 EM iterations and use the learned
parameters to initialize Model II which is trained for
another 100 EM iterations. Model III is simply an
interpolation of the above two models. As for the
reranking phase, we initialize the weight vector with
the zero vector 0, and run the averaged perceptron
algorithm for 10 iterations.
789
9.2 Evaluation Methodology
Following Wong (2007) and other previous work,
we report performance in terms of Precision (per-
centage of answered NL sentences that are correct),
Recall (percentage of correctly answered NL sen-
tences, out of all NL sentences) and F-score (har-
monic mean of Precision and Recall).
Again following Wong (2007), we define the cor-
rect output MR structure as follows. For the GEO-
QUERY corpus, an MR structure is considered cor-
rect if and only if it retrieves identical results as
the reference MR structure when both are issued as
queries to the underlying Prolog database. For the
ROBOCUP corpus, an MR structure is considered
correct if and only if it has the same string represen-
tation as the reference MR structure, up to reorder-
ing of children of MR productions whose function
symbols are commutative, such as and, or, etc.
9.3 Comparison over Three Models
Model GEOQUERY (880) ROBOCUP (300)Prec. Rec. F Prec. Rec. F
I 81.3 77.1 79.1 71.1 64.0 67.4
II 89.0 76.0 82.0 82.4 57.7 67.8
III 86.2 81.8 84.0 70.4 63.3 66.7
I+R 87.5 80.5 83.8 79.1 67.0 72.6
II+R 93.2 73.6 82.3 88.4 56.0 68.6
III+R 89.3 81.5 85.2 82.5 67.7 74.4
Table 3: Performance comparison over three models
(Prec.:precision, Rec.:recall, +R: with reranking)
We evaluated the three models, with and with-
out reranking. The results are presented in Table 3.
Comparing Model I and Model II, we noticed that
for both corpora, Model I in general achieves bet-
ter recall while Model II achieves better precision.
This observation conforms to our earlier expecta-
tions. Model III, as an interpolation of the above two
models, achieves a much better F-measure on GEO-
QUERY corpus. However, it is shown to be less ef-
fective on ROBOCUP corpus. We noticed that com-
pared to the GEOQUERY corpus, ROBOCUP corpus
contains longer sentences, larger MR structures, and
a significant amount of non-compositionality. These
factors combine to present a challenging problem for
parsing with the generative model. Interestingly, al-
though Model III fails to produce better best pre-
dictions for this corpus, we found that its top-k list
contains a relatively larger number of correct pre-
dictions than Model I or Model II. This indicates
the possibility of enhancing the performance with
reranking.
The reranking approach is shown to be quite ef-
fective. We observe a consistent improvement in
both precision and F-measure after employing the
reranking phase for each model.
9.4 Comparison with Other Models
Among all the previous models, SILT, WASP, and
KRISP are directly comparable to our model. They
required the same amount of supervision as our sys-
tem and were evaluated on the same corpora.
We compare our model with these models in Ta-
ble 4, where the performance scores for the previous
systems are taken from (Wong, 2007). For GEO-
QUERY corpus, our model performs substantially
better than all the three previous models, with a no-
table improvement in the recall score. In fact, if we
look at the recall scores alone, our best-performing
model achieves a 6.7% and 9.8% absolute improve-
ment over two other state-of-the-art models WASP
and KRISP respectively. This indicates that over-
all, our model is able to handle over 25% of the
inputs that could not be handled by previous sys-
tems. On the other hand, in terms of F-measure,
we gain a 4.1% absolute improvement over KRISP,
which leads to an error reduction rate of 22%. On
the ROBOCUP corpus, our model?s performance is
also ranked the highest1.
System GEOQUERY (880) ROBOCUP (300)Prec. Rec. F Prec. Rec. F
SILT 89.0 54.1 67.3 83.9 50.7 63.2
WASP 87.2 74.8 80.5 88.9 61.9 73.0
KRISP 93.3 71.7 81.1 85.2 61.9 71.7
Model III+R 89.3 81.5 85.2 82.5 67.7 74.4
Table 4: Performance comparison with other directly com-
parable systems
9.5 Performance on Other Languages
As a generic model that requires minimal assump-
tions on the natural language, our model is natural
language independent and is able to handle various
other natural languages than English. To validate
this point, we evaluated our system on a subset of
1We are unable to perform statistical significance tests be-
cause the detailed performance for each fold of previously pub-
lished research work is not available.
790
the GEOQUERY corpus consisting of 250 instances,
with four different NL annotations.
As we can see from Table 5, our model is able
to achieve performance comparable to WASP as re-
ported by Wong (2007).
System English SpanishPrec. Rec. F Prec. Rec. F
WASP 95.42 70.00 80.76 91.99 72.40 81.03
Model III+R 91.46 72.80 81.07 95.19 79.20 86.46
System Japanese TurkishPrec. Rec. F Prec. Rec. F
WASP 91.98 74.40 82.86 96.96 62.40 75.93
Model III+R 87.56 76.00 81.37 93.82 66.80 78.04
Table 5: Performance on different natural languages for
GEOQUERY-250 corpus
Our model is generic, which requires no domain-
dependent knowledge and should be applicable to
a wide range of different domains. Like all re-
search in this area, the ultimate goal is to scale to
more complex, open-domain language understand-
ing problems. In future, we would like to create a
larger corpus in another domain with multiple natu-
ral language annotations to further evaluate the scal-
ability and portability of our approach.
10 Conclusions
We presented a new generative model that simulta-
neously produces both NL sentences and their cor-
responding MR structures. The model can be effec-
tively applied to the task of transforming NL sen-
tences to their MR structures. We also developed
a new dynamic programming algorithm for efficient
training and decoding. We demonstrated that this
approach, augmented with a discriminative rerank-
ing technique, achieves state-of-the-art performance
when tested on standard benchmark corpora.
In future, we would like to extend the current
model to have a wider range of support of MR for-
malisms, such as the one with lambda-calculus sup-
port. We are also interested in investigating ways to
apply the generative model to the inverse task: gen-
eration of a NL sentence that explains a given MR
structure.
Acknowledgments
The authors would like to thank Leslie Pack Kael-
bling for her valuable feedback and comments on
this research. The authors would also like to thank
the anonymous reviewers for their thoughtful com-
ments on this paper. The research is partially sup-
ported by ARF grant R-252-000-240-112.
References
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. Journal of the Acoustical Society of America,
65:S132.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?70.
M. Collins. 2001. Ranking algorithms for named-entity
extraction: boosting and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2002), pages
489?496.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: theory and experiments with
perceptron algorithms. In Proceedings of the 2002
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002), pages 1?8.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?38.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL 2005), pages 9?
16.
R. Ge and R. J. Mooney. 2006. Discriminative rerank-
ing for semantic parsing. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association
for Computational Linguistics (COLING/ACL 2006),
pages 263?270.
R. J. Kate and R. J. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING/ACL
2006), pages 913?920.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learn-
ing to transform natural to formal languages. In Pro-
ceedings of the Twentieth National Conference on Ar-
tificial Intelligence (AAAI 2005), pages 1062?1068.
791
S. Katz. 1987. Estimation of probabilities from sparse
data for the language model component of a speech
recognizer. IEEE Transactions on Acoustics, Speech,
and Signal Processing, 35(3):400?401.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In Proceedings of the 12th Euro-
pean Conference on Machine Learning (ECML 2001),
pages 466?477.
Y. W. Wong and R. J. Mooney. 2006. Learning for
semantic parsing with statistical machine translation.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT-NAACL
2006), pages 439?446.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics (ACL
2007), pages 960?967.
Y. W. Wong. 2007. Learning for Semantic Parsing and
Natural Language Generation Using Statistical Ma-
chine Translation Techniques. Ph.D. thesis, The Uni-
versity of Texas at Austin.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of the 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523?530.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the 21st Conference on Uncertainty in Ar-
tificial Intelligence.
L. S. Zettlemoyer and M. Collins. 2007. Online learning
of relaxed CCG grammars for parsing to logical form.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), pages 678?687.
792
Proceedings of NAACL HLT 2007, Companion Volume, pages 209?212,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Selective Phrase Pair Extraction for
Improved Statistical Machine Translation
Luke S. Zettlemoyer
MIT CSAIL
Cambridge, MA 02139
lsz@csail.mit.edu
Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, WA 98052
bobmoore@microsoft.com
Abstract
Phrase-based statistical machine transla-
tion systems depend heavily on the knowl-
edge represented in their phrase transla-
tion tables. However, the phrase pairs
included in these tables are typically se-
lected using simple heuristics that poten-
tially leave much room for improvement.
In this paper, we present a technique for
selecting the phrase pairs to include in
phrase translation tables based on their es-
timated quality according to a translation
model. This method not only reduces the
size of the phrase translation table, but
also improves translation quality as mea-
sured by the BLEU metric.
1 Introduction
Phrase translation tables are the heart of phrase-
based statistical machine translation (SMT) systems.
They provide pairs of phrases that are used to con-
struct a large set of potential translations for each
input sentence, along with feature values associated
with each phrase pair that are used to select the best
translation from this set.1
The most widely used method for building phrase
translation tables (Koehn et al, 2003) selects, from
a word alignment of a parallel bilingual training cor-
pus, all pairs of phrases (up to a given length) that
are consistent with the alignment. This procedure
1A ?phrase? in this sense can be any contiguous sequence of
words, and need not be a complete linguistic constituent.
typically generates many phrase pairs that are not re-
motely reasonable translation candidates.2 To avoid
creating translations that use these pairs, a set of fea-
tures is computed for each pair. These features are
used to train a translation model, and phrase pairs
that produce low scoring translations are avoided. In
practice, it is often assumed that current translation
models are good enough to avoid building transla-
tions with these unreasonable phrase pairs.
In this paper, we question this assumption by in-
vestigating methods for pruning low quality phrase
pairs. We present a simple procedure that reduces
the overall phrase translation table size while in-
creasing translation quality. The basic idea is to
initially gather the phrase pairs and train an trans-
lation model as usual, but to then select a subset of
the overall phrases that performs the best, prune the
others, and retrain the translation model. In experi-
ments, this approach reduced the size of the phrase
tranlsation table by half, and improved the BLEU
score of the resulting translations by up to 1.5 points.
2 Background
As a baseline, we present a relatively standard SMT
approach, following Koehn et al (2003). Potential
translations are scored using a linear model where
the best translation is computed as
argmax
t,a
n?
i=1
?ifi(s, a, t)
where s is the input sentence, t is the output sen-
tence, and a is a phrasal alignment that specifies how
2In one experiment, we managed to generate more than
117,000 English phrases for the the French word ?de?.
209
Monsieur le Orateur , je invoque le Re`gement
"" ,,  
""
Mr. Speaker , I rise on a point of order
Figure 1: A word aligned sentence pair.
t is constructed from s. The weights ?i associated
with each feature fi are tuned to maximize the qual-
ity of the translations.
The training procedure starts by computing a
word alignment for each sentence pair in the train-
ing corpus. A word alignment is a relation between
the words in two sentences where, intuitively, words
are aligned to their translation in the other language.
In this work, we use a discriminatively trained word
aligner (Moore et al, 2006) that has state of the art
performance. Figure 1 presents a high quality align-
ment produced by this aligner.
Given a word aligned corpus, the second step is to
extract a phrase translation table. Each entry in this
table contains a source language phrase s, a target
language phrase t, and a list of feature values ?(s, t).
It is usual to extract every phrase pair, up to a cer-
tain phrase length, that is consistent with the word
alignment that is annotated in the corpus. Each con-
sistent pair must have at least one word alignment
between words within the phrases and no words in
either phrase can be aligned any words outside of the
phrases. For example, Figure 2 shows some of the
phrase pairs that would be extracted from the word-
aligned sentence pair in Figure 1. A full list using
phrases of up to three words would include 28 pairs.
For each extracted phrase pair (s, t), feature val-
ues ?(s, t) = ?log p(s|t), log p(t|s), log l(s, t)? are
computed. The first two features, the log translation
and inverse translation probabilities, are estimated
by counting phrase cooccurrences, following Koehn
et al (2003). The third feature is the logarithm of
a lexical score l(s, t) that provides a simple form of
smoothing by weighting a phrase pair based on how
likely individual words within the phrases are to be
translations of each other. We use a version from
Foster et al (2006), modified from (Koehn et al,
2003), which is an average of pairwise word transla-
tion probabilities.
In phrase-based SMT, the decoder produces trans-
lations by dividing the source sentence into a se-
quence of phrases, choosing a target language phrase
# Source Lang. Phrase Target Lang. Phrase
1 Monsieur Mr.
2 Monsieur le Mr.
3 Monsieur le Orateur Mr. Speaker
4 le Orateur Speaker
5 Orateur Speaker
. . . . . . . . .
23 le Re`glement point of order
24 le Re`glement of order
25 le Re`glement order
26 Re`glement point of order
27 Re`glement of order
28 Re`glement order
Figure 2: Phrase pairs consistent with the word
alignment in Figure 1.
as a translation for each source language phrase, and
ordering the target language phrases to build the fi-
nal translated sentence. Each potential translation is
scored according to a weighted linear model. We
use the three features from the phrase translation ta-
ble, summing their values for each phrase pair used
in the translation. We also use four additional fea-
tures: a target language model, a distortion penalty,
the target sentence word count, and the phrase pair
count, all computed as described in (Koehn, 2004).
For all of the experiments in this paper, we used the
Pharaoh beam-search decoder (Koehn, 2004) with
the features described above.
Finally, to estimate the parameters ?i of the
weighted linear model, we adopt the popular min-
imum error rate training procedure (Och, 2003)
which directly optimizes translation quality as mea-
sured by the BLEU metric.
3 Selective Phrase Pair Extraction
In order to improve performance, it is important to
select high quality phrase pairs for the phrase trans-
lation table. We use two key ideas to guide selection:
? Preferential Scoring: Phrase pairs are selected
using a function q(s, t) that returns a high score
for source, target phrase pairs (s, t) that lead to
high quality translations.
? Redundancy Constraints: Our intuition is
that each occurrence of a source or target lan-
guage phrase really has at most one translation
for that sentence pair. Redundancy constraints
minimize the number of possible translations
that are extracted for each phrase occurrence.
210
Selecting phrases that a translation model prefers
and eliminating at least some of the ambiguity that
comes with extracting multiple translations for a sin-
gle phrase occurrence creates a smaller phrase trans-
lation table with higher quality entries.
The ideal scoring metric would give high scores
to phrase pairs that lead to high-quality translations
and low scores to those that would decrease transla-
tion quality. The best such metric we have available
is provided by the overall translation model. Our
scoring metric q(s, t) is therefore computed by first
extracting a full phrase translation table, then train-
ing a full translation model, and finally using a sub-
part of the model to score individual phrase pairs in
isolation. Because the scoring is tied to a model that
is optimized to maximize translation quality, more
desirable phrase pairs should be given higher scores.
More specifically, q(s, t) = ?(s, t) ? ? where
?(s, t) is the length three vector that contains the
feature values stored with the phrase pair (s, t) in the
phrase translation table, and ? is a vector of the three
parameter values that were learned for these features
by the full translation model. The rest of the features
are ignored because they are either constant or de-
pend on the target language sentence which is fixed
during phrase extraction. In essence, we are using
the subpart of a full translation model that looks at
phrase pair identity and scoring the pair based on
how the full model would like it.
This scoring metric is used in a phrase pair se-
lection algorithm inspired by competitive linking
for word alignment (Melamed, 2000). Local com-
petitive linking extracts high scoring phrase pairs
while enforcing a redundancy constraint that mini-
mizes the number of phrase pairs that share a com-
mon phrase. For each sentence pair in the training
set, this algorithm marks the highest scoring phrase
pair, according to q(s, t), containing each source
language phrase and the highest scoring phrase pair
containing each target language phrase. Each of
these marked phrase pairs is selected and the phrase
translation table is rebuilt. This is a soft redundancy
constraint because a phrase pair will only be ex-
cluded if there is a higher scoring pair that shares
its source language phrase and a higher scoring pair
that shares its target language phrase. For example,
consider again the phrase pairs in Figure 2 and as-
sume they are sorted by their scores. Local compet-
itive linking will select every phrase pair except for
27 and 28. All other pairs are the highest scoring
options for at least one of their phrases.
Selective phrase extraction with competitive link-
ing can be seen as a Viterbi reestimation algorithm.
Because we are extracting fewer phrase pairs, the
features associated with each phrase pair will differ.
If the removed phrases were not real translations of
each other in the first place, the translation features
p(s|t) and p(t|s) should be better estimates because
the high quality phrases that remain will be given
the probability mass that was assigned to the pruned
phrase pairs. Although we are running it in a purely
discriminative setting, it has a similar feel to an EM
algorithm. First, a full phrase translation table and
parameter estimate is computed. Then, based on that
estimate, a subset of the phrases is selected which,
in turn, supplies a new estimate for the parameters.
One question is howmany times to run this reestima-
tion procedure. We found, on the development set,
that it never helped to run more than one iteration.
Perhaps because of the hard nature of the algorithm,
repeated iterations caused slight decreases in phrase
translation table size and overall performance.
4 Experiments
In this section, we report experiments conducted
with Canadian Hansards data from the 2003 HLT-
NAACL word-alignment workshop (Mihalcea and
Pedersen, 2003). Phrase pairs are extracted
from 500,000 word-aligned French-English sen-
tence pairs. Translation quality is evaluated accord-
ing to the BLEU metric (with one reference trans-
lation). Three additional disjoint data sets (from the
same source) were used, one with 500 sentence pairs
for minimum error rate training, another with 1000
pairs for development testing, and a final set of 2000
sentence pairs for the final test. For each experiment,
we trained the full translation model as described in
Section 2. Each trial varied only in the phrase trans-
lation table that was used.3
One important question is what the maximum
phrase length should be for extraction. To inves-
tigate this issue, we ran experiments on the devel-
3These experiments also used the default pruning from the
Pharaoh decoder, allowing only the 10 best output phrases to be
considered for each input phrase. This simple global pruning
cannot be substituted for the competitive linking described here.
211
 
26
 
27
 
28
 
29  3
 
4
 
5
 
6
 
7
BLEU Score
Maximum
 Phrase 
Length
Full Phra
se Trans
. Table
Local Co
mp. Linki
ng
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80  3
 
4
 
5
 
6
 
7
Num. of Phrase Pairs (Millions)
Maximum
 Phrase 
Length
Full Phra
se Trans
. Table
Local Co
mp. Linki
ng
Figure 3: Scaling the maximum phrase length.
opment set. Figure 3 shows a comparison of the
full phrase table to local competitive linking as the
maximum phrase length is varied. Local competi-
tive linking consistently outperforms the full table
and the difference in BLEU score seems to increase
with the length. The growth in the size of the phrase
translation table seems to be linear with maximum
phrase length in both cases, with the table size grow-
ing at a slower rate under local competitive linking.
To verify these results, we tested the model
trained with the full phrase translation table against
the model trained with the table selected by local
competitive linking on the heldout test data. Both ta-
bles included phrases up to length 7 and the models
were tested on a set of 2000 unseen sentence pairs.
The results matched the development experiments.
The full system scored 26.78 while the local linking
achieved 28.30, a difference of 1.52 BLEU points.
5 Discussion
The most closely related work attempts to create
higher quality phrase translation tables by learning
a generative model that directly incorporates phrase
pair selection. The original approach (Marcu and
Wong, 2002) was limited due to computational con-
straints but recent work (DeNero et al, 2006; Birch
et al, 2006) has improved the efficiency by using
word alignments as constraints on the set of possible
phrase pairs. The best results from this line of work
allow for a significantly smaller phrase translation
table, but never improve translation performance.
In this paper, we presented an algorithm that
improves translation quality by selecting a smaller
phrase translation table. We hope that this work
highlights the need to think carefully about the qual-
ity of the phrase translation table, which is the cen-
tral knowledge source for most modern statistical
machine translation systems. The methods used in
the experiments are so simple that we believe that
there is significant potential for improvement by us-
ing better methods for scoring phrase pairs and se-
lecting phrase pairs based those scores.
References
Alexandra Birch, Chris Callison-Burch, Miles Osborne, and
Philipp Koehn. 2006. Constraining the phrase-based, join
probability statistical translation model. In Proceedings of
the Workshop on Stastical Machine Translation.
John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006.
Why generative phrase models underperform surface heuris-
tics. In Proceedings of the Workshop on Stastical Machine
Translation.
George Foster, Roland Kuhn, and Howard Johnson. 2006.
Phrasetable smoothing for stastical machine translation. In
Proceedings of the Conference on Empirical Methods in Nat-
ural Language Processing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Stastical phrase-based translation. In Proceedings of the
North American Chapter of the Association for Computa-
tional Linguistics.
Philipp Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models. In Pro-
ceedings of The Sixth Conference of the Association for Ma-
chine Translation in the Americas.
Daniel Marcu and William Wong. 2002. A phrase-based, joint
probability model for statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in Natural
Language Processing.
I. Dan Melamed. 2000. Models of translation equivalence
amoung words. Computational Linguistics, 26(2):221?249.
RadaMihalcea and Ted Pedersen. 2003. An evaluation exercise
for word alignment. In Proceedings of the HLT-NAACL 2003
Workshop, Building and Using Parallel Texts: Data Driven
Machine Translation and Beyond.
Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006. Im-
proved discriminative bilingual word alignment. In Proceed-
ings of the 44th Annual Meeting of the Association for Com-
putational Linguistics.
Franz Josef Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics.
212
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 82?90,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Reinforcement Learning for Mapping Instructions to Actions
S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{branavan, harr, lsz, regina}@csail.mit.edu
Abstract
In this paper, we present a reinforce-
ment learning approach for mapping nat-
ural language instructions to sequences of
executable actions. We assume access to
a reward function that defines the qual-
ity of the executed actions. During train-
ing, the learner repeatedly constructs ac-
tion sequences for a set of documents, ex-
ecutes those actions, and observes the re-
sulting reward. We use a policy gradient
algorithm to estimate the parameters of a
log-linear model for action selection. We
apply our method to interpret instructions
in two domains ? Windows troubleshoot-
ing guides and game tutorials. Our results
demonstrate that this method can rival su-
pervised learning techniques while requir-
ing few or no annotated training exam-
ples.1
1 Introduction
The problem of interpreting instructions written
in natural language has been widely studied since
the early days of artificial intelligence (Winograd,
1972; Di Eugenio, 1992). Mapping instructions to
a sequence of executable actions would enable the
automation of tasks that currently require human
participation. Examples include configuring soft-
ware based on how-to guides and operating simu-
lators using instruction manuals. In this paper, we
present a reinforcement learning framework for in-
ducing mappings from text to actions without the
need for annotated training examples.
For concreteness, consider instructions from a
Windows troubleshooting guide on deleting tem-
porary folders, shown in Figure 1. We aim to map
1Code, data, and annotations used in this work are avail-
able at http://groups.csail.mit.edu/rbg/code/rl/
Figure 1: A Windows troubleshooting article de-
scribing how to remove the ?msdownld.tmp? tem-
porary folder.
this text to the corresponding low-level commands
and parameters. For example, properly interpret-
ing the third instruction requires clicking on a tab,
finding the appropriate option in a tree control, and
clearing its associated checkbox.
In this and many other applications, the valid-
ity of a mapping can be verified by executing the
induced actions in the corresponding environment
and observing their effects. For instance, in the
example above we can assess whether the goal
described in the instructions is achieved, i.e., the
folder is deleted. The key idea of our approach
is to leverage the validation process as the main
source of supervision to guide learning. This form
of supervision allows us to learn interpretations
of natural language instructions when standard su-
pervised techniques are not applicable, due to the
lack of human-created annotations.
Reinforcement learning is a natural framework
for building models using validation from an envi-
ronment (Sutton and Barto, 1998). We assume that
supervision is provided in the form of a reward
function that defines the quality of executed ac-
tions. During training, the learner repeatedly con-
structs action sequences for a set of given docu-
ments, executes those actions, and observes the re-
sulting reward. The learner?s goal is to estimate a
82
policy ? a distribution over actions given instruc-
tion text and environment state ? that maximizes
future expected reward. Our policy is modeled in a
log-linear fashion, allowing us to incorporate fea-
tures of both the instruction text and the environ-
ment. We employ a policy gradient algorithm to
estimate the parameters of this model.
We evaluate our method on two distinct applica-
tions: Windows troubleshooting guides and puz-
zle game tutorials. The key findings of our ex-
periments are twofold. First, models trained only
with simple reward signals achieve surprisingly
high results, coming within 11% of a fully su-
pervised method in the Windows domain. Sec-
ond, augmenting unlabeled documents with even
a small fraction of annotated examples greatly re-
duces this performance gap, to within 4% in that
domain. These results indicate the power of learn-
ing from this new form of automated supervision.
2 Related Work
Grounded Language Acquisition Our work
fits into a broader class of approaches that aim to
learn language from a situated context (Mooney,
2008a; Mooney, 2008b; Fleischman and Roy,
2005; Yu and Ballard, 2004; Siskind, 2001; Oates,
2001). Instances of such approaches include
work on inferring the meaning of words from
video data (Roy and Pentland, 2002; Barnard and
Forsyth, 2001), and interpreting the commentary
of a simulated soccer game (Chen and Mooney,
2008). Most of these approaches assume some
form of parallel data, and learn perceptual co-
occurrence patterns. In contrast, our emphasis
is on learning language by proactively interacting
with an external environment.
Reinforcement Learning for Language Pro-
cessing Reinforcement learning has been previ-
ously applied to the problem of dialogue manage-
ment (Scheffler and Young, 2002; Roy et al, 2000;
Litman et al, 2000; Singh et al, 1999). These
systems converse with a human user by taking ac-
tions that emit natural language utterances. The
reinforcement learning state space encodes infor-
mation about the goals of the user and what they
say at each time step. The learning problem is to
find an optimal policy that maps states to actions,
through a trial-and-error process of repeated inter-
action with the user.
Reinforcement learning is applied very differ-
ently in dialogue systems compared to our setup.
In some respects, our task is more easily amenable
to reinforcement learning. For instance, we are not
interacting with a human user, so the cost of inter-
action is lower. However, while the state space can
be designed to be relatively small in the dialogue
management task, our state space is determined by
the underlying environment and is typically quite
large. We address this complexity by developing
a policy gradient algorithm that learns efficiently
while exploring a small subset of the states.
3 Problem Formulation
Our task is to learn a mapping between documents
and the sequence of actions they express. Figure 2
shows how one example sentence is mapped to
three actions.
Mapping Text to Actions As input, we are
given a document d, comprising a sequence of sen-
tences (u1, . . . , u`), where each ui is a sequence
of words. Our goal is to map d to a sequence of
actions ~a = (a0, . . . , an?1). Actions are predicted
and executed sequentially.2
An action a = (c,R,W ?) encompasses a com-
mand c, the command?s parameters R, and the
words W ? specifying c and R. Elements of R re-
fer to objects available in the environment state, as
described below. Some parameters can also refer
to words in document d. Additionally, to account
for words that do not describe any actions, c can
be a null command.
The Environment The environment state E
specifies the set of objects available for interac-
tion, and their properties. In Figure 2, E is shown
on the right. The environment state E changes
in response to the execution of command c with
parameters R according to a transition distribu-
tion p(E ?|E , c, R). This distribution is a priori un-
known to the learner. As we will see in Section 5,
our approach avoids having to directly estimate
this distribution.
State To predict actions sequentially, we need to
track the state of the document-to-actions map-
ping over time. A mapping state s is a tuple
(E , d, j,W ), where E refers to the current environ-
ment state; j is the index of the sentence currently
being interpreted in document d; and W contains
words that were mapped by previous actions for
2That is, action ai is executed before ai+1 is predicted.
83
Figure 2: A three-step mapping from an instruction sentence to a sequence of actions in Windows 2000.
For each step, the figure shows the words selected by the action, along with the corresponding system
command and its parameters. The words of W ? are underlined, and the words of W are highlighted in
grey.
the same sentence. The mapping state s is ob-
served after each action.
The initial mapping state s0 for document d is
(Ed, d, 0, ?); Ed is the unique starting environment
state for d. Performing action a in state s =
(E , d, j,W ) leads to a new state s? according to
distribution p(s?|s, a), defined as follows: E tran-
sitions according to p(E ?|E , c, R), W is updated
with a?s selected words, and j is incremented if
all words of the sentence have been mapped. For
the applications we consider in this work, environ-
ment state transitions, and consequently mapping
state transitions, are deterministic.
Training During training, we are provided with
a set D of documents, the ability to sample from
the transition distribution, and a reward function
r(h). Here, h = (s0, a0, . . . , sn?1, an?1, sn) is
a history of states and actions visited while in-
terpreting one document. r(h) outputs a real-
valued score that correlates with correct action
selection.3 We consider both immediate reward,
which is available after each action, and delayed
reward, which does not provide feedback until the
last action. For example, task completion is a de-
layed reward that produces a positive value after
the final action only if the task was completed suc-
cessfully. We will also demonstrate how manu-
ally annotated action sequences can be incorpo-
rated into the reward.
3In most reinforcement learning problems, the reward
function is defined over state-action pairs, as r(s, a) ? in this
case, r(h) =
P
t r(st, at), and our formulation becomes a
standard finite-horizon Markov decision process. Policy gra-
dient approaches allow us to learn using the more general
case of history-based reward.
The goal of training is to estimate parameters ?
of the action selection distribution p(a|s, ?), called
the policy. Since the reward correlates with ac-
tion sequence correctness, the ? that maximizes
expected reward will yield the best actions.
4 A Log-Linear Model for Actions
Our goal is to predict a sequence of actions. We
construct this sequence by repeatedly choosing an
action given the current mapping state, and apply-
ing that action to advance to a new state.
Given a state s = (E , d, j,W ), the space of pos-
sible next actions is defined by enumerating sub-
spans of unused words in the current sentence (i.e.,
subspans of the jth sentence of d not in W ), and
the possible commands and parameters in envi-
ronment state E .4 We model the policy distribu-
tion p(a|s; ?) over this action space in a log-linear
fashion (Della Pietra et al, 1997; Lafferty et al,
2001), giving us the flexibility to incorporate a di-
verse range of features. Under this representation,
the policy distribution is:
p(a|s; ?) =
e???(s,a)
?
a?
e???(s,a
?)
, (1)
where ?(s, a) ? Rn is an n-dimensional feature
representation. During test, actions are selected
according to the mode of this distribution.
4For parameters that refer to words, the space of possible
values is defined by the unused words in the current sentence.
84
5 Reinforcement Learning
During training, our goal is to find the optimal pol-
icy p(a|s; ?). Since reward correlates with correct
action selection, a natural objective is to maximize
expected future reward ? that is, the reward we
expect while acting according to that policy from
state s. Formally, we maximize the value function:
V?(s) = Ep(h|?) [r(h)] , (2)
where the history h is the sequence of states and
actions encountered while interpreting a single
document d ? D. This expectation is averaged
over all documents in D. The distribution p(h|?)
returns the probability of seeing history h when
starting from state s and acting according to a pol-
icy with parameters ?. This distribution can be de-
composed into a product over time steps:
p(h|?) =
n?1?
t=0
p(at|st; ?)p(st+1|st, at). (3)
5.1 A Policy Gradient Algorithm
Our reinforcement learning problem is to find the
parameters ? that maximize V? from equation 2.
Although there is no closed form solution, policy
gradient algorithms (Sutton et al, 2000) estimate
the parameters ? by performing stochastic gradi-
ent ascent. The gradient of V? is approximated by
interacting with the environment, and the resulting
reward is used to update the estimate of ?. Policy
gradient algorithms optimize a non-convex objec-
tive and are only guaranteed to find a local opti-
mum. However, as we will see, they scale to large
state spaces and can perform well in practice.
To find the parameters ? that maximize the ob-
jective, we first compute the derivative of V?. Ex-
panding according to the product rule, we have:
?
??
V?(s) = Ep(h|?)
[
r(h)
?
t
?
??
log p(at|st; ?)
]
,
(4)
where the inner sum is over all time steps t in
the current history h. Expanding the inner partial
derivative we observe that:
?
??
log p(a|s; ?) = ?(s, a)?
?
a?
?(s, a?)p(a?|s; ?),
(5)
which is the derivative of a log-linear distribution.
Equation 5 is easy to compute directly. How-
ever, the complete derivative of V? in equation 4
Input: A document set D,
Feature representation ?,
Reward function r(h),
Number of iterations T
Initialization: Set ? to small random values.
for i = 1 . . . T do1
foreach d ? D do2
Sample history h ? p(h|?) where3
h = (s0, a0, . . . , an?1, sn) as follows:
3a for t = 0 . . . n? 1 do
3b Sample action at ? p(a|st; ?)
3c Execute at on state st: st+1 ? p(s|st, at)
end
??
P
t
`
?(st, at)?
P
a? ?(st, a
?)p(a?|st; ?)
?
4
? ? ? + r(h)?5
end
end
Output: Estimate of parameters ?
Algorithm 1: A policy gradient algorithm.
is intractable, because computing the expectation
would require summing over all possible histo-
ries. Instead, policy gradient algorithms employ
stochastic gradient ascent by computing a noisy
estimate of the expectation using just a subset of
the histories. Specifically, we draw samples from
p(h|?) by acting in the target environment, and
use these samples to approximate the expectation
in equation 4. In practice, it is often sufficient to
sample a single history h for this approximation.
Algorithm 1 details the complete policy gradi-
ent algorithm. It performs T iterations over the
set of documents D. Step 3 samples a history that
maps each document to actions. This is done by
repeatedly selecting actions according to the cur-
rent policy, and updating the state by executing the
selected actions. Steps 4 and 5 compute the empir-
ical gradient and update the parameters ?.
In many domains, interacting with the environ-
ment is expensive. Therefore, we use two tech-
niques that allow us to take maximum advantage
of each environment interaction. First, a his-
tory h = (s0, a0, . . . , sn) contains subsequences
(si, ai, . . . sn) for i = 1 to n ? 1, each with its
own reward value given by the environment as a
side effect of executing h. We apply the update
from equation 5 for each subsequence. Second,
for a sampled history h, we can propose alterna-
tive histories h? that result in the same commands
and parameters with different word spans. We can
again apply equation 5 for each h?, weighted by its
probability under the current policy, p(h
?|?)
p(h|?) .
85
The algorithm we have presented belongs to
a family of policy gradient algorithms that have
been successfully used for complex tasks such as
robot control (Ng et al, 2003). Our formulation is
unique in how it represents natural language in the
reinforcement learning framework.
5.2 Reward Functions and ML Estimation
We can design a range of reward functions to guide
learning, depending on the availability of anno-
tated data and environment feedback. Consider the
case when every training document d ? D is an-
notated with its correct sequence of actions, and
state transitions are deterministic. Given these ex-
amples, it is straightforward to construct a reward
function that connects policy gradient to maxi-
mum likelihood. Specifically, define a reward
function r(h) that returns one when h matches the
annotation for the document being analyzed, and
zero otherwise. Policy gradient performs stochas-
tic gradient ascent on the objective from equa-
tion 2, performing one update per document. For
document d, this objective becomes:
Ep(h|?)[r(h)] =
?
h
r(h)p(h|?) = p(hd|?),
where hd is the history corresponding to the an-
notated action sequence. Thus, with this reward
policy gradient is equivalent to stochastic gradient
ascent with a maximum likelihood objective.
At the other extreme, when annotations are
completely unavailable, learning is still possi-
ble given informative feedback from the environ-
ment. Crucially, this feedback only needs to cor-
relate with action sequence quality. We detail
environment-based reward functions in the next
section. As our results will show, reward func-
tions built using this kind of feedback can provide
strong guidance for learning. We will also con-
sider reward functions that combine annotated su-
pervision with environment feedback.
6 Applying the Model
We study two applications of our model: follow-
ing instructions to perform software tasks, and
solving a puzzle game using tutorial guides.
6.1 Microsoft Windows Help and Support
On its Help and Support website,5 Microsoft pub-
lishes a number of articles describing how to per-
5support.microsoft.com
Notation
o Parameter referring to an environment object
L Set of object class names (e.g. ?button?)
V Vocabulary
Features onW and object o
Test if o is visible in s
Test if o has input focus
Test if o is in the foreground
Test if o was previously interacted with
Test if o came into existence since last action
Min. edit distance between w ?W and object labels in s
Features on words inW , command c, and object o
?c? ? C, w ? V : test if c? = c and w ?W
?c? ? C, l ? L: test if c? = c and l is the class of o
Table 1: Example features in the Windows do-
main. All features are binary, except for the nor-
malized edit distance which is real-valued.
form tasks and troubleshoot problems in the Win-
dows operating systems. Examples of such tasks
include installing patches and changing security
settings. Figure 1 shows one such article.
Our goal is to automatically execute these sup-
port articles in the Windows 2000 environment.
Here, the environment state is the set of visi-
ble user interface (UI) objects, and object prop-
erties such as label, location, and parent window.
Possible commands include left-click, right-click,
double-click, and type-into, all of which take a UI
object as a parameter; type-into additionally re-
quires a parameter for the input text.
Table 1 lists some of the features we use for this
domain. These features capture various aspects of
the action under consideration, the current Win-
dows UI state, and the input instructions. For ex-
ample, one lexical feature measures the similar-
ity of a word in the sentence to the UI labels of
objects in the environment. Environment-specific
features, such as whether an object is currently in
focus, are useful when selecting the object to ma-
nipulate. In total, there are 4,438 features.
Reward Function Environment feedback can
be used as a reward function in this domain. An
obvious reward would be task completion (e.g.,
whether the stated computer problem was fixed).
Unfortunately, verifying task completion is a chal-
lenging system issue in its own right.
Instead, we rely on a noisy method of check-
ing whether execution can proceed from one sen-
tence to the next: at least one word in each sen-
tence has to correspond to an object in the envi-
86
Figure 3: Crossblock puzzle with tutorial. For this
level, four squares in a row or column must be re-
moved at once. The first move specified by the
tutorial is greyed in the puzzle.
ronment.6 For instance, in the sentence from Fig-
ure 2 the word ?Run? matches the Run... menu
item. If no words in a sentence match a current
environment object, then one of the previous sen-
tences was analyzed incorrectly. In this case, we
assign the history a reward of -1. This reward is
not guaranteed to penalize all incorrect histories,
because there may be false positive matches be-
tween the sentence and the environment. When
at least one word matches, we assign a positive
reward that linearly increases with the percentage
of words assigned to non-null commands, and lin-
early decreases with the number of output actions.
This reward signal encourages analyses that inter-
pret al of the words without producing spurious
actions.
6.2 Crossblock: A Puzzle Game
Our second application is to a puzzle game called
Crossblock, available online as a Flash game.7
Each of 50 puzzles is played on a grid, where some
grid positions are filled with squares. The object
of the game is to clear the grid by drawing vertical
or horizontal line segments that remove groups of
squares. Each segment must exactly cross a spe-
cific number of squares, ranging from two to seven
depending on the puzzle. Humans players have
found this game challenging and engaging enough
to warrant posting textual tutorials.8 A sample
puzzle and tutorial are shown in Figure 3.
The environment is defined by the state of the
grid. The only command is clear, which takes a
parameter specifying the orientation (row or col-
umn) and grid location of the line segment to be
6We assume that a word maps to an environment object if
the edit distance between the word and the object?s name is
below a threshold value.
7hexaditidom.deviantart.com/art/Crossblock-108669149
8www.jayisgames.com/archives/2009/01/crossblock.php
removed. The challenge in this domain is to seg-
ment the text into the phrases describing each ac-
tion, and then correctly identify the line segments
from references such as ?the bottom four from the
second column from the left.?
For this domain, we use two sets of binary fea-
tures on state-action pairs (s, a). First, for each
vocabulary word w, we define a feature that is one
if w is the last word of a?s consumed words W ?.
These features help identify the proper text seg-
mentation points between actions. Second, we in-
troduce features for pairs of vocabulary word w
and attributes of action a, e.g., the line orientation
and grid locations of the squares that a would re-
move. This set of features enables us to match
words (e.g., ?row?) with objects in the environ-
ment (e.g., a move that removes a horizontal series
of squares). In total, there are 8,094 features.
Reward Function For Crossblock it is easy to
directly verify task completion, which we use as
the basis of our reward function. The reward r(h)
is -1 if h ends in a state where the puzzle cannot
be completed. For solved puzzles, the reward is
a positive value proportional to the percentage of
words assigned to non-null commands.
7 Experimental Setup
Datasets For the Windows domain, our dataset
consists of 128 documents, divided into 70 for
training, 18 for development, and 40 for test. In
the puzzle game domain, we use 50 tutorials,
divided into 40 for training and 10 for test.9
Statistics for the datasets are shown below.
Windows Puzzle
Total # of documents 128 50
Total # of words 5562 994
Vocabulary size 610 46
Avg. words per sentence 9.93 19.88
Avg. sentences per document 4.38 1.00
Avg. actions per document 10.37 5.86
The data exhibits certain qualities that make
for a challenging learning problem. For instance,
there are a surprising variety of linguistic con-
structs ? as Figure 4 shows, in the Windows do-
main even a simple command is expressed in at
least six different ways.
9For Crossblock, because the number of puzzles is lim-
ited, we did not hold out a separate development set, and re-
port averaged results over five training/test splits.
87
Figure 4: Variations of ?click internet options on
the tools menu? present in the Windows corpus.
Experimental Framework To apply our algo-
rithm to the Windows domain, we use the Win32
application programming interface to simulate hu-
man interactions with the user interface, and to
gather environment state information. The operat-
ing system environment is hosted within a virtual
machine,10 allowing us to rapidly save and reset
system state snapshots. For the puzzle game do-
main, we replicated the game with an implemen-
tation that facilitates automatic play.
As is commonly done in reinforcement learn-
ing, we use a softmax temperature parameter to
smooth the policy distribution (Sutton and Barto,
1998), set to 0.1 in our experiments. For Windows,
the development set is used to select the best pa-
rameters. For Crossblock, we choose the parame-
ters that produce the highest reward during train-
ing. During evaluation, we use these parameters
to predict mappings for the test documents.
Evaluation Metrics For evaluation, we com-
pare the results to manually constructed sequences
of actions. We measure the number of correct ac-
tions, sentences, and documents. An action is cor-
rect if it matches the annotations in terms of com-
mand and parameters. A sentence is correct if all
of its actions are correctly identified, and analo-
gously for documents.11 Statistical significance is
measured with the sign test.
Additionally, we compute a word alignment
score to investigate the extent to which the input
text is used to construct correct analyses. This
score measures the percentage of words that are
aligned to the corresponding annotated actions in
correctly analyzed documents.
Baselines We consider the following baselines
to characterize the performance of our approach.
10VMware Workstation, available at www.vmware.com
11In these tasks, each action depends on the correct execu-
tion of all previous actions, so a single error can render the
remainder of that document?s mapping incorrect. In addition,
due to variability in document lengths, overall action accu-
racy is not guaranteed to be higher than document accuracy.
? Full Supervision Sequence prediction prob-
lems like ours are typically addressed us-
ing supervised techniques. We measure how
a standard supervised approach would per-
form on this task by using a reward signal
based on manual annotations of output ac-
tion sequences, as defined in Section 5.2. As
shown there, policy gradient with this re-
ward is equivalent to stochastic gradient as-
cent with a maximum likelihood objective.
? Partial Supervision We consider the case
when only a subset of training documents is
annotated, and environment reward is used
for the remainder. Our method seamlessly
combines these two kinds of rewards.
? Random and Majority (Windows) We con-
sider two na??ve baselines. Both scan through
each sentence from left to right. A com-
mand c is executed on the object whose name
is encountered first in the sentence. This
command c is either selected randomly, or
set to the majority command, which is left-
click. This procedure is repeated until no
more words match environment objects.
? Random (Puzzle) We consider a baseline
that randomly selects among the actions that
are valid in the current game state.12
8 Results
Table 2 presents evaluation results on the test sets.
There are several indicators of the difficulty of this
task. The random and majority baselines? poor
performance in both domains indicates that na??ve
approaches are inadequate for these tasks. The
performance of the fully supervised approach pro-
vides further evidence that the task is challenging.
This difficulty can be attributed in part to the large
branching factor of possible actions at each step ?
on average, there are 27.14 choices per action in
the Windows domain, and 9.78 in the Crossblock
domain.
In both domains, the learners relying only
on environment reward perform well. Although
the fully supervised approach performs the best,
adding just a few annotated training examples
to the environment-based learner significantly re-
duces the performance gap.
12Since action selection is among objects, there is no natu-
ral majority baseline for the puzzle.
88
Windows Puzzle
Action Sent. Doc. Word Action Doc. Word
Random baseline 0.128 0.101 0.000 ?? 0.081 0.111 ??
Majority baseline 0.287 0.197 0.100 ?? ?? ?? ??
Environment reward ? 0.647 ? 0.590 ? 0.375 0.819 ? 0.428 ? 0.453 0.686
Partial supervision  0.723 ? 0.702 0.475 0.989 0.575 ? 0.523 0.850
Full supervision  0.756 0.714 0.525 0.991 0.632 0.630 0.869
Table 2: Performance on the test set with different reward signals and baselines. Our evaluation measures
the proportion of correct actions, sentences, and documents. We also report the percentage of correct
word alignments for the successfully completed documents. Note the puzzle domain has only single-
sentence documents, so its sentence and document scores are identical. The partial supervision line
refers to 20 out of 70 annotated training documents for Windows, and 10 out of 40 for the puzzle. Each
result marked with ? or  is a statistically significant improvement over the result immediately above it;
? indicates p < 0.01 and  indicates p < 0.05.
Figure 5: Comparison of two training scenarios where training is done using a subset of annotated
documents, with and without environment reward for the remaining unannotated documents.
Figure 5 shows the overall tradeoff between an-
notation effort and system performance for the two
domains. The ability to make this tradeoff is one
of the advantages of our approach. The figure also
shows that augmenting annotated documents with
additional environment-reward documents invari-
ably improves performance.
The word alignment results from Table 2 in-
dicate that the learners are mapping the correct
words to actions for documents that are success-
fully completed. For example, the models that per-
form best in the Windows domain achieve nearly
perfect word alignment scores.
To further assess the contribution of the instruc-
tion text, we train a variant of our model without
access to text features. This is possible in the game
domain, where all of the puzzles share a single
goal state that is independent of the instructions.
This variant solves 34% of the puzzles, suggest-
ing that access to the instructions significantly im-
proves performance.
9 Conclusions
In this paper, we presented a reinforcement learn-
ing approach for inducing a mapping between in-
structions and actions. This approach is able to use
environment-based rewards, such as task comple-
tion, to learn to analyze text. We showed that hav-
ing access to a suitable reward function can signif-
icantly reduce the need for annotations.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168, grant IIS-0835445,
grant IIS-0835652, and a Graduate Research Fel-
lowship) and the ONR. Thanks to Michael Collins,
Amir Globerson, Tommi Jaakkola, Leslie Pack
Kaelbling, Dina Katabi, Martin Rinard, and mem-
bers of the MIT NLP group for their suggestions
and comments. Any opinions, findings, conclu-
sions, or recommendations expressed in this paper
are those of the authors, and do not necessarily re-
flect the views of the funding organizations.
89
References
Kobus Barnard and David A. Forsyth. 2001. Learning
the semantics of words and pictures. In Proceedings
of ICCV.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: a test of grounded language acqui-
sition. In Proceedings of ICML.
Stephen Della Pietra, Vincent J. Della Pietra, and
John D. Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Trans. Pattern Anal. Mach. Intell.,
19(4):380?393.
Barbara Di Eugenio. 1992. Understanding natural lan-
guage instructions: the case of purpose clauses. In
Proceedings of ACL.
Michael Fleischman and Deb Roy. 2005. Intentional
context in situated language learning. In Proceed-
ings of CoNLL.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML.
Diane J. Litman, Michael S. Kearns, Satinder Singh,
and Marilyn A. Walker. 2000. Automatic optimiza-
tion of dialogue management. In Proceedings of
COLING.
Raymond J. Mooney. 2008a. Learning language
from its perceptual context. In Proceedings of
ECML/PKDD.
Raymond J. Mooney. 2008b. Learning to connect lan-
guage and perception. In Proceedings of AAAI.
Andrew Y. Ng, H. Jin Kim, Michael I. Jordan, and
Shankar Sastry. 2003. Autonomous helicopter flight
via reinforcement learning. In Advances in NIPS.
James Timothy Oates. 2001. Grounding knowledge
in sensors: Unsupervised learning for language and
planning. Ph.D. thesis, University of Massachusetts
Amherst.
Deb K. Roy and Alex P. Pentland. 2002. Learn-
ing words from sights and sounds: a computational
model. Cognitive Science 26, pages 113?146.
Nicholas Roy, Joelle Pineau, and Sebastian Thrun.
2000. Spoken dialogue management using proba-
bilistic reasoning. In Proceedings of ACL.
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
HLT.
Satinder P. Singh, Michael J. Kearns, Diane J. Litman,
and Marilyn A. Walker. 1999. Reinforcement learn-
ing for spoken dialogue systems. In Advances in
NIPS.
Jeffrey Mark Siskind. 2001. Grounding the lexical se-
mantics of verbs in visual perception using force dy-
namics and event logic. J. Artif. Intell. Res. (JAIR),
15:31?90.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction. The MIT
Press.
Richard S. Sutton, David McAllester, Satinder Singh,
and Yishay Mansour. 2000. Policy gradient meth-
ods for reinforcement learning with function approx-
imation. In Advances in NIPS.
Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press.
Chen Yu and Dana H. Ballard. 2004. On the integra-
tion of grounding language and learning objects. In
Proceedings of AAAI.
90
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 976?984,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Learning Context-Dependent Mappings from Sentences to Logical Form
Luke S. Zettlemoyer and Michael Collins
MIT CSAIL
Cambridge, MA 02139
{lsz,mcollins}@csail.mit.com
Abstract
We consider the problem of learning
context-dependent mappings from sen-
tences to logical form. The training ex-
amples are sequences of sentences anno-
tated with lambda-calculus meaning rep-
resentations. We develop an algorithm that
maintains explicit, lambda-calculus repre-
sentations of salient discourse entities and
uses a context-dependent analysis pipeline
to recover logical forms. The method uses
a hidden-variable variant of the percep-
tion algorithm to learn a linear model used
to select the best analysis. Experiments
on context-dependent utterances from the
ATIS corpus show that the method recov-
ers fully correct logical forms with 83.7%
accuracy.
1 Introduction
Recently, researchers have developed algorithms
that learn to map natural language sentences to
representations of their underlying meaning (He
and Young, 2006; Wong and Mooney, 2007;
Zettlemoyer and Collins, 2005). For instance, a
training example might be:
Sent. 1: List flights to Boston on Friday night.
LF 1: ?x.flight(x) ? to(x, bos)
? day(x, fri) ? during(x, night)
Here the logical form (LF) is a lambda-calculus
expression defining a set of entities that are flights
to Boston departing on Friday night.
Most of this work has focused on analyzing sen-
tences in isolation. In this paper, we consider the
problem of learning to interpret sentences whose
underlying meanings can depend on the context in
which they appear. For example, consider an inter-
action where Sent. 1 is followed by the sentence:
Sent. 2: Show me the flights after 3pm.
LF 2: ?x.flight(x) ? to(x, bos)
?day(x, fri) ? depart(x) > 1500
In this case, the fact that Sent. 2 describes flights
to Boston on Friday must be determined based on
the context established by the first sentence.
We introduce a supervised, hidden-variable ap-
proach for learning to interpret sentences in con-
text. Each training example is a sequence of sen-
tences annotated with logical forms. Figure 1
shows excerpts from three training examples in the
ATIS corpus (Dahl et al, 1994).
For context-dependent analysis, we develop an
approach that maintains explicit, lambda-calculus
representations of salient discourse entities and
uses a two-stage pipeline to construct context-
dependent logical forms. The first stage uses
a probabilistic Combinatory Categorial Grammar
(CCG) parsing algorithm to produce a context-
independent, underspecified meaning representa-
tion. The second stage resolves this underspecified
meaning representation by making a sequence of
modifications to it that depend on the context pro-
vided by previous utterances.
In general, there are a large number of possi-
ble context-dependent analyses for each sentence.
To select the best one, we present a weighted lin-
ear model that is used to make a range of parsing
and context-resolution decisions. Since the train-
ing data contains only the final logical forms, we
model these intermediate decisions as hidden vari-
ables that must be estimated without explicit su-
pervision. We show that this model can be effec-
tively trained with a hidden-variable variant of the
perceptron algorithm.
In experiments on the ATIS DEC94 test set, the
approach recovers fully correct logical forms with
83.7% accuracy.
2 The Learning Problem
We assume access to a training set that consists of
n interactions D = ?I1, . . . , In?. The i?th interac-
tion Ii contains ni sentences,wi,1, . . . , wi,ni . Each
sentence wi,j is paired with a lambda-calculus ex-
976
Example #1:
(a) show me the flights from boston to philly
?x.flight(x) ? from(x, bos) ? to(x, phi)
(b) show me the ones that leave in the morning
?x.flight(x) ? from(x, bos) ? to(x, phi)
? during(x,morning)
(c) what kind of plane is used on these flights
?y.?x.flight(x) ? from(x, bos) ? to(x, phi)
? during(x,morning) ? aircraft(x) = y
Example #2:
(a) show me flights from milwaukee to orlando
?x.flight(x) ? from(x,mil) ? to(x, orl)
(b) cheapest
argmin(?x.flight(x) ? from(x,mil) ? to(x, orl),
?y.fare(y))
(c) departing wednesday after 5 o?clock
argmin(?x.flight(x) ? from(x,mil) ? to(x, orl)
? day(x,wed) ? depart(x) > 1700 ,
?y.fare(y))
Example #3:
(a) show me flights from pittsburgh to la thursday evening
?x.flight(x) ? from(x, pit) ? to(x, la)
? day(x, thur) ? during(x, evening)
(b) thursday afternoon
?x.flight(x) ? from(x, pit) ? to(x, la)
? day(x, thur) ? during(x, afternoon)
(c) thursday after 1700 hours
?x.flight(x) ? from(x, pit) ? to(x, la)
? day(x, thur) ? depart(x) > 1700
Figure 1: ATIS interaction excerpts.
pression zi,j specifying the target logical form.
Figure 1 contains example interactions.
The logical forms in the training set are repre-
sentations of each sentence?s underlying meaning.
In most cases, context (the previous utterances and
their interpretations) is required to recover the log-
ical form for a sentence. For instance, in Exam-
ple 1(b) in Figure 1, the sentence ?show me the
ones that leave in the morning? is paired with
?x.flight(x) ? from(x, bos) ? to(x, phi)
? during(x,morning)
Some parts of this logical form (from(x, bos) and
to(x, phi)) depend on the context. They have to be
recovered from the previous logical forms.
At step j in interaction i, we define the con-
text ?zi,1, . . . , zi,j?1? to be the j ? 1 preceding
logical forms.1 Now, given the training data, we
can create training examples (xi,j , zi,j) for i =
1 . . . n, j = 1 . . . ni. Each xi,j is a sentence and
a context, xi,j = (wi,j , ?zi,1, . . . , zi,j?1?). Given
this set up, we have a supervised learning problem
with input xi,j and output zi,j .
1In general, the context could also include the previous
sentences wi,k for k < j. In our data, we never observed any
interactions where the choice of the correct logical form zi,j
depended on the words in the previous sentences.
3 Overview of Approach
In general, the mapping from a sentence and a con-
text to a logical form can be quite complex. In this
section, we present an overview of our learning
approach. We assume the learning algorithm has
access to:
? A training set D, defined in Section 2.
? A CCG lexicon.2 See Section 4 for an
overview of CCG. Each entry in the lexicon
pairs a word (or sequence of words), with
a CCG category specifying both the syntax
and semantics for that word. One example
CCG entry would pair flights with the cate-
gory N : ?x.flight(x).
Derivations A derivation for the j?th sentence
in an interaction takes as input a pair x = (wj , C),
where C = ?z1 . . . zj?1? is the current context. It
produces a logical form z. There are two stages:
? First, the sentence wj is parsed using
the CCG lexicon to form an intermediate,
context-independent logical form pi.
? Second, in a series of steps, pi is mapped to z.
These steps depend on the context C.
As one sketch of a derivation, consider how we
might analyze Example 1(b) in Figure 1. In this
case the sentence is ?show me the ones that leave
in the morning.? The CCG parser would produce
the following context-independent logical form:
?x.!?e, t?(x) ? during(x,morning)
The subexpression !?e, t? results directly from the
referential phrase the ones; we discuss this in more
detail in Section 4.2, but intuitively this subexpres-
sion specifies that a lambda-calculus expression of
type ?e, t? must be recovered from the context and
substituted in its place.
In the second (contextually dependent) stage of
the derivation, the expression
?x.flight(x) ? from(x, bos) ? to(x, phi)
is recovered from the context, and substituted for
the !?e, t? subexpression, producing the desired fi-
nal logical form, seen in Example 1(b).
2Developing algorithms that learn the CCG lexicon from
the data described in this paper is an important area for future
work. We could possibly extend algorithms that learn from
context-independent data (Zettlemoyer and Collins, 2005).
977
In addition to substitutions of this type, we will
also perform other types of context-dependent res-
olution steps, as described in Section 5.
In general, both of the stages of the derivation
involve considerable ambiguity ? there will be a
large number of possible context-independent log-
ical forms pi for wj and many ways of modifying
each pi to create a final logical form zj .
Learning We model the problem of selecting
the best derivation as a structured prediction prob-
lem (Johnson et al, 1999; Lafferty et al, 2001;
Collins, 2002; Taskar et al, 2004). We present
a linear model with features for both the parsing
and context resolution stages of the derivation. In
our setting, the choice of the context-independent
logical form pi and all of the steps that map pi to
the output z are hidden variables; these steps are
not annotated in the training data. To estimate the
parameters of the model, we use a hidden-variable
version of the perceptron algorithm. We use an ap-
proximate search procedure to find the best deriva-
tion both while training the model and while ap-
plying it to test examples.
Evaluation We evaluate the approach on se-
quences of sentences ?w1, . . . , wk?. For each wj ,
the algorithm constructs an output logical form zj
which is compared to a gold standard annotation to
check correctness. At step j, the context contains
the previous zi, for i < j, output by the system.
4 Context-independent Parsing
In this section, we first briefly review the CCG
parsing formalism. We then define a set of ex-
tensions that allow the parser to construct logical
forms containing references, such as the !?e, t? ex-
pression from the example derivation in Section 3.
4.1 Background: CCG
CCG is a lexicalized, mildly context-sensitive
parsing formalism that models a wide range of
linguistic phenomena (Steedman, 1996; Steed-
man, 2000). Parses are constructed by combining
lexical entries according to a small set of relatively
simple rules. For example, consider the lexicon
flights := N : ?x.flight(x)
to := (N\N)/NP : ?y.?f.?x.f(x) ? to(x, y)
boston := NP : boston
Each lexical entry consists of a word and a cat-
egory. Each category includes syntactic and se-
mantic content. For example, the first entry
pairs the word flights with the category N :
?x.flight(x). This category has syntactic typeN ,
and includes the lambda-calculus semantic expres-
sion ?x.flight(x). In general, syntactic types can
either be simple types such as N , NP , or S, or
can be more complex types that make use of slash
notation, for example (N\N)/NP .
CCG parses construct parse trees according to
a set of combinator rules. For example, consider
the functional application combinators:3
A/B : f B : g ? A : f(g) (>)
B : g A\B : f ? A : f(g) (<)
The first rule is used to combine a category with
syntactic type A/B with a category to the right
of syntactic type B to create a new category of
type A. It also constructs a new lambda-calculus
expression by applying the function f to the
expression g. The second rule handles arguments
to the left. Using these rules, we can parse the
following phrase:
flights to boston
N (N\N)/NP NP
?x.flight(x) ?y.?f.?x.f(x) ? to(x, y) boston
>
(N\N)
?f.?x.f(x) ? to(x, boston)
<
N
?x.flight(x) ? to(x, boston)
The top-most parse operations pair each word with
a corresponding category from the lexicon. The
later steps are labeled with the rule that was ap-
plied (?> for the first and ?< for the second).
4.2 Parsing with References
In this section, we extend the CCG parser to intro-
duce references. We use an exclamation point fol-
lowed by a type expression to specify references
in a logical form. For example, !e is a reference to
an entity and !?e, t? is a reference to a function. As
motivated in Section 3, we introduce these expres-
sions so they can later be replaced with appropriate
lambda-calculus expressions from the context.
Sometimes references are lexically triggered.
For example, consider parsing the phrase ?show
me the ones that leave in the morning? from Ex-
ample 1(b) in Figure 1. Given the lexical entry:
ones := N : ?x.!?e, t?(x)
a CCG parser could produce the desired context-
3In addition to application, we make use of composition,
type raising and coordination combinators. A full description
of these combinators is beyond the scope of this paper. Steed-
man (1996; 2000) presents a detailed description of CCG.
978
independent logical form:
?x.!?e, t?(x) ? during(x,morning)
Our first extension is to simply introduce lexical
items that include references into the CCG lexi-
con. They describe anaphoric words, for example
including ?ones,? ?those,? and ?it.?
In addition, we sometimes need to introduce
references when there is no explicit lexical trig-
ger. For instance, Example 2(c) in Figure 1 con-
sists of the single word ?cheapest.? This query has
the same meaning as the longer request ?show me
the cheapest one,? but it does not include the lex-
ical reference. We add three CCG type-shifting
rules to handle these cases.
The first two new rules are applicable when
there is a category that is expecting an argument
with type ?e, t?. This argument is replaced with a
!?e, t? reference:
A/B : f ? A : f(?x.!?e, t?(x))
A\B : f ? A : f(?x.!?e, t?(x))
For example, using the first rule, we could produce
the following parse for Example 2(c)
cheapest
NP/N
?g.argmin(?x.g(x), ?y.fare(y))
NP
argmin(?x.!?e, t?(x), ?y.fare(y))
where the final category has the desired lambda-
caculus expression.
The third rule is motivated by examples such as
?show me nonstop flights.? Consider this sentence
being uttered after Example 1(a) in Figure 1. Al-
though there is a complete, context-independent
meaning, the request actually restricts the salient
set of flights to include only the nonstop ones. To
achieve this analysis, we introduce the rule:
A : f ? A : ?x.f(x) ? !?e, t?(x)
where f is an function of type ?e, t?.
With this rule, we can construct the parse
nonstop flights
N/N N
?f.?x.f(x) ? nonstop(x) ?x.flight(x)
>
N
?x.nonstop(x) ? flight(x)
N
?x.nonstop(x) ? flight(x) ? !?e, t?(x)
where the last parsing step is achieved with the
new type-shifting rule.
These three new parsing rules allow significant
flexibility when introducing references. Later, we
develop an approach that learns when to introduce
references and how to best resolve them.
5 Contextual Analysis
In this section, we first introduce the general pat-
terns of context-dependent analysis that we con-
sider. We then formally define derivations that
model these phenomena.
5.1 Overview
This section presents an overview of the ways that
the context C is used during the analysis.
References Every reference expression (!e or
!?e, t?) must be replaced with an expression from
the context. For example, in Section 3, we consid-
ered the following logical form:
?x.!?e, t?(x) ? during(x,morning)
In this case, we saw that replacing the !?e, t?
subexpression with the logical form for Exam-
ple 1(a), which is directly available in C, produces
the desired final meaning.
Elaborations Later statements can expand the
meaning of previous ones in ways that are diffi-
cult to model with references. For example, con-
sider analyzing Example 2(c) in Figure 1. Here the
phrase ?departing wednesday after 5 o?clock? has
a context-independent logical form:4
?x.day(x,wed) ? depart(x) > 1700 (1)
that must be combined with the meaning of the
previous sentence from the context C:
argmin(?x.fight(x) ? from(x,mil) ? to(x, orl),
?y.fare(y))
to produce the expression
argmin(?x.fight(x) ? from(x,mil) ? to(x, orl)
?day(x,wed) ? depart(x) > 1700,
?y.fare(y))
Intuitively, the phrase ?departing wednesday af-
ter 5 o?clock? is providing new constraints for the
set of flights embedded in the argmin expression.
We handle examples of this type by construct-
ing elaboration expressions from the zi in C. For
example, if we constructed the following function:
?f.argmin(?x.fight(x) ? from(x,mil)
? to(x, orl) ? f(x), (2)
?y.fare(y))
4Another possible option is the expression ?x.!?e, t? ?
day(x,wed)? depart(x) > 1700. However, there is no ob-
vious way to resolve the !?e, t? expression that would produce
the desired final meaning.
979
we could apply this function to Expression 1 and
produce the desired result. The introduction of the
new variable f provides a mechanism for expand-
ing the embedded subexpression.
References with Deletion When resolving ref-
erences, we will sometimes need to delete subparts
of the expressions that we substitute from the con-
text. For instance, consider Example 3(b) in Fig-
ure 1. The desired, final logical form is:
?x.flight(x) ? from(x, pit) ? to(x, la)
? day(x, thur) ? during(x, afternoon)
We need to construct this from the context-
independent logical form:
?x.!?e, t? ? day(x, thur) ? during(x, afternoon)
The reference !?e, t? must be resolved. The only
expression in the context C is the meaning from
the previous sentence, Example 3(a):
?x.flight(x) ? from(x, pit) ? to(x, la) (3)
? day(x, thur) ? during(x, evening)
Substituting this expression directly would pro-
duce the following logical form:
?x.flight(x) ? from(x, pit) ? to(x, la)
? day(x, thur) ? during(x, evening)
? day(x, thur) ? during(x, afternoon)
which specifies the day twice and has two different
time spans.
We can achieve the desired analysis by deleting
parts of expressions before they are substituted.
For example, we could remove the day and time
constraints from Expression 3 to create:
?x.flight(x) ? from(x, pit) ? to(x, la)
which would produce the desired final meaning
when substituted into the original expression.
Elaborations with Deletion We also allow
deletions for elaborations. In this case, we delete
subexpressions of the elaboration expression that
is constructed from the context.
5.2 Derivations
We now formally define a derivation that maps a
sentence wj and a context C = {z1, . . . , zj?1} to
an output logical form zj . We first introduce no-
tation for expressions in C that we will use in the
derivation steps. We then present a definition of
deletion. Finally, we define complete derivations.
Context Sets Given a context C, our algorithm
constructs three sets of expressions:
? Re(C): A set of e-type expressions that can
be used to resolve references.
? R?e,t?(C): A set of ?e, t?-type expressions
that can be used to resolve references.
? E(C): A set of possible elaboration expres-
sions (for example, see Expression 2).
We will provide the details of how these sets
are defined in Section 5.3. As an example, if C
contains only the logical form
?x.flight(x) ? from(x, pit) ? to(x, la)
then Re(C) = {pit, la} and R?e,t?(C) is a set that
contains a single entry, the complete logical form.
Deletion A deletion operator accepts a logical
form l and produces a new logical form l?. It con-
structs l? by removing a single subexpression that
appears in a coordination (conjunction or disjunc-
tion) in l. For example, if l is
?x.flight(x) ? from(x, pit) ? to(x, la)
there are three possible deletion operations, each
of which removes a single subexpression.
Derivations We now formally define a deriva-
tion to be a sequence d = (?, s1, . . . , sm). ? is a
CCG parse that constructs a context-independent
logical form pi with m? 1 reference expressions.5
Each si is a function that accepts as input a logi-
cal form, makes some change to it, and produces a
new logical form that is input to the next function
si+1. The initial si for i < m are reference steps.
The final sm is an optional elaboration step.
? Reference Steps: A reference step is a tuple
(l, l?, f, r, r1, . . . , rp). This operator selects a
reference f in the input logical form l and
an appropriately typed expression r from ei-
ther Re(C) or R?e,t?(C). It then applies a se-
quence of p deletion operators to create new
expressions r1 . . . rp. Finally, it constructs
the output logical form l? by substituting rp
for the selected reference f in l.
? Elaboration Steps: An elaboration step is a
tuple (l, l?, b, b1, . . . , bq). This operator se-
lects an expression b from E(C) and ap-
plies q deletions to create new expressions
b1 . . . bq. The output expression l? is bq(l).
5In practice, pi rarely contains more than one reference.
980
In general, the space of possible derivations is
large. In Section 6, we describe a linear model
and decoding algorithm that we use to find high
scoring derivations.
5.3 Context Sets
For a context C = {z1, . . . , zj?1}, we define sets
Re(C), R?e,t?(C), and E(C) as follows.
e-type Expressions Re(z) is a set of e-type ex-
pressions extracted from a logical form z. We de-
fine Re(C) =
?j?1
i=1 Re(zi).
Re(z) includes all e-type subexpressions of z.6
For example, if z is
argmin(?x.flight(x) ? from(x,mil) ? to(x, orl),
?y.fare(y))
the resulting set isRe(z) = {mil, orl, z}, where z
is included because the entire argmin expression
has type e.
?e, t?-type Expressions R?e,t?(z) is a set of
?e, t?-type expressions extracted from a logical
form z. We define R?e,t?(C) =
?j?1
i=1 R?e,t?(zi).
The set R?e,t?(z) contains all of the ?e, t?-type
subexpressions of z. For each quantified vari-
able x in z, it also contains a function ?x.g. The
expression g contains the subexpressions in the
scope of x that do not have free variables. For
example, if z is
?y.?x.flight(x) ? from(x, bos) ? to(x, phi)
? during(x,morning) ? aircraft(x) = y
R?e,t?(z) would contain two functions: the entire
expression z and the function
?x.flight(x) ? from(x, bos) ? to(x, phi)
? during(x,morning)
constructed from the variable x, where the subex-
pression aircraft(x) = y has been removed be-
cause it contains the free variable y.
Elaboration Expressions Finally, E(z) is a set
of elaboration expressions constructed from a log-
ical form z. We define E(C) =
?j?1
i=1 E(zi).
E(z) is defined by enumerating the places
where embedded variables are found in z. For
each logical variable x and each coordination
(conjunction or disjunction) in the scope of x, a
new expression is created by defining a function
?f.z? where z? has the function f(x) added to the
appropriate coordination. This procedure would
6A lambda-calculus expression can be represented as a
tree structure with flat branching for coordination (conjunc-
tion and disjunction). The subexpressions are the subtrees.
produce the example elaboration Expression 2 and
elaborations that expand other embedded expres-
sions, such as the quantifier in Example 1(c).
6 A Linear Model
In general, there will be many possible derivations
d for an input sentence w in the current context
C. In this section, we introduce a weighted lin-
ear model that scores derivations and a decoding
algorithm that finds high scoring analyses.
We define GEN(w;C) to be the set of possible
derivations d for an input sentence w given a con-
textC, as described in Section 5.2. Let ?(d) ? Rm
be an m-dimensional feature representation for a
derivation d and ? ? Rm be an m-dimensional pa-
rameter vector. The optimal derivation for a sen-
tence w given context C and parameters ? is
d?(w;C) = arg max
d?GEN(w;C)
? ? ?(d)
Decoding We now describe an approximate al-
gorithm for computing d?(w;C).
The CCG parser uses a CKY-style chart parsing
algorithm that prunes to the top N = 50 entries
for each span in the chart.
We use a beam search procedure to find the
best contextual derivations, with beam size N =
50. The beam is initialized to the top N logi-
cal forms from the CCG parser. The derivations
are extended with reference and elaboration steps.
The only complication is selecting the sequence of
deletions. For each possible step, we use a greedy
search procedure that selects the sequence of dele-
tions that would maximize the score of the deriva-
tion after the step is applied.
7 Learning
Figure 2 details the complete learning algorithm.
Training is online and error-driven. Step 1 parses
the current sentence in context. If the optimal logi-
cal form is not correct, Step 2 finds the best deriva-
tion that produces the labeled logical form7 and
does an additive, perceptron-style parameter up-
date. Step 3 updates the context. This algorithm is
a direct extension of the one introduced by Zettle-
moyer and Collins (2007). It maintains the context
but does not have the lexical induction step that
was previously used.
7For this computation, we use a modified version of the
beam search algorithm described in Section 6, which prunes
derivations that could not produce the desired logical form.
981
Inputs: Training examples {Ii|i = 1 . . . n}. Each Ii is a
sequence {(wi,j , zi,j) : j = 1 . . . ni} where wi,j is a
sentence and zi,j is a logical form. Number of training
iterations T . Initial parameters ?.
Definitions: The function ?(d) represents the features de-
scribed in Section 8. GEN(w;C) is the set of deriva-
tions for sentence w in context C. GEN(w, z;C) is
the set of derivations for sentence w in context C that
produce the final logical form z. The function L(d)
maps a derivation to its associated final logical form.
Algorithm:
? For t = 1 . . . T, i = 1 . . . n: (Iterate interactions)
? Set C = {}. (Reset context)
? For j = 1 . . . ni: (Iterate training examples)
Step 1: (Check correctness)
? Let d? = argmaxd?GEN(wi,j ;C) ? ? ?(d) .
? If L(d?) = zi,j , go to Step 3.
Step 2: (Update parameters)
? Let d? = argmaxd?GEN(wi,j ,zi,j ;C) ? ? ?(d) .
? Set ? = ? + ?(d?) ? ?(d?) .
Step 3: (Update context)
? Append zi,j to the current context C.
Output: Estimated parameters ?.
Figure 2: An online learning algorithm.
8 Features
We now describe the features for both the parsing
and context resolution stages of the derivation.
8.1 Parsing Features
The parsing features are used to score the context-
independent CCG parses during the first stage of
analysis. We use the set developed by Zettlemoyer
and Collins (2007), which includes features that
are sensitive to lexical choices and the structure of
the logical form that is constructed.
8.2 Context Features
The context features are functions of the deriva-
tion steps described in Section 5.2. In a deriva-
tion for sentence j of an interaction, let l be the
input logical form when considering a new step s
(a reference or elaboration step). Let c be the ex-
pression that s selects from a context set Re(zi),
R?e,t?(zi), or E(zi), where zi, i < j, is an ex-
pression in the current context. Also, let r be a
subexpression deleted from c. Finally, let f1 and
f2 be predicates, for example from or to.
Distance Features The distance features are bi-
nary indicators on the distance j ? i. These fea-
tures allow the model to, for example, favor re-
solving references with lambda-calculus expres-
sions recovered from recent sentences.
Copy Features For each possible f1 there is a
feature that tests if f1 is present in the context
expression c but not in the current expression l.
These features allow the model to learn to select
expressions from the context that introduce ex-
pected predicates. For example, flights usually
have a from predicate in the current expression.
Deletion Features For each pair (f1, f2) there
is a feature that tests if f1 is in the current expres-
sion l and f2 is in the deleted expression r. For
example, if f1 = f2 = days the model can favor
overriding old constraints about the departure day
with new ones introduced in the current utterance.
When f1 = during and f2 = depart time the
algorithm can learn that specific constraints on the
departure time override more general constraints
about the period of day.
9 Related Work
There has been a significant amount of work on
the problem of learning context-independent map-
pings from sentences to meaning representations.
Researchers have developed approaches using
models and algorithms from statistical machine
translation (Papineni et al, 1997; Ramaswamy
and Kleindienst, 2000; Wong and Mooney, 2007),
statistical parsing (Miller et al, 1996; Ge and
Mooney, 2005), inductive logic programming
(Zelle and Mooney, 1996; Tang and Mooney,
2000) and probabilistic push-down automata (He
and Young, 2006).
There were a large number of successful hand-
engineered systems developed for the original
ATIS task and other related tasks (e.g., (Carbonell
and Hayes, 1983; Seneff, 1992; Ward and Is-
sar, 1994; Levin et al, 2000; Popescu et al,
2004)). We are only aware of one system that
learns to construct context-dependent interpreta-
tions (Miller et al, 1996). The Miller et al (1996)
approach is fully supervised and produces a fi-
nal meaning representation in SQL. It requires
complete annotation of all of the syntactic, se-
mantic, and discourse decisions required to cor-
rectly analyze each training example. In contrast,
we learn from examples annotated with lambda-
calculus expressions that represent only the final,
context-dependent logical forms.
Finally, the CCG (Steedman, 1996; Steedman,
982
Train Dev. Test All
Interactions 300 99 127 526
Sentences 2956 857 826 4637
Table 1: Statistics of the ATIS training, development and
test (DEC94) sets, including the total number of interactions
and sentences. Each interaction is a sequence of sentences.
2000) parsing setup is closely related to previous
CCG research, including work on learning parsing
models (Clark and Curran, 2003), wide-coverage
semantic parsing (Bos et al, 2004) and grammar
induction (Watkinson and Manandhar, 1999).
10 Evaluation
Data In this section, we present experiments in
the context-dependent ATIS domain (Dahl et al,
1994). Table 1 presents statistics for the train-
ing, development, and test sets. To facilitate com-
parison with previous work, we used the standard
DEC94 test set. We randomly split the remaining
data to make training and development sets. We
manually converted the original SQL meaning an-
notations to lambda-calculus expressions.
Evaluation Metrics Miller et al (1996) report
accuracy rates for recovering correct SQL annota-
tions on the test set. For comparison, we report ex-
act accuracy rates for recovering completely cor-
rect lambda-calculus expressions.
We also present precision, recall and F-measure
for partial match results that test if individual at-
tributes, such as the from and to cities, are cor-
rectly assigned. See the discussion by Zettlemoyer
and Collins (2007) (ZC07) for the full details.
Initialization and Parameters The CCG lexi-
con is hand engineered. We constructed it by run-
ning the ZC07 algorithm to learn a lexicon on
the context-independent ATIS data set and making
manual corrections to improve performance on the
training set. We also added lexical items with ref-
erence expressions, as described in Section 4.
We ran the learning algorithm for T = 4 train-
ing iterations. The parsing feature weights were
initialized as in ZC07, the context distance fea-
tures were given small negative weights, and all
other feature weights were initially set to zero.
Test Setup During evaluation, the context C =
{z1 . . . zj?1} contains the logical forms output by
the learned system for the previous sentences. In
general, errors made while constructing these ex-
pressions can propogate if they are used in deriva-
tions for new sentences.
System
Partial Match Exact
Prec. Rec. F1 Acc.
Full Method 95.0 96.5 95.7 83.7
Miller et al ? ? ? 78.4
Table 2: Performance on the ATIS DEC94 test set.
Limited Context
Partial Match Exact
Prec. Rec. F1 Acc.
M = 0 96.2 57.3 71.8 45.4
M = 1 94.9 91.6 93.2 79.8
M = 2 94.8 93.2 94.0 81.0
M = 3 94.5 94.3 94.4 82.1
M = 4 94.9 92.9 93.9 81.6
M = 10 94.2 94.0 94.1 81.4
Table 3: Performance on the ATIS development set for
varying context window lengths M .
Results Table 2 shows performance on the ATIS
DEC94 test set. Our approach correctly recov-
ers 83.7% of the logical forms. This result com-
pares favorably to Miller et al?s fully-supervised
approach (1996) while requiring significantly less
annotation effort.
We also evaluated performance when the con-
text is limited to contain only the M most recent
logical forms. Table 3 shows results on the devel-
opment set for different values of M . The poor
performance with no context (M = 0) demon-
strates the need for context-dependent analysis.
Limiting the context to the most recent statement
(M = 1) significantly improves performance
while using the last three utterances (M = 3) pro-
vides the best results.
Finally, we evaluated a variation where the con-
text contains gold-standard logical forms during
evaluation instead of the output of the learned
model. On the development set, this approach
achieved 85.5% exact-match accuracy, an im-
provement of approximately 3% over the standard
approach. This result suggests that incorrect log-
ical forms in the context have a relatively limited
impact on overall performance.
11 Conclusion
In this paper, we addressed the problem of
learning context-dependent mappings from sen-
tences to logical form. We developed a context-
dependent analysis model and showed that it can
be effectively trained with a hidden-variable vari-
ant of the perceptron algorithm. In the experi-
ments, we showed that the approach recovers fully
correct logical forms with 83.7% accuracy.
983
References
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of the International Confer-
ence on Computational Linguistics.
Jaime G. Carbonell and Philip J. Hayes. 1983. Re-
covery strategies for parsing extragrammatical lan-
guage. American Journal of Computational Lin-
guistics, 9.
Stephen Clark and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: the ATIS-3 corpus. In ARPA HLT Workshop.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proceedings of the Conference on Com-
putational Natural Language Learning.
Yulan He and Steve Young. 2006. Spoken language
understanding using the hidden vector state model.
Speech Communication, 48(3-4).
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Proc.
of the Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky, M. Rahim, P. Ruscitti, and M.Walker.
2000. The AT&T darpa communicator mixed-
initiative spoken dialogue system. In Proceedings of
the International Conference on Spoken Language
Processing.
Scott Miller, David Stallard, Robert J. Bobrow, and
Richard L. Schwartz. 1996. A fully statistical ap-
proach to natural language interfaces. In Proc. of
the Association for Computational Linguistics.
K. A. Papineni, S. Roukos, and T. R. Ward. 1997.
Feature-based language understanding. In Proceed-
ings of European Conference on Speech Communi-
cation and Technology.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,
David Ko, and Alexander Yates. 2004. Modern
natural language interfaces to databases: Composing
statistical parsing with semantic tractability. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics.
Ganesh N. Ramaswamy and Jan Kleindienst. 2000.
Hierarchical feature-based translation for scalable
natural language understanding. In Proceedings of
International Conference on Spoken Language Pro-
cessing.
Stephanie Seneff. 1992. Robust parsing for spoken
language systems. In Proc. of the IEEE Conference
on Acoustics, Speech, and Signal Processing.
Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press.
Lappoon R. Tang and Raymond J. Mooney. 2000.
Automated construction of database interfaces: In-
tegrating statistical and relational learning for se-
mantic parsing. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Christopher Manning. 2004. Max-
margin parsing. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing.
Wayne Ward and Sunil Issar. 1994. Recent improve-
ments in the CMU spoken language understanding
system. In Proceedings of the workshop on Human
Language Technology.
Stephen Watkinson and Suresh Manandhar. 1999. Un-
supervised lexical learning with categorial gram-
mars using the LLL corpus. In Proceedings of the
1st Workshop on Learning Language in Logic.
Yuk Wah Wong and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the Asso-
ciation for Computational Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Conference on Un-
certainty in Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proc. of the Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
984
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223?1233,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Inducing Probabilistic CCG Grammars from Logical Form
with Higher-Order Unification
Tom Kwiatkowski?
t.m.kwiatkowksi@sms.ed.ac.uk
Luke Zettlemoyer?
lsz@cs.washington.edu
Sharon Goldwater?
sgwater@inf.ed.ac.uk
Mark Steedman?
steedman@inf.ed.ac.uk
?School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Computer Science & Engineering
University of Washington
Seattle, WA 98195
Abstract
This paper addresses the problem of learn-
ing to map sentences to logical form, given
training data consisting of natural language
sentences paired with logical representations
of their meaning. Previous approaches have
been designed for particular natural languages
or specific meaning representations; here we
present a more general method. The approach
induces a probabilistic CCG grammar that
represents the meaning of individual words
and defines how these meanings can be com-
bined to analyze complete sentences. We
use higher-order unification to define a hy-
pothesis space containing all grammars con-
sistent with the training data, and develop
an online learning algorithm that efficiently
searches this space while simultaneously es-
timating the parameters of a log-linear parsing
model. Experiments demonstrate high accu-
racy on benchmark data sets in four languages
with two different meaning representations.
1 Introduction
A key aim in natural language processing is to learn
a mapping from natural language sentences to for-
mal representations of their meaning. Recent work
has addressed this problem by learning semantic
parsers given sentences paired with logical meaning
representations (Thompson & Mooney, 2002; Kate
et al, 2005; Kate & Mooney, 2006; Wong &
Mooney, 2006, 2007; Zettlemoyer & Collins, 2005,
2007; Lu et al, 2008). For example, the training
data might consist of English sentences paired with
lambda-calculus meaning representations:
Sentence: which states border texas
Meaning: ?x.state(x) ? next to(x, tex)
Given pairs like this, the goal is to learn to map new,
unseen, sentences to their corresponding meaning.
Previous approaches to this problem have been
tailored to specific natural languages, specific mean-
ing representations, or both. Here, we develop an
approach that can learn to map any natural language
to a wide variety of logical representations of lin-
guistic meaning. In addition to data like the above,
this approach can also learn from examples such as:
Sentence: hangi eyaletin texas ye siniri vardir
Meaning: answer(state(borders(tex)))
where the sentence is in Turkish and the meaning
representation is a variable-free logical expression
of the type that has been used in recent work (Kate
et al, 2005; Kate & Mooney, 2006; Wong &
Mooney, 2006; Lu et al, 2008).
The reason for generalizing to multiple languages
is obvious. The need to learn over multiple repre-
sentations arises from the fact that there is no stan-
dard representation for logical form for natural lan-
guage. Instead, existing representations are ad hoc,
tailored to the application of interest. For example,
the variable-free representation above was designed
for building natural language interfaces to databases.
Our approach works by inducing a combinatory
categorial grammar (CCG) (Steedman, 1996, 2000).
A CCG grammar consists of a language-specific
lexicon, whose entries pair individual words and
phrases with both syntactic and semantic informa-
tion, and a universal set of combinatory rules that
1223
project that lexicon onto the sentences and meanings
of the language via syntactic derivations. The learn-
ing process starts by postulating, for each sentence
in the training data, a single multi-word lexical item
pairing that sentence with its complete logical form.
These entries are iteratively refined with a restricted
higher-order unification procedure (Huet, 1975) that
defines all possible ways to subdivide them, consis-
tent with the requirement that each training sentence
can still be parsed to yield its labeled meaning.
For the data sets we consider, the space of pos-
sible grammars is too large to explicitly enumerate.
The induced grammar is also typically highly am-
biguous, producing a large number of possible anal-
yses for each sentence. Our approach discriminates
between analyses using a log-linear CCG parsing
model, similar to those used in previous work (Clark
& Curran, 2003, 2007), but differing in that the syn-
tactic parses are treated as a hidden variable during
training, following the approach of Zettlemoyer &
Collins (2005, 2007). We present an algorithm that
incrementally learns the parameters of this model
while simultaneously exploring the space of possi-
ble grammars. The model is used to guide the pro-
cess of grammar refinement during training as well
as providing a metric for selecting the best analysis
for each new sentence.
We evaluate the approach on benchmark datasets
from a natural language interface to a database of
US Geography (Zelle & Mooney, 1996). We show
that accurate models can be learned for multiple
languages with both the variable-free and lambda-
calculus meaning representations introduced above.
We also compare performance to previous methods
(Kate & Mooney, 2006; Wong & Mooney, 2006,
2007; Zettlemoyer & Collins, 2005, 2007; Lu et al,
2008), which are designed with either language- or
representation- specific constraints that limit gener-
alization, as discussed in more detail in Section 6.
Despite being the only approach that is general
enough to run on all of the data sets, our algorithm
achieves similar performance to the others, even out-
performing them in several cases.
2 Overview of the Approach
The goal of our algorithm is to find a function
f : x ? z that maps sentences x to logical ex-
pressions z. We learn this function by inducing a
probabilistic CCG (PCCG) grammar from a train-
ing set {(xi, zi)|i = 1 . . . n} containing example
(sentence, logical-form) pairs such as (?New York
borders Vermont?, next to(ny, vt)). The induced
grammar consists of two components which the al-
gorithm must learn:
? A CCG lexicon, ?, containing lexical items
that define the space of possible parses y for
an input sentence x. Each parse contains both
syntactic and semantic information, and defines
the output logical form z.
? A parameter vector, ?, that defines a distribu-
tion over the possible parses y, conditioned on
the sentence x.
We will present the approach in two parts. The
lexical induction process (Section 4) uses a re-
stricted form of higher order unification along with
the CCG combinatory rules to propose new entries
for ?. The complete learning algorithm (Section 5)
integrates this lexical induction with a parameter es-
timation scheme that learns ?. Before presenting the
details, we first review necessary background.
3 Background
This section provides an introduction to the ways in
which we will use lambda calculus and higher-order
unification to construct meaning representations. It
also reviews the CCG grammar formalism and prob-
abilistic extensions to it, including existing parsing
and parameter estimation techniques.
3.1 Lambda Calculus and Higher-Order
Unification
We assume that sentence meanings are represented
as logical expressions, which we will construct from
the meaning of individual words by using the op-
erations defined in the lambda calculus. We use a
version of the typed lambda calculus (cf. Carpenter
(1997)), in which the basic types include e, for en-
tities; t, for truth values; and i for numbers. There
are also function types of the form ?e, t? that are as-
signed to lambda expressions, such as ?x.state(x),
which take entities and return truth values. We
represent the meaning of words and phrases using
1224
lambda-calculus expressions that can contain con-
stants, quantifiers, logical connectors, and lambda
abstractions.
The advantage of using the lambda calculus
lies in its generality. The meanings of individ-
ual words and phrases can be arbitrary lambda ex-
pressions, while the final meaning for a sentence
can take different forms. It can be a full lambda-
calculus expression, a variable-free expression such
as answer(state(borders(tex))), or any other log-
ical expression that can be built from the primitive
meanings via function application and composition.
The higher-order unification problem (Huet,
1975) involves finding a substitution for the free
variables in a pair of lambda-calculus expressions
that, when applied, makes the expressions equal
each other. This problem is notoriously complex;
in the unrestricted form (Huet, 1973), it is undecid-
able. In this paper, we will guide the grammar in-
duction process using a restricted version of higher-
order unification that is tractable. For a given ex-
pression h, we will need to find expressions for f
and g such that either h = f(g) or h = ?x.f(g(x)).
This limited form of the unification problem will al-
low us to define the ways to split h into subparts
that can be recombined with CCG parsing opera-
tions, which we will define in the next section, to
reconstruct h.
3.2 Combinatory Categorial Grammar
CCG (Steedman, 2000) is a linguistic formalism
that tightly couples syntax and semantics, and
can be used to model a wide range of language
phenomena. For present purposes a CCG grammar
includes a lexicon ? with entries like the following:
New York ` NP : ny
borders ` S\NP/NP : ?x?y.next to(y, x)
Vermont ` NP : vt
where each lexical item w`X : h has words w, a
syntactic categoryX , and a logical form h expressed
as a lambda-calculus expression. For the first exam-
ple, these are ?New York,? NP , and ny. CCG syn-
tactic categories may be atomic (such as S, NP ) or
complex (such as S\NP/NP ).
CCG combines categories using a set of com-
binatory rules. For example, the forward (>) and
backward (<) application rules are:
X/Y : f Y : g ? X : f(g) (>)
Y : g X\Y : f ? X : f(g) (<)
These rules apply to build syntactic and semantic
derivations under the control of the word order infor-
mation encoded in the slash directions of the lexical
entries. For example, given the lexicon above, the
sentence New York borders Vermont can be parsed
to produce:
New York borders Vermont
NP (S\NP )/NP NP
ny ?x?y.next to(y, x) vt
>
(S\NP )
?y.next to(y, vt)
<
S
next to(ny, vt)
where each step in the parse is labeled with the com-
binatory rule (? > or ? <) that was used.
CCG also includes combinatory rules of forward
(> B) and backward (< B) composition:
X/Y : f Y/Z : g ?X/Z : ?x.f(g(x)) (> B)
Y \Z : g X\Y : f ?X\Z : ?x.f(g(x)) (< B)
These rules provide for a relaxed notion of con-
stituency which will be useful during learning as we
reason about possible refinements of the grammar.
We also allow vertical slashes in CCG categories,
which act as wild cards. For example, with this
extension the forward application combinator (>)
could be used to combine the category S/(S|NP )
with any of S\NP , S/NP , or S|NP . Figure 1
shows two parses where the composition combina-
tors and vertical slashes are used. These parses
closely resemble the types of analyses that will be
possible under the grammars we learn in the experi-
ments described in Section 8.
3.3 Probabilistic CCGs
Given a CCG lexicon ?, there will, in general, be
many possible parses for each sentence. We select
the most likely alternative using a log-linear model,
which consists of a feature vector ? and a parame-
ter vector ?. The joint probability of a logical form
z constructed with a parse y, given a sentence x is
1225
hangi eyaletin texas ye siniri vardir
S/NP NP/NP NP NP\NP
?x.answer(x) ?x.state(x) tex ?x.border(x)
<
NP
border(tex)
>
NP
state(border(tex))
>
S
answer(state(border(tex)))
what states border texas
S/(S|NP ) S|NP/(S|NP ) S\NP/NP NP
?f?x.f(x) ?f?x.state(x)?f(x) ?y?x.next to(x, y) tex
>B
S|NP/NP
?y?x.state(x) ? next to(x, y)
>
S|NP
?x.state(x) ? next to(x, tex)
>
S
?x.state(x) ? next to(x, tex)
Figure 1: Two examples of CCG parses with different logical form representations.
defined as:
P (y, z|x; ?,?) =
e???(x,y,z)
?
(y?,z?) e
???(x,y?,z?)
(1)
Section 7 defines the features used in the experi-
ments, which include, for example, lexical features
that indicate when specific lexical items in ? are
used in the parse y. For parsing and parameter es-
timation, we use standard algorithms (Clark & Cur-
ran, 2007), as described below.
The parsing, or inference, problem is to find the
most likely logical form z given a sentence x, as-
suming the parameters ? and lexicon ? are known:
f(x) = arg max
z
p(z|x; ?,?) (2)
where the probability of the logical form is found by
summing over all parses that produce it:
p(z|x; ?,?) =
?
y
p(y, z|x; ?,?) (3)
In this approach the distribution over parse trees y
is modeled as a hidden variable. The sum over
parses in Eq. 3 can be calculated efficiently using
the inside-outside algorithm with a CKY-style pars-
ing algorithm.
To estimate the parameters themselves, we
use stochastic gradient updates (LeCun et al,
1998). Given a set of n sentence-meaning pairs
{(xi, zi) : i = 1...n}, we update the parameters ? it-
eratively, for each example i, by following the local
gradient of the conditional log-likelihood objective
Oi = logP (zi|xi; ?,?). The local gradient of the
individual parameter ?j associated with feature ?j
and training instance (xi, zi) is given by:
?Oi
??j
= Ep(y|xi,zi;?,?)[?j(xi, y, zi)]
?Ep(y,z|xi;?,?)[?j(xi, y, z)]
(4)
As with Eq. 3, all of the expectations in Eq. 4 are
calculated through the use of the inside-outside al-
gorithm on a pruned parse chart. In the experiments,
each chart cell was pruned to the top 200 entries.
4 Splitting Lexical Items
Before presenting a complete learning algorithm, we
first describe how to use higher-order unification to
define a procedure for splitting CCG lexical entries.
This splitting process is used to expand the lexicon
during learning. We seed the lexical induction with
a multi-word lexical item xi`S :zi for each training
example (xi, zi), consisting of the entire sentence xi
and its associated meaning representation zi. For ex-
ample, one initial lexical item might be:
New York borders Vermont `S:next to(ny, vt) (5)
Although these initial, sentential lexical items
can parse the training data, they will not generalize
well to unseen data. To learn effectively, we will
need to split overly specific entries of this type into
pairs of new, smaller, entries that generalize better.
For example, one possible split of the lexical entry
given in (5) would be the pair:
New York borders ` S/NP : ?x.next to(ny, x),
Vermont `NP : vt
where we broke the original logical expression into
two new ones ?x.next to(ny, x) and vt, and paired
them with syntactic categories that allow the new
lexical entries to be recombined to produce the orig-
inal analysis. The next three subsections define the
set of possible splits for any given lexical item. The
process is driven by solving a higher-order unifica-
tion problem that defines all of the ways of splitting
the logical expression into two parts, as described in
Section 4.1. Section 4.2 describes how to construct
1226
syntactic categories that are consistent with the two
new fragments of logical form and which will allow
the new lexical items to recombine. Finally, Sec-
tion 4.3 defines the full set of lexical entry pairs that
can be created by splitting a lexical entry.
As we will see, this splitting process is overly pro-
lific for any single language and will yield many
lexical items that do not generalize well. For
example, there is nothing in our original lexical
entry above that provides evidence that the split
should pair ?Vermont? with the constant vt and not
?x.next to(ny, x). Section 5 describes how we
estimate the parameters of a probabilistic parsing
model and how this parsing model can be used to
guide the selection of items to add to the lexicon.
4.1 Restricted Higher-Order Unification
The set of possible splits for a logical expression
h is defined as the solution to a pair of higher-
order unification problems. We find pairs of logi-
cal expressions (f, g) such that either f(g) = h or
?x.f(g(x)) = h. Solving these problems creates
new expressions f and g that can be recombined ac-
cording to the CCG combinators, as defined in Sec-
tion 3.2, to produce h.
In the unrestricted case, there can be infinitely
many solution pairs (f, g) for a given expression h.
For example, when h = tex and f = ?x.tex, the
expression g can be anything. Although it would be
simple enough to forbid vacuous variables in f and
g, the number of solutions would still be exponen-
tial in the size of h. For example, when h contains a
conjunction, such as h = ?x.city(x)?major(x)?
in(x, tex), any subset of the expressions in the con-
junction can be assigned to f (or g).
To limit the number of possible splits, we enforce
the following restrictions on the possible higher-
order solutions that will be used during learning:
? No Vacuous Variables: Neither g or f can be a
function of the form ?x.e where the expression
e does not contain the variable x. This rules out
functions such as ?x.tex.
? Limited Coordination Extraction: The ex-
pression g cannot contain more than N of the
conjuncts that appear in any coordination in
h. For example, with N = 1 the expression
g = ?x.city(x)?major(x) could not be used
as a solution given the h conjuction above. We
use N = 4 in our experimental evaluation.
? Limited Application: The function f can-
not contain new variables applied to any non-
variable subexpressions from h. For example,
if h = ?x.in(x, tex), the pair f = ?q.q(tex)
and g = ?y?x.in(x, y) is forbidden.
Together, these three restrictions guarantee that
the number of splits is, in the worst case, an N -
degree polynomial of the number of constants in h.
The constraints were designed to increase the effi-
ciency of the splitting algorithm without impacting
performance on the development data.
4.2 Splitting Categories
We define the set of possible splits for a category
X :h with syntax X and logical form h by enumer-
ating the solution pairs (f, g) to the higher-order
unification problems defined above and creating
syntactic categories for the resulting expressions.
For example, given X :h = S\NP :?x.in(x, tex),
f = ?y?x.in(x, y), and g = tex, we would
produce the following two pairs of new categories:
( S\NP/NP :?y?x.in(x, y) , NP :tex )
( NP :tex , S\NP\NP :?y?x.in(x, y) )
which were constructed by first choosing the syntac-
tic category for g, in this caseNP , and then enumer-
ating the possible directions for the new slash in the
category containing f . We consider each of these
two steps in more detail below.
The new syntactic category for g is determined
based on its type, T (g). For example, T (tex) = e
and T (?x.state(x)) = ?e, t?. Then, the function
C(T ) takes an input type T and returns the syntactic
category of T as follows:
C(T ) =
?
?
?
NP if T = e
S if T = t
C(T2)|C(T1) when T = ?T1, T2?
The basic types e and t are assigned syntactic
categories NP and S, and all functional types
are assigned categories recursively. For exam-
ple C(?e, t?) = S|NP and C(?e, ?e, t??) =
S|NP |NP . This definition of CCG categories is
unconventional in that it never assigns atomic cate-
gories to functional types. For example, there is no
1227
distinct syntactic category N for nouns (which have
semantic type ?e, t?). Instead, the more complex cat-
egory S|NP is used.
Now, we are ready to define the set of all category
splits. For a category A = X:h we can define
SC(A) = {FA(A) ? BA(A) ? FC(A) ? BC(A)}
which is a union of sets, each of which includes
splits for a single CCG operator. For example,
FA(X:h) is the set of category pairs
FA(X:h) = {(X/Y :f, Y :g) | h=f(g) ? Y=C(T (g))
}
where each pair can be combined with the forward
application combinator, described in Section 3.2, to
reconstruct X:h.
The remaining three sets are defined similarly,
and are associated with the backward application
and forward and backward composition operators,
respectively:
BA(X:h) = {(Y :g,X\Y :f) | h=f(g) ? Y=C(T (g))
}
FC(X/Y :h) = {(X/W :f,W/Y :g) |
h=?x.f(g(x)) ?W=C(T (g(x)))
}
BC(X\Y :h) = {(W\Y :g,X\W :f) |
h=?x.f(g(x)) ?W=C(T (g(x)))
}
where the composition sets FC and BC only accept
input categories with the appropriate outermost slash
direction, for example FC(X/Y :h).
4.3 Splitting Lexical Items
We can now define the lexical splits that will be used
during learning. For lexical entry w0:n ` A, with
word sequence w0:n = ?w0, . . . , wn? and CCG cat-
egory A, define the set SL of splits to be:
SL(w0:n`A) = {(w0:i`B,wi+1:n`C) |
0 ? i < n ? (B,C) ? SC(A)}
where we enumerate all ways of splitting the words
sequence w0:n and aligning the subsequences with
categories in SC(A), as defined in the last section.
5 Learning Algorithm
The previous section described how a splitting pro-
cedure can be used to break apart overly specific
lexical items into smaller ones that may generalize
better to unseen data. The space of possible lexi-
cal items supported by this splitting procedure is too
large to explicitly enumerate. Instead, we learn the
parameters of a PCCG, which is used both to guide
the splitting process, and also to select the best parse,
given a learned lexicon.
Figure 2 presents the unification-based learning
algorithm, UBL. This algorithm steps through the
data incrementally and performs two steps for each
training example. First, new lexical items are in-
duced for the training instance by splitting and merg-
ing nodes in the best correct parse, given the current
parameters. Next, the parameters of the PCCG are
updated by making a stochastic gradient update on
the marginal likelihood, given the updated lexicon.
Inputs and Initialization The algorithm takes as
input the training set of n (sentence, logical form)
pairs {(xi, zi) : i = 1...n} along with an NP list,
?NP , of proper noun lexical items such as Texas`
NP :tex. The lexicon, ?, is initialized with a single
lexical item xi`S :zi for each of the training pairs
along with the contents of the NP list. It is possible
to run the algorithm without the initial NP list; we
include it to allow direct comparisons with previous
approaches, which also included NP lists. Features
and initial feature weights are described in Section 7.
Step 1: Updating the Lexicon In the lexical up-
date step the algorithm first computes the best cor-
rect parse tree y? for the current training exam-
ple and then uses y? as input to the procedure
NEW-LEX, which determines which (if any) new
lexical items to add to ?. NEW-LEX begins by enu-
merating all pairs (C,wi:j), for i < j, where C is a
category occurring at a node in y? and wi:j are the
(two or more) words it spans. For example, in the
left parse in Figure 1, there would be four pairs: one
with the category C = NP\NP :?x.border(x) and
the phrase wi:j =?ye siniri vardir?, and one for each
non-leaf node in the tree.
For each pair (C,wi:j), NEW-LEX considers in-
troducing a new lexical item wi:j`C, which allows
for the possibility of a parse where the subtree rooted
at C is replaced with this new entry. (If C is a leaf
node, this item will already exist.) NEW-LEX also
considers adding each pair of new lexical items that
is obtained by splitting wi:j`C as described in Sec-
tion 4, thereby considering many different ways of
reanalyzing the node. This process creates a set of
possible new lexicons, where each lexicon expands
1228
? in a different way by adding the items from either
a single split or a single merge of a node in y?.
For each potential new lexicon ??, NEW-LEX
computes the probability p(y?|xi, zi; ??,??) of the
original parse y? under ?? and parameters ?? that are
the same as ? but have weights for the new lexical
items, as described in Section 7. It also finds the
best new parse y? = arg maxy p(y|xi, zi; ??,??).1
Finally, NEW-LEX selects the ?? with the largest
difference in log probability between y? and y?, and
returns the new entries in ??. If y? is the best parse
for every ??, NEW-LEX returns the empty set; the
lexicon will not change.
Step 2: Parameter Updates For each training ex-
ample we update the parameters ? using the stochas-
tic gradient updates given by Eq. 4.
Discussion The alternation between refining the
lexicon and updating the parameters drives the learn-
ing process. The initial model assigns a conditional
likelihood of one to each training example (there
is a single lexical item for each sentence xi, and
it contains the labeled logical form zi). Although
the splitting step often decreases the probability of
the data, the new entries it produces are less spe-
cific and should generalize better. Since we initially
assign positive weights to the parameters for new
lexical items, the overall approach prefers splitting;
trees with many lexical items will initially be much
more likely. However, if the learned lexical items
are used in too many incorrect parses, the stochastic
gradient updates will down weight them to the point
where the lexical induction step can merge or re-split
nodes in the trees that contain them. This allows the
approach to correct the lexicon and, hopefully, im-
prove future performance.
6 Related Work
Previous work has focused on a variety of different
meaning representations. Several approaches have
been designed for the variable-free logical repre-
sentations shown in examples throughout this pa-
per. For example, Kate & Mooney (2006) present a
method (KRISP) that extends an existing SVM learn-
ing algorithm to recover logical representations. The
1This computation can be performed efficiently by incre-
mentally updating the parse chart used to find y?.
Inputs: Training set {(xi, zi) : i = 1 . . . n} where each
example is a sentence xi paired with a logical form
zi. Set of NP lexical items ?NP . Number of iter-
ations T . Learning rate parameter ?0 and cooling
rate parameter c.
Definitions: The function NEW-LEX(y) takes a parse
y and returns a set of new lexical items found by
splitting and merging categories in y, as described
in Section 5. The distributions p(y|x, z; ?,?) and
p(y, z|x; ?,?) are defined by the log-linear model,
as described in Section 3.3.
Initialization:
? Set ? = {xi ` S : zi} for all i = 1 . . . n.
? Set ? = ? ? ?NP
? Initialize ? using coocurrence statistics, as de-
scribed in Section 7.
Algorithm:
For t = 1 . . . T, i = 1 . . . n :
Step 1: (Update Lexicon)
? Let y? = arg maxy p(y|xi, zi; ?,?)
? Set ? = ? ?NEW-LEX(y?) and expand the
parameter vector ? to contain entries for the
new lexical items, as described in Section 7.
Step 2: (Update Parameters)
? Let ? = ?01+c?k where k = i+ t? n.
? Let ? = Ep(y|xi,zi;?,?)[?(xi, y, zi)]
?Ep(y,z|xi;?,?)[?(xi, y, z)]
? Set ? = ? + ??
Output: Lexicon ? and parameters ?.
Figure 2: The UBL learning algorithm.
WASP system (Wong & Mooney, 2006) uses statis-
tical machine translation techniques to learn syn-
chronous context free grammars containing both
words and logic. Lu et al (2008) (Lu08) developed
a generative model that builds a single hybrid tree
of words, syntax and meaning representation. These
algorithms are all language independent but repre-
sentation specific.
Other algorithms have been designed to re-
cover lambda-calculus representations. For exam-
ple, Wong & Mooney (2007) developed a variant
of WASP (?-WASP) specifically designed for this
alternate representation. Zettlemoyer & Collins
(2005, 2007) developed CCG grammar induction
techniques where lexical items are proposed accord-
ing to a set of hand-engineered lexical templates.
1229
Our approach eliminates this need for manual effort.
Another line of work has focused on recover-
ing meaning representations that are not based on
logic. Examples include an early statistical method
for learning to fill slot-value representations (Miller
et al, 1996) and a more recent approach for recover-
ing semantic parse trees (Ge & Mooney, 2006). Ex-
ploring the extent to which these representations are
compatible with the logic-based learning approach
we developed is an important area for future work.
Finally, there is work on using categorial gram-
mars to solve other, related learning problems.
For example, Buszkowski & Penn (1990) describe
a unification-based approach for grammar discov-
ery from bracketed natural language sentences and
Villavicencio (2002) developed an approach for
modeling child language acquisition. Additionally,
Bos et al (2004) consider the challenging problem
of constructing broad-coverage semantic representa-
tions with CCG, but do not learn the lexicon.
7 Experimental Setup
Features We use two types of features in our
model. First, we include a set of lexical features:
For each lexical item L ? ?, we include a feature
?L that fires when L is used. Second, we include se-
mantic features that are functions of the output logi-
cal expression z. Each time a predicate p in z takes
an argument a with type T (a) in position i it trig-
gers two binary indicator features: ?(p,a,i) for the
predicate-argument relation; and ?(p,T (a),i) for the
predicate argument-type relation.
Initialization The weights for the semantic fea-
tures are initialized to zero. The weights for the lex-
ical features are initialized according to coocurrance
statistics estimated with the Giza++ (Och & Ney,
2003) implementation of IBM Model 1. We com-
pute translation scores for (word, constant) pairs that
cooccur in examples in the training data. The initial
weight for each ?L is set to ten times the average
score over the (word, constant) pairs in L, except for
the weights of seed lexical entries in ?NP which are
set to 10 (equivalent to the highest possible coocur-
rence score). We used the learning rate ?0 = 1.0
and cooling rate c = 10?5 in all training scenar-
ios, and ran the algorithm for T = 20 iterations.
These values were selected with cross validation on
the Geo880 development set, described below.
Data and Evaluation We evaluate our system
on the GeoQuery datasets, which contain natural-
language queries of a geographical database paired
with logical representations of each query?s mean-
ing. The full Geo880 dataset contains 880 (English-
sentence, logical-form) pairs, which we split into a
development set of 600 pairs and a test set of 280
pairs, following Zettlemoyer & Collins (2005). The
Geo250 dataset is a subset of Geo880 containing
250 sentences that have been translated into Turk-
ish, Spanish and Japanese as well as the original En-
glish. Due to the small size of this dataset we use
10-fold cross validation for evaluation. We use the
same folds as Wong & Mooney (2006, 2007) and Lu
et al (2008), allowing a direct comparison.
The GeoQuery data is annotated with both
lambda-calculus and variable-free meaning rep-
resentations, which we have seen examples of
throughout the paper. We report results for both rep-
resentations, using the standard measures of Recall
(percentage of test sentences assigned correct log-
ical forms), Precision (percentage of logical forms
returned that are correct) and F1 (the harmonic mean
of Precision and Recall).
Two-Pass Parsing To investigate the trade-off be-
tween precision and recall, we report results with a
two-pass parsing strategy. When the parser fails to
return an analysis for a test sentence due to novel
words or usage, we reparse the sentence and allow
the parser to skip words, with a fixed cost. Skip-
ping words can potentially increase recall, if the ig-
nored word is an unknown function word that does
not contribute semantic content.
8 Results and Discussion
Tables 1, 2, and 3 present the results for all of the ex-
periments. In aggregate, they demonstrate that our
algorithm, UBL, learns accurate models across lan-
guages and for both meaning representations. This
is a new result; no previous system is as general.
We also see the expected tradeoff between preci-
sion and recall that comes from the two-pass parsing
approach, which is labeled UBL-s. With the abil-
ity to skip words, UBL-s achieves the highest recall
of all reported systems for all evaluation conditions.
1230
System
English Spanish
Rec. Pre. F1 Rec. Pre. F1
WASP 70.0 95.4 80.8 72.4 91.2 81.0
Lu08 72.8 91.5 81.1 79.2 95.2 86.5
UBL 78.1 88.2 82.7 76.8 86.8 81.4
UBL-s 80.4 80.8 80.6 79.7 80.6 80.1
System
Japanese Turkish
Rec. Pre. F1 Rec. Pre. F1
WASP 74.4 92.0 82.9 62.4 97.0 75.9
Lu08 76.0 87.6 81.4 66.8 93.8 78.0
UBL 78.5 85.5 81.8 70.4 89.4 78.6
UBL-s 80.5 80.6 80.6 74.2 75.6 74.9
Table 1: Performance across languages on Geo250 with
variable-free meaning representations.
System
English Spanish
Rec. Pre. F1 Rec. Pre. F1
?-WASP 75.6 91.8 82.9 80.0 92.5 85.8
UBL 78.0 93.2 84.7 75.9 93.4 83.6
UBL-s 81.8 83.5 82.6 81.4 83.4 82.4
System
Japanese Turkish
Rec. Pre. F1 Rec. Pre. F1
?-WASP 81.2 90.1 85.8 68.8 90.4 78.1
UBL 78.9 90.9 84.4 67.4 93.4 78.1
UBL-s 83.0 83.2 83.1 71.8 77.8 74.6
Table 2: Performance across languages on Geo250 with
lambda-calculus meaning representations.
However, UBL achieves much higher precision and
better overall F1 scores, which are generally compa-
rable to the best performing systems.
The comparison to the CCG induction techniques
of ZC05 and ZC07 (Table 3) is particularly striking.
These approaches used language-specific templates
to propose new lexical items and also required as in-
put a set of hand-engineered lexical entries to model
phenomena such as quantification and determiners.
However, the use of higher-order unification allows
UBL to achieve comparable performance while au-
tomatically inducing these types of entries.
For a more qualitative evaluation, Table 4 shows a
selection of lexical items learned with high weights
for the lambda-calculus meaning representations.
Nouns such as ?state? or ?estado? are consistently
learned across languages with the category S|NP ,
which stands in for the more conventional N . The
algorithm also learns language-specific construc-
tions such as the Japanese case markers ?no? and
?wa?, which are treated as modifiers that do not add
semantic content. Language-specific word order is
also encoded, using the slash directions of the CCG
System
Variable Free Lambda Calculus
Rec. Pre. F1 Rec. Pre. F1
Cross Validation Results
KRISP 71.7 93.3 81.1 ? ? ?
WASP 74.8 87.2 80.5 ? ? ?
Lu08 81.5 89.3 85.2 ? ? ?
?-WASP ? ? ? 86.6 92.0 89.2
Independent Test Set
ZC05 ? ? ? 79.3 96.3 87.0
ZC07 ? ? ? 86.1 91.6 88.8
UBL 81.4 89.4 85.2 85.0 94.1 89.3
UBL-s 84.3 85.2 84.7 87.9 88.5 88.2
Table 3: Performance on the Geo880 data set, with varied
meaning representations.
categories. For example, ?what? and ?que? take
their arguments to the right in the wh-initial English
and Spanish. However, the Turkish wh-word ?nel-
erdir? and the Japanese question marker ?nan desu
ka? are sentence final, and therefore take their argu-
ments to the left. Learning regularities of this type
allows UBL to generalize well to unseen data.
There is less variation and complexity in the
learned lexical items for the variable-free represen-
tation. The fact that the meaning representation is
deeply nested influences the form of the induced
grammar. For example, recall that the sentence
?what states border texas? would be paired with the
meaning answer(state(borders(tex))). For this
representation, lexical items such as:
what ` S/NP : ?x.answer(x)
states `NP/NP : ?x.state(x)
border `NP/NP : ?x.borders(x)
texas `NP : tex
can be used to construct the desired output. In
practice, UBL often learns entries with only a sin-
gle slash, like those above, varying only in the di-
rection, as required for the language. Even the
more complex items, such as those for quantifiers,
are consistently simpler than those induced from
the lambda-calculus meaning representations. For
example, one of the most complex entries learned
in the experiments for English is the smallest `
NP\NP/(NP |NP ):?f?x.smallest one(f(x)).
There are also differences in the aggregate statis-
tics of the learned lexicons. For example, the aver-
age length of a learned lexical item for the (lambda-
calculus, variable-free) meaning representations is:
1231
(1.21,1.08) for Turkish, (1.34,1.19) for English,
(1.43,1.25) for Spanish and (1.63,1.42) for Japanese.
For both meaning representations the model learns
significantly more multiword lexical items for the
somewhat analytic Japanese than the agglutinative
Turkish. There are also variations in the average
number of learned lexical items in the best parses
during the final pass of training: 192 for Japanese,
206 for Spanish, 188 for English and 295 for Turk-
ish. As compared to the other languages, the mor-
pologically rich Turkish requires significantly more
lexical variation to explain the data.
Finally, there are a number of cases where the
UBL algorithm could be improved in future work.
In cases where there are multiple allowable word or-
ders, the UBL algorithm must learn individual en-
tries for each possibility. For example, the following
two categories are often learned with high weight for
the Japanese word ?chiisai?:
NP/(S|NP )\(NP |NP ):?f?g.argmin(x, g(x), f(x))
NP |(S|NP )/(NP |NP ):?f?g.argmin(x, g(x), f(x))
and are treated as distinct entries in the lexicon. Sim-
ilarly, the approach presented here does not model
morphology, and must repeatedly learn the correct
categories for the Turkish words ?nehri,? ?nehir,?
?nehirler,? and ?nehirlerin?, all of which correspond
to the logical form ?x.river(x).
9 Conclusions and Future Work
This paper has presented a method for inducing
probabilistic CCGs from sentences paired with log-
ical forms. The approach uses higher-order unifi-
cation to define the space of possible grammars in
a language- and representation-independent manner,
paired with an algorithm that learns a probabilistic
parsing model. We evaluated the approach on four
languages with two meaning representations each,
achieving high accuracy across all scenarios.
For future work, we are interested in exploring
the generality of the approach while extending it to
new understanding problems. One potential limi-
tation is in the constraints we introduced to ensure
the tractability of the higher-order unification proce-
dure. These restrictions will not allow the approach
to induce lexical items that would be used with,
among other things, many of the type-raised combi-
nators commonly employed in CCG grammars. We
English
population of ` NP/NP : ?x.population(x)
smallest ` NP/(S|NP ) : ?f.arg min(y, f(y), size(y))
what ` S|NP/(S|NP ) : ?f?x.f(x)
border ` S|NP/NP : ?x?y.next to(y, x)
state ` S|NP : ?x.state(x)
most ` NP/(S|NP )\(S|NP )\(S|NP |NP ) :
?f?g?h?x.argmax(y, g(y), count(z, f(z, y) ? h(z)))
Japanese
no ` NP |NP/(NP |NP ) : ?f?x.f(x)
shuu ` S|NP : ?x.state(x)
nan desu ka ` S\NP\(NP |NP ) : ?f?x.f(x)
wa ` NP |NP\(NP |NP ) : ?f?x.f(x)
ikutsu ` NP |(S|NP )\(S|NP |(S|NP )) :
?f?g.count(x, f(g(x)))
chiiki ` NP\NP :?x.area(x)
Turkish
nedir ` S\NP\(NP |NP ) : ?f?x.f(x)
sehir ` S|NP : ?x.city(x)
nufus yogunlugu ` NP |NP : ?x.density(x)
siniri` S|NP/NP : ?x?y.next to(y, x)
kac tane ` S\NP/(S|NP |NP )\(S|NP ) :
?f?g?x.count(y, f(y) ? g(y, x))
ya siniri ` S|NP\NP : ?x?y.next to(y, x)
Spanish
en ` S|NP/NP : ?x?y.loc(y, x)
que es la ` S/NP/(NP |NP ): ?f?x.f(x)
pequena ` NP\(S|NP )\(NP |NP ) :
?g?f.arg min(y, f(y), g(y))
estado ` S|NP : ?x.state(x)
mas ` S\(S|NP )/(S|NP )\(NP |NP |(S|NP )) :
?f?g?h.argmax(x, h(x), f(g, x))
mayores `S|NP\(S|NP ) :?f?x.f(x) ?major(x)
Table 4: Example learned lexical items for each language
on the Geo250 lambda-calculus data sets.
are also interested in developing similar grammar
induction techniques for context-dependent under-
standing problems, such as the one considered by
Zettlemoyer & Collins (2009). Such an approach
would complement ideas for using high-order unifi-
cation to model a wider range of language phenom-
ena, such as VP ellipsis (Dalrymple et al, 1991).
Acknowledgements
We thank the reviewers for useful feedback. This
work was supported by the EU under IST Cog-
nitive Systems grant IP FP6-2004-IST-4-27657
?Paco-Plus? and ERC Advanced Fellowship 249520
?GRAMPLUS? to Steedman. Kwiatkowski was
supported by an EPRSC studentship. Zettlemoyer
was supported by a US NSF International Research
Fellowship.
1232
References
Bos, J., Clark, S., Steedman, M., Curran, J. R., & Hock-
enmaier, J. (2004). Wide-coverage semantic represen-
tations from a CCG parser. In Proceedings of the In-
ternational Conference on Computational Linguistics.
Buszkowski, W. & Penn, G. (1990). Categorial grammars
determined from linguistic data by unification. Studia
Logica, 49, 431?454.
Carpenter, B. (1997). Type-Logical Semantics. The MIT
Press.
Clark, S. & Curran, J. R. (2003). Log-linear models
for wide-coverage CCG parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Clark, S. & Curran, J. R. (2007). Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4), 493?552.
Dalrymple, M., Shieber, S., & Pereira, F. (1991). Ellipsis
and higher-order unification. Linguistics and Philoso-
phy, 14, 399?452.
Ge, R. & Mooney, R. J. (2006). Discriminative rerank-
ing for semantic parsing. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
Huet, G. (1975). A unification algorithm for typed ?-
calculus. Theoretical Computer Science, 1, 27?57.
Huet, G. P. (1973). The undecidability of unification in
third order logic. Information and Control, 22(3), 257?
267.
Kate, R. J. & Mooney, R. J. (2006). Using string-kernels
for learning semantic parsers. In Proceedings of the
44th Annual Meeting of the Association for Computa-
tional Linguistics.
Kate, R. J., Wong, Y. W., & Mooney, R. J. (2005). Learn-
ing to transform natural to formal languages. In Pro-
ceedings of the National Conference on Artificial In-
telligence.
LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998).
Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE, 86(11), 2278?2324.
Lu, W., Ng, H. T., Lee, W. S., & Zettlemoyer, L. S.
(2008). A generative model for parsing natural lan-
guage to meaning representations. In Proceedings of
The Conference on Empirical Methods in Natural Lan-
guage Processing.
Miller, S., Stallard, D., Bobrow, R. J., & Schwartz, R. L.
(1996). A fully statistical approach to natural language
interfaces. In Proc. of the Association for Computa-
tional Linguistics.
Och, F. J. & Ney, H. (2003). A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1), 19?51.
Steedman, M. (1996). Surface Structure and Interpreta-
tion. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The MIT
Press.
Thompson, C. A. & Mooney, R. J. (2002). Acquiring
word-meaning mappings for natural language inter-
faces. Journal of Artificial Intelligence Research, 18.
Villavicencio, A. (2002). The acquisition of a unification-
based generalised categorial grammar. Ph.D. thesis,
University of Cambridge.
Wong, Y. W. & Mooney, R. (2006). Learning for seman-
tic parsing with statistical machine translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL.
Wong, Y. W. & Mooney, R. (2007). Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the Association for Com-
putational Linguistics.
Zelle, J. M. & Mooney, R. J. (1996). Learning to parse
database queries using inductive logic programming.
In Proceedings of the National Conference on Artifi-
cial Intelligence.
Zettlemoyer, L. S. & Collins, M. (2005). Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Artificial
Intelligence.
Zettlemoyer, L. S. & Collins, M. (2007). Online learning
of relaxed CCG grammars for parsing to logical form.
In Proc. of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.
Zettlemoyer, L. S. & Collins, M. (2009). Learning
context-dependent mappings from sentences to logical
form. In Proceedings of The Joint Conference of the
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing.
1233
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 421?432,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Bootstrapping Semantic Parsers from Conversations
Yoav Artzi and Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{yoav,lsz}@cs.washington.edu
Abstract
Conversations provide rich opportunities for
interactive, continuous learning. When some-
thing goes wrong, a system can ask for clari-
fication, rewording, or otherwise redirect the
interaction to achieve its goals. In this pa-
per, we present an approach for using con-
versational interactions of this type to induce
semantic parsers. We demonstrate learning
without any explicit annotation of the mean-
ings of user utterances. Instead, we model
meaning with latent variables, and introduce
a loss function to measure how well potential
meanings match the conversation. This loss
drives the overall learning approach, which in-
duces a weighted CCG grammar that could be
used to automatically bootstrap the semantic
analysis component in a complete dialog sys-
tem. Experiments on DARPA Communica-
tor conversational logs demonstrate effective
learning, despite requiring no explicit mean-
ing annotations.
1 Introduction
Conversational interactions provide significant op-
portunities for autonomous learning. A well-defined
goal allows a system to engage in remediations when
confused, such as asking for clarification, reword-
ing, or additional explanation. The user?s response
to such requests provides a strong, if often indirect,
signal that can be used to learn to avoid the orig-
inal confusion in future conversations. In this pa-
per, we show how to use this type of conversational
feedback to learn to better recover the meaning of
user utterances, by inducing semantic parsers from
unannotated conversational logs. We believe that
this style of learning will contribute to the long term
goal of building self-improving dialog systems that
continually learn from their mistakes, with little or
no human intervention.
Many dialog systems use a semantic parsing com-
ponent to analyze user utterances (e.g., Allen et al,
2007; Litman et al, 2009; Young et al, 2010). For
example, in a flight booking system, the sentence
Sent: I want to go to Seattle on Friday
LF: ?x.to(x, SEA) ? date(x, FRI)
might be mapped to the logical form (LF) meaning
representation above, a lambda-calculus expression
defining the set of flights that match the user?s de-
sired constraints. This LF is a representation of the
semantic content that comes from the sentence, and
would be input to a context-dependent understand-
ing component in a full dialog system, for example
to find the date that the symbol FRI refers to.
To induce semantic parsers from interactions, we
consider user statements in conversational logs and
model their meaning with latent variables. We
demonstrate that it is often possible to use the dia-
log that follows a statement (including remediations
such as clarifications, simplifications, etc.) to learn
the meaning of the original sentence. For example,
consider the first user utterance in Figure 1, where
the system failed to understand the user?s request.
To complete the task, the system must use a reme-
diation strategy. Here, it takes the initiative by ask-
ing for and confirming each flight constraint in turn.
This strategy produces an unnatural conversation but
provides supervision for learning the meaning of the
421
original utterance. We can easily record representa-
tions of the meanings the system intended to convey
at each step, as seen in Figure 1, and use this indirect
supervision for learning.
Learning from this weak signal is challenging. In
any specific conversation, the system?s remediations
can fail to recover aspects of the original user mean-
ing and can introduce spurious constraints, for ex-
ample when users change their goals mid conversa-
tion. To learn effectively, the model must accumu-
late evidence from many interactions to best recover
the meaning of each specific sentence.
We will learn semantic parsers defined by prob-
abilistic Combinatory Categorial Grammars (PC-
CGs), which include both a lexicon and a weighted
linear model for parse selection. The lexicon speci-
fies the meanings of individual words and phrases,
while the parameters of a parsing model define
how to best combine word- and phrase-level mean-
ings to analyze complete sentences. To learn with-
out labeled meaning representations, we make use
of a variant of the loss-sensitive Perceptron algo-
rithm (Singh-Miller and Collins, 2007). We define
loss functions to provide a rough measure of (1) how
well a candidate meaning for a utterance matches
the conversation that follows it and (2) how well the
candidate matches our expectations about the types
of things that are often said in the dialog?s domain.
These notions of loss drive not only the parameter
estimation but also the grammar induction process
that constructs the CCG lexicon. Experiments on
conversation logs from the DARPA Communicator
corpus (Walker et al, 2002) demonstrate the feasi-
bility of our approach.
This paper makes the following contributions:
? A formalization of the problem of learning
the meaning of user statements from conversa-
tional feedback, without requiring annotation.
? A new loss-sensitive learning algorithm for this
problem that induces semantic parsers from
conversation logs.
? Loss functions to measure the quality of hypo-
thetical utterance meanings within the conver-
sation in which they appear.
? An evaluation on logs from two dialog systems
SYSTEM: how can i help you? (OPEN TASK)
USER: i would like to fly from atlanta georgia to lon-
don england on september twenty fourth in the early
evening i would like to return on october first departing
from london in the late morning
SYSTEM: leaving what city? (ASK:?x.from(fl, x))
USER: atlanta georgia
SYSTEM: leaving atlanta. (CONFIRM:from(fl, ATL))
going to which city? (ASK:?x.to(fl, x))
USER: london
SYSTEM: arriving in london england.
(CONFIRM:to(fl, LON)) what date would you like to
depart atlanta? (ASK:?x.from(fl, ATL) ? depart-
date(fl, x))
USER: september twenty fourth in the early evening
[conversation continues]
Figure 1: Conversational excerpt from a DARPA Com-
municator travel-planning dialog. Each system statement
is labeled with representations of its speech act and log-
ical meaning, in parentheses. The user utterances have
no labels. Conversations of this type provide the training
data to learn semantic parsers for user utterances.
that demonstrate effective learning from con-
versations alone.
2 Problem
Our goal is to learn a function that maps a sentence
x to a lambda-calculus expression z. We assume ac-
cess to logs of conversations with automatically gen-
erated annotation of system utterance meanings, but
no explicit labeling of each user utterance meaning.
We define a conversation C = (~U,O) to be a se-
quence of utterances ~U = [u0, . . . , um] and a set
of conversational objects O. An object o ? O
is an entity that is being discussed, for example
there would be a unique object for each flight leg
discussed in a travel planning conversation. Each
utterance ui = (s, x, a, z) represents the speaker
s ? {User, System} producing the natural lan-
guage statement x which asserts a speech act a ?
{ASK,CONFIRM, . . .} with meaning represen-
tation z. For example, from the second system ut-
terance in Figure 1 the question x =?Leaving what
city?? is an a=ASK speech act with lambda-calculus
meaning z = ?x.from(fl, x). This meaning repre-
sents the fact that the system asked for the departure
city for the conversational object o = fl represent-
ing the flight leg that is currently being discussed.
We will learn from conversations where the speech
422
acts a and logical forms z for user utterances are un-
labeled. Such data can be generated by recording
interactions, along with each system?s internal rep-
resentation of its own utterances.
Finally, since we will be analyzing sentences at a
specific point in a complete conversation, we define
our training data as a set {(ji, Ci)|i = 1 . . . n}. Each
pair is a conversation Ci and the index ji of the user
utterance x in Ci whose meaning we will attempt to
learn to recover. In general, the same conversation
C can be used in multiple examples, each with a dif-
ferent sentence index. Section 8 provides the details
of how the data was gathered for our experiments.
3 Overview of Approach
We will present an algorithm for learning a weighted
CCG parser, as defined in Section 5, that can be used
to map sentences to logical forms. The approach
induces a lexicon to represent the meanings of words
and phrases while also estimating the parameters of
a weighted linear model for selecting the best parse
given the lexicon.
Learning As defined in Section 2, the algorithm
takes a set of n training examples, {(ji, Ci) : i =
1, . . . , n}. For each example, our goal is to learn to
parse the user utterance x at position ji in Ci. The
training data contains no direct evidence about the
logical form z that should be paired with x, or the
CCG analysis that would be used to construct z. We
model all of these choices as latent variables.
To learn effectively in this complex, latent space,
we introduce a loss function L(z, j, C) ? R that
measures how well a logical form z models the
meaning for the user utterance at position j in C. In
Section 6, we will present the details of the loss we
use, which is designed to be sensitive to remedia-
tions in C (system requests for clarification, etc.) but
also be robust to the fact that conversations often do
not uniquely determine which z should be selected,
for example when the user prematurely ends the dis-
cussion. Then, in Section 7, we present an approach
for incorporating this loss function into a complete
algorithm that induces a CCG lexicon and estimates
the parameters of the parsing model.
This learning setup focuses on a subproblem in
dialog; semantic interpretation. We do not yet learn
to recover user speech acts or integrate the logical
form into the context of the conversation. These are
important areas for future work.
Evaluation We will evaluate performance on a
test set {(xi, zi)|i = 1, . . . ,m} of m sentences xi
that have been explicitly labeled with logical forms
zi. This data will allow us to directly evaluate the
quality of the learned model. Each sentence is an-
alyzed with the learned model alone; the loss func-
tion and any conversational context are not used dur-
ing evaluation. Parsers that perform well in this set-
ting will be strong candidates for inclusion in a more
complete dialog system, as motivated in Section 1.
4 Related Work
Most previous work on learning from conversational
interactions has focused on the dialog sub-problems
of response planning (e.g., Levin et al, 2000; Singh
et al, 2002) and natural language generation (e.g.,
Lemon, 2011). We are not aware of previous work
on inducing semantic parsers from conversations.
There has been significant work on supervised
learning for inducing semantic parsers. Various
techniques were applied to the problem includ-
ing machine translation (Papineni et al, 1997;
Ramaswamy and Kleindienst, 2000; Wong and
Mooney, 2006; 2007; Matuszek et al, 2010), higher-
order unification (Kwiatkowski et al, 2010), parsing
(Ruifang and Mooney, 2006; Lu et al, 2008), induc-
tive logic programming (Zelle and Mooney, 1996;
Thompson and Mooney, 2003; Tang and Mooney,
2000), probabilistic push-down automata (He and
Young, 2005; 2006) and ideas from support vec-
tor machines and string kernels (Kate and Mooney,
2006; Nguyen et al, 2006). The algorithms we de-
velop in this paper build on previous work on su-
pervised learning of CCG parsers (Zettlemoyer and
Collins, 2005; 2007), as we describe in Section 5.3.
There is also work on learning to do semantic
analysis with alternate forms of supervision. Clarke
et al (2010) and Liang et al (2011) describe ap-
proaches for learning semantic parsers from ques-
tions paired with database answers, while Gold-
wasser et al (2011) presents work on unsuper-
vised learning. Our approach provides an alterna-
tive method of supervision that could complement
these approaches. Additionally, there has been sig-
nificant recent work on learning to do other, re-
423
I want to go from Boston to New York and then to Chicago
S/N (N\N)/NP NP (N\N)/NP NP CONJ[] (N\N)/NP NP
?f.f ?y.?f.?x.f(x) ? from(x, y) BOS ?y.?f.?x.f(x) ? to(x, y) NYC ?y.?f.?x.f(x) ? to(x, y) CHI
> > >
(N\N) (N\N) (N\N)
?f.?x.f(x) ? from(x,BOS) ?f.?x.f(x) ? to(x,NY C) ?f.?x.f(x) ? to(x,CHI)
<B
(N\N)
?f.?x.f(x) ? from(x,BOS) ? to(x,NY C)
<?>
(N\N)
?f.?x[].f(x) ? from(x[1], BOS) ? to(x[1], NY C) ? before(x[1], x[2]) ? to(x[2], CHI)
N
?x[].from(x[1], BOS) ? to(x[1], NY C) ? before(x[1], x[2]) ? to(x[2], CHI)
>
S
?x[].from(x[1], BOS) ? to(x[1], NY C) ? before(x[1], x[2]) ? to(x[2], CHI)
Figure 2: An example CCG parse. This parse shows the construction of a logical form with an array-typed variable x[]
that specifies a list of flight legs, indexed by x[1] and x[2]. The top-most parse steps introduce lexical items while the
lower ones create new nonterminals according the CCG combinators (>, <, etc.), see Steedman (2000) for details.
lated, natural language semantic analysis tasks from
context-dependent database queries (Miller et al,
1996; Zettlemoyer and Collins, 2009), grounded
event streams (Chen et al, 2010; Liang et al, 2009),
environment interactions (Branavan et al, 2009;
2010; Vogel and Jurafsky, 2010), and even unanno-
tated text (Poon and Domingos, 2009; 2010).
Finally, the DARPA Communicator data (Walker
et al, 2002) has been previously studied. Walker and
Passonneau (2001) introduced a schema of speech
acts for evaluation of the DARPA Communicator
system performance. Georgila et al (2009) extended
this annotation schema to user utterances using an
automatic process. Our speech acts extend this work
to additionally include full meaning representations.
5 Mapping Sentences to Logical Form
We will use a weighted linear CCG grammar for se-
mantic parsing, as briefly reviewed in this section.
5.1 Combinatory Categorial Grammars
Combinatory categorial grammars (CCGs) are a
linguistically-motivated model for a wide range of
language phenomena (Steedman, 1996; 2000). A
CCG is defined by a lexicon and a set of combina-
tors. The grammar defines a set of possible parse
trees, where each tree includes syntactic and seman-
tic information that can be used to construct logical
forms for sentences.
The lexicon contains entries that define categories
for words or phrases. For example, the second
lexical entry in the parse in Figure 2 is:
from := (N\N)/NP : ?y.?f.?x.f(x) ? from(x, y)
Each category includes both syntactic and seman-
tic information. For example, the phrase ?from?
is assigned the category with syntax (N\N)/NP
and semantics ?y.?f.?x.f(x) ? from(x, y). The
outermost syntactic forward slash specifies that the
entry must first be combined with an NP to the
right (the departure city), while the inner back slash
specifies that it will later modify a noun N to the
left (to add a constraint to a set of flights). The
lambda-calculus semantic expression is designed
to build the appropriate meaning representation at
each of these steps, as seen in the parse in Figure 2.
In general, we make use of typed lambda cal-
culus to represent meaning (Carpenter, 1997), both
in the lexicon and in intermediate parse tree nodes.
We also introduce an extension for modeling array-
typed variables to represent lists of individual en-
tries. These constructions are used, for example, to
model sentences describing a sequence of segments
while specifying flight preferences.
Figure 2 shows how a CCG parse builds a logical
form for a complete sentence with an array-typed
variable. Each intermediate node in the tree is con-
structed with one of a small set of CCG combina-
tor rules, see the explanation from Steedman (1996;
2000). We make use of the standard application,
composition and coordination combinators, as well
as type-shifting rules introduced by Zettlemoyer and
Collins (2007) to model spontaneous, unedited text.
5.2 Weighted Linear CCGs
A weighted linear CCG (Clark and Curran, 2007)
provides a ranking on the space of possible parses
under the grammar, which can be used to select
the best logical form for a sentence. This type of
model is closely related to several other approaches
(Ratnaparkhi et al, 1994; Johnson et al, 1999;
424
Lafferty et al, 2001; Collins, 2004; Taskar et al,
2004). Let x be a sentence, y be a CCG parse, and
GEN(x; ?) be the set of all possible CCG parses for
x given the lexicon ?. Define ?(x, y) ? Rd to be
a d-dimensional feature?vector representation and
? ? Rd to be a parameter vector. The optimal parse
for sentence x is
y?(x) = arg max
y?GEN(x;?) ? ? ?(x, y)
and the final output logical form z is the lambda-
calculus expression at the root of y?(x).
We compute y?(x) with a CKY-style chart pars-
ing algorithm. Since each chart entry contains a
full lambda-calculus meaning expression, we use
N -best pruning to control the number of options we
consider at each span. Learning a model of this form
involves learning the parameters ? and the lexicon
?. We will show that this is possible from conversa-
tional logs that do not contain any explicit labeling
of the logical forms for user utterances.
5.3 Supervised learning with GENLEX
Previous work on lexical induction, including the
GENLEX approach which we briefly review here,
has required labeled logical meaning representa-
tions. In Section 7, we will introduce a new way
of using GENLEX to learn from unannotated con-
versation logs.
The supervised CCG learning algorithms of
Zettlemoyer and Collins (2005; 2007) induce a
weighted CCG from training examples (xi, zi) for
i = 1 . . . n, where xi is a sentence and zi is the
corresponding lambda-calculus meaning representa-
tion. The output from the algorithm is a pair (?,?)
containing the learned parameters and CCG lexicon.
They defined the function GENLEX(x, z) to map
a sentence x and its meaning z to a large set of poten-
tial lexical entries. These entries are generated with
rules that consider the logical form z and guess po-
tential CCG categories. For example, the rule p ?
(N\N)/NP : ?y.?f.?x.f(x) ? p(x, y) introduces
categories commonly used to model certain types of
prepositions. This rule would, for example, intro-
duce the category (N\N)/NP : ?y.?f.?x.f(x) ?
to(x, y) for any logical form z that contains the con-
stant to. GENLEX uses a small set of rules of this
type to generate categories that are paired with all
Example Conversation:
(1) SYSTEM: how can i help you? (OPEN TASK)
(2) USER: i would like to fly from boston arriving to new york
and back from new york to boston
(3) SYSTEM: leaving boston (CONFIRM:from(fl1, BOS))
on what date? (ASK:?x.departdate(fl1, x))
(4) USER: september twenty four
(5) SYSTEM: leaving boston (CONFIRM:from(fl1, BOS))
on september twenty four (CONFIRM:depart-
date(fl1, sept 24)) to new york
(CONFIRM:to(fl1, NY C)) is new york your final
destination? (ASK:??x.fr(x,NY C))
(6) USER: yes i want to return to boston
(7) SYSTEM: sorry, i didn?t understand. (APOL-
OGY) where are you flying to from new york?
(ASK:?x.fr(fl2, NY C) ? to(fl2, x))
[conversation ends]
Candidate Logical Expressions for Utterance #2:
(a) ?x.to(x,BOS) ? from(x,NY C)
(b) ?x.from(x,BOS) ? to(x,NY C)
(c) ?x.to(x,BOS) ? to(x,NY C)
(d) ?x[].from(x[1], BOS) ? to(x[1], NY C)
? before(x[1], x[2]) ? return(x[2])
? from(x[2], NY C) ? to(x[2], BOS))
(e) ?x[].from(x[1], BOS) ? to(x[1], NY C)
? before(x[1], x[2]) ? return(x[2])
? from(x[2], BOS) ? to(x[2], NY C)
Figure 3: Conversation reflecting an interaction as seen
in the DARPA Communicator travel-planning dialogs.
possible substrings in x to form an overly general
lexicon. The complete learning algorithm then si-
multaneously selects a small subset of all entries
generated by GENLEX and estimates parameter val-
ues ?. Zettlemoyer and Collins (2005) present a
more detailed explanation.
6 Measuring Loss
In Section 7, we will present a loss-sensitive learn-
ing algorithm that models the meaning of user utter-
ances as latent variables to be estimated from con-
versational interactions.
We first introduce a loss function to measure the
quality of potential meaning representations. This
loss function L(z, j, C) ? R indicates how well a
logical expression z represents the meaning of the
j-th user utterance in conversation C. For example,
425
consider the first user utterance (j = 2) in Figure 3,
which is a request for a return trip from Boston to
New York. We would like to assign the lowest loss
to the meaning representation (d) in Figure 3 that
correctly encodes all of the stated constraints.
We make use of a loss function with two parts:
L(z, j, C) = Lc(z, j, C) + Ld(z). The conversa-
tion loss Lc (defined in Section 6.1) measures how
well the candidate meaning representation fits the
conversation, for example incorporating informa-
tion recovered through conversational remediations
as motivated in Section 1. The domain loss Ld (de-
scribed in Section 6.2) measures how well a logi-
cal form z matches domain expectations, such as
the fact that flights can only have a single origin.
These functions guide the types of meaning repre-
sentations we expect to see, but in many cases will
fail to specify a unique best option, for example
in conversations where the user prematurely termi-
nates the interaction. In Section 7, we will present a
complete, loss-driven learning algorithm that is ro-
bust to these types of ambiguities while inducing a
weighted CCG parser from conversations.
6.1 Conversation Loss
We will use a conversation loss function Lc(z, j, C)
that provides a rough indication of how well the log-
ical expression z represents a potential meaning for
the user utterance at position j in C. For example,
the first user utterance (j = 2) in Figure 3 is a re-
quest for a return trip from Boston to New York
where the user has explicitly mentioned both legs.
The figure also shows five options (a-e) for the logi-
cal form z. We want to assign the lowest loss to op-
tion (d), which includes all of the stated constraints.
The loss is computed in four steps for a user ut-
terance x at position j by (1) selecting a subset of
system utterances in the conversation C, (2) extract-
ing and computing loss for semantic content from
selected system utterances, (3) aligning the subex-
pressions in z to the extracted semantic content, and
(4) computing the minimal loss value from the best
alignment. In Figure 3, the loss for the candidate
logical forms is computed by considering the seg-
ment of system utterances up until the conversation
end. Within this segment, the matching for expres-
sion (d) involves mapping the origin and departure
constraints for the first leg (Boston - New York) onto
the earlier system confirmations while also align-
ing the ones for the second leg to system utterances
later in the selected portion of the conversation. Fi-
nally, the overall score depends on the quality of the
alignment, for example how many of the constraints
match to confirmations. This section presents the
full approach.
Segmentation For a user utterance at position j,
we select all system utterances from j ? 1 until the
system believes it has completed the current subtask,
as indicated by a reset action or final offer. We call
this selected segment C?. In Figure 3, C? ends with a
reset, but in a successful interaction it would have
ended with the offer of a specific flight.
Extracting Properties A property is a predicate-
entity-value triplet, where the entity can be a vari-
able from z or a conversational object. For example,
?from, fl, BOS? is a property where fl is a ob-
ject from C? and ?from, x,BOS? is a property from
z = ?x.from(x,BOS). We define PC? to be the
set of properties from logical forms for system ut-
terances in C?. Similarly, we define Pz to be the set
of properties in z.
Scoring System Properties For each system
property p ? PC? we compute its position value
pos(p), which is a normalized weighted average
over all the positions where it appears in a logi-
cal form. For each mention the weight is obtained
from its speech act. For example, properties that are
explicitly confirmed contribute more to the average
than those that were merely offered to the user in a
select statement.
We use pos(p) to compute a loss loss(p) for
each property p ? PC? . We first define P eC? to be allproperties in PC? with entity e. For entity e and po-
sition d, we define the entity-normalization function:
ne(d) =
d?minp?P eC? pos(p)
maxp?P eC? pos(p)?minp?P eC? pos(p)
.
For a given property p ? PC? with an entity e we
compute the loss value:
loss(p) = n?1e (1? ne(pos(p)))? 1 .
Where n?1e is the inverse of ne. This loss value is de-
signed to, first, provide less loss for later properties
so that it, for example, favors the last property in a
series of statements that finally resolves a confusion
426
in the conversation. Second, the loss value is lower
for objects mentioned closer to the user utterance x,
thereby preferring objects discussed sooner.
Matching Properties An alignment A maps vari-
ables in z to conversational objects in C?, for exam-
ple the flight legs fl1 and fl2 being discussed in
Figure 3. We will use alignments to match prop-
erties of z and C?. To do this we extend the align-
ment function A to apply to properties, for example
A(?from, x,BOS?) = ?from,A(x), BOS?.
Scoring Alignments Finally, we compute the
conversation loss Lc(z, j, C) as follows:
Lc(z, j, C) = minA
?
pu?Pz
?
ps?PC?
s(A(pu), ps) .
The function s(A(pu), ps) ? R computes the com-
patibility of the two input properties. It is zero if
A(pu) 6= ps. Otherwise, it returns loss(ps).
We approximate the min computation in Lc over
alignments A as follows. For a logical form z at
position j, we align the outer-most variable to the
conversational object in C? that is being discussed at
j. The remaining variables are aligned greedily to
minimize the loss, by selecting a single conversa-
tional object for each in turn.
Finally, for each aligned variable, we increase the
loss by one for each unmatched property from Pz .
This increases the loss of logical forms that include
spurious information. However, since a conversation
might stop prematurely and therefore won?t discuss
the entire user request, we only increase the loss for
variables that are already aligned. For this purpose,
we define an aligned variable to be one that has at
least one property matched successfully.
6.2 Domain Loss
We also make use of a domain loss functionLd(z) ?
R. The function takes a logical form z and returns
the number of violations there are in z to a set of
constraints on logical forms that occur commonly in
the dialog domain. For example, in a travel domain,
a violation might occur if a flight leg has two differ-
ent destination cities. The set of possible violations
must be specified for each dialog system, but can of-
ten be compiled from existing resources, such as a
database of valid flight ticketing options.
In our experiments, we will use a set of eight
simple constraints to check for violations in flight
Inputs: Training set {(ji, Ci) : i = 1 . . . n} where each exam-
ple includes the index ji of a sentence xi in the conversation
Ci. Initial lexicon ?0. Number of iterations T . Margin ?.
Beam size k for lexicon generation. Loss function L(x, j, C),
as described in Section 6.
Definitions: GENLEX(x, C) takes as input a sentence and a
conversation and returns a set of lexical items as described in
Section 7. GEN(x; ?) is the set of all possible CCG parses
for x given the lexicon ?. LF (y) returns the logical form
z at the root of the parse tree y. Let ?i(y) be shorthand for
the feature function ?(xi, y) defined in Section 5. Define
LEX(y) to be the set of lexical entries used in parse y. Fi-
nally, let MINLi(Y ) be {y|?y? ? Y,L(LF (y), ji, Ci) ?
L(LF (y?), ji, Ci)}, the set of minimal loss parses in Y .
Algorithm:
? = 0? , ? = ?0
For t = 1 . . . T, i = 1 . . . n :
Step 1: (Lexical generation)
a. Set ? = ? ?GENLEX(xi, Ci)
b. Let Y be the k highest scoring parses of xi using ?
c. Select new lexical entries from the lowest loss parses
?i =
?
y?MINLi(Y ){l|l ? LEX(y)}d. Set lexicon to ? = ? ? ?i
Step 2: (Update parameters)
a. Define Gi = MINLi(GEN(xi,?, ?)) and
Lmin to be the minimal loss
b. Set Bi = GEN(xi,?, ?)?Gi
c. Set the relative loss function: ?i(y) = L(y, Ci)?Lmin
d. Construct sets of margin violating good and bad parses:
Ri = {r|r ? Gi ?
?y? ? Bi s.t. ??,?i(r)? ?i(y?)? < ??i(r)}
Ei = {e|e ? Bi ?
?y? ? Gi s.t. ??,?i(y?)? ?i(e)? < ??i(e)}
e. Apply the additive update:
? = ? +
?
r?Ri
1
|Ri|?i(r)?
?
e?Ei
1
|Ei|?i(e)
Output: Parameters ? and lexicon ?
Figure 4: The learning algorithm.
itineraries, which can have multiple legs. These
include, for example, checking that the legs have
unique origins and destinations that match across the
entire itinerary. For example, in Figure 3 the logical
forms (a), (b) and (d) will have no violations; they
describe valid flights. Example (c) has a single vio-
lation: a flight has two origins. Example (e) violates
a more complex constraint: the second flight?s origin
is different from the first flight?s destination.
7 Learning
Figure 4 presents the complete learning algorithm.
We assume access to training examples, {(ji, Ci) :
i = 1, . . . , n}, where each example includes the in-
427
dex ji of a sentence xi in the conversation Ci. The al-
gorithm learns a weighted CCG parser, described in
Section 5, including both a lexicon ? and parameters
?. The approach is online, considering each example
in turn and performing two steps: (1) expanding the
lexicon and (2) updating the parameters.
Step 1: Lexical Induction We introduce new lex-
ical items by selecting candidates from the function
GENLEX , following previous work (Zettlemoyer
and Collins, 2005; 2007) as reviewed in Section 5.3.
However, we face the new challenge that there is
no labeled logical-form meaning z. Instead, let ZC?
be set of all logical forms that appear in system
utterances in the relevant conversation segment C?.
We will now define the conversational lexicon set:
GENLEX(x, C?) =
?
z?ZC?
GENLEX(x, z)
where we use logical forms from system utterances
to guess possible CCG categories for analyzing the
user utterance. This approach will overgeneralize,
when the system talks about things that are unrelated
to what the user said, and will also often be incom-
plete, for example when the system does not repeat
parts of the original content. However, it provides a
way of guessing lexical items that can be combined
with previously learned ones, which can fill in any
gaps and help select the best analysis.
Step 1(a) in Figure 4 uses GENLEX to tem-
porarily create a large set of potential categories
based on the conversation. Steps (b-d) select a small
subset of these entries to add to the current lexicon
?: we find the k-best parses under the model, re-
rank them according to loss, find the lexical items
used in the best trees, and add them to ?. This
approach favors lexical items that are used in high-
scoring but low-loss analyses, as computed given the
current model.
Step 2: Parameter Updates Given the loss func-
tion L(x, i, C), we use a variant of a loss-sensitive
perceptron to update the parameters (Singh-Miller
and Collins, 2007). In Steps (a-c), for the current
example i, we compute the relative loss function ?i
that scales with the loss achieved by the best and
worst possible parses under the model. In contrast
to previous work, we do not only compute the loss
over a fixed n-best list of possible outputs, but in-
stead use the current model score to recompute the
options at each update. Then, Steps (d-e) find the set
Ri of least loss analyses and Ei of higher-loss can-
didates whose models scores are not separated by at
least ??i, where ? is a margin scale constant. The
final update (Step f) is additive and increases the pa-
rameters for features indicative of the analyses with
less loss while down weighting those for parses that
were not sufficiently separated.
Discussion This algorithm uses the conversation
to drive learning in two ways: it guides the lexi-
cal items that are proposed while also providing the
conversational feedback that defines the loss used to
update the parameters. The resulting approach is,
at every step, using information about how the con-
versation progressed after a user utterance to recon-
struct the meaning of the original statement.
8 Data Sets
For evaluation, we used conversation logs from the
Lucent and BBN dialog systems in the DARPA
Communicator corpus (Walker et al, 2002). We se-
lected these systems since they provide significant
opportunities for learning. They asked relatively
open ended questions, allowing for more complex
user responses, while also using a number of sim-
ple remediating strategies to recover from misun-
derstandings. The original conversational logs in-
cluded unannotated transcripts of system and user
utterances. Inspired by the speech act labeling ap-
proach of Walker and Passonneau (2001), we wrote
a set of scripts to label the speech acts and logical
forms for system statements. This could be done
with high accuracy since the original text was gener-
ated with templates. These labels represent what the
system explicitly said and do not require complex,
potentially error-prone annotation of the full state of
the original dialog system. The set of speech acts in-
cludes confirmations, information requests, selects,
offers, instructions, and a miscellaneous category.
The data sets include a total of 376 conversations,
divided into training and testing sets. Table 1 pro-
vides details about the training and testing sets, as
well as general data set statistics. We developed our
system using 4-fold cross validation on the training
sets. Although there are approximately 12,000 user
428
Lucent BBN
# Conversations 214 162
Total # of utterances 11,974 12,579
Avg. utterances per conversation 55.95 77.65
Avg. tokens per user utterance 3.24 2.39
Total # of training utterances 208 67
Total # of testing utterances 96 67
Avg. tokens per selected utterance 11.72 9.53
Table 1: Data set statistics for Lucent and BBN systems.
utterances in the data sets, the vast majority are sim-
ple, short phrases (such as ?yes? or ?no?) which are
not useful for learning a semantic parser. We se-
lect user utterances with a small set of heuristics, in-
cluding a threshold (6 for Lucent, 4 for BBN) on the
number of words and requiring that at least one noun
phrase is present from our initial lexicon. This ap-
proach was manually developed to perform well on
the training sets, but is not perfect and does intro-
duce a small amount of noise into the data.
9 Experimental Setup
This section describes our experimental setup and
comparisons. We follow the setup of Zettlemoyer
and Collins (2007) where possible, including fea-
ture design, initialization of the semantic parser, and
evaluation metrics, as reviewed below.
Features and Parser The features include indica-
tors for lexical item use, properties of the logical
form that is being constructed, and indicators for
parsing operators used to build the tree. The parser
attempts to boost recall with a two-pass strategy that
allows for word skipping if the initial parse fails.
Initialization and Parameters We use an initial
lexicon that includes a list of domain-specific noun
phrases, such as city and airport names, and a list
of domain-independent categories for closed-class
words such as ?the? and ?and?. We also used a time
and number parser to expand this lexicon for each
input sentence with the BIU Number Normalizer.1
The learning parameters were tuned using the devel-
opment sets: the margin constant ? is set to 0.5, we
use 6 iterations and take the top 30 parses for lexical
generation (step 1, figure 4). The parser used for pa-
rameter update (step 2, figure 4) has a beam of 250.
The parameter vector is initialized to 0?.
1http://www.cs.biu.ac.il/?nlp/downloads/
Evaluation Metrics For evaluation, we measure
performance against gold standard labels. We report
both the number of exact matches, fully correct log-
ical forms, and a partial-credit number. We measure
partial-credit accuracy by mapping logical forms to
attribute-value pairs (for example, the expression
from(x, LA) will be mapped to from = LA) and
report precision and recall on attribute sets. This
more lenient measure does not test the overall struc-
ture of the logical expression, only its components.
Systems We compare performance with the fol-
lowing systems:
Full Supervision: We measured how a fully super-
vised approach would perform on our data by hand-
labeling the training data and using a 0-1 loss func-
tion that tests if the output logical form matches the
labeled one. For lexicon generation, the labels were
used instead of the conversation.
No Conversation Baseline: We also report results
for a no conversation baseline. This baseline sys-
tem is constructed by making two modifications to
the full approach. We remove the conversation loss
function and apply the GENLEX templates to every
possible logical constant, instead of only those in the
conversation. This baseline allows us to measure the
importance of having access to the conversations by
completely ignoring the context for each sentence.
Ablations: In addition to the baseline above, we
also do ablation tests by turning off various individ-
ual components of the complete algorithm.
10 Results
Table 2 shows exact match results for the develop-
ment sets, including different system configurations.
We report mean results across four folds. To ver-
ify their contributions, we include results where we
ablate the conversational loss and domain loss func-
tions. Both are essential.
The test results are listed in Table 3. The full
method significantly outperforms the baseline, indi-
cating that we are making effective use of the con-
versational feedback, although we do not yet match
the fully supervised result. The poor baseline per-
formance is not surprising, given the difficulty of the
task and lack of guidance when the conversations are
removed. The partial-credit numbers also demon-
strate an empirical trend that we observed; in many
429
Exact Match Metric Lucent BBNPrec. Rec. F1 Prec. Rec. F1
Without conversational loss 0.35 0.34 0.35 0.66 0.54 0.59
Without domain loss 0.42 0.42 0.42 0.69 0.56 0.61
Our Approach 0.63 0.61 0.62 0.77 0.64 0.69
Supervised method 0.76 0.75 0.75 0.81 0.67 0.73
Table 2: Mean exact-match results for cross fold evaluation on the development sets.
Exact Match Metric Lucent BBNPrec. Rec. F1 Prec. Rec. F1
No Conversations Baseline 0 0 0 0.16 0.15 0.15
Our Approach 0.58 0.55 0.56 0.85 0.75 0.79
Supervised method 0.7 0.68 0.69 0.87 0.78 0.82
Partial Credit Metric Lucent BBNPrec. Rec. F1 Prec. Rec. F1
No Conversations Baseline 0.26 0.35 0.29 0.26 0.33 0.29
Our Approach 0.68 0.63 0.65 0.97 0.57 0.72
Supervised method 0.75 0.68 0.72 0.96 0.68 0.79
Table 3: Exact- and partial-match results on the test sets.
cases where we do not produce the correct logical
form, the output is often close to correct, with only
one or two missed flight constraints.
The difference between the two systems is evi-
dent. The BBN system presents a simpler approach
to the dialog problem by creating a more constrained
conversation. This is done by handling one flight
at a time, in the case of flight planing, and pos-
ing simple and close ended questions to the user.
Such an approach encourages the user to make sim-
pler requests, with relatively few constraints in each
request. In contrast, the Lucent system presents a
less-constrained approach: interactions start with an
open ended prompt and the conversations flow in a
more natural, less constrained fashion. BBN?s sim-
plified approach makes it easier for learning, giving
us superior performance when compared to the Lu-
cent system, despite the smaller training set. This is
true for both our approach and supervised learning.
We compared the logical forms recovered by the
best conversational model to the labeled ones in the
training set. Many of the errors came from cases
where the dialog system never fully recovered from
confusions in the conversation. For example, the Lu-
cent system almost never understood user utterances
that specified flight arrival times. Since it was unable
to consistently recover and introduce this constraint,
the user would often just recalculate and specify a
departure time that would achieve the original goal.
This type of failure provides no signal for our learn-
ing algorithm, whereas the fully supervised algo-
rithm would use labeled logical forms to resolve the
confusion. Interestingly, the test set had more sen-
tences that suffered such failures than the develop-
ment set, which contributed to the performance gap.
11 Discussion
We presented a loss-driven learning approach that
induces the lexicon and parameters of a CCG parser
for mapping sentences to logical forms. The loss
was defined over the conversational context, without
requiring annotation of user utterances meaning.
The overall approach assumes that, in aggregate,
the conversations contain sufficient signal (remedia-
tions such as clarification, etc.) to learn effectively.
In this paper, we satisfied this requirement by us-
ing logs from automated systems that deployed rea-
sonably effective recovery strategies. An important
area for future work is to consider how this learning
can be best integrated into a complete dialog system.
This would include designing remediation strategies
that allow for the most effective learning and consid-
ering how similar techniques could be used simulta-
neously for other dialog subproblems.
Acknowledgments
The research was supported by funding from the
DARPA Computer Science Study Group. Thanks
to Dan Weld, Raphael Hoffmann, Jonathan Berant,
Hoifung Poon and Mark Yatskar for their sugges-
tions and comments. We also thank Shachar Mirkin
for providing access to the BIU Normalizer.
430
References
Allen, J., M. Manshadi, M. Dzikovska, and M. Swift.
2007. Deep linguistic processing for spoken dialogue
systems. In Proceedings of the Workshop on Deep Lin-
guistic Processing.
Branavan, SRK, H. Chen, L.S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint Con-
ference of the Association for Computational Linguis-
tics and the International Joint Conference on Natural
Language Processing.
Branavan, SRK, L.S. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: learning to map high-level
instructions to commands. In Proceedings of the Asso-
ciation for Computational Linguistics.
Carpenter, B. 1997. Type-Logical Semantics. The MIT
Press.
Chen, D.L., J. Kim, and R.J. Mooney. 2010. Training a
multilingual sportscaster: using perceptual context to
learn language. Journal of Artificial Intelligence Re-
search 37(1):397?436.
Clark, S. and J.R. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models.
Computational Linguistics 33(4):493?552.
Clarke, J., D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In Proceedings of the Conference on Computational
Natural Language Learning.
Collins, M. 2004. Parameter estimation for statistical
parsing models: Theory and practice of distribution-
free methods. In New Developments in Parsing Tech-
nology.
Georgila, K., O. Lemon, J. Henderson, and J.D. Moore.
2009. Automatic annotation of context and speech acts
for dialogue corpora. Natural Language Engineering
15(03):315?353.
Goldwasser, D., R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic pars-
ing. In Proceedings. of the Association of Computa-
tional Linguistics.
He, Y. and S. Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech and
Language 19:85?106.
He, Y. and S. Young. 2006. Spoken language understand-
ing using the hidden vector state model. Speech Com-
munication 48(3-4).
Johnson, M., S. Geman, S. Canon, Z. Chi, and S. Riezler.
1999. Estimators for stochastic ?unification-based?
grammars. In Proceedings of the Association for Com-
putational Linguistics.
Kate, R.J. and R.J. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proceedings of the
Association for Computational Linguistics.
Kwiatkowski, T., L.S. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic ccg gram-
mars from logical form with higher-order unification.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning.
Lemon, O. 2011. Learning what to say and how to say
it: Joint optimisation of spoken dialogue management
and natural language generation. Computer Speech &
Language 25(2):210?221.
Levin, E., R. Pieraccini, and W. Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. IEEE Transactions on Speech and
Audio Processing 8(1):11?23.
Liang, P., M.I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In Pro-
ceedings of the Joint Conference of the Association
for Computational Linguistics the International Joint
Conference on Natural Language Processing.
Liang, P., M.I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Pro-
ceedings of the Association for Computational Lin-
guistics.
Litman, D., J. Moore, M.O. Dzikovska, and E. Farrow.
2009. Using Natural Language Processing to Analyze
Tutorial Dialogue Corpora Across Domains Modali-
ties. In Proceeding of the Conference on Artificial In-
telligence in Education.
Lu, W., H.T. Ng, W.S. Lee, and L.S. Zettlemoyer. 2008.
A generative model for parsing natural language to
meaning representations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Matuszek, C., D. Fox, and K. Koscher. 2010. Follow-
ing directions using statistical machine translation. In
Proceeding of the international conference on Human-
robot interaction.
Miller, S., D. Stallard, R.J. Bobrow, and R.L. Schwartz.
1996. A fully statistical approach to natural language
interfaces. In Proceedings of the Association for Com-
putational Linguistics.
Nguyen, L., A. Shimazu, and X. Phan. 2006. Seman-
tic parsing with structured SVM ensemble classifica-
tion models. In Proceedings of the joint conference
431
of the International Committee on Computational Lin-
guistics and the Association for Computational Lin-
guistics.
Papineni, K.A., S. Roukos, and T.R. Ward. 1997. Feature-
based language understanding. In Proceedings of the
European Conference on Speech Communication and
Technology.
Poon, H. and P. Domingos. 2009. Unsupervised semantic
parsing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Poon, H. and P. Domingos. 2010. Unsupervised ontology
induction from text. In Proceedings of the Association
for Computational Linguistics.
Ramaswamy, G.N. and J. Kleindienst. 2000. Hierarchi-
cal feature-based translation for scalable natural lan-
guage understanding. In Proceedings of the Interna-
tional Conference on Spoken Language Processing.
Ratnaparkhi, A., S. Roukos, and R.T. Ward. 1994. A
maximum entropy model for parsing. In Proceedings
of the International Conference on Spoken Language
Processing.
Ruifang, G. and R.J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Porceedings of the
Association for Computational Linguistics.
Singh, S.P., D.J. Litman, M.J. Kearns, and M.A. Walker.
2002. Optimizing dialogue management with re-
inforcement learning: Experiments with the NJFun
system. Journal of Artificial Intelligence Research
16(1):105?133.
Singh-Miller, N. and M. Collins. 2007. Trigger-based
language modeling using a loss-sensitive perceptron
algorithm. In IEEE International Conference on
Acoustics, Speech and Signal Processing.
Steedman, M. 1996. Surface Structure and Interpreta-
tion. The MIT Press.
Steedman, M. 2000. The Syntactic Process. The MIT
Press.
Tang, L.R. and R.J. Mooney. 2000. Automated construc-
tion of database interfaces: Integrating statistical and
relational learning for semantic parsing. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora.
Taskar, B., D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Thompson, C.A. and R.J. Mooney. 2003. Acquiring
word-meaning mappings for natural language inter-
faces. Journal of Artificial Intelligence Research 18:1?
44.
Vogel, A. and D. Jurafsky. 2010. Learning to follow nav-
igational directions. In Proceedings of the Association
for Computational Linguistics.
Walker, M. and R. Passonneau. 2001. DATE: a dia-
logue act tagging scheme for evaluation of spoken di-
alogue systems. In Proceedings of the First Inter-
national Conference on Human Language Technology
Research.
Walker, M., A. Rudnicky, R. Prasad, J. Aberdeen,
E. Bratt, J. Garofolo, H. Hastie, A. Le, B. Pellom,
A. Potamianos, et al 2002. DARPA Communicator:
Cross-system results for the 2001 evaluation. In Pro-
ceedings of the International Conference on Spoken
Language Processing.
Wong, Y.W. and R.J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference of the North American Association for Compu-
tational Linguistics.
Wong, Y.W. and R.J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the Association for Com-
putational Linguistics.
Young, S., M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hidden
information state model: A practical framework for
POMDP-based spoken dialogue management. Com-
puter Speech & Language 24(2):150?174.
Zelle, J.M. and R.J. Mooney. 1996. Learning to parse
database queries using inductive logic programming.
In Proceedings of the National Conference on Artifi-
cial Intelligence.
Zettlemoyer, L.S. and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Artificial
Intelligence.
Zettlemoyer, L.S. and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning.
Zettlemoyer, L.S. and Michael Collins. 2009. Learning
context-dependent mappings from sentences to logical
form. In Proceedings of the Joint Conference of the
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing.
432
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1512?1523,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Lexical Generalization in CCG Grammar Induction for Semantic Parsing
Tom Kwiatkowski?
t.m.kwiatkowksi@sms.ed.ac.uk
Luke Zettlemoyer?
lsz@cs.washington.edu
Sharon Goldwater?
sgwater@inf.ed.ac.uk
Mark Steedman?
steedman@inf.ed.ac.uk
?School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Computer Science & Engineering
University of Washington
Seattle, WA 98195
Abstract
We consider the problem of learning fac-
tored probabilistic CCG grammars for seman-
tic parsing from data containing sentences
paired with logical-form meaning representa-
tions. Traditional CCG lexicons list lexical
items that pair words and phrases with syntac-
tic and semantic content. Such lexicons can
be inefficient when words appear repeatedly
with closely related lexical content. In this
paper, we introduce factored lexicons, which
include both lexemes to model word meaning
and templates to model systematic variation in
word usage. We also present an algorithm for
learning factored CCG lexicons, along with a
probabilistic parse-selection model. Evalua-
tions on benchmark datasets demonstrate that
the approach learns highly accurate parsers,
whose generalization performance benefits
greatly from the lexical factoring.
1 Introduction
Semantic parsers automatically recover representa-
tions of meaning from natural language sentences.
Recent work has focused on learning such parsers
directly from corpora made up of sentences paired
with logical meaning representations (Kate et al,
2005; Kate and Mooney, 2006; Wong and Mooney,
2006, 2007; Zettlemoyer and Collins, 2005, 2007;
Lu et al, 2008; Kwiatkowski et al, 2010).
For example, in a flight booking domain we
might have access to training examples such as:
Sentence: I want flights from Boston
Meaning: ?x. f light(x)? f rom(x,bos)
and the goal is to learn a grammar that can map new,
unseen, sentences onto their corresponding mean-
ings, or logical forms.
One approach to this problem has developed al-
gorithms for leaning probabilistic CCG grammars
(Zettlemoyer and Collins, 2005, 2007; Kwiatkowski
et al, 2010). These grammars are well-suited to the
task of semantic parsing, as they closely link syn-
tax and semantics. They can be used to model a
wide range of complex linguistic phenomena and are
strongly lexicalized, storing all language-specific
grammatical information directly with the words in
the lexicon. For example, a typical learned lexicon
might include entries such as:
(1) f light `N :?x. f light(x)
(2) f light `N/(S|NP) :? f?x. f light(x)? f (x)
(3) f light `N\N :? f?x. f light(x)? f (x)
(4) f are`N :?x.cost(x)
(5) f are`N/(S|NP) :? f?x.cost(x)? f (x)
(6) f are`N\N :? f?x.cost(x)? f (x)
(7) Boston`NP :bos
(8) Boston`N\N :? f?x. f rom(x,bos)? f (x)
(9) New York `NP :nyc
(10) New York `N\N :? f?x. f rom(x,nyc)? f (x)
Although lexicalization of this kind is useful
for learning, as we will see, these grammars can
also suffer from sparsity in the training data, since
closely related entries must be repeatedly learned for
all members of a certain class of words. For exam-
ple, the list above shows a selection of lexical items
that would have to be learned separately.
In this list, the word ?flight? is paired with the
predicate flight in three separate lexical items which
are required for different syntactic contexts. Item
1512
(1) has the standard N category for entries of this
type, item (2) allows the use of the word ?flight?
with that-less relative clauses such as ?flight depart-
ing Boston?, and item (3) is useful for phrases with
unconventional word order such as ?from Boston
flight to New York?. Representing these three lexi-
cal items separately is inefficient, since each word of
this class (such as ?fare?) will require three similarly
structured lexical entries differing only in predicate
name. There may also be systemtatic semantic vari-
ation between entries for a certain class of words.
For example, in (6) ?Boston? is paired with the con-
stant bos that represents its meaning. However, item
(7) also adds the predicate from to the logical form.
This might be used to analyse somewhat elliptical,
unedited sentences such as ?Show me flights Boston
to New York,? which can be challenging for seman-
tic parsers (Zettlemoyer and Collins, 2007).
This paper builds upon the insight that a large pro-
portion of the variation between lexical items for
a given class of words is systematic. Therefore it
should be represented once and applied to a small set
of basic lexical units. 1 We develop a factored lex-
icon that captures this insight by distinguishing lex-
emes, which pair words with logical constants, from
lexical templates, which map lexemes to full lexical
items. As we will see, this can lead to a significantly
more compact lexicon that can be learned from less
data. Each word or phrase will be associated with a
few lexemes that can be combined with a shared set
of general templates.
We develop an approach to learning factored,
probabilistic CCG grammars for semantic pars-
ing. Following previous work (Kwiatkowski et al,
2010), we make use of a higher-order unification
learning scheme that defines a space of CCG gram-
mars consistent with the (sentence, logical form)
training pairs. However, instead of constructing
fully specified lexical items for the learned grammar,
we automatically generate sets of lexemes and lexi-
cal templates to model each example. This is a dif-
ficult learning problem, since the CCG analyses that
1A related tactic is commonly used in wide-coverage CCG
parsers derived from treebanks, such as work by Hockenmaier
and Steedman (2002) and Clark and Curran (2007). These
parsers make extensive use of category-changing unary rules,
to avoid data sparsity for systematically related categories (such
as those related by type-raising). We will automatically learn to
represent these types of generalizations in the factored lexicon.
are required to construct the final meaning represen-
tations are not explicitly labeled in the training data.
Instead, we model them with hidden variables and
develop an online learning approach that simultane-
ously estimates the parameters of a log-linear pars-
ing model, while inducing the factored lexicon.
We evaluate the approach on the benchmark Atis
and GeoQuery domains. This is a challenging setup,
since the GeoQuery data has complex meaning rep-
resentations and sentences in multiple languages,
while the Atis data contains spontaneous, unedited
text that can be difficult to analyze with a formal
grammar representation. Our approach achieves at
or near state-of-the-art recall across all conditions,
despite having no English or domain-specific infor-
mation built in. We believe that ours is the only sys-
tem of sufficient generality to run with this degree of
success on all of these datasets.
2 Related work
There has been significant previous work on learn-
ing semantic parsers from training sentences la-
belled with logical form meaning representations.
We extend a line of research that has addressed
this problem by developing CCG grammar induc-
tion techniques. Zettlemoyer and Collins (2005,
2007) presented approaches that use hand gener-
ated, English-language specific rules to generate lex-
ical items from logical forms as well as English
specific type-shifting rules and relaxations of the
CCG combinators to model spontaneous, unedited
sentences. Zettlemoyer and Collins (2009) extends
this work to the case of learning in context depen-
dent environments. Kwiatkowski et al (2010) de-
scribed an approach for language-independent learn-
ing that replaces the hand-specified templates with
a higher-order-unification-based lexical induction
method, but their approach does not scale well to
challenging, unedited sentences. The learning ap-
proach we develop for inducing factored lexicons is
also language independent, but scales well to these
challenging sentences.
There have been a number of other approaches
for learning semantic parsers, including ones based
on machine translation techniques (Papineni et al,
1997; Ramaswamy and Kleindienst, 2000; Wong
and Mooney, 2006), parsing models (Miller et al,
1996; Ge and Mooney, 2006; Lu et al, 2008), in-
1513
ductive logic programming algorithms (Zelle and
Mooney, 1996; Thompson and Mooney, 2002; Tang
and Mooney, 2000), probabilistic automata (He and
Young, 2005, 2006), and ideas from string kernels
and support vector machines (Kate and Mooney,
2006; Nguyen et al, 2006).
More recent work has focused on training se-
mantic parsers without supervision in the form of
logical-form annotations. Clarke et al (2010) and
Liang et al (2011) replace semantic annotations in
the training set with target answers which are more
easily available. Goldwasser et al (2011) present
work on unsupervised learning of logical form struc-
ture. However, all of these systems require signifi-
cantly more domain and language specific initializa-
tion than the approach presented here.
Other work has learnt semantic analyses from text
in the context of interactions in computational envi-
ronments (Branavan et al (2010), Vogel and Juraf-
sky (2010)); text grounded in partial observations of
a world state (Liang et al, 2009); and from raw text
alone (Poon and Domingos, 2009, 2010).
There is also related work that uses the CCG
grammar formalism. Clark and Curran (2003)
present a method for learning the parameters of a
log-linear CCG parsing model from fully annotated
normal?form parse trees. Watkinson and Manand-
har (1999) describe an unsupervised approach for
learning syntactic CCG lexicons. Bos et al (2004)
present an algorithm for building semantic represen-
tations from CCG parses but requires fully?specified
CCG derivations in the training data.
3 Overview of the Approach
Here we give a formal definition of the problem and
an overview of the learning approach.
Problem We will learn a semantic parser that
takes a sentences x and returns a logical form z repre-
senting its underlying meaning. We assume we have
input data {(xi,zi)|i = 1 . . .n} containing sentences
xi and logical forms zi, for example xi =?Show me
flights to Boston? and zi = ?x. f light(x)? to(x,bos).
Model We will represent the parser as a factored,
probabilistic CCG (PCCG) grammar. A traditional
CCG lexical item would fully specify the syntax and
semantics for a word (reviewed in Section 4). For
example, Boston`NP : bos represents the entry for
the word ?Boston? with syntactic category NP and
meaning represented by the constant bos. Where a
lexicon would usually list lexical items such as this,
we instead use a factored lexicon (L,T ) containing:
? A list of lexemes L. Each lexeme pairs a word
or phrase with a list of logical constants that can
be used to construct its meaning. For example,
one lexeme might be (Boston, [bos]).
? A list of lexical templates T . Each template
takes a lexeme and maps it on to a full lexical
item. For example, there is a single template
that can map the lexeme above to the final lex-
ical entry Boston `NP : bos.
We will make central use of this factored repre-
sentation to provide a more compact representation
of the lexicon that can be learned efficiently.
The factored PCCG will also contain a parameter
vector, ? , that defines a log-linear distribution over
the possible parses y, conditioned on the sentence x.
Learning Our approach for learning factored PC-
CGs extends the work of Kwiatkowski et al (2010),
as reviewed in Section 7. Specifically, we modify
the lexical learning, to produce lexemes and tem-
plates, as well as the feature space of the model, but
reuse the existing parameter estimation techniques
and overall learning cycle, as described in Section 7.
We present the complete approach in three parts
by describing the factored representation of the lex-
icon (Section 5), techniques for proposing potential
new lexemes and templates (Section 6), and finally
a complete learning algorithm (Section 7). How-
ever, the next section first reviews the required back-
ground on semantic parsing with CCG.
4 Background
4.1 Lambda Calculus
We represent the meanings of sentences, words
and phrases with logical expressions that can con-
tain constants, quantifiers, logical connectors and
lambda abstractions. We construct the meanings of
sentences from the meanings of words and phrases
using lambda-calculus operations. We use a version
of the typed lambda calculus (Carpenter, 1997), in
which the basic types include e, for entities; t, for
truth values; and i for numbers. We also have func-
tion types that are assigned to lambda expressions.
1514
The expression ?x. f light(x) takes an entity and re-
turns a truth value, and has the function type ?e, t?.
4.2 Combinatory Categorial Grammar
CCG (Steedman, 1996, 2000) is a linguistic formal-
ism that tightly couples syntax and semantics, and
can be used to model a wide range of language phe-
nomena. A traditional CCG grammar includes a lex-
icon ? with entries like the following:
f lights`N :?x. f light(x)
to` (N\N)/NP :?y.? f .?x. f (x)? to(x,y)
Boston`NP :bos
where each lexical item w`X : h has words w, a syn-
tactic category X , and a logical form h. For the first
example, these are ?flights,? N, and ?x. f light(x).
In this paper, we introduce a new way of represent-
ing lexical items as (lexeme, template) pairs, as de-
scribed in section 5.
CCG syntactic categories may be atomic (such
as S or NP) or complex (such as (N\N)/NP)
where the slash combinators encode word order
information. CCG uses a small set of combinatory
rules to build syntactic parses and semantic repre-
sentations concurrently. Two example combinatory
rules are forward (>) and backward (<) application:
X/Y : f Y : g ? X : f (g) (>)
Y : g X\Y : f ? X : f (g) (<)
These rules apply to build syntactic and semantic
derivations under the control of the word order infor-
mation encoded in the slash directions of the lexical
entries. For example, given the lexicon above, the
phrase ?flights to Boston? can be parsed to produce:
flights to Boston
N (N\N)/NP NP?x. f light(x) ?y? f?x. f (x)? to(x,y) bos
>
(N\N)? f?x. f (x)? to(x,bos)
<N?x. f light(x)? to(x,bos)
where each step in the parse is labeled with the com-
binatory rule (?> or ?<) that was used.
CCG also includes combinatory rules of forward
(> B) and backward (< B) composition:
X/Y : f Y/Z : g? X/Z : ?x. f (g(x)) (> B)
Y\Z : g X\Y : f ? X\Z : ?x. f (g(x)) (< B)
These rules allow a relaxed notion of constituency
which helps limit the number of distinct CCG lexical
items required.
To the standard forward and backward slashes of
CCG we also add a vertical slash for which the di-
rection of application is underspecified. We shall see
examples of this in Section 10.
4.3 Probabilistic CCGs
Due to ambiguity in both the CCG lexicon and the
order in which combinators are applied, there will
be many parses for each sentence. We discriminate
between competing parses using a log-linear model
which has a feature vector ? and a parameter vector
? . The probability of a parse y that returns logical
form z, given a sentence x is defined as:
P(y,z|x;? ,?) = e
? ??(x,y,z)
?(y?,z?) e? ??(x,y?,z?) (1)
Section 8 fully defines the set of features used in the
system presented. The most important of these con-
trol the generation of lexical items from (lexeme,
template) pairs. Each (lexeme, template) pair used
in a parse fires three features as we will see in more
detail later.
The parsing, or inference, problem done at test
time requires us to find the most likely logical form
z given a sentence x, assuming the parameters ? and
lexicon ? are known:
f (x) = argmaxz p(z|x;? ,?) (2)
where the probability of the logical form is found by
summing over all parses that produce it:
p(z|x;? ,?) =?
y
p(y,z|x;? ,?) (3)
In this approach the distribution over parse trees y
is modeled as a hidden variable. The sum over
parses in Eq. 3 can be calculated efficiently using
the inside-outside algorithm with a CKY-style pars-
ing algorithm.
To estimate the parameters themselves, we
use stochastic gradient updates (LeCun et al,
1998). Given a set of n sentence-meaning pairs
{(xi,zi) : i = 1...n}, we update the parameters ? it-
eratively, for each example i, by following the local
gradient of the conditional log-likelihood objective
1515
Oi = logP(zi|xi;? ,?). The local gradient of the in-
dividual parameter ? j associated with feature ? j and
training instance (xi,zi) is given by:
?Oi
?? j = Ep(y|xi,zi;? ,?)[? j(xi,y,zi)]
?Ep(y,z|xi;? ,?)[? j(xi,y,z)]
(4)
As with Eq. 3, all of the expectations in Eq. 4 are
calculated through the use of the inside-outside al-
gorithm on a pruned parse chart. For a sentence
of length m, each parse chart span is pruned using
a beam width proportional to m 23 , to allow larger
beams for shorter sentences.
5 Factored Lexicons
A factored lexicon includes a set L of lexemes and
a set T of lexical templates. In this section, we for-
mally define these sets, and describe how they are
used to build CCG parses. We will use a set of lex-
ical items from our running example to discuss the
details of how the following lexical items:
(1) f light `N :?x. f light(x)
(2) f light `N/(S|NP) :? f?x. f light(x)? f (x)
. . .
(6) Boston`NP :bos
(7) Boston`N\N :? f?x. f rom(x,bos)? f (x)
are constructed from specific lexemes and templates.
5.1 Lexemes
A lexeme (w,~c) pairs a word sequence w with an
ordered list of logical constants ~c = [c1 . . .cm]. For
example, item (1) and (2) above would come from
a single lexeme (flight, [ f light]). Similar lexemes
would be represented for other predicates, for exam-
ple (fare, [cost]). Lexemes also can contain multiple
constants, for example (cheapest, [argmin,cost]),
which we will see more examples of later.
5.2 Lexical Templates
A lexical template takes a lexeme and produces a
lexical item. Templates have the general form
? (?,~v).[? `X : h~v]
where h~v is a logical expression that contains vari-
ables from the list ~v. Applying this template to the
input lexeme (w,~c) gives the full lexical item w `
X :h where the variable ? has been replaced with the
wordspan w and the logical form h has been created
by replacing each of the variables in~v with the coun-
terpart constant from ~c. For example, the lexical
item (6) above would be constructed from the lex-
eme (Boston, [bos]) using the template ? (?,~v).[? `
NP :v1]. Items (1) and (2) would both be constructed
from the single lexeme (flight, [ f light]) with the two
different templates ? (?,~v).[? ` N : ?x.v1(x)] and
? (?,~v).[? `N/(S|NP) :? f?x.v1(x)? f (x)]
5.3 Parsing with a Factored Lexicon
In general, there can by many different (lexeme,
template) pairs that produce the same lexical item.
For example, lexical item (7) in our running ex-
ample above can be constructed from the lexemes
(Boston, [bos]) and (Boston, [ f rom,bos]), given ap-
propriate templates.
To model this ambiguity, we include the selection
of a (lexeme, template) pair as a decision to be made
while constructing a CCG parse tree. Given the lex-
ical item produced by the chosen lexeme and tem-
plate, parsing continues with the traditional combi-
nators, as reviewed in Section 4.2. This direct inte-
gration allows for features that signal which lexemes
and templates have been used while also allowing
for well defined marginal probabilities, by summing
over all ways of deriving a specific lexical item.
6 Learning Factored Lexicons
To induce factored lexicons, we will make use of two
procedures, presented in this section, that factor lexi-
cal items into lexemes and templates. Section 7 will
describe how this factoring operation is integrated
into the complete learning algorithm.
6.1 Maximal Factorings
Given a lexical item l of the form w `X : h with
words w, a syntactic category X , and a logical form
h, we define the maximal factoring to be the unique
(lexeme, template) pair that can be used to recon-
struct l and includes all of the constants of h in
the lexeme (listed in a fixed order based on an
ordered traversal of h). For example, the maxi-
mal factoring for the lexical item Boston ` NP :
bos is the pair we saw before: (Boston, [bos]) and
? (?,~v).[? ` NP : v1]. Similarly, the lexical item
Boston ` N\N : ? f .?x. f (x)? f rom(x,bos) would
be factored to produce (Boston, [ f rom,bos]) and
? (?,~v).[? ` N\N :? f .?x. f (x)? v1(x,v2)].
As we will see in Section 7, this notion of factor-
1516
ing can be directly incorporated into existing algo-
rithms that learn CCG lexicons. When the original
algorithm would have added an entry l to the lexi-
con, we can instead compute the factoring of l and
add the corresponding lexeme and template to the
factored lexicon.
6.2 Introducing Templates with Content
Maximal factorings, as just described, provide for
significant lexical generalization but do not handle
all of the cases needed to learn effectively. For
instance, the maximal split for the item Boston `
N\N : ? f .?x. f (x) ? f rom(x,bos) would introduce
the lexeme (Boston, [ f rom,bos]), which is subopti-
mal since each possible city would need a lexeme
of this type, with the additional from constant in-
cluded. Instead, we would ideally like to learn the
lexeme (Boston, [bos]) and have a template that in-
troduces the from constant. This would model the
desired generalization with a single lexeme per city.
In order to permit the introduction of extra con-
stants into lexical items, we allow the creation of
templates that contain logical constants through par-
tial factorings. For instance, the template below can
introduce the predicate from
? (?,~v).[? `N\N :? f .?x. f (x)? f rom(x,v1)]
The use of templates to introduce extra semantic
constants into a lexical item is similar to, but more
general than, the English-specific type-shifting rules
used in Zettlemoyer and Collins (2007), which were
introduced to model spontaneous, unedited text.
They are useful, as we will see, in learning to re-
cover semantic content that is implied, but not ex-
plicitly stated, such as our original motivating phrase
?flights Boston to New York.?
To propose templates which introduce semantic
content, during learning, we build on the intuition
that we need to recover from missing words, such
as in the example above. In this scenario, there
should also be other sentences that actually include
the word, in our example this would be something
like ?flights from Boston.? We will also assume
that we have learned a good factored lexicon for the
complete example that could produce the parse:
flights from Boston
N (N\N)/NP NP?x. f light(x) ?y? f?x. f (x)? f rom(x,y) bos
>
(N\N)? f?x. f (x)? f rom(x,bos)
<N?x. f light(x)? f rom(x,bos)
Given analyses of this form, we introduce new
templates that will allow us to recover from miss-
ing words, for example if ?from? was dropped. We
identify commonly occurring nodes in the best parse
trees found during training, in this case the non-
terminal spanning ?from Boston,? and introduce
templates that can produce the nonterminal, even if
one of the words is missing. Here, this approach
would introduce the desired template ? (?,~v).[? `
N\N : ? f .?x. f (x) ? f rom(x,v1)] for mapping the
lexeme (Boston, [bos]) directly to the intermediate
structure.
Not all templates introduced this way will model
valid generalizations. However, we will incorporate
them into a learning algorithm with indicator fea-
tures that can be weighted to control their use. The
next section presents the complete approach.
7 Learning Factored PCCGs
Our Factored Unification Based Learning (FUBL)
method extends the UBL algorithm (Kwiatkowski
et al, 2010) to induce factored lexicons, while also
simultanously estimating the parameters of a log-
linear CCG parsing model. In this section, we first
review the NEW-LEX lexical induction procedure
from UBL, and then present the FUBL algorithm.
7.1 Background: NEW-LEX
NEW-LEX generates lexical items by splitting and
merging nodes in the best parse tree of each training
example. Each parse node has a CCG category X : h
and a sequence of words w that it spans. We will
present an overview of the approach using the run-
ning example with the phrase w =?in Boston? and
the category X : h = S\NP :?x.loc(x,bos), which is
of the type commonly seen during learning. The
splitting procedure is a two step process that first
splits the logical form h, then splits the CCG syn-
tactic category X and finally splits the string w.
The first step enumerates all possible splits of
the logical form h into a pair of new expressions
1517
( f ,g) that can be used to reconstruct h by ei-
ther function application (h = f (g)) or composition
(h = ?x. f (g(x))). For example, one possible split is:
( f = ?y.?x.loc(x,y) , g = bos)
which corresponds to the function application case.
The next two steps enumerate all ways of splitting
the syntactic category X and words w to introduce
two new lexical items which can be recombined with
CCG combinators (application or composition) to
recreate the original parse node X : h spanning w. In
our example, one possibility would be:
(in` (S\NP)/NP :?y.?x.loc(x,y) , Boston`NP :bos)
which could be recombined with the forward appli-
cation combinator from Section 4.2.
To assign categories while splitting, the grammar
used by NEW-LEX only uses two atomic syntac-
tic categories S and NP. This allows NEW-LEX to
make use of a direct mapping from semantic type
to syntactic category when proposing syntactic cate-
gories. In this schema, the standard syntactic cat-
egory N is replaced by the category S|NP which
matches the type ?e, t? and uses the vertical slash in-
troduced in Section 4.2. We will see categories such
as this in the evaluation.
7.2 The FUBL Algorithm
Figure 1 shows the FUBL learning algorithm. We
assume training data {(xi,zi) : i= 1 . . .n}where each
example is a sentence xi paired with a logical form
zi. The algorithm induces a factored PCCG, includ-
ing the lexemes L, templates T , and parameters ? .
The algorithm is online, repeatedly performing
both lexical expansion (Step 1) and a parameter up-
date (Step 2) for each training example. The over-
all approach is closely related to the UBL algo-
rithm (Kwiatkowski et al, 2010), but includes exten-
sions for updating the factored lexicon, as motivated
in Section 6.
Initialization The model is initialized with a fac-
tored lexicon as follows. MAX-FAC is a function
that takes a lexical item l and returns the maximal
factoring of it, that is the unique, maximal (lexeme,
template) pair that can be combined to construct l,
as described in Section 6.1. We apply MAX-FAC to
each of the training examples (xi,zi), creating a sin-
gle way of producing the desired meaning zi from a
Inputs: Training set {(xi,zi) : i = 1 . . .n} where each
example is a sentence xi paired with a logical form
zi. Set of entity name lexemes Le. Number of itera-
tions J. Learning rate parameter ?0 and cooling rate
parameter c. Empty lexeme set L. Empty template
set T .
Definitions: NEW-LEX(y) returns a set of new lex-
ical items from a parse y as described in Sec-
tion 7.1. MAX-FAC(l) generates a (lexeme, tem-
plate) pair from a lexical item l. PART-FAC(y)
generates a set of templates from parse y. Both of
these are described in Section 7.2. The distributions
p(y|x,z;? ,(L,T )) and p(y,z|x;? ,(L,T )) are defined
by the log-linear model described in Section 4.3.
Initialization:
? For i = 1 . . .n
? (?,pi) = MAX-FAC(xi ` S : zi)
? L = L?? , T = T ?pi
? Set L = L?Le.
? Initialize ? using coocurrence statistics, as de-
scribed in Section 8.
Algorithm:
For t = 1 . . .J, i = 1 . . .n :
Step 1: (Add Lexemes and Templates)
? Let y? = argmaxy p(y|xi,zi;? ,(L,T ))
? For l ? NEW-LEX(y?)
? (?,pi) = MAX-FAC(l)
? L = L?? , T = T ?pi
? ?= PART-FAC(y?) , T = T ??
Step 2: (Update Parameters)
? Let ? = ?01+c?k where k = i+ t?n.
? Let ?= Ep(y|xi,zi;? ,(L,T ))[?(xi,y,zi)]
?Ep(y,z|xi;? ,(L,T ))[?(xi,y,z)]
? Set ? = ? + ??
Output: Lexemes L, templates T , and parameters ? .
Figure 1: The FUBL learning algorithm.
lexeme containing all of the words in xi. The lex-
emes and templates created in this way provide the
initial factored lexicon.
Step 1 The first step of the learning algorithm in
Figure 1 adds lexemes and templates to the fac-
tored model given by performing manipulations on
the highest scoring correct parse y? of the current
training example (xi,zi). First the NEW-LEX pro-
cedure is run on y? as described in Section 6.1 to
1518
generate new lexical items. We then use the func-
tion MAX-FAC to create the maximal factorings of
each of these new lexical items as described in Sec-
tion 6 and these are added to the factored represen-
tation of the lexicon. New templates can also be in-
troduced through partial factorings of internal parse
nodes as described in Section 6.2. These templates
are generated by using the function PART-FAC to
abstract over the wordspan and a subset of the con-
stants contained in the internal parse nodes of y?.
This step allows for templates that introduce new
semantic content to model elliptical language, as de-
scribed in Section 6.2.
Step 2 The second step does a stochastic gradient
descent update on the parameters ? used in the pars-
ing model. This update is described in Section 4.3
Discussion The FUBL algorithm makes use of a
direct online approach, where lexemes and tem-
plates are introduced in place while analyzing spe-
cific sentences. In general, this will overgeneralize;
not all ways of combining lexemes and templates
will produce high quality lexical items. However,
the overall approach includes features, presented in
Section 8, that can be used to learn which ones are
best in practice. The complete algorithm iterates be-
tween adding new lexical content and updating the
parameters of the parsing model with each proce-
dure guiding the other.
8 Experimental setup
Data Sets We evaluate on two benchmark seman-
tic parsing datasets: GeoQuery, which is made up of
natural language queries to a database of geograph-
ical information; and Atis, which contains natural
language queries to a flight booking system. The
Geo880 dataset has 880 (English-sentence, logical-
form) pairs split into a training set of 600 pairs and
a test set of 280. The Geo250 data is a subset of
the Geo880 sentences that have been translated into
Japanese, Spanish and Turkish as well as the original
English. We follow the standard evaluation proce-
dure for Geo250, using 10-fold cross validation ex-
periments with the same splits of the data as Wong
and Mooney (2007). The Atis dataset contains 5410
(sentence, logical-form) pairs split into a 4480 ex-
ample training set, a 480 example development set
and a 450 example test set.
Evaluation Metrics We report exact match Re-
call (percentage of sentences for which the correct
logical-form was returned), Precision (percentage of
returned logical-forms that are correct) and F1 (har-
monic mean of Precision and Recall). For Atis we
also report partial match Recall (percentage of cor-
rect literals returned), Precision (percentage of re-
turned literals that are correct) and F1, computed as
described by Zettlemoyer and Collins (2007).
Features We introduce two types of features to
discriminate between parses: lexical features and
logical-form features.
Lexical features fire on the lexemes and templates
used to build the lexical items used in a parse. For
each (lexeme,template) pair used to create a lexi-
cal item we have indicator features ?l for the lex-
eme used, ?t for the template used, and ?(l,t) for the
pair that was used. We assign the features on lexi-
cal templates a weight of 0.1 to prevent them from
swamping the far less frequent but equally informa-
tive lexeme features.
Logical-form features are computed on the
lambda-calculus expression z returned at the root of
the parse. Each time a predicate p in z takes an
argument a with type Ty(a) in position i, it trig-
gers two binary indicator features: ?(p,a,i) for the
predicate-argument relation; and ?(p,Ty(a),i) for the
predicate argument-type relation. Boolean opera-
tor features look at predicates that occurr together
in conjunctions and disjunctions. For each variable
vi that fills argument slot i in two conjoined pred-
icates p1 and p2 we introduce a binary indicator
feature ?con j(i,p1,p2). We introduce similar features?dis j(i,p1,p2) for variables vi that are shared by predi-cates in a disjunction.
Initialization The weights for lexeme features are
initialized according to coocurrance statistics be-
tween words and logical constants. These are esti-
mated with the Giza++ (Och and Ney, 2003) imple-
mentation of IBM Model 1. The initial weights for
templates are set by adding ?0.1 for each slash in
the syntactic category and ?2 if the template con-
tains logical constants. Features on lexeme-template
pairs and all parse features are initialized to zero.
Systems We compare performance to all recently-
published, directly-comparable results. For Geo-
Query, this includes the ZC05, ZC07 (Zettlemoyer
1519
System Exact MatchRec. Pre. F1
ZC07 74.4 87.3 80.4
UBL 65.6 67.1 66.3
FUBL 81.9 82.1 82.0
Table 1: Performance on the Atis development set.
System Exact Match Partial MatchRec. Pre. F1. Rec. Pre. F1
ZC07 84.6 85.8 85.2 96.7 95.1 95.9
HY06 - - - - - 90.3
UBL 71.4 72.1 71.7 78.2 98.2 87.1
FUBL 82.8 82.8 82.8 95.2 93.6 94.6
Table 2: Performance on the Atis test set.
and Collins, 2005, 2007), ? -WASP (Wong and
Mooney, 2007), UBL (Kwiatkowski et al, 2010)
systems and DCS (Liang et al, 2011). For Atis,
we report results from HY06 (He and Young, 2006),
ZC07, and UBL.
9 Results
Tables 1-4 present the results on the Atis and Geo-
query domains. In all cases, FUBL achieves at or
near state-of-the-art recall (overall number of correct
parses) when compared to directly comparable sys-
tems and it significantly outperforms UBL on Atis.
On Geo880 the only higher recall is achieved
by DCS with prototypes - which uses signifi-
cant English-specific resources, including manually
specified lexical content, but does not require train-
ing sentences annotated with logical-forms. On
Geo250, FUBL achieves the highest recall across
languages. Each individual result should be inter-
preted with care, as a single percentage point cor-
responds to 2-3 sentences, but the overall trend is
encouraging.
On the Atis development set, FUBL outperforms
ZC07 by 7.5% of recall but on the Atis test set
FUBL lags ZC07 by 2%. The reasons for this dis-
crepancy are not clear, however, it is possible that
the syntactic constructions found in the Atis test set
do not exhibit the same degree of variation as those
seen in the development set. This would negate the
need for the very general lexicon learnt by FUBL.
Across the evaluations, despite achieving high re-
call, FUBL achieves significantly lower precision
than ZC07 and ? -WASP. This illustrates the trade-
off from having a very general model of proposing
lexical structure. With the ability to skip unseen
System Rec. Pre. F1
Labelled Logical Forms
ZC05 79.3 96.3 87.0
ZC07 86.1 91.6 88.8
UBL 87.9 88.5 88.2
FUBL 88.6 88.6 88.6
Labelled Question Answers
DCS 91.1 - -
Table 3: Exact match accuracy on the Geo880 test set.
System English SpanishRec. Pre. F1 Rec. Pre. F1
? -WASP 75.6 91.8 82.9 80.0 92.5 85.8
UBL 81.8 83.5 82.6 81.4 83.4 82.4
FUBL 83.7 83.7 83.7 85.6 85.8 85.7
System Japanese TurkishRec. Pre. F1 Rec. Pre. F1
? -WASP 81.2 90.1 85.8 68.8 90.4 78.1
UBL 83.0 83.2 83.1 71.8 77.8 74.6
FUBL 83.2 83.8 83.5 72.5 73.7 73.1
Table 4: Exact-match accuracy on the Geo250 data set.
words, FUBL returns a parse for all of the Atis test
sentences, since the factored lexicons we are learn-
ing can produce a very large number of lexical items.
These parses are, however, not always correct.
10 Analysis
The Atis results in Tables 1 and 2 highlight the ad-
vantages of factored lexicons. FUBL outperforms
the UBL baseline by 16 and 11 points respectively
in exact-match recall. Without making any modi-
fication to the CCG grammars or parsing combina-
tors, we are able to induce a lexicon that is general
enough model the natural occurring variations in the
data, for example due to sloppy, unedited sentences.
Figure 2 shows a parse returned by FUBL for
a sentence on which UBL failed. While
the word ?cheapest? is seen 208 times in the
training data, in only a handful of these in-
stances is it seen in the middle of an utter-
ance. For this reason, UBL never proposes
the lexical item, cheapest ` NP\(S|NP)/(S|NP) :
? f?g.argmin(?x. f (x)? g(x),?y.cost(y)), which is
used to parse the sentence in Figure 2. In contrast,
FUBL uses a lexeme learned from the same word in
different contexts, along with a template learnt from
similar words in a similar context, to learn to per-
1520
pittsburgh to atlanta the cheapest on july twentieth
NP (S|NP)\NP/NP NP NP\(S|NP)/(S|NP) (S|NP)/NP/NP NP NP
pit ?x?y? z.to(z,x) atl ? f?g.argmin(?x. f (x)?g(x),?y.cost(y)) ?x?y? z.month(z,x) jul 20
? f rom(z,y) ?day(z,y)
> >
(S|NP)\NP (S|NP)/NP?x?y.to(y,atl)? f rom(y,x) ?x?y.month(y, jul)?day(y,x)
< >
(S|NP) (S|NP)?x.to(x,atl)? f rom(x, pit) ?x.month(x, jul)?day(x,20)
>NP\(S|NP)? f .argmin(?x. f (x)?month(x, jul)?day(x,20),?y.cost(y))
<NP
argmin(?x. f rom(x, pit)? to(x,atl)?month(x, jul)?day(x,20),?y.cost(y))
Figure 2: An example learned parse. FUBL can learn this type of analysis with novel combinations of lexemes and
templates at test time, even if the individual words, like ?cheapest,? were never seen in similar syntactic constructions
during training, as described in Section 10.
form the desired analysis.
As well as providing a new way to search the lex-
icon during training, the factored lexicon provides a
way of proposing new, unseen, lexical items at test
time. We find that new, non-NP, lexical items are
used in 6% of the development set parses.
Interestingly, the addition of templates that intro-
duce semantic content (as described in Section 6.2)
account for only 1.2% of recall on the Atis develop-
ment set. This is suprising as elliptical constructions
are found in a much larger proportion of the sen-
tences than this. In practice, FUBL learns to model
many elliptical constructions with lexemes and tem-
plates introduced through maximal factorings. For
example, the lexeme (to, [ f rom, to]) can be used
with the correct lexical template to deal with our
motivating example ?flights Boston to New York?.
Templates that introduce content are therefore only
used in truly novel elliptical constructions for which
an alternative analysis could not be learned.
Table 5 shows a selection of lexemes and tem-
plates learned for Atis. Examples 2 and 3 show that
morphological variants of the same word must still
be stored in separate lexemes. However, as these
lexemes now share templates, the total number of
lexical variants that must be learned is reduced.
11 Discussion
We argued that factored CCG lexicons, which in-
clude both lexemes and lexical templates, provide
a compact representation of lexical knowledge that
can have advantages for learning. We also described
a complete approach for inducing factored, prob-
abilistic CCGs for semantic parsing, and demon-
Most common lexemes by type of constants in~c.
1 e (Boston, [bos]) (Denver, [den])
2 ?e, t? (flight, [ f light]) (flights, [ f light])
3 ?e, i? (fare, [cost]) (fares, [cost])
4 ?e,?e, t?? (from, [ f rom]) (to, [to])
5 ?e, i?, (cheapest, [argmin,cost])?e, t? (earliest, [argmin,dep time])
6 ?i,?i, t??, (after, [>,dep time])
?e, i? (before, [<,dep time])
Most common templates matching lexemes above.
1 ? (?,~v).? `NP :v1
2 ? (?,~v).? `S|NP :?x.v1(x)
3 ? (?,~v).? `NP|NP :?x.v1(x)
4 ? (?,~v).? `S|NP/NP\(S|NP) :?x?y.v1(x,y)
5 ? (?,~v).? `NP/(S|NP) :? f .v1(?x. f (x),?y,v2(y))
6 ? (?,~v).? `S|NP\(S|NP)/NP :
?x?y? z.v1(v2(z),x)? y(x)
Table 5: Example lexemes and templates learned from
the Atis development set.
strated strong performance across a wider range of
benchmark datasets that any previous approach.
In the future, it will also be important to ex-
plore morphological models, to better model vari-
ation within the existing lexemes. The factored lex-
ical representation also has significant potential for
lexical transfer learning, where we would need to
learn new lexemes for each target application, but
much of the information in the templates could, po-
tentially, be ported across domains.
Acknowledgements
The work was supported in part by EU ERC Ad-
vanced Fellowship 249520 GRAMPLUS, and an
ESPRC PhD studentship. We would like to thank
Yoav Artzi for helpful discussions.
1521
References
Bos, Johan, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In Pro-
ceedings of the International Conference on Computa-
tional Linguistics.
Branavan, S.R.K., Luke Zettlemoyer, and Regina Barzi-
lay. 2010. Reading between the lines: Learning to map
high-level instructions to commands. In Association
for Computational Linguistics (ACL).
Carpenter, Bob. 1997. Type-Logical Semantics. The MIT
Press.
Clark, Stephen and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing.
Clark, Stephen and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics
33(4):493?552.
Clarke, James, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world?s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL-2010). Uppsala, Sweden,
pages 18?27.
Ge, Ruifang and Raymond J. Mooney. 2006. Discrimina-
tive reranking for semantic parsing. In Proceedings of
the COLING/ACL 2006 Main Conference Poster Ses-
sions.
Goldwasser, Dan, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. In Association for Computational Linguistics
(ACL).
He, Yulan and Steve Young. 2005. Semantic processing
using the hidden vector state model. Computer Speech
and Language .
He, Yulan and Steve Young. 2006. Spoken language
understanding using the hidden vector state model.
Speech Communication 48(3-4).
Hockenmaier, Julia and Mark Steedman. 2002. Gener-
ative models for statistical parsing with Combinatory
Categorial Grammar. In Proceedings of the 40th Meet-
ing of the ACL. Philadelphia, PA, pages 335?342.
Kate, Rohit J. and Raymond J. Mooney. 2006. Using
string-kernels for learning semantic parsers. In Pro-
ceedings of the 44th Annual Meeting of the Association
for Computational Linguistics.
Kate, Rohit J., Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to transform natural to formal lan-
guages. In Proceedings of the National Conference
on Artificial Intelligence.
Kwiatkowski, Tom, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proceedings of the Conference on Em-
perical Methods in Natural Language Processing.
LeCun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE 86(11):2278?2324.
Liang, P., M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP).
Liang, P., M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Asso-
ciation for Computational Linguistics (ACL).
Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceedings
of The Conference on Empirical Methods in Natural
Language Processing.
Miller, Scott, David Stallard, Robert J. Bobrow, and
Richard L. Schwartz. 1996. A fully statistical approach
to natural language interfaces. In Proc. of the Associ-
ation for Computational Linguistics.
Nguyen, Le-Minh, Akira Shimazu, and Xuan-Hieu Phan.
2006. Semantic parsing with structured SVM ensem-
ble classification models. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics 29(1):19?51.
Papineni, K. A., S. Roukos, and T. R. Ward. 1997.
Feature-based language understanding. In Proceed-
ings of European Conference on Speech Communica-
tion and Technology.
Poon, Hoifung and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Poon, Hoifung and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Association for
Computational Linguistics (ACL).
Ramaswamy, Ganesh N. and Jan Kleindienst. 2000. Hier-
archical feature-based translation for scalable natural
language understanding. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
Steedman, Mark. 1996. Surface Structure and Interpre-
tation. The MIT Press.
1522
Steedman, Mark. 2000. The Syntactic Process. The MIT
Press.
Tang, Lappoon R. and Raymond J. Mooney. 2000. Au-
tomated construction of database interfaces: Integrat-
ing statistical and relational learning for semantic pars-
ing. In Proceedings of the Joint Conference on Empiri-
cal Methods in Natural Language Processing and Very
Large Corpora.
Thompson, Cynthia A. and Raymond J. Mooney. 2002.
Acquiring word-meaning mappings for natural lan-
guage interfaces. Journal of Artificial Intelligence Re-
search 18.
Vogel, Adam and Dan Jurafsky. 2010. Learning to follow
navigational directions. In Association for Computa-
tional Linguistics (ACL).
Watkinson, Stephen and Suresh Manandhar. 1999. Un-
supervised lexical learning with categorial grammars
using the LLL corpus. In Proceedings of the 1st Work-
shop on Learning Language in Logic.
Wong, Yuk Wah and Raymond Mooney. 2006. Learning
for semantic parsing with statistical machine transla-
tion. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL.
Wong, Yuk Wah and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the Association for
Computational Linguistics.
Zelle, John M. and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic pro-
gramming. In Proceedings of the National Conference
on Artificial Intelligence.
Zettlemoyer, Luke S. and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of the Conference on Uncertainty in Arti-
ficial Intelligence.
Zettlemoyer, Luke S. and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing to
logical form. In Proc. of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning.
Zettlemoyer, Luke S. and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of The Joint Conference
of the Association for Computational Linguistics and
International Joint Conference on Natural Language
Processing.
1523
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 289?299,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Coreference Resolution and Named-Entity Linking
with Multi-pass Sieves
Hannaneh Hajishirzi Leila Zilles Daniel S. Weld Luke Zettlemoyer
Department of Computer Science and Electrical Engineering
University of Washington
{hannaneh,lzilles,lsz,weld}@cs.washington.edu
Abstract
Many errors in coreference resolution come
from semantic mismatches due to inadequate
world knowledge. Errors in named-entity
linking (NEL), on the other hand, are of-
ten caused by superficial modeling of entity
context. This paper demonstrates that these
two tasks are complementary. We introduce
NECO, a new model for named entity linking
and coreference resolution, which solves both
problems jointly, reducing the errors made on
each. NECO extends the Stanford determinis-
tic coreference system by automatically link-
ing mentions to Wikipedia and introducing
new NEL-informed mention-merging sieves.
Linking improves mention-detection and en-
ables new semantic attributes to be incorpo-
rated from Freebase, while coreference pro-
vides better context modeling by propagat-
ing named-entity links within mention clus-
ters. Experiments show consistent improve-
ments across a number of datasets and ex-
perimental conditions, including over 11% re-
duction in MUC coreference error and nearly
21% reduction in F1 NEL error on ACE 2004
newswire data.
1 Introduction
Coreference resolution and named-entity linking are
closely related problems, but have been largely stud-
ied in isolation. This paper demonstrates that they
are complementary by introducing a simple joint
model that improves performance on both tasks.
Coreference resolution is the task of determining
when two textual mentions name the same individ-
[Michael Eisner]1 and [Donald Tsang]2 announced the
grand opening of [[Hong Kong]3 Disneyland]4 yester-
day. [Eisner]1 thanked [the President]2 and welcomed
[fans]5 to [the park]4.
Figure 1: A text passage illustrating interactions between
coreference resolution and NEL.
ual. The biggest challenge in coreference resolu-
tion ? accounting for 42% of errors in the state-
of-the-art Stanford system ? is the inability to rea-
son effectively about background semantic knowl-
edge (Lee et al, 2013). For example, consider the
sentence in Figure 1. ?President? refers to ?Donald
Tsang? and ?the park? refers to ?Hong Kong Dis-
neyland,? but automated algorithms typically lack
the background knowledge to draw such inferences.
Incorporating knowledge is challenging, and many
efforts to do so have actually hurt performance,
e.g. (Lee et al, 2011; Durrett and Klein, 2013).
Named-entity linking (NEL) is the task of match-
ing textual mentions to corresponding entities in a
knowledge base, such as Wikipedia or Freebase.
Such links provide rich sources of semantic knowl-
edge about entity attributes ? Freebase includes
president as Tsang?s title and Disneyland as hav-
ing the attribute park. But NEL is itself a chal-
lenging problem, and finding the correct link re-
quires disambiguating based on the mention string
and often non-local contextual features. For exam-
ple, ?Michael Eisner? is relatively unambiguous but
the isolated mention ?Eisner? is more challenging.
However, these mentions could be clustered with
a coreference model, allowing for improved NEL
through link propagation from the easier mentions.
289
We present NECO, a new algorithm for jointly solv-
ing named entity linking and coreference resolu-
tion. Our work is related to that of Ratinov and
Roth (2012), which also uses knowledge derived
from an NEL system to improve coreference. How-
ever, NECO is the first joint model we know of, is
purely deterministic with no learning phase, does
automatic mention detection, and improves perfor-
mance on both tasks.
NECO extends the Stanford?s sieve-based model,
in which a high recall mention detection phase is
followed by a sequence of cluster merging opera-
tions ordered by decreasing precision (Raghunathan
et al, 2010; Lee et al, 2013). At each step, it
merges two clusters only if all available information
about their respective entities is consistent. We use
NEL to increase recall during the mention detection
phase and introduce two new cluster-merging sieves,
which compare the Freebase attributes of entities.
NECO also improves NEL by initially favoring high
precision linking results and then propagating links
and attributes as clusters are formed.
In summary we make the following contributions:
? We introduce NECO, a novel, joint approach
to solving coreference and NEL, demonstrating
that these tasks are complementary by achiev-
ing joint error reduction.
? We present experiments showing improved per-
formance at coreference resolution, given both
gold and automatic mention detection: e.g.,
6.2 point improvement in MUC recall on ACE
2004 newswire text and 3.1 point improvement
in MUC precision the CoNLL 2011 test set.
? NECO also leads to better performance at
named-entity linking, given both gold and au-
tomatic linking, improving F1 from 61.7% to
69.2% on a newly labeled test set.1
2 Background
We make use of existing models for coreference res-
olution and named entity linking.
1Our corpus and the source code for NECO can be down-
loaded from https://www.cs.washington.edu/
research-projects/nlp/neco.
2.1 Coreference Resolution
Coreference resolution is the the task of identifying
all text spans (called mentions) that refer to the same
entity, forming mention clusters.
Stanford?s SieveModel is a state-of-the-art coref-
erence resolver comprising a pipeline of ?sieves?
that merge coreferent mentions according to deter-
ministic rules. Mentions are automatically predicted
by selecting all noun phrases (NP), pronouns, and
named entities. Each sieve either merges a cluster
to its single best antecedent from a list of previous
clusters, or declines to merge.
Higher precision sieves are applied earlier in the
pipeline according to the following order, looking at
different aspects of the text, including: (1) speaker
identification, (2-3) exact and relaxed string matches
between mentions, (4) precise constructs, including
appositives, acronyms and demonyms, (5-9) differ-
ent notions of strict and relaxed head matches be-
tween mentions, and finally (10) a number of syn-
tactic and distance cues for pronoun resolution.
2.2 Named Entity Linking
Named-entity linking (NEL) is the task of identi-
fying mentions in a text and linking them to the
entity they name in a knowledge base, usually
Wikipedia. NECO uses two existing NEL sys-
tems: GLOW (Ratinov et al, 2011) and Wikipedi-
aMiner (Milne and Witten, 2008).
WikipediaMiner links mentions based on a notion
of semantic similarity to Wikipedia pages, consider-
ing all substrings up to a fixed length. Since there
are often many possible links, it disambiguates by
choosing the entity whose Wikipedia page is most
semantically related to the nearby context of the
mention. The semantic scoring function includes n-
gram statistics and also counts shared links to other
unambiguous mentions in the text.
GLOW finds mentions by selecting all the NPs
and named entities in the text. Linking is framed
as an integer linear programming optimization prob-
lem that takes into account using similar local con-
straints but also includes global constraints such as
entity link co-occurrence.
Both systems return confidence values. To main-
tain high precision, NECO uses an ensemble of
290
? Let Exemplar(c) be a representative mention of the cluster c, computed as defined below
? Let cj be an antecedent cluster of ci if cj has a mention which is before the first mention of ci
? Let l(m) be a Wikipedia page linked to mention m or ? if there is no link
? Let l(c) be a Wikipedia page linked to mention Exemplar(c) or ? if there is no link
1. Initialize Linked Mentions:
(a) Let MNEL = {mi | i = 1 . . . p} be the NEL output mentions, mi, each with a link l(mi)
(b) Let MCR = {mi | i = 1 . . . q} be the mentions mi from coreference mention detection
(c) Let M ?MCR ?MNEL (Sec. 3.1)
(d) Update entity links for all m ?M and prune M (Sec. 3.2)
(e) Extract attributes from Wikipedia and Freebase for all m ?M (Sec. 3.3)
(f) Let C ?M be singleton mention clusters where Exemplar(ci) = mi, l(ci) = l(mi)
2. Merge Clusters: For every sieve S (including NEL sieves, Sec. 3.6) and cluster ci ? C
(a) For every cluster cj , j = [i? 1 . . . 1] (traverse the preceding clusters in reverse order)
i. NEL constraints: Prevent merge if l(ci) 6= l(cj) (Sec. 3.4)
ii. If all rules of sieve S are satisfied for clusters ci and cj
A. ck ? Merge(ci, cj), including entity link and attribute updates (Sec. 3.5)
B. C ? C ? {ck} \ {ci, cj}
3. Output: Coreference clusters C and linked Wikipedia pages l(ci)?ci ? C
Figure 2: NECO: A joint algorithm for named-entity linking and coreference resolution.
GLOW and WikipediaMiner, selecting only high
confidence links.
3 Joint Coreference and Linking
We introduce a joint model for coreference resolu-
tion and NEL. Building on the Stanford sieve ar-
chitecture, our algorithm incrementally constructs
clusters of mentions using deterministic coreference
rules under NEL constraints.
Figure 2 presents the complete algorithm. The in-
put to NECO is a document and the output is a set C
of coreference clusters, with links l(c) to Wikipedia
pages for a subset of the clusters c ? C. Step 1
detects mentions, merging the outputs of the base
systems (Sec. 3.1). Step 2 repeatedly merges coref-
erence clusters, while ensuring that NEL constraints
(Sec. 3.4) are satisfied. It uses the original Stan-
ford sieves and also two new NEL-informed sieves
(Sec. 3.6). NEL links are propagated to new clusters
as they are formed (Sec. 3.5).
3.1 Mention Detection
In Steps 1(a-c) in Fig. 2, NECO combines mentions
from the base coreference and NEL systems.
Let MCR be the set of mentions returned by us-
ing Stanford?s rule-based mention detection algo-
rithm (Lee et al, 2013). Let MNEL be the set of
mentions output by the two NEL systems. NECO
creates an initial set of mentions, M , by taking the
union of all the mentions in MNEL and MCR. In
practice, taking the union increases diversity in the
mention pool. For example, it is often the case that
MNEL will include sub-phrases such as ?Suharto?
when they are part of a larger mention ?ex-dictator
Suharto? that is detected in MCR.
3.2 Mention Entity Links and Pruning
Step 1(d) in Fig. 2 assigns Wikipedia links to a sub-
set of the detected mentions.
For mentions m output by the base NEL sys-
tems, we assign an exact link l(m) if the entire
mention span is linked. Mentions m? that differ
from an exact linked mention m by only a pre- or
post-fix stop word are similarly assigned exact links
l(m?) = l(m). For example, the mention ?the pres-
ident? will be assigned the same link as ?president?
but ?The governor of Alaska Sarah Palin? would not
be assigned an exact link to Sarah Palin.
For mentions m? that do not receive an exact link,
we assign a head link h(m?) if the head word2 m has
been linked, by setting h(m?) = l(m). For instance,
the head link for the mention ?President Clinton?
(with ?Clinton? as head word) will be the Wikipedia
title of Bill Clinton. We use head links for the
Relaxed NEL sieve (Sec. 3.6).
Next, we define L(m) to be the set con-
2A head word is assigned to every mention with the Stanford
parser head finding rules (Klein and Manning, 2003).
291
country president city area
company state region location
place agency power unit
body market park province
manager organization owner trial
site prosecutor attorney county
senator stadium network building
attraction government department person
origin plant airport kingdom
capital operation author period
nominee candidate film venue
Figure 3: The most commonly used fine-grained at-
tributes from Freebase and Wikipedia (out of over 500
total attributes).
taining l(m) and l(m?) for all sub-phrases m?
of m. We add the sub-phrase links only
if their confidence is higher than the confi-
dence for l(m). For instance, assuming ap-
propriate confidence values, L(m) would in-
clude the pages for {List of governors of
Alaska, Alaska, Sarah Palin} given the
mention ?The governor of Alaska Sarah Palin.? We
will use L(m) for NEL constraints and filtering
(Sec. 3.4).
After updating the entity links for all mentions,
NECO prunes spurious mentions that begin or
end with a stop word where the remaining sub-
expression of the mention exists in M . It also re-
moves time expressions and numbers from M if they
are not included in MNEL.
3.3 Mention Attributes
Step 1(e) in Fig. 2 also assigns attributes for a
mention m linked to Wikipedia page l(m), at both
coarse and fine-grained levels, based on information
from the Freebase entry corresponding to exact link
l(m) or head link h(m).
The coarse attributes include gender, type, and
NER classes such as PERSON, LOCATION, and OR-
GANIZATION. These attributes are part of the orig-
inal Stanford coreference system and are used to
avoid merging conflicting clusters. We use the Free-
base values for these attributes when available. For
instance, if the linked entity contains the Freebase
type location or organization, we include the coarse
type to LOCATION or ORGANIZATION respectively.
In order to account for both links to specific peo-
ple (Barack Obama) and generic links to positions
held by people (President), we include the type PER-
SON if the linked entity has any of the Freebase types
person, job title, or government office or title. If no
coarse Freebase types are available for an attribute,
we default to predicted NER classes.
We add fine-grained attributes from Freebase and
Wikipedia by importing additional type information.
We use all of the Freebase notable types, a set of
hundreds of commonly used Freebase types, rang-
ing from us president to tropical cyclone and syn-
thpop album. We also include all of the Wikipedia
categories, on average six per entity. For example,
the mention ?Indonesia? is assigned fine-grained at-
tributes such as book subject, military power, and
olympic participating country. Since many of these
fine-grained attributes are extremely specific, we use
the last word of each attribute to define an addi-
tional fine-grained attribute (see Fig. 3). These fine-
grained attributes are used in the Relaxed NEL sieve
(Sec. 3.6).
3.4 NEL Constraints
While applying sieves to merge clusters in Figure 2
Step 2(a), NECO uses NEL constraints to eliminate
some otherwise acceptable merges.
We avoid merging inconsistent clusters that link
to different entities. Clusters ci and cj are incon-
sistent if both are linked (i.e., both clusters have
non-null entity assignments) and l(ci) 6= l(cj) or
h(ci) 6= h(cj). Also, in order to consider an an-
tecedent cluster c as a merge candidate, we require a
pair of entities in the set of linked entities L(c) to be
related to one another in Freebase. Two entities are
related in Freebase if they both appear in a relation;
for example, Bill Clinton and Arkansas are
related because Bill Clinton has a ?governor-of? re-
lation with Arkansas.
3.5 Merging Clusters and Update Entity Links
When two clusters ci and cj are merged to form a
new cluster ck, the entity link information L(ck),
l(ck), and h(ck) must be updated (Step 2 of Fig. 2).
We set L(ck) to the union of the linked entities found
in l(ci) and l(cj) and merge coarse attributes at this
point.
In order to set the exact and head entity links
l(ck) and h(ck), we use the exemplar mention
292
Exemplar(ck) that denotes the most representative
mention of the cluster. Exemplar(c) is selected
according to a set of rules in the Stanford system,
based on textual position and mention type (proper
noun vs. common). We augment this function by
considering information from exact and head en-
tity links as well. Mentions appearing earlier in
text, proper mentions, and mentions that have ex-
act or head named-entity links are preferred to those
which do not. Given exemplars, we set l(ck) =
l(Exemplar(ck)) and h(ck) = h(Exemplar(ck)).
3.6 NEL Sieves
Finally, we introduce two new sieves that use NEL
information at the beginning and end of the Stan-
ford sieves pipeline in the merging stage (Step 2 of
Fig. 2).
Exact NEL sieve The Exact NEL sieve merges
two clusters ci and cj if both are linked and their
links match, l(ci) = l(cj). For example, all men-
tions that have been linked to Barack Obama will
become members of the same coreference cluster.
Because the Exact NEL sieve has high precision, we
place it at the very beginning of the pipeline.
Relaxed NEL sieve The Relaxed NEL sieve uses
fine-grained attributes of the linked mentions to
merge proper nouns with common nouns when they
share attributes. For example, this sieve is able to
merge the proper mention ?Disneyland? with the
?the mysterious park?, because park is one of the
fine-grained attributes assigned to Disneyland.
More formally, let mi = Exemplar(ci) and
mj = Exemplar(cj). For every common noun
mention mi, we merge ci with an antecedent clus-
ter cj if (1) mj is a linked proper noun, (2) if mi or
the title of its linked Wikipedia page is in the list of
fine-grained attributes of mj , or (3) if h(mj) is re-
lated to the head link h(mi) according to Freebase
as defined above.
Because this sieve has low precision, we only
allow merges between mentions that have a maxi-
mum distance of three sentences between one an-
other. We add the Relaxed NEL sieve near the end
of the pipeline, just before pronoun resolution.
4 Experimental Setup
Core Components and Baselines The Stanford
sieve-based coreference system (Lee et al, 2013),
the GLOW NEL system (Ratinov et al, 2011), and
WikipediaMiner (Milne and Witten, 2008) provide
core functionality for our joint model, and are also
the state-of-the-art baselines against which we mea-
sure performance.
Parameter Settings Based on performance on the
development set, we set the GLOW?s confidence pa-
rameter to 1.0 and WikipediaMiner?s to 0.4 to assure
high-precision NEL. We also optimized for the set of
fine-grained attributes to import from Wikipedia and
Freebase, and the best way to incorporate the NEL
constraints into the sieve architecture.
Datasets We report results on the following
three datasets: ACE????-NWIRE, CONLL????,
and ACE????-NWIRE-NEL. ACE????-NWIRE, the
newswire subset of the ACE 2004 corpus (NIST,
2004), includes 128 documents. The CONLL????
coreference dataset includes text from five different
domains: broadcast conversation (BC), broadcast
news (BN), magazine (MZ), newswire (NW), and
web data (WB) (Pradhan et al, 2011). The broadcast
conversation and broadcast news domains consist of
transcripts, whereas magazine and newswire contain
more standard written text. The development data
includes 303 documents and the test data includes
322 documents.
We created ACE????-NWIRE-NEL by taking a
subset of ACE????-NWIRE and annotating with
gold-standard entity links. We segment and link all
the expressions in text that refer to Wikipedia pages,
allowing for nested linking. For instance, both the
phrase ?Hong Kong Disneyland,? and the sub-phrase
?Hong Kong? are linked. This dataset includes 12
documents and 350 linked entities.
Metrics We evaluate our system using MUC (Vi-
lain et al, 1995), B3 (Bagga and Baldwin, 1998),
and pairwise scores. MUC is a link-based met-
ric which measures how many clusters need to be
merged to cover the gold clusters and favors larger
clusters; B3 computes the proportion of intersec-
tion between predicted and gold clusters for every
mention and favors singletons (Recasens and Hovy,
2010). We computed the scores using the Stanford
293
Method MUC B3 Pairwise
P R F1 P R F1 P R F1
Stanford Sieves 39.9 46.2 42.8 67.9 71.8 69.8 44.2 29.7 35.6
NECO 46.8 52.5 49.5 70.4 72.6 71.5 51.5 34.6 41.4
No NEL Mentions 46.1 48.3 47.2 71.4 70.0 70.9 49.7 30.9 38.1
No Mention Pruning 43.6 45.6 44.6 70.5 69.9 70.2 46.2 29.4 35.9
No Attributes 45.9 47.4 46.6 71.8 69.7 70.7 48.6 27.0 34.7
No Constraints 42.3 49.3 45.5 68.3 72.3 70.2 44.2 28.6 34.7
Table 1: Coreference results on ACE????-NWIRE with predicted mentions and automatic linking.
coreference software for ACE2004 and using the
CoNLL scorer for the CoNLL 2011 dataset.
5 Experimental Results
We first look at NECO?s performance at coreference
resolution and then evaluate its ability at NEL.
5.1 Coref. Results with Predicted Mentions
Overall System Performance on ACE Data Ta-
ble 1 shows NECO?s performance at coreference
resolution on ACE-???? compared to the Stanford
sieve implementation (Lee et al, 2013). The table
shows that NECO has both significantly improved
precision and recall compared to the Stanford base-
line, across all metrics. We generally observe larger
gains in MUC due to better mention detection and
the Relaxed NEL Sieve.
Contribution of System Components Table 1
also details the performance of four variants of our
system that ablate various components and features.
Specifically, we consider the following cases:
? No NEL Mentions: We discard additional
mentions, MNEL, provided by NEL (Sec. 3.1).
This increases B3 precision at the expense of
recall. Inspection shows that some of the errors
introduced by MNEL are actually due to cor-
rectly linked entities that were not annotated as
mentions in the dataset, but also some improp-
erly linked mentions.
? No Mention Pruning: We disable the initial
step of updating mention boundaries and re-
moving spurious mentions (Sec. 3.2). As ex-
pected, removing this step drops precision and
recall significantly, even compared to the No
NEL Mentions variant.
? No Attributes: Ablating coarse and fine-
grained attributes (Sec. 3.3) drops F1 and re-
call measures across all metrics. To under-
stand this effect, note that NECO uses at-
tributes in two different settings. Updating
coarse attributes tends to increase precision be-
cause it prevents dangerous merges, such as
merging ?Staples? with the mention ?it? in
a situation when ?Staples? refers to the per-
son entity Todd Staples. Fine-grained at-
tributes also help with recall, when merging
a specific name of an entity with a mention
that uses a more general term; for instance,
?Hong Kong Disneyland? can be merged with
?the mysterious park? because ?park? is a fine-
grained attribute for Disneyland. However,
when fine-grained attributes are used, precision
sometimes drops (e.g., when ?president? might
merge with ?Bush? when it should really merge
with ?Clinton?).
? No NEL Constraints: Removing these con-
straints (Sec. 3.4) drops precision dramatically
leading to drop in F1. In the case of incor-
rect linking, however, NEL constraints can af-
fect recall. For instance, NEL constraints might
prevent merging ?Staples? with ?Todd Staples?
if the former were linked to the company and
the latter to the politician.
Overall System Performance on CoNLL Data
We also compare our full system (with added NEL
sieves, constraints, and mention pruning3) with the
Stanford sieve coreference system on CoNLL data
3Due to CoNLL annotation guidelines, a named entity is
added to the mention list if it is not inside a larger mention with
an exact named entity link.
294
MUC B3
Category: Method P R F1 P R F1
BC: NECO 62.1 64.7 63.4 69.8 57.8 63.2
BC: Stanford Sieves 60.9 65.0 62.9 69.2 58.0 63.1
BN: NECO 69.3 59.4 64.0 78.8 60.8 68.6
BN: Stanford Sieves 68.0 58.9 63.1 79.0 60.2 68.3
MZ: NECO 67.6 62.9 65.2 78.4 61.1 68.7
MZ: Stanford Sieves 66.0 63.4 64.9 77.9 61.5 68.7
NW: NECO 62.0 54.5 58.0 74.9 57.4 65.0
NW: Stanford Sieves 60.0 54.2 56.9 75.3 57.0 64.9
Table 3: Coreference results on the individual categories of CoNLL 2011 development data. (BC=broadcast conver-
sation, BN=broadcast news, MZ=magazine, NW=newswire)
MUC B3
Method P R F1 P R F1
Development Data
NECO 64.1+ 59.4 61.7+ 74.7 58.7 65.7
Stanford 62.7 59.0 60.8 74.8 58.3 65.6
NECO* 56.4+ 50.0 53.0+ 72.6 51.6 60.3
Stanford* 53.5 50.0 51.6 71.8 51.3 59.9
Test Data
NECO 61.2+ 58.4 59.8+ 72.2 56.4 63.3
Stanford 59.2 58.8 59.0 71.3 56.1 62.8
NECO* 55.1+ 51.7 53.3+ 70.0 50.8 58.8
Stanford* 52.0 52.3+ 52.1 68.9 50.8 58.5
Table 2: Coreference results on CoNLL 2011 develop-
ment and test data, using predicted mentions. Rows de-
noted with * indicate runs using the fully automated Stan-
ford CoreNLP pipeline rather than the predicted annota-
tions provided with the CoNLL data. Given the relatively
close results, we ran the Mann-Whitney U test for this
table; values with the + superscript are significant with
p < 0.05.
(Table 2). We ran NECO and the baseline in two set-
tings: in the first, we use the standard predicted an-
notations (for POS, parses, NER, and speaker tags)
provided with the CoNLL data, and in the second,
we use the automated Stanford CoreNLP pipeline
to predict this information. On both the develop-
ment and test sets, we gain about 1 point in MUC
F1 as well as a smaller improvement in B3. Closer
inspection indicates that our system increases pre-
cision primarily due to mention pruning and NEL
constraints. Due to the differences in mention anno-
tation guidelines between ACE and CoNLL, perfor-
mance on ACE benefits more from improved men-
tion detection from NEL. Moreover, the ACE cor-
pus is all newswire text, which contains more enti-
ties that can benefit from linking. CoNLL, on the
other hand, contains a wider variety of texts, some
of which do not mention many named entities in
Wikipedia.
To examine the performance of our system on the
different domains covered by the CoNLL data, we
also test our system on each domain separately (Ta-
ble 3). We found NEL provided the biggest im-
provement for the news domains, broadcast news
(BN) and newswire (NW). These domains espe-
cially benefit from the improved mention detection
and pruning provided by NEL, and strong linking
benefitted both precision and recall in these do-
mains. We found that the magazine (MZ) section
of the corpus benefited the least from NEL, as there
were relatively few entities that our NEL systems
were able to connect to Wikipedia.
5.2 Coreference Results with Gold Linking
Some of the errors introduced in our system are due
to incorrect or incomplete links discovered by the
automatic linking system. To assess the effect of
NEL performance on NECO, we tested on a por-
tion of ACE????-NWIRE dataset for which we hand-
labeled correct links for the gold and predicted men-
tions. ?NECO + Gold NEL? denotes a version of our
system which uses gold links instead of those pre-
dicted by NEL. As shown in Table 4, gold linking
significantly improves the performance of our sys-
tem across all measures. This suggests that further
work to improve automatic NEL may have substan-
tial reward.
Gold linking improves precision for two main rea-
295
Method MUC B3 Pairwise
P R F1 P R F1 P R F1
Gold Mentions
NECO + Gold NEL 85.8 75.5 80.3 91.4 81.2 86.0 89.1 68.0 77.1
NECO 84.6 74.0 78.9 90.5 80.4 85.2 83.9 66.0 73.9
Stanford Sieves 84.5 72.2 77.8 89.9 77.7 83.4 89.9 57.3 68.1
Predicted Mentions
NECO + Gold NEL 56.4 58.8 57.5 78.2 78.3 78.3 68.0 54.3 60.4
NECO 51.3 53.5 52.4 76.5 76.4 76.5 61.2 45.6 52.2
Stanford Sieves 43.9 46.4 45.1 74.4 74.2 74.3 51.3 36.1 42.4
Table 4: Coreference results on ACE????-NWIRE-NEL with gold and predicted mentions and gold or automatic linking.
Method MUC B3 Pairwise
P R F1 P R F1 P R F1
NECO 85.0 76.6 80.6 87.6 76.4 81.6 79.3 56.1 65.8
Stanford Sieves 84.6 75.1 79.6 87.3 74.1 80.2 79.4 50.1 61.4
Haghighi and Klein (2009) 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7
Poon and Domingos (2008) 71.3 70.5 70.9 - - - 62.6 38.9 48.0
Finkel and Manning (2008) 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9
Table 5: Coreference results on ACE????-NWIRE with gold mentions and automatic linking.
sons. First, it reduces the coreference errors caused
by incorrect NEL links. For instance, gold link-
ing replaces the erroneous link generated by our
NEL systems for ?Nasser al-Kidwa? to the correct
Wikipedia entity. As another example, two men-
tions of ?Rutgers? will not be merged if one links
to the university and the other links to their football
team. Second, gold linking leads to better mention
detection and better linked mentions. For instance,
under gold linking, the whole mention, ?The gover-
nor of Alaska, Sarah Palin,? is linked to the politi-
cian, while automatic linking systems only link the
substring containing her name, ?Sarah Palin.? Still,
gold NEL cannot compensate for all coreference er-
rors in cases of generic or unlinked entities.
5.3 Coreference Results with Gold Mentions
Many of the previous papers evaluate coreference
resolution assuming gold mentions so we also run
under that condition (Table 5) using ACE????-
NWIRE data. As the table shows, with gold mentions
our system outperforms Haghighi and Klein (2009),
Poon and Domingos (2008), Finkel and Man-
ning (2008) and the Stanford sieve algorithm across
all metrics. Our method shows a relatively smaller
gain in precision, because this condition adds no
benefit to our technique of using NEL information
for pruning mentions.
5.4 Improving Named Entity Linking
While our previous experiments show that named-
entity linking can improve coreference resolution,
we now address the question of whether coreference
techniques can help NEL. We compare NECO with
a baseline ensemble4 composed of GLOW (Ratinov
et al, 2011) and WikipediaMiner (Milne and Witten,
2008) on our ACE????-NWIRE-NEL dataset (Table
6). Our system gains about 8% in absolute recall
and 5% in absolute precision. For instance, our sys-
tem correctly adds links from ?Bullock? to the en-
tity Sandra Bullock because coreference reso-
lution merges two mentions. In another example, it
correctly links ?company? to Nokia. Overall, there
is a 21% relative reduction in F1 error.
4We take the union of all the links returned by GLOW and
WikipediaMiner, but if they link a mention to two different en-
tities, we use only the output of WikipediaMiner.
296
Method F1 Precision Recall
NECO 70.6 72.0 69.2
Baseline NEL 64.4 67.4 61.7
Table 6: NEL performance of our system and the ensem-
ble baseline linker on ACE????-NWIRE-NEL.
5.5 Error Analysis
We analyzed 90 precision and recall errors and
present our findings in Table 7. Spurious mentions
accounted for the majority of non-semantic errors.
Despite the improvements that come from NEL, a
large portion of coreference errors can still be at-
tributed to incomplete semantic information, includ-
ing precision errors caused by incorrect linking. For
instance, the mention ?Disney? sometimes refers to
the company, and other times refers to the amuse-
ment park; however, the NEL systems we used had
difficulty disambiguating these cases, and NECO of-
ten incorrectly merges such mentions. Overly gen-
eral fine-grained attributes caused precision errors in
cases where many proper noun mentions were po-
tential antecedents for a common noun. Although
attributes such as country are useful for resolving a
generic ?country? mention, this information is insuf-
ficient when two distinct mentions such as ?China?
and ?Russia? both have the country attribute.
However, many recall errors are also caused by
the lack of fine-grained attributes. Finding the ideal
set of fine-grained attributes remains an open prob-
lem.
6 Related Work
Coreference resolution has a fifty year history which
defies brief summarization; see Ng (2010) for a
recent survey. Section 2.1 described the Stanford
multi-pass sieve algorithm, which is the foundation
for NECO.
Earlier coreference resolution systems used shal-
low semantics and pioneered knowledge extraction
from online encyclopedias (Ponzetto and Strube,
2006; Daume? III and Marcu, 2005; Ng, 2007). Some
recent work shows improvement in coreference res-
olution by incorporating semantic information from
Web-scale structured knowledge bases. Haghighi
and Klein (2009) use a rule-based system to extract
fine-grained attributes for mentions by analyzing
precise constructs (e.g., appositives) in Wikipedia
articles. Subsequently, Haghighi and Klein (2010)
used a generative approach to learn entity types from
an initial list of unambiguous mention types. Bansal
and Klein (2012) use statistical analysis of Web n-
gram features including lexical relations.
Rahman and Ng (2011) use YAGO to extract type
relations for all mentions. These methods incor-
porate knowledge about all possible meanings of a
mention. If a mention has multiple meanings, ex-
traneous information might be associated with it.
Zheng et al (2013) use a ranked list of candidate en-
tities for each mention and maintain the ranked list
when mentions are merged. Unlike previous work,
our method relies on NEL systems to disambiguate
possible meanings of a mention and capture high-
precision semantic knowledge from Wikipedia cate-
gories and Freebase notable types.
Ratinov and Roth (2012) investigated using NEL
to improve coreference resolution, but did not con-
sider a joint approach. They extracted attributes
from Wikipedia categories and used them as fea-
tures in a learned mention-pair model, but did not
do mention detection. Unfortunately, it is difficult
to compare directly to the results of both systems,
since they reported results on portions of ACE and
CoNLL datasets using gold mentions. However,
our approach provides independent evidence for the
benefit of NEL, and joint modeling in particular,
since it outperforms the state-of-the-art Stanford
sieve system (winner of the CoNLL 2011 shared
task (Pradhan et al, 2011)) and other recent com-
parable approaches on benchmark datasets.
Our work also builds on a long trajectory of
work in named entity resolution stemming from
SemTag (Dill et al, 2003). Section 2.2 discussed
GLOW and WikipediaMiner (Ratinov et al, 2011;
Milne and Witten, 2008). Kulkarni et al (2009)
present an elegant collective disambiguation model,
but do not exploit the syntactic nuances gleaned by
within-document coreference resolution. Hachey et
al. (2013) provide an insightful summary and evalu-
ation of different approaches to NEL.
7 Conclusions
Observing that existing coreference resolution and
named-entity linking have complementary strengths
297
Error Type Percentage Example
Extra mentions 31.1 The other thing Paula really important is that they talk a lot about the
fact ...
Pronoun 27.7 However , [all 3 women gymnasts , taking part in the internationals for
the first time], performed well , because they had strong events and their
movements had difficulty .
Contextual
semantic
16.6 [The Chinese side] hopes that each party concerned continues to make
constructive efforts to ...Considering the requirements of the Korean side
, ... the Chinese government decided to ...
NEL semantic 13.3 The most important thing about Disney is that it is a global brand. ... The
subway to Disney has already been constructed.
Attributes 11.1 The Hong Kong government turned over to Disney Corporation [200
hectares of land ...]. ... this area has become a prohibited zone in Hong
Kong.
Table 7: Examples of different error categories and the relative frequency of each. For every example, the mention to
be resolved is underlined, and the correct antecedent is italicized. For precision errors, the wrongly merged mention
is bolded. For recall errors, the missed mention is surrounded by [brackets].
and weaknesses, we present a joint approach. We
introduce NECO, a novel algorithm which solves
the problems jointly, demonstrating improved per-
formance on both tasks.
We envision several ways to improve the joint
model. While the current implementation of NECO
only introduces NEL once, we could also integrate
predictions with different levels of confidence into
different sieves. It would be interesting to more
tightly integrate the NEL system so it operates on
clusters rather than individual mentions ? after
each sieve merges an unlinked cluster, the algorithm
would retry NEL with the new context information.
NECO uses a relatively modest number of Freebase
attributes. While using more semantic knowledge
holds the promise of increased recall, the challenge
is maintaining precision. Finally, we would also like
to explore the extent to which a joint probabilistic
model (e.g., (Durrett and Klein, 2013)) might be
used to learn how to best make this tradeoff.
8 Acknowledgements
The research was supported in part by grants
from DARPA under the DEFT program through
the AFRL (FA8750-13-2-0019) and the CSSG
(N11AP20020), the ONR (N00014-12-1-0211), and
the NSF (IIS-1115966). Support was also provided
by a gift from Google, an NSF Graduate Research
Fellowship, and the WRF / TJ Cable Professor-
ship. The authors thank Greg Durrett, Heeyoung
Lee, Mitchell Koch, Xiao Ling, Mark Yatskar, Ken-
ton Lee, Eunsol Choi, Gabriel Schubiner, Nicholas
FitzGerald, Tom Kwiatkowski, and the anonymous
reviewers for helpful comments and feedback on the
work.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In International Confer-
ence on Language Resources and Evaluation Work-
shop on Linguistics Coreference.
Mohit Bansal and Dan Klein. 2012. Coreference se-
mantics from web features. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing.
Stephen Dill, Nadav Eiron, David Gibson, Daniel Gruhl,
R. Guha, Anant Jhingran, Tapas Kanungo, Sridhar Ra-
jagopalan, Andrew Tomkins, John A. Tomlin, and Ja-
son Y. Zien. 2003. SemTag and Seeker: bootstrapping
the semantic web via automated semantic annotation.
In Proceedings of the 12th International Conference
on World Wide Web.
298
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies.
Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-
nibal, and James R. Curran. 2013. Evaluating entity
linking with Wikipedia. Artificial Intelligence Jour-
nal, 194.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Hu-
man Language Technologies: Annual Conference of
the North American Chapter of the Association for
Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation of
Wikipedia entities in Web text. In Proceedings of the
2009 Conference on Knowledge Discovery and Data
Mining.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings of
the Conference on Computational Natural Language
Learning.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4).
Dan Milne and Ian H. Witten. 2008. Learning to link
with Wikipedia. In Proceedings of the ACM Confer-
ence on Information and Knowledge Management.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics.
NIST. 2004. The ACE 2004 evaluation planXPToolkit
architecture.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, Wordnet and
Wikipedia for coreference resolution. In Proceedings
of the North American Association for Natural Lan-
guage Processing on Human Language Technologies.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies.
Lev Ratinov and Dan Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to Wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics.
Marta Recasens and Eduard Hovy. 2010. Coreference
resolution across corpora: languages, coding schemes,
and preprocessing information. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of the 6th conference on Message Understanding.
Jiaping Zheng, Luke Vilnis, Sameer Singh, Jinho D.
Choi, and Andrew McCallum. 2013. Dynamic
knowledge-base alignment for coreference resolution.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning.
299
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417?1421,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatic Idiom Identification in Wiktionary
Grace Muzny and Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{muznyg,lsz}@cs.washington.edu
Abstract
Online resources, such as Wiktionary, provide
an accurate but incomplete source of idiomatic
phrases. In this paper, we study the problem
of automatically identifying idiomatic dictio-
nary entries with such resources. We train
an idiom classifier on a newly gathered cor-
pus of over 60,000 Wiktionary multi-word
definitions, incorporating features that model
whether phrase meanings are constructed
compositionally. Experiments demonstrate
that the learned classifier can provide high
quality idiom labels, more than doubling the
number of idiomatic entries from 7,764 to
18,155 at precision levels of over 65%. These
gains also translate to idiom detection in sen-
tences, by simply using known word sense
disambiguation algorithms to match phrases
to their definitions. In a set of Wiktionary def-
inition example sentences, the more complete
set of idioms boosts detection recall by over
28 percentage points.
1 Introduction
Idiomatic language is common and provides unique
challenges for language understanding systems. For
example, a diamond in the rough can be the literal
unpolished object or a crude but lovable person. Un-
derstanding such distinctions is important for many
applications, including parsing (Sag et al, 2002) and
machine translation (Shutova et al, 2012).
We use Wiktionary as a large, but incomplete, ref-
erence for idiomatic entries; individual entries can
be marked as idiomatic but, in practice, most are
not. Using these incomplete annotations as super-
vision, we train a binary Perceptron classifier for
identifying idiomatic dictionary entries. We intro-
duce new lexical and graph-based features that use
WordNet and Wiktionary to compute semantic re-
latedness. This allows us to learn, for example, that
the words in the phrase diamond in the rough are
more closely related to the words in its literal defi-
nition than the idiomatic one. Experiments demon-
strate that the classifier achieves precision of over
65% at recall over 52% and that, when used to fill in
missing Wiktionary idiom labels, it more than dou-
bles the number of idioms from 7,764 to 18,155.
These gains also translate to idiom detection in
sentences, by simply using the Lesk word sense
disambiguation (WSD) algorithm (1986) to match
phrases to their definitions. This approach allows
for scalable detection with no restrictions on the syn-
tactic structure or context of the target phrase. In a
set of Wiktionary definition example sentences, the
more complete set of idioms boosts detection recall
by over 28 percentage points.
2 Related Work
To the best of our knowledge, this work represents
the first attempt to identify dictionary entries as id-
iomatic and the first to reduce idiom detection to
identification via a dictionary.
Previous idiom detection systems fall in one
of two paradigms: phrase classification, where a
phrase p is always idiomatic or literal, e.g. (Gedigian
et al, 2006; Shutova et al, 2010), or token classifi-
cation, where each occurrence of a phrase p can be
idiomatic or literal, e.g. (Katz and Giesbrecht, 2006;
1417
Birke and Sarkar, 2006; Li and Sporleder, 2009).
Most previous idiom detection systems have focused
on specific syntactic constructions. For instance,
Shutova et al (2010) consider subject/verb (cam-
paign surged) and verb/direct-object idioms (stir ex-
citement) while Fazly and Stevenson (2006), Cook
et al (2007), and Diab and Bhutada (2009) de-
tect verb/noun idioms (blow smoke). Fothergill and
Baldwin (2012) are syntactically unconstrained, but
only study Japanese idioms. Although we focus on
identifying idiomatic dictionary entries, one advan-
tage of our approach is that it enables syntactically
unconstrained token-level detection for any phrase
in the dictionary.
3 Formal Problem Definitions
Identification For identification, we assume data
of the form {(?pi, di?, yi) : i = 1 . . . n} where
pi is the phrase associated with definition di and
yi ? {literal, idiomatic}. For example, this would
include both the literal pair ? ?leave for dead?, ?To
abandon a person or other living creature that is in-
jured or otherwise incapacitated, assuming that the
death of the one abandoned will soon follow.?? and
the idiomatic pair ? ?leave for dead?, ?To disregard
or bypass as unimportant.? ?. Given ?pi, di?, we aim
to predict yi.
Detection To evaluate identification in the con-
text of detection, we assume data {(?pi, ei?, yi) :
i = 1 . . . n}. Here, pi is the phrase in exam-
ple sentence ei whose idiomatic status is labeled
yi ? {idiomatic, literal}. One such idiomatic pair
is ??heart to heart?, ?They sat down and had a
long overdue heart to heart about the future of their
relationship.??. Given ?pi, ei?, we again aim to pre-
dict yi.
4 Data
We gathered phrases, definitions, and example sen-
tences from the English-language Wiktionary dump
from November 13th, 2012.1
Identification Phrase, definition pairs ?p, d? were
gathered with the following restrictions: the title of
the Wiktionary entry must be English, p must com-
posed of two or more words w, and ?p, d?must be in
1We used the Java Wiktionary Library (Zesch et al, 2008).
Data Set Literal Idiomatic Total
All 56,037 7,764 63,801
Train 47,633 6,600 54,233
Unannotated Dev 2,801 388 3,189
Annotated Dev 2,212 958 3,170
Unannotated Test 5,603 776 6,379
Annotated Test 4,510 1,834 6,344
Figure 1: Number of dictionary entries with each class
for the Wiktionary identification data.
Data Set Literal Idiomatic Total
Dev 171 330 501
Test 360 695 1055
Figure 2: Number of sentences of each class for the Wik-
tionary detection data.
its base form?senses that are not defined as a dif-
ferent tense of a phrase?e.g. the pair ? ?weapons of
mass destruction?, ?Plural form of weapon of mass
destruction? ?was removed while the pair ? ?weapon
of mass destruction?, ?A chemical, biological, radio-
logical, nuclear or other weapon that ... ?? was kept.
Each pair ?p, d? was assigned label y according
to the idiom labels in Wiktionary, producing the
Train, Unannotated Dev, and Unannotated Test data
sets. In practice, this produces a noisy assignment
because a majority of the idiomatic senses are not
marked. The development and test sets were anno-
tated to correct these potential omissions. Annota-
tors used the definition of an idiom as a ?phrase with
a non-compositional meaning? to produce the An-
notated Dev and Annotated Test data sets. Figure 1
presents the data statistics.
We measured inter-annotator agreement on 1,000
examples. Two annotators marked each dictionary
entry as literal, idiomatic, or indeterminable. Less
than one half of one percent could not be deter-
mined2?the computed kappa was 81.85. Given
this high level of agreement, the rest of the data
were only labeled by a single annotator, follow-
ing the methodology used with the VNC-Tokens
Dataset (Cook et al, 2008).
Detection For detection, we gathered the example
sentences provided, when available, for each defi-
nition used in our annotated identification data sets.
These sentences provide a clean source of develop-
2The indeterminable pairs were omitted from the data.
1418
ment and test data containing idiomatic and literal
phrase usages. In all, there were over 1,300 unique
phrases, half of which had more than one possible
dictionary definition in Wiktionary. Figure 2 pro-
vides the complete statistics.
5 Identification Model
For identification, we use a linear model that pre-
dicts class y? ? {literal, idiomatic} for an input pair
?p, d? with phrase p and definition d. We assign the
class:
y? = argmax
y
? ? ?(p, d, y)
given features ?(p, d, y) ? Rn with associated pa-
rameters ? ? Rn.
Learning In this work, we use the averaged Per-
ceptron algorithm (Freund and Schapire, 1999) to
perform learning, which was optimized in terms of
iterations T , bounded by range [1, 100], by maxi-
mizing F-measure on the development set.
The models described correspond to the features
they use. All models are trained on the same, unan-
notated training data.
Features The features that were developed fall
into two categories: lexical and graph-based fea-
tures. The lexical features were motivated by the
intuition that literal phrases are more likely to have
closely related words in d to those in p because lit-
eral phrases do not break the principle of compo-
sitionality. All words compared are stemmed ver-
sions. Let count(w, t) = number of times word w
appears in text t.
? synonym overlap: Let S be the set of syn-
onyms as defined in Wiktionary for all words
in p. Then, we define the synonym overlap =
1
|S|
?
s?S count(s, d).
? antonym overlap: Let A be the set of antonyms
as defined in Wiktionary for all words in
p. Then, we define the antonym overlap =
1
|A|
?
a?A count(a, d).
? average number of capitals:3 The value of
number of capital letters in p
number of words in p .
3In practice, this feature identifies most proper nouns.
Graph-based features use the graph structure of
WordNet 3.0 to calculate path distances. Let
distance(w, v, rel, n) be the minimum distance via
links of type rel in WordNet from a word w to a
word v, up to a threshold max integer value n, and 0
otherwise. The features compute:
? closest synonym:
min
w?p,v?d
distance(w, v, synonym, 5)
? closest antonym:4
min
w?p,v?d
distance(w, v, antonym, 5)
? average synonym distance:
1
|p|
?
w?p,v?d
distance(w, v, synonym, 5)
? average hyponym:
1
|p|
?
w?p,v?d
distance(w, v, hyponym, 5)
? synsets connected by an antonym: This feature in-
dicates whether the following is true. The set of
synsets Synp, all synsets from all words in p, and
the set of synsets Synd, all synsets from all words
in d, are connected by a shared antonym. This fea-
ture follows an approach described by Budanitsky
et al (2006).
6 Experiments
We report identification and detection results, vary-
ing the data labeling and choice of feature sets.
6.1 Identification
Random Baseline We use a proportionally ran-
dom baseline for the identification task that classi-
fies according to the proportion of literal definitions
seen in the training data.
Results Figure 3 provides the results for the base-
line, the full approach, and variations with subsets
of the features. Results are reported for the origi-
nal, unannotated test set, and the same test examples
with corrected idiom labels. All models increased
4The first relation expanded was the antonym relation. All
subsequent expansions were via synonym relations.
1419
Data Set Model Rec. Prec. F1
Unannotated Lexical 85.8 21.9 34.9
Graph 62.4 26.6 37.3
Lexical+Graph 70.5 28.1 40.1
Baseline 12.2 11.9 12.0
Annotated Lexical 81.2 49.3 61.4
Graph 64.3 51.3 57.1
Lexical+Graph 75.0 52.9 62.0
Baseline 29.5 12.5 17.6
Figure 3: Results for idiomatic definition identification.
Figure 4: Precision and recall with varied features on the
annotated test set.
over their corresponding baselines by more than 22
points and both feature families contributed.5
Figure 4 shows the complete precision, recall
curve. We selected our operating point to optimize
F-measure, but we see that the graph features per-
form well across all recall levels and that adding the
lexical features provides consistent improvement in
precision. However, other points are possible, es-
pecially when aiming for high precision to extend
the labels in Wiktionary. For example, the original
7,764 entries can be extended to 18,155 at 65% pre-
cision, 9,594 at 80%, or 27,779 at 52.9%.
Finally, Figures 5 and 6 present qualitative results,
including newly discovered idioms and high scoring
false identifications. Analysis reveals where our sys-
tem has room to improve?errors most often occur
with phrases that are specific to a certain field, such
5We also ran ablations demonstrating that removing each
feature from the Lexical+Graph model hurt performance, but
omit the detailed results for space.
Phrase Definition
feel free You have my permission.
live down To get used to something shameful.
nail down To make something
(e.g. a decision or plan) firm or certain.
make after To chase.
get out To say something with difficulty.
good riddance A welcome departure.
to bad rubbish
as all hell To a great extent or degree; very.
roll around To happen, occur, take place.
Figure 5: Newly discovered idioms.
Phrase Definition
put asunder To sunder; disjoin; separate;
disunite; divorce; annul; dissolve.
add up To take a sum.
peel off To remove (an outer layer or
covering, such as clothing).
straighten up To become straight, or straighter.
wild potato The edible root of this plant.
shallow embedding The act of representing one logic
or language with another by
providing a syntactic translation.
Figure 6: High scoring false identifications.
as sports or mathematics, and with phrases whose
words also appear in their definitions.
6.2 Detection
Approach We use the Lesk (1986) algorithm to
perform WSD, matching an input phrase p from sen-
tence e to the definition d in Wiktionary that defines
the sense p is being used in. The final classification y
is then assigned to ?p, d? by the identification model.
Results Figure 7 shows detection results. The
baseline for this experiment is a model that assigns
the default labels within Wiktionary to the disam-
biguated definition. The Annotated model is the
Lexical+Graph model shown in Figure 3 evaluated
on the annotated data. The +Default setting aug-
ments the identification model by labeling the ?p, e?
as idiomatic if either the model or the original label
within Wiktionary identifies it as such.
7 Conclusions
We presented a supervised approach to classifying
definitions as idiomatic or literal that more than dou-
1420
Model Rec. Prec. F1
Default 60.5 1 75.4
Annotated 78.3 76.7 77.5
Annotated+Default 89.2 79.0 83.8
Figure 7: Detection results.
bles the number of marked idioms in Wiktionary,
even when training on incomplete data. When com-
bined with the Lesk word sense algorithm, this ap-
proach provides a complete idiom detector for any
phrase in the dictionary.
We expect that semi-supervised learning tech-
niques could better recover the missing labels and
boost overall performance. We also think it should
be possible to scale the detection approach, perhaps
with automatic dictionary definition discovery, and
evaluate it on more varied sentence types.
Acknowledgments
The research was supported in part by the Na-
tional Science Foundation (IIS-1115966) and a
Mary Gates Research Scholarship. The authors
thank Nicholas FitzGerald, Sarah Vieweg, and Mark
Yatskar for helpful discussions and feedback.
References
J. Birke and A. Sarkar. 2006. A clustering approach
for nearly unsupervised recognition of nonliteral lan-
guage. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics.
A. Budanitsky and G. Hirst. 2006. Evaluating wordnet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32(1):13?47.
P. Cook, A. Fazly, and S. Stevenson. 2007. Pulling their
weight: Exploiting syntactic forms for the automatic
identification of idiomatic expressions in context. In
Proceedings of the workshop on a broader perspective
on multiword expressions.
P. Cook, A. Fazly, and S. Stevenson. 2008. The
vnc-tokens dataset. In Proceedings of the Language
Resources and Evaluation Conference Workshop To-
wards a Shared Task for Multiword Expressions.
M. Diab and P. Bhutada. 2009. Verb noun construction
mwe token supervised classification. In Proceedings
of the Workshop on Multiword Expressions: Identifica-
tion, Interpretation, Disambiguation and Applications.
A. Fazly and S. Stevenson. 2006. Automatically con-
structing a lexicon of verb phrase idiomatic combina-
tions. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics.
R. Fothergill and T. Baldwin. 2012. Combining re-
sources for mwe-token classification. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
learning, 37(3):277?296.
M. Gedigian, J. Bryant, S. Narayanan, and B. Ciric.
2006. Catching metaphors. In Proceedings of the
Third Workshop on Scalable Natural Language Un-
derstanding.
G. Katz and E. Giesbrecht. 2006. Automatic identi-
fication of non-compositional multi-word expressions
using latent semantic analysis. In Proceedings of the
Workshop on Multiword Expressions: Identifying and
Exploiting Underlying Properties.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone
from an ice cream cone. In Proceedings of Special
Interest Group on the Design of Communication.
L. Li and C. Sporleder. 2009. Classifier combination for
contextual idiom detection without labelled data. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2002. Multiword expressions: A pain
in the neck for nlp. In Computational Linguistics and
Intelligent Text Processing. Springer.
E. Shutova, L. Sun, and A. Korhonen. 2010. Metaphor
identification using verb and noun clustering. In Pro-
ceedings of the International Conference on Computa-
tional Linguistics.
E. Shutova, S. Teufel, and A. Korhonen. 2012. Statisti-
cal metaphor processing. Computational Linguistics,
39(2):301?353.
T. Zesch, C. Mu?ller, and I. Gurevych. 2008. Extracting
lexical semantic knowledge from wikipedia and wik-
tionary. In Proceedings of the International Confer-
ence on Language Resources and Evaluation.
1421
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545?1556,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Scaling Semantic Parsers with On-the-fly Ontology Matching
Tom Kwiatkowski Eunsol Choi Yoav Artzi Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{tomk,eunsol,yoav,lsz}@cs.washington.edu
Abstract
We consider the challenge of learning seman-
tic parsers that scale to large, open-domain
problems, such as question answering with
Freebase. In such settings, the sentences cover
a wide variety of topics and include many
phrases whose meaning is difficult to rep-
resent in a fixed target ontology. For ex-
ample, even simple phrases such as ?daugh-
ter? and ?number of people living in? can-
not be directly represented in Freebase, whose
ontology instead encodes facts about gen-
der, parenthood, and population. In this pa-
per, we introduce a new semantic parsing ap-
proach that learns to resolve such ontologi-
cal mismatches. The parser is learned from
question-answer pairs, uses a probabilistic
CCG to build linguistically motivated logical-
form meaning representations, and includes
an ontology matching model that adapts the
output logical forms for each target ontology.
Experiments demonstrate state-of-the-art per-
formance on two benchmark semantic parsing
datasets, including a nine point accuracy im-
provement on a recent Freebase QA corpus.
1 Introduction
Semantic parsers map sentences to formal represen-
tations of their underlying meaning. Recently, al-
gorithms have been developed to learn such parsers
for many applications, including question answering
(QA) (Kwiatkowski et al, 2011; Liang et al, 2011),
relation extraction (Krishnamurthy and Mitchell,
2012), robot control (Matuszek et al, 2012; Kr-
ishnamurthy and Kollar, 2013), interpreting instruc-
tions (Chen and Mooney, 2011; Artzi and Zettle-
moyer, 2013), and generating programs (Kushman
and Barzilay, 2013).
In each case, the parser uses a predefined set
of logical constants, or an ontology, to construct
meaning representations. In practice, the choice
of ontology significantly impacts learning. For
example, consider the following questions (Q) and
candidate meaning representations (MR):
Q1: What is the population of Seattle?
Q2: How many people live in Seattle?
MR1: ?x.population(Seattle, x)
MR2: count(?x.person(x) ? live(x, Seattle))
A semantic parser might aim to construct MR1 for
Q1 and MR2 for Q2; these pairings align constants
(count, person, etc.) directly to phrases (?How
many,? ?people,? etc.). Unfortunately, few ontologies
have sufficient coverage to support both meaning
representations, for example many QA databases
would only include the population relation required
for MR1. Most existing approaches would, given
this deficiency, simply aim to produce MR1 for Q2,
thereby introducing significant lexical ambiguity
that complicates learning. Such ontological mis-
matches become increasingly common as domain
and language complexity increases.
In this paper, we introduce a semantic parsing ap-
proach that supports scalable, open-domain ontolog-
ical reasoning. The parser first constructs a linguis-
tically motivated domain-independent meaning rep-
resentation. For example, possibly producing MR1
for Q1 and MR2 for Q2 above. It then uses a learned
ontology matching model to transform this represen-
1545
x : How many people visit the public library of New York annually
l0 : ?x.eq(x, count(?y.people(y) ? ?e.visit(y, ?z.public(z) ? library(z) ? of(z, new york), e) ? annually(e)))
y : ?x.library.public library system.annual visits(x, new york public library)
a : 13,554,002
x : What works did Mozart dedicate to Joseph Haydn
l0 : ?x.works(x) ? ?e.dedicate(mozart, x, e) ? to(haydn, e)))
y : ?x.dedicated work(x) ? ?e.dedicated by(mozart, e) ? dedication(x, e) ? dedicated to(haydn, e)))
a : { String Quartet No. 19, Haydn Quartets, String Quartet No. 16, String Quartet No. 18, String Quartet No. 17 }
Figure 1: Examples of sentences x, domain-independent underspecified logical forms l0, fully specified
logical forms y, and answers a drawn from the Freebase domain.
tation for the target domain. In our example, pro-
ducing either MR1, MR2 or another more appropri-
ate option, depending on the QA database schema.
This two stage approach enables parsing without
any domain-dependent lexicon that pairs words with
logical constants. Instead, word meaning is filled
in on-the-fly through ontology matching, enabling
the parser to infer the meaning of previously un-
seen words and more easily transfer across domains.
Figure 1 shows the desired outputs for two example
Freebase sentences.
The first parsing stage uses a probabilistic combi-
natory categorial grammar (CCG) (Steedman, 2000;
Clark and Curran, 2007) to map sentences to
new, underspecified logical-form meaning represen-
tations containing generic logical constants that are
not tied to any specific ontology. This approach en-
ables us to share grammar structure across domains,
instead of repeatedly re-learning different grammars
for each target ontology. The ontology-matching
step considers a large number of type-equivalent
domain-specific meanings. It enables us to incorpo-
rate a number of cues, including the target ontology
structure and lexical similarity between the names of
the domain-independent and dependent constants, to
construct the final logical forms.
During learning, we estimate a linear model over
derivations that include all of the CCG parsing de-
cisions and the choices for ontology matching. Fol-
lowing a number of recent approaches (Clarke et al,
2010; Liang et al, 2011), we treat all intermediate
decisions as latent and learn from data containing
only easily gathered question answer pairs. This ap-
proach aligns naturally with our two-stage parsing
setup, where the final logical expression can be di-
rectly used to provide answers.
We report performance on two benchmark
datasets: GeoQuery (Zelle and Mooney, 1996) and
Freebase QA (FQ) (Cai and Yates, 2013a). Geo-
Query includes a geography database with a small
ontology and questions with relatively complex,
compositional structure. FQ includes questions to
Freebase, a large community-authored database that
spans many sub-domains. Experiments demonstrate
state-of-the-art performance in both cases, including
a nine point improvement in recall for the FQ test.
2 Formal Overview
Task Let an ontology O be a set of logical con-
stants and a knowledge base K be a collection of
logical statements constructed with constants from
O. For example, K could be facts in Freebase (Bol-
lacker et al, 2008) and O would define the set
of entities and relation types used to encode those
facts. Also, let y be a logical expression that can
be executed against K to return an answer a =
EXEC(y,K). Figure 1 shows example queries and
answers for Freebase. Our goal is to build a function
y = PARSE(x,O) for mapping a natural language
sentence x to a domain-dependent logical form y.
Parsing We use a two-stage approach to define
the space of possible parses GEN(x,O) (Section 5).
First, we use a CCG and word-class information
from Wiktionary1 to build domain-independent un-
derspecified logical forms, which closely mirror the
linguistic structure of the sentence but do not use
constants from O. For example, in Figure 1, l0 de-
notes the underspecified logical forms paired with
each sentence x. The parser then maps this interme-
diate representation to a logical form that uses con-
stants from O, such as the y seen in Figure 1.
1www.wiktionary.com
1546
Learning We assume access to data containing
question-answer pairs {(xi, ai) : i = 1 . . . n} and
a corresponding knowledge base K. The learn-
ing algorithm (Section 7.1) estimates the parame-
ters of a linear model for ranking the possible en-
tires in GEN(x,O). Unlike much previous work
(e.g., Zettlemoyer and Collins (2005)), we do not
induce a CCG lexicon. The lexicon is open domain,
using no symbols from the ontology O for K. This
allows us to write a single set of lexical templates
that are reused in every domain (Section 5.1). The
burden of learning word meaning is shifted to the
second, ontology matching, stage of parsing (Sec-
tion 5.2), and modeled with a number of new fea-
tures (Section 7.2) as part of the joint model.
Evaluation We evaluate on held out question-
answer pairs in two benchmark domains, Freebase
and GeoQuery. Following Cai and Yates (2013a),
we also report a cross-domain evaluation where the
Freebase data is divided by topics such as sports,
film, and business. This condition ensures that the
test data has a large percentage of previously unseen
words, allowing us to measure the effectiveness of
the real time ontology matching.
3 Related Work
Supervised approaches for learning semantic parsers
have received significant attention, e.g. (Kate and
Mooney, 2006; Wong and Mooney, 2007; Muresan,
2011; Kwiatkowski et al, 2010, 2011, 2012; Jones
et al, 2012). However, these techniques require
training data with hand-labeled domain-specific log-
ical expressions. Recently, alternative forms of su-
pervision were introduced, including learning from
question-answer pairs (Clarke et al, 2010; Liang
et al, 2011), from conversational logs (Artzi and
Zettlemoyer, 2011), with distant supervision (Kr-
ishnamurthy and Mitchell, 2012; Cai and Yates,
2013b), and from sentences paired with system
behavior (Goldwasser and Roth, 2011; Chen and
Mooney, 2011; Artzi and Zettlemoyer, 2013). Our
work adds to these efforts by demonstrating a new
approach for learning with latent meaning represen-
tations that scales to large databases like Freebase.
Cai and Yates (2013a) present the most closely
related work. They applied schema matching tech-
niques to expand a CCG lexicon learned with the
UBL algorithm (Kwiatkowski et al, 2010). This ap-
proach was one of the first to scale to Freebase, but
required labeled logical forms and did not jointly
model semantic parsing and ontological reasoning.
This method serves as the state of the art for our
comparison in Section 9.
We build on a number of existing algorithmic
ideas, including using CCGs to build meaning rep-
resentations (Zettlemoyer and Collins, 2005, 2007;
Kwiatkowski et al, 2010, 2011), building deriva-
tions to transform the output of the CCG parser
based on context (Zettlemoyer and Collins, 2009),
and using weakly supervised margin-sensitive pa-
rameter updates (Artzi and Zettlemoyer, 2011,
2013). However, we introduce the idea of learning
an open-domain CCG semantic parser; all previous
methods suffered, to various degrees, from the onto-
logical mismatch problem that motivates our work.
The challenge of ontological mismatch has been
previously recognized in many settings. Hobbs
(1985) describes the need for ontological promiscu-
ity in general language understanding. Many pre-
vious hand-engineered natural language understand-
ing systems (Grosz et al, 1987; Alshawi, 1992; Bos,
2008) are designed to build general meaning rep-
resentations that are adapted for different domains.
Recent efforts to build natural language interfaces to
large databases, for example DBpedia (Yahya et al,
2012; Unger et al, 2012), have also used hand-
engineered ontology matching techniques. Fader
et al (2013) recently presented a scalable approach
to learning an open domain QA system, where onto-
logical mismatches are resolved with learned para-
phrases. Finally, the databases research commu-
nity has a long history of developing schema match-
ing techniques (Doan et al, 2004; Euzenat et al,
2007), which has inspired more recent work on dis-
tant supervision for relation extraction with Free-
base (Zhang et al, 2012).
4 Background
Semantic Modeling We use the typed lambda cal-
culus to build logical forms that represent the mean-
ings of words, phrases and sentences. Logical forms
contain constants, variables, lambda abstractions,
and literals. In this paper, we use the term literal to
refer to the application of a constant to a sequence of
1547
library of new york
N N\N/NP NP
?x.library(x) ?y?f?x.f(x) ? loc(x, y) NY C
>
N\N
?f.?x.f(x) ? loc(x,NY C)
<
N
?x.library(x) ? loc(x,NY C)
Figure 2: A sample CCG parse.
arguments. We include types for entities e, truth val-
ues t, numbers i, events ev, and higher-order func-
tions, such as ?e, t? and ??e, t?, e?. We use David-
sonian event semantics (Davidson, 1967) to explic-
itly represent events using event-typed variables and
conjunctive modifiers to capture thematic roles.
Combinatory Categorial Grammars (CCG)
CCGs are a linguistically-motivated formalism
for modeling a wide range of language phenom-
ena (Steedman, 1996, 2000). A CCG is defined by
a lexicon and a set of combinators. The lexicon
contains entries that pair words or phrases with
CCG categories. For example, the lexical entry
library ` N : ?x.library(x) in Figure 2 pairs
the word ?library? with the CCG category that has
syntactic category N and meaning ?x.library(x).
A CCG parse starts from assigning lexical entries to
words and phrases. These are then combined using
the set of CCG combinators to build a logical form
that captures the meaning of the entire sentence. We
use the application, composition, and coordination
combinators. Figure 2 shows an example parse.
5 Parsing Sentences to Meanings
The function GEN(x,O) defines the set of possible
derivations for an input sentence x. Each derivation
d = ??,M? builds a logical form y using constants
from the ontology O. ? is a CCG parse tree that
maps x to an underspecified logical form l0. M is an
ontological match that maps l0 onto the fully spec-
ified logical form y. This section describes, with
reference to the example in Figure 3, the operations
used by ? and M .
5.1 Domain Independent Parsing
Domain-independent CCG parse trees ? are built
using a predefined set of 56 underspecified lexi-
cal categories, 49 domain-independent lexical items,
and the combinatory rules introduced in Section 4.
An underspecified CCG lexical category has a
syntactic category and a logical form containing no
constants from the domain ontology O. Instead, the
logical form includes underspecified constants that
are typed placeholders which will later be replaced
during ontology matching. For example, a noun
might be assigned the lexical category N : ?x.p(x),
where p is an underspecified ?e, t?-type constant.
During parsing, lexical categories are created dy-
namically. We manually define a set of POS tags for
each underspecified lexical category, and use Wik-
tionary as a tag dictionary to define the possible POS
tags for words and phrases. Each phrase is assigned
every matching lexical category. For example, the
word ?visit? can be either a verb or a noun in Wik-
tionary. We accordingly assign it all underspecified
categories for the classes, including:
N :?x.p(x) , S\NP/NP :?x?y?ev.p(y, x, ev)
for nouns and transitive verbs respectively.
We also define domain-independent lexical items
for function words such as ?what,? ?when,? and
?how many,? ?and,? and ?is.? These lexi-
cal items pair a word with a lexical cate-
gory containing only domain-independent con-
stants. For example, how many ` S/(S\NP)/N :
?f.?g.?x.eq(x, count(?y.f(y) ? g(y))) contains
the function count and the predicate eq.
Figure 3a shows the lexical categories and combi-
nator applications used to construct the underspeci-
fied logical form l0. Underspecified constants in this
figure have been labeled with the words that they are
associated with for readability.
5.2 Ontological Matching
The second, domain specific, step M maps the un-
derspecified logical form l0 onto the fully specified
logical form y. The mapping from constants in l0
to constants in y is not one-to-one. For example, in
Figure 3, l0 contains 11 constants but y contains only
2. The ontological match is a sequence of matching
operations M = ?o1 . . . , on? that can transform the
structure of the logical form or replace underspeci-
fied constants with constants from O.
1548
(a) Underspecified CCG parse ?: Map words onto underspecified lexical categories as described in Section 5.1. Use
the CCG combinators to combine lexical categories to give the full underpecified logical form l0.
how many people visit the public library of new york annually
S/(S\NP )/N N S\NP/NP NP/N N/N N N\N/NP NP AP
?f.?g.?x.eq(x, count( ?x.People(x) ?x.?y.?ev. ?f.?x.f(x) ?f.?x.f(x)? ?x.Library(x) ?y.?f.?x.Of NewY ork ?ev.Annually(ev)
?y.f(y) ? g(y))) V isit(y, x, ev) Public(x) (x, y) ? f(x)
> >
<
>
>
> <
>
S
l0 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y, ?z.Public(z) ? Library(z) ? Of(z,NewY ork)) ? Annually(e)))
(b) Structure Matching Steps in M : Use the operators described in Section 5.2.1 and Figure 4 to transform l0. In
each step one of the operators is applied to a subexpression of the existing logical form to generate a modified logical
form with a new underspecified constant marked in bold.
l0 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y, ?z.Public(z) ? Library(z) ?Of(z,NewY ork), e) ?Annually(e)))
l1 : ?x.eq(x, count(?y.People(y) ? ?e.V isit(y,PublicLibraryOfNewYork, e) ?Annually(e)))
l2 : ?x.HowManyPeopleVisitAnnually(x, PublicLibraryOfNewY ork)))
(c) Constant Matching Steps in M : Replace all underspecified constants in the transformed logical form with a
similarly typed constant from O, as described in Section 5.2.2. The underspecified constant to be replaced is marked
in bold and constants from O are written in typeset.
?x.HowManyPeopleV isitAnnually(x,PublicLibraryOfNewYork)
l3 : 7? ?x.HowManyPeopleV isitAnnually(x, new york public library)
?x.HowManyPeopleVisitAnnually(x, new york public library)
y : 7? ?x.public library system.annual visits(x, new york public library)
Figure 3: Example derivation for the query ?how many people visit the public library of new york annu-
ally.? Underspecified constants are labelled with the words from the query that they are associated with for
readability. Constants from O, written in typeset, are introduced in step (c).
Operator Definition and Conditions Example
a.
Collapse
Literal
to
Constant
P (a1, . . . , an) 7? c
?z.Public(z) ? Library(z) ?Of(z,NewY ork))
7? PublicLibraryOfNewY ork
s.t. type(P (a1, . . . , an)) = type(c) Input and output have type e.
type(c) ? {e, i} e is allowed in O.
freev(P (a1, . . . , an)) = ? Input contains no free variables.
b.
Collapse
Literal
to
Literal
P (a1, . . . , an) 7? Q(b1, . . . , bm)
eq(x, count(?y.People(y) ? ?e.V isit(y,
PublicLibraryOfNewY ork) ?Annually(e)))
7? CountPeopleV isitAnnually(x,
PublicLibraryOfNewY ork)
s.t. type(P (a1, . . . , an)) = type(Q(b1, . . . , bm)) Input and output have type t.
type(Q) ? {type(c) : c ? O} New constant has type ?i, ?e, t??, allowed in O.
freev(P (a1, . . . , an)) = freev(Q(b1, . . . , bm)) Input and output contain single free variable x.
{b1, . . . , bm} ? subexps(P (a1, . . . , an)) Arguments of output literal are subexpressions of input.
c. Split
Literal
P (a1, . . . , ak, x, ak+1, . . . , an)
7? Q(b1, . . . , x, . . . bn) ?Q??(c1, . . . , x, . . . cm)
Dedicate(Mozart,Haydn, ev)
7? Dedicate(Mozart, ev) ?Dedicate??(Haydn, ev)
s.t. type(P (. . . )) = t Input has type t. This matches output type by definition.
{type(Q), type(Q??)} ? {type(c) : c ? O} New constants have allowed type ?e, ?ev, t??.
{b1, . . . , bn, c1, . . . , cm} = {a1, . . . , an} All arguments of input literal are preserved in output.
Figure 4: Definition of the operations used to transform the structure of the underspecified logical form l0 to
match the ontology O. The function type(c) calculates a constant c?s type. The function freev(lf) returns
the set of variables that are free in lf (not bound by a lambda term or quantifier). The function subexps(lf)
generates the set of all subexpressions of the lambda calculus expression lf .
1549
5.2.1 Structure Matching
Three structure matching operators, illustrated in
Figure 4, are used to collapse or expand literals in
l0. Collapses merge a subexpression from l0 to cre-
ate a new underspecified constant, generating a log-
ical form with fewer constants. Expansions split a
subexpression from l0 to generate a new logical form
containing one extra constant.
Collapsing Operators The collapsing operator
defined in Figure 4a merges all constants in a
literal to generate a single constant of the same
type. This operator is used to map ?z.Public(z)?
Library(z)?Of(z,NewY ork) to PublicLibraryOfNewY ork
in Figure 3b. Its operation is limited to entity typed
expressions that do not contain free variables.
The operator in Figure 4b, in contrast, can be used
to collapse the expression eq(x,count(?y.People(y)?
?e.V isit(y,PublicLibraryOfNewY ork,e))?Annually(e))),
which contains free variable x onto a new expression
CountPeopleV isitAnnually(x,PublicLibraryOfNewY ork).
This is only possible when the type of the newly
created constant is allowed in O and the variable x
is free in the output expression. Subsets of conjuncts
can be collapsed using the operator in Figure 4b by
creating ad-hoc conjunctions that encapsulate them.
Disjunctions are treated similarly.
Performing collapses on the underspecified logi-
cal form allows non-contiguous phrases to be rep-
resented in the collapsed form. In this exam-
ple, the logical form representing the phrase ?how
many people visit? has been merged with the logi-
cal form representing the non-adjacent adverb ?an-
nually.? This generates a new underspecified con-
stant that can be mapped onto the Freebase relation
public library system annual visits that re-
lates to both phrases.
The collapsing operations preserve semantic type,
ensuring that all logical forms generated by the
derivation sequence are well typed. The full set of
allowed collapses of l0 is given by the transitive clo-
sure of the collapsing operations. The size of this
set is limited by the number of constants in l0, since
each collapse removes at least one constant. At each
step, the number of possible collapses is polynomial
in the number of constants in l0 and exponential in
the arity of the most complex type in O. For do-
mains of interest this arity is unlikely to be high and
for triple stores such as Freebase it is 2.
Expansion Operators The fully specified logical
form y can contain constants relating to multiple
words in x. It can also use multiple constants to rep-
resent the meaning of a single word. For example,
Freebase does not contain a relation representing the
concept ?daughter?, instead using two relations rep-
resenting ?female? and ?child?. The expansion oper-
ator in Figure 4c allows a single predicate to be split
into a pair of conjoined predicates sharing an argu-
ment variable. For example, in Figure 1, the constant
for ?dedicate? is split in two to match its represen-
tation in Freebase. Underspecified constants from
l0 can be split once. For the experiments in Sec-
tion 8, we constrain the expansion operator to work
on event modifiers but the procedure generalizes to
all predicates.
5.2.2 Constant Matching
To build an executable logical form y, all under-
specified constants must be replaced with constants
from O. This is done through a sequence of con-
stant replacement operations, each of which replaces
a single underspecified constant with a constant of
the same type from O. Two example replacements
are shown in Figure 3c. The output from the last re-
placement operation is a fully specified logical form.
6 Building and Scoring Derivations
This section introduces a dynamic program used to
construct derivations and a linear scoring model.
6.1 Building Derivations
The space of derivations is too large to explicitly
enumerate. However, each logical form (both final
and interim) can be constructed with many differ-
ent derivations, and we only need to find the highest
scoring one. This allows us to develop a simple dy-
namic program for our two-stage semantic parser.
We use a CKY style chart parser to calculate the
k-best logical forms output by parses of x. We then
store each interim logical form generated by an op-
erator in M once in a hyper-graph chart structure.
The branching factor of this hypergraph is polyno-
mial in the number of constants in l0 and linear in
the size of O. Subsequently, there are too many
possible logical forms to enumerate explicitly; we
1550
prune as follows. We allow the top N scoring on-
tological matches for each original subexpression in
l0 and remove matches that differ from score from
the maximum scoring match by more than a thresh-
old ? . When building derivations, we apply constant
matching operators as soon as they are applicable to
new underspecified constants created by collapses
and expansions. This allows the scoring function
used by the pruning strategy to take advantage of all
features defined in Section 7.2.
6.2 Ranking Derivations
Given feature vector ? and weight vector ?, the score
of a derivation d = ??,M? is a linear function that
decomposes over the parse tree ? and the individual
ontology-matching steps o.
SCORE(d) = ?(d)? (1)
= ?(?)? +
?
o?M
?(o)?
The function PARSE(x,O) introduced as our goal in
Section 2 returns the logical form associated with
the highest scoring derivation of x:
PARSE(x,O) = arg max
d?GEN(x,O)
(SCORE(d))
The features and learning algorithm used to estimate
? are defined in the next section.
7 Learning
This section describes an online learning algorithm
for question-answering data, along with the domain-
independent feature set.
7.1 Learning Model Parameters
Our learning algorithm estimates the parameters ?
from a set {(xi, ai) : i = 1 . . . n} of questions xi
paired with answers ai from the knowledge base
K. Each derivation d generated by the parser is
associated with a fully specified logical form y =
YIELD(d) that can be executed in K. A derivation d
of xi is correct if EXEC(YIELD(d),K) = ai. We use
a perceptron to estimate a weight vector ? that sup-
port a separation of ? between correct and incorrect
answers. Figure 5 presents the learning algorithm.
Input: Q/A pairs {(xi, ai) : i = 1 . . . n}; Knowledge base
K; Ontology O; Function GEN(x,O) that computes deriva-
tions of x; Function YIELD(d)that returns logical form yield
of derivation d; Function EXEC(y,K) that calculates execu-
tion of y in K; Margin ?; Number of iterations T .
Output: Linear model parameters ?.
Algorithm:
For t = 1 . . . T, i = 1 . . . n :
C = {d : d ? GEN(xi,O); EXEC(YIELD(d),K) = ai}
W = {d : d ? GEN(xi,O); EXEC(YIELD(d),K) 6= ai}
C? = argmaxd?C(?(d)?)
W ? = {d : d ?W ; ?c ? C? s.t. ?(c)? ? ?(d)? < ?)}
If |C?| > 0 ? |W ?| > 0 :
? = ? + 1|C?|
?
c?C? ?(c)?
1
|W?|
?
e?W? ?(e)
Figure 5: Parameter estimation from Q/A pairs.
7.2 Features
The feature vector ?(d) introduced in Section 6.2
decomposes over each of the derivation steps in d.
CCG Parse Features Each lexical item in ? has
three indicator features. The first indicates the num-
ber of times each underspecified category is used.
For example, the parse in Figure 3a uses the under-
specified category N : ?x.p(x) twice. The second
feature indicates (word, category) pairings ? e.g.
that N : ?x.p(x) is paired with ?library? and ?pub-
lic? once each in Figure 3a. The final lexical feature
indicates (part-of-speech, category) pairings for all
parts of speech associated with the word.
Structural Features The structure matching op-
erators (Section 5.2.1) in M generate new under-
specified constants that define the types of constants
in the output logical form y. These operators are
scored using features that indicate the type of each
complex-typed constant present in y and the iden-
tity of domain-independent functional constants in
y. The logical form y generated in Figure 3 contains
one complex typed constant with type ?i, ?e, t?? and
no domain-independent functional constants. Struc-
tural features allow the model to adapt to different
knowledge bases K. They allow it to determine, for
example, whether a numeric quantity such as ?pop-
ulation? is likely to be explicitly listed in K or if it
should be computed with the count function.
Lexical Features Each constant replacement op-
erator (Section 5.2.2) in M replaces an underspec-
1551
ified constant cu with a constant cO from O. The
underspecified constant cu is associated with the se-
quence of words ~wu used in the CCG lexical en-
tries that introduced it in ?. We assume that each
of the constants cO in O is associated with a string
label ~wO. This allows us to introduce five domain-
independent features that measure the similarity of
~wu and ~wO.
The feature ?np(cu, cO) signals the replacement
of an entity-typed constant cu with entity cO that has
label ~wu. For the second example in Figure 1 this
feature indicates the replacement of the underspeci-
fied constant associated with the word ?mozart? with
the Freebase entity mozart. Stem and synonymy
features ?stem(cu, cO) and ?syn(cu, cO) signal the
existence of words wu ? ~wu and wu ? ~wO that
share a stem or synonym respectively. Stems are
computed with the Porter stemmer and synonyms
are extracted from Wiktionary. A single Freebase
specific feature ?fp:stem(cu, cO) indicates a word
stem match between wu ? ~wu and the word filling
the most specific position in ~wu under Freebase?s hi-
erarchical naming schema.
A final feature ?gl(cu, cO) calculates the overlap
between Wiktionary definitions for ~wu and ~wO. Let
gl(w) be the Wiktionary definition for w. Then:
?gl(cu, cO) =
?
wu? ~wu;wO? ~wO
2?|gl(wO)?gl(wc)|
| ~wO |?| ~wu|?|gl(wO)|+|gl(wc)|
Domain-indepedent lexical features allow the
model to reason about the meaning of unseen words.
In small domains, however, the majority of word us-
ages may be covered by training data. We make use
of this fact in the GeoQuery domain with features
?m(cu, cO) that indicate the pairing of ~wu with cO.
Knowledge Base Features Guided by the obser-
vation that we generally want to create queries y
which have answers in knowledge base K, we de-
fine features to signal whether each operation could
build a logical form y with an answer in K.
If a predicate-argument relation in y does not
exist in K, then the execution of y against K
will not return an answer. Two features indicate
whether predicate-argument relations in y exist inK.
?direct(y,K) indicates predicate-argument applica-
tions in y that exists in K. For example, if the appli-
cation of dedicated by to mozart in Figure 1 ex-
ists in Freebase, ?direct(y,K) will fire. ?join(y,K)
indicates entities separated from a predicate by one
join in y, such as mozart and dedicated to in Fig-
ure 1, that exist in the same relationship in K.
If two predicates that share a variable in y
do not share an argument in that position in K
then the execution of y against K will fail. The
predicate-predicate ?pp(y,K) feature indicates pairs
of predicates that share a variable in y but can-
not occur in this relationship in K. For ex-
ample, since the subject of the Freebase prop-
erty date of birth does not take arguments of
type location, ?pp(y,K) will fire if y con-
tains the logical form ?x?y.date of birth(x, y)?
location(x).
Both the predicate-argument and predicate-
predicate features operate on subexpressions of y.
We also define the execution features: ?emp(y,K) to
signal an empty answer for y in K; ?0(y,K) to sig-
nal a zero-valued answer created by counting over
an empty set; and ?1(y,K) to signal a one-valued
answer created by counting over a singleton set.
As with the lexical cues, we use knowledge base
features as soft constraints since it is possible for
natural language queries to refer to concepts that do
not exist in K.
8 Experimental Setup
Data We evaluate performance on the benchmark
GeoQuery dataset (Zelle and Mooney, 1996), and a
newly introduced Freebase Query (FQ) dataset (Cai
and Yates, 2013a). FQ contains 917 questions la-
beled with logical form meaning representations for
querying Freebase. We gathered question answer la-
bels by executing the logical forms against Freebase,
and manually correcting any inconsistencies.
Freebase (Bollacker et al, 2008) is a large, col-
laboratively authored database containing almost 40
million entities and two billion facts, covering more
than 100 domains. We filter Freebase to cover the
domains contained in the FQ dataset resulting in a
database containing 18 million entities, 2072 rela-
tions, 635 types, 135 million facts and 81 domains,
including for example film, sports, and business. We
use this schema to define our target domain, allow-
ing for a wider variety of queries than could be en-
coded with the 635 collapsed relations previously
used to label the FQ data.
1552
We report two different experiments on the FQ
data: test results on the existing 642/275 train/test
split and domain adaptation results where the data is
split three ways, partitioning the topics so that the
logical meaning expressions do not share any sym-
bols across folds. We report on the standard 600/280
training/test split for GeoQuery.
Parameter Initialization and Training We ini-
tialize weights for ?np and ?direct to 10, and weights
for ?stem and ?join to 5. This promotes the use of
entities and relations named in sentences. We ini-
tialize weights for ?pp and ?emp to -1 to favour log-
ical forms that have an interpretation in the knowl-
edge base K. All other feature weights are initial-
ized to 0. We run the training algorithm for one it-
eration on the Freebase data, at which point perfor-
mance on the development set had converged. This
fast convergence is due to the very small number of
matching parameters used (5 lexical features and 8
K features). For GeoQuery, we include the larger
domain specific feature set introduced in Section 7.2
and train for 10 iterations. We set the pruning pa-
rameters from Section 6.1 as follows: k = 5 for
Freebase, k = 30 for GeoQuery, N = 50, ? = 10.
Comparison Systems We compare performance
to state-of-the-art systems in both domains. On
GeoQuery, we report results from DCS (Liang
et al, 2011) without special initialization (DCS) and
with an small hand-engineered lexicon (DCS with
L+). We also include results for the FUBL algo-
rithm (Kwiatkowski et al, 2011), the CCG learning
approach that is most closely related to our work. On
FQ, we compare to Cai and Yates (2013a) (CY13).
Evaluation We evaluate by comparing the pro-
duced question answers to the labeled ones, with no
partial credit. Because the parser can fail to pro-
duce a complete query, we report recall, the percent
of total questions answered correctly, and precision,
the percentage of produced queries with correct an-
swers. CY13 and FUBL report fully correct logical
forms, which is a close proxy to our numbers.
9 Results
Quantitative Analysis For FQ, we report results
on the test set and in the cross-domain setting, as de-
fined in Section 8. Figure 6 shows both results. Our
Setting System R P F1
Test Our Approach 68.0 76.7 72.1
CY13 59 67 63
Cross Our Approach 67.9 73.5 71.5
Domain CY13 60 69 65
Figure 6: Results on the FQ dataset.
R P F1
All Features 68.6 72.0 70.3
Without Wiktionary 67.2 70.7 68.9
Without K Features 61.8 62.5 62.1
Figure 7: Ablation Results
approach outperforms the previous state of the art,
achieving a nine point improvement in test recall,
while not requiring labeled logical forms in train-
ing. We also see consistent improvements on both
scenarios, indicating that our approach is generaliz-
ing well across topic domains. The learned ontology
matching model is able to reason about previously
unseen ontological subdomains as well as if it was
provided explicit, in-domain training data.
We also performed feature ablations with 5-fold
cross validation on the training set, as seen in Fig-
ure 7. Both the Wiktionary features and knowledge
base features were helpful. Without the Wiktionary
features, the model must rely on word stem matches
which, in combination with graph constraints, can
still recover many of the correct queries. However,
without the knowledge base constraints, the model
produces many queries that return empty answers,
and significantly impacts overall performance.
For GeoQuery, we report test results in Figure 8.
Our approach outperforms the most closely related
CCG model (FUBL) and DCS without initialization,
but falls short of DCS with a small hand-built initial
lexicon. Given the small size of the test set, it is fair
to say that all algorithms are performing at state-of-
the-art levels. This result demonstrates that our ap-
Recall
FUBL 88.6
DCS 87.9
DCS with L+ 91.1
Our Approach 89.0
Figure 8: GeoQuery Results
1553
Parse Failures (20%)
1. Query in what year did motorola have the most revenue
2 Query on how many projects was james walker a design engineer
Structural Matching Failure (30%)
Query how many children does jerry seinfeld have
3. Labeled ?x.eq(x, count(?y.people.person.children(jerry seinfeld, y)))
Predicted ?x.eq(x, count(?y.people.person.children(y, jerry seinfeld)))
Incomplete Database (10%)
Query how many countries participated in the 2006 winter olympics
4. Labeled ?y.olympics.olympic games.number of countries(2006 winter olympics, y)
Predicted ?y.eq(y, count(?y.olympic participation country.olympics participated in(x, 2006 winter olympics)))
Query what programming languages were used for aol instant messenger
5. Labeled ?y.computer.software.languages used(aol instant messenger, y)
Predicted ?y.computer.software.languages used(aol instant messenger, y) ? computer.programming language(y)
Lexical Ambiguity (35%)
Query when was the frida kahlo exhibit at the philadelphia art museum
Labeled ?y.?x.exhibition run.exhibition(x, frida kahlo)?
6. exhibition venue.exhibitions at(philadelphia art museum, x) ? exhibition run.opened on(x, y)
Predicted ?y.?x.exhibition run.exhibition(x, frida kahlo)?
exhibition venue.exhibitions at(philadelphia art museum, x) ? exhibition run.closed on(x, y)
Figure 9: Example error cases, with associated frequencies, illustrating system output and gold standard
references. 5% of the cases were miscellaneous or otherwise difficult to categorize.
proach can handle the high degree of lexical ambi-
guity in the FQ data, without sacrificing the ability
to understanding the rich, compositional phenomena
that are common in the GeoQuery data.
Qualitative Analysis We also did a qualitative
analysis of errors in the FQ domain. The model
learns to correctly produce complex forms that join
multiple relations. However, there are a number of
systematic error cases, grouped into four categories
as seen in Figure 9.
The first and second examples show parse fail-
ures, where the underspecified CCG grammar did
not have sufficient coverage. The third shows a
failed structural match, where all of the correct logi-
cal constants are selected, but the argument order is
reversed for one of the literals. The fourth and fifth
examples demonstrate a failures due to database in-
completeness. In both cases, the predicted queries
would have returned the same answers as the gold-
truth ones if Freebase contained all of the required
facts. Developing models that are robust to database
incompleteness is a challenging problem for future
work. Finally, the last example demonstrates a lex-
ical ambiguity, where the system was unable to de-
termine if the query should include the opening date
or the closing date for the exhibit.
10 Conclusion
We considered the problem of learning domain-
independent semantic parsers, with application to
QA against large knowledge bases. We introduced
a new approach for learning a two-stage semantic
parser that enables scalable, on-the-fly ontological
matching. Experiments demonstrated state-of-the-
art performance on benchmark datasets, including
effective generalization to previously unseen words.
We would like to investigate more nuanced no-
tions of semantic correctness, for example to support
many of the essentially equivalent meaning repre-
sentations we found in the error analysis. Although
we focused exclusively on QA applications, the gen-
eral two-stage analysis approach should allow for
the reuse of learned grammars across a number of
different domains, including robotics or dialog ap-
plications, where data is more challenging to gather.
11 Acknowledgements
This research was supported in part by DARPA un-
der the DEFT program through the AFRL (FA8750-
13-2-0019) and the CSSG (N11AP20020), the ARO
(W911NF-12-1-0197), the NSF (IIS-1115966), and
by a gift from Google. The authors thank Anthony
Fader, Nicholas FitzGerald, Adrienne Wang, Daniel
Weld, and the anonymous reviewers for their helpful
comments and feedback.
1554
References
Alshawi, H. (1992). The core language engine. The
MIT Press.
Artzi, Y. and Zettlemoyer, L. (2011). Bootstrapping
semantic parsers from conversations. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Artzi, Y. and Zettlemoyer, L. (2013). Weakly super-
vised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Associ-
ation for Computational Linguistics, 1(1):49?62.
Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and
Taylor, J. (2008). Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In Proceedings of the ACM SIGMOD Inter-
national Conference on Management of Data.
Bos, J. (2008). Wide-coverage semantic analysis
with boxer. In Proceedings of the Conference on
Semantics in Text Processing.
Cai, Q. and Yates, A. (2013a). Large-scale semantic
parsing via schema matching and lexicon exten-
sion. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Cai, Q. and Yates, A. (2013b). Semantic parsing
freebase: Towards open-domain semantic pars-
ing. In Proceedings of the Joint Conference on
Lexical and Computational Semantics.
Chen, D. and Mooney, R. (2011). Learning to inter-
pret natural language navigation instructions from
observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
Clark, S. and Curran, J. (2007). Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?
552.
Clarke, J., Goldwasser, D., Chang, M., and Roth,
D. (2010). Driving semantic parsing from the
world?s response. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing.
Davidson, D. (1967). The logical form of action sen-
tences. Essays on actions and events, pages 105?
148.
Doan, A., Madhavan, J., Domingos, P., and Halevy,
A. (2004). Ontology matching: A machine
learning approach. In Handbook on ontologies.
Springer.
Euzenat, J., Euzenat, J., Shvaiko, P., et al (2007).
Ontology matching. Springer.
Fader, A., Zettlemoyer, L., and Etzioni, O. (2013).
Paraphrase-driven learning for open question an-
swering. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Goldwasser, D. and Roth, D. (2011). Learning from
natural instructions. In Proceedings of the In-
ternational Joint Conference on Artificial Intelli-
gence.
Grosz, B. J., Appelt, D. E., Martin, P. A., and
Pereira, F. (1987). TEAM: An experiment in
the design of transportable natural language inter-
faces. Artificial Intelligence, 32(2):173?243.
Hobbs, J. R. (1985). Ontological promiscuity. In
Proceedings of the Annual Meeting on Associa-
tion for Computational Linguistics.
Jones, B. K., Johnson, M., and Goldwater, S. (2012).
Semantic parsing with bayesian tree transducers.
In Proceedings of the 50th Annual Meeting of the
Association of Computational Linguistics.
Kate, R. and Mooney, R. (2006). Using string-
kernels for learning semantic parsers. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics.
Krishnamurthy, J. and Kollar, T. (2013). Jointly
learning to parse and perceive: Connecting nat-
ural language to the physical world. Transactions
of the Association for Computational Linguistics,
1(2).
Krishnamurthy, J. and Mitchell, T. (2012). Weakly
supervised training of semantic parsers. In Pro-
ceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning.
Kushman, N. and Barzilay, R. (2013). Using se-
mantic unification to generate regular expressions
from natural language. In Proceedings of the Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics.
1555
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L.,
and Steedman, M. (2012). A probabilistic model
of syntactic and semantic acquisition from child-
directed utterances and their meanings. Proceed-
ings of the Conference of the European Chapter
of the Association of Computational Linguistics.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2010). Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2011). Lexical generalization
in CCG grammar induction for semantic parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Liang, P., Jordan, M., and Klein, D. (2011). Learn-
ing dependency-based compositional semantics.
In Proceedings of the Conference of the Associ-
ation for Computational Linguistics.
Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo,
L., and Fox, D. (2012). A joint model of language
and perception for grounded attribute learning. In
Proceedings of the International Conference on
Machine Learning.
Muresan, S. (2011). Learning for deep language un-
derstanding. In Proceedings of the International
Joint Conference on Artificial Intelligence.
Steedman, M. (1996). Surface Structure and Inter-
pretation. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The
MIT Press.
Unger, C., Bu?hmann, L., Lehmann, J.,
Ngonga Ngomo, A., Gerber, D., and Cimiano, P.
(2012). Template-based question answering over
RDF data. In Proceedings of the International
Conference on World Wide Web.
Wong, Y. and Mooney, R. (2007). Learning syn-
chronous grammars for semantic parsing with
lambda calculus. In Proceedings of the Confer-
ence of the Association for Computational Lin-
guistics.
Yahya, M., Berberich, K., Elbassuoni, S., Ramanath,
M., Tresp, V., and Weikum, G. (2012). Natural
language questions for the web of data. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Zelle, J. and Mooney, R. (1996). Learning to parse
database queries using inductive logic program-
ming. In Proceedings of the National Conference
on Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2005). Learning
to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars.
In Proceedings of the Conference on Uncertainty
in Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2007). Online
learning of relaxed CCG grammars for parsing to
logical form. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
Zettlemoyer, L. and Collins, M. (2009). Learn-
ing context-dependent mappings from sentences
to logical form. In Proceedings of the Joint Con-
ference of the Association for Computational Lin-
guistics and International Joint Conference on
Natural Language Processing.
Zhang, C., Hoffmann, R., and Weld, D. S. (2012).
Ontological smoothing for relation extraction
with minimal supervision. In Proceeds of the
Conference on Artificial Intelligence.
1556
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1914?1925,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning Distributions over Logical Forms
for Referring Expression Generation
Nicholas FitzGerald Yoav Artzi Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{nfitz,yoav,lsz}@cs.washington.edu
Abstract
We present a new approach to referring ex-
pression generation, casting it as a density es-
timation problem where the goal is to learn
distributions over logical expressions identi-
fying sets of objects in the world. Despite
an extremely large space of possible expres-
sions, we demonstrate effective learning of
a globally normalized log-linear distribution.
This learning is enabled by a new, multi-stage
approximate inference technique that uses a
pruning model to construct only the most
likely logical forms. We train and evaluate
the approach on a new corpus of references
to sets of visual objects. Experiments show
the approach is able to learn accurate models,
which generate over 87% of the expressions
people used. Additionally, on the previously
studied special case of single object reference,
we show a 35% relative error reduction over
previous state of the art.
1 Introduction
Understanding and generating natural language re-
quires reasoning over a large space of possible
meanings; while many statements might achieve the
same goal in a certain situation, some are more
likely to be used than others. In this paper, we model
these preferences by learning distributions over sit-
uated meaning use.
We focus on the task of referring expression gen-
eration (REG), where the goal is to produce an ex-
pression which uniquely identifies a pre-defined ob-
ject or set of objects in an environment. In prac-
tice, many such expressions can be produced. Fig-
ure 1 shows referring expressions provided by hu-
man subjects for a set of objects (Figure 1a), demon-
strating variation in utterances (Figure 1b) and their
corresponding meaning representations (Figure 1c).
Although nearly a third of the people simply listed
the colors of the desired objects, many other strate-
gies were also used and no single option dominated.
Learning to model such variation would enable sys-
tems to better anticipate what people are likely to
say and avoid repetition during generation, by pro-
ducing appropriately varied utterances themselves.
With these goals in mind, we cast REG as a den-
sity estimation problem, where the goal is to learn a
distribution over logical forms.
Learning such distributions is challenging. For a
target set of objects, the number of logical forms
that can be used to describe it grows combinatori-
ally with the number of observable properties, such
as color and shape. However, only a tiny fraction
of these possibilities are ever actually used by peo-
ple. We must learn to efficiently find these few, and
accurately estimate their associated likelihoods.
We demonstrate effective learning of a globally
normalized log-linear distribution with features to
account for context dependence and communicative
goals. We use a stochastic gradient descent algo-
rithm, where the key challenge is the need to com-
pute feature expectations over all possible logical
forms. For that purpose, we present a multi-stage
inference algorithm, which progressively constructs
meaning representations with increasing complex-
ity, and learns a pruning model to retain only those
that are likely to lead to high probability expres-
sions. This approach allows us to consider a large
1914
(a)
The green, red, orange and yellow toys. (1)
The green, red, yellow, and orange objects. (1)
The red, green, yellow and orange toys. (1)
The red, yellow, orange and green objects. (1)
All the green, red, yellow and orange toys. (1)
All the yellow, orange, red and green objects. (1)
All the pieces that are not blue or brown. (2)
All items that are not brown or blue. (2)
All items that are not brown or blue. (2)
Everything that is not brown or blue. (3)
Everything that is not purple or blue. (3)
All but the black and blue ones. (4)
Any toy but the blue and brown toys. (4)
Everything that is green, red, orange or yellow. (5)
All objects that are not triangular or blue. (6)
Everything that is not blue or a wedge. (7)
Everything that is not a brown or blue toy. (8)
All but the blue piece and brown wedge. (9)
Everything except the brown wedge and the blue object. (10)
All pieces but the blue piece and brown triangle shape. (11)
(b)
P? (z|S,G) z
0.30 ?(?x.(yellow(x) ? orange(x) ? red(x) ? green(x)) ? object(x) ? plu(x)) (1)
0.15 ?(?x.?(brown(x) ? blue(x)) ? object(x) ? plu(x)) (2)
0.10 Every(?x.?(brown(x) ? blue(x)) ? object(x) ? sg(x)) (3)
0.10 Every(?x.object(x) ? sg(x)) \ [?(?x.(blue(x) ? brown(x)) ? object(x) ? plu(x)] (4)
0.05 Every(?x.(yellow(x) ? orange(x) ? red(x) ? green(x)) ? object(x) ? sg(x)) (5)
0.05 ?(?x.(triangle(x) ? blue(x)) ? object(x) ? plu(x)) (6)
0.05 Every(?x.object(x) ? sg(x)?(blue(x) ? equal(x,A(?y.triangle(y) ? sg(y))))) (7)
0.05 Every(?x.object(x) ? sg(x) ? ?equal(x,A(?y.(brown(y) ? blue(y)) ? object(y) ? sg(y)))) (8)
0.05 Every(?x.object(x) ? sg(x)) \ [?(?x.(blue(x) ? object(x) ? sg(x)) ? (brown(x) ? triangle(x) ? sg(x))] (9)
0.05 Every(?x.object(x) ? sg(x)) \ [?(?x.brown(x) ? triangle(x) ? sg(x)) ? ?(?y.blue(y) ? object(y) ? sg(x))] (10)
0.05 ?(?x.object(x) ? plu(x)) \ [?(?x.(blue(x) ? object(x) ? sg(x)) ? (brown(x) ? triangle(x) ? object(x) ? sg(x))] (11)
(c)
Figure 1: An example scene from our object selection dataset. Figure 1a shows the image shown to subjects
on Amazon Mechanical Turk. The target set G is the circled objects. Figure 1b shows the 20 sentences
provided as responses. Figure 1c shows the empirical distribution P? (z|G,S) for this scene, estimated by
labeling the sentences in Figure 1b. The correspondence between a sentence in 1b and its labeled logical
expression in 1c is indicated by the number in parentheses. Section 5.1 presents a discussion of the space of
possible logical forms.
set of possible meanings, while maintaining compu-
tational tractability.
To represent meaning we build on previous ap-
proaches that use lambda calculus (Carpenter, 1997;
Zettlemoyer and Collins, 2005; Artzi and Zettle-
moyer, 2013b). We extend these techniques by mod-
eling the types of plurality and coordination that are
prominent in expressions which refer to sets.
We also present a new corpus for the task of re-
ferring expression generation.1 While most previ-
ous REG data focused on naming single objects,
1The corpus was collected using Amazon Mechanical Turk
and is available on the authors? websites.
to the best of our knowledge, this is the first cor-
pus with sufficient coverage for learning to name
sets of objects. Experiments demonstrate highly ac-
curate learned models, able to generate over 87%
of the expressions people used. On the previously
studied special case of single object reference, we
achieve state-of-the-art performance, with over 35%
relative error reduction over previous state of the
art (Mitchell et al, 2013).
2 Related Work
Referring expression generation has been exten-
sively studied in the natural language generation
1915
community, dating as far back as SHRDLU (Wino-
grad, 1972). Most work has built on variations of
the Incremental Algorithm (Dale and Reiter, 1995),
a deterministic algorithm for naming single ob-
jects that constructs conjunctive logical expressions.
REG systems are used in generation pipelines (Dale
and Reiter, 2000) and are also commonly designed
to be cognitively plausible, for example by following
Gricean maxims (Grice, 1975). Krahmer and van
Deemter (2012) and van Deemter et al (2012a) sur-
vey recent literature on REG.
Different approaches have been proposed for gen-
erating referring expressions for sets of objects.
Van Deemter (2002) extended the Incremental Al-
gorithm to allow disjunction and negation, enabling
reference to sets. Further work attempted to re-
solve the unnaturally long expressions which could
be generated by this approach (Gardent, 2002; Ho-
racek, 2004; Gatt and van Deemter, 2007). Later, de-
scription logic was used to name sets (Areces et al,
2008; Ren et al, 2010). All of these algorithms are
manually engineered and deterministic.
In practice, human utterances are surprisingly
varied, loosely following the Gricean ideals (van
Deemter et al, 2012b). Much recent work in REG
has identified the importance of modeling the vari-
ation observed in human-generated referring ex-
pressions (Viethen and Dale, 2010; Viethen et al,
2013; van Deemter et al, 2012b; Mitchell et al,
2013), and some approaches have applied machine-
learning techniques to single-object references (Vi-
ethen and Dale, 2010; Mitchell et al, 2011a,b). Re-
cently, Mitchell et al (2013) introduced a proba-
bilistic approach for conjunctive descriptions of sin-
gle objects, which will provide a comparison base-
line for experiments in Section 8. To the best of
our knowledge, this paper presents the first learned
probabilistic model for referring expressions defin-
ing sets, and is the first effort to treat REG as a den-
sity estimation problem.
REG is related to content selection, which
has been studied for generating text from
databases (Konstas and Lapata, 2012), event
streams (Chen et al, 2010), images (Berg et al,
2012; Zitnick and Parikh, 2013), and text (Barzilay
and Lapata, 2005; Carenini et al, 2006). However,
most approaches to this problem output bags of con-
cepts, while we construct full logical expressions,
allowing our approach to capture complex relations
between attributes.
Finally, our approach to modeling meaning us-
ing lambda calculus is related to a number of ap-
proaches that used similar logical representation
in various domains, including database query in-
terfaces (Zelle and Mooney, 1996; Zettlemoyer
and Collins, 2005, 2007), natural language instruc-
tions (Chen and Mooney, 2011; Matuszek et al,
2012b; Kim and Mooney, 2012; Artzi and Zettle-
moyer, 2013b), event streams (Liang et al, 2009;
Chen et al, 2010), and visual descriptions (Ma-
tuszek et al, 2012a; Krishnamurthy and Kollar,
2013). Our use of logical forms follows this line of
work, while extending it to handle plurality and co-
ordination, as described in Section 4.1. In addition,
lambda calculus was shown to enable effective nat-
ural language generation from logical forms (White
and Rajkumar, 2009; Lu and Ng, 2011). If com-
bined with these approaches, our approach would
allow the creation of a complete REG pipeline.
3 Technical Overview
Task Let Z be a set of logical expressions that se-
lect a target set of objects G in a world state S, as
formally defined in Section 5.1. We aim to learn a
probability distribution P (z | S,G), with z ? Z .
For example, in the referring expressions domain
we work with, the state S = {o1, . . . , on} is a set
of n objects oi. Each oi has three properties: color,
shape and type. The target setG ? S is the subset of
objects to be described. Figure 1a shows an example
scene. The world state S includes the 11 objects in
the image, where each object is assigned color (yel-
low, green . . . ), shape (cube, cylinder . . . ) and type
(broccoli, apple . . . ). The target set G contains the
circled objects. Our task is to predict a distribution
which closely matches the empirical distribution in
Figure 1c.
Model and Inference We model P (z|S,G) as a
globally normalized log-linear model, using features
of the logical form z, and its execution with respect
to S and G. Since enumerating all z ? Z is in-
tractable, we develop an approximate inference al-
gorithm which constructs a high quality candidate
set, using a learned pruning model. Section 5.2 de-
scribes the globally scored log-linear model. Sec-
1916
tion 5.3 presents a detailed description of the infer-
ence procedure.
Learning We use stochastic gradient descent to
learn both the global scoring model and the explicit
pruning model, as described in section 6. Our data
consists of human-generated referring expressions,
gathered from Amazon Mechanical Turk. These
sentences are automatically labelled with logical
forms with a learned semantic parser, providing a
stand-in for manually labeled data (see Section 7).
Evaluation Our goal is to output a distribution
that closely matches the distribution that would be
produced by humans. We therefore evaluate our
model with gold standard labeling of crowd-sourced
referring expressions, which are treated as samples
from the implicit distribution we are trying to model.
The data and evaluation procedure are described in
Section 7. The results are presented in Section 8.
4 Modeling Referring Expressions
4.1 Semantic Modeling
Our semantic modeling approach uses simply-typed
lambda-calculus following previous work (Carpen-
ter, 1997; Zettlemoyer and Collins, 2005; Artzi and
Zettlemoyer, 2013b), extending it in one important
way: we treat sets of objects as a primitive type,
rather than individuals. This allows us to model plu-
rality, cardinality, and coordination for the language
observed in our data, and is further motivated by re-
cent cognitive science evidence that sets and their
properties are represented as single units in human
cognition (Scontras et al, 2012).
Plurals Traditionally, noun phrases are identified
with the entity-type e and pick out individual ob-
jects (Carpenter, 1997). This makes it difficult to
interpret plural noun-phrases which pick out a set of
objects, like ?The red cubes?. Previous approaches
would map this sentence to the same logical expres-
sion as the singular ?The red cube?, ignoring the se-
mantic distinction encoded by the plural.
Instead, we define the primitive entity e to range
over sets of objects. ?e, t?-type expressions are
therefore functions from sets to a truth-value. These
are used in two ways, modeling both distributive and
collective predicates (cf. Stone, 2000):
1. Distributive predicates are ?e, t?-type expres-
sions which will return true if every individual
in the set has a given property. For example, the
expression ?x.red(x) will be true for all sets
which contain only objects for which the value
red is true.
2. Collective predicates are ?e, t?-type expres-
sions which indicate a property of the set it-
self. For example, in the phrase ?the two
cubes?, ?two? corresponds to the expression
?x.cardinality2(x) which will return true
only for sets which have exactly two members.
We define semantic plurality in terms of two spe-
cial collective predicates: sg for singular and plu
for plural. For examples, ?cube? is interpreted as
?x.cube(x) ? sg(x), whereas ?cubes? is interpreted
as ?x.cube(x) ? plu(x). The sg predicate returns
true only for singleton sets. The plu predicate re-
turns true for sets that contain two or more objects.
We also model three kinds of determiners,
functional-type ??e, t?, e?-type expressions which
select a single set from the power-set represented
by their ?e, t?-type argument. The definite deter-
miner ?the? is modeled with the predicate ?, which
resolves to the maximal set amongst those licensed
by its argument. The determinerEvery only accepts
?e, t?-type arguments that define singleton sets (i.e.
the argument includes the sg predicate) and returns
a set containing the union of these singletons. For
example, although ?red cube? is a singular expres-
sion, ?Every red cube? refers to a set. Finally, the
indefinite determiner ?a? is modeled with the logical
constant A, which picks a singleton set by implic-
itly introducing an existential quantifier (Artzi and
Zettlemoyer, 2013b).2
Coordination Two types of coordination are
prominent in set descriptions. The first is attribute
coordination, which is typically modeled with the
boolean operators: ? for conjunction and ? for dis-
junction. For example, the phrase ?the red cubes
and green rectangle? involves a disjunction that joins
two conjunctive expressions, both within the scope
of the definite determiner: ?(?x.(red(x)?cube(x)?
plu(x)) ? (green(x) ? rectangle(x) ? sg(x))).
2This treatment of the indefinite determiner is related to gen-
eralized skolem terms as described by Steedman (2011).
1917
The second kind of coordination, a new addition
of this work, occurs when two sets are coordinated.
This can either be set union (?) as in the phrase ?The
cubes and the rectangle? (?(?x.cube(x)? plu(x))?
?(?x.rectangle(x) ? sg(x)))), or set difference
(\) as in the phrase ?All blocks except the green
cube?: (?(?x.object(x)?plu(x))\?(?x.green(x)?
cube(x) ? sg(x))).
4.2 Visual Domain
Objects in our scenes are labeled with attribute val-
ues for four attribute types: color (7 values, such
as red, green), shape (9 values, such as cube,
sphere), type (16 values, such as broccoli, apple)
and a special object property, which is true for
all objects. The special object property captures
the role of descriptions that are true for all objects,
such as ?toy? or ?item?. Each of these 33 attribute
values corresponds to an ?e, t?-type predicate.
5 Model and Inference
In this section, we describe our approach to mod-
eling the probability P (z | S,G) of a logical form
z ? Z that names a set of objects G in a world S, as
defined in Section 3. We first define Z (Section 5.1),
and then present the distribution (Section 5.2) and an
approximate inference approach that makes use of a
learned pruning model (Section 5.3).
5.1 Space of Possible Meanings
The set Z defines logical expressions that we will
consider for picking the target set G in state S. In
general, we can construct infinitely many such ex-
pressions. For example, every z ? Z can be triv-
ially extended to form a new candidate z? for Z
by adding a true clause to any conjunct it contains.
However, the vast majority of such expressions are
overly complex and redundant, and would never be
used in practice as a referring expression.
To avoid this explosion, we limit the type and
complexity of the logical expressions that are in-
cluded in Z . We consider only e-type expressions,
since they name sets, and furthermore only include
expressions that name the desired target set G.3 We
3We do not attempt to model underspecified or otherwise
incorrect expressions, although our model could handle this by
considering all e-type expressions.
? p : ??e, t?, e?, e1 : ?e, t? ? p(e1) : e
e.g.
p = ? : ??e, t?, e?
e1 = ?x.cube(x) ? sg(x) : ?e, t?
?(?x.cube(x) ? sg(x)) : e
? p : ?t, t?, e1 : ?e, t? ? ?x.p(e1(x)) : ?e, t?
e.g.
p = ? : ?t, t?
e1 = ?x.red(x) : ?e, t?
?x.?(red(x)) : ?e, t?
? p : ?e, ?e, t??, e1 : e? ?x.(p(x))(e1)
e.g.
p = equal : ?e, ?e, t??
e1 = A(?y.cube(y) ? sg(y)) : e
?x.equal(x,A(?y.cube(y) ? sg(y)))
? p : ?e, ?e, e??, e1 : e, e2 : e? (p(e1))(e2) : e
e.g.
p = \ : ?e, ?e, e??
e1 = ?(?x.cube(x) ? plu(x)) : e
e2 = Every(?x.object(x) ? sg(x)) : e
Every(?x.object(x) ? sg(x)) \
?(?x.cube(x) ? plu(x)) : e
? p : ?t, ?t, t??, e1 : ?e, t?, e2?e, t? ?
?x.(p(e1(x)))(e2(x)) : ?e, t?
e.g.
p = ? : ?t, ?t, t??
e1 = ?x.red(x) : ?e, t?
e2 = ?x.cube(x) : ?e, t?
?x.red(x) ? cube(x) : e
Figure 2: The five rules used during generation.
Each rule is a template which takes a predicate p : t
of type t and one or two arguments ei : ti, with type
ti. The output is the logical expression after the ar-
row?, constructed using the inputs as shown.
also limit the overall complexity of each z ? Z , to
contain not more than M logical constants.
To achieve these constraints, we define an induc-
tive procedure for enumerating Z , in order of com-
plexity. We first define Aj to be the set of all e- and
?e, t?-type expressions that contain exactly j logi-
cal constants. Figure 2 presents five rules that can be
used to constructAj by induction, for j = 1, . . . ,?,
by repeatedly adding new constants to expressions
in Aj? for j? < j. Intuitively, Aj is the set of all
complexity j expressions that can be used as sub-
expressions for higher complexity entires in our final
set Z . Next, we define Zj to be the e-type expres-
sions inAj that name the correct setG. And, finally,
Z = ?j=1...MZj of all correct expressions up to a
maximum complexity of M .
This construction allows for a finite Z with good
1918
empirical coverage, as we will see in the experi-
ments in Section 8. However, Z is still prohibitively
large for the maximum complexities used in practise
(for example M = 20). Section 5.3 presents an ap-
proach for learning models to prune Z , while still
achieving good empirical coverage.
5.2 Global Model
Given a finite Z , we can now define our desired
globally normalized log-linear model, conditioned
on the state S and set of target objects G:
PG(z | S,G; ?) =
1
C
e???(z,S,G) (1)
where ? ? Rn is a parameter vector, ?(z, S,G) ?
Rn is a feature function and C is the normalization
constant. Section 5.4 defines the features we use.
5.3 Pruning Z
As motivated in Section 5.1, the key challenge for
our global model in Equation 1 is that the set Z is
too large to be explicitly enumerated. Instead, we
designed an approach for learning to approximate Z
with a subset of the highly likely entries, and use this
subset as a proxy for Z during inference.
More specifically, we define a binary distribution
that is used to classify whether each a ? Aj is likely
to be used as a sub-expression in Z , and prune each
Aj to keep only the top k most likely entries. This
distribution is a logistic regression model:
Pj(a | S,G;pij) =
epij ??(a,S,G)
1 + epij ??(a,S,G)
(2)
with features ?(a, S,G) ? Rn and parameters pij ?
Rn. This distribution uses the same features as the
global model presented in Equation 1, which we de-
scribe in Section 5.4.
Together, the pruning model and global model de-
fine the distribution P? (z | G,S; ?,?) over z ? Z ,
conditioned on the world state S and target set G,
and parameterized by both the parameters ? of the
global model and the parameters ? = {pi1, . . . , piM}
of the pruning models.
5.4 Features
We use three kinds of features: logical expression
structure features, situated features and a complexity
feature. All features but the complexity feature are
shared between the global model in Equation 1 and
the pruning model in Equation 2. In order to avoid
overly specific features, the attribute value predi-
cates in the logical expressions are replaced with
their attribute type (ie. red ? color). In addition,
the special constants sg and plu are ignored when
computing features.
In the following description of our features, all
examples are computed for the logical expression
?(?x.red(x) ? object(x) ? plu(x)), with respect to
the scene and target set in Figure 1a.
Structure Features We use binary features that
account for the presence of certain structures in the
logical form, allowing the model to learn common
usage patterns.
? Head Predicate - indicator for use of a logi-
cal constant as a head predicate in every sub-
expression of the expression. A head predicate
is the top-level operator of an expression. For
example, the head predicate of the expression
??x.red(x) ? object(x)? is ??? and the head
of ?x.red(x) is red. For our running example,
the head features are ?, ?, color, object.
? Head-Predicate Bigrams and Trigrams -
head-predicate bigrams are defined to be the
head predicate of a logical form, and the head
predicate of one of its children. Trigrams
are similarly defined. E.g. bigrams: [?,?],
[?, color], [?, object], and trigrams: [?,?, red],
[?,?, object].
? Conjunction Duplicate - this feature fires if a
conjunctive expression contains duplicate sub-
expressions amongst its children.
? Coordination Children - this feature set indi-
cates the presence of a coordination subexpres-
sion (?, ?, ? or \) and the head expressions
of all pairs and triples of its child expressions.
E.g. [?; red, object].
Situated Features These features take into ac-
count the evaluation of the logical form z with re-
spect to the state S and target set G. They capture
common patterns between the target set G and the
object groups named by subexpressions of z.
1919
? Head Predicate and Coverage - this fea-
ture set indicates the head predicate of ev-
ery sub-expression of the logical form, com-
bined with a comparison between the execu-
tion of the sub-expression and the target set
G. The possible values for this comparison
(which we call the ?coverage? of the expres-
sion with respect to G) are: EQUAL, SUBSET
(SUB), SUPERSET (SPR), DISJOINT, ALL,
EMPTY and OTHER. E.g. [?,SUB], [?,SUB],
[color,SUB], [object,ALL]
? Coordination Child Coverage - this feature
set indicates the head-predicate of a coordina-
tion subexpression, combined with the cover-
age of all pairs and triples of its child expres-
sions. E.g. [?;SUB,ALL].
? Coordination Child Relative Coverage - this
feature set indicates, for every pair of child sub-
expressions of coordination expressions in the
logical form, the coverage of the child sub-
expressions relative to each other. The pos-
sible relative coverage values are: SUB-SPR,
DISJOINT, OTHER. E.g. [?;SUB-SPR].
Complexity Features We use a single real-
numbered feature to account for the complexity of
the logical form. We define the complexity of a log-
ical form to be the number of logical constants used.
Our running example has a complexity of 4. This
feature is only used in the global model, since the
pruning model always considers logical expressions
of fixed complexity.
6 Learning
Figure 3 presents the complete learning algorithm.
The algorithm is online, using stochastic gradi-
ent descent updates for both the globally scored
density estimation model and the learned pruning
model. The algorithm assumes a dataset of the form
{(Zi, Si, Gi) : i = 1 . . . n} where each example
scene includes a list of logical expressions Zi, a
world state Si, and a target set of objects, Gi, which
will be identified by the resulting logical expres-
sions. The output is learned parameters for both the
globally scored density estimation model ?, and for
the learned pruning models ?.
Inputs: Training set {(Zi, Si, Gi) : i = 1 . . . n}, where Zi is
a list of logical forms, Si is a world state, and Gi is a target
set of objects. Number of iterations T . Learning rate ?0.
Decay parameter c. Complexity threshold M , as described
in Section 5.3.
Definitions: Let P? (z | Gi, Si; ?,?) be the predicted global
probability from Equation 1. Let P?j(z | Gi, Si;pij) be the
predicted pruning probability from Equation 2. Let A?j be
the set of all complexity-M logical expressions, after prun-
ing (see Section 5.1). Let SUB(j, z) be all complexity-j
sub-expressions of logical expression z. Let Qi(z | Si, Gi)
be the empirical probability over z ? Z , estimated from
Zi. Finally, let ?i(z) be a shorthand for the feature function
?(z, Si, Gi) as defined in Section 5.4.
Algorithm:
Initialize ? ? ~0, pij ? ~0 for j = 1 . . .M
For t = 1 . . . T, i = 1 . . . n:
Step 1: (Update Global Model)
a. Compute the stochastic gradient:
?? ? EQi(z|Si,Gi)[?i(z)]? EP? (z|Gi,Si;?,?)[?i(z)]
b. Update the parameters:
? ? ?01+c?? where ? = i+ t? n
? ? ? + ???
Step 2: (Update Pruning Model)
For j = 1 . . .M
a. Construct a set of positive and negative examples:
D+ ?
?
z?Zi
SUB(j, z).
D? ? A?j \ D+
b. Compute mini-batch stochastic gradient, normalizing
for data skew:
?pij ?
1
|D+|
?
z?D+(1? Pj(z | Si, Gi;pij))?i(z)
? 1
|D?|
?
z?D? Pj(z | Si, G;pij)?i(z)
c. Update complexity-j pruning parameters:
pij ? pij + ??pij
Output: ? and ? = [pi1, . . . , piM ]
Figure 3: The learning algorithm.
6.1 Global Model Updates
The parameters ? of the globally scored density es-
timation model are trained to maximize the log-
likelihood of the data:
Oi = log
?
z?Zi
PG(z | Si, Gi) (3)
Taking the derivative of this objective with re-
spect to ? yields the gradient in Step 1a of Fig-
ure 3. The marginals, EP? (z|Gi,Si;?,?)(?i(z)), are
computed over the approximate finite subset of Z
constructed with the inference procedure described
in Section 5.3.
1920
6.2 Pruning Model Updates
To update each of the M pruning models, we first
construct a set of positive and negative examples
(Step 2a). The positive examples, D+, include those
sub-expressions which should be in the beam - these
are all complexity j sub-expressions of logical ex-
pressions in Zi. The negative examples, D?, in-
clude all complexity-j expressions constructed dur-
ing beam search, minus those which are in D+. The
gradient (Set 2b) is a binary mini-batch gradient,
normalized to correct for data skew.
7 Experimental Setup
Data Collection Our dataset consists of 118 im-
ages, taken with a Microsoft Kinect camera. These
are the same images used by Matuszek et al
(2012a), but we create multiple prompts for each im-
age by circling different objects, giving 269 scenes
in total. These scenes were shown to workers on
Amazon Mechanical Turk4 who were asked to imag-
ine giving instructions to a robot and complete the
sentence ?Please pick up ? in reference to the
circled objects. Twenty referring expressions were
collected for each scene, a total of 5380 expressions.
From this data, 43 scenes (860 expressions) were
held-out for use in a test set. Of the remaining
scenes, the sentences of 30 were labeled with log-
ical forms. 10 of these scenes (200 expressions) are
used as a labeled initialization set, and 20 are used
as a development test set (400 expressions). A small
number of expressions (?5%) from the labeled ini-
tial set were discarded, either because they did not
correctly name the target set, or because they used
very rare attributes (such as texture, or location) to
name the target objects.
Surrogate Labeling To avoid hand labeling the
large majority of the scenes, we label the data
with a learned semantic parser (Zettlemoyer and
Collins, 2005). We created a hand-made lexicon
for the entire training set, which greatly simplifies
the learning problem, and learned the parameters
of the parsing moder on the 10-scene initialization
set. The weights were then further tuned using
semi-supervised techniques (Artzi and Zettlemoyer,
2011, 2013b) on the data to be labeled. Testing on
4http://www.mturk.com
the development set shows that this parser achieves
roughly 95% precision and 70% recall.
Using this parser, we label the sentences in our
training set. We only use scenes where at least 15
sentences were successfully parsed. This gives a
training set of 141 scenes (2587 expressions). Com-
bining the automatically labeled training set with the
hand-labelled initialization, development and held-
out data, our labelled corpus totals 3938 labeled ex-
pressions. By contrast, the popular TUNA furniture
sub corpus (Gatt et al, 2007) contains 856 descrip-
tions of 20 scenes, and although some of these refer
to sets, these sets contain two objects at most.
Framework Our experiments were implemented
using the University of Washington Semantic Pars-
ing Framework (Artzi and Zettlemoyer, 2013a).
Hyperparameters Our inference procedure re-
quires two hyperparameters: M , the maximum com-
plexity threshold, and k, the beam size. In practice,
we set these to the highest possible values which
still allow for training to complete in a reasonable
amount of time (under 12 hours). M is set to 20,
which is sufficient to cover 99.5% of the observed
expressions. The beam-size k is 100 for the first
three complexity levels, and 50 thereafter.
For learning, we use the following hyperparam-
eters, which were tuned on the development set:
learning rate ?0 = .25, decay rate c = .02, num-
ber of epochs T = 10.
Evaluation Metrics Evaluation metrics used in
REG research have assumed a system that produces
a single output. Our goal is to achieve a distribution
over logical forms that closely matches the distribu-
tion observed from human subjects. Therefore, we
compare our learned model to the labeled test data
with mean absolute error:
MAE =
1
2n
n?
i=1
?
z?Z
|P (z | Si, Gi)?Q(z | Si, Gi)|
where Q is the empirical distribution observed in the
training data. MAE measures the total probability
mass which is assigned differently in the predicted
distribution than in the empirical distribution. We
use MAE as opposed to KL divergence or data like-
lihood as both of these measures are uninformative
when the support of the two distributions differ.
1921
? MAE %dup %uniq Top1
VOA 39.7 98.2 92.5 72.7
GenX
25.8 100 100 72.7
(5.0) (0) (0) (0)
Table 1: Single object referring expression gener-
ation results. Our approach (GenX) is compared
to the approach from Mitchell et al (2013) (VOA).
Standard deviation over five shuffles of training set
is reported in parentheses.
This metric is quite strict; small differences in the
estimated probabilities over a large number of logi-
cal expressions can result in a large error, even if the
relative ordering is quite similar. Therefore, we re-
port the percentage of observed logical expressions
which the model produces, either giving credit mul-
tiple times for duplicates (%dup) or counting each
unique logical expression in a scene once (%uniq).
Put another way, %dup counts logical expression to-
kens, whereas %uniq counts types. We also report
the proportion of scenes where the most likely log-
ical expression according to the model matched the
most common one in the data (Top1).
Single Object Baseline In order to compare our
method against the state of the art for generating
referring expressions for single objects, we use the
subset of our corpus where the target set is a sin-
gle object. This sub-corpus consists of 44 scenes for
training and 11 held out for testing.
For comparison we re-implemented the proba-
bilistic Visual Objects Algorithm (VOA) of Mitchell
et al (2013). We refer the readers to the original
paper for details of the approach. The parameters
of the model were tuned on the training data: the
prior likelihood estimates for each of the four at-
tribute types (?att) were estimated as the relative
frequency of each attribute in the data. We pick the
ordering of attributes and the length penalty, ?, from
the cross-product of all possible 4! orderings and all
integers on the range of [1,10], choosing the setting
which results in the lowest average absolute error
(AAE) on the training set. This process resulted
in the following parameter settings: ?color = .916,
?shape = .586, ?type = .094, ?object = .506, AP
ordering = [type, shape, object, color], ? = 4. In-
ference was done using 10,000 samples per scene.
? MAE %dup %uniq Top1
Full GenX
54.3 87.4 72.9 52.6
(4.5) (0.6) (1.1) (8.3)
NoPrune
71.8 42.2 16.1 40.0
(2.5) (2.7) (1.7) (5.0)
NoCOV
87.0 26.0 11.2 14.9
(6.7) (3.7) (2.1) (9.7)
NoSTRUC
60.2 79.6 61.9 44.6
(1.7) (0.3) (0.5) (4.5)
HeadExpOnly
88.8 21.9 9.3 14.0
(6.4) (8.6) (3.5) (7.9)
Table 2: Results on the complete corpus for the
complete system (Full GenX), ablating the pruning
model (NoPrune) and the different features: without
coverage features (NoCOV), without structure fea-
tures (NoSTRUC) and using only the logical expres-
sion HeadExp features (HeadExpOnly). Standard
deviation over five runs is shown in parentheses.
8 Results
We report results on both the single-object subset of
our data and the full dataset. Since our approach is
online, and therefore sensitive to data ordering, we
average results over 5 different runs with randomly
ordered data, and report variance.
Single Objects Table 1 shows the different metrics
for generating referring expression for single objects
only. Our approach outperforms VOA (Mitchell
et al, 2013) on all metrics, including an average of
approximately 35% relative reduction in MAE. In
addition, unlike VOA, our system (GenX) produces
every logical expression used to refer to single ob-
jects in our dataset, including a small number which
use negation and equality.
Object Sets Table 2 lists results on the full dataset.
Our learned pruning approach produces an average
72.9% of the unique logical expressions used present
in our dataset ? over 87% when these counts are
weighted by their frequency. The globally scored
model achieves a mean absolute error of 54.3, and
correctly assigns the highest probability to the most
likely expression over 52% of the time.
Also shown in Table 2 are results obtained when
elements of our approach are ablated. Using the
global model for pruning instead of an explicitly
trained model causes a large drop in performance,
demonstrating that our global model is inappropri-
1922
Q P? z
.750 .320 ?(?x.object(x) ? (yellow(x) ? red(x)))
.114 ?(?x.lego(x)) ? ?(?x.red(x) ? apple(x))
.114 ?(?x.yellow(x) ? lego(x))) ? ?(?x.apple(x))
.044 ?(?x.lego(x) ? (red(x) ? apple(x)))
.044 ?(?x.(yellow(x) ? lego(x)) ? apple(x))
.036 ?(?x.lego(x)) ? ?(?x.red(x) ? sphere(x))
.026 ?(?x.red(x) ? lego(x)) ? ?(?x.red(x) ? sphere(x))
.050 .021 ?(?x.(lego(x) ? yellow(x)) ? (red(x) ? apple(x)))
.017 ?(?x.(lego(x) ? yellow(x)) ? (red(x) ? sphere(x)))
.014 ?(?x.yellow(x) ? lego(x)) ? ?(?x.red(x) ? sphere(x))
.100 .010 ?(?x.yellow(x) ? object(x)) ? ?(?x.apple(x))
.050 .007 ?(?x.yellow(x) ? object(x)) ? ?(?x.red(x) ? sphere(x))
.050 .005 ?(?x.yellow(x) ? object(x)) ? ?(?x.red(x) ? object(x))
(a) (b)
Figure 4: Example output of our system for the scene on the right. We show the top 10 expressions (z) from
the predicted distribution (P? ) compared to the empirical distribution estimated from our labeled data (Q).
The bottom section shows the predicted probability of the three expressions which were not in the top 10 of
the predicted distribution. Although the mean absolute error (MAE) of P? and Q is 63.8, P? covers all of the
entries in Q in the correct relative order and also fills in many other plausible candidates.
ate for pruning. We also ablate subsets of our fea-
tures, demonstrating that the coverage and structural
features are both crucial for performance.
Qualitatively, we found the learned distributions
were often higher quality than the seemingly high
mean absolute error would imply. Figure 4 shows
an example output where the absolute error of the
predicted distribution was 63.8. Much of the error
can be attributed to probability mass assigned to log-
ical expressions which, although not observed in our
test data, are reasonable referring expressions. This
might be due to the fact that our estimate of the em-
pirical distribution comes from a fairly small sample
(20), or other factors which we do not model that
make these expressions less likely.
9 Conclusion
In this paper, we modeled REG as a density-
estimation problem. We demonstrated that we can
learn to produce distributions over logical referring
expressions using a globally normalized model. Key
to the approach was the use of a learned pruning
model to define the space of logical expression that
are explicitly enumerated during inference. Exper-
iments demonstrate state-of-the-art performance on
single object reference and the first results for learn-
ing to name sets of objects, correctly recovering over
87% of the observed logical forms.
This approach suggests several directions for fu-
ture work. Lambda-calculus meaning represen-
tations can be designed for many semantic phe-
nomena, such as spatial relations, superlatives, and
graded properties, that are not common in our data.
Collecting new datasets would allow us to study the
extent to which the approach would scale to domains
with such phenomena.
Although the focus of this paper is on REG, the
approach is also applicable to learning distributions
over logical meaning representations for many other
tasks. Such learned models could provide a range
of possible inputs for systems that map logical ex-
pressions to sentences (White and Rajkumar, 2009;
Lu and Ng, 2011), and could also provide a valuable
prior on the logical forms constructed by semantic
parsers in grounded settings (Artzi and Zettlemoyer,
2013b; Matuszek et al, 2012a).
Acknowledgements
This research was supported in part by the In-
tel Science and Technology Center for Pervasive
Computing, by DARPA under the DEFT program
through the AFRL (FA8750-13-2-0019) and the
CSSG (N11AP20020), the ARO (W911NF-12-1-
0197), and the NSF (IIS-1115966). The authors
wish to thank Margaret Mitchell, Mark Yatskar,
Anthony Fader, Kenton Lee, Eunsol Choi, Gabriel
Schubiner, Leila Zilles, Adrienne Wang, and the
anonymous reviewers for their helpful comments.
1923
References
Areces, C., Koller, A., and Striegnitz, K. (2008).
Referring expressions as formulas of description
logic. In Proceedings of the International Natu-
ral Language Generation Conference.
Artzi, Y. and Zettlemoyer, L. (2011). Bootstrapping
semantic parsers from conversations. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Artzi, Y. and Zettlemoyer, L. (2013a). UW SPF:
The University of Washington Semantic Parsing
Framework.
Artzi, Y. and Zettlemoyer, L. (2013b). Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the As-
sociation for Computational Linguistics, 1(1):49?
62.
Barzilay, R. and Lapata, M. (2005). Collective
content selection for concept-to-text generation.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Berg, A. C., Berg, T. L., Daume, H., Dodge, J.,
Goyal, A., Han, X., Mensch, A., Mitchell, M.,
Sood, A., Stratos, K., et al (2012). Under-
standing and predicting importance in images. In
IEEE Conference on Computer Vision and Pattern
Recognition.
Carenini, G., Ng, R. T., and Pauls, A. (2006). Multi-
document summarization of evaluative text. In
Proceedings of the Conference of the European
Chapter of the Association for Computational
Linguistics.
Carpenter, B. (1997). Type-Logical Semantics. The
MIT Press.
Chen, D., Kim, J., and Mooney, R. (2010). Train-
ing a multilingual sportscaster: using perceptual
context to learn language. Journal of Artificial In-
telligence Research, 37(1):397?436.
Chen, D. and Mooney, R. (2011). Learning to inter-
pret natural language navigation instructions from
observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
Dale, R. and Reiter, E. (1995). Computational in-
terpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19:233?264.
Dale, R. and Reiter, E. (2000). Building natural lan-
guage generation systems. Cambridge University
Press.
Gardent, C. (2002). Generating minimal definite de-
scriptions. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics.
Gatt, A. and van Deemter, K. (2007). Incremental
generation of plural descriptions: Similarity and
partitioning. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Gatt, A., Van Der Sluis, I., and Van Deemter, K.
(2007). Evaluating algorithms for the generation
of referring expressions using a balanced corpus.
In Proceedings of the European Workshop on Nat-
ural Language Generation.
Grice, H. P. (1975). Logic and conversation. 1975,
pages 41?58.
Horacek, H. (2004). On referring to sets of objects
naturally. In Natural Language Generation, pages
70?79. Springer.
Kim, J. and Mooney, R. J. (2012). Unsupervised
PCFG induction for grounded language learning
with highly ambiguous supervision. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Konstas, I. and Lapata, M. (2012). Unsupervised
concept-to-text generation with hypergraphs. In
Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics.
Krahmer, E. and van Deemter, K. (2012). Computa-
tional generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173?218.
Krishnamurthy, J. and Kollar, T. (2013). Jointly
learning to parse and perceive: Connecting nat-
ural language to the physical world. Transactions
of the Association for Computational Linguistics,
1(2):193?206.
Liang, P., Jordan, M., and Klein, D. (2009). Learn-
ing semantic correspondences with less supervi-
sion. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
1924
Lu, W. and Ng, H. T. (2011). A probabilis-
tic forest-to-string model for language generation
from typed lambda calculus expressions. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo,
L., and Fox, D. (2012a). A joint model of lan-
guage and perception for grounded attribute learn-
ing. Proceedings of the International Conference
on Machine Learning.
Matuszek, C., Herbst, E., Zettlemoyer, L. S., and
Fox, D. (2012b). Learning to parse natural lan-
guage commands to a robot control system. In
Proceedings of the International Symposium on
Experimental Robotics.
Mitchell, M., van Deemter, K., and Reiter, E.
(2011a). Applying machine learning to the choice
of size modifiers. In Proceedings of the PRE-
CogSci Workshop.
Mitchell, M., Van Deemter, K., and Reiter, E.
(2011b). Two approaches for generating size
modifiers. In Proceedings of the European Work-
shop on Natural Language Generation.
Mitchell, M., van Deemter, K., and Reiter, E. (2013).
Generating expressions that refer to visible ob-
jects. In Proceedings of Conference of the North
American Chapter of the Association for Compu-
tational Linguistics.
Ren, Y., Van Deemter, K., and Pan, J. Z. (2010).
Charting the potential of description logic for the
generation of referring expressions. In Proceed-
ings of the International Natural Language Gen-
eration Conference.
Scontras, G., Graff, P., and Goodman, N. D. (2012).
Comparing pluralities. Cognition, 123(1):190?
197.
Steedman, M. (2011). Taking Scope. The MIT Press.
Stone, M. (2000). On identifying sets. In Proceed-
ings of the International Conference on Natural
Language Generation.
van Deemter, K. (2002). Generating referring ex-
pressions: Boolean extensions of the incremental
algorithm. Computational Linguistics, 28:37?52.
van Deemter, K., Gatt, A., Sluis, I. v. d., and Power,
R. (2012a). Generation of referring expressions:
Assessing the incremental algorithm. Cognitive
Science, 36(5):799?836.
van Deemter, K., Gatt, A., van Gompel, R. P., and
Krahmer, E. (2012b). Toward a computational
psycholinguistics of reference production. Topics
in Cognitive Science, 4(2):166?183.
Viethen, J. and Dale, R. (2010). Speaker-dependent
variation in content selection for referring ex-
pression generation. In Proceedings of the Aus-
tralasian Language Technology Workshop.
Viethen, J., Mitchell, M., and Krahmer, E. (2013).
Graphs and spatial relations in the generation of
referring expressions. In Proceedings of the Eu-
ropean Workshop on Natural Language Genera-
tion.
White, M. and Rajkumar, R. (2009). Perceptron
reranking for ccg realization. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.
Winograd, T. (1972). Understanding natural lan-
guage. Cognitive Psychology, 3(1):1?191.
Zelle, J. and Mooney, R. (1996). Learning to parse
database queries using inductive logic program-
ming. In Proceedings of the National Conference
on Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2005). Learning
to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars.
In Proceedings of the Conference on Uncertainty
in Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2007). Online
learning of relaxed CCG grammars for parsing to
logical form. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
Zitnick, C. L. and Parikh, D. (2013). Bringing se-
mantics into focus using visual abstraction. In
IEEE Conference on Computer Vision and Pattern
Recognition.
1925
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1284?1295,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Morpho-syntactic Lexical Generalization
for CCG Semantic Parsing
Adrienne Wang
Computer Science & Engineering
University of Washington
Seattle, WA
axwang@cs.washington.edu
Tom Kwiatkowski
Allen Institute for AI
Seattle, WA
tomk@allenai.org
Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
In this paper, we demonstrate that
significant performance gains can be
achieved in CCG semantic parsing
by introducing a linguistically moti-
vated grammar induction scheme. We
present a new morpho-syntactic fac-
tored lexicon that models systematic
variations in morphology, syntax, and
semantics across word classes. The
grammar uses domain-independent
facts about the English language to
restrict the number of incorrect parses
that must be considered, thereby
enabling effective learning from less
data. Experiments in benchmark
domains match previous models with
one quarter of the data and provide
new state-of-the-art results with all
available data, including up to 45%
relative test-error reduction.
1 Introduction
Semantic parsers map sentences to formal
representations of their meaning (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005;
Liang et al., 2011). One common approach is
to induce a probabilistic CCG grammar, which
defines the meanings of individual words and
phrases and how to best combine them to an-
alyze complete sentences. There has been re-
cent work developing learning algorithms for
CCG semantic parsers (Kwiatkowski et al.,
2010; Artzi and Zettlemoyer, 2011) and using
them for applications ranging from question
answering (Cai and Yates, 2013b; Kwiatkowski
et al., 2013) to robot control (Matuszek et al.,
2012; Krishnamurthy and Kollar, 2013).
One key learning challenge for this style
of learning is to induce the CCG lexicon,
which lists possible meanings for each phrase
and defines a set of possible parses for
each sentence. Previous approaches have
either hand-engineered a small set of lexi-
cal templates (Zettlemoyer and Collins, 2005,
2007) or automatically learned such tem-
plates (Kwiatkowski et al., 2010, 2011). These
methods are designed to learn grammars that
overgenerate; they produce spurious parses
that can complicate parameter estimation.
In this paper, we demonstrate that signif-
icant gains can instead be achieved by using
a more constrained, linguistically motivated
grammar induction scheme. The grammar
is restricted by introducing detailed syntac-
tic modeling of a wider range of constructions
than considered in previous work, for example
introducing explicit treatments of relational
nouns, Davidsonian events, and verb tense.
We also introduce a new lexical generalization
model that abstracts over systematic morpho-
logical, syntactic, and semantic alternations
within word classes. This includes, for exam-
ple, the facts that verbs in relative clauses and
nominal constructions (e.g., ?flights departing
NYC? and ?departing flights?) should be in-
finitival while verbs in phrases (e.g., ?What
flights depart at noon??) should have tense.
These grammar modeling techniques use uni-
versal, domain-independent facts about the
English language to restrict word usage to ap-
propriate syntactic contexts, and as such are
potentially applicable to any semantic parsing
application.
More specifically, we introduce a new
morpho-syntactic, factored CCG lexicon that
imposes our grammar restrictions during
learning. Each lexical entry has (1) a word
stem, automatically constructed from Wik-
tionary, with part-of-speech and morpholog-
ical attributes, (2) a lexeme that is learned
1284
and pairs the stem with semantic content that
is invariant to syntactic usage, and (3) a lexi-
cal template that specifies the remaining syn-
tactic and semantic content. The full set of
templates is defined in terms of a small set of
base templates and template transformations
that model morphological variants such as pas-
sivization and nominalization of verbs. This
approach allows us to efficiently encode a gen-
eral grammar for semantic parsing while also
eliminating large classes of incorrect analyses
considered by previous work.
We perform experiments in two benchmark
semantic parsing datasets: GeoQuery (Zelle
and Mooney, 1996) and ATIS (Dahl et al.,
1994). In both cases, our approach
achieves state-of-the-art performance, includ-
ing a nearly 45% relative error reduction on
the ATIS test set. We also show that the gains
increase with less data, including matching
previous model?s performance with less than
25% of the training data. Such gains are par-
ticularly practical for semantic parsers; they
can greatly reduce the amount of data that is
needed for each new application domain.
2 Related Work
Grammar induction methods for CCG seman-
tic parsers have either used hand-engineered
lexical templates, e.g. (Zettlemoyer and
Collins, 2005, 2007; Artzi and Zettlemoyer,
2011), or algorithms to learn such templates
directly from data, e.g. (Kwiatkowski et al.,
2010, 2011). Here, we extend the first ap-
proach, and show that better lexical general-
ization provides significant performance gains.
Although CCG is a common choice
for semantic parsers, many other for-
malisms have been studied, including DCS
trees (Liang et al., 2011), integer linear pro-
grams (Clarke et al., 2010), and synchronous
grammars (Wong and Mooney, 2007; Jones
et al., 2012; Andreas et al., 2013). All of these
approaches build complete meaning represen-
tations for individual sentences, but the data
we use has also been studied in related work on
cross-sentence reasoning (Miller et al., 1996;
Zettlemoyer and Collins, 2009) and model-
ing semantic interpretation as a tagging prob-
lem (Tur et al., 2013; Heck et al., 2013). Al-
though we focus on full analysis with CCG,
the general idea of using linguistic constraints
to improve learning is broadly applicable.
Semantic parsers are also commonly learned
from a variety of different types of supervision,
including logical forms (Kate and Mooney,
2006; Wong and Mooney, 2007; Muresan,
2011; Kwiatkowski et al., 2012), question-
answer pairs (Clarke et al., 2010; Liang et al.,
2011), conversational logs (Artzi and Zettle-
moyer, 2011), distant supervision (Krishna-
murthy and Mitchell, 2012; Cai and Yates,
2013b), sentences paired with system behav-
ior (Goldwasser and Roth, 2011; Chen and
Mooney, 2011; Artzi and Zettlemoyer, 2013b),
and even from database constraints with no
explicit semantic supervision (Poon, 2013).
We learn from logical forms, but CCG learn-
ing algorithms have been developed for each
case above, making our techniques applicable.
There has been significant related work that
influenced the design of our morpho-syntactic
grammars. This includes linguistics stud-
ies of relational nouns (Partee and Borschev,
1998; de Bruin and Scha, 1988), Davidsonian
events (Davidson, 1967), parsing as abduc-
tion (Hobbs et al., 1988), and other more gen-
eral theories for lexicons (Pustejovsky, 1991)
and CCG (Steedman, 2011). It also includes
work on using morphology in CCG syntac-
tic parsing (Honnibal et al., 2010) and more
broad-coverage semantics in CCG (Bos, 2008;
Lewis and Steedman, 2013). However, our
work is unique in studying the use of related
ideas for semantic parsing.
Finally, there has also been recent progress
on semantic parsing against large, open do-
main databases such as Freebase (Cai and
Yates, 2013a; Kwiatkowski et al., 2013; Berant
et al., 2013). Unfortuantely, existing Freebase
datasets are not a good fit to test our approach
because the sentences they include have rela-
tively simple structure and can be interepreted
accurately using only factoid lookups with no
database joins (Yao and Van Durme, 2014).
Our work focuses on learning more syntacti-
cally rich models that support compositional
reasoning.
3 Background
Lambda Calculus We represent the mean-
ings of sentences, words and phrases with
1285
list one way flights from various cities
S/N N/N N PP/NP NP/N N
?f.f ?f?x.oneway(x) ? f(x) ?x.flight(x) ?x?y.from(y, x) ?fAx.f(x) ?x.city(x)
>
NP
Ax.city(x)
>
PP
?x.from(x,Ay.city(y))
>T
N\N
?x.from(x,Ay.city(y))
<
N
?x.flight(x) ? from(x,Ay.city(y))
>
N
?x.flight(x) ? from(x,Ay.city(y)) ? oneway(x)
>
S
?x.flight(x) ? from(x,Ay.city(y)) ? oneway(x)
Figure 1: An example CCG parse.
lambda calculus logical expressions. We use a
version of the typed lambda calculus (Carpen-
ter, 1997), in which the basic types include en-
tities, events, truth values and numbers. Func-
tion types are assigned to lambda expressions.
The expression ?x.flight(x) with type ?e, t?
takes an entity and returns a truth value, and
represents a set of flights.
Combinatory Categorial Grammar
CCG (Steedman, 1996, 2000) is a formalism
that tightly couples syntax and semantics,
and can be used to model a wide range of
linguistic phenomena. A traditional CCG
grammar includes a lexicon ? with lexical
entries like the following:
flights ` N :?x.flight(x)
from ` PP/NP :?y.?x.from(x, y)
cities ` N :?x.city(x)
where a lexical item w `X : h has words w,
syntactic category X, and logical expression h.
CCG uses a small set of combinatory rules
to jointly build syntactic parses and semantic
representations. Two common combinatory
rules are forward (>) and backward (<)
application:
X/Y : f Y : g ? X : f(g) (>)
Y : g X\Y : f ? X : f(g) (<)
CCG also includes combinatory rules of for-
ward (> B) and backward (< B) composition:
X/Y : f Y/Z : g ? X/Z : ?x.f(g(x)) (> B)
Y \Z : g X\Y : f ? X\Z : ?x.f(g(x)) (< B)
These rules apply to build syntactic and se-
mantic derivations concurrently.
In this paper, we also implement type
raising rules for compact representation of
PP (prepositional phrase) and AP (adverbial
phrase).
PP : g ? N\N : ?f?x.f(x) ? g(x) (T)
AP : g ? S\S : ?f?e.f(e) ? g(e) (T)
AP : g ? S/S : ?f?e.f(e) ? g(e) (T)
Figure 1 shows an example CCG
parse (Steedman, 1996, 2000) where the
lexical entries are listed across the top and
the output lambda-calculus meaning repre-
sentation is at the bottom. This meaning is a
function (denoted by ?x...) that defines a set
of flights with certain properties and includes
a generalized Skolem constant (Steedman,
2011) (Ay...) that performs existential quan-
tification. Following recent work (Artzi and
Zettlemoyer, 2013b), we use meaning repre-
sentations that model a variety of linguistic
constructions, for example including Skolem
constants for plurals and Davidson quantifiers
for events, which we will introduce briefly
throughout this paper as they appear.
Weighted CCGs A weighted CCG gram-
mar is defined as G = (?,?), where ? is a
CCG lexicon and ? ? R
d
is a d-dimensional
parameter vector, which will be used to rank
the parses allowed under ?.
For a sentence x, G produces a set of candi-
1286
date parse trees Y = Y(x;G). Given a feature
vector ? ? R
d
, each parse tree y for sentence
x is scored by S(y; ?) = ? ??(x, y). The output
logical form z? is then defined to be at the root
of the highest-scoring parse y?:
y? = arg max
y?Y(x;G)
S(y; ?) (1)
We use existing CKY-style parsing algo-
rithms for this computation, implemented
with UW SPF (Artzi and Zettlemoyer, 2013a).
Section 7 describes the set of features we use
in the learned models.
Learning with GENLEX We will also
make use of an existing learning algo-
rithm (Zettlemoyer and Collins, 2007) (ZC07).
We first briefly review the ZC07 algorithm,
and describe our modifications in Section 7.
Given a set of training examples D =
{(x
i
, z
i
) : i = 1...n}, x
i
being the ith sentence
and z
i
being its annotated logical form, the al-
gorithm learns a set of parameters ? for the
grammar, while also inducing the lexicon ?.
The ZC07 learning algorithm uses a function
GENLEX(x, z) to define a set of lexical entries
that could be used to parse the sentence x to
construct the logical form z. For each training
example (x, z), GENLEX(x, z) maps all sub-
strings x to a set of potential lexical entries,
generated by exhaustively pairing the logical
constants in z using a set of hand-engineered
templates. The example is then parsed with
this much bigger lexicon and lexical entries
from the highest scoring parses are added to ?.
The parameters ? used to score parses are up-
dated using a perceptron learning algorithm.
4 Morpho-Syntactic Lexicon
This section defines our morpho-syntactic lex-
ical formalism. Table 1 shows examples of how
lexemes, templates, and morphological trans-
formations are used to build lexical entries for
example verbs. In this section, we formally de-
fine each of these components and show how
they are used to specify the space of possible
lexical entries that can be built for each input
word. In the following two sections, we will
provide more discussion of the complete sets
of templates (Section 5) and transformations
(Section 6).
Verb, Noun, Preposition, Pronoun, Adjective,
Adverb, Conjunction, Numeral, Symbol,
Proper Noun, Interjection, Expression
Table 2: Part-of-Speech types
POS Attribute Values
Noun Number singular, plural
Verb Person first, second, third
Verb Voice active, passive
Verb Tense present, past
Verb Aspect simple, progressive, perfect
Verb Participle present participle,
past participle
Adj, Degree of comparative, superlative
Adv, comparison
Det
Table 3: Morphological attributes and values.
We build on the factored CCG lexicon in-
troduced by Kwiatkowski et al. (2011) but (a)
further generalize lexemes to represent word
stems, (b) constrain the use of templates with
widely available syntactic information, and (c)
efficiently model common morphological vari-
ations between related words.
The first step, given an input word w, is
to do morphological and part-of-speech analy-
sis with the morpho-syntactic function F .
F maps a word to a set of possible morpho-
syntactic representations, each containing a
triple (s, p,m) of word stem s, part-of-speech
p and morphological category m. For exam-
ple, F maps the word flies to two possible
representations:
F (flies) = {(fly,Noun, (plural)),
(fly,Verb, (third, singular, simple, present))}
for the plural noun and present-tense verb
senses of the word. F is defined based on the
stems, part-of-speech types, and morpholog-
ical attributes marked for each definition in
Wiktionary.
1
The full sets of possible part-of-
speech and morphological types required for
our domains are shown in Table 2 and Table 3.
Each morpho-syntactic analysis a ? F (w)
is then paired with lexemes based on stem
match. A lexeme (s,~c) pairs a word stem
s with a list of logical constants ~c = [c
1
. . . c
k
].
Table 1 shows the words ?depart?, ?departing?,
?departure?, which are all assigned the lex-
eme (depart, [depart]). In general, there can
1
www.wiktionary.com
1287
Word Lexeme : Base Template Trans Lexical entry
depart
(depart, [depart]) :
I depart `S\NP :?x?e.depart(e, x)
departing I departing `S\NP :?x?e.depart(e, x)
departing
? `S\NP :?x?e.v
1
(e, x)
f
pres
departing `PP :?x?e.depart(e, x)
departure f
nom
departure `N :?x?e.depart(e, x)
use
(use, [airline]) :
I use `S\NP/NP :?x?y?e.airline(e, y, x)
using I using `S\NP/NP :?x?y?e.airline(e, y, x)
using
? `S\NP/NP :?x?y?e.v
1
(e, y, x)
f
pres
using `PP/NP :?x?e.airline(e, y, x)
use f
nom
use `N/NP :?x?y?e.airline(e, y, x)
Trans Template Transformation
f
pres
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `PP/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
f
nom
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
Table 1: Lexical entries constructed by combining a lexeme, base template, and transformation
for the intransitive verb ?depart? and the transitive verb ?use?.
be many different lexemes for each stem, that
vary in the selection of which logical constants
are included.
Given analysis (s, p,m) and lexeme (s,~c), we
can use a lexical template to construct a
lexical entry. Each template has the form:
?(?,~v).[? `X : h
~v
]
where ? and ~v are variables that abstract over
the words and logical constants that will be
used to define a lexical entry with syntax X
and templated logical form h
~v
.
To instantiate a template, ? is filled with the
original word w and the constants in ~c replace
the variables ~v. For example, the template
?(?,~v).[? ` S\NP : ?x?e.v
1
(e, x)] could be
used with the word ?departing? and the lexeme
(depart, [depart]) to produce the lexical entry
departing ` S\NP : ?x?e.depart(e, x). When
clear from context, we will omit the function
signature ?
p
(?,~v). for all templates, as seen in
Table 1.
In general, there can be many applicable
templates, which we organize as follows. Each
final template is defined by applying a mor-
phological transformation to one of a small
set of possible base templates. The pairing
is found based on the morphological analysis
(s, p,m), where each base template is associ-
ated with part-of-speech p and each transfor-
mation is indexed by the morphology m. A
transformation f
m
is a function:
f
m
(?
p
(?,~v).[? `X : h
~v
]) = ?
p
(?,~v).[? `X
?
: h
?
~v
]
that takes the base template as input and pro-
duces a new template to model the inflected
form specified by m.
For example, both base templates in Ta-
ble 1 are for verbs. The template ? `
S\NP : ?x?e.v
1
(e, x) can be translated into
three other templates based on the transfor-
mations I, f
pres
, and f
nom
, depending on the
analysis of the original words. These transfor-
mations generalize across word type; they can
be used for the transitive verb ?use? as well as
the intransitive ?depart.? Each resulting tem-
plate, potentially including the original input
if the identity transformation I is available,
can then be used to make an output lexical
entry, as we described above.
5 Lexical Templates
The templates in our lexicon, as introduced
in Section 4, model the syntactic and seman-
tic aspects of lexical entries that are shared
within each word class. Previous approaches
have also used hand-engineered lexical tem-
plates, as described in Section 2, but we dif-
fer by (1) using more templates allowing for
more fine grained analysis and (2) using word
class information to restrict template use, for
example ensuring that words which cannot be
verbs are never paired with templates designed
for verbs. This section describes the templates
used during learning, first presenting those de-
signed to model grammatical sentences and
then a small second set designed for more el-
liptical spoken utterances.
Base Forms Table 4 lists the primary tem-
plate set, where each row shows an example
with a sentence illustrating its use. Templates
are also grouped by the word classes, including
adjectives, adverbs, prepositions, and several
types of nouns and verbs. While there is not
enough space to discuss each row, it is worth
1288
word class example usage base template
Noun phrase Boston ? `NP : v
Noun (regular) What flight is provided by delta? ? `N : ?x.v(x)
Noun (relation) I need fares of flights ? `N/PP : ?x?y.v(x, y)
delta schedule ? `N\(N/N) : ?f?x.v(Ay.f(?z.true, y), x)
Noun (function) size of California ? `NP/NP : ?x.v(x)
V
intrans
What flights depart from New York? ? `S\NP : ?x?e.v(e, x)
V
trans
Which airlines serve Seattle (active verb) ? `S\NP/NP :?x?y?e.v(e, y, x)
What airlines have flights (passive verb) ? `S\NP/NP :?x?y?e.v(e, x, y)
V
ditrans
They give him a book ? `S\NP/NP/NP : ?x?y?z?e.v(e, z, y, x)
V
imperson
It costs $500 to fly to Boston ? `S\NP/NP/NP :?x?y?z?e.v(e, y, x)
V
aux
The flights have arrived at Boston ? `S\NP/(S\NP ) :?f.f
? `S/NP/(S/NP ) :?f.f
Does delta provide flights from Seattle? ? `S/S :?f.f
V
copula
The flights are from Boston ? `S\NP/PP :?f?x.f(x)
What flight is cheap? ? `S\NP/(N/N) :?f?x.f(?y.true, x)
Alaska is the state with the most rivers ? `S\NP/NP :?x?y.equals(y, x)
Adjective I need a one way flight ? `N/N :?f?x.f(x) ? v(x)
Boston flights round trip ? `PP :?x.v(x)
How long is mississippi? ? `DEG :?x.v(x)
Preposition List flights from Boston ? `PP/NP :?x?y.v(y, x)
List flights that go to Dallas ? `AP/NP :?x?e.v(e, x)
List flights between Dallas and Boston ? `PP/NP/NP :?x?y?z.v
1
(z, x) ? v
2
(z, y)
What flights leave between 8am and 9am? ? `AP/NP/NP :?x?y?e.v
1
(e, x) ? v
2
(e, y)
Adverb Which flight departs daily? ? `AP :?e.v(e)
How early does the flight arrive? ? `DEG :?x.v(x)
Determiner Which airline has a flight from Boston? ? `NP/N :?fAx.f(x)
Table 4: Base templates that define different syntactic roles.
type example usage base template
t
elliptical
flights Newark to Cleveland ? `PP :?x.P (x, v)
flights arriving 2pm ? `AP :?e.P (e, v)
american airline from Denver ? `N :?x.P (x, v)
t
metonymy
List airlines from Seattle ? `N/PP :?f?x.v(x) ? P (Ay.f(y), x))
Shat airlines depart from Seattle? ? `N/(S\NP ) :?f?x.v(x) ? P (Ay.f(y), x)
fares from miami to New York ? `N/PP :?f?x.v(Ay.f(y), x)
Table 5: Base templates for ungrammatical linguistic phenomena
considering nouns as an illustrative example.
We model nouns as denoting a set of entities
that satisfy a given property. Regular nouns
are represented using unary predicates. Rela-
tional nouns syntactically function as regular
nouns but semantically describe sets of enti-
ties that have some relationship with a comple-
ment (Partee and Borschev, 1998). For exam-
ple, the relational noun fare describes a binary
relationship between flights and their price in-
formation, as we see in this parse:
fares of flights
N/PP PP/NP N
?x?y.fare(x, y) ?x.x ?x.flight(x)
>T
NP
Ax.flight(x)
>
PP
Ax.flight(x)
>
N
?x.fare(Ay.flight(y), x)
This analysis differs from previous ap-
proaches (Zettlemoyer and Collins, 2007),
where relational nouns were treated as regu-
lar nouns and prepositions introduced the bi-
nary relationship. The relational noun model
reduces lexical ambiguity for the prepositions,
which are otherwise highly polysemous.
Adjectives are nominal modifiers that take
a noun or a noun phrase as an argument and
add properties through conjunction. Preposi-
tions take nominal objects and function as ad-
jectival modifiers for nouns or adverbial modi-
fiers for verbs. Verbs can be subcategorized
by their grammatical structures into transi-
tive (V
trans
), intransitive (V
intrans
), imper-
sonal (V
imperson
), auxiliary (V
aux
) and copula
(V
copula
). Adverbs are verb modifiers defin-
ing aspects like time, rate and duration. The
adoption of event semantics allows adverbial
modifiers to be represented by predicates and
1289
linked by the shared events. Determiners pre-
cede nouns or noun phrases and distinguish
a reference of the noun. Following the gen-
eralized Skolem terms, we model determiners,
including indefinite and definite articles, as a
??e, t?, e? function that selects a unique indi-
vidual from a ?e, t?-typed function defining a
singleton set.
Missing Words The templates presented so
far model grammatically correct input. How-
ever, in dialogue domains such as ATIS, speak-
ers often omit words. For example, speak-
ers can drop the preposition ?from? in ?flights
from Newark to Cleveland? to create the ellip-
tical utterance ?flights Newark to Cleveland?.
We address this issue with the templates
t
elliptical
illustrated in Table 5. Each of these
adds a binary relation P to a lexeme with a
single entity typed constant. For our example,
the word ?Newark? could be assigned the lexi-
cal item Newark `PP : ?x.from(x, newark)
by selecting the first template and P = from.
Another common problem is the use of
metonymy. In the utterance ?What airlines
depart from New York??, the word ?airlines?
is used to reference flight services operated by
a specific airline. This is problematic because
the word ?depart? needs to modify an event of
type flight. We solve this with the t
metonymy
templates in Table 5. These introduce a binary
predicate P that would, in the case of our ex-
ample, map airlines on to the flights that they
operate.
The templates in Table 5 handle the ma-
jor cases of missing words seen in our data
and are more efficient than the approach taken
by (Zettlemoyer and Collins, 2007) who intro-
duced complex type shifting rules and relaxed
the grammar to allow every word order.
6 Morphological Transformations
Finally, the morpho-syntactic lexicon intro-
duces morphological transformations, which
are functions from base lexical templates to
lexical templates that model the syntactic and
semantic variation as the word is inflected.
These transformations allow us to compactly
model, for example, the facts that argument
order is reversed when moving from active to
passive forms of the same verb, and that the
subject can be omitted. To the best of our
knowledge, we are the first to study such trans-
formations for semantic parsing.
Table 6 shows the transformations. Each
row groups a set of transformations by linguis-
tic category, including singular vs. plural num-
ber, active vs. passive voice, and so on, and
also includes example sentences where the out-
put templates could be used. Again, for space,
we do not detail the motivation for every class,
but it is worth looking at some of the alterna-
tions for verbs and nouns as our prototypical
example.
Some verbs can act as noun modifiers. For
example, the present participle ?using? mod-
ifies ?flights? in ?flights using twa?. To
model this variation, we use the transforma-
tion f
present part
, a mapping that changes the
root of the verb signature S\NP to PP :
f
present part
: ? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
)
? ? `PP/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
where T = [,NP,NP/NP ] instantiates this
rule for verbs that take different sets of argu-
ments, effectively allowing any verb that is in
its finite or -ing form to behave syntactically
like a prepositional phrase.
Intransitive present participles can also act
as prenominal adjectival modifiers as in ?the
departing flight?. We add a second mapping
that maps the intransitive category S\NP to
the noun modifier N/N .
f
present part
: ? `S\NP :?x?e.v(e, x)
? ? `N/N : ?f?x?e.f(x) ? v(e, x)
Finally, verbal nouns have meanings derived
from actions typically described by verbs but
syntactically function as nouns. For example,
landing in the phrase ?landing from jfk? is the
gerundive use of the verb land. We add the
following mapping to f
present part
and f
nominal
:
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ?
? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
)
with T from above. This allows for reuse of the
same meaning across quite different syntac-
tic constructs, including for example ?flights
that depart from Boston? and ?departure from
Boston.?
1290
Template transformations f
m
Example usage
Plural Number (f
plural
)
I flight ? early flights
? `N : ?x.v(x) ? ? `NP :Ax.v(x) city ? flights to cities
Singular Number (f
singular
)
I flight ? flight
Possessive (f
possess
)
? `NP : v ? ? `N/N :?f?x.f(x) ? P (x, v) delta ? delta?s flights
? `N : ?x.v(x) ? ? `N/N :?f?x.f(x) ? P (x,Ay.v(y)) airline ? airline?s flights
Passive Voice (f
passive
)
? `Y/NP :?x
1
..x
n
?e.v(e, x
1
..x
n
) ? ? `Y/PP : ?x
1
..x
n
?e.v(e, x
n
..x
1
) serves ?is served by
? `Y/NP :?x
1
..x
n
?e.v(e, x
1
, .., x
n
) ? ? `Y : ?x
1
..x
n?1
?e.v(e, x
n?1
..x
1
) name ?city named Austin
Present Participle (f
present
)
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `PP/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
) use ?flights using twa
? `S\NP :?x?e.v(e, x) ? ? `N/N : ?f?x?e.f(x) ? v(e, x) arrive ?arriving flights
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
) land ? landings at jfk
Past Participle (f
past
)
? `S\NP/NP :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `PP/PP : ?x
1
..x
n
?e.v(e, x
1
..x
n
) use ? plane used by
Nominalization (f
nominal
)
? `S\NP/T :?x
1
..x
n
?e.v(e, x
n
..x
1
) ? ? `N/T : ?x
1
..x
n
?e.v(e, x
n
..x
1
) depart ? departure
Comparative (f
comp
)
? `DEG :?x.v(x) ? ? `PP/PP :?x?y.v(y) < v(x) short ? shorter
? `DEG :?x.v(x) ? ? `PP/PP :?x?y.v(y) > v(x) long ? longer
Superlative (f
super
)
? `DEG :?x.v(x) ? ? `NP/N :?f.argmin(?x.f(x), ?x.v(x)) short ? shortest
? `DEG :?x.v(x) ? ? `NP/N :?f.argmax(?x.f(x), ?x.v(x)) long ? longest
Table 6: Morphological transformations with examples. T = [,NP,NP/NP ] and Y =
[S\NP,S\NP/NP ] allow a single transformation to generalize across word type.
Nouns can be inflected by number to de-
note singular and plural forms or by adding
an apostrophe to mark a possessive case. The
transformation function f
singular
is an identity
transformation. Plurals may have different in-
terpretations: one is the generic ?e, t? set rep-
resentation, which requires no transformation
on the base, or plurals can occur without overt
determiners (bare plurals), but semantically
imply quantification. We create a plural to
singular type shifting rule which implements
the ??e, t?, e? skolem function to select a unique
individual from the set. The possessive trans-
formation f
possess
transfers the base template
to a noun modifier, and adds a binary predi-
cate P that encodes the relation.
There are also a number of instances of the
identity transformation function I, which does
not change the base template. Because the se-
mantics we are constructing was designed to
answer questions against a static database, it
does not need to represent certain phenomena
to return the correct answer. This includes
more advanced variants of person, tense, as-
pect, and potentially many others. Ideally,
these morphological attributes should add se-
mantic modifiers to the base meaning, for ex-
ample, tense can constrain the time at which
an event occurs. However, none of our do-
mains support such reasoning, so we assign the
identity transformation, and leave the explo-
ration of these issues to future work.
7 Learning
One advantage of our morpho-syntactic, fac-
tored lexicon is that it can be easily learned
with small modifications to existing algo-
rithms (Zettlemoyer and Collins, 2007). We
only need to modify the GENLEX proce-
dure that defines the space of possible lexi-
cal entries. For each training example (x, z),
GENLEX(x, z, F ) first maps each substring in
the sentence x into the morphological repre-
sentation (s, p, c) using F introduced in Sec-
tion 4. A candidate lexeme set L
?
is then gen-
erated by exhaustively pairing the word stems
with all subsets of the logical constants from
z. Lexical templates are applied to the lexemes
in L
?
to generate candidate lexical entries for
x. Finally, the lexemes that participate in the
top scoring correct parse of x are added to the
permanent lexicon.
Initialization Following standard practice,
we compile an initial lexicon ?
0
, which con-
sists of a list of domain independent lexical
1291
items for function words, such as interrogative
words and conjunctions. These lexical items
are mostly semantically vacuous and serve par-
ticular syntactic functions that are not gener-
alizable to other word classes. We also initial-
ize the lexemes with a list of NP entities com-
plied from the database, e.g., (Boston, [bos]).
Features We use two types of features in
the model for discriminating parses. Four lex-
ical features are fired on each lexical item:
?
(s,~c)
for the lexeme, ?
t
p
for the base tem-
plate, ?
t
m
for the morphologically modified
template, and ?
l
for the complete lexical
item. We also compute the standard logical
expression features (Zettlemoyer and Collins,
2007) on the root semantics to track the pair-
wise predicate-argument relations and the co-
occuring predicate-predicate relations in con-
junctions and disjunctions.
8 Experimental Setup
Data and Metrics We evaluate perfor-
mance on two benchmark semantic pars-
ing datasets, Geo880 and ATIS. We use
the standard data splits, including 600/280
train/test for Geo880 and 4460/480/450
train/develop/test for ATIS. To support the
new representations in Section 5, we sys-
tematically convert annotations with existen-
tial quantifiers, temporal events and relational
nouns to new logical forms with equivalent
meanings. All systems are evaluated with ex-
act match accuracy, the percentage of fully
correct logical forms.
Initialization We assign positive initial
weights to the indicator features for entries in
the initial lexicon, as defined in Section 7, to
encourage their use. The elliptical template
and metonymy template features are initial-
ized with negative weights to initially discour-
age word skipping.
Comparison Systems We compare perfor-
mance with all recent CCG grammar induc-
tion algorithms that work with our datasets.
This includes methods that used a limited
set of hand-engineered templates for inducing
the lexicon, ZC05 (Zettlemoyer and Collins,
2005) and ZC07 (Zettlemoyer and Collins,
2007), and those that learned grammar struc-
ture by automatically splitting the labeled log-
System Test
ZC05 79.3
ZC07 86.1
UBL 87.9
FUBL 88.6
DCS 87.9
FULL 90.4
DCS
+
91.1
Table 7: Exact-match Geo880 test accuracy.
System Dev Test
ZC07 74.4 84.6
UBL 65.6 71.4
FUBL 81.9 82.8
GUSP - 83.5
TEMP-ONLY 85.5 87.2
FULL 87.5 91.3
Table 8: Exact-match accuracy on the ATIS
development and test sets.
ical forms, UBL (Kwiatkowski et al., 2010)
and FUBL (Kwiatkowski et al., 2011). We
also compare the state-of-the-art for Geo880
(DCS (Liang et al., 2011) and DCS+ which in-
cludes an engineered seed lexicon) and ATIS
(which is ZC07). Finally, we include results
for GUSP (Poon, 2013), a recent unsupervised
approach for ATIS.
System Variants We report results for a
complete approach (Full), and variants which
use different aspects of the morpho-syntactic
lexicon. The TEMP-ONLY variant learned
with the templates from Section 5 but, like
ZC07, does not use any word class information
to restrict their use. The TEMP-POS removes
morphology from the lexemes, but includes the
word class information from Wiktionary. Fi-
nally, we also include DCS
+
, which initialize a
set of words with POS tag JJ, NN, and NNS.
9 Results
Full Models Tables 7 and 8 report the
main learning results. Our approach achieves
state-of-the-art accuracies on both datasets,
demonstrating that our new grammar induc-
tion scheme provides a type of linguistically
motivated regularization; restricting the algo-
rithm to consider a much smaller hypothesis
space allows to learn better models.
1292
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 100  500  1000  2000  4460
Re
cal
l
Training samples
TEMP_ONLY
TEMP_POS
FULL
FUBL
Figure 2: ATIS Learning Curve
On Geo880 the full method edges out the
best systems by 2% absolute on the test set,
as compared to other systems with no domain-
specific lexical initialization. Although DCS
requires less supervision, it also uses external
signals including a POS tagger.
We see similarly strong results for ATIS,
outperforming FUBL on the ATIS develop-
ment set by 6.8%, and improving the accu-
racy on the test set by 7.9% over the previous
best system ZC07. Unlike FUBL, which excels
at the development set but trails ZC07?s tem-
plated grammar by almost 2 points on the test
set, our approach demonstrates consistent im-
provements on both. Additionally, although
the unsupervised model (GUSP) rivals previ-
ous approaches, we are able to show that more
careful use of supervision open a much wider
performance gap.
Learning Curve with Ablations Figure 2
presents a learning curve for the ATIS domain,
demonstrating that the learning improvements
become even more dramatic for smaller train-
ing set sizes. Our model outperforms FUBL by
wide margins, matching its final accuracy with
only 22% of the total training examples. Our
full model also consistently beats the variants
with fewer word class restrictions, although
by smaller margins. Again, these results fur-
ther highlight the benefit of importing external
syntactic resources and enforcing linguistically
motivated constraints during learning.
Learned Lexicon The learned lexicon is
also more compact. Table 9 summarizes
statistics on unique lexical entries required
to parse the ATIS development set. The
System Lexical Entries Lexemes
FUBL 1019 721
Our Approach 818 495
Table 9: Lexicon size comparison on the ATIS
dev set (460 unique tokens).
morpho-syntactic model uses 80.3% of the lex-
ical entries and 63.7% of the lexemes that
FUBL needs, while increase performance by
nearly 7 points. Upon inspection, our model
achieves better lexical decomposition by learn-
ing shorter lexical units, for example, the
adoption of Davidsonian events allows us to
learn unambiguous adverbial modifiers, and
the formal modeling of nominalized nouns and
relational nouns treats prepositions as syntac-
tic modifiers, instead of being encoded in the
semantics. Such restrictions generalize to a
much wider variety of syntactic contexts.
10 Summary and Future Work
We demonstrated that significant performance
gains can be achieved in CCG semantic pars-
ing by introducing a more constrained, linguis-
tically motivated grammar induction scheme.
We introduced a morpho-syntactic factored
lexicon that uses domain-independent facts
about the English language to restrict the
number of incorrect parses that must be con-
sidered and demonstrated empirically that it
enables effective learning of complete parsers,
achieving state-of-the-art performance.
Because our methods are domain indepen-
dent they should also benefit other semantic
parsing applications and other learning algo-
rithms that use different types of supervision,
as we hope to verify in future work. We would
also like to study how to generalize these gains
to languages other than English, by inducing
more of the syntactic structure.
Acknowledgements
The research was supported by the NSF (IIS-
1115966, IIS-1252835) and the Intel Center
for Pervasive Computing at the Univeristy
of Washington. The authors thank Robert
Gens, Xiao Ling, Xu Miao, Mark Yatskar and
the UW NLP group for helpful discussions,
and the anonymous reviewers for helpful com-
ments.
1293
References
Andreas, J., Vlachos, A., and Clark, S. (2013).
Semantic parsing as machine translation.
Artzi, Y. and Zettlemoyer, L. (2011). Boot-
strapping semantic parsers from conversa-
tions. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing.
Artzi, Y. and Zettlemoyer, L. (2013a). UW
SPF: The University of Washington Seman-
tic Parsing Framework.
Artzi, Y. and Zettlemoyer, L. (2013b). Weakly
supervised learning of semantic parsers for
mapping instructions to actions. Transac-
tions of the Association for Computational
Linguistics, 1(1):49?62.
Berant, J., Chou, A., Frostig, R., and Liang, P.
(2013). Semantic parsing on freebase from
question-answer pairs. In Proceedings of the
Conference on Empirical Methods in Natu-
ral Language Processing.
Bos, J. (2008). Wide-coverage semantic anal-
ysis with boxer. In Proceedings of the Con-
ference on Semantics in Text Processing.
Cai, Q. and Yates, A. (2013a). Large-scale
semantic parsing via schema matching and
lexicon extension. In Proceedings of the An-
nual Meeting of the Association for Compu-
tational Linguistics.
Cai, Q. and Yates, A. (2013b). Semantic pars-
ing freebase: Towards open-domain seman-
tic parsing. In Proceedings of the Joint Con-
ference on Lexical and Computational Se-
mantics.
Carpenter, B. (1997). Type-Logical Semantics.
The MITPress.
Chen, D. and Mooney, R. (2011). Learning
to interpret natural language navigation in-
structions from observations. In Proceedings
of the National Conference on Artificial In-
telligence.
Clarke, J., Goldwasser, D., Chang, M., and
Roth, D. (2010). Driving semantic parsing
from the world?s response. In Proceedings
of the Conference on Computational Natural
Language Learning.
Dahl, D. A., Bates, M., Brown, M., Fisher,
W., Hunicke-Smith, K., Pallett, D., Pao, C.,
Rudnicky, A., and Shriberg, E. (1994). Ex-
panding the scope of the atis task: The atis-
3 corpus. In Proceedings of the workshop on
Human Language Technology.
Davidson, D. (1967). The logical form of
action sentences. Essays on actions and
events, pages 105?148.
de Bruin, J. and Scha, R. (1988). The interpre-
tation of relational nouns. In Proceedings of
the Conference of the Association of Com-
putational Linguistics, pages 25?32. ACL.
Goldwasser, D. and Roth, D. (2011). Learning
from natural instructions. In Proceedings of
the International Joint Conference on Arti-
ficial Intelligence.
Heck, L., Hakkani-Tu?r, D., and Tur, G.
(2013). Leveraging knowledge graphs for
web-scale unsupervised semantic parsing. In
Proc. of the INTERSPEECH.
Hobbs, J. R., Stickel, M., Martin, P., and Ed-
wards, D. (1988). Interpretation as abduc-
tion. In Proceedings of the Association for
Computational Linguistics.
Honnibal, M., Kummerfeld, J. K., and Cur-
ran, J. R. (2010). Morphological analysis
can improve a ccg parser for english. In Pro-
ceedings of the International Conference on
Computational Linguistics.
Jones, B. K., Johnson, M., and Goldwater, S.
(2012). Semantic parsing with bayesian tree
transducers. In Proceedings of Association
of Computational Linguistics.
Kate, R. and Mooney, R. (2006). Using string-
kernels for learning semantic parsers. In
Proceedings of the Conference of the Asso-
ciation for Computational Linguistics.
Krishnamurthy, J. and Kollar, T. (2013).
Jointly learning to parse and perceive: Con-
necting natural language to the physical
world. Transactions of the Association for
Computational Linguistics, 1(2).
Krishnamurthy, J. and Mitchell, T. (2012).
Weakly supervised training of semantic
parsers. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Lan-
guage Processing and Computational Natu-
ral Language Learning.
Kwiatkowski, T., Choi, E., Artzi, Y., and
1294
Zettlemoyer, L. (2013). Scaling semantic
parsers with on-the-fly ontology matching.
Kwiatkowski, T., Goldwater, S., Zettlemoyer,
L., and Steedman, M. (2012). A probabilis-
tic model of syntactic and semantic acquisi-
tion from child-directed utterances and their
meanings. Proceedings of the Conference of
the European Chapter of the Association of
Computational Linguistics.
Kwiatkowski, T., Zettlemoyer, L., Goldwa-
ter, S., and Steedman, M. (2010). Induc-
ing probabilistic CCG grammars from log-
ical form with higher-order unification. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwa-
ter, S., and Steedman, M. (2011). Lexical
generalization in CCG grammar induction
for semantic parsing. In Proceedings of the
Conference on Empirical Methods in Natu-
ral Language Processing.
Lewis, M. and Steedman, M. (2013). Com-
bined distributional and logical semantics.
Transactions of the Association for Compu-
tational Linguistics, 1:179?192.
Liang, P., Jordan, M., and Klein, D. (2011).
Learning dependency-based compositional
semantics. In Proceedings of the Conference
of the Association for Computational Lin-
guistics.
Matuszek, C., FitzGerald, N., Zettlemoyer, L.,
Bo, L., and Fox, D. (2012). A joint model
of language and perception for grounded at-
tribute learning. In Proceedings of the Inter-
national Conference on Machine Learning.
Miller, S., Stallard, D., Bobrow, R., and
Schwartz, R. (1996). A fully statistical ap-
proach to natural language interfaces. In
Proceedings Association for Computational
Linguistics.
Muresan, S. (2011). Learning for deep lan-
guage understanding. In Proceedings of the
International Joint Conference on Artificial
Intelligence.
Partee, B. H. and Borschev, V. (1998). Inte-
grating lexical and formal sematics: Gen-
itives, relational nouns, and type-shifting.
In Proceedings of the Second Tbilisi Sympo-
sium on Language, Logic, and Computation.
Poon, H. (2013). Grounded unsupervised se-
mantic parsing. In Association for Compu-
tational Linguistics (ACL).
Pustejovsky, J. (1991). The generative lexicon.
volume 17.
Steedman, M. (1996). Surface Structure and
Interpretation. The MIT Press.
Steedman, M. (2000). The Syntactic Process.
The MIT Press.
Steedman, M. (2011). Taking Scope. The MIT
Press.
Tur, G., Deoras, A., and Hakkani-Tur, D.
(2013). Semantic parsing using word con-
fusion networks with conditional random
fields. In Proc. of the INTERSPEECH.
Wong, Y. and Mooney, R. (2007). Learning
synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the
Conference of the Association for Computa-
tional Linguistics.
Yao, X. and Van Durme, B. (2014). Informa-
tion extraction over structured data: Ques-
tion answering with freebase. In Association
for Computational Linguistics (ACL).
Zelle, J. and Mooney, R. (1996). Learning to
parse database queries using inductive logic
programming. In Proceedings of the Na-
tional Conference on Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2005). Learn-
ing to map sentences to logical form: Struc-
tured classification with probabilistic cate-
gorial grammars. In Proceedings of the Con-
ference on Uncertainty in Artificial Intelli-
gence.
Zettlemoyer, L. and Collins, M. (2007). On-
line learning of relaxed CCG grammars for
parsing to logical form. In Proceedings of
the Joint Conference on Empirical Methods
in Natural Language Processing and Com-
putational Natural Language Learning.
Zettlemoyer, L. and Collins, M. (2009). Learn-
ing context-dependent mappings from sen-
tences to logical form. In Proceedings of
the Joint Conference of the Association
for Computational Linguistics and Interna-
tional Joint Conference on Natural Lan-
guage Processing.
1295
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 234?244,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A Probabilistic Model of Syntactic and Semantic Acquisition from
Child-Directed Utterances and their Meanings
Tom Kwiatkowski* ?
tomk@cs.washington.edu
Sharon Goldwater?
sgwater@inf.ed.ac.uk
Luke Zettlemoyer?
lsz@cs.washington.edu
Mark Steedman?
steedman@inf.ed.ac.uk
? ILCC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Computer Science & Engineering
University of Washington
Seattle, WA, 98195, USA
Abstract
This paper presents an incremental prob-
abilistic learner that models the acquis-
tion of syntax and semantics from a cor-
pus of child-directed utterances paired with
possible representations of their meanings.
These meaning representations approxi-
mate the contextual input available to the
child; they do not specify the meanings of
individual words or syntactic derivations.
The learner then has to infer the meanings
and syntactic properties of the words in the
input along with a parsing model. We use
the CCG grammatical framework and train
a non-parametric Bayesian model of parse
structure with online variational Bayesian
expectation maximization. When tested on
utterances from the CHILDES corpus, our
learner outperforms a state-of-the-art se-
mantic parser. In addition, it models such
aspects of child acquisition as ?fast map-
ping,? while also countering previous crit-
icisms of statistical syntactic learners.
1 Introduction
Children learn language by mapping the utter-
ances they hear onto what they believe those ut-
terances mean. The precise nature of the child?s
prelinguistic representation of meaning is not
known. We assume for present purposes that
it can be approximated by compositional logical
representations such as (1), where the meaning is
a logical expression that describes a relationship
have between the person you refers to and the
object another(x, cookie(x)):
Utterance : you have another cookie (1)
Meaning : have(you, another(x, cookie(x)))
Most situations will support a number of plausi-
ble meanings, so the child has to learn in the face
of propositional uncertainty1, from a set of con-
textually afforded meaning candidates, as here:
Utterance : you have another cookie
Candidate
Meanings
?
?
?
have(you, another(x, cookie(x)))
eat(you, your(x, cake(x)))
want(i, another(x, cookie(x)))
The task is then to learn, from a sequence of such
(utterance, meaning-candidates) pairs, the correct
lexicon and parsing model. Here we present a
probabilistic account of this task with an empha-
sis on cognitive plausibility.
Our criteria for plausibility are that the learner
must not require any language-specific informa-
tion prior to learning and that the learning algo-
rithm must be strictly incremental: it sees each
training instance sequentially and exactly once.
We define a Bayesian model of parse structure
with Dirichlet process priors and train this on a
set of (utterance, meaning-candidates) pairs de-
rived from the CHILDES corpus (MacWhinney,
2000) using online variational Bayesian EM.
We evaluate the learnt grammar in three ways.
First, we test the accuracy of the trained model
in parsing unseen utterances onto gold standard
annotations of their meaning. We show that
it outperforms a state-of-the-art semantic parser
(Kwiatkowski et al 2010) when run with similar
training conditions (i.e., neither system is given
the corpus based initialization originally used by
Kwiatkowski et al. We then examine the learn-
ing curves of some individual words, showing that
the model can learn word meanings on the ba-
sis of a single exposure, similar to the fast map-
ping phenomenon observed in children (Carey
and Bartlett, 1978). Finally, we show that our
1Similar to referential uncertainty but relating to propo-
sitions rather than referents.
234
learner captures the step-like learning curves for
word order regularities that Thornton and Tesan
(2007) claim children show. This result coun-
ters Thornton and Tesan?s criticism of statistical
grammar learners?that they tend to exhibit grad-
ual learning curves rather than the abrupt changes
in linguistic competence observed in children.
1.1 Related Work
Models of syntactic acquisition, whether they
have addressed the task of learning both syn-
tax and semantics (Siskind, 1992; Villavicencio,
2002; Buttery, 2006) or syntax alone (Gibson
and Wexler, 1994; Sakas and Fodor, 2001; Yang,
2002) have aimed to learn a single, correct, deter-
ministic grammar. With the exception of Buttery
(2006) they also adopt the Principles and Param-
eters grammatical framework, which assumes de-
tailed knowledge of linguistic regularities2. Our
approach contrasts with all previous models in as-
suming a very general kind of linguistic knowl-
edge and a probabilistic grammar. Specifically,
we use the probabilistic Combinatory Categorial
Grammar (CCG) framework, and assume only
that the learner has access to a small set of general
combinatory schemata and a functional mapping
from semantic type to syntactic category. Further-
more, this paper is the first to evaluate a model
of child syntactic-semantic acquisition by parsing
unseen data.
Models of child word learning have focused
on semantics only, learning word meanings from
utterances paired with either sets of concept sym-
bols (Yu and Ballard, 2007; Frank et al 2008; Fa-
zly et al 2010) or a compositional meaning rep-
resentation of the type used here (Siskind, 1996).
The models of Alishahi and Stevenson (2008)
and Maurits et al(2009) learn, as well as word-
meanings, orderings for verb-argument structures
but not the full parsing model that we learn here.
Semantic parser induction as addressed by
Zettlemoyer and Collins (2005, 2007, 2009), Kate
and Mooney (2007), Wong and Mooney (2006,
2007), Lu et al(2008), Chen et al(2010),
Kwiatkowski et al(2010, 2011) and Bo?rschinger
et al(2011) has the same task definition as the
one addressed by this paper. However, the learn-
ing approaches presented in those previous pa-
2This linguistic use of the term ?parameter? is distinct
from the statistical use found elsewhere in this paper.
pers are not designed to be cognitively plausible,
using batch training algorithms, multiple passes
over the data, and language specific initialisations
(lists of noun phrases and additional corpus statis-
tics), all of which we dispense with here. In
particular, our approach is closely related that of
Kwiatkowski et al(2010) but, whereas that work
required careful initialisation and multiple passes
over the training data to learn a discriminative
parsing model, here we learn a generative parsing
model without either.
1.2 Overview of the approach
Our approach takes, as input, a corpus of (ut-
terance, meaning-candidates) pairs {(si, {m}i) :
i = 1, . . . , N}, and learns a CCG lexicon ? and
the probability of each production a ? b that
could be used in a parse. Together, these define
a probabilistic parser that can be used to find the
most probable meaning for any new sentence.
We learn both the lexicon and production prob-
abilities from allowable parses of the training
pairs. The set of allowable parses {t} for a sin-
gle (utterance, meaning-candidates) pair consists
of those parses that map the utterance onto one of
the meanings. This set is generated with the func-
tional mapping T :
{t} = T (s,m), (2)
which is defined, following Kwiatkowski et al
(2010), using only the CCG combinators and a
mapping from semantic type to syntactic category
(presented in in Section 4).
The CCG lexicon ? is learnt by reading off
the lexical items used in all parses of all training
pairs. Production probabilities are learnt in con-
junction with ? through the use of an incremen-
tal parameter estimation algorithm, online Varia-
tional Bayesian EM, as described in Section 5.
Before presenting the probabilistic model, the
mapping T , and the parameter training algorithm,
we first provide some background on the meaning
representations we use and on CCG.
2 Background
2.1 Meaning Representations
We represent the meanings of utterances in first-
order predicate logic using the lambda-calculus.
An example logical expression (henceforth also
referred to as a lambda expression) is:
like(eve,mummy) (3)
235
which expresses a logical relationship like be-
tween the entity eve and the entity mummy. In
Section 6.1 we will see how logical expressions
like this are created for a set of child-directed ut-
terances (to use in training our model).
The lambda-calculus uses ? operators to define
functions. These may be used to represent func-
tional meanings of utterances but they may also be
used as a ?glue language?, to compose elements of
first order logical expressions. For example, the
function ?x?y.like(y, x) can be combined with
the object mummy to give the phrasal mean-
ing ?y.like(y,mummy) through the lambda-
calculus operation of function application.
2.2 CCG
Combinatory Categorial Grammar (CCG; Steed-
man 2000) is a strongly lexicalised linguistic for-
malism that tightly couples syntax and seman-
tics. Each CCG lexical item in the lexicon ? is
a triple, written as word ` syntactic category :
logical expression . Examples are:
You ` NP : you
read ` S\NP/NP : ?x?y.read(y, x)
the ` NP/N : ?f.the(x, f(x))
book ` N : ?x.book(x)
A full CCG category X : h has syntactic cate-
gory X and logical expression h. Syntactic cat-
egories may be atomic (e.g., S or NP) or com-
plex (e.g., (S\NP)/NP). Slash operators in com-
plex categories define functions from the range on
the right of the slash to the result on the left in
much the same way as lambda operators do in the
lambda-calculus. The direction of the slash de-
fines the linear order of function and argument.
CCG uses a small set of combinatory rules to
concurrently build syntactic parses and semantic
representations. Two example combinatory rules
are forward (>) and backward (<) application:
X/Y : f Y : g ? X : f(g) (>)
Y : g X\Y : f ? X : f(g) (<)
Given the lexicon above, the phrase ?You read the
book? can be parsed using these rules, as illus-
trated in Figure 1 (with additional notation dis-
cussed in the following section)..
CCG also includes combinatory rules of
forward (> B) and backward (< B) composition:
X/Y : f Y/Z : g ?X/Z : ?x.f(g(x)) (> B)
Y \Z : g X\Y : f ?X\Z : ?x.f(g(x)) (< B)
3 Modelling Derivations
The objective of our learning algorithm is to
learn the correct parameterisation of a probabilis-
tic model P (s,m, t) over (utterance, meaning,
derivation) triples. This model assigns a proba-
bility to each of the grammar productions a ? b
used to build the derivation tree t. The probabil-
ity of any given CCG derivation t with sentence
s and semantics m is calculated as the product of
all of its production probabilities.
P (s,m, t) =
?
a?b?t
P (b|a) (4)
For example, the derivation in Figure 1 contains
13 productions, and its probability is the product
of the 13 production probabilities. Grammar pro-
ductions may be either syntactic?used to build a
syntactic derivation tree, or lexical?used to gen-
erate logical expressions and words at the leaves
of this tree.
A syntactic production Ch ? R expands a
head node Ch into a result R that is either an
ordered pair of syntactic parse nodes ?Cl,Cr?
(for a binary production) or a single parse node
(for a unary production). Only two unary syn-
tactic productions are allowed in the grammar:
START? A to generate A as the top syntactic
node of a parse tree and A? [A]lex to indicate
that A is a leaf node in the syntactic derivation
and should be used to generate a logical expres-
sion and word. Syntactic derivations are built by
recursively applying syntactic productions to non-
leaf nodes in the derivation tree. Each syntactic
production Ch ? R has conditional probability
P (R|Ch). There are 3 binary and 5 unary syntac-
tic productions in Figure 1.
Lexical productions have two forms. Logical
expressions are produced from leaf nodes in the
syntactic derivation tree Alex ? m with condi-
tional probability P (m|Alex). Words are then pro-
duced from these logical expressions with condi-
tional probability P (w|m). An example logical
production from Figure 1 is [NP]lex ? you. An
example word production is you? You.
Every production a ? b used in a parse tree t
is chosen from the set of productions that could
be used to expand a head node a. If there are a
finite K productions that could expand a then a
K-dimensional Multinomial distribution parame-
terised by ?a can be used to model the categorical
236
START
Sdcl
NP
[NP]lex
you
You
Sdcl\NP
(Sdcl\NP)/NP
[(Sdcl\NP)/NP]lex
?x?y.read(y, x)
read
NP
NP/N
[NP/N]lex
?f?x.the(x, f(x))
the
N
[N]lex
?x.book(x)
book
Figure 1: Derivation of sentence You read the
book with meaning read(you, the(x, book(x))).
choice of production:
b ? Multinomial(?a) (5)
However, before training a model of language ac-
quisition the dimensionality and contents of both
the syntactic grammar and lexicon are unknown.
In order to maintain a probability model with
cover over the countably infinite number of pos-
sible productions, we define a Dirichlet Process
(DP) prior for each possible production head a.
For the production head a, DP (?a, Ha) assigns
some probability mass to all possible production
targets {b} covered by the base distribution Ha.
It is possible to use the DP as an infinite prior
from which the parameter set of a finite dimen-
sional Multinomial may be drawn provided that
we can choose a suitable partition of {b}. When
calculating the probability of an (s,m, t) triple,
the choice of this partition is easy. For any given
production head a there is a finite set of usable
production targets {b1, . . . , bk?1} in t. We create
a partition that includes one entry for each of these
along with a final entry {bk, . . . } that includes all
other ways in which a could be expanded in dif-
ferent contexts. Then, by applying the distribution
Ga drawn from the DP to this partition, we get a
parameter vector ?a that is equivalent to a draw
from a k dimensional Dirichlet distribution:
Ga ? DP (?a, Ha) (6)
?a = (Ga(b1), . . . , Ga(bk?1), Ga({bk, . . . })
? Dir(?aH(b1), . . . , ?aHa(bk?1), (7)
?aHa({bk, . . . }))
Together, Equations 4-7 describe the joint distri-
bution P (X,S, ?) over the observed training data
X = {(si, {m}i) : i = 1, . . . , N}, the latent vari-
ables S (containing the productions used in each
parse t) and the parsing parameters ?.
4 Generating Parses
The previous section defined a parameterisation
over parses assuming that the CCG lexicon ? was
known. In practice ? is empty prior to training
and must be populated with the lexical items from
parses t consistent with training pairs (s, {m}).
The set of allowed parses {t} is defined by the
function T from Equation 2. Here we review the
splitting procedure of Kwiatkowski et al(2010)
that is used to generate CCG lexical items and de-
scribe how it is used by T to create a packed chart
representation of all parses {t} that are consistent
with s and at least one of the meaning represen-
tations in {m}. In this section we assume that s
is paired at each point with only a single meaning
m. Later we will show how T is used multiple
times to create the set of parses consistent with s
and a set of candidate meanings {m}.
The splitting procedure takes as input a CCG
category X :h, such as NP : a(x, cookie(x)), and
returns a set of category splits. Each category split
is a pair of CCG categories (Cl :ml,Cr :mr) that
can be recombined to give X : h using one of the
CCG combinators in Section 2.2. The CCG cat-
egory splitting procedure has two parts: logical
splitting of the category semantics h; and syntac-
tic splitting of the syntactic category X. Each logi-
cal split of h is a pair of lambda expressions (f, g)
in the following set:
{(f, g) | h = f(g) ? h = ?x.f(g(x))}, (8)
which means that f and g can be recombined us-
ing either function application or function com-
position to give the original lambda expression
h. An example split of the lambda expression
h = a(x, cookie(x)) is the pair
(?y.a(x, y(x)), ?x.cookie(x)), (9)
where ?y.a(x, y(x)) applied to ?x.cookie(x) re-
turns the original expression a(x, cookie(x)).
Syntactic splitting assigns linear order and syn-
tactic categories to the two lambda expressions f
and g. The initial syntactic category X is split by
a reversal of the CCG application combinators in
Section 2.2 if f and g can be recombined to give
237
Syntactic Category Semantic Type Example Phrase
Sdcl ?ev, t? I took it ` Sdcl :?e.took(i, it, e)
St t I?m angry ` St :angry(i)
Swh ?e, ?ev, t?? Who took it? ` Swh :?x?e.took(x, it, e)
Sq ?ev, t? Did you take it? ` Sq :?e.Q(take(you, it, e))
N ?e, t? cookie `N:?x.cookie(x)
NP e John `NP:john
PP ?ev, t? on John ` PP:?e.on(john, e)
Figure 2: Atomic Syntactic Categories.
h with function application:
{(X/Y : f Y : g), (10)
(Y : g : X\Y : f)|h = f(g)}
or by a reversal of the CCG composition combi-
nators if f and g can be recombined to give hwith
function composition:
{(X/Z : f Z/Y : g, (11)
(Z\Y : g : X\Z : f)|h = ?x.f(g(x))}
Unknown category names in the result of a
split (Y in (10) and Z in (11)) are labelled via a
functional mapping cat from semantic type T to
syntactic category:
cat(T ) =
?
?
?
Atomic(T ) if T ? Figure 2
cat(T1)/cat(T2) if T = ?T1, T2?
cat(T1)\cat(T2) if T = ?T1, T2?
?
?
?
which uses the Atomic function illustrated
in Figure 2 to map semantic-type to basic CCG
syntactic category. As an example, the logical
split in (9) supports two CCG category splits, one
for each of the CCG application rules.
(NP/N :?y.a(x, y(x)), N :?x.cookie(x)) (12)
(N :?x.cookie(x), NP\N :?y.a(x, y(x))) (13)
The parse generation algorithm T uses the func-
tion split to generate all CCG category pairs that
are an allowed split of an input category X :h:
{(Cl :ml,Cr :mr)} = split(X :h),
and then packs a chart representation of {t} in a
top-down fashion starting with a single cell entry
Cm :m for the top node shared by all parses {t}.
For the utterance and meaning in (1) the top parse
node, spanning the entire word-string, is
S :have(you, another(x, cookie(x))).
T cycles over all cell entries in increasingly small
spans and populates the chart with their splits. For
any cell entry X :h spanning more than one word
T generates a set of pairs representing the splits of
X :h. For each split (Cl :ml,Cr :mr) and every bi-
nary partition (wi:k, wk:j) of the word-span T cre-
ates two new cell entries in the chart: (Cl :ml)i:k
and (Cr :mr)k:j .
Input : Sentence [w1, . . . , wn], top node Cm :m
Output: Packed parse chart Ch containing {t}
Ch = [ [{}1, . . . , {}n]1, . . . , [{}1, . . . , {}n]n ]
Ch[1][n? 1] = Cm :m
for i = n, . . . , 2; j = 1 . . . (n? i) + 1 do
for X:h ? Ch[j][i] do
for (Cl :ml,Cr :mr) ? split(X:h) do
for k = 1, . . . , i? 1 do
Ch[j][k]? Cl :ml
Ch[j + k][i? k]? Cr :mr
Algorithm 1: Generating {t} with T .
Algorithm 1 shows how the learner uses T to
generate a packed chart representation of {t} in
the chart Ch. The function T massively overgen-
erates parses for any given natural language. The
probabilistic parsing model introduced in Sec-
tion 3 is used to choose the best parse from the
overgenerated set.
5 Training
5.1 Parameter Estimation
The probabilistic model of the grammar describes
a distribution over the observed training data X,
latent variables S, and parameters ?. The goal of
training is to estimate the posterior distribution:
p(S, ?|X) = p(S,X|?)p(?)
p(X)
(14)
which we do with online Variational Bayesian Ex-
pectation Maximisation (oVBEM; Sato (2001),
Hoffman et al(2010)). oVBEM is an online
238
Bayesian extension of the EM algorithm that
accumulates observation pseudocounts na?b for
each of the productions a ? b in the grammar.
These pseudocounts define the posterior over pro-
duction probabilities as follows:
(?a?b1 , . . . , ?a?b{k,... })) | X,S ? (15)
Dir(?H(b1) + na?b1 , . . . ,
??
j=k
?H(bj) + na?bj )
These pseudocounts are computed in two steps:
oVBE-step For the training pair (si, {m}i)
which supports the set of parses {t}, the expec-
tation E{t}[a ? b] of each production a ? b is
calculated by creating a packed chart representa-
tion of {t} and running the inside-outside algo-
rithm. This is similar to the E-step in standard
EM apart from the fact that each production is
scored with the current expectation of its parame-
ter weight ??i?1a?b, where:
??i?1a?b =
e?(?aHa(a?b)+n
i?1
a?b)
e
?
(?K
{b?} ?aHa(a?b
?)+ni?1
a?b?
) (16)
and ? is the digamma function (Beal, 2003).
oVBM-step The expectations from the oVBE
step are used to update the pseudocounts in Equa-
tion 15 as follows,
nia?b = n
i?1
a?b + ?i(N ? E{t}[a? b]? ni?1a?b)
(17)
where ?i is the learning rate and N is the size of
the dataset.
5.2 The Training Algorithm
Now the training algorithm used to learn the lex-
icon ? and pseudocounts {na?b} can be defined.
The algorithm, shown in Algorithm 2, passes over
the training data only once and one training in-
stance at a time. For each (si, {m}i) it uses the
function T |{m}i| times to generate a set of con-
sistent parses {t}?. The lexicon is populated by
using the lex function to read all of the lexical
items off from the derivations in each {t}?. In
the parameter update step, the training algorithm
updates the pseudocounts associated with each of
the productions a ? b that have ever been seen
during training according to Equation (17).
Only non-zero pseudocounts are stored in our
model. The count vector is expanded with a new
entry every time a new production is used. While
Input : Corpus D = {(si, {m}i)|i = 1, . . . , N},
Function T , Semantics to syntactic cate-
gory mapping cat, function lex to read
lexical items off derivations.
Output: Lexicon ?, Pseudocounts {na?b}.
? = {}, {t} = {}
for i = 1, . . . , N do
{t}i = {}
form? ? {m}i do
Cm? = cat(m?)
{t}? = T (si,Cm? :m?)
{t}i = {t}i ? {t}?, {t} = {t} ? {t}?
? = ? ? lex ({t}?)
for a? b ? {t} do
nia?b = n
i?1
a?b + ?i(N ? E{t}i [a? b]?
ni?1a?b)
Algorithm 2: Learning ? and {na?b}
the parameter update step cycles over all produc-
tions in {t} it is not neccessary to store {t}, just
the set of productions that it uses.
6 Experimental Setup
6.1 Data
The Eve corpus, collected by Brown (1973), con-
tains 14, 124 English utterances spoken to a sin-
gle child between the ages of 18 and 27 months.
These have been hand annotated by Sagae et al
(2004) with labelled syntactic dependency graphs.
An example annotation is shown in Figure 3.
While these annotations are designed to rep-
resent syntactic information, the parent-child re-
lationships in the parse can also be viewed as a
proxy for the predicate-argument structure of the
semantics. We developed a template based de-
terministic procedure for mapping this predicate-
argument structure onto logical expressions of the
type discussed in Section 2.1. For example, the
dependency graph in Figure 3 is automatically
transformed into the logical expression
?e.have(you,another(y, cookie(y)), e) (18)
? on(the(z, table(z)), e),
where e is a Davidsonian event variable used to
deal with adverbial and prepositional attachments.
The deterministic mapping to logical expressions
uses 19 templates, three of which are used in this
example: one for the verb and its arguments, one
for the prepositional attachment and one (used
twice) for the quantifier-noun constructions.
239
SUBJ ROOT DET OBJ JCT DET POBJ
pro|you v|have qn|another n|cookie prep|on det|the n|table
You have another cookie on the table
Figure 3: Syntactic dependency graph from Eve corpus.
This mapping from graph to logical expression
makes use of a predefined dictionary of allowed,
typed, logical constants. The mapping is success-
ful for 31% of the child-directed utterances in the
Eve corpus3. The remaining data is mostly ac-
counted for by one-word utterances that have no
straightforward interpretation in our typed logi-
cal language (e.g. what; okay; alright; no; yeah;
hmm; yes; uhhuh; mhm; thankyou), missing ver-
bal arguments that cannot be properly guessed
from the context (largely in imperative sentences
such as drink the water), and complex noun con-
structions that are hard to match with a small set
of templates (e.g. as top to a jar). We also re-
move the small number of utterances containing
more than 10 words for reasons of computational
efficiency (see discussion in Section 8).
Following Alishahi and Stevenson (2010), we
generate a context set {m}i for each utterance si
by pairing that utterance with its correct logical
expression along with the logical expressions of
the preceding and following (|{m}i|?1)/2 utter-
ances.
6.2 Base Distributions and Learning Rate
Each of the production heads a in the grammar
requires a base distribution Ha and concentration
parameter ?a. For word-productions the base dis-
tribution is a geometric distribution over character
strings and spaces. For syntactic-productions the
base distribution is defined in terms of the new
category to be named by cat and the probability
of splitting the rule by reversing either the appli-
cation or composition combinators.
Semantic-productions? base distributions are
defined by a probabilistic branching process con-
ditioned on the type of the syntactic category.
This distribution prefers less complex logical ex-
pressions. All concentration parameters are set to
1.0. The learning rate for parameter updates is
?i = (0.8 + i)?0.5.
3Data available at www.tomkwiat.com/resources.html
0.0 0.2 0.4 0.6 0.8 1.0Proportion of Data Seen0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Accu
racy
Our ApproachOur Approach + Guess UBL
1
UBL10
Figure 4: Meaning Prediction: Train on files 1, . . . , n
test on file n+ 1.
7 Experiments
7.1 Parsing Unseen Sentences
We test the parsing model that is learnt by training
on the first i files of the longitudinally ordered Eve
corpus and testing on file i + 1, for i = 1 . . . 19.
For each utterance s? in the test file we use the
parsing model to predict a meaning m? and com-
pare this to the target meaning m?. We report the
proportion of utterances for which the prediction
m? is returned correctly both with and without
word-meaning guessing. When a word has never
been seen at training time our parser has the abil-
ity to ?guess? a typed logical meaning with place-
holders for constant and predicate names.
For comparison we use the UBL semantic
parser of Kwiatkowski et al(2010) trained in
a similar setting?i.e., with no language specific
initialisation4. Figure 4 shows accuracy for our
approach with and without guessing, for UBL
4Kwiatkowski et al(2010) initialise lexical weights in
their learning algorithm using corpus-wide alignment statis-
tics across words and meaning elements. Instead we run
UBL with small positive weight for all lexical items. When
run with Giza++ parameter initialisations, UBL10 achieves
48.1% across folds compared to 49.2% for our approach.
240
when run over the training data once (UBL1) and
for UBL when run over the training data 10 times
(UBL10) as in Kwiatkowski et al(2010). Each
of the points represents accuracy on one of the
19 test files. All of these results are from parsers
trained on utterances paired with a single candi-
date meaning. The lines of best fit show the up-
ward trend in parser performance over time.
Despite only seeing each training instance
once, our approach, due to its broader lexi-
cal search strategy, outperforms both versions of
UBL which performs a greedy search in the space
of lexicons and requires initialisation with co-
occurence statistics between words and logical
constants to guide this search. These statistics are
not justified in a model of language acquisition
and so they are not used here. The low perfor-
mance of all systems is due largely to the sparsity
of the data with 32.9% of all sentences containing
a previously unseen word.
7.2 Word Learning
Due to the sparsity of the data, the training algo-
rithm needs to be able to learn word-meanings on
the basis of very few exposures. This is also a de-
sirable feature from the perspective of modelling
language acquisition as Carey and Bartlett (1978)
have shown that children have the ability to learn
word meanings on the basis of one, or very few,
exposures through the process of fast mapping.
0 500 1000 1500 20000.0
0.20.4
0.60.8
1.0
P(m|w
) 1 Meaning
0 500 1000 1500 2000
3 Meanings
0 500 1000 1500 2000Number of Utterances0.0
0.20.4
0.60.8
1.0
P(m|w
) 5 Meanings
0 500 1000 1500 2000Number of Utterances
7 Meanings
f = 168 a? ?f.a(x, f (x))f = 10 another? ?f.another(x, f (x))f = 2 any? ?f.any(x, f (x))
Figure 5: Learning quantifiers with frequency f.
Figure 5 shows the posterior probability of the
correct meanings for the quantifiers ?a?, ?another?
and ?any? over the course of training with 1, 3,
5 and 7 candidate meanings for each utterance5.
These three words are all of the same class but
have very different frequencies in the training
subset shown (168, 10 and 2 respectively). In all
training settings, the word ?a? is learnt gradually
from many observations but the rarer words ?an-
other? and ?any? are learnt (when they are learnt)
through large updates to the posterior on the ba-
sis of few observations. These large updates re-
sult from a syntactic bootstrapping effect (Gleit-
man, 1990). When the model has great confidence
about the derivation in which an unseen lexical
item occurs, the pseudocounts for that lexical item
get a large update under Equation 17. This large
update has a greater effect on rare words which
are associated with small amounts of probability
mass than it does on common ones that have al-
ready accumulated large pseudocounts. The fast
learning of rare words later in learning correlates
with observations of word learning in children.
7.3 Word Order Learning
Figure 6 shows the posterior probability of the
correct SVO word order learnt from increasing
amounts of training data. This is calculated by
summing over all lexical items containing transi-
tive verb semantics and sampling in the space of
parse trees that could have generated them. With
no propositional uncertainty in the training data
the correct word order is learnt very quickly and
stabilises. As the amount of propositional uncer-
tainty increases, the rate at which this rule is learnt
decreases. However, even in the face of ambigu-
ous training data, the model can learn the cor-
rect word-order rule. The distribution over word
orders also exhibits initial uncertainty, followed
by a sharp convergence to the correct analysis.
This ability to learn syntactic regularities abruptly
means that our system is not subject to the crit-
icisms that Thornton and Tesan (2007) levelled
at statistical models of language acquisition?that
their learning rates are too gradual.
5The term ?fast mapping? is generally used to refer to
noun learning. We chose to examine quantifier learning here
as there is a greater variation in quantifier frequencies. Fast
mapping of nouns is also achieved.
241
0 500 1000 1500 2000Number of Utterances
7 Meanings
0 500 1000 1500 2000Number of Utterances0.00.2
0.40.6
0.81.0
P(word
order)
5 Meanings 0 500 1000 1500 2000
3 Meanings
0 500 1000 1500 20000.0
0.20.4
0.60.8
1.0
P(word
order)
1 Meaning
vsosvo ovssov vososv
Figure 6: Learning SVO word order.
8 Discussion
We have presented an incremental model of lan-
guage acquisition that learns a probabilistic CCG
grammar from utterances paired with one or
more potential meanings. The model assumes
no language-specific knowledge, but does assume
that the learner has access to language-universal
correspondences between syntactic and semantic
types, as well as a Bayesian prior encouraging
grammars with heavy reuse of existing rules and
lexical items. We have shown that this model
not only outperforms a state-of-the-art semantic
parser, but also exhibits learning curves similar
to children?s: lexical items can be acquired on a
single exposure and word order is learnt suddenly
rather than gradually.
Although we use a Bayesian model, our ap-
proach is different from many of the Bayesian
models proposed in cognitive science and lan-
guage acquisition (Xu and Tenenbaum, 2007;
Goldwater et al 2009; Frank et al 2009; Grif-
fiths and Tenenbaum, 2006; Griffiths, 2005; Per-
fors et al 2011). These models are intended
as ideal observer analyses, demonstrating what
would be learned by a probabilistically optimal
learner. Our learner uses a more cognitively plau-
sible but approximate online learning algorithm.
In this way, it is similar to other cognitively plau-
sible approximate Bayesian learners (Pearl et al
2010; Sanborn et al 2010; Shi et al 2010).
Of course, despite the incremental nature of our
learning algorithm, there are still many aspects
that could be criticized as cognitively implausi-
ble. In particular, it generates all parses consistent
with each training instance, which can be both
memory- and processor-intensive. It is unlikely
that children do this once they have learnt at least
some of the target language. In future, we plan
to investigate more efficient parameter estimation
methods. One possibility would be an approxi-
mate oVBEM algorithm in which the expectations
in Equation 17 are calculated according to a high
probability subset of the parses {t}. Another op-
tion would be particle filtering, which has been
investigated as a cognitively plausible method for
approximate Bayesian inference (Shi et al 2010;
Levy et al 2009; Sanborn et al 2010).
As a crude approximation to the context in
which an utterance is heard, the logical represen-
tations of meaning that we present to the learner
are also open to criticism. However, Steedman
(2002) argues that children do have access to
structured meaning representations from a much
older apparatus used for planning actions and we
wish to eventually ground these in sensory input.
Despite the limitations listed above, our ap-
proach makes several important contributions to
the computational study of language acquisition.
It is the first model to learn syntax and seman-
tics concurrently; previous systems (Villavicen-
cio, 2002; Buttery, 2006) learnt categorial gram-
mars from sentences where all word meanings
were known. Our model is also the first to be
evaluated by parsing sentences onto their mean-
ings, in contrast to the work mentioned above and
that of Gibson and Wexler (1994), Siskind (1992)
Sakas and Fodor (2001), and Yang (2002). These
all evaluate their learners on the basis of a small
number of predefined syntactic parameters.
Finally, our work addresses a misunderstand-
ing about statistical learners?that their learn-
ing curves must be gradual (Thornton and Tesan,
2007). By demonstrating sudden learning of word
order and fast mapping, our model shows that sta-
tistical learners can account for sudden changes in
children?s grammars. In future, we hope to extend
these results by examining other learning behav-
iors and testing the model on other languages.
9 Acknowledgements
We thank Mark Johnson for suggesting an analy-
sis of learning rates. This work was funded by the
ERC Advanced Fellowship 24952 GramPlus and
EU IP grant EC-FP7-270273 Xperience.
242
References
Alishahi and Stevenson, S. (2008). A computa-
tional model for early argument structure ac-
quisition. Cognitive Science, 32:5:789?834.
Alishahi, A. and Stevenson, S. (2010). Learning
general properties of semantic roles from usage
data: a computational model. Language and
Cognitive Processes, 25:1.
Beal, M. J. (2003). Variational algorithms for ap-
proximate Bayesian inference. Technical re-
port, Gatsby Institute, UCL.
Bo?rschinger, B., Jones, B. K., and Johnson, M.
(2011). Reducing grounded learning tasks
to grammatical inference. In Proceedings of
the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1416?
1425, Edinburgh, Scotland, UK. Association
for Computational Linguistics.
Brown, R. (1973). A First Language: the Early
Stages. Harvard University Press, Cambridge
MA.
Buttery, P. J. (2006). Computational models for
first language acquisition. Technical Report
UCAM-CL-TR-675, University of Cambridge,
Computer Laboratory.
Carey, S. and Bartlett, E. (1978). Acquring a sin-
gle new word. Papers and Reports on Child
Language Development, 15.
Chen, D. L., Kim, J., and Mooney, R. J. (2010).
Training a multilingual sportscaster: Using per-
ceptual context to learn language. J. Artif. In-
tell. Res. (JAIR), 37:397?435.
Fazly, A., Alishahi, A., and Stevenson, S. (2010).
A probabilistic computational model of cross-
situational word learning. Cognitive Science,
34(6):1017?1063.
Frank, M., Goodman, S., and Tenenbaum, J.
(2009). Using speakers referential intentions
to model early cross-situational word learning.
Psychological Science, 20(5):578?585.
Frank, M. C., Goodman, N. D., and Tenenbaum,
J. B. (2008). A bayesian framework for cross-
situational word-learning. Advances in Neural
Information Processing Systems 20.
Gibson, E. and Wexler, K. (1994). Triggers. Lin-
guistic Inquiry, 25:355?407.
Gleitman, L. (1990). The structural sources of
verb meanings. Language Acquisition, 1:1?55.
Goldwater, S., Griffiths, T. L., and Johnson, M.
(2009). A Bayesian framework for word seg-
mentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Griffiths, T. L., . T. J. B. (2005). Structure and
strength in causal induction. Cognitive Psy-
chology, 51:354?384.
Griffiths, T. L. and Tenenbaum, J. B. (2006). Op-
timal predictions in everyday cognition. Psy-
chological Science.
Hoffman, M., Blei, D. M., and Bach, F. (2010).
Online learning for latent dirichlet alcation.
In NIPS.
Kate, R. J. and Mooney, R. J. (2007). Learning
language semantics from ambiguous supervi-
sion. In Proceedings of the 22nd Conference
on Artificial Intelligence (AAAI-07).
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2010). Inducing proba-
bilistic CCG grammars from logical form with
higher-order unification. In Proceedings of the
Conference on Emperical Methods in Natural
Language Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2011). Lexical general-
ization in ccg grammar induction for semantic
parsing. In Proceedings of the Conference on
Emperical Methods in Natural Language Pro-
cessing.
Levy, R., Reali, F., and Griffiths, T. (2009). Mod-
eling the effects of memory on human online
sentence processing with particle filters. In Ad-
vances in Neural Information Processing Sys-
tems 21.
Lu, W., Ng, H. T., Lee, W. S., and Zettlemoyer,
L. S. (2008). A generative model for parsing
natural language to meaning representations. In
Proceedings of The Conference on Empirical
Methods in Natural Language Processing.
MacWhinney, B. (2000). The CHILDES project:
tools for analyzing talk. Lawrence Erlbaum,
Mahwah, NJ u.a. EN.
Maurits, L., Perfors, A., and Navarro, D. (2009).
Joint acquisition of word order and word refer-
ence. In Proceedings of the 31th Annual Con-
ference of the Cognitive Science Society.
Pearl, L., Goldwater, S., and Steyvers, M. (2010).
How ideal are we? Incorporating human limi-
243
tations into Bayesian models of word segmen-
tation. pages 315?326, Somerville, MA. Cas-
cadilla Press.
Perfors, A., Tenenbaum, J. B., and Regier, T.
(2011). The learnability of abstract syntactic
principles. Cognition, 118(3):306 ? 338.
Sagae, K., MacWhinney, B., and Lavie, A.
(2004). Adding syntactic annotations to tran-
scripts of parent-child dialogs. In Proceed-
ings of the 4th International Conference on
Language Resources and Evaluation. Lisbon,
LREC.
Sakas, W. and Fodor, J. D. (2001). The struc-
tural triggers learner. In Bertolo, S., editor,
Language Acquisition and Learnability, pages
172?233. Cambridge University Press, Cam-
bridge.
Sanborn, A. N., Griffiths, T. L., and Navarro,
D. J. (2010). Rational approximations to ratio-
nal models: Alternative algorithms for category
learning. Psychological Review.
Sato, M. (2001). Online model selection based
on the variational bayes. Neural Computation,
13(7):1649?1681.
Shi, L., Griffiths, T. L., Feldman, N. H., and San-
born, A. N. (2010). Exemplar models as a
mechanism for performing bayesian inference.
Psychonomic Bulletin & Review, 17(4):443?
464.
Siskind, J. M. (1992). Naive Physics, Event Per-
ception, Lexical Semantics, and Language Ac-
quisition. PhD thesis, Massachusetts Institute
of Technology.
Siskind, J. M. (1996). A computational study of
cross-situational techniques for learning word-
to-meaning mappings. Cognition, 61(1-2):1?
38.
Steedman, M. (2000). The Syntactic Process.
MIT Press, Cambridge, MA.
Steedman, M. (2002). Plans, affordances, and
combinatory grammar. Linguistics and Philos-
ophy, 25.
Thornton, R. and Tesan, G. (2007). Categori-
cal acquisition: Parameter setting in universal
grammar. Biolinguistics, 1.
Villavicencio, A. (2002). The acquisition of a
unification-based generalised categorial gram-
mar. Technical Report UCAM-CL-TR-533,
University of Cambridge, Computer Labora-
tory.
Wong, Y. W. and Mooney, R. (2006). Learning for
semantic parsing with statistical machine trans-
lation. In Proceedings of the Human Language
Technology Conference of the NAACL.
Wong, Y. W. and Mooney, R. (2007). Learn-
ing synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of
the Association for Computational Linguistics.
Xu, F. and Tenenbaum, J. B. (2007). Word learn-
ing as Bayesian inference. Psychological Re-
view, 114:245?272.
Yang, C. (2002). Knowledge and Learning in Nat-
ural Language. Oxford University Press, Ox-
ford.
Yu, C. and Ballard, D. H. (2007). A unified model
of early word learning: Integrating statisti-
cal and social cues. Neurocomputing, 70(13-
15):2149 ? 2165.
Zettlemoyer, L. S. and Collins, M. (2005). Learn-
ing to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Conference on
Uncertainty in Artificial Intelligence.
Zettlemoyer, L. S. and Collins, M. (2007). Online
learning of relaxed CCG grammars for pars-
ing to logical form. In Proc. of the Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning.
Zettlemoyer, L. S. and Collins, M. (2009). Learn-
ing context-dependent mappings from sen-
tences to logical form. In Proceedings of The
Joint Conference of the Association for Com-
putational Linguistics and International Joint
Conference on Natural Language Processing.
244
Proceedings of NAACL-HLT 2013, pages 416?425,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Learning to Relate Literal and Sentimental Descriptions of Visual Properties
Mark Yatskar
Computer Science & Engineering
University of Washington
Seattle, WA
my89@cs.washington.edu
Svitlana Volkova
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Asli Celikyilmaz
Conversational Understanding Sciences
Microsoft
Mountain View, CA
asli@ieee.org
Bill Dolan
NLP Group
Microsoft Research
Redmond, WA
billdol@microsoft.edu
Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
Language can describe our visual world at
many levels, including not only what is lit-
erally there but also the sentiment that it in-
vokes. In this paper, we study visual language,
both literal and sentimental, that describes the
overall appearance and style of virtual char-
acters. Sentimental properties, including la-
bels such as ?youthful? or ?country western,?
must be inferred from descriptions of the more
literal properties, such as facial features and
clothing selection. We present a new dataset,
collected to describe Xbox avatars, as well as
models for learning the relationships between
these avatars and their literal and sentimen-
tal descriptions. In a series of experiments,
we demonstrate that such learned models can
be used for a range of tasks, including pre-
dicting sentimental words and using them to
rank and build avatars. Together, these re-
sults demonstrate that sentimental language
provides a concise (though noisy) means of
specifying low-level visual properties.
1 Introduction
Language can describe varied aspects of our visual
world, including not only what is literally there but
also the social, cultural, and emotional sentiment it
invokes. Recently, there has been a growing effort
to study literal language that describes directly ob-
servable properties, such as object color, shape, or
This is a light tan young man
with short and trim haircut. He
has straight eyebrows and large
brown eyes. He has a neat and
trim appearance.
State of mind: angry, upset,
determined. Likes: country
western, rodeo. Occupation:
cowboy, wrangler, horse trainer.
Overall: youthful, cowboy.
Figure 1: (A) Literal avatar descriptions and (B) sen-
timental descriptions of four avatar properties, in-
cluding possible occupations and interests.
category (Farhadi et al, 2009; Mitchell et al, 2010;
Matuszek et al, 2012). Here, we add a focus on
sentimental visual language, which compactly de-
scribes more subjective properties such as if a person
looks determined, if a resume looks professional, or
if a restaurant looks romantic. Such models enable
many new applications, such as text editors that au-
tomatically select properties including font, color, or
text alignment to best match high level descriptions
such as ?professional? or ?artistic.?
416
In this paper, we study visual language, both lit-
eral and sentimental, that describes the overall ap-
pearance and style of virtual characters, like those in
Figure 1. We use literal language as feature norms, a
tool used for studying semantic information in cog-
nitive science (Mcrae et al, 2005). Literal words,
such ?black? or ?hat,? are annotated for objects to in-
dicate how people perceive visual properties. Such
feature norms provide our gold-standard visual de-
tectors, and allow us to focus on learning to model
sentimental language, such as ?youthful? or ?goth.?
We introduce a new corpus of descriptions of
Xbox avatars created by actual gamers. Each avatar
is specified by 19 attributes, including clothing and
body type, allowing for more than 1020 possibil-
ities. Using Amazon Mechanical Turk,1 we col-
lected literal and sentimental descriptions of com-
plete avatars and many of their component parts,
such as the cowboy hat in Figure 1(B). In all, there
are over 100K descriptions. To demonstrate poten-
tial for learning, we also report an A/B test which
shows that native speakers can use sentimental de-
scriptions to distinguish the labeled avatars from
random distractors. This new data will enable study
of the relationships between the co-occurring literal
and sentimental text in a rich visual setting.2
We describe models for three tasks: (i) classify-
ing when words match avatars, (ii) ranking avatars
given a description, and (iii) constructing avatars to
match a description. Each model includes literal part
descriptions as feature norms, enabling us to learn
which literal and sentinel word pairs best predict
complete avatars.
Experiments demonstrate the potential for jointly
modeling literal and sentimental visual descriptions
on our new dataset. The approach outperforms sev-
eral baselines and learns varied relationships be-
tween the sentimental and literal descriptions. For
example, in one experiment ?nerdy student? is pre-
dictive of an avatar with features indicating its shirt
is ?plaid? and glasses are ?large? and faces that are
not ?bearded.? We also show that individual sen-
timental words can be predicted but that multiple
avatars can match a single sentimental description.
Finally, we use our model to build complete avatars
1www.mturk.com
2Data available at http://homes.cs.washington.
edu/?my89/avatar.
and show that we can accurately predict the senti-
mental terms annotators ascribe to them.
2 Related Work
To the best of our knowledge, our focus on learn-
ing to understand visual sentiment descriptions is
novel. However, visual sentiment has been stud-
ied from other perspectives. Jrgensen (1998) pro-
vides examples which show that visual descriptions
communicate social status and story information in
addition to literal object and properties. Tousch et
al. (2012) draw the distinction between ?of-ness?
(objective and concrete) and ?about-ness? (subjec-
tive and abstract) in image retrieval, and observe
that many image queries are abstract (for example,
images about freedom). Finally, in descriptions of
people undergoing emotional distress, Fussell and
Moss (1998) show that literal descriptions co-occur
frequently with sentimental ones.
There has been significant work on more lit-
eral aspects of grounded language understand-
ing, both visual and non-visual. The Words-
Eye project (Coyne and Sproat, 2001) generates
3D scenes from literal paragraph-length descrip-
tions. Generating literal textual descriptions of vi-
sual scenes has also been studied, including both
captions (Kulkarni et al, 2011; Yang et al, 2011;
Feng and Lapata, 2010) and descriptions (Farhadi
et al, 2010). Furthermore, Chen and Dolan (2011)
collected literal descriptions of videos with the
goal of learning paraphrases while Zitnick and
Parikh (2013) describe a corpus of descriptions for
clip art that supports the discovery of semantic ele-
ments of visual scenes.
There has also been significant recent work on au-
tomatically recovering visual attributes, both abso-
lute (Farhadi et al, 2009) and relative (Kovashka et
al., 2012), a challenge that we avoid having to solve
with our use of feature norms (Mcrae et al, 2005).
Grounded language understanding has also re-
ceived significant attention, where the goal is to
learn to understand situated non-visual language
use. For example, there has been work on learning
to execute instructions (Branavan et al, 2009; Chen
and Mooney, 2011; Artzi and Zettlemoyer, 2013),
provide sports commentary (Chen et al, 2010), un-
derstand high level strategy guides to improve game
417
Figure 2: The number of assets per category and ex-
ample images from the hair, shirt and hat categories.
play (Branavan et al, 2011; Eisenstein et al, 2009),
and understand referring expression (Matuszek et
al., 2012).
Finally, our work is similar in spirit to sentiment
analysis (Pang et al, 2002), emotion detection from
images and speech (Zeng et al, 2009), and metaphor
understanding (Shutova, 2010a; Shutova, 2010b).
However, we focus on more general visual context.
3 Data Collection
We gathered a large number of natural language de-
scriptions from Mechanical Turk (MTurk). They in-
clude: (1) literal descriptions of specific facial fea-
tures, clothing or accessories and (2) high level sub-
jective descriptions of human-generated avatars.3
Literal Descriptions We showed annotators a sin-
gle image of clothing, a facial feature or an acces-
sory and asked them to produce short descriptions.
Figure 2 shows the distribution over object types.
We restricted descriptions to be between 3 and 15
words. In all, we collected 33.2K descriptions and
had on average 7 words per descriptions. The ex-
ample annotations with highlighted overlapping pat-
terns are in Table 1.
Sentimental Descriptions We also collected 1913
gamer-created avatars from the web. The avatars
were filtered to contain only items from the set of
665 for which we gathered literal descriptions. The
gender distribution is 95% male.
3(2) also has phrases describing emotional reactions. We
also collected (3) multilingual literal, (4) relative literal and (5)
comprehensive full-body descriptions. We do not use this data,
but it will be included in the public release.
LITERAL DESCRIPTIONS
full-sleeved executive blue shirt
blue , long-sleeved button-up shirt
mens blue button dress shirt with dark blue stripes
multi-blue striped long-sleeve button-up dress
shirt with cuffs and breast pocket
Table 1: Literal descriptions of shirt in Figure 2.
To gather high level sentimental descriptions, an-
notators were presented with an image of an avatar
and asked to list phrases in response to the follow
different aspects:
- State of mind of the avatar.
- Things the avatar might care about.
- What the avatar might do for a living.
- Overall appearance of the avatar.
6144 unique vocabulary items occurred in these
descriptions, but only 1179 occurred more than 10
times. Figure 1 (B) shows an avatar and its corre-
sponding sentimental descriptions.
Quality Control All annotations in our dataset are
produced by non-expert annotators. We relied on
manual spot checks to limit poor annotations. Over
time, we developed a trusted crowd of annotators
who produced only high quality annotations during
the earliest stage of data collection.
4 Feasibility
Our hypothesis is that sentimental language does not
uniquely identify an avatar, but instead summarizes
or otherwise describes its overall look. In general,
there is a trade off between concise and precise de-
scriptions. For example, given a single word you
might be able to generally describe the overall look
of an avatar, but a long, detailed, literal description
would be required to completely specify their ap-
pearance.
To demonstrate that the sentimental descriptions
we collected are precise enough to be predictive
of appearance, we conducted an experiment that
prompts people to judge when avatars match de-
scriptions. We created an A/B test where we show
English speakers two avatars and one sentimental
description. They were asked to select which avatar
is better matched by the description and how dif-
ficult they felt, on a scale from 1 to 4, it was to
judge. For 100 randomly selected descriptions, we
418
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2  
1
 1.5
 2
 2.5
 3
 3.5
data 
diffic
ulty l
ess t
han X
Kapp
a vs 
Cum
ulativ
e Dif
ficult
y
game
r is m
ajorit
y lab
el kapp
a
portio
n of d
ata
Figure 3: Judged task difficulty versus agreement,
gamer avatar preference, and percentage of data cov-
ered. The difficulty axis is cumulative.
asked 5 raters to compare the gamer avatars to ran-
domly generated ones (where each asset is selected
independently according to a uniform distribution).
Figure 3 shows a plot of Kappa and the percent of
the time a majority of the raters selected the gamer
avatar. The easiest 20% of the data pairs had the
strongest agreement, with kappa=.92, and two thirds
of the data has kappa = .70. While agreement falls
off to .52 for the full data set, the gamer avatar re-
mains the majority judgment 81% of the time.
The fact that random avatars are sometimes pre-
ferred indicates that it can be difficult to judge sen-
timental descriptions. Consider the avatars in Fig-
ure 4. Neither conforms to a clear sentimental de-
scription based on the questions we asked. The
right one is described with conflicting words and
the words describing the left one are very general
(like ?dumb?). This corresponds to our intuition that
while many avatars can be succinctly summarized
with our questions, some would be more easily de-
scribed using literal language.
5 Tasks and Evaluation
We formulate three tasks to study the feasibility of
learning the relationship between sentimental and
literal descriptions. In this section, we first define
the space of possible avatars, followed by the tasks.
Avatars Figure 5 summarizes the notation we will
develop to describe the data. An avatar is defined by
a 19 dimensional vector ~a where each position is an
State of mind:
playful, happy;
Likes: sex
Occupation: hobo
Overall: dumb
State of mind: content, humble, satisfied,
peaceful, relaxed, calm. Likes: fashion,
friends, money, cars, music, education.
Occupation: teacher, singer, actor,
performer, dancer, computer engineer.
Overall: nerdy, cool, smart, comfy,
easygoing, reserved
Figure 4: Avatars rated as difficult.
index into a list of possible items~i. Each dimension
represents a position on the avatar, for example, hat
or nose. Each possible item is called an asset and
is associated with a set of positions it can fill. Most
assets take up exactly one position, while there are
a few cases where assets take multiple positions.4
An avatar ~a is valid if all of its mandatory positions
are filled, and no two assets conflict on a position.
Mandatory positions include hair, eyes, ears, eye-
brows, nose, mouth, chin, shirt, pants, and shoes.
All other positions are optional. We refer to this set
of valid ~a as A. Practically speaking, if an avatar is
not valid, it cannot be reliably rendered graphically.
Each item i is associated with the literal descrip-
tions ~di ? D where D is the set of literal descrip-
tions. Furthermore, every avatar~a is associated a list
of sentimental query words ~q, describing subjective
aspects of an avatar.5
Sentimental Word Prediction We first study in-
dividual words. The word prediction task is to de-
cide whether a given avatar can be described with a
4For example, long sleeve shirts cover up watches, so they
take up both shirt and wristwear positions. Costumes tend to
span many more positions, for example there a suit that takes
up shirt, pants, wristwear and shoes positions.
5We do not distinguish which prompt (e.g., ?state of mind?
or ?occupation?) a word in ~q came from, although the vocabu-
laries are relatively disjoint.
419
Figure 5: Avatars, queries, items, literal descriptions.
particular sentimental word q?. We evaluate perfor-
mance with F-score.
Avatar Ranking We also consider an avatar re-
trieval task, where the goal is to rank the set of
avatars in our data, ?j=1...n ~aj , according to which
one best matches a sentimental description, ~qi. As
an automated evaluation, we report the average per-
centile position assigned to the true ~ai for each ex-
ample. However, in general, many different avatars
can match each ~qi, an interesting phenomena we will
further study with human evaluation.
Avatar Generation Finally, we consider the prob-
lem of generating novel, previously unseen avatars,
by selecting a set of items that best embody some
sentimental description. As with ranking, we aim to
construct the avatar ~ai that matches each sentimen-
tal description ~qi. We evaluate by considering the
item overlap between ~ai and the output avatar ~a?,
discounting for empty positions:6
f =
?| ~a?|
j=1 I( ~a
?
j = ~aij)
max(numparts( ~a?), numparts(~ai))
, (1)
where numparts returns the number of non-empty
avatar positions. The score is a conservative measure
because some items are significantly more visually
salient than others. For instance, shirts and pants oc-
cupy a large portion of the physical realization of the
avatar, while rings are small and virtually unnotice-
able. We additionally perform a human evaluation
in Section 8 to better understand these challenges.
6Optional items are infrequently used. Therefore not pre-
dicting them at all offers a strong baseline. Yet doing this
demonstrates nothing about an algorithm?s ability to predict
items which contribute to the sentimental qualities of an avatar.
6 Methods
We present two different models: one that considers
words in isolation and another that jointly models
the query words. This section defines the models
and how we learn them.
6.1 Independent Sentimental Word Model
The independent word model (S-Independent) as-
sumes that each word independently describes the
avatar. We construct a separate linear model for each
word in the vocabulary.
To train these model, we transform the data to
form a binary classification problem for each word,
where the positive data includes all avatars the word
was seen with, (q, ~ai, 1) for all i and q ? ~qi, and the
rest are negative, (q, ~ai, 0) for all i and q /? ~qi.
We use the following features:
? an indicator feature for the cross product of a
sentiment query word q, a literal description
word w ? D, and the avatar position index j
(for example, q = ?angry? with w = ?pointy?
and j = eyebrows):
I(q ? ~qi, w ? ~daij , j)
? a bias feature for keeping a position empty:
I(q ? ~qi, aij = empty, j)
These features will allow the model to capture
correlations between our feature norms which pro-
vide descriptions of visual attributes, like black, and
sentimental words, like gothic.
420
S-Independent is used for both word prediction
and ranking. For prediction, we train a linear model
using averaged binary perceptron. For ranking, we
try to rank all positive instances above negative in-
stances. We use an averaged structured perceptron
to train the ranker (Collins, 2002). To rank with re-
spect to an entire query ~qi, we sum the scores of each
word q ? ~qi.
6.2 Joint Sentimental Model
The second approach (S-Joint) jointly models the
query words to learn the relationships between lit-
eral and sentimental words with score s:
s(~a|~q,D) =
|~a|?
i=1
|~q|?
j=1
?T f(~ai, ~qj , ~dai)
Where every word in the query has a separate factor
and every position is treated independently subject
to the constraint that ~a is valid. The feature function
f uses the same features as the word independent
model above.
This model is used for ranking and generation.
For ranking, we try to rank the avatar ai for query
qi above all other avatars in the candidate set. For
generation, we try to score ai above all other valid
avatars given the query qi. In both cases, we train
with averaged structured perceptron (Collins, 2002)
on the original data, containing query, avatar pairs
(~qi, ~ai).
7 Experimental Setup
Random Baseline For the ranking and avatar gen-
eration tasks, we report random baselines. For rank-
ing, we randomly order the avatars. In the genera-
tion case, we select an item randomly for every posi-
tion. This baseline does not generate optional assets
because they are rare in the real data.
Sentimental-Literal Overlap (SL-Overlap) We
also report a baseline that measures the overlap be-
tween words in the sentiment query ~qi and words in
the literal asset descriptions D. In generation, for
each position in the avatar, ~ai, SL-Overlap selects
the item whose literal description has the most words
in common with ~qi. If no item had overlap with the
query, we backoff to a random choice. In the case of
ranking, it orders avatars by the sum over every po-
sition of the number of words in common between
Word F-Score Precision Recall N
happi 0.84 0.89 0.78 149
student 0.78 0.82 0.74 129
friend 0.76 0.84 0.70 153
music 0.74 0.89 0.63 148
confid 0.74 0.82 0.76 157
sport 0.69 0.62 0.76 76
casual 0.63 0.6 0.67 84
youth 0.6 0.57 0.64 88
waitress 0.59 0.42 1 5
smart 0.57 0.54 0.6 88
fashion 0.54 0.54 0.54 70
monei 0.54 0.52 0.56 76
cool 0.54 0.52 0.56 84
relax 0.53 0.52 0.56 90
game 0.51 0.44 0.62 61
musician 0.51 0.44 0.61 66
parti 0.51 0.43 0.62 58
content 0.5 0.47 0.53 75
friendli 0.49 0.42 0.6 56
smooth 0.49 0.4 0.63 57
Table 2: Top 20 words (stemmed) for classification.
N is the number of occurances in the test set.
the literal description and the query, ~qi. This base-
line tests the degree to which literal and sentimental
descriptions overlap lexically.
Feature Generation For all models that use lexi-
cal features, we limited the number of words. 6144
unique vocabulary items occur in the query set, and
3524 in the literal description set. There are over
400 million entries in the full set of features that in-
clude the cross product of these sets with all possible
avatar positions, as described in Section 6. Since this
would present a challenge for learning, we prune in
two ways. We stem all words with a Porter stemmer.
We also filter out all features which do not occur at
least 10 times in our training set. The final model
has approximately 700k features.
8 Results
We present results for the tasks described in Sec-
tion 5 with the appropriate models from Section 6.
8.1 Word Prediction Results
The goal of our first experiment is to study when
individual sentiment words can be accurately pre-
dicted. We computed sentimental word classifica-
tion accuracy for 1179 word classes with 10 or more
421
Algorithm Percentile Rank
S-joint 77.3
S-independant 73.5
SL-overlap 60.4
Random 48.8
Table 3: Automatic evaluation of ranking. The aver-
age percentile that a test avatar was ranked given its
sentimental description.
mentions. Table 2 shows the top 20 words ordered
by F-score.7 Many common words can be predicted
with relatively high accuracy. Words with strong
individual cues like happy (a smiling mouth), and
confidence (wide eyes) and nerdi (particular glasses)
can be predicted well.
The average F-score among all words was .085.
33.2% of words have an F-score of zero. These zeros
include words like: unusual, bland, sarcastic, trust,
prepared, limber, healthy and poetry. Some of these
words indicate broad classes of avatars (e.g., unusual
avatars) and others indicate subtle modifications to
looks that without other words are not specific (e.g.,
a prepared surfer vs. a prepared business man). Fur-
thermore, evaluation was done assuming that when
a word is not mentioned, it is should be predicted as
negative. This fails to account for the fact that peo-
ple do not mention everything that?s true, but instead
make choices about what to mention based on the
most relevant qualities. Despite these difficulties,
the classification performance shows that we can ac-
curately capture usage patterns for many words.
8.2 Ranking Results
Ranking allows us to test the hypothesis that multi-
ple avatars are valid for a high level description. Fur-
thermore, we consider the differences between S-
Joint and S-Independent, showing that jointly mod-
elings all words improves ranking performance.
Automatic Evaluation The results are shown in
Table 3. Both S-Independent and S-Joint outperform
the SL-overlap baseline. SL-Overlap?s poor perfor-
mance can be attributed to low direct overlap be-
tween sentimental words and literal words. S-Joint
also outperforms the S-Independent.
7Accuracy numbers are inappropriate in this case because
the number of negative instances, in most cases, is far larger
than the number of positive ones.
Inspection of the parameters shows that S-Joint
does better than S-Independent in modeling words
that only relate to a subset of body positions. For
example, in one case we found that for the word
?puzzled? nearly 50% of the weights were on fea-
tures that related to eyebrows and eyes. This type
of specialization was far more pronounced for S-
Joint. The joint nature of the learning allows the fea-
tures for individual words to specialize for specific
positions. In contrast, S-Independent must indepen-
dently predict all parts for every word.
Human Evaluation We report human relevancy
judgments for the top-5 returned results from S-
Joint. On average, 56.2% were marked to be rele-
vant. This shows that S-Joint is performing better
than automatic numbers would indicate, confirming
our intuition that there is a one-to-many relationship
between a sentimental description and avatars. Sen-
timental descriptions, while having significant sig-
nal, are not exact. These results also indicate that
relying on automatic measures of accuracy that as-
sume a single reference avatar underestimates per-
formance. Figure 6 shows the top ranked results
returned by S-Joint for a sentimental description
where the model performs well.
8.3 Generation Results
Finally we evaluate three models for avatar genera-
tion: Random, SL-Overlap and S-Joint using auto-
matic measures and human evaluation.
Automatic Evaluation Table 4 presents results
for automatic evaluation. The Random baseline per-
forms badly, on average assigning items correctly to
less than 1 position in the generated avatar. The SL-
Overlap baseline improves, but still performs quite
poorly. The S-Joint model performs significantly
better, correctly guessing 2-3 items for each output
avatar. However, as we will see in the manual eval-
uation, many of the non-matching parts it produces
are still a good fit for the query.
Human Evaluation As before, there are many
reasonable avatars that could match as well as the
reference avatars. Therefore, we also evaluated gen-
eration with A/B tests, much like in Section 4. An-
notators were asked to judge which of two avatars
better matched a sentimental description. They
422
pensive,confrontational; music,socializing; musician,bar tending,club owner; smart,cool.
Figure 6: A sentimental description paired with the highest ranked avatars found by S-Joint.
Model Overlap
Random 0.041
SL-Overlap 0.049
S-Joint 0.126
Table 4: Automatic generation evaluation results.
The item overlap metric is defined in Section 5.
Kappa Majority Random Sys.
SL-Overlap 0.20 0.25 0.34 0.32
S-Joint 0.52 0.90 0.07 0.81
Gamer 0.52 0.81 0.08 0.77
Table 5: Human evaluation of automatically gener-
ated avatars. Majority represents the percentage of
time the system output is preferred by a majority of
raters. Random and System (Sys.) indicate the per-
centage of time each was preferred.
could rate System A or System B as better, or re-
port that they were equal or that neither matched
the description. We consider two comparisons: SL-
Overlap vs. Random and S-Joint vs Random. Five
annotators performed each condition, rating 100 ex-
amples with randomly ordered avatars.
We report the results for human evaluation includ-
ing kappa, majority judgments, and a distribution
over judgments in Table 5. The SL-Overlap baseline
is indistinguishable from a random avatar. This con-
trasts with the ranking case, where this simple base-
line showed improvement, indicating that generation
is a much harder problem. Furthermore, agreement
is low; people felt the need to make a choice but
were not consistent.
We also see in Table 5 that people prefer the S-
Joint model outputs to random avatars as often as
they prefer gamer to random. While this does not
necessarily imply that S-Joint creates gamer-quality
avatars, it indicates substantial progress by learning
a mapping between literal and sentimental words.
Qualitative Results Table 6 presents the highest
and lowest weighted features for different sentimen-
tal query words. Figure 7 shows four descriptions
that were assigned high quality avatars.
In general, many of the weaker avatars had as-
pects of the descriptions but lacked such distinctive
overall looks. This was especially true when the
descriptions contained seemingly contradictory in-
formation. For example, one avatar was described
as being both nerdy and popular. We generated a
look that had aspects of both of these descriptions,
including a head that contained both conservative el-
ements (like glasses) and less conservative elements
(like crazy hair and earrings). However, the combi-
nation would not be described as nerdy or popular,
because of difficult to predict global interactions be-
tween the co-occurring words and items. This is an
important area for future work.
9 Conclusions
We explored how visual language, both literal and
sentimental, maps to the overall physical appearance
and style of virtual characters. While this paper fo-
cused on avatar design, our approach has implica-
tions for a broad class of natural language-driven
423
Ambition; business,
fashion, success;
salesman; smooth,
professional.
Capable, confident, firm; heavy metal,
extreme sports, motorcycles; engineer,
mechanic, machinist; aggressive,
strong, protective.
Stressed, bored,
discontent; emo music;
works at a record store;
goth, dark, drab.
Happy, content, confident,
home, career, family,
secretary,student,
classy,clean,casual
Figure 7: Avatars automatically generated with the S-Joint model.
Sentiment positive features negative features
happi mouth:thick, mouth:smilei, mouth:make, mouth:open mouth:tight, mouth:emotionless, mouth:brownish, mouth:attract
gothic shoes:brown, shirt:black, pants:hot, shirt:band shirt:half, shirt:tight, pants:sexi, hair:brownish
retro eyebrows:men, eyebrows:large, hair:round, pants:light eyebrows:beauti, pants:side; eyebrows:trim, pants:cut
beach pants:yello, pants:half, nose:narrow, pants:white shirt:brown, shirt:side; shoes:long, pants:jean
Table 6: Most positive and negative features for a word stem. A feature is [position]:[literal word].
dialog scenarios. In many situations, a user may
be perfectly able to formulate a high-level descrip-
tion of their intent (?Make my resume look cleaner?
?Buy me clothes for a summer wedding,? or ?Play
something more danceable?) while having little or
no understanding of the complex parameter space
that the underlying software must manipulate in or-
der to achieve this result.
We demonstrated that these high-level sentimen-
tal specifications can have a strong relationship to
literal aspects of a problem space and showed that
sentimental language is a concise, yet noisy, way
of specifying high level characteristics. Sentimen-
tal language is an unexplored avenue for improving
natural language systems that operate in situated set-
tings. It has the potential to bridge the gap between
lay and expert understandings of a problem domain.
Acknowledgments
This work is partially supported by the DARPA
CSSG (N11AP20020) and the NSF (IIS-1115966).
The authors would like to thank Chris Brockett,
Noelle Sophy, Rico Malvar for helping with collect-
ing and processing the data. We would also like
to thank Tom Kwiatkowski and Nicholas FitzGer-
ald and the anonymous reviewers for their helpful
comments.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics, 1(1):49?62.
SRK Branavan, H. Chen, L.S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 82?90.
SRK Branavan, David Silver, and Regina Barzilay. 2011.
Learning to win by reading manuals in a monte-carlo
framework. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 268?
277.
David L. Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 190?200.
D.L. Chen and R.J. Mooney. 2011. Learning to interpret
natural language navigation instructions from observa-
424
tions. In Proceedings of the 25th AAAI Conference on
Artificial Intelligence (AAAI-2011), pages 859?865.
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal of Artificial
Intelligence Research, 37:397?435.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing, pages 1?8.
B. Coyne and R. Sproat. 2001. Wordseye: an automatic
text-to-scene conversion system. In Proceedings of the
28th annual conference on Computer graphics and in-
teractive techniques, pages 487?496.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 958?967.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
In Proceedings of the IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a story:
generating sentences from images. In Proceedings of
the 11th European conference on Computer Vision,
ECCV?10, pages 15?29.
Yansong Feng and Mirella Lapata. 2010. Topic models
for image annotation and text illustration. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 831?839.
Susan R Fussell and Mallie M Moss. 1998. Figura-
tive language in emotional communication. Social and
cognitive approaches to interpersonal communication,
page 113.
Corinne Jrgensen. 1998. Attributes of images in describ-
ing tasks. Information Processing & Management,
34(23):161 ? 174.
Adriana Kovashka, Devi Parikh, and Kristen Grauman.
2012. Whittlesearch: Image search with relative at-
tribute feedback. In Computer Vision and Pattern
Recognition (CVPR), pages 2973?2980.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.
Berg, and T.L. Berg. 2011. Baby talk: Understanding
and generating simple image descriptions. In Com-
puter Vision and Pattern Recognition (CVPR), pages
1601?1608.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A Joint
Model of Language and Perception for Grounded At-
tribute Learning. In Proc. of the 2012 International
Conference on Machine Learning.
Ken Mcrae, George S. Cree, Mark S. Seidenberg, and
Chris Mcnorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37(4):547?559.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2010. Natural reference to objects in a visual domain.
In Proceedings of the 6th International Natural Lan-
guage Generation Conference, INLG ?10, pages 95?
104.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language processing,
pages 79?86.
Ekaterina Shutova. 2010a. Automatic metaphor inter-
pretation as a paraphrasing task. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 1029?1037.
Ekaterina Shutova. 2010b. Models of metaphor in nlp.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL ?10, pages
688?697.
Anne-Marie Tousch, Stphane Herbin, and Jean-Yves Au-
dibert. 2012. Semantic hierarchies for image annota-
tion: A survey. Pattern Recognition, 45(1):333 ? 345.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing.
Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009.
A survey of affect recognition methods: Audio, vi-
sual, and spontaneous expressions. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
31(1):39?58.
C Lawrence Zitnick and Devi Parikh. 2013. Bringing
semantics into focus using visual abstraction. In Com-
puter Vision and Pattern Recognition (To Appear).
425
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1268?1277,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Reading Between the Lines:
Learning to Map High-level Instructions to Commands
S.R.K. Branavan, Luke S. Zettlemoyer, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{branavan, lsz, regina}@csail.mit.edu
Abstract
In this paper, we address the task of
mapping high-level instructions to se-
quences of commands in an external en-
vironment. Processing these instructions
is challenging?they posit goals to be
achieved without specifying the steps re-
quired to complete them. We describe
a method that fills in missing informa-
tion using an automatically derived envi-
ronment model that encodes states, tran-
sitions, and commands that cause these
transitions to happen. We present an ef-
ficient approximate approach for learning
this environment model as part of a policy-
gradient reinforcement learning algorithm
for text interpretation. This design enables
learning for mapping high-level instruc-
tions, which previous statistical methods
cannot handle.1
1 Introduction
In this paper, we introduce a novel method for
mapping high-level instructions to commands in
an external environment. These instructions spec-
ify goals to be achieved without explicitly stat-
ing all the required steps. For example, consider
the first instruction in Figure 1 ? ?open control
panel.? The three GUI commands required for its
successful execution are not explicitly described
in the text, and need to be inferred by the user.
This dependence on domain knowledge makes the
automatic interpretation of high-level instructions
particularly challenging.
The standard approach to this task is to start
with both a manually-developed model of the en-
vironment, and rules for interpreting high-level in-
structions in the context of this model (Agre and
1Code, data, and annotations used in this work are avail-
able at http://groups.csail.mit.edu/rbg/code/rl-hli/
Chapman, 1988; Di Eugenio and White, 1992;
Di Eugenio, 1992; Webber et al, 1995). Given
both the model and the rules, logic-based infer-
ence is used to automatically fill in the intermedi-
ate steps missing from the original instructions.
Our approach, in contrast, operates directly on
the textual instructions in the context of the in-
teractive environment, while requiring no addi-
tional information. By interacting with the en-
vironment and observing the resulting feedback,
our method automatically learns both the mapping
between the text and the commands, and the un-
derlying model of the environment. One partic-
ularly noteworthy aspect of our solution is the in-
terplay between the evolving mapping and the pro-
gressively acquired environment model as the sys-
tem learns how to interpret the text. Recording the
state transitions observed during interpretation al-
lows the algorithm to construct a relevant model
of the environment. At the same time, the envi-
ronment model enables the algorithm to consider
the consequences of commands before they are ex-
ecuted, thereby improving the accuracy of inter-
pretation. Our method efficiently achieves both of
these goals as part of a policy-gradient reinforce-
ment learning algorithm.
We apply our method to the task of mapping
software troubleshooting guides to GUI actions in
the Windows environment (Branavan et al, 2009;
Kushman et al, 2009). The key findings of our
experiments are threefold. First, the algorithm
can accurately interpret 61.5% of high-level in-
structions, which cannot be handled by previous
statistical systems. Second, we demonstrate that
explicitly modeling the environment also greatly
improves the accuracy of processing low-level in-
structions, yielding a 14% absolute increase in
performance over a competitive baseline (Brana-
van et al, 2009). Finally, we show the importance
of constructing an environment model relevant to
the language interpretation task ? using textual
1268
"open control panel, double click system, then go to the advanced tab"
Document (input)D
"open control panel"
left-click Advanced
double-click System
left-click Control Panel
left-click Settings
left-click Start
InstructionsD
DocDumenemt
o (ip)Iios 
msrumenemt
o (ip)Iios (
Command Sequence (output)D
: :
:
:
:
:
"double click system"
"go to the advanced tab"
:
:
Figure 1: An example mapping of a document containing high-level instructions into a candidate se-
quence of five commands. The mapping process involves segmenting the document into individual in-
struction word spans Wa, and translating each instruction into the sequence ~c of one or more commands
it describes. During learning, the correct output command sequence is not provided to the algorithm.
instructions enables us to bias exploration toward
transitions relevant for language learning. This ap-
proach yields superior performance compared to a
policy that relies on an environment model con-
structed via random exploration.
2 Related Work
Interpreting Instructions Our approach is most
closely related to the reinforcement learning algo-
rithm for mapping text instructions to commands
developed by Branavan et al (2009) (see Section 4
for more detail). Their method is predicated on the
assumption that each command to be executed is
explicitly specified in the instruction text. This as-
sumption of a direct correspondence between the
text and the environment is not unique to that pa-
per, being inherent in other work on grounded lan-
guage learning (Siskind, 2001; Oates, 2001; Yu
and Ballard, 2004; Fleischman and Roy, 2005;
Mooney, 2008; Liang et al, 2009; Matuszek et
al., 2010). A notable exception is the approach
of Eisenstein et al (2009), which learns how an
environment operates by reading text, rather than
learning an explicit mapping from the text to the
environment. For example, their method can learn
the rules of a card game given instructions for how
to play.
Many instances of work on instruction inter-
pretation are replete with examples where in-
structions are formulated as high-level goals, tar-
geted at users with relevant knowledge (Winograd,
1972; Di Eugenio, 1992; Webber et al, 1995;
MacMahon et al, 2006). Not surprisingly, auto-
matic approaches for processing such instructions
have relied on hand-engineered world knowledge
to reason about the preconditions and effects of
environment commands. The assumption of a
fully specified environment model is also com-
mon in work on semantics in the linguistics lit-
erature (Lascarides and Asher, 2004). While our
approach learns to analyze instructions in a goal-
directed manner, it does not require manual speci-
fication of relevant environment knowledge.
Reinforcement Learning Our work combines
ideas of two traditionally disparate approaches to
reinforcement learning (Sutton and Barto, 1998).
The first approach, model-based learning, con-
structs a model of the environment in which the
learner operates (e.g., modeling location, velocity,
and acceleration in robot navigation). It then com-
putes a policy directly from the rich information
represented in the induced environment model.
In the NLP literature, model-based reinforcement
learning techniques are commonly used for dia-
log management (Singh et al, 2002; Lemon and
Konstas, 2009; Schatzmann and Young, 2009).
However, if the environment cannot be accurately
approximated by a compact representation, these
methods perform poorly (Boyan and Moore, 1995;
Jong and Stone, 2007). Our instruction interpreta-
tion task falls into this latter category,2 rendering
standard model-based learning ineffective.
The second approach ? model-free methods
such as policy learning ? aims to select the opti-
2For example, in the Windows GUI domain, clicking on
the File menu will result in a different submenu depending on
the application. Thus it is impossible to predict the effects of
a previously unseen GUI command.
1269
left-cikdo-uteo
-ft-btoiuu
eioiiiiii
LEFT_CLICK"ooooooopstart
-eoiii
iuProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 541?550,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Knowledge-Based Weak Supervision for Information Extraction
of Overlapping Relations
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, Daniel S. Weld
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{raphaelh,clzhang,xiaoling,lsz,weld}@cs.washington.edu
Abstract
Information extraction (IE) holds the promise
of generating a large-scale knowledge
base from the Web?s natural language text.
Knowledge-based weak supervision, using
structured data to heuristically label a training
corpus, works towards this goal by enabling
the automated learning of a potentially
unbounded number of relation extractors.
Recently, researchers have developed multi-
instance learning algorithms to combat the
noisy training data that can come from
heuristic labeling, but their models assume
relations are disjoint ? for example they
cannot extract the pair Founded(Jobs,
Apple) and CEO-of(Jobs, Apple).
This paper presents a novel approach for
multi-instance learning with overlapping re-
lations that combines a sentence-level extrac-
tion model with a simple, corpus-level compo-
nent for aggregating the individual facts. We
apply our model to learn extractors for NY
Times text using weak supervision from Free-
base. Experiments show that the approach
runs quickly and yields surprising gains in
accuracy, at both the aggregate and sentence
level.
1 Introduction
Information-extraction (IE), the process of generat-
ing relational data from natural-language text, con-
tinues to gain attention. Many researchers dream of
creating a large repository of high-quality extracted
tuples, arguing that such a knowledge base could
benefit many important tasks such as question an-
swering and summarization. Most approaches to IE
use supervised learning of relation-specific exam-
ples, which can achieve high precision and recall.
Unfortunately, however, fully supervised methods
are limited by the availability of training data and are
unlikely to scale to the thousands of relations found
on the Web.
A more promising approach, often called ?weak?
or ?distant? supervision, creates its own training
data by heuristically matching the contents of a
database to corresponding text (Craven and Kum-
lien, 1999). For example, suppose that r(e1, e2) =
Founded(Jobs,Apple) is a ground tuple in the
database and s =?Steve Jobs founded Apple, Inc.?
is a sentence containing synonyms for both e1 =
Jobs and e2 = Apple, then s may be a natural
language expression of the fact that r(e1, e2) holds
and could be a useful training example.
While weak supervision works well when the tex-
tual corpus is tightly aligned to the database con-
tents (e.g., matching Wikipedia infoboxes to as-
sociated articles (Hoffmann et al, 2010)), Riedel
et al (2010) observe that the heuristic leads to
noisy data and poor extraction performance when
the method is applied more broadly (e.g., matching
Freebase records to NY Times articles). To fix
this problem they cast weak supervision as a form of
multi-instance learning, assuming only that at least
one of the sentences containing e1 and e2 are ex-
pressing r(e1, e2), and their method yields a sub-
stantial improvement in extraction performance.
However, Riedel et al?s model (like that of
previous systems (Mintz et al, 2009)) assumes
that relations do not overlap ? there cannot
exist two facts r(e1, e2) and q(e1, e2) that are
both true for any pair of entities, e1 and e2.
Unfortunately, this assumption is often violated;
541
for example both Founded(Jobs, Apple) and
CEO-of(Jobs, Apple) are clearly true. In-
deed, 18.3% of the weak supervision facts in Free-
base that match sentences in the NY Times 2007 cor-
pus have overlapping relations.
This paper presents MULTIR, a novel model of
weak supervision that makes the following contri-
butions:
? MULTIR introduces a probabilistic, graphical
model of multi-instance learning which handles
overlapping relations.
? MULTIR also produces accurate sentence-level
predictions, decoding individual sentences as
well as making corpus-level extractions.
? MULTIR is computationally tractable. Inference
reduces to weighted set cover, for which it uses
a greedy approximation with worst case running
time O(|R| ? |S|) where R is the set of possi-
ble relations and S is largest set of sentences for
any entity pair. In practice, MULTIR runs very
quickly.
? We present experiments showing that MULTIR
outperforms a reimplementation of Riedel
et al (2010)?s approach on both aggregate (cor-
pus as a whole) and sentential extractions.
Additional experiments characterize aspects of
MULTIR?s performance.
2 Weak Supervision from a Database
Given a corpus of text, we seek to extract facts about
entities, such as the company Apple or the city
Boston. A ground fact (or relation instance), is
an expression r(e) where r is a relation name, for
example Founded or CEO-of, and e = e1, . . . , en
is a list of entities.
An entity mention is a contiguous sequence of tex-
tual tokens denoting an entity. In this paper we as-
sume that there is an oracle which can identify all
entity mentions in a corpus, but the oracle doesn?t
normalize or disambiguate these mentions. We use
ei ? E to denote both an entity and its name (i.e.,
the tokens in its mention).
A relation mention is a sequence of text (in-
cluding one or more entity mentions) which states
that some ground fact r(e) is true. For example,
?Steve Ballmer, CEO of Microsoft, spoke recently
at CES.? contains three entity mentions as well as a
relation mention for CEO-of(Steve Ballmer,
Microsoft). In this paper we restrict our atten-
tion to binary relations. Furthermore, we assume
that both entity mentions appear as noun phrases in
a single sentence.
The task of aggregate extraction takes two inputs,
?, a set of sentences comprising the corpus, and an
extraction model; as output it should produce a set
of ground facts, I , such that each fact r(e) ? I is
expressed somewhere in the corpus.
Sentential extraction takes the same input and
likewise produces I , but in addition it also produces
a function, ? : I ? P(?), which identifies, for
each r(e) ? I , the set of sentences in ? that contain
a mention describing r(e). In general, the corpus-
level extraction problem is easier, since it need only
make aggregate predictions, perhaps using corpus-
wide statistics. In contrast, sentence-level extrac-
tion must justify each extraction with every sentence
which expresses the fact.
The knowledge-based weakly supervised learning
problem takes as input (1) ?, a training corpus, (2)
E, a set of entities mentioned in that corpus, (3) R,
a set of relation names, and (4), ?, a set of ground
facts of relations in R. As output the learner pro-
duces an extraction model.
3 Modeling Overlapping Relations
We define an undirected graphical model that al-
lows joint reasoning about aggregate (corpus-level)
and sentence-level extraction decisions. Figure 1(a)
shows the model in plate form.
3.1 Random Variables
There exists a connected component for each pair of
entities e = (e1, e2) ? E ? E that models all of
the extraction decisions for this pair. There is one
Boolean output variable Y r for each relation name
r ? R, which represents whether the ground fact
r(e) is true. Including this set of binary random
variables enables our model to extract overlapping
relations.
Let S(e1,e2) ? ? be the set of sentences which
contain mentions of both of the entities. For each
sentence xi ? S(e1,e2) there exists a latent variable
Zi which ranges over the relation names r ? R and,
542
E ? E 
? 
R 
S 
?? 
(a)
Steve Jobs was founder  
of Apple. 
Steve Jobs, Steve Wozniak and 
Ronald Wayne founded Apple. 
Steve Jobs is CEO of  
Apple. 
founder ?? founder none 
0 
?????? 
1 0 0 
?? ?? 
... 
... 
... 
????????? ????????? ????????? 
(b)
Figure 1: (a) Network structure depicted as plate model and (b) an example network instantiation for the pair of entities
Steve Jobs, Apple.
importantly, also the distinct value none. Zi should
be assigned a value r ? R only when xi expresses
the ground fact r(e), thereby modeling sentence-
level extraction.
Figure 1(b) shows an example instantiation of the
model with four relation names and three sentences.
3.2 A Joint, Conditional Extraction Model
We use a conditional probability model that defines
a joint distribution over all of the extraction random
variables defined above. The model is undirected
and includes repeated factors for making sentence
level predictions as well as globals factors for ag-
gregating these choices.
For each entity pair e = (e1, e2), define x to
be a vector concatenating the individual sentences
xi ? S(e1,e2), Y to be vector of binary Y
r random
variables, one for each r ? R, and Z to be the vec-
tor of Zi variables, one for each sentence xi. Our
conditional extraction model is defined as follows:
p(Y = y,Z = z|x; ?)
def
=
1
Zx
?
r
?join(yr, z)
?
i
?extract(zi, xi)
where the parameter vector ? is used, below, to de-
fine the factor ?extract.
The factors ?join are deterministic OR operators
?join(yr, z)
def
=
{
1 if yr = true ? ?i : zi = r
0 otherwise
which are included to ensure that the ground fact
r(e) is predicted at the aggregate level for the as-
signment Y r = yr only if at least one of the sen-
tence level assignments Zi = zi signals a mention
of r(e).
The extraction factors ?extract are given by
?extract(zi, xi)
def
= exp
?
?
?
j
?j?j(zi, xi)
?
?
where the features ?j are sensitive to the relation
name assigned to extraction variable zi, if any, and
cues from the sentence xi. We will make use of the
Mintz et al (2009) sentence-level features in the ex-
peiments, as described in Section 7.
3.3 Discussion
This model was designed to provide a joint approach
where extraction decisions are almost entirely driven
by sentence-level reasoning. However, defining the
Y r random variables and tying them to the sentence-
level variables, Zi, provides a direct method for
modeling weak supervision. We can simply train the
model so that the Y variables match the facts in the
database, treating the Zi as hidden variables that can
take any value, as long as they produce the correct
aggregate predictions.
This approach is related to the multi-instance
learning approach of Riedel et al (2010), in that
both models include sentence-level and aggregate
random variables. However, their sentence level
variables are binary and they only have a single ag-
gregate variable that takes values r ? R ? {none},
thereby ruling out overlapping relations. Addition-
ally, their aggregate decisions make use of Mintz-
style aggregate features (Mintz et al, 2009), that col-
lect evidence from multiple sentences, while we use
543
Inputs:
(1) ?, a set of sentences,
(2)E, a set of entities mentioned in the sentences,
(3) R, a set of relation names, and
(4) ?, a database of atomic facts of the form
r(e1, e2) for r ? R and ei ? E.
Definitions:
We define the training set {(xi,yi)|i = 1 . . . n},
where i is an index corresponding to a particu-
lar entity pair (ej , ek) in ?, xi contains all of
the sentences in ? with mentions of this pair, and
yi = relVector(ej , ek).
Computation:
initialize parameter vector ?? 0
for t = 1...T do
for i = 1...n do
(y?, z?)? arg maxy,z p(y, z|xi; ?)
if y? 6= yi then
z? ? arg maxz p(z|xi,yi; ?)
?? ? + ?(xi, z?)? ?(xi, z?)
end if
end for
end for
Return ?
Figure 2: The MULTIR Learning Algorithm
only the deterministic OR nodes. Perhaps surpris-
ing, we are still able to improve performance at both
the sentential and aggregate extraction tasks.
4 Learning
We now present a multi-instance learning algo-
rithm for our weak-supervision model that treats the
sentence-level extraction random variables Zi as la-
tent, and uses facts from a database (e.g., Freebase)
as supervision for the aggregate-level variables Y r.
As input we have (1) ?, a set of sentences, (2)
E, a set of entities mentioned in the sentences, (3)
R, a set of relation names, and (4) ?, a database
of atomic facts of the form r(e1, e2) for r ? R and
ei ? E. Since we are using weak learning, the Y r
variables in Y are not directly observed, but can be
approximated from the database ?. We use a proce-
dure, relVector(e1, e2) to return a bit vector whose
jth bit is one if rj(e1, e2) ? ?. The vector does not
have a bit for the special none relation; if there is no
relation between the two entities, all bits are zero.
Finally, we can now define the training set to be
pairs {(xi,yi)|i = 1 . . . n}, where i is an index
corresponding to a particular entity pair (ej , ek), xi
contains all of the sentences with mentions of this
pair, and yi = relVector(ej , ek).
Given this form of supervision, we would like to
find the setting for ? with the highest likelihood:
O(?) =
?
i
p(yi|xi; ?) =
?
i
?
z
p(yi, z|xi; ?)
However, this objective would be difficult to op-
timize exactly, and algorithms for doing so would
be unlikely to scale to data sets of the size we con-
sider. Instead, we make two approximations, de-
scribed below, leading to a Perceptron-style addi-
tive (Collins, 2002) parameter update scheme which
has been modified to reason about hidden variables,
similar in style to the approaches of (Liang et al,
2006; Zettlemoyer and Collins, 2007), but adapted
for our specific model. This approximate algorithm
is computationally efficient and, as we will see,
works well in practice.
Our first modification is to do online learning
instead of optimizing the full objective. Define the
feature sums ?(x, z) =
?
j ?(xj , zj) which range
over the sentences, as indexed by j. Now, we can
define an update based on the gradient of the local
log likelihood for example i:
? logOi(?)
??j
= Ep(z|xi,yi;?)[?j(xi, z)]
?Ep(y,z|xi;?)[?j(xi, z)]
where the deterministic OR ?join factors ensure that
the first expectation assigns positive probability only
to assignments that produce the labeled facts yi but
that the second considers all valid sets of extractions.
Of course, these expectations themselves, espe-
cially the second one, would be difficult to com-
pute exactly. Our second modification is to do
a Viterbi approximation, by replacing the expecta-
tions with maximizations. Specifically, we compute
the most likely sentence extractions for the label
facts arg maxz p(z|xi,yi; ?) and the most likely ex-
traction for the input, without regard to the labels,
arg maxy,z p(y, z|xi; ?). We then compute the fea-
tures for these assignments and do a simple additive
update. The final algorithm is detailed in Figure 2.
544
5 Inference
To support learning, as described above, we need
to compute assignments arg maxz p(z|x,y; ?) and
arg maxy,z p(y, z|x; ?). In this section, we describe
algorithms for both cases that use the deterministic
OR nodes to simplify the required computations.
Predicting the most likely joint extraction
arg maxy,z p(y, z|x; ?) can be done efficiently
given the structure of our model. In particular, we
note that the factors ?join represent deterministic de-
pendencies between Z and Y, which when satisfied
do not affect the probability of the solution. It is thus
sufficient to independently compute an assignment
for each sentence-level extraction variable Zi, ignor-
ing the deterministic dependencies. The optimal set-
ting for the aggregate variables Y is then simply the
assignment that is consistent with these extractions.
The time complexity is O(|R| ? |S|).
Predicting sentence level extractions given weak
supervision facts, arg maxz p(z|x,y; ?), is more
challenging. We start by computing extraction
scores ?extract(xi, zi) for each possible extraction as-
signment Zi = zi at each sentence xi ? S, and
storing the values in a dynamic programming table.
Next, we must find the most likely assignment z that
respects our output variables y. It turns out that
this problem is a variant of the weighted, edge-cover
problem, for which there exist polynomial time op-
timal solutions.
Let G = (E ,V = VS ? Vy) be a complete
weighted bipartite graph with one node vSi ? V
S for
each sentence xi ? S and one node v
y
r ? Vy for each
relation r ? R where yr = 1. The edge weights are
given by c((vSi , v
y
r ))
def
= ?extract(xi, zi). Our goal is
to select a subset of the edges which maximizes the
sum of their weights, subject to each node vSi ? V
S
being incident to exactly one edge, and each node
vyr ? Vy being incident to at least one edge.
Exact Solution An exact solution can be obtained
by first computing the maximum weighted bipartite
matching, and adding edges to nodes which are not
incident to an edge. This can be computed in time
O(|V|(|E| + |V| log |V|)), which we can rewrite as
O((|R|+ |S|)(|R||S|+ (|R|+ |S|) log(|R|+ |S|))).
Approximate Solution An approximate solution
can be obtained by iterating over the nodes in Vy,
????????  ???????????  
?? ?? ?? 
????????????? ???????????????? 
????? 
Figure 3: Inference of arg maxz p(Z = z|x,y) requires
solving a weighted, edge-cover problem.
and each time adding the highest weight incident
edge whose addition doesn?t violate a constraint.
The running time is O(|R||S|). This greedy search
guarantees each fact is extracted at least once and
allows any additional extractions that increase the
overall probability of the assignment. Given the
computational advantage, we use it in all of the ex-
perimental evaluations.
6 Experimental Setup
We follow the approach of Riedel et al (2010) for
generating weak supervision data, computing fea-
tures, and evaluating aggregate extraction. We also
introduce new metrics for measuring sentential ex-
traction performance, both relation-independent and
relation-specific.
6.1 Data Generation
We used the same data sets as Riedel et al (2010)
for weak supervision. The data was first tagged with
the Stanford NER system (Finkel et al, 2005) and
then entity mentions were found by collecting each
continuous phrase where words were tagged iden-
tically (i.e., as a person, location, or organization).
Finally, these phrases were matched to the names of
Freebase entities.
Given the set of matches, define ? to be set of NY
Times sentences with two matched phrases, E to be
the set of Freebase entities which were mentioned in
one or more sentences, ? to be the set of Freebase
facts whose arguments, e1 and e2 were mentioned in
a sentence in ?, and R to be set of relations names
used in the facts of ?. These sets define the weak
supervision data.
6.2 Features and Initialization
We use the set of sentence-level features described
by Riedel et al (2010), which were originally de-
545
veloped by Mintz et al (2009). These include in-
dicators for various lexical, part of speech, named
entity, and dependency tree path properties of entity
mentions in specific sentences, as computed with the
Malt dependency parser (Nivre and Nilsson, 2004)
and OpenNLP POS tagger1. However, unlike the
previous work, we did not make use of any features
that explicitly aggregate these properties across mul-
tiple mention instances.
The MULTIR algorithm has a single parameter T ,
the number of training iterations, that must be spec-
ified manually. We used T = 50 iterations, which
performed best in development experiments.
6.3 Evaluation Metrics
Evaluation is challenging, since only a small per-
centage (approximately 3%) of sentences match
facts in Freebase, and the number of matches is
highly unbalanced across relations, as we will see
in more detail later. We use the following metrics.
Aggregate Extraction Let ?e be the set of ex-
tracted relations for any of the systems; we com-
pute aggregate precision and recall by comparing
?e with ?. This metric is easily computed but un-
derestimates extraction accuracy because Freebase
is incomplete and some true relations in ?e will be
marked wrong.
Sentential Extraction Let Se be the sentences
where some system extracted a relation and SF be
the sentences that match the arguments of a fact in
?. We manually compute sentential extraction ac-
curacy by sampling a set of 1000 sentences from
Se ? SF and manually labeling the correct extrac-
tion decision, either a relation r ? R or none. We
then report precision and recall for each system on
this set of sampled sentences. These results provide
a good approximation to the true precision but can
overestimate the actual recall, since we did not man-
ually check the much larger set of sentences where
no approach predicted extractions.
6.4 Precision / Recall Curves
To compute precision / recall curves for the tasks,
we ranked the MULTIR extractions as follows. For
sentence-level evaluations, we ordered according to
1http://opennlp.sourceforge.net/
Recall
Precision
0.00 0.05 0.10 0.15 0.20 0.25 0.30
0.0
0.2
0.4
0.6
0.8
1.0
SOLOR
Riedel et al, 2010
MULTIR
Figure 4: Aggregate extraction precision / recall curves
for Riedel et al (2010), a reimplementation of that ap-
proach (SOLOR), and our algorithm (MULTIR).
the extraction factor score ?extract(zi, xi). For aggre-
gate comparisons, we set the score for an extraction
Y r = true to be the max of the extraction factor
scores for the sentences where r was extracted.
7 Experiments
To evaluate our algorithm, we first compare it to an
existing approach for using multi-instance learning
with weak supervision (Riedel et al, 2010), using
the same data and features. We report both aggregate
extraction and sentential extraction results. We then
investigate relation-specific performance of our sys-
tem. Finally, we report running time comparisons.
7.1 Aggregate Extraction
Figure 4 shows approximate precision / recall curves
for three systems computed with aggregate metrics
(Section 6.3) that test how closely the extractions
match the facts in Freebase. The systems include the
original results reported by Riedel et al (2010) as
well as our new model (MULTIR). We also compare
with SOLOR, a reimplementation of their algorithm,
which we built in Factorie (McCallum et al, 2009),
and will use later to evaluate sentential extraction.
MULTIR achieves competitive or higher preci-
sion over all ranges of recall, with the exception
of the very low recall range of approximately 0-
1%. It also significantly extends the highest recall
achieved, from 20% to 25%, with little loss in preci-
sion. To investigate the low precision in the 0-1% re-
call range, we manually checked the ten highest con-
546
Recall
Precision
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0.0
0.2
0.4
0.6
0.8
1.0
SOLOR
MULTIR
Figure 5: Sentential extraction precision / recall curves
for MULTIR and SOLOR.
fidence extractions produced by MULTIR that were
marked wrong. We found that all ten were true facts
that were simply missing from Freebase. A manual
evaluation, as we perform next for sentential extrac-
tion, would remove this dip.
7.2 Sentential Extraction
Although their model includes variables to model
sentential extraction, Riedel et al (2010) did not re-
port sentence level performance. To generate the
precision / recall curve we used the joint model as-
signment score for each of the sentences that con-
tributed to the aggregate extraction decision.
Figure 4 shows approximate precision / recall
curves for MULTIR and SOLOR computed against
manually generated sentence labels, as defined in
Section 6.3. MULTIR achieves significantly higher
recall with a consistently high level of precision. At
the highest recall point, MULTIR reaches 72.4% pre-
cision and 51.9% recall, for an F1 score of 60.5%.
7.3 Relation-Specific Performance
Since the data contains an unbalanced number of in-
stances of each relation, we also report precision and
recall for each of the ten most frequent relations. Let
SMr be the sentences where MULTIR extracted an
instance of relation r ? R, and let SFr be the sen-
tences that match the arguments of a fact about re-
lation r in ?. For each r, we sample 100 sentences
from both SMr and S
F
r and manually check accu-
racy. To estimate precision P?r we compute the ratio
of true relation mentions in SMr , and to estimate re-
call R?r we take the ratio of true relation mentions in
SFr which are returned by our system.
Table 1 presents this approximate precision and
recall for MULTIR on each of the relations, along
with statistics we computed to measure the qual-
ity of the weak supervision. Precision is high for
the majority of relations but recall is consistently
lower. We also see that the Freebase matches are
highly skewed in quantity and can be low quality for
some relations, with very few of them actually cor-
responding to true extractions. The approach gener-
ally performs best on the relations with a sufficiently
large number of true matches, in many cases even
achieving precision that outperforms the accuracy of
the heuristic matches, at reasonable recall levels.
7.4 Overlapping Relations
Table 1 also highlights some of the effects of learn-
ing with overlapping relations. For example, in the
data, almost all of the matches for the administra-
tive divisions relation overlap with the contains re-
lation, because they both model relationships for a
pair of locations. Since, in general, sentences are
much more likely to describe a contains relation, this
overlap leads to a situation were almost none of the
administrate division matches are true ones, and we
cannot accurately learn an extractor. However, we
can still learn to accurately extract the contains rela-
tion, despite the distracting matches. Similarly, the
place of birth and place of death relations tend to
overlap, since it is often the case that people are born
and die in the same city. In both cases, the precision
outperforms the labeling accuracy and the recall is
relatively high.
To measure the impact of modeling overlapping
relations, we also evaluated a simple, restricted
baseline. Instead of labeling each entity pair with
the set of all true Freebase facts, we created a dataset
where each true relation was used to create a dif-
ferent training example. Training MULTIR on this
data simulates effects of conflicting supervision that
can come from not modeling overlaps. On average
across relations, precision increases 12 points but re-
call drops 26 points, for an overall reduction in F1
score from 60.5% to 40.3%.
7.5 Running Time
One final advantage of our model is the mod-
est running time. Our implementation of the
547
Relation
Freebase Matches MULTIR
#sents % true P? R?
/business/person/company 302 89.0 100.0 25.8
/people/person/place lived 450 60.0 80.0 6.7
/location/location/contains 2793 51.0 100.0 56.0
/business/company/founders 95 48.4 71.4 10.9
/people/person/nationality 723 41.0 85.7 15.0
/location/neighborhood/neighborhood of 68 39.7 100.0 11.1
/people/person/children 30 80.0 100.0 8.3
/people/deceased person/place of death 68 22.1 100.0 20.0
/people/person/place of birth 162 12.0 100.0 33.0
/location/country/administrative divisions 424 0.2 N/A 0.0
Table 1: Estimated precision and recall by relation, as well as the number of matched sentences (#sents) and accuracy
(% true) of matches between sentences and facts in Freebase.
Riedel et al (2010) approach required approxi-
mately 6 hours to train on NY Times 05-06 and 4
hours to test on the NY Times 07, each without pre-
processing. Although they do sampling for infer-
ence, the global aggregation variables require rea-
soning about an exponentially large (in the number
of sentences) sample space.
In contrast, our approach required approximately
one minute to train and less than one second to test,
on the same data. This advantage comes from the
decomposition that is possible with the determinis-
tic OR aggregation variables. For test, we simply
consider each sentence in isolation and during train-
ing our approximation to the weighted assignment
problem is linear in the number of sentences.
7.6 Discussion
The sentential extraction results demonstrates the
advantages of learning a model that is primarily
driven by sentence-level features. Although previ-
ous approaches have used more sophisticated fea-
tures for aggregating the evidence from individual
sentences, we demonstrate that aggregating strong
sentence-level evidence with a simple deterministic
OR that models overlapping relations is more effec-
tive, and also enables training of a sentence extractor
that runs with no aggregate information.
While the Riedel et al approach does include a
model of which sentences express relations, it makes
significant use of aggregate features that are primar-
ily designed to do entity-level relation predictions
and has a less detailed model of extractions at the
individual sentence level. Perhaps surprisingly, our
model is able to do better at both the sentential and
aggregate levels.
8 Related Work
Supervised-learning approaches to IE were intro-
duced in (Soderland et al, 1995) and are too nu-
merous to summarize here. While they offer high
precision and recall, these methods are unlikely to
scale to the thousands of relations found in text on
the Web. Open IE systems, which perform self-
supervised learning of relation-independent extrac-
tors (e.g., Preemptive IE (Shinyama and Sekine,
2006), TEXTRUNNER (Banko et al, 2007; Banko
and Etzioni, 2008) and WOE (Wu and Weld, 2010))
can scale to millions of documents, but don?t output
canonicalized relations.
8.1 Weak Supervision
Weak supervision (also known as distant- or self su-
pervision) refers to a broad class of methods, but
we focus on the increasingly-popular idea of using
a store of structured data to heuristicaly label a tex-
tual corpus. Craven and Kumlien (1999) introduced
the idea by matching the Yeast Protein Database
(YPD) to the abstracts of papers in PubMed and
training a naive-Bayes extractor. Bellare and Mc-
Callum (2007) used a database of BibTex records
to train a CRF extractor on 12 bibliographic rela-
tions. The KYLIN system aplied weak supervision
to learn relations from Wikipedia, treating infoboxes
as the associated database (Wu and Weld, 2007);
Wu et al (2008) extended the system to use smooth-
ing over an automatically generated infobox taxon-
548
omy. Mintz et al (2009) used Freebase facts to train
100 relational extractors on Wikipedia. Hoffmann
et al (2010) describe a system similar to KYLIN,
but which dynamically generates lexicons in order
to handle sparse data, learning over 5000 Infobox
relations with an average F1 score of 61%. Yao
et al (2010) perform weak supervision, while using
selectional preference constraints to a jointly reason
about entity types.
The NELL system (Carlson et al, 2010) can also
be viewed as performing weak supervision. Its ini-
tial knowledge consists of a selectional preference
constraint and 20 ground fact seeds. NELL then
matches entity pairs from the seeds to a Web cor-
pus, but instead of learning a probabilistic model,
it bootstraps a set of extraction patterns using semi-
supervised methods for multitask learning.
8.2 Multi-Instance Learning
Multi-instance learning was introduced in order to
combat the problem of ambiguously-labeled train-
ing data when predicting the activity of differ-
ent drugs (Dietterich et al, 1997). Bunescu and
Mooney (2007) connect weak supervision with
multi-instance learning and extend their relational
extraction kernel to this context.
Riedel et al (2010), combine weak supervision
and multi-instance learning in a more sophisticated
manner, training a graphical model, which assumes
only that at least one of the matches between the
arguments of a Freebase fact and sentences in the
corpus is a true relational mention. Our model may
be seen as an extension of theirs, since both models
include sentence-level and aggregate random vari-
ables. However, Riedel et al have only a single ag-
gregate variable that takes values r ? R ? {none},
thereby ruling out overlapping relations. We have
discussed the comparison in more detail throughout
the paper, including in the model formulation sec-
tion and experiments.
9 Conclusion
We argue that weak supervision is promising method
for scaling information extraction to the level where
it can handle the myriad, different relations on the
Web. By using the contents of a database to heuris-
tically label a training corpus, we may be able to
automatically learn a nearly unbounded number of
relational extractors. Since the processs of match-
ing database tuples to sentences is inherently heuris-
tic, researchers have proposed multi-instance learn-
ing algorithms as a means for coping with the result-
ing noisy data. Unfortunately, previous approaches
assume that all relations are disjoint ? for exam-
ple they cannot extract the pair Founded(Jobs,
Apple) and CEO-of(Jobs, Apple), because
two relations are not allowed to have the same argu-
ments.
This paper presents a novel approach for multi-
instance learning with overlapping relations that
combines a sentence-level extraction model with a
simple, corpus-level component for aggregating the
individual facts. We apply our model to learn extrac-
tors for NY Times text using weak supervision from
Freebase. Experiments show improvements for both
sentential and aggregate (corpus level) extraction,
and demonstrate that the approach is computation-
ally efficient.
Our early progress suggests many interesting di-
rections. By joining two or more Freebase tables,
we can generate many more matches and learn more
relations. We also wish to refine our model in order
to improve precision. For example, we would like
to add type reasoning about entities and selectional
preference constraints for relations. Finally, we are
also interested in applying the overall learning ap-
proaches to other tasks that could be modeled with
weak supervision, such as coreference and named
entity classification.
The source code of our system, its out-
put, and all data annotations are available at
http://cs.uw.edu/homes/raphaelh/mr.
Acknowledgments
We thank Sebastian Riedel and Limin Yao for shar-
ing their data and providing valuable advice. This
material is based upon work supported by a WRF /
TJ Cable Professorship, a gift from Google and by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the author(s)
and do not necessarily reflect the view of the Air
Force Research Laboratory (AFRL).
549
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-08), pages
28?36.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of the 20th International Joint Conference on Artificial
Intelligence (IJCAI-07), pages 2670?2676.
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In Sixth International Workshop on Infor-
mation Integration on the Web.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
07).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the AAAI
Conference on Artificial Intelligence (AAAI-10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2002).
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology, pages 77?86.
Thomas G. Dietterich, Richard H. Lathrop, and Toma?s
Lozano-Pe?rez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artificial Intel-
ligence, 89:31?71, January.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL-
05), pages 363?370.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-10), pages
286?295.
Percy Liang, A. Bouchard-Co?te?, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach
to machine translation. In International Conference on
Computational Linguistics and Association for Com-
putational Linguistics (COLING/ACL).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. Factorie: Probabilistic programming via imper-
atively defined factor graphs. In Neural Information
Processing Systems Conference (NIPS).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL-2009), pages 1003?1011.
Joakim Nivre and Jens Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of the Conference
on Natural Language Learning (CoNLL-04), pages
49?56.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the Sixteenth Euro-
pean Conference on Machine Learning (ECML-2010),
pages 148?163.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computation Linguistics (HLT-
NAACL-06).
Stephen Soderland, David Fisher, Jonathan Aseltine, and
Wendy G. Lehnert. 1995. Crystal: Inducing a concep-
tual dictionary. In Proceedings of the Fourteenth In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-1995), pages 1314?1321.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the Inter-
national Conference on Information and Knowledge
Management (CIKM-2007), pages 41?50.
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the wikipedia infobox ontology. In Proceedings of
the 17th International Conference on World Wide Web
(WWW-2008), pages 635?644.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In The Annual Meeting of
the Association for Computational Linguistics (ACL-
2010), pages 118?127.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2010), pages 1013?1023.
Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to log-
ical form. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL-2007).
550
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 845?853,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Discriminative Learning for Joint Template Filling
Einat Minkov
Information Systems
University of Haifa
Haifa 31905, Israel
einatm@is.haifa.ac.il
Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
lsz@cs.washington.edu
Abstract
This paper presents a joint model for tem-
plate filling, where the goal is to automati-
cally specify the fields of target relations such
as seminar announcements or corporate acqui-
sition events. The approach models mention
detection, unification and field extraction in
a flexible, feature-rich model that allows for
joint modeling of interdependencies at all lev-
els and across fields. Such an approach can,
for example, learn likely event durations and
the fact that start times should come before
end times. While the joint inference space is
large, we demonstrate effective learning with
a Perceptron-style approach that uses simple,
greedy beam decoding. Empirical results in
two benchmark domains demonstrate consis-
tently strong performance on both mention de-
tection and template filling tasks.
1 Introduction
Information extraction (IE) systems recover struc-
tured information from text. Template filling is an IE
task where the goal is to populate the fields of a tar-
get relation, for example to extract the attributes of a
job posting (Califf and Mooney, 2003) or to recover
the details of a corporate acquisition event from a
news story (Freitag and McCallum, 2000).
This task is challenging due to the wide range
of cues from the input documents, as well as non-
textual background knowledge, that must be consid-
ered to find the best joint assignment for the fields
of the extracted relation. For example, Figure 1
shows an extraction from CMU seminar announce-
ment corpus (Freitag and McCallum, 2000). Here,
the goal is to perform mention detection and extrac-
tion, by finding all of the text spans, or mentions,
Date 5/5/1995
Start Time 3:30PM
Location Wean Hall 5409
Speaker Raj Reddy
Title Some Necessary Conditions for a Good User Interface
End Time ?
Figure 1: An example email and its template. Field men-
tions are highlighted in the text, grouped by color.
that describe field values, unify these mentions by
grouping them according to target field, and normal-
izing the results within each group to provide the
final extractions. Each of these steps requires sig-
nificant knowledge about the target relation. For ex-
ample, in Figure 1, the mention ?3:30? appears three
times and provides the only reference to a time. We
must infer that this is the starting time, that the end
time is never explicitly mentioned, and also that the
event is in the afternoon. Such inferences may not
hold in more general settings, such as extraction for
medical emergencies or related events.
In this paper, we present a joint modeling and
learning approach for the combined tasks of men-
tion detection, unification, and template filling, as
described above. As we will see in Section 2, pre-
vious work has mostly focused on learning tagging
845
models for mention detection, which can be diffi-
cult to aggregate into a full template extraction, or
directly learning template field value extractors, of-
ten in isolation and with no reasoning across differ-
ent fields in the same relation. We present a simple,
feature-rich, discriminative model that readily incor-
porates a broad range of possible constraints on the
mentions and joint field assignments.
Such an approach allows us to learn, for each tar-
get relation, an integrated model to weight the dif-
ferent extraction options, including for example the
likely lengths for events, or the fact that start times
should come before end times. However, there are
significant computation challenges that come with
this style of joint learning. We demonstrate empiri-
cally that these challenges can be solved with a com-
bination of greedy beam decoding, performed di-
rectly in the joint space of possible mention clusters
and field assignments, and structured Perceptron-
style learning algorithm (Collins, 2002).
We report experimental evaluations on two bench-
mark datasets in different genres, the CMU semi-
nar announcements and corporate acquisitions (Fre-
itag and McCallum, 2000). In each case, we evalu-
ated both template extraction and mention detection
performance. Our joint learning approach provides
consistently strong results across every setting, in-
cluding new state-of-the-art results. We also demon-
strate, through ablation studies on the feature set, the
need for joint modeling and the relative importance
of the different types of joint constraints.
2 Related Work
Research on the task of template filling has focused
on the extraction of field value mentions from the
underlying text. Typically, these values are extracted
based on local evidence, where the most likely entity
is assigned to each slot (Roth and Yih, 2001; Siefkes,
2008). There has been little effort towards a compre-
hensive approach that includes mention unification,
as well as considers the structure of the target rela-
tional schema to create semantically valid outputs.
Recently, Haghighi and Klein (2010) presented
a generative semi-supervised approach for template
filling. In their model, slot-filling entities are first
generated, and entity mentions are then realized in
text. Thus, their approach performs coreference at
slot level. In addition to proper nouns (named en-
tity mentions) that are considered in this work, they
also account for nominal and pronominal noun men-
tions. This work presents a discriminative approach
to this problem. An advantage of a discriminative
framework is that it allows the incorporation of rich
and possibly overlapping features. In addition, we
enforce label consistency and semantic coherence at
record level.
Other related works perform structured relation
discovery for different settings of information ex-
traction. In open IE, entities and relations may be in-
ferred jointly (Roth and Yih, 2002; Yao et al, 2011).
In this IE task, the target relation must agree with the
entity types assigned to it; e.g., born-in relation re-
quires a place as its argument. In addition, extracted
relations may be required to be consistent with an
existing ontology (Carlson et al, 2010). Compared
with the extraction of tuples of entity mention pairs,
template filling is associated with a more complex
target relational schema.
Interestingly, several researchers have attempted
to model label consistency and high-level relational
constraints using state-of-the-art sequential models
of named entity recognition (NER). Mainly, pre-
determined word-level dependencies were repre-
sented as links in the underlying graphical model
(Sutton and McCallum, 2004; Finkel et al, 2005).
Finkel et al (2005) further modelled high-level se-
mantic constraints; for example, using the CMU
seminar announcements dataset, spans labeled as
start time or end time were required to be seman-
tically consistent. In the proposed framework we
take a bottom-up approach to identifying entity men-
tions in text, where given a noisy set of candidate
named entities, described using rich semantic and
surface features, discriminative learning is applied
to label these mentions. We will show that this ap-
proach yields better performance on the CMU semi-
nar announcement dataset when evaluated in terms
of NER. Our approach is complimentary to NER
methods, as it can consolidate noisy overlapping
predictions from multiple systems into coherent sets.
3 Problem Setting
In the template filling task, a target relation r is pro-
vided, comprised of attributes (also referred to as
846
Figure 2: The relational schema for the seminars domain.
Figure 3: A record partially populated from text.
fields, or slots) A(r). Given a document d, which
is known to describe a tuple of the underlying re-
lation, the goal is to populate the fields with values
based on the text.
The relational schema. In this work, we describe
domain knowledge through an extended relational
database schema R. In this schema, every field of
the target relation maps to a tuple of another rela-
tion, giving rise to a hierarchical view of template
filling. Figure 2 describes a relational schema for
the seminar announcement domain. As shown, each
field of the seminar relation maps to another rela-
tion; e.g., speaker?s values correspond to person tu-
ples. According to the outlined schema, most re-
lations (e.g., person) consist of a single attribute,
whereas the date and time relations are characterised
with multiple attributes; for example, the time rela-
tion includes the fields of hour, minutes and ampm.
We will make use of limited domain knowledge,
expressed as relation-level constraints that are typi-
cally realized in a database. Namely, the following
tests are supported for each relation.
Tuple validity. This test reflects data integrity. The
attributes of a relation may be defined as mandatory
or optional. Mandatory attributes are denoted with a
solid boundary in Figure 2 (e.g., seminar.date), and
optional attributes are denoted with a dashed bound-
ary (e.g., seminar.title). Similar constraints can be
posed on a set of attributes; e.g., either day-of-month
or day-of-week must be populated in the date rela-
tion. Finally, a combination of field values may be
required to be valid, e.g., the values of day, month,
year and day-of-week must be consistent.
Tuple contradiction. This function checks
whether two valid tuples v1 and v2 are inconsis-
tent, implying a negation of possible unification of
these tuples. In this work, we consider date and time
tuples as contradictory if they contain semantically
different values for some field; tuples of location,
person and title are required to have minimal over-
lap in their string values to avoid contradiction.
Template filling. Given document d, the hierar-
chical schema R is populated in a bottom-up fash-
ion. Generally, parent-free relations in the hierar-
chy correspond to generic entities, realized as en-
tity mentions in the text. In Figure 2, these relations
are denoted by double-line boundary, including lo-
cation, person, title, date and time; every tuple of
these relations maps to a named entity mention.1
Figure 3 demonstrates the correct mapping of
named entity mentions to tuples, as well as tuple uni-
fication, for the example shown in Figure 1. For ex-
ample, the mentions ?Wean 5409? and ?Wean Hall
5409? correspond to tuples of the location relation,
where the two tuples are resolved into a unified set.
To complete template filling, the remaining relations
of the schema are populated bottom-up, where each
field links to a unified set of populated tuples. For
example, in Figure 3, the seminar.location field is
linked to {?Wean Hall 5409?,?Wean 5409?}.
Value normalization of the unified tuples is an-
other component of template filling. We partially ad-
dress normalization: tuples of semantically detailed
(multi-attribute) relations, e.g., date and time, are re-
solved into their semantic union, while textual tuples
(e.g., location) are normalized to the longest string
in the set. In this work, we assume that each tem-
plate slot contains at most one value. This restriction
can be removed, at the cost of increasing the size of
the decoding search space.
1In the multi-attribute relations of date and time, each at-
tribute maps to a text span, where the set of spans at tuple-level
is required to be sequential (up to a small distance d).
847
4 Structured Learning
Next, we describe how valid candidate extrac-
tions are instantiated (Sec. 4.1) and how learning
is applied to assess the quality of the candidates
(Sec. 4.2), where beam search is used to find the top
scoring candidates efficiently (Sec. 4.3).
4.1 Candidate Generation
Named entity recognition. A set of candidate men-
tions Sd(a) is extracted from document d per each
attribute a of a relation r ? L, where L is the set
of parent-free relations in T . We aim at high-recall
extractions; i.e., Sd(a) is expected to contain the cor-
rect mentions with high probability. Various IE tech-
niques, as well as an ensemble of methods, can be
employed for this purpose. For each relation r ? L,
valid candidate tuples Ed(r) are constructed from
the candidate mentions that map to its attributes.
Unification. For every relation r ? L, we con-
struct candidate sets of unified tuples, {Cd(r) ?
Ed(r)}. Naively, the number of candidate sets is
exponential in the size of Ed(t). Importantly, how-
ever, the tuples within a candidate unification set are
required to be non-contradictory. In addition, the
text spans that comprise the mentions within each
set must not overlap. Finally, we do not split tuples
with identical string values between different sets.
Candidate tuples. To construct the space of candi-
date tuples of the target relation, the remaining rela-
tions r ? {T?L} are visited bottom-up, where each
field a ? A(r) is mapped in turn to a (possibly uni-
fied) populated tuple of its type. The valid (and non-
overlapping) combinations of field mappings consti-
tute a set of candidate tuples of r.
The candidate tuples generated using this proce-
dure are structured entities, constructed using typed
named entity recognition, unification, and hierarchi-
cal assignment of field values (Figure 3). We will
derive features that describe local and global prop-
erties of the candidate tuples, encoding both surface
and semantic information.
4.2 Learning
We employ a discriminative learning algorithm, fol-
lowing Collins (2002). Our goal is to find the candi-
Algorithm 1: The beam search procedure
1. Populate every low-level relation r ? L from text d:
? Construct a set of candidate valid tuples Ed(r) given
high-recall typed candidate text spans Sd(a), a ? A(r).
? Group Ed(r) into possibly overlapping unified sets,
{Cd(r) ? Ed(r)}.
2. Iterate bottom-up through relations r ? {T ? L}:
? Initialize the set of candidate tuples Ed(r) to an empty
set.
? Iterate through attributes a ? A(r):
? Retrieve the set of candidate tuples (or unified tuple
sets) Ed(r?), where r? is the relation that attribute a
links to in T . Add an empty tuple to the set.
? For every pair of candidate tuples e ? Ed(r) and
e? ? Ed(r?), modify e by linking attribute a(e) to
tuple e?.
? Add the modified tuples, if valid, to Ed(r).
? Apply Equation 1 to rank the partially filled candi-
date tuples e ? Ed(r). Keep the k top scoring can-
didates in Ed(r), and discard the rest.
3. Apply Equation 1 to output a ranked list of extracted records
Ed(r?), where r? is the target relation.
date that maximizes:
F (y, ??) =
m
?
j=1
?jfj(y, d, T ) (1)
where fj(d, y, T ), j = 1, ..,m, are pre-defined fea-
ture functions describing a candidate record y of the
target relation given document d and the extended
schema T . The parameter weights ?j are to be
learned from labeled instances. The training pro-
cedure involves initializing the weights ?? to zero.
Given ??, an inference procedure is applied to find
the candidate that maximizes Equation 1. If the top-
scoring candidate is different from the correct map-
ping known, then: (i) ?? is incremented with the fea-
ture vector of the correct candidate, and (ii) the fea-
ture vector of the top-scoring candidate is subtracted
from ??. This procedure is repeated for a fixed num-
ber of epochs. Following Collins, we employ the av-
eraged Perceptron online algorithm (Collins, 2002;
Freund and Schapire, 1999) for weight learning.
4.3 Beam Search
Unfortunately, optimal local decoding algorithms
(such as the Viterbi algorithm in tagging problems
(Collins, 2002)) can not be applied to our prob-
lem. We therefore propose using beam search to ef-
ficiently find the top scoring candidate. This means
848
that rather than instantiate the full space of valid can-
didate records (Section 4.1), we are interested in in-
stantiating only those candidates that are likely to be
assigned a high score by F . Algorithm 1 outlines
the proposed beam search procedure. As detailed,
only a set of top scoring tuples of size k (beam size)
is maintained per relation r ? T during candidate
generation. A given relation is populated incremen-
tally, having each of its attributes a ? A(r) map in
turn to populated tuples of its type, and using Equa-
tion 1 to find the k highest scoring partially popu-
lated tuples; this limits the number of candidate tu-
ples evaluated to k2 per attribute, and to nk2 for a
relation with n attributes. While beam search is effi-
cient, performance may be compromised compared
with an unconstrained search. The beam size k al-
lows controlling the trade-off between performance
and cost. An advantage of the proposed approach is
that rather than output a single prediction, a list of
coherent candidate tuples may be generated, ranked
according to Equation 1.
5 Seminar Extraction Task
Dataset The CMU seminar announcement dataset
(Freitag and McCallum, 2000) includes 485 emails
containing seminar announcements. The dataset has
been originally annotated with text spans referring to
four slots: speaker, location, stime, and etime. We
have annotated this dataset with two additional at-
tributes: date and title.2 We consider this corpus as
an example of semi-structured text, where some of
the field values appear in the email header, in a tabu-
lar structure, or using special formatting (Califf and
Mooney, 1999; Minkov et al, 2005).3
We used a set of rules to extract candidate named
entities per the types specified in Figure 2.4 The
rules encode information typically used in NER, in-
cluding content and contextual patterns, as well as
lookups in available dictionaries (Finkel et al, 2005;
Minkov et al, 2005). The extracted candidates are
high-recall and overlapping. In order to increase
recall further, additional candidates were extracted
based on document structure (Siefkes, 2008). The
2A modified dataset is available on the author?s homepage.
3Such structure varies across messages. Otherwise, the
problem would reduce to wrapper learning (Zhu et al, 2006).
4The rule language used is based on cascaded finite state
machines (Minorthird, 2008).
recall for the named entities of type date and time is
near perfect, and is estimated at 96%, 91% and 90%
for location, speaker and title, respectively.
Features The categories of the features used are
described below. All features are binary and typed.5
Lexical. These features indicate the value and
pattern of words within the text spans correspond-
ing to each field. For example, lexical features per
Figure 1 include location.content.word.wean, loca-
tion.pattern.capitalized. Similar features are derived
for a window of three words to the right and to the
left of the included spans. In addition, we observe
whether the words that comprise the text spans ap-
pear in relevant dictionaries: e.g., whether the spans
assigned to the location field include words typi-
cal of location, such as ?room? or ?hall?. Lex-
ical features of this form are commonly used in
NER (Finkel et al, 2005; Minkov et al, 2005).
Structural. It has been previously shown that
the structure available in semi-structured documents
such as email messages is useful for information ex-
traction (Minkov et al, 2005; Siefkes, 2008). As
shown in Figure 1, an email message includes a
header, specifying textual fields such as topic, dates
and time. In addition, space lines and line breaks are
used to emphasize blocks of important information.
We propose a set of features that model correspon-
dence between the text spans assigned to each field
and document structure. Specifically, these features
model whether at least one of the spans mapped to
each field appears in the email header; captures a
full line in the document; is indent; appears within
space lines; or in a tabular format. In Figure 1, struc-
tural active features include location.inHeader, lo-
cation.fullLine, title.withinSpaceLines, etc.
Semantic. These features refer to the semantic
interpretation of field values. According to the re-
lational schema (Figure 2), date and time include
detailed attributes, whereas other relations are rep-
resented as strings. The semantic features encoded
therefore refer to date and time only. Specifically,
these features indicate whether a unified set of tu-
ples defines a value for all attributes; for example,
in Figure 1, the union of entities that map to the
date field specify all of the attribute values of this
relation, including day-of-month, month, year, and
5Real-value features were discretized into segments.
849
Date Stime Etime Location Speaker Title
Full model 96.1 99.3 98.7 96.4 87.5 69.5
No structural features 94.9 99.1 98.0 96.1 83.8 65.1
No semantic features 96.1 98.7 95.4 96.4 87.5 69.5
No unification 87.2 97.0 95.1 94.5 76.0 62.7
Individual fields 96.5 97.2 - 96.4 86.8 64.5
Table 1: Seminar extraction results (5-fold CV): Field-level F1
Date Stime Etime Location Speaker Title
SNOW (Roth and Yih, 2001) - 99.6 96.3 75.2 73.8 -
BIEN (Peshkin and Pfeffer, 2003) - 96.0 98.8 87.1 76.9 -
Elie (Finn, 2006) - 98.5 96.4 86.5 88.5 -
TIE (Siefkes, 2008) - 99.3 97.1 81.7 85.4 -
Full model 96.3 99.1 98.0 96.9 85.8 67.7
Table 2: Seminar extraction results (5-fold CV, trained on 50% of corpus): Field-level F1
day-of-week. Another feature encodes the size of the
most semantically detailed named entity that maps
to a field; for example, the most detailed entity men-
tion of type stime in Figure 1 is ?3:30?, compris-
ing of two attribute values, namely hour and min-
utes. Similarly, the total number of semantic units
included in a unified set is represented as a feature.
These features were designed to favor semantically
detailed mentions and unified sets. Finally, domain-
specific semantic knowledge is encoded as features,
including the duration of the seminar, and whether a
time value is round (minutes divide by 5).
In addition to the features described, one may
be interested in modeling cross-field information.
We have experimented with features that encode
the shortest distance between named entity mentions
mapping to different fields (measured in terms of
separating lines or sentences), based on the hypoth-
esis that field values typically co-appear in the same
segments of the document. These features were not
included in the final model since their contribution
was marginal. We leave further exploration of cross-
field features in this domain to future work.
Experiments We conducted 5-fold cross vali-
dation experiments using the seminar extraction
dataset. As discussed earlier, we assume that a sin-
gle record is described in each document, and that
each field corresponds to a single value. These
assumptions are violated in a minority of cases.
In evaluating the template filling task, only exact
matches are accepted as true positives, where partial
matches are counted as errors (Siefkes, 2008). No-
tably, the annotated labels as well as corpus itself are
not error-free; for example, in some announcements
the date and day-of-week specified are inconsistent.
Our evaluation is strict, where non-empty predicted
values are counted as errors in such cases.
Table 1 shows the results of our full model us-
ing beam size k = 10, as well as model variants.
In order to evaluate the contribution of the proposed
features, we eliminated every feature group in turn.
As shown in the table, removing the structural fea-
tures hurt performance consistently across fields. In
particular, structure is informative for the title field,
which is otherwise characterised with low content
and contextual regularity. Removal of the semantic
features affected performance on the stime and etime
fields, modeled by these features. In particular, the
optional etime field, which has fewer occurrences in
the dataset, benefits from modeling semantics.
An important question to be addressed in evalu-
ation is to what extent the joint modeling approach
contributes to performance. In another experiment
we therefore mimic the typical scenario of template
filling, in which the value of the highest scoring
named entity is assigned to each field. In our frame-
work, this corresponds to a setting in which a unified
set includes no more than a single entity. The results
are shown in Table 1 (?no unification?). Due to re-
duced evidence given a single entity versus a a coref-
erent set of entities, this results in significantly de-
graded performance. Finally, we experimented with
populating every field of the target schema indepen-
dently of the other fields. While results are overall
comparable on most fields, this had negative impact
on the title field. This is largely due to erroneous as-
signments of named entities of other types (mainly,
person) as titles; such errors are avoided in the full
joint model, where tuple validity is enforced.
Table 2 provides a comparison of the full model
850
Date Stime Etime Location Speaker Title
(Sutton and McCallum, 2004) - 96.7 97.2 88.1 80.4 -
(Finkel et al, 2005) - 97.1 97.9 90.0 84.2 -
Full model 95.4 97.1 97.9 97.0 86.5 75.5
Table 3: Seminar extraction results: Token-level F1
against previous state-of-the-art results. These re-
sults were all obtained using half of the corpus for
training, and its remaining half for evaluation; the
reported figures were averaged over five random
splits. For comparison, we used 5-fold cross vali-
dation, where only a subset of each train fold that
corresponds to 50% of the corpus was used for train-
ing. Due to the reduced training data, the results are
slightly lower than in Table 1. (Note that we used the
same test examples in both cases.) The best results
per field are marked in boldface. The proposed ap-
proach yields the best or second-best performance
on all target fields, and gives the best performance
overall. While a variety of methods have been ap-
plied in previous works, none has modeled template
filling in a joint fashion. As argued before, joint
modeling is especially important for irregular fields,
such as title; we provide first results on this field.
Previously, Sutton and McCallum (2004) and
later Finkel et-al. (2005), applied sequential models
to perform NER on this dataset, identifying named
entities that pertain to the template slots. Both of
these works incorporated coreference and high-level
semantic information to a limited extent. We com-
pare our approach to their work, having obtained and
used the same 5-fold cross validation splits as both
works. Table 3 shows results in terms of token F1.
Our results evaluated on the named mention recogni-
tion task are superior overall, giving comparable or
best performance on all fields. We believe that these
results demonstrate the benefit of performing men-
tion recognition as part of a joint model that takes
into account detailed semantics of the underlying re-
lational schema, when available.
Finally, we evaluate the global quality of the ex-
tracted records. Rather than assess performance at
field-level, this stricter evaluation mode considers a
whole tuple, requiring the values assigned to all of
its fields to be correct. Overall, our full model (Table
1) extracts globally correct records for 52.6% of the
examples. To our knowledge, this is the first work
that provides this type of evaluation on this dataset.
Importantly, an advantage of the proposed approach
Figure 4: The relational schema for acquisitions.
is that it readily outputs a ranked list of coherent pre-
dictions. While the performance at the top of the
output lists was roughly comparable, increasing k
gives higher oracle recall: the correct record was
included in the output k-top list 69.7%, 76.1% and
80.4% of the time, for k = 5, 10, 20 respectively.
6 Corporate Acquisitions
Dataset The corporate acquisitions corpus con-
tains 600 newswire articles, describing factual or po-
tential corporate acquisition events. The corpus has
been annotated with the official names of the parties
to an acquisition: acquired, purchaser and seller, as
well as their corresponding abbreviated names and
company codes.6 We describe the target schema us-
ing the relational structure depicted in Figure 4. The
schema includes two relations: the corp relation de-
scribes a corporate entity, including its full name,
abbreviated name and code as attributes; the target
acquisition relation includes three role-designating
attributes, each linked to a corp tuple.
Candidate name mentions in this strictly gram-
matical genre correspond to noun phrases. Docu-
ments were pre-processed to extract noun phrases,
similarly to Haghighi and Klein (2010).
Features We model syntactic features, following
Haghighi and Klein (2010). In order to compen-
sate for parsing errors, shallow syntactic features
were added, representing the values of neighboring
verbs and prepositions (Cohen et al, 2005). While
newswire documents are mostly unstructured, struc-
tural features are used to indicate whether any of the
purchaser, acquired and seller text spans appears in
6In this work, we ignore other fields annotated, as they are
inconsistently defined, have low number of occurrences in the
corpus, and are loosely inter-related semantically.
851
purname purabr purcode acqname acqabr acqcode sellname sellabr sellcode
TIE (batch) 55.7 58.1 - 53.5 55.0 - 31.8 25.8 -
TIE (inc) 51.6 55.3 - 49.2 51.7 - 26.0 24.0 -
Full model 48.9 55.0 70.2 50.7 55.2 67.2 33.2 36.8 55.4
Model variants:
No inter-type and struct. ftrs 45.1 50.5 66.8 49.8 53.9 66.4 34.9 42.2 56.0
No semantic features 42.6 38.4 58.1 40.5 36.5 44.8 32.2 26.6 46.6
Individual roles 43.9 48.7 62.5 45.0 47.2 52.7 34.1 40.3 47.8
Table 4: Corp. acquisition extraction results: Field-level F1
purname purabr purcode acqname acqabr acqcode sellname sellabr sellcode
TIE (batch) 52.6 40.5 - 49.2 43.7 28.7 16.4 -
TIE (inc) 48.4 38.6 - 44.7 42.7 - 23.6 14.5 -
Full model 45.0 48.3 69.8 46.4 59.5 66.9 31.6 33.0 55.0
Table 5: Corp. acquisition extraction results: Entity-level F1
the article?s header. Semantic features are applied
to corp tuples: we model whether the abbreviated
name is a subset of the full name; whether the cor-
porate code forms exact initials of the full or abbre-
viated names; or whether it has high string similarity
to any of these values. Finally, cross-type features
encode the shortest string between spans mapping
to different roles in the acquisition relation.
Experiments We applied beam search, where
corp tuples are extracted first, and acquisition tuples
are constructed using the top scoring corp entities.
We used a default beam size k = 10. The dataset is
split into a 300/300 train/test subsets.
Table 4 shows results of our full model in terms of
field-level F1, compared against TIE, a state-of-the-
art discriminative system (Siefkes, 2008). Unfortu-
nately, we can not directly compare against a gener-
ative joint model evaluated on this dataset (Haghighi
and Klein, 2010).7 The best results per attribute are
shown in boldface. Our full model performs bet-
ter overall than TIE trained incrementally (similarly
to our system), and is competitive with TIE using
batch learning. Interestingly, the performance of our
model on the code fields is high; these fields do
not involve boundary prediction, and thus reflect the
quality of role assignment.
Table 4 also shows the results of model vari-
ants. Removing the inter type and structural fea-
tures mildly hurt performance, on average. In con-
trast, the semantic features, which account for the
semantic cohesiveness of the populated corp tuples,
are shown to be necessary. In particular, remov-
7They report average performance on a different set of
fields; in addition, their results include modeling of pronouns
and nominal mentions, which are not considered here.
ing them degrades the extraction of the abbreviated
names; these features allow prediction of abbrevi-
ated names jointly with the full corporate names,
which are more regular (e.g., include a distinctive
suffix). Finally, we show results of predicting each
role filler individually. Inferring the roles jointly
(?full model?) significantly improves performance.
Table 5 further shows results on NER, the task of
recovering the sets of named entity mentions per-
taining to each target field. As shown, the proposed
joint approach performs overall significantly better
than previous results reported. These results are con-
sistent with the case study of seminar extraction.
7 Summary and Future Work
We presented a joint approach for template filling
that models mention detection, unification, and field
extraction in a flexible, feature-rich model. This ap-
proach allows for joint modeling of interdependen-
cies at all levels and across fields. Despite the com-
putational challenges of this joint inference space,
we obtained effective learning with a Perceptron-
style approach and simple beam decoding.
An interesting direction of future research is
to apply reranking to the output list of candidate
records using additional evidence, such as support-
ing evidence on the Web (Banko et al, 2008). Also,
modeling additional features or feature combina-
tions in this framework as well as effective feature
selection or improved parameter estimation (Cram-
mer et al, 2009) may boost performance. Finally,
it is worth exploring scaling the approach to unre-
stricted event extraction, and jointly model extract-
ing more than one relation per document.
852
References
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2008. Open in-
formation extraction from the web. In Proceedings of
IJCAI.
Mary Elaine Califf and Raymond J. Mooney. 1999. Re-
lational learning of pattern-match rules for information
extraction. In AAAI/IAAI.
Mary Elaine Califf and Raymond J. Mooney. 2003.
Bottom-up relational learning of pattern matching
rules for information extraction. Journal of Machine
Learning Research, 4.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of WSDM.
William W. Cohen, Einat Minkov, and Anthony Toma-
sic. 2005. Learning to understand web site update re-
quests. In Proceedings of the international joint con-
ference on Artificial intelligence (IJCAI).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Koby Crammer, Alex Kulesza, and Mark Dredze. 2009.
Adaptive regularization of weight vectors. In Ad-
vances in Neural Information Processing Systems
(NIPS).
Jenny Rose Finkel, Trond Grenager, , and Christopher D.
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL.
Aidan Finn. 2006. A multi-level boundary classification
approach to information extraction. In PhD thesis.
Dayne Freitag and Andrew McCallum. 2000. In-
formation extraction with hmm structures learned by
stochastic optimization. In AAAI/IAAI.
Yoav Freund and Rob Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37(3).
Aria Haghighi and Dan Klein. 2010. An entity-level ap-
proach to information extraction. In Proceedings of
ACL.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from emails: Ap-
plying named entity recognition to informal text. In
HLT/EMNLP.
Minorthird. 2008. Methods for identifying names and
ontological relations in text using heuristics for in-
ducing regularities from data. http://http://
minorthird.sourceforge.net.
Leonid Peshkin and Avi Pfeffer. 2003. Bayesian infor-
mation extraction network. In Proceedings of the in-
ternational joint conference on Artificial intelligence
(IJCAI).
Dan Roth and Wen-tau Yih. 2001. Relational learning
via propositional algorithms: An information extrac-
tion case study. In Proceedings of the international
joint conference on Artificial intelligence (IJCAI).
Dan Roth and Wen-tau Yih. 2002. Probabilistic reason-
ing for entity and relation recognition. In COLING.
Christian Siefkes. 2008. In An Incrementally Trainable
Statistical Approach to Information Extraction. VDM
Verlag.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive segmentation and labeling of distant entities in in-
formation extraction. In Technical Report no. 04-49,
University of Massachusetts.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of EMNLP.
Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and Wei-
Ying Ma. 2006. Simultaneous record detection and
attribute labeling in web data extraction. In Proc. of
the ACM SIGKDD Intl. Conf. on Knowledge Discovery
and Data Mining (KDD).
853
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1608?1618,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Paraphrase-Driven Learning for Open Question Answering
Anthony Fader Luke Zettlemoyer Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{afader, lsz, etzioni}@cs.washington.edu
Abstract
We study question answering as a ma-
chine learning problem, and induce a func-
tion that maps open-domain questions to
queries over a database of web extrac-
tions. Given a large, community-authored,
question-paraphrase corpus, we demon-
strate that it is possible to learn a se-
mantic lexicon and linear ranking func-
tion without manually annotating ques-
tions. Our approach automatically gener-
alizes a seed lexicon and includes a scal-
able, parallelized perceptron parameter es-
timation scheme. Experiments show that
our approach more than quadruples the re-
call of the seed lexicon, with only an 8%
loss in precision.
1 Introduction
Open-domain question answering (QA) is a long-
standing, unsolved problem. The central challenge
is to automate every step of QA system construc-
tion, including gathering large databases and an-
swering questions against these databases. While
there has been significant work on large-scale in-
formation extraction (IE) from unstructured text
(Banko et al, 2007; Hoffmann et al, 2010; Riedel
et al, 2010), the problem of answering questions
with the noisy knowledge bases that IE systems
produce has received less attention. In this paper,
we present an approach for learning to map ques-
tions to formal queries over a large, open-domain
database of extracted facts (Fader et al, 2011).
Our system learns from a large, noisy, question-
paraphrase corpus, where question clusters have
a common but unknown query, and can span
a diverse set of topics. Table 1 shows exam-
ple paraphrase clusters for a set of factual ques-
tions. Such data provides strong signal for learn-
ing about lexical variation, but there are a number
Who wrote the Winnie the Pooh books?
Who is the author of winnie the pooh?
What was the name of the authur of winnie the pooh?
Who wrote the series of books for Winnie the poo?
Who wrote the children?s storybook ?Winnie the Pooh??
Who is poohs creator?
What relieves a hangover?
What is the best cure for a hangover?
The best way to recover from a hangover?
Best remedy for a hangover?
What takes away a hangover?
How do you lose a hangover?
What helps hangover symptoms?
What are social networking sites used for?
Why do people use social networking sites worldwide?
Advantages of using social network sites?
Why do people use social networks a lot?
Why do people communicate on social networking sites?
What are the pros and cons of social networking sites?
How do you say Santa Claus in Sweden?
Say santa clause in sweden?
How do you say santa clause in swedish?
How do they say santa in Sweden?
In Sweden what is santa called?
Who is sweden santa?
Table 1: Examples of paraphrase clusters from the
WikiAnswers corpus. Within each cluster, there is
a wide range of syntactic and lexical variations.
of challenges. Given that the data is community-
authored, it will inevitably be incomplete, contain
incorrectly tagged paraphrases, non-factual ques-
tions, and other sources of noise.
Our core contribution is a new learning ap-
proach that scalably sifts through this para-
phrase noise, learning to answer a broad class
of factual questions. We focus on answer-
ing open-domain questions that can be answered
with single-relation queries, e.g. all of the para-
phrases of ?Who wrote Winnie the Pooh?? and
?What cures a hangover?? in Table 1. The
algorithm answers such questions by mapping
them to executable queries over a tuple store
containing relations such as authored(milne,
winnie-the-pooh) and treat(bloody-mary,
hangover-symptoms).
1608
The approach automatically induces lexical
structures, which are combined to build queries for
unseen questions. It learns lexical equivalences for
relations (e.g., wrote, authored, and creator), en-
tities (e.g., Winnie the Pooh or Pooh Bear), and
question templates (e.g., Who r the e books? and
Who is the r of e?). Crucially, the approach
does not require any explicit labeling of the ques-
tions in our paraphrase corpus. Instead, we use
16 seed question templates and string-matching to
find high-quality queries for a small subset of the
questions. The algorithm uses learned word align-
ments to aggressively generalize the seeds, pro-
ducing a large set of possible lexical equivalences.
We then learn a linear ranking model to filter the
learned lexical equivalences, keeping only those
that are likely to answer questions well in practice.
Experimental results on 18 million paraphrase
pairs gathered from WikiAnswers1 demonstrate
the effectiveness of the overall approach. We
performed an end-to-end evaluation against a
database of 15 million facts automatically ex-
tracted from general web text (Fader et al, 2011).
On known-answerable questions, the approach
achieved 42% recall, with 77% precision, more
than quadrupling the recall over a baseline system.
In sum, we make the following contributions:
? We introduce PARALEX, an end-to-end open-
domain question answering system.
? We describe scalable learning algorithms that
induce general question templates and lexical
variants of entities and relations. These algo-
rithms require no manual annotation and can
be applied to large, noisy databases of rela-
tional triples.
? We evaluate PARALEX on the end-task of an-
swering questions from WikiAnswers using a
database of web extractions, and show that it
outperforms baseline systems.
? We release our learned lexicon and
question-paraphrase dataset to the
research community, available at
http://openie.cs.washington.edu.
2 Related Work
Our work builds upon two major threads of re-
search in natural language processing: informa-
tion extraction (IE), and natural language inter-
faces to databases (NLIDB).
1http://wiki.answers.com/
Research in IE has been moving towards the
goal of extracting facts from large text corpora,
across many domains, with minimal supervision
(Mintz et al, 2009; Hoffmann et al, 2010; Riedel
et al, 2010; Hoffmann et al, 2011; Banko et al,
2007; Yao et al, 2012). While much progress
has been made in converting text into structured
knowledge, there has been little work on an-
swering natural language questions over these
databases. There has been some work on QA over
web text (Kwok et al, 2001; Brill et al, 2002), but
these systems do not operate over extracted rela-
tional data.
The NLIDB problem has been studied for
decades (Grosz et al, 1987; Katz, 1997). More
recently, researchers have created systems that
use machine learning techniques to automatically
construct question answering systems from data
(Zelle and Mooney, 1996; Popescu et al, 2004;
Zettlemoyer and Collins, 2005; Clarke et al, 2010;
Liang et al, 2011). These systems have the abil-
ity to handle questions with complex semantics
on small domain-specific databases like GeoQuery
(Tang and Mooney, 2001) or subsets of Freebase
(Cai and Yates, 2013), but have yet to scale to the
task of general, open-domain question answering.
In contrast, our system answers questions with
more limited semantics, but does so at a very large
scale in an open-domain manner. Some work has
been made towards more general databases like
DBpedia (Yahya et al, 2012; Unger et al, 2012),
but these systems rely on hand-written templates
for question interpretation.
The learning algorithms presented in this pa-
per are similar to algorithms used for paraphrase
extraction from sentence-aligned corpora (Barzi-
lay and McKeown, 2001; Barzilay and Lee, 2003;
Quirk et al, 2004; Bannard and Callison-Burch,
2005; Callison-Burch, 2008; Marton et al, 2009).
However, we use a paraphrase corpus for extract-
ing lexical items relating natural language patterns
to database concepts, as opposed to relationships
between pairs of natural language utterances.
3 Overview of the Approach
In this section, we give a high-level overview of
the rest of the paper.
Problem Our goal is to learn a function that will
map a natural language question x to a query z
over a database D. The database D is a collection
of assertions in the form r(e1, e2) where r is a bi-
1609
nary relation from a vocabulary R, and e1 and e2
are entities from a vocabulary E. We assume that
the elements of R and E are human-interpretable
strings like population or new-york. In our
experiments, R and E contain millions of en-
tries representing ambiguous and overlapping con-
cepts. The database is equipped with a simple in-
terface that accepts queries in the form r(?, e2) or
r(e1, ?). When executed, these queries return all
entities e that satisfy the given relationship. Thus,
our task is to find the query z that best captures the
semantics of the question x.
Model The question answering model includes a
lexicon and a linear ranking function. The lexicon
L associates natural language patterns to database
concepts, thereby defining the space of queries
that can be derived from the input question (see
Table 2). Lexical entries can pair strings with
database entities (nyc and new-york), strings with
database relations (big and population), or ques-
tion patterns with templated database queries (how
r is e? and r(?,e)). We describe this model in
more detail in Section 4.
Learning The learning algorithm induces a lex-
icon L and estimates the parameters ? of the
linear ranking function. We learn L by boot-
strapping from an initial seed lexicon L0 over a
corpus of question paraphrases C = {(x, x?) :
x? is a paraphrase of x}, like the examples in Ta-
ble 1. We estimate ? by using the initial lexicon to
automatically label queries in the paraphrase cor-
pus, as described in Section 5.2. The final result
is a scalable learning algorithm that requires no
manual annotation of questions.
Evaluation In Section 8, we evaluate our system
against various baselines on the end-task of ques-
tion answering against a large database of facts
extracted from the web. We use held-out known-
answerable questions from WikiAnswers as a test
set.
4 Question Answering Model
To answer questions, we must find the best query
for a given natural language question.
4.1 Lexicon and Derivations
To define the space of possible queries, PARALEX
uses a lexicon L that encodes mappings from nat-
ural language to database concepts (entities, rela-
tions, and queries). Each entry in L is a pair (p, d)
Entry Type NL Pattern DB Concept
Entity nyc new-york
Relation big population
Question (1-Arg.) how big is e population(?, e)
Question (2-Arg.) how r is e r(?, e)
Table 2: Example lexical entries.
where p is a pattern and d is an associated database
concept. Table 2 gives examples of the entry types
in L: entity, relation, and question patterns.
Entity patterns match a contiguous string of
words and are associated with some database en-
tity e ? E.
Relation patterns match a contiguous string of
words and are associated with a relation r ? R and
an argument ordering (e.g. the string child could
be modeled as either parent-of or child-of with
opposite argument ordering).
Question patterns match an entire question
string, with gaps that recursively match an en-
tity or relation patterns. Question patterns are as-
sociated with a templated database query, where
the values of the variables are determined by the
matched entity and relation patterns. A question
pattern may be 1-Argument, with a variable for
an entity pattern, or 2-Argument, with variables
for an entity pattern and a relation pattern. A 2-
argument question pattern may also invert the ar-
gument order of the matched relation pattern, e.g.
who r e? may have the opposite argument order
of who did e r?
The lexicon is used to generate a derivation y
from an input question x to a database query z.
For example, the entries in Table 2 can be used
to make the following derivation from the ques-
tion How big is nyc? to the query population(?,
new-york):
This derivation proceeds in two steps: first match-
ing a question form like How r is e? and then
mapping big to population and nyc to new-york.
Factoring the derivation this way allows the lexi-
cal entries for big and nyc to be reused in semanti-
1610
cally equivalent variants like nyc how big is it? or
approximately how big is nyc? This factorization
helps the system generalize to novel questions that
do not appear in the training set.
We model a derivation as a set of (pi, di) pairs,
where each pi matches a substring of x, the sub-
strings cover all words in x, and the database con-
cepts di compose to form z. Derivations are rooted
at either a 1-argument or 2-argument question en-
try and have entity or relation entries as leaves.
4.2 Linear Ranking Function
In general, multiple queries may be derived from a
single input question x using a lexicon L. Many of
these derivations may be incorrect due to noise in
L. Given a question x, we consider all derivations
y and score them with ? ??(x, y), where ?(x, y) is
a n-dimensional feature representation and ? is a
n-dimensional parameter vector. Let GEN(x;L)
be the set of all derivations y that can be generated
from x using L. The best derivation y?(x) accord-
ing to the model (?, L) is given by:
y?(x) = argmax
y?GEN(x;L)
? ? ?(x, y)
The best query z?(x) can be computed directly
from the derivation y?(x).
Computing the set GEN(x;L) involves finding
all 1-Argument and 2-Argument question patterns
that match x, and then enumerating all possible
database concepts that match entity and relation
strings. When the database and lexicon are large,
this becomes intractable. We prune GEN(x;L)
using the model parameters ? by only considering
the N -best question patterns that match x, before
additionally enumerating any relations or entities.
For the end-to-end QA task, we return a ranked
list of answers from the k highest scoring queries.
We score an answer a with the highest score of all
derivations that generate a query with answer a.
5 Learning
PARALEX uses a two-part learning algorithm; it
first induces an overly general lexicon (Section
5.1) and then learns to score derivations to increase
accuracy (Section 5.2). Both algorithms rely on an
initial seed lexicon, which we describe in Section
7.4.
5.1 Lexical Learning
The lexical learning algorithm constructs a lexi-
con L from a corpus of question paraphrases C =
{(x, x?) : x? is a paraphrase of x}, where we as-
sume that all paraphrased questions (x, x?) can be
answered with a single, initially unknown, query
(Table 1 shows example paraphrases). This as-
sumption allows the algorithm to generalize from
the initial seed lexicon L0, greatly increasing the
lexical coverage.
As an example, consider the paraphrase pair x
= What is the population of New York? and x? =
How big is NYC? Suppose x can be mapped to a
query under L0 using the following derivation y:
what is the r of e = r(?, e)
population = population
new york = new-york
We can induce new lexical items by aligning the
patterns used in y to substrings in x?. For example,
suppose we know that the words in (x, x?) align in
the following way:
Using this information, we can hypothesize that
how r is e, big, and nyc should have the same in-
terpretations as what is the r of e, population, and
new york, respectively, and create the new entries:
how r is e = r(?, e)
big = population
nyc = new-york
We call this procedure InduceLex(x, x?, y, A),
which takes a paraphrase pair (x, x?), a derivation
y of x, and a word alignment A, and returns a new
set of lexical entries. Before formally describing
InduceLex we need to introduce some definitions.
Let n and n? be the number of words in x and
x?. Let [k] denote the set of integers {1, . . . , k}.
A word alignment A between x and x? is a subset
of [n] ? [n?]. A phrase alignment is a pair of in-
dex sets (I, I ?) where I ? [n] and I ? ? [n?]. A
phrase alignment (I, I ?) is consistent with a word
alignment A if for all (i, i?) ? A, i ? I if and only
if i? ? I ?. In other words, a phrase alignment is
consistent with a word alignment if the words in
the phrases are aligned only with each other, and
not with any outside words.
We will now define InduceLex(x, x?, y, A) for
the case where the derivation y consists of a 2-
argument question entry (pq, dq), a relation entry
1611
function LEARNLEXICON
Inputs:
- A corpus C of paraphrases (x, x?). (Table 1)
- An initial lexicon L0 of (pattern, concept) pairs.
- A word alignment function WordAlign(x, x?).
(Section 6)
- Initial parameters ?0.
- A function GEN(x;L) that derives queries from
a question x using lexicon L. (Section 4)
- A function InduceLex(x, x?, y, A) that induces
new lexical items from the paraphrases (x, x?) us-
ing their word alignment A and a derivation y of
x. (Section 5.1)
Output: A learned lexicon L.
L = {}
for all x, x? ? C do
if GEN(x;L0) is not empty then
A?WordAlign(x, x?)
y? ? argmaxy?GEN(x;L0) ?0 ? ?(x, y)
L? L ? InduceLex(x, x?, y?, A)
return L
Figure 1: Our lexicon learning algorithm.
(pr, dr), and an entity entry (pe, de), as shown in
the example above.2 InduceLex returns the set of
all triples (p?q, dq), (p?r, dr), (p?e, de) such that for
all p?q, p?r, p?e such that
1. p?q, p?r, p?e are a partition of the words in x?.
2. The phrase pairs (pq, p?q), (pr, p?r), (pe, p?e)
are consistent with the word alignment A.
3. The p?r and p?e are contiguous spans of words
in x?.
Figure 1 shows the complete lexical learning al-
gorithm. In practice, for a given paraphrase pair
(x, x?) and alignment A, InduceLex will gener-
ate multiple sets of new lexical entries, resulting
in a lexicon with millions of entries. We use an
existing statistical word alignment algorithm for
WordAlign (see Section 6). In the next section,
we will introduce a scalable approach for learning
to score derivations to filter out lexical items that
generalize poorly.
5.2 Parameter Learning
Parameter learning is necessary for filtering out
derivations that use incorrect lexical entries like
new mexico = mexico, which arise from noise in
the paraphrases and noise in the word alignment.
2InduceLex has similar behavior for the other type of
derivation, which consists of a 1-argument question entry
(pq, dq) and an entity (pe, de).
We use the hidden variable structured perceptron
algorithm to learn ? from a list of (question x,
query z) training examples. We adopt the itera-
tive parameter mixing variation of the perceptron
(McDonald et al, 2010) to scale to a large number
of training examples.
Figure 2 shows the parameter learning algo-
rithm. The parameter learning algorithm operates
in two stages. First, we use the initial lexicon
L0 to automatically generate (question x, query z)
training examples from the paraphrase corpus C.
Then we feed the training examples into the learn-
ing algorithm, which estimates parameters for the
learned lexicon L.
Because the number of training examples is
large, we adopt a parallel perceptron approach.
We first randomly partition the training data T
into K equally-sized subsets T1, . . . , TK . We then
perform perceptron learning on each partition in
parallel. Finally, the learned weights from each
parallel run are aggregated by taking a uniformly
weighted average of each partition?s parameter
vector. This procedure is repeated for T iterations.
The training data consists of (question x, query
z) pairs, but our scoring model is over (question
x, derivation y) pairs, which are unobserved in
the training data. We use a hidden variable ver-
sion of the perceptron algorithm (Collins, 2002),
where the model parameters are updated using the
highest scoring derivation y? that will generate the
correct query z using the learned lexicon L.
6 Data
For our database D, we use the publicly avail-
able set of 15 million REVERB extractions (Fader
et al, 2011).3 The database consists of a set
of triples r(e1, e2) over a vocabulary of ap-
proximately 600K relations and 2M entities, ex-
tracted from the ClueWeb09 corpus.4 The RE-
VERB database contains a large cross-section of
general world-knowledge, and thus is a good
testbed for developing an open-domain QA sys-
tem. However, the extractions are noisy, unnor-
malized (e.g., the strings obama, barack-obama,
and president-obama all appear as distinct en-
tities), and ambiguous (e.g., the relation born-in
contains facts about both dates and locations).
3We used version 1.1, downloaded from http://
reverb.cs.washington.edu/.
4The full set of REVERB extractions from ClueWeb09
contains over six billion triples. We used the smaller subset
of triples to simplify our experiments.
1612
function LEARNPARAMETERS
Inputs:
- A corpus C of paraphrases (x, x?). (Table 1)
- An initial lexicon L0 of (pattern, db concept)
pairs.
- A learned lexiconL of (pattern, db concept) pairs.
- Initial parameters ?0.
- Number of perceptron epochs T .
- Number of training-data shards K.
- A function GEN(x;L) that derives queries from
a question x using lexicon L. (Section 4)
- A function PerceptronEpoch(T , ?, L) that runs
a single epoch of the hidden-variable structured
perceptron algorithm on training set T with initial
parameters ?, returning a new parameter vector
??. (Section 5.2)
Output: A learned parameter vector ?.
// Step 1: Generate Training Examples T
T = {}
for all x, x? ? C do
if GEN(x;L0) is not empty then
y? ? argmaxy?GEN(x;L0) ?0 ? ?(x, y)
z? ? query of y?
Add (x?, z?) to T
// Step 2: Learn Parameters from T
Randomly partition T into shards T1, . . . , TK
for t = 1 . . . T do
// Executed on k processors
?k,t = PerceptronEpoch(Tk, ?t?1, L)
// Average the weights
?t = 1K
?
k ?k,t
return ?T
Figure 2: Our parameter learning algorithm.
Our paraphrase corpus C was constructed from
the collaboratively edited QA site WikiAnswers.
WikiAnswers users can tag pairs of questions as
alternate wordings of each other. We harvested
a set of 18M of these question-paraphrase pairs,
with 2.4M distinct questions in the corpus.
To estimate the precision of the paraphrase cor-
pus, we randomly sampled a set of 100 pairs and
manually tagged them as ?paraphrase? or ?not-
paraphrase.? We found that 55% of the sampled
pairs are valid paraphrased. Most of the incorrect
paraphrases were questions that were related, but
not paraphrased e.g. How big is the biggest mall?
and Most expensive mall in the world?
We word-aligned each paraphrase pair using
the MGIZA++ implementation of IBM Model 4
(Och and Ney, 2000; Gao and Vogel, 2008). The
word-alignment algorithm was run in each direc-
tion (x, x?) and (x?, x) and then combined using
the grow-diag-final-and heuristic (Koehn et al,
2003).
7 Experimental Setup
We compare the following systems:
? PARALEX: the full system, using the lexical
learning and parameter learning algorithms
from Section 5.
? NoParam: PARALEX without the learned
parameters.
? InitOnly: PARALEX using only the initial
seed lexicon.
We evaluate the systems? performance on the end-
task of QA on WikiAnswers questions.
7.1 Test Set
A major challenge for evaluation is that the RE-
VERB database is incomplete. A system may cor-
rectly map a test question to a valid query, only
to return 0 results when executed against the in-
complete database. We factor out this source of
error by semi-automatically constructing a sample
of questions that are known to be answerable us-
ing the REVERB database, and thus allows for a
meaningful comparison on the task of question un-
derstanding.
To create the evaluation set, we identified ques-
tions x in a held out portion of the WikiAnswers
corpus such that (1) x can be mapped to some
query z using an initial lexicon (described in Sec-
tion 7.4), and (2) when z is executed against the
database, it returns at least one answer. We then
add x and all of its paraphrases as our evaluation
set. For example, the question What is the lan-
guage of Hong-Kong satisfies these requirements,
so we added these questions to the evaluation set:
What is the language of Hong-Kong?
What language do people in hong kong use?
How many languages are spoken in hong kong?
How many languages hong kong people use?
In Hong Kong what language is spoken?
Language of Hong-kong?
This methodology allows us to evaluate the sys-
tems? ability to handle syntactic and lexical varia-
tions of questions that should have the same an-
swers. We created 37 question clusters, result-
ing in a total of 698 questions. We removed all
of these questions and their paraphrases from the
training set. We also manually filtered out any in-
correct paraphrases that appeared in the test clus-
ters.
We then created a gold-standard set of (x, a, l)
triples, where x is a question, a is an answer, and l
1613
Question Pattern Database Query
who r e r(?, e)
what r e r(?, e)
who does e r r(e, ?)
what does e r r(e, ?)
what is the r of e r(?, e)
who is the r of e r(?, e)
what is r by e r(e, ?)
who is e?s r r(?, e)
what is e?s r r(?, e)
who is r by e r(e, ?)
when did e r r-in(e, ?)
when did e r r-on(e, ?)
when was e r r-in(e, ?)
when was e r r-on(e, ?)
where was e r r-in(e, ?)
where did e r r-in(e, ?)
Table 3: The question patterns used in the initial
lexicon L0.
is a label (correct or incorrect). To create the gold-
standard, we first ran each system on the evalua-
tion questions to generate (x, a) pairs. Then we
manually tagged each pair with a label l. This
resulted in a set of approximately 2, 000 human
judgments. If (x, a) was tagged with label l and x?
is a paraphrase of x, we automatically added the
labeling (x?, a, l), since questions in the same clus-
ter should have the same answer sets. This process
resulted in a gold standard set of approximately
48, 000 (x, a, l) triples.
7.2 Metrics
We use two types of metrics to score the systems.
The first metric measures the precision and recall
of each system?s highest ranked answer. Precision
is the fraction of predicted answers that are cor-
rect and recall is the fraction of questions where a
correct answer was predicted. The second metric
measures the accuracy of the entire ranked answer
set returned for a question. We compute the mean
average precision (MAP) of each systems? output,
which measures the average precision over all lev-
els of recall.
7.3 Features and Settings
The feature representation ?(x, y) consists of in-
dicator functions for each lexical entry (p, d) ? L
used in the derivation y. For parameter learning,
we use an initial weight vector ?0 = 0, use T = 20
F1 Precision Recall MAP
PARALEX 0.54 0.77 0.42 0.22
NoParam 0.30 0.53 0.20 0.08
InitOnly 0.18 0.84 0.10 0.04
Table 4: Performance on WikiAnswers questions
known to be answerable using REVERB.
F1 Precision Recall MAP
PARALEX 0.54 0.77 0.42 0.22
No 2-Arg. 0.40 0.86 0.26 0.12
No 1-Arg 0.35 0.81 0.22 0.11
No Relations 0.18 0.84 0.10 0.03
No Entity 0.36 0.55 0.27 0.15
Table 5: Ablation of the learned lexical items.
0.0 0.1 0.2 0.3 0.4 0.5
Recall
0.5
0.6
0.7
0.8
0.9
1.0
Pr
eci
sio
n
PARALEX
No 2-Arg.
Initial Lexicon
Figure 3: Precision-recall curves for PARALEX
with and without 2-argument question patterns.
iterations and shard the training data into K = 10
pieces. We limit each system to return the top 100
database queries for each test sentence. All input
words are lowercased and lemmatized.
7.4 Initial Lexicon
Both the lexical learning and parameter learning
algorithms rely on an initial seed lexicon L0. The
initial lexicon allows the learning algorithms to
bootstrap from the paraphrase corpus.
We construct L0 from a set of 16 hand-written
2-argument question patterns and the output of the
identity transformation on the entity and relation
strings in the database. Table 3 shows the question
patterns that were used in L0.
8 Results
Table 4 shows the performance of PARALEX on
the test questions. PARALEX outperforms the
baseline systems in terms of both F1 and MAP.
The lexicon-learning algorithm boosts the recall
by a factor of 4 over the initial lexicon, show-
ing the utility of the InduceLex algorithm. The
1614
String Learned Database Relations for String
get rid of treatment-for, cause, get-rid-of, cure-for, easiest-way-to-get-rid-of
word word-for, slang-term-for, definition-of, meaning-of, synonym-of
speak speak-language-in, language-speak-in, principal-language-of, dialect-of
useful main-use-of, purpose-of, importance-of, property-of, usefulness-of
String Learned Database Entities for String
smoking smoking, tobacco-smoking, cigarette, smoking-cigar, smoke, quit-smoking
radiation radiation, electromagnetic-radiation, nuclear-radiation
vancouver vancouver, vancouver-city, vancouver-island, vancouver-british-columbia
protein protein, protein-synthesis, plasma-protein, monomer, dna
Table 6: Examples of relation and entity synonyms learned from the WikiAnswers paraphrase corpus.
parameter-learning algorithm also results in a
large gain in both precision and recall: InduceLex
generates a noisy set of patterns, so selecting the
best query for a question is more challenging.
Table 5 shows an ablation of the different types
of lexical items learned by PARALEX. For each
row, we removed the learned lexical items from
each of the types described in Section 4, keeping
only the initial seed lexical items. The learned 2-
argument question templates significantly increase
the recall of the system. This increased recall
came at a cost, lowering precision from 0.86 to
0.77. Thresholding the query score allows us to
trade precision for recall, as shown in Figure 3.
Table 6 shows some examples of the learned en-
tity and relation synonyms.
The 2-argument question templates help PAR-
ALEX generalize over different variations of the
same question, like the test questions shown in
Table 7. For each question, PARALEX combines
a 2-argument question template (shown below the
questions) with the rules celebrate = holiday-of
and christians = christians to derive a full
query. Factoring the problem this way allows
PARALEX to reuse the same rules in different
syntactic configurations. Note that the imperfect
training data can lead to overly-specific templates
like what are the religious r of e, which can lower
accuracy.
9 Error Analysis
To understand how close we are to the goal of
open-domain QA, we ran PARALEX on an unre-
stricted sample of questions from WikiAnswers.
We used the same methodology as described in the
previous section, where PARALEX returns the top
answer for each question using REVERB.
We found that PARALEX performs significantly
worse on this dataset, with recall maxing out at ap-
Celebrations for Christians?
r for e?
Celebrations of Christians?
r of e?
What are some celebrations for Christians?
what are some r for e?
What are some celebrations of the Christians?
what are some r of e?
What are some of Christians celebrations?
what are some of e r?
What celebrations do Christians do?
what r do e do?
What did Christians celebrate?
what did e r?
What are the religious celebrations of Christians?
what are the religious r of e?
What celebration do Christians celebrate?
what r do e celebrate?
Table 7: Questions from the test set with 2-
argument question patterns that PARALEX used to
derive a correct query.
proximately 6% of the questions answered at pre-
cision 0.4. This is not surprising, since the test
questions are not restricted to topics covered by
the REVERB database, and may be too complex to
be answered by any database of relational triples.
We performed an error analysis on a sample
of 100 questions that were either incorrectly an-
swered or unanswered. We examined the can-
didate queries that PARALEX generated for each
question and tagged each query as correct (would
return a valid answer given a correct and com-
plete database) or incorrect. Because the input
questions are unrestricted, we also judged whether
the questions could be faithfully represented as a
r(?, e) or r(e, ?) query over the database vocabu-
lary. Table 8 shows the distribution of errors.
The largest source of error (36%) were on com-
1615
plex questions that could not be represented as a
query for various reasons. We categorized these
questions into groups. The largest group (14%)
were questions that need n-ary or higher-order
database relations, for example How long does
it take to drive from Sacramento to Cancun? or
What do cats and dogs have in common? Approx-
imately 13% of the questions were how-to ques-
tions like How do you make axes in minecraft?
whose answers are a sequence of steps, instead
of a database entity. Lastly, 9% of the questions
require database operators like joins, for example
When were Bobby Orr?s children born?
The second largest source of error (32%) were
questions that could be represented as a query, but
where PARALEX was unable to derive any cor-
rect queries. For example, the question Things
grown on Nigerian farms? was not mapped to
any queries, even though the REVERB database
contains the relation grown-in and the entity
nigeria. We found that 13% of the incorrect
questions were cases where the entity was not rec-
ognized, 12% were cases where the relation was
not recognized, and 6% were cases where both the
entity and relation were not recognized.
We found that 28% of the errors were cases
where PARALEX derived a query that we judged to
be correct, but returned no answers when executed
against the database. For example, given the ques-
tion How much can a dietician earn? PARALEX
derived the query salary-of(?, dietician) but
this returned no answers in the REVERB database.
Finally, approximately 4% of the questions in-
cluded typos or were judged to be inscrutable, for
example Barovier hiriacy of evidence based for
pressure sore?
Discussion Our experiments show that the learn-
ing algorithms described in Section 5 allow PAR-
ALEX to generalize beyond an initial lexicon and
answer questions with significantly higher accu-
racy. Our error analysis on an unrestricted set of
WikiAnswers questions shows that PARALEX is
still far from the goal of truly high-recall, open-
domain QA. We found that many questions asked
on WikiAnswers are either too complex to be
mapped to a simple relational query, or are not
covered by the REVERB database. Further, ap-
proximately one third of the missing recall is due
to entity and relation recognition errors.
Incorrectly Answered/Unanswered Questions
36% Complex Questions
Need n-ary or higher-order relations (14%)
Answer is a set of instructions (13%)
Need database operators e.g. joins (9%)
32% Entity or Relation Recognition Errors
Entity recognition errors (13%)
Relation recognition errors (12%)
Entity & relation recognition errors (7%)
28% Incomplete Database
Derived a correct query, but no answers
4% Typos/Inscrutable Questions
Table 8: Error distribution of PARALEX on an un-
restricted sample of questions from the WikiAn-
swers dataset.
10 Conclusion
We introduced a new learning approach that in-
duces a complete question-answering system from
a large corpus of noisy question-paraphrases. Us-
ing only a seed lexicon, the approach automat-
ically learns a lexicon and linear ranking func-
tion that demonstrated high accuracy on a held-out
evaluation set.
A number of open challenges remain. First,
precision could likely be improved by adding
new features to the ranking function. Second,
we would like to generalize the question under-
standing framework to produce more complex
queries, constructed within a compositional se-
mantic framework, but without sacrificing scala-
bility. Third, we would also like to extend the
system with other large databases like Freebase or
DBpedia. Lastly, we believe that it would be pos-
sible to leverage the user-provided answers from
WikiAnswers as a source of supervision.
Acknowledgments
This research was supported in part by ONR grant
N00014-11-1-0294, DARPA contract FA8750-09-
C-0179, a gift from Google, a gift from Vulcan
Inc., and carried out at the University of Washing-
ton?s Turing Center. We would like to thank Yoav
Artzi, Tom Kwiatkowski, Yuval Marton, Mausam,
Dan Weld, and the anonymous reviewers for their
helpful comments.
1616
References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open Information Extraction from the Web. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting Paraphrases from a Parallel Corpus. In
Proceedings of the 39th Annual Meeting on Associ-
ation for Computational Linguistics.
Eric Brill, Susan Dumais, and Michele Banko. 2002.
An Analysis of the AskMSR Question-Answering
System. In Proceedings of Empirical Methods in
Natural Language Processing.
Qingqing Cai and Alexander Yates. 2013. Large-scale
Semantic Parsing via Schema Matching and Lexicon
Extension. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving Semantic Parsing from
the World?s Response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing.
Qin Gao and Stephan Vogel. 2008. Parallel Imple-
mentations of Word Alignment Tool. In Proc. of the
ACL 2008 Software Engineering, Testing, and Qual-
ity Assurance Workshop.
Barbara J. Grosz, Douglas E. Appelt, Paul A. Mar-
tin, and Fernando C. N. Pereira. 1987. TEAM:
An Experiment in the Design of Transportable
Natural-Language Interfaces. Artificial Intelligence,
32(2):173?243.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-Based Weak Supervision for Informa-
tion Extraction of Overlapping Relations. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics.
Boris Katz. 1997. Annotating the World Wide Web
using Natural Language. In RIAO, pages 136?159.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001.
Scaling Question Answering to the Web. ACM
Trans. Inf. Syst., 19(3):242?262.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning Dependency-Based Compositional Se-
mantics. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved Statistical Machine Trans-
lation Using Monolingually-Derived Paraphrases.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant Supervision for Relation Ex-
traction Without Labeled Data. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,
David Ko, and Alexander Yates. 2004. Modern
Natural Language Interfaces to Databases: Compos-
ing Statistical Parsing with Semantic Tractability. In
Proceedings of the Twentieth International Confer-
ence on Computational Linguistics.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Pro-
cessing.
1617
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling Relations and Their Mentions with-
out Labeled Text. In Proceedings of the 2010 Euro-
pean conference on Machine learning and Knowl-
edge Discovery in Databases.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing Multiple Clause Constructors in Inductive Logic
Programming for Semantic Parsing.
Christina Unger, Lorenz Bu?hmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-Based Question
Answering over RDF Data. In Proceedings of the
21st World Wide Web Conference 2012.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural Language Questions for
the Web of Data. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised Relation Discovery with Sense
Disambiguation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to Parse Database Queries Using Inductive Logic
Programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to Map Sentences to Logical Form: Struc-
tured Classification with Probabilistic Categorial
Grammars. In Proceedings of the 21st Conference
in Uncertainty in Artificial Intelligence.
1618
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1669?1679,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Lightly Supervised Learning of Procedural Dialog Systems
Svitlana Volkova
CLSP
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Pallavi Choudhury, Chris Quirk, Bill Dolan
NLP Group
Microsoft Research
Redmond, WA
pallavic,chrisq,
billdol@microsoft.com
Luke Zettlemoyer
Computer Science and Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
Procedural dialog systems can help users
achieve a wide range of goals. However,
such systems are challenging to build,
currently requiring manual engineering of
substantial domain-specific task knowl-
edge and dialog management strategies. In
this paper, we demonstrate that it is pos-
sible to learn procedural dialog systems
given only light supervision, of the type
that can be provided by non-experts. We
consider domains where the required task
knowledge exists in textual form (e.g., in-
structional web pages) and where system
builders have access to statements of user
intent (e.g., search query logs or dialog
interactions). To learn from such tex-
tual resources, we describe a novel ap-
proach that first automatically extracts task
knowledge from instructions, then learns a
dialog manager over this task knowledge
to provide assistance. Evaluation in a Mi-
crosoft Office domain shows that the indi-
vidual components are highly accurate and
can be integrated into a dialog system that
provides effective help to users.
1 Introduction
Procedural dialog systems aim to assist users
with a wide range of goals. For example, they
can guide visitors through a museum (Traum et
al., 2012; Aggarwal et al, 2012), teach students
physics (Steinhauser et al, 2011; Dzikovska et
al., 2011), or enable interaction with a health care
U: ?I want to add page numbers and a title?
S: ?Top or Bottom of the page??
U: ?Top?
S: ?Please select page design from the tem-
plates? (*System shows drop down menu*)
U: *User selects from menu*
S: ?Enter header or footer content?
U: ?C.V.?
S: ?Task completed.?
Figure 1: An example dialog interaction between
a system (S) and user (U) that can be automatically
achieved by learning from instructional web page
and query click logs.
system (Morbini et al, 2012; Rizzo et al, 2011).
However, such systems are challenging to build,
currently requiring expensive, expert engineering
of significant domain-specific task knowledge and
dialog management strategies.
In this paper, we present a new approach for
learning procedural dialog systems from task-
oriented textual resources in combination with
light, non-expert supervision. Specifically, we as-
sume access to task knowledge in textual form
(e.g., instructional web pages) and examples of
user intent statements (e.g., search query logs or
dialog interactions). Such instructional resources
are available in many domains, ranging from
recipes that describe how to cook meals to soft-
ware help web pages that describe how to achieve
goals by interacting with a user interface.1
1ehow.com,wikianswers.com
1669
There are two key challenges: we must (1)
learn to convert the textual knowledge into a us-
able form and (2) learn a dialog manager that pro-
vides robust assistance given such knowledge. For
example, Figure 1 shows the type of task assis-
tance that we are targeting in the Microsoft Office
setting, where the system should learn from web
pages and search query logs. Our central contribu-
tion is to show that such systems can be built with-
out the help of knowledge engineers or domain ex-
perts. We present new approaches for both of our
core problems. First, we introduce a method for
learning to map instructions to tree representations
of the procedures they describe. Nodes in the tree
represent points of interaction with the questions
the system can ask the user, while edges represent
user responses. Next, we present an approach that
uses example user intent statements to simulate di-
alog interactions, and learns how to best map user
utterances to nodes in these induced dialog trees.
When combined, these approaches produce a com-
plete dialog system that can engage in conversa-
tions by automatically moving between the nodes
of a large collection of induced dialog trees.
Experiments in the Windows Office help do-
main demonstrate that it is possible to build an
effective end-to-end dialog system. We evaluate
the dialog tree construction and dialog manage-
ment components in isolation, demonstrating high
accuracy (in the 80-90% range). We also conduct
a small-scale user study which demonstrates that
users can interact productively with the system,
successfully completing over 80% of their tasks.
Even when the system does fail, it often does so in
a graceful way, for example by asking redundant
questions but still reaching the goal within a few
additional turns.
2 Overview of Approach
Our task-oriented dialog system understands user
utterances by mapping them to nodes in dialog
trees generated from instructional text. Figure 2
shows an example of a set of instructions and the
corresponding dialog tree. This section describes
the problems that we must solve to enable such in-
teractions, and outlines our approach for each.
Knowledge Acquisition We extract task knowl-
edge from instructional text (e.g., Figure 2, left)
that describes (1) actions to be performed, such
as clicking a button, and (2) places where input
is needed from the user, for example to enter the
contents of the footer or header they are trying to
create. We aim to convert this text into a form that
will enable a dialog system to automatically assist
with the described task. To this end, we construct
dialog trees (e.g., Figure 2, right) with nodes to
represent entire documents (labeled as topics t),
nodes to represent user goals or intents (g), and
system action nodes (a) that enable execution of
specific commands. Finally, each node has an as-
sociated system action as, which can prompt user
input (e.g., with the question ?Top or bottom of
the page??) and one or more user actions au that
represent possible responses. All nodes connect
to form a tree structure that follows the workflow
described in the document. Section 3 presents a
scalable approach for inducing dialog trees.
Dialog Management To understand user intent
and provide task assistance, we need a dialog man-
agement approach that specifies what the system
should do and say. We adopt a simple approach
that at all times maintains an index into a node in
a dialog tree. Each system utterance is then simply
the action as for that node. However, the key chal-
lenge comes in interpreting user utterances. After
each user statement, we must automatically up-
date our node index. At any point, the user can
state a general goal (e.g., ?I want to add page num-
bers?), refine their goal (e.g., ?in a footer?), or both
(e.g.,?I want to add page numbers in the footer?).
Users can also change their goals in the process of
completing the tasks.
We develop a simple classification approach
that is robust to these different types of user behav-
ior. Specifically, we learn classifiers that, given the
dialog interaction history, predict how to pick the
next tree node from the space of all nodes in the di-
alog trees that define the task knowledge. We iso-
late two specific cases, classifying initial user ut-
terances (Section 4) and classifying all subsequent
utterances (Section 5). This approach allows us to
isolate the difference in language for the two cases,
and bias the second case to prefer tree nodes near
the current one. The resulting approach allows for
significant flexibility in traversing the dialog trees.
Data and Evaluation We collected a large set of
such naturally-occurring web search queries that
resulted in a user click on a URL in the Microsoft
Office help domain.2 We found that queries longer
that 4-5 words often resembled natural language
utterances that could be used for dialog interac-
2http://office.microsoft.com
1670
Figure 2: An example instructional text paired with a section of the corresponding dialog tree.
tions, for example how do you add borders, how
can I add a footer, how to insert continuous page
numbers, and where is the header and footer.
We also collected instructional texts from the
web pages that describe how to solve 76 of the
most pressing user goals, as indicated by query
click log statistics. On average 1,000 user queries
were associated with each goal. To some extent
clickthroughs can be treated as a proxy for user
frustration; popular search targets probably repre-
sent user pain points.
3 Building Dialog Trees from
Instructions
Our first problem is to convert sets of instructions
for user goals to dialog trees, as shown in Figure
2. These goals are broadly grouped into topics
(instructional pages). In addition, we manually
associate each node in a dialog tree with a train-
ing set of 10 queries. For the 76 goals (246 in-
structions) in our data, this annotation effort took
a single annotator a total of 41 hours. Scaling this
approach to the entire Office help domain would
require a focused annotation effort. Crucially,
though, this annotation work can be carried out by
non-specialists, and could even be crowdsourced
(Bernstein et al, 2010).
Problem Definition As input, we are given in-
structional text (p1 . . . pn), comprised of topics
(t1 . . . tn) describing:
(1) high-level user intents (e.g., t1 ? ?add and for-
mat page numbers?)
(2) goals (g1, . . . , gk) that represent more spe-
cific user intents (e.g., g1 ? ?add header or
footer content to a preformatted page number
design?, g2 ? ?place the page number in the
side margin of the page?).
Given instructional text p1 . . . pn and queries
q1 . . . qm per topic ti, our goals are as follows:
Figure 3: Relationships between user queries and
OHP with goals, instructions and dialog trees.
- for every instructional page pi extract a topic
ti and a set of goals g1 . . . gk;
- for every goal gj for a topic ti, extract a set of
instructions i1 . . . il;
- from topics, goals and instructions, construct
dialog trees f1 . . . fn (one dialog tree per
topic). Classify instructions to user interac-
tion types thereby identifying system action
nodes a1s . . . als. Transitions between these
nodes are the user actions a1u . . . alu.
Figure 2 (left) presents an example of a topic
extracted from the help page, and a set of goals
and instructions annotated with user action types.
In the next few sections of the paper, we out-
line an overall system component design demon-
strating how queries and topics are mapped to the
dialog trees in Figure 3. The figure shows many-
to-one relations between queries and topics, one-
to-many relations between topics and goals, goals
and instructions, and one-to-one relations between
topics and dialog trees.
User Action Classification We aim to classify
instructional text (i1 . . . il) for every goal gj in the
decision tree into four categories: binary, selec-
tion, input or none.
Given a single instruction i with category au,
we use a log-linear model to represent the distri-
1671
bution over the space of possible user actions. Un-
der this representation, the user action distribution
is defined as:
p(au|i, ?) =
e???(au,i)?
a?u e
???(au,i) , (1)
where ?(au, i) ? Rn is an n-dimensional fea-
ture representation and ~? is a parameter vector we
aim to learn. Features are indicator functions of
properties of the instructions and a particular class.
For smoothing we use a zero mean, unit variance
Gaussian prior (0, 1) that penalizes ~? for drifting
too far from the mean, along with the following
optimization function:
log p(Au, ?|I) = log p(Au|I, ?)? log p(?) =
=
?
au,i?(Au,I)
p(au|i, ?)?
?
i
(? ? ?i)2
2?2i
+ k
(2)
We use L-BFGS (Nocedal and Wright, 2000) as
an optimizer.
Experimental Setup As described in Section 2,
our dataset consists of 76 goals grouped into 30
topics (average 2-3 goals per topic) for a total of
246 instructions (average 3 instructions per goal).
We manually label all instructions with user ac-
tion au categories. The distribution over cate-
gories is binary=14, input=23, selection=80 and
none=129. The data is skewed towards the cat-
egories none and selection. Many instruction do
not require any user input and can be done auto-
matically, e.g., ?On the Insert tab, in the Header
and Footer group, click Page Number?. The ex-
ample instructions with corresponding user action
labels are shown in Figure 2 (left) . Finally, we di-
vide the 246 instructions into 2 sets: 80% training
and 20% test, 199 and 47 instructions respectively.
Results We apply the user action type classifi-
cation model described in the Eq.1 and Eq.2 to
classify instructions from the test set into 4 cate-
gories. In Table 1 we report classification results
for 2 baselines: a majority class and heuristic-
based approach, and 2 models with different fea-
ture types: ngrams and ngrams + stems. For a
heuristic baseline, we use simple lexical clues to
classify instructions (e.g., X or Y for binary, select
Y for selection and type X, insert Y for input). Ta-
ble 1 summarizes the results of mapping instruc-
tional text to user actions.
Features # Features Accuracy
Baseline 1: Majority ? 0.53
Baseline 2: Heuristic ? 0.64
Ngrams 10,556 0.89
Ngrams + Stems 12,196 0.89
Table 1: Instruction classification results.
Building the Dialog Trees Based on the classi-
fied user action types, we identify system actions
a1s . . . als which correspond to 3 types of user ac-
tions a1s . . . als (excluding none type) for every goal
in a topic ti. This involved associating all words
from an instruction il with a system action als. Fi-
nally, for every topic we automatically construct a
dialog tree as shown in Figure 2 (right). The dia-
log tree includes a topic t1 with goals g1 . . . g4, and
actions (user actions au and system actions as).
Definition 1. A dialog tree encodes a user-system
dialog flow about a topic ti represented as a di-
rected unweighted graph fi = (V,E) where top-
ics, goals and actions are nodes of correspond-
ing types {t1 . . . tn}, {g1 . . . gk}, {a1 . . . al} ? V .
There is a hierarchical dependency between topic,
goal and action nodes. User interactions are
represented by edges ti ? {g1 . . . gk}, a1u =
(gj , a1) . . . alu = (ak?1, ak) ? E.
For example, in the dialog tree in Figure 2 there
is a relation t1 ? g4 between the topic t1 ?add
and format page numbers? and the goal g4 ?in-
clude page of page X of Y with the page number?.
Moreover, in the dialog tree, the topic level node
has one index i ? [1..n], where n is the number
of topics. Every goal node includes information
about its parent (topic) node and has double index
i.j, where j ? [1..k]. Finally, action nodes include
information about their parent (goal) and grand-
parent (topic) nodes and have triple index i.j.z,
where z ? [1..l].
4 Understanding Initial Queries
This section presents a model for classifying ini-
tial user queries to nodes in a dialog tree, which
allows for a variety of different types of queries.
They can be under-specified, including informa-
tion about a topic only (e.g., ?add or delete page
numbers?); partially specified, including informa-
tion about a goal (e.g., ?insert page number?); or
over-specified, including information about an ac-
tion ( e.g., ?page numbering at bottom page?.)
1672
Figure 4: Mapping initial user queries to the nodes
on different depth in a dialog tree.
Problem Definition Given an initial query, the
dialog system initializes to a state s0, searches for
the deepest relevant node given a query, and maps
the query to a node on a topic ti, goal gj or action
ak level in the dialog tree fi, as shown in Figure 4.
More formally, as input, we are given automati-
cally constructed dialog trees f1 . . . fn for instruc-
tional text (help pages) annotated with topic, goal
and action nodes and associated with system ac-
tions as shown in Figure 2 (right). From the query
logs, we associate queries with each node type:
topic qt, goal qg and action qa. This is shown in
Figure 2 and 4. We join these dialog trees repre-
senting different topics into a dialog network by
introducing a global root. Within the network,
we aim to find (1) an initial dialog state s0 that
maximizes the probability of state given a query
p(s0|q, ?); and (2) the deepest relevant node v ? V
on topic ti, goal gj or action ak depth in the tree.
Initial Dialog State Model We aim to predict
the best node in a dialog tree ti, gj , al ? V based
on a user query q. A query-to-node mapping is en-
coded as an initial dialog state s0 represented by a
binary vector over all nodes in the dialog network:
s0 = [t1, g1.1, g1.2, g1.2.1 . . . , tn, gn.1, gn.1.1].
We employ a log-linear model and try to maxi-
mize initial dialog state distribution over the space
of all nodes in a dialog network:
p(s0|q, ?) =
e
?
i ?i?i(s0,q)
?
s?0 e
?
i ?i?i(s?0,q)
, (3)
Optimization follows Eq. 2.
We experimented with a variety of features.
Lexical features included query ngrams (up to 3-
grams) associated with every node in a dialog tree
with removed stopwords and stemming query un-
igrams. We also used network structural features:
Accuracy
Features Topic Goal Action
Random 0.10 0.04 0.04
TFIDF 1Best 0.81 0.21 0.45
Lexical (L) 0.92 0.66 0.63
L + 10TFIDF 0.94 0.66 0.64
L + 10TFIDF + PO 0.94 0.65 0.65
L + 10TFIDF + QO 0.95 0.72 0.69
All above + QHistO 0.96 0.73 0.71
Table 2: Initial dialog state classification results
where L stands for lexical features, 10TFIDF - 10
best tf-idf scores, PO - prompt overlap, QO - query
overlap, and QHistO - query history overlap.
tf-idf scores, query ngram overlap with the topic
and goal descriptions, as well as system action
prompts, and query ngram overlap with a history
including queries from parent nodes.
Experimental Setup For each dialog tree,
nodes corresponding to single instructions were
hand-annotated with a small set of user queries,
as described in Section 3. Approximately 60% of
all action nodes have no associated queries3 For
the 76 goals, the resulting dataset consists of 972
node-query pairs, 80% training and 20% test.
Results The initial dialog state classification
model of finding a single node given an initial
query is described in Eq. 3.
We chose two simple baselines: (1) randomly
select a node in a dialog network and (2) use a tf-
idf 1-best model.4 Stemming, stopword removal
and including top 10 tf-idf results as features led
to a 19% increase in accuracy on an action node
level over baseline (2). Adding the following fea-
tures led to an overall 26% improvement: query
overlap with a system prompt (PO), query overlap
with other node queries (QO), and query overlap
with its parent queries (QHistO) .
We present more detailed results for topic, goal
and action nodes in Table 2. For nodes deeper in
the network, the task of mapping a user query to an
action becomes more challenging. Note, however,
that the action node accuracy numbers actually un-
3There are multiple possible reasons for this: the soft-
ware user interface may already make it clear how to accom-
plish this intent, the user may not understand that the software
makes this fine-grained option available to them, or their ex-
perience with search engines may lead them to state their in-
tent in a more coarse-grained way.
4We use cosine similarity to rank all nodes in a dialog
network and select the node with the highest rank.
1673
derstate the utility of the resulting dialog system.
The reason is that even incorrect node assignments
can lead to useful system performance. As long
as a misclassification results being assigned to a
too-high node within the correct dialog tree, the
user will experience a graceful failure: they may
be forced to answer some redundant questions, but
they will still be able to accomplish the task.
5 Understanding Query Refinements
We also developed a classifier model for mapping
followup queries to the nodes in a dialog network,
while maintaining a dialog state that summarizes
the history of the current interaction.
Problem Definition Similar to the problem def-
inition in Section 4, we are given a network of di-
alog trees f1 . . . fn and a query q?, but in addition
we are given the previous dialog state s, which
contains the previous user utterance q and the last
system action as. We aim to find a new dialog
state s? that pairs a node from the dialog tree with
updated history information, thereby undergoing a
dialog state update.
We learn a linear classifier that models
p(s?|q?, q, as, ?), the dialog state update distribu-
tion, where we constrain the new state s? to contain
the new utterance q? we are interpreting. This dis-
tribution models 3 transition types: append, over-
ride and reset.
Definition 2. An append action defines a dialog
state update when transitioning from a node to its
children at any depth in the same dialog tree e.g.,
ti ? gi.j (from a topic to a goal node), gi.j ?
ai.j.z (from a goal to an action node) etc.
Definition 3. An override action defines a dialog
state update when transitioning from a goal to its
sibling node. It could also be from an action node5
to another in its parent sibling node in the same di-
alog tree e.g., gi.j?1 ? gi.j (from one goal to an-
other goal in the same topic tree), ai.j.z ? ai.?j.z
(from an action node to another action node in a
different goal in the same dialog tree) etc.
Definition 4. A reset action defines a dialog state
update when transitioning from a node in a current
dialog tree to any other node at any depth in a
dialog tree other than the current dialog tree e.g.,
ti ? t?i, (from one topic node to another topic
5A transition from ai.j.z must be to a different goal or an
action node in a different goal but in the same dialog tree.
(a) Updates from topic node ti
(b) Updates from goal node gj
(c) Updates from action node al
Figure 5: Information state updates: append, reset
and override updates based on Definition 2, 3 and
4, respectively, from topic, goal and action nodes.
node) ti ? g?i.j (from a topic node to a goal node
in a different topic subtree), etc.
The append action should be selected when the
user?s intent is to clarify a previous query (e.g.,
?insert page numbers? ? ?page numbers in the
footer?). An override action is appropriate when
the user?s intent is to change a goal within the
same topic (e.g., ?insert page number? ?change
page number?). Finally, a reset action should be
used when the user?s intent is to restart the dialog
(e.g., ?insert page x of y? ? ?set default font?).
We present more examples for append, override
and reset dialog state update actions in Table 3.
1674
Previous Utterance, q User Utterance, q? Transition Update Action, a
inserting page numbers qt1 add a background ti ? t?i 2, reset-T, reset
how to number pages qt2 insert numbers on pages in margin ti ? si.j 1.4, append-G, append
page numbers qt3 set a page number in a footer ti ? ai.j.z 1.2.1, append-A, append
page number a document qt4 insert a comment ti ? g?i.j 21.1, reset-G, reset
page number qt5 add a comment ?redo? ti ? a?i.j.z 21.2.1, reset-A, reset
page x of y qg1 add a border gi.j ? t?i 6, reset-T, resetformat page x of x qg2 enter text and page numbers gi.j ? gi.?j 1.1, override-G, overrideenter page x of y qg3 page x of y in footer gi.j ? ai.j.z 1.3.1, append-A, appendinserting page x of y qg4 setting a default font gi.j ? g?i.j 6.1, reset-G, resetshowing page x of x qg5 set default font and style gi.j ? a?i.j.z 6.4.1, reset-A, resetpage numbers bottom qa1 make a degree symbol ai.j.z ? t?i 13, reset-T, reset
numbering at bottom page qa2 insert page numbers ai.j.z ? gi.?j 1.1, override-G, override
insert footer page numbers qa3 page number design ai.j.z?1 ? ai.j.z 1.2.2, append-A, append
headers page number qa4 comments in document ai.j.z ? g?i.j 21.1, reset-G, reset
page number in a footer qa5 changing initials in a comment ai.j.z ? a?i.j.z 21.2.1, reset-A, reset
Table 3: Example q and q? queries for append, override and reset dialog state updates.
Figure 5 illustrates examples of append, over-
ride and reset dialog state updates. All transitions
presented in Figure 5 are aligned with the example
q and q? queries in Table 3.
Dialog State Update Model We use a log-linear
model to maximize a dialog state distribution over
the space of all nodes in a dialog network:
p(s?|q?, q, as?) =
e
?
i ?i?i(s?,q?,as,q)
?
s?? e
?
i ?i?i(s??,q?,as,q)
, (4)
Optimization is done as described in Section 3.
Experimental Setup Ideally, dialog systems
should be evaluated relative to large volumes of
real user interaction data. Our query log data,
however, does not include dialog turns, and so we
turn to simulated user behavior to test our system.
Our approach, inspired by recent work (Schatz-
mann et al, 2006; Scheffler and Young, 2002;
Georgila et al, 2005), involves simulating dialog
turns as follows. To define a state s we sam-
ple a query q from a set of queries per node v
and get a corresponding system action as for this
node; to define a state s?, we sample a new query
q? from another node v? ? V, v 6= v? which
is sampled using a prior probability biased to-
wards append: p(append)=0.7, p(override)=0.2,
p(reset)=0.1. This prior distribution defines a dia-
log strategy where the user primarily continues the
current goal and rarely resets.
We simulate 1100 previous state and new query
pairs for training and 440 pairs for testing. The
features were lexical, including word ngrams,
stems with no stopwords; we also tested network
structure, such as:
- old q and new q? query overlap (QO);
- q? overlap with a system prompt as (PO);
- q? ngram overlap with all queries from the old
state s (SQO);
- q? ngram overlap with all queries from the
new state s? (S?QO);
- q? ngram overlap with all queries from the
new state parents (S?ParQO).
Results Table 4 reports results for dialog state
updates for topic, goal and action nodes. We also
report performance for two types of dialog updates
such as: append (App.) and override (Over.).
We found that the combination of lexical and
query overlap with the previous and new state
queries yielded the best accuracies: 0.95, 0.84 and
0.83 for topic, goal and action node level, respec-
tively. As in Section 4, the accuracy on the topic
level node was highest. Perhaps surprisingly, the
reset action was perfectly predicted (accuracy is
100% for all feature combinations, not included
in figure). The accuracies for append and override
actions are also high (append 95%, override 90%).
Features Topic Goal Action App. Over.
L 0.92 0.76 0.78 0.90 0.89
L+Q 0.93 0.80 0.80 0.92 0.83
L+P 0.93 0.80 0.79 0.91 0.85
L+Q+P 0.94 0.80 0.80 0.93 0.85
L+SQ 0.94 0.82 0.81 0.93 0.85
L+S?Q 0.93 0.80 0.80 0.91 0.90
L+S?+ParQ 0.94 0.80 0.80 0.91 0.86
L+Q+S?Q 0.94 0.81 0.81 0.91 0.88
L+SQ+S?Q 0.95 0.84 0.83 0.94 0.88
Table 4: Dialog state updates classification ac-
curacies where L stands for lexical features, Q -
query overlap, P - prompt overlap, SQ - previous
state query overlap, S?Q - new state query overlap,
S?ParQ - new state parent query overlap.
1675
6 The Complete Dialog System
Following the overall setup described in Section 2,
we integrate the learned models into a complete
dialog system. To evaluate the quality of the in-
teractions with this system, we performed a small
scale user study, as described here.
Experimental Setup We randomly chose 8
goals, across topics, from the 76 used to train our
models. For each goal, six users were shown a Mi-
crosoft Word document before and after the goal
was executed. Users were allowed to interact with
the system in an unconstrained manner providing a
total of 48 interactions. If at any point they thought
the system response was irrelevant to their intent,
they could either rephrase or start over.
User Study We objectively measured task com-
pletion and the quality of individual system utter-
ances. We also asked the users to rate the system
(on a scale of 1-5, with 5 being best) on various
measures, including whether the system asked rel-
evant questions. Table 5 summarizes the results.
In general, users were able to complete the tasks
and were reasonably satisfied.
Discussion Figure 6 shows four example di-
alogs. User utterances ranged from single key-
words to rich, sentence-like goal descriptions. For
the shorter inputs, the system was generally able
to identify a small cohort of high-probability in-
tents. For longer inputs, especially with explicitly
specified parameter settings, the system was usu-
ally able to elicit whatever additional information
was required to complete the task. The majority of
errors fell into two categories when the system:
I. missed key information in the input and
asked redundant questions ? Figure 6 (C);
II. failed to correctly identify the intent, and
asked an irrelevant question ? Figure 6 (D).
Table 5 shows the frequency of each error type.
Objective Measures Observed Rates
User achieved task 83%
System asked irrelevant questions 19%
System asked redundant questions 18%
User Satisfaction Score Std.Dev.
Overall satisfaction 3.42 0.49
Questions were relevant 4.17 0.41
Ambiguity was managed 4.00 0.89
Table 5: User study results.
U: ?I want a page number and title at the bot-
tom of the page?
S: ?Choose a page number design from the
gallery of designs?
U: *User selects design*
S: ?To insert header or footer content before the
page number, type the content, and then press
TAB to position the content?
U: ?Professional Resume?
S: ?Task Completed?
(A)
U: ?Alphabetize bullets?
S: ?Do you want to sort a list alphabetically?
U: ?Sort a list?
S: ?Select the text in a bulleted or numbered
list?
U: *User selection*
S: ?Ascending or Descending??
U: ?Ascending?
S: ?Task Completed?
(B)
U: ?I want the background color to be yellow?
S: ?Choose a color for the document?
U: ?Yellow ?
S: ?Task completed?
(C)
U: ?I want to change the font face and size?
S: ?Choose the default font?
U: ?No, reduce the size of the text?
S: *System Restarts*
(D)
Figure 6: Four example dialogs from the user
study, including cases that (A and B) complete
successfully, (C) have a redundant question, and
(D) fail to recognize the user intent.
7 Related work
To the best of our knowledge, this paper presents
the first effort to induce full procedural dialog sys-
tems from instructional text and query click logs.
Grounded Language Learning There has been
significant interest in grounded language learn-
ing. Perhaps the most closely related work
learns to understand instructions and automati-
1676
cally complete the tasks they describe (Branavan
et al, 2009; Vogel and Jurafsky, 2010; Kush-
man et al, 2009; Branavan et al, 2010; Artzi and
Zettlemoyer, 2013). However, these approaches
did not model user interaction. There are also
many related approaches for other grounded lan-
guage problems, including understanding game
strategy guides (Branavan et al, 2011), model-
ing users goals in a Windows domain (Horvitz
et al, 1998), learning from conversational inter-
action (Artzi and Zettlemoyer, 2011), learning
to sportscast (Chen and Mooney, 2011), learning
from event streams (Liang et al, 2009), and learn-
ing paraphrases from crowdsourced captions of
video snippets (Chen and Dolan, 2011).
Dialog Generation from Text Similarly to Pi-
wek?s work (2007; 2010; 2011), we study extract-
ing dialog knowledge from documents (mono-
logues or instructions). However, Piwek?s ap-
proach generates static dialogs, for example to
generate animations of virtual characters having a
conversation. There is no model of dialog man-
agement or user interaction, and the approach does
not use any machine learning. In contrast, to the
best of our knowledge, we are the first to demon-
strate it is possible to learn complete, interactive
dialog systems using instructional texts (and non-
expert annotation).
Learning from Web Query Logs Web query
logs have been extensively studied. For example,
they are widely used to represent user intents in
spoken language dialogs (Tu?r et al, 2011; Celiky-
ilmaz et al, 2011; Celikyilmaz and Hakkani-Tur,
2012). Web query logs are also used in many other
NLP tasks, including entity linking (Pantel et al,
2012) and training product and job intent classi-
fiers (Li et al, 2008).
Dialog Modeling and User Simulation Many
existing dialog systems learn dialog strategies
from user interactions (Young, 2010; Rieser and
Lemon, 2008). Moreover, dialog data is often lim-
ited and, therefore, user simulation is commonly
used (Scheffler and Young, 2002; Schatzmann et
al., 2006; Georgila et al, 2005).
Our overall approach is also related to many
other dialog management approaches, including
those that construct dialog graphs from dialog data
via clustering (Lee et al, 2009), learn information
state updates using discriminative classification
models (Hakkani-Tur et al, 2012; Mairesse et al,
2009), optimize dialog strategy using reinforce-
ment learning (RL) (Scheffler and Young, 2002;
Rieser and Lemon, 2008), or combine RL with
information state update rules (Heeman, 2007).
However, our approach is unique in the use of in-
ducing task and domain knowledge with light su-
pervision to assist the user with many goals.
8 Conclusions and Future Work
This paper presented a novel approach for au-
tomatically constructing procedural dialog sys-
tems with light supervision, given only textual re-
sources such as instructional text and search query
click logs. Evaluations demonstrated highly accu-
rate performance, on automatic benchmarks and
through a user study.
Although we showed it is possible to build com-
plete systems, more work will be required to scale
the approach to new domains, scale the complex-
ity of the dialog manager, and explore the range of
possible textual knowledge sources that could be
incorporated. We are particularly interested in sce-
narios that would enable end users to author new
goals by writing procedural instructions in natural
language.
Acknowledgments
The authors would like to thank Jason Williams
and the anonymous reviewers for their helpful
comments and suggestions.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, An-
thanasios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David R. Traum. 2012. The twins
corpus of museum visitor questions. In Proceedings
of LREC.
Yoav Artzi and Luke Zettlemoyer. 2011. Learning
to recover meaning from unannotated conversational
interactions. In NIPS Workshop In Learning Seman-
tics.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjo?rn Hartmann, Mark S. Ackerman, David R.
Karger, David Crowell, and Katrina Panovich.
2010. Soylent: a word processor with a crowd in-
side. In Proceedings of ACM Symposium on User
Interface Software and Technology.
1677
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,
and Regina Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In Proceedings
of ACL.
S. R. K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading between the lines: learn-
ing to map high-level instructions to commands. In
Proceedings of ACL.
S. R. K. Branavan, David Silver, and Regina Barzi-
lay. 2011. Learning to win by reading manuals in
a monte-carlo framework. In Proceedings of ACL.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2012. A
joint model for discovery of aspects in utterances.
In Proceedings of ACL.
Asli Celikyilmaz, Dilek Hakkani-Tu?r, and Gokhan Tu?r.
2011. Mining search query logs for spoken language
understanding. In Proceedings of ICML.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of ACL.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of AAAI.
Myroslava Dzikovska, Amy Isard, Peter Bell, Jo-
hanna D. Moore, Natalie B. Steinhauser, Gwen-
dolyn E. Campbell, Leanne S. Taylor, Simon Caine,
and Charlie Scott. 2011. Adaptive intelligent tuto-
rial dialogue in the beetle ii system. In Proceedings
of AIED.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2005. Learning user simulations for infor-
mation state update dialogue systems. In Proceed-
ings of Eurospeech.
Dilek Hakkani-Tur, Gokhan Tur, Larry Heck, Ashley
Fidler, and Asli Celikyilmaz. 2012. A discrimi-
native classification-based approach to information
state updates for a multi-domain dialog system. In
Proceedings of Interspeech.
Peter Heeman. 2007. Combining Reinforcement
Learning with Information-State Update Rules. In
Proceedings of ACL.
Eric Horvitz, Jack Breese, David Heckerman, David
Hovel, and Koos Rommelse. 1998. The Lumiere
project: Bayesian user modeling for inferring the
goals and needs of software users. In Proceedings
of Uncertainty in Artificial Intelligence.
Nate Kushman, Micah Brodsky, S. R. K. Branavan,
Dina Katabi, Regina Barzilay, and Martin Rinard.
2009. WikiDo. In ACM HotNets.
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, and
Gary Geunbae Lee. 2009. Automatic agenda graph
construction from human-human dialogs using clus-
tering method. In Proceedings of NAACL.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In
Proceedings of SIGIR.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of ACL-IJCNLP.
F. Mairesse, M. Gasic, F. Jurcicek, S. Keizer, B. Thom-
son, K. Yu, and S. Young. 2009. Spoken lan-
guage understanding from unaligned data using dis-
criminative classification models. In Proceedings of
Acoustics, Speech and Signal Processing.
Fabrizio Morbini, Eric Forbell, David DeVault, Kenji
Sagae, David R. Traum, and Albert A. Rizzo. 2012.
A mixed-initiative conversational dialogue system
for healthcare. In Proceedings of SIGDIAL.
Jorge Nocedal and Stephen J. Wright. 2000. Numeri-
cal Optimization. Springer.
Patric Pantel, Thomas Lin, and Michael Gamon. 2012.
Mining entity types from query logs via user intent.
In Proceedings of ACL.
Paul Piwek and Svetlana Stoyanchev. 2010. Generat-
ing expository dialogue from monologue: Motiva-
tion, corpus and preliminary rules. In Proceedings
of NAACL.
Paul Piwek and Svetlana Stoyanchev. 2011. Data-
oriented monologue-to-dialogue generation. In Pro-
ceedings of ACL, pages 242?247.
Paul Piwek, Hugo Hernault, Helmut Prendinger, and
Mitsuru Ishizuka. 2007. T2d: Generating dialogues
between virtual agents automatically from text. In
Proceedings of Intelligent Virtual Agents.
Verena Rieser and Oliver Lemon. 2008. Learning ef-
fective multimodal dialogue strategies from wizard-
of-oz data: Bootstrapping and evaluation. In Pro-
ceedings of ACL.
A. Rizzo, Kenji Sagae, E. Forbell, J. Kim, B. Lange,
J. Buckwalter, J. Williams, T. Parsons, P. Kenny,
David R. Traum, J. Difede, and B. Rothbaum. 2011.
Simcoach: An intelligent virtual human system for
providing healthcare information and support. In
Proceedings of ITSEC.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowledge Engineer-
ing Review, 21(2).
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
Human Language Technology Research.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava Dzikovska, and Johanna D. Moore. 2011.
1678
Talk like an electrician: Student dialogue mimick-
ing behavior in an intelligent tutoring system. In
Proceedings of AIED.
David R. Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William R. Swartout. 2012.
Ada and grace: Direct interaction with museum vis-
itors. In Proceedings of Intelligent Virtual Agents.
Go?khan Tu?r, Dilek Z. Hakkani-Tu?r, Dustin Hillard, and
Asli C?elikyilmaz. 2011. Towards unsupervised spo-
ken language understanding: Exploiting query click
logs for slot filling. In Proceedings of Interspeech.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of ACL.
Steve Young. 2010. Cognitive user interfaces. In IEEE
Signal Processing Magazine.
1679
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, page 2,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Parsing with Combinatory Categorial Grammars
Yoav Artzi, Nicholas FitzGerald and Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{yoav,nfitz,lsz}@cs.washington.edu
1 Abstract
Semantic parsers map natural language sentences
to formal representations of their underlying
meaning. Building accurate semantic parsers
without prohibitive engineering costs is a long-
standing, open research problem.
The tutorial will describe general principles for
building semantic parsers. The presentation will
be divided into two main parts: modeling and
learning. The modeling section will include best
practices for grammar design and choice of se-
mantic representation. The discussion will be
guided by examples from several domains. To il-
lustrate the choices to be made and show how they
can be approached within a real-life representation
language, we will use ?-calculus meaning repre-
sentations. In the learning part, we will describe
a unified approach for learning Combinatory Cat-
egorial Grammar (CCG) semantic parsers, that in-
duces both a CCG lexicon and the parameters of
a parsing model. The approach learns from data
with labeled meaning representations, as well as
from more easily gathered weak supervision. It
also enables grounded learning where the seman-
tic parser is used in an interactive environment, for
example to read and execute instructions.
The ideas we will discuss are widely appli-
cable. The semantic modeling approach, while
implemented in ?-calculus, could be applied to
many other formal languages. Similarly, the al-
gorithms for inducing CCGs focus on tasks that
are formalism independent, learning the meaning
of words and estimating parsing parameters. No
prior knowledge of CCGs is required. The tuto-
rial will be backed by implementation and exper-
iments in the University of Washington Semantic
Parsing Framework (UW SPF).1
1http://yoavartzi.com/spf
2 Outline
1. Introduction to CCGs
2. Modeling
(a) Questions for database queries
(b) Plurality and determiner resolution in
grounded applications
(c) Event semantics and imperatives in in-
structional language
3. Learning
(a) A unified learning algorithm
(b) Learning with supervised data
i. Lexical induction with templates
ii. Unification-based learning
(c) Weakly supervised learning without la-
beled meaning representations
3 Instructors
Yoav Artzi is a Ph.D. candidate in the Computer
Science & Engineering department at the Univer-
sity of Washington. His research studies the acqui-
sition of grounded natural language understanding
within interactive systems. His work focuses on
modeling semantic representations and designing
weakly supervised learning algorithms. He is a re-
cipient of the 2012 Yahoo KSC award.
Nicholas FitzGerald is a Ph.D. student at the
University of Washington. His research interests
are grounded natural language understanding and
generation. He is a recipient of an Intel Science
and Technology Center Fellowship and an NSERC
Postgraduate Scholarship.
Luke Zettlemoyer is an Assistant Professor in
the Computer Science & Engineering department
at the University of Washington. His research in-
terests are in the intersections of natural language
processing, machine learning and decision mak-
ing under uncertainty. Honors include best paper
awards at UAI 2005 and ACL 2009, selection to
the DARPA CSSG, and an NSF CAREER Award.
2
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 271?281,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning to Automatically Solve Algebra Word Problems
Nate Kushman
?
, Yoav Artzi
?
, Luke Zettlemoyer
?
, and Regina Barzilay
?
?
Computer Science and Articial Intelligence Laboratory, Massachusetts Institute of Technology
{nkushman, regina}@csail.mit.edu
?
Computer Science & Engineering, University of Washington
{yoav, lsz}@cs.washington.edu
Abstract
We present an approach for automatically
learning to solve algebra word problems.
Our algorithm reasons across sentence
boundaries to construct and solve a sys-
tem of linear equations, while simultane-
ously recovering an alignment of the vari-
ables and numbers in these equations to
the problem text. The learning algorithm
uses varied supervision, including either
full equations or just the final answers. We
evaluate performance on a newly gathered
corpus of algebra word problems, demon-
strating that the system can correctly an-
swer almost 70% of the questions in the
dataset. This is, to our knowledge, the first
learning result for this task.
1 Introduction
Algebra word problems concisely describe a world
state and pose questions about it. The described
state can be modeled with a system of equations
whose solution specifies the questions? answers.
For example, Figure 1 shows one such problem.
The reader is asked to infer how many children and
adults were admitted to an amusement park, based
on constraints provided by ticket prices and overall
sales. This paper studies the task of learning to
automatically solve such problems given only the
natural language.
1
Solving these problems requires reasoning
across sentence boundaries to find a system of
equations that concisely models the described se-
mantic relationships. For example, in Figure 1,
the total ticket revenue computation in the second
equation summarizes facts about ticket prices and
total sales described in the second, third, and fifth
1
The code and data for this work are available
at http://groups.csail.mit.edu/rbg/code/
wordprobs/.
Word problem
An amusement park sells 2 kinds of tickets.
Tickets for children cost $1.50. Adult tickets
cost $4. On a certain day, 278 people entered
the park. On that same day the admission fees
collected totaled $792. How many children
were admitted on that day? How many adults
were admitted?
Equations
x+ y = 278
1.5x+ 4y = 792
Solution
x = 128 y = 150
Figure 1: An example algebra word problem. Our
goal is to map a given problem to a set of equations
representing its algebraic meaning, which are then
solved to get the problem?s answer.
sentences. Furthermore, the first equation models
an implicit semantic relationship, namely that the
children and adults admitted are non-intersecting
subsets of the set of people who entered the park.
Our model defines a joint log-linear distribu-
tion over full systems of equations and alignments
between these equations and the text. The space
of possible equations is defined by a set of equa-
tion templates, which we induce from the train-
ing examples, where each template has a set of
slots. Number slots are filled by numbers from
the text, and unknown slots are aligned to nouns.
For example, the system in Figure 1 is gener-
ated by filling one such template with four spe-
cific numbers (1.5, 4, 278, and 792) and align-
ing two nouns (?Tickets? in ?Tickets for children?,
and ?tickets? in ?Adult tickets?). These inferred
correspondences are used to define cross-sentence
features that provide global cues to the model.
For instance, in our running example, the string
271
pairs (?$1.50?, ?children?) and (?$4?,?adults?)
both surround the word ?cost,? suggesting an out-
put equation with a sum of two constant-variable
products.
We consider learning with two different levels
of supervision. In the first scenario, we assume ac-
cess to each problem?s numeric solution (see Fig-
ure 1) for most of the data, along with a small
set of seed examples labeled with full equations.
During learning, a solver evaluates competing hy-
potheses to drive the learning process. In the sec-
ond scenario, we are provided with a full system
of equations for each problem. In both cases, the
available labeled equations (either the seed set, or
the full set) are abstracted to provide the model?s
equation templates, while the slot filling and align-
ment decisions are latent variables whose settings
are estimated by directly optimizing the marginal
data log-likelihood.
The approach is evaluated on a new corpus of
514 algebra word problems and associated equa-
tion systems gathered from Algebra.com. Pro-
vided with full equations during training, our al-
gorithm successfully solves over 69% of the word
problems from our test set. Furthermore, we find
the algorithm can robustly handle weak supervi-
sion, achieving more than 70% of the above per-
formance when trained exclusively on answers.
2 Related Work
Our work is related to three main areas of research:
situated semantic interpretation, information ex-
traction, and automatic word problem solvers.
Situated Semantic Interpretation There is a
large body of research on learning to map nat-
ural language to formal meaning representations,
given varied forms of supervision. Reinforcement
learning can be used to learn to read instructions
and perform actions in an external world (Brana-
van et al, 2009; Branavan et al, 2010; Vogel
and Jurafsky, 2010). Other approaches have re-
lied on access to more costly annotated logical
forms (Zelle and Mooney, 1996; Thompson and
Mooney, 2003; Wong and Mooney, 2006; Zettle-
moyer and Collins, 2005; Kwiatkowski et al,
2010). These techniques have been generalized
more recently to learn from sentences paired with
indirect feedback from a controlled application.
Examples include question answering (Clarke et
al., 2010; Cai and Yates, 2013a; Cai and Yates,
2013b; Berant et al, 2013; Kwiatkowski et al,
2013), dialog systems (Artzi and Zettlemoyer,
2011), robot instruction (Chen and Mooney, 2011;
Chen, 2012; Kim and Mooney, 2012; Matuszek et
al., 2012; Artzi and Zettlemoyer, 2013), and pro-
gram executions (Kushman and Barzilay, 2013;
Lei et al, 2013). We focus on learning from varied
supervision, including question answers and equa-
tion systems, both can be obtained reliably from
annotators with no linguistic training and only ba-
sic math knowledge.
Nearly all of the above work processed sin-
gle sentences in isolation. Techniques that con-
sider multiple sentences typically do so in a se-
rial fashion, processing each in turn with limited
cross-sentence reasoning (Branavan et al, 2009;
Zettlemoyer and Collins, 2009; Chen and Mooney,
2011; Artzi and Zettlemoyer, 2013). We focus on
analyzing multiple sentences simultaneously, as
is necessary to generate the global semantic rep-
resentations common in domains such as algebra
word problems.
Information Extraction Our approach is related
to work on template-based information extraction,
where the goal is to identify instances of event
templates in text and extract their slot fillers. Most
work has focused on the supervised case, where
the templates are manually defined and data is la-
beled with alignment information, e.g. (Grishman
et al, 2005; Maslennikov and Chua, 2007; Ji and
Grishman, 2008; Reichart and Barzilay, 2012).
However, some recent work has studied the au-
tomatic induction of the set of possible templates
from data (Chambers and Jurafsky, 2011; Ritter et
al., 2012). In our approach, systems of equations
are relatively easy to specify, providing a type of
template structure, and the alignment of the slots
in these templates to the text is modeled primar-
ily with latent variables during learning. Addition-
ally, mapping to a semantic representation that can
be executed allows us to leverage weaker supervi-
sion during learning.
Automatic Word Problem Solvers Finally, there
has been research on automatically solving vari-
ous types of mathematical word problems. The
dominant existing approach is to hand engineer
rule-based systems to solve math problem in spe-
cific domains (Mukherjee and Garain, 2008; Lev
et al, 2004). Our focus is on learning a model
for the end-to-end task of solving word problems
given only a training corpus of questions paired
with equations or answers.
272
Derivation 1
Word
problem
An amusement park sells 2 kinds of tickets. Tickets for children cost $ 1.50 . Adult
tickets cost $ 4 . On a certain day, 278 people entered the park. On that same day the
admission fees collected totaled $ 792 . How many children were admitted on that
day? How many adults were admitted?
Aligned
template
u
1
1
+ u
1
2
? n
1
= 0 n
2
? u
2
1
+ n
3
? u
2
2
? n
4
= 0
Instantiated
equations
x+ y ? 278 = 0 1.5x+ 4y ? 792 = 0
Answer
x = 128
y = 150
Derivation 2
Word
problem
A motorist drove 2 hours at one speed and then for 3 hours at another speed. He
covered a distance of 252 kilometers. If he had traveled 4 hours at the first speed and
1 hour at the second speed , he would have covered 244 kilometers. Find two speeds?
Aligned
template
n
1
? u
1
1
+ n
2
? u
1
2
? n
3
= 0 n
4
? u
2
1
+ n
5
? u
2
2
? n
6
= 0
Instantiated
equations
2x+ 3y ? 252 = 0 4x+ 1y ? 244 = 0
Answer
x = 48
y = 52
Figure 2: Two complete derivations for two different word problems. Derivation 1 shows an alignment
where two instances of the same slot are aligned to the same word (e.g., u
1
1
and u
2
1
both are aligned to
?Tickets?). Derivation 2 includes an alignment where four identical nouns are each aligned to different
slot instances in the template (e.g., the first ?speed? in the problem is aligned to u
1
1
).
3 Mapping Word Problems to Equations
We define a two step process to map word prob-
lems to equations. First, a template is selected
to define the overall structure of the equation sys-
tem. Next, the template is instantiated with num-
bers and nouns from the text. During inference we
consider these two steps jointly.
Figure 2 shows both steps for two derivations.
The template dictates the form of the equations in
the system and the type of slots in each: u slots
represent unknowns and n slots are for numbers
that must be filled from the text. In Derivation 1,
the selected template has two unknown slots, u
1
and u
2
, and four number slots, n
1
to n
4
. Slots
can be shared between equations, for example, the
unknown slots u
1
and u
2
in the example appear
in both equations. A slot may have different in-
stances, for example u
1
1
and u
2
1
are the two in-
stances of u
1
in the example.
We align each slot instance to a word in the
problem. Each number slot n is aligned to a num-
ber, and each unknown slot u is aligned to a noun.
For example, Derivation 1 aligns the number 278
to n
1
, 1.50 to n
2
, 4 to n
3
, and 792 to n
4
. It also
aligns both instances of u
1
(e.g., u
1
1
and u
2
1
) to
?Tickets?, and both instances of u
2
to ?tickets?.
In contrast, in Derivation 2, instances of the same
unknown slot (e.g. u
1
1
and u
2
1
) are aligned to two
different words in the problem (different occur-
rences of the word ?speed?). This allows for a
tighter mapping between the natural language and
the system template, where the words aligned to
the first equation in the template come from the
first two sentences, and the words aligned to the
second equation come from the third.
Given an alignment, the template can then be
instantiated: each number slot n is replaced with
the aligned number, and each unknown slot u with
a variable. This output system of equations is then
automatically solved to generate the final answer.
273
3.1 Derivations
Definitions Let X be the set of all word problems.
A word problem x ? X is a sequence of k words
?w
1
, . . . w
k
?. Also, define an equation template t
to be a formulaA = B, whereA andB are expres-
sions. An expression A is one of the following:
? A number constant f .
? A number slot n.
? An unknown slot u.
? An application of a mathematical relation R
to two expressions (e.g., n
1
? u
1
).
We define a system template T to be a set of l
equation templates {t
0
, . . . , t
l
}. T is the set of
all system templates. A slot may occur more than
once in a system template, to allow variables to
be reused in different equations. We denote a spe-
cific instance i of a slot, u for example, as u
i
. For
brevity, we omit the instance index when a slot ap-
pears only once. To capture a correspondence be-
tween the text of x and a template T , we define an
alignment p to be a set of pairs (w, s), where w is
a token in x and s is a slot instance in T .
Given the above definitions, an equation e can
be constructed from a template t where each num-
ber slot n is replaced with a real number, each un-
known slot u is replaced with a variable, and each
number constant f is kept as is. We call the pro-
cess of turning a template into an equation tem-
plate instantiation. Similarly, an equation system
E is a set of l equations {e
0
, . . . , e
l
}, which can
be constructed by instantiating each of the equa-
tion templates in a system template T . Finally, an
answer a is a tuple of real numbers.
We define a derivation y from a word problem
to an answer as a tuple (T, p, a), where T is the se-
lected system template, p is an alignment between
T and x, and a is the answer generated by instan-
tiating T using x through p and solving the gener-
ated equations. Let Y be the set of all derivations.
The Space of Possible Derivations We aim to
map each word problem x to an equation system
E. The space of equation systems considered is
defined by the set of possible system templates T
and the words in the original problem x, that are
available for filling slots. In practice, we gener-
ate T from the training data, as described in Sec-
tion 4.1. Given a system template T ? T , we
create an alignment p between T and x. The set
of possible alignment pairs is constrained as fol-
An amusement park sells 2 kinds of tickets.
Tickets for children cost $ 1.50 . Adult tick-
ets cost $ 4 . On a certain day, 278 people
entered the park. On that same day the ad-
mission fees collected totaled $ 792 . How
many children were admitted on that day?
How many adults were admitted?
u
1
1
+ u
1
2
? n
1
= 0
n
2
? u
2
1
+ n
3
? u
2
2
? n
4
= 0
Figure 3: The first example problem and selected
system template from Figure 2 with all potential
aligned words marked. Nouns (boldfaced) may be
aligned to unknown slot instances u
j
i
, and num-
ber words (highlighted) may be aligned to number
slots n
i
.
lows: each number slot n ? T can be aligned to
any number in the text, a number word can only
be aligned to a single slot n, and must be aligned
to all instances of that slot. Additionally, an un-
known slot instance u ? T can only be aligned to
a noun word. A complete derivation?s alignment
pairs all slots in T with words in x.
Figure 3 illustrates the space of possible align-
ments for the first problem and system template
from Figure 2. Nouns (shown in boldface) can
be aligned to any of the unknown slot instances
in the selected template (u
1
1
, u
2
1
, u
1
2
, and u
2
2
for the
template selected). Numbers (highlighted) can be
aligned to any of the number slots (n
1
, n
2
, n
3
, and
n
4
in the template).
3.2 Probabilistic Model
Due to the ambiguity in selecting the system tem-
plate and alignment, there will be many possible
derivations y ? Y for each word problem x ? X .
We discriminate between competing analyses us-
ing a log-linear model, which has a feature func-
tion ? : X ? Y ? R
d
and a parameter vector
? ? R
d
. The probability of a derivation y given a
problem x is defined as:
p(y|x; ?) =
e
???(x,y)
?
y
?
?Y
e
???(x,y
?
)
Section 6 defines the full set of features used.
The inference problem at test time requires us
to find the most likely answer a given a problem
274
x, assuming the parameters ? are known:
f(x) = argmax
a
p(a|x; ?)
Here, the probability of the answer is marginalized
over template selection and alignment:
p(a|x; ?) =
?
y?Y
s.t. AN(y)=a
p(y|x; ?) (1)
where AN(y) extracts the answer a out of deriva-
tion y. In this way, the distribution over deriva-
tions y is modeled as a latent variable. We use a
beam search inference procedure to approximately
compute Equation 1, as described in Section 5.
4 Learning
To learn our model, we need to induce the struc-
ture of system templates in T and estimate the
model parameters ?.
4.1 Template Induction
It is possible to generate system templates T when
provided access to a set of n training examples
{(x
i
, E
i
) : i = 1, . . . , n}, where x
i
is a word
problem and E
i
is a set of equations. We general-
ize eachE to a system template T by (a) replacing
each variable with an unknown slot, and (b) re-
placing each number mentioned in the text with a
number slot. Numbers not mentioned in the prob-
lem text remain in the template as constants. This
allows us to solve problems that require numbers
that are implied by the problem semantics rather
than appearing directly in the text, such as the per-
cent problem in Figure 4.
4.2 Parameter Estimation
For parameter estimation, we assume access to
n training examples {(x
i
,V
i
) : i = 1, . . . , n},
each containing a word problem x
i
and a val-
idation function V
i
. The validation function
V : Y ? {0, 1} maps a derivation y ? Y to 1 if
it is correct, or 0 otherwise.
We can vary the validation function to learn
from different types of supervision. In Sec-
tion 8, we will use validation functions that check
whether the derivation y has either (1) the cor-
rect system of equations E, or (2) the correct an-
swer a. Also, using different types of validation
functions on different subsets of the data enables
semi-supervised learning. This approach is related
to Artzi and Zettlemoyer (2013).
Word problem
A chemist has a solution that is 18 % alco-
hol and one that is 50 % alcohol. He wants
to make 80 liters of a 30 % solution. How
many liters of the 18 % solution should he
add? How many liters of the 30 % solution
should he add?
Labeled equations
18? 0.01? x + 50? 0.01? y = 30? 0.01? 80
x + y = 80
Induced template system
n
1
? 0.01? u
1
1
+ n
2
? 0.01? u
1
2
= n
3
? 0.01? n
4
u
2
1
+ u
2
2
= n
5
Figure 4: During template induction, we automat-
ically detect the numbers in the problem (high-
lighted above) to generalize the labeled equations
to templates. Numbers not present in the text are
considered part of the induced template.
We estimate ? by maximizing the conditional
log-likelihood of the data, marginalizing over all
valid derivations:
O =
?
i
?
y?Y
s.t. V
i
(y)=1
log p(y|x
i
; ?)
We use L-BFGS (Nocedal and Wright, 2006) to
optimize the parameters. The gradient of the indi-
vidual parameter ?
j
is given by:
?O
??
j
=
?
i
E
p(y|x
i
,V
i
(y)=1;?)
[?
j
(x
i
, y)]?
E
p(y|x
i
;?)
[?
j
(x
i
, y)]
(2)
Section 5 describes how we approximate the
two terms of the gradient using beam search.
5 Inference
Computing the normalization constant for Equa-
tion 1 requires summing over all templates and all
possible ways to instantiate them. This results in
a search space exponential in the number of slots
in the largest template in T , the set of available
system templates. Therefore, we approximate this
computation using beam search. We initialize the
beam with all templates in T and iteratively align
slots from the templates in the beam to words in
the problem text. For each template, the next slot
275
to be considered is selected according to a pre-
defined canonicalized ordering for that template.
After each iteration we prune the beam to keep the
top-k partial derivations according to the model
score. When pruning the beam, we allow at most l
partial derivations for each template, to ensure that
a small number of templates don?t monopolize the
beam. We continue this process until all templates
in the beam are fully instantiated.
During learning we compute the second term in
the gradient (Equation 2) using our beam search
approximation. Depending on the available vali-
dation function V (as defined in Section 4.2), we
can also accurately prune the beam for the com-
putation of the first half of the gradient. Specifi-
cally, when assuming access to labeled equations,
we can constrain the search to consider only par-
tial hypotheses that could possibly be completed
to produce the labeled equations.
6 Model Details
Template Canonicalization There are many syn-
tactically different but semantically equivalent
ways to express a given system of equations. For
example, the phrase ?John is 3 years older than
Bill? can be written as j = b+ 3 or j ? 3 = b.
To avoid such ambiguity, we canonicalize tem-
plates into a normal form representation. We per-
form this canonicalization by obtaining the sym-
bolic solution for the unknown slots in terms of
the number slots and constants using the mathe-
matical solver Maxima (Maxima, 2014).
Slot Signature In a template like s
1
+s
2
= s
3
, the
slot s
1
is distinct from the slot s
2
, but we would
like them to share many of the features used in de-
ciding their alignment. To facilitate this, we gener-
ate signatures for each slot and slot pair. The sig-
nature for a slot indicates the system of equations
it appears in, the specific equation it is in, and the
terms of the equation it is a part of. Pairwise slot
signatures concatenate the signatures for the two
slots as well as indicating which terms are shared.
This allows, for example, n
2
and n
3
in Derivation
1 in Figure 2 to have the same signature, while the
pairs ?n
2
, u
1
? and ?n
3
, u
1
? have different ones. To
share features across templates, slot and slot-pair
signatures are generated for both the full template,
as well as for each of the constituent equations.
Features The features ?(x, y) are computed for a
derivation y and problem x and cover all deriva-
Document level
Unigrams
Bigrams
Single slot
Has the same lemma as a question object
Is a question object
Is in a question sentence
Is equal to one or two (for numbers)
Word lemma X nearby constant
Slot pair
Dep. path contains: Word
Dep. path contains: Dep. Type
Dep. path contains: Word X Dep. Type
Are the same word instance
Have the same lemma
In the same sentence
In the same phrase
Connected by a preposition
Numbers are equal
One number is larger than the other
Equivalent relationship
Solution Features
Is solution all positive
Is solution all integer
Table 1: The features divided into categories.
tion decisions, including template and alignment
selection. When required, we use standard tools
to generate part-of-speech tags, lematizations, and
dependency parses to compute features.
2
For each
number word in y we also identify the closest noun
in the dependency parse. For example, the noun
for 278 in Derivation 1, Figure 2 would be ?peo-
ple.? The features are calculated based on these
nouns, rather than the number words.
We use four types of features: document level
features, features that look at a single slot entry,
features that look at pairs of slot entries, and fea-
tures that look at the numeric solutions. Table 1
lists all the features used. Unless otherwise noted,
when computing slot and slot pair features, a sep-
arate feature is generated for each of the signature
types discussed earlier.
Document level features Oftentimes the natural
language in x will contain words or phrases which
are indicative of a certain template, but are not as-
sociated with any of the words aligned to slots in
the template. For example, the word ?chemist?
2
In our experiments these are generated using the Stan-
ford parser (de Marneffe et al, 2006)
276
might indicate a template like the one seen in Fig-
ure 4. We include features that connect each tem-
plate with the unigrams and bigrams in the word
problem. We also include an indicator feature for
each system template, providing a bias for its use.
Single Slot Features The natural language x al-
ways contains one or more questions or commands
indicating the queried quantities. For example, the
first problem in Figure 2 asks ?How many children
were admitted on that day?? The queried quanti-
ties, the number of children in this case, must be
represented by an unknown in the system of equa-
tions. We generate a set of features which look at
both the word overlap and the noun phrase overlap
between slot words and the objects of a question or
command sentence. We also compute a feature in-
dicating whether a slot is filled from a word in a
question sentence. Additionally, algebra problems
frequently use phrases such as ?2 kinds of tickets?
(e.g., Figure 2). These numbers do not typically
appear in the equations. To account for this, we
add a single feature indicating whether a number
is one or two. Lastly, many templates contain con-
stants which are identifiable from words used in
nearby slots. For example, in Figure 4 the con-
stant 0.01 is related to the use of ?%? in the text.
To capture such usage, we include a set of lexical-
ized features which concatenate the word lemma
with nearby constants in the equation. These fea-
tures do not include the slot signature.
Slot Pair Features The majority of features we
compute account for relationships between slot
words. This includes features that trigger for
various equivalence relations between the words
themselves, as well as features of the dependency
path between them. We also include features that
look at the numerical relationship of two num-
bers, where the numeric values of the unknowns
are generated by solving the system of equations.
This helps recognize that, for example, the total of
a sum is typically larger than each of the (typically
positive) summands.
Additionally, we also have a single feature look-
ing at shared relationships between pairs of slots.
For example, in Figure 2 the relationship between
?tickets for children? and ?$1.50? is ?cost?. Sim-
ilarly the relationship between ?Adult tickets? and
?$4? is also ?cost?. Since the actual nature of this
relationship is not important, this feature is not
lexicalized, instead it is only triggered for the pres-
ence of equality. We consider two cases: subject-
# problems 514
# sentences 1616
# words 19357
Vocabulary size 2352
Mean words per problem 37
Mean sentences per problem 3.1
Mean nouns per problem 13.4
# unique equation systems 28
Mean slots per system 7
Mean derivations per problem 4M
Table 2: Dataset statistics.
object relationships where the intervening verb
is equal, and noun-to-preposition object relation-
ships where the intervening preposition is equal.
Solution Features By grounding our semantics in
math, we are able to include features which look
at the final answer, a, to learn which answers are
reasonable for the algebra problems we typically
see. For example, the solution to many, but not all,
of the problems involves the size of some set of
objects which must be both positive and integer.
7 Experimental Setup
Dataset We collected a new dataset of alge-
bra word problems from Algebra.com, a crowd-
sourced tutoring website. The questions were
posted by students for members of the community
to respond with solutions. Therefore, the problems
are highly varied, and are taken from real prob-
lems given to students. We heuristically filtered
the data to get only linear algebra questions which
did not require any explicit background knowl-
edge. From these we randomly chose a set of
1024 questions. As the questions are posted to a
web forum, the posts often contained additional
comments which were not part of the word prob-
lems and the solutions are embedded in long free-
form natural language descriptions. To clean the
data we asked Amazon Mechanical Turk workers
to extract from the text: the algebra word prob-
lem itself, the solution equations, and the numeric
answer. We manually verified both the equations
and the numbers to ensure they were correct. To
ensure each problem type is seen at least a few
times in the training data, we removed the infre-
quent problem types. Specifically, we induced the
system template from each equation system, as de-
scribed in Section 4.1, and removed all problems
for which the associated system template appeared
277
less than 6 times in the dataset. This left us with
514 problems. Table 2 provides the data statistics.
Forms of Supervision We consider both semi-
supervised and supervised learning. In the semi-
supervised scenario, we assume access to the nu-
merical answers of all problems in the training cor-
pus and to a small number of problems paired with
full equation systems. To select which problems
to annotate with equations, we identified the five
most common types of questions in the data and
annotated a randomly sampled question of each
type. 5EQ+ANS uses this form of weak supervi-
sion. To show the benefit of using the weakly su-
pervised data, we also provide results for a base-
line scenario 5EQ, where the training data includes
only the five seed questions annotated with equa-
tion systems. In the fully supervised scenario
ALLEQ, we assume access to full equation sys-
tems for the entire training set.
Evaluation Protocol We run all our experiments
using 5-fold cross-validation. Since our model
generates a solution for every problem, we report
only accuracy. We report two metrics: equation
accuracy to measure how often the system gener-
ates the correct equation system, and answer accu-
racy to evaluate how often the generated numerical
answer is correct. When comparing equations, we
avoid spurious differences by canonicalizing the
equation system, as described in Section 6. To
compare answer tuples we disregard the ordering
and require each number appearing in the refer-
ence answer to appear in the generated answer.
Parameters and Solver In our experiments we set
k in our beam search algorithm (Section 5) to 200,
and l to 20. We run the L-BFGS computation for
50 iterations. We regularize our learning objec-
tive using the L
2
-norm and a ? value of 0.1. The
set of mathematical relations supported by our im-
plementation is {+,?,?, /}.Our implementation
uses the Gaussian Elimination function in the Effi-
cient Java Matrix Library (EJML) (Abeles, 2014)
to generate answers given a set of equations.
8 Results
8.1 Impact of Supervision
Table 3 summarizes the results. As expected, hav-
ing access to the full system of equations (ALLEQ)
at training time results in the best learned model,
with nearly 69% accuracy. However, training
from primarily answer annotations (5EQ+ANS)
Equation Answer
accuracy accuracy
5EQ 20.4 20.8
5EQ+ANS 45.7 46.1
ALLEQ 66.1 68.7
Table 3: Cross-validation accuracy results for var-
ious forms of supervision.
Equation Answer % of
accuracy accuracy data
? 10 43.6 50.8 25.5
11? 15 46.6 45.1 10.5
16? 20 44.2 52.0 11.3
> 20 85.7 86.1 52.7
Table 4: Performance on different template fre-
quencies for ALLEQ.
results in performance which is almost 70% of
ALLEQ, demonstrating the value of weakly super-
vised data. In contrast, 5EQ, which cannot use this
weak supervision, performs much worse.
8.2 Performance and Template Frequency
To better understand the results, we also measured
equation accuracy as a function of the frequency
of each equation template in the data set. Table 4
reports results for ALLEQ after grouping the prob-
lems into four different frequency bins. We can
see that the system correctly answers more than
85% of the question types which occur frequently
while still achieving more than 50% accuracy on
those that occur relatively infrequently. We do not
include template frequency results for 5EQ+ANS
since in this setup our system is given only the top
five most common templates. This limited set of
templates covers only those questions in the > 20
bin, or about 52% of the data. However, on this
subset 5EQ+ANS performs very well, answering
88% of them correctly, which is approximately the
same as the 86% achieved by ALLEQ. Thus while
the weak supervision is not helpful in generating
the space of possible equations, it is very helpful
in learning to generate the correct answer when
given an appropriate space of equations.
8.3 Ablation Analysis
Table 5 shows ablation results for each group of
features. The results along the diagonal show the
performance when a single group of features is
ablated, while the off-diagonal numbers show the
278
w/o w/o w/o w/o
pair document solution single
w/o pair 42.8 25.7 19.0 39.6
w/o document ? 63.8 50.4 57.6
w/o solution ? ? 63.6 62.0
w/o single ? ? ? 65.9
Table 5: Cross-validation accuracy results with
different feature groups ablated for ALLEQ. Re-
sults are for answer accuracy which is 68.7% with-
out any features ablated.
performance when two groups of features are ab-
lated together. We can see that all of the features
contribute to the overall performance, and that the
pair features are the most important followed by
the document and solution features. We also see
that the pair features can compensate for the ab-
sence of other features. For example, the perfor-
mance drops only slightly when either the docu-
ment or solution features are removed in isolation.
However, the drop is much more dramatic when
they are removed along with the pair features.
8.4 Qualitative Error Analysis
We examined our system output on one fold of
ALLEQ and identified two main classes of errors.
The first, accounting for approximately one-
quarter of the cases, includes mistakes where
more background or world knowledge might have
helped. For example, Problem 1 in Figure 5 re-
quires understanding the relation between the di-
mensions of a painting, and how this relation is
maintained when the painting is printed, and Prob-
lem 2 relies on understanding concepts of com-
merce, including cost, sale price, and profit. While
these relationships could be learned in our model
with enough data, as it does for percentage prob-
lems (e.g., Figure 4), various outside resources,
such as knowledge bases (e.g. Freebase) or distri-
butional statistics from a large text corpus, might
help us learn them with less training data.
The second category, which accounts for about
half of the errors, includes mistakes that stem from
compositional language. For example, the second
sentence in Problem 3 in Figure 5 could generate
the equation 2x?y = 5, with the phrase ?twice of
one of them? generating the expression 2x. Given
the typical shallow nesting, it?s possible to learn
templates for these cases given enough data, and in
the future it might also be possible to develop new,
cross-sentence semantic parsers to enable better
generalization from smaller datasets.
(1)
A painting is 10 inches tall and 15 inches
wide. A print of the painting is 25 inches
tall, how wide is the print in inches?
(2)
A textbook costs a bookstore 44 dollars,
and the store sells it for 55 dollars. Find
the amount of profit based on the selling
price.
(3)
The sum of two numbers is 85. The dif-
ference of twice of one of them and the
other one is 5. Find both numbers.
(4)
The difference between two numbers is
6. If you double both numbers, the sum
is 36. Find the two numbers.
Figure 5: Examples of problems our system does
not solve correctly.
9 Conclusion
We presented an approach for automatically learn-
ing to solve algebra word problems. Our algorithm
constructs systems of equations, while aligning
their variables and numbers to the problem text.
Using a newly gathered corpus we measured the
effects of various forms of weak supervision on
performance. To the best of our knowledge, we
present the first learning result for this task.
There are still many opportunities to improve
the reported results, and extend the approach to
related domains. We would like to develop tech-
niques to learn compositional models of mean-
ing for generating new equations. Furthermore,
the general representation of mathematics lends it-
self to many different domains including geome-
try, physics, and chemistry. Eventually, we hope
to extend the techniques to synthesize even more
complex structures, such as computer programs,
from natural language.
Acknowledgments
The authors acknowledge the support of Battelle
Memorial Institute (PO#300662) and NSF (grant
IIS-0835652). We thank Nicholas FitzGerald, the
MIT NLP group, the UW NLP group and the
ACL reviewers for their suggestions and com-
ments. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
279
References
Peter Abeles. 2014. Efficient java matrix library.
https://code.google.com/p/efficient
-java-matrix-library/.
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrap-
ping semantic parsers from conversations. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
S.R.K Branavan, Luke Zettlemoyer, and Regina Barzi-
lay. 2010. Reading between the lines: Learning to
map high-level instructions to commands. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics.
Qingqing Cai and Alexander Yates. 2013a. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics.
Qingqing Cai and Alexander Yates. 2013b. Seman-
tic parsing freebase: Towards open-domain seman-
tic parsing. In Proceedings of the Joint Conference
on Lexical and Computational Semantics.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
David Chen and Raymond Mooney. 2011. Learning
to interpret natural language navigation instructions
from observations. In Proceedings of the Confer-
ence on Artificial Intelligence.
David Chen. 2012. Fast online lexicon learning for
grounded language acquisition. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proceedings of the Conference
on Computational Natural Language Learning. As-
sociation for Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of the Conference on Language Re-
sources and Evaluation.
Ralph Grishman, David Westbrook, and Adam Mey-
ers. 2005. NYUs English ACE 2005 System De-
scription. In Proceedings of the Automatic Content
Extraction Evaluation Workshop.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics.
Joohyun Kim and Raymond Mooney. 2012. Unsuper-
vised pcfg induction for grounded language learning
with highly ambiguous supervision. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Nate Kushman and Regina Barzilay. 2013. Using se-
mantic unification to generate regular expressions
from natural language. In Proceeding of the Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of the Conference
on Empirical Methods on Natural Language Pro-
cessing.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Empirical Methods in Natural Language Process-
ing.
Tao Lei, Fan Long, Regina Barzilay, and Martin Ri-
nard. 2013. From natural language specifications to
program input parsers. In Proceeding of the Associ-
ation for Computational Linguistics.
Iddo Lev, Bill MacCartney, Christopher Manning, and
Roger Levy. 2004. Solving logic puzzles: From
robust processing to precise semantics. In Proceed-
ings of the Workshop on Text Meaning and Interpre-
tation. Association for Computational Linguistics.
Mstislav Maslennikov and Tat-Seng Chua. 2007. A
multi-resolution framework for information extrac-
tion from free text. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.
Maxima. 2014. Maxima, a computer algebra system.
version 5.32.1.
280
Anirban Mukherjee and Utpal Garain. 2008. A review
of methods for automatic understanding of natural
language mathematical problems. Artificial Intelli-
gence Review, 29(2).
Jorge Nocedal and Stephen Wright. 2006. Numeri-
cal optimization, series in operations research and
financial engineering. Springer, New York.
Roi Reichart and Regina Barzilay. 2012. Multi-event
extraction guided by global constraints. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the Conference on Knowledge Dis-
covery and Data Mining.
Cynthia Thompson and Raymond Mooney. 2003.
Acquiring word-meaning mappings for natural lan-
guage interfaces. Journal of Artificial Intelligence
Research, 18(1).
Adam Vogel and Dan Jurafsky. 2010. Learning to
follow navigational directions. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics.
Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Annual Meeting
of the North American Chapter of the Association of
Computational Linguistics. Association for Compu-
tational Linguistics.
John Zelle and Raymond Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the Conference on Ar-
tificial Intelligence.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured
classification with probabilistic categorial gram-
mars. In Proceedings of the Conference on Uncer-
tainty in Artificial Intelligence.
Luke Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of the Joint Confer-
ence of the Association for Computational Linguis-
tics and International Joint Conference on Natural
Language Processing.
281
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1437?1447,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Context-dependent Semantic Parsing for Time Expressions
Kenton Lee
?
, Yoav Artzi
?
, Jesse Dodge
??
, and Luke Zettlemoyer
?
?
Computer Science & Engineering, University of Washington, Seattle, WA
{kentonl, yoav, lsz}@cs.washington.edu
?
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA
jessed@cs.cmu.edu
Abstract
We present an approach for learning
context-dependent semantic parsers to
identify and interpret time expressions.
We use a Combinatory Categorial Gram-
mar to construct compositional meaning
representations, while considering contex-
tual cues, such as the document creation
time and the tense of the governing verb,
to compute the final time values. Exper-
iments on benchmark datasets show that
our approach outperforms previous state-
of-the-art systems, with error reductions of
13% to 21% in end-to-end performance.
1 Introduction
Time expressions present a number of challenges
for language understanding systems. They have
rich, compositional structure (e.g., ?2nd Friday of
July?), can be easily confused with non-temporal
phrases (e.g., the word ?May? can be a month
name or a verb), and can vary in meaning in dif-
ferent linguistic contexts (e.g., the word ?Friday?
refers to different dates in the sentences ?We met
on Friday? and ?We will meet on Friday?). Recov-
ering the meaning of time expressions is therefore
challenging, but provides opportunities to study
context-dependent language use. In this paper, we
present the first context-dependent semantic pars-
ing approach for learning to identify and interpret
time expressions, addressing all three challenges.
Existing state-of-the-art methods use hand-
engineered rules for reasoning about time expres-
sions (Str?otgen and Gertz, 2013). This includes
both detection, identifying a phrase as a time ex-
pression, and resolution, mapping such a phrase
into a standardized time value. While rule-based
approaches provide a natural way to express ex-
pert knowledge, it is relatively difficult to en-
?
Work conducted at the University of Washington.
code preferences between similar competing hy-
potheses and provide prediction confidence. Re-
cently, methods for learning probabilistic seman-
tic parsers have been shown to address such limi-
tations (Angeli et al, 2012; Angeli and Uszkoreit,
2013). However, these approaches do not account
for any surrounding linguistic context and were
mainly evaluated with gold standard mentions.
We propose to use a context-dependent se-
mantic parser for both detection and resolution
of time expressions. For both tasks, we make
use of a hand-engineered Combinatory Catego-
rial Grammar (CCG) to construct a set of mean-
ing representations that identify the time being
described. For example, this grammar maps the
phrase ?2nd Friday of July? to the meaning repre-
sentation intersect(nth(2 , friday), july), which
encodes the set of all such days. Detection is then
performed with a binary classifier to prune the set
of text spans that can be parsed with the gram-
mar (e.g., to tell that ?born in 2000? has a time
expression but ?a 2000 piece puzzle? does not).
For resolution, we consider mentions sequentially
and use a log-linear model to select the most likely
meaning for each. This choice depends on contex-
tual cues such as previous time expressions and
the tense of the governing verb (e.g., as required
to correctly resolve cases like ?We should meet on
the 2nd Friday of July?).
Such an approach provides a good balance be-
tween hand engineering and learning. For the rel-
atively closed-class time expressions, we demon-
strate that it is possible to engineer a high quality
CCG lexicon. We take a data-driven approach for
grammar design, preferring a grammar with high
coverage even if it results in parsing ambiguities.
We then learn a model to accurately select between
competing parses and incorporate signals from the
surrounding context, both more difficult to model
with deterministic rules.
For both problems, we learn from TimeML an-
1437
notations (Pustejovsky et al, 2005), which mark
mentions and the specific times they reference.
Training the detector is a supervised learning
problem, but resolution is more challenging, re-
quiring us to reason about latent parsing and
context-dependent decisions.
We evaluate performance in two domains: the
TempEval-3 corpus of newswire text (Uzzaman et
al., 2013) and the WikiWars corpus of Wikipedia
history articles (Mazur and Dale, 2010). On these
benchmark datasets, we present new state-of-the-
art results, with error reductions of up to 28% for
the detection task and 21% for the end-to-end task.
2 Formal Overview
Time Expressions We follow the TIMEX3 stan-
dard (Pustejovsky et al, 2005) for defining time
expressions within documents. Let a document
D = ?w
1
, . . . , w
n
? be a sequence of n words w
i
and a mention m = (i, j) indicate start and end
indices for a phrase ?w
i
, . . . , w
j
? in D. Define
a time expression e = (t, v) to include both a
temporal type t and value v.
1
The temporal type
t ? {Date, Time, Duration, Set} can take one of
four possible values, indicating if the expression
e is a date (e.g., ?January 10, 2014?), time (e.g.,
?11:59 pm?), duration (e.g., ?6 months?), or set
(e.g., ?every year?). The value v is an extension
of the ISO 8601 standard, which encodes the time
that mentionm refers to in the context provided by
document D. For example, in a document written
on Tuesday, January 7, 2014, ?Friday,? ?three days
later,? and ?January 10th? would all resolve to the
value 2014-01-10. The time values are similarly
defined for a wide range of expressions, such as
underspecified dates (e.g., XXXX-01-10 for ?Ja-
nunary 10th? when the year is not inferable from
context) and durations (P2D for ?two days?).
Tasks Our goal is to find all time expressions in
an input document. We divide the problem into
two parts: detection and resolution. The detection
problem is to take an input documentD and output
a mention set M = {m
i
| i = 1 . . . n} of phrases
in D that describe time expressions. The resolu-
tion problem (often also called normalization) is,
given a document D and a set of mentions M , to
1
Time expressions also have optional modifier values
for non-TIMEX properties (e.g., the modifier would contain
EARLY for the phrase ?early march?). We do recover these
modifiers but omit them from the discussion since they are
not part of the official evaluation metrics.
map each m ? M to the referred time expression
e. This paper addresses both of these tasks.
Approach We learn separate, but related, mod-
els for detection and resolution. For both tasks, we
define the space of possible compositional mean-
ing representations Z , where each z ? Z defines
a unique time expression e. We use a log-linear
CCG (Steedman, 1996; Clark and Curran, 2007)
to rank possible meanings z ? Z for each men-
tion m in a document D, as described in Sec-
tion 4. Both detection (Section 5) and resolution
(Section 6) rely on the semantic parser to identify
likely mentions and resolve them within context.
For learning we assume access to TimeML data
containing documents labeled with time expres-
sions. Each document D has a set {(m
i
, e
i
)|i =
1 . . . n}, where each mention m
i
marks a phrase
that resolves to the time expression e
i
.
Evaluation We evaluate performance (Sec-
tion 8) for both newswire text and Wikipedia
articles. We compare to the state-of-the-art
systems for end-to-end resolution (Str?otgen and
Gertz, 2013) and resolution given gold men-
tions (Bethard, 2013b), both of which do not use
any machine learning techniques.
3 Representing Time
We use simply typed lambda calculus to represent
time expressions. Our representation draws heav-
ily from the representation proposed by Angeli et
al. (2012), who introduced semantic parsing for
this task. There are five primitive types: duration
d, sequence s, range r, approximate reference a,
and numeral n, as described below. Table 1 lists
the available constants for each type.
Duration A period of time. Each duration is a
multiple of one of a closed set of possible base
durations (e.g., hour, day, and quarter), which
we refer to as its granularity. Table 1 includes the
complete set of base durations used.
Range A specific interval of time, following an
interval-based theory of time (Allen, 1981). The
interval length is one of the base durations, which
is the granularity of the range. Given two ranges
R and R
?
, we say that R ? R
?
if the endpoints of
R lie on or within R
?
.
Sequence A set of ranges with identical granu-
larity. The granularity of the sequence is that of
its members. For example, thursday , which has a
1438
Type Primitive Constants
Duration second , minute , hour , timeofday , day ,
month , season , quarter , weekend ,
week , year , decade , century , temp d
Sequence monday , tuesday , wednesday ,
thursday , friday , saturday , sunday ,
january , february , march , april ,
may , june , july , august , september ,
october , november , december , winter ,
spring , summer , fall , night , morning ,
afternoon , evening
Range ref time
Approximate
reference
present , future , past , unknown
Numeral 1 , 2 , 3 , 1999 , 2000 , 2001 , . . .
Table 1: The types and primitive logical constants sup-
ported by the logical language for time.
day granularity, denotes the set of all day-granular
ranges enclosing specific Thursdays. Given a
range R and sequence S, we say that R ? S if
R is a member of S. Given two sequences S and
S
?
we say that S ? S
?
if R ? S implies R ? S
?
.
Approximate Reference An approximate time
relative to the reference time. For example, past
and future. To handle mentions such as ?a while,?
we add the constant unknown .
Numeral An integer, for example, 5 or 1990 .
Numerals are used to denote specific ranges, such
as the year 2001, or to modify a duration?s length.
Functions We also allow for functional types,
for example ?s, r? is assigned to a function that
maps from sequences to ranges. Table 2 lists all
supported functions with example mentions.
Context Dependent Constants To mark places
where context-dependent choices will need to be
made during resolution, we use two placeholder
constants. First, ref time denotes the mention ref-
erence time, which is later set to either the docu-
ment time or a previously resolved mention. Sec-
ond, temp d is used in the shift function to deter-
mine its return granularity, as described in Table 2,
and is later replaced with the granularity of either
the first or second argument of the enclosing shift
function. Section 4.3 describes how these deci-
sions are made.
4 Parsing Time Expressions
We define a three-step derivation to resolve men-
tions to their TIMEX3 value. First, we use a CCG
to generate an initial logical form for the mention.
Next, we apply a set of operations that modify the
one week ago
C N NP\NP
1 week ?x.shift(ref time,?1 ? x, temp d)
N/N
?x.1 ? x
>
N
1 ? week
NP
1 ? week
<
NP
shift(ref time,?1 ? 1 ? week , temp d)
Figure 1: A CCG parse tree for the mention ?one week
ago.? The tree includes forward (>) and backward (<)
application, as well as two type-shifting operations
initial logical form, as appropriate for its context.
Finally, the logical form is resolved to a TIMEX3
value using a deterministic process.
4.1 Combinatory Categorial Grammars
CCG is a linguistically motivated categorial for-
malism for modeling a wide range of language
phenomena (Steedman, 1996; Steedman, 2000). A
CCG is defined by a lexicon and a set of combina-
tors. The lexicon pairs words with categories and
the combinators define how to combine categories
to create complete parse trees.
For example, Figure 1 shows a CCG parse tree
for the phrase ?one week ago.? The parse tree is
read top to bottom, starting from assigning cate-
gories to words using the lexicon. The lexical en-
try ago ` NP\NP : ?x.shift(ref time,?1 ?
x, temp d) for the word ?ago? pairs it with a cate-
gory that has syntactic type NP\NP and seman-
tics ?x.shift(ref time,?1 ? x, temp d). Each
intermediate parse node is then constructed by ap-
plying one of a small set of binary or unary opera-
tions (Steedman, 1996; Steedman, 2000), which
modify both the syntax and semantics. We use
backward (<) and forward (>) application and
several unary type-shifting rules to handle number
combinations. For example, in Figure 1 the cate-
gory of the span ?one week? is combined with the
category of ?ago? using backward application (<).
Parsing concludes with a logical form representing
the meaning of the complete mention.
Hand Engineered Lexicon To parse time ex-
pressions, we use a CCG lexicon that includes 287
manually designed entries, along with automati-
cally generated entries such as numbers and com-
mon formats of dates and times. Figure 2 shows
example entries from our lexicon.
1439
Function Description Example
Operations on durations.
?
?n,?d,d??
Given a duration D and a numeral N , return a duration D
?
that is N times longer than D.
?after three days of questioning?
3? day
some
?d,d?
Given a durationD, returnsD
?
, s.t. D
?
is the result ofD?n
for some n > 1.
?he left for a few days?
some(day)
seq
?d,s?
Given a duration D, generate a sequence S, s.t. S includes
all ranges of type D.
?went to last year?s event?
previous(seq(year), ref time)
Operations for extracting a specific range from a sequence.
this
?s,?r,r??
Given a sequence S and a range R, returns the range R
?
?
S, s.t. there exists a range R
??
where R ? R
??
and R
?
?
R
??
, and the length of R
??
is minimal.
?a meeting this friday?
this(friday, ref time)
next
?s,?r,r??
previous
?s,?r,r??
Given a sequenceS and a rangeR, returns the range R
?
? S
that is the one after/before this(S,R).
?arriving next month?
next(seq(month), ref time)
nearest forward
?s,?r,r??
nearest backward
?s,?r,r??
Given a sequenceS and a rangeR, returns the range R
?
? S
that is closest to R in the forward/backward direction.
?during the coming weekend?
nearest forward(seq(weekend), ref time)
Operations for sequences.
nth
?n,?s,?s,s???
nth
?n,?s,s??
Given a number N , a sequence S and a sequence S
?
, returns
a sequence S
??
? S s.t. for each Q ? S
??
there exists
P ? S
?
and Q is the N -th entry in S that is a sub-interval
of P . For the two-argument version, we use heuristics to
infer the third argument by determining a sequence of higher
granularity that is likely to contain the second argument.
?until the second quarter of the year?
nth(2 , seq(quarter), seq(year))
intersect
?s,?s,s??
Given sequences S, S
?
, where the duration of entries in S is
shorter than these in S
?
, return a sequence S
??
? S, where
for each R ? S
??
there exists R
?
? S
?
s.t. R ? R
?
.
?starts on June 28?
intersect(june,nth(28 , seq(day),
seq(month)))
shift
?r,?d,?d,r???
Given a range R, a duration D, and a duration G, return the
range R
?
, s.t. the starting point of R
?
is moved by the length
of D. R
?
is converted to represent a range of granularity G
by expanding if G has larger granularity, and is undefined if
G has smaller granularity.
?a week ago, we went home?
shift(ref time,?1 ? 1 ? week , temp d)
Operations on numbers.
?
?n,?n,n??
Given two numerals, N
?
and N
??
, returns a numeral N
???
representing their product N
?
?N
??
.
?the battle lasted for one hundred days?
1 ? 100 ? day
+
?n,?n,n??
Given two numerals, N
?
and N
??
, returns a numeral N
???
representing their sum N
?
+ N
??
.
?open twenty four hours?
(20 + 4)? hour
Operations to mark sequences for specific TIMEX3 type annotations.
every
?s,s?
Given a sequence S, returns a sequence with SET temporal
type.
?one dose each day?
every(seq(day))
bc
?s,s?
Convert a year to BC.
?during five hundred BC?
bc(nth(500 , seq(year)))
Table 2: Functional constants used to build logical expressions for representing time.
Manually Designed Entries:
several ` NP/N : ?x.some(x)
this ` NP/N : ?x.this(x, ref time)
each ` NP/N : ?x.every(x)
before ` N\NP/NP :
?x.?y.shift(x,?1 ? y, temp d)
year ` N : year
wednesday ` N : wednesday
?20s ` N : nth(192 , seq(decade))
yesterday ` N : shift(ref time,?1 ? day , temp d)
Automatically Generated Entries:
1992 ` N : nth(1992 , seq(year))
nineteen ninety two ` N : nth(1992 , seq(year))
09:30 ` N : intersect(nth(10 , seq(hour), seq(day)),
nth(31 , seq(minute), seq(hour)))
3rd ` N\N :
?x.intersect(x,nth(3 , seq(day), seq(month)))
Figure 2: Example lexical entries.
4.2 Context-dependent Operations
To correctly resolve mentions to TIMEX3 val-
ues, the system must account for contextual in-
formation from various sources, including previ-
ous mentions in the document, the document cre-
ation time, and the sentence containing the men-
tion. We consider three types of context opera-
tions, each takes as input a logical form z
?
, mod-
ifies it and returns a new logical form z. Each
context-dependent parse y specifies one operator
of each type, which are applied to the logical form
constructed by the CCG grammar, to produce the
final, context-dependent logical form LF(y).
Reference Time Resolution The logical con-
stant ref time is replaced by either dct , repre-
senting the document creation time, or last range,
the last r-typed mention resolved in the document.
For example, consider the mention ?the follow-
ing year?, which is represented using the logical
form next(seq(year), ref time). Within the sen-
tence ?1998 was colder than the following year?,
the resolution of ?the following year? depends on
the previous mention ?1998?. In contrast, in ?The
following year will be warmer?, its resolution de-
pends on the document creation time.
1440
Directionality Resolution If z
?
is s-typed
we modify it to nearest forward(z
?
, ref time),
nearest backward(z
?
, ref time), or z
?
. For ex-
ample, given the sentence ?. . . will be launched
in april?, the mention ?april?, and its logi-
cal form april , we would like to resolve it to
the coming April, and therefore modify it to
nearest forward(april , ref time).
Shifting Granularity Every occurrence of the
logical constant temp d , which is used as an ar-
gument to the function shift (see Table 2), is re-
placed with the granularity of either the first argu-
ment, the origin of the shift, or the second argu-
ment, the delta of the shift. This determines the
final granularity of the output. For example, if the
reference time is 2002-01, the mention ?two years
earlier? would resolve to either a month (since the
reference time is of month granularity) or a year
(since the delta is of year granularity).
4.3 Resolving Logical Forms
For a context-dependent parse y, we compute the
TIMEX3 value TM(y) from the logical form z =
LF(y) with a deterministic step that performs a
single traversal of z. Each primitive logical con-
stant from Table 1 contributes to setting part of the
TIMEX3 value (for example, specifying the day of
the week) and the functional constants in Table 2
dictate transformations on the TIMEX3 values (for
example, shifting forward or backward in time).
2
5 Detection
The detection problem is to take an input docu-
ment D and output a mention set M = {m
i
| i =
1, . . . , n}, where each mention m
i
indexes a spe-
cific phrase in D that delimits a time expression.
Algorithm The detection algorithm considers
all phrases that our CCG grammar ? (Section 4)
can parse, uses a learned classifier to further filter
this set, and finally resolves conflicts between any
overlapping predictions. We use a CKY algorithm
to efficiently determine which phrases the CCG
grammar can parse and only allow logical forms
for which there exists some context in which they
would produce a valid time expression, e.g. rul-
ing out intersect(monday , tuesday). Finally, we
build the set M of non-overlapping mentions us-
ing a step similar to non-maximum suppression:
2
The full details are beyond the scope of this paper, but an
implementation is available on the author?s website.
the mentions are sorted by length (longest first)
and iteratively added to M , as long as they do not
overlap with any mention already in M .
Filtering Model Given a mention m, its docu-
ment D, a feature function ?, the CCG lexicon ?,
and feature weights ?, we use a logistic regression
model to define the probability distribution:
P (t|m,D; ?, ?) =
e
???(m,D,?)
1 + e
???(m,D,?)
where t indicates whether m is a time expression.
Features We use three types of indicator fea-
tures that test properties of the words in and
around the potential mention m.
Context tokens Indicate the presence of a set of
manually specified tokens near the mention. These
include quotations around the mention, the word
?old? after the mention, and prepositions of time
(such as ?in?, ?until?, and ?during?) before.
Part of speech Indicators that pair each word
with its part of speech, as assigned by the Stanford
tagger (Toutanova et al, 2003).
Lexical group Each lexical entry belongs to one
of thirteen manually defined lexical groups which
cluster entries that contribute to the final time ex-
pression similarly. These groups include numbers,
days of the week, months, seasons, etc. For each
group, we include a feature indicating whether the
parse includes a lexical entry from that group.
Determiner dependency Indicates the presence
of a determiner in the mention and whether its par-
ent in the dependency tree (generated by the Stan-
ford parser (de Marneffe et al, 2006)) also resides
within the mention.
Learning Finally, we construct the training data
by considering all spans that (1) the CCG tempo-
ral grammar can parse and (2) are not strict sub-
spans of an annotated mention. All spans that ex-
actly matched the gold labels are used as positive
examples and all others are negatives. Given this
relaxed data, we learn the feature weights ? with
L1-regularization. We set the probability thresh-
old for detecting a time expression by optimizing
the F1 score over the training data.
6 Resolution
The resolution problem is to, given a document D
and a set of mentions M , map each m ? M to
the correct time expression e. Section 4 defined
1441
the space of possible time expression that can be
constructed for an input mention m in the context
of a document D. In general, there will be many
different possible derivations, and we will learn a
model for selecting the best one.
Model Let y be a context-dependent CCG parse,
which includes a parse tree TR(y), a set of context
operations CNTX(y) applied to the logical form
at the root of the tree, a final context-dependent
logical form LF(y) and a TIMEX3 value TM(y).
Define ?(m,D, y) ? R
d
to be a d-dimensional
feature?vector representation and ? ? R
d
to be a
parameter vector. The probability of a parse y for
mention m and document D is:
P (y|m,D; ?,?) =
e
???(m,D,y)
?
y
?
e
???(m,D,y
?
)
The inference problem at test time requires find-
ing the best resolution by solving y
?
(m,D) =
arg max
y
P (y|m,D; ?,?), where the final output
TIMEX3 value is TM(y
?
(m,D)).
Inference We find the best context-dependent
parse y by enumeration, as follows. We first
parse the input mention m with a CKY-style algo-
rithm, following previous work (Zettlemoyer and
Collins, 2005). Due to the short length of time
expressions and the manually constructed lexicon,
we can perform exact inference. Given a parse,
we then enumerate all possible outcomes for the
context resolution operators. In practice, there are
never more than one hundred possibilities.
Features The resolution features test properties
of the linguistic context surrounding the mention
m, relative to the context-dependent CCG parse y.
Governor verb We define the governor verb to be
the nearest ancestor verb in the dependency parse
of any token in m. We include features indicat-
ing the concatenation of the part-of-speech of the
governor verb, its auxiliary verb if present, and
the selected direction resolution operator (see Sec-
tion 4.2). This feature helps to distinguish ?They
met on Friday? from ?They will meet on Friday.?
Temporal offset If the final logical form LF (y)
is a range, we define t to be the time difference
between TM(y) and the reference time. For ex-
ample, if the reference time is 2000-01-10 and the
mention resolves to 2000-01-01, then t is -9 days.
This feature indicates one of eleven bucketed val-
ues for t, including same day, less than a week,
less than a month, etc. It allows the model to en-
code the likely temporal progression of a narrative.
This feature is ignored if the granularity of TM(y)
or the reference time is greater than a year.
Shift granularity The logical constant shift (Ta-
ble 2) takes three arguments: the origin (range),
the delta (duration), and the output granularity
(duration). This indicator feature is the concate-
nation of each argument?s granularity for every
shift in LF (y). It allows the model to determine
whether ?a year ago? refers to a year or a day.
Reference type Let r denote whether the refer-
ence time is the document creation time dct or the
last range last range. Let g
l
and g
r
denote the
granularities of LF (y) and the reference time, re-
spectively. We include features indicating the con-
catenations: r+g
l
, r+g
r
, and r+g
l
+g
r
. Addition-
ally, we include features indicating the concatena-
tion of r with each lexical entry used in the parse
TR(y). These features allow the model to encode
preferences in selecting the correct reference time.
Fine-grained type These features indicate the
fine-grained type of TM(y), such as day of the
month or week of the year. We also include a fea-
ture indicating the concatenation of each of these
features with the direction resolution operator that
was used. These features allow the model to repre-
sent, for example, that minutes of the year are less
likely than days of the month.
Intersections These features indicate the concate-
nation of the granularities of any two sequences
that appear as arguments to an intersect constant.
Learning To estimate the model parameters ?
we assume access to a set of training examples
{(m
i
, d
i
, e
i
) : i = 1, . . . , n}, where each mention
m
i
is paired with a document d
i
and a TIMEX3
value e
i
. We use the AdaGrad algorithm (Duchi
et al, 2011) to optimize the conditional, marginal
log-likelihood of the data. For each mention, we
marginalize over all possible context-dependent
parses, using the predictions from the model on the
previous gold mentions to fill in missing context,
where necessary. After parameter estimation, we
set a probability threshold for retaining a resolved
time expression by optimizing value F1 (see Sec-
tion 8) over the training data.
7 Related Work
Semantic parsers map sentences to logical repre-
sentations of their underlying meaning, e.g., Zelle
1442
and Mooney (1996), Zettlemoyer and Collins
(2005), and Wong and Mooney (2007). Re-
cently, research in this area has focused on learn-
ing for various forms of relatively weak but eas-
ily gathered supervision. This includes learn-
ing from question-answer pairs (Clarke et al,
2010; Liang et al, 2011; Kwiatkowski et al,
2013), from conversational logs (Artzi and Zettle-
moyer, 2011), with distant supervision (Krish-
namurthy and Mitchell, 2012; Cai and Yates,
2013), and from sentences paired with system be-
havior (Goldwasser and Roth, 2011; Chen and
Mooney, 2011; Artzi and Zettlemoyer, 2013b).
Recently, Angeli et al introduced the idea of
learning semantic parsers to resolve time expres-
sions (Angeli et al, 2012) and showed that the ap-
proach can generalize to multiple languages (An-
geli and Uszkoreit, 2013). Similarly, Bethard
demonstrated that a hand-engineered semantic
parser is also effective (Bethard, 2013b). How-
ever, these approaches did not use the semantic
parser for detection and did not model linguistic
context during resolution.
We build on a number of existing algorithmic
ideas, including using CCGs to build meaning
representations (Zettlemoyer and Collins, 2005;
Zettlemoyer and Collins, 2007; Kwiatkowski et
al., 2010; Kwiatkowski et al, 2011), building
derivations to transform the output of the CCG
parser based on context (Zettlemoyer and Collins,
2009), and using weakly supervised parameter up-
dates (Artzi and Zettlemoyer, 2011; Artzi and
Zettlemoyer, 2013b). However, we are the first to
use a semantic parsing grammar within a mention
detection algorithm, thereby avoiding the need to
represent the meaning of complete sentences, and
the first to develop a context-dependent model for
semantic parsing of time expressions.
Time expressions have been extensively stud-
ied as part of the TimeEx task, including 9 teams
who competed in the 2013 TempEval-3 com-
petition (Uzzaman et al, 2013). This line of
work builds on ideas from TimeBank (Puste-
jovsky et al, 2003) and a number of different
formal models for temporal reasoning, e.g. Allen
(1983), Moens and Steedman (1988). In 2013,
HeidelTime (Str?otgen and Gertz, 2013) was the
top performing system. It used deterministic rules
defined over regular expressions to perform both
detection and resolution, and will provide a com-
parison system for our evaluation in Section 9. In
Corpus Doc. Token TimeEx
TempEval-3 (Dev) 256 95,391 1,822
TempEval-3 (Test) 20 6,375 138
WikiWars (Dev) 17 98,746 2,228
WikiWars (Test) 5 19,052 363
Figure 3: Corpus statistics.
general, many different rule-based systems, e.g.
NavyTime (Chambers, 2013) and SUTime (Chang
and Manning, 2012), and learning systems, e.g.
ClearTK (Bethard, 2013a) and MANTime (Filan-
nino et al, 2013), did well for detection. How-
ever, rule-based approaches dominated in resolu-
tion; none of the top performers attempted to learn
to do resolution. Our approach is a hybrid of rule
based and learning, by using latent-variable learn-
ing techniques to estimate CCG parsing and con-
text resolution models from the provided data.
8 Experimental Setup
Data We evaluate performance on the
TempEval-3 (Uzzaman et al, 2013) and Wiki-
Wars (Mazur and Dale, 2010) datasets. Figure 3
shows summary statistics for both datasets. For
the TempEval-3 corpus, we use the given training
and testing set splits. Since the training set
has lower inter-annotator agreement than the
testing set (Uzzaman et al, 2013), we manually
corrected all of the mistakes we found in the
training data.
3
The original training set is denoted
Dev* and the corrected Dev. We report (1)
cross-validation development results on Dev*, (2)
cross-validation development and ablation results
for Dev, and (3) held-out test results after training
with Dev. For WikiWars, we randomly assigned
the data to include 17 training documents (2,228
time expressions) and 5 test documents (363 time
expressions). We use cross-validation on the train-
ing data for development. All cross-validation
experiments used 10 folds.
Implementation Our system was implemented
using the open source University of Washington
Semantic Parsing Framework (Artzi and Zettle-
moyer, 2013a). We used LIBLINEAR (Fan et al,
2008) to learn the detection model.
Parameter Settings We use the same set of pa-
rameters for both datasets, chosen based on devel-
opment experiments. For detection, we set the reg-
ularization parameter to 10 with a stopping crite-
3
We modified the annotations for 18% of the mentions.
This relabeled corpus is available on the author?s website.
1443
System
Strict Detection Relaxed Detection Type Res. Value Resolution
Pre. Rec. F1 Pre. Rec. F1 Acc. F1 Acc. Pre. Rec. F1
D
e
v
*
This work 84.6 83.4 84.0 92.8 91.5 92.1 94.6 87.1 84.0 77.9 76.8 77.4
HeidelTime 83.7 83.4 83.5 91.7 91.4 91.6 95.0 87.0 84.1 77.1 76.8 77.0
D
e
v
This work 92.7 89.6 91.1 97.4 94.1 95.7 97.1 92.9 91.5 89.1 86.1 87.6
Context ablation 92.7 89.3 91.0 97.5 93.9 95.7 97.1 92.9 89.8 87.6 84.3 85.9
HeidelTime 90.2 84.8 87.4 96.5 90.7 93.5 96.1 89.9 88.4 85.3 80.2 82.7
T
e
s
t
This work 86.1 80.4 83.1 94.6 88.4 91.4 93.4 85.4 90.2 85.3 79.7 82.4
HeidelTime 83.9 79.0 81.3 93.1 87.7 90.3 90.9 82.1 86.0 80.1 75.4 77.7
NavyTime 78.7 80.4 79.6 89.4 91.3 90.3 88.9 80.3 78.6 70.3 71.8 71.0
ClearTK 85.9 79.7 82.7 93.8 87.0 90.2 93.3 84.2 71.7 67.3 62.4 64.7
Figure 4: TempEval-3 development and test results, compared to the top systems in the shared task.
System
Strict Detection Relaxed Detection Value Resolution
Pre. Rec. F1 Pre. Rec. F1 Acc. Pre. Rec. F1
D
e
v
This work 90.3 83.0 86.5 98.1 90.1 93.9 87.6 85.9 78.9 82.3
Context ablation 90.9 80.1 85.2 98.2 86.5 92.0 68.5 67.3 59.3 63.0
HeidelTime 86.0 75.3 80.3 95.4 83.5 89.0 90.5 86.3 75.6 80.6
T
e
s
t
This work 87.7 78.8 83.0 97.6 87.6 92.3 84.6 82.5 74.1 78.1
HeidelTime 85.2 79.3 82.1 92.6 86.2 89.3 83.7 77.5 72.1 74.7
Figure 5: WikiWars development and test results.
rion of 0.01. For resolution, we set the learning
rate to 0.25 and ran AdaGrad for 5 iterations. All
features are initialized to have zero weights.
Evaluation Metrics We use the official
TempEval-3 scoring script and report the standard
metrics. We report detection precision, recall and
F1 with relaxed and strict metrics; a gold mention
is considered detected for the relaxed metric if
any of the output candidates overlap with it and is
detected for the strict metric if the extent of any
output candidates matches exactly. For resolution,
we report value accuracy, measuring correctness
of time expressions detected according to the
relaxed metric. We also report value precision,
recall, and F1, which score an expression as
correct if it is both correctly detected (relaxed)
and resolved. For end-to-end performance, value
F1 is the primary metric. Finally, we report
accuracy and F1 for temporal types, as defined in
Section 2, for the TempEval dataset (WikiWars
does not include type labels).
Comparison Systems We compare our system
primarily to HeidelTime (Str?otgen and Gertz,
2013), which is state of the art in the end-to-
end task. For the TempEval-3 dataset, we also
compare to two other strong participants of the
shared task. These include NavyTime (Chambers,
2013), which had the top relaxed detection score,
and ClearTK (Bethard, 2013a), which had the top
strict detection score and type F1 score. We also
include a comparison with Bethard?s synchronous
System Dev* Dev Test
This work 81.8 90.1 82.6
SCFG 77.0 81.6 78.9
Figure 6: TempEval-3 gold mention value accuracy.
context free grammar (SCFG) (Bethard, 2013b),
which is state-of-the-art in the task of resolution
with gold mention boundaries.
9 Results
End-to-end results Figure 4 shows develop-
ment and test results for TempEval-3. Figure 5
shows these numbers for WikiWars. In both
datasets, we achieve state-of-the-art test scores.
For detection, we show up to 3-point improve-
ments in strict and relaxed F1 scores. These num-
bers outperform all systems participating in the
shared task, which used a variety of techniques in-
cluding hand-engineered rules, CRF tagging mod-
els, and SVMs. For resolution, we show up to
4-point improvements in the value F1 score, also
outperforming participating systems, all of which
used hand-engineered rules for resolution.
Gold Mentions Figure 6 reports development
and test results with gold mentions.
4
Our approach
outperforms the state of the art, SCFG (Bethard,
2013b), which also used a hand engineered gram-
mar, but did not use machine learning techniques.
4
These numbers vary slightly from those reported; we did
not count the document creation times as mentions.
1444
65 70 75 80 85 90
84
86
88
90
92
94
V
a
l
u
e
P
r
e
c
i
s
i
o
n
(
%
)
65 70 75 80 85 90
84
86
88
90
92
94
This work
HeidelTime
TempEval-3 Dev
WikiWars Dev
Value Recall (%)
Figure 7: Value precision vs. recall for 10-fold cross
validation on TempEval-3 Dev and WikiWars Dev.
Precision vs. Recall Our probabilistic model
of time expression resolution allows us to eas-
ily tradeoff precision and recall for end-to-end
performance by varying the resolution probability
threshold. Figure 7 shows the precision vs. recall
of the resolved values from 10-fold cross valida-
tion of TempEval-3 Dev and WikiWars Dev. We
are able to achieve precision at or above 90% with
reasonable recall, nearly 70% for WikiWars and
over 85% for TempEval-3.
Ablation Study Figures 4-5 also show compar-
isons for our system with no context. We ablate
the ability to refer to the context during resolution
by removing contextual information from the res-
olution features and only allowing the document
creation time to be the reference time.
We see an interesting asymmetry in the effect of
modeling context across the two domains. We find
that context is much more important in WikiWars
(19 point difference) than in TempEval (2 point
difference). This result reaffirms the difference in
domains that Str?otgen and Gertz (2012) noted dur-
ing the development of HeidelTime: history arti-
cles have narrative structure that moves back and
forth through time while newspaper text typically
describes events happening near the document cre-
ation time. This difference helps us to understand
why previous learning systems have been able to
ignore context and perform well on newswire text.
Error Analysis To investigate the source of er-
ror, we compute oracle results for resolving gold
mentions over the TempEval-3 Dev dataset. We
found that our system produces a correct candidate
derivation for 96% of the mentions.
We also manually categorized all resolution
errors for end-to-end performance with 10-fold
cross validation of the TempEval-3 Dev dataset,
Error description %
Wrong directionality context operator 34.6
Wrong reference time context operator 15.7
Wrong shifting granularity context operator 14.4
Requires joint reasoning with events 9.2
Cascading error due to wrong detection 7.8
CCG parse error 2.0
Other error 16.3
Figure 8: Resolution errors from 10-fold cross valida-
tion of the TempEval-3 Dev dataset.
shown in Figure 8. The lexicon allows for effec-
tive parsing, contributing to only 2% of the overall
errors. However, context is more challenging. The
three largest categories, responsible for 64.7% of
the errors, were incorrect use of the context oper-
ators. More expressive modeling will be required
to fully capture the complex pragmatics involved
in understanding time expressions.
10 Conclusion
We presented the first context-dependent semantic
parsing system to detect and resolve time expres-
sions. Both models used a Combinatory Catego-
rial Grammar (CCG) to construct a set of possible
temporal meaning representations. This grammar
defined the possible phrases for detection and the
inputs to a context-dependent reasoning step that
was used to construct the output time expression
during resolution. Experiments demonstrated that
our approach outperforms state-of-the-art systems.
In the future, we aim to develop joint models
for reasoning about events and time expressions,
including detection and resolution of temporal re-
lations. We are also interested in testing coverage
in new domains and investigating techniques for
semi-supervised learning and learning with noisy
data. We hypothesize that semantic parsing tech-
niques could help in all of these settings, provid-
ing a unified mechanism for compositional analy-
sis within temporal understanding problems.
Acknowledgments
The research was supported in part by
DARPA under the DEFT program through
the AFRL (FA8750-13-2-0019) and the CSSG
(N11AP20020), and the NSF (IIS-1115966, IIS-
1252835). The authors thank Nicholas FitzGerald,
Tom Kwiatkowski, and Mark Yatskar for helpful
discussions, and the anonymous reviewers for
helpful comments.
1445
References
James F. Allen. 1981. An interval-based representa-
tion of temporal knowledge. In Proceedings of the
7th International Joint Conference on Artificial In-
telligence.
James F Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Gabor Angeli and Jakob Uszkoreit. 2013. Language-
independent discriminative parsing of temporal ex-
pressions. In Proceedings of the Conference of the
Association of Computational Linguistics.
Gabor Angeli, Christopher D Manning, and Daniel Ju-
rafsky. 2012. Parsing time: Learning to interpret
time expressions. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics.
Y. Artzi and L.S. Zettlemoyer. 2011. Bootstrapping se-
mantic parsers from conversations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Y. Artzi and L.S. Zettlemoyer. 2013a. UW SPF: The
University of Washington Semantic Parsing Frame-
work.
Y. Artzi and L.S. Zettlemoyer. 2013b. Weakly super-
vised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Steven Bethard. 2013a. Cleartk-timeml: A minimalist
approach to tempeval 2013. In Second Joint Confer-
ence on Lexical and Computational Semantics.
Steven Bethard. 2013b. A synchronous context free
grammar for time normalization. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Q. Cai and A. Yates. 2013. Semantic parsing free-
base: Towards open-domain semantic parsing. In
Joint Conference on Lexical and Computational Se-
mantics: Proceedings of the Main Conference and
the Shared Task: Semantic Textual Similarity.
Nathanael Chambers. 2013. Navytime: Event and
time ordering from raw text. In Second Joint Con-
ference on Lexical and Computational Semantics.
Angel X Chang and Christopher Manning. 2012. Su-
time: A library for recognizing and normalizing time
expressions. In Proceedings of the 8th International
Conference on Language Resources and Evaluation.
D.L. Chen and R.J. Mooney. 2011. Learning to in-
terpret natural language navigation instructions from
observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
S. Clark and J. R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?552.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s re-
sponse. In Proceedings of the Conference on Com-
putational Natural Language Learning.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Michele Filannino, Gavin Brown, and Goran Nenadic.
2013. Mantime: Temporal expression identification
and normalization in the tempeval-3 challenge. In
Second Joint Conference on Lexical and Computa-
tional Semantics.
D. Goldwasser and D. Roth. 2011. Learning from
natural instructions. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence.
J. Krishnamurthy and T. Mitchell. 2012. Weakly su-
pervised training of semantic parsers. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.
T. Kwiatkowski, L.S. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order uni-
fication. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
T. Kwiatkowski, L.S. Zettlemoyer, S. Goldwater, and
M. Steedman. 2011. Lexical Generalization in CCG
Grammar Induction for Semantic Parsing. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing.
P. Liang, M.I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Proceedings of the Conference of the Association for
Computational Linguistics.
1446
Pawet Mazur and Robert Dale. 2010. Wikiwars: a new
corpus for research on temporal expressions. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational lin-
guistics, 14(2):15?28.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003. The timebank corpus. In Corpus
linguistics.
James Pustejovsky, Bob Ingria, Roser Sauri, Jose Cas-
tano, Jessica Littman, Rob Gaizauskas, Andrea Set-
zer, Graham Katz, and Inderjeet Mani. 2005. The
specification language timeml. The language of
time: A reader, pages 545?557.
M. Steedman. 1996. Surface Structure and Interpreta-
tion. The MIT Press.
M. Steedman. 2000. The Syntactic Process. The MIT
Press.
Jannik Str?otgen and Michael Gertz. 2012. Tempo-
ral tagging on different domains: Challenges, strate-
gies, and gold standards. In Proceedings of the Eigth
International Conference on Language Resources
and Evaluation.
Jannik Str?otgen and Michael Gertz. 2013. Multilin-
gual and cross-domain temporal tagging. Language
Resources and Evaluation, 47(2):269?298.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1.
N. Uzzaman, H. Llorens, L. Derczynski, M. Verhagen,
J. Allen, and J. Pustejovsky. 2013. Semeval-2013
task 1: Tempeval-3: Evaluating time expressions,
events, and temporal relations. In Proceedings of the
International Workshop on Semantic Evaluation.
Y.W. Wong and R.J. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the Conference
of the Association for Computational Linguistics.
J.M. Zelle and R.J. Mooney. 1996. Learning to
parse database queries using inductive logic pro-
gramming. In Proceedings of the National Confer-
ence on Artificial Intelligence.
L.S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Artifi-
cial Intelligence.
L.S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logi-
cal form. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
L.S. Zettlemoyer and M. Collins. 2009. Learning
context-dependent mappings from sentences to log-
ical form. In Proceedings of the Joint Conference
of the Association for Computational Linguistics
and International Joint Conference on Natural Lan-
guage Processing.
1447
Transactions of the Association for Computational Linguistics, 1 (2013) 49?62. Action Editor: Jason Eisner.
Submitted 11/2012; Published 3/2013. c?2013 Association for Computational Linguistics.
Weakly Supervised Learning of Semantic Parsers
for Mapping Instructions to Actions
Yoav Artzi and Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{yoav,lsz}@cs.washington.edu
Abstract
The context in which language is used pro-
vides a strong signal for learning to recover
its meaning. In this paper, we show it can be
used within a grounded CCG semantic parsing
approach that learns a joint model of mean-
ing and context for interpreting and executing
natural language instructions, using various
types of weak supervision. The joint nature
provides crucial benefits by allowing situated
cues, such as the set of visible objects, to di-
rectly influence learning. It also enables algo-
rithms that learn while executing instructions,
for example by trying to replicate human ac-
tions. Experiments on a benchmark naviga-
tional dataset demonstrate strong performance
under differing forms of supervision, includ-
ing correctly executing 60% more instruction
sets relative to the previous state of the art.
1 Introduction
The context in which natural language is used pro-
vides a strong signal to reason about its meaning.
However, using such a signal to automatically learn
to understand unrestricted natural language remains
a challenging, unsolved problem.
For example, consider the instructions in Figure 1.
Correct interpretation requires us to solve many sub-
problems, such as resolving all referring expres-
sions to specific objects in the environment (includ-
ing, ?the corner? or ?the third intersection?), disam-
biguating word sense based on context (e.g., ?the
chair? could refer to a chair or sofa), and finding
executable action sequences that satisfy stated con-
straints (such as ?twice? or ?to face the blue hall?).
move forward twice to the chair
?a.move(a) ? dir(a, forward) ? len(a, 2) ?
to(a, ?x.chair(x))
at the corner turn left to face the blue hall
?a.pre(a, ?x.corner(x)) ? turn(a) ? dir(a, left) ?
post(a, front(you, ?x.blue(x) ? hall(x)))
move to the chair in the third intersection
?a.move(a) ? to(a, ?x.sofa(x)) ?
intersect(order(?y.junction(y), frontdist, 3), x)
Figure 1: A sample navigation instruction set, paired
with lambda-calculus meaning representations.
We must also understand implicit requests, for ex-
ample from the phrase ?at the corner,? that describe
goals to be achieved without specifying the specific
steps. Finally, to do all of this robustly without pro-
hibitive engineering effort, we need grounded learn-
ing approaches that jointly reason about meaning
and context to learn directly from their interplay,
with as little human intervention as possible.
Although many of these challenges have been
studied separately, as we will review in Section 3,
this paper represents, to the best of our knowledge,
the first attempt at a comprehensive model that ad-
dresses them all. Our approach induces a weighted
Combinatory Categorial Grammar (CCG), includ-
ing both the parameters of the linear model and a
CCG lexicon. To model complex instructional lan-
guage, we introduce a new semantic modeling ap-
proach that can represent a number of key linguistic
constructs that are common in spatial and instruc-
tional language. To learn from indirect supervision,
we define the notion of a validation function, for
example that tests the state of the agent after in-
terpreting an instruction. We then show how this
function can be used to drive online learning. For
49
that purpose, we adapt the loss-sensitive Perceptron
algorithm (Singh-Miller & Collins, 2007; Artzi &
Zettlemoyer, 2011) to use a validation function and
coarse-to-fine inference for lexical induction.
The joint nature of this approach provides crucial
benefits in that it allows situated cues, such as the
set of visible objects, to directly influence parsing
and learning. It also enables the model to be learned
while executing instructions, for example by trying
to replicate actions taken by humans. In particular,
we show that, given only a small seed lexicon and
a task-specific executor, we can induce high quality
models for interpreting complex instructions.
We evaluate the method on a benchmark naviga-
tional instructions dataset (MacMahon et al, 2006;
Chen & Mooney, 2011). Our joint approach suc-
cessfully completes 60% more instruction sets rel-
ative to the previous state of the art. We also re-
port experiments that vary supervision type, finding
that observing the final position of an instruction ex-
ecution is nearly as informative as observing the en-
tire path. Finally, we present improved results on a
new version of the MacMahon et al (2006) corpus,
which we filtered to include only executable instruc-
tions paired with correct traces.
2 Technical Overview
Task Let S be the set of possible environment
states and A be the set of possible actions. Given
a start state s ? S and a natural language instruc-
tion x, we aim to generate a sequence of actions
~a = ?a1, . . . , an?, with each ai ? A, that performs
the steps described in x.
For example, in the navigation domain (MacMa-
hon et al, 2006), S is a set of positions on a map.
Each state s = (x, y, o) is a triple, where x and y are
integer grid coordinates and o ? {0, 90, 180, 270} is
an orientation. Figure 2 shows an example map with
36 states; the ones we use in our experiments con-
tain an average of 141. The space of possible actions
A is {LEFT, RIGHT,MOVE, NULL}. Actions change
the state of the world according to a transition func-
tion T : A ? S ? S. In our navigation example,
moving forward can change the x or y coordinates
while turning changes the orientation o.
Model To map instructions to actions, we jointly
reason about linguistic meaning and action execu-
tion. We use a weighted CCG grammar to rank pos-
sible meanings z for each instruction x. Section 6
defines how to design such grammars for instruc-
tional language. Each logical form z is mapped to a
sequence of actions ~a with a deterministic executor,
as described in Section 7. The final grounded CCG
model, detailed in Section 6.3, jointly constructs and
scores z and ~a, allowing for robust situated reason-
ing during semantic interpretation.
Learning We assume access to a training set con-
taining n examples {(xi, si,Vi) : i = 1 . . . n}, each
containing a natural language sentence xi, a start
state si, and a validation function Vi. The validation
function Vi : A ? {0, 1} maps an action sequence
~a ? A to 1 if it?s correct according to available su-
pervision, or 0 otherwise. This training data contains
no direct evidence about the logical form zi for each
xi, or the grounded CCG analysis used to construct
zi. We model all these choices as latent variables.
We experiment with two validation functions. The
first, VD(~a), has access to an observable demonstra-
tion of the execution ~ai, a given ~a is valid iff ~a = ~ai.
The second, VSi (~a), only encodes the final state s?i
of the execution of x, therefore ~a is valid iff its final
state is s?i. Since numerous logical forms often ex-
ecute identically, both functions provide highly am-
biguous supervision.
Evaluation We evaluate task completion for sin-
gle instructions on a test set {(xi, si, s?i) : i =
1 . . . n}, where s?i is the final state of an oracle agent
following the execution of xi starting at state si. We
will also report accuracies for correctly interpreting
instruction sequences ~x, where a single error can
cause the entire sequence to fail. Finally, we report
accuracy on recovering correct logical forms zi on a
manually annotated subset of the test set.
3 Related Work
Our learning is inspired by the reinforcement learn-
ing (RL) approach of Branavan et al (2009), and
related methods (Vogel & Jurafsky, 2010), but uses
latent variable model updates within a semantic
parser. Branavan et al (2010) extended their RL ap-
proach to model high-level instructions, which cor-
respond to implicit actions in our domain. Wei et al
(2009) and Kollar et al (2010) used shallow linguis-
tic representations for instructions. Recently, Tellex
50
et al (2011) used a graphical model semantics rep-
resentation to learn from instructions paired with
demonstrations. In contrast, we model significantly
more complex linguistic phenomena than these ap-
proaches, as required for the navigation domain.
Other research has adopted expressive meaning
representations, with differing learning approaches.
Matuszek et al (2010, 2012) describe supervised al-
gorithms that learn semantic parsers for navigation
instructions. Chen and Mooney (2011), Chen (2012)
and Kim and Mooney (2012) present state-of-the-
art algorithms for the navigation task, by training a
supervised semantic parser from automatically in-
duced labels. Our work differs in the use of joint
learning and inference approaches.
Supervised approaches for learning semantic
parsers have received significant attention, e.g. Kate
and Mooney (2006), Wong and Mooney (2007),
Muresan (2011) and Kwiatkowski et al (2010,
2012). The algorithms we develop in this pa-
per combine ideas from previous supervised CCG
learning work (Zettlemoyer & Collins, 2005, 2007;
Kwiatkowski et al, 2011), as we describe in Sec-
tion 4. Recently, various alternative forms of su-
pervision were introduced. Clarke et al (2010),
Goldwasser and Roth (2011) and Liang et al (2011)
describe approaches for learning semantic parsers
from sentences paired with responses, Krishna-
murthy and Mitchell (2012) describe using distant
supervision, Artzi and Zettlemoyer (2011) use weak
supervision from conversational logs and Gold-
wasser et al (2011) present work on unsupervised
learning. We discuss various forms of supervision
that complement these approaches. There has also
been work on learning for semantic analysis tasks
from grounded data, including event streams (Liang
et al, 2009; Chen et al, 2010) and language paired
with visual perception (Matuszek et al, 2012).
Finally, the topic of executing instructions in
non-learning settings has received significant atten-
tion (e.g., Winograd (1972), Di Eugenio and White
(1992), Webber et al (1995), Bugmann et al (2004),
MacMahon et al (2006) and Dzifcak et al (2009)).
4 Background
We use a weighted linear CCG grammar for seman-
tic parsing, as briefly reviewed in this section.
Combinatory Categorial Grammars (CCGs)
CCGs are a linguistically-motivated formalism for
modeling a wide range of language phenom-
ena (Steedman, 1996, 2000). A CCG is defined by a
lexicon and a set of combinators. The lexicon con-
tains entries that pair words or phrases with cate-
gories. For example, the lexical entry chair ` N :
?x.chair(x) for the word ?chair? in the parse in Fig-
ure 4 pairs it with a category that has syntactic type
N and meaning ?x.chair(x). Figure 4 shows how a
CCG parse builds a logical form for a complete sen-
tence in our example navigation domain. Starting
from lexical entries, each intermediate parse node,
including syntax and semantics, is constructed with
one of a small set of CCG combinators (Steedman,
1996, 2000). We use the application, composition
and coordination combinators, and three others de-
scribed in Section 6.3.
Factored CCG Lexicons Recently, Kwiatkowski
et al (2011) introduced a factored CCG lexicon
representation. Each lexical item is composed of
a lexeme and a template. For example, the entry
chair ` N : ?x.chair(x) would be constructed by
combining the lexeme chair ` [chair], which con-
tains a word paired with logical constants, with the
template ?v.[N : ?x.v(x)], that defines the rest of
the category by abstracting over logical constants.
This approach allows the reuse of common syntactic
structures through a small set of templates. Section 8
describes how we learn such lexical entries.
Weighted Linear CCGs A weighted linear
CCG (Clark & Curran, 2007) ranks the space of
possible parses under the grammar, and is closely
related to several other approaches (Lafferty et al,
2001; Collins, 2004; Taskar et al, 2004). Let x be a
sentence, y be a CCG parse, and GEN(x; ?) be the
set of all possible CCG parses for x given the lexi-
con ?. Define ?(x, y) ? Rd to be a d-dimensional
feature?vector representation and ? ? Rd to be a pa-
rameter vector. The optimal parse for sentence x is
y?(x) = arg max
y?GEN(x;?) ? ? ?(x, y)
and the final output logical form z is the ?-calculus
expression at the root of y?(x). Section 7.2 de-
scribes how we efficiently compute an approxima-
tion to y?(x) within the joint interpretation and exe-
cution model.
51
Supervised learning with GENLEX Previous
work (Zettlemoyer & Collins, 2005) introduced a
function GENLEX(x, z) to map a sentence x and its
meaning z to a large set of potential lexical entries.
These entries are generated by rules that consider the
logical form z and guess potential CCG categories.
For example, the rule p ? N : ?x.p(x) introduces
categories commonly used to model certain types of
nouns. This rule would, for example, introduce the
category N : ?x.chair(x) for any logical form z
that contains the constant chair. GENLEX uses a
small set of such rules to generate categories that
are paired with all possible substrings in x, to create
a large set of lexical entries. The complete learning
algorithm then simultaneously selects a small sub-
set of these entries and estimates parameter values
?. In Section 8, we will introduce a new way of
using GENLEX to learn from different signals that,
crucially, do not require a labeled logical form z.
5 Spatial Environment Modeling
We will execute instructions in an environment, see
Section 2, which has a set of positions. A position
is a triple (x, y, o), where x and y are horizontal and
vertical coordinates, and o ? O = {0, 90, 180, 270}
is an orientation. A position also includes properties
indicating the object it contains, its floor pattern and
its wallpaper. For example, the square at (4, 3) in
Figure 2 has four positions, one per orientation.
Because instructional language refers to objects
and other structures in an environment, we introduce
the notion of a position set. For example, in Figure 2,
the position set D = {(5, 3, o) : o ? O} represents
a chair, while B = {(x, 3, o) : o ? O, x ? [0 . . . 5]}
represents the blue floor. Both sets contain all ori-
entations for each (x, y) pair, thereby representing
properties of regions. Position sets can have many
properties. For example, E, in addition to being a
chair, is also an intersection because it overlaps with
the neighboring halls A and B. The set of possi-
ble entities includes all position sets and a few addi-
tional entries. For example, set C = {(4, 3, 90)} in
Figure 2 represents the agent?s position.
6 Modeling Instructional Language
We aim to design a semantic representation that is
learnable, models grounded phenomena such as spa-
X	 ?y	 ? 1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
1	 ?
2	 ?
3	 ?
4	 ?
5	 ?
270	 ?90	 ?
0	 ?
180	 ?
C	 ?
D	 ?E	 ?
A	 ?
B	 ?
{ D	 ? E	 ? } (a) chair
?x.chair(x){ A	 ? B	 ?} (b) hall
?x.hall(x)E	 ? (c) the chair
?x.chair(x)C	 ? (d) you
you{ B	 ?} (e) blue hall
?x.hall(x) ? blue(x)
{ E	 ? } (f) chair in the intersection?x.chair(x) ?
intersect(?y.junction(y), x){ A	 ? B	 ? E	 ? } (g) in front of you
?x.in front of(you, x)
Figure 2: Schematic diagram of a map environment
and example of semantics of spatial phrases.
tial relations and object reference, and is executable.
Our semantic representation combines ideas from
Carpenter (1997) and Neo-Davidsonian event se-
mantics (Parsons, 1990) in a simply typed ?-
calculus. There are four basic types: (1) entities e
that are objects in the world, (2) events ev that spec-
ify actions in the world, (3) truth values t, and (4)
meta-entities m, such as numbers or directions. We
also allow functional types, which are defined by in-
put and output types. For example, ?e, t? is the type
of function from entities to truth values.
6.1 Spatial Language Modeling
Nouns and Noun Phrases Noun phrases are
paired with e-type constants that name specific en-
tities and nouns are mapped to ?e, t?-type expres-
sions that define a property. For example, the noun
?chair? (Figure 2a) is paired with the expression
?x.chair(x), which defines the set of objects for
52
which the constant chair returns true. The deno-
tation of this expression is the set {D,E} in Fig-
ure 2 and the denotation of ?x.hall(x) (Figure 2b)
is {A,B}. Also, the noun phrase ?you? (Figure 2d),
which names the agent, is represented by the con-
stant you with denotation C, the agent?s position.
Determiners Noun phrases can also be formed by
combining nouns with determiners that pick out spe-
cific objects in the world. We consider both definite
reference, which names contextually unique objects,
and indefinites, which are less constrained.
The definite article is paired with a logical expres-
sion ? of type ??e, t?, e?,1 which will name a sin-
gle object in the world. For example, the phrase
?the chair? in Figure 2c will be represented by
?x.chair(x) which will denote the appropriate chair.
However, computing this denotation is challenging
when there is perceptual ambiguity, for positions
where multiple chairs are visible. We adopt a sim-
ple heuristic approach that ranks referents based on
a combination of their distance from the agent and
whether they are in front of it. For our example,
from position C our agent would pick the chair E
in front of it as the denotation. The approach dif-
fers from previous, non-grounded models that fail to
name objects when faced with such ambiguity (e.g.,
Carpenter (1997), Heim and Kratzer (1998)).
To model the meaning of indefinite articles, we
depart from the Frege-Montague tradition of us-
ing existential quantifiers (Lewis, 1970; Montague,
1973; Barwise & Cooper, 1981), and instead in-
troduce a new quantifier A that, like ?, has type
??e, t?, e?. For example, the phrase ?a chair? would
be paired with Ax.chair(x) which denotes an arbi-
trary entry from the set of chairs in the world. Com-
puting the denotation for such expressions in a world
will require picking a specific object, without fur-
ther restrictions. This approach is closely related to
Steedman?s generalized Skolem terms (2011).2
Meta Entities We use m-typed terms to represent
non-physical entities, such as numbers (1, 2, etc.)
and directions (left, right, etc.) whose denotations
1Although quantifiers are logical constants with type
??e, t?, e? or ??e, t?, t?, we use a notation similar to that used
for first-order logic. For example, the notation ?x.f(x) repre-
sents the logical expression ?(?x.f(x))
2Steedman (2011) uses generalized Skolem terms as a tool
for resolving anaphoric pronouns, which we do not model.
are fixed. The ability to refer to directions allows
us to manipulate position sets. For example, the
phrase ?your left? is mapped to the logical expres-
sion orient(you, left), which denotes the position
set containing the position to the left of the agent.
Prepositions and Adjectives Noun phrases with
modifiers, such as adjectives and prepositional
phrases are ?e, t?-type expressions that implement
set intersection with logical conjunctions. For ex-
ample in Figure 2, the phrase ?blue hall? is paired
with ?x.hall(x)? blue(x) with denotation {B} and
the phrase ?chair in the intersection? is paired with
?x.chair(x) ? intersect(?y.junction(y), x) with
denotation {E}. Intuitively, the adjective ?blue?
introduces the constant blue and ?in the? adds a
intersect. We will describe the full details of how
these expressions are constructed in Section 6.3.
Spatial Relations The semantic representation al-
lows more complex reasoning over position sets and
the relations between them. For example, the bi-
nary relation in front of (Figure 2g) tests if the
first argument is in front of the second from the point
of view of the agent. Additional relations are used
to model set intersection, relative direction, relative
distance, and relative position by distance.
6.2 Modeling Instructions
To model actions in the world, we adopt Neo-
Davidsonian event semantics (Davidson, 1967; Par-
sons, 1990), which treats events as ev-type primitive
objects. Such an approach allows for a compact lex-
icon where adverbial modifiers introduce predicates,
which are linked by a shared event argument.
Instructional language is characterized by heavy
usage of imperatives, which we model as func-
tions from events to truth values.3 For example, an
imperative such as ?move? would have the mean-
ing ?a.move(a), which defines a set of events that
match the specified constraints. Here, this set would
include all events that involve moving actions.
The denotation of ev-type terms is a sequence
of n instances of the same action. In this way, an
event defines a function ev : s ? s?, where s is
the start state and s? the end state. For example, the
3Imperatives are ?ev, t?-type, much like ?e, t?-type wh-
interrogatives. Both define sets, the former includes actions to
execute, the later defines answers to a question.
53
denotation of ?a.move(a) is the set of move action
sequences {?MOVE1, . . . ,MOVEn? : n ? 1}. Al-
though performing actions often require performing
additional ones (e.g., the agent might have to turn
before being able to move), we treat such actions as
implicit (Section 7.1), and don?t model them explic-
itly within the logical form.
Predicates such as move (seen above) and
turn are introduced by verbs. Events can also
be modified by adverbials, which are intersective,
much like prepositional phrases. For example in the
imperative, logical form (LF) pair:
Imp.: move from the sofa to the chair
LF: ?a.move(a) ? to(a, ?x.chair(x)) ?
from(a, ?y.sofa(y))
Each adverbial phrase provides a constraint, and
changing their order will not change the LF.
6.3 Parsing Instructional Language with CCG
To compose logical expressions from sentences we
use CCG, as described in Section 4. Figures 3 and 4
present a sample of lexical entries and how they are
combined, as we will describe in this section. The
basic syntactic categories are N (noun), NP (noun
phrase), S (sentence), PP (prepositional phrase),
AP (adverbial phrase), ADJ (adjective) and C (a
special category for coordinators).
Type Raising To compactly model syntactic vari-
ations, we follow Carpenter (1997), who argues for
polymorphic typing. We include the more simple, or
lower type, entry in the lexicon and introduce type-
raising rules to reconstruct the other when necessary
at parse time. We use four rules:
PP : g ? N\N : ?f.?x.f(x) ? g(x)
ADJ : g ? N/N : ?f.?x.f(x) ? g(x)
AP : g ? S\S : ?f.?a.f(a) ? g(a)
AP : g ? S/S : ?f.?a.f(a) ? g(a)
where the first three are for prepositional, adjectival
and adverbial modifications, and the fourth models
the fact that adverbials are often topicalized.4 Fig-
ures 3 and 4 show parses that use type-raising rules.
Indefinites As discussed in Section 6.1, we use
a new syntactic analysis for indefinites, follow-
4Using type-raising rules can be particularly useful when
learning from sparse data. For example, it will no longer be
necessary to learn three lexical entries for each adverbial phrase
(with syntax AP , S\S, and S/S).
chair in the corner
N PP/NP NP/N N
?x.chair(x) ?x.?y.intersect(x, y) ?f.Ax.f(x) ?x.corner(x)
>
NP
?x.corner(x)
>
PP
?y.intersect(?x.corner(x), y)
N\N
?f.?y.f(y) ? intersect(?x.chair(x), y)
<
N
?y.chair(y) ? intersect(?x.chair(x), y)
Figure 3: A CCG parse with a prepositional phrase.
ing Steedman (2011). Previous approaches would
build parses such as
with a lamp
PP/NP PP\(PP/NP )/N N
?x.?y.intersect(x, y) ?f.?g.?y.?x.g(x, y) ? f(x) ?x.lamp(x)
>
PP\(PP/NP )
?g.?y.?x.g(x, y) ? lamp(x)
<
PP
?y.?x.intersect(x, y) ? lamp(x)
where ?a? has the relatively complex syntactic cate-
gory PP\(PP/NP )/N and where similar entries
would be needed to quantify over different types
of verbs (e.g., S\(S/NP )/N ) and adverbials (e.g.,
AP\(AP/NP )/N ). Instead, we include a single
lexical entry a ` NP/N : ?f.Ax.f(x) which can
be used to construct the correct meaning in all cases.
7 Joint Parsing and Execution
Our inference includes an execution component and
a parser. The parser maps sentences to logical forms,
and incorporates the grounded execution model. We
first discuss how to execute logical forms, and then
describe the joint model for execution and parsing.
7.1 Executing Logical Expressions
Dynamic Models In spatial environments, such as
the ones in our task, the agent?s ability to observe the
world depends on its current state. Taking this aspect
of spatial environments into account is challenging,
but crucial for correct evaluation.
To represent the agent?s point of view, for each
state s ? S, as defined in Section 2, let Ms be the
state-dependent logical model. A model M consists
of a domain DM,T of objects for each type T and
an interpretation function IM,T : OT ? DM,T ,
where OT is the set of T -type constants. IM,T
maps logical symbols to T -type objects, for exam-
ple, it will map you to the agent?s position. We have
domains for position sets, actions and so on. Fi-
nally, let VT be the set of variables of type T , and
54
facing the lamp go until you reach a chair
AP/NP NP/N N S AP/S NP S\NP/NP NP/N N
?x.?a.pre(a, ?f.?x.f(x) ?x.lamp(x) ?a.move(a) ?s.?a.post(a, s) you ?x.?y.intersect(x, y) ?f.Ax.f(x) ?x.chair(x)
front(you, x))
> >
NP NP
?x.lamp(x) Ax.chair(x)
> >
AP S\NP
?a.pre(a, front(you, ?x.lamp(x))) ?y.intersect(Ax.chair(x), y)
<
S/S S
?f.?a.f(a) ? pre(a, front(you, ?x.lamp(x))) intersect(Ax.chair(x), you)
>
AP
?a.post(a, intersect(Ax.chair(x), you))
S\S
?f.?a.f(a) ? post(a, intersect(Ax.chair(x), you))
<
S
?a.move(a) ? post(a, intersect(Ax.chair(x), you))
>
S
?a.move(a) ? post(a, intersect(Ax.chair(x), you)) ? pre(a, front(you, ?x.lamp(x)))
Figure 4: A CCG parse showing adverbial phrases and topicalization.
AT : VT ?
?
s?S DMs,T be the assignment func-
tion, which maps variables to domain objects.
For each model Ms the domain DMs,ev is a set
of action sequences {?a1, ..., an? : n ? 1}. Each ~a
defines a sequences of states si, as defined in Sec-
tion 6.2, and associated models Msi . The key chal-
lenge for execution is that modifiers of the event will
need to be evaluated under different models from
this sequence. For example, consider the sentence
in Figure 4. To correctly execute, the pre literal, in-
troduced by the ?facing? phrase, it must be evaluated
in the model Ms0 for the initial state s0. Similarly,
the literal including post requires the final model
Msn+1 . Such state dependent predicates, including
pre and post, are called stateful. The list of stateful
predicates is pre-defined and includes event modi-
fiers, as well the ? quantifier, which is evaluated un-
der Ms0 , since definite determiners are assumed to
name objects visible from the start position. In gen-
eral, a logical expression is traversed depth first and
the model is updated every time a stateful predicate
is reached. For example, the two e-type you con-
stants in Figure 4 will be evaluated under different
models: the one within the pre literal under Ms0 ,
and the one inside the post literal under Msn+1 .
Evaluation Given a logical expression l, we can
compute the interpretation IMs0 ,T (l) by recursivelymapping each subexpression to an entry on the ap-
propriate model M .
To reflect the changing state of the agent during
evaluation, we define the function update(~a, pred).
Given an action sequence ~a and a stateful predi-
cate pred, update returns a model Ms, where s
is the state under which the literal containing pred
should be interpreted, either the initial state or one
visited while executing ~a. For example, given the
predicate post and the action sequence ?a1, . . . , an?,
update(?a1, . . . , an?, post) = Msn+1 , where sn+1
the state of the agent following action an. By con-
vention, we place the event variable as the first argu-
ment in literals that include one.
Given a T -type logical expression l and a start-
ing state s0, we compute its interpretation IMs0 ,T (l)recursively, following these three base cases:
? If l is a ? operator of type ?T1, T2? binding vari-
able v and body b, IMs,T (l) is a set of pairs
from DT1 ?DT2 , where DT1 , DT2 ? Ms. For
each object o ? DT1 , we create a pair (o, i)
where i is the interpretation IMs,T2(b) com-
puted under a variable assignment function ex-
tended to map AT2(v) = o.
? If l is a literal c(c1, . . . , cn) with n argu-
ments where c has type P and each ci has
type Pi, IMs,T (l) is computed by first in-
terpreting the predicate c to the function
f = IMs,T (c). In most cases, IMs,T (l) =
f(IMs,P1(c1), . . . , IMs,Pn(cn)). However, if c
is a stateful predicate, such as pre or post, we
instead first retrieve the appropriate new model
Ms? = update(IMs,P1(c1), c), where c1 is the
event argument and IMs,P1(c1) is its interpre-
tation. Then, the final results is IMs,T (l) =
f(IMs? ,P1(c1), . . . , IMs? ,Pn(cn)).
? If l is a T -type constant or variable, IMs,T (l).
The worst case complexity of the process is ex-
ponential in the number of bound variables. Al-
though in practice we observed tractable evaluation
in the majority of development cases we considered,
a more comprehensive and tractable evaluation pro-
cedure is an issue that we leave for future work.
55
Implicit Actions Instructional language rarely
specifies every action required for execution, see
MacMahon (2007) for a detailed discussion in the
maps domain. For example, the sentence in Fig-
ure 4 can be said even if the agent is not facing a
blue hallway, with the clear implicit request that it
should turn to face such a hallway before moving.
To allow our agent to perform implicit actions, we
extend the domain of ev-type variables by allowing
the agent to prefix up to kI action sequences before
each explicit event. For example, in the agent?s po-
sition in Figure 2 (set C), the set of possible events
includes ?MOVEI ,MOVEI , RIGHTI ,MOVE?, which
contains two implicit sequences (marked by I).
Resolving Action Ambiguity Logical forms of-
ten fail to determine a unique action sequences,
due to instruction ambiguity. For example, con-
sider the instruction ?go forward? and the agent state
as specified in Figure 2 (set C). The instruction,
which maps to ?a.move(a) ? forward(a), evalu-
ates to the set containing ?MOVE?, ?MOVE,MOVE?
and ?MOVE,MOVE,MOVE?, as well as five other se-
quences that have implicit prefixes followed by ex-
plicit MOVE actions. To resolve such ambiguity, we
prefer shorter actions without implicit actions. In
the example above, we will select ?MOVE?, which
includes a single action and no implicit actions.
7.2 Joint Inference
We incorporate the execution procedure described
above with a linear weighted CCG parser, as de-
scribed in Section 4, to create a joint model of pars-
ing and execution. Specifically, we execute logi-
cal forms in the current state and observe the result
of their execution. For example, the word ?chair?
can be used to refer to different types of objects, in-
cluding chairs, sofas, and barstools, in the maps do-
mains. Our CCG grammar would include a lexical
item for each meaning, but execution might fail de-
pending on the presence of objects in the world, in-
fluencing the final parse output. Similarly, allowing
implicit actions provides robustness when resolv-
ing these and other ambiguities. For example, an
instruction with the precondition phrase ?from the
chair? might require additional actions to reach the
position with the named object.
To allow such joint reasoning we define an ex-
ecution e to include a parse tree ey and trace e~a,
and define our feature function to be ?(xi, si, e),
where xi is an instruction and si is the start state.
This approach allows joint dependencies: the state
of the world influences how the agent interprets
words, phrases and even complete sentences, while
language understanding determines actions.
Finally, to execute sequences of instructions, we
execute each starting from the end state of the previ-
ous one, using a beam of size ks.
8 Learning
Figure 5 presents the complete learning algorithm.
Our approach is online, considering each example in
turn and performing two steps: expanding the lex-
icon and updating parameters. The algorithm as-
sumes access to a training set {(xi, si,Vi) : i =
1 . . . n}, where each example includes an instruction
xi, starting state si and a validation function Vi, as
defined in Section 2. In addition the algorithm takes
a seed lexicon ?0. The output is a joint model, that
includes a lexicon ? and parameters ?.
Coarse Lexical Generation To generate po-
tential lexical entries we use the function
GENLEX(x, s,V; ?, ?), where x is an in-
struction, s is a state and V is a validation function.
? is the current lexicon and ? is a parameter vector.
In GENLEX we use coarse logical constants,
as described below, to efficiently prune the set of
potential lexical entries. This set is then pruned
further using more precise inference in Step 1.
To compute GENLEX , we initially generate a
large set of lexical entries and then prune most of
them. The full set is generated by taking the cross
product of a set of templates, computed by factor-
ing out all templates in the seed lexicon ?0, and all
logical constants. For example, if ?0 has a lexical
item with the categoryAP/NP : ?x.?a.to(a, x) we
would create entries w ` AP/NP : ?x.?a.p(a, x)
for every phrase w in x and all constants p with the
same type as to.5
In our development work, this approach often
generated nearly 100k entries per sentence. To ease
5Generalizing previous work (Kwiatkowski et al, 2011), we
allow templates that abstract subsets of the constants in a lex-
ical item. For example, the seed entry facing ` AP/NP :
?x.?a.pre(a, front(you, x)) would create 7 templates.
56
Inputs: Training set {(xi, si,Vi) : i = 1 . . . n} where xi is a
sentence, si is a state and Vi is a validation function, as de-
scribed in Section 2. Initial lexicon ?0. Number of iterations
T . Margin ?. Beam size k for lexicon generation.
Definitions: Let an execution e include a parse tree ey and
a trace e~a. GEN(x, s; ?) is the set of all possible execu-
tions for the instruction x and state s, given the lexicon ?.
LEX(y) is the set of lexical entries used in the parse tree y.
Let ?i(e) be shorthand for the feature function ?(xi, si, e)
defined in Section 7.2. Define ?i(e, e?) = |?i(e)??i(e?)|1.
GENLEX(x, s,V;?, ?) takes as input an instruction x,
state s, validation function V , lexicon ? and model param-
eters ?, and returns a set of lexical entries, as defined in Sec-
tion 8. Finally, for a set of executions E let MAXVi(E; ?)
be {e|?e? ? E, ??,?i(e?)? ? ??,?i(e)? ? Vi(e~a) = 1}, the
set of highest scoring valid executions.
Algorithm:
Initialize ? using ?0 , ?? ?0
For t = 1 . . . T, i = 1 . . . n :
Step 1: (Lexical generation)
a. Set ?G ? GENLEX(xi, si,Vi; ?, ?), ?? ? ? ?G
b. Let E be the k highest scoring executions from
GEN(xi, si;?) which use at most one entry from ?G
c. Select lexical entries from the highest scoring valid
parses: ?i ? ?e?MAXVi(E;?) LEX(ey)d. Update lexicon: ?? ? ? ?i
Step 2: (Update parameters)
a. Set Gi ?MAXVi(GEN(xi, si; ?); ?)
and Bi ? {e|e ? GEN(xi, si; ?) ? Vi(e~a) 6= 1}
b. Construct sets of margin violating good and bad parses:
Ri ? {g|g ? Gi ?
?b ? Bi s.t. ??,?i(g)? ?i(b)? < ??i(g, b)}
Ei ? {b|b ? Bi ?
?g ? Gi s.t. ??,?i(g)? ?i(b)? < ??i(g, b)}
c. Apply the additive update:
? ? ? + 1|Ri|
?
r?Ri ?i(r)?
1
|Ei|
?
e?Ei ?i(e)
Output: Parameters ? and lexicon ?
Figure 5: The learning algorithm.
the cost of parsing at this scale, we developed a
coarse-to-fine two-pass parsing approach that lim-
its the number of new entries considered. The algo-
rithm first parses with coarse lexical entries that ab-
stract the identities of the logical constants in their
logical forms, thereby greatly reducing the search
space. It then uses the highest scoring coarse parses
to constrain the lexical entries for a final, fine parse.
Formally, we construct the coarse lexicon ?a by
replacing all constants of the same type with a single
newly created, temporary constant. We then parse to
create a set of trees A, such that each y ? A
1. is a parse for sentence x, given the world state
s with the combined lexicon ? ? ?a,
2. scored higher than ey by at least a margin of
?L, where ey is the tree of e, the highest scoring
execution of x, at position s under the current
model, s.t. V(e~a) = 1,
3. contains at most one entry from ?a.
Finally, from each entry l ? {l|l ? ?a ? l ?
y ? y ? A}, we create multiple lexical entries by
replacing all temporary constants with all possible
appropriately typed constants from the original set.
GENLEX returns all these lexical entries, which
will be used to form our final fine-level analysis.
Step 1: Lexical Induction To expand our model?s
lexicon, we use GENLEX to generate candidate
lexical entries and then further refine this set by pars-
ing with the current model. Step 1(a) in Figure 5
uses GENLEX to create a temporary set of po-
tential lexical entries ?G. Steps (b-d) select a small
subset of these lexical entries to add to the current
lexicon ?: we find the k-best executions under the
model, which use at most one entry from ?G, find
the entries used in the best valid executions and add
them to the current lexicon.
Step 2: Parameter Update We use a variant of
a loss-driven perceptron (Singh-Miller & Collins,
2007; Artzi & Zettlemoyer, 2011) for parameter up-
dates. However, instead of taking advantage of a loss
function we use a validation signal. In step (a) we
collect the highest scoring valid parses and all in-
valid parses. Then, in step (b) we construct the set
Ri of valid analyses and Ei of invalid ones, such
that their model scores are not separated by a mar-
gin ? scaled by the number of wrong features (Taskar
et al, 2003). Finally, step (f) applies the update.
Discussion The algorithm uses the validation sig-
nal to drive both lexical induction and parameter
updates. Unlike previous work (Zettlemoyer &
Collins, 2005, 2007; Artzi & Zettlemoyer, 2011),
we have no access to a set of logical constants,
either through the the labeled logical form or the
weak supervision signal, to guide the GENLEX
procedure. Therefore, to avoid over-generating lex-
ical entries, thereby making parsing and learning
intractable, we leverage typing for coarse parsing
to prune the generated set. By allowing a single
57
Oracle SAIL
# of instruction sequences 501 706
# of instruction sequences
with implicit actions
431
Total # of sentences 2679 3233
Avg. sentences per sequence 5.35 4.61
Avg. tokens per sentence 7.5 7.94
Vocabulary size 373 522
Table 1: Corpora statistics (lower-cased data).
new entry per parse, we create a conservative, cas-
cading effect, whereas a lexical entry that is intro-
duced opens the way for many other sentence to be
parsed and introduce new lexical entries. Further-
more, grounded features improve parse selection,
thereby generating higher quality lexical entries.
9 Experimental Setup
Data For evaluation, we use the navigation task
from MacMahon et al (2006), which includes three
environments and the SAIL corpus of instructions
and follower traces. Chen and Mooney (2011) seg-
mented the data, aligned traces to instructions, and
merged traces created by different subjects. The
corpus includes raw sentences, without any form of
linguistic annotation. The original collection pro-
cess (MacMahon et al, 2006) created many unin-
terpretable instructions and incorrect traces. To fo-
cus on the learning and interpretation tasks, we also
created a new dataset that includes only accurate in-
structions labeled with a single, correct execution
trace. From this oracle corpus, we randomly sam-
pled 164 instruction sequences (816 sentences) for
evaluation, leaving 337 (1863 sentences) for train-
ing. This simple effort will allow us to measure the
effects of noise on the learning approach and pro-
vides a resource for building more accurate algo-
rithms. Table 1 compares the two sets.
Features and Parser Following Zettlemoyer and
Collins (2005), we use a CKY parser with a beam
of k. To boost recall, we adopt a two-pass strategy,
which allows for word skipping if the initial parse
fails. We use features that indicate usage of lexical
entries, templates, lexemes and type-raising rules, as
described in Section 6.3, and repetitions in logical
coordinations. Finally, during joint parsing, we con-
sider only parses executable at si as complete.
Seed Lexicon To construct our seed lexicon we la-
beled 12 instruction sequences with 141 lexical en-
Single Sentence Sequence
Final state validation
Complete system 81.98 (2.33) 59.32 (6.66)
No implicit actions 77.7 (3.7) 38.46 (1.12)
No joint execution 73.27 (3.98) 31.51 (6.66)
Trace validation
Complete system 82.74 (2.53) 58.95 (6.88)
No implicit actions 77.64 (3.46) 38.34 (6.23)
No joint execution 72.85 (4.73) 30.89 (6.08)
Table 2: Cross-validation development accuracy and
standard deviation on the oracle corpus.
tries. The sequences were randomly selected from
the training set, so as to include two sequences for
each participant in the original experiment. Fig-
ures 3 and 4 include a sample of our seed lexicon.
Initialization and Parameters We set the weight
of each template indicator feature to the number of
times it is used in the seed lexicon and each repeti-
tion feature to -10. Learning parameters were tuned
using cross-validation on the training set: the mar-
gin ? is set to 1, the GENLEX margin ?L is set to
2, we use 6 iterations (8 for experiments on SAIL)
and take the 250 top parses during lexical genera-
tion (step 1, Figure 5). For parameter update (step
2, Figure 5) we use a parser with a beam of 100.
GENLEX generates lexical entries for token se-
quences up to length 4. ks, the instruction sequence
execution beam, is set to 10. Finally, kI is set to
2, allowing up to two implicit action sequences per
explicit one.
Evaluation Metrics To evaluate single instruc-
tions x, we compare the agent?s end state to a labeled
state s?, as described in Section 2. We use a similar
method to evaluate the execution of instruction se-
quences ~x, but disregard the orientation, since end
goals in MacMahon et al (2006) are defined with-
out orientation. When evaluating logical forms we
measure exact match accuracy.
10 Results
We repeated each experiment five times, shuffling
the training set between runs. For the development
cross-validation runs, we also shuffled the folds. As
our learning approach is online, this allows us to ac-
count for performance variations arising from train-
ing set ordering. We report mean accuracy and stan-
dard deviation across all runs (and all folds).
58
Single Sentence Sequence
Chen and Mooney (2011) 54.4 16.18
Chen (2012) 57.28 19.18
+ additional data 57.62 20.64
Kim and Mooney (2012) 57.22 20.17
Trace validation 65.28 (5.09) 31.93 (3.26)
Final state validation 64.25 (5.12) 30.9 (2.16)
Table 3: Cross-validation accuracy and standard de-
viation for the SAIL corpus.
Table 2 shows accuracy for 5-fold cross-
validation on the oracle training data. We first varied
the validation signal by providing the complete ac-
tion sequence or the final state only, as described in
Section 2. Although the final state signal is weaker,
the results are similar. The relatively large difference
between single sentence and sequence performance
is due to (1) cascading errors in the more difficult
task of sequential execution, and (2) corpus repe-
titions, where simple sentences are common (e.g.,
?turn left?). Next, we disabled the system?s ability
to introduce implicit actions, which was especially
harmful to the full sequence performance. Finally,
ablating the joint execution decreases performance,
showing the benefit of the joint model.
Table 3 lists cross validation results on the SAIL
corpus. To compare to previous work (Chen &
Mooney, 2011), we report cross-validation results
over the three maps. The approach was able to cor-
rectly execute 60% more sequences then the previ-
ous state of the art (Kim & Mooney, 2012). We
also outperform the results of Chen (2012), which
used 30% more training data.6 Using the weaker
validation signal creates a marginal decrease in per-
formance. However, we still outperform all previ-
ous work, despite using weaker supervision. Inter-
estingly, these increases were achieved with a rel-
atively simple executor, while previous work used
MARCO (MacMahon et al, 2006), which supports
sophisticated recovery strategies.
Finally, we evaluate our approach on the held out
test set for the oracle corpus (Table 4). In contrast
to experiments on the Chen and Mooney (2011) cor-
pus, we use a held out set for evaluation. Due to this
discrepancy, all development was done on the train-
ing set only. The increase in accuracy over learning
with the original corpus demonstrates the significant
impact of noise on our performance. In addition to
6This additional training data isn?t publicly available.
Validation Single Sentence Sequence LF
Final state 77.6 (1.14) 54.63 (3.5) 44 (6.12)
Trace 78.63 (0.84) 58.05 (3.12) 51.05 (1.14)
Table 4: Oracle corpus test accuracy and standard
deviation results.
execution results, we also report exact match logi-
cal form (LF) accuracy results. For this purpose, we
annotated 18 instruction sequences (105 sentences)
with logical forms. The gap between execution and
LF accuracy can be attributed to the complexity of
the linguistic representation and redundancy in in-
structions. These results provide a new baseline for
studying learning from cleaner supervision.
11 Discussion
We showed how to do grounded learning of a CCG
semantic parser that includes a joint model of mean-
ing and context for executing natural language in-
structions. The joint nature allows situated cues
to directly influence parsing and also enables algo-
rithms that learn while executing instructions.
This style of algorithm, especially when using the
weaker end state validation, is closely related to re-
inforcement learning approaches (Branavan et al,
2009, 2010). However, we differ on optimization
and objective function, where we aim for minimal
loss. We expect many RL techniques to be useful
to scale to more complex environments, including
sampling actions and using an exploration strategy.
We also designed a semantic representation to
closely match the linguistic structure of instructional
language, combining ideas from many semantic
theories, including, for example, Neo-Davidsonian
events (Parsons, 1990). This approach allowed us to
learn a compact and executable grammar that gen-
eralized well. We expect, in future work, that such
modeling can be reused for more general language.
Acknowledgments
The research was supported in part by DARPA un-
der the DEFT program through the AFRL (FA8750-
13-2-0019) and the CSSG (N11AP20020), the ARO
(W911NF-12-1-0197), and the NSF (IIS-1115966).
The authors thank Tom Kwiatkowski, Nicholas
FitzGerald and Alan Ritter for helpful discussions,
David Chen for providing the evaluation corpus, and
the anonymous reviewers for helpful comments.
59
References
Artzi, Y., & Zettlemoyer, L. (2011). Bootstrapping Se-
mantic Parsers from Conversations. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Barwise, J., & Cooper, R. (1981). Generalized Quanti-
fiers and Natural Language. Linguistics and Phi-
losophy, 4(2), 159?219.
Branavan, S., Chen, H., Zettlemoyer, L., & Barzilay, R.
(2009). Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint
Conference of the Association for Computational
Linguistics and the International Joint Conference
on Natural Language Processing.
Branavan, S., Zettlemoyer, L., & Barzilay, R. (2010).
Reading between the lines: learning to map high-
level instructions to commands. In Proceedings
of the Conference of the Association for Computa-
tional Linguistics.
Bugmann, G., Klein, E., Lauria, S., & Kyriacou, T.
(2004). Corpus-based robotics: A route instruc-
tion example. In Proceedings of Intelligent Au-
tonomous Systems.
Carpenter, B. (1997). Type-Logical Semantics. The MIT
Press.
Chen, D. L. (2012). Fast Online Lexicon Learning for
Grounded Language Acquisition. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics.
Chen, D., Kim, J., & Mooney, R. (2010). Training a mul-
tilingual sportscaster: using perceptual context to
learn language. Journal of Artificial Intelligence
Research, 37(1), 397?436.
Chen, D., & Mooney, R. (2011). Learning to Interpret
Natural Language Navigation Instructions from
Observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
Clark, S., & Curran, J. (2007). Wide-coverage efficient
statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4), 493?552.
Clarke, J., Goldwasser, D., Chang, M., & Roth, D.
(2010). Driving Semantic Parsing from the
World?s Response. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing.
Collins, M. (2004). Parameter estimation for statis-
tical parsing models: Theory and practice of
distribution-free methods. In New Developments
in Parsing Technology.
Davidson, D. (1967). The logical form of action sen-
tences. Essays on actions and events, 105?148.
Di Eugenio, B., & White, M. (1992). On the Interpre-
tation of Natural Language Instructions. In Pro-
ceedings of the Conference of the Association of
Computational Linguistics.
Dzifcak, J., Scheutz, M., Baral, C., & Schermerhorn, P.
(2009). What to Do and How to Do It: Trans-
lating Natural Language Directives Into Temporal
and Dynamic Logic Representation for Goal Man-
agement and Action Execution. In Proceedings
of the IEEE International Conference on Robotics
and Automation.
Goldwasser, D., Reichart, R., Clarke, J., & Roth, D.
(2011). Confidence Driven Unsupervised Seman-
tic Parsing. In Proceedings of the Association of
Computational Linguistics.
Goldwasser, D., & Roth, D. (2011). Learning from Nat-
ural Instructions. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence.
Heim, I., & Kratzer, A. (1998). Semantics in Generative
Grammar. Blackwell Oxford.
Kate, R., & Mooney, R. (2006). Using String-Kernels for
Learning Semantic Parsers. In Proceedings of the
Conference of the Association for Computational
Linguistics.
Kim, J., & Mooney, R. J. (2012). Unsupervised PCFG
Induction for Grounded Language Learning with
Highly Ambiguous Supervision. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing.
Kollar, T., Tellex, S., Roy, D., & Roy, N. (2010). Toward
Understanding Natural Language Directions. In
Proceedings of the ACM/IEEE International Con-
ference on Human-Robot Interaction.
Krishnamurthy, J., & Mitchell, T. (2012). Weakly Super-
vised Training of Semantic Parsers. In Proceed-
ings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning.
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L., &
Steedman, M. (2012). A probabilistic model
of syntactic and semantic acquisition from child-
directed utterances and their meanings. Proceed-
ings of the Conference of the European Chapter of
the Association of Computational Linguistics.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., &
Steedman, M. (2010). Inducing probabilistic CCG
grammars from logical form with higher-order
60
unification. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., &
Steedman, M. (2011). Lexical Generalization in
CCG Grammar Induction for Semantic Parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Con-
ditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In Pro-
ceedings of the International Conference on Ma-
chine Learning.
Lewis, D. (1970). General Semantics. Synthese, 22(1),
18?67.
Liang, P., Jordan, M., & Klein, D. (2009). Learning se-
mantic correspondences with less supervision. In
Proceedings of the Joint Conference of the Asso-
ciation for Computational Linguistics the Interna-
tional Joint Conference on Natural Language Pro-
cessing.
Liang, P., Jordan, M., & Klein, D. (2011). Learning
Dependency-Based Compositional Semantics. In
Proceedings of the Conference of the Association
for Computational Linguistics.
MacMahon, M. (2007). Following Natural Language
Route Instructions. Ph.D. thesis, University of
Texas at Austin.
MacMahon, M., Stankiewics, B., & Kuipers, B. (2006).
Walk the Talk: Connecting Language, Knowl-
edge, Action in Route Instructions. In Proceed-
ings of the National Conference on Artificial Intel-
ligence.
Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo, L., &
Fox, D. (2012). A Joint Model of Language and
Perception for Grounded Attribute Learning. Pro-
ceedings of the International Conference on Ma-
chine Learning.
Matuszek, C., Fox, D., & Koscher, K. (2010). Follow-
ing directions using statistical machine translation.
In Proceedings of the international conference on
Human-robot interaction.
Matuszek, C., Herbst, E., Zettlemoyer, L. S., & Fox, D.
(2012). Learning to Parse Natural Language Com-
mands to a Robot Control System. In Proceedings
of the International Symposium on Experimental
Robotics.
Montague, R. (1973). The Proper Treatment of Quantifi-
cation in Ordinary English. Approaches to natural
language, 49, 221?242.
Muresan, S. (2011). Learning for Deep Language Under-
standing. In Proceedings of the International Joint
Conference on Artificial Intelligence.
Parsons, T. (1990). Events in the Semantics of English.
The MIT Press.
Singh-Miller, N., & Collins, M. (2007). Trigger-based
language modeling using a loss-sensitive percep-
tron algorithm. In IEEE International Conference
on Acoustics, Speech and Signal Processing.
Steedman, M. (1996). Surface Structure and Interpreta-
tion. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The MIT
Press.
Steedman, M. (2011). Taking Scope. The MIT Press.
Taskar, B., Guestrin, C., & Koller, D. (2003). Max-
Margin Markov Networks. In Proceedings of
the Conference on Neural Information Processing
Systems.
Taskar, B., Klein, D., Collins, M., Koller, D., & Manning,
C. (2004). Max-Margin Parsing. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.
Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee,
A., Teller, S., & Roy, N. (2011). Understanding
Natural Language Commands for Robotic Naviga-
tion and Mobile Manipulation. In Proceedings of
the National Conference on Artificial Intelligence.
Vogel, A., & Jurafsky, D. (2010). Learning to follow nav-
igational directions. In Proceedings of the Con-
ference of the Association for Computational Lin-
guistics.
Webber, B., Badler, N., Di Eugenio, B., Geib, C., Lev-
ison, L., & Moore, M. (1995). Instructions, In-
tentions and Expectations. Artificial Intelligence,
73(1), 253?269.
Wei, Y., Brunskill, E., Kollar, T., & Roy, N. (2009).
Where To Go: Interpreting Natural Directions
Using Global Inference. In Proceedings of the
IEEE International Conference on Robotics and
Automation.
Winograd, T. (1972). Understanding Natural Language.
Cognitive Psychology, 3(1), 1?191.
Wong, Y., & Mooney, R. (2007). Learning Synchronous
Grammars for Semantic Parsing with Lambda Cal-
culus. In Proceedings of the Conference of the As-
sociation for Computational Linguistics.
61
Zettlemoyer, L., & Collins, M. (2005). Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Ar-
tificial Intelligence.
Zettlemoyer, L., & Collins, M. (2007). Online learning
of relaxed CCG grammars for parsing to logical
form. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing.
62
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 110?120,
Dublin, Ireland, August 23-24 2014.
See No Evil, Say No Evil:
Description Generation from Densely Labeled Images
Mark Yatskar
1
?
my89@cs.washington.edu
1
Computer Science & Engineering
University of Washington
Seattle, WA, 98195, USA
Michel Galley
2
mgalley@microsoft.com
Lucy Vanderwende
2
lucyv@microsoft.com
Luke Zettlemoyer
1
lsz@cs.washington.edu
2
Microsoft Research
One Microsoft Way
Redmond, WA, 98052, USA
Abstract
This paper studies generation of descrip-
tive sentences from densely annotated im-
ages. Previous work studied generation
from automatically detected visual infor-
mation but produced a limited class of sen-
tences, hindered by currently unreliable
recognition of activities and attributes. In-
stead, we collect human annotations of ob-
jects, parts, attributes and activities in im-
ages. These annotations allow us to build
a significantly more comprehensive model
of language generation and allow us to
study what visual information is required
to generate human-like descriptions. Ex-
periments demonstrate high quality output
and that activity annotations and relative
spatial location of objects contribute most
to producing high quality sentences.
1 Introduction
Image descriptions compactly summarize com-
plex visual scenes. For example, consider the de-
scriptions of the image in Figure 1, which vary in
content but focus on the women and what they are
doing. Automatically generating such descriptions
is challenging: a full system must understand the
image, select the relevant visual content to present,
and construct complete sentences. Existing sys-
tems aim to address all of these challenges but
use visual detectors for only a small vocabulary
of words, typically nouns, associated with objects
that can be reliably found.
1
Such systems are blind
?
This work was conducted at Microsoft Research.
1
While object recognition is improving (ImageNet accu-
racy is over 90% for 1000 classes) progress in activity recog-
nition has been slower; the state of the art is below 50% mean
average precision for 40 activity classes (Yao et al., 2011).
cars (Count:3) Isa: ride, vehicle,? Doing: parking,? Has: steering wheel,? Attrib: black, shiny,? 
children (Count:2) Isa: kids, children ? Doing: biking, riding ? Has: pants, bike ? Attrib: young, small ? 
bike (Count:1) Isa: bike, bicycle,? Doing: playing,? Has: chain, pedal,? Attrib: silver, white,? 
women(Count:3) Isa: girls, models,? Doing: smiling,...  Has: shorts, bags,? Attrib: young, tan,? 
purses(Count:3) Isa: accessory,? Doing: containing,? Has: body, straps,? Attrib: black, soft,? 
sidewalk(Count:1) Isa: sidewalk, street,? Doing: laying,? Has: stone, cracks,? Attrib: flat, wide,? 
woman(Count:1) Isa: person, female,? Doing: pointing,? Has: nose, legs,? Attrib: tall, skinny,? 
tree(Count:1) Isa: plant,? Doing: growing,? Has: branches,? Attrib: tall, green,? 
kids(Count:5) Isa: group, teens,? Doing: walking,? Has: shoes, bags,? Attrib: young,? 
Five young people on the street, two sharing a bicycle.
Several young people are walking near parked vehicles.
Three girls with large handbags walking down the sidewalk.
Three women walk down a city street, as seen from above.
Three young woman walking down a sidewalk looking up.
Figure 1: An annotated image with human generated sen-
tence descriptions. Each bounding polygon encompasses one
or more objects and is associated with a count and text la-
bels.This image has 9 high level objects annotated with over
250 textual labels.
to much of the visual content needed to generate
complete, human-like sentences.
In this paper, we instead study generation with
more complete visual support, as provided by hu-
man annotations, allowing us to develop more
comprehensive models than previously consid-
ered. Such models have the dual benefit of (1)
providing new insights into how to construct more
human-like sentences and (2) allowing us to per-
form experiments that systematically study the
contribution of different visual cues in generation,
suggesting which automatic detectors would be
most beneficial for generation.
In an effort to approximate relatively complete
visual recognition, we collected manually labeled
representations of objects, parts, attributes and ac-
tivities for a benchmark caption generation dataset
that includes images paired with human authored
110
descriptions (Rashtchian et al., 2010).
2
As seen
in Figure 1, the labels include object boundaries
and descriptive text, here including the facts that
the children are ?riding? and ?walking? and that
they are ?young.? Our goal is to be as exhaustive
as possible, giving equal treatment to all objects.
For example, the annotations in Figure 1 contain
enough information to generate the first three sen-
tences and most of the content in the remaining
two. Labels gathered in this way are a type of fea-
ture norms (McRae et al., 2005), which have been
used in the cognitive science literature to approxi-
mate human perception and were recently used as
a visual proxy in distributional semantics (Silberer
and Lapata, 2012). We present the first effort, that
we are aware of, for using feature norms to study
image description generation.
Such rich data allows us to develop significantly
more comprehensive generation models. We di-
vide generation into choices about which visual
content to select and how to realize a sentence that
describes that content. Our approach is grammar-
based, feature-rich, and jointly models both deci-
sions. The content selection model includes la-
tent variables that align phrases to visual objects
and features that, for example, measure how vi-
sual salience and spatial relationships influence
which objects are mentioned. The realization ap-
proach considers a number of cues, including lan-
guage model scores, word specificity, and relative
spatial information (e.g. to produce the best spa-
tial prepositions), when producing the final sen-
tence. When used with a reranking model, includ-
ing global cues such as sentence length, this ap-
proach provides a full generation system.
Our experiments demonstrate high quality vi-
sual content selection, within 90% of human per-
formance on unigram BLEU, and improved com-
plete sentence generation, nearly halving the dif-
ference from human performance to two base-
lines on 4-gram BLEU. In ablations, we measure
the importance of different annotations and visual
cues, showing that annotation of activities and rel-
ative bounding box information between objects
are crucial to generating human-like description.
2 Related Work
A number of approaches have been proposed
for constructing sentences from images, includ-
ing copying captions from other images (Farhadi
2
Available at : http://homes.cs.washington.edu/?my89/
et al., 2010; Ordonez et al., 2011), using text
surrounding an image in a news article (Feng
and Lapata, 2010), filling visual sentence tem-
plates (Kulkarni et al., 2011; Yang et al., 2011;
Elliott and Keller, 2013), and stitching together ex-
isting sentence descriptions (Gupta and Mannem,
2012; Kuznetsova et al., 2012). However, due to
the lack of reliable detectors, especially for activi-
ties, many previous systems have a small vocab-
ulary and must generate many words, including
verbs, with no direct visual support. These prob-
lems also extend to video caption systems (Yu and
Siskind, 2013; Krishnamoorthy et al., 2013).
The Midge algorithm (Mitchell et al., 2012)
is most closely related to our approach, and will
provide a baseline in our experiments. Midge is
syntax-driven but again uses a small vocabulary
without direct visual support for every word. It
outputs a large set of sentences to describe all
triplets of recognized objects in the scene, but does
not include a content selection model to select the
best sentence. We extend Midge with content and
sentence selection rules to use it as a baseline.
The visual facts we annotate are motivated by
research in machine vision. Attributes are a
good intermediate representation for categoriza-
tion (Farhadi et al., 2009). Activity recognition
is an emerging area in images (Li and Fei-Fei,
2007; Yao et al., 2011; Sharma et al., 2013) and
video (Weinland et al., 2011), although less stud-
ied than object recognition. Also, parts have been
widely used in object recognition (Felzenszwalb
et al., 2010). Yet, no work tests the contribution of
these labels for sentence generation.
There is also a significant amount of work
on other grounded language problems, where re-
lated models have been developed. Visual re-
ferring expression generation systems (Krahmer
and Van Deemter, 2012; Mitchell et al., 2013;
FitzGerald et al., 2013) aim to identify specific
objects, a sub-problem we deal with when de-
scribing images more generally. Other research
generates descriptions in simulated worlds and,
like this work, uses feature rich models (Angeli
et al., 2010), or syntactic structures like PCFGs
(Chen et al., 2010; Konstas and Lapata, 2012) but
does not combine the two. Finally, Zitnick and
Parikh (2013) study sentences describing clipart
scenes. They present a number of factors influenc-
ing overall descriptive quality, several of which we
use in sentence generation for the first time.
111
3 Dataset
We collected a dataset of richly annotated images
to approximate gold standard visual recognition.
In collecting the data, we sought a visual annota-
tion with sufficient coverage to support the gen-
eration of as many of the words in the original
image descriptions as possible. We also aimed to
make it as visually exhaustive as possible?giving
equal treatment to all visible objects. This ensures
less bias from annotators? perception about which
objects are important, since one of the problems
we would like to solve is content selection. This
dataset will be available for future experiments.
We built on the dataset from (Rashtchian et
al., 2010) which contained 8,000 Flickr images
and associated descriptions gathered using Ama-
zon Mechanical Turk (MTurk). Restricting our-
selves to Creative Commons images, we sampled
500 images for annotation.
We collected annotations of images in three
stages using MTurk, and assigned each annotation
task to 3-5 workers to improve quality through re-
dundancy (Callison-Burch, 2009). Below we de-
scribe the process for annotating a single image.
Stage 1: We prompted five turkers to list all ob-
jects in an image, ignoring objects that are parts of
larger objects (e.g., the arms of a person), which
we collected later in Stage 3. This list also in-
cluded groups, such as crowds of people.
Stage 2: For each unique object label from
Stage 1, we asked two turkers to draw a polygon
around the object identified.
3
In cases where the
object is a group, we also asked for the number of
objects present (1-6 or many). Finally, we created
a list of all references to the object from the first
stage, which we call the Object facet.
Stage 3: For each object or group, we prompted
three turkers to provide descriptive phrases of:
? Doing ? actions the object participates in, e.g.
?jumping.?
? Parts ? physical parts e.g. ?legs?, or other
items in the possession of the object e.g.
?shirt.?
? Attributes ? adjectives describing the object,
e.g. ?red.?
? Isa ? alternative names for a object e.g.
?boy?, ?rider.?
Figure 1 shows more examples for objects
3
We modified LabelMe (Torralba et al., 2010).
in a labeled image.
4
We refer to all of these
annotations, including the merged Object la-
bels, as facets. These labels provide feature
norms (McRae et al., 2005), which have recently
used as a visual proxy in distributional seman-
tics (Silberer and Lapata, 2012; Silberer et al.,
2013) but have not been previous studied for gen-
eration. This annotation of 500 images (2500
sentences) yielded over 4000 object instances and
100,000 textual labels.
4 Approach
Given such rich annotations, we can now de-
velop significantly more comprehensive genera-
tion models. In this section, we present an ap-
proach that first uses a generative model and then
a reranker. The generative model defines a dis-
tribution over content selection and content real-
ization choices, using diverse cues from the image
annotations. The reranker trades off our generative
model score, language model score (to encourage
fluency), and length to produce the final sentence.
Generative Model We want to generate a sen-
tence ~w = ?w
1
. . . w
n
? where each word w
i
? V
comes from a fixed vocabulary V . The vocabu-
lary V includes all 2700 words used in descriptive
sentences in the training set.
5
The model conditions on an annotated image I
that contains a set of objects O, where each ob-
ject o ? O has a bounding polygon and a number
of facets containing string labels. To model the
naming of specific objects, words w
i
can be asso-
ciated with alignment variables a
i
that range over
O. One such variable is introduced for each head
noun in the sentence. Figure 2 shows alignment
variable settings with colors that match objects in
the image. Finally, as a byproduct of the hierarchi-
cal generative process, we construct an undirected
dependency tree
~
d over the words in ~w.
The complete generative model defines the
probability p(~w,~a,
~
d | I) of a sentence ~w, word
alignments ~a, and undirected dependency tree
~
d,
given the annotated input image I . The overall
process unfolds recursively, as seen in Figure 3.
4
In the experiments, Parts and Isa facets do not improve
performance, so we do not use them in the final model. Isa
is redundant with the Object facet, as seen in Figure 1. Also
parts like clothing, were often annotated as separate objects.
5
We do not generate from image facets directly, because
only 20% of the sentences in our data can be produced like
this. Instead, we develop features which consider the similar-
ity between labels in the image and words in the vocabulary.
112
Three girls  with large  handbags  walking  down  the  sidewalk 
? = ?= ? = 
Figure 2: One path through the generative model and the
Bayesian network it induces. The first row of colored circles
are alignment variables to objects in the image. The second
row is words, generated conditioned on alignments.
The main clause is produced by first selecting the
subject alignment a
s
followed by the subject word
w
s
. It then chooses the verb and optionally the ob-
ject alignment a
o
and word w
o
. The process then
continues recursively, modifying the subject, verb,
and object of the sentence with noun and prepo-
sitional modifiers. The recursion begins at Step
2 in Figure 3. Given a parent word w and that
word?s relevant alignment variable a, the model
creates attachments where w is the grammatical
head of subsequently produced words. Choices
about whether to create noun modifiers or preposi-
tional modifiers are made in steps (a) and (b). The
process chooses values for the alignment variables
and then chooses content words, adding connec-
tive prepositions in the case of prepositional mod-
ifiers. It then chooses to end or submits new word-
alignment pairs to be recursively modified.
Each line defines a decision that must be made
according to a local probability distribution. For
example, Step 1.a defines the probability of align-
ing a subject word to various objects in the im-
age. The distributions are maximum entropy mod-
els, similar to previous work (Angeli et al., 2010),
using features described in the next section. The
induced undirected dependency tree
~
d has an edge
between each word and the previously generated
word (or the input word w in Steps 2.a.i and 2.a.ii,
when no previous word is available). Figure 2
shows a possible output from the process, along
with the Bayesian network that encodes what each
decision was conditioned on during generation.
Learning We learn the model from data
{(~w
i
,
~
d
i
, I
i
) | i = 1 . . .m} containing sentences
~w
i
, dependency trees
~
d
i
, computed with the Stan-
ford parser (de Marneffe et al., 2006), and images
1. for a main clause (d,e are optional), select:
(a) subject a
s
alignment from p
a
(a).
(b) subject word w
s
from p
n
(w | a
s
,
~
d
c
)
(c) verb word w
v
from p
v
(w | a
s
,
~
d
c
)
(d) object alignment a
o
from p
a
(a
?
| a
s
, w
v
,
~
d
c
)
(e) object word w
o
from p
n
(w | a
o
,
~
d
c
)
(f) end with p
stop
or go to (2) with (w
s
, a
s
)
(g) end with p
stop
or go to (2) with (w
v
, a
s
)
(h) end with p
stop
or go to (2) with (w
o
, a
o
)
2. for a (word, alignment) (w
?
, a) (a,b are optional):
(a) if w
?
not verb: modify w
?
with noun, select:
i. modifier word w
n
from p
n
(w | a,
~
d
c
).
ii. end with p
stop
or go to (2) with (a
m
, w
n
)
(b) modify w
?
with preposition, select:
i. preposition word w
p
if w
?
not a verb: from p
p
(w | a,
~
d
c
)
else: from p
p
(w | a,w
v
,
~
d
c
)
ii. object alignment a
p
from p
a
(a
?
| a,w
p
,
~
d
c
)
iii. object word w
n
from p
n
(w | a
p
,
~
d
c
).
iv. end with p
stop
or go to (2) with (a
p
, w
n
)
Figure 3: Generative process for producing words ~w, align-
ments ~a and dependencies
~
d. Each distribution is conditioned
on the partially complete path through generative process
~
d
c
to establish sentence context. The notation p
stop
is short hand
for p
stop
(STOP |~w,
~
d
c
) the stopping distribution.
I
i
. The dependency trees define the path that was
taken through the generative process in Figure 3
and are used to create a Bayesian network for ev-
ery sentence, like in Figure 2. However, object
alignments ~a
i
are latent during learning and we
must marginalize over them.
The model is trained to maximize the condi-
tional marginal log-likelihood of the data with reg-
ularization:
L(?) =
?
i
log
?
~a
p(~a, ~w
i
,
~
d
i
| I
i
; ?)? r|?|
2
where ? is the set of parameters and r is the regu-
larization coefficient. In essence, we maximize the
likelihood of every sentence?s observed Bayesian
network, while marginalizing over content selec-
tion variables we did not observe.
Because the model only includes pairwise de-
pendencies between the hidden alignment vari-
ables ~a, the inference problem is quadratic in the
number of objects and non-convex because ~a is
unobserved. We optimize this objective directly
with L-BFGS, using the junction-tree algorithm to
compute the sum and the gradient.
6
6
To compute the gradient, we differentiate the recurrence
in the junction-tree algorithm by applying the product rule.
113
Inference To describe an image, we need to
maximize over word, alignment, and the depen-
dency parse variables:
argmax
~w,~a,
~
d
p(~w,~a,
~
d | I)
This computation is intractable because we
need to consider all possible sentences, so we use
beam search for strings up to a fixed length.
Reranking Generating directly from the process
in Figure 3 results in sentences that may be short
and repetitive because the model score is a product
of locally normalized distributions. The reranker
takes as input a candidate list c, for an image I , as
decoded from the generative model. The candidate
list includes the top-k scoring hypotheses for each
sentence length up to a fixed maximum. A linear
scoring function is used for reranking optimized
with MERT (Och, 2003) to maximize BLEU-2.
5 Features
We construct indicator features to capture vari-
ation in usage in different parts of the sen-
tence, types of objects that are mentioned, visual
salience, and semantic and visual coordination be-
tween objects. The features are included in the
maximum entropy models used to parameterize
the distributions described in Figure 3. Whenever
possible, we use WordNet Synsets (Miller, 1995)
instead of lexical features to limit over-fitting.
Features in the generative model use tests for
local properties, such as the identity of a synset
of a word in WordNet, conjoined with an iden-
tifier that indicates context in the generative pro-
cess.
7
Generative model features indicate (1) vi-
sual and semantic information about objects in dis-
tributions over alignments (content selection) and
(2) preferences for referring to objects in distribu-
tions over words (content realization). Features in
the reranking model indicate global properties of
candidate sentences. Exact formulas for comput-
ing the features are in the appendix.
Visual features, such as an object?s position in
the image, are used for content selection. Pairwise
visual information between two objects, for exam-
ple the bounding box overlap between objects or
the relative position of the two objects, is included
in distributions where selection of an alignment
7
For example, in Figure 2 the context for the word ?side-
walk? would be ?word,syntactic-object,verb,preposition? in-
dicating it is a word, in the syntactic object of a preposition,
which was attached to a verb modifying prepositional phrase.
variable conditions on previously generated align-
ments. For verbs (Step 1.d in Figure 3) and prepo-
sitions (Step 2.b.ii), these features are conjoined
with the stem of the connective.
Semantic types of objects are also used in con-
tent selection. We define semantic types by finding
synsets of labels in objects that correspond to high
level types, a list motivated by the animacy hierar-
chy (Zaenen et al., 2004).
8
Type features indicate
the type of the object referred to by an alignment
variable as well as the cross product of types when
an alignment variable is on conditioning side of
a distribution (e.g. Step 1.d). Like above, in the
presence of a connective word, these features are
conjoined with the stem of the connective.
Content realization features help select words
when conditioning on chosen alignments (e.g.
Step 1.b). These features include the identity of
the WordNet synset corresponding to a word, the
word?s depth in the synset hierarchy, the language
model score for adding that word
9
and whether the
word matches labels in facets corresponding to the
object referenced by an alignment variable.
Reranking features are primarily used to over-
come issues of repetition and length in the genera-
tive distributions, more commonly used for align-
ment, than to create sentences. We use only four
features: length, the number of repetitions, gener-
ative model score, and language model score.
6 Experimental Setup
Data We used 70% of the data for training (1750
sentences, 350 images), 15% for development, and
15% for testing (375 sentences, 75 images).
Parameters The regularization parameter was
set on the held out data to r = 8. The reranker
candidate list included the top 500 sentences for
each sentence length up to 15 and weights were
optimized with Z-MERT (Zaidan, 2009).
Metrics Our evaluation is based on BLEU-n
(Papineni et al., 2001), which considers all n-
grams up to length n. To assess human perfor-
mance using BLEU, we score each of the five ref-
erences against the four other ones and finally av-
erage the five BLEU scores. In order to make these
results comparable to BLEU scores for our model
8
For example, human, animal, artifact (a human created
object), natural body (trees, water, ect.), or natural artifact
(stick, leaf, rock).
9
We use tri-grams with Kneser-Ney smoothing over the 1
million caption data set (Ordonez et al., 2011).
114
and baselines, we perform the same five-fold aver-
aging when computing BLEU for each system.
We also compute accuracy for different syn-
tactic positions in the sentence. We look at a
number of categories: the main clause?s compo-
nents (S,V,O), prepositional phrase components,
the preposition (Pp) and their objects (Po) and
noun modifying words (N), including determiners.
Phrases match if they have an exact string match
and share context identifiers as defined in the fea-
tures sections.
Human Evaluation Annotators rated sentences
output by our full model against either human or a
baseline system generated descriptions. Three cri-
teria were evaluated: grammaticality, which sen-
tence is more complete and well formed; truthful-
ness, which sentence is more accurately capturing
something true in the image; and salience, which
sentence is capturing important things in the image
while still being concise. Two annotators anno-
tated all test pairs for all criteria for a given pair of
systems. Six annotators were used (none authors)
and agreement was high (Cohen?s kappa = 0.963,
0.823 and 0.703 for grammar, truth and salience).
Machine Translation Baseline The first base-
line is designed to see if it is possible to generate
good sentences from the facet string labels alone,
with no visual information. We use an extension of
phrase-based machine translation techniques (Och
et al., 1999). We created a virtual bitext by pair-
ing each image description (the target sentence)
with a sequence
10
of visual identifiers (the source
?sentence?) listing strings from the facet labels.
Since phrases produced by turkers lack many of
the functions words needed to create fluent sen-
tences, we added one of 47 function words either
at the start or the end of each output phrase.
The translation model included standard fea-
tures such as language model score (using our cap-
tion language model described previously), word
count, phrase count, linear distortion, and the
count of deleted source words. We also define
three features that count the number of Object, Isa,
and Doing phrases, to learn a preference for types
of phrases. The feature weights are tuned with
MERT (Och, 2003) to maximize BLEU-4.
Midge Baseline As described in related work,
the Midge system creates a set of sentences to de-
scribe everything in an input image. These sen-
10
We defined a consistent ordering of visual identifiers and
set the distortion limit of the phrase-based decoder to infinity.
BL-1 BL-2 BL-3 BL-4
Human 61.0 42.0 27.8 18.3
Full Model 57.1 35.7 18.3 9.5
MT Baseline 39.8 23.6 13.2 6.1
Midge Baseline 43.5 20.2 9.4 0.0
Table 1: Results for the test set for the BLEU1-4 metrics.
Grammar Full Other Equal
Full vs Human 7.65 19.4 72.94
Full vs MT 6.47 5.29 88.23
Full vs Midge 40.59 15.88 43.53
Truth Full Other Equal
Full vs Human 0.59 67.65 31.76
Full vs MT 30.0 10.59 59.41
Full vs Midge 51.76 27.71 23.53
Salience Full Other Equal
Full vs Human 8.82 88.24 2.94
Full vs MT 51.76 16.47 31.77
Full vs Midge 71.18 14.71 14.12
Table 2: Human evaluation of our Full-Model in heads
up tests against Human authored sentences and baseline sys-
tems, the machine translation baseline (MT) and the Midge
inspired baseline. Bold indicates the better system. Other is
not the Full system. Equal indicates neither sentence is better.
tences must all be true, but do not have to select
the same content that a person would. It can be
adapted to our task by adding object selection and
sentence ranking rules. For object selection, we
choose the three most frequently named objects
in the scene according to a background corpus of
image descriptions. For sentence selection, we
take all sentences within one word of the average
length of a sentence in our corpus, 11, and select
the one with best Midge generation score.
7 Results
We report experiments for our generation pipeline
and ablations that remove data and features.
Overall Performance Table 1 shows the re-
sults on the test set. The full model consis-
tently achieves the highest BLEU scores. Overall,
these numbers suggest strong content selection by
getting high recall for individual words (BLEU-
1), but fall further behind human performance as
the length of the n-gram grows (BLEU-2 through
BLEU-4). These number match our perception
that the model is learning to produce high quality
sentences, but does not always describe all of the
important aspects of the scene or use exactly the
expected wording. Table 4 presents example out-
put, which we will discuss in more detail shortly.
115
Model BL-1 BL-2 BL-3 BL-4 S V O Pp Po N
Human 64.7 46.0 31.5 20.1 - - - - - -
Full-Model 59.0 36.9 19.3 10.5 64.9 40.4 36.8 50.0 20.7 69.1
? doing 51.1 32.6 16.9 9.2 63.2 15.8 10.5 45.5 21.6 69.7
? count 55.4 33.5 16.0 8.5 59.6 35.1 15.4 53.7 19.5 66.7
? properties 57.8 37.2 18.8 10.0 61.4 36.8 36.8 47.1 20.7 73.5
? visual 56.7 35.1 18.9 9.4 64.9 36.8 50.0 41.8 15.3 71.6
? pairwise 56.9 35.5 16.5 8.2 64.9 40.4 45.5 42.4 21.2 70.9
Table 3: Ablation results on development data using BLEU1-4 and reporting match accuracy for sentence structures.
S: A girl playing a
guitar in the grass
R: A woman with a nylon stringed
guitar is playing in a field
S: A man playing with two
dogs in the water
R: A man is throwing a log into
a waterway while two dogs watch
S: Two men playing with
a bench in the grass
R: Nine men are playing a game
in the park, shirts versus skins
S: Three kids sitting on a road
R: A boy runs in a race
while onlookers watch
Table 4: Two good examples of output (top), and two ex-
amples of poor performance (bottom). Each image has two
captions, the system output S and a human reference R.
Human Evaluation Table 2 presents the results
of a human evaluation. The full model outper-
forms all baselines on every measure, but is not
always competitive with human descriptions. It
performs the best on grammaticality, where it is
judged to be as grammatical as humans. How-
ever, surprisingly, in many cases it is also often
judged equal to the other baselines. Examination
of baseline output reveals that the MT baseline of-
ten generates short sentences, having little chance
of being judged ungrammatical. Furthermore, the
Midge baseline, like our system, is a syntax-based
system and therefore often produces grammatical
sentences. Although our system performs well
with respect to the baselines on truthfulness, of-
ten the system constructs sentences with incorrect
prepositions, an issue that could be improved with
better estimates of 3-d position in the image. On
truthfulness, the MT baseline is comparable to our
system, often being judged equal, because its out-
put is short. Our system?s strength is salience, a
factor the baselines do not model.
Data Ablation Table 3 shows annotation abla-
tion experiments on the development set, where
we remove different classes of data labels to mea-
sure the performance that can be achieved with
less visual information. In all cases, the overall
behavior of the system varies, as it tries to learn to
compensate for the missing information.
Ablating actions is by far the most detrimental.
Overall BLEU score suffers and prediction accu-
racy of the verb (V) degrades significantly causing
cascading errors that affect the object of the verb
(O). Removing count information affects noun at-
tachment (N) performance. Images where deter-
miner use is important or where groups of objects
are best identified by the number (for example,
three dogs) are difficult to describe naturally. Fi-
nally, we see a tradeoff when removing properties.
There is an increase in noun modifier accuracy (N)
but a decrease in content selection quality (BL-1),
showing recall has gone down. In essence, the ap-
proach learns to stop trying to generate adjectives
and other modifiers that would rely on the missing
properties. The difference in BLEU score with the
Full-Model is small, even without these modifiers,
because there often still exists a a short output with
high accuracy.
Feature Ablation The bottom two rows in Ta-
ble 3 show ablations of the visual and pairwise
features, measuring the contribution of the visual
information provided by the bounding box anno-
tations. The ablated visual information includes
bounding-box positions and relative pairwise vi-
sual information. The pairwise ablation removes
the ability to model any interactions between ob-
jects, for example, relative bounding box or pair-
wise object type information.
Overall, prepositional phrase accuracy is most
affected. Ablating visual features significantly im-
pacts accuracy of prepositional phrases (Pp and
Po), affecting the use of preposition words the
most, and lowering fluency (BL-4). Precision in
116
the object of the verb (O) rises; the model makes
? 50% fewer predictions in that position than the
Full-Model because it lacks features to coordinate
subject and object of the verb. Ablating pairwise
features has similar results. While the model cor-
rects errors in the object of the preposition (Po)
with the addition of visual features, fluency is still
worse than Full-Model, as reflected by BL-4.
Qualitative Results Table 4 has examples of
good and bad system output. The first two im-
ages are good examples, including both system
output (S) and a human reference (R). The sec-
ond two contain lower quality outputs. Overall,
the model captures common ways to refer to peo-
ple and scenes. However, it does better for images
with fewer sentient objects because content selec-
tion is less ambiguous.
Our system does well at finding important ob-
jects. For example, in the first good image, we
mention the guitar instead of the house, both of
which are prominent and have high overlap with
the woman. In the second case, we identify that
both dogs and humans tend to be important actors
in scenes but poorly identify their relationship.
The bad examples show difficult scenes. In the
first description the broad context is not identi-
fied, instead focusing on the bench (highlighted in
red). The second example identifies a weakness
in our annotation: it encodes contradictory group-
ings of the people. The groupings covers all of
the children, including the boy running, and many
subsets of the people near the grass. This causes
ambiguity and our methods cannot differentiate
them, incorrectly mentioning just the children and
picking an inappropriate verb (one participant in
the group is not sitting). Improved annotation of
groups would enable the study of generation for
more complex scenes, such as these.
8 Conclusion
In this work we used dense annotations of images
to study description generation. The annotations
allowed us to not only develop new models, better
capable of generating human-like sentences, but
also to explore what visual information is crucial
for description generation. Experiments showed
that activity and bounding-box information is im-
portant and demonstrated areas of future work. In
images that are more complex, for example multi-
ple sentient objects, object grouping and reference
will be important to generating good descriptions.
Issues of this type can be explored with annota-
tions of increasing complexity.
Appendix A
This appendix describes the feature templates for
the generative model in greater detail.
Features in the generative model conjoin indica-
tors for local tests, such as STEM(w) which in-
dicates the stem of a wordw, with a global contex-
tual identifier CONTEXT(v, d) that indicates
properties of the generation history, as described
in detail below. Table 5 provides a reference for
which feature templates are used in the generative
model distributions, as defined in Figure 3.
8.1 Feature Templates
CONTEXT(n, d) is an indicator for a contex-
tual identifier for a variable n in the model de-
pending on the dependency structure d. There is
an indicator for all combinations of the type of n
(alignment or word), the position of n (subject,
syntactic object, verb, noun-modifier, or preposi-
tion), the position of the earliest variable along
the path to generate n, and the type of attach-
ment to that variable (noun or prepositional mod-
ifier). For example, in Figure 2 the context for
the word ?sidewalk? would be ?word,syntactic-
object,verb,preposition? indicating it is a word, the
object of a preposition, whose path was along a
verb modifying prepositional phrase.
11
TYPE(a) indicates the high level type of an
object referred to by alignment variable a. We
use synsets to define high level types including
human, animal, artifact, natural artifact and var-
ious synsets that capture scene information,
12
a
list motivated by the animacy hierarchy (Zaenen
et al., 2004). Each object is assigned a type by
finding the synset for its name (object facet), and
tracing the hypernym structure in Wordnet to find
the appropriate class, if one exists. Additionally,
the type indicates whether the object is a group or
not. For example, in Figure 2, the blue polygon
has type ?person,group?, or the red bike polygon
has type ?artifact,single.?
11
Similarly ?large? is ?word,noun,subject,preposition?
while ?girls? is special cased to ?word,subject,root? be-
cause it has no initial attachment. The alignment vari-
able above the word handbags is ?alignment,syntactic-
object,subject,preposition? because it an alignment variable,
is in the syntactic object position of a preposition and can be
located by following a subject attached pp.
12
WordNet divides these into synsets expressing water,
weather, nature and a few more.
117
Feature Family Included In Steps
CONTEXT(a
?
,
~
d
c
)?
{TYPE(a
?
),MENTION(a
?
, do),MENTION(a
?
, obj ),VISUAL(a
?
)}
p
a
(a
?
|
~
d
c
)
p
a
(a
?
| a,w,
~
d
c
)
1.a, 1.d, 2.b.ii
CONTEXT(a
?
,
~
d
c
)? {TYPE(a)?TYPE(a
?
),VISUAL2(a, a
?
)} p
a
(a
?
| a,w,
~
d
c
) 1.d, 2.b.i
CONTEXT(a
?
,
~
d
c
)?
{TYPE(a)?TYPE(a
?
)? STEM(w),VISUAL2(a, a
?
)? STEM(w)}
p
a
(a
?
| a,w,
~
d
c
) 1.d, 2.b.i
CONTEXT(a,
~
d
c
)?
{WORDNET(w),MATCH(w, a),SPECIFICITY(w, a),
ADJECTIVE(w, a),DETERMINER(w, a)}
p
n
(w | a,
~
d
c
) 1.b, 1.e, 2.a.i
2.b.ii
CONTEXT(a,
~
d
c
)? {MATCH(w, a),TYPE(a)? STEM(w)} p
v
(w | a,
~
d
c
) 1.c
CONTEXT(a
?
,
~
d
c
)?TYPE(a)? STEM(w
p
) p
p
(w | a,
~
d
c
)
p
p
(w | a,w
v
,
~
d
c
)
2.b.i
CONTEXT(a
?
,
~
d
c
)? STEM(w
v
)? STEM(w) p
p
(w | a,w
v
,
~
d
c
) 2.b.i
Table 5: Feature families and distributions that include them. ? indicates the cross-product of the indi-
cator features. Distributions are listed more than once to indicate they use multiple feature families.
VISUAL(a) returns indicators for visual facts
about the object that a aligns to. There is an in-
dicator for two quantities: (1) overlap of object?s
polygon with every horizontal third of the image,
as a fraction of the object?s area, and (2) the ob-
ject?s distance to the center of the image as frac-
tion of the diagonal of the image. Each quantity,
v, is put into three overlapping buckets: if v > .1,
if v > .5, and if v > .9.
VISUAL2(a, a
?
) indicates pairwise visual
facts about two objects. There is an indicator for
the following quantities bucketed: the amount of
overlap between the polygons for a and a
?
as a
fraction of the size of a?s polygon, the distance
between the center of the polygon for a and a
?
as
a fraction of image?s diagonal, and the slope be-
tween the center of a and a
?
. Each quantity, v, is
put into three overlapping buckets: if v > .1, if
v > .5, and if v > .9. There is an indicator for the
relative position of extremities a and a
?
: whether
the rightmost point of a is further right than a
?
?s
rightmost or leftmost point, and the same for top,
left, and bottom.
WORDNET(w) returns indicators for all hy-
pernyms of a word w. The two most specific
synsets are not used when there at least 8 options.
MENTION(a, facet) returns the union of the
WORDNET(w) features for all words w in the
facet facet for the object referred to alignment a.
ADJECTIVE(w, a) indicates four types
of features specific to adjective usage. If
MENTION(w,Attributes) is not empty, indi-
cate : (1) the satellite adjective synset of w in
Wordnet, (2) the head adjective synset of w in
Wordnet, (3) the head adjective synset conjoined
withTYPE(a), and (4) the number of times there
exists a label in the Attributes facet of a that has
the same head adjective synset as w.
DETERMINER(w, a) indicates four deter-
miner specific features. If w is a determiner, then
indicate : (1) the identity of w conjoined with the
count (the label for numerosity) of a, (2) the iden-
tity of w conjoined with an indicator for if the
count of a is greater than one, (3) the identity of w
conjoined with TYPE(a) and (4) the frequency
with which w appears before its head word in the
Flikr corpus (Ordonez et al., 2011).
MATCH(w, a), indicates all facets of object
a that contain words with the same stem as w.
SPECIFICITY(w, a) is an indicator of the
specificity of the word w when referring to the ob-
ject aligned to a. Indicates the relative depth of
w in Wordnet, as compared to all words w
?
where
MATCH(w
?
, a) is not empty. The depth is buck-
eted into quintiles.
STEM(w) returns the Porter2 stem of w.
13
The distribution for stopping, p
stop
(STOP |
~
d
c
, ~w), contains two types of features. (1) Struc-
tural features indicating for the number of times
a contextual identifier has appeared so far in the
derivation and (2) mention features indicating the
types of objects mentioned.
14
To compute men-
tion features, we consider all possible types of ob-
jects, t, then there is an indicator for: (1) if ?o, ?w ?
~w : MATCH(w, o) 6= ? ?TYPE(o) = t, (2) whether
?o, 6 ?w ? ~w : MATCH(w, o) 6= ??TYPE(o) = t and
(3) if (1) does not hold but (2) does.
Acknowledgments This work is partially funded by DARPA
CSSG (D11AP00277) and ARO (W911NF-12-1-0197). We
thank L. Zitnick, B. Dolan, M. Mitchell, C. Quirk, A. Farhadi,
B. Russell for helpful conversations. Also, L. Zilles, Y. Atrzi,
N. FitzGerald, T. Kwiatkowski and reviewers for comments.
13
http://snowball.tartarus.org/algorithms/english/stemmer.html
14
Object mention features cannot contain ~a because that
creates large dependencies in inference for learning.
118
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP, pages 286?295, August.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. JAIR,
37:397?435.
Marie-Catherine de Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In LREC, volume 6, pages 449?454.
Desmond Elliott and Frank Keller. 2013. Image De-
scription using Visual Dependency Representations.
In EMNLP.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Computer Vision and Pattern Recogni-
tion, 2009. CVPR 2009. IEEE Conference on, pages
1778?1785. IEEE.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: Generating sentences from images.
In Proceedings of the 11th European conference on
Computer Vision, ECCV?10, pages 15?29.
Pedro F Felzenszwalb, Ross B Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part-based
models. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 32(9):1627?1645.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? Automatic caption gener-
ation for news images. In ACL, pages 1239?1249.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logi-
cal forms for referring expression generation. In
EMNLP.
Ankush Gupta and Prashanth Mannem. 2012. From
image annotation to image description. In NIPS,
volume 7667, pages 196?204.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
ACL, pages 369?378.
Emiel Krahmer and Kees Van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173?218.
Niveda Krishnamoorthy, Girish Malkarnenkar, Ray-
mond Mooney, Kate Saenko, and Sergio Guadar-
rama. 2013. Generating natural-language video de-
scriptions using text-mined knowledge. Procedings
of AAAI, 2013(2):3.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.
Berg, and T.L. Berg. 2011. Baby talk: Understand-
ing and generating simple image descriptions. In
Computer Vision and Pattern Recognition (CVPR),
pages 1601?1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C
Berg, Tamara L Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL, pages 359?368.
Li-Jia Li and Li Fei-Fei. 2007. What, where and who?
Classifying events by scene and object recognition.
In ICCV, pages 1?8. IEEE.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris Mcnorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior Research Methods, 37(4):547?
559.
George A Miller. 1995. WordNet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daum?e, III.
2012. Midge: Generating image descriptions from
computer vision detections. In EACL, pages 747?
756.
Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2013. Generating expressions that refer to vis-
ible objects. In Proceedings of NAACL-HLT, pages
1174?1184.
F. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint Conf. of Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 20?28.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing images using 1 million
captioned photographs. In NIPS, pages 1143?1151.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL.
C. Rashtchian, P. Young, M. Hodosh, and J. Hock-
enmaier. 2010. Collecting image annotations us-
ing Amazon?s Mechanical Turk. In Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, pages 139?147.
119
Gaurav Sharma, Fr?ed?eric Jurie, Cordelia Schmid, et al.
2013. Expanded parts model for human attribute
and action recognition in still images. In CVPR.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In EMNLP, July.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In ACL, pages 572?582.
Antonio Torralba, Bryan C Russell, and Jenny Yuen.
2010. LabelMe: Online image annotation and appli-
cations. Proceedings of the IEEE, 98(8):1467?1484.
Daniel Weinland, Remi Ronfard, and Edmond Boyer.
2011. A survey of vision-based methods for action
representation, segmentation and recognition. Com-
puter Vision and Image Understanding, 115(2):224?
241.
Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing.
Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai
Lin, Leonidas J. Guibas, and Li Fei-Fei. 2011.
Action recognition by learning bases of action at-
tributes and parts. In ICCV, Barcelona, Spain,
November.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, volume 1, pages 53?63.
Annie Zaenen, Jean Carletta, Gregory Garretson,
Joan Bresnan, Andrew Koontz-Garboden, Tatiana
Nikitina, M Catherine O?Connor, and Tom Wasow.
2004. Animacy encoding in English: why and how.
In ACL Workshop on Discourse Annotation, pages
118?125.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
C. Lawrence Zitnick and Devi Parikh. 2013. Bring-
ing semantics into focus using visual abstraction. In
CVPR.
120
