Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 34?42,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
UM-Checker: A Hybrid System for English Grammatical Error Cor-
rection 
 
 
Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,  
Department of Computer and Information Science, 
University of Macau, Macau S.A.R., China 
nlp2ct.{vincent, anson}@gmail.com,  
{derekfw, lidiasc}@umac.mo, nlp2ct.samuel@gmail.com 
 
  
 
Abstract 
This paper describes the NLP2CT Grammati-
cal Error Detection and Correction system for 
the CoNLL 2013 shared task, with a focus on 
the errors of article or determiner (ArtOrDet), 
noun number (Nn), preposition (Prep), verb 
form (Vform) and subject-verb agreement 
(SVA). A hybrid model is adopted for this spe-
cial task. The process starts with spell-
checking as a preprocessing step to correct any 
possible erroneous word. We used a Maxi-
mum Entropy classifier together with manual-
ly rule-based filters to detect the grammatical 
errors in English. A language model based on 
the Google N-gram corpus was employed to 
select the best correction candidate from a 
confusion matrix. We also explored a graph-
based label propagation approach to overcome 
the sparsity problem in training the model. Fi-
nally, a number of deterministic rules were 
used to increase the precision and recall. The 
proposed model was evaluated on the test set 
consisting of 50 essays and with about 500 
words in each essay. Our system achieves the 
5
th
 and 3
rd
 F1 scores on official test set among 
all 17 participating teams based on gold-
standard edits before and after revision, re-
spectively.  
1 Introduction 
With the increasing number of people all over 
the world who study English as their second lan-
guage1, grammatical errors in writing often oc-
curs due to cultural diversity, language habits, 
education background, etc. Thus, there is a sub-
stantial and increasing need of using computer 
                                                 
    1 A well-known fact is that the most popular language 
chosen as a first foreign language is English. 
techniques to improve the writing ability for sec-
ond language learners. Grammatical error correc-
tion is the task of automatically detecting and 
correction erroneous word usage and ill-formed 
grammatical constructions in text (Dahlmeier et 
al., 2012). 
In recent decades, this special task has gained 
more attention by some organizations such as the 
Helping Our Own (HOO) challenge (Dale and 
Kilgarriff, 2010; Dale et al, 2012). Although the 
performance of grammatical error correction sys-
tems has been improved, it is still mostly limited 
to dealing with the determiner and preposition 
error types with a very low recall and precision. 
This year, the CoNLL-2013 shared task extends 
to include a more comprehensive list of error 
types, as shown in Table 1. 
To take on this challenge, this paper proposes 
pipe-line architecture in combination with sever-
al error detection and correction models based on 
a hybrid approach. As a preprocessing step we 
firstly employ a spelling correction to correct the 
misspelled words. To correct the grammatical 
errors, a hybrid system is designed that integrat-
ed with Maximum Entropy (ME) classifier, de-
terministic filter and N-gram language model 
scorer, each of which is constructed as an indi-
vidual model. According to the phenomena of 
the problems, we use different combinations of 
the models trained on specific data to tackle the 
corresponding types of errors. For instance, Prep 
and Nn have a strong inter-relation with the 
words (surface) that are preceding and following 
the active word. This can be detected and recov-
ered by using a language model. On the other 
hand, SVA is more complicated and it is more 
effective to determine the mistakes by using the 
linguistic and grammatical rules. The correction
34
Error Type Description Example 
Vform 
Replacement The solution can be obtain (obtained) by using technology. 
Insertion 
However, the world has always beyond our imagination and ? (has) 
never let us down. 
Deletion It also indicates that the economy has been (?) dramatically grown. 
SVA 
Subject-verb-
Agreement 
My brothers is (are) nutritionists. 
ArtOrDet 
Replacement 
The leakage of these (this) confidential information can be a sensitive 
issue to personal, violation of freedom and breakdown of safety. 
Insertion The survey was done by ? (the) United Nations. 
Deletion 
The air cargo of the (?) Valujet plane was on fire after the plane had 
taken off. 
Nn Noun number He receives two letter (letters). 
Prep 
Replacement They work under (in) a conductive environment. 
Insertion 
Definitely, there are point of view that agree ? (with) the technology 
but also the voices of objection. 
Deletion 
Today, the surveillance technology has become almost manifest to (?) 
wherever we go. 
 
Table 1: The error types with descriptions and examples. 
 
components are combined into a pipeline of cor-
rection steps to form an end-to-end correction 
system. Different types of corrections may inter-
act with each other. Therefore, only for each fo-
cus word in a sentence will pass the filter and 
predict by the system. 
Take the sentence for example, ?The patent 
applications do not need to be censored.?, if the 
word ?applications? is changed to ?application? 
(Nn error) by a correction module, then the fol-
lowing auxiliary verb ?do? should be revised to 
?does? (SVA error) accordingly. That is, if a mis-
take is introduced by a component in the prior 
step, subsequent analyses are most likely affect-
ed negatively. To avoid the errors propagated 
into further components, we proposed to deploy 
the analytical (pipelined) components in the or-
der of Nn, ArtOrDet, Vform, SVA and Prep. 
For non-native language learners, over 90% 
usage of prepositions and articles are correctly 
used, which makes the errors very sparse (Ro-
zovskaya and Roth, 2010c) in a text, and about 
10% error is not ?sparse? by the way. This factor 
severely restricts the improvement of data-driven 
systems. Different from the previous methods to 
overcome error sparsity, we explored a graph-
based label propagation method that makes use 
of the prediction on large amount of unlabeled 
data. The predicted data are then used to 
resample our training data. This semi-supervised 
method may fix a skewed label distribution in the 
training set and is helpful to enhance the models.  
The paper is organized as follows. We firstly 
review and discuss the related work. The data 
used to construct the models is described in Sec-
tion 3. Section 4 discusses the proposed model 
based on semi-supervised learning, and the over-
all hybrid system is given in Section 5. The 
methods of grammatical error detection and cor-
rection are detailed in Section 6, followed by an 
evaluation, discussion and a conclusion to end 
the paper. 
2 Related Work 
The issues of grammatical error correction have 
been discussed from different perspectives for 
several decades. In this section, we briefly re-
view some related methods. 
The use of machine learning methods to tackle 
this problem has shown a promising perfor-
mance. These methods are normally created 
based on a large corpus of well-formed native 
English texts (Tetreault and Chodorow 2008; 
Tetreault et al, 2010) or annotated non-native 
data (Gamon, 2010; Han et al, 2010). Although 
the manually error-tagged text is much more ex-
pensive, it has shown improvements over the 
models trained solely on well-formed native text 
(Kochmar et al, 2012). Additionally, both gener-
ative and discriminative classifiers were widely 
used. Among them, Maximum Entropy was gen-
erally used (Rozovskaya and Roth, 2011; 
Sakaguchi et al, 2012; Quan et al, 2012) and 
obtained a good result for preposition and article 
correction using a large feature set. Naive Bayes 
35
were also applied to recognize or correct the er-
rors in speech or texts (Lynch et al, 2012). How-
ever, only using classifiers always cannot give a 
satisfied performance. Thus, grammar rules and 
probabilistic language model can be used as a 
simple but effective assistant for correction of 
spelling (Kantrowitz et al 2003) and grammati-
cal errors (Dahlmeier et al, 2012; Lynch et al, 
2012; Quan et al, 2012; Rozovskaya et al, 
2012). 
3 Data Set 
The training data is the NUS Corpus of Learner 
English (NUCLE) that provided by the National 
University of Singapore (Dahlmeier et al, 2013). 
The NUCLE contains more than one million 
words (1,400 essays) and has been annotated 
with error-tags and correction-labels. There are 
27 categories of errors, with 45,106 errors in to-
tal. In this CoNLL-2013 shared task, five types 
of errors (around 32% of the total errors) are 
concerned. Figure 1 shows the statistics infor-
mation of error types. 
 
 
 
Figure 1. The distribution of different error types in 
the training set. 
 
As the distribution of different errors respects 
the real environment, there is a serious problem 
hidden in it. Roughly estimated, the ratio be-
tween the correct and error classes in NUCLE is 
around 100:1, or even more. The imbalance 
problem may be heavily harmful to machine 
learning methods. Therefore, researchers (Ro-
zovskaya et al, 2012; Dahlmeier et al, 2012) 
provided several approaches such as reducing 
correct instances to deal with error sparsity. In-
stead of downsampling the data, we try to up-
sample error instances. Different from UI system 
(Rozovskaya et al, 2012) which simulates learn-
ers to make mistakes artificially, we propose a 
semi-supervised learning method that makes use 
of a large amount of unlabeled data which is easy 
to collect. In practice, semi-supervised learning 
requires less human effort and gives higher accu-
racy in creating a model.  
4 Error Examples Expansion Using 
Graph-Based Label Propagation  
As mentioned before, the corpus contains a low 
amount of error examples, which results in a 
high sparsity in the label distribution. In reality, 
the balance between the error and correct data is 
crucial for training a robust grammar detection 
models. Our experiment results demonstrate that 
too many correct data lead to unfavorable error 
detection rate. In order to resolve this obstacle, 
this paper introduces to using external data 
sources, i.e., a large amount of easily accessible 
raw texts, to automatically achieve more labeled 
example for training a stronger model. This pa-
per employs transductive graph-based semi-
supervised learning approach. 
4.1 Graph-Based Label Propagation 
Graph-based label propagation is one of the criti-
cal subclasses of SSL. Graph-based label propa-
gation methods have recently shown they can 
outperform the state-of-the-art in several natural 
language processing (NLP) tasks, e.g., POS tag-
ging (Subramanya et al, 2010), knowledge ac-
quisition (Talukdar et al, 2008), shallow seman-
tic parsing for unknown predicate (Das and 
Smith, 2011).  This study uses graph SSL to en-
rich training data, mainly the examples with in-
correct tag, from raw texts.  
This approach constructs a k nearest-neighbor 
(k-nn) similarity graph over the labeled and un-
labeled data in the first step. The vertices in the 
constructed graph consist of all instances (feature 
vector) that occur in labeled and unlabeled text, 
and edge weights between vertices are computed 
using their Euclidean distance. Pairs of vertices 
are connected by weighted edges which encode 
the degree to which they are expected to have the 
same label (Zhu, 2003). In the second step, label 
propagation operates on the constructed graph. 
The primary objective is to propagate labels from 
a few labeled vertices to the unlabeled ones by 
optimizing a loss function based on the con-
straints or properties derived from the graph, e.g. 
smoothness (Zhu et al, 2003; Subramanya and 
Bilmes, 2008; Talukdar et al, 2009), or sparsity 
(Das and Smith, 2012). This paper uses propaga-
tion method (MAD) in (Talukdar et al, 2009).  
Vform
9%
SVA
10%
ArtOrDet
42%
Nn
24%
P ep
15%
36
  
 
Figure 2. Workflow of our proposed system. 
4.2 Implementation 
In this paper, the labeled data is taken from NU-
CLE corpus. They are regarded as the ?seed? 
data, including 93,000 correct and 1,200 incor-
rect instances. The unlabeled data is collected 
from the English side of news magazine corpus 
(LDC2005T10). Based on that, a 5-NN similarity 
graph is constructed. With the graph and the 
properties of the labeled data derived from the 
NUCLE, the MAD algorithm is used to propa-
gate the error-tag (label) from labeled vertices to 
the unlabeled vertices. Afterwards, the unlabeled 
examples with incorrect tag are added into the 
original training data for training. 
5 System Description 
This section describes the details of our system, 
including preprocessing of training set, confusion 
set generating, classifier training and language 
models building. The grammatical error correc-
tion procedure is shown in Figure 2. 
5.1 Preprocessing 
As mentioned in Section 3, there is a large 
amount (68%) of other error types which may 
result in new errors or confuse the system with 
wrong information in correction. In order to 
make the best use of the corpus, it needs to filter 
all errors not covered by the CoNLL 2013 shared 
task, and then generate a separate corpus for each 
error type. Therefore, we recovered other irrele-
vant errors accordingly. For each error type, we 
also recover other 4 types of errors, and then we 
got a pure training data set which only includes 
one error type.  
For the misspelled problem, we used an open 
source toolkit (JMySpell2) which allows us to 
use the dictionaries form OpenOffice. JMySpell 
                                                 
    2 Available at https://kenai.com/projects/jmyspell. 
gives a list of suggestion candidate words, and 
we select the first one to replace the original 
word.  
5.2 Confusion Set Generating 
Confusion sets include the correction candidates 
which are used to modify the wrong places of a 
sentence. We generated a confusion set for each 
type of error correction component.  
The confusion set for Nn, Vform and SVA was 
built on Penn Treebank3. The format can be de-
scribed as that each prototype word follows all 
possible combinations with Part-Of-Speech (POS) 
and variants. For instance, the format of the word 
?look? in confusion set should looks like ?look 
look#VB look#VBP looking#VBG looks#VBZ 
looked#VBN look#NN looks#NNS?. The proto-
type ?look? and POS are the constraints for 
choosing the correct candidate. In order to quick-
ly find the candidates according to each detected 
error place, we indexed the confusion set in Lu-
cene4 which is another open source toolkit with a 
high-performance, full-featured text search en-
gine library. 
For ArtOrDet and Prep, the confusion sets are 
manually created because the possible modifica-
tions are not so many which are discussed in 
Section 6.1 and 6.2. 
5.3 Maximum Entropy Classifier 
The machine learning algorithm we used to train 
the detection models is Maximum Entropy (ME), 
which can classify the data by giving a probabil-
ity distribution. It is similar to multiclass logistic 
regression models, but much more profitable 
with sparse explanatory feature vectors. For ME 
classifier, the feature of text data is suitable for 
training the model, so we choose it as our detec-
tion classifier.  
                                                 
    3 Available at http://www.cis.upenn.edu/~treebank/. 
    4 Available at http://lucene.apache.org/. 
Source
Text
Rule-based 
Filter
ArtOrDet
LM Scorer
Nn
ME
classifier
LM Scorer
Vform
SVA
ME
classifier
Rule-based 
Filter
Rule-based 
Filter
Rule-based 
Filter
Prep
LM Scorer
Hybrid System
Correct
Text
37
We employed Stanford Classifier5 which is a 
Java implementation of maximum entropy 
(Manning & Klein, 2003).  
5.4 N-gram Language Model 
The probabilistic language model is constructed 
on Google Web 1T 5-gram corpus (Brants and 
Franz, 2006) by using the SRILM toolkit 
(Stolcke, 2002). All generated modification can-
didates are scored by it and only candidates that 
strictly increase than a threshold can be kept.  
The normalized language model score is de-
fined as 
1 log Pr( )lmscore ss?
                (1) 
in which s is the corrected sentence and |s| is the 
sentence length in tokens (Dahlmeier et al, 
2012). 
6 Grammatical Error Correction 
6.1 Article and Determiner 
The component for ArtOrDet task integrates with 
the language model and rule-based techniques. 
Language models are constructed to select the 
best candidate from a confusion set of possible 
article choices {a, the, an, ?}, given the pre-
corrected sentence. Each Noun Phrase (NP) in 
the test sentence will be pre-corrected as correc-
tion candidates. However, only using a language 
model to determine the best correction will often 
result in a low precision, because a certain 
amount of correct usages of ArtOrDet are mis-
judged. 
In order to avoid this problem, we proposed a 
voting method based on multiple language mod-
els. We integrated two separate language models: 
one was converted from the large Google corpus 
(general LM) and the other one was constructed 
from a small in-domain corpus (in-domain LM). 
Additionally, the in-domain corpus involves two 
parts. One is the training data which has been 
totally corrected according to the gold answer. 
The other one includes the sentences which are 
similar to the test set. We extracted them from 
some well-formed native English corpora such as 
English News Magazine of LDC2005T106 using 
term frequency-inverse document frequency (TF-
IDF) as the similarity score. Each document Di is 
                                                 
    5 Available at 
http://nlp.stanford.edu/software/classifier.shtml. 
    
6
 Available at http://www.ldc.upenn.edu/Catalog/catalog 
Entry.jsp?catalogId=LDC2005T10. 
represented as a vector (wi1, wi2,?, win), and n is 
the size of the vocabulary. So wij is calculated as 
follows: 
 )log( jijij idftfw ??  (2) 
where tfij is term frequency (TF) of the j-th word 
in the vocabulary in the document Di, and idfj is 
the is the inverse document frequency (IDF) of 
the j-th word calculated. The similarity between 
two sentences is then defined as the cosine of the 
angle between two vectors.  
Each candidate sentence will be scored by 
these two LMs and compared with a threshold. 
Only if both of the LMs agree, the modification 
will be kept. We believe this method could filter 
a lot of wrong modification and improve the pre-
cision. 
6.2 Preposition 
For Prep error type, we used the same method as 
ArtOrDet. The only difference is confusion ma-
trix. Our system corrects the unnecessary, miss-
ing and unwanted errors for the five most fre-
quently prepositions which are in, for, to, of and 
on. While developing our system, we found that 
adding more prepositions did not increase per-
formance in our experiments. Thus the confusion 
set is {in, for, to, of, on, ?}. 
6.3 Noun Number 
A single noun in the sentence that is hard to dis-
tinguish whether it is singular or plural, so we 
treat a noun phrase as a observe subject. Our 
strategy of correcting noun number error is to use 
a filter contains rule-based and machine learning 
method. It can filter a part of nouns that absolute-
ly right, and the rest of nouns will be detected by 
the language model generated by SRILM7. 
The rule-based filter of our system contains 
several criteria. It can detect the noun phrase by 
article, i.e. it can simply find out that the noun is 
singular which with an article of ?a? or ?an?. 
The determiner and cardinal number also will be 
taken into consider by the rule-based model such 
as ?I have three apple.?, then system can find out 
the ?apple? should be ?apples?. The correct noun 
will keep the original one, and the incorrect noun 
will be replaced with a new candidate. 
After the first level filtering by the rules, the 
rest of noun phrases are indeterminacy by system. 
Therefore, we use a ME classifier for further fil-
tering. We use lexical, POS and dependency 
                                                 
    7 http://www.speech.sri.com/projects/srilm/. 
38
parse information as features. The features are 
listed in Table 2.  
In previous steps, most of the error can be de-
tected, but also it may give a lot of wrong sug-
gests, in order to reduce this situation, we use N-
gram language model scorer to evaluate on the 
candidates and choose the highest probability 
one. 
 
Feature Example 
Observer word 
Word (w0) resource 
POS (p0) NN 
First word in NP 
Word (wNP-1st) a 
POS (pNP-1st) DT 
Dependency Relation det 
Previous word before observed word 
Word (w-1) good 
POS (p-1) JJ 
Word after observed word 
Word (w1) and 
POS (p1) CC 
Head word of observed word 
Word (whead) water 
POS (phead) NN 
Dependency relation rcomd 
Word Combination 
w0 + wNP-1st resource + a 
w0 + w-1 resource + good 
w0 + w1 resource + and 
w0 + whead resource + water 
wNP-1st + whead a + water 
POS Combination 
p0 + pNP-1st NN + DT 
p0 + p-1 NN + JJ 
p0 + p1 NN + CC 
p0 + phead NN + NN 
pNP-1st + phead DT + NN 
 
Table 2: Features for Nn and the example: ?An exam-
ple is water which is a good resource and is plentiful.? 
6.4 Verb Form 
Determining the correct form of a verb in Eng-
lish is complex, involving a relatively wide range 
of choices. A verb can have many forms, such as 
base, gerund, preterite, past participle and so on. 
To detect the tense of verb error is much more 
related to the semantics level than syntax level. 
Therefore, it is hard to extract a common feature 
for training model. We chose to separate it into 
several problems and use rule-based model to do 
the Vform correction. 
For auxiliary verbs, there are three categories, 
one is modal verbs (do, can, may, will, might, 
should, must, need and dare), the other is the 
form of ?be? and ?have?. In a verb phrase, nor-
mally modals precede ?have? and ?be?, and 
?have? proceed ?be?, then we can get the order-
ing like this: Modal, Have, Be. Auxiliary verbs 
can incorporate with other verbs, and have dif-
ferent combination. Based on the previous study 
of the core language engine (Alshawi, 1992), we 
define the rules that contain the type of verb, 
which tense of verbs can be used with, and their 
entries in the lexicon. For example: 
 
(can (aux (modal) (vform pres)  (COMPFORM bare)) 
 
This means ?can? is a modal verb, it can be 
used with a verb that in the present tense, when 
?can? used alone with the main verb should as 
complement the base (bare) form. In here, the 
COMPFORM attribute is the entry condition in 
the grammar.  
6.5 Subject-Verb Agreement 
The basic principle of Subject-Verb Agreement 
is singular subjects need singular verbs; plural 
subjects need plural verbs, such as following sen-
tences: 
My brother is a nutritionist. 
My sisters are dancers. 
Therefore, the subject of the sentence is the 
key point. To decide whether the verb is singular 
or plural should look into the context and find 
out the POS of the subject. We utilize the exist-
ing information given by NUCLE to extract the 
subject of the verb. For example, the sentence 
?Statistics show that the number are continuing 
to grow with the existing population explosion.? 
Figure 3 shows the parse tree of this sentence. 
 
Figure 3. Parse tree of the example sentence. 
Root
S1
NP1
VP1
VBP1NNPS
SBAR
IN1
S2
NP2 VP2
?DT2 NN2 VBP2
arenumberthe
that
showStatistics
.
.
?
39
Through Figure 3, the observed words are 
?show? and ?are?, the subjects are ?statistics? 
and ?number? respectively that we can conclude 
?statistics? should use plural verb and ?number? 
should use singular verb ?is? instead of ?are?. 
The other features extracted for training are 
listed in Table 3. 
 
Feature Example 
Observer word 
Word (w0) are 
POS (p0) VBP 
Subject NP 
First word (wNP-1st) the 
POS of first word (pNP-1st) DT 
Head word (wNP-head) number 
POS of head word (pNP-head) NN 
Previous word before observed word 
Word (w-1) number 
POS (p-1) NN 
NP after observed word 
First word (wNPa-1st) the 
POS of first word (pNPa-1st) DT 
Head word (wNPa-head) explosion 
POS of head word (pNPa-head) NN 
Word combination 
w0 + wNP-1st are + the 
w0 + wNP-head are + number 
w0 + w-1 are + number 
w0 + wNPa-1st are + the 
w0 + wNPa-head are + explosion 
POS combination 
p0 + pNP-1st VBP + DT 
p0 + pNP-head VBP + NN 
p0 + p-1 VBP + NN 
p0 + pNPa-1st VBP + DT 
p0 + pNPa-head VBP + NN 
 
Table 3: Features for SVA and the example: ?Statis-
tics show that the number are continuing to grow with 
the existing population explosion.? 
 
The purpose of extracting the noun phrase af-
ter the observed word is in the situation of the 
subject is after the verb, such as ?Where are my 
scissors??, ?scissors? is the subject of this sen-
tence. 
7 Evaluation and Discussion 
The evaluation is provided by the organizer and 
generated by M2 scorer (Dahlmeier & Ng, 2012). 
The result consists of precision, recall and F-
score. Our grammatical error correction system 
has proposed 1,011 edits. The evaluation result 
of our system output for the CoNLL-2013 test 
data is shown in Table 4. 
 
Results Precision Recall F-score 
Before 
Revision 
0.2849 0.1753 0.2170 
After  
Revision 
0.3712 0.2366 0.2890 
 
Table 4: Evaluation result of Precision, Recall and F-
score. 
 
Error Type Error # Correct # % 
ArtOrDet 690 145 21.01 
Nn 396 92 23.23 
Vform 122 8 6.55 
SVA 124 37 29.83 
Prep 311 6 1.93 
 
Table 5: Detail information of evaluation result (Be-
fore Revision). 
 
Error Type Error # Correct # % 
ArtOrDet 725 177 24.42 
Nn 484 132 27.27 
Vform 151 16 10.60 
SVA 138 47 34.06 
Prep 325 9 2.77 
 
Table 6: Detail information of evaluation result (After 
Revision). 
 
The data in table 5 and 6 are the detailed in-
formation for each error type which was calcu-
lated by us, the table 5 is the data before revision, 
and the table 6 is that after revision. Second col-
umn is the amount of the gold edits, and the third 
column is the amount of our correct edits, and 
the last column is the percentage of correct edits. 
We analyzed the results in detail, and found sev-
eral critical reasons of causing low recall. Firstly, 
the five error types are associated relatively, if 
one is modified, it may cause a chain reaction, 
such as the article will affect the noun number, 
and the noun number will cause the SVA errors. 
Some Nn errors still cannot be detected or given 
a wrong correction by our system, which de-
creases the precision and recall of SVA. Another 
reason is our system does not perform well in 
Vform and Prep error correction. In our output, 
just a few errors have been revised. This means 
the quantity of correction rules is not enough that 
cannot cover all the linguistic phenomena. For 
40
instance, the situation of missing verb or unnec-
essary verb cannot be detected. On the other 
hand, the hybrid method of our system has fil-
tered some wrong suggestion candidates that im-
prove the precision. 
8 Conclusion 
We have presented the hybrid system for English 
grammatical error correction. It achieves a 28.9% 
F1-score on the official test set. We believe that if 
we find more appropriate features, our system 
can still be improved and achieve a better per-
formance. 
 
Acknowledgments 
 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for our research, 
under the reference No. 017/2009/A and 
MYRG076(Y1-L2)-FST13-WF. The authors also 
wish to thank the anonymous reviewers for many 
helpful comments as well as Liangye He, Yuchu 
Lin and Jiaji Zhou who give us a lot of help. 
References  
Hiyan Alshawi. 1992. The core language engine. The 
MIT Press. 
Jon Louis Bentley. 1980. Multidimensional divide-
and-conquer. Communications of the ACM, 
23:214?229. 
Alina Beygelzimer, Sham Kakade, and John Lang-
ford. 2006. Cover trees for nearest neighbor. In: 
Proceedings of the 23rd International Confer-
ence on Machine Learning, pp. 97?104. 
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1. Linguistic Data Consortium, 
Philadelohia, PA. 
Olivier Chapelle, Bernhard Sch?lkopf, Alexander 
Zien, and others. 2006. Semi-supervised learning. 
MIT press Cambridge. 
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng 
Ng. 2012. NUS at the HOO 2012 Shared Task. In: 
Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 216?224. 
Daniel Dahlmeier & Hwee Tou Ng, and Siew Mei 
Wu (2013). Building a Large Annotated Corpus of 
Learner English: The NUS Corpus of Learner Eng-
lish. To appear in Proceedings of the 8th Work-
shop on Innovative Use of NLP for Building 
Educational Applications (BEA 2013). Atlanta, 
Georgia, USA. 
Daniel Dahlmeier, and Hwee Tou Ng (2012). Better 
Evaluation for Grammatical Error Correction. 
Proceedings of the 2012 Conference of the 
North American Chapter of the Association 
for Computational Linguistics (NAACL 2012), 
pp. 568 ? 572. 
Robert Dale, Ilya Anisimoff, and George Narroway. 
2012. HOO 2012: A report on the preposition and 
determiner error correction shared task. In: Pro-
ceedings of the Seventh Workshop on Building 
Educational Applications Using NLP, pp. 54?
62. 
Robert Dale and Adam Kilgarriff. 2011. Helping our 
own: The HOO 2011 pilot shared task. In: 
Proceedings of the 13th European Workshop 
on Natural Language Generation, pp. 242?249. 
Dipanjan Das and Noah A. Smith 2012. Graph-based 
lexicon expansion with sparsity-inducing penalties. 
In: Proceedings of the 2012 Conference of the 
North American Chapter of the Association 
for Computational Linguistics: Human Lan-
guage Technologies, pp. 677?687. 
Michael Gamon. 2010. Using mostly native data to 
correct errors in learners? writing: a meta-classifier 
approach. In: Human Language Technologies: 
The 2010 Annual Conference of the North 
American Chapter of the Association for 
Computational Linguistics, pp. 163?171. 
Andrew B. Goldberg and Xiaojin Zhu. 2006. Seeing 
stars when there aren?t many stars: graph-based 
semi-supervised learning for sentiment categoriza-
tion. In: Proceedings of the First Workshop on 
Graph Based Methods for Natural Language 
Processing, pp. 45?52. 
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an error-annotated learner 
corpus to develop an ESL/EFL error correction 
system. In: Proceedings of LREC, pp. 763?770. 
Mark Kantrowitz. 2003. Method and apparatus for 
analyzing affect and emotion in text. U.S. Patent 
No. 6,622,140. 
Ekaterina Kochmar. 2011. Identification of a writer?s 
native language by error analysis. Master?s thesis, 
University of Cambridge. 
Gerard Lynch, Erwan Moreau, and Carl Vogel. 2012. 
A Naive Bayes classifier for automatic correction 
of preposition and determiner errors in ESL text. 
In: Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 257?262. 
41
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, Maxent Models, and Conditional Estimation 
without Magic. Tutorial at HLT-NAACL 2003 
and ACL 2003. 
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian 
Hadiwinoto, and Joel Tetreault (2013). The 
CoNLL-2013 Shared Task on Grammatical Error 
Correction. To appear in Proceedings of the Sev-
enteenth Conference on Computational Natu-
ral Language Learning. 
Li Quan, Oleksandr Kolomiyets, and Marie-Francine 
Moens. 2012. KU Leuven at HOO-2012: a hybrid 
approach to detection and correction of determiner 
and preposition errors in non-native English text. 
In: Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 263?271. 
Juan Ramos. 2003. Using tf-idf to determine word 
relevance in document queries. In: Proceedings of 
the First Instructional Conference on Machine 
Learning. 
Alla Rozovskaya and Dan Roth. 2010. Training para-
digms for correcting errors in grammar and usage. 
In: Human Language Technologies: The 2010 
Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics, pp. 154?162. 
Alla Rozovskaya, Mark Sammons, and Dan Roth. 
2012. The UI system in the HOO 2012 shared task 
on error correction. In: Proceedings of the Sev-
enth Workshop on Building Educational Ap-
plications Using NLP, pp. 272?280. 
Keisuke Sakaguchi, Yuta Hayashibe, Shuhei Kondo, 
Lis Kanashiro, Tomoya Mizumoto, Mamoru Ko-
machi, and Yuji Matsumoto. 2012. NAIST at the 
HOO 2012 Shared Task. In: Proceedings of the 
Seventh Workshop on Building Educational 
Applications Using NLP, pp. 281?288. 
Andreas Stolcke and others. 2002. SRILM-an exten-
sible language modeling toolkit. In: Proceedings 
of the International Conference on Spoken 
Language Processing, pp. 901?904. 
Partha Pratim Talukdar and Koby Crammer. 2009. 
New regularized algorithms for transductive learn-
ing. In: Machine Learning and Knowledge 
Discovery in Databases. Springer, pp. 442?457. 
Joel Tetreault, Jennifer Foster, and Martin Chodorow. 
2010. Using parse features for preposition selection 
and error detection. In: Proceedings of the Acl 
2010 Conference Short Papers, pp. 353?358. 
Joel R. Tetreault and Martin Chodorow. 2008. The 
ups and downs of preposition error detection in 
ESL writing. In: Proceedings of the 22nd Inter-
national Conference on Computational Lin-
guistics Volume 1, pp. 865?872. 
 
42
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 83?90,
Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational Linguistics
Factored Statistical Machine Translation for Grammatical Error 
Correction 
 
 
Yiming Wang, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng, Yi Lu 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,  
Department of Computer and Information Science, 
University of Macau, Macau S.A.R., China 
{wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, 
lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo 
 
  
 
Abstract 
This paper describes our ongoing work on 
grammatical error correction (GEC). Focusing 
on all possible error types in a real-life 
environment, we propose a factored statistical 
machine translation (SMT) model for this task. 
We consider error correction as a series of 
language translation problems guided by 
various linguistic information, as factors that 
influence translation results. Factors included 
in our study are morphological information, i.e. 
word stem, prefix, suffix, and Part-of-Speech 
(PoS) information. In addition, we also 
experimented with different combinations of 
translation models (TM), phrase-based and 
factor-based, trained on various datasets to 
boost the overall performance. Empirical 
results show that the proposed model yields an 
improvement of 32.54% over a baseline 
phrase-based SMT model. The system 
participated in the CoNLL 2014 shared task 
and achieved the 7
th
 and 5
th
 F0.5 scores
1 on the 
official test set among the thirteen 
participating teams. 
 
1 Introduction 
The task of grammatical error detection and 
correction (GEC) is to make use of 
computational methods to fix the mistakes in a 
written text. It is useful in two aspects. For a 
non-native English learner it may help to 
improve the grammatical quality of the written 
text. For a native speaker the tool may help to 
remedy mistakes automatically. Automatic 
                                                          
1  These two rankings are based on gold-standard edits 
without and with alternative answers, respectively. 
correction of grammatical errors is an active 
research topic, aiming at improving the writing 
process with the help of artificial intelligent 
techniques. Second language learning is a user 
group of particular interest. 
Recently, Helping Our Own (HOO) and 
CoNLL held a number of shared tasks on this 
topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 
2014). Previous studies based on rules (Sidorov 
et al., 2013), data-driven methods (Berend et al., 
2013, Yi et al., 2013) and hybrid methods (Putra 
and Szab?, 2013, Xing et al., 2013) have shown 
substantial gains for some frequent error types 
over baseline methods. Most proposed methods 
share the commonality that a sub-model is built 
for a specific type of error, on top of which a 
strategy is applied to combine a number of these 
individual models. Also, detection and correction 
are often split into two steps. For example, Xing 
et al. (2013) presented the UM-Checker for five 
error types in the CoNLL 2013 shared task. The 
system implements a cascade of five individual 
detection-and-correction models for different 
types of error. Given an input sentence, errors are 
detected and corrected one-by-one by each sub-
model at the level of its corresponding error type.  
The specifics of an error type are fully 
considered in each sub-model, which is easier to 
realize for a single error type than for multiple 
types in a single model. In addition, dividing the 
error detection and correction into two steps 
alleviates the application of machine learning 
classifiers. However, an approach that considers 
error types individually may have negative 
effects: 
? This approach assumes independence 
between each error type. It ignores the 
interaction of neighboring errors. Results 
(Xing et al., 2013) have shown that 
83
consecutive errors of multiple types tend to 
hinder solving these errors individually. 
? As the number of error types increases, the 
complexities of analyzing, designing, and 
implementing the model increase, in 
particular when combinatorial errors are 
taken into account. 
? Looking for an optimal model combination 
becomes complex. A simple pipeline 
approach would result in interference and the 
generation of new errors, and hence to 
propagating those errors to the subsequent 
processes. 
? Separating the detection and correction tasks 
may result in more errors. For instance, once 
a candidate is misidentified as an error, it 
would be further revised and turned into an 
error by the correction model. In this 
scenario the model risks losing precision. 
In the shared task of this year (Ng et la., 
2014), two novelties are introduced: 1) all types 
of errors present in an essay are to be detected 
and corrected (i.e., there is no restriction on the 
five error types of the 2013 shared task); 2) the 
official evaluation metric of this year adopts F0.5, 
weighting precision twice as much as recall. This 
requires us to explore an alternative universal 
joint model that can tackle various kinds of 
grammatical errors as well as join the detection 
and correction processes together. Regarding 
grammatical error correction as a process of 
translation has been shown to be effective (Ehsan 
and Faili, 2013, Mizumoto et al., 2011, 
Yoshimoto et al., 2013, Yuan and Felice, 2013). 
We treat the problematic sentences and golden 
sentences as pairs of source and target sentences. 
In SMT, a translation model is trained on a 
parallel corpus that consists of the source 
sentences (i.e. sentences that may contain 
grammatical errors) and the targeted translations 
(i.e. the grammatically well-formed sentences). 
The challenge is that we need a large amount of 
these parallel sentences for constructing such a 
data-driven SMT system. Some researches 
(Brockett et al., 2006, Yuan and Felice, 2013) 
explore generating artificial errors to resolve this 
sparsity problem. Other studies (Ehsan and Faili, 
2013, Yoshimoto et al., 2013, Yuan and Felice, 
2013) focus on using syntactic information (such 
as PoS or tree structure) to enhance the SMT 
models.  
In this paper, we propose a factored SMT 
model by taking into account not only the surface 
information contained in the sentence, but also 
morphological and syntactic clues (i.e., word 
stem, prefix, suffix and finer PoS information). 
To counter the sparsity problem we do not use 
artificial or manual approaches to enrich the 
training data. Instead we apply factored and 
transductive learning techniques to enhance the 
model on a small dataset. In addition, we also 
experimented with different combinations of 
translation models (TM), phrase- and factor-
based, that are trained on different datasets to 
boost the overall performance. Empirical results 
show that the proposed model yields an 
improvement of 32.54% over a baseline phrase-
based SMT model. 
The remainder of this paper is organized as 
follows: Section 2 describes our proposed 
methods. Section 3 reports on the design of our 
experiments. We discuss the result, including the 
official shared task results, in Section 4,. We 
summarize our conclusions in Section 5. 
2 Methodology 
In contrast with phrase-based translation models, 
factored models make use of additional linguistic 
clues to guide the system such that it generates 
translated sentences in which morphological and 
syntactic constraints are met (Koehn and Hoang, 
2007). The linguistic clues are taken as factors in 
a factored model; words are represented as 
vectors of factors rather than as a single token. 
This requires us to pre-process the training data 
to factorize all words. In this study, we explore 
the use of various types of morphological 
information and PoS as factors. For each possible 
factor we build an individual translation model. 
The effectiveness of all factors is analyzed by 
comparing the performance of the corresponding 
models on the grammatical error correction task. 
Furthermore, two approaches are proposed to 
combine those models. One adopts the model 
cascading method based on transductive learning. 
The second approach relies on learning and 
decoding multiple factors learning. The details of 
each approach are discussed in the following 
sub-sections. 
2.1 Data Preparation 
In order to construct a SMT model, we convert 
the training data into a parallel corpus where the 
problematic sentences that ought to be corrected 
are regarded as source sentences, while the 
reference sentences are treated as the 
corresponding target translations. We discovered 
that a number of sentences is absent at the target 
side due to incorrect annotations in the golden 
84
data. We removed these unparalleled sentences 
from the data. Secondly, the initial 
capitalizations of sentences are converted to their 
most probable casing using the Moses truecaser2. 
URLs are quite common in the corpus, but they 
are not useful for learning and even may cause 
the model to apply unnecessary correction on it. 
Thus, we mark all of the ULRs with XML 
markups, signaling the SMT decoder not to 
analyze an URL and output it as is.  
2.2 Model Construction 
In this study we explore four different factors: 
prefix, suffix, stem, and PoS. This linguistic 
information not only helps to capture the local 
constraints of word morphologies and the 
interaction of adjacent words, but also helps to 
prevent data sparsity caused by inflected word 
variants and insufficient training data.  
Word stem: Instead of lemmas, we prefer  
word stemming as one of the factors, considering 
that stemming does not requires deep 
morphological analysis and is easier to obtain. 
Second, during the whole error detection and 
correction process, stemming information is used 
as auxiliary information in addition to the 
original word form. Third, for grammatical error 
correction using word lemmas or word stems in 
factored translation model shows no significant 
difference. This is because we are translating text 
of the same language, and the translation of this 
factor, stem or lemma, is straightforwardly 
captured by the model. Hence, we do not rely on 
the word lemma. In this work, we use the 
English Porter stemmer (Porter, 1980) for 
generating word stems.  
Prefix: The second type of morphological 
information we explored is the word prefix. 
Although a prefix does not present strong 
evidence to be useful to the grammatical error 
correction, we include it in our study in order to 
fully investigate all types of morphological 
information. We believe the prefix can be an 
important factor in the correction of initial 
capitalization, e.g. ?In this era, engineering 
designs?? should be changed to ?In this era, 
engineering designs?? In model construction, 
we take the first three letters of a word as its 
prefix. If the length of a word is less than three, 
we use the word as the prefix factor. 
Suffix: Suffix, one of the important factors, 
helps to capture the grammatical agreements 
between predicates and arguments within a 
                                                          
2 After decoding, we will de-truecase all these words. 
sentence. Particularly the endings of plural nouns 
and inflected verb variants are useful for the 
detection of agreement violations that shown up 
in word morphologies. Similar to how we 
represent the prefix, we are interested in the last 
three characters of a word.  
 Examples 
Sentence 
this card contains biometric data to 
add security and reduce the risk of 
falsification 
Original 
POS 
DT NN BVZ JJ NNS TO VB NN 
CC VB DT NN IN NN 
Specific 
POS 
DT NN VBZ JJ NNS TO_to VB 
NN CC VB DT_the NN IN_of 
NN 
Table 1: Example of modified PoS. 
According to the description of factors, Figure 
1 illustrates the forms of various factors 
extracted from a given example sentence.  
Surface 
constantly combining ideas will 
result in better solutions being 
formulated 
Prefix con com ide wil res in bet sol bei for 
Suffix tly ing eas ill ult in ter ons ing ted 
Stem 
constantli combin idea will result in 
better solut be formul 
Specific 
POS 
RB VBG NNS MD VB IN JJR NNS 
VBG VBN 
Figure 1: The factorized sentence. 
PoS: Part-of-Speech tags denote the morpho-
syntactic category of a word. The use of PoS 
sequences enables us to some extent to recover 
missing determiners, articles, prepositions, as 
well as the modal verb in a sentence. Empirical 
studies (Yuan and Felice, 2013) have 
demonstrated that the use of this information can 
greatly improve the accuracy of the grammatical 
error correction. To obtain the PoS, we adopt the 
Penn Treebank tag set (Marcus et al., 1993), 
which contains 45 PoS tags. The Stanford parser 
(Klein and Manning, 2002) is used to extract the 
PoS information. Inspired by Yuan and Felice 
(2013), who used preposition-specific tags to fix 
the problem of being unable to distinguish 
between prepositions and obtained good 
performance, we create specific tags both for 
determiners (i.e., a, an, the) and prepositions. 
Table 1 provides an example of this modification, 
where prepositions, TO and IN, and determiner, 
85
DT, are revised to TO_to, IN_of and DT_the, 
respectively. 
2.3 Model Combination 
In addition to the design of different factored 
translation models, two model combination 
strategies are designed to treat grammatical error 
correction problem as a series of translation 
processes, where an incorrect sentence is 
translated into the correct one. In both 
approaches we pipeline two translation models, 
    and    . In the first approach, we derive 
four combinations of different models that 
trained on different sources.  
? In case I,    
  and    
  are both factored 
models but trained on different factors, e.g. 
for     
 training on ?surface + factori? and 
    
  on ?surface + factori?j?. Both models 
use the same training sentences, but different 
factors.  
? In case II,     
  is trained on sentences that 
paired with the output from the previous 
model,     
 , and the golden correct sentences. 
We want to create a second model that can 
also tackle the new errors introduced by the 
first model. 
? In case III, similar to case II, the second 
translation model,    
  is replaced by a 
phrase-based translation model.  
? In case IV, the quality of training data is 
considered vital to the construction of a good 
translation model. The present training dataset 
is not large enough. To complement this, the 
second model,     
 , is trained on an enlarged 
data set, by combining the training data of 
both models, i.e. the original parallel data 
(official incorrect and correct sentence pairs) 
and the supplementary parallel data 
(sentences output from the first model,     
 , 
and the correct sentences). Note that we do 
not de-duplicate sentences.  
In all cases, the testing process is carried out 
as follows. The test set is translated by the first 
translation model,     
 . The output from the first 
model is then fed into the second translation 
model,     
 . The output of the second model is 
used as the final corrections. 
The second combination approach is to make 
use of multiple factors for model construction. 
The question is whether multiple factors when 
used together may improve the correction results. 
In this setting we combine two factors together 
with the word surface form to build a multi-
factored translation model. All pairs of factors 
are used, e.g. stem and PoS. The decoding 
sequence is as follows: translate the input stems 
into target stems; translate the PoS; and generate 
the surface form given the factors of stem and 
PoS. 
3 Experiment Setup  
3.1 Dataset 
We pre-process the NUCLE corpus (Dahlmeier 
et al., 2013) as described in Section 2 for training 
different translation models. We use both the 
official golden sentences and additional 
WMT2014 English monolingual data3 to train an 
in-domain and a general-domain language model 
(LM), respectively. These language models are 
linearly interpolated in the decoding phase. We 
also randomly select a number of sentence pairs 
from the parallel corpus as a development set and 
a test set, disjoint from the training data. Table 2 
summarizes the statistics of all the datasets.  
Corpus Sentences Tokens 
Parallel 
Corpus 
55,503 
1,124,521 / 
1,114,040 
Additional 
Monolingual 
85,254,788 2,033,096,800 
Dev. Set 500 10,532 / 10,438 
Test Set 900 18,032 / 17,906 
Table 2: Statistics of used corpora. 
The experiments were carried out with 
MOSES 1.04 (Philipp Koehn et al., 2007). The 
translation and the re-ordering model utilizes the 
?grow-diag-final? symmetrized word-to-word 
alignments created with GIZA++5 (Och and Ney, 
2003) and the training scripts of MOSES. A 5-
gram LM was trained using the SRILM toolkit6 
(Stolcke et al., 2002), exploiting the improved 
modified Kneser-Ney smoothing (Kneser and 
Ney, 1995), and quantizing both probabilities 
and back-off weights. For the log-linear model 
training, we take minimum-error-rate training 
(MERT) method as described in (Och, 2003). 
The result is evaluated by M2 Scorer (Dahlmeier 
and Ng, 2012) computing precision, recall and 
F0.5.  
                                                          
3 http://www.statmt.org/wmt14/translation-task.html. 
4 http://www.statmt.org/moses/. 
5 http://code.google.com/p/giza-pp/. 
6 http://www.speech.sri.com/projects/srilm/. 
86
In total, one baseline system, five individual 
systems, and four combination systems are 
evaluated in this study. The baseline system 
(Baseline) is trained on the words-only corpus 
using a phrase-based translation model. For the 
individual systems we adopt the factored 
translation model that are trained respectively on 
1) surface and stem factors (Sys+stem), 2) surface 
and suffix factors (Sys+suf), 3) surface and prefix 
factors (Sys+pref), 4) surface and PoS factors 
(Sys+PoS), and 5) surface and modified-PoS 
factors (Sys+MPoS). The combination systems 
include: 1) the combination of ?factored + 
phrase-based? and ?factored + factored? for 
models cascading; and 2) the factors of surface, 
stem and modified-PoS (Sys+stem+MPoS) are 
combined for constructing a correction system 
based on a multi-factor model. 
4 Results and Discussions 
We report our results in terms of the precision, 
recall and F0.5 obtained by each of the individual 
models and combined models.  
4.1 Individual Model 
Table 3 shows the absolute measures for the 
baseline system, while the other individual 
models are listed with values relative to the 
baseline.  
Model Precision  Recall  F0.5 
Baseline 25.58 3.53 11.37 
Sys+stem -14.84 +13.00 +0.18 
Sys+suf -14.57 +14.77 +0.60 
Sys+pref -15.74 +12.20 -0.77 
Sys+PoS -11.63 +9.79 +2.45 
Sys+MPoS -10.25 +10.60 +3.70 
Table 3: Performance of various models. 
The baseline system has the highest precision 
score but the lowest recall. Nearly all individual 
models except Sys+pref show improvements in the 
correction result (F0.5) over the baseline. Overall, 
Sys+MPoS achieves the best result for the 
grammatical error correction task. It shows a 
significant improvement over the other models 
and outperforms the baseline model by 3.7 F0.5 
score. The Sys+stem and Sys+suf models obtain an 
improvement of 0.18 and 0.60 in F0.5 scores, 
respectively, compared to the baseline. Although 
the differences are not significant, it confirms our 
hypothesis that morphological clues do help to 
improve error correction. The F0.5 score of 
Sys+pref is the lowest among the models including 
the baseline, showing a drop of 0.77 in F0.5 score 
against the baseline. One possible reason is that 
few errors (in the training corpus) involve word 
prefixes. Thus, the prefix does not seem to be a 
suitable factor for tackling the GEC problem. 
Type 
Sys+stem 
(%) 
Sys+suf 
(%) 
Sys+MPoS 
(%) 
Error 
Num. 
Vt 17.07 12.20 12.20 41 
ArtOrDet 37.65 36.47 29.41 85 
Nn 33.33 19.61 23.53 51 
Prep 10.26 10.26 12.82 39 
Wci 9.10 10.61 6.10 66 
Rloc- 15.20 13.92 10.13 79 
Table 4: The capacity of different models in 
handling six frequent error types. 
We analyze the capacities of the models on 
different types of errors. Sys+PoS and Sys+MPoS are 
built by using the PoS and modified PoS. Both of 
them yield an improvement in F0.5 score. Overall, 
Sys+MPoS produces more accurate results than 
Sys+pref. Therefore, we specifically compare and 
evaluate the best three models, Sys+stem, Sys+suf 
and Sys+MPoS. Table 4 presents evaluation scores 
of these models for the six most frequent error 
types, which take up a large part of the training 
and test data. Among them, Sys+stem displays a 
powerful capacity to handle determiner and 
noun/number agreement errors, up to 37.65% 
and 33.33%. Sys+suf shows the ability to correct 
determiner errors at 36.47%; Sys+MPoS yields a 
similar performance to Sys+suf. All three 
individual models exhibit a relatively high 
capacity to handle determiner errors. The likely 
reason is that this mistake constitutes the largest 
portion in training data and test set, giving the 
learning models many examples to capture this 
problem well. In the case of preposition errors, 
Sys+MPoS demonstrates a better performance. This, 
once again, confirms the result (Yuan and Felice, 
2013) that the modified PoS factor is effective 
for every preposition word. For these six error 
types, the individual models show a weak 
capacity to handle the word collocation or idiom 
error category (Wci). Although Sys+MPoS 
achieves the highest F0.5 score in the overall 
evaluation, it only achieves 6.10% in handling 
this error type. The likely reason is that idioms 
are not frequent in the training data, and also that 
in most of the cases they contain out-of-
vocabulary words never seen in training data. 
4.2 Model Combination 
We intend to further boost the overall 
performance of the correction system by 
87
combining the strengths of individual models 
through model combination, and compare against 
the baseline. The systems compared here cover 
three pipelined models and a multi-factored 
model, as described earlier in Section 3. The 
combined systems include: 1) CSyssuf+phrase: the 
combination of Sys+suf and the baseline phrase-
based translation model; 2) CSyssuf+suf: we 
combine two similar factored models with suffix 
factors, Sys+suf, which is trained on the same 
corpus; and 3) TSyssuf+phrase: similar to 
CSyssuf+phrase, but the training data for the second 
phrase-based model is augmented by adding the 
output sentences from the previous model (paired 
with the correct sentences). Our intention is to 
enlarge the size of the training data. The 
evaluation results are presented in Table 5. 
Model Precision Recall F0.5 
Baseline 25.58 3.53 11.37 
CSyssuf+phrase -14.70 +14.61 +0.45 
CSyssuf+suf -15.04 +14.13 +0.09 
TSyssuf+phrase -14.76 +14.61 +0.40 
Sys+stem+MPoS -15.87 +11.72 -0.90 
Table 5: Evaluation results of combined models. 
In Table 5 we observe that Sys+stem+MPoS hurts 
performance and shows a drop of 0.9% in F0.5 
score. Both the CSyssuf+phrase and CSyssuf+suf 
show minor improvements over the baseline 
system. Even when we enrich the training data 
for the second model in TSyssuf+phrase, it cannot 
help in boosting the overall performance of the 
system. One of the problems we observe is that, 
with this combination structure, new incorrect 
sentences are introduced by the model at each 
step. The errors are propagated and accumulated 
to the final result. Although CSyssuf+phrase and 
CSyssuf+suf produce a better F0.5 score over the 
baseline, they are not as good as the individual 
models, Sys+PoS and Sys+MPoS, which are trained 
on PoS and modified-PoS, respectively. 
4.3 The Official Result 
After fully evaluating the designed individual 
models as well as the integrated ones, we adopt 
Sys+MPoS as our designated system for this 
grammatical error correction task. The official 
test set consists of 50 essays, and 2,203 errors. 
Table 6 shows the final result obtained by our 
submitted system.  
Table 7 details the correction rate of the five 
most frequent error types obtained by our system. 
The result suggests that the proposed system has 
a better ability in handling the verb, article and 
determiner error than other error types. 
Criteria Result Alt. Result 
P 0.3127 0.4317 
R 0.1446 0.1972 
F0.5 0.2537 0.3488 
Table 6: The official correction results of our 
submitted system. 
Type Error Correct % 
Vt 203/201 21/22 10.34/10.94 
V0 57/54 9/9 15.79/16.67 
Vform 156/169 11/18 7.05/10.65 
ArtOrDet 569/656 84/131 14.76/19.97 
Nn 319/285 31/42 9.72/10.91 
Table 7: Detailed error information of evaluation 
system (with alternative result). 
5 Conclusion 
This paper describes our proposed grammatical 
error detection and correction system based on a 
factored statistical machine translation approach. 
We have investigated the effectiveness of models 
trained with different linguistic information 
sources, namely morphological clues and 
syntactic PoS information. In addition, we also 
explore some ways to combine different models 
in the system to tackle the correction problem. 
The constructed models are compared against the 
baseline model, a phrase-based translation model. 
Results show that PoS information is a very 
effective factor, and the model trained with this 
factor outperforms the others. One difficulty of 
this year?s shared task is that participants have to 
tackle all 28 types of errors, which is five times 
more than last year. From the results, it is 
obvious there are still many rooms for improving 
the current system. 
Acknowledgements 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS. 
The authors also wish to thank the anonymous 
reviewers for many helpful comments with 
special thanks to Antal van den Bosch for his 
generous help on this manuscript. 
  
88
References  
 G?bor Berend, Veronika Vincze, Sina Zarriess, 
and Rich?rd Farkas. 2013. LFG-based 
Features for Noun Number and Article 
Grammatical Errors. CoNLL-2013. 
Chris Brockett, William B. Dolan, and Michael 
Gamon. 2006. Correcting ESL errors using 
phrasal SMT techniques. Proceedings of the 
21st International Conference on 
Computational Linguistics and the 44th 
annual meeting of the Association for 
Computational Linguistics pages 249?256. 
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei 
Wu. 2013. Building a Large Annotated 
Corpus of Learner English: The NUS Corpus 
of Learner English. Proceedings of the Eighth 
Workshop on Innovative Use of NLP for 
Building Educational Applications. pages 22-
31. 
Robert Dale, Ilya Anisimoff, and George 
Narroway. 2012. HOO 2012: A report on the 
preposition and determiner error correction 
shared task. Proceedings of the Seventh 
Workshop on Building Educational 
Applications Using NLP pages 54?62. 
Nava Ehsan, and Heshaam Faili. 2013. 
Grammatical and context-sensitive error 
correction using a statistical machine 
translation framework. Software: Practice and 
Experience. Wiley Online Library. 
D. Klein, and C. D. Manning. 2002. Fast exact 
inference with a factored model for natural 
language parsing. Advances in neural 
information processing systems. 
Reinhard Kneser, and Hermann Ney. 1995. 
Improved backing-off for m-gram language 
modeling. Acoustics, Speech, and Signal 
Processing, 1995. ICASSP-95., 1995 
International Conference on Vol. 1, pages 
181?184. 
P. Koehn, and H. Hoang. 2007. Factored 
translation models. Proceedings of the Joint 
Conference on Empirical Methods in Natural 
Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL) 
Vol. 868, pages 876?876. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, 
Chris Callison-Burch, Marcello Federico, 
Nicola Bertoldi, Brooke Cowan, et al. 2007. 
Moses: Open source toolkit for statistical 
machine translation. Proceedings of the 45th 
Annual Meeting of the ACL on Interactive 
Poster and Demonstration Sessions pages 
177?180. 
M. P. Marcus, M. A. Marcinkiewicz, and B. 
Santorini. 1993. Building a large annotated 
corpus of English: The Penn Treebank. 
Computational linguistics. MIT Press. 
Tomoya Mizumoto, Mamoru Komachi, Masaaki 
Nagata, and Yuji Matsumoto. 2011. Mining 
Revision Log of Language Learning SNS for 
Automated Japanese Error Correction of 
Second Language Learners. IJCNLP pages 
147?155. 
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, 
Christian Hadiwinoto, Raymond Hendy 
Susanto, and Bryant Christopher. 2014. The 
conll-2014 shared task on grammatical error 
correction. Proceedings of CoNLL. Baltimore, 
Maryland, USA. 
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, 
Christian Hadiwinoto, and Joel Tetreault. 
2013. The conll-2013 shared task on 
grammatical error correction. Proceedings of 
CoNLL. 
Franz Josef Och. 2003. Minimum Error Rate 
Training in Statistical Machine Translation, 
160?167. 
Franz Josef Och, and Hermann Ney. 2003. A 
systematic comparison of various statistical 
alignment models. Computational linguistics. 
MIT Press. 
Martin F. Porter. 1980. An algorithm for suffix 
stripping. Program: electronic library and 
information systems. MCB UP Ltd. 
Desmond Darma Putra, and Lili Szab?. 2013. 
UdS at the CoNLL 2013 Shared Task. 
CoNLL-2013. 
Grigori Sidorov, Anubhav Gupta, Martin Tozer, 
Dolors Catala, Angels Catena, and Sandrine 
Fuentes. 2013. Rule-based System for 
Automatic Grammar Correction Using 
Syntactic N-grams for English Language 
Learning (L2). CoNLL-2013. 
Andreas Stolcke, and others. 2002. SRILM-an 
extensible language modeling toolkit. 
INTERSPEECH. 
Junwen Xing, Longyue Wang, Derek F. Wong, 
Lidia S. Chao, and Xiaodong Zeng. 2013. 
UM-Checker: A Hybrid System for English 
Grammatical Error Correction. Proceedings of 
the Seventeenth Conference on Computational 
Natural Language Learning: Shared Task, 
34?42. Sofia, Bulgaria: Association for 
Computational Linguistics. Retrieved from 
http://www.aclweb.org/anthology/W13-3605 
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang 
Rim. 2013. KUNLP Grammatical Error 
Correction System For CoNLL-2013 Shared 
89
Task. CoNLL-2013. 
Ippei Yoshimoto, Tomoya Kose, Kensuke 
Mitsuzawa, Keisuke Sakaguchi, Tomoya 
Mizumoto, Yuta Hayashibe, Mamoru 
Komachi, et al. 2013. NAIST at 2013 CoNLL 
grammatical error correction shared task. 
CoNLL-2013. 
Zheng Yuan, and Mariano Felice. 2013. 
Constrained grammatical error correction 
using Statistical Machine Translation. 
CoNLL-2013. 
  
90
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 233?238,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Domain Adaptation for Medical Text Translation Using Web Re-
sources
Yi Lu, Longyue Wang, Derek F. Wong, Lidia S. Chao, Yiming Wang, Francisco Oliveira
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,
Department of Computer and Information Science,
University of Macau, Macau, China
takamachi660@gmail.com, vincentwang0229@hotmail.com,
derekfw@umac.mo, lidiasc@umac.mo, wang2008499@gmail.com, 
olifran@umac.mo
Abstract
This paper describes adapting statistical 
machine translation (SMT) systems to 
medical domain using in-domain and 
general-domain data as well as web-
crawled in-domain resources. In order to 
complement the limited in-domain corpo-
ra, we apply domain focused web-
crawling approaches to acquire in-
domain monolingual data and bilingual 
lexicon from the Internet. The collected 
data is used for adapting the language 
model and translation model to boost the 
overall translation quality. Besides, we 
propose an alternative filtering approach
to clean the crawled data and to further 
optimize the domain-specific SMT sys-
tem. We attend the medical summary
sentence unconstrained translation task of 
the Ninth Workshop on Statistical Ma-
chine Translation (WMT2014). Our sys-
tems achieve the second best BLEU 
scores for Czech-English, fourth for 
French-English, English-French language 
pairs and the third best results for re-
minding pairs.
1 Introduction
In this paper, we report the experiments carried 
out by the NLP2CT Laboratory at University of 
Macau for WMT2014 medical sentence transla-
tion task on six language pairs: Czech-English 
(cs-en), French-English (fr-en), German-English 
(de-en) and the reverse direction pairs (i.e., en-cs, 
en-fr and en-de). 
As data in specific domain are usually rela-
tively scarce, the use of web resources to com-
plement the training resources provides an effec-
tive way to enhance the SMT systems (Resnik 
and smith, 2003; Espl?-Gomis and Forcada, 2010; 
Pecina et al., 2011; Pecina et al., 2012; Pecina et 
al., 2014). In our experiments, we not only use 
all available training data provided by the
WMT2014 standard translation task 1 (general-
domain data) and medical translation task2 (in-
domain data), but also acquire addition in-
domain bilingual translations (i.e. dictionary) and 
monolingual data from online sources.
First of all, we collect the medical terminolo-
gies from the web. This tiny but significant par-
allel data are helpful to reduce the out-of-
vocabulary words (OOVs) in translation models. 
In addition, the use of larger language models 
during decoding is aided by more efficient stor-
age and inference (Heafield, 2011). Thus, we 
crawl more in-domain monolingual data from the 
Internet based on domain focused web-crawling
approach. In order to detect and remove out-
domain data from the crawled data, we not only 
explore text-to-topic classifier, but also propose 
an alternative filtering approach combined the 
existing one (text-to-topic classifier) with per-
plexity. After carefully pre-processing all the 
available training data, we apply language model 
adaptation and translation model adaptation us-
ing various kinds of training corpora. Experi-
mental results show that the presented approach-
es are helpful to further boost the baseline system.
The reminder of this paper is organized as fol-
lows. In Section 2, we detail the workflow of 
web resources acquisition. Section 3 describes 
the pre-processing steps for the corpora. Section 
5 presents the baseline system. Section 6 reports 
the experimental results and discussions. Finally, 
                                                
1 http://www.statmt.org/wmt14/translation-task.html.
2 http://www.statmt.org/wmt14/medical-task/.
233
the submitted systems and the official results are 
reported in Section 7.
2 Domain Focused Web-Crawling
In this section, we introduce our domain focused 
web-crawling approaches on acquisition of in-
domain translation terminologies and monolin-
gual sentences. 
2.1 Bilingual Dictionary
Terminology is a system of words used to name 
things in a particular discipline. The in-domain 
vocabulary size directly affects the performance 
of domain-specific SMT systems. Small size of 
in-domain vocabulary may result in serious 
OOVs problem in a translation system. Therefore, 
we crawl medical terminologies from some 
online sources such as dict.cc3, where the vocab-
ularies are divided into different subjects. We 
obtain the related bilingual entries in medicine 
subject by using Scala build-in XML parser and 
XPath. After cleaning, we collected 28,600, 
37,407, and 37,600 entries in total for cs-en, de-
en, and fr-en respectively.
2.2 Monolingual Data
The workflow for acquiring in-domain resources 
consists of a number of steps such as domain 
identification, text normalization, language iden-
tification, noise filtering, and post-processing as 
well as parallel sentence identification.
Firstly we use an open-source crawler, Com-
bine4, to crawl webpages from the Internet. In 
order to classify these webpages as relevant to 
the medical domain, we use a list of triplets 
<term, relevance weight, topic class> as the 
basic entries to define the topic. Term is a word 
or phrase. We select terms for each language 
from the following sources: 
? The Wikipedia title corpus, a WMT2014 of-
ficial data set consisting of titles of medical 
articles. 
? The dict.cc dictionary, as is described in Sec-
tion 2.1.
? The DrugBank corpus, which is a WMT2014 
official data set on bioinformatics and 
cheminformatics.
For the parallel data, i.e. Wikipedia and dict.cc 
dictionary, we separate the source and target text 
into individual text and use either side of them
for constructing the term list for different lan-
                                                
3 http://www.dict.cc/.
4 http://combine.it.lth.se/.
guages. Regarding the DrugBank corpus, we di-
rectly extract the terms from the ?name? field. 
The vocabulary size of collected text for each 
language is shown in Table 1.
EN CS DE FR
Wikipedia Titles 12,684 3,404 10,396 8,436
dict.cc 29,294 16,564 29,963 22,513
DrugBank 2,788
Total 44,766 19,968 40,359 30,949
Table 1: Size of terms used for topic definition.
Relevance weight is the score for each occur-
rence of the term, which is assigned by its length, 
i.e., number of tokens. The topic class indicates 
the topics. In this study, we are interested in 
medical domain, the topic class is always marked 
with ?MED? in our topic definition. 
The topic relevance of each document is cal-
culated5 as follows:
  ? ?      
   
  
   
 
   (1)
where is the amount of terms in the topic defi-
nition;   
 is the weight of term  ;   
 is the 
weight of term at location  .    is the number of 
occurrences of term  at  position. In implemen-
tation, we use the default values for setting and
parameters. Another input required by the crawl-
er is a list of seed URLs, which are web sites that 
related to medical topic. We limit the crawler 
from getting the pages within the http domain 
guided by the seed links. We acquired the list 
from the Open Directory Project6, which is a re-
pository maintained by volunteer editors. Totally, 
we collected 12,849 URLs from the medicine
category.
Text normalization is to convert the text of 
each HTML page into UTF-8 encoding accord-
ing to the content_charset of the header. In addi-
tion, HTML pages often consist of a number of 
irrelevant contents such as the navigation links, 
advertisements disclaimers, etc., which may neg-
atively affect the performance of SMT system. 
Therefore, we use the Boilerpipe tool 
(Kohlsch?tter et al., 2010) to filter these noisy
data and preserve the useful content that is 
marked by the tag, <canonicalDocument>. The 
resulting text is saved in an XML file, which will 
be further processed by the subsequent tasks. For 
language identification, we use the language-
detection7 toolkit to determine the possible lan-
                                                
5
http://combine.it.lth.se/documentation/DocMain/node6.html.
6 http://www.dmoz.org/Health/Medicine/.
7 https://code.google.com/p/language-detection/.
234
guage of the text, and discard the articles which 
are in the right language we are interested.
2.3 Data Filtering
The web-crawled documents (described in Sec-
tion 2.2) may consist a number of out-domain 
data, which would harm the domain-specific lan-
guage and translation models. We explore and 
propose two filtering approaches for this task. 
The first one is to filter the documents based on 
their relative score, Eq. (1). We rank all the doc-
uments according to their relative scores and se-
lect top K percentage of entire collection for fur-
ther processing. 
Second, we use a combination method, which 
takes both the perplexity and relative score into 
account for the selection. Perplexity-based data 
selection has shown to be a powerful mean on 
SMT domain adaptation (Wang et al., 2013; 
Wang et al., 2014; Toral, 2013; Rubino et al., 
2013; Duh et al., 2013). The combination method 
is carried out as follows: we first retrieve the 
documents based on their relative scores. The 
documents are then split into sentences, and
ranked according to their perplexity using Eq. (2)
(Stolcke et al., 2002). The used language model 
is trained on the official in-domain data. Finally, 
top N percentage of ranked sentences are consid-
ered as additional relevant in-domain data. 
    ( )        
 ( )
    (2)
where  is a input sentence or document,  ( ) is 
the probability of  -gram segments estimated 
from the training set.     is the number of 
tokens of an input string.
3 Pre-processing
Both official training data and web-crawled re-
sources are processed using the Moses scripts8, 
this includes the text tokenization, truecasing and 
length cleaning. For trusecasing, we use both the 
target side of parallel corpora and monolingual 
data to train the trucase models. We consider the 
target system is intended for summary translation, 
the sentences tend to be short in length. We re-
move sentence pairs which are more than 80 
words at length in either sides of the parallel text.
In addition to these general data filtering steps,
we introduce some extra steps to pre-process the 
training data. The first step is to remove the du-
plicate sentences. In data-driven methods, the 
more frequent a term occurs, the higher probabil-
                                                
8 http://www.statmt.org/moses/?n=Moses.Baseline.
ity it biases. Duplicate data may lead to unpre-
dicted behavior during the decoding. Therefore, 
we keep only the distinct sentences in monolin-
gual corpus. By taking into account multiple 
translations in parallel corpus, we remove the 
duplicate sentence pairs. We also use a biomedi-
cal sentence splitter9 (Rune et al., 2007) to split 
sentences in monolingual corpora. The statistics 
of the data are provided in Table 2.
4 Baseline System
We built our baseline system on an optimized 
level. It is trained on all official in-domain train-
ing corpora and a portion of general-domain data. 
We apply the Moore-Lewis method (Moore and 
Lewis, 2010) and modified Moore-Lewis method 
(Axelrod et al., 2011) for selecting in-domain 
data from the general-domain monolingual and 
parallel corpora, respectively. The top M per-
centages of ranked sentences are selected as a 
pseudo in-domain data to train an additional LM
and TM. For LM, we linearly interpolate the ad-
ditional LM with in-domain LM. For TM, the 
additional model is log-linearly interpolated with 
the in-domain model using the multi-decoding 
method described in (Koehn and Schroeder, 
2007). Finally, LM adaptation and TM adapta-
tion are combined to further improve the transla-
tion quality of baseline system.
5 Experiments and Results
The official medical summary development sets 
(dev) are used for tuning and evaluating the 
comparative systems. The official medical sum-
mary test sets (test) are only used in our final 
submitted systems.
The experiments were carried out with the 
Moses 1.010 (Koehn et al., 2007). The translation 
and the re-ordering model utilizes the ?grow-
diag-final? symmetrized word-to-word align-
ments created with MGIZA++11 (Och and Ney, 
2003; Gao and Vogel, 2008) and the training 
scripts from Moses. A 5-gram LM was trained 
using the SRILM toolkit12 (Stolcke et al., 2002), 
exploiting improved modified Kneser-Ney 
smoothing, and quantizing both probabilities and 
back-off weights. For the log-linear model train-
ing, we take the minimum-error-rate training 
(MERT) method as described in (Och, 2003).
                                                
9 http://www.nactem.ac.uk/y-matsu/geniass/.
10 http://www.statmt.org/moses/.
11 http://www.kyloo.net/software/doku.php/mgiza:overview.
12 http://www.speech.sri.com/projects/srilm/.
235
In the following sub-sections, we describe the
results of baseline systems, which are trained on 
the official corpora. We also present the en-
hanced systems that make use of the web-
crawled bilingual dictionary and monolingual 
data as the additional training resources. Two
variants of enhanced system are constructed 
based on different filtering criteria.
5.1 Baseline System
The baseline systems is constructed based on the 
combination of TM adaptation and LM adapta-
tion, where the corresponding selection thresh-
olds ( ) are manually tuned. Table 3 shows the 
BLEU scores of baseline systems as well as the
threshold values of for general-domain mono-
lingual corpora and parallel corpora selection, 
respectively.
By looking into the results, we find that en-cs 
system performs poorly, because of the limited 
in-domain parallel and monolingual corpora 
(shown in Table 2). While the fr-en and en-fr 
systems achieve the best scores, due the availa-
bility of the high volume training data. We ex-
periment with different values of ={0, 25, 50, 
75, 100} that indicates the percentages of sen-
tences out of the general corpus used for con-
structing the LM adaptation and TM adaptation. 
After tuning the parameter  , we find that
BLEU scores of different systems peak at differ-
ent values of . LM adaptation can achieve the 
best translation results for cs-en, en-fr and de-en 
pairs when  =25, en-cs and en-de pairs when 
 =50, and fr-en pair when  =75. While TM 
adaptation yields the best scores for en-fr and en-
de pairs at  =25 and cs-en and fr-en pairs at 
 =50, de-en pair when =75 and en-cs pair at 
 =100.
Lang. Pair BLEU
Mono.
(M%)
Parallel
(M%)
en-cs 17.57 50% 100%
cs-en 31.29 25% 50%
en-fr 38.36 25% 25%
fr-en 44.36 75% 50%
en-de 18.01 50% 25%
de-en 32.50 25% 75%
Table 3: BLEU scores of baseline systems for 
different language pairs.
5.2 Based on Relevance Score Filtering
As described in Section 2.3, we use the relevance
score to filter out the non-in-domain documents. 
Once again, we evaluate different values of 
Data Set Lang. Sent. Words Vocab. Ave. Len. Sites Docs
In-domain 
Parallel Data
cs/en 1,770,421
9,373,482/
10,605,222
134,998/
156,402
5.29/
5.99
de/en 3,894,099
52,211,730/
58,544,608
1,146,262/
487,850
13.41/
15.03
fr/en 4,579,533
77,866,237/
68,429,649
495,856/
556,587
17.00/
14.94
General-
domain 
Parallel Data
cs/en 12,426,374
180,349,215/
183,841,805
1,614,023/
1,661,830
14.51/
14.79
de/en 4,421,961
106,001,775/
112,294,414
1,912,953/
919,046
23.97/
25.39
fr/en 36,342,530
1,131,027,766/
953,644,980
3,149,336/
3,324,481
31.12/
26.24
In-domain 
Mono. Data
cs 106,548 1,779,677 150,672 16.70
fr 1,424,539 53,839,928 644,484 37.79
de 2,222,502 53,840,304 1,415,202 24.23
en 7,802,610 199430649 1,709,594 25.56
General-
domain 
Mono. Data
cs 33,408,340 567,174,266 3,431,946 16.98
fr 30,850,165 780,965,861 2,142,470 25.31
de 84,633,641 1,548,187,668 10,726,992 18.29
en 85,254,788 2,033,096,800 4,488,816 23.85
Web-crawled 
In-domain 
Mono. Data
en 8,448,566 280,211,580 3,047,758 33.16 26 1,601
cs 44,198 1,280,326 137,179 28.96 4 388
de 473,171 14,087,687 728,652 29.77 17 968
fr 852,036 35,339,445 718,141 41.47 10 683
Table 2: Statistics summary of corpora after pre-processing.
236
 ={0, 25, 50, 75, 100} that represents the per-
centages of crawled documents we used for 
training the LMs. In Table 4, we show the abso-
lute BLEU scores of the evaluated systems, listed 
with the optimized thresholds, and the relative 
improvements (?%) in compared to the baseline 
system. The size of additional training data (for 
LM) is displayed at the last column.
Lang. 
Pair
Docs
( %)
BLEU
? 
(%)
Sent.
en-cs 50 17.59 0.11 31,065 
en-de 75 18.52 2.83 435,547 
en-fr 50 39.08 1.88 743,735 
cs-en 75 32.22 2.97 7,943,931
de-en 25 33.50 3.08 4,951,189
fr-en 100 45.45 2.46 8,448,566
Table 4: Evaluation results for systems that 
trained on relevance-score-filtered documents.
The relevance score filtering approach yields 
an improvement of 3.08% of BLEU score for de-
en pair that is the best result among the language 
pairs. On the other hand, en-cs pair obtains a 
marginal gain. The reason is very obvious that 
the training data is very insufficient. Empirical 
results of all language pairs expect fr-en indicate
that data filtering is the necessity to improve the 
system performance.
5.3 Based on Moore-Lewis Filtering
In this approach, we need to determine the values 
of two parameters, top  documents and top  
sentences, where  ={100, 75, 50} and  ={75, 
50, 25},    . When  =100, it is a conven-
tional perplexity-based data selection method, i.e. 
no document will be filtered. Table 5 shows the 
combination of different  and  that gives the 
best translation score for each language pair. We 
provide the absolute BLEU for each system, to-
gether with relative improvements (?%) that 
compared to the baseline system.
Lang.  
Pair
Docs
( %)
Target 
Size ( %)
BLEU ? (%)
en-cs 50 25 17.69 0.68
en-de 100 50 18.03 0.11
en-fr 100 50 38.73 0.96
cs-en 100 25 32.20 2.91
de-en 100 25 33.10 1.85
fr-en 100 25 45.22 1.94
Table 5: Evaluation results for systems that 
trained on combination filtering approach.
In this shared task, we have a quality and 
quantity in-domain monolingual training data for 
English. All the systems that take English as the 
target translation always outperform the other
reverse pairs. Besides, we found the systems 
based on the perplexity data selection method
tend to achieve a better scores in BLEU.
6 Official Results and Conclusions
We described our study on developing uncon-
strained systems in the medical translation task 
of 2014 Workshop on Statistical Machine Trans-
lation. In this work, we adopt the web crawling 
strategy for acquiring the in-domain monolingual 
data.  In detection the domain data, we exploited 
Moore-Lewis data selection method to filter the 
collected data in addition to the build-in scoring 
model provided by the crawler toolkit. However, 
after investigation, we found that the two meth-
ods are very competitive to each other.
The systems we submitted to the shared task 
were built using the language models and trans-
lation models that yield the best results in the 
individual testing. The official test set is convert-
ed into the recased and detokenized SGML for-
mat. Table 9 presents the official results of our 
submissions for every language pair.
Lang. 
Pair
BLEU of Combined 
systems
Official 
BLEU
en-cs 23.16 (+5.59) 22.10
cs-en 36.8 (+5.51) 37.40
en-fr 40.34 (+1.98) 40.80
fr-en 45.79 (+1.43) 43.80
en-de 19.36 (+1.35) 18.80
de-en 34.17 (+1.67) 32.70
Table 6: BLEU scores of the submitted systems 
for the medical translation task in six language 
pairs.
Acknowledgments
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS.
References 
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of EMNLP, pages 355-
362.
237
K. Duh, G. Neubig, K. Sudoh, H. Tsukada. 2013. Ad-
aptation data selection using neural language mod-
els: Experiments in machine translation. In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, pages, 678?683.
M. Espl?-Gomis and M. L. Forcada. 2010. Combining 
Content-Based and URL-Based Heuristics toHar-
vest Aligned Bitexts from Multilingual Sites with 
Bitextor. The Prague Bulletin of Mathemathical 
Lingustics, 93:77?86.
Qin Gao and Stephan Vogel. 2008. Parallel Imple-
mentations of Word Alignment Tool. Software En-
gineering, Testing, and Quality Assurance for Nat-
ural Language Processing, pp. 49-57.
Kenneth Heafield. 2011. KenLM: Faster and smaller 
language model queries. In Proceedings of the 
Sixth Workshop on Statistical Machine Translation, 
pages 187-197.
Papineni, Kishore, Salim Roukos, ToddWard, and-
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In 40th Annu-
al Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 311?318, Philadelphia, 
USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran et al. 
2007. Moses: Open source toolkit for statistical 
machine translation. In Proceedings of ACL, pages
177-180.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine 
translation. In Proceedings of the 2nd ACL Work-
shop on Statistical Machine Translation, pages 
224-227.
Christian Kohlsch?tter, Peter Fankhauser, and Wolf-
gang Nejdl. 2010. Boilerplate detection using shal-
low text features. In Proceedings of the 3rd ACM
International Conference on Web Search and Data
Mining, pages 441-450.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In 
Proceedings of ACL: Short Papers, pages 220-224.
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. Proceedings of 
ACL, pp. 160-167.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29:19-51.
P. Pecina, A. Toral, A. Way, V. Papavassiliou, P. 
Prokopidis, and M. Giagkou. 2011. Towards Using 
WebCrawled Data for Domain Adaptation in Sta-
tistical Machine Translation. In Proceedings of the 
15th Annual Conference of the European Associta-
tion for Machine Translation, pages 297-304.
P. Pecina, A. Toral, V. Papavassiliou, P. Prokopidis, J. 
van Genabith,  and R. I. C. Athena. 2012. Domain 
adaptation of statistical machine translation using 
web-crawled resources: a case study. In Proceed-
ings of the 16th Annual Conference of the Europe-
an Association for Machine Translation, pp. 145-
152.
P. Pecina, O. Du?ek, L. Goeuriot, J. Haji?, J. Hla-
v??ov?, G. J. Jones, and Z. Ure?ov?. 2014. Adapta-
tion of machine translation for multilingual infor-
mation retrieval in the medical domain. Artificial 
intelligence in medicine, pages 1-25.
Philip Resnik and Noah A. Smith. 2003. The Web as 
a parallel corpus. Computational Linguistics, 
29:349?380
Raphael Rubino, Antonio Toral, Santiago Cort?s 
Va?llo, Jun Xie, Xiaofeng Wu, Stephen Doherty, 
and Qun Liu. 2013. The CNGL-DCU-Prompsit 
translation systems for WMT13. In Proceedings of 
the Eighth Workshop on Statistical Machine Trans-
lation, pages 213-218.
S?tre Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiro Matsubayashi and Tomo-
ko Ohta. 2007. AKANE System: Protein-Protein 
Interaction Pairs in BioCreAtIvE2 Challenge, PPI-
IPS subtask. In Proceedings of the Second BioCre-
ative Challenge Evaluation Workshop, pp. 209-212.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. Proceedings of the Inter-
national Conference on Spoken Language Pro-
cessing, pp. 901-904.
Antonio Toral. 2013. Hybrid selection of language 
model training data using linguistic information 
and perplexity. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation.
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu, 
and Junwen Xing. 2014. A Systematic Com-
parison of Data Selection Criteria for SMT Domain 
Adaptation. The Scientific World Journal, vol. 
2014, Article ID 745485, 10 pages.
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu, 
Junwen Xing. 2013. iCPE: A Hybrid Data Selec-
tion Model for SMT Domain Adaptation. Chinese 
Computational Linguistics and Natural Language 
Processing Based on Naturally Annotated Big Da-
ta. Springer Berlin Heidelberg. pages, 280-290
238
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 254?259,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Combining Domain Adaptation Approaches for Medical Text Transla-
tion 
 
Longyue Wang, Yi Lu, Derek F. Wong, Lidia S. Chao, Yiming Wang, Francisco Oliveira 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory, 
Department of Computer and Information Science, 
University of Macau, Macau, China 
vincentwang0229@hotmail.com,  
{mb25435, derekfw, lidiasc, mb25433, olifran}@umac.mo 
 
 
Abstract 
This paper explores a number of simple 
and effective techniques to adapt statisti-
cal machine translation (SMT) systems in 
the medical domain. Comparative exper-
iments are conducted on large corpora for 
six language pairs. We not only compare 
each adapted system with the baseline, 
but also combine them to further improve 
the domain-specific systems. Finally, we 
attend the WMT2014 medical summary 
sentence translation constrained task and 
our systems achieve the best BLEU 
scores for Czech-English, English-
German, French-English language pairs 
and the second best BLEU scores for re-
minding pairs. 
 
1. Introduction 
This paper presents the experiments conducted 
by the NLP2CT Laboratory at the University of 
Macau for WMT2014 medical sentence transla-
tion task on six language pairs: Czech-English 
(cs-en), French-English (fr-en), German-English 
(de-en) and the reverse direction pairs, i.e., en-cs, 
en-fr and en-de.  
By comparing the medical text with common 
text, we discovered some interesting phenomena 
in medical genre. We apply domain-specific 
techniques in data pre-processing, language 
model adaptation, translation model adaptation, 
numeric and hyphenated words translation.  
Compared to the baseline systems (detailed in 
Section 2 & 3), the results of each method show 
reasonable gains. We combine individual ap-
proach to further improve the performance of our 
systems. To validate the robustness and lan-
guage-independency of individual and combined 
systems, we conduct experiments on the official 
training data (detailed in Section 3) in all six lan-
guage pairs. We anticipate the numeric compari-
son (BLEU scores) on these individual and com-
bined domain adaptation approaches that could 
be valuable for others on building a real-life do-
main-specific system. 
The reminder of this paper is organized as fol-
lows. In Section 2, we detail the configurations 
of our experiments as well as the baseline sys-
tems. Section 3 presents the specific pre-
processing for medical data. In Section 4 and 5, 
we describe the language model (LM) and trans-
lation model (TM) adaptation, respectively. Be-
sides, the techniques for numeric and hyphenated 
words translation are reported in Section 6 and 7. 
Finally, the performance of design systems and 
the official results are reported in Section 8. 
2. Experimental Setup 
All available training data from both WMT2014 
standard translation task1 (general-domain data) 
and medical translation task 2  (in-domain data) 
are used in this study. The official medical sum-
mary development sets (dev) are used for tuning 
and evaluating all the comparative systems. The 
official medical summary test sets (test) are only 
used in our final submitted systems.  
The experiments were carried out with the 
Moses 1.03 (Koehn et al., 2007). The translation 
and the re-ordering model utilizes the ?grow-
diag-final? symmetrized word-to-word align-
ments created with MGIZA++4 (Och and Ney, 
                                                 
1 http://www.statmt.org/wmt14/translation-task.html. 
2 http://www.statmt.org/wmt14/medical-task/. 
3 http://www.statmt.org/moses/. 
4 http://www.kyloo.net/software/doku.php/mgiza:overview. 
254
2003; Gao and Vogel, 2008) and the training 
scripts from Moses. A 5-gram LM was trained 
using the SRILM toolkit5 (Stolcke et al., 2002), 
exploiting improved modified Kneser-Ney 
smoothing, and quantizing both probabilities and 
back-off weights. For the log-linear model train-
ing, we take the minimum-error-rate training 
(MERT) method as described in (Och, 2003). 
3. Task Oriented Pre-processing 
A careful pre-processing on training data is sig-
nificant for building a real-life SMT system. In 
addition to the general data preparing steps used 
for constructing the baseline system, we intro-
duce some extra steps to pre-process the training 
data. 
The first step is to remove the duplicate sen-
tences. In data-driven methods, the more fre-
quent a term occurs, the higher probability it bi-
ases. Duplicate data may lead to unpredicted be-
havior during the decoding. Therefore, we keep 
only the distinct sentences in monolingual cor-
pus. By taking into account multiple translations 
in parallel corpus, we remove the duplicate sen-
tence pairs. The second concern in pre-
processing is symbol normalization. Due to the 
nature of medical genre, symbols such as num-
bers and punctuations are commonly-used to pre-
sent chemical formula, measuring unit, terminol-
ogy and expression. Fig. 1 shows the examples 
of this case. These symbols are more frequent in 
medical article than that in the common texts. 
Besides, the punctuations of apostrophe and sin-
gle quotation are interchangeably used in French 
text, e.g. ?l?effet de l'inhibition?. We unify it by 
replacing with the apostrophe. In addition, we 
observe that some monolingual training subsets 
(e.g., Gene Regulation Event Corpus) contain 
sentences of more than 3,000 words in length. To 
avoid the long sentences from harming the true-
case model, we split them into sentences with a 
sentence splitter6 (Rune et al., 2007) that is opti-
mized for biomedical texts. On the other hand, 
we consider the target system is intended for 
summary translation, the sentences tend to be 
short in length. For instance, the average sen-
tence lengths in development sets of cs, fr, de 
and en are around 15, 21, 17 and 18, respective-
ly. We remove sentence pairs which are more 
than 80 words at length. In order to that our ex-
periments are reproducible, we give the detailed 
                                                 
5 http://www.speech.sri.com/projects/srilm/. 
6 http://www.nactem.ac.uk/y-matsu/geniass/. 
statistics of task oriented pre-processed training 
data in Table 2. 
1,25-OH 
47 to 80% 
10-20 ml/kg 
A&E department 
Infective endocarditis (IE) 
Figure 1. Examples of the segments with sym-
bols in medical texts. 
To validate the effectiveness of the pre-
processing, we compare the SMT systems 
trained on original data 7 (Baseline1) and task-
oriented-processed data (Baseline2), respective-
ly. Table 1 shows the results of the baseline sys-
tems. We found all the Baseline2 systems outper-
form the Baseline1 models, showing that the sys-
tems can benefit from using the processed data. 
For cs-en and en-cs pairs, the BLEU scores im-
prove quite a lot. For other language pairs, the 
translation quality improves slightly.  
By analyzing the Baseline2 results (in Table 1) 
and the statistics of training corpora (in Table 2), 
we can further elaborate and explain the results. 
The en-cs system performs poorly, because of 
the short average length of training sentences, as 
well as the limited size of in-domain parallel and 
monolingual corpora. On the other hand, the fr-
en system achieves the best translation score, as 
we have sufficient training data. The translation 
quality of cs-en, en-fr, fr-en and de-en pairs is 
much higher than those in the other pairs. Hence, 
Baseline2 will be used in the subsequent compar-
isons with the proposed systems described in 
Section 4, 5, 6 and 7. 
Lang. Pair Baseline1 Baseline2 Diff. 
en-cs 12.92 17.57 +4.65 
cs-en 20.85 31.29 +10.44 
en-fr 38.31 38.36 +0.05 
fr-en 44.27 44.36 +0.09 
en-de 17.81 18.01 +0.20 
de-en 32.34 32.50 +0.16 
Table 1: BLEU scores of two baseline systems 
trained on original and processed corpora for 
different language pairs. 
4. Language Model Adaptation 
The use of LMs (trained on large data) during 
decoding is aided by more efficient storage and 
inference (Heafield, 2011). Therefore, we not 
                                                 
7 Data are processed according to Moses baseline tutorial: 
http://www.statmt.org/moses/?n=Moses.Baseline. 
255
Data Set Lang. Sent. Words Vocab. Ave. Len. 
In-domain  
Parallel Data 
cs/en 1,770,421 
9,373,482/ 
10,605,222 
134,998/ 
156,402 
5.29/ 
5.99 
de/en 3,894,099 
52,211,730/ 
58,544,608 
1,146,262/ 
487,850 
13.41/ 
15.03 
fr/en 4,579,533 
77,866,237/ 
68,429,649 
495,856/ 
556,587 
17.00/ 
14.94 
General-domain  
Parallel Data 
cs/en 12,426,374 
180,349,215/ 
183,841,805 
1,614,023/ 
1,661,830 
14.51/ 
14.79 
de/en 4,421,961 
106,001,775/ 
112,294,414 
1,912,953/ 
919,046 
23.97/ 
25.39 
fr/en 36,342,530 
1,131,027,766/ 
953,644,980 
3,149,336/ 
3,324,481 
31.12/ 
26.24 
In-domain  
Mono. Data 
cs 106,548 1,779,677 150,672 16.70 
fr 1,424,539 53,839,928 644,484 37.79 
de 2,222,502 53,840,304 1,415,202 24.23 
en 7,802,610 199430649 1,709,594 25.56 
General-domain  
Mono. Data 
cs 33,408,340 567,174,266 3,431,946 16.98 
fr 30,850,165 780,965,861 2,142,470 25.31 
de 84,633,641 1,548,187,668 10,726,992 18.29 
en 85,254,788 2,033,096,800 4,488,816 23.85 
Table 2: Statistics summary of corpora after pre-processing. 
only use the in-domain training data, but also the 
selected pseudo in-domain data 8  from general-
domain corpus to enhance the LMs (Toral, 2013; 
Rubino et al., 2013; Duh et al., 2013). Firstly, 
each sentence s in general-domain monolingual 
corpus is scored using the cross-entropy differ-
ence method in (Moore and Lewis, 2010), which 
is calculated as follows: 
 ( ) ( ) ( )I Gscore s H s H s? ? (1) 
where H(s) is the length-normalized cross-
entropy. I and G are the in-domain and general-
domain corpora, respectively. G is a random sub-
set (same size as the I) of the general-domain 
corpus. Then top N percentages of ranked data 
sentences are selected as a pseudo in-domain 
subset to train an additional LM. Finally, we lin-
early interpolate the additional LM with in-
domain LM.  
We use the top N% of ranked results, where 
N={0, 25, 50, 75, 100} percentages of sentences 
out of the general corpus. Table 3 shows the ab-
solute BLEU points for Baseline2 (N=0), while 
the LM adapted systems are listed with values 
relative to the Baseline2. The results indicate that 
LM adaptation can gain a reasonable improve-
ment if the LMs are trained on more relevant 
data for each pair, instead of using the whole 
training data. For different systems, their BLEU 
                                                 
8 Axelrod et al. (2011) names the selected data as pseudo in-
domain data. We adopt both terminologies in this paper. 
scores peak at different values of N. It gives the 
best results for cs-en, en-fr and de-en pairs when 
N=25, en-cs and en-de pairs when N=50, and fr-
en pair when N=75. Among them, en-cs and en-
fr achieve the highest BLEU scores. The reason 
is that their original monolingual (in-domain) 
data for training the LMs are not sufficient. 
When introducing the extra pseudo in-domain 
data, the systems improve the translation quality 
by around 2 BLEU points. While for cs-en, fr-en 
and de-en pairs, the gains are small. However, it 
can still achieve a significant improvement of 
0.60 up to 1.12 BLEU points. 
Lang. N=0 N=25 N=50 N=75 N=100 
en-cs 17.57 +1.66 +2.08 +1.72 +2.04 
cs-en 31.29 +0.94 +0.60 +0.66 +0.47 
en-fr 38.36 +1.82 +1.66 +1.60 +0.08 
fr-en 44.36 +0.91 +1.09 +1.12 +0.92 
en-de 18.01 +0.57 +1.02 -4.48 -4.54 
de-en 32.50 +0.60 +0.50 +0.56 +0.38 
Table 3: BLEU scores of LM adapted systems. 
5. Translation Model Adaptation 
As shown in Table 2, general-domain parallel 
corpora are around 1 to 7 times larger than the 
in-domain ones. We suspect if general-domain 
corpus is broad enough to cover some in-domain 
sentences. To observe the domain-specificity of 
general-domain corpus, we firstly evaluate sys-
tems trained on general-domain corpora. In Ta-
256
ble 4, we show the BLEU scores of general-
domain systems9 on translating the medical sen-
tences. The BLEU scores of the compared sys-
tems are relative to the Baseline2 and the size of 
the used general-domain corpus is relative to the 
corresponding in-domain one. For en-cs, cs-en, 
en-fr and fr-en pairs, the general-domain parallel 
corpora we used are 6 times larger than the orig-
inal ones and we obtain the improved BLEU 
scores by 1.72 up to 3.96 points. While for en-de 
and de-en pairs, the performance drops sharply 
due to the limited training corpus we used. 
Hence we can draw a conclusion: the general-
domain corpus is able to aid the domain-specific 
translation task if the general-domain data is 
large and broad enough in content.  
Lang. Pair BLEU Diff. Corpus 
en-cs 21.53 +3.96 
+601.89% 
cs-en 33.01 +1.72 
en-fr 41.57 +3.21 
+693.59% 
fr-en 47.33 +2.97 
en-de 16.54 -1.47 
+13.63% 
de-en 27.35 -5.15 
Table 4: The BLEU scores of systems trained on 
general-domain corpora. 
Taking into account the performance of gen-
eral-domain system, we explore various data se-
lection methods to derive the pseudo in-domain 
sentence pairs from general-domain parallel cor-
pus for enhancing the TMs (Wang et al., 2013; 
Wang et al., 2014). Firstly, sentence pair in cor-
responding general-domain corpora is scored by 
the modified Moore-Lewis (Axelrod et al., 
2011), which is calculated as follows: 
 ? ?
g g
( ) ( ) ( )
( ) ( )
I src G src
I t t G t t
score s H s H s
H s H s
? ?
? ?
? ?
? ?? ?? ?
 (2) 
which is similar to Eq. (1) and the only differ-
ence is that it considers the both the source (src) 
and target (tgt) sides of parallel corpora. Then 
top N percentage of ranked sentence pairs are 
selected as a pseudo in-domain subset to train an 
individual translation model. The additional 
model is log-linearly interpolated with the in-
domain model (Baseline2) using the multi-
decoding method described in (Koehn and 
Schroeder, 2007). 
Similar to LM adaptation, we use the top N% 
of ranked results, where N={0, 25, 50, 75, 100} 
percentages of sentences out of the general cor-
                                                 
9  General-domain systems are trained only on genera-
domain training corpora (i.e., parallel, monolingual). 
pus. Table 5 shows the absolute BLEU points for 
Baseline2 (N=0), while for the TM adapted sys-
tems we show the values relative to the Base-
line2. For different systems, their BLEU peak at 
different N. For en-fr and en-de pairs, it gives the 
best translation results at N=25. Regarding cs-en 
and fr-en pairs, the optimal performance is 
peaked at N=50. While the best results for de-en 
and en-cs pairs are N=75 and N=100 respective-
ly. Besides, performance of TM adapted system 
heavily depends on the size and (domain) broad-
ness of the general-domain data. For example, 
the improvements of en-de and de-en systems are 
slight due to the small general-domain corpora. 
While the quality of other systems improve about 
3 BLEU points, because of their large and broad 
general-domain corpora.  
Lang. N=0 N=25 N=50 N=75 N=100 
en-cs 17.57 +0.84 +1.53 +1.74 +2.55 
cs-en 31.29 +2.03 +3.12 +3.12 +2.24 
en-fr 38.36 +3.87 +3.66 +3.53 +2.88 
fr-en 44.36 +1.29 +3.36 +1.84 +1.65 
en-de 18.01 +0.02 -0.13 -0.07 0 
de-en 32.50 -0.12 +0.06 +0.31 +0.24 
Table 5: BLEU scores of TM adapted systems. 
6. Numeric Adaptation 
As stated in Section 3, numeric occurs frequently 
in medical texts. However, numeric expression in 
dates, time, measuring unit, chemical formula are 
often sparse, which may lead to OOV problems 
in phrasal translation and reordering. Replacing 
the sparse numbers with placeholders may pro-
duce more reliable statistics for the MT models.  
Moses has support using placeholders in train-
ing and decoding. Firstly, we replace all the 
numbers in monolingual and parallel training 
corpus with a common symbol (a sample phrase 
is illustrated in Fig. 2). Models are then trained 
on these processed data. We use the XML 
markup translation method for decoding.  
Original: Vitamin D 1,25-OH  
Replaced: Vitamin D @num@, @num@-OH 
Figure 2. Examples of placeholders. 
Table 6 shows the results on this number ad-
aptation approach as well as the improvements 
compared to the Baseline2. The method im-
proves the Baseline2 systems by 0.23 to 0.40 
BLEU scores. Although the scores increase 
slightly, we still believe this adaptation method is 
significant for medical domain. The WMT2014 
medical task only focuses on the summary of 
257
medical text, which may contain fewer chemical 
expression in compared with the full article. As 
the used of numerical instances increases, place-
holder may play a more important role in domain 
adaptation.  
Lang. Pair BLEU (Dev) Diff. 
en-cs 17.80 +0.23 
cs-en 31.52 +0.23 
en-fr 38.72 +0.36 
fr-en 44.69 +0.33 
en-de 18.41 +0.40 
de-en 32.88 +0.38 
Table 6: BLEU scores of numeric adapted sys-
tems. 
7. Hyphenated Word Adaptation 
Medical texts prefer a kind of compound words, 
hyphenated words, which is composed of more 
than one word. For instance, ?slow-growing? and 
?easy-to-use? are composed of words and linked 
with hyphens. These hyphenated words occur 
quite frequently in medical texts. We analyze the 
development sets of cs, fr, en and de respective-
ly, and observe that there are approximately 
3.2%, 11.6%, 12.4% and 19.2% of sentences that 
contain one or more hyphenated words. The high 
ratio of such compound words results in Out-Of-
Vocabulary words (OOV) 10 , and harms the 
phrasal translation and reordering. However, a 
number of those hyphenated words still have 
chance to be translated, although it is not precise-
ly, when they are tokenized into individual 
words.  
Algorithm: Alternative-translation Method 
Input: 
1. A sentence, s, with M hyphenated words 
2. Translation lexicon 
Run: 
1. For i = 1, 2, ?, M 
2.   Split the ith hyphenated word (Ci) into 
Pi 
3.   Translate  Pi into Ti 
4.   If (Ti are not OOVs): 
5.      Put alternative translation Ti in XML 
6.    Else: keep Ci unchanged 
Output: 
Sentence, s?, embedded with alternative 
translations for all Ti. 
End 
Table 7: Alternative-translation algorithm. 
                                                 
10 Default tokenizer does not handle the hyphenated words. 
To resolve this problem, we present an alter-
native-translation method in decoding. Table 7 
shows the proposed algorithm. 
In the implementation, we apply XML markup 
to record the translation (terminology) for each 
compound word. During the decoding, a hyphen-
ated word delimited with markup will be re-
placed with its corresponding translation. Table 8 
shows the BLEU scores of adapted systems ap-
plied to hyphenated translation. This method is 
effective for most language pairs. While the 
translation systems for en-cs and cs-en do not 
benefit from this adaptation, because the hy-
phenated words ratio in the en and cs dev are 
asymmetric. Thus, we only apply this method for 
en-fr, fr-en, de-en and en-de pairs. 
Lang. Pair BLEU (Dev) Diff. 
en-cs 16.84 -0.73 
cs-en 31.23 -0.06 
en-fr 39.12 +0.76 
fr-en 45.02 +0.66 
en-de 18.64 +0.63 
de-en 33.01 +0.51 
Table 8: BLEU scores of hyphenated word 
adapted systems. 
3. Final Results and Conclusions 
According to the performance of each individual 
domain adaptation approach, we combined the 
corresponding models for each language pair. In 
Table 10, we show the BLEU scores and its in-
crements (compared to the Baseline2) of com-
bined systems in the second column. The official 
test set is converted into the recased and deto-
kenized SGML format. The official results of our 
submissions are given in the last column of Table 
9. 
Lang. 
Pair 
BLEU of Com-
bined systems 
Official 
BLEU 
en-cs 23.66 (+6.09) 22.60 
cs-en 38.05 (+6.76) 37.60 
en-fr 42.30 (+3.94) 41.20 
fr-en 48.25 (+3.89) 47.10 
en-de 21.14 (+3.13) 20.90 
de-en 36.03 (+3.53) 35.70 
Table 9: BLEU scores of the submitted systems 
for the medical translation task. 
This paper presents a set of experiments con-
ducted on all available training data for six lan-
guage pairs. We explored various domain adap-
tation approaches for adapting medical transla-
258
tion systems. Compared with other methods, lan-
guage model adaptation and translation model 
adaptation are more effective. Other adapted 
techniques are still necessary and important for 
building a real-life system. Although all individ-
ual methods are not fully additive, combining 
them together can further boost the performance 
of the overall domain-specific system. We be-
lieve these empirical approaches could be valua-
ble for SMT development. 
Acknowledgments 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS. 
The authors also wish to thank the colleagues in 
CNGL, Dublin City University (DCU) for their 
helpful suggestion and guidance on related work. 
Reference 
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of EMNLP, pages 355-
362. 
K. Duh, G. Neubig, K. Sudoh, H. Tsukada. 2013. Ad-
aptation data selection using neural language mod-
els: Experiments in machine translation. In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, pages, 678?683. 
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49-57. 
Kenneth Heafield. 2011. KenLM: Faster and smaller 
language model queries. In Proceedings of the 
Sixth Workshop on Statistical Machine Transla-
tion, pages 187-197. 
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine 
translation. In Proceedings of the 2nd ACL Work-
shop on Statistical Machine Translation, pages 
224-227. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine Moran 
et al. 2007. Moses: open source toolkit for statisti-
cal machine translation. In Proceedings of ACL, 
pages 177-180. 
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In 
Proceedings of ACL: Short Papers, pages 220-224. 
S?tre Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiro Matsubayashi and Tomo-
ko Ohta. 2007. AKANE system: protein-protein in-
teraction pairs in BioCreAtIvE2 challenge, PPI-IPS 
subtask. In Proceedings of the Second BioCreative 
Challenge Evaluation Workshop, pages 209-212.  
Raphael Rubino, Antonio Toral, Santiago Cort?s 
Va?llo, Jun Xie, Xiaofeng Wu, Stephen Doherty, 
and Qun Liu. 2013. The CNGL-DCU-Prompsit 
translation systems for WMT13. In Proceedings of 
the Eighth Workshop on Statistical Machine Trans-
lation, pages 213-218. 
Andreas Stolcke and others. 2002. SRILM-An exten-
sible language modeling toolkit. In Proceedings of 
the International Conference on Spoken Language 
Processing, pages 901-904. 
Antonio Toral. 2013. Hybrid selection of language 
model training data using linguistic information 
and perplexity. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, 29:19-51. 
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceedings of 
the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160-167. 
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi 
Lu, and Junwen Xing. 2014 ?A Systematic Com-
parison of Data Selection Criteria for SMT Domain 
Adaptation,? The Scientific World Journal, vol. 
2014, Article ID 745485, 10 pages. 
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi 
Lu, Junwen Xing. 2013. iCPE: A Hybrid Data Se-
lection Model for SMT Domain Adaptation. Chi-
nese Computational Linguistics and Natural Lan-
guage Processing Based on Naturally Annotated 
Big Data. Springer Berlin Heidelberg. pages, 280-
290. 
 
259
