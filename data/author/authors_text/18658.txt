Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774?1778,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Joint Learning of Chinese Words, Terms and Keywords
Ziqiang Cao
1
Sujian Li
1
Heng Ji
2
1
Key Laboratory of Computational Linguistics, Peking University, MOE, China
2
Computer Science Department, Rensselaer Polytechnic Institute, USA
{ziqiangyeah, lisujian}@pku.edu.cn jih@rpi.edu
Abstract
Previous work often used a pipelined
framework where Chinese word segmen-
tation is followed by term extraction and
keyword extraction. Such framework suf-
fers from error propagation and is un-
able to leverage information in later mod-
ules for prior components. In this paper,
we propose a four-level Dirichlet Process
based model (DP-4) to jointly learn the
word distributions from the corpus, do-
main and document levels simultaneously.
Based on the DP-4 model, a sentence-wise
Gibbs sampler is adopted to obtain proper
segmentation results. Meanwhile, terms
and keywords are acquired in the sampling
process. Experimental results have shown
the effectiveness of our method.
1 Introduction
For Chinese language which does not contain ex-
plicitly marked word boundaries, word segmenta-
tion (WS) is usually the first important step for
many Natural Language Processing (NLP) tasks
including term extraction (TE) and keyword ex-
traction (KE). Generally, Chinese terms and key-
words can be regarded as words which are repre-
sentative of one domain or one document respec-
tively. Previous work of TE and KE normally used
the pipelined approaches which first conducted
WS and then extracted important word sequences
as terms or keywords.
It is obvious that the pipelined approaches are
prone to suffer from error propagation and fail to
leverage information for word segmentation from
later stages. Here, we provide one example in the
disease domain, to demonstrate the common prob-
lems in current pipelined approaches and propose
the basic idea of our joint learning of words, terms
and keywords.
Example: @??(thrombocytopenia) (with)
{? (heparinoid) 	(have) s?(relation).
This is a correctly segmented Chinese sen-
tence. The document containing the example sen-
tence mainly talks about the property of ?{?
 (heparinoid)? which can be regarded as one key-
word of the document. At the same time, the
word@??(thrombocytopenia) appears fre-
quently in the disease domain and can be treated
as a domain-specific term.
However, for such a simple sentence, current
segmentation tools perform poorly. The segmen-
tation result with the state-of-the-art Conditional
Random Fields (CRFs) approach (Zhao et al.,
2006) is as follows:
@(blood platelet) ?(reduction) ?(symptom)
{(of same kind) ?(liver) 	(always)s?(relation)
where @?? is segmented into three com-
mon Chinese words and {? is mixed with its
neighbors.
In a text processing pipeline of WS, TE and
KE, it is obvious that imprecise WS results will
make the overall system performance unsatisfy-
ing. At the same time, we can hardly make use of
domain-level and document-level information col-
lected in TE and KE to promote the performance
of WS. Thus, one question comes to our minds:
can words, terms and keywords be jointly learned
with consideration of all the information from the
corpus, domain, and document levels?
Recently, the hierarchical Dirichlet process
(HDP) model has been used as a smoothed bigram
model to conduct word segmentation (Goldwater
et al., 2006; Goldwater et al., 2009). Meanwhile,
one strong point of the HDP based models is that
they can model the diversity and commonality in
multiple correlated corpora (Ren et al., 2008; Xu
et al., 2008; Zhang et al., 2010; Li et al., 2012;
Chang et al., 2014). Inspired by such existing
work, we propose a four-level DP based model,
1774
0G1GwH mwH imjw imwH1mwH NmmwH3? ?????jmN2?1?0? M| |V
Figure 1: DP-4 Model
named DP-4, to adapt to three levels: corpus, do-
main and document. In our model, various DPs
are designed to reflect the smoothed word distri-
butions in the whole corpus, different domains and
different documents. Same as the DP based seg-
mentation models, our model can be easily used
as a semi-supervised framework, through exerting
on the corpus level the word distributions learned
from the available segmentation results. Refer-
ring to the work of Mochihashi et al. (2009), we
conduct word segmentation using a sentence-wise
Gibbs sampler, which combines the Gibbs sam-
pling techniques with the dynamic programming
strategy. During the sampling process, the impor-
tance values of segmented words are measured in
domains and documents respectively, and words,
terms and keywords are jointly learned.
2 DP-4 Model
Goldwater et al. (2006) applied the HDP model on
the word segmentation task. In essence, Goldwa-
ter?s model can be viewed as a bigram language
model with a unigram back-off. With the lan-
guage model, word segmentation is implemented
by a character-based Gibbs sampler which repeat-
edly samples the possible word boundary posi-
tions between two neighboring words, conditioned
on the current values of all other words. How-
ever, Goldwater?s model can be deemed as mod-
eling the whole corpus only, and does not distin-
guish between domains and documents. To jointly
learn the word information from the corpus, do-
main and document levels, we extend Goldwater?s
model by adding two levels (domain level and doc-
ument level) of DPs, as illustrated in Figure 1.
2.1 Model Description
M DPs (H
m
w
;1 ? m ? M ) are designed specif-
ically to word w to model the bigram distribu-
tions in each domain and these DPs share an
overall base measure H
w
, which is drawn from
DP (?
0
, G
1
) and gives the bigram distribution for
the whole corpus. Assuming the m
th
domain in-
cludes N
m
documents, we use H
m
j
w
(1 ? j ?
N
m
) to model the bigram distribution of the i
th
document in the domain. Usually, given a do-
main, the bigram distributions of different docu-
ments are not conditionally independent and simi-
lar documents exhibit similar bigram distributions.
Thus, the bigram distribution of one document is
generated according to both the bigram distribu-
tion of the domain and the bigram distributions
of other documents in the same domain. That is,
H
m
j
w
? g(?
3
, H
m
w
, H
m
?j
w
) where H
m
?j
w
repre-
sents the bigram distributions of the documents in
the m
th
domain except the j
th
document. Assum-
ing the j
th
document in the m
th
domain contains
N
j
m
words, each word is drawn according toH
m
j
w
.
That is, w
m
j
i
? H
m
j
w
(1 ? i ? N
j
m
). Thus, our
four-level DP model can be summarized formally
as follows:
G
1
? DP (?
0
, G
0
) ;H
w
? DP (?
1
, G
1
)
H
m
w
? DP (?
2
, H
w
) ;H
m
j
w
? g
(
?
3
, H
m
w
, H
m
?j
w
)
w
m
j
i
|w
i?1
= w ? H
d
w
Here, we provide for our model the Chinese
Restaurant Process (CRP) metaphor, which can
create a partition of items into groups. In our
model, the word type of the previous word w
i?1
corresponds to a restaurant and the current word
w
i
corresponds to a customer. Each domain is
analogous to a floor in a restaurant and a room de-
notes a document. Now, we can see that there are
|V | restaurants and each restaurant consists of M
floors. Them
th
floor containsN
m
rooms and each
room has an infinite number of tables with infinite
seating capacity. Customers enter a specific room
on a specific floor of one restaurant and seat them-
selves at a table with the label of a word type. Dif-
ferent from the standard HDP, each customer sits
at an occupied table with probability proportional
to both the numbers of customers already seated
there and the numbers of customers with the same
word type seated in the neighboring rooms, and at
an unoccupied table with probability proportional
to both the constant ?
3
and the probability that the
1775
customers with the same word type are seated on
the same floor.
2.2 Model Inference
It is important to build an accurate G
0
which de-
termines the prior word distribution p
0
(w). Sim-
ilar to the work of Mochihashi et al. (2009), we
consider the dependence between characters and
calculate the prior distribution of a word w
i
using
the string frequency statistics (Krug, 1998):
p
0
(w
i
) =
n
s
(w
i
)
?
n
s
(.)
(1)
where n
s
(w
i
) counts the character string com-
posed of w
i
and the symbol ?.? represents any
word in the vocabulary V .
Then, with the CRP metaphor, we can obtain the
expected word unigram and bigram distributions
on the corpus level according to G
1
and H
w
:
p
1
(w
i
) =
n (w
i
) + ?
0
p
0
(w
i
)
?
n (.) + ?
0
(2)
p
2
(w
i
|w
i?1
= w) =
n
w
(w
i
) + ?
1
p
1
(w
i
)
?
n
w
(.) + ?
1
(3)
where the subscript numbers indicate the corre-
sponding DP levels. n(w
i
) denotes the number of
w
i
and n
w
(w
i
) denotes the number of the bigram
< w,w
i
> occurring in the corpus. Next, we can
easily get the bigram distribution on the domain
level by extending to the third DP.
p
m
3
(w
i
|w
i?1
= w) =
n
m
w
(w
i
) + ?
2
p
2
(w
i
|w
i?1
)
?
n
m
w
(.) + ?
2
(4)
where n
m
w
(w
i
) is the number of the bigram <
w,w
i
> occurring in the m
th
domain.
To model the bigram distributions on the docu-
ment level, it is beneficial to consider the influence
of related documents in the same domain (Wan
and Xiao, 2008). Here, we only consider the in-
fluence from theK most similar documents with a
simple similarity metric s(d
1
, d
2
) which calculates
the Chinese character overlap ratio of two docu-
ments d
1
and d
2
. Let d
j
m
denote the j
th
document
in the m
th
domain and d
j
m
[k](1 ? k ? K) the K
most similar documents. d
j
m
can be deemed to be
?lengthened? by d
j
m
[k](1 ? k ? K). Therefore,
we estimate the count of w
i
in d
j
m
as:
t
d
j
m
w
(w
i
) = n
d
j
m
w
(w
i
)+
?
k
s(d
j
m
[k], d
j
m
)n
d
j
m
[k]
w
(w
i
)
(5)
where n
d
j
m
[k]
w
(w
i
) denotes the count of the bigram
< w,w
i
> occurring in d
j
m
[k]. Next, we model
the bigram distribution in d
j
m
as a DP with the base
measure H
m
w
:
p
d
j
m
4
(w
i
|w
i?1
= w) =
t
d
j
m
w
(w
i
) + ?
3
p
m
3
(w
i
|w
i?1
)
?
t
d
j
m
w
(.) + ?
3
(6)
With CRP, we can also easily estimate the un-
igram probabilities p
m
3
(w
i
) and p
d
j
m
4
(w
i
) respec-
tively on the domain and document levels, through
combining all the restaurants.
To measure whether a word is eligible to be a
term, the score function TH
m
(?) is defined as:
TH
m
(w
i
) =
p
m
3
(w
i
)
p
1
(w
i
)
(7)
This equation is inspired by the work of Nazar
(2011), which extracts terms with consideration of
both the frequency in the domain corpus and the
frequency in the general reference corpus. Similar
to Eq. 7, we define the functionKH
d
j
m
(?) to judge
whether w
i
is an appropriate keyword.
KH
d
j
m
(w
i
) =
p
d
j
m
4
(w
i
)
p
1
(w
i
)
(8)
During each sampling, we make use of Eqs. (7)
and (8) to identify the most possible terms and
keywords. Once a word is identified as a term
or keyword, it will drop out of the sampling pro-
cess in the following iterations. Its CRP explana-
tion is that some customers (terms and keywords)
find their proper tables and keep sitting there after-
wards.
2.3 Sentence-wise Gibbs Sampler
The character-based Gibbs sampler for word seg-
mentation (Goldwater et al., 2006) is extremely
slow to converge, since there exists high correla-
tion between neighboring words. Here, we intro-
duce the sentence-wise Gibbs sampling technique
as well as efficient dynamic programming strat-
egy proposed by Mochihashi et al. (2009). The
basic idea is that we randomly select a sentence
in each sampling process and use the Viterbi al-
gorithm (Viterbi, 1967) to find the optimal seg-
mentation results according to the word distribu-
tions derived from other sentences. Different from
Mochihashi?s work, once terms or keywords are
1776
identified, we do not consider them in the segmen-
tation process. Due to space limitation, the algo-
rithm is not detailed here and can be referred in
(Mochihashi et al., 2009).
3 Experiment
3.1 Data and Setting
It is indeed difficult to find a standard evaluation
corpus for our joint tasks, especially in different
domains. As a result, we spent a lot of time to col-
lect and annotate a new corpus
1
composed of ten
domains (including Physics, Computer, Agricul-
ture, Sports, Disease, Environment, History, Art,
Politics and Economy) and each domain is com-
posed of 200 documents. On average each doc-
ument consists of about 4800 Chinese characters.
For these 2000 documents, three annotators have
manually checked the segmented words, terms and
keywords as the gold standard results for evalu-
ation. As we know, there exists a large amount
of manually-checked segmented text for the gen-
eral domain, which can be used as the training data
for further segmentation. As with other nonpara-
metric Bayesian models (Goldwater et al., 2006;
Mochihashi et al., 2009), our DP-4 model can be
easily amenable to semi-supervised learning by
imposing the word distributions of the segmented
text on the corpus level. The news texts pro-
vided by Peking University (named PKU corpus)
2
is used as the training data. This corpus contains
about 1,870,000 Chinese characters and has been
manually segmented into words.
In our experiments, the concentration coeffi-
cient (?
0
) is finally set to 20 and the other three
(?
1?3
) are set to 15. The parameter K which con-
trols the number of similar documents is set to 3.
3.2 Performance Evaluation
The following baselines are implemented for com-
parison of segmentation results: (1) Forward max-
imum matching (FMM) algorithm with a vocab-
ulary compiled from the PKU corpus; (2) Re-
verse maximum matching (RMM) algorithm with
the compiled vocabulary; (3) Conditional Random
Fields (CRFs)
3
based supervised algorithm trained
from the PKU corpus; (4) HDP based semi-
supervised algorithm (Goldwater et al., 2006) us-
1
Nine domains are from http://www.datatang.
com/data/44139 and we add an extra Disease domain.
2
http://icl.pku.edu.cn
3
We adopt CRF++(http://crfpp.googlecode.
com/svn/trunk/doc/index.html)
ing the PKU corpus. The strength of Mochi-
hashi et al. (2009)?s NPYLM based segmentation
model is its speed due to the sentence-wise sam-
pling technique, and its performance is similar to
Goldwater et al. (2006)?s model. Thus, we do not
consider the NPYLM based model for compari-
son here. Then, the segmentation results of FMM,
RMM, CRF, and HDP methods are used respec-
tively for further extracting terms and keywords.
We use the mutual information to identify the can-
didate terms or keywords composed of more than
two segmented words. As for DP-4, this recogni-
tion process has been done implicitly during sam-
pling. To measure the candidate terms or key-
words, we refer to the metric in Nazar (2011) to
calculate their importance in some specific domain
or document.
The metrics of F
1
and the out-of-vocabulary
Recall (OOV-R) are used to evaluate the segmenta-
tion results, referring to the gold standard results.
The second and third columns of Table 1 show the
F
1
and OOV-R scores averaged on the 10 domains
for all the compared methods. Our method sig-
nificantly outperforms FMM, RMM and HDP ac-
cording to t-test (p-value ? 0.05). From the seg-
mentation results, we can see that the FMM and
RMM methods are highly dependent on the com-
piled vocabulary and their identified OOV words
are mainly the ones composed of a single Chinese
character. The HDP method is heavily influenced
by the segmented text, but it also exhibits the abil-
ity of learning new words. Our method only shows
a slight advantage over the CRF approach. We
check our segmentation results and find that the
performance of the DP-4 model is depressed by
the identified terms and keywords which may be
composed of more than two words in the gold
standard results, because the DP-4 model always
treats the term or keyword as a single word. For
example, in the gold standard, ??W?((Lingnan
Culture)? is segmented into two words ??W? and
???, ?pn??(data interface)? is segmented
into ?pn? and ???? and so on. In fact, our seg-
mentation results correctly treat ??W?? and ?p
n??? as words.
To evaluate the TE and KE performance, the top
50 (TE-50) and 100 (TE-100) accuracy are mea-
sured for the identified terms of one domain, while
the top 5 (KE-5) and 10 (KE-10) accuracy for the
keywords in one document, are shown in the right
four columns of Table 1. We can see that DP-
1777
4 performs significantly better than all the other
methods in TE and KE results.
As for the ten domains, we find our approach
behaves much better than the other approaches on
the following three domains: Disease, Physics and
Computer. It is because the language of these
three domains is much different from that of the
general domain (PKU corpus), while the rest do-
mains are more similar to the general domain.
Method F1 OOV-R TE-50 TE-100 KE-5 KE-10
FMM 0.796 0.136 0.420 0.360 0.476 0.413
RMM 0.794 0.136 0.424 0.352 0.478 0.414
HDP 0.808 0.356 0.672 0.592 0.552 0.506
CRF 0.817 0.330 0.624 0.560 0.543 0.511
DP-4 0.821 0.374 0.704 0.640 0.571 0.545
Table 1: Comparison of WS, TE and KE Perfor-
mance (averaged on the 10 domains).
4 Conclusion
This paper proposes a four-level DP based model
to construct the word distributions from the cor-
pus, domain and document levels simultaneously,
through which Chinese words, terms and key-
words can be learned jointly and effectively. In
the future, we plan to explore how to combine
more features such as part-of-speech tags into our
model.
Acknowledgments
We thank the three anonymous reviewers for
their helpful comments. This work was par-
tially supported by National High Technology Re-
search and Development Program of China (No.
2012AA011101), National Key Basic Research
Program of China (No. 2014CB340504), Na-
tional Natural Science Foundation of China (No.
61273278), and National Key Technology R&D
Program (No: 2011BAH10B04-03). The contact
author of this paper, according to the meaning
given to this role by Peking University, is Sujian
Li.
References
Baobao Chang, Wenzhe Pei, and Miaohong Chen.
2014. Inducing word sense with automatically
learned hidden concepts. In Proceedings of COL-
ING 2014, pages 355?364, Dublin, Ireland, August.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 673?
680.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Manfred Krug. 1998. String frequency: A cognitive
motivating factor in coalescence, language process-
ing, and linguistic change. Journal of English Lin-
guistics, 26(4):286?320.
Jiwei Li, Sujian Li, Xun Wang, Ye Tian, and Baobao
Chang. 2012. Update summarization using a multi-
level hierarchical dirichlet process model. In Pro-
ceedings of Coling 2012, pages 1603?1618, Mum-
bai, India.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested pitman-yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, pages 100?108.
Rogelio Nazar. 2011. A statistical approach to term
extraction. IJES, International Journal of English
Studies, 11(2):159?182.
Lu Ren, David B. Dunson, and Lawrence Carin. 2008.
The dynamic hierarchical dirichlet process. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 824?831.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. IEEE Transactions on Information
Theory, pages 260?269.
Xiaojun Wan and Jianguo Xiao. 2008. Single
document keyphrase extraction using neighborhood
knowledge. In AAAI, volume 8, pages 855?860.
Tianbing Xu, Zhongfei Zhang, Philip S. Yu, and
Bo Long. 2008. Dirichlet process based evolution-
ary clustering. In ICDM?08, pages 648?657.
Jianwen Zhang, Yangqiu Song, Changshui Zhang, and
Shixia Liu. 2010. Evolutionary hierarchical dirich-
let processes for multiple correlated time-varying
corpora. In Proceedings of the 16th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 1079?1088, New York, NY,
USA.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, volume 1082117.
1778
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 25?35,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Text-level Discourse Dependency Parsing 
 
Sujian Li1 Liang Wang1 Ziqiang Cao1 Wenjie Li2 
1 Key Laboratory of Computational Linguistics, Peking University, MOE, China 
2 Department of Computing, The Hong Kong Polytechnic University, HongKong 
{lisujian,intfloat,ziqiangyeah}@pku.edu.cn 
cswjli@comp.polyu.edu.hk 
  
 
Abstract 
Previous researches on Text-level discourse 
parsing mainly made use of constituency 
structure to parse the whole document into 
one discourse tree. In this paper, we present 
the limitations of constituency based dis-
course parsing and first propose to use de-
pendency structure to directly represent the 
relations between elementary discourse 
units (EDUs). The state-of-the-art depend-
ency parsing techniques, the Eisner algo-
rithm and maximum spanning tree (MST) 
algorithm, are adopted to parse an optimal 
discourse dependency tree based on the arc-
factored model and the large-margin learn-
ing techniques. Experiments show that our 
discourse dependency parsers achieve a 
competitive performance on text-level dis-
course parsing.  
1 Introduction 
It is widely agreed that no units of the text can be 
understood in isolation, but in relation to their 
context. Researches in discourse parsing aim to 
acquire such relations in text, which is funda-
mental to many natural language processing ap-
plications such as question answering, automatic 
summarization and so on. 
One important issue behind discourse parsing 
is the representation of discourse structure. Rhe-
torical Structure Theory (RST) (Mann and 
Thompson, 1988), one of the most influential 
discourse theories, posits a hierarchical genera-
tive tree representation, as illustrated in Figure 1. 
The leaves of a tree correspond to contiguous 
text spans called Elementary Discourse Units 
(EDUs)1. The adjacent EDUs are combined into 
                                                          
1 EDU segmentation is a relatively trivial step in discourse 
parsing. Since our work focus here is not EDU segmenta-
tion but discourse parsing. We assume EDUs are already 
known. 
the larger text spans by rhetorical relations (e.g., 
Contrast and Elaboration) and the larger text 
spans continue to be combined until the whole 
text constitutes a parse tree. The text spans 
linked by rhetorical relations are annotated as 
either nucleus or satellite depending on how sali-
ent they are for interpretation. It is attractive and 
challenging to parse the whole text into one tree.  
Since such a hierarchical discourse tree is 
analogous to a constituency based syntactic tree 
except that the constituents in the discourse trees 
are text spans, previous researches have explored 
different constituency based syntactic parsing 
techniques (eg. CKY and chart parsing) and var-
ious features (eg. length, position et al) for dis-
course parsing (Soricut and Marcu, 2003; Joty et 
al., 2012; Reitter, 2003; LeThanh et al, 2004; 
Baldridge and Lascarides, 2005; Subba and Di 
Eugenio, 2009; Sagae, 2009; Hernault et al, 
2010b; Feng and Hirst, 2012). However, the ex-
isting approaches suffer from at least one of the 
following three problems. First, it is difficult to 
design a set of production rules as in syntactic 
parsing, since there are no determinate genera-
tive rules for the interior text spans. Second, the 
different levels of discourse units (e.g. EDUs or 
larger text spans) occurring in the generative 
process are better represented with different fea-
tures, and thus a uniform framework for dis-
course analysis is hard to develop. Third, to 
reduce the time complexity of the state-of-the-art 
constituency based parsing techniques, the ap-
proximate parsing approaches are prone to trap 
in local maximum. 
In this paper, we propose to adopt the depend-
ency structure in discourse representation to 
overcome the limitations mentioned above. Here 
is the basic idea: the discourse structure consists 
of EDUs which are linked by the binary, asym-
metrical relations called dependency relations. A 
dependency relation holds between a subordinate 
EDU called the dependent, and another EDU on 
25
which it depends called the head, as illustrated in 
Figure 2. Each EDU has one head. So, the de-
pendency structure can be seen as a set of head-
dependent links, which are labeled by functional 
relations. Now, we can analyze the relations be-
tween EDUs directly, without worrying about 
any interior text spans. Since dependency trees 
contain much fewer nodes and on average they 
are simpler than constituency based trees, the 
current dependency parsers can have a relatively 
low computational complexity. Moreover, con-
cerning linearization, it is well known that de-
pendency structures can deal with non-projective 
relations, while constituency-based models need 
the addition of complex mechanisms like trans-
formations, movements and so on. In our work, 
we adopt the graph based dependency parsing 
techniques learned from large sets of annotated 
dependency trees. The Eisner (1996) algorithm 
and maximum spanning tree (MST) algorithm 
are used respectively to parse the optimal projec-
tive and non-projective dependency trees with 
the large-margin learning technique (Crammer 
and Singer, 2003). To the best of our knowledge, 
we are the first to apply the dependency structure 
and introduce the dependency parsing techniques 
into discourse analysis.  
The rest of this paper is organized as follows. 
Section 2 formally defines discourse dependency 
structure and introduces how to build a discourse 
dependency treebank from the existing RST cor-
pus. Section 3 presents the discourse parsing ap-
proach based on the Eisner and MST algorithms. 
Section 4 elaborates on the large-margin learning 
technique as well as the features we use. Section 
5 discusses the experimental results. Section 6 
introduces the related work and Section 7 con-
cludes the paper. 
1
e2
e1-e2
*
e3
e1- 2
*
-e3
e1 e2
e1-e2
*
e3
e1-e2-e3
*
e1 e2
e1
*
-e2
e3
e1-e2-e3
*
e1
e2
e1
*
-e2
e3
e1
*
-e2-e3
e1 e2
e2
*
-e3
e3
e1-e2
*
-e3
e1 e2
e3 e1 e2
e3
e1
*
-e2-e3
e2
*
-e3 e2-e3
*
e1
*
-e2-e3
1 2 3 4
5 6 7 8
e1
e2 e3
e1- 2- 3
*
e2-e3
*
 
Figure 1: Headed Constituency based Discourse Tree Structure (e1,e2 and e3 denote three EDUs, 
and * denotes the NUCLEUS constituent) 
e1 e2 e3 e1 e2 e3 e1 e2 e3 e1 e2 e3
e1 e2 e3e1 e2 e3 e1 e2 e3e1 e2 e3
1' 2' 3' 4'
5' 6' 7' 8' 9'
e1 e2 e3
e0e0 e0 e0
e0 e0 e0 e0 e0
 
Figure 2: Discourse Dependency Tree Structures (e1,e2 and e3 denote three EDUS, and the directed 
arcs  denote one dependency relations. The artificial e0 is also displayed here. ) 
2 Discourse Dependency Structure and 
Tree Bank 
2.1 Discourse Dependency Structure 
Similar to the syntactic dependency structure 
defined by McDonald (2005a, 2005b), we insert 
an artificial EDU e0 in the beginning for each 
document and label the dependency relation link-
ing from e0 as ROOT. This treatment will sim-
plify both formal definitions and computational 
implementations. Normally, we assume that each 
EDU should have one and only one head except 
for e0. A labeled directed arc is used to represent 
the dependency relation from one head to its de-
pendent. Then, discourse dependency structure 
can be formalized as the labeled directed graph, 
where nodes correspond to EDUs and labeled 
arcs correspond to labeled dependency relations. 
26
We assume that the text2  T is composed of 
n+1 EDUs including the artificial e0. That is 
T=e0 e1 e2 ? en. Let R={r1,r2, ? ,rm} denote a 
finite set of functional relations that hold be-
tween two EDUs. Then a discourse dependency 
graph can be denoted by G=<V, A> where V de-
notes a set of nodes and A denotes a set of la-
beled directed arcs, such that for the text T=e0 e1 
e2 ? en and the label set R the following holds: 
(1) V = { e0, e1, e2, ? en } 
(2) A ? V? R ? V, where <ei, r, ej>?A represents 
an arc from the head ei to the dependent ej 
labeled with the relation r. 
(3) If <ei, r, ej>?A then <ek, r?, ej>?A for all k?i  
(4) If <ei, r, ej>?A then <ei, r?, ej>?A for all r??r 
The third condition assures that each EDU has 
one and only one head and the fourth tells that 
only one kind of dependency relation holds be-
tween two EDUs. According to the definition, 
we illustrate all the 9 possible unlabeled depend-
ency trees for a text containing three EDUs in 
Figure 2. The dependency trees 1? to 7? are pro-
jective while 8? and 9? are non-projective with 
crossing arcs. 
2.2 Our Discourse Dependency Treebank  
To automatically conduct discourse dependency 
parsing, constructing a discourse dependency 
treebank is fundamental. It is costly to manually 
construct such a treebank from scratch. Fortu-
nately, RST Discourse Treebank (RST-DT) 
(Carlson et al, 2001) is an available resource to 
help with.  
A RST tree constitutes a hierarchical structure 
for one document through rhetorical relations. A 
total of 110 fine-grained relations (e.g. Elabora-
tion-part-whole and List) were used for tagging 
RST-DT. They can be categorized into 18 classes 
(e.g. Elaboration and Joint). All these relations 
can be hypotactic (?mononuclear?) or paratactic 
(?multi-nuclear?). A hypotactic relation holds 
between a nucleus span and an adjacent satellite 
span, while a paratactic relation connects two or 
more equally important adjacent nucleus spans. 
For convenience of computation, we convert the 
n-ary (n>2) RST trees3 to binary trees through 
adding a new node for the latter n-1 nodes and 
assume each relation is connected to only one 
nucleus4. This departure from the original theory 
                                                          
2 The two terms ?text? and ?document? are used inter-
changeably and represent the same meaning. 
3 According to our statistics, there are totally 381 n-ary rela-
tions in RST-DT.  
4 We set the first nucleus as the only nucleus. 
is not such a major step as it may appear, since 
any nucleus is known to contribute to the essen-
tial meaning. Now, each RST tree can be seen as 
a headed constituency based binary tree where 
the nuclei are heads and the children of each 
node are linearly ordered. Given three EDUs5, 
Figure 1 shows the possible 8 headed constituen-
cy based trees where the superscript * denotes 
the heads (nuclei). We use dependency trees to 
simulate the headed constituency based trees.  
Contrasting Figure 1 with Figure 2, we use 
dependency tree 1? to simulate binary trees 1 and 
8, and dependency tress 2?- 7? to simulate binary 
trees 2-7 correspondingly. The rhetorical rela-
tions in RST trees are kept as the functional rela-
tions which link the two EDUs in dependency 
trees. With this kind of conversion, we can get 
our discourse dependency treebank. It is worth 
noting that the non-projective trees like 8? and 9? 
do not exist in our dependency treebank, though 
they are eligible according to the definition of 
discourse dependency graph.  
3 Discourse Dependency Parsing 
3.1 System Overview 
As stated above, T=e0 e1 ?en represents an input 
text (document) where ei denotes the i
th EDU of 
T. We use V to denote all the EDU nodes and 
V?R?V-0 (V-0 =V-{e0}) denote all the possible 
discourse dependency arcs. The goal of discourse 
dependency parsing is to parse an optimal span-
ning tree from V?R?V-0. Here we follow the arc 
factored method and define the score of a de-
pendency tree as the sum of the scores of all the 
arcs in the tree. Thus, the optimal dependency 
tree for T is a spanning tree with the highest 
score and obtained through the function DT(T,w): 
0
0
0
, ,
, ,
( , )( , )
( , , )
( , , )f
T
T
i j T
T
i j T
G V R V
G V R V i j
e r e G
G V R V i j
e r e G
TDT T argmax
argmax e r e
argmax e r
s ore T G
e
c
?
?
?
?
? ? ?
? ? ?
? ??
? ? ?
? ??
?
?
? ?
?
?
w
w
where GT means a possible spanning tree with 
( , )Tscore T G  and ?(       ) denotes the score of 
the arc <ei, r, ej> which is calculated according to 
its feature representation f(ei,r,ej) and a weight 
vector w. 
Next, two basic problems need to be solved: 
how to find the dependency tree with the highest 
                                                          
5 We can easily get al possible headed binary trees for one 
more complex text containing more than three EDUs, by 
extending the 8 possible situations for three EDUs.  
27
score for T given all the arc scores (i.e. a parsing 
problem), and how to learn and compute the 
scores of arcs according to a set of arc features 
(i.e. a learning problem).  
The following of this section addresses the 
first problem. Given the text T, we first reduce 
the multi-digraph composed of all possible arcs 
to the digraph. The digraph keeps only one arc 
<ei, r, ej> between two nodes which satisfies 
?(       )                  . Thus, we can 
proceed with a reduction from labeled parsing to 
unlabeled parsing. Next, two algorithms, i.e. the 
Eisner algorithm and MST algorithm, are pre-
sented to parse the projective and non-projective 
unlabeled dependency trees respectively. 
3.2 Eisner Algorithm 
It is well known that projective dependency pars-
ing can be handled with the Eisner algorithm 
(1996) which is based on the bottom-up dynamic 
programming techniques with the time complexi-
ty of O(n3). The basic idea of the Eisner algo-
rithm is to parse the left and right dependents of 
an EDU independently and combine them at a 
later stage. This reduces the overhead of index-
ing heads. Only two binary variables, i.e. c and d, 
are required to specify whether the heads occur 
leftmost or rightmost and whether an item is 
complete. 
 
Eisner(T,  ?) 
Input: Text T=e0 e1? en; Arc scores ?(ei,ej) 
1   Instantiate E[i, i, d, c]=0.0 for all i, d, c 
2   For m := 1 to n 
3       For i := 1 to n 
4          j = i + m 
5          if j> n then break;  
6          # Create subgraphs with c=0 by adding arcs 
7         E[i, j, 0, 0]=maxi?q?j (E[i,q,1,1]+E[q+1,j,0,1]+?(ej,ei)) 
8         E[i, j, 1, 0]=maxi?q?j (E[i,q,1,1]+E[q+1,j,0,1]+?(ei,ej)) 
9          # Add corresponding left/right subgraphs 
10        E[i, j, 0, 1]=maxi?q?j (E[i,q,0,1]+E[q,j,0,0] 
11        E[i, j, 1, 1]=maxi?q?j (E[i,q,1,0]+E[q,j,1,1]) 
Figure 3: Eisner Algorithm 
Figure 3 shows the pseudo-code of the Eisner 
algorithm. A dynamic programming table 
E[i,j,d,c] is used to represent the highest scored 
subtree spanning ei to ej. d indicates whether ei is 
the head (d=1) or ej is head (d=0). c indicates 
whether the subtree will not take any more de-
pendents (c=1) or it needs to be completed (c=0). 
The algorithm begins by initializing all length-
one subtrees to a score of 0.0. In the inner loop, 
the first two steps (Lines 7 and 8) are to construct 
the new dependency arcs by taking the maximum 
over all the internal indices (i?q?j) in the span, 
and calculating the value of merging the two sub-
trees and adding one new arc. The last two steps 
(Lines 10 and 11) attempt to achieve an optimal 
left/right subtree in the span by adding the corre-
sponding left/right subtree to the arcs that have 
been added previously. This algorithm considers 
all the possible subtrees. We can then get the 
optimal dependency tree with the score 
E[0,n,1,1] . 
3.3 Maximum Spanning Tree Algorithm  
As the bottom-up Eisner Algorithm must main-
tain the nested structural constraint, it cannot 
parse the non-projective dependency trees like 8? 
and 9? in Figure 2. However, the non-projective 
dependency does exist in real discourse. For ex-
ample, the earlier text mainly talks about the top-
ic A with mentioning the topic B, while the latter 
text gives a supplementary explanation for the 
topic B. This example can constitute a non-
projective tree and its pictorial diagram is exhib-
ited in Figure 4. Following the work of McDon-
ald (2005b), we formalize discourse dependency 
parsing as searching for a maximum spanning 
tree (MST) in a directed graph. 
... ...
A A AB B
...
 
Figure 4: Pictorial Diagram of Non-projective 
Trees 
Chu and Liu (1965) and Edmonds (1967) in-
dependently proposed the virtually identical al-
gorithm named the Chu-Liu/Edmonds algorithm, 
for finding MSTs on directed graphs (McDonald 
et al 2005b). Figure 5 shows the details of the 
Chu-Liu/Edmonds algorithm for discourse pars-
ing. Each node in the graph greedily selects the 
incoming arc with the highest score. If one tree 
results, the algorithm ends. Otherwise, there 
must exist a cycle. The algorithm contracts the 
identified cycle into a single node and recalcu-
lates the scores of the arcs which go in and out of 
the cycle. Next, the algorithm recursively call 
itself on the contracted graph. Finally, those arcs 
which go in or out of one cycle will recover 
themselves to connect with the original nodes in 
V. Like McDonald et al (2005b), we adopt an 
efficient implementation of the Chu-
Liu/Edmonds algorithm that is proposed by Tar-
jan (1997) with O(n2) time complexity. 
 
28
Chu-Liu-Edmonds(G, ?) 
Input: Text T=e0 e1? en; Arc scores ?(ei,ej) 
1      A? = {<ei, ej>| ei = argmax ?(ei,ej); 1?j?|V|} 
2      G? = (V, A?) 
3      If G? has no cycles, then return G?  
4      Find an arc set AC that is a cycle in G? 
5      <GC, ep> = contract(G, AC, ?) 
6      G = (V, A)=Chu-Liu-Edmonds(GC, ?) 
7      For the arc <ei,eC> where ep(ei,eC)=ej: 
8              A=A?AC?{<ei,ej)}-{<ei,eC>, <a(ej),ej>} 
9      For the arc <eC, ei> where ep(eC ,ei)=ej:  
10            A=A?{<ej,ei>}-{<eC,ei>} 
11    V = V 
12    Return G 
Contract(G=(V,A), AC, ?) 
1   Let GC be the subgraph of G excluding nodes in C 
2   Add a node eC to GC denoting the cycle C 
3   For ej ?V-C : ?ei?C <ei,ej>?A 
4        Add arc <eC,ej> to GC with  
ep(eC,ej)=          ?(ei,ej) 
5        ?(eC,ej) = ?(ep(eC,ej),ej) 
6    For ei ?V-C: ?ej?C   (ei,ej)?A 
7         Add arc <ei,eC> to GC with 
                  ep(ei,eC)= =           [?(ei,ej)-?(a(ei),ej)] 
8         ?(ei,eC) =?(ei,ej)-?(a(ei),ej)+score(C) 
9   Return <GC, ep> 
Figure 5: Chu-Liu/Edmonds MST Algorithm 
4 Learning 
In Section 3, we assume that the arc scores are 
available. In fact, the score of each arc is calcu-
lated as a linear combination of feature weights. 
Thus, we need to determine the features for arc 
representation first. With referring to McDonald 
et al (2005a; 2005b), we use the Margin Infused 
Relaxed Algorithm (MIRA) to learn the feature 
weights based on a training set of documents 
annotated with dependency structures ? ?? ? 1, Ni iT ?iy  
where yi denotes the correct dependency tree for 
the text Ti. 
4.1 Features 
Following (Feng and Hirst, 2012; Lin et al, 2009; 
Hernault et al, 2010b), we explore the following 
6 feature types combined with relations to repre-
sent each labeled arc <ei, r, ej> . 
(1) WORD: The first one word, the last one 
word, and the first bigrams in each EDU, the pair 
of the two first words and the pair of the two last 
words in the two EDUs are extracted as features. 
(2) POS: The first one and two POS tags in each 
EDU, and the pair of the two first POS tags in 
the two EDUs are extracted as features. 
(3) Position: These features concern whether the 
two EDUs are included in the same sentence, and 
the positions where the two EDUs are located in 
one sentence, one paragraph, or one document. 
(4) Length: The length of each EDU.  
(5) Syntactic:  POS tags of the dominating nodes 
as defined in Soricut and Marcu (2003) are ex-
tracted as features. We use the syntactic trees 
from the Penn Treebank to find the dominating 
nodes,. 
(6) Semantic similarity: We compute the se-
mantic relatedness between the two EDUs based 
on WordNet. The word pairs are extracted from 
(ei, ej) and their similarity is calculated. Then, we 
can get a weighted complete bipartite graph 
where words are deemed as nodes and similarity 
as weights. From this bipartite graph, we get the 
maximum weighted matching and use the aver-
aged weight of the matches as the similarity be-
tween ei and ej. In particular, we use 
path_similarity, wup_similarity, res_similarity, 
jcn_similarity and lin_similarity provided by the 
nltk.wordnet.similarity (Bird et. al., 2009) pack-
age for calculating word similarity. 
As for relations, we experiment two sets of 
relation labels from RST-DT. One is composed 
of 19 coarse-grained relations and the other 111 
fine-grained relations6.  
4.2 MIRA based Learning 
Margin Infused Relaxed Algorithm (MIRA) is an 
online algorithm for multiclass classification and 
is extended by Taskar et al (2003) to cope with 
structured classification.  
 
MIRA   Input: a training set ? ?? ? 1, Ni iT ?iy  
1      w0 = 0; v = 0; j = 0  
2      For iter := 1 to K 
3            For i := 1 to N 
4                   update w according to ? ?,iT iy : 
1min j j? ?w w
 
                  s.t.  ( , ) ( , ') ( , ')
where  ' ( , )
i i i i i i
j
i i
s T s T L
DT T
? ?
?
y y y y
y w
 
5                      v = v + wj ; 
6                      j = j+1   
7       w = v/(K*N) 
Figure 6: MIRA based Learning 
Figure 6 gives the pseudo-code of the MIRA 
algorithm (McDonld et al, 2005b). This algo-
rithm is designed to update the parameters w us-
ing a single training instance ? ?,iT iy  in each 
iteration. On each update, MIRA attempts to 
keep the norm of the change to the weight vector 
                                                          
6 19 relations include the original 18 relation in RST-DT 
plus one artificial ROOT relation. The 111 relations also 
include the ROOT relation. 
29
as small as possible, which is subject to con-
structing the correct dependency tree under con-
sideration with a margin at least as large as the 
loss of the incorrect dependency trees. We define 
the loss of a discourse dependency tree 'iy  (de-
noted by ( , ')i iL y y  ) as the number of the EDUs 
that have incorrect heads. Since there are expo-
nentially many possible incorrect dependency 
trees and thus exponentially many margin con-
straints, here we relax the optimization and stay 
with a single best dependency tree 
' ( , )ji iDT T?y w  which is parsed under the weight 
vector wj. In this algorithm, the successive up-
dated values of w are accumulated and averaged 
to avoid overfitting.  
5 Experiments 
5.1 Preparation 
We test our methods experimentally using the 
discourse dependency treebank which is built as 
in Section 2. The training part of the corpus is 
composed of 342 documents and contains 18,765 
EDUs, while the test part consists of 38 docu-
ments and 2,346 EDUs. The number of EDUs in 
each document ranges between 2 and 304. Two 
sets of relations are adopted. One is composed of 
19 relations and Table 1 shows the number of 
each relation in the training and test corpus. The 
other is composed of 111 relations. Due to space 
limitation, Table 2 only lists the 10 highest-
distributed relations with regard to their frequen-
cy in the training corpus.  
The following experiments are conducted: (1) 
to measure the parsing performance with differ-
ent relation sets and different feature types; (2) to 
compare our parsing methods with the state-of-
the-art discourse parsing methods.  
 
Relations Train Test Relations Train Test 
Elaboration 6879 796 Temporal 426 73 
Attribution 2641 343 ROOT 342 38 
Joint 1711 212 Compari. 273 29 
Same-unit 1230 127 Condition 258 48 
Contrast 944 146 Manner. 191 27 
Explanation 849 110 Summary 188 32 
Background 786 111 Topic-Cha. 187 13 
Cause 785 82 Textual 147 9 
Evaluation 502 80 TopicCom. 126 24 
Enablement 500 46 Total 18765 2346 
Table 1: Coarse-grained Relation Distribution 
 
 
Relations Train Test 
Elaboration-additional 2912 312 
Attribution 2474 329 
Elaboration-object-attribute-e 2274 250 
List 1690 206 
Same-unit 1230 127 
Elaboration-additional-e 747 69 
Circumstance 545 80 
Explanation-argumentative 524 70 
Purpose 430 43 
Contrast 358 64 
Table 2: 10 Highest Distributed Fine-grained 
Relations 
5.2 Feature Influence on Two Relation Sets 
So far, researches on discourse parsing avoid 
adopting too fine-grained relations and the rela-
tion sets containing around 20 labels are widely 
used. In our experiments, we observe that adopt-
ing a fine-grained relation set can even be helpful 
to building the discourse trees. Here, we conduct 
experiments on two relation sets that contain 19 
and 111 labels respectively. At the same time, 
different feature types are tested their effects on 
discourse parsing.  
Method Features Unlabeled 
Acc. 
Labeled 
Acc. 
Eisner 1+2 0.3602 0.2651 
1+2+3 0.7310 0.4855 
1+2+3+4 0.7370 0.4868 
1+2+3+4+5 0.7447 0.4957 
1+2+3+4+5+6 0.7455 0.4983 
MST 1+2 0.1957 0.1479 
1+2+3 0.7246 0.4783 
1+2+3+4 0.7280 0.4795 
1+2+3+4+5 0.7340 0.4915 
1+2+3+4+5+6 0.7331 0.4851 
Table 3: Performance Using Coarse-grained Re-
lations. 
Method Feature types Unlabeled 
Acc. 
Labeled 
Acc. 
Eisner 1+2 0.3743 0.2421 
1+2+3 0.7451 0.4079 
1+2+3+4 0.7472 0.4041 
1+2+3+4+5 0.7506 0.4254 
1+2+3+4+5+6 0.7485 0.4288 
MST 1+2 0.2080 0.1300 
1+2+3 0.7366 0.4054 
1+2+3+4 0.7468 0.4071 
1+2+3+4+5 0.7494 0.4288 
1+2+3+4+5+6 0.7460 0.4309 
Table 4: Performance Using Fine-grained Rela-
tions. 
Based on the MIRA leaning algorithm, the 
Eisner algorithm and MST algorithm are used to 
parse the test documents respectively. Referring 
to the evaluation of syntactic dependency parsing, 
30
we use unlabeled accuracy to calculate the ratio 
of EDUs that correctly identify their heads, la-
beled accuracy the ratio of EDUs that have both 
correct heads and correct relations. Table 3 and 
Table 4 show the performance on two relation 
sets. The numbers (1-6) represent the corre-
sponding feature types described in Section 4.1.  
From Table 3 and Table 4, we can see that the 
addition of more feature types, except the 6th fea-
ture type (semantic similarity), can promote the 
performance of relation labeling, whether using 
the coarse-grained 19 relations and the fine-
grained 111 relations. As expected, the first and 
second types of features (WORD and POS) are 
the ones which play an important role in building 
and labeling the discourse dependency trees. 
These two types of features attain similar per-
formance on two relation sets. The Eisner algo-
rithm can achieve unlabeled accuracy around 
0.36 and labeled accuracy around 0.26, while 
MST algorithm achieves unlabeled accuracy 
around 0.20 and labeled accuracy around 0.14. 
The third feature type (Position) is also very 
helpful to discourse parsing. With the addition of 
this feature type, both unlabeled accuracy and 
labeled accuracy exhibit a marked increase. Es-
pecially, when applying MST algorithm on dis-
course parsing, unlabeled accuracy rises from 
around 0.20 to around 0.73. This result is con-
sistent with Hernault?s work (2010b) whose ex-
periments have exhibited the usefulness of those 
position-related features. The other two types of 
features which are related to length and syntactic 
parsing, only promote the performance slightly.  
As we employed the MIRA learning algorithm, 
it is possible to identify which specific features 
are useful, by looking at the weights learned to 
each feature using the training data. Table 5 se-
lects 10 features with the highest weights in ab-
solute value for the parser which uses the coarse-
grained relations, while Table 6 selects the top 
10 features for the parser using the fine-grained 
relations. Each row denotes one feature: the left 
part before the symbol ?&? is from one of the 6 
feature types and the right part denotes a specific 
relation. From Table 5 and Table 6, we can see 
that some features are reasonable. For example, 
The sixth feature in Table 5 represents that the 
dependency relation is preferred to be labeled 
Explanation with the fact that ?because? is the 
first word of the dependent EDU. From these 
two tables, we also observe that most of the 
heavily weighted features are usually related to 
those highly distributed relations. When using 
the coarse-grained relations, the popular relations 
(eg. Elaboration, Attribution and Joint) are al-
ways preferred to be labeled. When using the 
fine-grained relations, the large relations includ-
ing List and Elaboration-object-attribute-e are 
given the precedence of labeling. This phenome-
non is mainly caused by the sparseness of the 
training corpus and the imbalance of relations. 
To solve this problem, the augment of training 
corpus is necessary. 
 
 Feature description Weight 
1 
Last two words in dependent EDU are  
?appeals court?  & Joint 
0.475 
2 
First word in dependent EDU is ?racked? 
& Elaboration 
0.445 
3 
First two words in head EDU are ?I ?d? 
& Attribution 
0.324 
4 
Last word in dependent EDU is ?in?  
& Elaboration 
-0.323 
5 
The res_similarity between two EDUs is 0  
& Elaboration 
0.322 
6 
First word in dependent EDU is ?because? 
& Explanation 
0.306 
7 First POS in head EDU is ?DT? & Joint -0.299 
8 
First two words in dependent EDU are ?that 
required? & Elaboration 
0.287 
9 
First two words in dependent EDU are ?that 
the? & Elaboration 
0.277 
10 
First word in dependent EDU is ?because? 
& Cause 
0.265 
Table 5: Top 10 Feature Weights for Coarse-
grained Relation Labeling (Eisner Algorithm) 
 Features Weight 
1 Last two words in dependent EDU are ?ap-
peals court?  & List 
0.576 
2 First two words in head EDU are ?I ?d?  
& Attribution 
0.385 
3 First two words in dependent EDU is ?that 
the? & Elaboration-object-attribute-e 
0.348 
4 First POS in head EDU is ?DT? & List -0.323 
5 Last word in dependent EDU is ?in? & List -0.286 
6 First word in dependent EDU is ?racked? & 
Elaboration-object-attribute-e 
0.445 
7 First two word pairs are <?In an?,?But 
even?>  & List 
-0.252 
8 Dependent EDU has a dominating node 
tagged ?CD?& Elaboration-object-attribute-e 
-0.244 
9 First two words in dependent EDU are ?pa-
tents disputes? & Purpose 
0.231 
10 First word in dependent EDU is ?to?  
& Purpose 
0.230 
Table 6: Top 10 Feature Weights for Coarse-
grained Relation Labeling (Eisner Algorithm) 
Unlike previous discourse parsing approaches, 
our methods combine tree building and relation 
labeling into a uniform framework naturally. 
This means that relations play a role in building 
the dependency tree structure. From Table 3 and 
Table 4, we can see that fine-grained relations 
are more helpful to building unlabeled discourse 
31
trees more than the coarse-grained relations. The 
best result of unlabeled accuracy using 111 rela-
tions is 0.7506, better than the best performance 
(0.7447) using 19 relations. We can also see that 
the labeled accuracy using the fine-grained rela-
tions can achieve 0.4309, only 0.06 lower than 
the best labeled accuracy (0.4915) using the 
coarse-grained relations. 
In addition, comparing the MST algorithm 
with the Eisner algorithm, Table 3 and Table 4 
show that their performances are not significant-
ly different from each other. But we think that 
MST algorithm has more potential in discourse 
dependency parsing, because our converted dis-
course dependency treebank contains only pro-
jective trees and somewhat suppresses the MST 
algorithm to exhibit its advantage of parsing non-
projective trees. In fact, we observe that some 
non-projective dependencies produced by the 
MST algorithm are even reasonable than what 
they are in the dependency treebank. Thus, it is 
important to build a manually labeled discourse 
dependency treebank, which will be our future 
work. 
5.3 Comparison with Other Systems  
The state-of-the-art discourse parsing methods 
normally produce the constituency based dis-
course trees. To comprehensively evaluate the 
performance of a labeled constituency tree, the 
blank tree structure (?S?), the tree structure with 
nuclearity indication (?N?), and the tree structure 
with rhetorical relation indication but no nuclear-
ity indication (?R?) are evaluated respectively 
using the F measure (Marcu 2000).  
To compare our discourse parsers with others, 
we adopt MIRA and Eisner algorithm to conduct 
discourse parsing with all the 6 types of features 
and then convert the produced projective de-
pendency trees to constituency based trees 
through their correspondence as stated in Section 
2. Our parsers using two relation sets are named 
Our-coarse and Our-fine respectively. The in-
putted EDUs of our parsers are from the standard 
segmentation of RST-DT. Other text-level dis-
course parsing methods include: (1) Percep-
coarse: we replace MIRA with the averaged per-
ceptron learning algorithm and the other settings 
are the same with Our-coarse; (2) HILDA-
manual and HILDA-seg are from Hernault 
(2010b)?s work, and their inputted EDUs are 
from RST-DT and their own EDU segmenter 
respectively; (3) LeThanh indicates the results 
given by LeThanh el al. (2004), which built a 
multi-level rule based parser and used 14 rela-
tions evaluated on 21 documents from RST-DT; 
(4) Marcu denotes the results given by Mar-
cu(2000)?s decision-tree based parser which used 
15 relations evaluated on unspecified documents.  
Table 7 shows the performance comparison 
for all the parsers mentioned above. Human de-
notes the manual agreement between two human 
annotators. From this table, we can see that both 
our parsers perform better than all the other 
parsers as a whole, though our parsers are not 
developed directly for constituency based trees. 
Our parsers do not exhibit obvious advantage 
than HILDA-manual on labeling the blank tree 
structure, because our parsers and HILDA-
manual all perform over 94% of Human and this 
performance level somewhat reaches a bottle-
neck to promote more. However, our parsers 
outperform the other parsers on both nuclearity 
and relation labeling. Our-coarse achieves 94.2% 
and 91.8% of the human F-scores, on labeling 
nuclearity and relation respectively, while Our-
fine achieves 95.2% and 87.6%. We can also see 
that the averaged perceptron learning algorithm, 
though simple, can achieve a comparable per-
formance, better than HILDA-manual. The 
parsers HILDA-seg, LeThanh and Marcu use 
their own automatic EDU segmenters and exhibit 
a relatively low performance. This means that 
EDU segmentation is important to a practical 
discourse parser and worth further investigation. 
  
 S N R 
Our-coarse 82.9 73.0 60.6 
Our-fine 83.4 73.8 57.8 
Percep-coarse 82.3 72.6 59.4 
HILDA-manual 83.0 68.4 55.3 
HILDA-seg 72.3 59.1 47.8 
LeThanh 53.7 47.1 39.9 
Marcu 44.8 30.9 18.8 
Human 88.1 77.5 66.0 
Table 7: Full Parser Evaluation 
 MAFS WAFS Acc 
Our-coarse 0.454 0.643 66.84 
Percep-coarse 0.438 0.633 65.37 
Feng 0.440 0.607 65.30 
HILDA-manual 0.428 0.604 64.18 
Baseline - - 35.82 
Table 8: Relation Labeling Performance  
To further compare the performance of rela-
tion labeling, we follow Hernault el al. (2010a) 
and use Macro-averaged F-score (MAFS) to 
evaluate each relation. Due to space limitation, 
we do not list the F scores for each relation. 
Macro-averaged F-score is not influenced by the 
number of instances that are contained in each 
32
relation. Weight-averaged F-score (WAFS) 
weights the performance of each relation by the 
number of its existing instances. Table 8 com-
pares our parser Our-coarse with other parsers 
HILDA-manual, Feng (Feng and Hirst, 2012) 
and Baseline. Feng (Feng and Hirst, 2012) can 
be seen as a strengthened version of HILDA 
which adopts more features and conducts feature 
selection. Baseline always picks the most fre-
quent relation (i.e. Elaboration). From the results, 
we find that Our-coarse consistently provides 
superior performance for most relations over 
other parsers, and therefore results in higher 
MAFS and WAFS.  
6 Related Work 
So far, the existing discourse parsing techniques 
are mainly based on two well-known treebanks. 
One is the Penn Discourse TreeBank (PDTB) 
(Prasad et al, 2007) and the other is RST-DT.  
PDTB adopts the predicate-arguments repre-
sentation by taking an implicit/explicit connec-
tive as a predication of two adjacent sentences 
(arguments). Then the discourse relation between 
each pair of sentences is annotated independently 
to characterize its predication. A majority of re-
searches regard discourse parsing as a classifica-
tion task and mainly focus on exploiting various 
linguistic features and classifiers when using 
PDTB (Wellner et al, 2006; Pitler et al, 2009; 
Wang et al, 2010). However, the predicate-
arguments annotation scheme itself has such a 
limitation that one can only obtain the local dis-
course relations without knowing the rich context. 
In contrast, RST and its treebank enable peo-
ple to derive a complete representation of the 
whole discourse. Researches have begun to in-
vestigate how to construct a RST tree for the 
given text. Since the RST tree is similar to the 
constituency based syntactic tree except that the 
constituent nodes are different, the syntactic 
parsing techniques have been borrowed for dis-
course parsing (Soricut and Marcu, 2003; 
Baldridge and Lascarides, 2005; Sagae, 2009; 
Hernault et al, 2010b; Feng and Hirst, 2012). 
Soricut and Marcu (2003) use a standard bottom-
up chart parsing algorithm to determine the dis-
course structure of sentences. Baldridge and Las-
carides (2005) model the process of discourse 
parsing with the probabilistic head driven parsing 
techniques. Sagae (2009) apply a transition based 
constituent parsing approach to construct a RST 
tree for a document. Hernault et al (2010b) de-
velop a greedy bottom-up tree building strategy 
for discourse parsing. The two adjacent text 
spans with the closest relations are combined in 
each iteration. As the extension of Hernault?s 
work, Feng and Hirst (2012) further explore var-
ious features aiming to achieve better perfor-
mance. However, as analyzed in Section 1, there 
exist three limitations with the constituency 
based discourse representation and parsing. We 
innovatively adopt the dependency structure, 
which can be benefited from the existing RST-
DT, to represent the discourse. To the best of our 
knowledge, this work is the first to apply de-
pendency structure and dependency parsing 
techniques in discourse analysis. 
7 Conclusions 
In this paper, we present the benefits and feasi-
bility of applying dependency structure in text-
level discourse parsing. Through the correspond-
ence between constituency-based trees and de-
pendency trees, we build a discourse dependency 
treebank by converting the existing RST-DT. 
Based on dependency structure, we are able to 
directly analyze the relations between the EDUs 
without worrying about the additional interior 
text spans, and apply the existing state-of-the-art 
dependency parsing techniques which have a 
relatively low time complexity. In our work, we 
use the graph based dependency parsing tech-
niques learned from the annotated dependency 
trees. The Eisner algorithm and the MST algo-
rithm are applied to parse the optimal projective 
and non-projective dependency trees respectively 
based on the arc-factored model. To calculate the 
score for each arc, six types of features are ex-
plored to represent the arcs and the feature 
weights are learned based on the MIRA learning 
technique. Experimental results exhibit the effec-
tiveness of the proposed approaches. In the fu-
ture, we will focus on non-projective discourse 
dependency parsing and explore more effective 
features. 
Acknowledgments 
This work was partially supported by National 
High Technology Research and Development 
Program of China (No. 2012AA011101), Na-
tional Key Basic Research Program of China (No. 
2014CB340504), National Natural Science 
Foundation of China (No. 61273278), and Na-
tional Key Technology R&D Program (No: 
2011BAH10B04-03). We also thank the three 
anonymous reviewers for their helpful comments. 
33
References 
Jason Baldridge and Alex Lascarides. 2005. Probabil-
istic Head-driven Parsing for Discourse Structure. 
In Proceedings of the Ninth Conference on Com-
putational Natural Language Learning, pages 96?
103. 
Steven Bird, Ewan Klein, and Edward Loper. 2009. 
Natural Language Processing with Python ? Ana-
lyzing Text with the Natural Language Toolkit. 
O?Reilly. 
Lynn Carlson, Daniel Marcu, and Mary E. Okurowski. 
2001. Building a Discourse-tagged Corpus in the 
Framework of Rhetorical Structure Theory. Pro-
ceedings of the Second SIGdial Workshop on Dis-
course and Dialogue-Volume 16, pages 1?10. 
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the 
Shortest Arborescence of a Directed Graph, Sci-
ence Sinica, v.14, pp.1396-1400.  
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative Online Algorithms for Multiclass Prob-
lems. JMLR. 
Jack Edmonds. 1967. Optimum Branchings, J. Re-
search of the National Bureau of Standards, 71B, 
pp.233-240.  
Jason Eisner. 1996. Three New Probabilistic Models 
for Dependency Parsing: An Exploration. In Proc. 
COLING. 
Vanessa Wei Feng and Graeme Hirst. Text-level Dis-
course Parsing with Rich Linguistic Features, Pro-
ceedings of the 50th Annual Meeting of the 
Association for Computational Linguistics, pages 
60?68, Jeju, Republic of Korea, 8-14 July 2012. 
Hugo Hernault, Danushka Bollegala, and Mitsuru 
Ishizuka. 2010a. A Semi-supervised Approach to 
Improve Classification of Infrequent Discourse Re-
lations Using Feature Vector Extension. In Pro-
ceedings of the 2010 Conference on Empirical 
Methods in Natural Language Processing, pages 
399?409, Cambridge, MA, October. Association 
for Computational Linguistics. 
Hugo Hernault, Helmut Prendinger, David A. duVerle, 
and Mitsuru Ishizuka. 2010b. HILDA: A Discourse 
Parser Using Support Vector Machine Classifica-
tion. Dialogue and Discourse, 1(3):1?33. 
Shafiq Joty, Giuseppe Carenini and Raymond T. Ng. 
A Novel Discriminative Framework for Sentence-
level Discourse Analysis. EMNLP-CoNLL '12 
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing 
and Computational Natural Language Learning 
Stroudsburg, PA, USA. 
Huong LeThanh, Geetha Abeysinghe, and Christian 
Huyck. 2004. Generating Discourse Structures for 
Written Texts. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics, 
pages 329? 335. 
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. 
Recognizing Implicit Discourse Relations in the 
Penn Discourse Treebank. In Proceedings of the 
2009 Conference on Empirical Method in Natural 
Language Processing, Vol. 1, EMNLP?09, pages 
343-351. 
William Mann and Sandra Thompson. 1988. Rhetori-
cal Structure Theory: Toward a Functional Theory 
of Text Organization. Text, 8(3):243?281. 
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press, 
Cambridge, MA, USA. 
Ryan McDonald, Koby Crammer, and Fernando Pe-
reira. 2005a. Online Large-Margin Training of De-
pendency Parsers, 43rd Annual Meeting of the 
Association for Computational Linguistics (ACL 
2005) .  
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and 
Jan Hajic. 2005b. Non-projective Dependency 
Parsing using Spanning Tree Algorithms, Proceed-
ings of HLT/EMNLP 2005. 
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. 
Automatic Sense Prediction for Implicit Discourse 
Relations in Text, In Proc. of the 47th ACL. pages 
683-691.  
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan 
Lee, Aravind Joshi, Livio Robaldo, and Bonnie 
Webber. 2007. The Penn Discourse Treebank 2.0 
Annotation Manual. The PDTB Research Group, 
December. 
David Reitter. 2003. Simple Signals for Complex 
Rhetorics: On Rhetorical Analysis with Rich-
feature Support Vector Models. LDV Forum, 
18(1/2):38?52. 
Kenji Sagae. 2009. Analysis of discourse structure 
with syntactic dependencies and data-driven shift-
reduce parsing. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, pages 
81-84. 
Radu Soricut and Daniel Marcu. 2003. Sentence level 
discourse parsing using syntactic and lexical in-
formation. In Proceedings of the 2003 Conference 
34
of the North American Chapter of the Association 
for Computational Linguistics on Human Lan-
guage Technology, Volume 1, pages 149?156. 
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic in-
formation. In Proceedings of Human Language 
Technologies: The 2009 Annual Conference of the 
North American Chapter of the Association for 
Computational Linguistics, pages 566?574. 
Robert Endre Tarjan, 1977. Finding Optimum 
Branchings, Networks, v.7, pp.25-35. 
Ben Taskar, Carlos Guestrin and Daphne Koller. 2003. 
Max-margin Markov Networks. In Proc. NIPS. 
Bonnie Webber. 2004. D-LTAG: Extending Lexical-
ized TAG to Discourse. Cognitive Science, 
28(5):751?779. 
Wen Ting Wang, Jian Su and Chew Lim Tan. 2010. 
Kernel based Discourse Relation Recognition with 
Temporal Ordering Information, In Proc. of 
ACL?10. pages 710-719. 
Ben Wellner, James Pustejovsky, Catherine Havasi, 
Anna Rumshisky and Roser Sauri. 2006. Classifi-
cation of Discourse Coherence Relations: an Ex-
ploratory Study Using Multiple Knowledge 
Sources. In Proc.of the 7th SIGDIAL Workshop on 
Discourse and Dialogue. pages 117-125. 
 
35
